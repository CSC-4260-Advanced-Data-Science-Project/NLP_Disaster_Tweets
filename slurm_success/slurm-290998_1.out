Processing dataset: dropped_v2_no_emojis_mentions

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: dropped_v2_no_emojis_mentions (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on dropped_v2_no_emojis_mentions...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on dropped_v2_no_emojis_mentions...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on dropped_v2_no_emojis_mentions...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on dropped_v2_no_emojis_mentions...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on dropped_v2_no_emojis_mentions...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67741800
Iteration 2, loss = 0.61863098
Iteration 3, loss = 0.51918956
Iteration 4, loss = 0.40767983
Iteration 5, loss = 0.30779628
Iteration 6, loss = 0.23081065
Iteration 7, loss = 0.17630143
Iteration 8, loss = 0.13852555
Iteration 9, loss = 0.11260803
Iteration 10, loss = 0.09404782
Iteration 11, loss = 0.08075305
Iteration 12, loss = 0.07043408
Iteration 13, loss = 0.06306222
Iteration 14, loss = 0.05699390
Iteration 15, loss = 0.05230874
Iteration 16, loss = 0.04860994
Iteration 17, loss = 0.04538552
Iteration 18, loss = 0.04287588
Iteration 19, loss = 0.04071030
Iteration 20, loss = 0.03878518
Iteration 21, loss = 0.03734552
Iteration 22, loss = 0.03610170
Iteration 23, loss = 0.03467749
Iteration 24, loss = 0.03355011
Iteration 25, loss = 0.03272496
Iteration 26, loss = 0.03185551
Iteration 27, loss = 0.03134207
Iteration 28, loss = 0.03061143
Iteration 29, loss = 0.03023631
Iteration 30, loss = 0.02978285
Iteration 31, loss = 0.02918650
Iteration 32, loss = 0.02885242
Iteration 33, loss = 0.02845809
Iteration 34, loss = 0.02807629
Iteration 35, loss = 0.02808593
Iteration 36, loss = 0.02736313
Iteration 37, loss = 0.02750820
Iteration 38, loss = 0.02711953
Iteration 39, loss = 0.02674621
Iteration 40, loss = 0.02652418
Iteration 41, loss = 0.02626851
Iteration 42, loss = 0.02621637
Iteration 43, loss = 0.02594169
Iteration 44, loss = 0.02607058
Iteration 45, loss = 0.02583380
Iteration 46, loss = 0.02585828
Iteration 47, loss = 0.02554443
Iteration 48, loss = 0.02524442
Iteration 49, loss = 0.02552158
Iteration 50, loss = 0.02533756
Iteration 51, loss = 0.02498520
Iteration 52, loss = 0.02524899
Iteration 53, loss = 0.02493107
Iteration 54, loss = 0.02490425
Iteration 55, loss = 0.02483402
Iteration 56, loss = 0.02477356
Iteration 57, loss = 0.02467679
Iteration 58, loss = 0.02463641
Iteration 59, loss = 0.02480722
Iteration 60, loss = 0.02465873
Iteration 61, loss = 0.02459688
Iteration 62, loss = 0.02451715
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67397732
Iteration 2, loss = 0.60290764
Iteration 3, loss = 0.49252037
Iteration 4, loss = 0.38011404
Iteration 5, loss = 0.28630398
Iteration 6, loss = 0.21539278
Iteration 7, loss = 0.16560123
Iteration 8, loss = 0.13158851
Iteration 9, loss = 0.10759500
Iteration 10, loss = 0.09080484
Iteration 11, loss = 0.07877378
Iteration 12, loss = 0.06930715
Iteration 13, loss = 0.06189577
Iteration 14, loss = 0.05672506
Iteration 15, loss = 0.05208211
Iteration 16, loss = 0.04855335
Iteration 17, loss = 0.04553453
Iteration 18, loss = 0.04275653
Iteration 19, loss = 0.04083563
Iteration 20, loss = 0.03905898
Iteration 21, loss = 0.03749234
Iteration 22, loss = 0.03621375
Iteration 23, loss = 0.03504324
Iteration 24, loss = 0.03418689
Iteration 25, loss = 0.03342143
Iteration 26, loss = 0.03233627
Iteration 27, loss = 0.03166008
Iteration 28, loss = 0.03089828
Iteration 29, loss = 0.03054657
Iteration 30, loss = 0.02996864
Iteration 31, loss = 0.02957771
Iteration 32, loss = 0.02923690
Iteration 33, loss = 0.02893365
Iteration 34, loss = 0.02877332
Iteration 35, loss = 0.02834044
Iteration 36, loss = 0.02795514
Iteration 37, loss = 0.02789999
Iteration 38, loss = 0.02783321
Iteration 39, loss = 0.02730571
Iteration 40, loss = 0.02762836
Iteration 41, loss = 0.02712335
Iteration 42, loss = 0.02662421
Iteration 43, loss = 0.02651538
Iteration 44, loss = 0.02638336
Iteration 45, loss = 0.02642882
Iteration 46, loss = 0.02630525
Iteration 47, loss = 0.02629266
Iteration 48, loss = 0.02638096
Iteration 49, loss = 0.02630481
Iteration 50, loss = 0.02561748
Iteration 51, loss = 0.02594326
Iteration 52, loss = 0.02569892
Iteration 53, loss = 0.02546968
Iteration 54, loss = 0.02551062
Iteration 55, loss = 0.02533998
Iteration 56, loss = 0.02526799
Iteration 57, loss = 0.02530422
Iteration 58, loss = 0.02516597
Iteration 59, loss = 0.02543028
Iteration 60, loss = 0.02506760
Iteration 61, loss = 0.02509243
Iteration 62, loss = 0.02511476
Iteration 63, loss = 0.02519195
Iteration 64, loss = 0.02512768
Iteration 65, loss = 0.02497621
Iteration 66, loss = 0.02478380
Iteration 67, loss = 0.02475053
Iteration 68, loss = 0.02497145
Iteration 69, loss = 0.02505588
Iteration 70, loss = 0.02506111
Iteration 71, loss = 0.02457352
Iteration 72, loss = 0.02473603
Iteration 73, loss = 0.02493426
Iteration 74, loss = 0.02471725
Iteration 75, loss = 0.02479936
Iteration 76, loss = 0.02477220
Iteration 77, loss = 0.02476599
Iteration 78, loss = 0.02463080
Iteration 79, loss = 0.02468954
Iteration 80, loss = 0.02475826
Iteration 81, loss = 0.02463772
Iteration 82, loss = 0.02442057
Iteration 83, loss = 0.02463989
Iteration 84, loss = 0.02500211
Iteration 85, loss = 0.02432248
Iteration 86, loss = 0.02451502
Iteration 87, loss = 0.02426417
Iteration 88, loss = 0.02418943
Iteration 89, loss = 0.02440416
Iteration 90, loss = 0.02432644
Iteration 91, loss = 0.02414013
Iteration 92, loss = 0.02430187
Iteration 93, loss = 0.02424004
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71455058
Iteration 2, loss = 0.63112642
Iteration 3, loss = 0.52043924
Iteration 4, loss = 0.39472362
Iteration 5, loss = 0.29097350
Iteration 6, loss = 0.21542553
Iteration 7, loss = 0.16367496
Iteration 8, loss = 0.12844733
Iteration 9, loss = 0.10444041
Iteration 10, loss = 0.08738052
Iteration 11, loss = 0.07479905
Iteration 12, loss = 0.06559468
Iteration 13, loss = 0.05844115
Iteration 14, loss = 0.05293985
Iteration 15, loss = 0.04842668
Iteration 16, loss = 0.04507562
Iteration 17, loss = 0.04166806
Iteration 18, loss = 0.03937445
Iteration 19, loss = 0.03711432
Iteration 20, loss = 0.03537421
Iteration 21, loss = 0.03393298
Iteration 22, loss = 0.03243309
Iteration 23, loss = 0.03144747
Iteration 24, loss = 0.03067209
Iteration 25, loss = 0.02998870
Iteration 26, loss = 0.02895179
Iteration 27, loss = 0.02823261
Iteration 28, loss = 0.02761388
Iteration 29, loss = 0.02709441
Iteration 30, loss = 0.02657644
Iteration 31, loss = 0.02612218
Iteration 32, loss = 0.02573435
Iteration 33, loss = 0.02554509
Iteration 34, loss = 0.02535650
Iteration 35, loss = 0.02506548
Iteration 36, loss = 0.02449407
Iteration 37, loss = 0.02407524
Iteration 38, loss = 0.02402234
Iteration 39, loss = 0.02385509
Iteration 40, loss = 0.02358029
Iteration 41, loss = 0.02349413
Iteration 42, loss = 0.02339560
Iteration 43, loss = 0.02308719
Iteration 44, loss = 0.02299215
Iteration 45, loss = 0.02307809
Iteration 46, loss = 0.02294303
Iteration 47, loss = 0.02269074
Iteration 48, loss = 0.02280706
Iteration 49, loss = 0.02274396
Iteration 50, loss = 0.02253509
Iteration 51, loss = 0.02207084
Iteration 52, loss = 0.02220548
Iteration 53, loss = 0.02204809
Iteration 54, loss = 0.02224515
Iteration 55, loss = 0.02201028
Iteration 56, loss = 0.02217868
Iteration 57, loss = 0.02222721
Iteration 58, loss = 0.02192858
Iteration 59, loss = 0.02193615
Iteration 60, loss = 0.02179359
Iteration 61, loss = 0.02165234
Iteration 62, loss = 0.02143028
Iteration 63, loss = 0.02147479
Iteration 64, loss = 0.02140205
Iteration 65, loss = 0.02155177
Iteration 66, loss = 0.02145105
Iteration 67, loss = 0.02124729
Iteration 68, loss = 0.02127439
Iteration 69, loss = 0.02115176
Iteration 70, loss = 0.02114430
Iteration 71, loss = 0.02110314
Iteration 72, loss = 0.02109400
Iteration 73, loss = 0.02119537
Iteration 74, loss = 0.02099116
Iteration 75, loss = 0.02122623
Iteration 76, loss = 0.02120366
Iteration 77, loss = 0.02112249
Iteration 78, loss = 0.02133908
Iteration 79, loss = 0.02098190
Iteration 80, loss = 0.02101927
Iteration 81, loss = 0.02083219
Iteration 82, loss = 0.02106741
Iteration 83, loss = 0.02082760
Iteration 84, loss = 0.02081345
Iteration 85, loss = 0.02119801
Iteration 86, loss = 0.02098032
Iteration 87, loss = 0.02080227
Iteration 88, loss = 0.02107507
Iteration 89, loss = 0.02108528
Iteration 90, loss = 0.02101240
Iteration 91, loss = 0.02097777
Iteration 92, loss = 0.02092055
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67504891
Iteration 2, loss = 0.60372863
Iteration 3, loss = 0.49524789
Iteration 4, loss = 0.38388943
Iteration 5, loss = 0.28855874
Iteration 6, loss = 0.21638259
Iteration 7, loss = 0.16531992
Iteration 8, loss = 0.13050664
Iteration 9, loss = 0.10603018
Iteration 10, loss = 0.08873860
Iteration 11, loss = 0.07610822
Iteration 12, loss = 0.06687360
Iteration 13, loss = 0.05959429
Iteration 14, loss = 0.05379920
Iteration 15, loss = 0.04927905
Iteration 16, loss = 0.04573138
Iteration 17, loss = 0.04265394
Iteration 18, loss = 0.03997897
Iteration 19, loss = 0.03783587
Iteration 20, loss = 0.03594888
Iteration 21, loss = 0.03450827
Iteration 22, loss = 0.03306827
Iteration 23, loss = 0.03217183
Iteration 24, loss = 0.03119651
Iteration 25, loss = 0.03055573
Iteration 26, loss = 0.02961504
Iteration 27, loss = 0.02864186
Iteration 28, loss = 0.02824469
Iteration 29, loss = 0.02775486
Iteration 30, loss = 0.02739004
Iteration 31, loss = 0.02669663
Iteration 32, loss = 0.02625628
Iteration 33, loss = 0.02622054
Iteration 34, loss = 0.02561640
Iteration 35, loss = 0.02545011
Iteration 36, loss = 0.02491390
Iteration 37, loss = 0.02502983
Iteration 38, loss = 0.02458542
Iteration 39, loss = 0.02447351
Iteration 40, loss = 0.02422002
Iteration 41, loss = 0.02397225
Iteration 42, loss = 0.02405282
Iteration 43, loss = 0.02373145
Iteration 44, loss = 0.02360567
Iteration 45, loss = 0.02356183
Iteration 46, loss = 0.02340478
Iteration 47, loss = 0.02315376
Iteration 48, loss = 0.02297867
Iteration 49, loss = 0.02312364
Iteration 50, loss = 0.02296081
Iteration 51, loss = 0.02267837
Iteration 52, loss = 0.02264926
Iteration 53, loss = 0.02271004
Iteration 54, loss = 0.02265571
Iteration 55, loss = 0.02243042
Iteration 56, loss = 0.02242283
Iteration 57, loss = 0.02235591
Iteration 58, loss = 0.02229212
Iteration 59, loss = 0.02230693
Iteration 60, loss = 0.02218086
Iteration 61, loss = 0.02217996
Iteration 62, loss = 0.02209474
Iteration 63, loss = 0.02195746
Iteration 64, loss = 0.02201949
Iteration 65, loss = 0.02188059
Iteration 66, loss = 0.02203837
Iteration 67, loss = 0.02161768
Iteration 68, loss = 0.02166716
Iteration 69, loss = 0.02176281
Iteration 70, loss = 0.02182667
Iteration 71, loss = 0.02163666
Iteration 72, loss = 0.02174372
Iteration 73, loss = 0.02161608
Iteration 74, loss = 0.02163109
Iteration 75, loss = 0.02154367
Iteration 76, loss = 0.02149853
Iteration 77, loss = 0.02159459
Iteration 78, loss = 0.02136957
Iteration 79, loss = 0.02138862
Iteration 80, loss = 0.02139923
Iteration 81, loss = 0.02105197
Iteration 82, loss = 0.02140059
Iteration 83, loss = 0.02148802
Iteration 84, loss = 0.02138937
Iteration 85, loss = 0.02142549
Iteration 86, loss = 0.02117078
Iteration 87, loss = 0.02145627
Iteration 88, loss = 0.02119964
Iteration 89, loss = 0.02160136
Iteration 90, loss = 0.02152731
Iteration 91, loss = 0.02123433
Iteration 92, loss = 0.02114587
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70764796
Iteration 2, loss = 0.63922478
Iteration 3, loss = 0.53660252
Iteration 4, loss = 0.41683577
Iteration 5, loss = 0.31098721
Iteration 6, loss = 0.23117117
Iteration 7, loss = 0.17473648
Iteration 8, loss = 0.13662401
Iteration 9, loss = 0.11046793
Iteration 10, loss = 0.09188805
Iteration 11, loss = 0.07863500
Iteration 12, loss = 0.06855489
Iteration 13, loss = 0.06098334
Iteration 14, loss = 0.05483455
Iteration 15, loss = 0.05009208
Iteration 16, loss = 0.04635099
Iteration 17, loss = 0.04348453
Iteration 18, loss = 0.04078525
Iteration 19, loss = 0.03850437
Iteration 20, loss = 0.03650875
Iteration 21, loss = 0.03497275
Iteration 22, loss = 0.03375169
Iteration 23, loss = 0.03260328
Iteration 24, loss = 0.03149755
Iteration 25, loss = 0.03074786
Iteration 26, loss = 0.02984254
Iteration 27, loss = 0.02901828
Iteration 28, loss = 0.02842437
Iteration 29, loss = 0.02784084
Iteration 30, loss = 0.02717444
Iteration 31, loss = 0.02697079
Iteration 32, loss = 0.02649209
Iteration 33, loss = 0.02601405
Iteration 34, loss = 0.02578825
Iteration 35, loss = 0.02565172
Iteration 36, loss = 0.02572028
Iteration 37, loss = 0.02509193
Iteration 38, loss = 0.02491494
Iteration 39, loss = 0.02467730
Iteration 40, loss = 0.02450810
Iteration 41, loss = 0.02406602
Iteration 42, loss = 0.02410101
Iteration 43, loss = 0.02383507
Iteration 44, loss = 0.02372389
Iteration 45, loss = 0.02344422
Iteration 46, loss = 0.02339207
Iteration 47, loss = 0.02334248
Iteration 48, loss = 0.02364456
Iteration 49, loss = 0.02304035
Iteration 50, loss = 0.02328944
Iteration 51, loss = 0.02293495
Iteration 52, loss = 0.02262314
Iteration 53, loss = 0.02259481
Iteration 54, loss = 0.02264118
Iteration 55, loss = 0.02252032
Iteration 56, loss = 0.02250707
Iteration 57, loss = 0.02222177
Iteration 58, loss = 0.02248856
Iteration 59, loss = 0.02239905
Iteration 60, loss = 0.02226761
Iteration 61, loss = 0.02235871
Iteration 62, loss = 0.02224744
Iteration 63, loss = 0.02209793
Iteration 64, loss = 0.02218232
Iteration 65, loss = 0.02189340
Iteration 66, loss = 0.02197211
Iteration 67, loss = 0.02179622
Iteration 68, loss = 0.02180012
Iteration 69, loss = 0.02172782
Iteration 70, loss = 0.02183088
Iteration 71, loss = 0.02191335
Iteration 72, loss = 0.02186949
Iteration 73, loss = 0.02183951
Iteration 74, loss = 0.02182890
Iteration 75, loss = 0.02174435
Iteration 76, loss = 0.02172092
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67741800
Iteration 2, loss = 0.61863098
Iteration 3, loss = 0.51918956
Iteration 4, loss = 0.40767983
Iteration 5, loss = 0.30779628
Iteration 6, loss = 0.23081065
Iteration 7, loss = 0.17630143
Iteration 8, loss = 0.13852555
Iteration 9, loss = 0.11260803
Iteration 10, loss = 0.09404782
Iteration 11, loss = 0.08075305
Iteration 12, loss = 0.07043408
Iteration 13, loss = 0.06306222
Iteration 14, loss = 0.05699390
Iteration 15, loss = 0.05230874
Iteration 16, loss = 0.04860994
Iteration 17, loss = 0.04538552
Iteration 18, loss = 0.04287588
Iteration 19, loss = 0.04071030
Iteration 20, loss = 0.03878518
Iteration 21, loss = 0.03734552
Iteration 22, loss = 0.03610170
Iteration 23, loss = 0.03467749
Iteration 24, loss = 0.03355011
Iteration 25, loss = 0.03272496
Iteration 26, loss = 0.03185551
Iteration 27, loss = 0.03134207
Iteration 28, loss = 0.03061143
Iteration 29, loss = 0.03023631
Iteration 30, loss = 0.02978285
Iteration 31, loss = 0.02918650
Iteration 32, loss = 0.02885242
Iteration 33, loss = 0.02845809
Iteration 34, loss = 0.02807629
Iteration 35, loss = 0.02808593
Iteration 36, loss = 0.02736313
Iteration 37, loss = 0.02750820
Iteration 38, loss = 0.02711953
Iteration 39, loss = 0.02674621
Iteration 40, loss = 0.02652418
Iteration 41, loss = 0.02626851
Iteration 42, loss = 0.02621637
Iteration 43, loss = 0.02594169
Iteration 44, loss = 0.02607058
Iteration 45, loss = 0.02583380
Iteration 46, loss = 0.02585828
Iteration 47, loss = 0.02554443
Iteration 48, loss = 0.02524442
Iteration 49, loss = 0.02552158
Iteration 50, loss = 0.02533756
Iteration 51, loss = 0.02498520
Iteration 52, loss = 0.02524899
Iteration 53, loss = 0.02493107
Iteration 54, loss = 0.02490425
Iteration 55, loss = 0.02483402
Iteration 56, loss = 0.02477356
Iteration 57, loss = 0.02467679
Iteration 58, loss = 0.02463641
Iteration 59, loss = 0.02480722
Iteration 60, loss = 0.02465873
Iteration 61, loss = 0.02459688
Iteration 62, loss = 0.02451715
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67397732
Iteration 2, loss = 0.60290764
Iteration 3, loss = 0.49252037
Iteration 4, loss = 0.38011404
Iteration 5, loss = 0.28630398
Iteration 6, loss = 0.21539278
Iteration 7, loss = 0.16560123
Iteration 8, loss = 0.13158851
Iteration 9, loss = 0.10759500
Iteration 10, loss = 0.09080484
Iteration 11, loss = 0.07877378
Iteration 12, loss = 0.06930715
Iteration 13, loss = 0.06189577
Iteration 14, loss = 0.05672506
Iteration 15, loss = 0.05208211
Iteration 16, loss = 0.04855335
Iteration 17, loss = 0.04553453
Iteration 18, loss = 0.04275653
Iteration 19, loss = 0.04083563
Iteration 20, loss = 0.03905898
Iteration 21, loss = 0.03749234
Iteration 22, loss = 0.03621375
Iteration 23, loss = 0.03504324
Iteration 24, loss = 0.03418689
Iteration 25, loss = 0.03342143
Iteration 26, loss = 0.03233627
Iteration 27, loss = 0.03166008
Iteration 28, loss = 0.03089828
Iteration 29, loss = 0.03054657
Iteration 30, loss = 0.02996864
Iteration 31, loss = 0.02957771
Iteration 32, loss = 0.02923690
Iteration 33, loss = 0.02893365
Iteration 34, loss = 0.02877332
Iteration 35, loss = 0.02834044
Iteration 36, loss = 0.02795514
Iteration 37, loss = 0.02789999
Iteration 38, loss = 0.02783321
Iteration 39, loss = 0.02730571
Iteration 40, loss = 0.02762836
Iteration 41, loss = 0.02712335
Iteration 42, loss = 0.02662421
Iteration 43, loss = 0.02651538
Iteration 44, loss = 0.02638336
Iteration 45, loss = 0.02642882
Iteration 46, loss = 0.02630525
Iteration 47, loss = 0.02629266
Iteration 48, loss = 0.02638096
Iteration 49, loss = 0.02630481
Iteration 50, loss = 0.02561748
Iteration 51, loss = 0.02594326
Iteration 52, loss = 0.02569892
Iteration 53, loss = 0.02546968
Iteration 54, loss = 0.02551062
Iteration 55, loss = 0.02533998
Iteration 56, loss = 0.02526799
Iteration 57, loss = 0.02530422
Iteration 58, loss = 0.02516597
Iteration 59, loss = 0.02543028
Iteration 60, loss = 0.02506760
Iteration 61, loss = 0.02509243
Iteration 62, loss = 0.02511476
Iteration 63, loss = 0.02519195
Iteration 64, loss = 0.02512768
Iteration 65, loss = 0.02497621
Iteration 66, loss = 0.02478380
Iteration 67, loss = 0.02475053
Iteration 68, loss = 0.02497145
Iteration 69, loss = 0.02505588
Iteration 70, loss = 0.02506111
Iteration 71, loss = 0.02457352
Iteration 72, loss = 0.02473603
Iteration 73, loss = 0.02493426
Iteration 74, loss = 0.02471725
Iteration 75, loss = 0.02479936
Iteration 76, loss = 0.02477220
Iteration 77, loss = 0.02476599
Iteration 78, loss = 0.02463080
Iteration 79, loss = 0.02468954
Iteration 80, loss = 0.02475826
Iteration 81, loss = 0.02463772
Iteration 82, loss = 0.02442057
Iteration 83, loss = 0.02463989
Iteration 84, loss = 0.02500211
Iteration 85, loss = 0.02432248
Iteration 86, loss = 0.02451502
Iteration 87, loss = 0.02426417
Iteration 88, loss = 0.02418943
Iteration 89, loss = 0.02440416
Iteration 90, loss = 0.02432644
Iteration 91, loss = 0.02414013
Iteration 92, loss = 0.02430187
Iteration 93, loss = 0.02424004
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71455058
Iteration 2, loss = 0.63112642
Iteration 3, loss = 0.52043924
Iteration 4, loss = 0.39472362
Iteration 5, loss = 0.29097350
Iteration 6, loss = 0.21542553
Iteration 7, loss = 0.16367496
Iteration 8, loss = 0.12844733
Iteration 9, loss = 0.10444041
Iteration 10, loss = 0.08738052
Iteration 11, loss = 0.07479905
Iteration 12, loss = 0.06559468
Iteration 13, loss = 0.05844115
Iteration 14, loss = 0.05293985
Iteration 15, loss = 0.04842668
Iteration 16, loss = 0.04507562
Iteration 17, loss = 0.04166806
Iteration 18, loss = 0.03937445
Iteration 19, loss = 0.03711432
Iteration 20, loss = 0.03537421
Iteration 21, loss = 0.03393298
Iteration 22, loss = 0.03243309
Iteration 23, loss = 0.03144747
Iteration 24, loss = 0.03067209
Iteration 25, loss = 0.02998870
Iteration 26, loss = 0.02895179
Iteration 27, loss = 0.02823261
Iteration 28, loss = 0.02761388
Iteration 29, loss = 0.02709441
Iteration 30, loss = 0.02657644
Iteration 31, loss = 0.02612218
Iteration 32, loss = 0.02573435
Iteration 33, loss = 0.02554509
Iteration 34, loss = 0.02535650
Iteration 35, loss = 0.02506548
Iteration 36, loss = 0.02449407
Iteration 37, loss = 0.02407524
Iteration 38, loss = 0.02402234
Iteration 39, loss = 0.02385509
Iteration 40, loss = 0.02358029
Iteration 41, loss = 0.02349413
Iteration 42, loss = 0.02339560
Iteration 43, loss = 0.02308719
Iteration 44, loss = 0.02299215
Iteration 45, loss = 0.02307809
Iteration 46, loss = 0.02294303
Iteration 47, loss = 0.02269074
Iteration 48, loss = 0.02280706
Iteration 49, loss = 0.02274396
Iteration 50, loss = 0.02253509
Iteration 51, loss = 0.02207084
Iteration 52, loss = 0.02220548
Iteration 53, loss = 0.02204809
Iteration 54, loss = 0.02224515
Iteration 55, loss = 0.02201028
Iteration 56, loss = 0.02217868
Iteration 57, loss = 0.02222721
Iteration 58, loss = 0.02192858
Iteration 59, loss = 0.02193615
Iteration 60, loss = 0.02179359
Iteration 61, loss = 0.02165234
Iteration 62, loss = 0.02143028
Iteration 63, loss = 0.02147479
Iteration 64, loss = 0.02140205
Iteration 65, loss = 0.02155177
Iteration 66, loss = 0.02145105
Iteration 67, loss = 0.02124729
Iteration 68, loss = 0.02127439
Iteration 69, loss = 0.02115176
Iteration 70, loss = 0.02114430
Iteration 71, loss = 0.02110314
Iteration 72, loss = 0.02109400
Iteration 73, loss = 0.02119537
Iteration 74, loss = 0.02099116
Iteration 75, loss = 0.02122623
Iteration 76, loss = 0.02120366
Iteration 77, loss = 0.02112249
Iteration 78, loss = 0.02133908
Iteration 79, loss = 0.02098190
Iteration 80, loss = 0.02101927
Iteration 81, loss = 0.02083219
Iteration 82, loss = 0.02106741
Iteration 83, loss = 0.02082760
Iteration 84, loss = 0.02081345
Iteration 85, loss = 0.02119801
Iteration 86, loss = 0.02098032
Iteration 87, loss = 0.02080227
Iteration 88, loss = 0.02107507
Iteration 89, loss = 0.02108528
Iteration 90, loss = 0.02101240
Iteration 91, loss = 0.02097777
Iteration 92, loss = 0.02092055
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67504891
Iteration 2, loss = 0.60372863
Iteration 3, loss = 0.49524789
Iteration 4, loss = 0.38388943
Iteration 5, loss = 0.28855874
Iteration 6, loss = 0.21638259
Iteration 7, loss = 0.16531992
Iteration 8, loss = 0.13050664
Iteration 9, loss = 0.10603018
Iteration 10, loss = 0.08873860
Iteration 11, loss = 0.07610822
Iteration 12, loss = 0.06687360
Iteration 13, loss = 0.05959429
Iteration 14, loss = 0.05379920
Iteration 15, loss = 0.04927905
Iteration 16, loss = 0.04573138
Iteration 17, loss = 0.04265394
Iteration 18, loss = 0.03997897
Iteration 19, loss = 0.03783587
Iteration 20, loss = 0.03594888
Iteration 21, loss = 0.03450827
Iteration 22, loss = 0.03306827
Iteration 23, loss = 0.03217183
Iteration 24, loss = 0.03119651
Iteration 25, loss = 0.03055573
Iteration 26, loss = 0.02961504
Iteration 27, loss = 0.02864186
Iteration 28, loss = 0.02824469
Iteration 29, loss = 0.02775486
Iteration 30, loss = 0.02739004
Iteration 31, loss = 0.02669663
Iteration 32, loss = 0.02625628
Iteration 33, loss = 0.02622054
Iteration 34, loss = 0.02561640
Iteration 35, loss = 0.02545011
Iteration 36, loss = 0.02491390
Iteration 37, loss = 0.02502983
Iteration 38, loss = 0.02458542
Iteration 39, loss = 0.02447351
Iteration 40, loss = 0.02422002
Iteration 41, loss = 0.02397225
Iteration 42, loss = 0.02405282
Iteration 43, loss = 0.02373145
Iteration 44, loss = 0.02360567
Iteration 45, loss = 0.02356183
Iteration 46, loss = 0.02340478
Iteration 47, loss = 0.02315376
Iteration 48, loss = 0.02297867
Iteration 49, loss = 0.02312364
Iteration 50, loss = 0.02296081
Iteration 51, loss = 0.02267837
Iteration 52, loss = 0.02264926
Iteration 53, loss = 0.02271004
Iteration 54, loss = 0.02265571
Iteration 55, loss = 0.02243042
Iteration 56, loss = 0.02242283
Iteration 57, loss = 0.02235591
Iteration 58, loss = 0.02229212
Iteration 59, loss = 0.02230693
Iteration 60, loss = 0.02218086
Iteration 61, loss = 0.02217996
Iteration 62, loss = 0.02209474
Iteration 63, loss = 0.02195746
Iteration 64, loss = 0.02201949
Iteration 65, loss = 0.02188059
Iteration 66, loss = 0.02203837
Iteration 67, loss = 0.02161768
Iteration 68, loss = 0.02166716
Iteration 69, loss = 0.02176281
Iteration 70, loss = 0.02182667
Iteration 71, loss = 0.02163666
Iteration 72, loss = 0.02174372
Iteration 73, loss = 0.02161608
Iteration 74, loss = 0.02163109
Iteration 75, loss = 0.02154367
Iteration 76, loss = 0.02149853
Iteration 77, loss = 0.02159459
Iteration 78, loss = 0.02136957
Iteration 79, loss = 0.02138862
Iteration 80, loss = 0.02139923
Iteration 81, loss = 0.02105197
Iteration 82, loss = 0.02140059
Iteration 83, loss = 0.02148802
Iteration 84, loss = 0.02138937
Iteration 85, loss = 0.02142549
Iteration 86, loss = 0.02117078
Iteration 87, loss = 0.02145627
Iteration 88, loss = 0.02119964
Iteration 89, loss = 0.02160136
Iteration 90, loss = 0.02152731
Iteration 91, loss = 0.02123433
Iteration 92, loss = 0.02114587
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70764796
Iteration 2, loss = 0.63922478
Iteration 3, loss = 0.53660252
Iteration 4, loss = 0.41683577
Iteration 5, loss = 0.31098721
Iteration 6, loss = 0.23117117
Iteration 7, loss = 0.17473648
Iteration 8, loss = 0.13662401
Iteration 9, loss = 0.11046793
Iteration 10, loss = 0.09188805
Iteration 11, loss = 0.07863500
Iteration 12, loss = 0.06855489
Iteration 13, loss = 0.06098334
Iteration 14, loss = 0.05483455
Iteration 15, loss = 0.05009208
Iteration 16, loss = 0.04635099
Iteration 17, loss = 0.04348453
Iteration 18, loss = 0.04078525
Iteration 19, loss = 0.03850437
Iteration 20, loss = 0.03650875
Iteration 21, loss = 0.03497275
Iteration 22, loss = 0.03375169
Iteration 23, loss = 0.03260328
Iteration 24, loss = 0.03149755
Iteration 25, loss = 0.03074786
Iteration 26, loss = 0.02984254
Iteration 27, loss = 0.02901828
Iteration 28, loss = 0.02842437
Iteration 29, loss = 0.02784084
Iteration 30, loss = 0.02717444
Iteration 31, loss = 0.02697079
Iteration 32, loss = 0.02649209
Iteration 33, loss = 0.02601405
Iteration 34, loss = 0.02578825
Iteration 35, loss = 0.02565172
Iteration 36, loss = 0.02572028
Iteration 37, loss = 0.02509193
Iteration 38, loss = 0.02491494
Iteration 39, loss = 0.02467730
Iteration 40, loss = 0.02450810
Iteration 41, loss = 0.02406602
Iteration 42, loss = 0.02410101
Iteration 43, loss = 0.02383507
Iteration 44, loss = 0.02372389
Iteration 45, loss = 0.02344422
Iteration 46, loss = 0.02339207
Iteration 47, loss = 0.02334248
Iteration 48, loss = 0.02364456
Iteration 49, loss = 0.02304035
Iteration 50, loss = 0.02328944
Iteration 51, loss = 0.02293495
Iteration 52, loss = 0.02262314
Iteration 53, loss = 0.02259481
Iteration 54, loss = 0.02264118
Iteration 55, loss = 0.02252032
Iteration 56, loss = 0.02250707
Iteration 57, loss = 0.02222177
Iteration 58, loss = 0.02248856
Iteration 59, loss = 0.02239905
Iteration 60, loss = 0.02226761
Iteration 61, loss = 0.02235871
Iteration 62, loss = 0.02224744
Iteration 63, loss = 0.02209793
Iteration 64, loss = 0.02218232
Iteration 65, loss = 0.02189340
Iteration 66, loss = 0.02197211
Iteration 67, loss = 0.02179622
Iteration 68, loss = 0.02180012
Iteration 69, loss = 0.02172782
Iteration 70, loss = 0.02183088
Iteration 71, loss = 0.02191335
Iteration 72, loss = 0.02186949
Iteration 73, loss = 0.02183951
Iteration 74, loss = 0.02182890
Iteration 75, loss = 0.02174435
Iteration 76, loss = 0.02172092
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67741800
Iteration 2, loss = 0.61863098
Iteration 3, loss = 0.51918956
Iteration 4, loss = 0.40767983
Iteration 5, loss = 0.30779628
Iteration 6, loss = 0.23081065
Iteration 7, loss = 0.17630143
Iteration 8, loss = 0.13852555
Iteration 9, loss = 0.11260803
Iteration 10, loss = 0.09404782
Iteration 11, loss = 0.08075305
Iteration 12, loss = 0.07043408
Iteration 13, loss = 0.06306222
Iteration 14, loss = 0.05699390
Iteration 15, loss = 0.05230874
Iteration 16, loss = 0.04860994
Iteration 17, loss = 0.04538552
Iteration 18, loss = 0.04287588
Iteration 19, loss = 0.04071030
Iteration 20, loss = 0.03878518
Iteration 21, loss = 0.03734552
Iteration 22, loss = 0.03610170
Iteration 23, loss = 0.03467749
Iteration 24, loss = 0.03355011
Iteration 25, loss = 0.03272496
Iteration 26, loss = 0.03185551
Iteration 27, loss = 0.03134207
Iteration 28, loss = 0.03061143
Iteration 29, loss = 0.03023631
Iteration 30, loss = 0.02978285
Iteration 31, loss = 0.02918650
Iteration 32, loss = 0.02885242
Iteration 33, loss = 0.02845809
Iteration 34, loss = 0.02807629
Iteration 35, loss = 0.02808593
Iteration 36, loss = 0.02736313
Iteration 37, loss = 0.02750820
Iteration 38, loss = 0.02711953
Iteration 39, loss = 0.02674621
Iteration 40, loss = 0.02652418
Iteration 41, loss = 0.02626851
Iteration 42, loss = 0.02621637
Iteration 43, loss = 0.02594169
Iteration 44, loss = 0.02607058
Iteration 45, loss = 0.02583380
Iteration 46, loss = 0.02585828
Iteration 47, loss = 0.02554443
Iteration 48, loss = 0.02524442
Iteration 49, loss = 0.02552158
Iteration 50, loss = 0.02533756
Iteration 51, loss = 0.02498520
Iteration 52, loss = 0.02524899
Iteration 53, loss = 0.02493107
Iteration 54, loss = 0.02490425
Iteration 55, loss = 0.02483402
Iteration 56, loss = 0.02477356
Iteration 57, loss = 0.02467679
Iteration 58, loss = 0.02463641
Iteration 59, loss = 0.02480722
Iteration 60, loss = 0.02465873
Iteration 61, loss = 0.02459688
Iteration 62, loss = 0.02451715
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67397732
Iteration 2, loss = 0.60290764
Iteration 3, loss = 0.49252037
Iteration 4, loss = 0.38011404
Iteration 5, loss = 0.28630398
Iteration 6, loss = 0.21539278
Iteration 7, loss = 0.16560123
Iteration 8, loss = 0.13158851
Iteration 9, loss = 0.10759500
Iteration 10, loss = 0.09080484
Iteration 11, loss = 0.07877378
Iteration 12, loss = 0.06930715
Iteration 13, loss = 0.06189577
Iteration 14, loss = 0.05672506
Iteration 15, loss = 0.05208211
Iteration 16, loss = 0.04855335
Iteration 17, loss = 0.04553453
Iteration 18, loss = 0.04275653
Iteration 19, loss = 0.04083563
Iteration 20, loss = 0.03905898
Iteration 21, loss = 0.03749234
Iteration 22, loss = 0.03621375
Iteration 23, loss = 0.03504324
Iteration 24, loss = 0.03418689
Iteration 25, loss = 0.03342143
Iteration 26, loss = 0.03233627
Iteration 27, loss = 0.03166008
Iteration 28, loss = 0.03089828
Iteration 29, loss = 0.03054657
Iteration 30, loss = 0.02996864
Iteration 31, loss = 0.02957771
Iteration 32, loss = 0.02923690
Iteration 33, loss = 0.02893365
Iteration 34, loss = 0.02877332
Iteration 35, loss = 0.02834044
Iteration 36, loss = 0.02795514
Iteration 37, loss = 0.02789999
Iteration 38, loss = 0.02783321
Iteration 39, loss = 0.02730571
Iteration 40, loss = 0.02762836
Iteration 41, loss = 0.02712335
Iteration 42, loss = 0.02662421
Iteration 43, loss = 0.02651538
Iteration 44, loss = 0.02638336
Iteration 45, loss = 0.02642882
Iteration 46, loss = 0.02630525
Iteration 47, loss = 0.02629266
Iteration 48, loss = 0.02638096
Iteration 49, loss = 0.02630481
Iteration 50, loss = 0.02561748
Iteration 51, loss = 0.02594326
Iteration 52, loss = 0.02569892
Iteration 53, loss = 0.02546968
Iteration 54, loss = 0.02551062
Iteration 55, loss = 0.02533998
Iteration 56, loss = 0.02526799
Iteration 57, loss = 0.02530422
Iteration 58, loss = 0.02516597
Iteration 59, loss = 0.02543028
Iteration 60, loss = 0.02506760
Iteration 61, loss = 0.02509243
Iteration 62, loss = 0.02511476
Iteration 63, loss = 0.02519195
Iteration 64, loss = 0.02512768
Iteration 65, loss = 0.02497621
Iteration 66, loss = 0.02478380
Iteration 67, loss = 0.02475053
Iteration 68, loss = 0.02497145
Iteration 69, loss = 0.02505588
Iteration 70, loss = 0.02506111
Iteration 71, loss = 0.02457352
Iteration 72, loss = 0.02473603
Iteration 73, loss = 0.02493426
Iteration 74, loss = 0.02471725
Iteration 75, loss = 0.02479936
Iteration 76, loss = 0.02477220
Iteration 77, loss = 0.02476599
Iteration 78, loss = 0.02463080
Iteration 79, loss = 0.02468954
Iteration 80, loss = 0.02475826
Iteration 81, loss = 0.02463772
Iteration 82, loss = 0.02442057
Iteration 83, loss = 0.02463989
Iteration 84, loss = 0.02500211
Iteration 85, loss = 0.02432248
Iteration 86, loss = 0.02451502
Iteration 87, loss = 0.02426417
Iteration 88, loss = 0.02418943
Iteration 89, loss = 0.02440416
Iteration 90, loss = 0.02432644
Iteration 91, loss = 0.02414013
Iteration 92, loss = 0.02430187
Iteration 93, loss = 0.02424004
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71455058
Iteration 2, loss = 0.63112642
Iteration 3, loss = 0.52043924
Iteration 4, loss = 0.39472362
Iteration 5, loss = 0.29097350
Iteration 6, loss = 0.21542553
Iteration 7, loss = 0.16367496
Iteration 8, loss = 0.12844733
Iteration 9, loss = 0.10444041
Iteration 10, loss = 0.08738052
Iteration 11, loss = 0.07479905
Iteration 12, loss = 0.06559468
Iteration 13, loss = 0.05844115
Iteration 14, loss = 0.05293985
Iteration 15, loss = 0.04842668
Iteration 16, loss = 0.04507562
Iteration 17, loss = 0.04166806
Iteration 18, loss = 0.03937445
Iteration 19, loss = 0.03711432
Iteration 20, loss = 0.03537421
Iteration 21, loss = 0.03393298
Iteration 22, loss = 0.03243309
Iteration 23, loss = 0.03144747
Iteration 24, loss = 0.03067209
Iteration 25, loss = 0.02998870
Iteration 26, loss = 0.02895179
Iteration 27, loss = 0.02823261
Iteration 28, loss = 0.02761388
Iteration 29, loss = 0.02709441
Iteration 30, loss = 0.02657644
Iteration 31, loss = 0.02612218
Iteration 32, loss = 0.02573435
Iteration 33, loss = 0.02554509
Iteration 34, loss = 0.02535650
Iteration 35, loss = 0.02506548
Iteration 36, loss = 0.02449407
Iteration 37, loss = 0.02407524
Iteration 38, loss = 0.02402234
Iteration 39, loss = 0.02385509
Iteration 40, loss = 0.02358029
Iteration 41, loss = 0.02349413
Iteration 42, loss = 0.02339560
Iteration 43, loss = 0.02308719
Iteration 44, loss = 0.02299215
Iteration 45, loss = 0.02307809
Iteration 46, loss = 0.02294303
Iteration 47, loss = 0.02269074
Iteration 48, loss = 0.02280706
Iteration 49, loss = 0.02274396
Iteration 50, loss = 0.02253509
Iteration 51, loss = 0.02207084
Iteration 52, loss = 0.02220548
Iteration 53, loss = 0.02204809
Iteration 54, loss = 0.02224515
Iteration 55, loss = 0.02201028
Iteration 56, loss = 0.02217868
Iteration 57, loss = 0.02222721
Iteration 58, loss = 0.02192858
Iteration 59, loss = 0.02193615
Iteration 60, loss = 0.02179359
Iteration 61, loss = 0.02165234
Iteration 62, loss = 0.02143028
Iteration 63, loss = 0.02147479
Iteration 64, loss = 0.02140205
Iteration 65, loss = 0.02155177
Iteration 66, loss = 0.02145105
Iteration 67, loss = 0.02124729
Iteration 68, loss = 0.02127439
Iteration 69, loss = 0.02115176
Iteration 70, loss = 0.02114430
Iteration 71, loss = 0.02110314
Iteration 72, loss = 0.02109400
Iteration 73, loss = 0.02119537
Iteration 74, loss = 0.02099116
Iteration 75, loss = 0.02122623
Iteration 76, loss = 0.02120366
Iteration 77, loss = 0.02112249
Iteration 78, loss = 0.02133908
Iteration 79, loss = 0.02098190
Iteration 80, loss = 0.02101927
Iteration 81, loss = 0.02083219
Iteration 82, loss = 0.02106741
Iteration 83, loss = 0.02082760
Iteration 84, loss = 0.02081345
Iteration 85, loss = 0.02119801
Iteration 86, loss = 0.02098032
Iteration 87, loss = 0.02080227
Iteration 88, loss = 0.02107507
Iteration 89, loss = 0.02108528
Iteration 90, loss = 0.02101240
Iteration 91, loss = 0.02097777
Iteration 92, loss = 0.02092055
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67504891
Iteration 2, loss = 0.60372863
Iteration 3, loss = 0.49524789
Iteration 4, loss = 0.38388943
Iteration 5, loss = 0.28855874
Iteration 6, loss = 0.21638259
Iteration 7, loss = 0.16531992
Iteration 8, loss = 0.13050664
Iteration 9, loss = 0.10603018
Iteration 10, loss = 0.08873860
Iteration 11, loss = 0.07610822
Iteration 12, loss = 0.06687360
Iteration 13, loss = 0.05959429
Iteration 14, loss = 0.05379920
Iteration 15, loss = 0.04927905
Iteration 16, loss = 0.04573138
Iteration 17, loss = 0.04265394
Iteration 18, loss = 0.03997897
Iteration 19, loss = 0.03783587
Iteration 20, loss = 0.03594888
Iteration 21, loss = 0.03450827
Iteration 22, loss = 0.03306827
Iteration 23, loss = 0.03217183
Iteration 24, loss = 0.03119651
Iteration 25, loss = 0.03055573
Iteration 26, loss = 0.02961504
Iteration 27, loss = 0.02864186
Iteration 28, loss = 0.02824469
Iteration 29, loss = 0.02775486
Iteration 30, loss = 0.02739004
Iteration 31, loss = 0.02669663
Iteration 32, loss = 0.02625628
Iteration 33, loss = 0.02622054
Iteration 34, loss = 0.02561640
Iteration 35, loss = 0.02545011
Iteration 36, loss = 0.02491390
Iteration 37, loss = 0.02502983
Iteration 38, loss = 0.02458542
Iteration 39, loss = 0.02447351
Iteration 40, loss = 0.02422002
Iteration 41, loss = 0.02397225
Iteration 42, loss = 0.02405282
Iteration 43, loss = 0.02373145
Iteration 44, loss = 0.02360567
Iteration 45, loss = 0.02356183
Iteration 46, loss = 0.02340478
Iteration 47, loss = 0.02315376
Iteration 48, loss = 0.02297867
Iteration 49, loss = 0.02312364
Iteration 50, loss = 0.02296081
Iteration 51, loss = 0.02267837
Iteration 52, loss = 0.02264926
Iteration 53, loss = 0.02271004
Iteration 54, loss = 0.02265571
Iteration 55, loss = 0.02243042
Iteration 56, loss = 0.02242283
Iteration 57, loss = 0.02235591
Iteration 58, loss = 0.02229212
Iteration 59, loss = 0.02230693
Iteration 60, loss = 0.02218086
Iteration 61, loss = 0.02217996
Iteration 62, loss = 0.02209474
Iteration 63, loss = 0.02195746
Iteration 64, loss = 0.02201949
Iteration 65, loss = 0.02188059
Iteration 66, loss = 0.02203837
Iteration 67, loss = 0.02161768
Iteration 68, loss = 0.02166716
Iteration 69, loss = 0.02176281
Iteration 70, loss = 0.02182667
Iteration 71, loss = 0.02163666
Iteration 72, loss = 0.02174372
Iteration 73, loss = 0.02161608
Iteration 74, loss = 0.02163109
Iteration 75, loss = 0.02154367
Iteration 76, loss = 0.02149853
Iteration 77, loss = 0.02159459
Iteration 78, loss = 0.02136957
Iteration 79, loss = 0.02138862
Iteration 80, loss = 0.02139923
Iteration 81, loss = 0.02105197
Iteration 82, loss = 0.02140059
Iteration 83, loss = 0.02148802
Iteration 84, loss = 0.02138937
Iteration 85, loss = 0.02142549
Iteration 86, loss = 0.02117078
Iteration 87, loss = 0.02145627
Iteration 88, loss = 0.02119964
Iteration 89, loss = 0.02160136
Iteration 90, loss = 0.02152731
Iteration 91, loss = 0.02123433
Iteration 92, loss = 0.02114587
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70764796
Iteration 2, loss = 0.63922478
Iteration 3, loss = 0.53660252
Iteration 4, loss = 0.41683577
Iteration 5, loss = 0.31098721
Iteration 6, loss = 0.23117117
Iteration 7, loss = 0.17473648
Iteration 8, loss = 0.13662401
Iteration 9, loss = 0.11046793
Iteration 10, loss = 0.09188805
Iteration 11, loss = 0.07863500
Iteration 12, loss = 0.06855489
Iteration 13, loss = 0.06098334
Iteration 14, loss = 0.05483455
Iteration 15, loss = 0.05009208
Iteration 16, loss = 0.04635099
Iteration 17, loss = 0.04348453
Iteration 18, loss = 0.04078525
Iteration 19, loss = 0.03850437
Iteration 20, loss = 0.03650875
Iteration 21, loss = 0.03497275
Iteration 22, loss = 0.03375169
Iteration 23, loss = 0.03260328
Iteration 24, loss = 0.03149755
Iteration 25, loss = 0.03074786
Iteration 26, loss = 0.02984254
Iteration 27, loss = 0.02901828
Iteration 28, loss = 0.02842437
Iteration 29, loss = 0.02784084
Iteration 30, loss = 0.02717444
Iteration 31, loss = 0.02697079
Iteration 32, loss = 0.02649209
Iteration 33, loss = 0.02601405
Iteration 34, loss = 0.02578825
Iteration 35, loss = 0.02565172
Iteration 36, loss = 0.02572028
Iteration 37, loss = 0.02509193
Iteration 38, loss = 0.02491494
Iteration 39, loss = 0.02467730
Iteration 40, loss = 0.02450810
Iteration 41, loss = 0.02406602
Iteration 42, loss = 0.02410101
Iteration 43, loss = 0.02383507
Iteration 44, loss = 0.02372389
Iteration 45, loss = 0.02344422
Iteration 46, loss = 0.02339207
Iteration 47, loss = 0.02334248
Iteration 48, loss = 0.02364456
Iteration 49, loss = 0.02304035
Iteration 50, loss = 0.02328944
Iteration 51, loss = 0.02293495
Iteration 52, loss = 0.02262314
Iteration 53, loss = 0.02259481
Iteration 54, loss = 0.02264118
Iteration 55, loss = 0.02252032
Iteration 56, loss = 0.02250707
Iteration 57, loss = 0.02222177
Iteration 58, loss = 0.02248856
Iteration 59, loss = 0.02239905
Iteration 60, loss = 0.02226761
Iteration 61, loss = 0.02235871
Iteration 62, loss = 0.02224744
Iteration 63, loss = 0.02209793
Iteration 64, loss = 0.02218232
Iteration 65, loss = 0.02189340
Iteration 66, loss = 0.02197211
Iteration 67, loss = 0.02179622
Iteration 68, loss = 0.02180012
Iteration 69, loss = 0.02172782
Iteration 70, loss = 0.02183088
Iteration 71, loss = 0.02191335
Iteration 72, loss = 0.02186949
Iteration 73, loss = 0.02183951
Iteration 74, loss = 0.02182890
Iteration 75, loss = 0.02174435
Iteration 76, loss = 0.02172092
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67741800
Iteration 2, loss = 0.61863098
Iteration 3, loss = 0.51918956
Iteration 4, loss = 0.40767983
Iteration 5, loss = 0.30779628
Iteration 6, loss = 0.23081065
Iteration 7, loss = 0.17630143
Iteration 8, loss = 0.13852555
Iteration 9, loss = 0.11260803
Iteration 10, loss = 0.09404782
Iteration 11, loss = 0.08075305
Iteration 12, loss = 0.07043408
Iteration 13, loss = 0.06306222
Iteration 14, loss = 0.05699390
Iteration 15, loss = 0.05230874
Iteration 16, loss = 0.04860994
Iteration 17, loss = 0.04538552
Iteration 18, loss = 0.04287588
Iteration 19, loss = 0.04071030
Iteration 20, loss = 0.03878518
Iteration 21, loss = 0.03734552
Iteration 22, loss = 0.03610170
Iteration 23, loss = 0.03467749
Iteration 24, loss = 0.03355011
Iteration 25, loss = 0.03272496
Iteration 26, loss = 0.03185551
Iteration 27, loss = 0.03134207
Iteration 28, loss = 0.03061143
Iteration 29, loss = 0.03023631
Iteration 30, loss = 0.02978285
Iteration 31, loss = 0.02918650
Iteration 32, loss = 0.02885242
Iteration 33, loss = 0.02845809
Iteration 34, loss = 0.02807629
Iteration 35, loss = 0.02808593
Iteration 36, loss = 0.02736313
Iteration 37, loss = 0.02750820
Iteration 38, loss = 0.02711953
Iteration 39, loss = 0.02674621
Iteration 40, loss = 0.02652418
Iteration 41, loss = 0.02626851
Iteration 42, loss = 0.02621637
Iteration 43, loss = 0.02594169
Iteration 44, loss = 0.02607058
Iteration 45, loss = 0.02583380
Iteration 46, loss = 0.02585828
Iteration 47, loss = 0.02554443
Iteration 48, loss = 0.02524442
Iteration 49, loss = 0.02552158
Iteration 50, loss = 0.02533756
Iteration 51, loss = 0.02498520
Iteration 52, loss = 0.02524899
Iteration 53, loss = 0.02493107
Iteration 54, loss = 0.02490425
Iteration 55, loss = 0.02483402
Iteration 56, loss = 0.02477356
Iteration 57, loss = 0.02467679
Iteration 58, loss = 0.02463641
Iteration 59, loss = 0.02480722
Iteration 60, loss = 0.02465873
Iteration 61, loss = 0.02459688
Iteration 62, loss = 0.02451715
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67397732
Iteration 2, loss = 0.60290764
Iteration 3, loss = 0.49252037
Iteration 4, loss = 0.38011404
Iteration 5, loss = 0.28630398
Iteration 6, loss = 0.21539278
Iteration 7, loss = 0.16560123
Iteration 8, loss = 0.13158851
Iteration 9, loss = 0.10759500
Iteration 10, loss = 0.09080484
Iteration 11, loss = 0.07877378
Iteration 12, loss = 0.06930715
Iteration 13, loss = 0.06189577
Iteration 14, loss = 0.05672506
Iteration 15, loss = 0.05208211
Iteration 16, loss = 0.04855335
Iteration 17, loss = 0.04553453
Iteration 18, loss = 0.04275653
Iteration 19, loss = 0.04083563
Iteration 20, loss = 0.03905898
Iteration 21, loss = 0.03749234
Iteration 22, loss = 0.03621375
Iteration 23, loss = 0.03504324
Iteration 24, loss = 0.03418689
Iteration 25, loss = 0.03342143
Iteration 26, loss = 0.03233627
Iteration 27, loss = 0.03166008
Iteration 28, loss = 0.03089828
Iteration 29, loss = 0.03054657
Iteration 30, loss = 0.02996864
Iteration 31, loss = 0.02957771
Iteration 32, loss = 0.02923690
Iteration 33, loss = 0.02893365
Iteration 34, loss = 0.02877332
Iteration 35, loss = 0.02834044
Iteration 36, loss = 0.02795514
Iteration 37, loss = 0.02789999
Iteration 38, loss = 0.02783321
Iteration 39, loss = 0.02730571
Iteration 40, loss = 0.02762836
Iteration 41, loss = 0.02712335
Iteration 42, loss = 0.02662421
Iteration 43, loss = 0.02651538
Iteration 44, loss = 0.02638336
Iteration 45, loss = 0.02642882
Iteration 46, loss = 0.02630525
Iteration 47, loss = 0.02629266
Iteration 48, loss = 0.02638096
Iteration 49, loss = 0.02630481
Iteration 50, loss = 0.02561748
Iteration 51, loss = 0.02594326
Iteration 52, loss = 0.02569892
Iteration 53, loss = 0.02546968
Iteration 54, loss = 0.02551062
Iteration 55, loss = 0.02533998
Iteration 56, loss = 0.02526799
Iteration 57, loss = 0.02530422
Iteration 58, loss = 0.02516597
Iteration 59, loss = 0.02543028
Iteration 60, loss = 0.02506760
Iteration 61, loss = 0.02509243
Iteration 62, loss = 0.02511476
Iteration 63, loss = 0.02519195
Iteration 64, loss = 0.02512768
Iteration 65, loss = 0.02497621
Iteration 66, loss = 0.02478380
Iteration 67, loss = 0.02475053
Iteration 68, loss = 0.02497145
Iteration 69, loss = 0.02505588
Iteration 70, loss = 0.02506111
Iteration 71, loss = 0.02457352
Iteration 72, loss = 0.02473603
Iteration 73, loss = 0.02493426
Iteration 74, loss = 0.02471725
Iteration 75, loss = 0.02479936
Iteration 76, loss = 0.02477220
Iteration 77, loss = 0.02476599
Iteration 78, loss = 0.02463080
Iteration 79, loss = 0.02468954
Iteration 80, loss = 0.02475826
Iteration 81, loss = 0.02463772
Iteration 82, loss = 0.02442057
Iteration 83, loss = 0.02463989
Iteration 84, loss = 0.02500211
Iteration 85, loss = 0.02432248
Iteration 86, loss = 0.02451502
Iteration 87, loss = 0.02426417
Iteration 88, loss = 0.02418943
Iteration 89, loss = 0.02440416
Iteration 90, loss = 0.02432644
Iteration 91, loss = 0.02414013
Iteration 92, loss = 0.02430187
Iteration 93, loss = 0.02424004
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71455058
Iteration 2, loss = 0.63112642
Iteration 3, loss = 0.52043924
Iteration 4, loss = 0.39472362
Iteration 5, loss = 0.29097350
Iteration 6, loss = 0.21542553
Iteration 7, loss = 0.16367496
Iteration 8, loss = 0.12844733
Iteration 9, loss = 0.10444041
Iteration 10, loss = 0.08738052
Iteration 11, loss = 0.07479905
Iteration 12, loss = 0.06559468
Iteration 13, loss = 0.05844115
Iteration 14, loss = 0.05293985
Iteration 15, loss = 0.04842668
Iteration 16, loss = 0.04507562
Iteration 17, loss = 0.04166806
Iteration 18, loss = 0.03937445
Iteration 19, loss = 0.03711432
Iteration 20, loss = 0.03537421
Iteration 21, loss = 0.03393298
Iteration 22, loss = 0.03243309
Iteration 23, loss = 0.03144747
Iteration 24, loss = 0.03067209
Iteration 25, loss = 0.02998870
Iteration 26, loss = 0.02895179
Iteration 27, loss = 0.02823261
Iteration 28, loss = 0.02761388
Iteration 29, loss = 0.02709441
Iteration 30, loss = 0.02657644
Iteration 31, loss = 0.02612218
Iteration 32, loss = 0.02573435
Iteration 33, loss = 0.02554509
Iteration 34, loss = 0.02535650
Iteration 35, loss = 0.02506548
Iteration 36, loss = 0.02449407
Iteration 37, loss = 0.02407524
Iteration 38, loss = 0.02402234
Iteration 39, loss = 0.02385509
Iteration 40, loss = 0.02358029
Iteration 41, loss = 0.02349413
Iteration 42, loss = 0.02339560
Iteration 43, loss = 0.02308719
Iteration 44, loss = 0.02299215
Iteration 45, loss = 0.02307809
Iteration 46, loss = 0.02294303
Iteration 47, loss = 0.02269074
Iteration 48, loss = 0.02280706
Iteration 49, loss = 0.02274396
Iteration 50, loss = 0.02253509
Iteration 51, loss = 0.02207084
Iteration 52, loss = 0.02220548
Iteration 53, loss = 0.02204809
Iteration 54, loss = 0.02224515
Iteration 55, loss = 0.02201028
Iteration 56, loss = 0.02217868
Iteration 57, loss = 0.02222721
Iteration 58, loss = 0.02192858
Iteration 59, loss = 0.02193615
Iteration 60, loss = 0.02179359
Iteration 61, loss = 0.02165234
Iteration 62, loss = 0.02143028
Iteration 63, loss = 0.02147479
Iteration 64, loss = 0.02140205
Iteration 65, loss = 0.02155177
Iteration 66, loss = 0.02145105
Iteration 67, loss = 0.02124729
Iteration 68, loss = 0.02127439
Iteration 69, loss = 0.02115176
Iteration 70, loss = 0.02114430
Iteration 71, loss = 0.02110314
Iteration 72, loss = 0.02109400
Iteration 73, loss = 0.02119537
Iteration 74, loss = 0.02099116
Iteration 75, loss = 0.02122623
Iteration 76, loss = 0.02120366
Iteration 77, loss = 0.02112249
Iteration 78, loss = 0.02133908
Iteration 79, loss = 0.02098190
Iteration 80, loss = 0.02101927
Iteration 81, loss = 0.02083219
Iteration 82, loss = 0.02106741
Iteration 83, loss = 0.02082760
Iteration 84, loss = 0.02081345
Iteration 85, loss = 0.02119801
Iteration 86, loss = 0.02098032
Iteration 87, loss = 0.02080227
Iteration 88, loss = 0.02107507
Iteration 89, loss = 0.02108528
Iteration 90, loss = 0.02101240
Iteration 91, loss = 0.02097777
Iteration 92, loss = 0.02092055
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67504891
Iteration 2, loss = 0.60372863
Iteration 3, loss = 0.49524789
Iteration 4, loss = 0.38388943
Iteration 5, loss = 0.28855874
Iteration 6, loss = 0.21638259
Iteration 7, loss = 0.16531992
Iteration 8, loss = 0.13050664
Iteration 9, loss = 0.10603018
Iteration 10, loss = 0.08873860
Iteration 11, loss = 0.07610822
Iteration 12, loss = 0.06687360
Iteration 13, loss = 0.05959429
Iteration 14, loss = 0.05379920
Iteration 15, loss = 0.04927905
Iteration 16, loss = 0.04573138
Iteration 17, loss = 0.04265394
Iteration 18, loss = 0.03997897
Iteration 19, loss = 0.03783587
Iteration 20, loss = 0.03594888
Iteration 21, loss = 0.03450827
Iteration 22, loss = 0.03306827
Iteration 23, loss = 0.03217183
Iteration 24, loss = 0.03119651
Iteration 25, loss = 0.03055573
Iteration 26, loss = 0.02961504
Iteration 27, loss = 0.02864186
Iteration 28, loss = 0.02824469
Iteration 29, loss = 0.02775486
Iteration 30, loss = 0.02739004
Iteration 31, loss = 0.02669663
Iteration 32, loss = 0.02625628
Iteration 33, loss = 0.02622054
Iteration 34, loss = 0.02561640
Iteration 35, loss = 0.02545011
Iteration 36, loss = 0.02491390
Iteration 37, loss = 0.02502983
Iteration 38, loss = 0.02458542
Iteration 39, loss = 0.02447351
Iteration 40, loss = 0.02422002
Iteration 41, loss = 0.02397225
Iteration 42, loss = 0.02405282
Iteration 43, loss = 0.02373145
Iteration 44, loss = 0.02360567
Iteration 45, loss = 0.02356183
Iteration 46, loss = 0.02340478
Iteration 47, loss = 0.02315376
Iteration 48, loss = 0.02297867
Iteration 49, loss = 0.02312364
Iteration 50, loss = 0.02296081
Iteration 51, loss = 0.02267837
Iteration 52, loss = 0.02264926
Iteration 53, loss = 0.02271004
Iteration 54, loss = 0.02265571
Iteration 55, loss = 0.02243042
Iteration 56, loss = 0.02242283
Iteration 57, loss = 0.02235591
Iteration 58, loss = 0.02229212
Iteration 59, loss = 0.02230693
Iteration 60, loss = 0.02218086
Iteration 61, loss = 0.02217996
Iteration 62, loss = 0.02209474
Iteration 63, loss = 0.02195746
Iteration 64, loss = 0.02201949
Iteration 65, loss = 0.02188059
Iteration 66, loss = 0.02203837
Iteration 67, loss = 0.02161768
Iteration 68, loss = 0.02166716
Iteration 69, loss = 0.02176281
Iteration 70, loss = 0.02182667
Iteration 71, loss = 0.02163666
Iteration 72, loss = 0.02174372
Iteration 73, loss = 0.02161608
Iteration 74, loss = 0.02163109
Iteration 75, loss = 0.02154367
Iteration 76, loss = 0.02149853
Iteration 77, loss = 0.02159459
Iteration 78, loss = 0.02136957
Iteration 79, loss = 0.02138862
Iteration 80, loss = 0.02139923
Iteration 81, loss = 0.02105197
Iteration 82, loss = 0.02140059
Iteration 83, loss = 0.02148802
Iteration 84, loss = 0.02138937
Iteration 85, loss = 0.02142549
Iteration 86, loss = 0.02117078
Iteration 87, loss = 0.02145627
Iteration 88, loss = 0.02119964
Iteration 89, loss = 0.02160136
Iteration 90, loss = 0.02152731
Iteration 91, loss = 0.02123433
Iteration 92, loss = 0.02114587
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70764796
Iteration 2, loss = 0.63922478
Iteration 3, loss = 0.53660252
Iteration 4, loss = 0.41683577
Iteration 5, loss = 0.31098721
Iteration 6, loss = 0.23117117
Iteration 7, loss = 0.17473648
Iteration 8, loss = 0.13662401
Iteration 9, loss = 0.11046793
Iteration 10, loss = 0.09188805
Iteration 11, loss = 0.07863500
Iteration 12, loss = 0.06855489
Iteration 13, loss = 0.06098334
Iteration 14, loss = 0.05483455
Iteration 15, loss = 0.05009208
Iteration 16, loss = 0.04635099
Iteration 17, loss = 0.04348453
Iteration 18, loss = 0.04078525
Iteration 19, loss = 0.03850437
Iteration 20, loss = 0.03650875
Iteration 21, loss = 0.03497275
Iteration 22, loss = 0.03375169
Iteration 23, loss = 0.03260328
Iteration 24, loss = 0.03149755
Iteration 25, loss = 0.03074786
Iteration 26, loss = 0.02984254
Iteration 27, loss = 0.02901828
Iteration 28, loss = 0.02842437
Iteration 29, loss = 0.02784084
Iteration 30, loss = 0.02717444
Iteration 31, loss = 0.02697079
Iteration 32, loss = 0.02649209
Iteration 33, loss = 0.02601405
Iteration 34, loss = 0.02578825
Iteration 35, loss = 0.02565172
Iteration 36, loss = 0.02572028
Iteration 37, loss = 0.02509193
Iteration 38, loss = 0.02491494
Iteration 39, loss = 0.02467730
Iteration 40, loss = 0.02450810
Iteration 41, loss = 0.02406602
Iteration 42, loss = 0.02410101
Iteration 43, loss = 0.02383507
Iteration 44, loss = 0.02372389
Iteration 45, loss = 0.02344422
Iteration 46, loss = 0.02339207
Iteration 47, loss = 0.02334248
Iteration 48, loss = 0.02364456
Iteration 49, loss = 0.02304035
Iteration 50, loss = 0.02328944
Iteration 51, loss = 0.02293495
Iteration 52, loss = 0.02262314
Iteration 53, loss = 0.02259481
Iteration 54, loss = 0.02264118
Iteration 55, loss = 0.02252032
Iteration 56, loss = 0.02250707
Iteration 57, loss = 0.02222177
Iteration 58, loss = 0.02248856
Iteration 59, loss = 0.02239905
Iteration 60, loss = 0.02226761
Iteration 61, loss = 0.02235871
Iteration 62, loss = 0.02224744
Iteration 63, loss = 0.02209793
Iteration 64, loss = 0.02218232
Iteration 65, loss = 0.02189340
Iteration 66, loss = 0.02197211
Iteration 67, loss = 0.02179622
Iteration 68, loss = 0.02180012
Iteration 69, loss = 0.02172782
Iteration 70, loss = 0.02183088
Iteration 71, loss = 0.02191335
Iteration 72, loss = 0.02186949
Iteration 73, loss = 0.02183951
Iteration 74, loss = 0.02182890
Iteration 75, loss = 0.02174435
Iteration 76, loss = 0.02172092
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67103485
Iteration 2, loss = 0.58305344
Iteration 3, loss = 0.45068651
Iteration 4, loss = 0.32667016
Iteration 5, loss = 0.23299567
Iteration 6, loss = 0.17017003
Iteration 7, loss = 0.13012844
Iteration 8, loss = 0.10387324
Iteration 9, loss = 0.08627331
Iteration 10, loss = 0.07382360
Iteration 11, loss = 0.06477553
Iteration 12, loss = 0.05798435
Iteration 13, loss = 0.05276391
Iteration 14, loss = 0.04880346
Iteration 15, loss = 0.04534631
Iteration 16, loss = 0.04260847
Iteration 17, loss = 0.04056797
Iteration 18, loss = 0.03832755
Iteration 19, loss = 0.03710336
Iteration 20, loss = 0.03592423
Iteration 21, loss = 0.03459159
Iteration 22, loss = 0.03372839
Iteration 23, loss = 0.03295502
Iteration 24, loss = 0.03249828
Iteration 25, loss = 0.03178044
Iteration 26, loss = 0.03113433
Iteration 27, loss = 0.03070850
Iteration 28, loss = 0.03025472
Iteration 29, loss = 0.02945975
Iteration 30, loss = 0.02937998
Iteration 31, loss = 0.02895100
Iteration 32, loss = 0.02880457
Iteration 33, loss = 0.02821915
Iteration 34, loss = 0.02801848
Iteration 35, loss = 0.02787814
Iteration 36, loss = 0.02760802
Iteration 37, loss = 0.02756603
Iteration 38, loss = 0.02759741
Iteration 39, loss = 0.02746097
Iteration 40, loss = 0.02714436
Iteration 41, loss = 0.02724010
Iteration 42, loss = 0.02676971
Iteration 43, loss = 0.02673638
Iteration 44, loss = 0.02676210
Iteration 45, loss = 0.02666768
Iteration 46, loss = 0.02645917
Iteration 47, loss = 0.02659792
Iteration 48, loss = 0.02617390
Iteration 49, loss = 0.02612109
Iteration 50, loss = 0.02627555
Iteration 51, loss = 0.02614283
Iteration 52, loss = 0.02617525
Iteration 53, loss = 0.02611423
Iteration 54, loss = 0.02602906
Iteration 55, loss = 0.02599064
Iteration 56, loss = 0.02547810
Iteration 57, loss = 0.02582222
Iteration 58, loss = 0.02572006
Iteration 59, loss = 0.02577624
Iteration 60, loss = 0.02559446
Iteration 61, loss = 0.02559907
Iteration 62, loss = 0.02537054
Iteration 63, loss = 0.02576084
Iteration 64, loss = 0.02560852
Iteration 65, loss = 0.02520143
Iteration 66, loss = 0.02561236
Iteration 67, loss = 0.02547111
Iteration 68, loss = 0.02551399
Iteration 69, loss = 0.02555145
Iteration 70, loss = 0.02541731
Iteration 71, loss = 0.02549215
Iteration 72, loss = 0.02523254
Iteration 73, loss = 0.02559360
Iteration 74, loss = 0.02533543
Iteration 75, loss = 0.02517347
Iteration 76, loss = 0.02543942
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on dropped_v2_no_emojis_mentions...
Iteration 1, loss = 0.66554765
Iteration 2, loss = 0.64897133
Iteration 3, loss = 0.63460832
Iteration 4, loss = 0.62022810
Iteration 5, loss = 0.60530251
Iteration 6, loss = 0.58954098
Iteration 7, loss = 0.57275926
Iteration 8, loss = 0.55470777
Iteration 9, loss = 0.53580151
Iteration 10, loss = 0.51588440
Iteration 11, loss = 0.49528262
Iteration 12, loss = 0.47393500
Iteration 13, loss = 0.45220572
Iteration 14, loss = 0.43042252
Iteration 15, loss = 0.40839633
Iteration 16, loss = 0.38678634
Iteration 17, loss = 0.36534118
Iteration 18, loss = 0.34423125
Iteration 19, loss = 0.32418384
Iteration 20, loss = 0.30477112
Iteration 21, loss = 0.28604249
Iteration 22, loss = 0.26842820
Iteration 23, loss = 0.25182084
Iteration 24, loss = 0.23589659
Iteration 25, loss = 0.22115282
Iteration 26, loss = 0.20709882
Iteration 27, loss = 0.19415297
Iteration 28, loss = 0.18194135
Iteration 29, loss = 0.17066261
Iteration 30, loss = 0.16012208
Iteration 31, loss = 0.15041980
Iteration 32, loss = 0.14127909
Iteration 33, loss = 0.13297513
Iteration 34, loss = 0.12521284
Iteration 35, loss = 0.11806368
Iteration 36, loss = 0.11141897
Iteration 37, loss = 0.10529004
Iteration 38, loss = 0.09960053
Iteration 39, loss = 0.09434354
Iteration 40, loss = 0.08946712
Iteration 41, loss = 0.08497940
Iteration 42, loss = 0.08075584
Iteration 43, loss = 0.07690919
Iteration 44, loss = 0.07327380
Iteration 45, loss = 0.06988481
Iteration 46, loss = 0.06676456
Iteration 47, loss = 0.06385996
Iteration 48, loss = 0.06106933
Iteration 49, loss = 0.05849806
Iteration 50, loss = 0.05609087
Iteration 51, loss = 0.05386964
Iteration 52, loss = 0.05174382
Iteration 53, loss = 0.04976592
Iteration 54, loss = 0.04793204
Iteration 55, loss = 0.04615509
Iteration 56, loss = 0.04451856
Iteration 57, loss = 0.04296374
Iteration 58, loss = 0.04148777
Iteration 59, loss = 0.04010512
Iteration 60, loss = 0.03880626
Iteration 61, loss = 0.03757688
Iteration 62, loss = 0.03637405
Iteration 63, loss = 0.03526054
Iteration 64, loss = 0.03422272
Iteration 65, loss = 0.03321120
Iteration 66, loss = 0.03225008
Iteration 67, loss = 0.03135358
Iteration 68, loss = 0.03047254
Iteration 69, loss = 0.02965050
Iteration 70, loss = 0.02887291
Iteration 71, loss = 0.02811740
Iteration 72, loss = 0.02740178
Iteration 73, loss = 0.02670731
Iteration 74, loss = 0.02604288
Iteration 75, loss = 0.02541703
Iteration 76, loss = 0.02482539
Iteration 77, loss = 0.02423735
Iteration 78, loss = 0.02365641
Iteration 79, loss = 0.02314118
Iteration 80, loss = 0.02262365
Iteration 81, loss = 0.02212823
Iteration 82, loss = 0.02165260
Iteration 83, loss = 0.02121715
Iteration 84, loss = 0.02077068
Iteration 85, loss = 0.02036584
Iteration 86, loss = 0.01993848
Iteration 87, loss = 0.01956087
Iteration 88, loss = 0.01916390
Iteration 89, loss = 0.01881209
Iteration 90, loss = 0.01845027
Iteration 91, loss = 0.01811408
Iteration 92, loss = 0.01776778
Iteration 93, loss = 0.01749394
Iteration 94, loss = 0.01716669
Iteration 95, loss = 0.01687701
Iteration 96, loss = 0.01659349
Iteration 97, loss = 0.01631169
Iteration 98, loss = 0.01608603
Iteration 99, loss = 0.01580030
Iteration 100, loss = 0.01553997
Iteration 101, loss = 0.01528408
Iteration 102, loss = 0.01504649
Iteration 103, loss = 0.01481043
Iteration 104, loss = 0.01460461
Iteration 105, loss = 0.01439218
Iteration 106, loss = 0.01418490
Iteration 107, loss = 0.01397675
Iteration 108, loss = 0.01378659
Iteration 109, loss = 0.01363748
Iteration 110, loss = 0.01343853
Iteration 111, loss = 0.01329089
Iteration 112, loss = 0.01308851
Iteration 113, loss = 0.01293140
Iteration 114, loss = 0.01274016
Iteration 115, loss = 0.01257232
Iteration 116, loss = 0.01241511
Iteration 117, loss = 0.01225651
Iteration 118, loss = 0.01211589
Iteration 119, loss = 0.01195885
Iteration 120, loss = 0.01180651
Iteration 121, loss = 0.01165319
Iteration 122, loss = 0.01153339
Iteration 123, loss = 0.01139921
Iteration 124, loss = 0.01129568
Iteration 125, loss = 0.01115476
Iteration 126, loss = 0.01103633
Iteration 127, loss = 0.01092520
Iteration 128, loss = 0.01081343
Iteration 129, loss = 0.01069485
Iteration 130, loss = 0.01058507
Iteration 131, loss = 0.01048156
Iteration 132, loss = 0.01038799
Iteration 133, loss = 0.01029473
Iteration 134, loss = 0.01015588
Iteration 135, loss = 0.01005987
Iteration 136, loss = 0.00997384
Iteration 137, loss = 0.00987937
Iteration 138, loss = 0.00978422
Iteration 139, loss = 0.00969478
Iteration 140, loss = 0.00961219
Iteration 141, loss = 0.00952720
Iteration 142, loss = 0.00943935
Iteration 143, loss = 0.00936756
Iteration 144, loss = 0.00927616
Iteration 145, loss = 0.00919891
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68949106
Iteration 2, loss = 0.67521943
Iteration 3, loss = 0.66352998
Iteration 4, loss = 0.65237733
Iteration 5, loss = 0.64075023
Iteration 6, loss = 0.62791994
Iteration 7, loss = 0.61409043
Iteration 8, loss = 0.59881359
Iteration 9, loss = 0.58185873
Iteration 10, loss = 0.56353463
Iteration 11, loss = 0.54346151
Iteration 12, loss = 0.52228884
Iteration 13, loss = 0.49995356
Iteration 14, loss = 0.47690601
Iteration 15, loss = 0.45318716
Iteration 16, loss = 0.42955679
Iteration 17, loss = 0.40593311
Iteration 18, loss = 0.38285022
Iteration 19, loss = 0.36030096
Iteration 20, loss = 0.33839901
Iteration 21, loss = 0.31746166
Iteration 22, loss = 0.29755768
Iteration 23, loss = 0.27861098
Iteration 24, loss = 0.26074798
Iteration 25, loss = 0.24404955
Iteration 26, loss = 0.22831655
Iteration 27, loss = 0.21370426
Iteration 28, loss = 0.20013353
Iteration 29, loss = 0.18758590
Iteration 30, loss = 0.17584618
Iteration 31, loss = 0.16498789
Iteration 32, loss = 0.15495795
Iteration 33, loss = 0.14575204
Iteration 34, loss = 0.13717570
Iteration 35, loss = 0.12928957
Iteration 36, loss = 0.12198968
Iteration 37, loss = 0.11529369
Iteration 38, loss = 0.10907598
Iteration 39, loss = 0.10331138
Iteration 40, loss = 0.09795378
Iteration 41, loss = 0.09297331
Iteration 42, loss = 0.08845584
Iteration 43, loss = 0.08420800
Iteration 44, loss = 0.08021534
Iteration 45, loss = 0.07655658
Iteration 46, loss = 0.07311494
Iteration 47, loss = 0.06994692
Iteration 48, loss = 0.06688270
Iteration 49, loss = 0.06409151
Iteration 50, loss = 0.06149095
Iteration 51, loss = 0.05899527
Iteration 52, loss = 0.05668411
Iteration 53, loss = 0.05450552
Iteration 54, loss = 0.05246727
Iteration 55, loss = 0.05058008
Iteration 56, loss = 0.04877905
Iteration 57, loss = 0.04705804
Iteration 58, loss = 0.04547564
Iteration 59, loss = 0.04394003
Iteration 60, loss = 0.04246488
Iteration 61, loss = 0.04113644
Iteration 62, loss = 0.03983656
Iteration 63, loss = 0.03860366
Iteration 64, loss = 0.03746338
Iteration 65, loss = 0.03633100
Iteration 66, loss = 0.03527722
Iteration 67, loss = 0.03426855
Iteration 68, loss = 0.03333515
Iteration 69, loss = 0.03239360
Iteration 70, loss = 0.03150752
Iteration 71, loss = 0.03070553
Iteration 72, loss = 0.02988387
Iteration 73, loss = 0.02911981
Iteration 74, loss = 0.02840130
Iteration 75, loss = 0.02769873
Iteration 76, loss = 0.02703961
Iteration 77, loss = 0.02640231
Iteration 78, loss = 0.02580882
Iteration 79, loss = 0.02521674
Iteration 80, loss = 0.02464658
Iteration 81, loss = 0.02410988
Iteration 82, loss = 0.02359382
Iteration 83, loss = 0.02309277
Iteration 84, loss = 0.02262750
Iteration 85, loss = 0.02217283
Iteration 86, loss = 0.02167502
Iteration 87, loss = 0.02126726
Iteration 88, loss = 0.02081856
Iteration 89, loss = 0.02044404
Iteration 90, loss = 0.02004825
Iteration 91, loss = 0.01964768
Iteration 92, loss = 0.01929534
Iteration 93, loss = 0.01893143
Iteration 94, loss = 0.01860199
Iteration 95, loss = 0.01828032
Iteration 96, loss = 0.01794259
Iteration 97, loss = 0.01764857
Iteration 98, loss = 0.01736206
Iteration 99, loss = 0.01706819
Iteration 100, loss = 0.01684492
Iteration 101, loss = 0.01655002
Iteration 102, loss = 0.01628652
Iteration 103, loss = 0.01604299
Iteration 104, loss = 0.01578669
Iteration 105, loss = 0.01555932
Iteration 106, loss = 0.01532406
Iteration 107, loss = 0.01510448
Iteration 108, loss = 0.01488620
Iteration 109, loss = 0.01466879
Iteration 110, loss = 0.01446670
Iteration 111, loss = 0.01427808
Iteration 112, loss = 0.01408243
Iteration 113, loss = 0.01388545
Iteration 114, loss = 0.01368221
Iteration 115, loss = 0.01348531
Iteration 116, loss = 0.01333696
Iteration 117, loss = 0.01319053
Iteration 118, loss = 0.01302625
Iteration 119, loss = 0.01286914
Iteration 120, loss = 0.01269731
Iteration 121, loss = 0.01255350
Iteration 122, loss = 0.01240195
Iteration 123, loss = 0.01225894
Iteration 124, loss = 0.01212702
Iteration 125, loss = 0.01198964
Iteration 126, loss = 0.01184912
Iteration 127, loss = 0.01173469
Iteration 128, loss = 0.01162310
Iteration 129, loss = 0.01146781
Iteration 130, loss = 0.01134078
Iteration 131, loss = 0.01122181
Iteration 132, loss = 0.01108492
Iteration 133, loss = 0.01096745
Iteration 134, loss = 0.01086917
Iteration 135, loss = 0.01075474
Iteration 136, loss = 0.01066275
Iteration 137, loss = 0.01057066
Iteration 138, loss = 0.01047500
Iteration 139, loss = 0.01037953
Iteration 140, loss = 0.01028991
Iteration 141, loss = 0.01018725
Iteration 142, loss = 0.01009820
Iteration 143, loss = 0.01001228
Iteration 144, loss = 0.00992078
Iteration 145, loss = 0.00983552
Iteration 146, loss = 0.00974201
Iteration 147, loss = 0.00965553
Iteration 148, loss = 0.00958719
Iteration 149, loss = 0.00949255
Iteration 150, loss = 0.00942147
Iteration 151, loss = 0.00935813
Iteration 152, loss = 0.00930660
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70022026
Iteration 2, loss = 0.67025954
Iteration 3, loss = 0.63710101
Iteration 4, loss = 0.59865773
Iteration 5, loss = 0.55315076
Iteration 6, loss = 0.50353991
Iteration 7, loss = 0.45338505
Iteration 8, loss = 0.40413602
Iteration 9, loss = 0.35788624
Iteration 10, loss = 0.31552485
Iteration 11, loss = 0.27741442
Iteration 12, loss = 0.24378405
Iteration 13, loss = 0.21463809
Iteration 14, loss = 0.18953202
Iteration 15, loss = 0.16800075
Iteration 16, loss = 0.14971432
Iteration 17, loss = 0.13417245
Iteration 18, loss = 0.12091560
Iteration 19, loss = 0.10962861
Iteration 20, loss = 0.09985490
Iteration 21, loss = 0.09136855
Iteration 22, loss = 0.08412483
Iteration 23, loss = 0.07779845
Iteration 24, loss = 0.07238425
Iteration 25, loss = 0.06747223
Iteration 26, loss = 0.06313551
Iteration 27, loss = 0.05928911
Iteration 28, loss = 0.05585669
Iteration 29, loss = 0.05272995
Iteration 30, loss = 0.05004942
Iteration 31, loss = 0.04762054
Iteration 32, loss = 0.04530322
Iteration 33, loss = 0.04326333
Iteration 34, loss = 0.04132028
Iteration 35, loss = 0.03977006
Iteration 36, loss = 0.03825022
Iteration 37, loss = 0.03679688
Iteration 38, loss = 0.03552355
Iteration 39, loss = 0.03433081
Iteration 40, loss = 0.03323033
Iteration 41, loss = 0.03220938
Iteration 42, loss = 0.03141106
Iteration 43, loss = 0.03079057
Iteration 44, loss = 0.02990861
Iteration 45, loss = 0.02910288
Iteration 46, loss = 0.02826310
Iteration 47, loss = 0.02759822
Iteration 48, loss = 0.02695461
Iteration 49, loss = 0.02635280
Iteration 50, loss = 0.02577893
Iteration 51, loss = 0.02530580
Iteration 52, loss = 0.02493377
Iteration 53, loss = 0.02438600
Iteration 54, loss = 0.02402072
Iteration 55, loss = 0.02361238
Iteration 56, loss = 0.02328854
Iteration 57, loss = 0.02288901
Iteration 58, loss = 0.02250940
Iteration 59, loss = 0.02236388
Iteration 60, loss = 0.02189426
Iteration 61, loss = 0.02158256
Iteration 62, loss = 0.02133208
Iteration 63, loss = 0.02112670
Iteration 64, loss = 0.02101073
Iteration 65, loss = 0.02069270
Iteration 66, loss = 0.02056914
Iteration 67, loss = 0.02042669
Iteration 68, loss = 0.02007164
Iteration 69, loss = 0.01989091
Iteration 70, loss = 0.01966541
Iteration 71, loss = 0.01948225
Iteration 72, loss = 0.01923417
Iteration 73, loss = 0.01907523
Iteration 74, loss = 0.01889223
Iteration 75, loss = 0.01875604
Iteration 76, loss = 0.01866374
Iteration 77, loss = 0.01850063
Iteration 78, loss = 0.01849408
Iteration 79, loss = 0.01833246
Iteration 80, loss = 0.01822025
Iteration 81, loss = 0.01826181
Iteration 82, loss = 0.01819531
Iteration 83, loss = 0.01779641
Iteration 84, loss = 0.01765538
Iteration 85, loss = 0.01761492
Iteration 86, loss = 0.01749907
Iteration 87, loss = 0.01743975
Iteration 88, loss = 0.01727893
Iteration 89, loss = 0.01716911
Iteration 90, loss = 0.01709501
Iteration 91, loss = 0.01698808
Iteration 92, loss = 0.01701874
Iteration 93, loss = 0.01696715
Iteration 94, loss = 0.01689328
Iteration 95, loss = 0.01668124
Iteration 96, loss = 0.01664772
Iteration 97, loss = 0.01644050
Iteration 98, loss = 0.01646192
Iteration 99, loss = 0.01663907
Iteration 100, loss = 0.01663378
Iteration 101, loss = 0.01655284
Iteration 102, loss = 0.01637768
Iteration 103, loss = 0.01633030
Iteration 104, loss = 0.01615692
Iteration 105, loss = 0.01608925
Iteration 106, loss = 0.01622115
Iteration 107, loss = 0.01625390
Iteration 108, loss = 0.01625601
Iteration 109, loss = 0.01613217
Iteration 110, loss = 0.01599672
Iteration 111, loss = 0.01583995
Iteration 112, loss = 0.01577514
Iteration 113, loss = 0.01555545
Iteration 114, loss = 0.01551557
Iteration 115, loss = 0.01550333
Iteration 116, loss = 0.01545561
Iteration 117, loss = 0.01570357
Iteration 118, loss = 0.01610926
Iteration 119, loss = 0.01626727
Iteration 120, loss = 0.01593450
Iteration 121, loss = 0.01563877
Iteration 122, loss = 0.01544168
Iteration 123, loss = 0.01541115
Iteration 124, loss = 0.01528945
Iteration 125, loss = 0.01515522
Iteration 126, loss = 0.01508991
Iteration 127, loss = 0.01504314
Iteration 128, loss = 0.01505988
Iteration 129, loss = 0.01494301
Iteration 130, loss = 0.01497511
Iteration 131, loss = 0.01492810
Iteration 132, loss = 0.01492616
Iteration 133, loss = 0.01496163
Iteration 134, loss = 0.01500505
Iteration 135, loss = 0.01492204
Iteration 136, loss = 0.01487947
Iteration 137, loss = 0.01483468
Iteration 138, loss = 0.01479158
Iteration 139, loss = 0.01501852
Iteration 140, loss = 0.01491486
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71817266
Iteration 2, loss = 0.69044488
Iteration 3, loss = 0.65870016
Iteration 4, loss = 0.61858460
Iteration 5, loss = 0.57271108
Iteration 6, loss = 0.52345866
Iteration 7, loss = 0.47286653
Iteration 8, loss = 0.42276989
Iteration 9, loss = 0.37500396
Iteration 10, loss = 0.33049501
Iteration 11, loss = 0.29026826
Iteration 12, loss = 0.25495390
Iteration 13, loss = 0.22389745
Iteration 14, loss = 0.19726703
Iteration 15, loss = 0.17435681
Iteration 16, loss = 0.15491121
Iteration 17, loss = 0.13841088
Iteration 18, loss = 0.12421202
Iteration 19, loss = 0.11206303
Iteration 20, loss = 0.10180472
Iteration 21, loss = 0.09287450
Iteration 22, loss = 0.08521318
Iteration 23, loss = 0.07842401
Iteration 24, loss = 0.07257668
Iteration 25, loss = 0.06735279
Iteration 26, loss = 0.06274562
Iteration 27, loss = 0.05866453
Iteration 28, loss = 0.05512279
Iteration 29, loss = 0.05192092
Iteration 30, loss = 0.04904951
Iteration 31, loss = 0.04644835
Iteration 32, loss = 0.04412370
Iteration 33, loss = 0.04199629
Iteration 34, loss = 0.04008018
Iteration 35, loss = 0.03827822
Iteration 36, loss = 0.03661301
Iteration 37, loss = 0.03523205
Iteration 38, loss = 0.03402977
Iteration 39, loss = 0.03277480
Iteration 40, loss = 0.03163718
Iteration 41, loss = 0.03034931
Iteration 42, loss = 0.02975077
Iteration 43, loss = 0.02887749
Iteration 44, loss = 0.02801079
Iteration 45, loss = 0.02715815
Iteration 46, loss = 0.02636730
Iteration 47, loss = 0.02574310
Iteration 48, loss = 0.02510031
Iteration 49, loss = 0.02443733
Iteration 50, loss = 0.02385702
Iteration 51, loss = 0.02347850
Iteration 52, loss = 0.02296899
Iteration 53, loss = 0.02257013
Iteration 54, loss = 0.02212614
Iteration 55, loss = 0.02167074
Iteration 56, loss = 0.02120613
Iteration 57, loss = 0.02088312
Iteration 58, loss = 0.02047060
Iteration 59, loss = 0.02012703
Iteration 60, loss = 0.01983511
Iteration 61, loss = 0.01956705
Iteration 62, loss = 0.01936530
Iteration 63, loss = 0.01899770
Iteration 64, loss = 0.01868825
Iteration 65, loss = 0.01854345
Iteration 66, loss = 0.01842689
Iteration 67, loss = 0.01808323
Iteration 68, loss = 0.01775207
Iteration 69, loss = 0.01772446
Iteration 70, loss = 0.01796312
Iteration 71, loss = 0.01793660
Iteration 72, loss = 0.01754648
Iteration 73, loss = 0.01720466
Iteration 74, loss = 0.01694842
Iteration 75, loss = 0.01673198
Iteration 76, loss = 0.01661922
Iteration 77, loss = 0.01641052
Iteration 78, loss = 0.01623224
Iteration 79, loss = 0.01608870
Iteration 80, loss = 0.01596198
Iteration 81, loss = 0.01583534
Iteration 82, loss = 0.01581185
Iteration 83, loss = 0.01565439
Iteration 84, loss = 0.01558735
Iteration 85, loss = 0.01547555
Iteration 86, loss = 0.01553898
Iteration 87, loss = 0.01539404
Iteration 88, loss = 0.01551667
Iteration 89, loss = 0.01546582
Iteration 90, loss = 0.01522427
Iteration 91, loss = 0.01539995
Iteration 92, loss = 0.01537884
Iteration 93, loss = 0.01513986
Iteration 94, loss = 0.01479567
Iteration 95, loss = 0.01486751
Iteration 96, loss = 0.01470375
Iteration 97, loss = 0.01464267
Iteration 98, loss = 0.01453075
Iteration 99, loss = 0.01431986
Iteration 100, loss = 0.01429820
Iteration 101, loss = 0.01426501
Iteration 102, loss = 0.01415943
Iteration 103, loss = 0.01410783
Iteration 104, loss = 0.01406824
Iteration 105, loss = 0.01396171
Iteration 106, loss = 0.01390748
Iteration 107, loss = 0.01389693
Iteration 108, loss = 0.01387776
Iteration 109, loss = 0.01372693
Iteration 110, loss = 0.01376073
Iteration 111, loss = 0.01370329
Iteration 112, loss = 0.01360714
Iteration 113, loss = 0.01357865
Iteration 114, loss = 0.01351429
Iteration 115, loss = 0.01348069
Iteration 116, loss = 0.01362682
Iteration 117, loss = 0.01363806
Iteration 118, loss = 0.01368876
Iteration 119, loss = 0.01355786
Iteration 120, loss = 0.01344548
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67678747
Iteration 2, loss = 0.63374258
Iteration 3, loss = 0.57007332
Iteration 4, loss = 0.48741537
Iteration 5, loss = 0.40577564
Iteration 6, loss = 0.33142148
Iteration 7, loss = 0.26901325
Iteration 8, loss = 0.21848882
Iteration 9, loss = 0.17914090
Iteration 10, loss = 0.14896914
Iteration 11, loss = 0.12579963
Iteration 12, loss = 0.10801252
Iteration 13, loss = 0.09405220
Iteration 14, loss = 0.08307363
Iteration 15, loss = 0.07451704
Iteration 16, loss = 0.06731846
Iteration 17, loss = 0.06148686
Iteration 18, loss = 0.05655478
Iteration 19, loss = 0.05236495
Iteration 20, loss = 0.04883661
Iteration 21, loss = 0.04603503
Iteration 22, loss = 0.04352321
Iteration 23, loss = 0.04131003
Iteration 24, loss = 0.03926561
Iteration 25, loss = 0.03769417
Iteration 26, loss = 0.03633271
Iteration 27, loss = 0.03518127
Iteration 28, loss = 0.03407596
Iteration 29, loss = 0.03283763
Iteration 30, loss = 0.03180429
Iteration 31, loss = 0.03103806
Iteration 32, loss = 0.03033799
Iteration 33, loss = 0.02996304
Iteration 34, loss = 0.02904662
Iteration 35, loss = 0.02878129
Iteration 36, loss = 0.02801243
Iteration 37, loss = 0.02756990
Iteration 38, loss = 0.02709797
Iteration 39, loss = 0.02670921
Iteration 40, loss = 0.02626237
Iteration 41, loss = 0.02595410
Iteration 42, loss = 0.02574140
Iteration 43, loss = 0.02554908
Iteration 44, loss = 0.02515513
Iteration 45, loss = 0.02481945
Iteration 46, loss = 0.02455396
Iteration 47, loss = 0.02422016
Iteration 48, loss = 0.02425234
Iteration 49, loss = 0.02394298
Iteration 50, loss = 0.02374011
Iteration 51, loss = 0.02375558
Iteration 52, loss = 0.02353436
Iteration 53, loss = 0.02332602
Iteration 54, loss = 0.02318428
Iteration 55, loss = 0.02303000
Iteration 56, loss = 0.02303647
Iteration 57, loss = 0.02310557
Iteration 58, loss = 0.02262807
Iteration 59, loss = 0.02265460
Iteration 60, loss = 0.02252124
Iteration 61, loss = 0.02246577
Iteration 62, loss = 0.02238953
Iteration 63, loss = 0.02216615
Iteration 64, loss = 0.02203538
Iteration 65, loss = 0.02207906
Iteration 66, loss = 0.02209381
Iteration 67, loss = 0.02179817
Iteration 68, loss = 0.02196817
Iteration 69, loss = 0.02167578
Iteration 70, loss = 0.02193799
Iteration 71, loss = 0.02172125
Iteration 72, loss = 0.02195911
Iteration 73, loss = 0.02168946
Iteration 74, loss = 0.02145180
Iteration 75, loss = 0.02145553
Iteration 76, loss = 0.02141184
Iteration 77, loss = 0.02140166
Iteration 78, loss = 0.02161350
Iteration 79, loss = 0.02131283
Iteration 80, loss = 0.02122314
Iteration 81, loss = 0.02111621
Iteration 82, loss = 0.02114469
Iteration 83, loss = 0.02124562
Iteration 84, loss = 0.02110365
Iteration 85, loss = 0.02119246
Iteration 86, loss = 0.02100584
Iteration 87, loss = 0.02085142
Iteration 88, loss = 0.02110717
Iteration 89, loss = 0.02094619
Iteration 90, loss = 0.02085690
Iteration 91, loss = 0.02083624
Iteration 92, loss = 0.02082097
Iteration 93, loss = 0.02099481
Iteration 94, loss = 0.02086241
Iteration 95, loss = 0.02076415
Iteration 96, loss = 0.02098188
Iteration 97, loss = 0.02086590
Iteration 98, loss = 0.02075634
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67830803
Iteration 2, loss = 0.64720267
Iteration 3, loss = 0.60740521
Iteration 4, loss = 0.55488725
Iteration 5, loss = 0.49477577
Iteration 6, loss = 0.43300337
Iteration 7, loss = 0.37282217
Iteration 8, loss = 0.31783265
Iteration 9, loss = 0.26992012
Iteration 10, loss = 0.22894694
Iteration 11, loss = 0.19480741
Iteration 12, loss = 0.16691628
Iteration 13, loss = 0.14396798
Iteration 14, loss = 0.12534012
Iteration 15, loss = 0.10999546
Iteration 16, loss = 0.09744211
Iteration 17, loss = 0.08713676
Iteration 18, loss = 0.07841294
Iteration 19, loss = 0.07129859
Iteration 20, loss = 0.06521565
Iteration 21, loss = 0.05974774
Iteration 22, loss = 0.05532363
Iteration 23, loss = 0.05153467
Iteration 24, loss = 0.04820133
Iteration 25, loss = 0.04530131
Iteration 26, loss = 0.04270556
Iteration 27, loss = 0.04035505
Iteration 28, loss = 0.03833677
Iteration 29, loss = 0.03656893
Iteration 30, loss = 0.03495915
Iteration 31, loss = 0.03353860
Iteration 32, loss = 0.03221751
Iteration 33, loss = 0.03095350
Iteration 34, loss = 0.02999444
Iteration 35, loss = 0.02903095
Iteration 36, loss = 0.02823296
Iteration 37, loss = 0.02737565
Iteration 38, loss = 0.02652737
Iteration 39, loss = 0.02593642
Iteration 40, loss = 0.02538480
Iteration 41, loss = 0.02467572
Iteration 42, loss = 0.02420358
Iteration 43, loss = 0.02358569
Iteration 44, loss = 0.02315533
Iteration 45, loss = 0.02272712
Iteration 46, loss = 0.02229778
Iteration 47, loss = 0.02194030
Iteration 48, loss = 0.02153715
Iteration 49, loss = 0.02127972
Iteration 50, loss = 0.02095894
Iteration 51, loss = 0.02080717
Iteration 52, loss = 0.02057968
Iteration 53, loss = 0.02019240
Iteration 54, loss = 0.01996277
Iteration 55, loss = 0.01979730
Iteration 56, loss = 0.01948527
Iteration 57, loss = 0.01926824
Iteration 58, loss = 0.01915611
Iteration 59, loss = 0.01886674
Iteration 60, loss = 0.01885659
Iteration 61, loss = 0.01860790
Iteration 62, loss = 0.01849904
Iteration 63, loss = 0.01839330
Iteration 64, loss = 0.01809498
Iteration 65, loss = 0.01804168
Iteration 66, loss = 0.01788952
Iteration 67, loss = 0.01787134
Iteration 68, loss = 0.01772173
Iteration 69, loss = 0.01766056
Iteration 70, loss = 0.01747311
Iteration 71, loss = 0.01737606
Iteration 72, loss = 0.01731763
Iteration 73, loss = 0.01717566
Iteration 74, loss = 0.01704273
Iteration 75, loss = 0.01713259
Iteration 76, loss = 0.01703006
Iteration 77, loss = 0.01684565
Iteration 78, loss = 0.01685076
Iteration 79, loss = 0.01666331
Iteration 80, loss = 0.01672490
Iteration 81, loss = 0.01672611
Iteration 82, loss = 0.01645464
Iteration 83, loss = 0.01636080
Iteration 84, loss = 0.01641295
Iteration 85, loss = 0.01635294
Iteration 86, loss = 0.01626080
Iteration 87, loss = 0.01623433
Iteration 88, loss = 0.01622262
Iteration 89, loss = 0.01612048
Iteration 90, loss = 0.01613875
Iteration 91, loss = 0.01623648
Iteration 92, loss = 0.01584375
Iteration 93, loss = 0.01589329
Iteration 94, loss = 0.01601535
Iteration 95, loss = 0.01584910
Iteration 96, loss = 0.01593301
Iteration 97, loss = 0.01575492
Iteration 98, loss = 0.01590183
Iteration 99, loss = 0.01568756
Iteration 100, loss = 0.01575788
Iteration 101, loss = 0.01576109
Iteration 102, loss = 0.01561071
Iteration 103, loss = 0.01573581
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.66718097
Iteration 2, loss = 0.65053596
Iteration 3, loss = 0.63597538
Iteration 4, loss = 0.62118744
Iteration 5, loss = 0.60533863
Iteration 6, loss = 0.58872797
Iteration 7, loss = 0.57068445
Iteration 8, loss = 0.55173267
Iteration 9, loss = 0.53162308
Iteration 10, loss = 0.51099476
Iteration 11, loss = 0.48954641
Iteration 12, loss = 0.46799016
Iteration 13, loss = 0.44607670
Iteration 14, loss = 0.42435064
Iteration 15, loss = 0.40281068
Iteration 16, loss = 0.38169614
Iteration 17, loss = 0.36121195
Iteration 18, loss = 0.34138553
Iteration 19, loss = 0.32226720
Iteration 20, loss = 0.30413258
Iteration 21, loss = 0.28690199
Iteration 22, loss = 0.27043866
Iteration 23, loss = 0.25476869
Iteration 24, loss = 0.24002512
Iteration 25, loss = 0.22623889
Iteration 26, loss = 0.21305627
Iteration 27, loss = 0.20085489
Iteration 28, loss = 0.18905758
Iteration 29, loss = 0.17840943
Iteration 30, loss = 0.16819096
Iteration 31, loss = 0.15879597
Iteration 32, loss = 0.14999129
Iteration 33, loss = 0.14169284
Iteration 34, loss = 0.13393620
Iteration 35, loss = 0.12680699
Iteration 36, loss = 0.12007000
Iteration 37, loss = 0.11385744
Iteration 38, loss = 0.10803464
Iteration 39, loss = 0.10262101
Iteration 40, loss = 0.09754460
Iteration 41, loss = 0.09295114
Iteration 42, loss = 0.08855558
Iteration 43, loss = 0.08449649
Iteration 44, loss = 0.08070888
Iteration 45, loss = 0.07717449
Iteration 46, loss = 0.07384776
Iteration 47, loss = 0.07076313
Iteration 48, loss = 0.06786344
Iteration 49, loss = 0.06512182
Iteration 50, loss = 0.06252553
Iteration 51, loss = 0.06017050
Iteration 52, loss = 0.05792402
Iteration 53, loss = 0.05577410
Iteration 54, loss = 0.05379160
Iteration 55, loss = 0.05189652
Iteration 56, loss = 0.05016395
Iteration 57, loss = 0.04850319
Iteration 58, loss = 0.04691866
Iteration 59, loss = 0.04541712
Iteration 60, loss = 0.04402352
Iteration 61, loss = 0.04272919
Iteration 62, loss = 0.04147268
Iteration 63, loss = 0.04026265
Iteration 64, loss = 0.03911624
Iteration 65, loss = 0.03806090
Iteration 66, loss = 0.03703075
Iteration 67, loss = 0.03603823
Iteration 68, loss = 0.03511055
Iteration 69, loss = 0.03418731
Iteration 70, loss = 0.03336089
Iteration 71, loss = 0.03253109
Iteration 72, loss = 0.03174351
Iteration 73, loss = 0.03102044
Iteration 74, loss = 0.03032530
Iteration 75, loss = 0.02963170
Iteration 76, loss = 0.02896890
Iteration 77, loss = 0.02837840
Iteration 78, loss = 0.02778935
Iteration 79, loss = 0.02718810
Iteration 80, loss = 0.02666433
Iteration 81, loss = 0.02611544
Iteration 82, loss = 0.02562369
Iteration 83, loss = 0.02515933
Iteration 84, loss = 0.02466432
Iteration 85, loss = 0.02424674
Iteration 86, loss = 0.02380327
Iteration 87, loss = 0.02342167
Iteration 88, loss = 0.02302998
Iteration 89, loss = 0.02263121
Iteration 90, loss = 0.02224498
Iteration 91, loss = 0.02190435
Iteration 92, loss = 0.02152879
Iteration 93, loss = 0.02120143
Iteration 94, loss = 0.02086924
Iteration 95, loss = 0.02052653
Iteration 96, loss = 0.02023465
Iteration 97, loss = 0.01994015
Iteration 98, loss = 0.01967813
Iteration 99, loss = 0.01938491
Iteration 100, loss = 0.01912417
Iteration 101, loss = 0.01884950
Iteration 102, loss = 0.01860674
Iteration 103, loss = 0.01834801
Iteration 104, loss = 0.01810307
Iteration 105, loss = 0.01787941
Iteration 106, loss = 0.01761789
Iteration 107, loss = 0.01740736
Iteration 108, loss = 0.01718822
Iteration 109, loss = 0.01698668
Iteration 110, loss = 0.01681852
Iteration 111, loss = 0.01665261
Iteration 112, loss = 0.01646219
Iteration 113, loss = 0.01626742
Iteration 114, loss = 0.01610781
Iteration 115, loss = 0.01594205
Iteration 116, loss = 0.01578312
Iteration 117, loss = 0.01561072
Iteration 118, loss = 0.01548107
Iteration 119, loss = 0.01529878
Iteration 120, loss = 0.01518766
Iteration 121, loss = 0.01507710
Iteration 122, loss = 0.01488623
Iteration 123, loss = 0.01472390
Iteration 124, loss = 0.01457830
Iteration 125, loss = 0.01447618
Iteration 126, loss = 0.01432099
Iteration 127, loss = 0.01417345
Iteration 128, loss = 0.01405771
Iteration 129, loss = 0.01399415
Iteration 130, loss = 0.01384088
Iteration 131, loss = 0.01373835
Iteration 132, loss = 0.01360842
Iteration 133, loss = 0.01351460
Iteration 134, loss = 0.01340462
Iteration 135, loss = 0.01330459
Iteration 136, loss = 0.01322413
Iteration 137, loss = 0.01311494
Iteration 138, loss = 0.01303687
Iteration 139, loss = 0.01293403
Iteration 140, loss = 0.01284061
Iteration 141, loss = 0.01278150
Iteration 142, loss = 0.01264762
Iteration 143, loss = 0.01253868
Iteration 144, loss = 0.01249524
Iteration 145, loss = 0.01238478
Iteration 146, loss = 0.01232010
Iteration 147, loss = 0.01221456
Iteration 148, loss = 0.01217379
Iteration 149, loss = 0.01206879
Iteration 150, loss = 0.01197671
Iteration 151, loss = 0.01192806
Iteration 152, loss = 0.01184770
Iteration 153, loss = 0.01178244
Iteration 154, loss = 0.01174769
Iteration 155, loss = 0.01166088
Iteration 156, loss = 0.01168295
Iteration 157, loss = 0.01159843
Iteration 158, loss = 0.01150651
Iteration 159, loss = 0.01145422
Iteration 160, loss = 0.01137087
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70014420
Iteration 2, loss = 0.66616363
Iteration 3, loss = 0.62577894
Iteration 4, loss = 0.57732656
Iteration 5, loss = 0.52095642
Iteration 6, loss = 0.46048237
Iteration 7, loss = 0.40061346
Iteration 8, loss = 0.34440712
Iteration 9, loss = 0.29401721
Iteration 10, loss = 0.25029541
Iteration 11, loss = 0.21327695
Iteration 12, loss = 0.18229357
Iteration 13, loss = 0.15690491
Iteration 14, loss = 0.13597879
Iteration 15, loss = 0.11889015
Iteration 16, loss = 0.10470945
Iteration 17, loss = 0.09301908
Iteration 18, loss = 0.08331596
Iteration 19, loss = 0.07500787
Iteration 20, loss = 0.06814204
Iteration 21, loss = 0.06220884
Iteration 22, loss = 0.05717746
Iteration 23, loss = 0.05278820
Iteration 24, loss = 0.04901868
Iteration 25, loss = 0.04575074
Iteration 26, loss = 0.04273079
Iteration 27, loss = 0.04019431
Iteration 28, loss = 0.03791059
Iteration 29, loss = 0.03579012
Iteration 30, loss = 0.03397130
Iteration 31, loss = 0.03237836
Iteration 32, loss = 0.03091191
Iteration 33, loss = 0.02968868
Iteration 34, loss = 0.02834377
Iteration 35, loss = 0.02717339
Iteration 36, loss = 0.02620838
Iteration 37, loss = 0.02539106
Iteration 38, loss = 0.02448686
Iteration 39, loss = 0.02374395
Iteration 40, loss = 0.02296422
Iteration 41, loss = 0.02233430
Iteration 42, loss = 0.02174768
Iteration 43, loss = 0.02118271
Iteration 44, loss = 0.02061228
Iteration 45, loss = 0.02022175
Iteration 46, loss = 0.01968010
Iteration 47, loss = 0.01925752
Iteration 48, loss = 0.01885148
Iteration 49, loss = 0.01847349
Iteration 50, loss = 0.01817353
Iteration 51, loss = 0.01786731
Iteration 52, loss = 0.01763394
Iteration 53, loss = 0.01711823
Iteration 54, loss = 0.01695136
Iteration 55, loss = 0.01668461
Iteration 56, loss = 0.01642248
Iteration 57, loss = 0.01617243
Iteration 58, loss = 0.01590280
Iteration 59, loss = 0.01588731
Iteration 60, loss = 0.01560958
Iteration 61, loss = 0.01542218
Iteration 62, loss = 0.01526060
Iteration 63, loss = 0.01505648
Iteration 64, loss = 0.01491432
Iteration 65, loss = 0.01477291
Iteration 66, loss = 0.01464533
Iteration 67, loss = 0.01451570
Iteration 68, loss = 0.01434584
Iteration 69, loss = 0.01418759
Iteration 70, loss = 0.01409857
Iteration 71, loss = 0.01404223
Iteration 72, loss = 0.01390889
Iteration 73, loss = 0.01378552
Iteration 74, loss = 0.01372889
Iteration 75, loss = 0.01363316
Iteration 76, loss = 0.01352122
Iteration 77, loss = 0.01347637
Iteration 78, loss = 0.01330935
Iteration 79, loss = 0.01325794
Iteration 80, loss = 0.01318740
Iteration 81, loss = 0.01309914
Iteration 82, loss = 0.01297970
Iteration 83, loss = 0.01297750
Iteration 84, loss = 0.01290606
Iteration 85, loss = 0.01279381
Iteration 86, loss = 0.01274620
Iteration 87, loss = 0.01267119
Iteration 88, loss = 0.01264370
Iteration 89, loss = 0.01264222
Iteration 90, loss = 0.01254833
Iteration 91, loss = 0.01251388
Iteration 92, loss = 0.01243316
Iteration 93, loss = 0.01232461
Iteration 94, loss = 0.01233043
Iteration 95, loss = 0.01232665
Iteration 96, loss = 0.01222067
Iteration 97, loss = 0.01217546
Iteration 98, loss = 0.01223718
Iteration 99, loss = 0.01207669
Iteration 100, loss = 0.01209350
Iteration 101, loss = 0.01203076
Iteration 102, loss = 0.01200755
Iteration 103, loss = 0.01199272
Iteration 104, loss = 0.01192404
Iteration 105, loss = 0.01197670
Iteration 106, loss = 0.01187013
Iteration 107, loss = 0.01186979
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67798548
Iteration 2, loss = 0.63458619
Iteration 3, loss = 0.56787479
Iteration 4, loss = 0.48610988
Iteration 5, loss = 0.40540250
Iteration 6, loss = 0.33218306
Iteration 7, loss = 0.26995319
Iteration 8, loss = 0.21949987
Iteration 9, loss = 0.18016824
Iteration 10, loss = 0.14967210
Iteration 11, loss = 0.12625967
Iteration 12, loss = 0.10832152
Iteration 13, loss = 0.09398406
Iteration 14, loss = 0.08288018
Iteration 15, loss = 0.07388485
Iteration 16, loss = 0.06662506
Iteration 17, loss = 0.06061781
Iteration 18, loss = 0.05549383
Iteration 19, loss = 0.05139502
Iteration 20, loss = 0.04779089
Iteration 21, loss = 0.04496213
Iteration 22, loss = 0.04225957
Iteration 23, loss = 0.04010760
Iteration 24, loss = 0.03816944
Iteration 25, loss = 0.03638828
Iteration 26, loss = 0.03477631
Iteration 27, loss = 0.03340519
Iteration 28, loss = 0.03234613
Iteration 29, loss = 0.03117761
Iteration 30, loss = 0.03038600
Iteration 31, loss = 0.02945505
Iteration 32, loss = 0.02848816
Iteration 33, loss = 0.02765347
Iteration 34, loss = 0.02712393
Iteration 35, loss = 0.02663468
Iteration 36, loss = 0.02632205
Iteration 37, loss = 0.02581000
Iteration 38, loss = 0.02525082
Iteration 39, loss = 0.02466575
Iteration 40, loss = 0.02430505
Iteration 41, loss = 0.02395709
Iteration 42, loss = 0.02360081
Iteration 43, loss = 0.02352816
Iteration 44, loss = 0.02284465
Iteration 45, loss = 0.02296105
Iteration 46, loss = 0.02231694
Iteration 47, loss = 0.02241587
Iteration 48, loss = 0.02220303
Iteration 49, loss = 0.02178868
Iteration 50, loss = 0.02182718
Iteration 51, loss = 0.02166315
Iteration 52, loss = 0.02139065
Iteration 53, loss = 0.02119139
Iteration 54, loss = 0.02095168
Iteration 55, loss = 0.02098213
Iteration 56, loss = 0.02106356
Iteration 57, loss = 0.02063135
Iteration 58, loss = 0.02046107
Iteration 59, loss = 0.02036418
Iteration 60, loss = 0.02025017
Iteration 61, loss = 0.02007191
Iteration 62, loss = 0.01993174
Iteration 63, loss = 0.02028409
Iteration 64, loss = 0.02009843
Iteration 65, loss = 0.01980310
Iteration 66, loss = 0.01949041
Iteration 67, loss = 0.01964739
Iteration 68, loss = 0.01948508
Iteration 69, loss = 0.01932727
Iteration 70, loss = 0.01938010
Iteration 71, loss = 0.01945619
Iteration 72, loss = 0.01924850
Iteration 73, loss = 0.01913354
Iteration 74, loss = 0.01911503
Iteration 75, loss = 0.01923513
Iteration 76, loss = 0.01915314
Iteration 77, loss = 0.01916266
Iteration 78, loss = 0.01895914
Iteration 79, loss = 0.01900606
Iteration 80, loss = 0.01890456
Iteration 81, loss = 0.01883812
Iteration 82, loss = 0.01898182
Iteration 83, loss = 0.01875690
Iteration 84, loss = 0.01897951
Iteration 85, loss = 0.01864117
Iteration 86, loss = 0.01878299
Iteration 87, loss = 0.01877350
Iteration 88, loss = 0.01880074
Iteration 89, loss = 0.01882537
Iteration 90, loss = 0.01846318
Iteration 91, loss = 0.01851824
Iteration 92, loss = 0.01869832
Iteration 93, loss = 0.01849182
Iteration 94, loss = 0.01853731
Iteration 95, loss = 0.01848426
Iteration 96, loss = 0.01834867
Iteration 97, loss = 0.01820040
Iteration 98, loss = 0.01816288
Iteration 99, loss = 0.01839484
Iteration 100, loss = 0.01833960
Iteration 101, loss = 0.01817835
Iteration 102, loss = 0.01839816
Iteration 103, loss = 0.01811392
Iteration 104, loss = 0.01807490
Iteration 105, loss = 0.01820162
Iteration 106, loss = 0.01827156
Iteration 107, loss = 0.01813496
Iteration 108, loss = 0.01799771
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71134551
Iteration 2, loss = 0.64918815
Iteration 3, loss = 0.56697270
Iteration 4, loss = 0.47253902
Iteration 5, loss = 0.38064999
Iteration 6, loss = 0.30196793
Iteration 7, loss = 0.23886332
Iteration 8, loss = 0.19078073
Iteration 9, loss = 0.15467100
Iteration 10, loss = 0.12778279
Iteration 11, loss = 0.10775962
Iteration 12, loss = 0.09235165
Iteration 13, loss = 0.08047524
Iteration 14, loss = 0.07137839
Iteration 15, loss = 0.06381935
Iteration 16, loss = 0.05788259
Iteration 17, loss = 0.05308679
Iteration 18, loss = 0.04885305
Iteration 19, loss = 0.04551463
Iteration 20, loss = 0.04259179
Iteration 21, loss = 0.04067304
Iteration 22, loss = 0.03840483
Iteration 23, loss = 0.03632853
Iteration 24, loss = 0.03462813
Iteration 25, loss = 0.03333606
Iteration 26, loss = 0.03209429
Iteration 27, loss = 0.03094752
Iteration 28, loss = 0.02999872
Iteration 29, loss = 0.02928029
Iteration 30, loss = 0.02837405
Iteration 31, loss = 0.02771762
Iteration 32, loss = 0.02699891
Iteration 33, loss = 0.02664671
Iteration 34, loss = 0.02597561
Iteration 35, loss = 0.02543983
Iteration 36, loss = 0.02484150
Iteration 37, loss = 0.02470657
Iteration 38, loss = 0.02410772
Iteration 39, loss = 0.02403083
Iteration 40, loss = 0.02355432
Iteration 41, loss = 0.02329141
Iteration 42, loss = 0.02308618
Iteration 43, loss = 0.02285398
Iteration 44, loss = 0.02243689
Iteration 45, loss = 0.02261670
Iteration 46, loss = 0.02209295
Iteration 47, loss = 0.02200675
Iteration 48, loss = 0.02174737
Iteration 49, loss = 0.02157745
Iteration 50, loss = 0.02149433
Iteration 51, loss = 0.02129279
Iteration 52, loss = 0.02124906
Iteration 53, loss = 0.02099831
Iteration 54, loss = 0.02104325
Iteration 55, loss = 0.02099350
Iteration 56, loss = 0.02084533
Iteration 57, loss = 0.02071157
Iteration 58, loss = 0.02073760
Iteration 59, loss = 0.02038223
Iteration 60, loss = 0.02049554
Iteration 61, loss = 0.02011891
Iteration 62, loss = 0.02024682
Iteration 63, loss = 0.02021565
Iteration 64, loss = 0.01993338
Iteration 65, loss = 0.02006844
Iteration 66, loss = 0.01974807
Iteration 67, loss = 0.01991417
Iteration 68, loss = 0.01972511
Iteration 69, loss = 0.01975293
Iteration 70, loss = 0.01989194
Iteration 71, loss = 0.01957410
Iteration 72, loss = 0.01972385
Iteration 73, loss = 0.01966385
Iteration 74, loss = 0.01933696
Iteration 75, loss = 0.01928479
Iteration 76, loss = 0.01953513
Iteration 77, loss = 0.01946104
Iteration 78, loss = 0.01938498
Iteration 79, loss = 0.01924386
Iteration 80, loss = 0.01922660
Iteration 81, loss = 0.01920242
Iteration 82, loss = 0.01919836
Iteration 83, loss = 0.01936081
Iteration 84, loss = 0.01930547
Iteration 85, loss = 0.01917823
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69007924
Iteration 2, loss = 0.64243952
Iteration 3, loss = 0.57810742
Iteration 4, loss = 0.49760706
Iteration 5, loss = 0.41614292
Iteration 6, loss = 0.34124016
Iteration 7, loss = 0.27719841
Iteration 8, loss = 0.22510093
Iteration 9, loss = 0.18406154
Iteration 10, loss = 0.15268944
Iteration 11, loss = 0.12857166
Iteration 12, loss = 0.10998837
Iteration 13, loss = 0.09584925
Iteration 14, loss = 0.08447809
Iteration 15, loss = 0.07523044
Iteration 16, loss = 0.06792842
Iteration 17, loss = 0.06221975
Iteration 18, loss = 0.05713466
Iteration 19, loss = 0.05291226
Iteration 20, loss = 0.04951854
Iteration 21, loss = 0.04670150
Iteration 22, loss = 0.04407362
Iteration 23, loss = 0.04179260
Iteration 24, loss = 0.04013384
Iteration 25, loss = 0.03827566
Iteration 26, loss = 0.03683249
Iteration 27, loss = 0.03567396
Iteration 28, loss = 0.03427840
Iteration 29, loss = 0.03317193
Iteration 30, loss = 0.03230510
Iteration 31, loss = 0.03149250
Iteration 32, loss = 0.03054710
Iteration 33, loss = 0.03002377
Iteration 34, loss = 0.02931867
Iteration 35, loss = 0.02892034
Iteration 36, loss = 0.02818850
Iteration 37, loss = 0.02809344
Iteration 38, loss = 0.02775513
Iteration 39, loss = 0.02693147
Iteration 40, loss = 0.02670919
Iteration 41, loss = 0.02619284
Iteration 42, loss = 0.02603121
Iteration 43, loss = 0.02560503
Iteration 44, loss = 0.02531073
Iteration 45, loss = 0.02498456
Iteration 46, loss = 0.02482196
Iteration 47, loss = 0.02455788
Iteration 48, loss = 0.02459158
Iteration 49, loss = 0.02415951
Iteration 50, loss = 0.02400160
Iteration 51, loss = 0.02377801
Iteration 52, loss = 0.02368250
Iteration 53, loss = 0.02344922
Iteration 54, loss = 0.02360597
Iteration 55, loss = 0.02321681
Iteration 56, loss = 0.02312832
Iteration 57, loss = 0.02268266
Iteration 58, loss = 0.02315779
Iteration 59, loss = 0.02282265
Iteration 60, loss = 0.02281365
Iteration 61, loss = 0.02276350
Iteration 62, loss = 0.02249374
Iteration 63, loss = 0.02271725
Iteration 64, loss = 0.02245964
Iteration 65, loss = 0.02216383
Iteration 66, loss = 0.02206655
Iteration 67, loss = 0.02207447
Iteration 68, loss = 0.02190936
Iteration 69, loss = 0.02184326
Iteration 70, loss = 0.02194755
Iteration 71, loss = 0.02161644
Iteration 72, loss = 0.02161127
Iteration 73, loss = 0.02173196
Iteration 74, loss = 0.02161146
Iteration 75, loss = 0.02171725
Iteration 76, loss = 0.02133806
Iteration 77, loss = 0.02137069
Iteration 78, loss = 0.02132076
Iteration 79, loss = 0.02136878
Iteration 80, loss = 0.02155641
Iteration 81, loss = 0.02153570
Iteration 82, loss = 0.02161001
Iteration 83, loss = 0.02120338
Iteration 84, loss = 0.02142869
Iteration 85, loss = 0.02139632
Iteration 86, loss = 0.02107991
Iteration 87, loss = 0.02103886
Iteration 88, loss = 0.02121015
Iteration 89, loss = 0.02142145
Iteration 90, loss = 0.02148096
Iteration 91, loss = 0.02104291
Iteration 92, loss = 0.02100523
Iteration 93, loss = 0.02097757
Iteration 94, loss = 0.02106131
Iteration 95, loss = 0.02113426
Iteration 96, loss = 0.02101484
Iteration 97, loss = 0.02082762
Iteration 98, loss = 0.02081592
Iteration 99, loss = 0.02084193
Iteration 100, loss = 0.02103770
Iteration 101, loss = 0.02089082
Iteration 102, loss = 0.02086101
Iteration 103, loss = 0.02096426
Iteration 104, loss = 0.02053237
Iteration 105, loss = 0.02078806
Iteration 106, loss = 0.02076317
Iteration 107, loss = 0.02058969
Iteration 108, loss = 0.02052710
Iteration 109, loss = 0.02075315
Iteration 110, loss = 0.02061389
Iteration 111, loss = 0.02069202
Iteration 112, loss = 0.02092204
Iteration 113, loss = 0.02069952
Iteration 114, loss = 0.02095014
Iteration 115, loss = 0.02051169
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71653091
Iteration 2, loss = 0.66820599
Iteration 3, loss = 0.59678786
Iteration 4, loss = 0.50678589
Iteration 5, loss = 0.41551976
Iteration 6, loss = 0.33224293
Iteration 7, loss = 0.26365301
Iteration 8, loss = 0.21026750
Iteration 9, loss = 0.16978827
Iteration 10, loss = 0.13983710
Iteration 11, loss = 0.11717880
Iteration 12, loss = 0.10015251
Iteration 13, loss = 0.08704443
Iteration 14, loss = 0.07701299
Iteration 15, loss = 0.06906352
Iteration 16, loss = 0.06272491
Iteration 17, loss = 0.05729257
Iteration 18, loss = 0.05286828
Iteration 19, loss = 0.04930529
Iteration 20, loss = 0.04618467
Iteration 21, loss = 0.04382620
Iteration 22, loss = 0.04142117
Iteration 23, loss = 0.03948675
Iteration 24, loss = 0.03805156
Iteration 25, loss = 0.03641308
Iteration 26, loss = 0.03512270
Iteration 27, loss = 0.03398448
Iteration 28, loss = 0.03288794
Iteration 29, loss = 0.03202683
Iteration 30, loss = 0.03134847
Iteration 31, loss = 0.03043374
Iteration 32, loss = 0.02990892
Iteration 33, loss = 0.02926061
Iteration 34, loss = 0.02890968
Iteration 35, loss = 0.02809549
Iteration 36, loss = 0.02767647
Iteration 37, loss = 0.02725366
Iteration 38, loss = 0.02712298
Iteration 39, loss = 0.02635912
Iteration 40, loss = 0.02652216
Iteration 41, loss = 0.02591126
Iteration 42, loss = 0.02563338
Iteration 43, loss = 0.02557638
Iteration 44, loss = 0.02517013
Iteration 45, loss = 0.02497428
Iteration 46, loss = 0.02472726
Iteration 47, loss = 0.02444522
Iteration 48, loss = 0.02432556
Iteration 49, loss = 0.02421845
Iteration 50, loss = 0.02396768
Iteration 51, loss = 0.02401135
Iteration 52, loss = 0.02386760
Iteration 53, loss = 0.02341617
Iteration 54, loss = 0.02328154
Iteration 55, loss = 0.02350824
Iteration 56, loss = 0.02323441
Iteration 57, loss = 0.02297112
Iteration 58, loss = 0.02301450
Iteration 59, loss = 0.02279245
Iteration 60, loss = 0.02279154
Iteration 61, loss = 0.02286766
Iteration 62, loss = 0.02258424
Iteration 63, loss = 0.02235717
Iteration 64, loss = 0.02243751
Iteration 65, loss = 0.02231887
Iteration 66, loss = 0.02239656
Iteration 67, loss = 0.02245234
Iteration 68, loss = 0.02250413
Iteration 69, loss = 0.02201986
Iteration 70, loss = 0.02218057
Iteration 71, loss = 0.02194972
Iteration 72, loss = 0.02187747
Iteration 73, loss = 0.02198824
Iteration 74, loss = 0.02184647
Iteration 75, loss = 0.02203425
Iteration 76, loss = 0.02163110
Iteration 77, loss = 0.02163027
Iteration 78, loss = 0.02169109
Iteration 79, loss = 0.02181031
Iteration 80, loss = 0.02171130
Iteration 81, loss = 0.02162207
Iteration 82, loss = 0.02160326
Iteration 83, loss = 0.02134662
Iteration 84, loss = 0.02177730
Iteration 85, loss = 0.02129937
Iteration 86, loss = 0.02130256
Iteration 87, loss = 0.02133821
Iteration 88, loss = 0.02126838
Iteration 89, loss = 0.02134064
Iteration 90, loss = 0.02129493
Iteration 91, loss = 0.02150068
Iteration 92, loss = 0.02120962
Iteration 93, loss = 0.02151277
Iteration 94, loss = 0.02122710
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69413025
Iteration 2, loss = 0.66276547
Iteration 3, loss = 0.61633976
Iteration 4, loss = 0.55454848
Iteration 5, loss = 0.48407387
Iteration 6, loss = 0.41258948
Iteration 7, loss = 0.34574020
Iteration 8, loss = 0.28726832
Iteration 9, loss = 0.23858875
Iteration 10, loss = 0.19881803
Iteration 11, loss = 0.16714615
Iteration 12, loss = 0.14194907
Iteration 13, loss = 0.12196279
Iteration 14, loss = 0.10610610
Iteration 15, loss = 0.09324100
Iteration 16, loss = 0.08291110
Iteration 17, loss = 0.07426288
Iteration 18, loss = 0.06725784
Iteration 19, loss = 0.06139324
Iteration 20, loss = 0.05631664
Iteration 21, loss = 0.05205046
Iteration 22, loss = 0.04842033
Iteration 23, loss = 0.04516312
Iteration 24, loss = 0.04244840
Iteration 25, loss = 0.04012262
Iteration 26, loss = 0.03791942
Iteration 27, loss = 0.03624868
Iteration 28, loss = 0.03448501
Iteration 29, loss = 0.03288748
Iteration 30, loss = 0.03174158
Iteration 31, loss = 0.03038686
Iteration 32, loss = 0.02937614
Iteration 33, loss = 0.02841137
Iteration 34, loss = 0.02747557
Iteration 35, loss = 0.02666437
Iteration 36, loss = 0.02591454
Iteration 37, loss = 0.02531603
Iteration 38, loss = 0.02471538
Iteration 39, loss = 0.02417097
Iteration 40, loss = 0.02360398
Iteration 41, loss = 0.02308429
Iteration 42, loss = 0.02270703
Iteration 43, loss = 0.02217841
Iteration 44, loss = 0.02187670
Iteration 45, loss = 0.02150336
Iteration 46, loss = 0.02132120
Iteration 47, loss = 0.02075327
Iteration 48, loss = 0.02054047
Iteration 49, loss = 0.02044909
Iteration 50, loss = 0.02006742
Iteration 51, loss = 0.01980869
Iteration 52, loss = 0.01949622
Iteration 53, loss = 0.01924932
Iteration 54, loss = 0.01923655
Iteration 55, loss = 0.01889533
Iteration 56, loss = 0.01882847
Iteration 57, loss = 0.01862658
Iteration 58, loss = 0.01853235
Iteration 59, loss = 0.01838060
Iteration 60, loss = 0.01815157
Iteration 61, loss = 0.01815507
Iteration 62, loss = 0.01794661
Iteration 63, loss = 0.01775773
Iteration 64, loss = 0.01770449
Iteration 65, loss = 0.01743824
Iteration 66, loss = 0.01753318
Iteration 67, loss = 0.01729255
Iteration 68, loss = 0.01717819
Iteration 69, loss = 0.01715181
Iteration 70, loss = 0.01722089
Iteration 71, loss = 0.01681869
Iteration 72, loss = 0.01679622
Iteration 73, loss = 0.01671701
Iteration 74, loss = 0.01669989
Iteration 75, loss = 0.01666906
Iteration 76, loss = 0.01665901
Iteration 77, loss = 0.01653059
Iteration 78, loss = 0.01640740
Iteration 79, loss = 0.01633876
Iteration 80, loss = 0.01640530
Iteration 81, loss = 0.01628337
Iteration 82, loss = 0.01619538
Iteration 83, loss = 0.01623429
Iteration 84, loss = 0.01610860
Iteration 85, loss = 0.01601655
Iteration 86, loss = 0.01595656
Iteration 87, loss = 0.01601070
Iteration 88, loss = 0.01591292
Iteration 89, loss = 0.01600710
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69533702
Iteration 2, loss = 0.65340816
Iteration 3, loss = 0.59940479
Iteration 4, loss = 0.53572402
Iteration 5, loss = 0.46613508
Iteration 6, loss = 0.39787043
Iteration 7, loss = 0.33503843
Iteration 8, loss = 0.27990797
Iteration 9, loss = 0.23335604
Iteration 10, loss = 0.19524769
Iteration 11, loss = 0.16425454
Iteration 12, loss = 0.13979618
Iteration 13, loss = 0.12011879
Iteration 14, loss = 0.10444655
Iteration 15, loss = 0.09179526
Iteration 16, loss = 0.08145089
Iteration 17, loss = 0.07307728
Iteration 18, loss = 0.06615661
Iteration 19, loss = 0.06030108
Iteration 20, loss = 0.05527330
Iteration 21, loss = 0.05106141
Iteration 22, loss = 0.04742323
Iteration 23, loss = 0.04435261
Iteration 24, loss = 0.04174245
Iteration 25, loss = 0.03925367
Iteration 26, loss = 0.03724409
Iteration 27, loss = 0.03526973
Iteration 28, loss = 0.03357823
Iteration 29, loss = 0.03220415
Iteration 30, loss = 0.03112853
Iteration 31, loss = 0.02967077
Iteration 32, loss = 0.02850645
Iteration 33, loss = 0.02761818
Iteration 34, loss = 0.02672807
Iteration 35, loss = 0.02597332
Iteration 36, loss = 0.02517468
Iteration 37, loss = 0.02456381
Iteration 38, loss = 0.02397489
Iteration 39, loss = 0.02333867
Iteration 40, loss = 0.02289668
Iteration 41, loss = 0.02252234
Iteration 42, loss = 0.02190919
Iteration 43, loss = 0.02160672
Iteration 44, loss = 0.02108274
Iteration 45, loss = 0.02076532
Iteration 46, loss = 0.02051540
Iteration 47, loss = 0.02020960
Iteration 48, loss = 0.01990657
Iteration 49, loss = 0.01966363
Iteration 50, loss = 0.01928423
Iteration 51, loss = 0.01916892
Iteration 52, loss = 0.01884764
Iteration 53, loss = 0.01858266
Iteration 54, loss = 0.01849686
Iteration 55, loss = 0.01845048
Iteration 56, loss = 0.01802387
Iteration 57, loss = 0.01795257
Iteration 58, loss = 0.01784958
Iteration 59, loss = 0.01761593
Iteration 60, loss = 0.01763799
Iteration 61, loss = 0.01741960
Iteration 62, loss = 0.01722839
Iteration 63, loss = 0.01708543
Iteration 64, loss = 0.01710782
Iteration 65, loss = 0.01697453
Iteration 66, loss = 0.01690102
Iteration 67, loss = 0.01668934
Iteration 68, loss = 0.01658349
Iteration 69, loss = 0.01645467
Iteration 70, loss = 0.01656991
Iteration 71, loss = 0.01633914
Iteration 72, loss = 0.01636345
Iteration 73, loss = 0.01615766
Iteration 74, loss = 0.01615561
Iteration 75, loss = 0.01611199
Iteration 76, loss = 0.01613134
Iteration 77, loss = 0.01602370
Iteration 78, loss = 0.01580357
Iteration 79, loss = 0.01593918
Iteration 80, loss = 0.01579735
Iteration 81, loss = 0.01571804
Iteration 82, loss = 0.01577977
Iteration 83, loss = 0.01558814
Iteration 84, loss = 0.01546173
Iteration 85, loss = 0.01547207
Iteration 86, loss = 0.01539054
Iteration 87, loss = 0.01541310
Iteration 88, loss = 0.01528729
Iteration 89, loss = 0.01526976
Iteration 90, loss = 0.01523344
Iteration 91, loss = 0.01520498
Iteration 92, loss = 0.01517510
Iteration 93, loss = 0.01519534
Iteration 94, loss = 0.01519817
Iteration 95, loss = 0.01508627
Iteration 96, loss = 0.01533843
Iteration 97, loss = 0.01531639
Iteration 98, loss = 0.01499466
Iteration 99, loss = 0.01514429
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71656907
Iteration 2, loss = 0.69663902
Iteration 3, loss = 0.67859185
Iteration 4, loss = 0.65757044
Iteration 5, loss = 0.63247599
Iteration 6, loss = 0.60345904
Iteration 7, loss = 0.57083085
Iteration 8, loss = 0.53553945
Iteration 9, loss = 0.49896002
Iteration 10, loss = 0.46237609
Iteration 11, loss = 0.42587189
Iteration 12, loss = 0.39098111
Iteration 13, loss = 0.35762527
Iteration 14, loss = 0.32623538
Iteration 15, loss = 0.29717201
Iteration 16, loss = 0.27031743
Iteration 17, loss = 0.24595478
Iteration 18, loss = 0.22377379
Iteration 19, loss = 0.20374016
Iteration 20, loss = 0.18592221
Iteration 21, loss = 0.16990581
Iteration 22, loss = 0.15566664
Iteration 23, loss = 0.14301114
Iteration 24, loss = 0.13170104
Iteration 25, loss = 0.12164174
Iteration 26, loss = 0.11268560
Iteration 27, loss = 0.10467812
Iteration 28, loss = 0.09755770
Iteration 29, loss = 0.09110061
Iteration 30, loss = 0.08539828
Iteration 31, loss = 0.08012202
Iteration 32, loss = 0.07548568
Iteration 33, loss = 0.07122435
Iteration 34, loss = 0.06735870
Iteration 35, loss = 0.06388947
Iteration 36, loss = 0.06064963
Iteration 37, loss = 0.05771568
Iteration 38, loss = 0.05505684
Iteration 39, loss = 0.05255824
Iteration 40, loss = 0.05028421
Iteration 41, loss = 0.04819782
Iteration 42, loss = 0.04621293
Iteration 43, loss = 0.04445184
Iteration 44, loss = 0.04279452
Iteration 45, loss = 0.04125602
Iteration 46, loss = 0.03975528
Iteration 47, loss = 0.03841903
Iteration 48, loss = 0.03711420
Iteration 49, loss = 0.03593840
Iteration 50, loss = 0.03485233
Iteration 51, loss = 0.03382189
Iteration 52, loss = 0.03281114
Iteration 53, loss = 0.03189929
Iteration 54, loss = 0.03102821
Iteration 55, loss = 0.03021976
Iteration 56, loss = 0.02944605
Iteration 57, loss = 0.02870379
Iteration 58, loss = 0.02803052
Iteration 59, loss = 0.02738736
Iteration 60, loss = 0.02675806
Iteration 61, loss = 0.02615373
Iteration 62, loss = 0.02560843
Iteration 63, loss = 0.02509708
Iteration 64, loss = 0.02458482
Iteration 65, loss = 0.02408270
Iteration 66, loss = 0.02361978
Iteration 67, loss = 0.02317840
Iteration 68, loss = 0.02275377
Iteration 69, loss = 0.02242325
Iteration 70, loss = 0.02198699
Iteration 71, loss = 0.02167485
Iteration 72, loss = 0.02129444
Iteration 73, loss = 0.02094826
Iteration 74, loss = 0.02069158
Iteration 75, loss = 0.02030100
Iteration 76, loss = 0.02005190
Iteration 77, loss = 0.01974418
Iteration 78, loss = 0.01946560
Iteration 79, loss = 0.01920054
Iteration 80, loss = 0.01897410
Iteration 81, loss = 0.01874862
Iteration 82, loss = 0.01851352
Iteration 83, loss = 0.01829008
Iteration 84, loss = 0.01809976
Iteration 85, loss = 0.01787136
Iteration 86, loss = 0.01768005
Iteration 87, loss = 0.01746309
Iteration 88, loss = 0.01731537
Iteration 89, loss = 0.01710852
Iteration 90, loss = 0.01697335
Iteration 91, loss = 0.01676162
Iteration 92, loss = 0.01659708
Iteration 93, loss = 0.01649574
Iteration 94, loss = 0.01628304
Iteration 95, loss = 0.01615604
Iteration 96, loss = 0.01602803
Iteration 97, loss = 0.01585284
Iteration 98, loss = 0.01575007
Iteration 99, loss = 0.01561730
Iteration 100, loss = 0.01550143
Iteration 101, loss = 0.01537035
Iteration 102, loss = 0.01527883
Iteration 103, loss = 0.01513223
Iteration 104, loss = 0.01505412
Iteration 105, loss = 0.01491177
Iteration 106, loss = 0.01483451
Iteration 107, loss = 0.01474031
Iteration 108, loss = 0.01461231
Iteration 109, loss = 0.01453531
Iteration 110, loss = 0.01442225
Iteration 111, loss = 0.01435918
Iteration 112, loss = 0.01426789
Iteration 113, loss = 0.01417402
Iteration 114, loss = 0.01407480
Iteration 115, loss = 0.01400993
Iteration 116, loss = 0.01395142
Iteration 117, loss = 0.01383816
Iteration 118, loss = 0.01384871
Iteration 119, loss = 0.01379830
Iteration 120, loss = 0.01365992
Iteration 121, loss = 0.01374599
Iteration 122, loss = 0.01356679
Iteration 123, loss = 0.01348814
Iteration 124, loss = 0.01337540
Iteration 125, loss = 0.01338815
Iteration 126, loss = 0.01328272
Iteration 127, loss = 0.01327756
Iteration 128, loss = 0.01316644
Iteration 129, loss = 0.01309868
Iteration 130, loss = 0.01306918
Iteration 131, loss = 0.01300802
Iteration 132, loss = 0.01294617
Iteration 133, loss = 0.01287333
Iteration 134, loss = 0.01285923
Iteration 135, loss = 0.01281546
Iteration 136, loss = 0.01274332
Iteration 137, loss = 0.01273651
Iteration 138, loss = 0.01265609
Iteration 139, loss = 0.01263401
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67751324
Iteration 2, loss = 0.65687050
Iteration 3, loss = 0.63691889
Iteration 4, loss = 0.61381277
Iteration 5, loss = 0.58669255
Iteration 6, loss = 0.55615097
Iteration 7, loss = 0.52272590
Iteration 8, loss = 0.48734631
Iteration 9, loss = 0.45184971
Iteration 10, loss = 0.41692242
Iteration 11, loss = 0.38336449
Iteration 12, loss = 0.35100271
Iteration 13, loss = 0.32085552
Iteration 14, loss = 0.29262084
Iteration 15, loss = 0.26675003
Iteration 16, loss = 0.24304405
Iteration 17, loss = 0.22157182
Iteration 18, loss = 0.20226051
Iteration 19, loss = 0.18475109
Iteration 20, loss = 0.16931692
Iteration 21, loss = 0.15540293
Iteration 22, loss = 0.14297078
Iteration 23, loss = 0.13193809
Iteration 24, loss = 0.12195305
Iteration 25, loss = 0.11316198
Iteration 26, loss = 0.10528732
Iteration 27, loss = 0.09815219
Iteration 28, loss = 0.09177838
Iteration 29, loss = 0.08609202
Iteration 30, loss = 0.08085369
Iteration 31, loss = 0.07617695
Iteration 32, loss = 0.07195298
Iteration 33, loss = 0.06805080
Iteration 34, loss = 0.06455982
Iteration 35, loss = 0.06128430
Iteration 36, loss = 0.05833186
Iteration 37, loss = 0.05568524
Iteration 38, loss = 0.05324486
Iteration 39, loss = 0.05091460
Iteration 40, loss = 0.04879289
Iteration 41, loss = 0.04684803
Iteration 42, loss = 0.04505064
Iteration 43, loss = 0.04335434
Iteration 44, loss = 0.04179474
Iteration 45, loss = 0.04035401
Iteration 46, loss = 0.03905835
Iteration 47, loss = 0.03777885
Iteration 48, loss = 0.03659099
Iteration 49, loss = 0.03542421
Iteration 50, loss = 0.03435759
Iteration 51, loss = 0.03347040
Iteration 52, loss = 0.03250185
Iteration 53, loss = 0.03163828
Iteration 54, loss = 0.03081339
Iteration 55, loss = 0.03012611
Iteration 56, loss = 0.02930761
Iteration 57, loss = 0.02864804
Iteration 58, loss = 0.02802617
Iteration 59, loss = 0.02740251
Iteration 60, loss = 0.02674071
Iteration 61, loss = 0.02619865
Iteration 62, loss = 0.02570354
Iteration 63, loss = 0.02516966
Iteration 64, loss = 0.02472701
Iteration 65, loss = 0.02423837
Iteration 66, loss = 0.02380807
Iteration 67, loss = 0.02340342
Iteration 68, loss = 0.02298820
Iteration 69, loss = 0.02264178
Iteration 70, loss = 0.02230934
Iteration 71, loss = 0.02189081
Iteration 72, loss = 0.02158902
Iteration 73, loss = 0.02128425
Iteration 74, loss = 0.02094525
Iteration 75, loss = 0.02064903
Iteration 76, loss = 0.02037828
Iteration 77, loss = 0.02014307
Iteration 78, loss = 0.01985272
Iteration 79, loss = 0.01963283
Iteration 80, loss = 0.01936908
Iteration 81, loss = 0.01917628
Iteration 82, loss = 0.01900328
Iteration 83, loss = 0.01875157
Iteration 84, loss = 0.01848473
Iteration 85, loss = 0.01830074
Iteration 86, loss = 0.01813352
Iteration 87, loss = 0.01797603
Iteration 88, loss = 0.01778427
Iteration 89, loss = 0.01764157
Iteration 90, loss = 0.01745295
Iteration 91, loss = 0.01728952
Iteration 92, loss = 0.01713937
Iteration 93, loss = 0.01700407
Iteration 94, loss = 0.01684210
Iteration 95, loss = 0.01669580
Iteration 96, loss = 0.01654172
Iteration 97, loss = 0.01643744
Iteration 98, loss = 0.01633560
Iteration 99, loss = 0.01617635
Iteration 100, loss = 0.01604987
Iteration 101, loss = 0.01591834
Iteration 102, loss = 0.01584365
Iteration 103, loss = 0.01572528
Iteration 104, loss = 0.01567583
Iteration 105, loss = 0.01554943
Iteration 106, loss = 0.01542834
Iteration 107, loss = 0.01534446
Iteration 108, loss = 0.01522410
Iteration 109, loss = 0.01511518
Iteration 110, loss = 0.01505924
Iteration 111, loss = 0.01496706
Iteration 112, loss = 0.01492833
Iteration 113, loss = 0.01479453
Iteration 114, loss = 0.01475449
Iteration 115, loss = 0.01464839
Iteration 116, loss = 0.01461503
Iteration 117, loss = 0.01457872
Iteration 118, loss = 0.01444820
Iteration 119, loss = 0.01440590
Iteration 120, loss = 0.01433114
Iteration 121, loss = 0.01425587
Iteration 122, loss = 0.01418699
Iteration 123, loss = 0.01411505
Iteration 124, loss = 0.01405935
Iteration 125, loss = 0.01403481
Iteration 126, loss = 0.01396127
Iteration 127, loss = 0.01393216
Iteration 128, loss = 0.01383736
Iteration 129, loss = 0.01378152
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67795330
Iteration 2, loss = 0.63628171
Iteration 3, loss = 0.57299549
Iteration 4, loss = 0.49301236
Iteration 5, loss = 0.41051041
Iteration 6, loss = 0.33544900
Iteration 7, loss = 0.27166555
Iteration 8, loss = 0.22006608
Iteration 9, loss = 0.17977559
Iteration 10, loss = 0.14888080
Iteration 11, loss = 0.12523974
Iteration 12, loss = 0.10717008
Iteration 13, loss = 0.09258463
Iteration 14, loss = 0.08141593
Iteration 15, loss = 0.07237470
Iteration 16, loss = 0.06506739
Iteration 17, loss = 0.05932287
Iteration 18, loss = 0.05425134
Iteration 19, loss = 0.04996544
Iteration 20, loss = 0.04651087
Iteration 21, loss = 0.04351180
Iteration 22, loss = 0.04089905
Iteration 23, loss = 0.03873520
Iteration 24, loss = 0.03674099
Iteration 25, loss = 0.03485537
Iteration 26, loss = 0.03345018
Iteration 27, loss = 0.03215752
Iteration 28, loss = 0.03073055
Iteration 29, loss = 0.02966635
Iteration 30, loss = 0.02884531
Iteration 31, loss = 0.02792658
Iteration 32, loss = 0.02710924
Iteration 33, loss = 0.02642970
Iteration 34, loss = 0.02583082
Iteration 35, loss = 0.02506370
Iteration 36, loss = 0.02469656
Iteration 37, loss = 0.02413239
Iteration 38, loss = 0.02389676
Iteration 39, loss = 0.02342914
Iteration 40, loss = 0.02298469
Iteration 41, loss = 0.02270662
Iteration 42, loss = 0.02249572
Iteration 43, loss = 0.02210048
Iteration 44, loss = 0.02180881
Iteration 45, loss = 0.02153262
Iteration 46, loss = 0.02124548
Iteration 47, loss = 0.02099025
Iteration 48, loss = 0.02072897
Iteration 49, loss = 0.02070665
Iteration 50, loss = 0.02033333
Iteration 51, loss = 0.02023169
Iteration 52, loss = 0.02011403
Iteration 53, loss = 0.01986327
Iteration 54, loss = 0.01984561
Iteration 55, loss = 0.01970515
Iteration 56, loss = 0.01954526
Iteration 57, loss = 0.01951022
Iteration 58, loss = 0.01914725
Iteration 59, loss = 0.01940152
Iteration 60, loss = 0.01909705
Iteration 61, loss = 0.01887517
Iteration 62, loss = 0.01892626
Iteration 63, loss = 0.01893866
Iteration 64, loss = 0.01868976
Iteration 65, loss = 0.01862565
Iteration 66, loss = 0.01842054
Iteration 67, loss = 0.01843400
Iteration 68, loss = 0.01839931
Iteration 69, loss = 0.01841881
Iteration 70, loss = 0.01816286
Iteration 71, loss = 0.01818038
Iteration 72, loss = 0.01809175
Iteration 73, loss = 0.01793001
Iteration 74, loss = 0.01811585
Iteration 75, loss = 0.01793707
Iteration 76, loss = 0.01779437
Iteration 77, loss = 0.01769652
Iteration 78, loss = 0.01792173
Iteration 79, loss = 0.01786139
Iteration 80, loss = 0.01792933
Iteration 81, loss = 0.01764478
Iteration 82, loss = 0.01787814
Iteration 83, loss = 0.01771860
Iteration 84, loss = 0.01769569
Iteration 85, loss = 0.01748772
Iteration 86, loss = 0.01759917
Iteration 87, loss = 0.01767388
Iteration 88, loss = 0.01734645
Iteration 89, loss = 0.01761102
Iteration 90, loss = 0.01772970
Iteration 91, loss = 0.01746530
Iteration 92, loss = 0.01737878
Iteration 93, loss = 0.01743668
Iteration 94, loss = 0.01731431
Iteration 95, loss = 0.01706920
Iteration 96, loss = 0.01711063
Iteration 97, loss = 0.01718465
Iteration 98, loss = 0.01713101
Iteration 99, loss = 0.01701759
Iteration 100, loss = 0.01713995
Iteration 101, loss = 0.01713470
Iteration 102, loss = 0.01734657
Iteration 103, loss = 0.01718279
Iteration 104, loss = 0.01727097
Iteration 105, loss = 0.01707283
Iteration 106, loss = 0.01698140
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73265893
Iteration 2, loss = 0.71376232
Iteration 3, loss = 0.69844554
Iteration 4, loss = 0.68410208
Iteration 5, loss = 0.66970264
Iteration 6, loss = 0.65428219
Iteration 7, loss = 0.63823871
Iteration 8, loss = 0.62101631
Iteration 9, loss = 0.60244892
Iteration 10, loss = 0.58254341
Iteration 11, loss = 0.56143578
Iteration 12, loss = 0.53951420
Iteration 13, loss = 0.51633035
Iteration 14, loss = 0.49281700
Iteration 15, loss = 0.46882649
Iteration 16, loss = 0.44493010
Iteration 17, loss = 0.42132142
Iteration 18, loss = 0.39802080
Iteration 19, loss = 0.37536027
Iteration 20, loss = 0.35350386
Iteration 21, loss = 0.33249015
Iteration 22, loss = 0.31246319
Iteration 23, loss = 0.29347206
Iteration 24, loss = 0.27550023
Iteration 25, loss = 0.25859799
Iteration 26, loss = 0.24282665
Iteration 27, loss = 0.22805437
Iteration 28, loss = 0.21425110
Iteration 29, loss = 0.20132133
Iteration 30, loss = 0.18935185
Iteration 31, loss = 0.17828999
Iteration 32, loss = 0.16799066
Iteration 33, loss = 0.15841018
Iteration 34, loss = 0.14955613
Iteration 35, loss = 0.14141939
Iteration 36, loss = 0.13380150
Iteration 37, loss = 0.12675693
Iteration 38, loss = 0.12024886
Iteration 39, loss = 0.11422216
Iteration 40, loss = 0.10861579
Iteration 41, loss = 0.10336736
Iteration 42, loss = 0.09850133
Iteration 43, loss = 0.09399754
Iteration 44, loss = 0.08979787
Iteration 45, loss = 0.08586370
Iteration 46, loss = 0.08217010
Iteration 47, loss = 0.07879245
Iteration 48, loss = 0.07557988
Iteration 49, loss = 0.07253088
Iteration 50, loss = 0.06975957
Iteration 51, loss = 0.06713572
Iteration 52, loss = 0.06463552
Iteration 53, loss = 0.06231517
Iteration 54, loss = 0.06013178
Iteration 55, loss = 0.05807563
Iteration 56, loss = 0.05609005
Iteration 57, loss = 0.05428608
Iteration 58, loss = 0.05254416
Iteration 59, loss = 0.05088040
Iteration 60, loss = 0.04931951
Iteration 61, loss = 0.04782083
Iteration 62, loss = 0.04641236
Iteration 63, loss = 0.04509621
Iteration 64, loss = 0.04379136
Iteration 65, loss = 0.04257357
Iteration 66, loss = 0.04141895
Iteration 67, loss = 0.04033973
Iteration 68, loss = 0.03927656
Iteration 69, loss = 0.03828447
Iteration 70, loss = 0.03737391
Iteration 71, loss = 0.03647032
Iteration 72, loss = 0.03558108
Iteration 73, loss = 0.03471997
Iteration 74, loss = 0.03391416
Iteration 75, loss = 0.03321866
Iteration 76, loss = 0.03241506
Iteration 77, loss = 0.03171967
Iteration 78, loss = 0.03103523
Iteration 79, loss = 0.03041685
Iteration 80, loss = 0.02978117
Iteration 81, loss = 0.02920089
Iteration 82, loss = 0.02863351
Iteration 83, loss = 0.02807494
Iteration 84, loss = 0.02755800
Iteration 85, loss = 0.02705651
Iteration 86, loss = 0.02658023
Iteration 87, loss = 0.02605944
Iteration 88, loss = 0.02561716
Iteration 89, loss = 0.02517027
Iteration 90, loss = 0.02474103
Iteration 91, loss = 0.02433153
Iteration 92, loss = 0.02396138
Iteration 93, loss = 0.02354546
Iteration 94, loss = 0.02315230
Iteration 95, loss = 0.02278166
Iteration 96, loss = 0.02243768
Iteration 97, loss = 0.02210526
Iteration 98, loss = 0.02175252
Iteration 99, loss = 0.02146122
Iteration 100, loss = 0.02115834
Iteration 101, loss = 0.02083836
Iteration 102, loss = 0.02059945
Iteration 103, loss = 0.02029356
Iteration 104, loss = 0.02004796
Iteration 105, loss = 0.01978934
Iteration 106, loss = 0.01950761
Iteration 107, loss = 0.01928812
Iteration 108, loss = 0.01903597
Iteration 109, loss = 0.01879605
Iteration 110, loss = 0.01856860
Iteration 111, loss = 0.01835275
Iteration 112, loss = 0.01815146
Iteration 113, loss = 0.01793944
Iteration 114, loss = 0.01770729
Iteration 115, loss = 0.01756540
Iteration 116, loss = 0.01735727
Iteration 117, loss = 0.01712959
Iteration 118, loss = 0.01695393
Iteration 119, loss = 0.01676870
Iteration 120, loss = 0.01660070
Iteration 121, loss = 0.01644327
Iteration 122, loss = 0.01627630
Iteration 123, loss = 0.01613696
Iteration 124, loss = 0.01594300
Iteration 125, loss = 0.01577670
Iteration 126, loss = 0.01566183
Iteration 127, loss = 0.01556390
Iteration 128, loss = 0.01540921
Iteration 129, loss = 0.01530241
Iteration 130, loss = 0.01514839
Iteration 131, loss = 0.01504843
Iteration 132, loss = 0.01491050
Iteration 133, loss = 0.01477791
Iteration 134, loss = 0.01466124
Iteration 135, loss = 0.01453766
Iteration 136, loss = 0.01440814
Iteration 137, loss = 0.01430133
Iteration 138, loss = 0.01419462
Iteration 139, loss = 0.01408649
Iteration 140, loss = 0.01397607
Iteration 141, loss = 0.01386409
Iteration 142, loss = 0.01377048
Iteration 143, loss = 0.01370359
Iteration 144, loss = 0.01356310
Iteration 145, loss = 0.01350423
Iteration 146, loss = 0.01335923
Iteration 147, loss = 0.01328323
Iteration 148, loss = 0.01317506
Iteration 149, loss = 0.01315074
Iteration 150, loss = 0.01300815
Iteration 151, loss = 0.01291134
Iteration 152, loss = 0.01287531
Iteration 153, loss = 0.01274302
Iteration 154, loss = 0.01271449
Iteration 155, loss = 0.01257863
Iteration 156, loss = 0.01251646
Iteration 157, loss = 0.01248543
Iteration 158, loss = 0.01237288
Iteration 159, loss = 0.01229423
Iteration 160, loss = 0.01223293
Iteration 161, loss = 0.01215049
Iteration 162, loss = 0.01207101
Iteration 163, loss = 0.01200957
Iteration 164, loss = 0.01194156
Iteration 165, loss = 0.01187730
Iteration 166, loss = 0.01182056
Iteration 167, loss = 0.01175590
Iteration 168, loss = 0.01168754
Iteration 169, loss = 0.01164314
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67720872
Iteration 2, loss = 0.62371121
Iteration 3, loss = 0.54058964
Iteration 4, loss = 0.44828997
Iteration 5, loss = 0.35973639
Iteration 6, loss = 0.28378289
Iteration 7, loss = 0.22268119
Iteration 8, loss = 0.17670785
Iteration 9, loss = 0.14329971
Iteration 10, loss = 0.11861969
Iteration 11, loss = 0.10057924
Iteration 12, loss = 0.08655085
Iteration 13, loss = 0.07598232
Iteration 14, loss = 0.06747002
Iteration 15, loss = 0.06225979
Iteration 16, loss = 0.05708603
Iteration 17, loss = 0.05244096
Iteration 18, loss = 0.04877588
Iteration 19, loss = 0.04549782
Iteration 20, loss = 0.04279369
Iteration 21, loss = 0.04080163
Iteration 22, loss = 0.03880526
Iteration 23, loss = 0.03674710
Iteration 24, loss = 0.03563931
Iteration 25, loss = 0.03415588
Iteration 26, loss = 0.03306171
Iteration 27, loss = 0.03219029
Iteration 28, loss = 0.03122301
Iteration 29, loss = 0.03028660
Iteration 30, loss = 0.02960601
Iteration 31, loss = 0.02925404
Iteration 32, loss = 0.02850462
Iteration 33, loss = 0.02781844
Iteration 34, loss = 0.02743688
Iteration 35, loss = 0.02692988
Iteration 36, loss = 0.02641840
Iteration 37, loss = 0.02608889
Iteration 38, loss = 0.02581590
Iteration 39, loss = 0.02537241
Iteration 40, loss = 0.02499107
Iteration 41, loss = 0.02495317
Iteration 42, loss = 0.02446796
Iteration 43, loss = 0.02426138
Iteration 44, loss = 0.02408512
Iteration 45, loss = 0.02371470
Iteration 46, loss = 0.02368958
Iteration 47, loss = 0.02339256
Iteration 48, loss = 0.02312184
Iteration 49, loss = 0.02308868
Iteration 50, loss = 0.02304722
Iteration 51, loss = 0.02262965
Iteration 52, loss = 0.02254047
Iteration 53, loss = 0.02258592
Iteration 54, loss = 0.02236325
Iteration 55, loss = 0.02244139
Iteration 56, loss = 0.02195701
Iteration 57, loss = 0.02198088
Iteration 58, loss = 0.02211681
Iteration 59, loss = 0.02171476
Iteration 60, loss = 0.02160393
Iteration 61, loss = 0.02163031
Iteration 62, loss = 0.02157012
Iteration 63, loss = 0.02130501
Iteration 64, loss = 0.02146913
Iteration 65, loss = 0.02154011
Iteration 66, loss = 0.02130711
Iteration 67, loss = 0.02119262
Iteration 68, loss = 0.02113318
Iteration 69, loss = 0.02101526
Iteration 70, loss = 0.02088178
Iteration 71, loss = 0.02222437
Iteration 72, loss = 0.02262042
Iteration 73, loss = 0.02244564
Iteration 74, loss = 0.02209900
Iteration 75, loss = 0.02203084
Iteration 76, loss = 0.02170414
Iteration 77, loss = 0.02155086
Iteration 78, loss = 0.02156600
Iteration 79, loss = 0.02121182
Iteration 80, loss = 0.02116615
Iteration 81, loss = 0.02103772
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67741800
Iteration 2, loss = 0.61863098
Iteration 3, loss = 0.51918956
Iteration 4, loss = 0.40767983
Iteration 5, loss = 0.30779628
Iteration 6, loss = 0.23081065
Iteration 7, loss = 0.17630143
Iteration 8, loss = 0.13852555
Iteration 9, loss = 0.11260803
Iteration 10, loss = 0.09404782
Iteration 11, loss = 0.08075305
Iteration 12, loss = 0.07043408
Iteration 13, loss = 0.06306222
Iteration 14, loss = 0.05699390
Iteration 15, loss = 0.05230874
Iteration 16, loss = 0.04860994
Iteration 17, loss = 0.04538552
Iteration 18, loss = 0.04287588
Iteration 19, loss = 0.04071030
Iteration 20, loss = 0.03878518
Iteration 21, loss = 0.03734552
Iteration 22, loss = 0.03610170
Iteration 23, loss = 0.03467749
Iteration 24, loss = 0.03355011
Iteration 25, loss = 0.03272496
Iteration 26, loss = 0.03185551
Iteration 27, loss = 0.03134207
Iteration 28, loss = 0.03061143
Iteration 29, loss = 0.03023631
Iteration 30, loss = 0.02978285
Iteration 31, loss = 0.02918650
Iteration 32, loss = 0.02885242
Iteration 33, loss = 0.02845809
Iteration 34, loss = 0.02807629
Iteration 35, loss = 0.02808593
Iteration 36, loss = 0.02736313
Iteration 37, loss = 0.02750820
Iteration 38, loss = 0.02711953
Iteration 39, loss = 0.02674621
Iteration 40, loss = 0.02652418
Iteration 41, loss = 0.02626851
Iteration 42, loss = 0.02621637
Iteration 43, loss = 0.02594169
Iteration 44, loss = 0.02607058
Iteration 45, loss = 0.02583380
Iteration 46, loss = 0.02585828
Iteration 47, loss = 0.02554443
Iteration 48, loss = 0.02524442
Iteration 49, loss = 0.02552158
Iteration 50, loss = 0.02533756
Iteration 51, loss = 0.02498520
Iteration 52, loss = 0.02524899
Iteration 53, loss = 0.02493107
Iteration 54, loss = 0.02490425
Iteration 55, loss = 0.02483402
Iteration 56, loss = 0.02477356
Iteration 57, loss = 0.02467679
Iteration 58, loss = 0.02463641
Iteration 59, loss = 0.02480722
Iteration 60, loss = 0.02465873
Iteration 61, loss = 0.02459688
Iteration 62, loss = 0.02451715
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71628708
Iteration 2, loss = 0.66126551
Iteration 3, loss = 0.59107350
Iteration 4, loss = 0.50174548
Iteration 5, loss = 0.41102259
Iteration 6, loss = 0.32825860
Iteration 7, loss = 0.26122571
Iteration 8, loss = 0.20828072
Iteration 9, loss = 0.16864524
Iteration 10, loss = 0.13917219
Iteration 11, loss = 0.11699936
Iteration 12, loss = 0.10042250
Iteration 13, loss = 0.08756288
Iteration 14, loss = 0.07754242
Iteration 15, loss = 0.06944576
Iteration 16, loss = 0.06311671
Iteration 17, loss = 0.05777060
Iteration 18, loss = 0.05331757
Iteration 19, loss = 0.04976075
Iteration 20, loss = 0.04688172
Iteration 21, loss = 0.04393513
Iteration 22, loss = 0.04187129
Iteration 23, loss = 0.03983709
Iteration 24, loss = 0.03832675
Iteration 25, loss = 0.03662586
Iteration 26, loss = 0.03572420
Iteration 27, loss = 0.03430575
Iteration 28, loss = 0.03316908
Iteration 29, loss = 0.03245451
Iteration 30, loss = 0.03154590
Iteration 31, loss = 0.03099855
Iteration 32, loss = 0.03021795
Iteration 33, loss = 0.02953529
Iteration 34, loss = 0.02898605
Iteration 35, loss = 0.02845464
Iteration 36, loss = 0.02796724
Iteration 37, loss = 0.02775168
Iteration 38, loss = 0.02724730
Iteration 39, loss = 0.02702749
Iteration 40, loss = 0.02660128
Iteration 41, loss = 0.02636779
Iteration 42, loss = 0.02592854
Iteration 43, loss = 0.02583403
Iteration 44, loss = 0.02576706
Iteration 45, loss = 0.02539973
Iteration 46, loss = 0.02500508
Iteration 47, loss = 0.02497473
Iteration 48, loss = 0.02473494
Iteration 49, loss = 0.02446733
Iteration 50, loss = 0.02431797
Iteration 51, loss = 0.02432572
Iteration 52, loss = 0.02401777
Iteration 53, loss = 0.02392810
Iteration 54, loss = 0.02394948
Iteration 55, loss = 0.02355746
Iteration 56, loss = 0.02354214
Iteration 57, loss = 0.02359794
Iteration 58, loss = 0.02345032
Iteration 59, loss = 0.02343817
Iteration 60, loss = 0.02323175
Iteration 61, loss = 0.02311913
Iteration 62, loss = 0.02296463
Iteration 63, loss = 0.02294794
Iteration 64, loss = 0.02293462
Iteration 65, loss = 0.02282320
Iteration 66, loss = 0.02278245
Iteration 67, loss = 0.02285209
Iteration 68, loss = 0.02266919
Iteration 69, loss = 0.02249980
Iteration 70, loss = 0.02266879
Iteration 71, loss = 0.02241743
Iteration 72, loss = 0.02236529
Iteration 73, loss = 0.02243443
Iteration 74, loss = 0.02250355
Iteration 75, loss = 0.02227206
Iteration 76, loss = 0.02228811
Iteration 77, loss = 0.02231365
Iteration 78, loss = 0.02193125
Iteration 79, loss = 0.02208141
Iteration 80, loss = 0.02197392
Iteration 81, loss = 0.02212558
Iteration 82, loss = 0.02202387
Iteration 83, loss = 0.02205606
Iteration 84, loss = 0.02209977
Iteration 85, loss = 0.02192432
Iteration 86, loss = 0.02191055
Iteration 87, loss = 0.02190043
Iteration 88, loss = 0.02192440
Iteration 89, loss = 0.02170456
Iteration 90, loss = 0.02185744
Iteration 91, loss = 0.02164672
Iteration 92, loss = 0.02163798
Iteration 93, loss = 0.02180211
Iteration 94, loss = 0.02183826
Iteration 95, loss = 0.02163248
Iteration 96, loss = 0.02149603
Iteration 97, loss = 0.02162527
Iteration 98, loss = 0.02175155
Iteration 99, loss = 0.02154049
Iteration 100, loss = 0.02147057
Iteration 101, loss = 0.02158004
Iteration 102, loss = 0.02157210
Iteration 103, loss = 0.02164661
Iteration 104, loss = 0.02150741
Iteration 105, loss = 0.02128825
Iteration 106, loss = 0.02130187
Iteration 107, loss = 0.02158449
Iteration 108, loss = 0.02151012
Iteration 109, loss = 0.02136062
Iteration 110, loss = 0.02130041
Iteration 111, loss = 0.02144701
Iteration 112, loss = 0.02137157
Iteration 113, loss = 0.02135520
Iteration 114, loss = 0.02120530
Iteration 115, loss = 0.02165524
Iteration 116, loss = 0.02146296
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68305422
Iteration 2, loss = 0.62129978
Iteration 3, loss = 0.52603707
Iteration 4, loss = 0.41721095
Iteration 5, loss = 0.31976862
Iteration 6, loss = 0.24308386
Iteration 7, loss = 0.18700261
Iteration 8, loss = 0.14755254
Iteration 9, loss = 0.11983247
Iteration 10, loss = 0.09999871
Iteration 11, loss = 0.08569497
Iteration 12, loss = 0.07463389
Iteration 13, loss = 0.06638993
Iteration 14, loss = 0.06021150
Iteration 15, loss = 0.05499099
Iteration 16, loss = 0.05090515
Iteration 17, loss = 0.04748836
Iteration 18, loss = 0.04468819
Iteration 19, loss = 0.04207613
Iteration 20, loss = 0.04018652
Iteration 21, loss = 0.03850239
Iteration 22, loss = 0.03698590
Iteration 23, loss = 0.03561714
Iteration 24, loss = 0.03439681
Iteration 25, loss = 0.03327842
Iteration 26, loss = 0.03263882
Iteration 27, loss = 0.03197411
Iteration 28, loss = 0.03108363
Iteration 29, loss = 0.03075435
Iteration 30, loss = 0.03020876
Iteration 31, loss = 0.02966337
Iteration 32, loss = 0.02941322
Iteration 33, loss = 0.02879191
Iteration 34, loss = 0.02832798
Iteration 35, loss = 0.02818371
Iteration 36, loss = 0.02774021
Iteration 37, loss = 0.02743185
Iteration 38, loss = 0.02722517
Iteration 39, loss = 0.02694581
Iteration 40, loss = 0.02673769
Iteration 41, loss = 0.02647321
Iteration 42, loss = 0.02651878
Iteration 43, loss = 0.02616837
Iteration 44, loss = 0.02649344
Iteration 45, loss = 0.02571322
Iteration 46, loss = 0.02583658
Iteration 47, loss = 0.02564037
Iteration 48, loss = 0.02529824
Iteration 49, loss = 0.02523738
Iteration 50, loss = 0.02523546
Iteration 51, loss = 0.02516091
Iteration 52, loss = 0.02475363
Iteration 53, loss = 0.02508420
Iteration 54, loss = 0.02486789
Iteration 55, loss = 0.02455356
Iteration 56, loss = 0.02473297
Iteration 57, loss = 0.02466464
Iteration 58, loss = 0.02441136
Iteration 59, loss = 0.02459071
Iteration 60, loss = 0.02434261
Iteration 61, loss = 0.02433475
Iteration 62, loss = 0.02440775
Iteration 63, loss = 0.02451297
Iteration 64, loss = 0.02433684
Iteration 65, loss = 0.02445480
Iteration 66, loss = 0.02364599
Iteration 67, loss = 0.02394359
Iteration 68, loss = 0.02437309
Iteration 69, loss = 0.02399939
Iteration 70, loss = 0.02393351
Iteration 71, loss = 0.02387044
Iteration 72, loss = 0.02406007
Iteration 73, loss = 0.02396604
Iteration 74, loss = 0.02365917
Iteration 75, loss = 0.02369891
Iteration 76, loss = 0.02372846
Iteration 77, loss = 0.02367765
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70825183
Iteration 2, loss = 0.67835012
Iteration 3, loss = 0.64456601
Iteration 4, loss = 0.60383277
Iteration 5, loss = 0.55822741
Iteration 6, loss = 0.50856837
Iteration 7, loss = 0.45737286
Iteration 8, loss = 0.40708209
Iteration 9, loss = 0.35999438
Iteration 10, loss = 0.31650717
Iteration 11, loss = 0.27761992
Iteration 12, loss = 0.24332361
Iteration 13, loss = 0.21361125
Iteration 14, loss = 0.18795482
Iteration 15, loss = 0.16611266
Iteration 16, loss = 0.14747340
Iteration 17, loss = 0.13159414
Iteration 18, loss = 0.11809549
Iteration 19, loss = 0.10672932
Iteration 20, loss = 0.09700034
Iteration 21, loss = 0.08847741
Iteration 22, loss = 0.08122211
Iteration 23, loss = 0.07488204
Iteration 24, loss = 0.06949153
Iteration 25, loss = 0.06458499
Iteration 26, loss = 0.06040050
Iteration 27, loss = 0.05675233
Iteration 28, loss = 0.05340457
Iteration 29, loss = 0.05043709
Iteration 30, loss = 0.04776233
Iteration 31, loss = 0.04527925
Iteration 32, loss = 0.04310389
Iteration 33, loss = 0.04114326
Iteration 34, loss = 0.03933270
Iteration 35, loss = 0.03778768
Iteration 36, loss = 0.03628584
Iteration 37, loss = 0.03500832
Iteration 38, loss = 0.03403975
Iteration 39, loss = 0.03305166
Iteration 40, loss = 0.03183933
Iteration 41, loss = 0.03078910
Iteration 42, loss = 0.02966978
Iteration 43, loss = 0.02883892
Iteration 44, loss = 0.02802994
Iteration 45, loss = 0.02729920
Iteration 46, loss = 0.02661191
Iteration 47, loss = 0.02595349
Iteration 48, loss = 0.02540420
Iteration 49, loss = 0.02492144
Iteration 50, loss = 0.02446376
Iteration 51, loss = 0.02411732
Iteration 52, loss = 0.02362766
Iteration 53, loss = 0.02316114
Iteration 54, loss = 0.02277680
Iteration 55, loss = 0.02234022
Iteration 56, loss = 0.02189522
Iteration 57, loss = 0.02155245
Iteration 58, loss = 0.02125195
Iteration 59, loss = 0.02095938
Iteration 60, loss = 0.02062760
Iteration 61, loss = 0.02034555
Iteration 62, loss = 0.02009949
Iteration 63, loss = 0.01979344
Iteration 64, loss = 0.01959609
Iteration 65, loss = 0.01943635
Iteration 66, loss = 0.01922025
Iteration 67, loss = 0.01910148
Iteration 68, loss = 0.01885336
Iteration 69, loss = 0.01871041
Iteration 70, loss = 0.01876544
Iteration 71, loss = 0.01841308
Iteration 72, loss = 0.01828208
Iteration 73, loss = 0.01806609
Iteration 74, loss = 0.01790845
Iteration 75, loss = 0.01776073
Iteration 76, loss = 0.01760218
Iteration 77, loss = 0.01742610
Iteration 78, loss = 0.01729137
Iteration 79, loss = 0.01713022
Iteration 80, loss = 0.01725007
Iteration 81, loss = 0.01733132
Iteration 82, loss = 0.01729012
Iteration 83, loss = 0.01706825
Iteration 84, loss = 0.01718720
Iteration 85, loss = 0.01708601
Iteration 86, loss = 0.01680721
Iteration 87, loss = 0.01658408
Iteration 88, loss = 0.01635216
Iteration 89, loss = 0.01623178
Iteration 90, loss = 0.01612758
Iteration 91, loss = 0.01606352
Iteration 92, loss = 0.01591993
Iteration 93, loss = 0.01589100
Iteration 94, loss = 0.01577172
Iteration 95, loss = 0.01568866
Iteration 96, loss = 0.01571989
Iteration 97, loss = 0.01554831
Iteration 98, loss = 0.01552178
Iteration 99, loss = 0.01543279
Iteration 100, loss = 0.01539817
Iteration 101, loss = 0.01536414
Iteration 102, loss = 0.01529477
Iteration 103, loss = 0.01519403
Iteration 104, loss = 0.01518676
Iteration 105, loss = 0.01518116
Iteration 106, loss = 0.01519517
Iteration 107, loss = 0.01517466
Iteration 108, loss = 0.01530485
Iteration 109, loss = 0.01549590
Iteration 110, loss = 0.01529739
Iteration 111, loss = 0.01500053
Iteration 112, loss = 0.01486534
Iteration 113, loss = 0.01480569
Iteration 114, loss = 0.01474321
Iteration 115, loss = 0.01470860
Iteration 116, loss = 0.01471493
Iteration 117, loss = 0.01470555
Iteration 118, loss = 0.01481881
Iteration 119, loss = 0.01472309
Iteration 120, loss = 0.01469951
Iteration 121, loss = 0.01458021
Iteration 122, loss = 0.01463402
Iteration 123, loss = 0.01467420
Iteration 124, loss = 0.01457033
Iteration 125, loss = 0.01453364
Iteration 126, loss = 0.01443613
Iteration 127, loss = 0.01488664
Iteration 128, loss = 0.01433674
Iteration 129, loss = 0.01441308
Iteration 130, loss = 0.01475938
Iteration 131, loss = 0.01471261
Iteration 132, loss = 0.01453824
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68901064
Iteration 2, loss = 0.64262264
Iteration 3, loss = 0.56653189
Iteration 4, loss = 0.46715993
Iteration 5, loss = 0.37036244
Iteration 6, loss = 0.28790953
Iteration 7, loss = 0.22412035
Iteration 8, loss = 0.17715994
Iteration 9, loss = 0.14311795
Iteration 10, loss = 0.11827576
Iteration 11, loss = 0.10027548
Iteration 12, loss = 0.08677315
Iteration 13, loss = 0.07614680
Iteration 14, loss = 0.06814153
Iteration 15, loss = 0.06164801
Iteration 16, loss = 0.05637319
Iteration 17, loss = 0.05213151
Iteration 18, loss = 0.04864013
Iteration 19, loss = 0.04543158
Iteration 20, loss = 0.04319541
Iteration 21, loss = 0.04107145
Iteration 22, loss = 0.03904063
Iteration 23, loss = 0.03747869
Iteration 24, loss = 0.03597887
Iteration 25, loss = 0.03503338
Iteration 26, loss = 0.03376764
Iteration 27, loss = 0.03299922
Iteration 28, loss = 0.03177140
Iteration 29, loss = 0.03107759
Iteration 30, loss = 0.03060507
Iteration 31, loss = 0.03028451
Iteration 32, loss = 0.02954556
Iteration 33, loss = 0.02935813
Iteration 34, loss = 0.02849218
Iteration 35, loss = 0.02818136
Iteration 36, loss = 0.02758804
Iteration 37, loss = 0.02722772
Iteration 38, loss = 0.02696192
Iteration 39, loss = 0.02640835
Iteration 40, loss = 0.02619601
Iteration 41, loss = 0.02596950
Iteration 42, loss = 0.02564295
Iteration 43, loss = 0.02533015
Iteration 44, loss = 0.02530907
Iteration 45, loss = 0.02475917
Iteration 46, loss = 0.02457836
Iteration 47, loss = 0.02459361
Iteration 48, loss = 0.02444722
Iteration 49, loss = 0.02460197
Iteration 50, loss = 0.02428479
Iteration 51, loss = 0.02406238
Iteration 52, loss = 0.02402210
Iteration 53, loss = 0.02355197
Iteration 54, loss = 0.02356618
Iteration 55, loss = 0.02355337
Iteration 56, loss = 0.02333197
Iteration 57, loss = 0.02328918
Iteration 58, loss = 0.02325222
Iteration 59, loss = 0.02301364
Iteration 60, loss = 0.02308286
Iteration 61, loss = 0.02318697
Iteration 62, loss = 0.02302479
Iteration 63, loss = 0.02272341
Iteration 64, loss = 0.02263723
Iteration 65, loss = 0.02276044
Iteration 66, loss = 0.02226129
Iteration 67, loss = 0.02245185
Iteration 68, loss = 0.02267510
Iteration 69, loss = 0.02305622
Iteration 70, loss = 0.02264747
Iteration 71, loss = 0.02381529
Iteration 72, loss = 0.02405031
Iteration 73, loss = 0.02331452
Iteration 74, loss = 0.02283620
Iteration 75, loss = 0.02263311
Iteration 76, loss = 0.02334949
Iteration 77, loss = 0.02338561
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72713389
Iteration 2, loss = 0.70172874
Iteration 3, loss = 0.67066170
Iteration 4, loss = 0.63155589
Iteration 5, loss = 0.58626194
Iteration 6, loss = 0.53642559
Iteration 7, loss = 0.48292986
Iteration 8, loss = 0.42877171
Iteration 9, loss = 0.37680507
Iteration 10, loss = 0.33005846
Iteration 11, loss = 0.28808604
Iteration 12, loss = 0.25156126
Iteration 13, loss = 0.21995366
Iteration 14, loss = 0.19314598
Iteration 15, loss = 0.17044000
Iteration 16, loss = 0.15109896
Iteration 17, loss = 0.13475007
Iteration 18, loss = 0.12103546
Iteration 19, loss = 0.10940079
Iteration 20, loss = 0.09939951
Iteration 21, loss = 0.09091950
Iteration 22, loss = 0.08360923
Iteration 23, loss = 0.07726326
Iteration 24, loss = 0.07166836
Iteration 25, loss = 0.06666484
Iteration 26, loss = 0.06236156
Iteration 27, loss = 0.05868408
Iteration 28, loss = 0.05539561
Iteration 29, loss = 0.05207355
Iteration 30, loss = 0.04932431
Iteration 31, loss = 0.04695245
Iteration 32, loss = 0.04474625
Iteration 33, loss = 0.04268446
Iteration 34, loss = 0.04088576
Iteration 35, loss = 0.03927359
Iteration 36, loss = 0.03771221
Iteration 37, loss = 0.03628341
Iteration 38, loss = 0.03498113
Iteration 39, loss = 0.03401700
Iteration 40, loss = 0.03276114
Iteration 41, loss = 0.03224067
Iteration 42, loss = 0.03128698
Iteration 43, loss = 0.03046468
Iteration 44, loss = 0.02949866
Iteration 45, loss = 0.02895079
Iteration 46, loss = 0.02816350
Iteration 47, loss = 0.02744596
Iteration 48, loss = 0.02685410
Iteration 49, loss = 0.02606141
Iteration 50, loss = 0.02551027
Iteration 51, loss = 0.02497237
Iteration 52, loss = 0.02448463
Iteration 53, loss = 0.02405247
Iteration 54, loss = 0.02363011
Iteration 55, loss = 0.02331767
Iteration 56, loss = 0.02307750
Iteration 57, loss = 0.02275322
Iteration 58, loss = 0.02236151
Iteration 59, loss = 0.02200991
Iteration 60, loss = 0.02179305
Iteration 61, loss = 0.02153258
Iteration 62, loss = 0.02112215
Iteration 63, loss = 0.02099498
Iteration 64, loss = 0.02077326
Iteration 65, loss = 0.02062808
Iteration 66, loss = 0.02041058
Iteration 67, loss = 0.02012503
Iteration 68, loss = 0.01984607
Iteration 69, loss = 0.01959471
Iteration 70, loss = 0.01956011
Iteration 71, loss = 0.01954683
Iteration 72, loss = 0.01936797
Iteration 73, loss = 0.01904147
Iteration 74, loss = 0.01884128
Iteration 75, loss = 0.01871222
Iteration 76, loss = 0.01855521
Iteration 77, loss = 0.01851575
Iteration 78, loss = 0.01857389
Iteration 79, loss = 0.01820117
Iteration 80, loss = 0.01812121
Iteration 81, loss = 0.01789663
Iteration 82, loss = 0.01792280
Iteration 83, loss = 0.01773096
Iteration 84, loss = 0.01751712
Iteration 85, loss = 0.01752956
Iteration 86, loss = 0.01753409
Iteration 87, loss = 0.01740940
Iteration 88, loss = 0.01732033
Iteration 89, loss = 0.01721901
Iteration 90, loss = 0.01708352
Iteration 91, loss = 0.01695011
Iteration 92, loss = 0.01680610
Iteration 93, loss = 0.01665499
Iteration 94, loss = 0.01666389
Iteration 95, loss = 0.01681610
Iteration 96, loss = 0.01670091
Iteration 97, loss = 0.01676981
Iteration 98, loss = 0.01667454
Iteration 99, loss = 0.01652808
Iteration 100, loss = 0.01647936
Iteration 101, loss = 0.01628830
Iteration 102, loss = 0.01635990
Iteration 103, loss = 0.01619716
Iteration 104, loss = 0.01617768
Iteration 105, loss = 0.01605617
Iteration 106, loss = 0.01630831
Iteration 107, loss = 0.01608123
Iteration 108, loss = 0.01620837
Iteration 109, loss = 0.01603173
Iteration 110, loss = 0.01605672
Iteration 111, loss = 0.01606231
Iteration 112, loss = 0.01586174
Iteration 113, loss = 0.01576615
Iteration 114, loss = 0.01560346
Iteration 115, loss = 0.01555562
Iteration 116, loss = 0.01546063
Iteration 117, loss = 0.01546353
Iteration 118, loss = 0.01547948
Iteration 119, loss = 0.01536950
Iteration 120, loss = 0.01529875
Iteration 121, loss = 0.01524537
Iteration 122, loss = 0.01562905
Iteration 123, loss = 0.01551664
Iteration 124, loss = 0.01551523
Iteration 125, loss = 0.01537493
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70687152
Iteration 2, loss = 0.65700610
Iteration 3, loss = 0.58857387
Iteration 4, loss = 0.49859238
Iteration 5, loss = 0.40499814
Iteration 6, loss = 0.32152802
Iteration 7, loss = 0.25342327
Iteration 8, loss = 0.20119769
Iteration 9, loss = 0.16224668
Iteration 10, loss = 0.13342119
Iteration 11, loss = 0.11195097
Iteration 12, loss = 0.09573775
Iteration 13, loss = 0.08318730
Iteration 14, loss = 0.07349333
Iteration 15, loss = 0.06574195
Iteration 16, loss = 0.05952711
Iteration 17, loss = 0.05458346
Iteration 18, loss = 0.05029543
Iteration 19, loss = 0.04693619
Iteration 20, loss = 0.04391081
Iteration 21, loss = 0.04128012
Iteration 22, loss = 0.03895825
Iteration 23, loss = 0.03734446
Iteration 24, loss = 0.03559834
Iteration 25, loss = 0.03421291
Iteration 26, loss = 0.03305240
Iteration 27, loss = 0.03166226
Iteration 28, loss = 0.03082665
Iteration 29, loss = 0.02990515
Iteration 30, loss = 0.02899315
Iteration 31, loss = 0.02844154
Iteration 32, loss = 0.02782220
Iteration 33, loss = 0.02728675
Iteration 34, loss = 0.02668238
Iteration 35, loss = 0.02610617
Iteration 36, loss = 0.02575303
Iteration 37, loss = 0.02509227
Iteration 38, loss = 0.02480747
Iteration 39, loss = 0.02452929
Iteration 40, loss = 0.02434658
Iteration 41, loss = 0.02390688
Iteration 42, loss = 0.02360419
Iteration 43, loss = 0.02340188
Iteration 44, loss = 0.02309893
Iteration 45, loss = 0.02290985
Iteration 46, loss = 0.02268398
Iteration 47, loss = 0.02248629
Iteration 48, loss = 0.02235565
Iteration 49, loss = 0.02216495
Iteration 50, loss = 0.02198588
Iteration 51, loss = 0.02186785
Iteration 52, loss = 0.02164057
Iteration 53, loss = 0.02162637
Iteration 54, loss = 0.02147029
Iteration 55, loss = 0.02124514
Iteration 56, loss = 0.02126595
Iteration 57, loss = 0.02101698
Iteration 58, loss = 0.02094848
Iteration 59, loss = 0.02090764
Iteration 60, loss = 0.02088700
Iteration 61, loss = 0.02079844
Iteration 62, loss = 0.02056901
Iteration 63, loss = 0.02063420
Iteration 64, loss = 0.02045970
Iteration 65, loss = 0.02048446
Iteration 66, loss = 0.02047537
Iteration 67, loss = 0.02026936
Iteration 68, loss = 0.02041256
Iteration 69, loss = 0.02015784
Iteration 70, loss = 0.02025680
Iteration 71, loss = 0.02037535
Iteration 72, loss = 0.02010079
Iteration 73, loss = 0.01986359
Iteration 74, loss = 0.02002078
Iteration 75, loss = 0.01994725
Iteration 76, loss = 0.01978830
Iteration 77, loss = 0.02021547
Iteration 78, loss = 0.01986554
Iteration 79, loss = 0.01967297
Iteration 80, loss = 0.01983847
Iteration 81, loss = 0.01985625
Iteration 82, loss = 0.01973781
Iteration 83, loss = 0.01968056
Iteration 84, loss = 0.01976646
Iteration 85, loss = 0.01970352
Iteration 86, loss = 0.01958401
Iteration 87, loss = 0.01966806
Iteration 88, loss = 0.01941889
Iteration 89, loss = 0.01943942
Iteration 90, loss = 0.01933796
Iteration 91, loss = 0.01936387
Iteration 92, loss = 0.01950352
Iteration 93, loss = 0.01937818
Iteration 94, loss = 0.01933331
Iteration 95, loss = 0.01952552
Iteration 96, loss = 0.01926703
Iteration 97, loss = 0.01929449
Iteration 98, loss = 0.01942686
Iteration 99, loss = 0.01909958
Iteration 100, loss = 0.01931575
Iteration 101, loss = 0.01945892
Iteration 102, loss = 0.01920643
Iteration 103, loss = 0.01938135
Iteration 104, loss = 0.01909307
Iteration 105, loss = 0.01922448
Iteration 106, loss = 0.01918610
Iteration 107, loss = 0.01926320
Iteration 108, loss = 0.01927316
Iteration 109, loss = 0.01923354
Iteration 110, loss = 0.01955866
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69222625
Iteration 2, loss = 0.67352537
Iteration 3, loss = 0.65795377
Iteration 4, loss = 0.64300953
Iteration 5, loss = 0.62805717
Iteration 6, loss = 0.61251930
Iteration 7, loss = 0.59625433
Iteration 8, loss = 0.57908436
Iteration 9, loss = 0.56080116
Iteration 10, loss = 0.54171827
Iteration 11, loss = 0.52160458
Iteration 12, loss = 0.50066710
Iteration 13, loss = 0.47894204
Iteration 14, loss = 0.45679472
Iteration 15, loss = 0.43421351
Iteration 16, loss = 0.41154427
Iteration 17, loss = 0.38888701
Iteration 18, loss = 0.36661015
Iteration 19, loss = 0.34500252
Iteration 20, loss = 0.32405719
Iteration 21, loss = 0.30393864
Iteration 22, loss = 0.28487110
Iteration 23, loss = 0.26677849
Iteration 24, loss = 0.24981599
Iteration 25, loss = 0.23388085
Iteration 26, loss = 0.21901622
Iteration 27, loss = 0.20510004
Iteration 28, loss = 0.19220503
Iteration 29, loss = 0.18020561
Iteration 30, loss = 0.16909342
Iteration 31, loss = 0.15874135
Iteration 32, loss = 0.14923118
Iteration 33, loss = 0.14037651
Iteration 34, loss = 0.13222090
Iteration 35, loss = 0.12461415
Iteration 36, loss = 0.11764184
Iteration 37, loss = 0.11117896
Iteration 38, loss = 0.10518320
Iteration 39, loss = 0.09965572
Iteration 40, loss = 0.09453876
Iteration 41, loss = 0.08973011
Iteration 42, loss = 0.08529303
Iteration 43, loss = 0.08114874
Iteration 44, loss = 0.07735757
Iteration 45, loss = 0.07374557
Iteration 46, loss = 0.07041314
Iteration 47, loss = 0.06724326
Iteration 48, loss = 0.06436380
Iteration 49, loss = 0.06163220
Iteration 50, loss = 0.05904000
Iteration 51, loss = 0.05662469
Iteration 52, loss = 0.05437328
Iteration 53, loss = 0.05225533
Iteration 54, loss = 0.05028073
Iteration 55, loss = 0.04836969
Iteration 56, loss = 0.04659024
Iteration 57, loss = 0.04491757
Iteration 58, loss = 0.04330642
Iteration 59, loss = 0.04182882
Iteration 60, loss = 0.04041727
Iteration 61, loss = 0.03903636
Iteration 62, loss = 0.03775793
Iteration 63, loss = 0.03653912
Iteration 64, loss = 0.03537051
Iteration 65, loss = 0.03422923
Iteration 66, loss = 0.03318783
Iteration 67, loss = 0.03217940
Iteration 68, loss = 0.03120481
Iteration 69, loss = 0.03029505
Iteration 70, loss = 0.02940071
Iteration 71, loss = 0.02857799
Iteration 72, loss = 0.02775795
Iteration 73, loss = 0.02698423
Iteration 74, loss = 0.02625906
Iteration 75, loss = 0.02553874
Iteration 76, loss = 0.02486999
Iteration 77, loss = 0.02421559
Iteration 78, loss = 0.02359629
Iteration 79, loss = 0.02300962
Iteration 80, loss = 0.02241646
Iteration 81, loss = 0.02186382
Iteration 82, loss = 0.02133333
Iteration 83, loss = 0.02081801
Iteration 84, loss = 0.02031813
Iteration 85, loss = 0.01984137
Iteration 86, loss = 0.01938981
Iteration 87, loss = 0.01895168
Iteration 88, loss = 0.01852277
Iteration 89, loss = 0.01811089
Iteration 90, loss = 0.01771651
Iteration 91, loss = 0.01733270
Iteration 92, loss = 0.01696781
Iteration 93, loss = 0.01662776
Iteration 94, loss = 0.01626521
Iteration 95, loss = 0.01593517
Iteration 96, loss = 0.01560862
Iteration 97, loss = 0.01529701
Iteration 98, loss = 0.01499899
Iteration 99, loss = 0.01469907
Iteration 100, loss = 0.01441711
Iteration 101, loss = 0.01414730
Iteration 102, loss = 0.01387878
Iteration 103, loss = 0.01361799
Iteration 104, loss = 0.01336536
Iteration 105, loss = 0.01312948
Iteration 106, loss = 0.01289865
Iteration 107, loss = 0.01266623
Iteration 108, loss = 0.01245007
Iteration 109, loss = 0.01222687
Iteration 110, loss = 0.01202000
Iteration 111, loss = 0.01181263
Iteration 112, loss = 0.01161081
Iteration 113, loss = 0.01141602
Iteration 114, loss = 0.01122561
Iteration 115, loss = 0.01104315
Iteration 116, loss = 0.01086016
Iteration 117, loss = 0.01068134
Iteration 118, loss = 0.01051615
Iteration 119, loss = 0.01034833
Iteration 120, loss = 0.01018528
Iteration 121, loss = 0.01003023
Iteration 122, loss = 0.00987533
Iteration 123, loss = 0.00973342
Iteration 124, loss = 0.00958274
Iteration 125, loss = 0.00944394
Iteration 126, loss = 0.00930443
Iteration 127, loss = 0.00916946
Iteration 128, loss = 0.00904046
Iteration 129, loss = 0.00891272
Iteration 130, loss = 0.00878538
Iteration 131, loss = 0.00866050
Iteration 132, loss = 0.00854288
Iteration 133, loss = 0.00841976
Iteration 134, loss = 0.00830586
Iteration 135, loss = 0.00819110
Iteration 136, loss = 0.00808425
Iteration 137, loss = 0.00797455
Iteration 138, loss = 0.00787139
Iteration 139, loss = 0.00776972
Iteration 140, loss = 0.00766578
Iteration 141, loss = 0.00757053
Iteration 142, loss = 0.00747643
Iteration 143, loss = 0.00738502
Iteration 144, loss = 0.00729148
Iteration 145, loss = 0.00720451
Iteration 146, loss = 0.00711507
Iteration 147, loss = 0.00702972
Iteration 148, loss = 0.00694382
Iteration 149, loss = 0.00686454
Iteration 150, loss = 0.00678162
Iteration 151, loss = 0.00670460
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68610275
Iteration 2, loss = 0.62142409
Iteration 3, loss = 0.52662823
Iteration 4, loss = 0.41579120
Iteration 5, loss = 0.31835480
Iteration 6, loss = 0.24174463
Iteration 7, loss = 0.18572295
Iteration 8, loss = 0.14654085
Iteration 9, loss = 0.11877070
Iteration 10, loss = 0.09859334
Iteration 11, loss = 0.08370188
Iteration 12, loss = 0.07283331
Iteration 13, loss = 0.06415784
Iteration 14, loss = 0.05756873
Iteration 15, loss = 0.05221778
Iteration 16, loss = 0.04824603
Iteration 17, loss = 0.04440997
Iteration 18, loss = 0.04161254
Iteration 19, loss = 0.03905207
Iteration 20, loss = 0.03679926
Iteration 21, loss = 0.03501086
Iteration 22, loss = 0.03341978
Iteration 23, loss = 0.03220062
Iteration 24, loss = 0.03134754
Iteration 25, loss = 0.03001559
Iteration 26, loss = 0.02895270
Iteration 27, loss = 0.02814189
Iteration 28, loss = 0.02752265
Iteration 29, loss = 0.02687799
Iteration 30, loss = 0.02640966
Iteration 31, loss = 0.02578611
Iteration 32, loss = 0.02529582
Iteration 33, loss = 0.02505508
Iteration 34, loss = 0.02446558
Iteration 35, loss = 0.02389288
Iteration 36, loss = 0.02385733
Iteration 37, loss = 0.02374159
Iteration 38, loss = 0.02323309
Iteration 39, loss = 0.02295043
Iteration 40, loss = 0.02274361
Iteration 41, loss = 0.02258040
Iteration 42, loss = 0.02234634
Iteration 43, loss = 0.02222280
Iteration 44, loss = 0.02182882
Iteration 45, loss = 0.02181933
Iteration 46, loss = 0.02189651
Iteration 47, loss = 0.02152057
Iteration 48, loss = 0.02145294
Iteration 49, loss = 0.02107737
Iteration 50, loss = 0.02095734
Iteration 51, loss = 0.02096562
Iteration 52, loss = 0.02089943
Iteration 53, loss = 0.02088002
Iteration 54, loss = 0.02068213
Iteration 55, loss = 0.02045298
Iteration 56, loss = 0.02028946
Iteration 57, loss = 0.02053130
Iteration 58, loss = 0.02026100
Iteration 59, loss = 0.02062557
Iteration 60, loss = 0.02037179
Iteration 61, loss = 0.02009706
Iteration 62, loss = 0.02031463
Iteration 63, loss = 0.02015984
Iteration 64, loss = 0.01994767
Iteration 65, loss = 0.02004594
Iteration 66, loss = 0.01994618
Iteration 67, loss = 0.01973201
Iteration 68, loss = 0.01990623
Iteration 69, loss = 0.01967116
Iteration 70, loss = 0.01984862
Iteration 71, loss = 0.02013006
Iteration 72, loss = 0.01956408
Iteration 73, loss = 0.01948048
Iteration 74, loss = 0.01977292
Iteration 75, loss = 0.01965269
Iteration 76, loss = 0.01953589
Iteration 77, loss = 0.01952627
Iteration 78, loss = 0.01959814
Iteration 79, loss = 0.01946443
Iteration 80, loss = 0.01946096
Iteration 81, loss = 0.01966139
Iteration 82, loss = 0.01937544
Iteration 83, loss = 0.01947083
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68312400
Iteration 2, loss = 0.65666163
Iteration 3, loss = 0.62212888
Iteration 4, loss = 0.57487195
Iteration 5, loss = 0.51758387
Iteration 6, loss = 0.45716633
Iteration 7, loss = 0.39783776
Iteration 8, loss = 0.34273348
Iteration 9, loss = 0.29332551
Iteration 10, loss = 0.25045865
Iteration 11, loss = 0.21390521
Iteration 12, loss = 0.18335774
Iteration 13, loss = 0.15806360
Iteration 14, loss = 0.13700469
Iteration 15, loss = 0.11991670
Iteration 16, loss = 0.10568237
Iteration 17, loss = 0.09387557
Iteration 18, loss = 0.08391107
Iteration 19, loss = 0.07552907
Iteration 20, loss = 0.06853855
Iteration 21, loss = 0.06254085
Iteration 22, loss = 0.05735750
Iteration 23, loss = 0.05283581
Iteration 24, loss = 0.04905406
Iteration 25, loss = 0.04567736
Iteration 26, loss = 0.04274027
Iteration 27, loss = 0.04009085
Iteration 28, loss = 0.03786918
Iteration 29, loss = 0.03571636
Iteration 30, loss = 0.03386758
Iteration 31, loss = 0.03221403
Iteration 32, loss = 0.03081020
Iteration 33, loss = 0.02957441
Iteration 34, loss = 0.02817869
Iteration 35, loss = 0.02714923
Iteration 36, loss = 0.02621671
Iteration 37, loss = 0.02520677
Iteration 38, loss = 0.02432168
Iteration 39, loss = 0.02359316
Iteration 40, loss = 0.02289528
Iteration 41, loss = 0.02221695
Iteration 42, loss = 0.02154779
Iteration 43, loss = 0.02102641
Iteration 44, loss = 0.02055232
Iteration 45, loss = 0.02004076
Iteration 46, loss = 0.01957543
Iteration 47, loss = 0.01926428
Iteration 48, loss = 0.01878778
Iteration 49, loss = 0.01843144
Iteration 50, loss = 0.01806625
Iteration 51, loss = 0.01776310
Iteration 52, loss = 0.01746471
Iteration 53, loss = 0.01725999
Iteration 54, loss = 0.01695076
Iteration 55, loss = 0.01663208
Iteration 56, loss = 0.01645794
Iteration 57, loss = 0.01619410
Iteration 58, loss = 0.01596789
Iteration 59, loss = 0.01577890
Iteration 60, loss = 0.01557748
Iteration 61, loss = 0.01535094
Iteration 62, loss = 0.01539688
Iteration 63, loss = 0.01504719
Iteration 64, loss = 0.01492029
Iteration 65, loss = 0.01484821
Iteration 66, loss = 0.01482920
Iteration 67, loss = 0.01454086
Iteration 68, loss = 0.01438573
Iteration 69, loss = 0.01424953
Iteration 70, loss = 0.01415570
Iteration 71, loss = 0.01402651
Iteration 72, loss = 0.01396193
Iteration 73, loss = 0.01381665
Iteration 74, loss = 0.01369074
Iteration 75, loss = 0.01361314
Iteration 76, loss = 0.01355597
Iteration 77, loss = 0.01349870
Iteration 78, loss = 0.01348347
Iteration 79, loss = 0.01333119
Iteration 80, loss = 0.01322069
Iteration 81, loss = 0.01317158
Iteration 82, loss = 0.01314984
Iteration 83, loss = 0.01322270
Iteration 84, loss = 0.01299181
Iteration 85, loss = 0.01301367
Iteration 86, loss = 0.01289430
Iteration 87, loss = 0.01286001
Iteration 88, loss = 0.01274776
Iteration 89, loss = 0.01272229
Iteration 90, loss = 0.01264341
Iteration 91, loss = 0.01270979
Iteration 92, loss = 0.01247428
Iteration 93, loss = 0.01256365
Iteration 94, loss = 0.01251684
Iteration 95, loss = 0.01244707
Iteration 96, loss = 0.01243009
Iteration 97, loss = 0.01227929
Iteration 98, loss = 0.01229694
Iteration 99, loss = 0.01228080
Iteration 100, loss = 0.01224240
Iteration 101, loss = 0.01219816
Iteration 102, loss = 0.01230029
Iteration 103, loss = 0.01213979
Iteration 104, loss = 0.01205367
Iteration 105, loss = 0.01208273
Iteration 106, loss = 0.01199105
Iteration 107, loss = 0.01202259
Iteration 108, loss = 0.01190300
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70362090
Iteration 2, loss = 0.67580147
Iteration 3, loss = 0.64980206
Iteration 4, loss = 0.62098323
Iteration 5, loss = 0.58948354
Iteration 6, loss = 0.55549277
Iteration 7, loss = 0.51988299
Iteration 8, loss = 0.48334506
Iteration 9, loss = 0.44719783
Iteration 10, loss = 0.41204956
Iteration 11, loss = 0.37856658
Iteration 12, loss = 0.34683913
Iteration 13, loss = 0.31714770
Iteration 14, loss = 0.28977613
Iteration 15, loss = 0.26467885
Iteration 16, loss = 0.24173039
Iteration 17, loss = 0.22076691
Iteration 18, loss = 0.20193924
Iteration 19, loss = 0.18490077
Iteration 20, loss = 0.16958100
Iteration 21, loss = 0.15594310
Iteration 22, loss = 0.14353600
Iteration 23, loss = 0.13249013
Iteration 24, loss = 0.12259654
Iteration 25, loss = 0.11368174
Iteration 26, loss = 0.10571494
Iteration 27, loss = 0.09848079
Iteration 28, loss = 0.09207670
Iteration 29, loss = 0.08623412
Iteration 30, loss = 0.08097410
Iteration 31, loss = 0.07627815
Iteration 32, loss = 0.07188634
Iteration 33, loss = 0.06802626
Iteration 34, loss = 0.06439333
Iteration 35, loss = 0.06113384
Iteration 36, loss = 0.05817488
Iteration 37, loss = 0.05544555
Iteration 38, loss = 0.05295731
Iteration 39, loss = 0.05058797
Iteration 40, loss = 0.04848257
Iteration 41, loss = 0.04649222
Iteration 42, loss = 0.04461869
Iteration 43, loss = 0.04294906
Iteration 44, loss = 0.04139667
Iteration 45, loss = 0.03990623
Iteration 46, loss = 0.03856058
Iteration 47, loss = 0.03728016
Iteration 48, loss = 0.03609029
Iteration 49, loss = 0.03497859
Iteration 50, loss = 0.03394020
Iteration 51, loss = 0.03299388
Iteration 52, loss = 0.03202478
Iteration 53, loss = 0.03114354
Iteration 54, loss = 0.03032923
Iteration 55, loss = 0.02960911
Iteration 56, loss = 0.02883651
Iteration 57, loss = 0.02817317
Iteration 58, loss = 0.02752935
Iteration 59, loss = 0.02690528
Iteration 60, loss = 0.02630969
Iteration 61, loss = 0.02578992
Iteration 62, loss = 0.02527355
Iteration 63, loss = 0.02475372
Iteration 64, loss = 0.02431109
Iteration 65, loss = 0.02381042
Iteration 66, loss = 0.02344614
Iteration 67, loss = 0.02304141
Iteration 68, loss = 0.02261726
Iteration 69, loss = 0.02225649
Iteration 70, loss = 0.02188867
Iteration 71, loss = 0.02153549
Iteration 72, loss = 0.02123252
Iteration 73, loss = 0.02089669
Iteration 74, loss = 0.02058692
Iteration 75, loss = 0.02031665
Iteration 76, loss = 0.02005511
Iteration 77, loss = 0.01978629
Iteration 78, loss = 0.01956946
Iteration 79, loss = 0.01929913
Iteration 80, loss = 0.01912060
Iteration 81, loss = 0.01882723
Iteration 82, loss = 0.01861178
Iteration 83, loss = 0.01838664
Iteration 84, loss = 0.01821207
Iteration 85, loss = 0.01802511
Iteration 86, loss = 0.01783149
Iteration 87, loss = 0.01769937
Iteration 88, loss = 0.01751652
Iteration 89, loss = 0.01731956
Iteration 90, loss = 0.01721890
Iteration 91, loss = 0.01704051
Iteration 92, loss = 0.01686762
Iteration 93, loss = 0.01671910
Iteration 94, loss = 0.01662490
Iteration 95, loss = 0.01648021
Iteration 96, loss = 0.01631377
Iteration 97, loss = 0.01620896
Iteration 98, loss = 0.01607964
Iteration 99, loss = 0.01596141
Iteration 100, loss = 0.01585081
Iteration 101, loss = 0.01574394
Iteration 102, loss = 0.01563942
Iteration 103, loss = 0.01554375
Iteration 104, loss = 0.01542928
Iteration 105, loss = 0.01535081
Iteration 106, loss = 0.01524046
Iteration 107, loss = 0.01514446
Iteration 108, loss = 0.01504683
Iteration 109, loss = 0.01496414
Iteration 110, loss = 0.01486204
Iteration 111, loss = 0.01480058
Iteration 112, loss = 0.01474109
Iteration 113, loss = 0.01462621
Iteration 114, loss = 0.01455694
Iteration 115, loss = 0.01448749
Iteration 116, loss = 0.01441459
Iteration 117, loss = 0.01433253
Iteration 118, loss = 0.01432742
Iteration 119, loss = 0.01423657
Iteration 120, loss = 0.01415289
Iteration 121, loss = 0.01407014
Iteration 122, loss = 0.01398024
Iteration 123, loss = 0.01401699
Iteration 124, loss = 0.01390959
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67819103
Iteration 2, loss = 0.62874578
Iteration 3, loss = 0.56152180
Iteration 4, loss = 0.48137866
Iteration 5, loss = 0.40235348
Iteration 6, loss = 0.32938570
Iteration 7, loss = 0.26771231
Iteration 8, loss = 0.21767224
Iteration 9, loss = 0.17860316
Iteration 10, loss = 0.14859083
Iteration 11, loss = 0.12542860
Iteration 12, loss = 0.10745451
Iteration 13, loss = 0.09333607
Iteration 14, loss = 0.08205714
Iteration 15, loss = 0.07335690
Iteration 16, loss = 0.06600540
Iteration 17, loss = 0.06016453
Iteration 18, loss = 0.05493194
Iteration 19, loss = 0.05075621
Iteration 20, loss = 0.04741046
Iteration 21, loss = 0.04417127
Iteration 22, loss = 0.04166856
Iteration 23, loss = 0.03921187
Iteration 24, loss = 0.03734980
Iteration 25, loss = 0.03572268
Iteration 26, loss = 0.03393534
Iteration 27, loss = 0.03255626
Iteration 28, loss = 0.03137221
Iteration 29, loss = 0.03045910
Iteration 30, loss = 0.02926813
Iteration 31, loss = 0.02860659
Iteration 32, loss = 0.02783903
Iteration 33, loss = 0.02701976
Iteration 34, loss = 0.02636670
Iteration 35, loss = 0.02586553
Iteration 36, loss = 0.02512121
Iteration 37, loss = 0.02468545
Iteration 38, loss = 0.02441548
Iteration 39, loss = 0.02397796
Iteration 40, loss = 0.02357447
Iteration 41, loss = 0.02302763
Iteration 42, loss = 0.02282103
Iteration 43, loss = 0.02269760
Iteration 44, loss = 0.02213481
Iteration 45, loss = 0.02194693
Iteration 46, loss = 0.02175017
Iteration 47, loss = 0.02137385
Iteration 48, loss = 0.02122506
Iteration 49, loss = 0.02093648
Iteration 50, loss = 0.02084821
Iteration 51, loss = 0.02071937
Iteration 52, loss = 0.02062856
Iteration 53, loss = 0.02085572
Iteration 54, loss = 0.02042628
Iteration 55, loss = 0.02011416
Iteration 56, loss = 0.02010445
Iteration 57, loss = 0.01995254
Iteration 58, loss = 0.01979247
Iteration 59, loss = 0.01971603
Iteration 60, loss = 0.01951648
Iteration 61, loss = 0.01919548
Iteration 62, loss = 0.01922357
Iteration 63, loss = 0.01889697
Iteration 64, loss = 0.01889775
Iteration 65, loss = 0.01894222
Iteration 66, loss = 0.01890117
Iteration 67, loss = 0.01877085
Iteration 68, loss = 0.01898266
Iteration 69, loss = 0.01866056
Iteration 70, loss = 0.01910679
Iteration 71, loss = 0.01865314
Iteration 72, loss = 0.01848022
Iteration 73, loss = 0.01845893
Iteration 74, loss = 0.01859116
Iteration 75, loss = 0.01834742
Iteration 76, loss = 0.01814886
Iteration 77, loss = 0.01815505
Iteration 78, loss = 0.01814585
Iteration 79, loss = 0.01811654
Iteration 80, loss = 0.01812512
Iteration 81, loss = 0.01812142
Iteration 82, loss = 0.01799852
Iteration 83, loss = 0.01802494
Iteration 84, loss = 0.01787524
Iteration 85, loss = 0.01795730
Iteration 86, loss = 0.01788597
Iteration 87, loss = 0.01765716
Iteration 88, loss = 0.01772943
Iteration 89, loss = 0.01775622
Iteration 90, loss = 0.01780987
Iteration 91, loss = 0.01767520
Iteration 92, loss = 0.01769611
Iteration 93, loss = 0.01739367
Iteration 94, loss = 0.01754714
Iteration 95, loss = 0.01745260
Iteration 96, loss = 0.01746213
Iteration 97, loss = 0.01756587
Iteration 98, loss = 0.01740128
Iteration 99, loss = 0.01752634
Iteration 100, loss = 0.01722434
Iteration 101, loss = 0.01758428
Iteration 102, loss = 0.01720798
Iteration 103, loss = 0.01760286
Iteration 104, loss = 0.01744717
Iteration 105, loss = 0.01720632
Iteration 106, loss = 0.01738396
Iteration 107, loss = 0.01744083
Iteration 108, loss = 0.01737402
Iteration 109, loss = 0.01727188
Iteration 110, loss = 0.01735614
Iteration 111, loss = 0.01711840
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67699098
Iteration 2, loss = 0.61271028
Iteration 3, loss = 0.51846232
Iteration 4, loss = 0.42122915
Iteration 5, loss = 0.33387872
Iteration 6, loss = 0.26107971
Iteration 7, loss = 0.20464025
Iteration 8, loss = 0.16369109
Iteration 9, loss = 0.13355453
Iteration 10, loss = 0.11149473
Iteration 11, loss = 0.09558844
Iteration 12, loss = 0.08312166
Iteration 13, loss = 0.07358915
Iteration 14, loss = 0.06610529
Iteration 15, loss = 0.06035617
Iteration 16, loss = 0.05538512
Iteration 17, loss = 0.05143474
Iteration 18, loss = 0.04799433
Iteration 19, loss = 0.04525747
Iteration 20, loss = 0.04299934
Iteration 21, loss = 0.04091507
Iteration 22, loss = 0.03944125
Iteration 23, loss = 0.03767614
Iteration 24, loss = 0.03630982
Iteration 25, loss = 0.03567642
Iteration 26, loss = 0.03521109
Iteration 27, loss = 0.03364306
Iteration 28, loss = 0.03274883
Iteration 29, loss = 0.03208542
Iteration 30, loss = 0.03165410
Iteration 31, loss = 0.03079957
Iteration 32, loss = 0.03041148
Iteration 33, loss = 0.03001496
Iteration 34, loss = 0.02926684
Iteration 35, loss = 0.02872869
Iteration 36, loss = 0.02882386
Iteration 37, loss = 0.02862027
Iteration 38, loss = 0.02813380
Iteration 39, loss = 0.02767307
Iteration 40, loss = 0.02803906
Iteration 41, loss = 0.02790461
Iteration 42, loss = 0.02827288
Iteration 43, loss = 0.02773225
Iteration 44, loss = 0.02748965
Iteration 45, loss = 0.02710144
Iteration 46, loss = 0.02679683
Iteration 47, loss = 0.02665474
Iteration 48, loss = 0.02660986
Iteration 49, loss = 0.02619296
Iteration 50, loss = 0.02615833
Iteration 51, loss = 0.02601010
Iteration 52, loss = 0.02570823
Iteration 53, loss = 0.02540135
Iteration 54, loss = 0.02532786
Iteration 55, loss = 0.02519329
Iteration 56, loss = 0.02494971
Iteration 57, loss = 0.02517970
Iteration 58, loss = 0.02479484
Iteration 59, loss = 0.02485236
Iteration 60, loss = 0.02487437
Iteration 61, loss = 0.02438583
Iteration 62, loss = 0.02444187
Iteration 63, loss = 0.02394081
Iteration 64, loss = 0.02405409
Iteration 65, loss = 0.02408390
Iteration 66, loss = 0.02405492
Iteration 67, loss = 0.02388913
Iteration 68, loss = 0.02366583
Iteration 69, loss = 0.02373859
Iteration 70, loss = 0.02365998
Iteration 71, loss = 0.02346426
Iteration 72, loss = 0.02327764
Iteration 73, loss = 0.02331848
Iteration 74, loss = 0.02310382
Iteration 75, loss = 0.02346965
Iteration 76, loss = 0.02325040
Iteration 77, loss = 0.02331433
Iteration 78, loss = 0.02343208
Iteration 79, loss = 0.02313311
Iteration 80, loss = 0.02317858
Iteration 81, loss = 0.02311121
Iteration 82, loss = 0.02291952
Iteration 83, loss = 0.02294347
Iteration 84, loss = 0.02298015
Iteration 85, loss = 0.02294453
Iteration 86, loss = 0.02280971
Iteration 87, loss = 0.02280281
Iteration 88, loss = 0.02351944
Iteration 89, loss = 0.02343154
Iteration 90, loss = 0.02301400
Iteration 91, loss = 0.02296501
Iteration 92, loss = 0.02325842
Iteration 93, loss = 0.02292367
Iteration 94, loss = 0.02284418
Iteration 95, loss = 0.02301470
Iteration 96, loss = 0.02284840
Iteration 97, loss = 0.02256950
Iteration 98, loss = 0.02258996
Iteration 99, loss = 0.02257752
Iteration 100, loss = 0.02269557
Iteration 101, loss = 0.02246690
Iteration 102, loss = 0.02299650
Iteration 103, loss = 0.02312973
Iteration 104, loss = 0.02285674
Iteration 105, loss = 0.02288927
Iteration 106, loss = 0.02264963
Iteration 107, loss = 0.02283976
Iteration 108, loss = 0.02266754
Iteration 109, loss = 0.02270108
Iteration 110, loss = 0.02271525
Iteration 111, loss = 0.02246995
Iteration 112, loss = 0.02260971
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69015115
Iteration 2, loss = 0.67126436
Iteration 3, loss = 0.65355159
Iteration 4, loss = 0.63318231
Iteration 5, loss = 0.60922773
Iteration 6, loss = 0.58132582
Iteration 7, loss = 0.54979158
Iteration 8, loss = 0.51580083
Iteration 9, loss = 0.48021833
Iteration 10, loss = 0.44413850
Iteration 11, loss = 0.40848611
Iteration 12, loss = 0.37413496
Iteration 13, loss = 0.34171401
Iteration 14, loss = 0.31133355
Iteration 15, loss = 0.28333938
Iteration 16, loss = 0.25759422
Iteration 17, loss = 0.23436312
Iteration 18, loss = 0.21326821
Iteration 19, loss = 0.19436762
Iteration 20, loss = 0.17743197
Iteration 21, loss = 0.16235268
Iteration 22, loss = 0.14893534
Iteration 23, loss = 0.13692347
Iteration 24, loss = 0.12623287
Iteration 25, loss = 0.11678485
Iteration 26, loss = 0.10828974
Iteration 27, loss = 0.10067270
Iteration 28, loss = 0.09392296
Iteration 29, loss = 0.08788355
Iteration 30, loss = 0.08240240
Iteration 31, loss = 0.07740518
Iteration 32, loss = 0.07300005
Iteration 33, loss = 0.06891446
Iteration 34, loss = 0.06523484
Iteration 35, loss = 0.06193208
Iteration 36, loss = 0.05895770
Iteration 37, loss = 0.05611302
Iteration 38, loss = 0.05359248
Iteration 39, loss = 0.05116216
Iteration 40, loss = 0.04900144
Iteration 41, loss = 0.04712712
Iteration 42, loss = 0.04516174
Iteration 43, loss = 0.04354909
Iteration 44, loss = 0.04183929
Iteration 45, loss = 0.04035279
Iteration 46, loss = 0.03901386
Iteration 47, loss = 0.03769475
Iteration 48, loss = 0.03651495
Iteration 49, loss = 0.03529995
Iteration 50, loss = 0.03434225
Iteration 51, loss = 0.03325853
Iteration 52, loss = 0.03233567
Iteration 53, loss = 0.03142934
Iteration 54, loss = 0.03069346
Iteration 55, loss = 0.02985217
Iteration 56, loss = 0.02909908
Iteration 57, loss = 0.02841829
Iteration 58, loss = 0.02775025
Iteration 59, loss = 0.02710298
Iteration 60, loss = 0.02655820
Iteration 61, loss = 0.02603514
Iteration 62, loss = 0.02543982
Iteration 63, loss = 0.02497516
Iteration 64, loss = 0.02448172
Iteration 65, loss = 0.02398144
Iteration 66, loss = 0.02356858
Iteration 67, loss = 0.02317378
Iteration 68, loss = 0.02277323
Iteration 69, loss = 0.02238499
Iteration 70, loss = 0.02199780
Iteration 71, loss = 0.02166440
Iteration 72, loss = 0.02130020
Iteration 73, loss = 0.02107241
Iteration 74, loss = 0.02071546
Iteration 75, loss = 0.02046019
Iteration 76, loss = 0.02019552
Iteration 77, loss = 0.01989499
Iteration 78, loss = 0.01962324
Iteration 79, loss = 0.01935256
Iteration 80, loss = 0.01911040
Iteration 81, loss = 0.01888485
Iteration 82, loss = 0.01868550
Iteration 83, loss = 0.01846592
Iteration 84, loss = 0.01830031
Iteration 85, loss = 0.01806964
Iteration 86, loss = 0.01796004
Iteration 87, loss = 0.01774704
Iteration 88, loss = 0.01756368
Iteration 89, loss = 0.01744875
Iteration 90, loss = 0.01723504
Iteration 91, loss = 0.01701722
Iteration 92, loss = 0.01688497
Iteration 93, loss = 0.01675563
Iteration 94, loss = 0.01663255
Iteration 95, loss = 0.01645748
Iteration 96, loss = 0.01629199
Iteration 97, loss = 0.01625503
Iteration 98, loss = 0.01605164
Iteration 99, loss = 0.01600332
Iteration 100, loss = 0.01578531
Iteration 101, loss = 0.01572035
Iteration 102, loss = 0.01555945
Iteration 103, loss = 0.01552677
Iteration 104, loss = 0.01537649
Iteration 105, loss = 0.01525476
Iteration 106, loss = 0.01519396
Iteration 107, loss = 0.01511570
Iteration 108, loss = 0.01500755
Iteration 109, loss = 0.01489533
Iteration 110, loss = 0.01479403
Iteration 111, loss = 0.01474513
Iteration 112, loss = 0.01467035
Iteration 113, loss = 0.01458148
Iteration 114, loss = 0.01449446
Iteration 115, loss = 0.01441804
Iteration 116, loss = 0.01433064
Iteration 117, loss = 0.01427549
Iteration 118, loss = 0.01419424
Iteration 119, loss = 0.01412045
Iteration 120, loss = 0.01408664
Iteration 121, loss = 0.01401412
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68312114
Iteration 2, loss = 0.64983767
Iteration 3, loss = 0.60694077
Iteration 4, loss = 0.55099556
Iteration 5, loss = 0.48826527
Iteration 6, loss = 0.42566593
Iteration 7, loss = 0.36744792
Iteration 8, loss = 0.31461817
Iteration 9, loss = 0.26842160
Iteration 10, loss = 0.22884567
Iteration 11, loss = 0.19541704
Iteration 12, loss = 0.16775968
Iteration 13, loss = 0.14498964
Iteration 14, loss = 0.12629608
Iteration 15, loss = 0.11087348
Iteration 16, loss = 0.09811411
Iteration 17, loss = 0.08764539
Iteration 18, loss = 0.07881939
Iteration 19, loss = 0.07133987
Iteration 20, loss = 0.06504857
Iteration 21, loss = 0.05962694
Iteration 22, loss = 0.05512305
Iteration 23, loss = 0.05105496
Iteration 24, loss = 0.04763517
Iteration 25, loss = 0.04451551
Iteration 26, loss = 0.04190268
Iteration 27, loss = 0.03943659
Iteration 28, loss = 0.03743891
Iteration 29, loss = 0.03558808
Iteration 30, loss = 0.03390473
Iteration 31, loss = 0.03242359
Iteration 32, loss = 0.03098064
Iteration 33, loss = 0.02980081
Iteration 34, loss = 0.02878451
Iteration 35, loss = 0.02769230
Iteration 36, loss = 0.02678472
Iteration 37, loss = 0.02603161
Iteration 38, loss = 0.02513924
Iteration 39, loss = 0.02441234
Iteration 40, loss = 0.02375642
Iteration 41, loss = 0.02315579
Iteration 42, loss = 0.02264564
Iteration 43, loss = 0.02214871
Iteration 44, loss = 0.02167149
Iteration 45, loss = 0.02112040
Iteration 46, loss = 0.02075652
Iteration 47, loss = 0.02041746
Iteration 48, loss = 0.01992514
Iteration 49, loss = 0.01971706
Iteration 50, loss = 0.01938102
Iteration 51, loss = 0.01904250
Iteration 52, loss = 0.01874406
Iteration 53, loss = 0.01857836
Iteration 54, loss = 0.01832163
Iteration 55, loss = 0.01798133
Iteration 56, loss = 0.01782945
Iteration 57, loss = 0.01754867
Iteration 58, loss = 0.01746502
Iteration 59, loss = 0.01720662
Iteration 60, loss = 0.01708754
Iteration 61, loss = 0.01696686
Iteration 62, loss = 0.01673719
Iteration 63, loss = 0.01653370
Iteration 64, loss = 0.01642184
Iteration 65, loss = 0.01627334
Iteration 66, loss = 0.01620467
Iteration 67, loss = 0.01615357
Iteration 68, loss = 0.01592193
Iteration 69, loss = 0.01581048
Iteration 70, loss = 0.01569604
Iteration 71, loss = 0.01552938
Iteration 72, loss = 0.01547809
Iteration 73, loss = 0.01550078
Iteration 74, loss = 0.01552494
Iteration 75, loss = 0.01528587
Iteration 76, loss = 0.01519640
Iteration 77, loss = 0.01504200
Iteration 78, loss = 0.01505016
Iteration 79, loss = 0.01496488
Iteration 80, loss = 0.01485962
Iteration 81, loss = 0.01486504
Iteration 82, loss = 0.01466078
Iteration 83, loss = 0.01495317
Iteration 84, loss = 0.01480445
Iteration 85, loss = 0.01460516
Iteration 86, loss = 0.01452870
Iteration 87, loss = 0.01447317
Iteration 88, loss = 0.01447960
Iteration 89, loss = 0.01425952
Iteration 90, loss = 0.01431171
Iteration 91, loss = 0.01422682
Iteration 92, loss = 0.01411119
Iteration 93, loss = 0.01410804
Iteration 94, loss = 0.01414804
Iteration 95, loss = 0.01402905
Iteration 96, loss = 0.01405622
Iteration 97, loss = 0.01400508
Iteration 98, loss = 0.01391725
Iteration 99, loss = 0.01389988
Iteration 100, loss = 0.01389947
Iteration 101, loss = 0.01377425
Iteration 102, loss = 0.01377388
Iteration 103, loss = 0.01367451
Iteration 104, loss = 0.01380205
Iteration 105, loss = 0.01368354
Iteration 106, loss = 0.01378264
Iteration 107, loss = 0.01365196
Iteration 108, loss = 0.01368174
Iteration 109, loss = 0.01353699
Iteration 110, loss = 0.01350951
Iteration 111, loss = 0.01353357
Iteration 112, loss = 0.01347893
Iteration 113, loss = 0.01356597
Iteration 114, loss = 0.01351308
Iteration 115, loss = 0.01343130
Iteration 116, loss = 0.01343434
Iteration 117, loss = 0.01342222
Iteration 118, loss = 0.01338717
Iteration 119, loss = 0.01330212
Iteration 120, loss = 0.01333754
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67807577
Iteration 2, loss = 0.63026010
Iteration 3, loss = 0.55729639
Iteration 4, loss = 0.47201399
Iteration 5, loss = 0.38753354
Iteration 6, loss = 0.31140471
Iteration 7, loss = 0.24828028
Iteration 8, loss = 0.19849948
Iteration 9, loss = 0.16085192
Iteration 10, loss = 0.13263992
Iteration 11, loss = 0.11134772
Iteration 12, loss = 0.09518508
Iteration 13, loss = 0.08284225
Iteration 14, loss = 0.07313500
Iteration 15, loss = 0.06538110
Iteration 16, loss = 0.05931278
Iteration 17, loss = 0.05413554
Iteration 18, loss = 0.05001082
Iteration 19, loss = 0.04643037
Iteration 20, loss = 0.04346938
Iteration 21, loss = 0.04101805
Iteration 22, loss = 0.03876511
Iteration 23, loss = 0.03678548
Iteration 24, loss = 0.03507478
Iteration 25, loss = 0.03374433
Iteration 26, loss = 0.03249747
Iteration 27, loss = 0.03149785
Iteration 28, loss = 0.03038641
Iteration 29, loss = 0.02938675
Iteration 30, loss = 0.02859412
Iteration 31, loss = 0.02796636
Iteration 32, loss = 0.02740797
Iteration 33, loss = 0.02683639
Iteration 34, loss = 0.02613348
Iteration 35, loss = 0.02549733
Iteration 36, loss = 0.02514178
Iteration 37, loss = 0.02479562
Iteration 38, loss = 0.02474011
Iteration 39, loss = 0.02425184
Iteration 40, loss = 0.02373189
Iteration 41, loss = 0.02365191
Iteration 42, loss = 0.02330744
Iteration 43, loss = 0.02304115
Iteration 44, loss = 0.02281308
Iteration 45, loss = 0.02238124
Iteration 46, loss = 0.02249684
Iteration 47, loss = 0.02209467
Iteration 48, loss = 0.02195315
Iteration 49, loss = 0.02183027
Iteration 50, loss = 0.02185535
Iteration 51, loss = 0.02143140
Iteration 52, loss = 0.02131138
Iteration 53, loss = 0.02122549
Iteration 54, loss = 0.02125913
Iteration 55, loss = 0.02106245
Iteration 56, loss = 0.02100216
Iteration 57, loss = 0.02081399
Iteration 58, loss = 0.02077861
Iteration 59, loss = 0.02054910
Iteration 60, loss = 0.02051632
Iteration 61, loss = 0.02054947
Iteration 62, loss = 0.02031281
Iteration 63, loss = 0.02045502
Iteration 64, loss = 0.02014932
Iteration 65, loss = 0.02025406
Iteration 66, loss = 0.02013158
Iteration 67, loss = 0.02009749
Iteration 68, loss = 0.01987429
Iteration 69, loss = 0.02002074
Iteration 70, loss = 0.01985809
Iteration 71, loss = 0.01992088
Iteration 72, loss = 0.01972487
Iteration 73, loss = 0.02002531
Iteration 74, loss = 0.01981571
Iteration 75, loss = 0.01970556
Iteration 76, loss = 0.01961310
Iteration 77, loss = 0.01951696
Iteration 78, loss = 0.01952152
Iteration 79, loss = 0.01962240
Iteration 80, loss = 0.01938134
Iteration 81, loss = 0.01953589
Iteration 82, loss = 0.01944529
Iteration 83, loss = 0.01931154
Iteration 84, loss = 0.01924341
Iteration 85, loss = 0.01944135
Iteration 86, loss = 0.01959116
Iteration 87, loss = 0.01933075
Iteration 88, loss = 0.01947692
Iteration 89, loss = 0.01926445
Iteration 90, loss = 0.01934188
Iteration 91, loss = 0.01920804
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70407059
Iteration 2, loss = 0.64114192
Iteration 3, loss = 0.54902929
Iteration 4, loss = 0.43802309
Iteration 5, loss = 0.33481120
Iteration 6, loss = 0.25293654
Iteration 7, loss = 0.19334444
Iteration 8, loss = 0.15179379
Iteration 9, loss = 0.12269901
Iteration 10, loss = 0.10199032
Iteration 11, loss = 0.08667904
Iteration 12, loss = 0.07557169
Iteration 13, loss = 0.06703294
Iteration 14, loss = 0.06035985
Iteration 15, loss = 0.05506245
Iteration 16, loss = 0.05081369
Iteration 17, loss = 0.04740503
Iteration 18, loss = 0.04418484
Iteration 19, loss = 0.04191935
Iteration 20, loss = 0.03994556
Iteration 21, loss = 0.03813991
Iteration 22, loss = 0.03665082
Iteration 23, loss = 0.03508529
Iteration 24, loss = 0.03446241
Iteration 25, loss = 0.03326656
Iteration 26, loss = 0.03226303
Iteration 27, loss = 0.03174483
Iteration 28, loss = 0.03073951
Iteration 29, loss = 0.03027362
Iteration 30, loss = 0.02974771
Iteration 31, loss = 0.02944811
Iteration 32, loss = 0.02873125
Iteration 33, loss = 0.02833109
Iteration 34, loss = 0.02813569
Iteration 35, loss = 0.02780879
Iteration 36, loss = 0.02727361
Iteration 37, loss = 0.02708206
Iteration 38, loss = 0.02673291
Iteration 39, loss = 0.02642924
Iteration 40, loss = 0.02624213
Iteration 41, loss = 0.02609423
Iteration 42, loss = 0.02580346
Iteration 43, loss = 0.02586787
Iteration 44, loss = 0.02548839
Iteration 45, loss = 0.02526401
Iteration 46, loss = 0.02536575
Iteration 47, loss = 0.02535448
Iteration 48, loss = 0.02477546
Iteration 49, loss = 0.02487815
Iteration 50, loss = 0.02470829
Iteration 51, loss = 0.02463334
Iteration 52, loss = 0.02447219
Iteration 53, loss = 0.02425374
Iteration 54, loss = 0.02419501
Iteration 55, loss = 0.02456547
Iteration 56, loss = 0.02408016
Iteration 57, loss = 0.02394760
Iteration 58, loss = 0.02386933
Iteration 59, loss = 0.02382017
Iteration 60, loss = 0.02384240
Iteration 61, loss = 0.02380077
Iteration 62, loss = 0.02377918
Iteration 63, loss = 0.02392625
Iteration 64, loss = 0.02351194
Iteration 65, loss = 0.02369812
Iteration 66, loss = 0.02352170
Iteration 67, loss = 0.02340327
Iteration 68, loss = 0.02345670
Iteration 69, loss = 0.02366538
Iteration 70, loss = 0.02339453
Iteration 71, loss = 0.02333740
Iteration 72, loss = 0.02332701
Iteration 73, loss = 0.02320138
Iteration 74, loss = 0.02347012
Iteration 75, loss = 0.02317668
Iteration 76, loss = 0.02324102
Iteration 77, loss = 0.02334024
Iteration 78, loss = 0.02320481
Iteration 79, loss = 0.02300518
Iteration 80, loss = 0.02304865
Iteration 81, loss = 0.02282318
Iteration 82, loss = 0.02328832
Iteration 83, loss = 0.02305599
Iteration 84, loss = 0.02283795
Iteration 85, loss = 0.02257151
Iteration 86, loss = 0.02294100
Iteration 87, loss = 0.02290833
Iteration 88, loss = 0.02306789
Iteration 89, loss = 0.02286306
Iteration 90, loss = 0.02312459
Iteration 91, loss = 0.02247593
Iteration 92, loss = 0.02276742
Iteration 93, loss = 0.02252953
Iteration 94, loss = 0.02251663
Iteration 95, loss = 0.02245518
Iteration 96, loss = 0.02255113
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71464122
Iteration 2, loss = 0.63194030
Iteration 3, loss = 0.52254647
Iteration 4, loss = 0.39824737
Iteration 5, loss = 0.29305408
Iteration 6, loss = 0.21641358
Iteration 7, loss = 0.16404767
Iteration 8, loss = 0.12874083
Iteration 9, loss = 0.10432794
Iteration 10, loss = 0.08728113
Iteration 11, loss = 0.07486152
Iteration 12, loss = 0.06542487
Iteration 13, loss = 0.05810509
Iteration 14, loss = 0.05279425
Iteration 15, loss = 0.04824053
Iteration 16, loss = 0.04470637
Iteration 17, loss = 0.04174359
Iteration 18, loss = 0.03940284
Iteration 19, loss = 0.03765602
Iteration 20, loss = 0.03524379
Iteration 21, loss = 0.03399605
Iteration 22, loss = 0.03248118
Iteration 23, loss = 0.03145808
Iteration 24, loss = 0.03043700
Iteration 25, loss = 0.02994023
Iteration 26, loss = 0.02911265
Iteration 27, loss = 0.02811177
Iteration 28, loss = 0.02746263
Iteration 29, loss = 0.02714853
Iteration 30, loss = 0.02647292
Iteration 31, loss = 0.02613714
Iteration 32, loss = 0.02568834
Iteration 33, loss = 0.02534195
Iteration 34, loss = 0.02503983
Iteration 35, loss = 0.02471246
Iteration 36, loss = 0.02448692
Iteration 37, loss = 0.02408969
Iteration 38, loss = 0.02412506
Iteration 39, loss = 0.02373560
Iteration 40, loss = 0.02347604
Iteration 41, loss = 0.02350898
Iteration 42, loss = 0.02335074
Iteration 43, loss = 0.02314183
Iteration 44, loss = 0.02309555
Iteration 45, loss = 0.02275293
Iteration 46, loss = 0.02250803
Iteration 47, loss = 0.02269256
Iteration 48, loss = 0.02242117
Iteration 49, loss = 0.02240608
Iteration 50, loss = 0.02227140
Iteration 51, loss = 0.02200838
Iteration 52, loss = 0.02217979
Iteration 53, loss = 0.02194779
Iteration 54, loss = 0.02186972
Iteration 55, loss = 0.02194760
Iteration 56, loss = 0.02189776
Iteration 57, loss = 0.02197101
Iteration 58, loss = 0.02176999
Iteration 59, loss = 0.02181810
Iteration 60, loss = 0.02152994
Iteration 61, loss = 0.02165156
Iteration 62, loss = 0.02152274
Iteration 63, loss = 0.02140408
Iteration 64, loss = 0.02150298
Iteration 65, loss = 0.02146045
Iteration 66, loss = 0.02124969
Iteration 67, loss = 0.02093442
Iteration 68, loss = 0.02112623
Iteration 69, loss = 0.02095713
Iteration 70, loss = 0.02140330
Iteration 71, loss = 0.02120803
Iteration 72, loss = 0.02116048
Iteration 73, loss = 0.02122291
Iteration 74, loss = 0.02095903
Iteration 75, loss = 0.02122130
Iteration 76, loss = 0.02095812
Iteration 77, loss = 0.02092414
Iteration 78, loss = 0.02106868
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67695681
Iteration 2, loss = 0.61028347
Iteration 3, loss = 0.52171979
Iteration 4, loss = 0.42729568
Iteration 5, loss = 0.34204072
Iteration 6, loss = 0.27106025
Iteration 7, loss = 0.21506563
Iteration 8, loss = 0.17308454
Iteration 9, loss = 0.14162200
Iteration 10, loss = 0.11898283
Iteration 11, loss = 0.10167345
Iteration 12, loss = 0.08864290
Iteration 13, loss = 0.07844295
Iteration 14, loss = 0.07023185
Iteration 15, loss = 0.06390356
Iteration 16, loss = 0.05886115
Iteration 17, loss = 0.05451598
Iteration 18, loss = 0.05103403
Iteration 19, loss = 0.04793795
Iteration 20, loss = 0.04529748
Iteration 21, loss = 0.04317600
Iteration 22, loss = 0.04154374
Iteration 23, loss = 0.03970483
Iteration 24, loss = 0.03867636
Iteration 25, loss = 0.03727550
Iteration 26, loss = 0.03600198
Iteration 27, loss = 0.03531511
Iteration 28, loss = 0.03447464
Iteration 29, loss = 0.03335158
Iteration 30, loss = 0.03262108
Iteration 31, loss = 0.03201883
Iteration 32, loss = 0.03140387
Iteration 33, loss = 0.03093083
Iteration 34, loss = 0.03040435
Iteration 35, loss = 0.02991034
Iteration 36, loss = 0.02945012
Iteration 37, loss = 0.02912121
Iteration 38, loss = 0.02880869
Iteration 39, loss = 0.02815416
Iteration 40, loss = 0.02804073
Iteration 41, loss = 0.02792377
Iteration 42, loss = 0.02779561
Iteration 43, loss = 0.02906658
Iteration 44, loss = 0.02908306
Iteration 45, loss = 0.02842488
Iteration 46, loss = 0.02790791
Iteration 47, loss = 0.02764260
Iteration 48, loss = 0.02726171
Iteration 49, loss = 0.02730617
Iteration 50, loss = 0.02690716
Iteration 51, loss = 0.02675469
Iteration 52, loss = 0.02678464
Iteration 53, loss = 0.02645164
Iteration 54, loss = 0.02623988
Iteration 55, loss = 0.02679464
Iteration 56, loss = 0.02767260
Iteration 57, loss = 0.02658814
Iteration 58, loss = 0.02625372
Iteration 59, loss = 0.02627100
Iteration 60, loss = 0.02609020
Iteration 61, loss = 0.02599110
Iteration 62, loss = 0.02566913
Iteration 63, loss = 0.02556471
Iteration 64, loss = 0.02523432
Iteration 65, loss = 0.02561195
Iteration 66, loss = 0.02534897
Iteration 67, loss = 0.02613051
Iteration 68, loss = 0.02589739
Iteration 69, loss = 0.02668912
Iteration 70, loss = 0.02628377
Iteration 71, loss = 0.02619412
Iteration 72, loss = 0.02589892
Iteration 73, loss = 0.02588503
Iteration 74, loss = 0.02587711
Iteration 75, loss = 0.02576212
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68253030
Iteration 2, loss = 0.65610002
Iteration 3, loss = 0.62703806
Iteration 4, loss = 0.59093708
Iteration 5, loss = 0.54693483
Iteration 6, loss = 0.49742448
Iteration 7, loss = 0.44620288
Iteration 8, loss = 0.39594696
Iteration 9, loss = 0.34921014
Iteration 10, loss = 0.30686550
Iteration 11, loss = 0.26909211
Iteration 12, loss = 0.23619775
Iteration 13, loss = 0.20804882
Iteration 14, loss = 0.18358315
Iteration 15, loss = 0.16291290
Iteration 16, loss = 0.14535553
Iteration 17, loss = 0.13031434
Iteration 18, loss = 0.11749390
Iteration 19, loss = 0.10666915
Iteration 20, loss = 0.09732683
Iteration 21, loss = 0.08925149
Iteration 22, loss = 0.08231744
Iteration 23, loss = 0.07632252
Iteration 24, loss = 0.07077197
Iteration 25, loss = 0.06613634
Iteration 26, loss = 0.06189651
Iteration 27, loss = 0.05822399
Iteration 28, loss = 0.05497520
Iteration 29, loss = 0.05223132
Iteration 30, loss = 0.04972480
Iteration 31, loss = 0.04754965
Iteration 32, loss = 0.04531187
Iteration 33, loss = 0.04328661
Iteration 34, loss = 0.04155866
Iteration 35, loss = 0.03986580
Iteration 36, loss = 0.03841348
Iteration 37, loss = 0.03722293
Iteration 38, loss = 0.03593059
Iteration 39, loss = 0.03480405
Iteration 40, loss = 0.03370279
Iteration 41, loss = 0.03273972
Iteration 42, loss = 0.03180991
Iteration 43, loss = 0.03099966
Iteration 44, loss = 0.03033465
Iteration 45, loss = 0.02994056
Iteration 46, loss = 0.02908718
Iteration 47, loss = 0.02855645
Iteration 48, loss = 0.02788356
Iteration 49, loss = 0.02730125
Iteration 50, loss = 0.02666515
Iteration 51, loss = 0.02605631
Iteration 52, loss = 0.02571725
Iteration 53, loss = 0.02512346
Iteration 54, loss = 0.02476135
Iteration 55, loss = 0.02427201
Iteration 56, loss = 0.02391646
Iteration 57, loss = 0.02362593
Iteration 58, loss = 0.02330694
Iteration 59, loss = 0.02317885
Iteration 60, loss = 0.02293355
Iteration 61, loss = 0.02268437
Iteration 62, loss = 0.02218847
Iteration 63, loss = 0.02212266
Iteration 64, loss = 0.02194277
Iteration 65, loss = 0.02163037
Iteration 66, loss = 0.02132491
Iteration 67, loss = 0.02107828
Iteration 68, loss = 0.02106493
Iteration 69, loss = 0.02082250
Iteration 70, loss = 0.02055639
Iteration 71, loss = 0.02036932
Iteration 72, loss = 0.02020308
Iteration 73, loss = 0.02005909
Iteration 74, loss = 0.01986022
Iteration 75, loss = 0.01961133
Iteration 76, loss = 0.01971354
Iteration 77, loss = 0.01957031
Iteration 78, loss = 0.01947944
Iteration 79, loss = 0.01919037
Iteration 80, loss = 0.01906658
Iteration 81, loss = 0.01889465
Iteration 82, loss = 0.01886707
Iteration 83, loss = 0.01859275
Iteration 84, loss = 0.01875992
Iteration 85, loss = 0.01863072
Iteration 86, loss = 0.01855209
Iteration 87, loss = 0.01832866
Iteration 88, loss = 0.01827741
Iteration 89, loss = 0.01812806
Iteration 90, loss = 0.01821155
Iteration 91, loss = 0.01804917
Iteration 92, loss = 0.01799114
Iteration 93, loss = 0.01784515
Iteration 94, loss = 0.01771623
Iteration 95, loss = 0.01763074
Iteration 96, loss = 0.01749978
Iteration 97, loss = 0.01764107
Iteration 98, loss = 0.01736855
Iteration 99, loss = 0.01748323
Iteration 100, loss = 0.01740535
Iteration 101, loss = 0.01719958
Iteration 102, loss = 0.01712752
Iteration 103, loss = 0.01720444
Iteration 104, loss = 0.01708636
Iteration 105, loss = 0.01693152
Iteration 106, loss = 0.01676482
Iteration 107, loss = 0.01674821
Iteration 108, loss = 0.01676873
Iteration 109, loss = 0.01675881
Iteration 110, loss = 0.01672454
Iteration 111, loss = 0.01667526
Iteration 112, loss = 0.01653825
Iteration 113, loss = 0.01648456
Iteration 114, loss = 0.01647415
Iteration 115, loss = 0.01642989
Iteration 116, loss = 0.01636013
Iteration 117, loss = 0.01636528
Iteration 118, loss = 0.01638237
Iteration 119, loss = 0.01646648
Iteration 120, loss = 0.01636273
Iteration 121, loss = 0.01624978
Iteration 122, loss = 0.01624727
Iteration 123, loss = 0.01627230
Iteration 124, loss = 0.01613912
Iteration 125, loss = 0.01590269
Iteration 126, loss = 0.01638697
Iteration 127, loss = 0.01631912
Iteration 128, loss = 0.01634643
Iteration 129, loss = 0.01634994
Iteration 130, loss = 0.01609102
Iteration 131, loss = 0.01636814
Iteration 132, loss = 0.01653161
Iteration 133, loss = 0.01629395
Iteration 134, loss = 0.01618014
Iteration 135, loss = 0.01600354
Iteration 136, loss = 0.01590918
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67780831
Iteration 2, loss = 0.62551040
Iteration 3, loss = 0.53180952
Iteration 4, loss = 0.42382008
Iteration 5, loss = 0.32587574
Iteration 6, loss = 0.24786383
Iteration 7, loss = 0.19065128
Iteration 8, loss = 0.14978046
Iteration 9, loss = 0.12093274
Iteration 10, loss = 0.10038441
Iteration 11, loss = 0.08529668
Iteration 12, loss = 0.07431639
Iteration 13, loss = 0.06564191
Iteration 14, loss = 0.05920871
Iteration 15, loss = 0.05352687
Iteration 16, loss = 0.04907993
Iteration 17, loss = 0.04560264
Iteration 18, loss = 0.04259985
Iteration 19, loss = 0.04018217
Iteration 20, loss = 0.03809985
Iteration 21, loss = 0.03638398
Iteration 22, loss = 0.03476375
Iteration 23, loss = 0.03345382
Iteration 24, loss = 0.03201287
Iteration 25, loss = 0.03083105
Iteration 26, loss = 0.03013168
Iteration 27, loss = 0.02931971
Iteration 28, loss = 0.02847283
Iteration 29, loss = 0.02788284
Iteration 30, loss = 0.02761307
Iteration 31, loss = 0.02722042
Iteration 32, loss = 0.02641074
Iteration 33, loss = 0.02592805
Iteration 34, loss = 0.02568960
Iteration 35, loss = 0.02526159
Iteration 36, loss = 0.02500267
Iteration 37, loss = 0.02473153
Iteration 38, loss = 0.02433616
Iteration 39, loss = 0.02434298
Iteration 40, loss = 0.02390187
Iteration 41, loss = 0.02377935
Iteration 42, loss = 0.02350065
Iteration 43, loss = 0.02323436
Iteration 44, loss = 0.02306080
Iteration 45, loss = 0.02284075
Iteration 46, loss = 0.02271629
Iteration 47, loss = 0.02275092
Iteration 48, loss = 0.02237154
Iteration 49, loss = 0.02238312
Iteration 50, loss = 0.02228961
Iteration 51, loss = 0.02225178
Iteration 52, loss = 0.02186443
Iteration 53, loss = 0.02205276
Iteration 54, loss = 0.02177848
Iteration 55, loss = 0.02196102
Iteration 56, loss = 0.02195791
Iteration 57, loss = 0.02166097
Iteration 58, loss = 0.02167139
Iteration 59, loss = 0.02176999
Iteration 60, loss = 0.02153408
Iteration 61, loss = 0.02142974
Iteration 62, loss = 0.02135958
Iteration 63, loss = 0.02119002
Iteration 64, loss = 0.02141004
Iteration 65, loss = 0.02096787
Iteration 66, loss = 0.02111455
Iteration 67, loss = 0.02078495
Iteration 68, loss = 0.02095038
Iteration 69, loss = 0.02119868
Iteration 70, loss = 0.02105841
Iteration 71, loss = 0.02083985
Iteration 72, loss = 0.02098044
Iteration 73, loss = 0.02095235
Iteration 74, loss = 0.02087612
Iteration 75, loss = 0.02109125
Iteration 76, loss = 0.02091331
Iteration 77, loss = 0.02045283
Iteration 78, loss = 0.02066589
Iteration 79, loss = 0.02086989
Iteration 80, loss = 0.02063001
Iteration 81, loss = 0.02077913
Iteration 82, loss = 0.02051887
Iteration 83, loss = 0.02074484
Iteration 84, loss = 0.02042160
Iteration 85, loss = 0.02060403
Iteration 86, loss = 0.02054170
Iteration 87, loss = 0.02039265
Iteration 88, loss = 0.02036430
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67397732
Iteration 2, loss = 0.60290764
Iteration 3, loss = 0.49252037
Iteration 4, loss = 0.38011404
Iteration 5, loss = 0.28630398
Iteration 6, loss = 0.21539278
Iteration 7, loss = 0.16560123
Iteration 8, loss = 0.13158851
Iteration 9, loss = 0.10759500
Iteration 10, loss = 0.09080484
Iteration 11, loss = 0.07877378
Iteration 12, loss = 0.06930715
Iteration 13, loss = 0.06189577
Iteration 14, loss = 0.05672506
Iteration 15, loss = 0.05208211
Iteration 16, loss = 0.04855335
Iteration 17, loss = 0.04553453
Iteration 18, loss = 0.04275653
Iteration 19, loss = 0.04083563
Iteration 20, loss = 0.03905898
Iteration 21, loss = 0.03749234
Iteration 22, loss = 0.03621375
Iteration 23, loss = 0.03504324
Iteration 24, loss = 0.03418689
Iteration 25, loss = 0.03342143
Iteration 26, loss = 0.03233627
Iteration 27, loss = 0.03166008
Iteration 28, loss = 0.03089828
Iteration 29, loss = 0.03054657
Iteration 30, loss = 0.02996864
Iteration 31, loss = 0.02957771
Iteration 32, loss = 0.02923690
Iteration 33, loss = 0.02893365
Iteration 34, loss = 0.02877332
Iteration 35, loss = 0.02834044
Iteration 36, loss = 0.02795514
Iteration 37, loss = 0.02789999
Iteration 38, loss = 0.02783321
Iteration 39, loss = 0.02730571
Iteration 40, loss = 0.02762836
Iteration 41, loss = 0.02712335
Iteration 42, loss = 0.02662421
Iteration 43, loss = 0.02651538
Iteration 44, loss = 0.02638336
Iteration 45, loss = 0.02642882
Iteration 46, loss = 0.02630525
Iteration 47, loss = 0.02629266
Iteration 48, loss = 0.02638096
Iteration 49, loss = 0.02630481
Iteration 50, loss = 0.02561748
Iteration 51, loss = 0.02594326
Iteration 52, loss = 0.02569892
Iteration 53, loss = 0.02546968
Iteration 54, loss = 0.02551062
Iteration 55, loss = 0.02533998
Iteration 56, loss = 0.02526799
Iteration 57, loss = 0.02530422
Iteration 58, loss = 0.02516597
Iteration 59, loss = 0.02543028
Iteration 60, loss = 0.02506760
Iteration 61, loss = 0.02509243
Iteration 62, loss = 0.02511476
Iteration 63, loss = 0.02519195
Iteration 64, loss = 0.02512768
Iteration 65, loss = 0.02497621
Iteration 66, loss = 0.02478380
Iteration 67, loss = 0.02475053
Iteration 68, loss = 0.02497145
Iteration 69, loss = 0.02505588
Iteration 70, loss = 0.02506111
Iteration 71, loss = 0.02457352
Iteration 72, loss = 0.02473603
Iteration 73, loss = 0.02493426
Iteration 74, loss = 0.02471725
Iteration 75, loss = 0.02479936
Iteration 76, loss = 0.02477220
Iteration 77, loss = 0.02476599
Iteration 78, loss = 0.02463080
Iteration 79, loss = 0.02468954
Iteration 80, loss = 0.02475826
Iteration 81, loss = 0.02463772
Iteration 82, loss = 0.02442057
Iteration 83, loss = 0.02463989
Iteration 84, loss = 0.02500211
Iteration 85, loss = 0.02432248
Iteration 86, loss = 0.02451502
Iteration 87, loss = 0.02426417
Iteration 88, loss = 0.02418943
Iteration 89, loss = 0.02440416
Iteration 90, loss = 0.02432644
Iteration 91, loss = 0.02414013
Iteration 92, loss = 0.02430187
Iteration 93, loss = 0.02424004
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68845907
Iteration 2, loss = 0.65674290
Iteration 3, loss = 0.61040105
Iteration 4, loss = 0.54718389
Iteration 5, loss = 0.47790095
Iteration 6, loss = 0.40745459
Iteration 7, loss = 0.34278599
Iteration 8, loss = 0.28569115
Iteration 9, loss = 0.23768102
Iteration 10, loss = 0.19863324
Iteration 11, loss = 0.16749026
Iteration 12, loss = 0.14258414
Iteration 13, loss = 0.12280761
Iteration 14, loss = 0.10694612
Iteration 15, loss = 0.09428364
Iteration 16, loss = 0.08398568
Iteration 17, loss = 0.07546697
Iteration 18, loss = 0.06842222
Iteration 19, loss = 0.06258003
Iteration 20, loss = 0.05762075
Iteration 21, loss = 0.05339370
Iteration 22, loss = 0.04994257
Iteration 23, loss = 0.04672668
Iteration 24, loss = 0.04403610
Iteration 25, loss = 0.04167251
Iteration 26, loss = 0.03957039
Iteration 27, loss = 0.03765827
Iteration 28, loss = 0.03621869
Iteration 29, loss = 0.03474787
Iteration 30, loss = 0.03333273
Iteration 31, loss = 0.03234666
Iteration 32, loss = 0.03124283
Iteration 33, loss = 0.03021403
Iteration 34, loss = 0.02957169
Iteration 35, loss = 0.02864560
Iteration 36, loss = 0.02796121
Iteration 37, loss = 0.02718605
Iteration 38, loss = 0.02674600
Iteration 39, loss = 0.02610536
Iteration 40, loss = 0.02548592
Iteration 41, loss = 0.02520979
Iteration 42, loss = 0.02473547
Iteration 43, loss = 0.02423809
Iteration 44, loss = 0.02392109
Iteration 45, loss = 0.02351594
Iteration 46, loss = 0.02319163
Iteration 47, loss = 0.02299465
Iteration 48, loss = 0.02266339
Iteration 49, loss = 0.02241326
Iteration 50, loss = 0.02225403
Iteration 51, loss = 0.02191381
Iteration 52, loss = 0.02172318
Iteration 53, loss = 0.02158076
Iteration 54, loss = 0.02134399
Iteration 55, loss = 0.02118908
Iteration 56, loss = 0.02106990
Iteration 57, loss = 0.02094251
Iteration 58, loss = 0.02066559
Iteration 59, loss = 0.02051995
Iteration 60, loss = 0.02042061
Iteration 61, loss = 0.02032436
Iteration 62, loss = 0.02012547
Iteration 63, loss = 0.01999633
Iteration 64, loss = 0.02008692
Iteration 65, loss = 0.01981235
Iteration 66, loss = 0.01975319
Iteration 67, loss = 0.01954348
Iteration 68, loss = 0.01946951
Iteration 69, loss = 0.01941272
Iteration 70, loss = 0.01933920
Iteration 71, loss = 0.01945246
Iteration 72, loss = 0.01920068
Iteration 73, loss = 0.01905864
Iteration 74, loss = 0.01901352
Iteration 75, loss = 0.01902070
Iteration 76, loss = 0.01917421
Iteration 77, loss = 0.01884938
Iteration 78, loss = 0.01878614
Iteration 79, loss = 0.01872540
Iteration 80, loss = 0.01858556
Iteration 81, loss = 0.01868939
Iteration 82, loss = 0.01856671
Iteration 83, loss = 0.01849174
Iteration 84, loss = 0.01852363
Iteration 85, loss = 0.01858400
Iteration 86, loss = 0.01856738
Iteration 87, loss = 0.01829571
Iteration 88, loss = 0.01827738
Iteration 89, loss = 0.01835524
Iteration 90, loss = 0.01822553
Iteration 91, loss = 0.01827816
Iteration 92, loss = 0.01808418
Iteration 93, loss = 0.01807739
Iteration 94, loss = 0.01800731
Iteration 95, loss = 0.01816453
Iteration 96, loss = 0.01808597
Iteration 97, loss = 0.01794676
Iteration 98, loss = 0.01789970
Iteration 99, loss = 0.01808079
Iteration 100, loss = 0.01776163
Iteration 101, loss = 0.01789040
Iteration 102, loss = 0.01791193
Iteration 103, loss = 0.01786614
Iteration 104, loss = 0.01782785
Iteration 105, loss = 0.01790591
Iteration 106, loss = 0.01801423
Iteration 107, loss = 0.01775869
Iteration 108, loss = 0.01764716
Iteration 109, loss = 0.01774229
Iteration 110, loss = 0.01768344
Iteration 111, loss = 0.01773940
Iteration 112, loss = 0.01777218
Iteration 113, loss = 0.01765736
Iteration 114, loss = 0.01752074
Iteration 115, loss = 0.01757753
Iteration 116, loss = 0.01760296
Iteration 117, loss = 0.01758377
Iteration 118, loss = 0.01752364
Iteration 119, loss = 0.01750111
Iteration 120, loss = 0.01733082
Iteration 121, loss = 0.01769969
Iteration 122, loss = 0.01743289
Iteration 123, loss = 0.01752628
Iteration 124, loss = 0.01734153
Iteration 125, loss = 0.01754104
Iteration 126, loss = 0.01739238
Iteration 127, loss = 0.01733318
Iteration 128, loss = 0.01745907
Iteration 129, loss = 0.01737890
Iteration 130, loss = 0.01733024
Iteration 131, loss = 0.01741321
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68137013
Iteration 2, loss = 0.61512275
Iteration 3, loss = 0.51663105
Iteration 4, loss = 0.40829411
Iteration 5, loss = 0.31380095
Iteration 6, loss = 0.23940305
Iteration 7, loss = 0.18514945
Iteration 8, loss = 0.14623223
Iteration 9, loss = 0.11894389
Iteration 10, loss = 0.09926941
Iteration 11, loss = 0.08460084
Iteration 12, loss = 0.07366337
Iteration 13, loss = 0.06520336
Iteration 14, loss = 0.05864417
Iteration 15, loss = 0.05295051
Iteration 16, loss = 0.04889153
Iteration 17, loss = 0.04528215
Iteration 18, loss = 0.04236834
Iteration 19, loss = 0.03979459
Iteration 20, loss = 0.03751227
Iteration 21, loss = 0.03598276
Iteration 22, loss = 0.03434619
Iteration 23, loss = 0.03306071
Iteration 24, loss = 0.03176109
Iteration 25, loss = 0.03106066
Iteration 26, loss = 0.02992403
Iteration 27, loss = 0.02883761
Iteration 28, loss = 0.02831521
Iteration 29, loss = 0.02757275
Iteration 30, loss = 0.02694349
Iteration 31, loss = 0.02647850
Iteration 32, loss = 0.02614125
Iteration 33, loss = 0.02555188
Iteration 34, loss = 0.02520080
Iteration 35, loss = 0.02473166
Iteration 36, loss = 0.02454045
Iteration 37, loss = 0.02419696
Iteration 38, loss = 0.02394138
Iteration 39, loss = 0.02367030
Iteration 40, loss = 0.02350751
Iteration 41, loss = 0.02308793
Iteration 42, loss = 0.02288601
Iteration 43, loss = 0.02322025
Iteration 44, loss = 0.02273327
Iteration 45, loss = 0.02253322
Iteration 46, loss = 0.02238725
Iteration 47, loss = 0.02207727
Iteration 48, loss = 0.02229726
Iteration 49, loss = 0.02188331
Iteration 50, loss = 0.02189989
Iteration 51, loss = 0.02185613
Iteration 52, loss = 0.02179840
Iteration 53, loss = 0.02144899
Iteration 54, loss = 0.02175756
Iteration 55, loss = 0.02134438
Iteration 56, loss = 0.02111239
Iteration 57, loss = 0.02150727
Iteration 58, loss = 0.02112060
Iteration 59, loss = 0.02121937
Iteration 60, loss = 0.02128305
Iteration 61, loss = 0.02120129
Iteration 62, loss = 0.02066572
Iteration 63, loss = 0.02081191
Iteration 64, loss = 0.02096037
Iteration 65, loss = 0.02066491
Iteration 66, loss = 0.02077093
Iteration 67, loss = 0.02037098
Iteration 68, loss = 0.02046720
Iteration 69, loss = 0.02044827
Iteration 70, loss = 0.02074607
Iteration 71, loss = 0.02070465
Iteration 72, loss = 0.02029495
Iteration 73, loss = 0.02052102
Iteration 74, loss = 0.02023153
Iteration 75, loss = 0.02036051
Iteration 76, loss = 0.02042367
Iteration 77, loss = 0.02021555
Iteration 78, loss = 0.02055177
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67878990
Iteration 2, loss = 0.63537089
Iteration 3, loss = 0.57925916
Iteration 4, loss = 0.50991782
Iteration 5, loss = 0.43847833
Iteration 6, loss = 0.37061126
Iteration 7, loss = 0.31030628
Iteration 8, loss = 0.25864249
Iteration 9, loss = 0.21613694
Iteration 10, loss = 0.18128921
Iteration 11, loss = 0.15378359
Iteration 12, loss = 0.13167277
Iteration 13, loss = 0.11411480
Iteration 14, loss = 0.09993993
Iteration 15, loss = 0.08846195
Iteration 16, loss = 0.07921816
Iteration 17, loss = 0.07147389
Iteration 18, loss = 0.06498267
Iteration 19, loss = 0.05957521
Iteration 20, loss = 0.05499624
Iteration 21, loss = 0.05101228
Iteration 22, loss = 0.04778274
Iteration 23, loss = 0.04498395
Iteration 24, loss = 0.04248168
Iteration 25, loss = 0.04005405
Iteration 26, loss = 0.03818683
Iteration 27, loss = 0.03657388
Iteration 28, loss = 0.03497657
Iteration 29, loss = 0.03362364
Iteration 30, loss = 0.03231557
Iteration 31, loss = 0.03137822
Iteration 32, loss = 0.03023777
Iteration 33, loss = 0.02931326
Iteration 34, loss = 0.02844645
Iteration 35, loss = 0.02778570
Iteration 36, loss = 0.02712935
Iteration 37, loss = 0.02646519
Iteration 38, loss = 0.02597255
Iteration 39, loss = 0.02528905
Iteration 40, loss = 0.02493551
Iteration 41, loss = 0.02456492
Iteration 42, loss = 0.02396291
Iteration 43, loss = 0.02369977
Iteration 44, loss = 0.02345190
Iteration 45, loss = 0.02294012
Iteration 46, loss = 0.02266721
Iteration 47, loss = 0.02254493
Iteration 48, loss = 0.02231003
Iteration 49, loss = 0.02187160
Iteration 50, loss = 0.02174560
Iteration 51, loss = 0.02142182
Iteration 52, loss = 0.02129152
Iteration 53, loss = 0.02100708
Iteration 54, loss = 0.02085401
Iteration 55, loss = 0.02075825
Iteration 56, loss = 0.02044172
Iteration 57, loss = 0.02028483
Iteration 58, loss = 0.02024693
Iteration 59, loss = 0.02018531
Iteration 60, loss = 0.01988781
Iteration 61, loss = 0.01985526
Iteration 62, loss = 0.01971159
Iteration 63, loss = 0.01946756
Iteration 64, loss = 0.01950475
Iteration 65, loss = 0.01943518
Iteration 66, loss = 0.01919532
Iteration 67, loss = 0.01929683
Iteration 68, loss = 0.01911368
Iteration 69, loss = 0.01901981
Iteration 70, loss = 0.01899337
Iteration 71, loss = 0.01887559
Iteration 72, loss = 0.01872653
Iteration 73, loss = 0.01859235
Iteration 74, loss = 0.01853637
Iteration 75, loss = 0.01841445
Iteration 76, loss = 0.01846051
Iteration 77, loss = 0.01838490
Iteration 78, loss = 0.01837772
Iteration 79, loss = 0.01823036
Iteration 80, loss = 0.01830825
Iteration 81, loss = 0.01832761
Iteration 82, loss = 0.01836527
Iteration 83, loss = 0.01823862
Iteration 84, loss = 0.01815109
Iteration 85, loss = 0.01799074
Iteration 86, loss = 0.01800890
Iteration 87, loss = 0.01785851
Iteration 88, loss = 0.01800309
Iteration 89, loss = 0.01778932
Iteration 90, loss = 0.01784753
Iteration 91, loss = 0.01771236
Iteration 92, loss = 0.01778802
Iteration 93, loss = 0.01767897
Iteration 94, loss = 0.01789434
Iteration 95, loss = 0.01756062
Iteration 96, loss = 0.01762644
Iteration 97, loss = 0.01764387
Iteration 98, loss = 0.01771942
Iteration 99, loss = 0.01758109
Iteration 100, loss = 0.01743118
Iteration 101, loss = 0.01749241
Iteration 102, loss = 0.01747425
Iteration 103, loss = 0.01739970
Iteration 104, loss = 0.01750281
Iteration 105, loss = 0.01740792
Iteration 106, loss = 0.01735520
Iteration 107, loss = 0.01724654
Iteration 108, loss = 0.01736202
Iteration 109, loss = 0.01748679
Iteration 110, loss = 0.01733250
Iteration 111, loss = 0.01728265
Iteration 112, loss = 0.01717634
Iteration 113, loss = 0.01735594
Iteration 114, loss = 0.01725735
Iteration 115, loss = 0.01723127
Iteration 116, loss = 0.01723357
Iteration 117, loss = 0.01713997
Iteration 118, loss = 0.01706901
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69909126
Iteration 2, loss = 0.62903202
Iteration 3, loss = 0.53775775
Iteration 4, loss = 0.43558389
Iteration 5, loss = 0.34432404
Iteration 6, loss = 0.26931082
Iteration 7, loss = 0.21268733
Iteration 8, loss = 0.17048065
Iteration 9, loss = 0.13910413
Iteration 10, loss = 0.11628933
Iteration 11, loss = 0.09892431
Iteration 12, loss = 0.08593157
Iteration 13, loss = 0.07560544
Iteration 14, loss = 0.06778425
Iteration 15, loss = 0.06180917
Iteration 16, loss = 0.05637485
Iteration 17, loss = 0.05224651
Iteration 18, loss = 0.04850768
Iteration 19, loss = 0.04561761
Iteration 20, loss = 0.04311969
Iteration 21, loss = 0.04095316
Iteration 22, loss = 0.03889094
Iteration 23, loss = 0.03725278
Iteration 24, loss = 0.03563065
Iteration 25, loss = 0.03460053
Iteration 26, loss = 0.03332356
Iteration 27, loss = 0.03220902
Iteration 28, loss = 0.03131652
Iteration 29, loss = 0.03060771
Iteration 30, loss = 0.03004480
Iteration 31, loss = 0.02928465
Iteration 32, loss = 0.02860841
Iteration 33, loss = 0.02809160
Iteration 34, loss = 0.02753972
Iteration 35, loss = 0.02717473
Iteration 36, loss = 0.02669519
Iteration 37, loss = 0.02623939
Iteration 38, loss = 0.02585558
Iteration 39, loss = 0.02549826
Iteration 40, loss = 0.02526661
Iteration 41, loss = 0.02509403
Iteration 42, loss = 0.02468807
Iteration 43, loss = 0.02457790
Iteration 44, loss = 0.02413184
Iteration 45, loss = 0.02413577
Iteration 46, loss = 0.02395324
Iteration 47, loss = 0.02353890
Iteration 48, loss = 0.02356395
Iteration 49, loss = 0.02332776
Iteration 50, loss = 0.02319804
Iteration 51, loss = 0.02311240
Iteration 52, loss = 0.02287126
Iteration 53, loss = 0.02291230
Iteration 54, loss = 0.02294964
Iteration 55, loss = 0.02260186
Iteration 56, loss = 0.02247495
Iteration 57, loss = 0.02233910
Iteration 58, loss = 0.02215240
Iteration 59, loss = 0.02231271
Iteration 60, loss = 0.02204699
Iteration 61, loss = 0.02185970
Iteration 62, loss = 0.02198129
Iteration 63, loss = 0.02176248
Iteration 64, loss = 0.02174020
Iteration 65, loss = 0.02188578
Iteration 66, loss = 0.02157982
Iteration 67, loss = 0.02150662
Iteration 68, loss = 0.02159921
Iteration 69, loss = 0.02135330
Iteration 70, loss = 0.02136665
Iteration 71, loss = 0.02126505
Iteration 72, loss = 0.02132006
Iteration 73, loss = 0.02134892
Iteration 74, loss = 0.02135693
Iteration 75, loss = 0.02119752
Iteration 76, loss = 0.02104525
Iteration 77, loss = 0.02114560
Iteration 78, loss = 0.02117464
Iteration 79, loss = 0.02100588
Iteration 80, loss = 0.02088496
Iteration 81, loss = 0.02104826
Iteration 82, loss = 0.02077579
Iteration 83, loss = 0.02076294
Iteration 84, loss = 0.02094184
Iteration 85, loss = 0.02097126
Iteration 86, loss = 0.02063414
Iteration 87, loss = 0.02092970
Iteration 88, loss = 0.02067468
Iteration 89, loss = 0.02075683
Iteration 90, loss = 0.02080383
Iteration 91, loss = 0.02052709
Iteration 92, loss = 0.02073160
Iteration 93, loss = 0.02050577
Iteration 94, loss = 0.02037280
Iteration 95, loss = 0.02073041
Iteration 96, loss = 0.02048413
Iteration 97, loss = 0.02026135
Iteration 98, loss = 0.02268626
Iteration 99, loss = 0.02349017
Iteration 100, loss = 0.02283233
Iteration 101, loss = 0.02227177
Iteration 102, loss = 0.02172694
Iteration 103, loss = 0.02154996
Iteration 104, loss = 0.02113174
Iteration 105, loss = 0.02118664
Iteration 106, loss = 0.02034269
Iteration 107, loss = 0.02014597
Iteration 108, loss = 0.02031309
Iteration 109, loss = 0.02017094
Iteration 110, loss = 0.02027978
Iteration 111, loss = 0.02024376
Iteration 112, loss = 0.01995831
Iteration 113, loss = 0.02000835
Iteration 114, loss = 0.01993389
Iteration 115, loss = 0.02011011
Iteration 116, loss = 0.02000387
Iteration 117, loss = 0.02013867
Iteration 118, loss = 0.02000867
Iteration 119, loss = 0.02001961
Iteration 120, loss = 0.02005193
Iteration 121, loss = 0.01995132
Iteration 122, loss = 0.01999413
Iteration 123, loss = 0.02011217
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68352369
Iteration 2, loss = 0.65499479
Iteration 3, loss = 0.61814731
Iteration 4, loss = 0.56676076
Iteration 5, loss = 0.50380692
Iteration 6, loss = 0.43950005
Iteration 7, loss = 0.37912556
Iteration 8, loss = 0.32416138
Iteration 9, loss = 0.27624589
Iteration 10, loss = 0.23504564
Iteration 11, loss = 0.20069743
Iteration 12, loss = 0.17221167
Iteration 13, loss = 0.14898514
Iteration 14, loss = 0.12981486
Iteration 15, loss = 0.11411619
Iteration 16, loss = 0.10123790
Iteration 17, loss = 0.09035245
Iteration 18, loss = 0.08152020
Iteration 19, loss = 0.07400858
Iteration 20, loss = 0.06753441
Iteration 21, loss = 0.06201744
Iteration 22, loss = 0.05721202
Iteration 23, loss = 0.05323439
Iteration 24, loss = 0.04972167
Iteration 25, loss = 0.04666400
Iteration 26, loss = 0.04394450
Iteration 27, loss = 0.04143869
Iteration 28, loss = 0.03933523
Iteration 29, loss = 0.03737875
Iteration 30, loss = 0.03564790
Iteration 31, loss = 0.03413959
Iteration 32, loss = 0.03284782
Iteration 33, loss = 0.03156690
Iteration 34, loss = 0.03037840
Iteration 35, loss = 0.02929906
Iteration 36, loss = 0.02832178
Iteration 37, loss = 0.02768239
Iteration 38, loss = 0.02667199
Iteration 39, loss = 0.02601075
Iteration 40, loss = 0.02541468
Iteration 41, loss = 0.02465388
Iteration 42, loss = 0.02412079
Iteration 43, loss = 0.02353941
Iteration 44, loss = 0.02307720
Iteration 45, loss = 0.02256023
Iteration 46, loss = 0.02222240
Iteration 47, loss = 0.02183248
Iteration 48, loss = 0.02141495
Iteration 49, loss = 0.02100009
Iteration 50, loss = 0.02064702
Iteration 51, loss = 0.02031638
Iteration 52, loss = 0.02005687
Iteration 53, loss = 0.01985445
Iteration 54, loss = 0.01950694
Iteration 55, loss = 0.01923394
Iteration 56, loss = 0.01922709
Iteration 57, loss = 0.01884670
Iteration 58, loss = 0.01879879
Iteration 59, loss = 0.01850309
Iteration 60, loss = 0.01828911
Iteration 61, loss = 0.01818318
Iteration 62, loss = 0.01786476
Iteration 63, loss = 0.01787537
Iteration 64, loss = 0.01784271
Iteration 65, loss = 0.01750555
Iteration 66, loss = 0.01746900
Iteration 67, loss = 0.01719580
Iteration 68, loss = 0.01707237
Iteration 69, loss = 0.01712338
Iteration 70, loss = 0.01695395
Iteration 71, loss = 0.01673169
Iteration 72, loss = 0.01679803
Iteration 73, loss = 0.01669254
Iteration 74, loss = 0.01654275
Iteration 75, loss = 0.01639672
Iteration 76, loss = 0.01632457
Iteration 77, loss = 0.01618169
Iteration 78, loss = 0.01620032
Iteration 79, loss = 0.01599194
Iteration 80, loss = 0.01582665
Iteration 81, loss = 0.01610386
Iteration 82, loss = 0.01604051
Iteration 83, loss = 0.01578090
Iteration 84, loss = 0.01587526
Iteration 85, loss = 0.01582646
Iteration 86, loss = 0.01557822
Iteration 87, loss = 0.01550857
Iteration 88, loss = 0.01547293
Iteration 89, loss = 0.01542751
Iteration 90, loss = 0.01539547
Iteration 91, loss = 0.01540693
Iteration 92, loss = 0.01532275
Iteration 93, loss = 0.01535116
Iteration 94, loss = 0.01516218
Iteration 95, loss = 0.01521105
Iteration 96, loss = 0.01514618
Iteration 97, loss = 0.01510449
Iteration 98, loss = 0.01507022
Iteration 99, loss = 0.01489345
Iteration 100, loss = 0.01495488
Iteration 101, loss = 0.01493371
Iteration 102, loss = 0.01498790
Iteration 103, loss = 0.01491538
Iteration 104, loss = 0.01493682
Iteration 105, loss = 0.01497406
Iteration 106, loss = 0.01492500
Iteration 107, loss = 0.01499927
Iteration 108, loss = 0.01483934
Iteration 109, loss = 0.01474916
Iteration 110, loss = 0.01469394
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67477591
Iteration 2, loss = 0.60395881
Iteration 3, loss = 0.49590962
Iteration 4, loss = 0.38440299
Iteration 5, loss = 0.28852512
Iteration 6, loss = 0.21614254
Iteration 7, loss = 0.16518851
Iteration 8, loss = 0.13006300
Iteration 9, loss = 0.10572066
Iteration 10, loss = 0.08839548
Iteration 11, loss = 0.07598693
Iteration 12, loss = 0.06649705
Iteration 13, loss = 0.05945924
Iteration 14, loss = 0.05362170
Iteration 15, loss = 0.04911500
Iteration 16, loss = 0.04549265
Iteration 17, loss = 0.04241889
Iteration 18, loss = 0.03980901
Iteration 19, loss = 0.03762951
Iteration 20, loss = 0.03607910
Iteration 21, loss = 0.03434939
Iteration 22, loss = 0.03330628
Iteration 23, loss = 0.03203353
Iteration 24, loss = 0.03099411
Iteration 25, loss = 0.03009357
Iteration 26, loss = 0.02946302
Iteration 27, loss = 0.02860917
Iteration 28, loss = 0.02821110
Iteration 29, loss = 0.02748532
Iteration 30, loss = 0.02701905
Iteration 31, loss = 0.02658235
Iteration 32, loss = 0.02638531
Iteration 33, loss = 0.02588055
Iteration 34, loss = 0.02538935
Iteration 35, loss = 0.02517816
Iteration 36, loss = 0.02519038
Iteration 37, loss = 0.02475835
Iteration 38, loss = 0.02451275
Iteration 39, loss = 0.02432174
Iteration 40, loss = 0.02391627
Iteration 41, loss = 0.02390441
Iteration 42, loss = 0.02354420
Iteration 43, loss = 0.02363237
Iteration 44, loss = 0.02325788
Iteration 45, loss = 0.02344262
Iteration 46, loss = 0.02307466
Iteration 47, loss = 0.02295579
Iteration 48, loss = 0.02309372
Iteration 49, loss = 0.02285291
Iteration 50, loss = 0.02281832
Iteration 51, loss = 0.02246765
Iteration 52, loss = 0.02285343
Iteration 53, loss = 0.02279389
Iteration 54, loss = 0.02241643
Iteration 55, loss = 0.02227837
Iteration 56, loss = 0.02278133
Iteration 57, loss = 0.02251350
Iteration 58, loss = 0.02199159
Iteration 59, loss = 0.02227880
Iteration 60, loss = 0.02202912
Iteration 61, loss = 0.02207654
Iteration 62, loss = 0.02188197
Iteration 63, loss = 0.02200895
Iteration 64, loss = 0.02194762
Iteration 65, loss = 0.02180188
Iteration 66, loss = 0.02197531
Iteration 67, loss = 0.02180008
Iteration 68, loss = 0.02161838
Iteration 69, loss = 0.02168470
Iteration 70, loss = 0.02165626
Iteration 71, loss = 0.02165505
Iteration 72, loss = 0.02176511
Iteration 73, loss = 0.02178057
Iteration 74, loss = 0.02167919
Iteration 75, loss = 0.02160705
Iteration 76, loss = 0.02182777
Iteration 77, loss = 0.02128800
Iteration 78, loss = 0.02139948
Iteration 79, loss = 0.02120168
Iteration 80, loss = 0.02122870
Iteration 81, loss = 0.02134790
Iteration 82, loss = 0.02152746
Iteration 83, loss = 0.02116170
Iteration 84, loss = 0.02118804
Iteration 85, loss = 0.02120980
Iteration 86, loss = 0.02129564
Iteration 87, loss = 0.02117921
Iteration 88, loss = 0.02102484
Iteration 89, loss = 0.02116842
Iteration 90, loss = 0.02096244
Iteration 91, loss = 0.02093566
Iteration 92, loss = 0.02089889
Iteration 93, loss = 0.02109582
Iteration 94, loss = 0.02122030
Iteration 95, loss = 0.02115084
Iteration 96, loss = 0.02105871
Iteration 97, loss = 0.02130056
Iteration 98, loss = 0.02100226
Iteration 99, loss = 0.02118366
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68830790
Iteration 2, loss = 0.66687248
Iteration 3, loss = 0.64661509
Iteration 4, loss = 0.62344597
Iteration 5, loss = 0.59699226
Iteration 6, loss = 0.56714589
Iteration 7, loss = 0.53462255
Iteration 8, loss = 0.50018223
Iteration 9, loss = 0.46449698
Iteration 10, loss = 0.42918951
Iteration 11, loss = 0.39450250
Iteration 12, loss = 0.36111233
Iteration 13, loss = 0.32996028
Iteration 14, loss = 0.30069240
Iteration 15, loss = 0.27378649
Iteration 16, loss = 0.24915228
Iteration 17, loss = 0.22675848
Iteration 18, loss = 0.20652606
Iteration 19, loss = 0.18836510
Iteration 20, loss = 0.17205485
Iteration 21, loss = 0.15750866
Iteration 22, loss = 0.14453541
Iteration 23, loss = 0.13298943
Iteration 24, loss = 0.12260967
Iteration 25, loss = 0.11341845
Iteration 26, loss = 0.10516132
Iteration 27, loss = 0.09781751
Iteration 28, loss = 0.09115624
Iteration 29, loss = 0.08522930
Iteration 30, loss = 0.07986232
Iteration 31, loss = 0.07503519
Iteration 32, loss = 0.07067981
Iteration 33, loss = 0.06670765
Iteration 34, loss = 0.06305637
Iteration 35, loss = 0.05983131
Iteration 36, loss = 0.05685020
Iteration 37, loss = 0.05412212
Iteration 38, loss = 0.05156829
Iteration 39, loss = 0.04927894
Iteration 40, loss = 0.04715578
Iteration 41, loss = 0.04518233
Iteration 42, loss = 0.04334928
Iteration 43, loss = 0.04170012
Iteration 44, loss = 0.04012098
Iteration 45, loss = 0.03867961
Iteration 46, loss = 0.03733522
Iteration 47, loss = 0.03611555
Iteration 48, loss = 0.03487530
Iteration 49, loss = 0.03384866
Iteration 50, loss = 0.03279848
Iteration 51, loss = 0.03181356
Iteration 52, loss = 0.03092569
Iteration 53, loss = 0.03009202
Iteration 54, loss = 0.02930764
Iteration 55, loss = 0.02848076
Iteration 56, loss = 0.02779443
Iteration 57, loss = 0.02713392
Iteration 58, loss = 0.02650178
Iteration 59, loss = 0.02590363
Iteration 60, loss = 0.02539678
Iteration 61, loss = 0.02476638
Iteration 62, loss = 0.02433148
Iteration 63, loss = 0.02383774
Iteration 64, loss = 0.02330656
Iteration 65, loss = 0.02287762
Iteration 66, loss = 0.02248856
Iteration 67, loss = 0.02208263
Iteration 68, loss = 0.02170516
Iteration 69, loss = 0.02136573
Iteration 70, loss = 0.02104597
Iteration 71, loss = 0.02065614
Iteration 72, loss = 0.02039521
Iteration 73, loss = 0.02008390
Iteration 74, loss = 0.01978571
Iteration 75, loss = 0.01950919
Iteration 76, loss = 0.01924518
Iteration 77, loss = 0.01902087
Iteration 78, loss = 0.01872252
Iteration 79, loss = 0.01851682
Iteration 80, loss = 0.01827285
Iteration 81, loss = 0.01807947
Iteration 82, loss = 0.01789989
Iteration 83, loss = 0.01769043
Iteration 84, loss = 0.01744925
Iteration 85, loss = 0.01730794
Iteration 86, loss = 0.01710682
Iteration 87, loss = 0.01699348
Iteration 88, loss = 0.01673893
Iteration 89, loss = 0.01656335
Iteration 90, loss = 0.01646174
Iteration 91, loss = 0.01629226
Iteration 92, loss = 0.01618097
Iteration 93, loss = 0.01601979
Iteration 94, loss = 0.01588337
Iteration 95, loss = 0.01585271
Iteration 96, loss = 0.01562691
Iteration 97, loss = 0.01551164
Iteration 98, loss = 0.01540342
Iteration 99, loss = 0.01526762
Iteration 100, loss = 0.01519487
Iteration 101, loss = 0.01504113
Iteration 102, loss = 0.01496670
Iteration 103, loss = 0.01487497
Iteration 104, loss = 0.01481207
Iteration 105, loss = 0.01470090
Iteration 106, loss = 0.01457532
Iteration 107, loss = 0.01450649
Iteration 108, loss = 0.01438705
Iteration 109, loss = 0.01435218
Iteration 110, loss = 0.01424426
Iteration 111, loss = 0.01418007
Iteration 112, loss = 0.01413177
Iteration 113, loss = 0.01406326
Iteration 114, loss = 0.01395647
Iteration 115, loss = 0.01386275
Iteration 116, loss = 0.01382773
Iteration 117, loss = 0.01372100
Iteration 118, loss = 0.01366202
Iteration 119, loss = 0.01360654
Iteration 120, loss = 0.01354499
Iteration 121, loss = 0.01352899
Iteration 122, loss = 0.01345111
Iteration 123, loss = 0.01339511
Iteration 124, loss = 0.01331558
Iteration 125, loss = 0.01324532
Iteration 126, loss = 0.01321169
Iteration 127, loss = 0.01314799
Iteration 128, loss = 0.01312256
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67765068
Iteration 2, loss = 0.63817904
Iteration 3, loss = 0.58133454
Iteration 4, loss = 0.51119925
Iteration 5, loss = 0.43859577
Iteration 6, loss = 0.37018900
Iteration 7, loss = 0.30870850
Iteration 8, loss = 0.25661998
Iteration 9, loss = 0.21370679
Iteration 10, loss = 0.17891657
Iteration 11, loss = 0.15128517
Iteration 12, loss = 0.12929214
Iteration 13, loss = 0.11185539
Iteration 14, loss = 0.09781935
Iteration 15, loss = 0.08639351
Iteration 16, loss = 0.07702780
Iteration 17, loss = 0.06939309
Iteration 18, loss = 0.06299574
Iteration 19, loss = 0.05759554
Iteration 20, loss = 0.05308535
Iteration 21, loss = 0.04924757
Iteration 22, loss = 0.04591956
Iteration 23, loss = 0.04287189
Iteration 24, loss = 0.04041429
Iteration 25, loss = 0.03810550
Iteration 26, loss = 0.03629604
Iteration 27, loss = 0.03442572
Iteration 28, loss = 0.03293934
Iteration 29, loss = 0.03165542
Iteration 30, loss = 0.03039358
Iteration 31, loss = 0.02925171
Iteration 32, loss = 0.02819782
Iteration 33, loss = 0.02732094
Iteration 34, loss = 0.02645901
Iteration 35, loss = 0.02577016
Iteration 36, loss = 0.02506443
Iteration 37, loss = 0.02453864
Iteration 38, loss = 0.02384122
Iteration 39, loss = 0.02337338
Iteration 40, loss = 0.02301841
Iteration 41, loss = 0.02244015
Iteration 42, loss = 0.02209850
Iteration 43, loss = 0.02144645
Iteration 44, loss = 0.02116564
Iteration 45, loss = 0.02073369
Iteration 46, loss = 0.02055576
Iteration 47, loss = 0.02020611
Iteration 48, loss = 0.02002439
Iteration 49, loss = 0.01973060
Iteration 50, loss = 0.01953123
Iteration 51, loss = 0.01917612
Iteration 52, loss = 0.01904493
Iteration 53, loss = 0.01892859
Iteration 54, loss = 0.01872790
Iteration 55, loss = 0.01849376
Iteration 56, loss = 0.01828576
Iteration 57, loss = 0.01807024
Iteration 58, loss = 0.01802358
Iteration 59, loss = 0.01789527
Iteration 60, loss = 0.01774737
Iteration 61, loss = 0.01762291
Iteration 62, loss = 0.01747471
Iteration 63, loss = 0.01737863
Iteration 64, loss = 0.01732654
Iteration 65, loss = 0.01702907
Iteration 66, loss = 0.01701935
Iteration 67, loss = 0.01689653
Iteration 68, loss = 0.01680262
Iteration 69, loss = 0.01674438
Iteration 70, loss = 0.01683097
Iteration 71, loss = 0.01651205
Iteration 72, loss = 0.01668312
Iteration 73, loss = 0.01649096
Iteration 74, loss = 0.01637120
Iteration 75, loss = 0.01635169
Iteration 76, loss = 0.01627411
Iteration 77, loss = 0.01606436
Iteration 78, loss = 0.01610158
Iteration 79, loss = 0.01607386
Iteration 80, loss = 0.01596185
Iteration 81, loss = 0.01599947
Iteration 82, loss = 0.01602276
Iteration 83, loss = 0.01598316
Iteration 84, loss = 0.01587839
Iteration 85, loss = 0.01597203
Iteration 86, loss = 0.01569410
Iteration 87, loss = 0.01570146
Iteration 88, loss = 0.01552304
Iteration 89, loss = 0.01574241
Iteration 90, loss = 0.01566594
Iteration 91, loss = 0.01547339
Iteration 92, loss = 0.01549708
Iteration 93, loss = 0.01551554
Iteration 94, loss = 0.01554966
Iteration 95, loss = 0.01546451
Iteration 96, loss = 0.01539107
Iteration 97, loss = 0.01540353
Iteration 98, loss = 0.01532711
Iteration 99, loss = 0.01536231
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70742019
Iteration 2, loss = 0.63817135
Iteration 3, loss = 0.53571351
Iteration 4, loss = 0.41551568
Iteration 5, loss = 0.31024099
Iteration 6, loss = 0.23053249
Iteration 7, loss = 0.17476828
Iteration 8, loss = 0.13653849
Iteration 9, loss = 0.11046944
Iteration 10, loss = 0.09203496
Iteration 11, loss = 0.07871528
Iteration 12, loss = 0.06863694
Iteration 13, loss = 0.06116883
Iteration 14, loss = 0.05515474
Iteration 15, loss = 0.05060539
Iteration 16, loss = 0.04680337
Iteration 17, loss = 0.04341390
Iteration 18, loss = 0.04120542
Iteration 19, loss = 0.03882468
Iteration 20, loss = 0.03671018
Iteration 21, loss = 0.03519886
Iteration 22, loss = 0.03363985
Iteration 23, loss = 0.03245165
Iteration 24, loss = 0.03142829
Iteration 25, loss = 0.03046563
Iteration 26, loss = 0.02990723
Iteration 27, loss = 0.02905846
Iteration 28, loss = 0.02830968
Iteration 29, loss = 0.02786155
Iteration 30, loss = 0.02723516
Iteration 31, loss = 0.02704796
Iteration 32, loss = 0.02662456
Iteration 33, loss = 0.02609033
Iteration 34, loss = 0.02590717
Iteration 35, loss = 0.02566594
Iteration 36, loss = 0.02529418
Iteration 37, loss = 0.02511463
Iteration 38, loss = 0.02474661
Iteration 39, loss = 0.02474969
Iteration 40, loss = 0.02434375
Iteration 41, loss = 0.02426619
Iteration 42, loss = 0.02395229
Iteration 43, loss = 0.02392736
Iteration 44, loss = 0.02367213
Iteration 45, loss = 0.02367999
Iteration 46, loss = 0.02370289
Iteration 47, loss = 0.02339865
Iteration 48, loss = 0.02336422
Iteration 49, loss = 0.02287204
Iteration 50, loss = 0.02292281
Iteration 51, loss = 0.02284925
Iteration 52, loss = 0.02274353
Iteration 53, loss = 0.02275854
Iteration 54, loss = 0.02253172
Iteration 55, loss = 0.02265789
Iteration 56, loss = 0.02278014
Iteration 57, loss = 0.02241093
Iteration 58, loss = 0.02244742
Iteration 59, loss = 0.02256534
Iteration 60, loss = 0.02235781
Iteration 61, loss = 0.02241477
Iteration 62, loss = 0.02218374
Iteration 63, loss = 0.02208741
Iteration 64, loss = 0.02213047
Iteration 65, loss = 0.02223995
Iteration 66, loss = 0.02211141
Iteration 67, loss = 0.02199404
Iteration 68, loss = 0.02210866
Iteration 69, loss = 0.02177329
Iteration 70, loss = 0.02182300
Iteration 71, loss = 0.02181666
Iteration 72, loss = 0.02180710
Iteration 73, loss = 0.02165033
Iteration 74, loss = 0.02160620
Iteration 75, loss = 0.02161053
Iteration 76, loss = 0.02151535
Iteration 77, loss = 0.02176669
Iteration 78, loss = 0.02153015
Iteration 79, loss = 0.02139708
Iteration 80, loss = 0.02156959
Iteration 81, loss = 0.02151903
Iteration 82, loss = 0.02185814
Iteration 83, loss = 0.02124329
Iteration 84, loss = 0.02186601
Iteration 85, loss = 0.02149412
Iteration 86, loss = 0.02152014
Iteration 87, loss = 0.02150396
Iteration 88, loss = 0.02160965
Iteration 89, loss = 0.02132846
Iteration 90, loss = 0.02134647
Iteration 91, loss = 0.02157107
Iteration 92, loss = 0.02142733
Iteration 93, loss = 0.02146790
Iteration 94, loss = 0.02137688
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	79m34.918s
user	345m17.690s
sys	35m30.945s
Elapsed time: 4775 seconds
