Processing dataset: prepended_v5_lemma_stem

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: prepended_v5_lemma_stem (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on prepended_v5_lemma_stem...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on prepended_v5_lemma_stem...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on prepended_v5_lemma_stem...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on prepended_v5_lemma_stem...
/opt/ohpc/pub/spack/v0.21.1/opt/spack/linux-rocky8-x86_64_v3/gcc-8.5.0/py-joblib-1.2.0-g6vh2usiiivwucfr57aoxwqvljmoxokc/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on prepended_v5_lemma_stem...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67287176
Iteration 2, loss = 0.59610111
Iteration 3, loss = 0.48177085
Iteration 4, loss = 0.37022378
Iteration 5, loss = 0.27766838
Iteration 6, loss = 0.20897817
Iteration 7, loss = 0.16086247
Iteration 8, loss = 0.12745660
Iteration 9, loss = 0.10460342
Iteration 10, loss = 0.08793621
Iteration 11, loss = 0.07605324
Iteration 12, loss = 0.06645502
Iteration 13, loss = 0.05974202
Iteration 14, loss = 0.05412942
Iteration 15, loss = 0.04954199
Iteration 16, loss = 0.04618917
Iteration 17, loss = 0.04328136
Iteration 18, loss = 0.04087233
Iteration 19, loss = 0.03901984
Iteration 20, loss = 0.03692484
Iteration 21, loss = 0.03580736
Iteration 22, loss = 0.03398697
Iteration 23, loss = 0.03326107
Iteration 24, loss = 0.03217610
Iteration 25, loss = 0.03162638
Iteration 26, loss = 0.03061756
Iteration 27, loss = 0.03000347
Iteration 28, loss = 0.02949553
Iteration 29, loss = 0.02897094
Iteration 30, loss = 0.02847291
Iteration 31, loss = 0.02813693
Iteration 32, loss = 0.02781042
Iteration 33, loss = 0.02754784
Iteration 34, loss = 0.02712562
Iteration 35, loss = 0.02688137
Iteration 36, loss = 0.02656597
Iteration 37, loss = 0.02632722
Iteration 38, loss = 0.02628005
Iteration 39, loss = 0.02597522
Iteration 40, loss = 0.02583588
Iteration 41, loss = 0.02559115
Iteration 42, loss = 0.02543807
Iteration 43, loss = 0.02549591
Iteration 44, loss = 0.02505550
Iteration 45, loss = 0.02511588
Iteration 46, loss = 0.02495579
Iteration 47, loss = 0.02537953
Iteration 48, loss = 0.02491128
Iteration 49, loss = 0.02464082
Iteration 50, loss = 0.02454828
Iteration 51, loss = 0.02471316
Iteration 52, loss = 0.02449041
Iteration 53, loss = 0.02433097
Iteration 54, loss = 0.02432380
Iteration 55, loss = 0.02417101
Iteration 56, loss = 0.02435942
Iteration 57, loss = 0.02401266
Iteration 58, loss = 0.02422787
Iteration 59, loss = 0.02415633
Iteration 60, loss = 0.02395048
Iteration 61, loss = 0.02390586
Iteration 62, loss = 0.02372482
Iteration 63, loss = 0.02390247
Iteration 64, loss = 0.02384008
Iteration 65, loss = 0.02369397
Iteration 66, loss = 0.02395469
Iteration 67, loss = 0.02364133
Iteration 68, loss = 0.02374366
Iteration 69, loss = 0.02387494
Iteration 70, loss = 0.02345643
Iteration 71, loss = 0.02366593
Iteration 72, loss = 0.02363844
Iteration 73, loss = 0.02343314
Iteration 74, loss = 0.02329614
Iteration 75, loss = 0.02334323
Iteration 76, loss = 0.02372816
Iteration 77, loss = 0.02347204
Iteration 78, loss = 0.02333935
Iteration 79, loss = 0.02336074
Iteration 80, loss = 0.02331709
Iteration 81, loss = 0.02353573
Iteration 82, loss = 0.02330480
Iteration 83, loss = 0.02314311
Iteration 84, loss = 0.02333259
Iteration 85, loss = 0.02305007
Iteration 86, loss = 0.02304150
Iteration 87, loss = 0.02309551
Iteration 88, loss = 0.02350633
Iteration 89, loss = 0.02313548
Iteration 90, loss = 0.02316139
Iteration 91, loss = 0.02314237
Iteration 92, loss = 0.02324957
Iteration 93, loss = 0.02299329
Iteration 94, loss = 0.02320407
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67194956
Iteration 2, loss = 0.59484996
Iteration 3, loss = 0.48275065
Iteration 4, loss = 0.37111608
Iteration 5, loss = 0.27757871
Iteration 6, loss = 0.20854142
Iteration 7, loss = 0.15983290
Iteration 8, loss = 0.12678692
Iteration 9, loss = 0.10377606
Iteration 10, loss = 0.08744543
Iteration 11, loss = 0.07556255
Iteration 12, loss = 0.06631286
Iteration 13, loss = 0.05941997
Iteration 14, loss = 0.05413378
Iteration 15, loss = 0.04989886
Iteration 16, loss = 0.04672813
Iteration 17, loss = 0.04352795
Iteration 18, loss = 0.04135086
Iteration 19, loss = 0.03941938
Iteration 20, loss = 0.03751717
Iteration 21, loss = 0.03622436
Iteration 22, loss = 0.03477084
Iteration 23, loss = 0.03394484
Iteration 24, loss = 0.03299665
Iteration 25, loss = 0.03230434
Iteration 26, loss = 0.03147362
Iteration 27, loss = 0.03085588
Iteration 28, loss = 0.03055679
Iteration 29, loss = 0.03005981
Iteration 30, loss = 0.02922371
Iteration 31, loss = 0.02900438
Iteration 32, loss = 0.02852791
Iteration 33, loss = 0.02830918
Iteration 34, loss = 0.02792133
Iteration 35, loss = 0.02761719
Iteration 36, loss = 0.02745406
Iteration 37, loss = 0.02703891
Iteration 38, loss = 0.02708867
Iteration 39, loss = 0.02681824
Iteration 40, loss = 0.02691378
Iteration 41, loss = 0.02633450
Iteration 42, loss = 0.02641261
Iteration 43, loss = 0.02595661
Iteration 44, loss = 0.02604781
Iteration 45, loss = 0.02583031
Iteration 46, loss = 0.02587484
Iteration 47, loss = 0.02579296
Iteration 48, loss = 0.02574832
Iteration 49, loss = 0.02515722
Iteration 50, loss = 0.02547119
Iteration 51, loss = 0.02523456
Iteration 52, loss = 0.02537481
Iteration 53, loss = 0.02519981
Iteration 54, loss = 0.02500391
Iteration 55, loss = 0.02515169
Iteration 56, loss = 0.02487508
Iteration 57, loss = 0.02536249
Iteration 58, loss = 0.02465680
Iteration 59, loss = 0.02514178
Iteration 60, loss = 0.02444818
Iteration 61, loss = 0.02478286
Iteration 62, loss = 0.02480711
Iteration 63, loss = 0.02484020
Iteration 64, loss = 0.02440370
Iteration 65, loss = 0.02444134
Iteration 66, loss = 0.02449861
Iteration 67, loss = 0.02474873
Iteration 68, loss = 0.02437589
Iteration 69, loss = 0.02429083
Iteration 70, loss = 0.02468121
Iteration 71, loss = 0.02451112
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69736184
Iteration 2, loss = 0.62023891
Iteration 3, loss = 0.51309369
Iteration 4, loss = 0.39663819
Iteration 5, loss = 0.29610458
Iteration 6, loss = 0.21983705
Iteration 7, loss = 0.16684823
Iteration 8, loss = 0.13055803
Iteration 9, loss = 0.10570965
Iteration 10, loss = 0.08789032
Iteration 11, loss = 0.07492790
Iteration 12, loss = 0.06564077
Iteration 13, loss = 0.05841473
Iteration 14, loss = 0.05267053
Iteration 15, loss = 0.04803444
Iteration 16, loss = 0.04464327
Iteration 17, loss = 0.04144774
Iteration 18, loss = 0.03911346
Iteration 19, loss = 0.03703503
Iteration 20, loss = 0.03533545
Iteration 21, loss = 0.03379443
Iteration 22, loss = 0.03287136
Iteration 23, loss = 0.03157879
Iteration 24, loss = 0.03068062
Iteration 25, loss = 0.02959491
Iteration 26, loss = 0.02922484
Iteration 27, loss = 0.02841451
Iteration 28, loss = 0.02753315
Iteration 29, loss = 0.02715492
Iteration 30, loss = 0.02682651
Iteration 31, loss = 0.02643924
Iteration 32, loss = 0.02614066
Iteration 33, loss = 0.02553912
Iteration 34, loss = 0.02548447
Iteration 35, loss = 0.02504913
Iteration 36, loss = 0.02499359
Iteration 37, loss = 0.02449144
Iteration 38, loss = 0.02464163
Iteration 39, loss = 0.02422801
Iteration 40, loss = 0.02412287
Iteration 41, loss = 0.02371900
Iteration 42, loss = 0.02350625
Iteration 43, loss = 0.02371312
Iteration 44, loss = 0.02352934
Iteration 45, loss = 0.02339602
Iteration 46, loss = 0.02306117
Iteration 47, loss = 0.02301509
Iteration 48, loss = 0.02301722
Iteration 49, loss = 0.02273574
Iteration 50, loss = 0.02281373
Iteration 51, loss = 0.02274578
Iteration 52, loss = 0.02279596
Iteration 53, loss = 0.02237808
Iteration 54, loss = 0.02260638
Iteration 55, loss = 0.02230502
Iteration 56, loss = 0.02237818
Iteration 57, loss = 0.02204563
Iteration 58, loss = 0.02240686
Iteration 59, loss = 0.02210337
Iteration 60, loss = 0.02203574
Iteration 61, loss = 0.02194931
Iteration 62, loss = 0.02219852
Iteration 63, loss = 0.02186730
Iteration 64, loss = 0.02202241
Iteration 65, loss = 0.02211994
Iteration 66, loss = 0.02230019
Iteration 67, loss = 0.02175033
Iteration 68, loss = 0.02209589
Iteration 69, loss = 0.02181980
Iteration 70, loss = 0.02165544
Iteration 71, loss = 0.02173438
Iteration 72, loss = 0.02164700
Iteration 73, loss = 0.02183594
Iteration 74, loss = 0.02170225
Iteration 75, loss = 0.02161001
Iteration 76, loss = 0.02164936
Iteration 77, loss = 0.02149378
Iteration 78, loss = 0.02171387
Iteration 79, loss = 0.02139961
Iteration 80, loss = 0.02167684
Iteration 81, loss = 0.02147262
Iteration 82, loss = 0.02129144
Iteration 83, loss = 0.02115932
Iteration 84, loss = 0.02135902
Iteration 85, loss = 0.02131397
Iteration 86, loss = 0.02144107
Iteration 87, loss = 0.02144097
Iteration 88, loss = 0.02121055
Iteration 89, loss = 0.02136832
Iteration 90, loss = 0.02125413
Iteration 91, loss = 0.02151441
Iteration 92, loss = 0.02110564
Iteration 93, loss = 0.02137851
Iteration 94, loss = 0.02139602
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70560521
Iteration 2, loss = 0.62670964
Iteration 3, loss = 0.51770586
Iteration 4, loss = 0.39578858
Iteration 5, loss = 0.29096984
Iteration 6, loss = 0.21442633
Iteration 7, loss = 0.16209013
Iteration 8, loss = 0.12671927
Iteration 9, loss = 0.10238131
Iteration 10, loss = 0.08537655
Iteration 11, loss = 0.07307543
Iteration 12, loss = 0.06395428
Iteration 13, loss = 0.05710170
Iteration 14, loss = 0.05156259
Iteration 15, loss = 0.04735269
Iteration 16, loss = 0.04388295
Iteration 17, loss = 0.04086798
Iteration 18, loss = 0.03830794
Iteration 19, loss = 0.03652033
Iteration 20, loss = 0.03471807
Iteration 21, loss = 0.03343539
Iteration 22, loss = 0.03200493
Iteration 23, loss = 0.03093026
Iteration 24, loss = 0.03001901
Iteration 25, loss = 0.02924300
Iteration 26, loss = 0.02865216
Iteration 27, loss = 0.02786730
Iteration 28, loss = 0.02751758
Iteration 29, loss = 0.02702088
Iteration 30, loss = 0.02674274
Iteration 31, loss = 0.02595096
Iteration 32, loss = 0.02559267
Iteration 33, loss = 0.02520841
Iteration 34, loss = 0.02513360
Iteration 35, loss = 0.02475168
Iteration 36, loss = 0.02454828
Iteration 37, loss = 0.02435105
Iteration 38, loss = 0.02393543
Iteration 39, loss = 0.02410684
Iteration 40, loss = 0.02371858
Iteration 41, loss = 0.02348541
Iteration 42, loss = 0.02351966
Iteration 43, loss = 0.02321362
Iteration 44, loss = 0.02311110
Iteration 45, loss = 0.02288600
Iteration 46, loss = 0.02262831
Iteration 47, loss = 0.02308519
Iteration 48, loss = 0.02246496
Iteration 49, loss = 0.02253370
Iteration 50, loss = 0.02255150
Iteration 51, loss = 0.02231269
Iteration 52, loss = 0.02223944
Iteration 53, loss = 0.02257945
Iteration 54, loss = 0.02221663
Iteration 55, loss = 0.02200193
Iteration 56, loss = 0.02207990
Iteration 57, loss = 0.02184498
Iteration 58, loss = 0.02198926
Iteration 59, loss = 0.02190972
Iteration 60, loss = 0.02206475
Iteration 61, loss = 0.02177642
Iteration 62, loss = 0.02188479
Iteration 63, loss = 0.02185261
Iteration 64, loss = 0.02154141
Iteration 65, loss = 0.02165565
Iteration 66, loss = 0.02190534
Iteration 67, loss = 0.02148687
Iteration 68, loss = 0.02161877
Iteration 69, loss = 0.02150654
Iteration 70, loss = 0.02174437
Iteration 71, loss = 0.02138887
Iteration 72, loss = 0.02134508
Iteration 73, loss = 0.02153438
Iteration 74, loss = 0.02121335
Iteration 75, loss = 0.02147372
Iteration 76, loss = 0.02140064
Iteration 77, loss = 0.02107047
Iteration 78, loss = 0.02116319
Iteration 79, loss = 0.02128279
Iteration 80, loss = 0.02098956
Iteration 81, loss = 0.02107876
Iteration 82, loss = 0.02123685
Iteration 83, loss = 0.02121561
Iteration 84, loss = 0.02119379
Iteration 85, loss = 0.02122056
Iteration 86, loss = 0.02101726
Iteration 87, loss = 0.02108664
Iteration 88, loss = 0.02092971
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67624198
Iteration 2, loss = 0.59015489
Iteration 3, loss = 0.46846460
Iteration 4, loss = 0.35411115
Iteration 5, loss = 0.26275959
Iteration 6, loss = 0.19658036
Iteration 7, loss = 0.15096117
Iteration 8, loss = 0.11927122
Iteration 9, loss = 0.09725263
Iteration 10, loss = 0.08162878
Iteration 11, loss = 0.07045002
Iteration 12, loss = 0.06160529
Iteration 13, loss = 0.05547647
Iteration 14, loss = 0.04985448
Iteration 15, loss = 0.04581094
Iteration 16, loss = 0.04246242
Iteration 17, loss = 0.03956756
Iteration 18, loss = 0.03762016
Iteration 19, loss = 0.03547598
Iteration 20, loss = 0.03390538
Iteration 21, loss = 0.03260929
Iteration 22, loss = 0.03154562
Iteration 23, loss = 0.03042648
Iteration 24, loss = 0.02971964
Iteration 25, loss = 0.02871043
Iteration 26, loss = 0.02810886
Iteration 27, loss = 0.02743126
Iteration 28, loss = 0.02678739
Iteration 29, loss = 0.02659676
Iteration 30, loss = 0.02603391
Iteration 31, loss = 0.02566371
Iteration 32, loss = 0.02534157
Iteration 33, loss = 0.02510179
Iteration 34, loss = 0.02470429
Iteration 35, loss = 0.02466944
Iteration 36, loss = 0.02418667
Iteration 37, loss = 0.02409411
Iteration 38, loss = 0.02379166
Iteration 39, loss = 0.02360508
Iteration 40, loss = 0.02343202
Iteration 41, loss = 0.02328025
Iteration 42, loss = 0.02307660
Iteration 43, loss = 0.02306970
Iteration 44, loss = 0.02276385
Iteration 45, loss = 0.02272726
Iteration 46, loss = 0.02258915
Iteration 47, loss = 0.02256966
Iteration 48, loss = 0.02270002
Iteration 49, loss = 0.02241299
Iteration 50, loss = 0.02211832
Iteration 51, loss = 0.02232023
Iteration 52, loss = 0.02228348
Iteration 53, loss = 0.02221638
Iteration 54, loss = 0.02188048
Iteration 55, loss = 0.02213817
Iteration 56, loss = 0.02219916
Iteration 57, loss = 0.02173365
Iteration 58, loss = 0.02178119
Iteration 59, loss = 0.02166874
Iteration 60, loss = 0.02181510
Iteration 61, loss = 0.02190704
Iteration 62, loss = 0.02223443
Iteration 63, loss = 0.02161787
Iteration 64, loss = 0.02141200
Iteration 65, loss = 0.02147866
Iteration 66, loss = 0.02139665
Iteration 67, loss = 0.02130304
Iteration 68, loss = 0.02176647
Iteration 69, loss = 0.02140827
Iteration 70, loss = 0.02139849
Iteration 71, loss = 0.02138120
Iteration 72, loss = 0.02152939
Iteration 73, loss = 0.02126572
Iteration 74, loss = 0.02147645
Iteration 75, loss = 0.02118512
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67287176
Iteration 2, loss = 0.59610111
Iteration 3, loss = 0.48177085
Iteration 4, loss = 0.37022378
Iteration 5, loss = 0.27766838
Iteration 6, loss = 0.20897817
Iteration 7, loss = 0.16086247
Iteration 8, loss = 0.12745660
Iteration 9, loss = 0.10460342
Iteration 10, loss = 0.08793621
Iteration 11, loss = 0.07605324
Iteration 12, loss = 0.06645502
Iteration 13, loss = 0.05974202
Iteration 14, loss = 0.05412942
Iteration 15, loss = 0.04954199
Iteration 16, loss = 0.04618917
Iteration 17, loss = 0.04328136
Iteration 18, loss = 0.04087233
Iteration 19, loss = 0.03901984
Iteration 20, loss = 0.03692484
Iteration 21, loss = 0.03580736
Iteration 22, loss = 0.03398697
Iteration 23, loss = 0.03326107
Iteration 24, loss = 0.03217610
Iteration 25, loss = 0.03162638
Iteration 26, loss = 0.03061756
Iteration 27, loss = 0.03000347
Iteration 28, loss = 0.02949553
Iteration 29, loss = 0.02897094
Iteration 30, loss = 0.02847291
Iteration 31, loss = 0.02813693
Iteration 32, loss = 0.02781042
Iteration 33, loss = 0.02754784
Iteration 34, loss = 0.02712562
Iteration 35, loss = 0.02688137
Iteration 36, loss = 0.02656597
Iteration 37, loss = 0.02632722
Iteration 38, loss = 0.02628005
Iteration 39, loss = 0.02597522
Iteration 40, loss = 0.02583588
Iteration 41, loss = 0.02559115
Iteration 42, loss = 0.02543807
Iteration 43, loss = 0.02549591
Iteration 44, loss = 0.02505550
Iteration 45, loss = 0.02511588
Iteration 46, loss = 0.02495579
Iteration 47, loss = 0.02537953
Iteration 48, loss = 0.02491128
Iteration 49, loss = 0.02464082
Iteration 50, loss = 0.02454828
Iteration 51, loss = 0.02471316
Iteration 52, loss = 0.02449041
Iteration 53, loss = 0.02433097
Iteration 54, loss = 0.02432380
Iteration 55, loss = 0.02417101
Iteration 56, loss = 0.02435942
Iteration 57, loss = 0.02401266
Iteration 58, loss = 0.02422787
Iteration 59, loss = 0.02415633
Iteration 60, loss = 0.02395048
Iteration 61, loss = 0.02390586
Iteration 62, loss = 0.02372482
Iteration 63, loss = 0.02390247
Iteration 64, loss = 0.02384008
Iteration 65, loss = 0.02369397
Iteration 66, loss = 0.02395469
Iteration 67, loss = 0.02364133
Iteration 68, loss = 0.02374366
Iteration 69, loss = 0.02387494
Iteration 70, loss = 0.02345643
Iteration 71, loss = 0.02366593
Iteration 72, loss = 0.02363844
Iteration 73, loss = 0.02343314
Iteration 74, loss = 0.02329614
Iteration 75, loss = 0.02334323
Iteration 76, loss = 0.02372816
Iteration 77, loss = 0.02347204
Iteration 78, loss = 0.02333935
Iteration 79, loss = 0.02336074
Iteration 80, loss = 0.02331709
Iteration 81, loss = 0.02353573
Iteration 82, loss = 0.02330480
Iteration 83, loss = 0.02314311
Iteration 84, loss = 0.02333259
Iteration 85, loss = 0.02305007
Iteration 86, loss = 0.02304150
Iteration 87, loss = 0.02309551
Iteration 88, loss = 0.02350633
Iteration 89, loss = 0.02313548
Iteration 90, loss = 0.02316139
Iteration 91, loss = 0.02314237
Iteration 92, loss = 0.02324957
Iteration 93, loss = 0.02299329
Iteration 94, loss = 0.02320407
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67194956
Iteration 2, loss = 0.59484996
Iteration 3, loss = 0.48275065
Iteration 4, loss = 0.37111608
Iteration 5, loss = 0.27757871
Iteration 6, loss = 0.20854142
Iteration 7, loss = 0.15983290
Iteration 8, loss = 0.12678692
Iteration 9, loss = 0.10377606
Iteration 10, loss = 0.08744543
Iteration 11, loss = 0.07556255
Iteration 12, loss = 0.06631286
Iteration 13, loss = 0.05941997
Iteration 14, loss = 0.05413378
Iteration 15, loss = 0.04989886
Iteration 16, loss = 0.04672813
Iteration 17, loss = 0.04352795
Iteration 18, loss = 0.04135086
Iteration 19, loss = 0.03941938
Iteration 20, loss = 0.03751717
Iteration 21, loss = 0.03622436
Iteration 22, loss = 0.03477084
Iteration 23, loss = 0.03394484
Iteration 24, loss = 0.03299665
Iteration 25, loss = 0.03230434
Iteration 26, loss = 0.03147362
Iteration 27, loss = 0.03085588
Iteration 28, loss = 0.03055679
Iteration 29, loss = 0.03005981
Iteration 30, loss = 0.02922371
Iteration 31, loss = 0.02900438
Iteration 32, loss = 0.02852791
Iteration 33, loss = 0.02830918
Iteration 34, loss = 0.02792133
Iteration 35, loss = 0.02761719
Iteration 36, loss = 0.02745406
Iteration 37, loss = 0.02703891
Iteration 38, loss = 0.02708867
Iteration 39, loss = 0.02681824
Iteration 40, loss = 0.02691378
Iteration 41, loss = 0.02633450
Iteration 42, loss = 0.02641261
Iteration 43, loss = 0.02595661
Iteration 44, loss = 0.02604781
Iteration 45, loss = 0.02583031
Iteration 46, loss = 0.02587484
Iteration 47, loss = 0.02579296
Iteration 48, loss = 0.02574832
Iteration 49, loss = 0.02515722
Iteration 50, loss = 0.02547119
Iteration 51, loss = 0.02523456
Iteration 52, loss = 0.02537481
Iteration 53, loss = 0.02519981
Iteration 54, loss = 0.02500391
Iteration 55, loss = 0.02515169
Iteration 56, loss = 0.02487508
Iteration 57, loss = 0.02536249
Iteration 58, loss = 0.02465680
Iteration 59, loss = 0.02514178
Iteration 60, loss = 0.02444818
Iteration 61, loss = 0.02478286
Iteration 62, loss = 0.02480711
Iteration 63, loss = 0.02484020
Iteration 64, loss = 0.02440370
Iteration 65, loss = 0.02444134
Iteration 66, loss = 0.02449861
Iteration 67, loss = 0.02474873
Iteration 68, loss = 0.02437589
Iteration 69, loss = 0.02429083
Iteration 70, loss = 0.02468121
Iteration 71, loss = 0.02451112
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69736184
Iteration 2, loss = 0.62023891
Iteration 3, loss = 0.51309369
Iteration 4, loss = 0.39663819
Iteration 5, loss = 0.29610458
Iteration 6, loss = 0.21983705
Iteration 7, loss = 0.16684823
Iteration 8, loss = 0.13055803
Iteration 9, loss = 0.10570965
Iteration 10, loss = 0.08789032
Iteration 11, loss = 0.07492790
Iteration 12, loss = 0.06564077
Iteration 13, loss = 0.05841473
Iteration 14, loss = 0.05267053
Iteration 15, loss = 0.04803444
Iteration 16, loss = 0.04464327
Iteration 17, loss = 0.04144774
Iteration 18, loss = 0.03911346
Iteration 19, loss = 0.03703503
Iteration 20, loss = 0.03533545
Iteration 21, loss = 0.03379443
Iteration 22, loss = 0.03287136
Iteration 23, loss = 0.03157879
Iteration 24, loss = 0.03068062
Iteration 25, loss = 0.02959491
Iteration 26, loss = 0.02922484
Iteration 27, loss = 0.02841451
Iteration 28, loss = 0.02753315
Iteration 29, loss = 0.02715492
Iteration 30, loss = 0.02682651
Iteration 31, loss = 0.02643924
Iteration 32, loss = 0.02614066
Iteration 33, loss = 0.02553912
Iteration 34, loss = 0.02548447
Iteration 35, loss = 0.02504913
Iteration 36, loss = 0.02499359
Iteration 37, loss = 0.02449144
Iteration 38, loss = 0.02464163
Iteration 39, loss = 0.02422801
Iteration 40, loss = 0.02412287
Iteration 41, loss = 0.02371900
Iteration 42, loss = 0.02350625
Iteration 43, loss = 0.02371312
Iteration 44, loss = 0.02352934
Iteration 45, loss = 0.02339602
Iteration 46, loss = 0.02306117
Iteration 47, loss = 0.02301509
Iteration 48, loss = 0.02301722
Iteration 49, loss = 0.02273574
Iteration 50, loss = 0.02281373
Iteration 51, loss = 0.02274578
Iteration 52, loss = 0.02279596
Iteration 53, loss = 0.02237808
Iteration 54, loss = 0.02260638
Iteration 55, loss = 0.02230502
Iteration 56, loss = 0.02237818
Iteration 57, loss = 0.02204563
Iteration 58, loss = 0.02240686
Iteration 59, loss = 0.02210337
Iteration 60, loss = 0.02203574
Iteration 61, loss = 0.02194931
Iteration 62, loss = 0.02219852
Iteration 63, loss = 0.02186730
Iteration 64, loss = 0.02202241
Iteration 65, loss = 0.02211994
Iteration 66, loss = 0.02230019
Iteration 67, loss = 0.02175033
Iteration 68, loss = 0.02209589
Iteration 69, loss = 0.02181980
Iteration 70, loss = 0.02165544
Iteration 71, loss = 0.02173438
Iteration 72, loss = 0.02164700
Iteration 73, loss = 0.02183594
Iteration 74, loss = 0.02170225
Iteration 75, loss = 0.02161001
Iteration 76, loss = 0.02164936
Iteration 77, loss = 0.02149378
Iteration 78, loss = 0.02171387
Iteration 79, loss = 0.02139961
Iteration 80, loss = 0.02167684
Iteration 81, loss = 0.02147262
Iteration 82, loss = 0.02129144
Iteration 83, loss = 0.02115932
Iteration 84, loss = 0.02135902
Iteration 85, loss = 0.02131397
Iteration 86, loss = 0.02144107
Iteration 87, loss = 0.02144097
Iteration 88, loss = 0.02121055
Iteration 89, loss = 0.02136832
Iteration 90, loss = 0.02125413
Iteration 91, loss = 0.02151441
Iteration 92, loss = 0.02110564
Iteration 93, loss = 0.02137851
Iteration 94, loss = 0.02139602
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70560521
Iteration 2, loss = 0.62670964
Iteration 3, loss = 0.51770586
Iteration 4, loss = 0.39578858
Iteration 5, loss = 0.29096984
Iteration 6, loss = 0.21442633
Iteration 7, loss = 0.16209013
Iteration 8, loss = 0.12671927
Iteration 9, loss = 0.10238131
Iteration 10, loss = 0.08537655
Iteration 11, loss = 0.07307543
Iteration 12, loss = 0.06395428
Iteration 13, loss = 0.05710170
Iteration 14, loss = 0.05156259
Iteration 15, loss = 0.04735269
Iteration 16, loss = 0.04388295
Iteration 17, loss = 0.04086798
Iteration 18, loss = 0.03830794
Iteration 19, loss = 0.03652033
Iteration 20, loss = 0.03471807
Iteration 21, loss = 0.03343539
Iteration 22, loss = 0.03200493
Iteration 23, loss = 0.03093026
Iteration 24, loss = 0.03001901
Iteration 25, loss = 0.02924300
Iteration 26, loss = 0.02865216
Iteration 27, loss = 0.02786730
Iteration 28, loss = 0.02751758
Iteration 29, loss = 0.02702088
Iteration 30, loss = 0.02674274
Iteration 31, loss = 0.02595096
Iteration 32, loss = 0.02559267
Iteration 33, loss = 0.02520841
Iteration 34, loss = 0.02513360
Iteration 35, loss = 0.02475168
Iteration 36, loss = 0.02454828
Iteration 37, loss = 0.02435105
Iteration 38, loss = 0.02393543
Iteration 39, loss = 0.02410684
Iteration 40, loss = 0.02371858
Iteration 41, loss = 0.02348541
Iteration 42, loss = 0.02351966
Iteration 43, loss = 0.02321362
Iteration 44, loss = 0.02311110
Iteration 45, loss = 0.02288600
Iteration 46, loss = 0.02262831
Iteration 47, loss = 0.02308519
Iteration 48, loss = 0.02246496
Iteration 49, loss = 0.02253370
Iteration 50, loss = 0.02255150
Iteration 51, loss = 0.02231269
Iteration 52, loss = 0.02223944
Iteration 53, loss = 0.02257945
Iteration 54, loss = 0.02221663
Iteration 55, loss = 0.02200193
Iteration 56, loss = 0.02207990
Iteration 57, loss = 0.02184498
Iteration 58, loss = 0.02198926
Iteration 59, loss = 0.02190972
Iteration 60, loss = 0.02206475
Iteration 61, loss = 0.02177642
Iteration 62, loss = 0.02188479
Iteration 63, loss = 0.02185261
Iteration 64, loss = 0.02154141
Iteration 65, loss = 0.02165565
Iteration 66, loss = 0.02190534
Iteration 67, loss = 0.02148687
Iteration 68, loss = 0.02161877
Iteration 69, loss = 0.02150654
Iteration 70, loss = 0.02174437
Iteration 71, loss = 0.02138887
Iteration 72, loss = 0.02134508
Iteration 73, loss = 0.02153438
Iteration 74, loss = 0.02121335
Iteration 75, loss = 0.02147372
Iteration 76, loss = 0.02140064
Iteration 77, loss = 0.02107047
Iteration 78, loss = 0.02116319
Iteration 79, loss = 0.02128279
Iteration 80, loss = 0.02098956
Iteration 81, loss = 0.02107876
Iteration 82, loss = 0.02123685
Iteration 83, loss = 0.02121561
Iteration 84, loss = 0.02119379
Iteration 85, loss = 0.02122056
Iteration 86, loss = 0.02101726
Iteration 87, loss = 0.02108664
Iteration 88, loss = 0.02092971
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67624198
Iteration 2, loss = 0.59015489
Iteration 3, loss = 0.46846460
Iteration 4, loss = 0.35411115
Iteration 5, loss = 0.26275959
Iteration 6, loss = 0.19658036
Iteration 7, loss = 0.15096117
Iteration 8, loss = 0.11927122
Iteration 9, loss = 0.09725263
Iteration 10, loss = 0.08162878
Iteration 11, loss = 0.07045002
Iteration 12, loss = 0.06160529
Iteration 13, loss = 0.05547647
Iteration 14, loss = 0.04985448
Iteration 15, loss = 0.04581094
Iteration 16, loss = 0.04246242
Iteration 17, loss = 0.03956756
Iteration 18, loss = 0.03762016
Iteration 19, loss = 0.03547598
Iteration 20, loss = 0.03390538
Iteration 21, loss = 0.03260929
Iteration 22, loss = 0.03154562
Iteration 23, loss = 0.03042648
Iteration 24, loss = 0.02971964
Iteration 25, loss = 0.02871043
Iteration 26, loss = 0.02810886
Iteration 27, loss = 0.02743126
Iteration 28, loss = 0.02678739
Iteration 29, loss = 0.02659676
Iteration 30, loss = 0.02603391
Iteration 31, loss = 0.02566371
Iteration 32, loss = 0.02534157
Iteration 33, loss = 0.02510179
Iteration 34, loss = 0.02470429
Iteration 35, loss = 0.02466944
Iteration 36, loss = 0.02418667
Iteration 37, loss = 0.02409411
Iteration 38, loss = 0.02379166
Iteration 39, loss = 0.02360508
Iteration 40, loss = 0.02343202
Iteration 41, loss = 0.02328025
Iteration 42, loss = 0.02307660
Iteration 43, loss = 0.02306970
Iteration 44, loss = 0.02276385
Iteration 45, loss = 0.02272726
Iteration 46, loss = 0.02258915
Iteration 47, loss = 0.02256966
Iteration 48, loss = 0.02270002
Iteration 49, loss = 0.02241299
Iteration 50, loss = 0.02211832
Iteration 51, loss = 0.02232023
Iteration 52, loss = 0.02228348
Iteration 53, loss = 0.02221638
Iteration 54, loss = 0.02188048
Iteration 55, loss = 0.02213817
Iteration 56, loss = 0.02219916
Iteration 57, loss = 0.02173365
Iteration 58, loss = 0.02178119
Iteration 59, loss = 0.02166874
Iteration 60, loss = 0.02181510
Iteration 61, loss = 0.02190704
Iteration 62, loss = 0.02223443
Iteration 63, loss = 0.02161787
Iteration 64, loss = 0.02141200
Iteration 65, loss = 0.02147866
Iteration 66, loss = 0.02139665
Iteration 67, loss = 0.02130304
Iteration 68, loss = 0.02176647
Iteration 69, loss = 0.02140827
Iteration 70, loss = 0.02139849
Iteration 71, loss = 0.02138120
Iteration 72, loss = 0.02152939
Iteration 73, loss = 0.02126572
Iteration 74, loss = 0.02147645
Iteration 75, loss = 0.02118512
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67287176
Iteration 2, loss = 0.59610111
Iteration 3, loss = 0.48177085
Iteration 4, loss = 0.37022378
Iteration 5, loss = 0.27766838
Iteration 6, loss = 0.20897817
Iteration 7, loss = 0.16086247
Iteration 8, loss = 0.12745660
Iteration 9, loss = 0.10460342
Iteration 10, loss = 0.08793621
Iteration 11, loss = 0.07605324
Iteration 12, loss = 0.06645502
Iteration 13, loss = 0.05974202
Iteration 14, loss = 0.05412942
Iteration 15, loss = 0.04954199
Iteration 16, loss = 0.04618917
Iteration 17, loss = 0.04328136
Iteration 18, loss = 0.04087233
Iteration 19, loss = 0.03901984
Iteration 20, loss = 0.03692484
Iteration 21, loss = 0.03580736
Iteration 22, loss = 0.03398697
Iteration 23, loss = 0.03326107
Iteration 24, loss = 0.03217610
Iteration 25, loss = 0.03162638
Iteration 26, loss = 0.03061756
Iteration 27, loss = 0.03000347
Iteration 28, loss = 0.02949553
Iteration 29, loss = 0.02897094
Iteration 30, loss = 0.02847291
Iteration 31, loss = 0.02813693
Iteration 32, loss = 0.02781042
Iteration 33, loss = 0.02754784
Iteration 34, loss = 0.02712562
Iteration 35, loss = 0.02688137
Iteration 36, loss = 0.02656597
Iteration 37, loss = 0.02632722
Iteration 38, loss = 0.02628005
Iteration 39, loss = 0.02597522
Iteration 40, loss = 0.02583588
Iteration 41, loss = 0.02559115
Iteration 42, loss = 0.02543807
Iteration 43, loss = 0.02549591
Iteration 44, loss = 0.02505550
Iteration 45, loss = 0.02511588
Iteration 46, loss = 0.02495579
Iteration 47, loss = 0.02537953
Iteration 48, loss = 0.02491128
Iteration 49, loss = 0.02464082
Iteration 50, loss = 0.02454828
Iteration 51, loss = 0.02471316
Iteration 52, loss = 0.02449041
Iteration 53, loss = 0.02433097
Iteration 54, loss = 0.02432380
Iteration 55, loss = 0.02417101
Iteration 56, loss = 0.02435942
Iteration 57, loss = 0.02401266
Iteration 58, loss = 0.02422787
Iteration 59, loss = 0.02415633
Iteration 60, loss = 0.02395048
Iteration 61, loss = 0.02390586
Iteration 62, loss = 0.02372482
Iteration 63, loss = 0.02390247
Iteration 64, loss = 0.02384008
Iteration 65, loss = 0.02369397
Iteration 66, loss = 0.02395469
Iteration 67, loss = 0.02364133
Iteration 68, loss = 0.02374366
Iteration 69, loss = 0.02387494
Iteration 70, loss = 0.02345643
Iteration 71, loss = 0.02366593
Iteration 72, loss = 0.02363844
Iteration 73, loss = 0.02343314
Iteration 74, loss = 0.02329614
Iteration 75, loss = 0.02334323
Iteration 76, loss = 0.02372816
Iteration 77, loss = 0.02347204
Iteration 78, loss = 0.02333935
Iteration 79, loss = 0.02336074
Iteration 80, loss = 0.02331709
Iteration 81, loss = 0.02353573
Iteration 82, loss = 0.02330480
Iteration 83, loss = 0.02314311
Iteration 84, loss = 0.02333259
Iteration 85, loss = 0.02305007
Iteration 86, loss = 0.02304150
Iteration 87, loss = 0.02309551
Iteration 88, loss = 0.02350633
Iteration 89, loss = 0.02313548
Iteration 90, loss = 0.02316139
Iteration 91, loss = 0.02314237
Iteration 92, loss = 0.02324957
Iteration 93, loss = 0.02299329
Iteration 94, loss = 0.02320407
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67194956
Iteration 2, loss = 0.59484996
Iteration 3, loss = 0.48275065
Iteration 4, loss = 0.37111608
Iteration 5, loss = 0.27757871
Iteration 6, loss = 0.20854142
Iteration 7, loss = 0.15983290
Iteration 8, loss = 0.12678692
Iteration 9, loss = 0.10377606
Iteration 10, loss = 0.08744543
Iteration 11, loss = 0.07556255
Iteration 12, loss = 0.06631286
Iteration 13, loss = 0.05941997
Iteration 14, loss = 0.05413378
Iteration 15, loss = 0.04989886
Iteration 16, loss = 0.04672813
Iteration 17, loss = 0.04352795
Iteration 18, loss = 0.04135086
Iteration 19, loss = 0.03941938
Iteration 20, loss = 0.03751717
Iteration 21, loss = 0.03622436
Iteration 22, loss = 0.03477084
Iteration 23, loss = 0.03394484
Iteration 24, loss = 0.03299665
Iteration 25, loss = 0.03230434
Iteration 26, loss = 0.03147362
Iteration 27, loss = 0.03085588
Iteration 28, loss = 0.03055679
Iteration 29, loss = 0.03005981
Iteration 30, loss = 0.02922371
Iteration 31, loss = 0.02900438
Iteration 32, loss = 0.02852791
Iteration 33, loss = 0.02830918
Iteration 34, loss = 0.02792133
Iteration 35, loss = 0.02761719
Iteration 36, loss = 0.02745406
Iteration 37, loss = 0.02703891
Iteration 38, loss = 0.02708867
Iteration 39, loss = 0.02681824
Iteration 40, loss = 0.02691378
Iteration 41, loss = 0.02633450
Iteration 42, loss = 0.02641261
Iteration 43, loss = 0.02595661
Iteration 44, loss = 0.02604781
Iteration 45, loss = 0.02583031
Iteration 46, loss = 0.02587484
Iteration 47, loss = 0.02579296
Iteration 48, loss = 0.02574832
Iteration 49, loss = 0.02515722
Iteration 50, loss = 0.02547119
Iteration 51, loss = 0.02523456
Iteration 52, loss = 0.02537481
Iteration 53, loss = 0.02519981
Iteration 54, loss = 0.02500391
Iteration 55, loss = 0.02515169
Iteration 56, loss = 0.02487508
Iteration 57, loss = 0.02536249
Iteration 58, loss = 0.02465680
Iteration 59, loss = 0.02514178
Iteration 60, loss = 0.02444818
Iteration 61, loss = 0.02478286
Iteration 62, loss = 0.02480711
Iteration 63, loss = 0.02484020
Iteration 64, loss = 0.02440370
Iteration 65, loss = 0.02444134
Iteration 66, loss = 0.02449861
Iteration 67, loss = 0.02474873
Iteration 68, loss = 0.02437589
Iteration 69, loss = 0.02429083
Iteration 70, loss = 0.02468121
Iteration 71, loss = 0.02451112
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69736184
Iteration 2, loss = 0.62023891
Iteration 3, loss = 0.51309369
Iteration 4, loss = 0.39663819
Iteration 5, loss = 0.29610458
Iteration 6, loss = 0.21983705
Iteration 7, loss = 0.16684823
Iteration 8, loss = 0.13055803
Iteration 9, loss = 0.10570965
Iteration 10, loss = 0.08789032
Iteration 11, loss = 0.07492790
Iteration 12, loss = 0.06564077
Iteration 13, loss = 0.05841473
Iteration 14, loss = 0.05267053
Iteration 15, loss = 0.04803444
Iteration 16, loss = 0.04464327
Iteration 17, loss = 0.04144774
Iteration 18, loss = 0.03911346
Iteration 19, loss = 0.03703503
Iteration 20, loss = 0.03533545
Iteration 21, loss = 0.03379443
Iteration 22, loss = 0.03287136
Iteration 23, loss = 0.03157879
Iteration 24, loss = 0.03068062
Iteration 25, loss = 0.02959491
Iteration 26, loss = 0.02922484
Iteration 27, loss = 0.02841451
Iteration 28, loss = 0.02753315
Iteration 29, loss = 0.02715492
Iteration 30, loss = 0.02682651
Iteration 31, loss = 0.02643924
Iteration 32, loss = 0.02614066
Iteration 33, loss = 0.02553912
Iteration 34, loss = 0.02548447
Iteration 35, loss = 0.02504913
Iteration 36, loss = 0.02499359
Iteration 37, loss = 0.02449144
Iteration 38, loss = 0.02464163
Iteration 39, loss = 0.02422801
Iteration 40, loss = 0.02412287
Iteration 41, loss = 0.02371900
Iteration 42, loss = 0.02350625
Iteration 43, loss = 0.02371312
Iteration 44, loss = 0.02352934
Iteration 45, loss = 0.02339602
Iteration 46, loss = 0.02306117
Iteration 47, loss = 0.02301509
Iteration 48, loss = 0.02301722
Iteration 49, loss = 0.02273574
Iteration 50, loss = 0.02281373
Iteration 51, loss = 0.02274578
Iteration 52, loss = 0.02279596
Iteration 53, loss = 0.02237808
Iteration 54, loss = 0.02260638
Iteration 55, loss = 0.02230502
Iteration 56, loss = 0.02237818
Iteration 57, loss = 0.02204563
Iteration 58, loss = 0.02240686
Iteration 59, loss = 0.02210337
Iteration 60, loss = 0.02203574
Iteration 61, loss = 0.02194931
Iteration 62, loss = 0.02219852
Iteration 63, loss = 0.02186730
Iteration 64, loss = 0.02202241
Iteration 65, loss = 0.02211994
Iteration 66, loss = 0.02230019
Iteration 67, loss = 0.02175033
Iteration 68, loss = 0.02209589
Iteration 69, loss = 0.02181980
Iteration 70, loss = 0.02165544
Iteration 71, loss = 0.02173438
Iteration 72, loss = 0.02164700
Iteration 73, loss = 0.02183594
Iteration 74, loss = 0.02170225
Iteration 75, loss = 0.02161001
Iteration 76, loss = 0.02164936
Iteration 77, loss = 0.02149378
Iteration 78, loss = 0.02171387
Iteration 79, loss = 0.02139961
Iteration 80, loss = 0.02167684
Iteration 81, loss = 0.02147262
Iteration 82, loss = 0.02129144
Iteration 83, loss = 0.02115932
Iteration 84, loss = 0.02135902
Iteration 85, loss = 0.02131397
Iteration 86, loss = 0.02144107
Iteration 87, loss = 0.02144097
Iteration 88, loss = 0.02121055
Iteration 89, loss = 0.02136832
Iteration 90, loss = 0.02125413
Iteration 91, loss = 0.02151441
Iteration 92, loss = 0.02110564
Iteration 93, loss = 0.02137851
Iteration 94, loss = 0.02139602
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70560521
Iteration 2, loss = 0.62670964
Iteration 3, loss = 0.51770586
Iteration 4, loss = 0.39578858
Iteration 5, loss = 0.29096984
Iteration 6, loss = 0.21442633
Iteration 7, loss = 0.16209013
Iteration 8, loss = 0.12671927
Iteration 9, loss = 0.10238131
Iteration 10, loss = 0.08537655
Iteration 11, loss = 0.07307543
Iteration 12, loss = 0.06395428
Iteration 13, loss = 0.05710170
Iteration 14, loss = 0.05156259
Iteration 15, loss = 0.04735269
Iteration 16, loss = 0.04388295
Iteration 17, loss = 0.04086798
Iteration 18, loss = 0.03830794
Iteration 19, loss = 0.03652033
Iteration 20, loss = 0.03471807
Iteration 21, loss = 0.03343539
Iteration 22, loss = 0.03200493
Iteration 23, loss = 0.03093026
Iteration 24, loss = 0.03001901
Iteration 25, loss = 0.02924300
Iteration 26, loss = 0.02865216
Iteration 27, loss = 0.02786730
Iteration 28, loss = 0.02751758
Iteration 29, loss = 0.02702088
Iteration 30, loss = 0.02674274
Iteration 31, loss = 0.02595096
Iteration 32, loss = 0.02559267
Iteration 33, loss = 0.02520841
Iteration 34, loss = 0.02513360
Iteration 35, loss = 0.02475168
Iteration 36, loss = 0.02454828
Iteration 37, loss = 0.02435105
Iteration 38, loss = 0.02393543
Iteration 39, loss = 0.02410684
Iteration 40, loss = 0.02371858
Iteration 41, loss = 0.02348541
Iteration 42, loss = 0.02351966
Iteration 43, loss = 0.02321362
Iteration 44, loss = 0.02311110
Iteration 45, loss = 0.02288600
Iteration 46, loss = 0.02262831
Iteration 47, loss = 0.02308519
Iteration 48, loss = 0.02246496
Iteration 49, loss = 0.02253370
Iteration 50, loss = 0.02255150
Iteration 51, loss = 0.02231269
Iteration 52, loss = 0.02223944
Iteration 53, loss = 0.02257945
Iteration 54, loss = 0.02221663
Iteration 55, loss = 0.02200193
Iteration 56, loss = 0.02207990
Iteration 57, loss = 0.02184498
Iteration 58, loss = 0.02198926
Iteration 59, loss = 0.02190972
Iteration 60, loss = 0.02206475
Iteration 61, loss = 0.02177642
Iteration 62, loss = 0.02188479
Iteration 63, loss = 0.02185261
Iteration 64, loss = 0.02154141
Iteration 65, loss = 0.02165565
Iteration 66, loss = 0.02190534
Iteration 67, loss = 0.02148687
Iteration 68, loss = 0.02161877
Iteration 69, loss = 0.02150654
Iteration 70, loss = 0.02174437
Iteration 71, loss = 0.02138887
Iteration 72, loss = 0.02134508
Iteration 73, loss = 0.02153438
Iteration 74, loss = 0.02121335
Iteration 75, loss = 0.02147372
Iteration 76, loss = 0.02140064
Iteration 77, loss = 0.02107047
Iteration 78, loss = 0.02116319
Iteration 79, loss = 0.02128279
Iteration 80, loss = 0.02098956
Iteration 81, loss = 0.02107876
Iteration 82, loss = 0.02123685
Iteration 83, loss = 0.02121561
Iteration 84, loss = 0.02119379
Iteration 85, loss = 0.02122056
Iteration 86, loss = 0.02101726
Iteration 87, loss = 0.02108664
Iteration 88, loss = 0.02092971
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67624198
Iteration 2, loss = 0.59015489
Iteration 3, loss = 0.46846460
Iteration 4, loss = 0.35411115
Iteration 5, loss = 0.26275959
Iteration 6, loss = 0.19658036
Iteration 7, loss = 0.15096117
Iteration 8, loss = 0.11927122
Iteration 9, loss = 0.09725263
Iteration 10, loss = 0.08162878
Iteration 11, loss = 0.07045002
Iteration 12, loss = 0.06160529
Iteration 13, loss = 0.05547647
Iteration 14, loss = 0.04985448
Iteration 15, loss = 0.04581094
Iteration 16, loss = 0.04246242
Iteration 17, loss = 0.03956756
Iteration 18, loss = 0.03762016
Iteration 19, loss = 0.03547598
Iteration 20, loss = 0.03390538
Iteration 21, loss = 0.03260929
Iteration 22, loss = 0.03154562
Iteration 23, loss = 0.03042648
Iteration 24, loss = 0.02971964
Iteration 25, loss = 0.02871043
Iteration 26, loss = 0.02810886
Iteration 27, loss = 0.02743126
Iteration 28, loss = 0.02678739
Iteration 29, loss = 0.02659676
Iteration 30, loss = 0.02603391
Iteration 31, loss = 0.02566371
Iteration 32, loss = 0.02534157
Iteration 33, loss = 0.02510179
Iteration 34, loss = 0.02470429
Iteration 35, loss = 0.02466944
Iteration 36, loss = 0.02418667
Iteration 37, loss = 0.02409411
Iteration 38, loss = 0.02379166
Iteration 39, loss = 0.02360508
Iteration 40, loss = 0.02343202
Iteration 41, loss = 0.02328025
Iteration 42, loss = 0.02307660
Iteration 43, loss = 0.02306970
Iteration 44, loss = 0.02276385
Iteration 45, loss = 0.02272726
Iteration 46, loss = 0.02258915
Iteration 47, loss = 0.02256966
Iteration 48, loss = 0.02270002
Iteration 49, loss = 0.02241299
Iteration 50, loss = 0.02211832
Iteration 51, loss = 0.02232023
Iteration 52, loss = 0.02228348
Iteration 53, loss = 0.02221638
Iteration 54, loss = 0.02188048
Iteration 55, loss = 0.02213817
Iteration 56, loss = 0.02219916
Iteration 57, loss = 0.02173365
Iteration 58, loss = 0.02178119
Iteration 59, loss = 0.02166874
Iteration 60, loss = 0.02181510
Iteration 61, loss = 0.02190704
Iteration 62, loss = 0.02223443
Iteration 63, loss = 0.02161787
Iteration 64, loss = 0.02141200
Iteration 65, loss = 0.02147866
Iteration 66, loss = 0.02139665
Iteration 67, loss = 0.02130304
Iteration 68, loss = 0.02176647
Iteration 69, loss = 0.02140827
Iteration 70, loss = 0.02139849
Iteration 71, loss = 0.02138120
Iteration 72, loss = 0.02152939
Iteration 73, loss = 0.02126572
Iteration 74, loss = 0.02147645
Iteration 75, loss = 0.02118512
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67287176
Iteration 2, loss = 0.59610111
Iteration 3, loss = 0.48177085
Iteration 4, loss = 0.37022378
Iteration 5, loss = 0.27766838
Iteration 6, loss = 0.20897817
Iteration 7, loss = 0.16086247
Iteration 8, loss = 0.12745660
Iteration 9, loss = 0.10460342
Iteration 10, loss = 0.08793621
Iteration 11, loss = 0.07605324
Iteration 12, loss = 0.06645502
Iteration 13, loss = 0.05974202
Iteration 14, loss = 0.05412942
Iteration 15, loss = 0.04954199
Iteration 16, loss = 0.04618917
Iteration 17, loss = 0.04328136
Iteration 18, loss = 0.04087233
Iteration 19, loss = 0.03901984
Iteration 20, loss = 0.03692484
Iteration 21, loss = 0.03580736
Iteration 22, loss = 0.03398697
Iteration 23, loss = 0.03326107
Iteration 24, loss = 0.03217610
Iteration 25, loss = 0.03162638
Iteration 26, loss = 0.03061756
Iteration 27, loss = 0.03000347
Iteration 28, loss = 0.02949553
Iteration 29, loss = 0.02897094
Iteration 30, loss = 0.02847291
Iteration 31, loss = 0.02813693
Iteration 32, loss = 0.02781042
Iteration 33, loss = 0.02754784
Iteration 34, loss = 0.02712562
Iteration 35, loss = 0.02688137
Iteration 36, loss = 0.02656597
Iteration 37, loss = 0.02632722
Iteration 38, loss = 0.02628005
Iteration 39, loss = 0.02597522
Iteration 40, loss = 0.02583588
Iteration 41, loss = 0.02559115
Iteration 42, loss = 0.02543807
Iteration 43, loss = 0.02549591
Iteration 44, loss = 0.02505550
Iteration 45, loss = 0.02511588
Iteration 46, loss = 0.02495579
Iteration 47, loss = 0.02537953
Iteration 48, loss = 0.02491128
Iteration 49, loss = 0.02464082
Iteration 50, loss = 0.02454828
Iteration 51, loss = 0.02471316
Iteration 52, loss = 0.02449041
Iteration 53, loss = 0.02433097
Iteration 54, loss = 0.02432380
Iteration 55, loss = 0.02417101
Iteration 56, loss = 0.02435942
Iteration 57, loss = 0.02401266
Iteration 58, loss = 0.02422787
Iteration 59, loss = 0.02415633
Iteration 60, loss = 0.02395048
Iteration 61, loss = 0.02390586
Iteration 62, loss = 0.02372482
Iteration 63, loss = 0.02390247
Iteration 64, loss = 0.02384008
Iteration 65, loss = 0.02369397
Iteration 66, loss = 0.02395469
Iteration 67, loss = 0.02364133
Iteration 68, loss = 0.02374366
Iteration 69, loss = 0.02387494
Iteration 70, loss = 0.02345643
Iteration 71, loss = 0.02366593
Iteration 72, loss = 0.02363844
Iteration 73, loss = 0.02343314
Iteration 74, loss = 0.02329614
Iteration 75, loss = 0.02334323
Iteration 76, loss = 0.02372816
Iteration 77, loss = 0.02347204
Iteration 78, loss = 0.02333935
Iteration 79, loss = 0.02336074
Iteration 80, loss = 0.02331709
Iteration 81, loss = 0.02353573
Iteration 82, loss = 0.02330480
Iteration 83, loss = 0.02314311
Iteration 84, loss = 0.02333259
Iteration 85, loss = 0.02305007
Iteration 86, loss = 0.02304150
Iteration 87, loss = 0.02309551
Iteration 88, loss = 0.02350633
Iteration 89, loss = 0.02313548
Iteration 90, loss = 0.02316139
Iteration 91, loss = 0.02314237
Iteration 92, loss = 0.02324957
Iteration 93, loss = 0.02299329
Iteration 94, loss = 0.02320407
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67194956
Iteration 2, loss = 0.59484996
Iteration 3, loss = 0.48275065
Iteration 4, loss = 0.37111608
Iteration 5, loss = 0.27757871
Iteration 6, loss = 0.20854142
Iteration 7, loss = 0.15983290
Iteration 8, loss = 0.12678692
Iteration 9, loss = 0.10377606
Iteration 10, loss = 0.08744543
Iteration 11, loss = 0.07556255
Iteration 12, loss = 0.06631286
Iteration 13, loss = 0.05941997
Iteration 14, loss = 0.05413378
Iteration 15, loss = 0.04989886
Iteration 16, loss = 0.04672813
Iteration 17, loss = 0.04352795
Iteration 18, loss = 0.04135086
Iteration 19, loss = 0.03941938
Iteration 20, loss = 0.03751717
Iteration 21, loss = 0.03622436
Iteration 22, loss = 0.03477084
Iteration 23, loss = 0.03394484
Iteration 24, loss = 0.03299665
Iteration 25, loss = 0.03230434
Iteration 26, loss = 0.03147362
Iteration 27, loss = 0.03085588
Iteration 28, loss = 0.03055679
Iteration 29, loss = 0.03005981
Iteration 30, loss = 0.02922371
Iteration 31, loss = 0.02900438
Iteration 32, loss = 0.02852791
Iteration 33, loss = 0.02830918
Iteration 34, loss = 0.02792133
Iteration 35, loss = 0.02761719
Iteration 36, loss = 0.02745406
Iteration 37, loss = 0.02703891
Iteration 38, loss = 0.02708867
Iteration 39, loss = 0.02681824
Iteration 40, loss = 0.02691378
Iteration 41, loss = 0.02633450
Iteration 42, loss = 0.02641261
Iteration 43, loss = 0.02595661
Iteration 44, loss = 0.02604781
Iteration 45, loss = 0.02583031
Iteration 46, loss = 0.02587484
Iteration 47, loss = 0.02579296
Iteration 48, loss = 0.02574832
Iteration 49, loss = 0.02515722
Iteration 50, loss = 0.02547119
Iteration 51, loss = 0.02523456
Iteration 52, loss = 0.02537481
Iteration 53, loss = 0.02519981
Iteration 54, loss = 0.02500391
Iteration 55, loss = 0.02515169
Iteration 56, loss = 0.02487508
Iteration 57, loss = 0.02536249
Iteration 58, loss = 0.02465680
Iteration 59, loss = 0.02514178
Iteration 60, loss = 0.02444818
Iteration 61, loss = 0.02478286
Iteration 62, loss = 0.02480711
Iteration 63, loss = 0.02484020
Iteration 64, loss = 0.02440370
Iteration 65, loss = 0.02444134
Iteration 66, loss = 0.02449861
Iteration 67, loss = 0.02474873
Iteration 68, loss = 0.02437589
Iteration 69, loss = 0.02429083
Iteration 70, loss = 0.02468121
Iteration 71, loss = 0.02451112
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69736184
Iteration 2, loss = 0.62023891
Iteration 3, loss = 0.51309369
Iteration 4, loss = 0.39663819
Iteration 5, loss = 0.29610458
Iteration 6, loss = 0.21983705
Iteration 7, loss = 0.16684823
Iteration 8, loss = 0.13055803
Iteration 9, loss = 0.10570965
Iteration 10, loss = 0.08789032
Iteration 11, loss = 0.07492790
Iteration 12, loss = 0.06564077
Iteration 13, loss = 0.05841473
Iteration 14, loss = 0.05267053
Iteration 15, loss = 0.04803444
Iteration 16, loss = 0.04464327
Iteration 17, loss = 0.04144774
Iteration 18, loss = 0.03911346
Iteration 19, loss = 0.03703503
Iteration 20, loss = 0.03533545
Iteration 21, loss = 0.03379443
Iteration 22, loss = 0.03287136
Iteration 23, loss = 0.03157879
Iteration 24, loss = 0.03068062
Iteration 25, loss = 0.02959491
Iteration 26, loss = 0.02922484
Iteration 27, loss = 0.02841451
Iteration 28, loss = 0.02753315
Iteration 29, loss = 0.02715492
Iteration 30, loss = 0.02682651
Iteration 31, loss = 0.02643924
Iteration 32, loss = 0.02614066
Iteration 33, loss = 0.02553912
Iteration 34, loss = 0.02548447
Iteration 35, loss = 0.02504913
Iteration 36, loss = 0.02499359
Iteration 37, loss = 0.02449144
Iteration 38, loss = 0.02464163
Iteration 39, loss = 0.02422801
Iteration 40, loss = 0.02412287
Iteration 41, loss = 0.02371900
Iteration 42, loss = 0.02350625
Iteration 43, loss = 0.02371312
Iteration 44, loss = 0.02352934
Iteration 45, loss = 0.02339602
Iteration 46, loss = 0.02306117
Iteration 47, loss = 0.02301509
Iteration 48, loss = 0.02301722
Iteration 49, loss = 0.02273574
Iteration 50, loss = 0.02281373
Iteration 51, loss = 0.02274578
Iteration 52, loss = 0.02279596
Iteration 53, loss = 0.02237808
Iteration 54, loss = 0.02260638
Iteration 55, loss = 0.02230502
Iteration 56, loss = 0.02237818
Iteration 57, loss = 0.02204563
Iteration 58, loss = 0.02240686
Iteration 59, loss = 0.02210337
Iteration 60, loss = 0.02203574
Iteration 61, loss = 0.02194931
Iteration 62, loss = 0.02219852
Iteration 63, loss = 0.02186730
Iteration 64, loss = 0.02202241
Iteration 65, loss = 0.02211994
Iteration 66, loss = 0.02230019
Iteration 67, loss = 0.02175033
Iteration 68, loss = 0.02209589
Iteration 69, loss = 0.02181980
Iteration 70, loss = 0.02165544
Iteration 71, loss = 0.02173438
Iteration 72, loss = 0.02164700
Iteration 73, loss = 0.02183594
Iteration 74, loss = 0.02170225
Iteration 75, loss = 0.02161001
Iteration 76, loss = 0.02164936
Iteration 77, loss = 0.02149378
Iteration 78, loss = 0.02171387
Iteration 79, loss = 0.02139961
Iteration 80, loss = 0.02167684
Iteration 81, loss = 0.02147262
Iteration 82, loss = 0.02129144
Iteration 83, loss = 0.02115932
Iteration 84, loss = 0.02135902
Iteration 85, loss = 0.02131397
Iteration 86, loss = 0.02144107
Iteration 87, loss = 0.02144097
Iteration 88, loss = 0.02121055
Iteration 89, loss = 0.02136832
Iteration 90, loss = 0.02125413
Iteration 91, loss = 0.02151441
Iteration 92, loss = 0.02110564
Iteration 93, loss = 0.02137851
Iteration 94, loss = 0.02139602
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70560521
Iteration 2, loss = 0.62670964
Iteration 3, loss = 0.51770586
Iteration 4, loss = 0.39578858
Iteration 5, loss = 0.29096984
Iteration 6, loss = 0.21442633
Iteration 7, loss = 0.16209013
Iteration 8, loss = 0.12671927
Iteration 9, loss = 0.10238131
Iteration 10, loss = 0.08537655
Iteration 11, loss = 0.07307543
Iteration 12, loss = 0.06395428
Iteration 13, loss = 0.05710170
Iteration 14, loss = 0.05156259
Iteration 15, loss = 0.04735269
Iteration 16, loss = 0.04388295
Iteration 17, loss = 0.04086798
Iteration 18, loss = 0.03830794
Iteration 19, loss = 0.03652033
Iteration 20, loss = 0.03471807
Iteration 21, loss = 0.03343539
Iteration 22, loss = 0.03200493
Iteration 23, loss = 0.03093026
Iteration 24, loss = 0.03001901
Iteration 25, loss = 0.02924300
Iteration 26, loss = 0.02865216
Iteration 27, loss = 0.02786730
Iteration 28, loss = 0.02751758
Iteration 29, loss = 0.02702088
Iteration 30, loss = 0.02674274
Iteration 31, loss = 0.02595096
Iteration 32, loss = 0.02559267
Iteration 33, loss = 0.02520841
Iteration 34, loss = 0.02513360
Iteration 35, loss = 0.02475168
Iteration 36, loss = 0.02454828
Iteration 37, loss = 0.02435105
Iteration 38, loss = 0.02393543
Iteration 39, loss = 0.02410684
Iteration 40, loss = 0.02371858
Iteration 41, loss = 0.02348541
Iteration 42, loss = 0.02351966
Iteration 43, loss = 0.02321362
Iteration 44, loss = 0.02311110
Iteration 45, loss = 0.02288600
Iteration 46, loss = 0.02262831
Iteration 47, loss = 0.02308519
Iteration 48, loss = 0.02246496
Iteration 49, loss = 0.02253370
Iteration 50, loss = 0.02255150
Iteration 51, loss = 0.02231269
Iteration 52, loss = 0.02223944
Iteration 53, loss = 0.02257945
Iteration 54, loss = 0.02221663
Iteration 55, loss = 0.02200193
Iteration 56, loss = 0.02207990
Iteration 57, loss = 0.02184498
Iteration 58, loss = 0.02198926
Iteration 59, loss = 0.02190972
Iteration 60, loss = 0.02206475
Iteration 61, loss = 0.02177642
Iteration 62, loss = 0.02188479
Iteration 63, loss = 0.02185261
Iteration 64, loss = 0.02154141
Iteration 65, loss = 0.02165565
Iteration 66, loss = 0.02190534
Iteration 67, loss = 0.02148687
Iteration 68, loss = 0.02161877
Iteration 69, loss = 0.02150654
Iteration 70, loss = 0.02174437
Iteration 71, loss = 0.02138887
Iteration 72, loss = 0.02134508
Iteration 73, loss = 0.02153438
Iteration 74, loss = 0.02121335
Iteration 75, loss = 0.02147372
Iteration 76, loss = 0.02140064
Iteration 77, loss = 0.02107047
Iteration 78, loss = 0.02116319
Iteration 79, loss = 0.02128279
Iteration 80, loss = 0.02098956
Iteration 81, loss = 0.02107876
Iteration 82, loss = 0.02123685
Iteration 83, loss = 0.02121561
Iteration 84, loss = 0.02119379
Iteration 85, loss = 0.02122056
Iteration 86, loss = 0.02101726
Iteration 87, loss = 0.02108664
Iteration 88, loss = 0.02092971
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67624198
Iteration 2, loss = 0.59015489
Iteration 3, loss = 0.46846460
Iteration 4, loss = 0.35411115
Iteration 5, loss = 0.26275959
Iteration 6, loss = 0.19658036
Iteration 7, loss = 0.15096117
Iteration 8, loss = 0.11927122
Iteration 9, loss = 0.09725263
Iteration 10, loss = 0.08162878
Iteration 11, loss = 0.07045002
Iteration 12, loss = 0.06160529
Iteration 13, loss = 0.05547647
Iteration 14, loss = 0.04985448
Iteration 15, loss = 0.04581094
Iteration 16, loss = 0.04246242
Iteration 17, loss = 0.03956756
Iteration 18, loss = 0.03762016
Iteration 19, loss = 0.03547598
Iteration 20, loss = 0.03390538
Iteration 21, loss = 0.03260929
Iteration 22, loss = 0.03154562
Iteration 23, loss = 0.03042648
Iteration 24, loss = 0.02971964
Iteration 25, loss = 0.02871043
Iteration 26, loss = 0.02810886
Iteration 27, loss = 0.02743126
Iteration 28, loss = 0.02678739
Iteration 29, loss = 0.02659676
Iteration 30, loss = 0.02603391
Iteration 31, loss = 0.02566371
Iteration 32, loss = 0.02534157
Iteration 33, loss = 0.02510179
Iteration 34, loss = 0.02470429
Iteration 35, loss = 0.02466944
Iteration 36, loss = 0.02418667
Iteration 37, loss = 0.02409411
Iteration 38, loss = 0.02379166
Iteration 39, loss = 0.02360508
Iteration 40, loss = 0.02343202
Iteration 41, loss = 0.02328025
Iteration 42, loss = 0.02307660
Iteration 43, loss = 0.02306970
Iteration 44, loss = 0.02276385
Iteration 45, loss = 0.02272726
Iteration 46, loss = 0.02258915
Iteration 47, loss = 0.02256966
Iteration 48, loss = 0.02270002
Iteration 49, loss = 0.02241299
Iteration 50, loss = 0.02211832
Iteration 51, loss = 0.02232023
Iteration 52, loss = 0.02228348
Iteration 53, loss = 0.02221638
Iteration 54, loss = 0.02188048
Iteration 55, loss = 0.02213817
Iteration 56, loss = 0.02219916
Iteration 57, loss = 0.02173365
Iteration 58, loss = 0.02178119
Iteration 59, loss = 0.02166874
Iteration 60, loss = 0.02181510
Iteration 61, loss = 0.02190704
Iteration 62, loss = 0.02223443
Iteration 63, loss = 0.02161787
Iteration 64, loss = 0.02141200
Iteration 65, loss = 0.02147866
Iteration 66, loss = 0.02139665
Iteration 67, loss = 0.02130304
Iteration 68, loss = 0.02176647
Iteration 69, loss = 0.02140827
Iteration 70, loss = 0.02139849
Iteration 71, loss = 0.02138120
Iteration 72, loss = 0.02152939
Iteration 73, loss = 0.02126572
Iteration 74, loss = 0.02147645
Iteration 75, loss = 0.02118512
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67194889
Iteration 2, loss = 0.57064395
Iteration 3, loss = 0.43151352
Iteration 4, loss = 0.30806248
Iteration 5, loss = 0.21945370
Iteration 6, loss = 0.16079427
Iteration 7, loss = 0.12332262
Iteration 8, loss = 0.09875252
Iteration 9, loss = 0.08196224
Iteration 10, loss = 0.07028805
Iteration 11, loss = 0.06169207
Iteration 12, loss = 0.05520585
Iteration 13, loss = 0.05057897
Iteration 14, loss = 0.04649519
Iteration 15, loss = 0.04333954
Iteration 16, loss = 0.04105124
Iteration 17, loss = 0.03885998
Iteration 18, loss = 0.03718030
Iteration 19, loss = 0.03590064
Iteration 20, loss = 0.03466662
Iteration 21, loss = 0.03358699
Iteration 22, loss = 0.03250246
Iteration 23, loss = 0.03182215
Iteration 24, loss = 0.03103176
Iteration 25, loss = 0.03052569
Iteration 26, loss = 0.03003758
Iteration 27, loss = 0.02960950
Iteration 28, loss = 0.02912357
Iteration 29, loss = 0.02866480
Iteration 30, loss = 0.02855860
Iteration 31, loss = 0.02842083
Iteration 32, loss = 0.02807153
Iteration 33, loss = 0.02750303
Iteration 34, loss = 0.02754214
Iteration 35, loss = 0.02735418
Iteration 36, loss = 0.02748076
Iteration 37, loss = 0.02725838
Iteration 38, loss = 0.02698226
Iteration 39, loss = 0.02655004
Iteration 40, loss = 0.02671603
Iteration 41, loss = 0.02643645
Iteration 42, loss = 0.02675329
Iteration 43, loss = 0.02641061
Iteration 44, loss = 0.02598789
Iteration 45, loss = 0.02608788
Iteration 46, loss = 0.02607060
Iteration 47, loss = 0.02595157
Iteration 48, loss = 0.02563378
Iteration 49, loss = 0.02566910
Iteration 50, loss = 0.02571902
Iteration 51, loss = 0.02565100
Iteration 52, loss = 0.02524193
Iteration 53, loss = 0.02559886
Iteration 54, loss = 0.02561027
Iteration 55, loss = 0.02541201
Iteration 56, loss = 0.02554371
Iteration 57, loss = 0.02575388
Iteration 58, loss = 0.02546927
Iteration 59, loss = 0.02540741
Iteration 60, loss = 0.02552098
Iteration 61, loss = 0.02550919
Iteration 62, loss = 0.02515267
Iteration 63, loss = 0.02534122
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on prepended_v5_lemma_stem...
Iteration 1, loss = 0.71566744
Iteration 2, loss = 0.69671817
Iteration 3, loss = 0.68124558
Iteration 4, loss = 0.66667996
Iteration 5, loss = 0.65209434
Iteration 6, loss = 0.63660706
Iteration 7, loss = 0.61989791
Iteration 8, loss = 0.60202783
Iteration 9, loss = 0.58311443
Iteration 10, loss = 0.56311403
Iteration 11, loss = 0.54234309
Iteration 12, loss = 0.52107648
Iteration 13, loss = 0.49910137
Iteration 14, loss = 0.47696866
Iteration 15, loss = 0.45474205
Iteration 16, loss = 0.43252633
Iteration 17, loss = 0.41040179
Iteration 18, loss = 0.38860123
Iteration 19, loss = 0.36725202
Iteration 20, loss = 0.34651731
Iteration 21, loss = 0.32641020
Iteration 22, loss = 0.30712883
Iteration 23, loss = 0.28865343
Iteration 24, loss = 0.27110542
Iteration 25, loss = 0.25446796
Iteration 26, loss = 0.23871148
Iteration 27, loss = 0.22388800
Iteration 28, loss = 0.21015052
Iteration 29, loss = 0.19714306
Iteration 30, loss = 0.18508649
Iteration 31, loss = 0.17387793
Iteration 32, loss = 0.16347972
Iteration 33, loss = 0.15378499
Iteration 34, loss = 0.14478777
Iteration 35, loss = 0.13645710
Iteration 36, loss = 0.12874839
Iteration 37, loss = 0.12159603
Iteration 38, loss = 0.11497491
Iteration 39, loss = 0.10884093
Iteration 40, loss = 0.10315561
Iteration 41, loss = 0.09788365
Iteration 42, loss = 0.09297590
Iteration 43, loss = 0.08841175
Iteration 44, loss = 0.08417703
Iteration 45, loss = 0.08021873
Iteration 46, loss = 0.07658483
Iteration 47, loss = 0.07316127
Iteration 48, loss = 0.06996995
Iteration 49, loss = 0.06698927
Iteration 50, loss = 0.06422863
Iteration 51, loss = 0.06158185
Iteration 52, loss = 0.05913454
Iteration 53, loss = 0.05684245
Iteration 54, loss = 0.05464925
Iteration 55, loss = 0.05267550
Iteration 56, loss = 0.05073640
Iteration 57, loss = 0.04896921
Iteration 58, loss = 0.04724724
Iteration 59, loss = 0.04560422
Iteration 60, loss = 0.04413668
Iteration 61, loss = 0.04267669
Iteration 62, loss = 0.04133033
Iteration 63, loss = 0.04007621
Iteration 64, loss = 0.03886665
Iteration 65, loss = 0.03770724
Iteration 66, loss = 0.03658504
Iteration 67, loss = 0.03552775
Iteration 68, loss = 0.03453986
Iteration 69, loss = 0.03359332
Iteration 70, loss = 0.03268712
Iteration 71, loss = 0.03182283
Iteration 72, loss = 0.03098086
Iteration 73, loss = 0.03018729
Iteration 74, loss = 0.02942664
Iteration 75, loss = 0.02870155
Iteration 76, loss = 0.02799835
Iteration 77, loss = 0.02733948
Iteration 78, loss = 0.02670511
Iteration 79, loss = 0.02606289
Iteration 80, loss = 0.02546695
Iteration 81, loss = 0.02495475
Iteration 82, loss = 0.02439041
Iteration 83, loss = 0.02387410
Iteration 84, loss = 0.02339328
Iteration 85, loss = 0.02290475
Iteration 86, loss = 0.02244799
Iteration 87, loss = 0.02197344
Iteration 88, loss = 0.02152152
Iteration 89, loss = 0.02109861
Iteration 90, loss = 0.02071048
Iteration 91, loss = 0.02030110
Iteration 92, loss = 0.01993316
Iteration 93, loss = 0.01956542
Iteration 94, loss = 0.01919959
Iteration 95, loss = 0.01887020
Iteration 96, loss = 0.01857000
Iteration 97, loss = 0.01822826
Iteration 98, loss = 0.01791283
Iteration 99, loss = 0.01763183
Iteration 100, loss = 0.01732818
Iteration 101, loss = 0.01704778
Iteration 102, loss = 0.01678345
Iteration 103, loss = 0.01651954
Iteration 104, loss = 0.01627367
Iteration 105, loss = 0.01602035
Iteration 106, loss = 0.01577451
Iteration 107, loss = 0.01554434
Iteration 108, loss = 0.01532323
Iteration 109, loss = 0.01510815
Iteration 110, loss = 0.01492124
Iteration 111, loss = 0.01468136
Iteration 112, loss = 0.01446630
Iteration 113, loss = 0.01427293
Iteration 114, loss = 0.01408281
Iteration 115, loss = 0.01388707
Iteration 116, loss = 0.01370758
Iteration 117, loss = 0.01353128
Iteration 118, loss = 0.01338505
Iteration 119, loss = 0.01320702
Iteration 120, loss = 0.01303483
Iteration 121, loss = 0.01289046
Iteration 122, loss = 0.01273705
Iteration 123, loss = 0.01260326
Iteration 124, loss = 0.01245185
Iteration 125, loss = 0.01229163
Iteration 126, loss = 0.01215164
Iteration 127, loss = 0.01202103
Iteration 128, loss = 0.01190140
Iteration 129, loss = 0.01177080
Iteration 130, loss = 0.01162893
Iteration 131, loss = 0.01151688
Iteration 132, loss = 0.01141310
Iteration 133, loss = 0.01127684
Iteration 134, loss = 0.01118168
Iteration 135, loss = 0.01105298
Iteration 136, loss = 0.01094539
Iteration 137, loss = 0.01082888
Iteration 138, loss = 0.01073290
Iteration 139, loss = 0.01062098
Iteration 140, loss = 0.01053455
Iteration 141, loss = 0.01044309
Iteration 142, loss = 0.01033126
Iteration 143, loss = 0.01024811
Iteration 144, loss = 0.01015303
Iteration 145, loss = 0.01006352
Iteration 146, loss = 0.00998361
Iteration 147, loss = 0.00989755
Iteration 148, loss = 0.00980469
Iteration 149, loss = 0.00972724
Iteration 150, loss = 0.00966256
Iteration 151, loss = 0.00959506
Iteration 152, loss = 0.00948721
Iteration 153, loss = 0.00941201
Iteration 154, loss = 0.00937139
Iteration 155, loss = 0.00925020
Iteration 156, loss = 0.00923125
Iteration 157, loss = 0.00912181
Iteration 158, loss = 0.00905072
Iteration 159, loss = 0.00898266
Iteration 160, loss = 0.00891945
Iteration 161, loss = 0.00885368
Iteration 162, loss = 0.00882104
Iteration 163, loss = 0.00874982
Iteration 164, loss = 0.00868818
Iteration 165, loss = 0.00863435
Iteration 166, loss = 0.00858924
Iteration 167, loss = 0.00854259
Iteration 168, loss = 0.00849459
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70183698
Iteration 2, loss = 0.67219708
Iteration 3, loss = 0.63401973
Iteration 4, loss = 0.58597309
Iteration 5, loss = 0.53012178
Iteration 6, loss = 0.47080307
Iteration 7, loss = 0.41150365
Iteration 8, loss = 0.35590974
Iteration 9, loss = 0.30572751
Iteration 10, loss = 0.26136551
Iteration 11, loss = 0.22344203
Iteration 12, loss = 0.19161965
Iteration 13, loss = 0.16507697
Iteration 14, loss = 0.14312547
Iteration 15, loss = 0.12508947
Iteration 16, loss = 0.11012514
Iteration 17, loss = 0.09771874
Iteration 18, loss = 0.08738128
Iteration 19, loss = 0.07870378
Iteration 20, loss = 0.07144805
Iteration 21, loss = 0.06529544
Iteration 22, loss = 0.05997145
Iteration 23, loss = 0.05548488
Iteration 24, loss = 0.05140981
Iteration 25, loss = 0.04811126
Iteration 26, loss = 0.04507323
Iteration 27, loss = 0.04239740
Iteration 28, loss = 0.03997510
Iteration 29, loss = 0.03798181
Iteration 30, loss = 0.03611784
Iteration 31, loss = 0.03446537
Iteration 32, loss = 0.03281087
Iteration 33, loss = 0.03155848
Iteration 34, loss = 0.03032769
Iteration 35, loss = 0.02917909
Iteration 36, loss = 0.02817172
Iteration 37, loss = 0.02730832
Iteration 38, loss = 0.02645270
Iteration 39, loss = 0.02572442
Iteration 40, loss = 0.02491159
Iteration 41, loss = 0.02426118
Iteration 42, loss = 0.02365601
Iteration 43, loss = 0.02312350
Iteration 44, loss = 0.02263258
Iteration 45, loss = 0.02211317
Iteration 46, loss = 0.02165480
Iteration 47, loss = 0.02121090
Iteration 48, loss = 0.02091178
Iteration 49, loss = 0.02048659
Iteration 50, loss = 0.02017912
Iteration 51, loss = 0.01975115
Iteration 52, loss = 0.01948147
Iteration 53, loss = 0.01928996
Iteration 54, loss = 0.01907976
Iteration 55, loss = 0.01887224
Iteration 56, loss = 0.01847855
Iteration 57, loss = 0.01830399
Iteration 58, loss = 0.01806598
Iteration 59, loss = 0.01777706
Iteration 60, loss = 0.01770079
Iteration 61, loss = 0.01758304
Iteration 62, loss = 0.01728400
Iteration 63, loss = 0.01716014
Iteration 64, loss = 0.01697818
Iteration 65, loss = 0.01680579
Iteration 66, loss = 0.01673339
Iteration 67, loss = 0.01653614
Iteration 68, loss = 0.01650193
Iteration 69, loss = 0.01629582
Iteration 70, loss = 0.01618988
Iteration 71, loss = 0.01615892
Iteration 72, loss = 0.01595851
Iteration 73, loss = 0.01605127
Iteration 74, loss = 0.01594275
Iteration 75, loss = 0.01574275
Iteration 76, loss = 0.01575278
Iteration 77, loss = 0.01565235
Iteration 78, loss = 0.01550674
Iteration 79, loss = 0.01536939
Iteration 80, loss = 0.01534270
Iteration 81, loss = 0.01528770
Iteration 82, loss = 0.01525185
Iteration 83, loss = 0.01512937
Iteration 84, loss = 0.01506585
Iteration 85, loss = 0.01498102
Iteration 86, loss = 0.01494000
Iteration 87, loss = 0.01504513
Iteration 88, loss = 0.01485007
Iteration 89, loss = 0.01466971
Iteration 90, loss = 0.01469801
Iteration 91, loss = 0.01465191
Iteration 92, loss = 0.01468916
Iteration 93, loss = 0.01456869
Iteration 94, loss = 0.01455965
Iteration 95, loss = 0.01442349
Iteration 96, loss = 0.01458336
Iteration 97, loss = 0.01449827
Iteration 98, loss = 0.01445810
Iteration 99, loss = 0.01429661
Iteration 100, loss = 0.01426973
Iteration 101, loss = 0.01450470
Iteration 102, loss = 0.01424152
Iteration 103, loss = 0.01428904
Iteration 104, loss = 0.01432305
Iteration 105, loss = 0.01419623
Iteration 106, loss = 0.01409957
Iteration 107, loss = 0.01415953
Iteration 108, loss = 0.01415891
Iteration 109, loss = 0.01403755
Iteration 110, loss = 0.01399420
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67631569
Iteration 2, loss = 0.62583500
Iteration 3, loss = 0.54923989
Iteration 4, loss = 0.45774123
Iteration 5, loss = 0.36962257
Iteration 6, loss = 0.29339906
Iteration 7, loss = 0.23215172
Iteration 8, loss = 0.18509115
Iteration 9, loss = 0.15008701
Iteration 10, loss = 0.12410821
Iteration 11, loss = 0.10468974
Iteration 12, loss = 0.09010720
Iteration 13, loss = 0.07878300
Iteration 14, loss = 0.06997080
Iteration 15, loss = 0.06313219
Iteration 16, loss = 0.05762062
Iteration 17, loss = 0.05266824
Iteration 18, loss = 0.04905886
Iteration 19, loss = 0.04591793
Iteration 20, loss = 0.04330322
Iteration 21, loss = 0.04112734
Iteration 22, loss = 0.03908962
Iteration 23, loss = 0.03745757
Iteration 24, loss = 0.03598913
Iteration 25, loss = 0.03474901
Iteration 26, loss = 0.03346495
Iteration 27, loss = 0.03260047
Iteration 28, loss = 0.03167629
Iteration 29, loss = 0.03096024
Iteration 30, loss = 0.03030762
Iteration 31, loss = 0.02980654
Iteration 32, loss = 0.02906333
Iteration 33, loss = 0.02847492
Iteration 34, loss = 0.02812763
Iteration 35, loss = 0.02763518
Iteration 36, loss = 0.02728715
Iteration 37, loss = 0.02702996
Iteration 38, loss = 0.02673686
Iteration 39, loss = 0.02633747
Iteration 40, loss = 0.02592316
Iteration 41, loss = 0.02575697
Iteration 42, loss = 0.02547614
Iteration 43, loss = 0.02529067
Iteration 44, loss = 0.02508653
Iteration 45, loss = 0.02498331
Iteration 46, loss = 0.02472438
Iteration 47, loss = 0.02464061
Iteration 48, loss = 0.02449521
Iteration 49, loss = 0.02423794
Iteration 50, loss = 0.02422375
Iteration 51, loss = 0.02399582
Iteration 52, loss = 0.02385923
Iteration 53, loss = 0.02376251
Iteration 54, loss = 0.02381821
Iteration 55, loss = 0.02359154
Iteration 56, loss = 0.02347304
Iteration 57, loss = 0.02345149
Iteration 58, loss = 0.02340119
Iteration 59, loss = 0.02321235
Iteration 60, loss = 0.02334109
Iteration 61, loss = 0.02310688
Iteration 62, loss = 0.02300091
Iteration 63, loss = 0.02303841
Iteration 64, loss = 0.02305220
Iteration 65, loss = 0.02309082
Iteration 66, loss = 0.02283221
Iteration 67, loss = 0.02264314
Iteration 68, loss = 0.02285043
Iteration 69, loss = 0.02255817
Iteration 70, loss = 0.02283763
Iteration 71, loss = 0.02255663
Iteration 72, loss = 0.02233665
Iteration 73, loss = 0.02268403
Iteration 74, loss = 0.02257544
Iteration 75, loss = 0.02234128
Iteration 76, loss = 0.02240258
Iteration 77, loss = 0.02254227
Iteration 78, loss = 0.02243051
Iteration 79, loss = 0.02231851
Iteration 80, loss = 0.02229178
Iteration 81, loss = 0.02206673
Iteration 82, loss = 0.02206167
Iteration 83, loss = 0.02213970
Iteration 84, loss = 0.02207889
Iteration 85, loss = 0.02233338
Iteration 86, loss = 0.02210332
Iteration 87, loss = 0.02196787
Iteration 88, loss = 0.02232167
Iteration 89, loss = 0.02214714
Iteration 90, loss = 0.02198742
Iteration 91, loss = 0.02198118
Iteration 92, loss = 0.02202476
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67626294
Iteration 2, loss = 0.63348666
Iteration 3, loss = 0.57494248
Iteration 4, loss = 0.50428391
Iteration 5, loss = 0.43147993
Iteration 6, loss = 0.36376009
Iteration 7, loss = 0.30316871
Iteration 8, loss = 0.25175541
Iteration 9, loss = 0.20947115
Iteration 10, loss = 0.17521814
Iteration 11, loss = 0.14801398
Iteration 12, loss = 0.12630627
Iteration 13, loss = 0.10909532
Iteration 14, loss = 0.09530337
Iteration 15, loss = 0.08410637
Iteration 16, loss = 0.07513934
Iteration 17, loss = 0.06769324
Iteration 18, loss = 0.06136565
Iteration 19, loss = 0.05618403
Iteration 20, loss = 0.05173490
Iteration 21, loss = 0.04795273
Iteration 22, loss = 0.04479122
Iteration 23, loss = 0.04202095
Iteration 24, loss = 0.03946156
Iteration 25, loss = 0.03733647
Iteration 26, loss = 0.03549221
Iteration 27, loss = 0.03392156
Iteration 28, loss = 0.03235787
Iteration 29, loss = 0.03112386
Iteration 30, loss = 0.02988198
Iteration 31, loss = 0.02864408
Iteration 32, loss = 0.02783251
Iteration 33, loss = 0.02697554
Iteration 34, loss = 0.02614720
Iteration 35, loss = 0.02537116
Iteration 36, loss = 0.02486911
Iteration 37, loss = 0.02417776
Iteration 38, loss = 0.02373484
Iteration 39, loss = 0.02325699
Iteration 40, loss = 0.02262135
Iteration 41, loss = 0.02231456
Iteration 42, loss = 0.02191084
Iteration 43, loss = 0.02149753
Iteration 44, loss = 0.02129008
Iteration 45, loss = 0.02087714
Iteration 46, loss = 0.02047603
Iteration 47, loss = 0.02025417
Iteration 48, loss = 0.02011332
Iteration 49, loss = 0.01976059
Iteration 50, loss = 0.01961246
Iteration 51, loss = 0.01940039
Iteration 52, loss = 0.01917359
Iteration 53, loss = 0.01890144
Iteration 54, loss = 0.01895241
Iteration 55, loss = 0.01855710
Iteration 56, loss = 0.01848943
Iteration 57, loss = 0.01829994
Iteration 58, loss = 0.01809783
Iteration 59, loss = 0.01802679
Iteration 60, loss = 0.01799608
Iteration 61, loss = 0.01774724
Iteration 62, loss = 0.01772413
Iteration 63, loss = 0.01747055
Iteration 64, loss = 0.01738362
Iteration 65, loss = 0.01729622
Iteration 66, loss = 0.01726051
Iteration 67, loss = 0.01730550
Iteration 68, loss = 0.01714206
Iteration 69, loss = 0.01713235
Iteration 70, loss = 0.01695672
Iteration 71, loss = 0.01682052
Iteration 72, loss = 0.01684278
Iteration 73, loss = 0.01659684
Iteration 74, loss = 0.01652685
Iteration 75, loss = 0.01648273
Iteration 76, loss = 0.01652095
Iteration 77, loss = 0.01656187
Iteration 78, loss = 0.01633713
Iteration 79, loss = 0.01651657
Iteration 80, loss = 0.01617725
Iteration 81, loss = 0.01628814
Iteration 82, loss = 0.01617933
Iteration 83, loss = 0.01615307
Iteration 84, loss = 0.01616749
Iteration 85, loss = 0.01599507
Iteration 86, loss = 0.01604949
Iteration 87, loss = 0.01601288
Iteration 88, loss = 0.01584204
Iteration 89, loss = 0.01594164
Iteration 90, loss = 0.01575541
Iteration 91, loss = 0.01595674
Iteration 92, loss = 0.01578668
Iteration 93, loss = 0.01576859
Iteration 94, loss = 0.01570731
Iteration 95, loss = 0.01564828
Iteration 96, loss = 0.01577169
Iteration 97, loss = 0.01560023
Iteration 98, loss = 0.01582103
Iteration 99, loss = 0.01564600
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71001184
Iteration 2, loss = 0.69180165
Iteration 3, loss = 0.67593499
Iteration 4, loss = 0.66055404
Iteration 5, loss = 0.64431284
Iteration 6, loss = 0.62711064
Iteration 7, loss = 0.60904485
Iteration 8, loss = 0.58984717
Iteration 9, loss = 0.56962162
Iteration 10, loss = 0.54811053
Iteration 11, loss = 0.52668754
Iteration 12, loss = 0.50460157
Iteration 13, loss = 0.48207855
Iteration 14, loss = 0.45947224
Iteration 15, loss = 0.43693974
Iteration 16, loss = 0.41481071
Iteration 17, loss = 0.39298626
Iteration 18, loss = 0.37150413
Iteration 19, loss = 0.35066793
Iteration 20, loss = 0.33054630
Iteration 21, loss = 0.31112335
Iteration 22, loss = 0.29261007
Iteration 23, loss = 0.27488014
Iteration 24, loss = 0.25812749
Iteration 25, loss = 0.24233106
Iteration 26, loss = 0.22743195
Iteration 27, loss = 0.21356947
Iteration 28, loss = 0.20051325
Iteration 29, loss = 0.18837726
Iteration 30, loss = 0.17706766
Iteration 31, loss = 0.16654006
Iteration 32, loss = 0.15666604
Iteration 33, loss = 0.14759847
Iteration 34, loss = 0.13915485
Iteration 35, loss = 0.13136372
Iteration 36, loss = 0.12408649
Iteration 37, loss = 0.11734216
Iteration 38, loss = 0.11112537
Iteration 39, loss = 0.10536272
Iteration 40, loss = 0.09999753
Iteration 41, loss = 0.09501727
Iteration 42, loss = 0.09043730
Iteration 43, loss = 0.08622371
Iteration 44, loss = 0.08223622
Iteration 45, loss = 0.07858197
Iteration 46, loss = 0.07511009
Iteration 47, loss = 0.07194591
Iteration 48, loss = 0.06892571
Iteration 49, loss = 0.06613267
Iteration 50, loss = 0.06357104
Iteration 51, loss = 0.06108688
Iteration 52, loss = 0.05879173
Iteration 53, loss = 0.05662764
Iteration 54, loss = 0.05459100
Iteration 55, loss = 0.05270565
Iteration 56, loss = 0.05091143
Iteration 57, loss = 0.04922040
Iteration 58, loss = 0.04760391
Iteration 59, loss = 0.04608924
Iteration 60, loss = 0.04463951
Iteration 61, loss = 0.04332447
Iteration 62, loss = 0.04204388
Iteration 63, loss = 0.04081634
Iteration 64, loss = 0.03964980
Iteration 65, loss = 0.03856492
Iteration 66, loss = 0.03752942
Iteration 67, loss = 0.03652675
Iteration 68, loss = 0.03557278
Iteration 69, loss = 0.03469759
Iteration 70, loss = 0.03383635
Iteration 71, loss = 0.03305330
Iteration 72, loss = 0.03225793
Iteration 73, loss = 0.03153547
Iteration 74, loss = 0.03083919
Iteration 75, loss = 0.03016842
Iteration 76, loss = 0.02952488
Iteration 77, loss = 0.02889015
Iteration 78, loss = 0.02829786
Iteration 79, loss = 0.02772547
Iteration 80, loss = 0.02719281
Iteration 81, loss = 0.02665944
Iteration 82, loss = 0.02617076
Iteration 83, loss = 0.02570288
Iteration 84, loss = 0.02525294
Iteration 85, loss = 0.02477425
Iteration 86, loss = 0.02436261
Iteration 87, loss = 0.02391533
Iteration 88, loss = 0.02351035
Iteration 89, loss = 0.02310140
Iteration 90, loss = 0.02270694
Iteration 91, loss = 0.02235043
Iteration 92, loss = 0.02202284
Iteration 93, loss = 0.02166819
Iteration 94, loss = 0.02131501
Iteration 95, loss = 0.02099217
Iteration 96, loss = 0.02070893
Iteration 97, loss = 0.02041502
Iteration 98, loss = 0.02014461
Iteration 99, loss = 0.01984670
Iteration 100, loss = 0.01957592
Iteration 101, loss = 0.01933765
Iteration 102, loss = 0.01906808
Iteration 103, loss = 0.01882123
Iteration 104, loss = 0.01857109
Iteration 105, loss = 0.01833426
Iteration 106, loss = 0.01811856
Iteration 107, loss = 0.01790559
Iteration 108, loss = 0.01771088
Iteration 109, loss = 0.01747573
Iteration 110, loss = 0.01727341
Iteration 111, loss = 0.01710031
Iteration 112, loss = 0.01692776
Iteration 113, loss = 0.01668785
Iteration 114, loss = 0.01654674
Iteration 115, loss = 0.01638887
Iteration 116, loss = 0.01617949
Iteration 117, loss = 0.01602425
Iteration 118, loss = 0.01587355
Iteration 119, loss = 0.01570519
Iteration 120, loss = 0.01559179
Iteration 121, loss = 0.01544027
Iteration 122, loss = 0.01528154
Iteration 123, loss = 0.01513358
Iteration 124, loss = 0.01498455
Iteration 125, loss = 0.01483612
Iteration 126, loss = 0.01472441
Iteration 127, loss = 0.01461716
Iteration 128, loss = 0.01449778
Iteration 129, loss = 0.01436256
Iteration 130, loss = 0.01423870
Iteration 131, loss = 0.01412781
Iteration 132, loss = 0.01399032
Iteration 133, loss = 0.01388206
Iteration 134, loss = 0.01379247
Iteration 135, loss = 0.01367222
Iteration 136, loss = 0.01356925
Iteration 137, loss = 0.01349206
Iteration 138, loss = 0.01340422
Iteration 139, loss = 0.01328924
Iteration 140, loss = 0.01322098
Iteration 141, loss = 0.01308977
Iteration 142, loss = 0.01298650
Iteration 143, loss = 0.01290374
Iteration 144, loss = 0.01283143
Iteration 145, loss = 0.01272771
Iteration 146, loss = 0.01264700
Iteration 147, loss = 0.01257309
Iteration 148, loss = 0.01250940
Iteration 149, loss = 0.01241169
Iteration 150, loss = 0.01233793
Iteration 151, loss = 0.01226780
Iteration 152, loss = 0.01220819
Iteration 153, loss = 0.01212106
Iteration 154, loss = 0.01206756
Iteration 155, loss = 0.01202847
Iteration 156, loss = 0.01193345
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68128547
Iteration 2, loss = 0.62108708
Iteration 3, loss = 0.52789958
Iteration 4, loss = 0.42595871
Iteration 5, loss = 0.33438119
Iteration 6, loss = 0.25928683
Iteration 7, loss = 0.20077955
Iteration 8, loss = 0.15810426
Iteration 9, loss = 0.12736367
Iteration 10, loss = 0.10555938
Iteration 11, loss = 0.09052521
Iteration 12, loss = 0.07843065
Iteration 13, loss = 0.06913084
Iteration 14, loss = 0.06173401
Iteration 15, loss = 0.05610674
Iteration 16, loss = 0.05156198
Iteration 17, loss = 0.04754671
Iteration 18, loss = 0.04428722
Iteration 19, loss = 0.04169334
Iteration 20, loss = 0.03949230
Iteration 21, loss = 0.03743486
Iteration 22, loss = 0.03577690
Iteration 23, loss = 0.03431355
Iteration 24, loss = 0.03301218
Iteration 25, loss = 0.03215616
Iteration 26, loss = 0.03084462
Iteration 27, loss = 0.03014345
Iteration 28, loss = 0.02943239
Iteration 29, loss = 0.02883685
Iteration 30, loss = 0.02804934
Iteration 31, loss = 0.02718980
Iteration 32, loss = 0.02709523
Iteration 33, loss = 0.02656277
Iteration 34, loss = 0.02591190
Iteration 35, loss = 0.02564676
Iteration 36, loss = 0.02526237
Iteration 37, loss = 0.02498165
Iteration 38, loss = 0.02482653
Iteration 39, loss = 0.02437610
Iteration 40, loss = 0.02407849
Iteration 41, loss = 0.02378604
Iteration 42, loss = 0.02351825
Iteration 43, loss = 0.02360780
Iteration 44, loss = 0.02353362
Iteration 45, loss = 0.02317730
Iteration 46, loss = 0.02265648
Iteration 47, loss = 0.02276443
Iteration 48, loss = 0.02258646
Iteration 49, loss = 0.02225335
Iteration 50, loss = 0.02217184
Iteration 51, loss = 0.02344948
Iteration 52, loss = 0.02427388
Iteration 53, loss = 0.02396292
Iteration 54, loss = 0.02382764
Iteration 55, loss = 0.02346367
Iteration 56, loss = 0.02305441
Iteration 57, loss = 0.02300679
Iteration 58, loss = 0.02277497
Iteration 59, loss = 0.02261111
Iteration 60, loss = 0.02256059
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73009580
Iteration 2, loss = 0.70514038
Iteration 3, loss = 0.67798981
Iteration 4, loss = 0.64352155
Iteration 5, loss = 0.60282075
Iteration 6, loss = 0.55796399
Iteration 7, loss = 0.51040002
Iteration 8, loss = 0.46244137
Iteration 9, loss = 0.41523202
Iteration 10, loss = 0.37059042
Iteration 11, loss = 0.32952982
Iteration 12, loss = 0.29231324
Iteration 13, loss = 0.25906244
Iteration 14, loss = 0.22958804
Iteration 15, loss = 0.20381718
Iteration 16, loss = 0.18139370
Iteration 17, loss = 0.16195178
Iteration 18, loss = 0.14528192
Iteration 19, loss = 0.13076647
Iteration 20, loss = 0.11827664
Iteration 21, loss = 0.10737703
Iteration 22, loss = 0.09807807
Iteration 23, loss = 0.09010717
Iteration 24, loss = 0.08301947
Iteration 25, loss = 0.07694833
Iteration 26, loss = 0.07160439
Iteration 27, loss = 0.06677220
Iteration 28, loss = 0.06248048
Iteration 29, loss = 0.05879325
Iteration 30, loss = 0.05536176
Iteration 31, loss = 0.05236507
Iteration 32, loss = 0.04961578
Iteration 33, loss = 0.04713388
Iteration 34, loss = 0.04500998
Iteration 35, loss = 0.04298904
Iteration 36, loss = 0.04113419
Iteration 37, loss = 0.03945145
Iteration 38, loss = 0.03788148
Iteration 39, loss = 0.03642864
Iteration 40, loss = 0.03519044
Iteration 41, loss = 0.03403462
Iteration 42, loss = 0.03293228
Iteration 43, loss = 0.03194128
Iteration 44, loss = 0.03103431
Iteration 45, loss = 0.03038243
Iteration 46, loss = 0.02941052
Iteration 47, loss = 0.02873221
Iteration 48, loss = 0.02823951
Iteration 49, loss = 0.02786241
Iteration 50, loss = 0.02701707
Iteration 51, loss = 0.02621382
Iteration 52, loss = 0.02574604
Iteration 53, loss = 0.02519635
Iteration 54, loss = 0.02470749
Iteration 55, loss = 0.02420653
Iteration 56, loss = 0.02369333
Iteration 57, loss = 0.02333916
Iteration 58, loss = 0.02286828
Iteration 59, loss = 0.02260940
Iteration 60, loss = 0.02218908
Iteration 61, loss = 0.02186301
Iteration 62, loss = 0.02154548
Iteration 63, loss = 0.02123559
Iteration 64, loss = 0.02108031
Iteration 65, loss = 0.02087670
Iteration 66, loss = 0.02061547
Iteration 67, loss = 0.02026974
Iteration 68, loss = 0.02019447
Iteration 69, loss = 0.02016806
Iteration 70, loss = 0.01979728
Iteration 71, loss = 0.01959682
Iteration 72, loss = 0.01947963
Iteration 73, loss = 0.01921016
Iteration 74, loss = 0.01893859
Iteration 75, loss = 0.01878413
Iteration 76, loss = 0.01860164
Iteration 77, loss = 0.01843504
Iteration 78, loss = 0.01832071
Iteration 79, loss = 0.01814144
Iteration 80, loss = 0.01801408
Iteration 81, loss = 0.01784221
Iteration 82, loss = 0.01772194
Iteration 83, loss = 0.01759058
Iteration 84, loss = 0.01749474
Iteration 85, loss = 0.01746504
Iteration 86, loss = 0.01731100
Iteration 87, loss = 0.01717196
Iteration 88, loss = 0.01705113
Iteration 89, loss = 0.01695129
Iteration 90, loss = 0.01692286
Iteration 91, loss = 0.01673428
Iteration 92, loss = 0.01669518
Iteration 93, loss = 0.01662339
Iteration 94, loss = 0.01663993
Iteration 95, loss = 0.01654963
Iteration 96, loss = 0.01653695
Iteration 97, loss = 0.01651320
Iteration 98, loss = 0.01642211
Iteration 99, loss = 0.01633047
Iteration 100, loss = 0.01624185
Iteration 101, loss = 0.01636212
Iteration 102, loss = 0.01620647
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71029651
Iteration 2, loss = 0.66953096
Iteration 3, loss = 0.61643182
Iteration 4, loss = 0.55381734
Iteration 5, loss = 0.48521201
Iteration 6, loss = 0.41674294
Iteration 7, loss = 0.35197956
Iteration 8, loss = 0.29444845
Iteration 9, loss = 0.24513287
Iteration 10, loss = 0.20438091
Iteration 11, loss = 0.17140565
Iteration 12, loss = 0.14492716
Iteration 13, loss = 0.12403551
Iteration 14, loss = 0.10723849
Iteration 15, loss = 0.09393580
Iteration 16, loss = 0.08309220
Iteration 17, loss = 0.07433247
Iteration 18, loss = 0.06686793
Iteration 19, loss = 0.06078280
Iteration 20, loss = 0.05571915
Iteration 21, loss = 0.05146076
Iteration 22, loss = 0.04762736
Iteration 23, loss = 0.04448542
Iteration 24, loss = 0.04173931
Iteration 25, loss = 0.03929863
Iteration 26, loss = 0.03724956
Iteration 27, loss = 0.03547210
Iteration 28, loss = 0.03378503
Iteration 29, loss = 0.03227595
Iteration 30, loss = 0.03105629
Iteration 31, loss = 0.02973127
Iteration 32, loss = 0.02875682
Iteration 33, loss = 0.02783443
Iteration 34, loss = 0.02703143
Iteration 35, loss = 0.02608023
Iteration 36, loss = 0.02544445
Iteration 37, loss = 0.02481666
Iteration 38, loss = 0.02431632
Iteration 39, loss = 0.02372828
Iteration 40, loss = 0.02315589
Iteration 41, loss = 0.02282343
Iteration 42, loss = 0.02228909
Iteration 43, loss = 0.02194786
Iteration 44, loss = 0.02156705
Iteration 45, loss = 0.02140724
Iteration 46, loss = 0.02087366
Iteration 47, loss = 0.02064730
Iteration 48, loss = 0.02029465
Iteration 49, loss = 0.02011942
Iteration 50, loss = 0.01979465
Iteration 51, loss = 0.01954344
Iteration 52, loss = 0.01934895
Iteration 53, loss = 0.01938984
Iteration 54, loss = 0.01903690
Iteration 55, loss = 0.01882686
Iteration 56, loss = 0.01867736
Iteration 57, loss = 0.01858240
Iteration 58, loss = 0.01835759
Iteration 59, loss = 0.01827000
Iteration 60, loss = 0.01814324
Iteration 61, loss = 0.01812963
Iteration 62, loss = 0.01786058
Iteration 63, loss = 0.01783832
Iteration 64, loss = 0.01752158
Iteration 65, loss = 0.01758674
Iteration 66, loss = 0.01747285
Iteration 67, loss = 0.01740835
Iteration 68, loss = 0.01724822
Iteration 69, loss = 0.01715653
Iteration 70, loss = 0.01707738
Iteration 71, loss = 0.01694908
Iteration 72, loss = 0.01701221
Iteration 73, loss = 0.01692837
Iteration 74, loss = 0.01687791
Iteration 75, loss = 0.01673733
Iteration 76, loss = 0.01666520
Iteration 77, loss = 0.01669966
Iteration 78, loss = 0.01663442
Iteration 79, loss = 0.01669950
Iteration 80, loss = 0.01641173
Iteration 81, loss = 0.01643280
Iteration 82, loss = 0.01630466
Iteration 83, loss = 0.01626545
Iteration 84, loss = 0.01642101
Iteration 85, loss = 0.01621098
Iteration 86, loss = 0.01616212
Iteration 87, loss = 0.01615403
Iteration 88, loss = 0.01618187
Iteration 89, loss = 0.01616621
Iteration 90, loss = 0.01603394
Iteration 91, loss = 0.01604093
Iteration 92, loss = 0.01589780
Iteration 93, loss = 0.01599984
Iteration 94, loss = 0.01594946
Iteration 95, loss = 0.01587325
Iteration 96, loss = 0.01595381
Iteration 97, loss = 0.01581883
Iteration 98, loss = 0.01574709
Iteration 99, loss = 0.01579700
Iteration 100, loss = 0.01569205
Iteration 101, loss = 0.01569373
Iteration 102, loss = 0.01570678
Iteration 103, loss = 0.01570033
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68768265
Iteration 2, loss = 0.64893463
Iteration 3, loss = 0.58848943
Iteration 4, loss = 0.50549249
Iteration 5, loss = 0.41988514
Iteration 6, loss = 0.34164077
Iteration 7, loss = 0.27535268
Iteration 8, loss = 0.22243276
Iteration 9, loss = 0.18108626
Iteration 10, loss = 0.14973421
Iteration 11, loss = 0.12565465
Iteration 12, loss = 0.10704918
Iteration 13, loss = 0.09253690
Iteration 14, loss = 0.08133948
Iteration 15, loss = 0.07236559
Iteration 16, loss = 0.06510075
Iteration 17, loss = 0.05921044
Iteration 18, loss = 0.05420300
Iteration 19, loss = 0.04996547
Iteration 20, loss = 0.04655301
Iteration 21, loss = 0.04370866
Iteration 22, loss = 0.04117872
Iteration 23, loss = 0.03900353
Iteration 24, loss = 0.03701848
Iteration 25, loss = 0.03560272
Iteration 26, loss = 0.03386224
Iteration 27, loss = 0.03270135
Iteration 28, loss = 0.03128841
Iteration 29, loss = 0.03026912
Iteration 30, loss = 0.02932475
Iteration 31, loss = 0.02849422
Iteration 32, loss = 0.02772874
Iteration 33, loss = 0.02697806
Iteration 34, loss = 0.02643892
Iteration 35, loss = 0.02578995
Iteration 36, loss = 0.02519253
Iteration 37, loss = 0.02491285
Iteration 38, loss = 0.02446657
Iteration 39, loss = 0.02403054
Iteration 40, loss = 0.02366511
Iteration 41, loss = 0.02339102
Iteration 42, loss = 0.02298397
Iteration 43, loss = 0.02296437
Iteration 44, loss = 0.02256950
Iteration 45, loss = 0.02214394
Iteration 46, loss = 0.02197973
Iteration 47, loss = 0.02157959
Iteration 48, loss = 0.02153157
Iteration 49, loss = 0.02121293
Iteration 50, loss = 0.02106790
Iteration 51, loss = 0.02102262
Iteration 52, loss = 0.02079745
Iteration 53, loss = 0.02054136
Iteration 54, loss = 0.02052409
Iteration 55, loss = 0.02044394
Iteration 56, loss = 0.02014737
Iteration 57, loss = 0.02022727
Iteration 58, loss = 0.02002131
Iteration 59, loss = 0.01986588
Iteration 60, loss = 0.01976037
Iteration 61, loss = 0.01958574
Iteration 62, loss = 0.01968913
Iteration 63, loss = 0.01945897
Iteration 64, loss = 0.01911982
Iteration 65, loss = 0.01930979
Iteration 66, loss = 0.01912020
Iteration 67, loss = 0.01921443
Iteration 68, loss = 0.01898220
Iteration 69, loss = 0.01921600
Iteration 70, loss = 0.01898047
Iteration 71, loss = 0.01893267
Iteration 72, loss = 0.01902479
Iteration 73, loss = 0.01916563
Iteration 74, loss = 0.01898477
Iteration 75, loss = 0.01875056
Iteration 76, loss = 0.01858155
Iteration 77, loss = 0.01859904
Iteration 78, loss = 0.01870123
Iteration 79, loss = 0.01861972
Iteration 80, loss = 0.01853456
Iteration 81, loss = 0.01851480
Iteration 82, loss = 0.01834037
Iteration 83, loss = 0.01833618
Iteration 84, loss = 0.01822060
Iteration 85, loss = 0.01832411
Iteration 86, loss = 0.01825649
Iteration 87, loss = 0.01821075
Iteration 88, loss = 0.01838308
Iteration 89, loss = 0.01808024
Iteration 90, loss = 0.01800903
Iteration 91, loss = 0.01804572
Iteration 92, loss = 0.01790468
Iteration 93, loss = 0.01785949
Iteration 94, loss = 0.01778737
Iteration 95, loss = 0.01787258
Iteration 96, loss = 0.01788226
Iteration 97, loss = 0.01798963
Iteration 98, loss = 0.01788923
Iteration 99, loss = 0.01796759
Iteration 100, loss = 0.01783535
Iteration 101, loss = 0.01773296
Iteration 102, loss = 0.01768131
Iteration 103, loss = 0.01780914
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68341801
Iteration 2, loss = 0.66854236
Iteration 3, loss = 0.65537620
Iteration 4, loss = 0.64217641
Iteration 5, loss = 0.62835999
Iteration 6, loss = 0.61334102
Iteration 7, loss = 0.59720341
Iteration 8, loss = 0.58001270
Iteration 9, loss = 0.56139575
Iteration 10, loss = 0.54215846
Iteration 11, loss = 0.52196188
Iteration 12, loss = 0.50109847
Iteration 13, loss = 0.47992184
Iteration 14, loss = 0.45838432
Iteration 15, loss = 0.43665741
Iteration 16, loss = 0.41507428
Iteration 17, loss = 0.39370789
Iteration 18, loss = 0.37265963
Iteration 19, loss = 0.35210221
Iteration 20, loss = 0.33220888
Iteration 21, loss = 0.31306058
Iteration 22, loss = 0.29454456
Iteration 23, loss = 0.27699104
Iteration 24, loss = 0.26036791
Iteration 25, loss = 0.24452100
Iteration 26, loss = 0.22949234
Iteration 27, loss = 0.21538413
Iteration 28, loss = 0.20208458
Iteration 29, loss = 0.18960842
Iteration 30, loss = 0.17793409
Iteration 31, loss = 0.16709883
Iteration 32, loss = 0.15685916
Iteration 33, loss = 0.14741158
Iteration 34, loss = 0.13874533
Iteration 35, loss = 0.13050452
Iteration 36, loss = 0.12297885
Iteration 37, loss = 0.11594745
Iteration 38, loss = 0.10937995
Iteration 39, loss = 0.10335350
Iteration 40, loss = 0.09770467
Iteration 41, loss = 0.09245092
Iteration 42, loss = 0.08762831
Iteration 43, loss = 0.08303748
Iteration 44, loss = 0.07888718
Iteration 45, loss = 0.07496520
Iteration 46, loss = 0.07133948
Iteration 47, loss = 0.06797041
Iteration 48, loss = 0.06479645
Iteration 49, loss = 0.06182816
Iteration 50, loss = 0.05909350
Iteration 51, loss = 0.05647539
Iteration 52, loss = 0.05412134
Iteration 53, loss = 0.05185195
Iteration 54, loss = 0.04973922
Iteration 55, loss = 0.04771582
Iteration 56, loss = 0.04585636
Iteration 57, loss = 0.04408564
Iteration 58, loss = 0.04243410
Iteration 59, loss = 0.04087035
Iteration 60, loss = 0.03939452
Iteration 61, loss = 0.03799081
Iteration 62, loss = 0.03664373
Iteration 63, loss = 0.03539970
Iteration 64, loss = 0.03421702
Iteration 65, loss = 0.03307474
Iteration 66, loss = 0.03201009
Iteration 67, loss = 0.03100680
Iteration 68, loss = 0.03003775
Iteration 69, loss = 0.02912398
Iteration 70, loss = 0.02826100
Iteration 71, loss = 0.02742167
Iteration 72, loss = 0.02661507
Iteration 73, loss = 0.02585743
Iteration 74, loss = 0.02512016
Iteration 75, loss = 0.02442283
Iteration 76, loss = 0.02375372
Iteration 77, loss = 0.02312035
Iteration 78, loss = 0.02250946
Iteration 79, loss = 0.02192561
Iteration 80, loss = 0.02135921
Iteration 81, loss = 0.02082467
Iteration 82, loss = 0.02029924
Iteration 83, loss = 0.01978848
Iteration 84, loss = 0.01931701
Iteration 85, loss = 0.01885940
Iteration 86, loss = 0.01841751
Iteration 87, loss = 0.01798575
Iteration 88, loss = 0.01759201
Iteration 89, loss = 0.01717664
Iteration 90, loss = 0.01679601
Iteration 91, loss = 0.01641367
Iteration 92, loss = 0.01606203
Iteration 93, loss = 0.01571942
Iteration 94, loss = 0.01538346
Iteration 95, loss = 0.01507022
Iteration 96, loss = 0.01474833
Iteration 97, loss = 0.01444678
Iteration 98, loss = 0.01416251
Iteration 99, loss = 0.01388046
Iteration 100, loss = 0.01360855
Iteration 101, loss = 0.01334421
Iteration 102, loss = 0.01308066
Iteration 103, loss = 0.01283752
Iteration 104, loss = 0.01260090
Iteration 105, loss = 0.01236981
Iteration 106, loss = 0.01214861
Iteration 107, loss = 0.01192318
Iteration 108, loss = 0.01171010
Iteration 109, loss = 0.01149965
Iteration 110, loss = 0.01129808
Iteration 111, loss = 0.01110582
Iteration 112, loss = 0.01091320
Iteration 113, loss = 0.01073001
Iteration 114, loss = 0.01055435
Iteration 115, loss = 0.01038367
Iteration 116, loss = 0.01021984
Iteration 117, loss = 0.01004740
Iteration 118, loss = 0.00989269
Iteration 119, loss = 0.00973458
Iteration 120, loss = 0.00958292
Iteration 121, loss = 0.00943062
Iteration 122, loss = 0.00928625
Iteration 123, loss = 0.00914915
Iteration 124, loss = 0.00901403
Iteration 125, loss = 0.00887935
Iteration 126, loss = 0.00875000
Iteration 127, loss = 0.00862285
Iteration 128, loss = 0.00850813
Iteration 129, loss = 0.00838665
Iteration 130, loss = 0.00827341
Iteration 131, loss = 0.00815531
Iteration 132, loss = 0.00804600
Iteration 133, loss = 0.00793780
Iteration 134, loss = 0.00783364
Iteration 135, loss = 0.00772856
Iteration 136, loss = 0.00763081
Iteration 137, loss = 0.00752849
Iteration 138, loss = 0.00743555
Iteration 139, loss = 0.00734228
Iteration 140, loss = 0.00725057
Iteration 141, loss = 0.00716155
Iteration 142, loss = 0.00707082
Iteration 143, loss = 0.00699058
Iteration 144, loss = 0.00690533
Iteration 145, loss = 0.00681718
Iteration 146, loss = 0.00673596
Iteration 147, loss = 0.00665273
Iteration 148, loss = 0.00657716
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67919392
Iteration 2, loss = 0.62378050
Iteration 3, loss = 0.54616461
Iteration 4, loss = 0.45559720
Iteration 5, loss = 0.36821665
Iteration 6, loss = 0.29297014
Iteration 7, loss = 0.23183385
Iteration 8, loss = 0.18505455
Iteration 9, loss = 0.15006730
Iteration 10, loss = 0.12390860
Iteration 11, loss = 0.10408541
Iteration 12, loss = 0.08930164
Iteration 13, loss = 0.07778189
Iteration 14, loss = 0.06895066
Iteration 15, loss = 0.06173906
Iteration 16, loss = 0.05575438
Iteration 17, loss = 0.05119970
Iteration 18, loss = 0.04730193
Iteration 19, loss = 0.04396077
Iteration 20, loss = 0.04117732
Iteration 21, loss = 0.03887824
Iteration 22, loss = 0.03696917
Iteration 23, loss = 0.03520021
Iteration 24, loss = 0.03358397
Iteration 25, loss = 0.03238205
Iteration 26, loss = 0.03119186
Iteration 27, loss = 0.03010591
Iteration 28, loss = 0.02935836
Iteration 29, loss = 0.02832090
Iteration 30, loss = 0.02775027
Iteration 31, loss = 0.02713607
Iteration 32, loss = 0.02672441
Iteration 33, loss = 0.02605431
Iteration 34, loss = 0.02577444
Iteration 35, loss = 0.02500079
Iteration 36, loss = 0.02454484
Iteration 37, loss = 0.02448206
Iteration 38, loss = 0.02394298
Iteration 39, loss = 0.02362992
Iteration 40, loss = 0.02321854
Iteration 41, loss = 0.02319286
Iteration 42, loss = 0.02274236
Iteration 43, loss = 0.02249227
Iteration 44, loss = 0.02249065
Iteration 45, loss = 0.02206907
Iteration 46, loss = 0.02212775
Iteration 47, loss = 0.02165751
Iteration 48, loss = 0.02159711
Iteration 49, loss = 0.02141248
Iteration 50, loss = 0.02145237
Iteration 51, loss = 0.02117726
Iteration 52, loss = 0.02110811
Iteration 53, loss = 0.02091136
Iteration 54, loss = 0.02102967
Iteration 55, loss = 0.02083261
Iteration 56, loss = 0.02073808
Iteration 57, loss = 0.02043311
Iteration 58, loss = 0.02053251
Iteration 59, loss = 0.02044076
Iteration 60, loss = 0.02026800
Iteration 61, loss = 0.02046113
Iteration 62, loss = 0.02029964
Iteration 63, loss = 0.02033869
Iteration 64, loss = 0.01993567
Iteration 65, loss = 0.02002295
Iteration 66, loss = 0.01999969
Iteration 67, loss = 0.02002072
Iteration 68, loss = 0.01990592
Iteration 69, loss = 0.01987427
Iteration 70, loss = 0.01961917
Iteration 71, loss = 0.01954603
Iteration 72, loss = 0.01970789
Iteration 73, loss = 0.01988826
Iteration 74, loss = 0.01952575
Iteration 75, loss = 0.01949099
Iteration 76, loss = 0.01935409
Iteration 77, loss = 0.01996572
Iteration 78, loss = 0.01949801
Iteration 79, loss = 0.01929557
Iteration 80, loss = 0.01927423
Iteration 81, loss = 0.01923543
Iteration 82, loss = 0.01937479
Iteration 83, loss = 0.01945939
Iteration 84, loss = 0.01924392
Iteration 85, loss = 0.01949055
Iteration 86, loss = 0.01910931
Iteration 87, loss = 0.01914908
Iteration 88, loss = 0.01949480
Iteration 89, loss = 0.01898699
Iteration 90, loss = 0.01909862
Iteration 91, loss = 0.01928283
Iteration 92, loss = 0.01894668
Iteration 93, loss = 0.01900341
Iteration 94, loss = 0.01888247
Iteration 95, loss = 0.01890680
Iteration 96, loss = 0.01914813
Iteration 97, loss = 0.01883240
Iteration 98, loss = 0.01908407
Iteration 99, loss = 0.01872450
Iteration 100, loss = 0.01892383
Iteration 101, loss = 0.01884836
Iteration 102, loss = 0.01872090
Iteration 103, loss = 0.01870943
Iteration 104, loss = 0.01898105
Iteration 105, loss = 0.01878414
Iteration 106, loss = 0.01866678
Iteration 107, loss = 0.01895818
Iteration 108, loss = 0.01867445
Iteration 109, loss = 0.01874643
Iteration 110, loss = 0.01874584
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72296995
Iteration 2, loss = 0.70510845
Iteration 3, loss = 0.68769848
Iteration 4, loss = 0.66646223
Iteration 5, loss = 0.64086535
Iteration 6, loss = 0.61070506
Iteration 7, loss = 0.57680319
Iteration 8, loss = 0.54025533
Iteration 9, loss = 0.50205495
Iteration 10, loss = 0.46337382
Iteration 11, loss = 0.42509520
Iteration 12, loss = 0.38788729
Iteration 13, loss = 0.35274561
Iteration 14, loss = 0.31989622
Iteration 15, loss = 0.28961961
Iteration 16, loss = 0.26206766
Iteration 17, loss = 0.23700672
Iteration 18, loss = 0.21461531
Iteration 19, loss = 0.19457905
Iteration 20, loss = 0.17674875
Iteration 21, loss = 0.16089527
Iteration 22, loss = 0.14672732
Iteration 23, loss = 0.13434125
Iteration 24, loss = 0.12334740
Iteration 25, loss = 0.11366339
Iteration 26, loss = 0.10505032
Iteration 27, loss = 0.09734000
Iteration 28, loss = 0.09049850
Iteration 29, loss = 0.08443002
Iteration 30, loss = 0.07899942
Iteration 31, loss = 0.07413380
Iteration 32, loss = 0.06971067
Iteration 33, loss = 0.06574326
Iteration 34, loss = 0.06216098
Iteration 35, loss = 0.05889119
Iteration 36, loss = 0.05596031
Iteration 37, loss = 0.05322505
Iteration 38, loss = 0.05079344
Iteration 39, loss = 0.04851862
Iteration 40, loss = 0.04645433
Iteration 41, loss = 0.04459574
Iteration 42, loss = 0.04293464
Iteration 43, loss = 0.04120214
Iteration 44, loss = 0.03968992
Iteration 45, loss = 0.03834765
Iteration 46, loss = 0.03708366
Iteration 47, loss = 0.03585435
Iteration 48, loss = 0.03475173
Iteration 49, loss = 0.03371012
Iteration 50, loss = 0.03269133
Iteration 51, loss = 0.03180974
Iteration 52, loss = 0.03097696
Iteration 53, loss = 0.03021436
Iteration 54, loss = 0.02941206
Iteration 55, loss = 0.02875954
Iteration 56, loss = 0.02805513
Iteration 57, loss = 0.02740479
Iteration 58, loss = 0.02685987
Iteration 59, loss = 0.02630153
Iteration 60, loss = 0.02574388
Iteration 61, loss = 0.02524610
Iteration 62, loss = 0.02480400
Iteration 63, loss = 0.02431799
Iteration 64, loss = 0.02388501
Iteration 65, loss = 0.02348585
Iteration 66, loss = 0.02311473
Iteration 67, loss = 0.02274195
Iteration 68, loss = 0.02237476
Iteration 69, loss = 0.02207580
Iteration 70, loss = 0.02178186
Iteration 71, loss = 0.02138767
Iteration 72, loss = 0.02110641
Iteration 73, loss = 0.02084879
Iteration 74, loss = 0.02054768
Iteration 75, loss = 0.02029533
Iteration 76, loss = 0.02003448
Iteration 77, loss = 0.01982465
Iteration 78, loss = 0.01961661
Iteration 79, loss = 0.01936705
Iteration 80, loss = 0.01916757
Iteration 81, loss = 0.01903716
Iteration 82, loss = 0.01876363
Iteration 83, loss = 0.01860290
Iteration 84, loss = 0.01842404
Iteration 85, loss = 0.01828856
Iteration 86, loss = 0.01815117
Iteration 87, loss = 0.01797175
Iteration 88, loss = 0.01777343
Iteration 89, loss = 0.01764379
Iteration 90, loss = 0.01749329
Iteration 91, loss = 0.01733174
Iteration 92, loss = 0.01725558
Iteration 93, loss = 0.01710860
Iteration 94, loss = 0.01698714
Iteration 95, loss = 0.01685423
Iteration 96, loss = 0.01674287
Iteration 97, loss = 0.01661765
Iteration 98, loss = 0.01649129
Iteration 99, loss = 0.01640658
Iteration 100, loss = 0.01639484
Iteration 101, loss = 0.01619077
Iteration 102, loss = 0.01612795
Iteration 103, loss = 0.01602812
Iteration 104, loss = 0.01597448
Iteration 105, loss = 0.01584006
Iteration 106, loss = 0.01575291
Iteration 107, loss = 0.01577654
Iteration 108, loss = 0.01565942
Iteration 109, loss = 0.01551409
Iteration 110, loss = 0.01550296
Iteration 111, loss = 0.01542925
Iteration 112, loss = 0.01532219
Iteration 113, loss = 0.01525692
Iteration 114, loss = 0.01513910
Iteration 115, loss = 0.01516502
Iteration 116, loss = 0.01503859
Iteration 117, loss = 0.01499961
Iteration 118, loss = 0.01493536
Iteration 119, loss = 0.01485752
Iteration 120, loss = 0.01485076
Iteration 121, loss = 0.01474196
Iteration 122, loss = 0.01471918
Iteration 123, loss = 0.01470368
Iteration 124, loss = 0.01459175
Iteration 125, loss = 0.01451499
Iteration 126, loss = 0.01450057
Iteration 127, loss = 0.01451943
Iteration 128, loss = 0.01439566
Iteration 129, loss = 0.01440567
Iteration 130, loss = 0.01432418
Iteration 131, loss = 0.01427558
Iteration 132, loss = 0.01420293
Iteration 133, loss = 0.01417014
Iteration 134, loss = 0.01414972
Iteration 135, loss = 0.01409827
Iteration 136, loss = 0.01407246
Iteration 137, loss = 0.01409354
Iteration 138, loss = 0.01401946
Iteration 139, loss = 0.01396400
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69611798
Iteration 2, loss = 0.67416088
Iteration 3, loss = 0.64721957
Iteration 4, loss = 0.61301740
Iteration 5, loss = 0.57188784
Iteration 6, loss = 0.52539198
Iteration 7, loss = 0.47693504
Iteration 8, loss = 0.42836200
Iteration 9, loss = 0.38154055
Iteration 10, loss = 0.33764034
Iteration 11, loss = 0.29745726
Iteration 12, loss = 0.26134339
Iteration 13, loss = 0.22984484
Iteration 14, loss = 0.20221056
Iteration 15, loss = 0.17854842
Iteration 16, loss = 0.15821236
Iteration 17, loss = 0.14080799
Iteration 18, loss = 0.12603536
Iteration 19, loss = 0.11362987
Iteration 20, loss = 0.10269445
Iteration 21, loss = 0.09356129
Iteration 22, loss = 0.08557281
Iteration 23, loss = 0.07871396
Iteration 24, loss = 0.07293225
Iteration 25, loss = 0.06768096
Iteration 26, loss = 0.06302933
Iteration 27, loss = 0.05910665
Iteration 28, loss = 0.05570050
Iteration 29, loss = 0.05231737
Iteration 30, loss = 0.04958474
Iteration 31, loss = 0.04685348
Iteration 32, loss = 0.04462322
Iteration 33, loss = 0.04253515
Iteration 34, loss = 0.04073795
Iteration 35, loss = 0.03903433
Iteration 36, loss = 0.03753941
Iteration 37, loss = 0.03610887
Iteration 38, loss = 0.03479136
Iteration 39, loss = 0.03368445
Iteration 40, loss = 0.03254434
Iteration 41, loss = 0.03159802
Iteration 42, loss = 0.03070838
Iteration 43, loss = 0.02983741
Iteration 44, loss = 0.02912503
Iteration 45, loss = 0.02832011
Iteration 46, loss = 0.02764964
Iteration 47, loss = 0.02699562
Iteration 48, loss = 0.02642392
Iteration 49, loss = 0.02590115
Iteration 50, loss = 0.02538840
Iteration 51, loss = 0.02492526
Iteration 52, loss = 0.02445421
Iteration 53, loss = 0.02421302
Iteration 54, loss = 0.02391327
Iteration 55, loss = 0.02357486
Iteration 56, loss = 0.02334842
Iteration 57, loss = 0.02294334
Iteration 58, loss = 0.02266357
Iteration 59, loss = 0.02233383
Iteration 60, loss = 0.02196972
Iteration 61, loss = 0.02167082
Iteration 62, loss = 0.02130938
Iteration 63, loss = 0.02095445
Iteration 64, loss = 0.02097859
Iteration 65, loss = 0.02066485
Iteration 66, loss = 0.02062596
Iteration 67, loss = 0.02029131
Iteration 68, loss = 0.02014452
Iteration 69, loss = 0.01980659
Iteration 70, loss = 0.01956206
Iteration 71, loss = 0.01942959
Iteration 72, loss = 0.01906529
Iteration 73, loss = 0.01913889
Iteration 74, loss = 0.01906232
Iteration 75, loss = 0.01917311
Iteration 76, loss = 0.01903860
Iteration 77, loss = 0.01893537
Iteration 78, loss = 0.01870919
Iteration 79, loss = 0.01872161
Iteration 80, loss = 0.01854265
Iteration 81, loss = 0.01825784
Iteration 82, loss = 0.01809789
Iteration 83, loss = 0.01782399
Iteration 84, loss = 0.01786574
Iteration 85, loss = 0.01765017
Iteration 86, loss = 0.01775863
Iteration 87, loss = 0.01764048
Iteration 88, loss = 0.01756212
Iteration 89, loss = 0.01745978
Iteration 90, loss = 0.01740461
Iteration 91, loss = 0.01725069
Iteration 92, loss = 0.01708637
Iteration 93, loss = 0.01710351
Iteration 94, loss = 0.01704774
Iteration 95, loss = 0.01710241
Iteration 96, loss = 0.01686909
Iteration 97, loss = 0.01682263
Iteration 98, loss = 0.01681453
Iteration 99, loss = 0.01670568
Iteration 100, loss = 0.01661962
Iteration 101, loss = 0.01657595
Iteration 102, loss = 0.01668655
Iteration 103, loss = 0.01667343
Iteration 104, loss = 0.01644987
Iteration 105, loss = 0.01638109
Iteration 106, loss = 0.01632730
Iteration 107, loss = 0.01601749
Iteration 108, loss = 0.01641915
Iteration 109, loss = 0.01645389
Iteration 110, loss = 0.01630078
Iteration 111, loss = 0.01606444
Iteration 112, loss = 0.01596797
Iteration 113, loss = 0.01599203
Iteration 114, loss = 0.01594746
Iteration 115, loss = 0.01590452
Iteration 116, loss = 0.01580387
Iteration 117, loss = 0.01580205
Iteration 118, loss = 0.01566260
Iteration 119, loss = 0.01572611
Iteration 120, loss = 0.01577904
Iteration 121, loss = 0.01580822
Iteration 122, loss = 0.01579505
Iteration 123, loss = 0.01582043
Iteration 124, loss = 0.01587500
Iteration 125, loss = 0.01591277
Iteration 126, loss = 0.01575848
Iteration 127, loss = 0.01567051
Iteration 128, loss = 0.01557109
Iteration 129, loss = 0.01548823
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69087930
Iteration 2, loss = 0.66751913
Iteration 3, loss = 0.64373222
Iteration 4, loss = 0.61677579
Iteration 5, loss = 0.58595444
Iteration 6, loss = 0.55280438
Iteration 7, loss = 0.51793939
Iteration 8, loss = 0.48227787
Iteration 9, loss = 0.44685316
Iteration 10, loss = 0.41238151
Iteration 11, loss = 0.37952310
Iteration 12, loss = 0.34832341
Iteration 13, loss = 0.31917611
Iteration 14, loss = 0.29213298
Iteration 15, loss = 0.26726226
Iteration 16, loss = 0.24440456
Iteration 17, loss = 0.22359711
Iteration 18, loss = 0.20461950
Iteration 19, loss = 0.18755624
Iteration 20, loss = 0.17200693
Iteration 21, loss = 0.15806953
Iteration 22, loss = 0.14558099
Iteration 23, loss = 0.13433887
Iteration 24, loss = 0.12416639
Iteration 25, loss = 0.11514142
Iteration 26, loss = 0.10697794
Iteration 27, loss = 0.09959543
Iteration 28, loss = 0.09307188
Iteration 29, loss = 0.08718104
Iteration 30, loss = 0.08176006
Iteration 31, loss = 0.07697882
Iteration 32, loss = 0.07252273
Iteration 33, loss = 0.06855249
Iteration 34, loss = 0.06496210
Iteration 35, loss = 0.06164649
Iteration 36, loss = 0.05863756
Iteration 37, loss = 0.05588748
Iteration 38, loss = 0.05333420
Iteration 39, loss = 0.05103691
Iteration 40, loss = 0.04892557
Iteration 41, loss = 0.04693827
Iteration 42, loss = 0.04512308
Iteration 43, loss = 0.04347063
Iteration 44, loss = 0.04192206
Iteration 45, loss = 0.04050253
Iteration 46, loss = 0.03909043
Iteration 47, loss = 0.03785793
Iteration 48, loss = 0.03668657
Iteration 49, loss = 0.03558184
Iteration 50, loss = 0.03455925
Iteration 51, loss = 0.03358319
Iteration 52, loss = 0.03270007
Iteration 53, loss = 0.03184643
Iteration 54, loss = 0.03108247
Iteration 55, loss = 0.03030299
Iteration 56, loss = 0.02963037
Iteration 57, loss = 0.02894938
Iteration 58, loss = 0.02829742
Iteration 59, loss = 0.02773377
Iteration 60, loss = 0.02719536
Iteration 61, loss = 0.02664305
Iteration 62, loss = 0.02618139
Iteration 63, loss = 0.02566956
Iteration 64, loss = 0.02519270
Iteration 65, loss = 0.02477184
Iteration 66, loss = 0.02435931
Iteration 67, loss = 0.02393802
Iteration 68, loss = 0.02361313
Iteration 69, loss = 0.02321672
Iteration 70, loss = 0.02290644
Iteration 71, loss = 0.02257329
Iteration 72, loss = 0.02224911
Iteration 73, loss = 0.02196333
Iteration 74, loss = 0.02163536
Iteration 75, loss = 0.02140386
Iteration 76, loss = 0.02113840
Iteration 77, loss = 0.02087469
Iteration 78, loss = 0.02063840
Iteration 79, loss = 0.02041392
Iteration 80, loss = 0.02020050
Iteration 81, loss = 0.01996369
Iteration 82, loss = 0.01976239
Iteration 83, loss = 0.01959919
Iteration 84, loss = 0.01943694
Iteration 85, loss = 0.01922608
Iteration 86, loss = 0.01906419
Iteration 87, loss = 0.01885412
Iteration 88, loss = 0.01875231
Iteration 89, loss = 0.01855623
Iteration 90, loss = 0.01844654
Iteration 91, loss = 0.01824534
Iteration 92, loss = 0.01809186
Iteration 93, loss = 0.01800919
Iteration 94, loss = 0.01782488
Iteration 95, loss = 0.01771241
Iteration 96, loss = 0.01756362
Iteration 97, loss = 0.01748194
Iteration 98, loss = 0.01732138
Iteration 99, loss = 0.01722848
Iteration 100, loss = 0.01714053
Iteration 101, loss = 0.01704401
Iteration 102, loss = 0.01694171
Iteration 103, loss = 0.01683376
Iteration 104, loss = 0.01673959
Iteration 105, loss = 0.01664304
Iteration 106, loss = 0.01656080
Iteration 107, loss = 0.01651874
Iteration 108, loss = 0.01637608
Iteration 109, loss = 0.01634189
Iteration 110, loss = 0.01620013
Iteration 111, loss = 0.01615497
Iteration 112, loss = 0.01607372
Iteration 113, loss = 0.01595639
Iteration 114, loss = 0.01590919
Iteration 115, loss = 0.01582433
Iteration 116, loss = 0.01579770
Iteration 117, loss = 0.01572277
Iteration 118, loss = 0.01563537
Iteration 119, loss = 0.01560881
Iteration 120, loss = 0.01551852
Iteration 121, loss = 0.01548447
Iteration 122, loss = 0.01540224
Iteration 123, loss = 0.01532379
Iteration 124, loss = 0.01530308
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72657475
Iteration 2, loss = 0.68619179
Iteration 3, loss = 0.63365304
Iteration 4, loss = 0.57024507
Iteration 5, loss = 0.49804104
Iteration 6, loss = 0.42464408
Iteration 7, loss = 0.35536080
Iteration 8, loss = 0.29456080
Iteration 9, loss = 0.24382445
Iteration 10, loss = 0.20220114
Iteration 11, loss = 0.16913666
Iteration 12, loss = 0.14291352
Iteration 13, loss = 0.12204253
Iteration 14, loss = 0.10563241
Iteration 15, loss = 0.09236868
Iteration 16, loss = 0.08181468
Iteration 17, loss = 0.07311768
Iteration 18, loss = 0.06586185
Iteration 19, loss = 0.05979668
Iteration 20, loss = 0.05490226
Iteration 21, loss = 0.05048112
Iteration 22, loss = 0.04687296
Iteration 23, loss = 0.04374069
Iteration 24, loss = 0.04105417
Iteration 25, loss = 0.03866074
Iteration 26, loss = 0.03656879
Iteration 27, loss = 0.03478167
Iteration 28, loss = 0.03313990
Iteration 29, loss = 0.03167506
Iteration 30, loss = 0.03042425
Iteration 31, loss = 0.02933272
Iteration 32, loss = 0.02829239
Iteration 33, loss = 0.02726360
Iteration 34, loss = 0.02644378
Iteration 35, loss = 0.02562349
Iteration 36, loss = 0.02492609
Iteration 37, loss = 0.02424730
Iteration 38, loss = 0.02367806
Iteration 39, loss = 0.02315619
Iteration 40, loss = 0.02265561
Iteration 41, loss = 0.02230811
Iteration 42, loss = 0.02179841
Iteration 43, loss = 0.02137387
Iteration 44, loss = 0.02105133
Iteration 45, loss = 0.02075765
Iteration 46, loss = 0.02046846
Iteration 47, loss = 0.02016186
Iteration 48, loss = 0.01975022
Iteration 49, loss = 0.01958284
Iteration 50, loss = 0.01954967
Iteration 51, loss = 0.01917487
Iteration 52, loss = 0.01885760
Iteration 53, loss = 0.01870196
Iteration 54, loss = 0.01850775
Iteration 55, loss = 0.01847417
Iteration 56, loss = 0.01813497
Iteration 57, loss = 0.01799154
Iteration 58, loss = 0.01771626
Iteration 59, loss = 0.01777345
Iteration 60, loss = 0.01750684
Iteration 61, loss = 0.01742355
Iteration 62, loss = 0.01725992
Iteration 63, loss = 0.01730751
Iteration 64, loss = 0.01707165
Iteration 65, loss = 0.01694019
Iteration 66, loss = 0.01690075
Iteration 67, loss = 0.01679662
Iteration 68, loss = 0.01674743
Iteration 69, loss = 0.01655915
Iteration 70, loss = 0.01651151
Iteration 71, loss = 0.01641553
Iteration 72, loss = 0.01634358
Iteration 73, loss = 0.01633462
Iteration 74, loss = 0.01623268
Iteration 75, loss = 0.01614183
Iteration 76, loss = 0.01633669
Iteration 77, loss = 0.01619638
Iteration 78, loss = 0.01598364
Iteration 79, loss = 0.01608623
Iteration 80, loss = 0.01602075
Iteration 81, loss = 0.01590016
Iteration 82, loss = 0.01571766
Iteration 83, loss = 0.01573570
Iteration 84, loss = 0.01565406
Iteration 85, loss = 0.01573039
Iteration 86, loss = 0.01565371
Iteration 87, loss = 0.01564637
Iteration 88, loss = 0.01559144
Iteration 89, loss = 0.01556153
Iteration 90, loss = 0.01542763
Iteration 91, loss = 0.01556892
Iteration 92, loss = 0.01525226
Iteration 93, loss = 0.01542193
Iteration 94, loss = 0.01541426
Iteration 95, loss = 0.01529348
Iteration 96, loss = 0.01531187
Iteration 97, loss = 0.01522461
Iteration 98, loss = 0.01532901
Iteration 99, loss = 0.01520901
Iteration 100, loss = 0.01515406
Iteration 101, loss = 0.01515078
Iteration 102, loss = 0.01527220
Iteration 103, loss = 0.01503085
Iteration 104, loss = 0.01503300
Iteration 105, loss = 0.01523050
Iteration 106, loss = 0.01495540
Iteration 107, loss = 0.01495542
Iteration 108, loss = 0.01492417
Iteration 109, loss = 0.01497871
Iteration 110, loss = 0.01493184
Iteration 111, loss = 0.01494447
Iteration 112, loss = 0.01494692
Iteration 113, loss = 0.01497446
Iteration 114, loss = 0.01487569
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68754195
Iteration 2, loss = 0.63165974
Iteration 3, loss = 0.55229453
Iteration 4, loss = 0.45749236
Iteration 5, loss = 0.36881807
Iteration 6, loss = 0.29277543
Iteration 7, loss = 0.23206913
Iteration 8, loss = 0.18580474
Iteration 9, loss = 0.15104195
Iteration 10, loss = 0.12545776
Iteration 11, loss = 0.10576855
Iteration 12, loss = 0.09112311
Iteration 13, loss = 0.07955535
Iteration 14, loss = 0.07056516
Iteration 15, loss = 0.06342298
Iteration 16, loss = 0.05753845
Iteration 17, loss = 0.05293356
Iteration 18, loss = 0.04894648
Iteration 19, loss = 0.04568403
Iteration 20, loss = 0.04303092
Iteration 21, loss = 0.04070561
Iteration 22, loss = 0.03873949
Iteration 23, loss = 0.03695877
Iteration 24, loss = 0.03549282
Iteration 25, loss = 0.03405981
Iteration 26, loss = 0.03294651
Iteration 27, loss = 0.03184721
Iteration 28, loss = 0.03093813
Iteration 29, loss = 0.03024993
Iteration 30, loss = 0.02933124
Iteration 31, loss = 0.02871071
Iteration 32, loss = 0.02805867
Iteration 33, loss = 0.02755987
Iteration 34, loss = 0.02726819
Iteration 35, loss = 0.02663044
Iteration 36, loss = 0.02618203
Iteration 37, loss = 0.02603461
Iteration 38, loss = 0.02578137
Iteration 39, loss = 0.02536086
Iteration 40, loss = 0.02463970
Iteration 41, loss = 0.02475548
Iteration 42, loss = 0.02447578
Iteration 43, loss = 0.02442192
Iteration 44, loss = 0.02415466
Iteration 45, loss = 0.02384282
Iteration 46, loss = 0.02361679
Iteration 47, loss = 0.02363777
Iteration 48, loss = 0.02317758
Iteration 49, loss = 0.02298433
Iteration 50, loss = 0.02291486
Iteration 51, loss = 0.02270630
Iteration 52, loss = 0.02291536
Iteration 53, loss = 0.02247417
Iteration 54, loss = 0.02252213
Iteration 55, loss = 0.02220253
Iteration 56, loss = 0.02216854
Iteration 57, loss = 0.02211966
Iteration 58, loss = 0.02205578
Iteration 59, loss = 0.02194310
Iteration 60, loss = 0.02186832
Iteration 61, loss = 0.02191262
Iteration 62, loss = 0.02182138
Iteration 63, loss = 0.02181453
Iteration 64, loss = 0.02175150
Iteration 65, loss = 0.02162740
Iteration 66, loss = 0.02146915
Iteration 67, loss = 0.02131671
Iteration 68, loss = 0.02154751
Iteration 69, loss = 0.02132802
Iteration 70, loss = 0.02146933
Iteration 71, loss = 0.02161625
Iteration 72, loss = 0.02115708
Iteration 73, loss = 0.02123995
Iteration 74, loss = 0.02144214
Iteration 75, loss = 0.02109226
Iteration 76, loss = 0.02111476
Iteration 77, loss = 0.02083803
Iteration 78, loss = 0.02087651
Iteration 79, loss = 0.02097768
Iteration 80, loss = 0.02082978
Iteration 81, loss = 0.02093592
Iteration 82, loss = 0.02083995
Iteration 83, loss = 0.02098403
Iteration 84, loss = 0.02072528
Iteration 85, loss = 0.02082839
Iteration 86, loss = 0.02082218
Iteration 87, loss = 0.02066064
Iteration 88, loss = 0.02056611
Iteration 89, loss = 0.02064580
Iteration 90, loss = 0.02067351
Iteration 91, loss = 0.02069307
Iteration 92, loss = 0.02051522
Iteration 93, loss = 0.02054932
Iteration 94, loss = 0.02047609
Iteration 95, loss = 0.02072044
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68467741
Iteration 2, loss = 0.64465138
Iteration 3, loss = 0.58176364
Iteration 4, loss = 0.50096803
Iteration 5, loss = 0.42032565
Iteration 6, loss = 0.34504169
Iteration 7, loss = 0.27997293
Iteration 8, loss = 0.22706428
Iteration 9, loss = 0.18543987
Iteration 10, loss = 0.15329850
Iteration 11, loss = 0.12900710
Iteration 12, loss = 0.10996968
Iteration 13, loss = 0.09532056
Iteration 14, loss = 0.08395501
Iteration 15, loss = 0.07484972
Iteration 16, loss = 0.06758282
Iteration 17, loss = 0.06150847
Iteration 18, loss = 0.05643013
Iteration 19, loss = 0.05217128
Iteration 20, loss = 0.04880859
Iteration 21, loss = 0.04599926
Iteration 22, loss = 0.04325931
Iteration 23, loss = 0.04129684
Iteration 24, loss = 0.03930710
Iteration 25, loss = 0.03756318
Iteration 26, loss = 0.03618299
Iteration 27, loss = 0.03519204
Iteration 28, loss = 0.03378552
Iteration 29, loss = 0.03280993
Iteration 30, loss = 0.03179581
Iteration 31, loss = 0.03095781
Iteration 32, loss = 0.03023668
Iteration 33, loss = 0.02947338
Iteration 34, loss = 0.02890715
Iteration 35, loss = 0.02844394
Iteration 36, loss = 0.02791949
Iteration 37, loss = 0.02744464
Iteration 38, loss = 0.02691311
Iteration 39, loss = 0.02658230
Iteration 40, loss = 0.02623433
Iteration 41, loss = 0.02588248
Iteration 42, loss = 0.02546171
Iteration 43, loss = 0.02522757
Iteration 44, loss = 0.02493900
Iteration 45, loss = 0.02477939
Iteration 46, loss = 0.02455330
Iteration 47, loss = 0.02437316
Iteration 48, loss = 0.02415291
Iteration 49, loss = 0.02397522
Iteration 50, loss = 0.02388911
Iteration 51, loss = 0.02374144
Iteration 52, loss = 0.02365356
Iteration 53, loss = 0.02326301
Iteration 54, loss = 0.02334673
Iteration 55, loss = 0.02317036
Iteration 56, loss = 0.02305537
Iteration 57, loss = 0.02279021
Iteration 58, loss = 0.02248864
Iteration 59, loss = 0.02241311
Iteration 60, loss = 0.02232689
Iteration 61, loss = 0.02221924
Iteration 62, loss = 0.02235908
Iteration 63, loss = 0.02202807
Iteration 64, loss = 0.02202949
Iteration 65, loss = 0.02201757
Iteration 66, loss = 0.02192741
Iteration 67, loss = 0.02188251
Iteration 68, loss = 0.02177303
Iteration 69, loss = 0.02170839
Iteration 70, loss = 0.02156087
Iteration 71, loss = 0.02161528
Iteration 72, loss = 0.02138709
Iteration 73, loss = 0.02144470
Iteration 74, loss = 0.02146803
Iteration 75, loss = 0.02144476
Iteration 76, loss = 0.02151804
Iteration 77, loss = 0.02154498
Iteration 78, loss = 0.02115616
Iteration 79, loss = 0.02132637
Iteration 80, loss = 0.02127488
Iteration 81, loss = 0.02122430
Iteration 82, loss = 0.02100846
Iteration 83, loss = 0.02091713
Iteration 84, loss = 0.02119553
Iteration 85, loss = 0.02093630
Iteration 86, loss = 0.02129927
Iteration 87, loss = 0.02097969
Iteration 88, loss = 0.02089395
Iteration 89, loss = 0.02059957
Iteration 90, loss = 0.02078219
Iteration 91, loss = 0.02093776
Iteration 92, loss = 0.02082020
Iteration 93, loss = 0.02086015
Iteration 94, loss = 0.02067207
Iteration 95, loss = 0.02073854
Iteration 96, loss = 0.02078213
Iteration 97, loss = 0.02056817
Iteration 98, loss = 0.02051001
Iteration 99, loss = 0.02067187
Iteration 100, loss = 0.02082639
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67714266
Iteration 2, loss = 0.64857265
Iteration 3, loss = 0.61604553
Iteration 4, loss = 0.57535594
Iteration 5, loss = 0.52815084
Iteration 6, loss = 0.47849297
Iteration 7, loss = 0.42818440
Iteration 8, loss = 0.37970064
Iteration 9, loss = 0.33438671
Iteration 10, loss = 0.29325073
Iteration 11, loss = 0.25681511
Iteration 12, loss = 0.22532592
Iteration 13, loss = 0.19788434
Iteration 14, loss = 0.17456130
Iteration 15, loss = 0.15485452
Iteration 16, loss = 0.13815992
Iteration 17, loss = 0.12397714
Iteration 18, loss = 0.11178378
Iteration 19, loss = 0.10128307
Iteration 20, loss = 0.09248379
Iteration 21, loss = 0.08472505
Iteration 22, loss = 0.07817915
Iteration 23, loss = 0.07234856
Iteration 24, loss = 0.06713177
Iteration 25, loss = 0.06290375
Iteration 26, loss = 0.05904701
Iteration 27, loss = 0.05538299
Iteration 28, loss = 0.05227594
Iteration 29, loss = 0.04951577
Iteration 30, loss = 0.04702276
Iteration 31, loss = 0.04458893
Iteration 32, loss = 0.04265078
Iteration 33, loss = 0.04088846
Iteration 34, loss = 0.03920580
Iteration 35, loss = 0.03783375
Iteration 36, loss = 0.03635504
Iteration 37, loss = 0.03506222
Iteration 38, loss = 0.03388799
Iteration 39, loss = 0.03279533
Iteration 40, loss = 0.03163591
Iteration 41, loss = 0.03076999
Iteration 42, loss = 0.02997249
Iteration 43, loss = 0.02922455
Iteration 44, loss = 0.02861575
Iteration 45, loss = 0.02804539
Iteration 46, loss = 0.02742769
Iteration 47, loss = 0.02664761
Iteration 48, loss = 0.02586980
Iteration 49, loss = 0.02538204
Iteration 50, loss = 0.02488643
Iteration 51, loss = 0.02438886
Iteration 52, loss = 0.02399270
Iteration 53, loss = 0.02357011
Iteration 54, loss = 0.02317124
Iteration 55, loss = 0.02279106
Iteration 56, loss = 0.02247214
Iteration 57, loss = 0.02216727
Iteration 58, loss = 0.02182051
Iteration 59, loss = 0.02155545
Iteration 60, loss = 0.02132003
Iteration 61, loss = 0.02102004
Iteration 62, loss = 0.02074821
Iteration 63, loss = 0.02053373
Iteration 64, loss = 0.02035928
Iteration 65, loss = 0.02012170
Iteration 66, loss = 0.02022475
Iteration 67, loss = 0.01991418
Iteration 68, loss = 0.01979059
Iteration 69, loss = 0.01960432
Iteration 70, loss = 0.01933709
Iteration 71, loss = 0.01929800
Iteration 72, loss = 0.01884625
Iteration 73, loss = 0.01915826
Iteration 74, loss = 0.01919818
Iteration 75, loss = 0.01860306
Iteration 76, loss = 0.01847510
Iteration 77, loss = 0.01820211
Iteration 78, loss = 0.01812047
Iteration 79, loss = 0.01797511
Iteration 80, loss = 0.01785883
Iteration 81, loss = 0.01775310
Iteration 82, loss = 0.01772468
Iteration 83, loss = 0.01750629
Iteration 84, loss = 0.01746496
Iteration 85, loss = 0.01739513
Iteration 86, loss = 0.01745329
Iteration 87, loss = 0.01737246
Iteration 88, loss = 0.01720899
Iteration 89, loss = 0.01700188
Iteration 90, loss = 0.01727138
Iteration 91, loss = 0.01732486
Iteration 92, loss = 0.01698681
Iteration 93, loss = 0.01679986
Iteration 94, loss = 0.01665598
Iteration 95, loss = 0.01664237
Iteration 96, loss = 0.01646629
Iteration 97, loss = 0.01641776
Iteration 98, loss = 0.01674171
Iteration 99, loss = 0.01667603
Iteration 100, loss = 0.01650790
Iteration 101, loss = 0.01638776
Iteration 102, loss = 0.01625788
Iteration 103, loss = 0.01610629
Iteration 104, loss = 0.01583706
Iteration 105, loss = 0.01622646
Iteration 106, loss = 0.01622550
Iteration 107, loss = 0.01602350
Iteration 108, loss = 0.01616347
Iteration 109, loss = 0.01597479
Iteration 110, loss = 0.01595052
Iteration 111, loss = 0.01594569
Iteration 112, loss = 0.01598117
Iteration 113, loss = 0.01606058
Iteration 114, loss = 0.01595399
Iteration 115, loss = 0.01623519
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70725108
Iteration 2, loss = 0.63674913
Iteration 3, loss = 0.54996673
Iteration 4, loss = 0.45761220
Iteration 5, loss = 0.36994865
Iteration 6, loss = 0.29579581
Iteration 7, loss = 0.23610161
Iteration 8, loss = 0.18998135
Iteration 9, loss = 0.15467302
Iteration 10, loss = 0.12822679
Iteration 11, loss = 0.10814450
Iteration 12, loss = 0.09274864
Iteration 13, loss = 0.08069662
Iteration 14, loss = 0.07131996
Iteration 15, loss = 0.06368502
Iteration 16, loss = 0.05758591
Iteration 17, loss = 0.05276981
Iteration 18, loss = 0.04857245
Iteration 19, loss = 0.04509372
Iteration 20, loss = 0.04215444
Iteration 21, loss = 0.03991307
Iteration 22, loss = 0.03763704
Iteration 23, loss = 0.03594237
Iteration 24, loss = 0.03438982
Iteration 25, loss = 0.03292444
Iteration 26, loss = 0.03167190
Iteration 27, loss = 0.03062554
Iteration 28, loss = 0.02964672
Iteration 29, loss = 0.02887745
Iteration 30, loss = 0.02828736
Iteration 31, loss = 0.02744741
Iteration 32, loss = 0.02683379
Iteration 33, loss = 0.02619180
Iteration 34, loss = 0.02577440
Iteration 35, loss = 0.02531457
Iteration 36, loss = 0.02503435
Iteration 37, loss = 0.02451032
Iteration 38, loss = 0.02425824
Iteration 39, loss = 0.02370656
Iteration 40, loss = 0.02362415
Iteration 41, loss = 0.02330694
Iteration 42, loss = 0.02299600
Iteration 43, loss = 0.02277498
Iteration 44, loss = 0.02241949
Iteration 45, loss = 0.02254379
Iteration 46, loss = 0.02209722
Iteration 47, loss = 0.02191297
Iteration 48, loss = 0.02184891
Iteration 49, loss = 0.02151485
Iteration 50, loss = 0.02158681
Iteration 51, loss = 0.02146450
Iteration 52, loss = 0.02150920
Iteration 53, loss = 0.02110887
Iteration 54, loss = 0.02098237
Iteration 55, loss = 0.02092630
Iteration 56, loss = 0.02069400
Iteration 57, loss = 0.02068538
Iteration 58, loss = 0.02066200
Iteration 59, loss = 0.02061255
Iteration 60, loss = 0.02047064
Iteration 61, loss = 0.02057662
Iteration 62, loss = 0.02038259
Iteration 63, loss = 0.02032012
Iteration 64, loss = 0.02020139
Iteration 65, loss = 0.02020226
Iteration 66, loss = 0.02000216
Iteration 67, loss = 0.01983122
Iteration 68, loss = 0.01989360
Iteration 69, loss = 0.01999296
Iteration 70, loss = 0.01973846
Iteration 71, loss = 0.01971788
Iteration 72, loss = 0.01975521
Iteration 73, loss = 0.01975205
Iteration 74, loss = 0.01946214
Iteration 75, loss = 0.01960210
Iteration 76, loss = 0.01976651
Iteration 77, loss = 0.01959290
Iteration 78, loss = 0.01972587
Iteration 79, loss = 0.01927951
Iteration 80, loss = 0.01950145
Iteration 81, loss = 0.01943429
Iteration 82, loss = 0.01984379
Iteration 83, loss = 0.01929276
Iteration 84, loss = 0.01949385
Iteration 85, loss = 0.01928013
Iteration 86, loss = 0.01930538
Iteration 87, loss = 0.01948275
Iteration 88, loss = 0.01920506
Iteration 89, loss = 0.01940630
Iteration 90, loss = 0.01924398
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69423111
Iteration 2, loss = 0.66663934
Iteration 3, loss = 0.63674948
Iteration 4, loss = 0.59939267
Iteration 5, loss = 0.55412737
Iteration 6, loss = 0.50341454
Iteration 7, loss = 0.45091409
Iteration 8, loss = 0.39932230
Iteration 9, loss = 0.35156220
Iteration 10, loss = 0.30775803
Iteration 11, loss = 0.26924788
Iteration 12, loss = 0.23546600
Iteration 13, loss = 0.20653532
Iteration 14, loss = 0.18179087
Iteration 15, loss = 0.16065140
Iteration 16, loss = 0.14279443
Iteration 17, loss = 0.12771602
Iteration 18, loss = 0.11482894
Iteration 19, loss = 0.10379741
Iteration 20, loss = 0.09451138
Iteration 21, loss = 0.08654956
Iteration 22, loss = 0.07953169
Iteration 23, loss = 0.07367100
Iteration 24, loss = 0.06825275
Iteration 25, loss = 0.06386262
Iteration 26, loss = 0.05982454
Iteration 27, loss = 0.05622864
Iteration 28, loss = 0.05284996
Iteration 29, loss = 0.04994495
Iteration 30, loss = 0.04726245
Iteration 31, loss = 0.04481453
Iteration 32, loss = 0.04274353
Iteration 33, loss = 0.04091375
Iteration 34, loss = 0.03938604
Iteration 35, loss = 0.03780079
Iteration 36, loss = 0.03633416
Iteration 37, loss = 0.03502039
Iteration 38, loss = 0.03379807
Iteration 39, loss = 0.03272935
Iteration 40, loss = 0.03172336
Iteration 41, loss = 0.03074895
Iteration 42, loss = 0.02984047
Iteration 43, loss = 0.02898354
Iteration 44, loss = 0.02826798
Iteration 45, loss = 0.02789845
Iteration 46, loss = 0.02724636
Iteration 47, loss = 0.02669218
Iteration 48, loss = 0.02604009
Iteration 49, loss = 0.02542396
Iteration 50, loss = 0.02491789
Iteration 51, loss = 0.02440906
Iteration 52, loss = 0.02426824
Iteration 53, loss = 0.02379764
Iteration 54, loss = 0.02350018
Iteration 55, loss = 0.02301529
Iteration 56, loss = 0.02264540
Iteration 57, loss = 0.02221817
Iteration 58, loss = 0.02191283
Iteration 59, loss = 0.02144613
Iteration 60, loss = 0.02138297
Iteration 61, loss = 0.02113851
Iteration 62, loss = 0.02078980
Iteration 63, loss = 0.02056776
Iteration 64, loss = 0.02034681
Iteration 65, loss = 0.02032802
Iteration 66, loss = 0.02032651
Iteration 67, loss = 0.02011290
Iteration 68, loss = 0.01979224
Iteration 69, loss = 0.01948342
Iteration 70, loss = 0.01925609
Iteration 71, loss = 0.01902822
Iteration 72, loss = 0.01906411
Iteration 73, loss = 0.01898499
Iteration 74, loss = 0.01876904
Iteration 75, loss = 0.01864720
Iteration 76, loss = 0.01838158
Iteration 77, loss = 0.01807353
Iteration 78, loss = 0.01789440
Iteration 79, loss = 0.01774994
Iteration 80, loss = 0.01761983
Iteration 81, loss = 0.01758761
Iteration 82, loss = 0.01741664
Iteration 83, loss = 0.01729510
Iteration 84, loss = 0.01728027
Iteration 85, loss = 0.01720452
Iteration 86, loss = 0.01724504
Iteration 87, loss = 0.01739026
Iteration 88, loss = 0.01736708
Iteration 89, loss = 0.01718637
Iteration 90, loss = 0.01715776
Iteration 91, loss = 0.01690831
Iteration 92, loss = 0.01673093
Iteration 93, loss = 0.01703079
Iteration 94, loss = 0.01717478
Iteration 95, loss = 0.01701270
Iteration 96, loss = 0.01675740
Iteration 97, loss = 0.01669049
Iteration 98, loss = 0.01658596
Iteration 99, loss = 0.01665509
Iteration 100, loss = 0.01650599
Iteration 101, loss = 0.01632200
Iteration 102, loss = 0.01606295
Iteration 103, loss = 0.01595181
Iteration 104, loss = 0.01589983
Iteration 105, loss = 0.01602566
Iteration 106, loss = 0.01601789
Iteration 107, loss = 0.01592781
Iteration 108, loss = 0.01573372
Iteration 109, loss = 0.01572693
Iteration 110, loss = 0.01571855
Iteration 111, loss = 0.01567247
Iteration 112, loss = 0.01568452
Iteration 113, loss = 0.01550283
Iteration 114, loss = 0.01559938
Iteration 115, loss = 0.01572371
Iteration 116, loss = 0.01572037
Iteration 117, loss = 0.01549728
Iteration 118, loss = 0.01547396
Iteration 119, loss = 0.01532680
Iteration 120, loss = 0.01531725
Iteration 121, loss = 0.01519675
Iteration 122, loss = 0.01531246
Iteration 123, loss = 0.01534012
Iteration 124, loss = 0.01528082
Iteration 125, loss = 0.01519886
Iteration 126, loss = 0.01515623
Iteration 127, loss = 0.01541127
Iteration 128, loss = 0.01527532
Iteration 129, loss = 0.01531547
Iteration 130, loss = 0.01526349
Iteration 131, loss = 0.01511697
Iteration 132, loss = 0.01525111
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68620610
Iteration 2, loss = 0.62408526
Iteration 3, loss = 0.53438811
Iteration 4, loss = 0.43484063
Iteration 5, loss = 0.34387955
Iteration 6, loss = 0.26906951
Iteration 7, loss = 0.21132415
Iteration 8, loss = 0.16849977
Iteration 9, loss = 0.13672558
Iteration 10, loss = 0.11362656
Iteration 11, loss = 0.09632223
Iteration 12, loss = 0.08329399
Iteration 13, loss = 0.07308399
Iteration 14, loss = 0.06544130
Iteration 15, loss = 0.05923863
Iteration 16, loss = 0.05412260
Iteration 17, loss = 0.04990209
Iteration 18, loss = 0.04673087
Iteration 19, loss = 0.04355622
Iteration 20, loss = 0.04127688
Iteration 21, loss = 0.03921101
Iteration 22, loss = 0.03755613
Iteration 23, loss = 0.03566008
Iteration 24, loss = 0.03452096
Iteration 25, loss = 0.03341292
Iteration 26, loss = 0.03236763
Iteration 27, loss = 0.03129482
Iteration 28, loss = 0.03055448
Iteration 29, loss = 0.03002871
Iteration 30, loss = 0.02916958
Iteration 31, loss = 0.02851882
Iteration 32, loss = 0.02813144
Iteration 33, loss = 0.02754836
Iteration 34, loss = 0.02722514
Iteration 35, loss = 0.02677219
Iteration 36, loss = 0.02632053
Iteration 37, loss = 0.02619897
Iteration 38, loss = 0.02562935
Iteration 39, loss = 0.02551866
Iteration 40, loss = 0.02588633
Iteration 41, loss = 0.02584330
Iteration 42, loss = 0.02526080
Iteration 43, loss = 0.02515722
Iteration 44, loss = 0.02522322
Iteration 45, loss = 0.02461997
Iteration 46, loss = 0.02445469
Iteration 47, loss = 0.02500897
Iteration 48, loss = 0.02560470
Iteration 49, loss = 0.02497831
Iteration 50, loss = 0.02473317
Iteration 51, loss = 0.02442969
Iteration 52, loss = 0.02429873
Iteration 53, loss = 0.02411652
Iteration 54, loss = 0.02403517
Iteration 55, loss = 0.02396904
Iteration 56, loss = 0.02352998
Iteration 57, loss = 0.02344348
Iteration 58, loss = 0.02326221
Iteration 59, loss = 0.02318915
Iteration 60, loss = 0.02307593
Iteration 61, loss = 0.02286133
Iteration 62, loss = 0.02287034
Iteration 63, loss = 0.02266544
Iteration 64, loss = 0.02246171
Iteration 65, loss = 0.02321271
Iteration 66, loss = 0.02417225
Iteration 67, loss = 0.02413517
Iteration 68, loss = 0.02372335
Iteration 69, loss = 0.02338102
Iteration 70, loss = 0.02325737
Iteration 71, loss = 0.02312472
Iteration 72, loss = 0.02290147
Iteration 73, loss = 0.02255400
Iteration 74, loss = 0.02252905
Iteration 75, loss = 0.02238582
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69840844
Iteration 2, loss = 0.62865887
Iteration 3, loss = 0.53274668
Iteration 4, loss = 0.42725465
Iteration 5, loss = 0.33536207
Iteration 6, loss = 0.26051648
Iteration 7, loss = 0.20505232
Iteration 8, loss = 0.16363414
Iteration 9, loss = 0.13353385
Iteration 10, loss = 0.11105825
Iteration 11, loss = 0.09443556
Iteration 12, loss = 0.08205004
Iteration 13, loss = 0.07275047
Iteration 14, loss = 0.06503505
Iteration 15, loss = 0.05914796
Iteration 16, loss = 0.05437873
Iteration 17, loss = 0.05033663
Iteration 18, loss = 0.04697247
Iteration 19, loss = 0.04447053
Iteration 20, loss = 0.04208007
Iteration 21, loss = 0.03998255
Iteration 22, loss = 0.03833494
Iteration 23, loss = 0.03674293
Iteration 24, loss = 0.03569680
Iteration 25, loss = 0.03444460
Iteration 26, loss = 0.03342806
Iteration 27, loss = 0.03239922
Iteration 28, loss = 0.03162883
Iteration 29, loss = 0.03086424
Iteration 30, loss = 0.03039770
Iteration 31, loss = 0.02975708
Iteration 32, loss = 0.02925270
Iteration 33, loss = 0.02867942
Iteration 34, loss = 0.02814963
Iteration 35, loss = 0.02793071
Iteration 36, loss = 0.02745218
Iteration 37, loss = 0.02727607
Iteration 38, loss = 0.02698524
Iteration 39, loss = 0.02649221
Iteration 40, loss = 0.02647747
Iteration 41, loss = 0.02598007
Iteration 42, loss = 0.02615341
Iteration 43, loss = 0.02565837
Iteration 44, loss = 0.02555820
Iteration 45, loss = 0.02549130
Iteration 46, loss = 0.02530424
Iteration 47, loss = 0.02524141
Iteration 48, loss = 0.02477030
Iteration 49, loss = 0.02469153
Iteration 50, loss = 0.02475144
Iteration 51, loss = 0.02543422
Iteration 52, loss = 0.02556281
Iteration 53, loss = 0.02487906
Iteration 54, loss = 0.02461534
Iteration 55, loss = 0.02450579
Iteration 56, loss = 0.02431697
Iteration 57, loss = 0.02439923
Iteration 58, loss = 0.02412422
Iteration 59, loss = 0.02441974
Iteration 60, loss = 0.02390667
Iteration 61, loss = 0.02399316
Iteration 62, loss = 0.02377934
Iteration 63, loss = 0.02373577
Iteration 64, loss = 0.02382055
Iteration 65, loss = 0.02362027
Iteration 66, loss = 0.02395587
Iteration 67, loss = 0.02352117
Iteration 68, loss = 0.02364335
Iteration 69, loss = 0.02338429
Iteration 70, loss = 0.02338175
Iteration 71, loss = 0.02317836
Iteration 72, loss = 0.02470452
Iteration 73, loss = 0.02630394
Iteration 74, loss = 0.02580598
Iteration 75, loss = 0.02570329
Iteration 76, loss = 0.02500103
Iteration 77, loss = 0.02499244
Iteration 78, loss = 0.02436398
Iteration 79, loss = 0.02417092
Iteration 80, loss = 0.02373989
Iteration 81, loss = 0.02380456
Iteration 82, loss = 0.02354670
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70014300
Iteration 2, loss = 0.68085642
Iteration 3, loss = 0.66254891
Iteration 4, loss = 0.64087558
Iteration 5, loss = 0.61539614
Iteration 6, loss = 0.58622234
Iteration 7, loss = 0.55343251
Iteration 8, loss = 0.51858243
Iteration 9, loss = 0.48275147
Iteration 10, loss = 0.44635295
Iteration 11, loss = 0.41114184
Iteration 12, loss = 0.37708851
Iteration 13, loss = 0.34484631
Iteration 14, loss = 0.31455279
Iteration 15, loss = 0.28655390
Iteration 16, loss = 0.26070354
Iteration 17, loss = 0.23705136
Iteration 18, loss = 0.21570904
Iteration 19, loss = 0.19654662
Iteration 20, loss = 0.17926707
Iteration 21, loss = 0.16378261
Iteration 22, loss = 0.15000627
Iteration 23, loss = 0.13771574
Iteration 24, loss = 0.12674305
Iteration 25, loss = 0.11700141
Iteration 26, loss = 0.10827929
Iteration 27, loss = 0.10047507
Iteration 28, loss = 0.09359785
Iteration 29, loss = 0.08735013
Iteration 30, loss = 0.08169138
Iteration 31, loss = 0.07667922
Iteration 32, loss = 0.07209605
Iteration 33, loss = 0.06799333
Iteration 34, loss = 0.06423061
Iteration 35, loss = 0.06083479
Iteration 36, loss = 0.05774389
Iteration 37, loss = 0.05494055
Iteration 38, loss = 0.05233413
Iteration 39, loss = 0.04993387
Iteration 40, loss = 0.04774494
Iteration 41, loss = 0.04573390
Iteration 42, loss = 0.04385681
Iteration 43, loss = 0.04217832
Iteration 44, loss = 0.04051758
Iteration 45, loss = 0.03909458
Iteration 46, loss = 0.03767346
Iteration 47, loss = 0.03638391
Iteration 48, loss = 0.03517031
Iteration 49, loss = 0.03408920
Iteration 50, loss = 0.03301472
Iteration 51, loss = 0.03208325
Iteration 52, loss = 0.03114972
Iteration 53, loss = 0.03024498
Iteration 54, loss = 0.02944704
Iteration 55, loss = 0.02863917
Iteration 56, loss = 0.02792530
Iteration 57, loss = 0.02722820
Iteration 58, loss = 0.02659581
Iteration 59, loss = 0.02598764
Iteration 60, loss = 0.02539420
Iteration 61, loss = 0.02485951
Iteration 62, loss = 0.02432598
Iteration 63, loss = 0.02382322
Iteration 64, loss = 0.02335752
Iteration 65, loss = 0.02291829
Iteration 66, loss = 0.02249055
Iteration 67, loss = 0.02208718
Iteration 68, loss = 0.02170829
Iteration 69, loss = 0.02137518
Iteration 70, loss = 0.02099249
Iteration 71, loss = 0.02064493
Iteration 72, loss = 0.02033129
Iteration 73, loss = 0.02000494
Iteration 74, loss = 0.01984269
Iteration 75, loss = 0.01948178
Iteration 76, loss = 0.01916269
Iteration 77, loss = 0.01892268
Iteration 78, loss = 0.01867815
Iteration 79, loss = 0.01841629
Iteration 80, loss = 0.01821966
Iteration 81, loss = 0.01804227
Iteration 82, loss = 0.01778342
Iteration 83, loss = 0.01757835
Iteration 84, loss = 0.01740118
Iteration 85, loss = 0.01718590
Iteration 86, loss = 0.01700579
Iteration 87, loss = 0.01681461
Iteration 88, loss = 0.01667117
Iteration 89, loss = 0.01651388
Iteration 90, loss = 0.01637545
Iteration 91, loss = 0.01619138
Iteration 92, loss = 0.01608434
Iteration 93, loss = 0.01596342
Iteration 94, loss = 0.01575449
Iteration 95, loss = 0.01562540
Iteration 96, loss = 0.01554513
Iteration 97, loss = 0.01539323
Iteration 98, loss = 0.01528954
Iteration 99, loss = 0.01516000
Iteration 100, loss = 0.01505255
Iteration 101, loss = 0.01494432
Iteration 102, loss = 0.01486292
Iteration 103, loss = 0.01469571
Iteration 104, loss = 0.01460545
Iteration 105, loss = 0.01452067
Iteration 106, loss = 0.01450144
Iteration 107, loss = 0.01431890
Iteration 108, loss = 0.01426202
Iteration 109, loss = 0.01418687
Iteration 110, loss = 0.01409580
Iteration 111, loss = 0.01399466
Iteration 112, loss = 0.01393373
Iteration 113, loss = 0.01382521
Iteration 114, loss = 0.01381718
Iteration 115, loss = 0.01367952
Iteration 116, loss = 0.01366123
Iteration 117, loss = 0.01354600
Iteration 118, loss = 0.01353438
Iteration 119, loss = 0.01340836
Iteration 120, loss = 0.01338399
Iteration 121, loss = 0.01330476
Iteration 122, loss = 0.01323705
Iteration 123, loss = 0.01325551
Iteration 124, loss = 0.01312495
Iteration 125, loss = 0.01305505
Iteration 126, loss = 0.01307929
Iteration 127, loss = 0.01298746
Iteration 128, loss = 0.01291658
Iteration 129, loss = 0.01290218
Iteration 130, loss = 0.01285161
Iteration 131, loss = 0.01290312
Iteration 132, loss = 0.01270708
Iteration 133, loss = 0.01275989
Iteration 134, loss = 0.01262814
Iteration 135, loss = 0.01260487
Iteration 136, loss = 0.01254265
Iteration 137, loss = 0.01248067
Iteration 138, loss = 0.01248641
Iteration 139, loss = 0.01239490
Iteration 140, loss = 0.01255529
Iteration 141, loss = 0.01239062
Iteration 142, loss = 0.01229779
Iteration 143, loss = 0.01227384
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70801861
Iteration 2, loss = 0.62924015
Iteration 3, loss = 0.52635604
Iteration 4, loss = 0.41310219
Iteration 5, loss = 0.31257946
Iteration 6, loss = 0.23502649
Iteration 7, loss = 0.17882363
Iteration 8, loss = 0.13995085
Iteration 9, loss = 0.11245388
Iteration 10, loss = 0.09305199
Iteration 11, loss = 0.07916359
Iteration 12, loss = 0.06845688
Iteration 13, loss = 0.06059974
Iteration 14, loss = 0.05432554
Iteration 15, loss = 0.04941266
Iteration 16, loss = 0.04521248
Iteration 17, loss = 0.04207713
Iteration 18, loss = 0.03929495
Iteration 19, loss = 0.03712198
Iteration 20, loss = 0.03497935
Iteration 21, loss = 0.03318350
Iteration 22, loss = 0.03178900
Iteration 23, loss = 0.03072281
Iteration 24, loss = 0.02953459
Iteration 25, loss = 0.02855177
Iteration 26, loss = 0.02781548
Iteration 27, loss = 0.02706877
Iteration 28, loss = 0.02628644
Iteration 29, loss = 0.02591350
Iteration 30, loss = 0.02532613
Iteration 31, loss = 0.02458562
Iteration 32, loss = 0.02448021
Iteration 33, loss = 0.02419787
Iteration 34, loss = 0.02355824
Iteration 35, loss = 0.02366115
Iteration 36, loss = 0.02305800
Iteration 37, loss = 0.02293706
Iteration 38, loss = 0.02303488
Iteration 39, loss = 0.02235088
Iteration 40, loss = 0.02210330
Iteration 41, loss = 0.02233482
Iteration 42, loss = 0.02196143
Iteration 43, loss = 0.02164101
Iteration 44, loss = 0.02147450
Iteration 45, loss = 0.02136511
Iteration 46, loss = 0.02125084
Iteration 47, loss = 0.02108615
Iteration 48, loss = 0.02100683
Iteration 49, loss = 0.02092931
Iteration 50, loss = 0.02079134
Iteration 51, loss = 0.02092848
Iteration 52, loss = 0.02064576
Iteration 53, loss = 0.02048169
Iteration 54, loss = 0.02050669
Iteration 55, loss = 0.02026044
Iteration 56, loss = 0.02039500
Iteration 57, loss = 0.02026555
Iteration 58, loss = 0.02022640
Iteration 59, loss = 0.02012952
Iteration 60, loss = 0.02012312
Iteration 61, loss = 0.01980852
Iteration 62, loss = 0.01994450
Iteration 63, loss = 0.01994483
Iteration 64, loss = 0.01976511
Iteration 65, loss = 0.02014729
Iteration 66, loss = 0.01996963
Iteration 67, loss = 0.01971175
Iteration 68, loss = 0.01984328
Iteration 69, loss = 0.01971216
Iteration 70, loss = 0.01957196
Iteration 71, loss = 0.01952929
Iteration 72, loss = 0.01967768
Iteration 73, loss = 0.01955916
Iteration 74, loss = 0.01929208
Iteration 75, loss = 0.01968258
Iteration 76, loss = 0.01949110
Iteration 77, loss = 0.01938034
Iteration 78, loss = 0.01943610
Iteration 79, loss = 0.01940367
Iteration 80, loss = 0.01947555
Iteration 81, loss = 0.01938280
Iteration 82, loss = 0.01928304
Iteration 83, loss = 0.01925839
Iteration 84, loss = 0.01954000
Iteration 85, loss = 0.01916166
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73390762
Iteration 2, loss = 0.70710141
Iteration 3, loss = 0.68250746
Iteration 4, loss = 0.65514282
Iteration 5, loss = 0.62391160
Iteration 6, loss = 0.58980834
Iteration 7, loss = 0.55444451
Iteration 8, loss = 0.51785678
Iteration 9, loss = 0.48108550
Iteration 10, loss = 0.44444276
Iteration 11, loss = 0.40871332
Iteration 12, loss = 0.37407023
Iteration 13, loss = 0.34103086
Iteration 14, loss = 0.31007435
Iteration 15, loss = 0.28130018
Iteration 16, loss = 0.25505125
Iteration 17, loss = 0.23141681
Iteration 18, loss = 0.21007319
Iteration 19, loss = 0.19099963
Iteration 20, loss = 0.17399054
Iteration 21, loss = 0.15893024
Iteration 22, loss = 0.14556912
Iteration 23, loss = 0.13368240
Iteration 24, loss = 0.12310146
Iteration 25, loss = 0.11373537
Iteration 26, loss = 0.10551247
Iteration 27, loss = 0.09793371
Iteration 28, loss = 0.09135897
Iteration 29, loss = 0.08534895
Iteration 30, loss = 0.08000746
Iteration 31, loss = 0.07516014
Iteration 32, loss = 0.07083317
Iteration 33, loss = 0.06697239
Iteration 34, loss = 0.06336047
Iteration 35, loss = 0.06010672
Iteration 36, loss = 0.05714765
Iteration 37, loss = 0.05439564
Iteration 38, loss = 0.05189871
Iteration 39, loss = 0.04969249
Iteration 40, loss = 0.04752779
Iteration 41, loss = 0.04562578
Iteration 42, loss = 0.04379524
Iteration 43, loss = 0.04211982
Iteration 44, loss = 0.04063116
Iteration 45, loss = 0.03913487
Iteration 46, loss = 0.03778705
Iteration 47, loss = 0.03662120
Iteration 48, loss = 0.03546221
Iteration 49, loss = 0.03437521
Iteration 50, loss = 0.03335515
Iteration 51, loss = 0.03238375
Iteration 52, loss = 0.03144059
Iteration 53, loss = 0.03062012
Iteration 54, loss = 0.02980625
Iteration 55, loss = 0.02907288
Iteration 56, loss = 0.02834566
Iteration 57, loss = 0.02771837
Iteration 58, loss = 0.02704289
Iteration 59, loss = 0.02646586
Iteration 60, loss = 0.02587779
Iteration 61, loss = 0.02536403
Iteration 62, loss = 0.02486026
Iteration 63, loss = 0.02437296
Iteration 64, loss = 0.02393025
Iteration 65, loss = 0.02345352
Iteration 66, loss = 0.02300360
Iteration 67, loss = 0.02266377
Iteration 68, loss = 0.02225498
Iteration 69, loss = 0.02189766
Iteration 70, loss = 0.02152720
Iteration 71, loss = 0.02120818
Iteration 72, loss = 0.02088440
Iteration 73, loss = 0.02060566
Iteration 74, loss = 0.02035253
Iteration 75, loss = 0.02002951
Iteration 76, loss = 0.01980592
Iteration 77, loss = 0.01951154
Iteration 78, loss = 0.01922650
Iteration 79, loss = 0.01900784
Iteration 80, loss = 0.01882147
Iteration 81, loss = 0.01857754
Iteration 82, loss = 0.01837219
Iteration 83, loss = 0.01814859
Iteration 84, loss = 0.01796229
Iteration 85, loss = 0.01776126
Iteration 86, loss = 0.01760994
Iteration 87, loss = 0.01741991
Iteration 88, loss = 0.01725405
Iteration 89, loss = 0.01705729
Iteration 90, loss = 0.01693079
Iteration 91, loss = 0.01680340
Iteration 92, loss = 0.01665147
Iteration 93, loss = 0.01649460
Iteration 94, loss = 0.01646621
Iteration 95, loss = 0.01625411
Iteration 96, loss = 0.01615016
Iteration 97, loss = 0.01600172
Iteration 98, loss = 0.01595016
Iteration 99, loss = 0.01574991
Iteration 100, loss = 0.01563347
Iteration 101, loss = 0.01550946
Iteration 102, loss = 0.01543607
Iteration 103, loss = 0.01531840
Iteration 104, loss = 0.01517681
Iteration 105, loss = 0.01519717
Iteration 106, loss = 0.01501842
Iteration 107, loss = 0.01493769
Iteration 108, loss = 0.01484169
Iteration 109, loss = 0.01471763
Iteration 110, loss = 0.01465269
Iteration 111, loss = 0.01459168
Iteration 112, loss = 0.01453951
Iteration 113, loss = 0.01443971
Iteration 114, loss = 0.01435585
Iteration 115, loss = 0.01443883
Iteration 116, loss = 0.01419699
Iteration 117, loss = 0.01419907
Iteration 118, loss = 0.01405615
Iteration 119, loss = 0.01400255
Iteration 120, loss = 0.01395319
Iteration 121, loss = 0.01394039
Iteration 122, loss = 0.01387129
Iteration 123, loss = 0.01382533
Iteration 124, loss = 0.01374140
Iteration 125, loss = 0.01361867
Iteration 126, loss = 0.01361153
Iteration 127, loss = 0.01363063
Iteration 128, loss = 0.01347469
Iteration 129, loss = 0.01347280
Iteration 130, loss = 0.01337992
Iteration 131, loss = 0.01329233
Iteration 132, loss = 0.01327747
Iteration 133, loss = 0.01329447
Iteration 134, loss = 0.01316985
Iteration 135, loss = 0.01316448
Iteration 136, loss = 0.01310790
Iteration 137, loss = 0.01311596
Iteration 138, loss = 0.01309122
Iteration 139, loss = 0.01302630
Iteration 140, loss = 0.01297960
Iteration 141, loss = 0.01288030
Iteration 142, loss = 0.01289407
Iteration 143, loss = 0.01285480
Iteration 144, loss = 0.01291436
Iteration 145, loss = 0.01282661
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67808934
Iteration 2, loss = 0.63036028
Iteration 3, loss = 0.56244788
Iteration 4, loss = 0.48176028
Iteration 5, loss = 0.40107623
Iteration 6, loss = 0.32753328
Iteration 7, loss = 0.26532887
Iteration 8, loss = 0.21492954
Iteration 9, loss = 0.17572553
Iteration 10, loss = 0.14538019
Iteration 11, loss = 0.12215086
Iteration 12, loss = 0.10415848
Iteration 13, loss = 0.09007162
Iteration 14, loss = 0.07902892
Iteration 15, loss = 0.07009101
Iteration 16, loss = 0.06299451
Iteration 17, loss = 0.05705098
Iteration 18, loss = 0.05230864
Iteration 19, loss = 0.04824892
Iteration 20, loss = 0.04477254
Iteration 21, loss = 0.04193906
Iteration 22, loss = 0.03966284
Iteration 23, loss = 0.03729583
Iteration 24, loss = 0.03535073
Iteration 25, loss = 0.03372425
Iteration 26, loss = 0.03226680
Iteration 27, loss = 0.03110559
Iteration 28, loss = 0.02997327
Iteration 29, loss = 0.02888675
Iteration 30, loss = 0.02797086
Iteration 31, loss = 0.02712968
Iteration 32, loss = 0.02638784
Iteration 33, loss = 0.02568064
Iteration 34, loss = 0.02493663
Iteration 35, loss = 0.02452254
Iteration 36, loss = 0.02417116
Iteration 37, loss = 0.02374209
Iteration 38, loss = 0.02326394
Iteration 39, loss = 0.02294532
Iteration 40, loss = 0.02239189
Iteration 41, loss = 0.02212027
Iteration 42, loss = 0.02175158
Iteration 43, loss = 0.02145732
Iteration 44, loss = 0.02105789
Iteration 45, loss = 0.02111818
Iteration 46, loss = 0.02079222
Iteration 47, loss = 0.02064415
Iteration 48, loss = 0.02051162
Iteration 49, loss = 0.02013830
Iteration 50, loss = 0.02012508
Iteration 51, loss = 0.01970287
Iteration 52, loss = 0.01980048
Iteration 53, loss = 0.01947745
Iteration 54, loss = 0.01936129
Iteration 55, loss = 0.01924137
Iteration 56, loss = 0.01920134
Iteration 57, loss = 0.01910944
Iteration 58, loss = 0.01896912
Iteration 59, loss = 0.01885677
Iteration 60, loss = 0.01861804
Iteration 61, loss = 0.01858764
Iteration 62, loss = 0.01845559
Iteration 63, loss = 0.01857405
Iteration 64, loss = 0.01822515
Iteration 65, loss = 0.01860542
Iteration 66, loss = 0.01835795
Iteration 67, loss = 0.01842120
Iteration 68, loss = 0.01820937
Iteration 69, loss = 0.01813738
Iteration 70, loss = 0.01796234
Iteration 71, loss = 0.01793682
Iteration 72, loss = 0.01792889
Iteration 73, loss = 0.01783692
Iteration 74, loss = 0.01777155
Iteration 75, loss = 0.01776610
Iteration 76, loss = 0.01781964
Iteration 77, loss = 0.01771364
Iteration 78, loss = 0.01780490
Iteration 79, loss = 0.01755692
Iteration 80, loss = 0.01753125
Iteration 81, loss = 0.01738340
Iteration 82, loss = 0.01727773
Iteration 83, loss = 0.01790343
Iteration 84, loss = 0.01748491
Iteration 85, loss = 0.01775572
Iteration 86, loss = 0.01736407
Iteration 87, loss = 0.01711884
Iteration 88, loss = 0.01720972
Iteration 89, loss = 0.01726659
Iteration 90, loss = 0.01717291
Iteration 91, loss = 0.01742837
Iteration 92, loss = 0.01729692
Iteration 93, loss = 0.01708606
Iteration 94, loss = 0.01689494
Iteration 95, loss = 0.01699617
Iteration 96, loss = 0.01712205
Iteration 97, loss = 0.01733639
Iteration 98, loss = 0.01716363
Iteration 99, loss = 0.01713980
Iteration 100, loss = 0.01715641
Iteration 101, loss = 0.01706666
Iteration 102, loss = 0.01680461
Iteration 103, loss = 0.01686347
Iteration 104, loss = 0.01679507
Iteration 105, loss = 0.01670542
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69653197
Iteration 2, loss = 0.68077715
Iteration 3, loss = 0.66781532
Iteration 4, loss = 0.65486104
Iteration 5, loss = 0.64128519
Iteration 6, loss = 0.62665543
Iteration 7, loss = 0.61103528
Iteration 8, loss = 0.59385064
Iteration 9, loss = 0.57525982
Iteration 10, loss = 0.55533537
Iteration 11, loss = 0.53417278
Iteration 12, loss = 0.51172941
Iteration 13, loss = 0.48857483
Iteration 14, loss = 0.46484005
Iteration 15, loss = 0.44086638
Iteration 16, loss = 0.41690354
Iteration 17, loss = 0.39332546
Iteration 18, loss = 0.37027534
Iteration 19, loss = 0.34793385
Iteration 20, loss = 0.32657052
Iteration 21, loss = 0.30614259
Iteration 22, loss = 0.28675791
Iteration 23, loss = 0.26840054
Iteration 24, loss = 0.25112667
Iteration 25, loss = 0.23492571
Iteration 26, loss = 0.21975053
Iteration 27, loss = 0.20556814
Iteration 28, loss = 0.19241047
Iteration 29, loss = 0.18016509
Iteration 30, loss = 0.16881595
Iteration 31, loss = 0.15832511
Iteration 32, loss = 0.14859716
Iteration 33, loss = 0.13960395
Iteration 34, loss = 0.13133445
Iteration 35, loss = 0.12369357
Iteration 36, loss = 0.11657134
Iteration 37, loss = 0.11003895
Iteration 38, loss = 0.10401385
Iteration 39, loss = 0.09838663
Iteration 40, loss = 0.09321305
Iteration 41, loss = 0.08842888
Iteration 42, loss = 0.08398195
Iteration 43, loss = 0.07988672
Iteration 44, loss = 0.07603478
Iteration 45, loss = 0.07247935
Iteration 46, loss = 0.06915818
Iteration 47, loss = 0.06604437
Iteration 48, loss = 0.06317366
Iteration 49, loss = 0.06049069
Iteration 50, loss = 0.05793694
Iteration 51, loss = 0.05558050
Iteration 52, loss = 0.05337584
Iteration 53, loss = 0.05126436
Iteration 54, loss = 0.04933518
Iteration 55, loss = 0.04749873
Iteration 56, loss = 0.04576012
Iteration 57, loss = 0.04414808
Iteration 58, loss = 0.04261613
Iteration 59, loss = 0.04115876
Iteration 60, loss = 0.03978350
Iteration 61, loss = 0.03848779
Iteration 62, loss = 0.03726562
Iteration 63, loss = 0.03611549
Iteration 64, loss = 0.03499584
Iteration 65, loss = 0.03394843
Iteration 66, loss = 0.03294945
Iteration 67, loss = 0.03199615
Iteration 68, loss = 0.03112142
Iteration 69, loss = 0.03023515
Iteration 70, loss = 0.02941134
Iteration 71, loss = 0.02865713
Iteration 72, loss = 0.02788734
Iteration 73, loss = 0.02715902
Iteration 74, loss = 0.02648599
Iteration 75, loss = 0.02583465
Iteration 76, loss = 0.02522605
Iteration 77, loss = 0.02461791
Iteration 78, loss = 0.02404216
Iteration 79, loss = 0.02349560
Iteration 80, loss = 0.02298103
Iteration 81, loss = 0.02247849
Iteration 82, loss = 0.02197669
Iteration 83, loss = 0.02151123
Iteration 84, loss = 0.02107048
Iteration 85, loss = 0.02063888
Iteration 86, loss = 0.02022304
Iteration 87, loss = 0.01984698
Iteration 88, loss = 0.01943611
Iteration 89, loss = 0.01906492
Iteration 90, loss = 0.01871057
Iteration 91, loss = 0.01837307
Iteration 92, loss = 0.01803054
Iteration 93, loss = 0.01770664
Iteration 94, loss = 0.01740670
Iteration 95, loss = 0.01710386
Iteration 96, loss = 0.01679882
Iteration 97, loss = 0.01656089
Iteration 98, loss = 0.01626724
Iteration 99, loss = 0.01598270
Iteration 100, loss = 0.01573266
Iteration 101, loss = 0.01548712
Iteration 102, loss = 0.01526333
Iteration 103, loss = 0.01501667
Iteration 104, loss = 0.01479617
Iteration 105, loss = 0.01458291
Iteration 106, loss = 0.01436346
Iteration 107, loss = 0.01417300
Iteration 108, loss = 0.01398181
Iteration 109, loss = 0.01381496
Iteration 110, loss = 0.01359653
Iteration 111, loss = 0.01342956
Iteration 112, loss = 0.01321830
Iteration 113, loss = 0.01303971
Iteration 114, loss = 0.01286409
Iteration 115, loss = 0.01271157
Iteration 116, loss = 0.01253386
Iteration 117, loss = 0.01237443
Iteration 118, loss = 0.01222307
Iteration 119, loss = 0.01207412
Iteration 120, loss = 0.01192925
Iteration 121, loss = 0.01180247
Iteration 122, loss = 0.01165416
Iteration 123, loss = 0.01152185
Iteration 124, loss = 0.01140408
Iteration 125, loss = 0.01126754
Iteration 126, loss = 0.01115596
Iteration 127, loss = 0.01104720
Iteration 128, loss = 0.01090572
Iteration 129, loss = 0.01079466
Iteration 130, loss = 0.01067806
Iteration 131, loss = 0.01058135
Iteration 132, loss = 0.01048567
Iteration 133, loss = 0.01042781
Iteration 134, loss = 0.01031315
Iteration 135, loss = 0.01020392
Iteration 136, loss = 0.01012804
Iteration 137, loss = 0.01001242
Iteration 138, loss = 0.00992794
Iteration 139, loss = 0.00984068
Iteration 140, loss = 0.00975468
Iteration 141, loss = 0.00967927
Iteration 142, loss = 0.00958383
Iteration 143, loss = 0.00950154
Iteration 144, loss = 0.00943230
Iteration 145, loss = 0.00934082
Iteration 146, loss = 0.00927423
Iteration 147, loss = 0.00920013
Iteration 148, loss = 0.00913369
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67632449
Iteration 2, loss = 0.65729097
Iteration 3, loss = 0.63782966
Iteration 4, loss = 0.61350740
Iteration 5, loss = 0.58391255
Iteration 6, loss = 0.54997427
Iteration 7, loss = 0.51338490
Iteration 8, loss = 0.47539458
Iteration 9, loss = 0.43781576
Iteration 10, loss = 0.40136931
Iteration 11, loss = 0.36657052
Iteration 12, loss = 0.33388558
Iteration 13, loss = 0.30356031
Iteration 14, loss = 0.27549328
Iteration 15, loss = 0.24989460
Iteration 16, loss = 0.22684419
Iteration 17, loss = 0.20592891
Iteration 18, loss = 0.18719809
Iteration 19, loss = 0.17062171
Iteration 20, loss = 0.15579481
Iteration 21, loss = 0.14258328
Iteration 22, loss = 0.13103290
Iteration 23, loss = 0.12067737
Iteration 24, loss = 0.11146270
Iteration 25, loss = 0.10331802
Iteration 26, loss = 0.09602726
Iteration 27, loss = 0.08953241
Iteration 28, loss = 0.08367450
Iteration 29, loss = 0.07848532
Iteration 30, loss = 0.07379833
Iteration 31, loss = 0.06962626
Iteration 32, loss = 0.06579518
Iteration 33, loss = 0.06240147
Iteration 34, loss = 0.05918560
Iteration 35, loss = 0.05642035
Iteration 36, loss = 0.05372159
Iteration 37, loss = 0.05133774
Iteration 38, loss = 0.04912632
Iteration 39, loss = 0.04711939
Iteration 40, loss = 0.04529725
Iteration 41, loss = 0.04352191
Iteration 42, loss = 0.04198698
Iteration 43, loss = 0.04047518
Iteration 44, loss = 0.03913260
Iteration 45, loss = 0.03785552
Iteration 46, loss = 0.03669267
Iteration 47, loss = 0.03556169
Iteration 48, loss = 0.03455820
Iteration 49, loss = 0.03358519
Iteration 50, loss = 0.03263461
Iteration 51, loss = 0.03186649
Iteration 52, loss = 0.03101108
Iteration 53, loss = 0.03025977
Iteration 54, loss = 0.02957306
Iteration 55, loss = 0.02891850
Iteration 56, loss = 0.02825929
Iteration 57, loss = 0.02768945
Iteration 58, loss = 0.02709023
Iteration 59, loss = 0.02659028
Iteration 60, loss = 0.02609536
Iteration 61, loss = 0.02559983
Iteration 62, loss = 0.02515234
Iteration 63, loss = 0.02471492
Iteration 64, loss = 0.02427158
Iteration 65, loss = 0.02386051
Iteration 66, loss = 0.02353035
Iteration 67, loss = 0.02314312
Iteration 68, loss = 0.02280264
Iteration 69, loss = 0.02248870
Iteration 70, loss = 0.02219529
Iteration 71, loss = 0.02190221
Iteration 72, loss = 0.02159837
Iteration 73, loss = 0.02131981
Iteration 74, loss = 0.02106495
Iteration 75, loss = 0.02079763
Iteration 76, loss = 0.02057149
Iteration 77, loss = 0.02033822
Iteration 78, loss = 0.02014476
Iteration 79, loss = 0.01987575
Iteration 80, loss = 0.01974655
Iteration 81, loss = 0.01948195
Iteration 82, loss = 0.01933280
Iteration 83, loss = 0.01912733
Iteration 84, loss = 0.01894724
Iteration 85, loss = 0.01888486
Iteration 86, loss = 0.01860516
Iteration 87, loss = 0.01849798
Iteration 88, loss = 0.01831989
Iteration 89, loss = 0.01816840
Iteration 90, loss = 0.01806040
Iteration 91, loss = 0.01788653
Iteration 92, loss = 0.01780887
Iteration 93, loss = 0.01768099
Iteration 94, loss = 0.01748181
Iteration 95, loss = 0.01736784
Iteration 96, loss = 0.01724586
Iteration 97, loss = 0.01722607
Iteration 98, loss = 0.01705768
Iteration 99, loss = 0.01695987
Iteration 100, loss = 0.01684959
Iteration 101, loss = 0.01675625
Iteration 102, loss = 0.01670622
Iteration 103, loss = 0.01657328
Iteration 104, loss = 0.01644133
Iteration 105, loss = 0.01639744
Iteration 106, loss = 0.01634500
Iteration 107, loss = 0.01622440
Iteration 108, loss = 0.01615019
Iteration 109, loss = 0.01604157
Iteration 110, loss = 0.01595505
Iteration 111, loss = 0.01593006
Iteration 112, loss = 0.01586678
Iteration 113, loss = 0.01578612
Iteration 114, loss = 0.01567634
Iteration 115, loss = 0.01562834
Iteration 116, loss = 0.01555634
Iteration 117, loss = 0.01552779
Iteration 118, loss = 0.01548673
Iteration 119, loss = 0.01541870
Iteration 120, loss = 0.01537657
Iteration 121, loss = 0.01529203
Iteration 122, loss = 0.01520608
Iteration 123, loss = 0.01518706
Iteration 124, loss = 0.01508776
Iteration 125, loss = 0.01507891
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67432598
Iteration 2, loss = 0.60367266
Iteration 3, loss = 0.50016860
Iteration 4, loss = 0.39365796
Iteration 5, loss = 0.30100763
Iteration 6, loss = 0.22670316
Iteration 7, loss = 0.17320433
Iteration 8, loss = 0.13574810
Iteration 9, loss = 0.10975358
Iteration 10, loss = 0.09130907
Iteration 11, loss = 0.07758822
Iteration 12, loss = 0.06769620
Iteration 13, loss = 0.05982656
Iteration 14, loss = 0.05376377
Iteration 15, loss = 0.04904486
Iteration 16, loss = 0.04525632
Iteration 17, loss = 0.04212263
Iteration 18, loss = 0.03931112
Iteration 19, loss = 0.03738780
Iteration 20, loss = 0.03546207
Iteration 21, loss = 0.03400300
Iteration 22, loss = 0.03216916
Iteration 23, loss = 0.03109723
Iteration 24, loss = 0.03039385
Iteration 25, loss = 0.02932767
Iteration 26, loss = 0.02852643
Iteration 27, loss = 0.02791783
Iteration 28, loss = 0.02720753
Iteration 29, loss = 0.02670305
Iteration 30, loss = 0.02603526
Iteration 31, loss = 0.02563342
Iteration 32, loss = 0.02510019
Iteration 33, loss = 0.02503755
Iteration 34, loss = 0.02463256
Iteration 35, loss = 0.02435365
Iteration 36, loss = 0.02402090
Iteration 37, loss = 0.02365250
Iteration 38, loss = 0.02351771
Iteration 39, loss = 0.02336861
Iteration 40, loss = 0.02304824
Iteration 41, loss = 0.02268947
Iteration 42, loss = 0.02258240
Iteration 43, loss = 0.02258281
Iteration 44, loss = 0.02238130
Iteration 45, loss = 0.02229882
Iteration 46, loss = 0.02232847
Iteration 47, loss = 0.02197738
Iteration 48, loss = 0.02186396
Iteration 49, loss = 0.02179519
Iteration 50, loss = 0.02163480
Iteration 51, loss = 0.02164987
Iteration 52, loss = 0.02162613
Iteration 53, loss = 0.02145462
Iteration 54, loss = 0.02130986
Iteration 55, loss = 0.02112253
Iteration 56, loss = 0.02114560
Iteration 57, loss = 0.02128577
Iteration 58, loss = 0.02119150
Iteration 59, loss = 0.02080824
Iteration 60, loss = 0.02092347
Iteration 61, loss = 0.02088125
Iteration 62, loss = 0.02063392
Iteration 63, loss = 0.02070735
Iteration 64, loss = 0.02069064
Iteration 65, loss = 0.02047276
Iteration 66, loss = 0.02061712
Iteration 67, loss = 0.02074929
Iteration 68, loss = 0.02065008
Iteration 69, loss = 0.02059808
Iteration 70, loss = 0.02014339
Iteration 71, loss = 0.02076831
Iteration 72, loss = 0.02072451
Iteration 73, loss = 0.02048393
Iteration 74, loss = 0.02042982
Iteration 75, loss = 0.02036438
Iteration 76, loss = 0.02040003
Iteration 77, loss = 0.02050568
Iteration 78, loss = 0.02067476
Iteration 79, loss = 0.02012973
Iteration 80, loss = 0.02016283
Iteration 81, loss = 0.02050897
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70040988
Iteration 2, loss = 0.63505048
Iteration 3, loss = 0.53947711
Iteration 4, loss = 0.42349874
Iteration 5, loss = 0.32117389
Iteration 6, loss = 0.24172884
Iteration 7, loss = 0.18520548
Iteration 8, loss = 0.14544433
Iteration 9, loss = 0.11752219
Iteration 10, loss = 0.09793588
Iteration 11, loss = 0.08344136
Iteration 12, loss = 0.07286394
Iteration 13, loss = 0.06468708
Iteration 14, loss = 0.05799667
Iteration 15, loss = 0.05310816
Iteration 16, loss = 0.04880338
Iteration 17, loss = 0.04550899
Iteration 18, loss = 0.04243157
Iteration 19, loss = 0.04050764
Iteration 20, loss = 0.03823537
Iteration 21, loss = 0.03675033
Iteration 22, loss = 0.03507662
Iteration 23, loss = 0.03382522
Iteration 24, loss = 0.03254656
Iteration 25, loss = 0.03186832
Iteration 26, loss = 0.03083788
Iteration 27, loss = 0.02989259
Iteration 28, loss = 0.02944814
Iteration 29, loss = 0.02894870
Iteration 30, loss = 0.02835651
Iteration 31, loss = 0.02791141
Iteration 32, loss = 0.02735102
Iteration 33, loss = 0.02689867
Iteration 34, loss = 0.02688411
Iteration 35, loss = 0.02645127
Iteration 36, loss = 0.02604858
Iteration 37, loss = 0.02580624
Iteration 38, loss = 0.02566888
Iteration 39, loss = 0.02541612
Iteration 40, loss = 0.02500563
Iteration 41, loss = 0.02498154
Iteration 42, loss = 0.02450520
Iteration 43, loss = 0.02473345
Iteration 44, loss = 0.02448594
Iteration 45, loss = 0.02423960
Iteration 46, loss = 0.02406630
Iteration 47, loss = 0.02396969
Iteration 48, loss = 0.02380471
Iteration 49, loss = 0.02401577
Iteration 50, loss = 0.02383568
Iteration 51, loss = 0.02372396
Iteration 52, loss = 0.02372050
Iteration 53, loss = 0.02332983
Iteration 54, loss = 0.02322839
Iteration 55, loss = 0.02340376
Iteration 56, loss = 0.02312362
Iteration 57, loss = 0.02315799
Iteration 58, loss = 0.02312776
Iteration 59, loss = 0.02291937
Iteration 60, loss = 0.02306254
Iteration 61, loss = 0.02277601
Iteration 62, loss = 0.02324485
Iteration 63, loss = 0.02296697
Iteration 64, loss = 0.02287871
Iteration 65, loss = 0.02278959
Iteration 66, loss = 0.02275371
Iteration 67, loss = 0.02254649
Iteration 68, loss = 0.02290506
Iteration 69, loss = 0.02238755
Iteration 70, loss = 0.02274517
Iteration 71, loss = 0.02234578
Iteration 72, loss = 0.02274160
Iteration 73, loss = 0.02276731
Iteration 74, loss = 0.02228388
Iteration 75, loss = 0.02230333
Iteration 76, loss = 0.02246770
Iteration 77, loss = 0.02205621
Iteration 78, loss = 0.02254387
Iteration 79, loss = 0.02249375
Iteration 80, loss = 0.02221927
Iteration 81, loss = 0.02222878
Iteration 82, loss = 0.02226432
Iteration 83, loss = 0.02211127
Iteration 84, loss = 0.02221979
Iteration 85, loss = 0.02231337
Iteration 86, loss = 0.02223525
Iteration 87, loss = 0.02190001
Iteration 88, loss = 0.02227877
Iteration 89, loss = 0.02227381
Iteration 90, loss = 0.02195031
Iteration 91, loss = 0.02215682
Iteration 92, loss = 0.02210302
Iteration 93, loss = 0.02204619
Iteration 94, loss = 0.02192693
Iteration 95, loss = 0.02209199
Iteration 96, loss = 0.02195523
Iteration 97, loss = 0.02198408
Iteration 98, loss = 0.02198521
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68909862
Iteration 2, loss = 0.65048134
Iteration 3, loss = 0.59979043
Iteration 4, loss = 0.53467248
Iteration 5, loss = 0.46168382
Iteration 6, loss = 0.38910200
Iteration 7, loss = 0.32367910
Iteration 8, loss = 0.26767049
Iteration 9, loss = 0.22156811
Iteration 10, loss = 0.18455597
Iteration 11, loss = 0.15529031
Iteration 12, loss = 0.13192712
Iteration 13, loss = 0.11354103
Iteration 14, loss = 0.09911268
Iteration 15, loss = 0.08741527
Iteration 16, loss = 0.07780378
Iteration 17, loss = 0.06997929
Iteration 18, loss = 0.06365168
Iteration 19, loss = 0.05828057
Iteration 20, loss = 0.05370027
Iteration 21, loss = 0.04984600
Iteration 22, loss = 0.04652530
Iteration 23, loss = 0.04369722
Iteration 24, loss = 0.04128038
Iteration 25, loss = 0.03909350
Iteration 26, loss = 0.03713846
Iteration 27, loss = 0.03560151
Iteration 28, loss = 0.03405510
Iteration 29, loss = 0.03273911
Iteration 30, loss = 0.03150705
Iteration 31, loss = 0.03043185
Iteration 32, loss = 0.02956051
Iteration 33, loss = 0.02869738
Iteration 34, loss = 0.02806835
Iteration 35, loss = 0.02714390
Iteration 36, loss = 0.02653128
Iteration 37, loss = 0.02607340
Iteration 38, loss = 0.02542061
Iteration 39, loss = 0.02490292
Iteration 40, loss = 0.02441863
Iteration 41, loss = 0.02400322
Iteration 42, loss = 0.02368838
Iteration 43, loss = 0.02325755
Iteration 44, loss = 0.02288542
Iteration 45, loss = 0.02247055
Iteration 46, loss = 0.02235872
Iteration 47, loss = 0.02192538
Iteration 48, loss = 0.02178000
Iteration 49, loss = 0.02157052
Iteration 50, loss = 0.02136265
Iteration 51, loss = 0.02117934
Iteration 52, loss = 0.02092126
Iteration 53, loss = 0.02072673
Iteration 54, loss = 0.02061184
Iteration 55, loss = 0.02047922
Iteration 56, loss = 0.02010845
Iteration 57, loss = 0.02007455
Iteration 58, loss = 0.01990404
Iteration 59, loss = 0.01979356
Iteration 60, loss = 0.01972975
Iteration 61, loss = 0.01964403
Iteration 62, loss = 0.01950708
Iteration 63, loss = 0.01930562
Iteration 64, loss = 0.01937683
Iteration 65, loss = 0.01923433
Iteration 66, loss = 0.01910069
Iteration 67, loss = 0.01909914
Iteration 68, loss = 0.01878906
Iteration 69, loss = 0.01872568
Iteration 70, loss = 0.01865817
Iteration 71, loss = 0.01864130
Iteration 72, loss = 0.01864453
Iteration 73, loss = 0.01863633
Iteration 74, loss = 0.01843606
Iteration 75, loss = 0.01840193
Iteration 76, loss = 0.01846189
Iteration 77, loss = 0.01837097
Iteration 78, loss = 0.01813336
Iteration 79, loss = 0.01808685
Iteration 80, loss = 0.01822889
Iteration 81, loss = 0.01820768
Iteration 82, loss = 0.01802282
Iteration 83, loss = 0.01821113
Iteration 84, loss = 0.01794267
Iteration 85, loss = 0.01781114
Iteration 86, loss = 0.01789124
Iteration 87, loss = 0.01785206
Iteration 88, loss = 0.01776423
Iteration 89, loss = 0.01768620
Iteration 90, loss = 0.01779527
Iteration 91, loss = 0.01765106
Iteration 92, loss = 0.01759696
Iteration 93, loss = 0.01773463
Iteration 94, loss = 0.01751183
Iteration 95, loss = 0.01751741
Iteration 96, loss = 0.01755019
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70667168
Iteration 2, loss = 0.64916773
Iteration 3, loss = 0.57762365
Iteration 4, loss = 0.49465103
Iteration 5, loss = 0.41244151
Iteration 6, loss = 0.33794052
Iteration 7, loss = 0.27415072
Iteration 8, loss = 0.22211229
Iteration 9, loss = 0.18116762
Iteration 10, loss = 0.14951281
Iteration 11, loss = 0.12493076
Iteration 12, loss = 0.10646059
Iteration 13, loss = 0.09176596
Iteration 14, loss = 0.08022510
Iteration 15, loss = 0.07111229
Iteration 16, loss = 0.06381651
Iteration 17, loss = 0.05781020
Iteration 18, loss = 0.05270313
Iteration 19, loss = 0.04852986
Iteration 20, loss = 0.04491428
Iteration 21, loss = 0.04216953
Iteration 22, loss = 0.03954733
Iteration 23, loss = 0.03718220
Iteration 24, loss = 0.03551069
Iteration 25, loss = 0.03361287
Iteration 26, loss = 0.03216708
Iteration 27, loss = 0.03096550
Iteration 28, loss = 0.02975078
Iteration 29, loss = 0.02889614
Iteration 30, loss = 0.02786782
Iteration 31, loss = 0.02687382
Iteration 32, loss = 0.02613630
Iteration 33, loss = 0.02565613
Iteration 34, loss = 0.02504439
Iteration 35, loss = 0.02425030
Iteration 36, loss = 0.02375360
Iteration 37, loss = 0.02334126
Iteration 38, loss = 0.02288921
Iteration 39, loss = 0.02259251
Iteration 40, loss = 0.02235822
Iteration 41, loss = 0.02197948
Iteration 42, loss = 0.02158284
Iteration 43, loss = 0.02136725
Iteration 44, loss = 0.02107914
Iteration 45, loss = 0.02059113
Iteration 46, loss = 0.02089766
Iteration 47, loss = 0.02044680
Iteration 48, loss = 0.01996206
Iteration 49, loss = 0.01988286
Iteration 50, loss = 0.01962364
Iteration 51, loss = 0.01942895
Iteration 52, loss = 0.01953325
Iteration 53, loss = 0.01926839
Iteration 54, loss = 0.01906814
Iteration 55, loss = 0.01898030
Iteration 56, loss = 0.01891575
Iteration 57, loss = 0.01888990
Iteration 58, loss = 0.01867578
Iteration 59, loss = 0.01842704
Iteration 60, loss = 0.01829442
Iteration 61, loss = 0.01834088
Iteration 62, loss = 0.01832363
Iteration 63, loss = 0.01809134
Iteration 64, loss = 0.01812110
Iteration 65, loss = 0.01809748
Iteration 66, loss = 0.01790205
Iteration 67, loss = 0.01789801
Iteration 68, loss = 0.01783957
Iteration 69, loss = 0.01771459
Iteration 70, loss = 0.01760594
Iteration 71, loss = 0.01753428
Iteration 72, loss = 0.01777535
Iteration 73, loss = 0.01745256
Iteration 74, loss = 0.01738138
Iteration 75, loss = 0.01742158
Iteration 76, loss = 0.01721850
Iteration 77, loss = 0.01728070
Iteration 78, loss = 0.01717168
Iteration 79, loss = 0.01719305
Iteration 80, loss = 0.01697271
Iteration 81, loss = 0.01710947
Iteration 82, loss = 0.01717709
Iteration 83, loss = 0.01689861
Iteration 84, loss = 0.01724468
Iteration 85, loss = 0.01710467
Iteration 86, loss = 0.01698206
Iteration 87, loss = 0.01699267
Iteration 88, loss = 0.01700119
Iteration 89, loss = 0.01681682
Iteration 90, loss = 0.01704993
Iteration 91, loss = 0.01714678
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69746631
Iteration 2, loss = 0.62052314
Iteration 3, loss = 0.51390102
Iteration 4, loss = 0.39703486
Iteration 5, loss = 0.29643650
Iteration 6, loss = 0.21995975
Iteration 7, loss = 0.16688773
Iteration 8, loss = 0.13057761
Iteration 9, loss = 0.10545492
Iteration 10, loss = 0.08804662
Iteration 11, loss = 0.07516547
Iteration 12, loss = 0.06523239
Iteration 13, loss = 0.05824471
Iteration 14, loss = 0.05238798
Iteration 15, loss = 0.04806149
Iteration 16, loss = 0.04457661
Iteration 17, loss = 0.04141849
Iteration 18, loss = 0.03882931
Iteration 19, loss = 0.03687315
Iteration 20, loss = 0.03502135
Iteration 21, loss = 0.03370572
Iteration 22, loss = 0.03264082
Iteration 23, loss = 0.03135603
Iteration 24, loss = 0.03034326
Iteration 25, loss = 0.02947888
Iteration 26, loss = 0.02871912
Iteration 27, loss = 0.02821662
Iteration 28, loss = 0.02772115
Iteration 29, loss = 0.02705197
Iteration 30, loss = 0.02650575
Iteration 31, loss = 0.02608501
Iteration 32, loss = 0.02581905
Iteration 33, loss = 0.02540897
Iteration 34, loss = 0.02530468
Iteration 35, loss = 0.02493937
Iteration 36, loss = 0.02459343
Iteration 37, loss = 0.02439601
Iteration 38, loss = 0.02417782
Iteration 39, loss = 0.02409823
Iteration 40, loss = 0.02370238
Iteration 41, loss = 0.02382390
Iteration 42, loss = 0.02331475
Iteration 43, loss = 0.02360475
Iteration 44, loss = 0.02352734
Iteration 45, loss = 0.02328131
Iteration 46, loss = 0.02300575
Iteration 47, loss = 0.02280286
Iteration 48, loss = 0.02278043
Iteration 49, loss = 0.02294821
Iteration 50, loss = 0.02287290
Iteration 51, loss = 0.02245413
Iteration 52, loss = 0.02241620
Iteration 53, loss = 0.02229166
Iteration 54, loss = 0.02214571
Iteration 55, loss = 0.02220329
Iteration 56, loss = 0.02220466
Iteration 57, loss = 0.02213633
Iteration 58, loss = 0.02187458
Iteration 59, loss = 0.02201104
Iteration 60, loss = 0.02177540
Iteration 61, loss = 0.02198369
Iteration 62, loss = 0.02183785
Iteration 63, loss = 0.02180612
Iteration 64, loss = 0.02183560
Iteration 65, loss = 0.02187721
Iteration 66, loss = 0.02166036
Iteration 67, loss = 0.02182089
Iteration 68, loss = 0.02145053
Iteration 69, loss = 0.02176926
Iteration 70, loss = 0.02180908
Iteration 71, loss = 0.02148693
Iteration 72, loss = 0.02146445
Iteration 73, loss = 0.02130683
Iteration 74, loss = 0.02129787
Iteration 75, loss = 0.02134425
Iteration 76, loss = 0.02163150
Iteration 77, loss = 0.02124512
Iteration 78, loss = 0.02144405
Iteration 79, loss = 0.02149321
Iteration 80, loss = 0.02131157
Iteration 81, loss = 0.02145940
Iteration 82, loss = 0.02150728
Iteration 83, loss = 0.02157104
Iteration 84, loss = 0.02148106
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67194956
Iteration 2, loss = 0.59484996
Iteration 3, loss = 0.48275065
Iteration 4, loss = 0.37111608
Iteration 5, loss = 0.27757871
Iteration 6, loss = 0.20854142
Iteration 7, loss = 0.15983290
Iteration 8, loss = 0.12678692
Iteration 9, loss = 0.10377606
Iteration 10, loss = 0.08744543
Iteration 11, loss = 0.07556255
Iteration 12, loss = 0.06631286
Iteration 13, loss = 0.05941997
Iteration 14, loss = 0.05413378
Iteration 15, loss = 0.04989886
Iteration 16, loss = 0.04672813
Iteration 17, loss = 0.04352795
Iteration 18, loss = 0.04135086
Iteration 19, loss = 0.03941938
Iteration 20, loss = 0.03751717
Iteration 21, loss = 0.03622436
Iteration 22, loss = 0.03477084
Iteration 23, loss = 0.03394484
Iteration 24, loss = 0.03299665
Iteration 25, loss = 0.03230434
Iteration 26, loss = 0.03147362
Iteration 27, loss = 0.03085588
Iteration 28, loss = 0.03055679
Iteration 29, loss = 0.03005981
Iteration 30, loss = 0.02922371
Iteration 31, loss = 0.02900438
Iteration 32, loss = 0.02852791
Iteration 33, loss = 0.02830918
Iteration 34, loss = 0.02792133
Iteration 35, loss = 0.02761719
Iteration 36, loss = 0.02745406
Iteration 37, loss = 0.02703891
Iteration 38, loss = 0.02708867
Iteration 39, loss = 0.02681824
Iteration 40, loss = 0.02691378
Iteration 41, loss = 0.02633450
Iteration 42, loss = 0.02641261
Iteration 43, loss = 0.02595661
Iteration 44, loss = 0.02604781
Iteration 45, loss = 0.02583031
Iteration 46, loss = 0.02587484
Iteration 47, loss = 0.02579296
Iteration 48, loss = 0.02574832
Iteration 49, loss = 0.02515722
Iteration 50, loss = 0.02547119
Iteration 51, loss = 0.02523456
Iteration 52, loss = 0.02537481
Iteration 53, loss = 0.02519981
Iteration 54, loss = 0.02500391
Iteration 55, loss = 0.02515169
Iteration 56, loss = 0.02487508
Iteration 57, loss = 0.02536249
Iteration 58, loss = 0.02465680
Iteration 59, loss = 0.02514178
Iteration 60, loss = 0.02444818
Iteration 61, loss = 0.02478286
Iteration 62, loss = 0.02480711
Iteration 63, loss = 0.02484020
Iteration 64, loss = 0.02440370
Iteration 65, loss = 0.02444134
Iteration 66, loss = 0.02449861
Iteration 67, loss = 0.02474873
Iteration 68, loss = 0.02437589
Iteration 69, loss = 0.02429083
Iteration 70, loss = 0.02468121
Iteration 71, loss = 0.02451112
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69490923
Iteration 2, loss = 0.63173542
Iteration 3, loss = 0.55009604
Iteration 4, loss = 0.45143485
Iteration 5, loss = 0.35643659
Iteration 6, loss = 0.27746047
Iteration 7, loss = 0.21697197
Iteration 8, loss = 0.17261937
Iteration 9, loss = 0.14019762
Iteration 10, loss = 0.11648616
Iteration 11, loss = 0.09905701
Iteration 12, loss = 0.08581774
Iteration 13, loss = 0.07623790
Iteration 14, loss = 0.06860175
Iteration 15, loss = 0.06218732
Iteration 16, loss = 0.05724009
Iteration 17, loss = 0.05328732
Iteration 18, loss = 0.04971130
Iteration 19, loss = 0.04703408
Iteration 20, loss = 0.04469077
Iteration 21, loss = 0.04242624
Iteration 22, loss = 0.04081729
Iteration 23, loss = 0.03924697
Iteration 24, loss = 0.03792152
Iteration 25, loss = 0.03680494
Iteration 26, loss = 0.03575538
Iteration 27, loss = 0.03489223
Iteration 28, loss = 0.03416843
Iteration 29, loss = 0.03369337
Iteration 30, loss = 0.03339871
Iteration 31, loss = 0.03305168
Iteration 32, loss = 0.03229623
Iteration 33, loss = 0.03191731
Iteration 34, loss = 0.03130104
Iteration 35, loss = 0.03057426
Iteration 36, loss = 0.03022124
Iteration 37, loss = 0.02981219
Iteration 38, loss = 0.02946930
Iteration 39, loss = 0.02897048
Iteration 40, loss = 0.02870912
Iteration 41, loss = 0.02858669
Iteration 42, loss = 0.02820863
Iteration 43, loss = 0.02845968
Iteration 44, loss = 0.02941884
Iteration 45, loss = 0.02876855
Iteration 46, loss = 0.02845159
Iteration 47, loss = 0.02854806
Iteration 48, loss = 0.02796742
Iteration 49, loss = 0.02796673
Iteration 50, loss = 0.02746631
Iteration 51, loss = 0.02731864
Iteration 52, loss = 0.02723624
Iteration 53, loss = 0.02696459
Iteration 54, loss = 0.02695285
Iteration 55, loss = 0.02694455
Iteration 56, loss = 0.02659193
Iteration 57, loss = 0.02665937
Iteration 58, loss = 0.02637819
Iteration 59, loss = 0.02616572
Iteration 60, loss = 0.02659366
Iteration 61, loss = 0.02831401
Iteration 62, loss = 0.02856697
Iteration 63, loss = 0.02839660
Iteration 64, loss = 0.02813142
Iteration 65, loss = 0.02803337
Iteration 66, loss = 0.02723921
Iteration 67, loss = 0.02711541
Iteration 68, loss = 0.02675247
Iteration 69, loss = 0.02639305
Iteration 70, loss = 0.02602286
Iteration 71, loss = 0.02594993
Iteration 72, loss = 0.02558890
Iteration 73, loss = 0.02561218
Iteration 74, loss = 0.02543194
Iteration 75, loss = 0.02527559
Iteration 76, loss = 0.02506925
Iteration 77, loss = 0.02510353
Iteration 78, loss = 0.02499776
Iteration 79, loss = 0.02481410
Iteration 80, loss = 0.02583816
Iteration 81, loss = 0.02587254
Iteration 82, loss = 0.02545897
Iteration 83, loss = 0.02533221
Iteration 84, loss = 0.02502865
Iteration 85, loss = 0.02519219
Iteration 86, loss = 0.02499921
Iteration 87, loss = 0.02515129
Iteration 88, loss = 0.02489086
Iteration 89, loss = 0.02465615
Iteration 90, loss = 0.02482690
Iteration 91, loss = 0.02486569
Iteration 92, loss = 0.02485965
Iteration 93, loss = 0.02449456
Iteration 94, loss = 0.02435521
Iteration 95, loss = 0.02431162
Iteration 96, loss = 0.02439956
Iteration 97, loss = 0.02420318
Iteration 98, loss = 0.02415898
Iteration 99, loss = 0.02418062
Iteration 100, loss = 0.02378909
Iteration 101, loss = 0.02392655
Iteration 102, loss = 0.02379636
Iteration 103, loss = 0.02405409
Iteration 104, loss = 0.02390741
Iteration 105, loss = 0.02399801
Iteration 106, loss = 0.02390550
Iteration 107, loss = 0.02384444
Iteration 108, loss = 0.02380778
Iteration 109, loss = 0.02371081
Iteration 110, loss = 0.02356624
Iteration 111, loss = 0.02371034
Iteration 112, loss = 0.02368483
Iteration 113, loss = 0.02367889
Iteration 114, loss = 0.02364632
Iteration 115, loss = 0.02377625
Iteration 116, loss = 0.02357482
Iteration 117, loss = 0.02348872
Iteration 118, loss = 0.02362322
Iteration 119, loss = 0.02344526
Iteration 120, loss = 0.02360209
Iteration 121, loss = 0.02334651
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67916894
Iteration 2, loss = 0.61180960
Iteration 3, loss = 0.51005060
Iteration 4, loss = 0.40193584
Iteration 5, loss = 0.30745055
Iteration 6, loss = 0.23344671
Iteration 7, loss = 0.17976772
Iteration 8, loss = 0.14204470
Iteration 9, loss = 0.11539094
Iteration 10, loss = 0.09621857
Iteration 11, loss = 0.08255844
Iteration 12, loss = 0.07207802
Iteration 13, loss = 0.06404769
Iteration 14, loss = 0.05800522
Iteration 15, loss = 0.05305189
Iteration 16, loss = 0.04888476
Iteration 17, loss = 0.04574439
Iteration 18, loss = 0.04311449
Iteration 19, loss = 0.04103089
Iteration 20, loss = 0.03906430
Iteration 21, loss = 0.03719475
Iteration 22, loss = 0.03573457
Iteration 23, loss = 0.03458794
Iteration 24, loss = 0.03377019
Iteration 25, loss = 0.03288444
Iteration 26, loss = 0.03180425
Iteration 27, loss = 0.03115573
Iteration 28, loss = 0.03052743
Iteration 29, loss = 0.02977396
Iteration 30, loss = 0.02958935
Iteration 31, loss = 0.02912337
Iteration 32, loss = 0.02842747
Iteration 33, loss = 0.02825868
Iteration 34, loss = 0.02793458
Iteration 35, loss = 0.02752974
Iteration 36, loss = 0.02725070
Iteration 37, loss = 0.02705269
Iteration 38, loss = 0.02696479
Iteration 39, loss = 0.02645533
Iteration 40, loss = 0.02670032
Iteration 41, loss = 0.02607643
Iteration 42, loss = 0.02613231
Iteration 43, loss = 0.02593717
Iteration 44, loss = 0.02570767
Iteration 45, loss = 0.02568486
Iteration 46, loss = 0.02531796
Iteration 47, loss = 0.02538019
Iteration 48, loss = 0.02491255
Iteration 49, loss = 0.02502110
Iteration 50, loss = 0.02489400
Iteration 51, loss = 0.02477306
Iteration 52, loss = 0.02513907
Iteration 53, loss = 0.02462999
Iteration 54, loss = 0.02456457
Iteration 55, loss = 0.02468151
Iteration 56, loss = 0.02445593
Iteration 57, loss = 0.02420778
Iteration 58, loss = 0.02434751
Iteration 59, loss = 0.02426785
Iteration 60, loss = 0.02434714
Iteration 61, loss = 0.02411219
Iteration 62, loss = 0.02419451
Iteration 63, loss = 0.02388152
Iteration 64, loss = 0.02405843
Iteration 65, loss = 0.02392816
Iteration 66, loss = 0.02407951
Iteration 67, loss = 0.02377747
Iteration 68, loss = 0.02397147
Iteration 69, loss = 0.02399528
Iteration 70, loss = 0.02404214
Iteration 71, loss = 0.02367990
Iteration 72, loss = 0.02371542
Iteration 73, loss = 0.02383059
Iteration 74, loss = 0.02347415
Iteration 75, loss = 0.02356356
Iteration 76, loss = 0.02359039
Iteration 77, loss = 0.02371675
Iteration 78, loss = 0.02345084
Iteration 79, loss = 0.02346070
Iteration 80, loss = 0.02363786
Iteration 81, loss = 0.02329018
Iteration 82, loss = 0.02344948
Iteration 83, loss = 0.02333926
Iteration 84, loss = 0.02328508
Iteration 85, loss = 0.02331934
Iteration 86, loss = 0.02329941
Iteration 87, loss = 0.02373016
Iteration 88, loss = 0.02320725
Iteration 89, loss = 0.02313288
Iteration 90, loss = 0.02352569
Iteration 91, loss = 0.02325083
Iteration 92, loss = 0.02312978
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68498828
Iteration 2, loss = 0.65578433
Iteration 3, loss = 0.61730130
Iteration 4, loss = 0.56502501
Iteration 5, loss = 0.50371730
Iteration 6, loss = 0.44148202
Iteration 7, loss = 0.38212640
Iteration 8, loss = 0.32798600
Iteration 9, loss = 0.27990628
Iteration 10, loss = 0.23857339
Iteration 11, loss = 0.20359914
Iteration 12, loss = 0.17462497
Iteration 13, loss = 0.15061836
Iteration 14, loss = 0.13093831
Iteration 15, loss = 0.11476993
Iteration 16, loss = 0.10137069
Iteration 17, loss = 0.09030224
Iteration 18, loss = 0.08144493
Iteration 19, loss = 0.07345311
Iteration 20, loss = 0.06710735
Iteration 21, loss = 0.06142120
Iteration 22, loss = 0.05672245
Iteration 23, loss = 0.05267507
Iteration 24, loss = 0.04906265
Iteration 25, loss = 0.04599431
Iteration 26, loss = 0.04320205
Iteration 27, loss = 0.04077200
Iteration 28, loss = 0.03864630
Iteration 29, loss = 0.03674195
Iteration 30, loss = 0.03505619
Iteration 31, loss = 0.03359950
Iteration 32, loss = 0.03217644
Iteration 33, loss = 0.03099048
Iteration 34, loss = 0.02992800
Iteration 35, loss = 0.02897116
Iteration 36, loss = 0.02800349
Iteration 37, loss = 0.02717374
Iteration 38, loss = 0.02637503
Iteration 39, loss = 0.02572205
Iteration 40, loss = 0.02508657
Iteration 41, loss = 0.02444918
Iteration 42, loss = 0.02390793
Iteration 43, loss = 0.02339689
Iteration 44, loss = 0.02292485
Iteration 45, loss = 0.02254131
Iteration 46, loss = 0.02215986
Iteration 47, loss = 0.02181971
Iteration 48, loss = 0.02125559
Iteration 49, loss = 0.02091512
Iteration 50, loss = 0.02066568
Iteration 51, loss = 0.02045231
Iteration 52, loss = 0.02013678
Iteration 53, loss = 0.01987632
Iteration 54, loss = 0.01961145
Iteration 55, loss = 0.01963834
Iteration 56, loss = 0.01921192
Iteration 57, loss = 0.01897685
Iteration 58, loss = 0.01882300
Iteration 59, loss = 0.01853313
Iteration 60, loss = 0.01839427
Iteration 61, loss = 0.01824603
Iteration 62, loss = 0.01810538
Iteration 63, loss = 0.01801511
Iteration 64, loss = 0.01788865
Iteration 65, loss = 0.01778073
Iteration 66, loss = 0.01747960
Iteration 67, loss = 0.01742288
Iteration 68, loss = 0.01727126
Iteration 69, loss = 0.01731245
Iteration 70, loss = 0.01724428
Iteration 71, loss = 0.01709368
Iteration 72, loss = 0.01692982
Iteration 73, loss = 0.01683789
Iteration 74, loss = 0.01669715
Iteration 75, loss = 0.01668697
Iteration 76, loss = 0.01667568
Iteration 77, loss = 0.01639465
Iteration 78, loss = 0.01642750
Iteration 79, loss = 0.01637729
Iteration 80, loss = 0.01634706
Iteration 81, loss = 0.01633226
Iteration 82, loss = 0.01612858
Iteration 83, loss = 0.01613788
Iteration 84, loss = 0.01604142
Iteration 85, loss = 0.01600341
Iteration 86, loss = 0.01594712
Iteration 87, loss = 0.01596870
Iteration 88, loss = 0.01597389
Iteration 89, loss = 0.01584434
Iteration 90, loss = 0.01565148
Iteration 91, loss = 0.01561231
Iteration 92, loss = 0.01575697
Iteration 93, loss = 0.01564130
Iteration 94, loss = 0.01564126
Iteration 95, loss = 0.01550097
Iteration 96, loss = 0.01559351
Iteration 97, loss = 0.01553257
Iteration 98, loss = 0.01543175
Iteration 99, loss = 0.01542085
Iteration 100, loss = 0.01536775
Iteration 101, loss = 0.01526535
Iteration 102, loss = 0.01541718
Iteration 103, loss = 0.01520574
Iteration 104, loss = 0.01520029
Iteration 105, loss = 0.01530738
Iteration 106, loss = 0.01520473
Iteration 107, loss = 0.01519220
Iteration 108, loss = 0.01508465
Iteration 109, loss = 0.01501832
Iteration 110, loss = 0.01510501
Iteration 111, loss = 0.01526220
Iteration 112, loss = 0.01499455
Iteration 113, loss = 0.01503635
Iteration 114, loss = 0.01486849
Iteration 115, loss = 0.01497490
Iteration 116, loss = 0.01489161
Iteration 117, loss = 0.01488556
Iteration 118, loss = 0.01496686
Iteration 119, loss = 0.01490002
Iteration 120, loss = 0.01486696
Iteration 121, loss = 0.01478518
Iteration 122, loss = 0.01483022
Iteration 123, loss = 0.01480590
Iteration 124, loss = 0.01482644
Iteration 125, loss = 0.01486438
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70320733
Iteration 2, loss = 0.68393061
Iteration 3, loss = 0.66688061
Iteration 4, loss = 0.64954546
Iteration 5, loss = 0.63134548
Iteration 6, loss = 0.61213606
Iteration 7, loss = 0.59184023
Iteration 8, loss = 0.57026650
Iteration 9, loss = 0.54792517
Iteration 10, loss = 0.52475351
Iteration 11, loss = 0.50076918
Iteration 12, loss = 0.47680672
Iteration 13, loss = 0.45279298
Iteration 14, loss = 0.42885876
Iteration 15, loss = 0.40528928
Iteration 16, loss = 0.38237471
Iteration 17, loss = 0.36026031
Iteration 18, loss = 0.33894891
Iteration 19, loss = 0.31858735
Iteration 20, loss = 0.29911178
Iteration 21, loss = 0.28072369
Iteration 22, loss = 0.26337668
Iteration 23, loss = 0.24706064
Iteration 24, loss = 0.23171454
Iteration 25, loss = 0.21743442
Iteration 26, loss = 0.20404683
Iteration 27, loss = 0.19155394
Iteration 28, loss = 0.17996763
Iteration 29, loss = 0.16914519
Iteration 30, loss = 0.15906499
Iteration 31, loss = 0.14979984
Iteration 32, loss = 0.14106700
Iteration 33, loss = 0.13307226
Iteration 34, loss = 0.12568852
Iteration 35, loss = 0.11870102
Iteration 36, loss = 0.11229043
Iteration 37, loss = 0.10636046
Iteration 38, loss = 0.10085997
Iteration 39, loss = 0.09574541
Iteration 40, loss = 0.09097552
Iteration 41, loss = 0.08652942
Iteration 42, loss = 0.08244560
Iteration 43, loss = 0.07858574
Iteration 44, loss = 0.07504627
Iteration 45, loss = 0.07170790
Iteration 46, loss = 0.06862306
Iteration 47, loss = 0.06570500
Iteration 48, loss = 0.06302707
Iteration 49, loss = 0.06051171
Iteration 50, loss = 0.05808505
Iteration 51, loss = 0.05587424
Iteration 52, loss = 0.05375031
Iteration 53, loss = 0.05181060
Iteration 54, loss = 0.04994509
Iteration 55, loss = 0.04821755
Iteration 56, loss = 0.04655469
Iteration 57, loss = 0.04498887
Iteration 58, loss = 0.04358906
Iteration 59, loss = 0.04215405
Iteration 60, loss = 0.04084090
Iteration 61, loss = 0.03959602
Iteration 62, loss = 0.03845317
Iteration 63, loss = 0.03732755
Iteration 64, loss = 0.03629863
Iteration 65, loss = 0.03528711
Iteration 66, loss = 0.03434971
Iteration 67, loss = 0.03342602
Iteration 68, loss = 0.03260438
Iteration 69, loss = 0.03178236
Iteration 70, loss = 0.03099224
Iteration 71, loss = 0.03025206
Iteration 72, loss = 0.02954109
Iteration 73, loss = 0.02887210
Iteration 74, loss = 0.02819825
Iteration 75, loss = 0.02758628
Iteration 76, loss = 0.02702456
Iteration 77, loss = 0.02642694
Iteration 78, loss = 0.02590727
Iteration 79, loss = 0.02536152
Iteration 80, loss = 0.02486898
Iteration 81, loss = 0.02437773
Iteration 82, loss = 0.02391556
Iteration 83, loss = 0.02346438
Iteration 84, loss = 0.02302921
Iteration 85, loss = 0.02260907
Iteration 86, loss = 0.02221423
Iteration 87, loss = 0.02182479
Iteration 88, loss = 0.02145859
Iteration 89, loss = 0.02109597
Iteration 90, loss = 0.02074053
Iteration 91, loss = 0.02041191
Iteration 92, loss = 0.02011523
Iteration 93, loss = 0.01980331
Iteration 94, loss = 0.01953546
Iteration 95, loss = 0.01922004
Iteration 96, loss = 0.01900579
Iteration 97, loss = 0.01869668
Iteration 98, loss = 0.01841923
Iteration 99, loss = 0.01817938
Iteration 100, loss = 0.01793099
Iteration 101, loss = 0.01770668
Iteration 102, loss = 0.01749092
Iteration 103, loss = 0.01723508
Iteration 104, loss = 0.01701687
Iteration 105, loss = 0.01678987
Iteration 106, loss = 0.01663542
Iteration 107, loss = 0.01647717
Iteration 108, loss = 0.01625046
Iteration 109, loss = 0.01605355
Iteration 110, loss = 0.01590476
Iteration 111, loss = 0.01571559
Iteration 112, loss = 0.01557300
Iteration 113, loss = 0.01538653
Iteration 114, loss = 0.01521120
Iteration 115, loss = 0.01505922
Iteration 116, loss = 0.01491241
Iteration 117, loss = 0.01475348
Iteration 118, loss = 0.01462361
Iteration 119, loss = 0.01447284
Iteration 120, loss = 0.01435022
Iteration 121, loss = 0.01419984
Iteration 122, loss = 0.01415449
Iteration 123, loss = 0.01395730
Iteration 124, loss = 0.01385856
Iteration 125, loss = 0.01370742
Iteration 126, loss = 0.01365199
Iteration 127, loss = 0.01356690
Iteration 128, loss = 0.01344165
Iteration 129, loss = 0.01330962
Iteration 130, loss = 0.01320769
Iteration 131, loss = 0.01308740
Iteration 132, loss = 0.01297854
Iteration 133, loss = 0.01286086
Iteration 134, loss = 0.01276597
Iteration 135, loss = 0.01268626
Iteration 136, loss = 0.01261165
Iteration 137, loss = 0.01253746
Iteration 138, loss = 0.01243647
Iteration 139, loss = 0.01235539
Iteration 140, loss = 0.01226053
Iteration 141, loss = 0.01218614
Iteration 142, loss = 0.01209891
Iteration 143, loss = 0.01204948
Iteration 144, loss = 0.01193510
Iteration 145, loss = 0.01183796
Iteration 146, loss = 0.01176432
Iteration 147, loss = 0.01173129
Iteration 148, loss = 0.01165514
Iteration 149, loss = 0.01162960
Iteration 150, loss = 0.01154895
Iteration 151, loss = 0.01151536
Iteration 152, loss = 0.01140517
Iteration 153, loss = 0.01132412
Iteration 154, loss = 0.01130109
Iteration 155, loss = 0.01119621
Iteration 156, loss = 0.01113904
Iteration 157, loss = 0.01112247
Iteration 158, loss = 0.01103704
Iteration 159, loss = 0.01097518
Iteration 160, loss = 0.01094058
Iteration 161, loss = 0.01085639
Iteration 162, loss = 0.01082116
Iteration 163, loss = 0.01075268
Iteration 164, loss = 0.01071678
Iteration 165, loss = 0.01072511
Iteration 166, loss = 0.01063124
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72276148
Iteration 2, loss = 0.69222239
Iteration 3, loss = 0.65886252
Iteration 4, loss = 0.61860843
Iteration 5, loss = 0.57207575
Iteration 6, loss = 0.52076187
Iteration 7, loss = 0.46830018
Iteration 8, loss = 0.41603613
Iteration 9, loss = 0.36617437
Iteration 10, loss = 0.32068392
Iteration 11, loss = 0.28011629
Iteration 12, loss = 0.24467847
Iteration 13, loss = 0.21408019
Iteration 14, loss = 0.18783656
Iteration 15, loss = 0.16564644
Iteration 16, loss = 0.14694485
Iteration 17, loss = 0.13091283
Iteration 18, loss = 0.11746044
Iteration 19, loss = 0.10585241
Iteration 20, loss = 0.09594308
Iteration 21, loss = 0.08739093
Iteration 22, loss = 0.08007861
Iteration 23, loss = 0.07362989
Iteration 24, loss = 0.06826343
Iteration 25, loss = 0.06352933
Iteration 26, loss = 0.05928586
Iteration 27, loss = 0.05544410
Iteration 28, loss = 0.05207745
Iteration 29, loss = 0.04916014
Iteration 30, loss = 0.04646617
Iteration 31, loss = 0.04401539
Iteration 32, loss = 0.04185985
Iteration 33, loss = 0.03990081
Iteration 34, loss = 0.03808394
Iteration 35, loss = 0.03645570
Iteration 36, loss = 0.03498288
Iteration 37, loss = 0.03360577
Iteration 38, loss = 0.03234908
Iteration 39, loss = 0.03114862
Iteration 40, loss = 0.03006642
Iteration 41, loss = 0.02914193
Iteration 42, loss = 0.02824900
Iteration 43, loss = 0.02737991
Iteration 44, loss = 0.02668567
Iteration 45, loss = 0.02601682
Iteration 46, loss = 0.02547368
Iteration 47, loss = 0.02472646
Iteration 48, loss = 0.02413260
Iteration 49, loss = 0.02365028
Iteration 50, loss = 0.02309599
Iteration 51, loss = 0.02260231
Iteration 52, loss = 0.02211139
Iteration 53, loss = 0.02172544
Iteration 54, loss = 0.02139963
Iteration 55, loss = 0.02093786
Iteration 56, loss = 0.02064916
Iteration 57, loss = 0.02021323
Iteration 58, loss = 0.01996284
Iteration 59, loss = 0.01962259
Iteration 60, loss = 0.01932762
Iteration 61, loss = 0.01902965
Iteration 62, loss = 0.01880247
Iteration 63, loss = 0.01855318
Iteration 64, loss = 0.01837917
Iteration 65, loss = 0.01815551
Iteration 66, loss = 0.01796185
Iteration 67, loss = 0.01781385
Iteration 68, loss = 0.01763456
Iteration 69, loss = 0.01757612
Iteration 70, loss = 0.01757976
Iteration 71, loss = 0.01737237
Iteration 72, loss = 0.01736754
Iteration 73, loss = 0.01739488
Iteration 74, loss = 0.01752897
Iteration 75, loss = 0.01722355
Iteration 76, loss = 0.01691188
Iteration 77, loss = 0.01702157
Iteration 78, loss = 0.01673281
Iteration 79, loss = 0.01656872
Iteration 80, loss = 0.01621520
Iteration 81, loss = 0.01596951
Iteration 82, loss = 0.01581481
Iteration 83, loss = 0.01567342
Iteration 84, loss = 0.01556855
Iteration 85, loss = 0.01544941
Iteration 86, loss = 0.01537652
Iteration 87, loss = 0.01520790
Iteration 88, loss = 0.01514150
Iteration 89, loss = 0.01498881
Iteration 90, loss = 0.01504885
Iteration 91, loss = 0.01498290
Iteration 92, loss = 0.01480321
Iteration 93, loss = 0.01482612
Iteration 94, loss = 0.01474252
Iteration 95, loss = 0.01454512
Iteration 96, loss = 0.01462412
Iteration 97, loss = 0.01465351
Iteration 98, loss = 0.01464783
Iteration 99, loss = 0.01463217
Iteration 100, loss = 0.01457240
Iteration 101, loss = 0.01451922
Iteration 102, loss = 0.01434487
Iteration 103, loss = 0.01428340
Iteration 104, loss = 0.01418315
Iteration 105, loss = 0.01408134
Iteration 106, loss = 0.01409100
Iteration 107, loss = 0.01402499
Iteration 108, loss = 0.01399003
Iteration 109, loss = 0.01408318
Iteration 110, loss = 0.01414998
Iteration 111, loss = 0.01399778
Iteration 112, loss = 0.01387916
Iteration 113, loss = 0.01378683
Iteration 114, loss = 0.01373692
Iteration 115, loss = 0.01368120
Iteration 116, loss = 0.01369517
Iteration 117, loss = 0.01357812
Iteration 118, loss = 0.01354934
Iteration 119, loss = 0.01349271
Iteration 120, loss = 0.01344285
Iteration 121, loss = 0.01340242
Iteration 122, loss = 0.01342741
Iteration 123, loss = 0.01328972
Iteration 124, loss = 0.01332810
Iteration 125, loss = 0.01330559
Iteration 126, loss = 0.01333843
Iteration 127, loss = 0.01319525
Iteration 128, loss = 0.01322108
Iteration 129, loss = 0.01314258
Iteration 130, loss = 0.01321917
Iteration 131, loss = 0.01353391
Iteration 132, loss = 0.01368893
Iteration 133, loss = 0.01357554
Iteration 134, loss = 0.01347747
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68175712
Iteration 2, loss = 0.62227458
Iteration 3, loss = 0.53707579
Iteration 4, loss = 0.44337417
Iteration 5, loss = 0.35658710
Iteration 6, loss = 0.28271468
Iteration 7, loss = 0.22360851
Iteration 8, loss = 0.17850593
Iteration 9, loss = 0.14474318
Iteration 10, loss = 0.11971401
Iteration 11, loss = 0.10094301
Iteration 12, loss = 0.08651604
Iteration 13, loss = 0.07557703
Iteration 14, loss = 0.06674744
Iteration 15, loss = 0.05992310
Iteration 16, loss = 0.05433946
Iteration 17, loss = 0.04958745
Iteration 18, loss = 0.04596793
Iteration 19, loss = 0.04265338
Iteration 20, loss = 0.04015275
Iteration 21, loss = 0.03778692
Iteration 22, loss = 0.03574215
Iteration 23, loss = 0.03416549
Iteration 24, loss = 0.03273266
Iteration 25, loss = 0.03138273
Iteration 26, loss = 0.03016727
Iteration 27, loss = 0.02920929
Iteration 28, loss = 0.02821321
Iteration 29, loss = 0.02746317
Iteration 30, loss = 0.02680142
Iteration 31, loss = 0.02617203
Iteration 32, loss = 0.02576620
Iteration 33, loss = 0.02508672
Iteration 34, loss = 0.02470462
Iteration 35, loss = 0.02421931
Iteration 36, loss = 0.02375724
Iteration 37, loss = 0.02351968
Iteration 38, loss = 0.02311778
Iteration 39, loss = 0.02285400
Iteration 40, loss = 0.02285446
Iteration 41, loss = 0.02242808
Iteration 42, loss = 0.02193133
Iteration 43, loss = 0.02193649
Iteration 44, loss = 0.02181345
Iteration 45, loss = 0.02170772
Iteration 46, loss = 0.02147602
Iteration 47, loss = 0.02148877
Iteration 48, loss = 0.02108028
Iteration 49, loss = 0.02081411
Iteration 50, loss = 0.02112185
Iteration 51, loss = 0.02061445
Iteration 52, loss = 0.02046402
Iteration 53, loss = 0.02059439
Iteration 54, loss = 0.02028643
Iteration 55, loss = 0.02014822
Iteration 56, loss = 0.02040367
Iteration 57, loss = 0.02003594
Iteration 58, loss = 0.01981678
Iteration 59, loss = 0.01996748
Iteration 60, loss = 0.01975051
Iteration 61, loss = 0.01964442
Iteration 62, loss = 0.01952995
Iteration 63, loss = 0.01950699
Iteration 64, loss = 0.01972687
Iteration 65, loss = 0.01940041
Iteration 66, loss = 0.01917174
Iteration 67, loss = 0.01945091
Iteration 68, loss = 0.01921310
Iteration 69, loss = 0.01946517
Iteration 70, loss = 0.01916917
Iteration 71, loss = 0.01897924
Iteration 72, loss = 0.01904433
Iteration 73, loss = 0.01909640
Iteration 74, loss = 0.01884505
Iteration 75, loss = 0.01902601
Iteration 76, loss = 0.01915548
Iteration 77, loss = 0.01862819
Iteration 78, loss = 0.01887510
Iteration 79, loss = 0.01896752
Iteration 80, loss = 0.01873775
Iteration 81, loss = 0.01872908
Iteration 82, loss = 0.01868421
Iteration 83, loss = 0.01875415
Iteration 84, loss = 0.01866813
Iteration 85, loss = 0.01886072
Iteration 86, loss = 0.01874180
Iteration 87, loss = 0.01861974
Iteration 88, loss = 0.01852217
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67287176
Iteration 2, loss = 0.59610111
Iteration 3, loss = 0.48177085
Iteration 4, loss = 0.37022378
Iteration 5, loss = 0.27766838
Iteration 6, loss = 0.20897817
Iteration 7, loss = 0.16086247
Iteration 8, loss = 0.12745660
Iteration 9, loss = 0.10460342
Iteration 10, loss = 0.08793621
Iteration 11, loss = 0.07605324
Iteration 12, loss = 0.06645502
Iteration 13, loss = 0.05974202
Iteration 14, loss = 0.05412942
Iteration 15, loss = 0.04954199
Iteration 16, loss = 0.04618917
Iteration 17, loss = 0.04328136
Iteration 18, loss = 0.04087233
Iteration 19, loss = 0.03901984
Iteration 20, loss = 0.03692484
Iteration 21, loss = 0.03580736
Iteration 22, loss = 0.03398697
Iteration 23, loss = 0.03326107
Iteration 24, loss = 0.03217610
Iteration 25, loss = 0.03162638
Iteration 26, loss = 0.03061756
Iteration 27, loss = 0.03000347
Iteration 28, loss = 0.02949553
Iteration 29, loss = 0.02897094
Iteration 30, loss = 0.02847291
Iteration 31, loss = 0.02813693
Iteration 32, loss = 0.02781042
Iteration 33, loss = 0.02754784
Iteration 34, loss = 0.02712562
Iteration 35, loss = 0.02688137
Iteration 36, loss = 0.02656597
Iteration 37, loss = 0.02632722
Iteration 38, loss = 0.02628005
Iteration 39, loss = 0.02597522
Iteration 40, loss = 0.02583588
Iteration 41, loss = 0.02559115
Iteration 42, loss = 0.02543807
Iteration 43, loss = 0.02549591
Iteration 44, loss = 0.02505550
Iteration 45, loss = 0.02511588
Iteration 46, loss = 0.02495579
Iteration 47, loss = 0.02537953
Iteration 48, loss = 0.02491128
Iteration 49, loss = 0.02464082
Iteration 50, loss = 0.02454828
Iteration 51, loss = 0.02471316
Iteration 52, loss = 0.02449041
Iteration 53, loss = 0.02433097
Iteration 54, loss = 0.02432380
Iteration 55, loss = 0.02417101
Iteration 56, loss = 0.02435942
Iteration 57, loss = 0.02401266
Iteration 58, loss = 0.02422787
Iteration 59, loss = 0.02415633
Iteration 60, loss = 0.02395048
Iteration 61, loss = 0.02390586
Iteration 62, loss = 0.02372482
Iteration 63, loss = 0.02390247
Iteration 64, loss = 0.02384008
Iteration 65, loss = 0.02369397
Iteration 66, loss = 0.02395469
Iteration 67, loss = 0.02364133
Iteration 68, loss = 0.02374366
Iteration 69, loss = 0.02387494
Iteration 70, loss = 0.02345643
Iteration 71, loss = 0.02366593
Iteration 72, loss = 0.02363844
Iteration 73, loss = 0.02343314
Iteration 74, loss = 0.02329614
Iteration 75, loss = 0.02334323
Iteration 76, loss = 0.02372816
Iteration 77, loss = 0.02347204
Iteration 78, loss = 0.02333935
Iteration 79, loss = 0.02336074
Iteration 80, loss = 0.02331709
Iteration 81, loss = 0.02353573
Iteration 82, loss = 0.02330480
Iteration 83, loss = 0.02314311
Iteration 84, loss = 0.02333259
Iteration 85, loss = 0.02305007
Iteration 86, loss = 0.02304150
Iteration 87, loss = 0.02309551
Iteration 88, loss = 0.02350633
Iteration 89, loss = 0.02313548
Iteration 90, loss = 0.02316139
Iteration 91, loss = 0.02314237
Iteration 92, loss = 0.02324957
Iteration 93, loss = 0.02299329
Iteration 94, loss = 0.02320407
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67841205
Iteration 2, loss = 0.65105150
Iteration 3, loss = 0.61491635
Iteration 4, loss = 0.56654644
Iteration 5, loss = 0.50884236
Iteration 6, loss = 0.44803555
Iteration 7, loss = 0.38860359
Iteration 8, loss = 0.33330326
Iteration 9, loss = 0.28406245
Iteration 10, loss = 0.24111691
Iteration 11, loss = 0.20486685
Iteration 12, loss = 0.17504107
Iteration 13, loss = 0.15026959
Iteration 14, loss = 0.13007888
Iteration 15, loss = 0.11370987
Iteration 16, loss = 0.10037825
Iteration 17, loss = 0.08914496
Iteration 18, loss = 0.07996097
Iteration 19, loss = 0.07217437
Iteration 20, loss = 0.06567027
Iteration 21, loss = 0.06027017
Iteration 22, loss = 0.05551309
Iteration 23, loss = 0.05137443
Iteration 24, loss = 0.04794331
Iteration 25, loss = 0.04488818
Iteration 26, loss = 0.04217156
Iteration 27, loss = 0.03990103
Iteration 28, loss = 0.03776858
Iteration 29, loss = 0.03612673
Iteration 30, loss = 0.03435822
Iteration 31, loss = 0.03277878
Iteration 32, loss = 0.03162174
Iteration 33, loss = 0.03028657
Iteration 34, loss = 0.02916228
Iteration 35, loss = 0.02831522
Iteration 36, loss = 0.02736895
Iteration 37, loss = 0.02653725
Iteration 38, loss = 0.02595640
Iteration 39, loss = 0.02519442
Iteration 40, loss = 0.02457242
Iteration 41, loss = 0.02382701
Iteration 42, loss = 0.02330464
Iteration 43, loss = 0.02286183
Iteration 44, loss = 0.02244959
Iteration 45, loss = 0.02202131
Iteration 46, loss = 0.02161324
Iteration 47, loss = 0.02118314
Iteration 48, loss = 0.02082615
Iteration 49, loss = 0.02056488
Iteration 50, loss = 0.02016321
Iteration 51, loss = 0.02006736
Iteration 52, loss = 0.01965034
Iteration 53, loss = 0.01957182
Iteration 54, loss = 0.01926279
Iteration 55, loss = 0.01903022
Iteration 56, loss = 0.01877491
Iteration 57, loss = 0.01858074
Iteration 58, loss = 0.01836023
Iteration 59, loss = 0.01828234
Iteration 60, loss = 0.01810280
Iteration 61, loss = 0.01786120
Iteration 62, loss = 0.01774053
Iteration 63, loss = 0.01770590
Iteration 64, loss = 0.01746335
Iteration 65, loss = 0.01736460
Iteration 66, loss = 0.01733631
Iteration 67, loss = 0.01709909
Iteration 68, loss = 0.01692428
Iteration 69, loss = 0.01693008
Iteration 70, loss = 0.01690941
Iteration 71, loss = 0.01668406
Iteration 72, loss = 0.01664902
Iteration 73, loss = 0.01655277
Iteration 74, loss = 0.01644907
Iteration 75, loss = 0.01624279
Iteration 76, loss = 0.01627931
Iteration 77, loss = 0.01626949
Iteration 78, loss = 0.01618724
Iteration 79, loss = 0.01611566
Iteration 80, loss = 0.01600169
Iteration 81, loss = 0.01598078
Iteration 82, loss = 0.01591399
Iteration 83, loss = 0.01576463
Iteration 84, loss = 0.01574355
Iteration 85, loss = 0.01571793
Iteration 86, loss = 0.01556061
Iteration 87, loss = 0.01561399
Iteration 88, loss = 0.01552691
Iteration 89, loss = 0.01557456
Iteration 90, loss = 0.01547811
Iteration 91, loss = 0.01547419
Iteration 92, loss = 0.01526817
Iteration 93, loss = 0.01525469
Iteration 94, loss = 0.01538255
Iteration 95, loss = 0.01537901
Iteration 96, loss = 0.01526059
Iteration 97, loss = 0.01518193
Iteration 98, loss = 0.01530065
Iteration 99, loss = 0.01518952
Iteration 100, loss = 0.01511808
Iteration 101, loss = 0.01512245
Iteration 102, loss = 0.01506313
Iteration 103, loss = 0.01506293
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70582831
Iteration 2, loss = 0.62687880
Iteration 3, loss = 0.51635804
Iteration 4, loss = 0.39335572
Iteration 5, loss = 0.28958395
Iteration 6, loss = 0.21333110
Iteration 7, loss = 0.16159994
Iteration 8, loss = 0.12635181
Iteration 9, loss = 0.10229700
Iteration 10, loss = 0.08544177
Iteration 11, loss = 0.07329113
Iteration 12, loss = 0.06388201
Iteration 13, loss = 0.05667497
Iteration 14, loss = 0.05132578
Iteration 15, loss = 0.04708954
Iteration 16, loss = 0.04348809
Iteration 17, loss = 0.04073013
Iteration 18, loss = 0.03826376
Iteration 19, loss = 0.03646402
Iteration 20, loss = 0.03463971
Iteration 21, loss = 0.03349119
Iteration 22, loss = 0.03209450
Iteration 23, loss = 0.03113223
Iteration 24, loss = 0.03023083
Iteration 25, loss = 0.02913872
Iteration 26, loss = 0.02858799
Iteration 27, loss = 0.02789954
Iteration 28, loss = 0.02727782
Iteration 29, loss = 0.02689185
Iteration 30, loss = 0.02638588
Iteration 31, loss = 0.02592736
Iteration 32, loss = 0.02578627
Iteration 33, loss = 0.02537329
Iteration 34, loss = 0.02515205
Iteration 35, loss = 0.02449239
Iteration 36, loss = 0.02451976
Iteration 37, loss = 0.02421235
Iteration 38, loss = 0.02425941
Iteration 39, loss = 0.02384447
Iteration 40, loss = 0.02381272
Iteration 41, loss = 0.02372323
Iteration 42, loss = 0.02333929
Iteration 43, loss = 0.02302381
Iteration 44, loss = 0.02311453
Iteration 45, loss = 0.02281124
Iteration 46, loss = 0.02285480
Iteration 47, loss = 0.02245150
Iteration 48, loss = 0.02252900
Iteration 49, loss = 0.02225468
Iteration 50, loss = 0.02234850
Iteration 51, loss = 0.02231618
Iteration 52, loss = 0.02225633
Iteration 53, loss = 0.02233700
Iteration 54, loss = 0.02208898
Iteration 55, loss = 0.02199049
Iteration 56, loss = 0.02218084
Iteration 57, loss = 0.02187358
Iteration 58, loss = 0.02190110
Iteration 59, loss = 0.02162140
Iteration 60, loss = 0.02163411
Iteration 61, loss = 0.02173686
Iteration 62, loss = 0.02159410
Iteration 63, loss = 0.02163128
Iteration 64, loss = 0.02143269
Iteration 65, loss = 0.02151122
Iteration 66, loss = 0.02178523
Iteration 67, loss = 0.02130015
Iteration 68, loss = 0.02153471
Iteration 69, loss = 0.02106989
Iteration 70, loss = 0.02126422
Iteration 71, loss = 0.02131534
Iteration 72, loss = 0.02104967
Iteration 73, loss = 0.02144613
Iteration 74, loss = 0.02121860
Iteration 75, loss = 0.02139657
Iteration 76, loss = 0.02113750
Iteration 77, loss = 0.02083258
Iteration 78, loss = 0.02136472
Iteration 79, loss = 0.02104109
Iteration 80, loss = 0.02107233
Iteration 81, loss = 0.02094907
Iteration 82, loss = 0.02080516
Iteration 83, loss = 0.02122783
Iteration 84, loss = 0.02113348
Iteration 85, loss = 0.02093639
Iteration 86, loss = 0.02100446
Iteration 87, loss = 0.02081051
Iteration 88, loss = 0.02090792
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70638556
Iteration 2, loss = 0.67870478
Iteration 3, loss = 0.63927457
Iteration 4, loss = 0.58796989
Iteration 5, loss = 0.52550813
Iteration 6, loss = 0.45955752
Iteration 7, loss = 0.39457391
Iteration 8, loss = 0.33514699
Iteration 9, loss = 0.28304572
Iteration 10, loss = 0.23893171
Iteration 11, loss = 0.20203710
Iteration 12, loss = 0.17177409
Iteration 13, loss = 0.14708708
Iteration 14, loss = 0.12704306
Iteration 15, loss = 0.11062252
Iteration 16, loss = 0.09725087
Iteration 17, loss = 0.08612622
Iteration 18, loss = 0.07695782
Iteration 19, loss = 0.06928211
Iteration 20, loss = 0.06273503
Iteration 21, loss = 0.05729950
Iteration 22, loss = 0.05259206
Iteration 23, loss = 0.04854992
Iteration 24, loss = 0.04506160
Iteration 25, loss = 0.04204187
Iteration 26, loss = 0.03937824
Iteration 27, loss = 0.03709702
Iteration 28, loss = 0.03491676
Iteration 29, loss = 0.03312549
Iteration 30, loss = 0.03159063
Iteration 31, loss = 0.03003661
Iteration 32, loss = 0.02854814
Iteration 33, loss = 0.02750784
Iteration 34, loss = 0.02639298
Iteration 35, loss = 0.02544985
Iteration 36, loss = 0.02449850
Iteration 37, loss = 0.02368800
Iteration 38, loss = 0.02296085
Iteration 39, loss = 0.02238313
Iteration 40, loss = 0.02165783
Iteration 41, loss = 0.02096302
Iteration 42, loss = 0.02063345
Iteration 43, loss = 0.02014274
Iteration 44, loss = 0.01953946
Iteration 45, loss = 0.01924454
Iteration 46, loss = 0.01873513
Iteration 47, loss = 0.01836408
Iteration 48, loss = 0.01797727
Iteration 49, loss = 0.01766080
Iteration 50, loss = 0.01736355
Iteration 51, loss = 0.01713398
Iteration 52, loss = 0.01685586
Iteration 53, loss = 0.01648321
Iteration 54, loss = 0.01640813
Iteration 55, loss = 0.01616602
Iteration 56, loss = 0.01583550
Iteration 57, loss = 0.01571339
Iteration 58, loss = 0.01555729
Iteration 59, loss = 0.01529600
Iteration 60, loss = 0.01517014
Iteration 61, loss = 0.01504782
Iteration 62, loss = 0.01500758
Iteration 63, loss = 0.01477553
Iteration 64, loss = 0.01454818
Iteration 65, loss = 0.01445948
Iteration 66, loss = 0.01432726
Iteration 67, loss = 0.01418977
Iteration 68, loss = 0.01417583
Iteration 69, loss = 0.01405435
Iteration 70, loss = 0.01392204
Iteration 71, loss = 0.01381689
Iteration 72, loss = 0.01367881
Iteration 73, loss = 0.01368763
Iteration 74, loss = 0.01356200
Iteration 75, loss = 0.01347834
Iteration 76, loss = 0.01337375
Iteration 77, loss = 0.01335668
Iteration 78, loss = 0.01329246
Iteration 79, loss = 0.01323940
Iteration 80, loss = 0.01307913
Iteration 81, loss = 0.01312255
Iteration 82, loss = 0.01305208
Iteration 83, loss = 0.01297264
Iteration 84, loss = 0.01284660
Iteration 85, loss = 0.01282737
Iteration 86, loss = 0.01275712
Iteration 87, loss = 0.01270527
Iteration 88, loss = 0.01270754
Iteration 89, loss = 0.01251248
Iteration 90, loss = 0.01272875
Iteration 91, loss = 0.01257463
Iteration 92, loss = 0.01255527
Iteration 93, loss = 0.01247566
Iteration 94, loss = 0.01234875
Iteration 95, loss = 0.01242156
Iteration 96, loss = 0.01231571
Iteration 97, loss = 0.01230365
Iteration 98, loss = 0.01227477
Iteration 99, loss = 0.01211635
Iteration 100, loss = 0.01215434
Iteration 101, loss = 0.01224499
Iteration 102, loss = 0.01214775
Iteration 103, loss = 0.01218065
Iteration 104, loss = 0.01210664
Iteration 105, loss = 0.01211417
Iteration 106, loss = 0.01200305
Iteration 107, loss = 0.01201874
Iteration 108, loss = 0.01208429
Iteration 109, loss = 0.01195389
Iteration 110, loss = 0.01199935
Iteration 111, loss = 0.01190913
Iteration 112, loss = 0.01194536
Iteration 113, loss = 0.01183854
Iteration 114, loss = 0.01185702
Iteration 115, loss = 0.01173560
Iteration 116, loss = 0.01189648
Iteration 117, loss = 0.01191127
Iteration 118, loss = 0.01193660
Iteration 119, loss = 0.01177120
Iteration 120, loss = 0.01164189
Iteration 121, loss = 0.01177020
Iteration 122, loss = 0.01179149
Iteration 123, loss = 0.01161088
Iteration 124, loss = 0.01171775
Iteration 125, loss = 0.01169963
Iteration 126, loss = 0.01166487
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69591583
Iteration 2, loss = 0.62233060
Iteration 3, loss = 0.52398525
Iteration 4, loss = 0.41642763
Iteration 5, loss = 0.32403809
Iteration 6, loss = 0.25135227
Iteration 7, loss = 0.19664781
Iteration 8, loss = 0.15637069
Iteration 9, loss = 0.12740798
Iteration 10, loss = 0.10585775
Iteration 11, loss = 0.09003076
Iteration 12, loss = 0.07821670
Iteration 13, loss = 0.06873568
Iteration 14, loss = 0.06157484
Iteration 15, loss = 0.05595201
Iteration 16, loss = 0.05129821
Iteration 17, loss = 0.04721185
Iteration 18, loss = 0.04434706
Iteration 19, loss = 0.04139737
Iteration 20, loss = 0.03912381
Iteration 21, loss = 0.03721721
Iteration 22, loss = 0.03557947
Iteration 23, loss = 0.03413430
Iteration 24, loss = 0.03281580
Iteration 25, loss = 0.03171228
Iteration 26, loss = 0.03084808
Iteration 27, loss = 0.02995211
Iteration 28, loss = 0.02929962
Iteration 29, loss = 0.02825993
Iteration 30, loss = 0.02780406
Iteration 31, loss = 0.02716672
Iteration 32, loss = 0.02668815
Iteration 33, loss = 0.02617950
Iteration 34, loss = 0.02575273
Iteration 35, loss = 0.02560301
Iteration 36, loss = 0.02503799
Iteration 37, loss = 0.02478059
Iteration 38, loss = 0.02472540
Iteration 39, loss = 0.02425973
Iteration 40, loss = 0.02425300
Iteration 41, loss = 0.02371526
Iteration 42, loss = 0.02394455
Iteration 43, loss = 0.02355243
Iteration 44, loss = 0.02317550
Iteration 45, loss = 0.02302641
Iteration 46, loss = 0.02293948
Iteration 47, loss = 0.02256362
Iteration 48, loss = 0.02251307
Iteration 49, loss = 0.02219340
Iteration 50, loss = 0.02199716
Iteration 51, loss = 0.02209521
Iteration 52, loss = 0.02187077
Iteration 53, loss = 0.02195967
Iteration 54, loss = 0.02181427
Iteration 55, loss = 0.02155515
Iteration 56, loss = 0.02179174
Iteration 57, loss = 0.02159441
Iteration 58, loss = 0.02131708
Iteration 59, loss = 0.02154282
Iteration 60, loss = 0.02107637
Iteration 61, loss = 0.02110135
Iteration 62, loss = 0.02118060
Iteration 63, loss = 0.02115966
Iteration 64, loss = 0.02095527
Iteration 65, loss = 0.02074309
Iteration 66, loss = 0.02084911
Iteration 67, loss = 0.02227085
Iteration 68, loss = 0.02220903
Iteration 69, loss = 0.02164928
Iteration 70, loss = 0.02145761
Iteration 71, loss = 0.02147955
Iteration 72, loss = 0.02183371
Iteration 73, loss = 0.02245507
Iteration 74, loss = 0.02202555
Iteration 75, loss = 0.02168232
Iteration 76, loss = 0.02150208
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69191245
Iteration 2, loss = 0.66069695
Iteration 3, loss = 0.61344835
Iteration 4, loss = 0.54629572
Iteration 5, loss = 0.47215500
Iteration 6, loss = 0.40009646
Iteration 7, loss = 0.33449163
Iteration 8, loss = 0.27774182
Iteration 9, loss = 0.23048724
Iteration 10, loss = 0.19198284
Iteration 11, loss = 0.16131318
Iteration 12, loss = 0.13688529
Iteration 13, loss = 0.11762553
Iteration 14, loss = 0.10225962
Iteration 15, loss = 0.08999913
Iteration 16, loss = 0.08002210
Iteration 17, loss = 0.07187145
Iteration 18, loss = 0.06515810
Iteration 19, loss = 0.05948079
Iteration 20, loss = 0.05472262
Iteration 21, loss = 0.05073027
Iteration 22, loss = 0.04735329
Iteration 23, loss = 0.04437929
Iteration 24, loss = 0.04194744
Iteration 25, loss = 0.03968137
Iteration 26, loss = 0.03769714
Iteration 27, loss = 0.03621092
Iteration 28, loss = 0.03464392
Iteration 29, loss = 0.03314689
Iteration 30, loss = 0.03192137
Iteration 31, loss = 0.03085590
Iteration 32, loss = 0.02995254
Iteration 33, loss = 0.02915135
Iteration 34, loss = 0.02836493
Iteration 35, loss = 0.02752399
Iteration 36, loss = 0.02696964
Iteration 37, loss = 0.02614037
Iteration 38, loss = 0.02568887
Iteration 39, loss = 0.02511605
Iteration 40, loss = 0.02466626
Iteration 41, loss = 0.02434850
Iteration 42, loss = 0.02402359
Iteration 43, loss = 0.02344234
Iteration 44, loss = 0.02318660
Iteration 45, loss = 0.02296853
Iteration 46, loss = 0.02263017
Iteration 47, loss = 0.02228655
Iteration 48, loss = 0.02228096
Iteration 49, loss = 0.02186593
Iteration 50, loss = 0.02147065
Iteration 51, loss = 0.02128726
Iteration 52, loss = 0.02125677
Iteration 53, loss = 0.02099734
Iteration 54, loss = 0.02081992
Iteration 55, loss = 0.02058560
Iteration 56, loss = 0.02043777
Iteration 57, loss = 0.02026912
Iteration 58, loss = 0.02025000
Iteration 59, loss = 0.02028752
Iteration 60, loss = 0.02030371
Iteration 61, loss = 0.01975508
Iteration 62, loss = 0.01966622
Iteration 63, loss = 0.01975417
Iteration 64, loss = 0.01953263
Iteration 65, loss = 0.01941953
Iteration 66, loss = 0.01937982
Iteration 67, loss = 0.01919628
Iteration 68, loss = 0.01918570
Iteration 69, loss = 0.01904694
Iteration 70, loss = 0.01893154
Iteration 71, loss = 0.01904814
Iteration 72, loss = 0.01869801
Iteration 73, loss = 0.01869054
Iteration 74, loss = 0.01871079
Iteration 75, loss = 0.01855984
Iteration 76, loss = 0.01852450
Iteration 77, loss = 0.01855037
Iteration 78, loss = 0.01856368
Iteration 79, loss = 0.01848433
Iteration 80, loss = 0.01834403
Iteration 81, loss = 0.01836313
Iteration 82, loss = 0.01837875
Iteration 83, loss = 0.01826797
Iteration 84, loss = 0.01824723
Iteration 85, loss = 0.01819822
Iteration 86, loss = 0.01796738
Iteration 87, loss = 0.01803468
Iteration 88, loss = 0.01804439
Iteration 89, loss = 0.01794253
Iteration 90, loss = 0.01797302
Iteration 91, loss = 0.01789279
Iteration 92, loss = 0.01782722
Iteration 93, loss = 0.01784184
Iteration 94, loss = 0.01775589
Iteration 95, loss = 0.01778856
Iteration 96, loss = 0.01773990
Iteration 97, loss = 0.01767390
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69586416
Iteration 2, loss = 0.66627029
Iteration 3, loss = 0.62669116
Iteration 4, loss = 0.57431364
Iteration 5, loss = 0.51249060
Iteration 6, loss = 0.44851705
Iteration 7, loss = 0.38697746
Iteration 8, loss = 0.33040659
Iteration 9, loss = 0.28041916
Iteration 10, loss = 0.23739256
Iteration 11, loss = 0.20143720
Iteration 12, loss = 0.17171674
Iteration 13, loss = 0.14737685
Iteration 14, loss = 0.12746997
Iteration 15, loss = 0.11118844
Iteration 16, loss = 0.09779033
Iteration 17, loss = 0.08683185
Iteration 18, loss = 0.07747939
Iteration 19, loss = 0.06990240
Iteration 20, loss = 0.06350916
Iteration 21, loss = 0.05797659
Iteration 22, loss = 0.05319719
Iteration 23, loss = 0.04913834
Iteration 24, loss = 0.04565386
Iteration 25, loss = 0.04258040
Iteration 26, loss = 0.03990180
Iteration 27, loss = 0.03752576
Iteration 28, loss = 0.03549418
Iteration 29, loss = 0.03361470
Iteration 30, loss = 0.03195536
Iteration 31, loss = 0.03051101
Iteration 32, loss = 0.02913203
Iteration 33, loss = 0.02795156
Iteration 34, loss = 0.02675633
Iteration 35, loss = 0.02575241
Iteration 36, loss = 0.02486612
Iteration 37, loss = 0.02401831
Iteration 38, loss = 0.02325929
Iteration 39, loss = 0.02257182
Iteration 40, loss = 0.02187223
Iteration 41, loss = 0.02137144
Iteration 42, loss = 0.02078158
Iteration 43, loss = 0.02024814
Iteration 44, loss = 0.01980882
Iteration 45, loss = 0.01931375
Iteration 46, loss = 0.01889474
Iteration 47, loss = 0.01859103
Iteration 48, loss = 0.01814086
Iteration 49, loss = 0.01789971
Iteration 50, loss = 0.01750698
Iteration 51, loss = 0.01734143
Iteration 52, loss = 0.01696984
Iteration 53, loss = 0.01668518
Iteration 54, loss = 0.01649773
Iteration 55, loss = 0.01630260
Iteration 56, loss = 0.01606389
Iteration 57, loss = 0.01580794
Iteration 58, loss = 0.01563370
Iteration 59, loss = 0.01545314
Iteration 60, loss = 0.01525880
Iteration 61, loss = 0.01525088
Iteration 62, loss = 0.01497039
Iteration 63, loss = 0.01486360
Iteration 64, loss = 0.01475318
Iteration 65, loss = 0.01453715
Iteration 66, loss = 0.01448792
Iteration 67, loss = 0.01429831
Iteration 68, loss = 0.01423561
Iteration 69, loss = 0.01409754
Iteration 70, loss = 0.01402113
Iteration 71, loss = 0.01388404
Iteration 72, loss = 0.01374356
Iteration 73, loss = 0.01363746
Iteration 74, loss = 0.01352095
Iteration 75, loss = 0.01351301
Iteration 76, loss = 0.01340127
Iteration 77, loss = 0.01336879
Iteration 78, loss = 0.01325783
Iteration 79, loss = 0.01322437
Iteration 80, loss = 0.01307026
Iteration 81, loss = 0.01308499
Iteration 82, loss = 0.01303924
Iteration 83, loss = 0.01298264
Iteration 84, loss = 0.01286911
Iteration 85, loss = 0.01280801
Iteration 86, loss = 0.01273957
Iteration 87, loss = 0.01270085
Iteration 88, loss = 0.01258085
Iteration 89, loss = 0.01259788
Iteration 90, loss = 0.01258048
Iteration 91, loss = 0.01253590
Iteration 92, loss = 0.01247290
Iteration 93, loss = 0.01235604
Iteration 94, loss = 0.01239801
Iteration 95, loss = 0.01236942
Iteration 96, loss = 0.01233519
Iteration 97, loss = 0.01215694
Iteration 98, loss = 0.01227416
Iteration 99, loss = 0.01228800
Iteration 100, loss = 0.01219031
Iteration 101, loss = 0.01224543
Iteration 102, loss = 0.01222642
Iteration 103, loss = 0.01210007
Iteration 104, loss = 0.01202362
Iteration 105, loss = 0.01203344
Iteration 106, loss = 0.01199819
Iteration 107, loss = 0.01195813
Iteration 108, loss = 0.01193864
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67636580
Iteration 2, loss = 0.58985917
Iteration 3, loss = 0.46857946
Iteration 4, loss = 0.35295488
Iteration 5, loss = 0.26138056
Iteration 6, loss = 0.19516883
Iteration 7, loss = 0.15013481
Iteration 8, loss = 0.11923546
Iteration 9, loss = 0.09758991
Iteration 10, loss = 0.08210795
Iteration 11, loss = 0.07071051
Iteration 12, loss = 0.06221708
Iteration 13, loss = 0.05527659
Iteration 14, loss = 0.05040538
Iteration 15, loss = 0.04625982
Iteration 16, loss = 0.04265527
Iteration 17, loss = 0.03996265
Iteration 18, loss = 0.03785325
Iteration 19, loss = 0.03557854
Iteration 20, loss = 0.03407429
Iteration 21, loss = 0.03259191
Iteration 22, loss = 0.03138552
Iteration 23, loss = 0.03053233
Iteration 24, loss = 0.02946664
Iteration 25, loss = 0.02876952
Iteration 26, loss = 0.02783235
Iteration 27, loss = 0.02739291
Iteration 28, loss = 0.02680430
Iteration 29, loss = 0.02661520
Iteration 30, loss = 0.02567738
Iteration 31, loss = 0.02574678
Iteration 32, loss = 0.02489176
Iteration 33, loss = 0.02474944
Iteration 34, loss = 0.02450360
Iteration 35, loss = 0.02430508
Iteration 36, loss = 0.02404351
Iteration 37, loss = 0.02386432
Iteration 38, loss = 0.02362606
Iteration 39, loss = 0.02344947
Iteration 40, loss = 0.02339847
Iteration 41, loss = 0.02307416
Iteration 42, loss = 0.02296044
Iteration 43, loss = 0.02302772
Iteration 44, loss = 0.02248580
Iteration 45, loss = 0.02262367
Iteration 46, loss = 0.02236543
Iteration 47, loss = 0.02245792
Iteration 48, loss = 0.02219408
Iteration 49, loss = 0.02215407
Iteration 50, loss = 0.02237024
Iteration 51, loss = 0.02233776
Iteration 52, loss = 0.02184205
Iteration 53, loss = 0.02195063
Iteration 54, loss = 0.02185099
Iteration 55, loss = 0.02156018
Iteration 56, loss = 0.02164197
Iteration 57, loss = 0.02144985
Iteration 58, loss = 0.02156306
Iteration 59, loss = 0.02165262
Iteration 60, loss = 0.02151756
Iteration 61, loss = 0.02151416
Iteration 62, loss = 0.02144998
Iteration 63, loss = 0.02129813
Iteration 64, loss = 0.02125039
Iteration 65, loss = 0.02129122
Iteration 66, loss = 0.02122789
Iteration 67, loss = 0.02135820
Iteration 68, loss = 0.02124193
Iteration 69, loss = 0.02108178
Iteration 70, loss = 0.02096235
Iteration 71, loss = 0.02128295
Iteration 72, loss = 0.02121126
Iteration 73, loss = 0.02113891
Iteration 74, loss = 0.02132869
Iteration 75, loss = 0.02128635
Iteration 76, loss = 0.02105508
Iteration 77, loss = 0.02079950
Iteration 78, loss = 0.02117522
Iteration 79, loss = 0.02104782
Iteration 80, loss = 0.02086810
Iteration 81, loss = 0.02076752
Iteration 82, loss = 0.02125838
Iteration 83, loss = 0.02079302
Iteration 84, loss = 0.02077514
Iteration 85, loss = 0.02078225
Iteration 86, loss = 0.02057140
Iteration 87, loss = 0.02052091
Iteration 88, loss = 0.02067414
Iteration 89, loss = 0.02079540
Iteration 90, loss = 0.02081261
Iteration 91, loss = 0.02105691
Iteration 92, loss = 0.02038454
Iteration 93, loss = 0.02040970
Iteration 94, loss = 0.02060372
Iteration 95, loss = 0.02057475
Iteration 96, loss = 0.02094603
Iteration 97, loss = 0.02036805
Iteration 98, loss = 0.02057099
Iteration 99, loss = 0.02082756
Iteration 100, loss = 0.02068945
Iteration 101, loss = 0.02044796
Iteration 102, loss = 0.02096068
Iteration 103, loss = 0.02040786
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70098870
Iteration 2, loss = 0.65907923
Iteration 3, loss = 0.59777239
Iteration 4, loss = 0.51511082
Iteration 5, loss = 0.42618152
Iteration 6, loss = 0.34422250
Iteration 7, loss = 0.27643788
Iteration 8, loss = 0.22239189
Iteration 9, loss = 0.18084179
Iteration 10, loss = 0.14935586
Iteration 11, loss = 0.12550920
Iteration 12, loss = 0.10720072
Iteration 13, loss = 0.09298478
Iteration 14, loss = 0.08192732
Iteration 15, loss = 0.07308881
Iteration 16, loss = 0.06590903
Iteration 17, loss = 0.05997748
Iteration 18, loss = 0.05516030
Iteration 19, loss = 0.05103175
Iteration 20, loss = 0.04763240
Iteration 21, loss = 0.04483644
Iteration 22, loss = 0.04224070
Iteration 23, loss = 0.04004103
Iteration 24, loss = 0.03819808
Iteration 25, loss = 0.03656848
Iteration 26, loss = 0.03526354
Iteration 27, loss = 0.03394054
Iteration 28, loss = 0.03262212
Iteration 29, loss = 0.03161391
Iteration 30, loss = 0.03062827
Iteration 31, loss = 0.02988406
Iteration 32, loss = 0.02925129
Iteration 33, loss = 0.02857145
Iteration 34, loss = 0.02791847
Iteration 35, loss = 0.02729947
Iteration 36, loss = 0.02697604
Iteration 37, loss = 0.02641962
Iteration 38, loss = 0.02610147
Iteration 39, loss = 0.02547111
Iteration 40, loss = 0.02543793
Iteration 41, loss = 0.02498447
Iteration 42, loss = 0.02440884
Iteration 43, loss = 0.02450371
Iteration 44, loss = 0.02411412
Iteration 45, loss = 0.02372451
Iteration 46, loss = 0.02384210
Iteration 47, loss = 0.02334198
Iteration 48, loss = 0.02324482
Iteration 49, loss = 0.02296456
Iteration 50, loss = 0.02275177
Iteration 51, loss = 0.02249536
Iteration 52, loss = 0.02260080
Iteration 53, loss = 0.02219424
Iteration 54, loss = 0.02208362
Iteration 55, loss = 0.02233841
Iteration 56, loss = 0.02178711
Iteration 57, loss = 0.02187671
Iteration 58, loss = 0.02174847
Iteration 59, loss = 0.02186969
Iteration 60, loss = 0.02158191
Iteration 61, loss = 0.02127929
Iteration 62, loss = 0.02131192
Iteration 63, loss = 0.02112802
Iteration 64, loss = 0.02113442
Iteration 65, loss = 0.02091444
Iteration 66, loss = 0.02082827
Iteration 67, loss = 0.02083211
Iteration 68, loss = 0.02095879
Iteration 69, loss = 0.02074750
Iteration 70, loss = 0.02064778
Iteration 71, loss = 0.02054599
Iteration 72, loss = 0.02067930
Iteration 73, loss = 0.02044487
Iteration 74, loss = 0.02029444
Iteration 75, loss = 0.02052187
Iteration 76, loss = 0.02052732
Iteration 77, loss = 0.02053076
Iteration 78, loss = 0.02015814
Iteration 79, loss = 0.02034823
Iteration 80, loss = 0.02056942
Iteration 81, loss = 0.02023412
Iteration 82, loss = 0.02013041
Iteration 83, loss = 0.02004850
Iteration 84, loss = 0.02013129
Iteration 85, loss = 0.02008008
Iteration 86, loss = 0.01973344
Iteration 87, loss = 0.01980054
Iteration 88, loss = 0.02001994
Iteration 89, loss = 0.02035013
Iteration 90, loss = 0.02008602
Iteration 91, loss = 0.01977333
Iteration 92, loss = 0.01983316
Iteration 93, loss = 0.01970130
Iteration 94, loss = 0.01968472
Iteration 95, loss = 0.01981191
Iteration 96, loss = 0.01978807
Iteration 97, loss = 0.01971102
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69752117
Iteration 2, loss = 0.61618007
Iteration 3, loss = 0.51105275
Iteration 4, loss = 0.40027500
Iteration 5, loss = 0.30511092
Iteration 6, loss = 0.23062153
Iteration 7, loss = 0.17673465
Iteration 8, loss = 0.13878936
Iteration 9, loss = 0.11194159
Iteration 10, loss = 0.09300193
Iteration 11, loss = 0.07894583
Iteration 12, loss = 0.06851425
Iteration 13, loss = 0.06070008
Iteration 14, loss = 0.05420318
Iteration 15, loss = 0.04935974
Iteration 16, loss = 0.04571893
Iteration 17, loss = 0.04213285
Iteration 18, loss = 0.03931873
Iteration 19, loss = 0.03703876
Iteration 20, loss = 0.03507043
Iteration 21, loss = 0.03351774
Iteration 22, loss = 0.03220425
Iteration 23, loss = 0.03067886
Iteration 24, loss = 0.02973021
Iteration 25, loss = 0.02900985
Iteration 26, loss = 0.02794871
Iteration 27, loss = 0.02747011
Iteration 28, loss = 0.02654042
Iteration 29, loss = 0.02605738
Iteration 30, loss = 0.02563039
Iteration 31, loss = 0.02497911
Iteration 32, loss = 0.02472772
Iteration 33, loss = 0.02438030
Iteration 34, loss = 0.02407711
Iteration 35, loss = 0.02353696
Iteration 36, loss = 0.02353486
Iteration 37, loss = 0.02302644
Iteration 38, loss = 0.02290649
Iteration 39, loss = 0.02279649
Iteration 40, loss = 0.02254358
Iteration 41, loss = 0.02240216
Iteration 42, loss = 0.02185304
Iteration 43, loss = 0.02241925
Iteration 44, loss = 0.02170289
Iteration 45, loss = 0.02179274
Iteration 46, loss = 0.02188512
Iteration 47, loss = 0.02143181
Iteration 48, loss = 0.02168937
Iteration 49, loss = 0.02114570
Iteration 50, loss = 0.02132983
Iteration 51, loss = 0.02108373
Iteration 52, loss = 0.02117447
Iteration 53, loss = 0.02077194
Iteration 54, loss = 0.02078973
Iteration 55, loss = 0.02070125
Iteration 56, loss = 0.02084285
Iteration 57, loss = 0.02071136
Iteration 58, loss = 0.02068102
Iteration 59, loss = 0.02065646
Iteration 60, loss = 0.02056736
Iteration 61, loss = 0.02039552
Iteration 62, loss = 0.02053737
Iteration 63, loss = 0.02034530
Iteration 64, loss = 0.02029514
Iteration 65, loss = 0.02011648
Iteration 66, loss = 0.02048769
Iteration 67, loss = 0.02028522
Iteration 68, loss = 0.02025151
Iteration 69, loss = 0.02011890
Iteration 70, loss = 0.02019409
Iteration 71, loss = 0.02025148
Iteration 72, loss = 0.02019832
Iteration 73, loss = 0.01997854
Iteration 74, loss = 0.01989620
Iteration 75, loss = 0.01990240
Iteration 76, loss = 0.01991408
Iteration 77, loss = 0.02002802
Iteration 78, loss = 0.01968664
Iteration 79, loss = 0.02002366
Iteration 80, loss = 0.01983626
Iteration 81, loss = 0.01973034
Iteration 82, loss = 0.01970616
Iteration 83, loss = 0.01960783
Iteration 84, loss = 0.01955231
Iteration 85, loss = 0.01972095
Iteration 86, loss = 0.01981427
Iteration 87, loss = 0.01957040
Iteration 88, loss = 0.01955024
Iteration 89, loss = 0.02004681
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	87m5.629s
user	379m43.501s
sys	46m59.498s
Elapsed time: 5226 seconds
