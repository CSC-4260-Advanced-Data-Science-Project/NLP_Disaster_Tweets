Processing dataset: dropped_v5_lemma_stem

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: dropped_v5_lemma_stem (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on dropped_v5_lemma_stem...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on dropped_v5_lemma_stem...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on dropped_v5_lemma_stem...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on dropped_v5_lemma_stem...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on dropped_v5_lemma_stem...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67272670
Iteration 2, loss = 0.59869920
Iteration 3, loss = 0.48984010
Iteration 4, loss = 0.37957189
Iteration 5, loss = 0.28619274
Iteration 6, loss = 0.21608892
Iteration 7, loss = 0.16668911
Iteration 8, loss = 0.13264888
Iteration 9, loss = 0.10866832
Iteration 10, loss = 0.09145869
Iteration 11, loss = 0.07911790
Iteration 12, loss = 0.06944883
Iteration 13, loss = 0.06193350
Iteration 14, loss = 0.05633801
Iteration 15, loss = 0.05203322
Iteration 16, loss = 0.04822823
Iteration 17, loss = 0.04505959
Iteration 18, loss = 0.04238091
Iteration 19, loss = 0.04028623
Iteration 20, loss = 0.03890080
Iteration 21, loss = 0.03710760
Iteration 22, loss = 0.03570707
Iteration 23, loss = 0.03448959
Iteration 24, loss = 0.03345809
Iteration 25, loss = 0.03284641
Iteration 26, loss = 0.03200425
Iteration 27, loss = 0.03131915
Iteration 28, loss = 0.03065541
Iteration 29, loss = 0.03030128
Iteration 30, loss = 0.02970765
Iteration 31, loss = 0.02901332
Iteration 32, loss = 0.02872459
Iteration 33, loss = 0.02831070
Iteration 34, loss = 0.02806868
Iteration 35, loss = 0.02773631
Iteration 36, loss = 0.02737583
Iteration 37, loss = 0.02705805
Iteration 38, loss = 0.02711907
Iteration 39, loss = 0.02680445
Iteration 40, loss = 0.02706597
Iteration 41, loss = 0.02652629
Iteration 42, loss = 0.02635824
Iteration 43, loss = 0.02590809
Iteration 44, loss = 0.02608095
Iteration 45, loss = 0.02606979
Iteration 46, loss = 0.02563244
Iteration 47, loss = 0.02588684
Iteration 48, loss = 0.02559091
Iteration 49, loss = 0.02557111
Iteration 50, loss = 0.02545198
Iteration 51, loss = 0.02536751
Iteration 52, loss = 0.02533068
Iteration 53, loss = 0.02507896
Iteration 54, loss = 0.02499773
Iteration 55, loss = 0.02492108
Iteration 56, loss = 0.02478462
Iteration 57, loss = 0.02466629
Iteration 58, loss = 0.02473724
Iteration 59, loss = 0.02460047
Iteration 60, loss = 0.02483509
Iteration 61, loss = 0.02457020
Iteration 62, loss = 0.02451677
Iteration 63, loss = 0.02457407
Iteration 64, loss = 0.02417746
Iteration 65, loss = 0.02436558
Iteration 66, loss = 0.02449101
Iteration 67, loss = 0.02428684
Iteration 68, loss = 0.02408831
Iteration 69, loss = 0.02453952
Iteration 70, loss = 0.02469866
Iteration 71, loss = 0.02391994
Iteration 72, loss = 0.02415684
Iteration 73, loss = 0.02410643
Iteration 74, loss = 0.02422496
Iteration 75, loss = 0.02408904
Iteration 76, loss = 0.02409412
Iteration 77, loss = 0.02418594
Iteration 78, loss = 0.02395913
Iteration 79, loss = 0.02374868
Iteration 80, loss = 0.02400418
Iteration 81, loss = 0.02377190
Iteration 82, loss = 0.02400026
Iteration 83, loss = 0.02370090
Iteration 84, loss = 0.02397781
Iteration 85, loss = 0.02374672
Iteration 86, loss = 0.02379395
Iteration 87, loss = 0.02387222
Iteration 88, loss = 0.02381462
Iteration 89, loss = 0.02347736
Iteration 90, loss = 0.02412359
Iteration 91, loss = 0.02393235
Iteration 92, loss = 0.02361552
Iteration 93, loss = 0.02353071
Iteration 94, loss = 0.02358065
Iteration 95, loss = 0.02369095
Iteration 96, loss = 0.02359444
Iteration 97, loss = 0.02387681
Iteration 98, loss = 0.02342086
Iteration 99, loss = 0.02348487
Iteration 100, loss = 0.02387494
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68121473
Iteration 2, loss = 0.60396773
Iteration 3, loss = 0.48985034
Iteration 4, loss = 0.37544482
Iteration 5, loss = 0.28198057
Iteration 6, loss = 0.21317216
Iteration 7, loss = 0.16489450
Iteration 8, loss = 0.13188143
Iteration 9, loss = 0.10849903
Iteration 10, loss = 0.09157957
Iteration 11, loss = 0.07941752
Iteration 12, loss = 0.07012897
Iteration 13, loss = 0.06324367
Iteration 14, loss = 0.05714970
Iteration 15, loss = 0.05274277
Iteration 16, loss = 0.04884358
Iteration 17, loss = 0.04567308
Iteration 18, loss = 0.04311494
Iteration 19, loss = 0.04109327
Iteration 20, loss = 0.03936790
Iteration 21, loss = 0.03787991
Iteration 22, loss = 0.03646691
Iteration 23, loss = 0.03531517
Iteration 24, loss = 0.03426756
Iteration 25, loss = 0.03356153
Iteration 26, loss = 0.03270015
Iteration 27, loss = 0.03177429
Iteration 28, loss = 0.03099979
Iteration 29, loss = 0.03039614
Iteration 30, loss = 0.03008102
Iteration 31, loss = 0.02937623
Iteration 32, loss = 0.02907501
Iteration 33, loss = 0.02906027
Iteration 34, loss = 0.02827608
Iteration 35, loss = 0.02833474
Iteration 36, loss = 0.02773834
Iteration 37, loss = 0.02767299
Iteration 38, loss = 0.02739531
Iteration 39, loss = 0.02713881
Iteration 40, loss = 0.02693179
Iteration 41, loss = 0.02670900
Iteration 42, loss = 0.02680636
Iteration 43, loss = 0.02628650
Iteration 44, loss = 0.02643971
Iteration 45, loss = 0.02611369
Iteration 46, loss = 0.02621826
Iteration 47, loss = 0.02594823
Iteration 48, loss = 0.02604745
Iteration 49, loss = 0.02569822
Iteration 50, loss = 0.02570631
Iteration 51, loss = 0.02568778
Iteration 52, loss = 0.02569406
Iteration 53, loss = 0.02544013
Iteration 54, loss = 0.02534658
Iteration 55, loss = 0.02547375
Iteration 56, loss = 0.02526828
Iteration 57, loss = 0.02508828
Iteration 58, loss = 0.02486194
Iteration 59, loss = 0.02499614
Iteration 60, loss = 0.02489140
Iteration 61, loss = 0.02474873
Iteration 62, loss = 0.02467115
Iteration 63, loss = 0.02469079
Iteration 64, loss = 0.02487932
Iteration 65, loss = 0.02472980
Iteration 66, loss = 0.02460344
Iteration 67, loss = 0.02453219
Iteration 68, loss = 0.02433212
Iteration 69, loss = 0.02427611
Iteration 70, loss = 0.02477911
Iteration 71, loss = 0.02472978
Iteration 72, loss = 0.02471423
Iteration 73, loss = 0.02432977
Iteration 74, loss = 0.02443794
Iteration 75, loss = 0.02427472
Iteration 76, loss = 0.02464284
Iteration 77, loss = 0.02453383
Iteration 78, loss = 0.02424356
Iteration 79, loss = 0.02445233
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67838916
Iteration 2, loss = 0.60216116
Iteration 3, loss = 0.49341850
Iteration 4, loss = 0.38195438
Iteration 5, loss = 0.28618145
Iteration 6, loss = 0.21510614
Iteration 7, loss = 0.16496715
Iteration 8, loss = 0.13039658
Iteration 9, loss = 0.10640868
Iteration 10, loss = 0.08909155
Iteration 11, loss = 0.07655878
Iteration 12, loss = 0.06713193
Iteration 13, loss = 0.06008258
Iteration 14, loss = 0.05415296
Iteration 15, loss = 0.04956322
Iteration 16, loss = 0.04584841
Iteration 17, loss = 0.04284274
Iteration 18, loss = 0.04055768
Iteration 19, loss = 0.03844762
Iteration 20, loss = 0.03634250
Iteration 21, loss = 0.03477268
Iteration 22, loss = 0.03348784
Iteration 23, loss = 0.03235324
Iteration 24, loss = 0.03124344
Iteration 25, loss = 0.03040170
Iteration 26, loss = 0.02950294
Iteration 27, loss = 0.02881312
Iteration 28, loss = 0.02834877
Iteration 29, loss = 0.02799767
Iteration 30, loss = 0.02729451
Iteration 31, loss = 0.02710605
Iteration 32, loss = 0.02640164
Iteration 33, loss = 0.02612936
Iteration 34, loss = 0.02579286
Iteration 35, loss = 0.02532945
Iteration 36, loss = 0.02510341
Iteration 37, loss = 0.02493579
Iteration 38, loss = 0.02476873
Iteration 39, loss = 0.02444614
Iteration 40, loss = 0.02415059
Iteration 41, loss = 0.02404083
Iteration 42, loss = 0.02380589
Iteration 43, loss = 0.02386552
Iteration 44, loss = 0.02378590
Iteration 45, loss = 0.02334181
Iteration 46, loss = 0.02338711
Iteration 47, loss = 0.02349675
Iteration 48, loss = 0.02302329
Iteration 49, loss = 0.02301345
Iteration 50, loss = 0.02315861
Iteration 51, loss = 0.02317319
Iteration 52, loss = 0.02294012
Iteration 53, loss = 0.02265897
Iteration 54, loss = 0.02260806
Iteration 55, loss = 0.02252293
Iteration 56, loss = 0.02253686
Iteration 57, loss = 0.02249087
Iteration 58, loss = 0.02254528
Iteration 59, loss = 0.02233260
Iteration 60, loss = 0.02202044
Iteration 61, loss = 0.02214192
Iteration 62, loss = 0.02200567
Iteration 63, loss = 0.02198599
Iteration 64, loss = 0.02185744
Iteration 65, loss = 0.02220397
Iteration 66, loss = 0.02199708
Iteration 67, loss = 0.02192493
Iteration 68, loss = 0.02189302
Iteration 69, loss = 0.02173508
Iteration 70, loss = 0.02162613
Iteration 71, loss = 0.02156310
Iteration 72, loss = 0.02172146
Iteration 73, loss = 0.02142474
Iteration 74, loss = 0.02177364
Iteration 75, loss = 0.02176723
Iteration 76, loss = 0.02146207
Iteration 77, loss = 0.02168517
Iteration 78, loss = 0.02159309
Iteration 79, loss = 0.02147330
Iteration 80, loss = 0.02145929
Iteration 81, loss = 0.02164557
Iteration 82, loss = 0.02134908
Iteration 83, loss = 0.02109303
Iteration 84, loss = 0.02152735
Iteration 85, loss = 0.02136894
Iteration 86, loss = 0.02125630
Iteration 87, loss = 0.02157173
Iteration 88, loss = 0.02149530
Iteration 89, loss = 0.02131758
Iteration 90, loss = 0.02188326
Iteration 91, loss = 0.02146798
Iteration 92, loss = 0.02135095
Iteration 93, loss = 0.02124725
Iteration 94, loss = 0.02119421
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68741544
Iteration 2, loss = 0.61625913
Iteration 3, loss = 0.51361337
Iteration 4, loss = 0.40378533
Iteration 5, loss = 0.30723075
Iteration 6, loss = 0.23181221
Iteration 7, loss = 0.17752956
Iteration 8, loss = 0.13965324
Iteration 9, loss = 0.11340415
Iteration 10, loss = 0.09417172
Iteration 11, loss = 0.08059403
Iteration 12, loss = 0.07025970
Iteration 13, loss = 0.06235966
Iteration 14, loss = 0.05634324
Iteration 15, loss = 0.05124252
Iteration 16, loss = 0.04752440
Iteration 17, loss = 0.04429189
Iteration 18, loss = 0.04127320
Iteration 19, loss = 0.03915438
Iteration 20, loss = 0.03715014
Iteration 21, loss = 0.03554691
Iteration 22, loss = 0.03427397
Iteration 23, loss = 0.03290159
Iteration 24, loss = 0.03181493
Iteration 25, loss = 0.03078110
Iteration 26, loss = 0.03008052
Iteration 27, loss = 0.02936623
Iteration 28, loss = 0.02858871
Iteration 29, loss = 0.02831932
Iteration 30, loss = 0.02755320
Iteration 31, loss = 0.02714177
Iteration 32, loss = 0.02669129
Iteration 33, loss = 0.02610634
Iteration 34, loss = 0.02590888
Iteration 35, loss = 0.02576518
Iteration 36, loss = 0.02540234
Iteration 37, loss = 0.02512608
Iteration 38, loss = 0.02483282
Iteration 39, loss = 0.02460755
Iteration 40, loss = 0.02470920
Iteration 41, loss = 0.02427010
Iteration 42, loss = 0.02412140
Iteration 43, loss = 0.02373912
Iteration 44, loss = 0.02371350
Iteration 45, loss = 0.02358756
Iteration 46, loss = 0.02354997
Iteration 47, loss = 0.02322718
Iteration 48, loss = 0.02335758
Iteration 49, loss = 0.02318837
Iteration 50, loss = 0.02288755
Iteration 51, loss = 0.02292135
Iteration 52, loss = 0.02303768
Iteration 53, loss = 0.02262191
Iteration 54, loss = 0.02263640
Iteration 55, loss = 0.02236508
Iteration 56, loss = 0.02262214
Iteration 57, loss = 0.02259725
Iteration 58, loss = 0.02249090
Iteration 59, loss = 0.02238507
Iteration 60, loss = 0.02278859
Iteration 61, loss = 0.02220985
Iteration 62, loss = 0.02235015
Iteration 63, loss = 0.02224144
Iteration 64, loss = 0.02192933
Iteration 65, loss = 0.02227184
Iteration 66, loss = 0.02194369
Iteration 67, loss = 0.02194136
Iteration 68, loss = 0.02201928
Iteration 69, loss = 0.02198866
Iteration 70, loss = 0.02184558
Iteration 71, loss = 0.02205146
Iteration 72, loss = 0.02204876
Iteration 73, loss = 0.02183333
Iteration 74, loss = 0.02166920
Iteration 75, loss = 0.02152857
Iteration 76, loss = 0.02167208
Iteration 77, loss = 0.02145570
Iteration 78, loss = 0.02150078
Iteration 79, loss = 0.02165564
Iteration 80, loss = 0.02161617
Iteration 81, loss = 0.02195078
Iteration 82, loss = 0.02152339
Iteration 83, loss = 0.02179365
Iteration 84, loss = 0.02144563
Iteration 85, loss = 0.02136231
Iteration 86, loss = 0.02132020
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67168779
Iteration 2, loss = 0.59161762
Iteration 3, loss = 0.48113885
Iteration 4, loss = 0.37317225
Iteration 5, loss = 0.28356678
Iteration 6, loss = 0.21551310
Iteration 7, loss = 0.16762552
Iteration 8, loss = 0.13371587
Iteration 9, loss = 0.10984170
Iteration 10, loss = 0.09241339
Iteration 11, loss = 0.07989652
Iteration 12, loss = 0.06988081
Iteration 13, loss = 0.06252271
Iteration 14, loss = 0.05667938
Iteration 15, loss = 0.05200970
Iteration 16, loss = 0.04783997
Iteration 17, loss = 0.04483058
Iteration 18, loss = 0.04243226
Iteration 19, loss = 0.03978407
Iteration 20, loss = 0.03769882
Iteration 21, loss = 0.03607582
Iteration 22, loss = 0.03460750
Iteration 23, loss = 0.03351500
Iteration 24, loss = 0.03252185
Iteration 25, loss = 0.03143294
Iteration 26, loss = 0.03052076
Iteration 27, loss = 0.02971515
Iteration 28, loss = 0.02917836
Iteration 29, loss = 0.02870967
Iteration 30, loss = 0.02802586
Iteration 31, loss = 0.02757607
Iteration 32, loss = 0.02719254
Iteration 33, loss = 0.02652296
Iteration 34, loss = 0.02635434
Iteration 35, loss = 0.02595947
Iteration 36, loss = 0.02580969
Iteration 37, loss = 0.02541680
Iteration 38, loss = 0.02534425
Iteration 39, loss = 0.02508861
Iteration 40, loss = 0.02483032
Iteration 41, loss = 0.02447224
Iteration 42, loss = 0.02448304
Iteration 43, loss = 0.02407427
Iteration 44, loss = 0.02426729
Iteration 45, loss = 0.02401541
Iteration 46, loss = 0.02376428
Iteration 47, loss = 0.02347568
Iteration 48, loss = 0.02350996
Iteration 49, loss = 0.02344413
Iteration 50, loss = 0.02306687
Iteration 51, loss = 0.02340652
Iteration 52, loss = 0.02293080
Iteration 53, loss = 0.02294200
Iteration 54, loss = 0.02305809
Iteration 55, loss = 0.02270143
Iteration 56, loss = 0.02282535
Iteration 57, loss = 0.02278363
Iteration 58, loss = 0.02236190
Iteration 59, loss = 0.02269006
Iteration 60, loss = 0.02247696
Iteration 61, loss = 0.02230177
Iteration 62, loss = 0.02237424
Iteration 63, loss = 0.02231985
Iteration 64, loss = 0.02217044
Iteration 65, loss = 0.02236521
Iteration 66, loss = 0.02203119
Iteration 67, loss = 0.02214422
Iteration 68, loss = 0.02225601
Iteration 69, loss = 0.02210211
Iteration 70, loss = 0.02204336
Iteration 71, loss = 0.02190989
Iteration 72, loss = 0.02212604
Iteration 73, loss = 0.02213131
Iteration 74, loss = 0.02225508
Iteration 75, loss = 0.02202334
Iteration 76, loss = 0.02185564
Iteration 77, loss = 0.02190603
Iteration 78, loss = 0.02177371
Iteration 79, loss = 0.02189239
Iteration 80, loss = 0.02170167
Iteration 81, loss = 0.02180731
Iteration 82, loss = 0.02172187
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67272670
Iteration 2, loss = 0.59869920
Iteration 3, loss = 0.48984010
Iteration 4, loss = 0.37957189
Iteration 5, loss = 0.28619274
Iteration 6, loss = 0.21608892
Iteration 7, loss = 0.16668911
Iteration 8, loss = 0.13264888
Iteration 9, loss = 0.10866832
Iteration 10, loss = 0.09145869
Iteration 11, loss = 0.07911790
Iteration 12, loss = 0.06944883
Iteration 13, loss = 0.06193350
Iteration 14, loss = 0.05633801
Iteration 15, loss = 0.05203322
Iteration 16, loss = 0.04822823
Iteration 17, loss = 0.04505959
Iteration 18, loss = 0.04238091
Iteration 19, loss = 0.04028623
Iteration 20, loss = 0.03890080
Iteration 21, loss = 0.03710760
Iteration 22, loss = 0.03570707
Iteration 23, loss = 0.03448959
Iteration 24, loss = 0.03345809
Iteration 25, loss = 0.03284641
Iteration 26, loss = 0.03200425
Iteration 27, loss = 0.03131915
Iteration 28, loss = 0.03065541
Iteration 29, loss = 0.03030128
Iteration 30, loss = 0.02970765
Iteration 31, loss = 0.02901332
Iteration 32, loss = 0.02872459
Iteration 33, loss = 0.02831070
Iteration 34, loss = 0.02806868
Iteration 35, loss = 0.02773631
Iteration 36, loss = 0.02737583
Iteration 37, loss = 0.02705805
Iteration 38, loss = 0.02711907
Iteration 39, loss = 0.02680445
Iteration 40, loss = 0.02706597
Iteration 41, loss = 0.02652629
Iteration 42, loss = 0.02635824
Iteration 43, loss = 0.02590809
Iteration 44, loss = 0.02608095
Iteration 45, loss = 0.02606979
Iteration 46, loss = 0.02563244
Iteration 47, loss = 0.02588684
Iteration 48, loss = 0.02559091
Iteration 49, loss = 0.02557111
Iteration 50, loss = 0.02545198
Iteration 51, loss = 0.02536751
Iteration 52, loss = 0.02533068
Iteration 53, loss = 0.02507896
Iteration 54, loss = 0.02499773
Iteration 55, loss = 0.02492108
Iteration 56, loss = 0.02478462
Iteration 57, loss = 0.02466629
Iteration 58, loss = 0.02473724
Iteration 59, loss = 0.02460047
Iteration 60, loss = 0.02483509
Iteration 61, loss = 0.02457020
Iteration 62, loss = 0.02451677
Iteration 63, loss = 0.02457407
Iteration 64, loss = 0.02417746
Iteration 65, loss = 0.02436558
Iteration 66, loss = 0.02449101
Iteration 67, loss = 0.02428684
Iteration 68, loss = 0.02408831
Iteration 69, loss = 0.02453952
Iteration 70, loss = 0.02469866
Iteration 71, loss = 0.02391994
Iteration 72, loss = 0.02415684
Iteration 73, loss = 0.02410643
Iteration 74, loss = 0.02422496
Iteration 75, loss = 0.02408904
Iteration 76, loss = 0.02409412
Iteration 77, loss = 0.02418594
Iteration 78, loss = 0.02395913
Iteration 79, loss = 0.02374868
Iteration 80, loss = 0.02400418
Iteration 81, loss = 0.02377190
Iteration 82, loss = 0.02400026
Iteration 83, loss = 0.02370090
Iteration 84, loss = 0.02397781
Iteration 85, loss = 0.02374672
Iteration 86, loss = 0.02379395
Iteration 87, loss = 0.02387222
Iteration 88, loss = 0.02381462
Iteration 89, loss = 0.02347736
Iteration 90, loss = 0.02412359
Iteration 91, loss = 0.02393235
Iteration 92, loss = 0.02361552
Iteration 93, loss = 0.02353071
Iteration 94, loss = 0.02358065
Iteration 95, loss = 0.02369095
Iteration 96, loss = 0.02359444
Iteration 97, loss = 0.02387681
Iteration 98, loss = 0.02342086
Iteration 99, loss = 0.02348487
Iteration 100, loss = 0.02387494
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68121473
Iteration 2, loss = 0.60396773
Iteration 3, loss = 0.48985034
Iteration 4, loss = 0.37544482
Iteration 5, loss = 0.28198057
Iteration 6, loss = 0.21317216
Iteration 7, loss = 0.16489450
Iteration 8, loss = 0.13188143
Iteration 9, loss = 0.10849903
Iteration 10, loss = 0.09157957
Iteration 11, loss = 0.07941752
Iteration 12, loss = 0.07012897
Iteration 13, loss = 0.06324367
Iteration 14, loss = 0.05714970
Iteration 15, loss = 0.05274277
Iteration 16, loss = 0.04884358
Iteration 17, loss = 0.04567308
Iteration 18, loss = 0.04311494
Iteration 19, loss = 0.04109327
Iteration 20, loss = 0.03936790
Iteration 21, loss = 0.03787991
Iteration 22, loss = 0.03646691
Iteration 23, loss = 0.03531517
Iteration 24, loss = 0.03426756
Iteration 25, loss = 0.03356153
Iteration 26, loss = 0.03270015
Iteration 27, loss = 0.03177429
Iteration 28, loss = 0.03099979
Iteration 29, loss = 0.03039614
Iteration 30, loss = 0.03008102
Iteration 31, loss = 0.02937623
Iteration 32, loss = 0.02907501
Iteration 33, loss = 0.02906027
Iteration 34, loss = 0.02827608
Iteration 35, loss = 0.02833474
Iteration 36, loss = 0.02773834
Iteration 37, loss = 0.02767299
Iteration 38, loss = 0.02739531
Iteration 39, loss = 0.02713881
Iteration 40, loss = 0.02693179
Iteration 41, loss = 0.02670900
Iteration 42, loss = 0.02680636
Iteration 43, loss = 0.02628650
Iteration 44, loss = 0.02643971
Iteration 45, loss = 0.02611369
Iteration 46, loss = 0.02621826
Iteration 47, loss = 0.02594823
Iteration 48, loss = 0.02604745
Iteration 49, loss = 0.02569822
Iteration 50, loss = 0.02570631
Iteration 51, loss = 0.02568778
Iteration 52, loss = 0.02569406
Iteration 53, loss = 0.02544013
Iteration 54, loss = 0.02534658
Iteration 55, loss = 0.02547375
Iteration 56, loss = 0.02526828
Iteration 57, loss = 0.02508828
Iteration 58, loss = 0.02486194
Iteration 59, loss = 0.02499614
Iteration 60, loss = 0.02489140
Iteration 61, loss = 0.02474873
Iteration 62, loss = 0.02467115
Iteration 63, loss = 0.02469079
Iteration 64, loss = 0.02487932
Iteration 65, loss = 0.02472980
Iteration 66, loss = 0.02460344
Iteration 67, loss = 0.02453219
Iteration 68, loss = 0.02433212
Iteration 69, loss = 0.02427611
Iteration 70, loss = 0.02477911
Iteration 71, loss = 0.02472978
Iteration 72, loss = 0.02471423
Iteration 73, loss = 0.02432977
Iteration 74, loss = 0.02443794
Iteration 75, loss = 0.02427472
Iteration 76, loss = 0.02464284
Iteration 77, loss = 0.02453383
Iteration 78, loss = 0.02424356
Iteration 79, loss = 0.02445233
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67838916
Iteration 2, loss = 0.60216116
Iteration 3, loss = 0.49341850
Iteration 4, loss = 0.38195438
Iteration 5, loss = 0.28618145
Iteration 6, loss = 0.21510614
Iteration 7, loss = 0.16496715
Iteration 8, loss = 0.13039658
Iteration 9, loss = 0.10640868
Iteration 10, loss = 0.08909155
Iteration 11, loss = 0.07655878
Iteration 12, loss = 0.06713193
Iteration 13, loss = 0.06008258
Iteration 14, loss = 0.05415296
Iteration 15, loss = 0.04956322
Iteration 16, loss = 0.04584841
Iteration 17, loss = 0.04284274
Iteration 18, loss = 0.04055768
Iteration 19, loss = 0.03844762
Iteration 20, loss = 0.03634250
Iteration 21, loss = 0.03477268
Iteration 22, loss = 0.03348784
Iteration 23, loss = 0.03235324
Iteration 24, loss = 0.03124344
Iteration 25, loss = 0.03040170
Iteration 26, loss = 0.02950294
Iteration 27, loss = 0.02881312
Iteration 28, loss = 0.02834877
Iteration 29, loss = 0.02799767
Iteration 30, loss = 0.02729451
Iteration 31, loss = 0.02710605
Iteration 32, loss = 0.02640164
Iteration 33, loss = 0.02612936
Iteration 34, loss = 0.02579286
Iteration 35, loss = 0.02532945
Iteration 36, loss = 0.02510341
Iteration 37, loss = 0.02493579
Iteration 38, loss = 0.02476873
Iteration 39, loss = 0.02444614
Iteration 40, loss = 0.02415059
Iteration 41, loss = 0.02404083
Iteration 42, loss = 0.02380589
Iteration 43, loss = 0.02386552
Iteration 44, loss = 0.02378590
Iteration 45, loss = 0.02334181
Iteration 46, loss = 0.02338711
Iteration 47, loss = 0.02349675
Iteration 48, loss = 0.02302329
Iteration 49, loss = 0.02301345
Iteration 50, loss = 0.02315861
Iteration 51, loss = 0.02317319
Iteration 52, loss = 0.02294012
Iteration 53, loss = 0.02265897
Iteration 54, loss = 0.02260806
Iteration 55, loss = 0.02252293
Iteration 56, loss = 0.02253686
Iteration 57, loss = 0.02249087
Iteration 58, loss = 0.02254528
Iteration 59, loss = 0.02233260
Iteration 60, loss = 0.02202044
Iteration 61, loss = 0.02214192
Iteration 62, loss = 0.02200567
Iteration 63, loss = 0.02198599
Iteration 64, loss = 0.02185744
Iteration 65, loss = 0.02220397
Iteration 66, loss = 0.02199708
Iteration 67, loss = 0.02192493
Iteration 68, loss = 0.02189302
Iteration 69, loss = 0.02173508
Iteration 70, loss = 0.02162613
Iteration 71, loss = 0.02156310
Iteration 72, loss = 0.02172146
Iteration 73, loss = 0.02142474
Iteration 74, loss = 0.02177364
Iteration 75, loss = 0.02176723
Iteration 76, loss = 0.02146207
Iteration 77, loss = 0.02168517
Iteration 78, loss = 0.02159309
Iteration 79, loss = 0.02147330
Iteration 80, loss = 0.02145929
Iteration 81, loss = 0.02164557
Iteration 82, loss = 0.02134908
Iteration 83, loss = 0.02109303
Iteration 84, loss = 0.02152735
Iteration 85, loss = 0.02136894
Iteration 86, loss = 0.02125630
Iteration 87, loss = 0.02157173
Iteration 88, loss = 0.02149530
Iteration 89, loss = 0.02131758
Iteration 90, loss = 0.02188326
Iteration 91, loss = 0.02146798
Iteration 92, loss = 0.02135095
Iteration 93, loss = 0.02124725
Iteration 94, loss = 0.02119421
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68741544
Iteration 2, loss = 0.61625913
Iteration 3, loss = 0.51361337
Iteration 4, loss = 0.40378533
Iteration 5, loss = 0.30723075
Iteration 6, loss = 0.23181221
Iteration 7, loss = 0.17752956
Iteration 8, loss = 0.13965324
Iteration 9, loss = 0.11340415
Iteration 10, loss = 0.09417172
Iteration 11, loss = 0.08059403
Iteration 12, loss = 0.07025970
Iteration 13, loss = 0.06235966
Iteration 14, loss = 0.05634324
Iteration 15, loss = 0.05124252
Iteration 16, loss = 0.04752440
Iteration 17, loss = 0.04429189
Iteration 18, loss = 0.04127320
Iteration 19, loss = 0.03915438
Iteration 20, loss = 0.03715014
Iteration 21, loss = 0.03554691
Iteration 22, loss = 0.03427397
Iteration 23, loss = 0.03290159
Iteration 24, loss = 0.03181493
Iteration 25, loss = 0.03078110
Iteration 26, loss = 0.03008052
Iteration 27, loss = 0.02936623
Iteration 28, loss = 0.02858871
Iteration 29, loss = 0.02831932
Iteration 30, loss = 0.02755320
Iteration 31, loss = 0.02714177
Iteration 32, loss = 0.02669129
Iteration 33, loss = 0.02610634
Iteration 34, loss = 0.02590888
Iteration 35, loss = 0.02576518
Iteration 36, loss = 0.02540234
Iteration 37, loss = 0.02512608
Iteration 38, loss = 0.02483282
Iteration 39, loss = 0.02460755
Iteration 40, loss = 0.02470920
Iteration 41, loss = 0.02427010
Iteration 42, loss = 0.02412140
Iteration 43, loss = 0.02373912
Iteration 44, loss = 0.02371350
Iteration 45, loss = 0.02358756
Iteration 46, loss = 0.02354997
Iteration 47, loss = 0.02322718
Iteration 48, loss = 0.02335758
Iteration 49, loss = 0.02318837
Iteration 50, loss = 0.02288755
Iteration 51, loss = 0.02292135
Iteration 52, loss = 0.02303768
Iteration 53, loss = 0.02262191
Iteration 54, loss = 0.02263640
Iteration 55, loss = 0.02236508
Iteration 56, loss = 0.02262214
Iteration 57, loss = 0.02259725
Iteration 58, loss = 0.02249090
Iteration 59, loss = 0.02238507
Iteration 60, loss = 0.02278859
Iteration 61, loss = 0.02220985
Iteration 62, loss = 0.02235015
Iteration 63, loss = 0.02224144
Iteration 64, loss = 0.02192933
Iteration 65, loss = 0.02227184
Iteration 66, loss = 0.02194369
Iteration 67, loss = 0.02194136
Iteration 68, loss = 0.02201928
Iteration 69, loss = 0.02198866
Iteration 70, loss = 0.02184558
Iteration 71, loss = 0.02205146
Iteration 72, loss = 0.02204876
Iteration 73, loss = 0.02183333
Iteration 74, loss = 0.02166920
Iteration 75, loss = 0.02152857
Iteration 76, loss = 0.02167208
Iteration 77, loss = 0.02145570
Iteration 78, loss = 0.02150078
Iteration 79, loss = 0.02165564
Iteration 80, loss = 0.02161617
Iteration 81, loss = 0.02195078
Iteration 82, loss = 0.02152339
Iteration 83, loss = 0.02179365
Iteration 84, loss = 0.02144563
Iteration 85, loss = 0.02136231
Iteration 86, loss = 0.02132020
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67168779
Iteration 2, loss = 0.59161762
Iteration 3, loss = 0.48113885
Iteration 4, loss = 0.37317225
Iteration 5, loss = 0.28356678
Iteration 6, loss = 0.21551310
Iteration 7, loss = 0.16762552
Iteration 8, loss = 0.13371587
Iteration 9, loss = 0.10984170
Iteration 10, loss = 0.09241339
Iteration 11, loss = 0.07989652
Iteration 12, loss = 0.06988081
Iteration 13, loss = 0.06252271
Iteration 14, loss = 0.05667938
Iteration 15, loss = 0.05200970
Iteration 16, loss = 0.04783997
Iteration 17, loss = 0.04483058
Iteration 18, loss = 0.04243226
Iteration 19, loss = 0.03978407
Iteration 20, loss = 0.03769882
Iteration 21, loss = 0.03607582
Iteration 22, loss = 0.03460750
Iteration 23, loss = 0.03351500
Iteration 24, loss = 0.03252185
Iteration 25, loss = 0.03143294
Iteration 26, loss = 0.03052076
Iteration 27, loss = 0.02971515
Iteration 28, loss = 0.02917836
Iteration 29, loss = 0.02870967
Iteration 30, loss = 0.02802586
Iteration 31, loss = 0.02757607
Iteration 32, loss = 0.02719254
Iteration 33, loss = 0.02652296
Iteration 34, loss = 0.02635434
Iteration 35, loss = 0.02595947
Iteration 36, loss = 0.02580969
Iteration 37, loss = 0.02541680
Iteration 38, loss = 0.02534425
Iteration 39, loss = 0.02508861
Iteration 40, loss = 0.02483032
Iteration 41, loss = 0.02447224
Iteration 42, loss = 0.02448304
Iteration 43, loss = 0.02407427
Iteration 44, loss = 0.02426729
Iteration 45, loss = 0.02401541
Iteration 46, loss = 0.02376428
Iteration 47, loss = 0.02347568
Iteration 48, loss = 0.02350996
Iteration 49, loss = 0.02344413
Iteration 50, loss = 0.02306687
Iteration 51, loss = 0.02340652
Iteration 52, loss = 0.02293080
Iteration 53, loss = 0.02294200
Iteration 54, loss = 0.02305809
Iteration 55, loss = 0.02270143
Iteration 56, loss = 0.02282535
Iteration 57, loss = 0.02278363
Iteration 58, loss = 0.02236190
Iteration 59, loss = 0.02269006
Iteration 60, loss = 0.02247696
Iteration 61, loss = 0.02230177
Iteration 62, loss = 0.02237424
Iteration 63, loss = 0.02231985
Iteration 64, loss = 0.02217044
Iteration 65, loss = 0.02236521
Iteration 66, loss = 0.02203119
Iteration 67, loss = 0.02214422
Iteration 68, loss = 0.02225601
Iteration 69, loss = 0.02210211
Iteration 70, loss = 0.02204336
Iteration 71, loss = 0.02190989
Iteration 72, loss = 0.02212604
Iteration 73, loss = 0.02213131
Iteration 74, loss = 0.02225508
Iteration 75, loss = 0.02202334
Iteration 76, loss = 0.02185564
Iteration 77, loss = 0.02190603
Iteration 78, loss = 0.02177371
Iteration 79, loss = 0.02189239
Iteration 80, loss = 0.02170167
Iteration 81, loss = 0.02180731
Iteration 82, loss = 0.02172187
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67272670
Iteration 2, loss = 0.59869920
Iteration 3, loss = 0.48984010
Iteration 4, loss = 0.37957189
Iteration 5, loss = 0.28619274
Iteration 6, loss = 0.21608892
Iteration 7, loss = 0.16668911
Iteration 8, loss = 0.13264888
Iteration 9, loss = 0.10866832
Iteration 10, loss = 0.09145869
Iteration 11, loss = 0.07911790
Iteration 12, loss = 0.06944883
Iteration 13, loss = 0.06193350
Iteration 14, loss = 0.05633801
Iteration 15, loss = 0.05203322
Iteration 16, loss = 0.04822823
Iteration 17, loss = 0.04505959
Iteration 18, loss = 0.04238091
Iteration 19, loss = 0.04028623
Iteration 20, loss = 0.03890080
Iteration 21, loss = 0.03710760
Iteration 22, loss = 0.03570707
Iteration 23, loss = 0.03448959
Iteration 24, loss = 0.03345809
Iteration 25, loss = 0.03284641
Iteration 26, loss = 0.03200425
Iteration 27, loss = 0.03131915
Iteration 28, loss = 0.03065541
Iteration 29, loss = 0.03030128
Iteration 30, loss = 0.02970765
Iteration 31, loss = 0.02901332
Iteration 32, loss = 0.02872459
Iteration 33, loss = 0.02831070
Iteration 34, loss = 0.02806868
Iteration 35, loss = 0.02773631
Iteration 36, loss = 0.02737583
Iteration 37, loss = 0.02705805
Iteration 38, loss = 0.02711907
Iteration 39, loss = 0.02680445
Iteration 40, loss = 0.02706597
Iteration 41, loss = 0.02652629
Iteration 42, loss = 0.02635824
Iteration 43, loss = 0.02590809
Iteration 44, loss = 0.02608095
Iteration 45, loss = 0.02606979
Iteration 46, loss = 0.02563244
Iteration 47, loss = 0.02588684
Iteration 48, loss = 0.02559091
Iteration 49, loss = 0.02557111
Iteration 50, loss = 0.02545198
Iteration 51, loss = 0.02536751
Iteration 52, loss = 0.02533068
Iteration 53, loss = 0.02507896
Iteration 54, loss = 0.02499773
Iteration 55, loss = 0.02492108
Iteration 56, loss = 0.02478462
Iteration 57, loss = 0.02466629
Iteration 58, loss = 0.02473724
Iteration 59, loss = 0.02460047
Iteration 60, loss = 0.02483509
Iteration 61, loss = 0.02457020
Iteration 62, loss = 0.02451677
Iteration 63, loss = 0.02457407
Iteration 64, loss = 0.02417746
Iteration 65, loss = 0.02436558
Iteration 66, loss = 0.02449101
Iteration 67, loss = 0.02428684
Iteration 68, loss = 0.02408831
Iteration 69, loss = 0.02453952
Iteration 70, loss = 0.02469866
Iteration 71, loss = 0.02391994
Iteration 72, loss = 0.02415684
Iteration 73, loss = 0.02410643
Iteration 74, loss = 0.02422496
Iteration 75, loss = 0.02408904
Iteration 76, loss = 0.02409412
Iteration 77, loss = 0.02418594
Iteration 78, loss = 0.02395913
Iteration 79, loss = 0.02374868
Iteration 80, loss = 0.02400418
Iteration 81, loss = 0.02377190
Iteration 82, loss = 0.02400026
Iteration 83, loss = 0.02370090
Iteration 84, loss = 0.02397781
Iteration 85, loss = 0.02374672
Iteration 86, loss = 0.02379395
Iteration 87, loss = 0.02387222
Iteration 88, loss = 0.02381462
Iteration 89, loss = 0.02347736
Iteration 90, loss = 0.02412359
Iteration 91, loss = 0.02393235
Iteration 92, loss = 0.02361552
Iteration 93, loss = 0.02353071
Iteration 94, loss = 0.02358065
Iteration 95, loss = 0.02369095
Iteration 96, loss = 0.02359444
Iteration 97, loss = 0.02387681
Iteration 98, loss = 0.02342086
Iteration 99, loss = 0.02348487
Iteration 100, loss = 0.02387494
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68121473
Iteration 2, loss = 0.60396773
Iteration 3, loss = 0.48985034
Iteration 4, loss = 0.37544482
Iteration 5, loss = 0.28198057
Iteration 6, loss = 0.21317216
Iteration 7, loss = 0.16489450
Iteration 8, loss = 0.13188143
Iteration 9, loss = 0.10849903
Iteration 10, loss = 0.09157957
Iteration 11, loss = 0.07941752
Iteration 12, loss = 0.07012897
Iteration 13, loss = 0.06324367
Iteration 14, loss = 0.05714970
Iteration 15, loss = 0.05274277
Iteration 16, loss = 0.04884358
Iteration 17, loss = 0.04567308
Iteration 18, loss = 0.04311494
Iteration 19, loss = 0.04109327
Iteration 20, loss = 0.03936790
Iteration 21, loss = 0.03787991
Iteration 22, loss = 0.03646691
Iteration 23, loss = 0.03531517
Iteration 24, loss = 0.03426756
Iteration 25, loss = 0.03356153
Iteration 26, loss = 0.03270015
Iteration 27, loss = 0.03177429
Iteration 28, loss = 0.03099979
Iteration 29, loss = 0.03039614
Iteration 30, loss = 0.03008102
Iteration 31, loss = 0.02937623
Iteration 32, loss = 0.02907501
Iteration 33, loss = 0.02906027
Iteration 34, loss = 0.02827608
Iteration 35, loss = 0.02833474
Iteration 36, loss = 0.02773834
Iteration 37, loss = 0.02767299
Iteration 38, loss = 0.02739531
Iteration 39, loss = 0.02713881
Iteration 40, loss = 0.02693179
Iteration 41, loss = 0.02670900
Iteration 42, loss = 0.02680636
Iteration 43, loss = 0.02628650
Iteration 44, loss = 0.02643971
Iteration 45, loss = 0.02611369
Iteration 46, loss = 0.02621826
Iteration 47, loss = 0.02594823
Iteration 48, loss = 0.02604745
Iteration 49, loss = 0.02569822
Iteration 50, loss = 0.02570631
Iteration 51, loss = 0.02568778
Iteration 52, loss = 0.02569406
Iteration 53, loss = 0.02544013
Iteration 54, loss = 0.02534658
Iteration 55, loss = 0.02547375
Iteration 56, loss = 0.02526828
Iteration 57, loss = 0.02508828
Iteration 58, loss = 0.02486194
Iteration 59, loss = 0.02499614
Iteration 60, loss = 0.02489140
Iteration 61, loss = 0.02474873
Iteration 62, loss = 0.02467115
Iteration 63, loss = 0.02469079
Iteration 64, loss = 0.02487932
Iteration 65, loss = 0.02472980
Iteration 66, loss = 0.02460344
Iteration 67, loss = 0.02453219
Iteration 68, loss = 0.02433212
Iteration 69, loss = 0.02427611
Iteration 70, loss = 0.02477911
Iteration 71, loss = 0.02472978
Iteration 72, loss = 0.02471423
Iteration 73, loss = 0.02432977
Iteration 74, loss = 0.02443794
Iteration 75, loss = 0.02427472
Iteration 76, loss = 0.02464284
Iteration 77, loss = 0.02453383
Iteration 78, loss = 0.02424356
Iteration 79, loss = 0.02445233
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67838916
Iteration 2, loss = 0.60216116
Iteration 3, loss = 0.49341850
Iteration 4, loss = 0.38195438
Iteration 5, loss = 0.28618145
Iteration 6, loss = 0.21510614
Iteration 7, loss = 0.16496715
Iteration 8, loss = 0.13039658
Iteration 9, loss = 0.10640868
Iteration 10, loss = 0.08909155
Iteration 11, loss = 0.07655878
Iteration 12, loss = 0.06713193
Iteration 13, loss = 0.06008258
Iteration 14, loss = 0.05415296
Iteration 15, loss = 0.04956322
Iteration 16, loss = 0.04584841
Iteration 17, loss = 0.04284274
Iteration 18, loss = 0.04055768
Iteration 19, loss = 0.03844762
Iteration 20, loss = 0.03634250
Iteration 21, loss = 0.03477268
Iteration 22, loss = 0.03348784
Iteration 23, loss = 0.03235324
Iteration 24, loss = 0.03124344
Iteration 25, loss = 0.03040170
Iteration 26, loss = 0.02950294
Iteration 27, loss = 0.02881312
Iteration 28, loss = 0.02834877
Iteration 29, loss = 0.02799767
Iteration 30, loss = 0.02729451
Iteration 31, loss = 0.02710605
Iteration 32, loss = 0.02640164
Iteration 33, loss = 0.02612936
Iteration 34, loss = 0.02579286
Iteration 35, loss = 0.02532945
Iteration 36, loss = 0.02510341
Iteration 37, loss = 0.02493579
Iteration 38, loss = 0.02476873
Iteration 39, loss = 0.02444614
Iteration 40, loss = 0.02415059
Iteration 41, loss = 0.02404083
Iteration 42, loss = 0.02380589
Iteration 43, loss = 0.02386552
Iteration 44, loss = 0.02378590
Iteration 45, loss = 0.02334181
Iteration 46, loss = 0.02338711
Iteration 47, loss = 0.02349675
Iteration 48, loss = 0.02302329
Iteration 49, loss = 0.02301345
Iteration 50, loss = 0.02315861
Iteration 51, loss = 0.02317319
Iteration 52, loss = 0.02294012
Iteration 53, loss = 0.02265897
Iteration 54, loss = 0.02260806
Iteration 55, loss = 0.02252293
Iteration 56, loss = 0.02253686
Iteration 57, loss = 0.02249087
Iteration 58, loss = 0.02254528
Iteration 59, loss = 0.02233260
Iteration 60, loss = 0.02202044
Iteration 61, loss = 0.02214192
Iteration 62, loss = 0.02200567
Iteration 63, loss = 0.02198599
Iteration 64, loss = 0.02185744
Iteration 65, loss = 0.02220397
Iteration 66, loss = 0.02199708
Iteration 67, loss = 0.02192493
Iteration 68, loss = 0.02189302
Iteration 69, loss = 0.02173508
Iteration 70, loss = 0.02162613
Iteration 71, loss = 0.02156310
Iteration 72, loss = 0.02172146
Iteration 73, loss = 0.02142474
Iteration 74, loss = 0.02177364
Iteration 75, loss = 0.02176723
Iteration 76, loss = 0.02146207
Iteration 77, loss = 0.02168517
Iteration 78, loss = 0.02159309
Iteration 79, loss = 0.02147330
Iteration 80, loss = 0.02145929
Iteration 81, loss = 0.02164557
Iteration 82, loss = 0.02134908
Iteration 83, loss = 0.02109303
Iteration 84, loss = 0.02152735
Iteration 85, loss = 0.02136894
Iteration 86, loss = 0.02125630
Iteration 87, loss = 0.02157173
Iteration 88, loss = 0.02149530
Iteration 89, loss = 0.02131758
Iteration 90, loss = 0.02188326
Iteration 91, loss = 0.02146798
Iteration 92, loss = 0.02135095
Iteration 93, loss = 0.02124725
Iteration 94, loss = 0.02119421
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68741544
Iteration 2, loss = 0.61625913
Iteration 3, loss = 0.51361337
Iteration 4, loss = 0.40378533
Iteration 5, loss = 0.30723075
Iteration 6, loss = 0.23181221
Iteration 7, loss = 0.17752956
Iteration 8, loss = 0.13965324
Iteration 9, loss = 0.11340415
Iteration 10, loss = 0.09417172
Iteration 11, loss = 0.08059403
Iteration 12, loss = 0.07025970
Iteration 13, loss = 0.06235966
Iteration 14, loss = 0.05634324
Iteration 15, loss = 0.05124252
Iteration 16, loss = 0.04752440
Iteration 17, loss = 0.04429189
Iteration 18, loss = 0.04127320
Iteration 19, loss = 0.03915438
Iteration 20, loss = 0.03715014
Iteration 21, loss = 0.03554691
Iteration 22, loss = 0.03427397
Iteration 23, loss = 0.03290159
Iteration 24, loss = 0.03181493
Iteration 25, loss = 0.03078110
Iteration 26, loss = 0.03008052
Iteration 27, loss = 0.02936623
Iteration 28, loss = 0.02858871
Iteration 29, loss = 0.02831932
Iteration 30, loss = 0.02755320
Iteration 31, loss = 0.02714177
Iteration 32, loss = 0.02669129
Iteration 33, loss = 0.02610634
Iteration 34, loss = 0.02590888
Iteration 35, loss = 0.02576518
Iteration 36, loss = 0.02540234
Iteration 37, loss = 0.02512608
Iteration 38, loss = 0.02483282
Iteration 39, loss = 0.02460755
Iteration 40, loss = 0.02470920
Iteration 41, loss = 0.02427010
Iteration 42, loss = 0.02412140
Iteration 43, loss = 0.02373912
Iteration 44, loss = 0.02371350
Iteration 45, loss = 0.02358756
Iteration 46, loss = 0.02354997
Iteration 47, loss = 0.02322718
Iteration 48, loss = 0.02335758
Iteration 49, loss = 0.02318837
Iteration 50, loss = 0.02288755
Iteration 51, loss = 0.02292135
Iteration 52, loss = 0.02303768
Iteration 53, loss = 0.02262191
Iteration 54, loss = 0.02263640
Iteration 55, loss = 0.02236508
Iteration 56, loss = 0.02262214
Iteration 57, loss = 0.02259725
Iteration 58, loss = 0.02249090
Iteration 59, loss = 0.02238507
Iteration 60, loss = 0.02278859
Iteration 61, loss = 0.02220985
Iteration 62, loss = 0.02235015
Iteration 63, loss = 0.02224144
Iteration 64, loss = 0.02192933
Iteration 65, loss = 0.02227184
Iteration 66, loss = 0.02194369
Iteration 67, loss = 0.02194136
Iteration 68, loss = 0.02201928
Iteration 69, loss = 0.02198866
Iteration 70, loss = 0.02184558
Iteration 71, loss = 0.02205146
Iteration 72, loss = 0.02204876
Iteration 73, loss = 0.02183333
Iteration 74, loss = 0.02166920
Iteration 75, loss = 0.02152857
Iteration 76, loss = 0.02167208
Iteration 77, loss = 0.02145570
Iteration 78, loss = 0.02150078
Iteration 79, loss = 0.02165564
Iteration 80, loss = 0.02161617
Iteration 81, loss = 0.02195078
Iteration 82, loss = 0.02152339
Iteration 83, loss = 0.02179365
Iteration 84, loss = 0.02144563
Iteration 85, loss = 0.02136231
Iteration 86, loss = 0.02132020
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67168779
Iteration 2, loss = 0.59161762
Iteration 3, loss = 0.48113885
Iteration 4, loss = 0.37317225
Iteration 5, loss = 0.28356678
Iteration 6, loss = 0.21551310
Iteration 7, loss = 0.16762552
Iteration 8, loss = 0.13371587
Iteration 9, loss = 0.10984170
Iteration 10, loss = 0.09241339
Iteration 11, loss = 0.07989652
Iteration 12, loss = 0.06988081
Iteration 13, loss = 0.06252271
Iteration 14, loss = 0.05667938
Iteration 15, loss = 0.05200970
Iteration 16, loss = 0.04783997
Iteration 17, loss = 0.04483058
Iteration 18, loss = 0.04243226
Iteration 19, loss = 0.03978407
Iteration 20, loss = 0.03769882
Iteration 21, loss = 0.03607582
Iteration 22, loss = 0.03460750
Iteration 23, loss = 0.03351500
Iteration 24, loss = 0.03252185
Iteration 25, loss = 0.03143294
Iteration 26, loss = 0.03052076
Iteration 27, loss = 0.02971515
Iteration 28, loss = 0.02917836
Iteration 29, loss = 0.02870967
Iteration 30, loss = 0.02802586
Iteration 31, loss = 0.02757607
Iteration 32, loss = 0.02719254
Iteration 33, loss = 0.02652296
Iteration 34, loss = 0.02635434
Iteration 35, loss = 0.02595947
Iteration 36, loss = 0.02580969
Iteration 37, loss = 0.02541680
Iteration 38, loss = 0.02534425
Iteration 39, loss = 0.02508861
Iteration 40, loss = 0.02483032
Iteration 41, loss = 0.02447224
Iteration 42, loss = 0.02448304
Iteration 43, loss = 0.02407427
Iteration 44, loss = 0.02426729
Iteration 45, loss = 0.02401541
Iteration 46, loss = 0.02376428
Iteration 47, loss = 0.02347568
Iteration 48, loss = 0.02350996
Iteration 49, loss = 0.02344413
Iteration 50, loss = 0.02306687
Iteration 51, loss = 0.02340652
Iteration 52, loss = 0.02293080
Iteration 53, loss = 0.02294200
Iteration 54, loss = 0.02305809
Iteration 55, loss = 0.02270143
Iteration 56, loss = 0.02282535
Iteration 57, loss = 0.02278363
Iteration 58, loss = 0.02236190
Iteration 59, loss = 0.02269006
Iteration 60, loss = 0.02247696
Iteration 61, loss = 0.02230177
Iteration 62, loss = 0.02237424
Iteration 63, loss = 0.02231985
Iteration 64, loss = 0.02217044
Iteration 65, loss = 0.02236521
Iteration 66, loss = 0.02203119
Iteration 67, loss = 0.02214422
Iteration 68, loss = 0.02225601
Iteration 69, loss = 0.02210211
Iteration 70, loss = 0.02204336
Iteration 71, loss = 0.02190989
Iteration 72, loss = 0.02212604
Iteration 73, loss = 0.02213131
Iteration 74, loss = 0.02225508
Iteration 75, loss = 0.02202334
Iteration 76, loss = 0.02185564
Iteration 77, loss = 0.02190603
Iteration 78, loss = 0.02177371
Iteration 79, loss = 0.02189239
Iteration 80, loss = 0.02170167
Iteration 81, loss = 0.02180731
Iteration 82, loss = 0.02172187
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67272670
Iteration 2, loss = 0.59869920
Iteration 3, loss = 0.48984010
Iteration 4, loss = 0.37957189
Iteration 5, loss = 0.28619274
Iteration 6, loss = 0.21608892
Iteration 7, loss = 0.16668911
Iteration 8, loss = 0.13264888
Iteration 9, loss = 0.10866832
Iteration 10, loss = 0.09145869
Iteration 11, loss = 0.07911790
Iteration 12, loss = 0.06944883
Iteration 13, loss = 0.06193350
Iteration 14, loss = 0.05633801
Iteration 15, loss = 0.05203322
Iteration 16, loss = 0.04822823
Iteration 17, loss = 0.04505959
Iteration 18, loss = 0.04238091
Iteration 19, loss = 0.04028623
Iteration 20, loss = 0.03890080
Iteration 21, loss = 0.03710760
Iteration 22, loss = 0.03570707
Iteration 23, loss = 0.03448959
Iteration 24, loss = 0.03345809
Iteration 25, loss = 0.03284641
Iteration 26, loss = 0.03200425
Iteration 27, loss = 0.03131915
Iteration 28, loss = 0.03065541
Iteration 29, loss = 0.03030128
Iteration 30, loss = 0.02970765
Iteration 31, loss = 0.02901332
Iteration 32, loss = 0.02872459
Iteration 33, loss = 0.02831070
Iteration 34, loss = 0.02806868
Iteration 35, loss = 0.02773631
Iteration 36, loss = 0.02737583
Iteration 37, loss = 0.02705805
Iteration 38, loss = 0.02711907
Iteration 39, loss = 0.02680445
Iteration 40, loss = 0.02706597
Iteration 41, loss = 0.02652629
Iteration 42, loss = 0.02635824
Iteration 43, loss = 0.02590809
Iteration 44, loss = 0.02608095
Iteration 45, loss = 0.02606979
Iteration 46, loss = 0.02563244
Iteration 47, loss = 0.02588684
Iteration 48, loss = 0.02559091
Iteration 49, loss = 0.02557111
Iteration 50, loss = 0.02545198
Iteration 51, loss = 0.02536751
Iteration 52, loss = 0.02533068
Iteration 53, loss = 0.02507896
Iteration 54, loss = 0.02499773
Iteration 55, loss = 0.02492108
Iteration 56, loss = 0.02478462
Iteration 57, loss = 0.02466629
Iteration 58, loss = 0.02473724
Iteration 59, loss = 0.02460047
Iteration 60, loss = 0.02483509
Iteration 61, loss = 0.02457020
Iteration 62, loss = 0.02451677
Iteration 63, loss = 0.02457407
Iteration 64, loss = 0.02417746
Iteration 65, loss = 0.02436558
Iteration 66, loss = 0.02449101
Iteration 67, loss = 0.02428684
Iteration 68, loss = 0.02408831
Iteration 69, loss = 0.02453952
Iteration 70, loss = 0.02469866
Iteration 71, loss = 0.02391994
Iteration 72, loss = 0.02415684
Iteration 73, loss = 0.02410643
Iteration 74, loss = 0.02422496
Iteration 75, loss = 0.02408904
Iteration 76, loss = 0.02409412
Iteration 77, loss = 0.02418594
Iteration 78, loss = 0.02395913
Iteration 79, loss = 0.02374868
Iteration 80, loss = 0.02400418
Iteration 81, loss = 0.02377190
Iteration 82, loss = 0.02400026
Iteration 83, loss = 0.02370090
Iteration 84, loss = 0.02397781
Iteration 85, loss = 0.02374672
Iteration 86, loss = 0.02379395
Iteration 87, loss = 0.02387222
Iteration 88, loss = 0.02381462
Iteration 89, loss = 0.02347736
Iteration 90, loss = 0.02412359
Iteration 91, loss = 0.02393235
Iteration 92, loss = 0.02361552
Iteration 93, loss = 0.02353071
Iteration 94, loss = 0.02358065
Iteration 95, loss = 0.02369095
Iteration 96, loss = 0.02359444
Iteration 97, loss = 0.02387681
Iteration 98, loss = 0.02342086
Iteration 99, loss = 0.02348487
Iteration 100, loss = 0.02387494
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68121473
Iteration 2, loss = 0.60396773
Iteration 3, loss = 0.48985034
Iteration 4, loss = 0.37544482
Iteration 5, loss = 0.28198057
Iteration 6, loss = 0.21317216
Iteration 7, loss = 0.16489450
Iteration 8, loss = 0.13188143
Iteration 9, loss = 0.10849903
Iteration 10, loss = 0.09157957
Iteration 11, loss = 0.07941752
Iteration 12, loss = 0.07012897
Iteration 13, loss = 0.06324367
Iteration 14, loss = 0.05714970
Iteration 15, loss = 0.05274277
Iteration 16, loss = 0.04884358
Iteration 17, loss = 0.04567308
Iteration 18, loss = 0.04311494
Iteration 19, loss = 0.04109327
Iteration 20, loss = 0.03936790
Iteration 21, loss = 0.03787991
Iteration 22, loss = 0.03646691
Iteration 23, loss = 0.03531517
Iteration 24, loss = 0.03426756
Iteration 25, loss = 0.03356153
Iteration 26, loss = 0.03270015
Iteration 27, loss = 0.03177429
Iteration 28, loss = 0.03099979
Iteration 29, loss = 0.03039614
Iteration 30, loss = 0.03008102
Iteration 31, loss = 0.02937623
Iteration 32, loss = 0.02907501
Iteration 33, loss = 0.02906027
Iteration 34, loss = 0.02827608
Iteration 35, loss = 0.02833474
Iteration 36, loss = 0.02773834
Iteration 37, loss = 0.02767299
Iteration 38, loss = 0.02739531
Iteration 39, loss = 0.02713881
Iteration 40, loss = 0.02693179
Iteration 41, loss = 0.02670900
Iteration 42, loss = 0.02680636
Iteration 43, loss = 0.02628650
Iteration 44, loss = 0.02643971
Iteration 45, loss = 0.02611369
Iteration 46, loss = 0.02621826
Iteration 47, loss = 0.02594823
Iteration 48, loss = 0.02604745
Iteration 49, loss = 0.02569822
Iteration 50, loss = 0.02570631
Iteration 51, loss = 0.02568778
Iteration 52, loss = 0.02569406
Iteration 53, loss = 0.02544013
Iteration 54, loss = 0.02534658
Iteration 55, loss = 0.02547375
Iteration 56, loss = 0.02526828
Iteration 57, loss = 0.02508828
Iteration 58, loss = 0.02486194
Iteration 59, loss = 0.02499614
Iteration 60, loss = 0.02489140
Iteration 61, loss = 0.02474873
Iteration 62, loss = 0.02467115
Iteration 63, loss = 0.02469079
Iteration 64, loss = 0.02487932
Iteration 65, loss = 0.02472980
Iteration 66, loss = 0.02460344
Iteration 67, loss = 0.02453219
Iteration 68, loss = 0.02433212
Iteration 69, loss = 0.02427611
Iteration 70, loss = 0.02477911
Iteration 71, loss = 0.02472978
Iteration 72, loss = 0.02471423
Iteration 73, loss = 0.02432977
Iteration 74, loss = 0.02443794
Iteration 75, loss = 0.02427472
Iteration 76, loss = 0.02464284
Iteration 77, loss = 0.02453383
Iteration 78, loss = 0.02424356
Iteration 79, loss = 0.02445233
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67838916
Iteration 2, loss = 0.60216116
Iteration 3, loss = 0.49341850
Iteration 4, loss = 0.38195438
Iteration 5, loss = 0.28618145
Iteration 6, loss = 0.21510614
Iteration 7, loss = 0.16496715
Iteration 8, loss = 0.13039658
Iteration 9, loss = 0.10640868
Iteration 10, loss = 0.08909155
Iteration 11, loss = 0.07655878
Iteration 12, loss = 0.06713193
Iteration 13, loss = 0.06008258
Iteration 14, loss = 0.05415296
Iteration 15, loss = 0.04956322
Iteration 16, loss = 0.04584841
Iteration 17, loss = 0.04284274
Iteration 18, loss = 0.04055768
Iteration 19, loss = 0.03844762
Iteration 20, loss = 0.03634250
Iteration 21, loss = 0.03477268
Iteration 22, loss = 0.03348784
Iteration 23, loss = 0.03235324
Iteration 24, loss = 0.03124344
Iteration 25, loss = 0.03040170
Iteration 26, loss = 0.02950294
Iteration 27, loss = 0.02881312
Iteration 28, loss = 0.02834877
Iteration 29, loss = 0.02799767
Iteration 30, loss = 0.02729451
Iteration 31, loss = 0.02710605
Iteration 32, loss = 0.02640164
Iteration 33, loss = 0.02612936
Iteration 34, loss = 0.02579286
Iteration 35, loss = 0.02532945
Iteration 36, loss = 0.02510341
Iteration 37, loss = 0.02493579
Iteration 38, loss = 0.02476873
Iteration 39, loss = 0.02444614
Iteration 40, loss = 0.02415059
Iteration 41, loss = 0.02404083
Iteration 42, loss = 0.02380589
Iteration 43, loss = 0.02386552
Iteration 44, loss = 0.02378590
Iteration 45, loss = 0.02334181
Iteration 46, loss = 0.02338711
Iteration 47, loss = 0.02349675
Iteration 48, loss = 0.02302329
Iteration 49, loss = 0.02301345
Iteration 50, loss = 0.02315861
Iteration 51, loss = 0.02317319
Iteration 52, loss = 0.02294012
Iteration 53, loss = 0.02265897
Iteration 54, loss = 0.02260806
Iteration 55, loss = 0.02252293
Iteration 56, loss = 0.02253686
Iteration 57, loss = 0.02249087
Iteration 58, loss = 0.02254528
Iteration 59, loss = 0.02233260
Iteration 60, loss = 0.02202044
Iteration 61, loss = 0.02214192
Iteration 62, loss = 0.02200567
Iteration 63, loss = 0.02198599
Iteration 64, loss = 0.02185744
Iteration 65, loss = 0.02220397
Iteration 66, loss = 0.02199708
Iteration 67, loss = 0.02192493
Iteration 68, loss = 0.02189302
Iteration 69, loss = 0.02173508
Iteration 70, loss = 0.02162613
Iteration 71, loss = 0.02156310
Iteration 72, loss = 0.02172146
Iteration 73, loss = 0.02142474
Iteration 74, loss = 0.02177364
Iteration 75, loss = 0.02176723
Iteration 76, loss = 0.02146207
Iteration 77, loss = 0.02168517
Iteration 78, loss = 0.02159309
Iteration 79, loss = 0.02147330
Iteration 80, loss = 0.02145929
Iteration 81, loss = 0.02164557
Iteration 82, loss = 0.02134908
Iteration 83, loss = 0.02109303
Iteration 84, loss = 0.02152735
Iteration 85, loss = 0.02136894
Iteration 86, loss = 0.02125630
Iteration 87, loss = 0.02157173
Iteration 88, loss = 0.02149530
Iteration 89, loss = 0.02131758
Iteration 90, loss = 0.02188326
Iteration 91, loss = 0.02146798
Iteration 92, loss = 0.02135095
Iteration 93, loss = 0.02124725
Iteration 94, loss = 0.02119421
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68741544
Iteration 2, loss = 0.61625913
Iteration 3, loss = 0.51361337
Iteration 4, loss = 0.40378533
Iteration 5, loss = 0.30723075
Iteration 6, loss = 0.23181221
Iteration 7, loss = 0.17752956
Iteration 8, loss = 0.13965324
Iteration 9, loss = 0.11340415
Iteration 10, loss = 0.09417172
Iteration 11, loss = 0.08059403
Iteration 12, loss = 0.07025970
Iteration 13, loss = 0.06235966
Iteration 14, loss = 0.05634324
Iteration 15, loss = 0.05124252
Iteration 16, loss = 0.04752440
Iteration 17, loss = 0.04429189
Iteration 18, loss = 0.04127320
Iteration 19, loss = 0.03915438
Iteration 20, loss = 0.03715014
Iteration 21, loss = 0.03554691
Iteration 22, loss = 0.03427397
Iteration 23, loss = 0.03290159
Iteration 24, loss = 0.03181493
Iteration 25, loss = 0.03078110
Iteration 26, loss = 0.03008052
Iteration 27, loss = 0.02936623
Iteration 28, loss = 0.02858871
Iteration 29, loss = 0.02831932
Iteration 30, loss = 0.02755320
Iteration 31, loss = 0.02714177
Iteration 32, loss = 0.02669129
Iteration 33, loss = 0.02610634
Iteration 34, loss = 0.02590888
Iteration 35, loss = 0.02576518
Iteration 36, loss = 0.02540234
Iteration 37, loss = 0.02512608
Iteration 38, loss = 0.02483282
Iteration 39, loss = 0.02460755
Iteration 40, loss = 0.02470920
Iteration 41, loss = 0.02427010
Iteration 42, loss = 0.02412140
Iteration 43, loss = 0.02373912
Iteration 44, loss = 0.02371350
Iteration 45, loss = 0.02358756
Iteration 46, loss = 0.02354997
Iteration 47, loss = 0.02322718
Iteration 48, loss = 0.02335758
Iteration 49, loss = 0.02318837
Iteration 50, loss = 0.02288755
Iteration 51, loss = 0.02292135
Iteration 52, loss = 0.02303768
Iteration 53, loss = 0.02262191
Iteration 54, loss = 0.02263640
Iteration 55, loss = 0.02236508
Iteration 56, loss = 0.02262214
Iteration 57, loss = 0.02259725
Iteration 58, loss = 0.02249090
Iteration 59, loss = 0.02238507
Iteration 60, loss = 0.02278859
Iteration 61, loss = 0.02220985
Iteration 62, loss = 0.02235015
Iteration 63, loss = 0.02224144
Iteration 64, loss = 0.02192933
Iteration 65, loss = 0.02227184
Iteration 66, loss = 0.02194369
Iteration 67, loss = 0.02194136
Iteration 68, loss = 0.02201928
Iteration 69, loss = 0.02198866
Iteration 70, loss = 0.02184558
Iteration 71, loss = 0.02205146
Iteration 72, loss = 0.02204876
Iteration 73, loss = 0.02183333
Iteration 74, loss = 0.02166920
Iteration 75, loss = 0.02152857
Iteration 76, loss = 0.02167208
Iteration 77, loss = 0.02145570
Iteration 78, loss = 0.02150078
Iteration 79, loss = 0.02165564
Iteration 80, loss = 0.02161617
Iteration 81, loss = 0.02195078
Iteration 82, loss = 0.02152339
Iteration 83, loss = 0.02179365
Iteration 84, loss = 0.02144563
Iteration 85, loss = 0.02136231
Iteration 86, loss = 0.02132020
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67168779
Iteration 2, loss = 0.59161762
Iteration 3, loss = 0.48113885
Iteration 4, loss = 0.37317225
Iteration 5, loss = 0.28356678
Iteration 6, loss = 0.21551310
Iteration 7, loss = 0.16762552
Iteration 8, loss = 0.13371587
Iteration 9, loss = 0.10984170
Iteration 10, loss = 0.09241339
Iteration 11, loss = 0.07989652
Iteration 12, loss = 0.06988081
Iteration 13, loss = 0.06252271
Iteration 14, loss = 0.05667938
Iteration 15, loss = 0.05200970
Iteration 16, loss = 0.04783997
Iteration 17, loss = 0.04483058
Iteration 18, loss = 0.04243226
Iteration 19, loss = 0.03978407
Iteration 20, loss = 0.03769882
Iteration 21, loss = 0.03607582
Iteration 22, loss = 0.03460750
Iteration 23, loss = 0.03351500
Iteration 24, loss = 0.03252185
Iteration 25, loss = 0.03143294
Iteration 26, loss = 0.03052076
Iteration 27, loss = 0.02971515
Iteration 28, loss = 0.02917836
Iteration 29, loss = 0.02870967
Iteration 30, loss = 0.02802586
Iteration 31, loss = 0.02757607
Iteration 32, loss = 0.02719254
Iteration 33, loss = 0.02652296
Iteration 34, loss = 0.02635434
Iteration 35, loss = 0.02595947
Iteration 36, loss = 0.02580969
Iteration 37, loss = 0.02541680
Iteration 38, loss = 0.02534425
Iteration 39, loss = 0.02508861
Iteration 40, loss = 0.02483032
Iteration 41, loss = 0.02447224
Iteration 42, loss = 0.02448304
Iteration 43, loss = 0.02407427
Iteration 44, loss = 0.02426729
Iteration 45, loss = 0.02401541
Iteration 46, loss = 0.02376428
Iteration 47, loss = 0.02347568
Iteration 48, loss = 0.02350996
Iteration 49, loss = 0.02344413
Iteration 50, loss = 0.02306687
Iteration 51, loss = 0.02340652
Iteration 52, loss = 0.02293080
Iteration 53, loss = 0.02294200
Iteration 54, loss = 0.02305809
Iteration 55, loss = 0.02270143
Iteration 56, loss = 0.02282535
Iteration 57, loss = 0.02278363
Iteration 58, loss = 0.02236190
Iteration 59, loss = 0.02269006
Iteration 60, loss = 0.02247696
Iteration 61, loss = 0.02230177
Iteration 62, loss = 0.02237424
Iteration 63, loss = 0.02231985
Iteration 64, loss = 0.02217044
Iteration 65, loss = 0.02236521
Iteration 66, loss = 0.02203119
Iteration 67, loss = 0.02214422
Iteration 68, loss = 0.02225601
Iteration 69, loss = 0.02210211
Iteration 70, loss = 0.02204336
Iteration 71, loss = 0.02190989
Iteration 72, loss = 0.02212604
Iteration 73, loss = 0.02213131
Iteration 74, loss = 0.02225508
Iteration 75, loss = 0.02202334
Iteration 76, loss = 0.02185564
Iteration 77, loss = 0.02190603
Iteration 78, loss = 0.02177371
Iteration 79, loss = 0.02189239
Iteration 80, loss = 0.02170167
Iteration 81, loss = 0.02180731
Iteration 82, loss = 0.02172187
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68287087
Iteration 2, loss = 0.60179482
Iteration 3, loss = 0.47713588
Iteration 4, loss = 0.35113881
Iteration 5, loss = 0.25268140
Iteration 6, loss = 0.18532584
Iteration 7, loss = 0.14138209
Iteration 8, loss = 0.11224429
Iteration 9, loss = 0.09264104
Iteration 10, loss = 0.07864596
Iteration 11, loss = 0.06879520
Iteration 12, loss = 0.06122015
Iteration 13, loss = 0.05551048
Iteration 14, loss = 0.05073819
Iteration 15, loss = 0.04724464
Iteration 16, loss = 0.04432679
Iteration 17, loss = 0.04179614
Iteration 18, loss = 0.04005055
Iteration 19, loss = 0.03819588
Iteration 20, loss = 0.03705049
Iteration 21, loss = 0.03545810
Iteration 22, loss = 0.03450663
Iteration 23, loss = 0.03376847
Iteration 24, loss = 0.03279700
Iteration 25, loss = 0.03181593
Iteration 26, loss = 0.03145176
Iteration 27, loss = 0.03137703
Iteration 28, loss = 0.03041094
Iteration 29, loss = 0.03006059
Iteration 30, loss = 0.02967545
Iteration 31, loss = 0.02921382
Iteration 32, loss = 0.02917420
Iteration 33, loss = 0.02886682
Iteration 34, loss = 0.02831911
Iteration 35, loss = 0.02819681
Iteration 36, loss = 0.02815865
Iteration 37, loss = 0.02776540
Iteration 38, loss = 0.02730165
Iteration 39, loss = 0.02744174
Iteration 40, loss = 0.02721963
Iteration 41, loss = 0.02738391
Iteration 42, loss = 0.02717360
Iteration 43, loss = 0.02693319
Iteration 44, loss = 0.02687739
Iteration 45, loss = 0.02687974
Iteration 46, loss = 0.02687255
Iteration 47, loss = 0.02629848
Iteration 48, loss = 0.02673382
Iteration 49, loss = 0.02619081
Iteration 50, loss = 0.02635613
Iteration 51, loss = 0.02641851
Iteration 52, loss = 0.02631006
Iteration 53, loss = 0.02602893
Iteration 54, loss = 0.02580820
Iteration 55, loss = 0.02609817
Iteration 56, loss = 0.02589927
Iteration 57, loss = 0.02626498
Iteration 58, loss = 0.02619900
Iteration 59, loss = 0.02581749
Iteration 60, loss = 0.02593874
Iteration 61, loss = 0.02564495
Iteration 62, loss = 0.02569975
Iteration 63, loss = 0.02592896
Iteration 64, loss = 0.02560661
Iteration 65, loss = 0.02582844
Iteration 66, loss = 0.02554708
Iteration 67, loss = 0.02557405
Iteration 68, loss = 0.02552303
Iteration 69, loss = 0.02527582
Iteration 70, loss = 0.02561016
Iteration 71, loss = 0.02532742
Iteration 72, loss = 0.02545993
Iteration 73, loss = 0.02541699
Iteration 74, loss = 0.02534375
Iteration 75, loss = 0.02510899
Iteration 76, loss = 0.02541636
Iteration 77, loss = 0.02520629
Iteration 78, loss = 0.02509822
Iteration 79, loss = 0.02523930
Iteration 80, loss = 0.02539388
Iteration 81, loss = 0.02527894
Iteration 82, loss = 0.02532049
Iteration 83, loss = 0.02519050
Iteration 84, loss = 0.02535488
Iteration 85, loss = 0.02516587
Iteration 86, loss = 0.02547877
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on dropped_v5_lemma_stem...
Iteration 1, loss = 0.67034307
Iteration 2, loss = 0.65734152
Iteration 3, loss = 0.64599110
Iteration 4, loss = 0.63450371
Iteration 5, loss = 0.62234760
Iteration 6, loss = 0.60892886
Iteration 7, loss = 0.59425807
Iteration 8, loss = 0.57821782
Iteration 9, loss = 0.56076628
Iteration 10, loss = 0.54209493
Iteration 11, loss = 0.52222150
Iteration 12, loss = 0.50127286
Iteration 13, loss = 0.47959892
Iteration 14, loss = 0.45767577
Iteration 15, loss = 0.43518066
Iteration 16, loss = 0.41298947
Iteration 17, loss = 0.39115362
Iteration 18, loss = 0.36968838
Iteration 19, loss = 0.34881002
Iteration 20, loss = 0.32861440
Iteration 21, loss = 0.30921589
Iteration 22, loss = 0.29078597
Iteration 23, loss = 0.27298067
Iteration 24, loss = 0.25628076
Iteration 25, loss = 0.24035013
Iteration 26, loss = 0.22564693
Iteration 27, loss = 0.21157478
Iteration 28, loss = 0.19845865
Iteration 29, loss = 0.18632386
Iteration 30, loss = 0.17489970
Iteration 31, loss = 0.16437712
Iteration 32, loss = 0.15452858
Iteration 33, loss = 0.14544072
Iteration 34, loss = 0.13698079
Iteration 35, loss = 0.12916396
Iteration 36, loss = 0.12191605
Iteration 37, loss = 0.11521416
Iteration 38, loss = 0.10896021
Iteration 39, loss = 0.10315412
Iteration 40, loss = 0.09778178
Iteration 41, loss = 0.09280501
Iteration 42, loss = 0.08820171
Iteration 43, loss = 0.08386227
Iteration 44, loss = 0.07984312
Iteration 45, loss = 0.07604494
Iteration 46, loss = 0.07258360
Iteration 47, loss = 0.06931062
Iteration 48, loss = 0.06629546
Iteration 49, loss = 0.06341998
Iteration 50, loss = 0.06069916
Iteration 51, loss = 0.05821097
Iteration 52, loss = 0.05583289
Iteration 53, loss = 0.05362070
Iteration 54, loss = 0.05154173
Iteration 55, loss = 0.04957680
Iteration 56, loss = 0.04772291
Iteration 57, loss = 0.04598517
Iteration 58, loss = 0.04432610
Iteration 59, loss = 0.04275335
Iteration 60, loss = 0.04126192
Iteration 61, loss = 0.03987047
Iteration 62, loss = 0.03852675
Iteration 63, loss = 0.03727353
Iteration 64, loss = 0.03605580
Iteration 65, loss = 0.03493537
Iteration 66, loss = 0.03384426
Iteration 67, loss = 0.03279077
Iteration 68, loss = 0.03180813
Iteration 69, loss = 0.03088178
Iteration 70, loss = 0.02999248
Iteration 71, loss = 0.02911412
Iteration 72, loss = 0.02830283
Iteration 73, loss = 0.02752084
Iteration 74, loss = 0.02678412
Iteration 75, loss = 0.02603712
Iteration 76, loss = 0.02534343
Iteration 77, loss = 0.02468106
Iteration 78, loss = 0.02404134
Iteration 79, loss = 0.02343694
Iteration 80, loss = 0.02283941
Iteration 81, loss = 0.02229445
Iteration 82, loss = 0.02174300
Iteration 83, loss = 0.02121915
Iteration 84, loss = 0.02071700
Iteration 85, loss = 0.02023851
Iteration 86, loss = 0.01975775
Iteration 87, loss = 0.01931470
Iteration 88, loss = 0.01886790
Iteration 89, loss = 0.01844147
Iteration 90, loss = 0.01804503
Iteration 91, loss = 0.01764615
Iteration 92, loss = 0.01726847
Iteration 93, loss = 0.01689111
Iteration 94, loss = 0.01652632
Iteration 95, loss = 0.01619104
Iteration 96, loss = 0.01586100
Iteration 97, loss = 0.01553712
Iteration 98, loss = 0.01522654
Iteration 99, loss = 0.01492849
Iteration 100, loss = 0.01464382
Iteration 101, loss = 0.01434803
Iteration 102, loss = 0.01408717
Iteration 103, loss = 0.01381568
Iteration 104, loss = 0.01356131
Iteration 105, loss = 0.01330722
Iteration 106, loss = 0.01307187
Iteration 107, loss = 0.01283315
Iteration 108, loss = 0.01260968
Iteration 109, loss = 0.01238257
Iteration 110, loss = 0.01217510
Iteration 111, loss = 0.01195942
Iteration 112, loss = 0.01176266
Iteration 113, loss = 0.01155651
Iteration 114, loss = 0.01136893
Iteration 115, loss = 0.01118290
Iteration 116, loss = 0.01099172
Iteration 117, loss = 0.01081716
Iteration 118, loss = 0.01064154
Iteration 119, loss = 0.01047106
Iteration 120, loss = 0.01031052
Iteration 121, loss = 0.01015017
Iteration 122, loss = 0.00999505
Iteration 123, loss = 0.00984739
Iteration 124, loss = 0.00969248
Iteration 125, loss = 0.00955009
Iteration 126, loss = 0.00940391
Iteration 127, loss = 0.00926954
Iteration 128, loss = 0.00912923
Iteration 129, loss = 0.00899317
Iteration 130, loss = 0.00886741
Iteration 131, loss = 0.00874115
Iteration 132, loss = 0.00861624
Iteration 133, loss = 0.00849346
Iteration 134, loss = 0.00838655
Iteration 135, loss = 0.00826489
Iteration 136, loss = 0.00815682
Iteration 137, loss = 0.00804352
Iteration 138, loss = 0.00793864
Iteration 139, loss = 0.00783179
Iteration 140, loss = 0.00773213
Iteration 141, loss = 0.00762971
Iteration 142, loss = 0.00752982
Iteration 143, loss = 0.00744288
Iteration 144, loss = 0.00734481
Iteration 145, loss = 0.00725507
Iteration 146, loss = 0.00716846
Iteration 147, loss = 0.00708029
Iteration 148, loss = 0.00699808
Iteration 149, loss = 0.00691286
Iteration 150, loss = 0.00682892
Iteration 151, loss = 0.00675043
Iteration 152, loss = 0.00667134
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68049754
Iteration 2, loss = 0.63845606
Iteration 3, loss = 0.57625221
Iteration 4, loss = 0.49667929
Iteration 5, loss = 0.41377886
Iteration 6, loss = 0.33843400
Iteration 7, loss = 0.27470351
Iteration 8, loss = 0.22340562
Iteration 9, loss = 0.18293434
Iteration 10, loss = 0.15245822
Iteration 11, loss = 0.12862460
Iteration 12, loss = 0.11051650
Iteration 13, loss = 0.09638639
Iteration 14, loss = 0.08507515
Iteration 15, loss = 0.07607574
Iteration 16, loss = 0.06872107
Iteration 17, loss = 0.06265096
Iteration 18, loss = 0.05766294
Iteration 19, loss = 0.05346055
Iteration 20, loss = 0.04988320
Iteration 21, loss = 0.04697142
Iteration 22, loss = 0.04451096
Iteration 23, loss = 0.04205941
Iteration 24, loss = 0.04015318
Iteration 25, loss = 0.03834184
Iteration 26, loss = 0.03687892
Iteration 27, loss = 0.03554858
Iteration 28, loss = 0.03433192
Iteration 29, loss = 0.03333110
Iteration 30, loss = 0.03239994
Iteration 31, loss = 0.03160729
Iteration 32, loss = 0.03076647
Iteration 33, loss = 0.02988345
Iteration 34, loss = 0.02922596
Iteration 35, loss = 0.02872096
Iteration 36, loss = 0.02838730
Iteration 37, loss = 0.02790514
Iteration 38, loss = 0.02770177
Iteration 39, loss = 0.02737743
Iteration 40, loss = 0.02661886
Iteration 41, loss = 0.02621665
Iteration 42, loss = 0.02584146
Iteration 43, loss = 0.02581769
Iteration 44, loss = 0.02573098
Iteration 45, loss = 0.02524711
Iteration 46, loss = 0.02494846
Iteration 47, loss = 0.02507929
Iteration 48, loss = 0.02453369
Iteration 49, loss = 0.02413799
Iteration 50, loss = 0.02386214
Iteration 51, loss = 0.02388618
Iteration 52, loss = 0.02360984
Iteration 53, loss = 0.02355502
Iteration 54, loss = 0.02346786
Iteration 55, loss = 0.02318331
Iteration 56, loss = 0.02313482
Iteration 57, loss = 0.02309232
Iteration 58, loss = 0.02300600
Iteration 59, loss = 0.02282901
Iteration 60, loss = 0.02281684
Iteration 61, loss = 0.02263216
Iteration 62, loss = 0.02236414
Iteration 63, loss = 0.02255795
Iteration 64, loss = 0.02209776
Iteration 65, loss = 0.02228468
Iteration 66, loss = 0.02214908
Iteration 67, loss = 0.02207059
Iteration 68, loss = 0.02190030
Iteration 69, loss = 0.02179373
Iteration 70, loss = 0.02229381
Iteration 71, loss = 0.02188481
Iteration 72, loss = 0.02163898
Iteration 73, loss = 0.02185859
Iteration 74, loss = 0.02197211
Iteration 75, loss = 0.02180689
Iteration 76, loss = 0.02147177
Iteration 77, loss = 0.02182528
Iteration 78, loss = 0.02157560
Iteration 79, loss = 0.02187641
Iteration 80, loss = 0.02156933
Iteration 81, loss = 0.02157216
Iteration 82, loss = 0.02126023
Iteration 83, loss = 0.02155712
Iteration 84, loss = 0.02132675
Iteration 85, loss = 0.02135872
Iteration 86, loss = 0.02113888
Iteration 87, loss = 0.02111908
Iteration 88, loss = 0.02126143
Iteration 89, loss = 0.02130105
Iteration 90, loss = 0.02133119
Iteration 91, loss = 0.02138904
Iteration 92, loss = 0.02121420
Iteration 93, loss = 0.02110202
Iteration 94, loss = 0.02075859
Iteration 95, loss = 0.02103373
Iteration 96, loss = 0.02094557
Iteration 97, loss = 0.02097326
Iteration 98, loss = 0.02092425
Iteration 99, loss = 0.02092613
Iteration 100, loss = 0.02106913
Iteration 101, loss = 0.02108796
Iteration 102, loss = 0.02078265
Iteration 103, loss = 0.02091485
Iteration 104, loss = 0.02072186
Iteration 105, loss = 0.02067412
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69348690
Iteration 2, loss = 0.67156538
Iteration 3, loss = 0.65139484
Iteration 4, loss = 0.62872275
Iteration 5, loss = 0.60289754
Iteration 6, loss = 0.57418933
Iteration 7, loss = 0.54309129
Iteration 8, loss = 0.51030415
Iteration 9, loss = 0.47654594
Iteration 10, loss = 0.44224425
Iteration 11, loss = 0.40877520
Iteration 12, loss = 0.37628452
Iteration 13, loss = 0.34531538
Iteration 14, loss = 0.31629836
Iteration 15, loss = 0.28925549
Iteration 16, loss = 0.26414658
Iteration 17, loss = 0.24130489
Iteration 18, loss = 0.22051900
Iteration 19, loss = 0.20170416
Iteration 20, loss = 0.18471175
Iteration 21, loss = 0.16946714
Iteration 22, loss = 0.15589412
Iteration 23, loss = 0.14364962
Iteration 24, loss = 0.13273666
Iteration 25, loss = 0.12292477
Iteration 26, loss = 0.11414448
Iteration 27, loss = 0.10633230
Iteration 28, loss = 0.09924116
Iteration 29, loss = 0.09288660
Iteration 30, loss = 0.08716827
Iteration 31, loss = 0.08194825
Iteration 32, loss = 0.07733235
Iteration 33, loss = 0.07295825
Iteration 34, loss = 0.06913240
Iteration 35, loss = 0.06564954
Iteration 36, loss = 0.06240559
Iteration 37, loss = 0.05935238
Iteration 38, loss = 0.05666062
Iteration 39, loss = 0.05412336
Iteration 40, loss = 0.05184697
Iteration 41, loss = 0.04969991
Iteration 42, loss = 0.04769844
Iteration 43, loss = 0.04582281
Iteration 44, loss = 0.04413179
Iteration 45, loss = 0.04254491
Iteration 46, loss = 0.04103414
Iteration 47, loss = 0.03967439
Iteration 48, loss = 0.03838368
Iteration 49, loss = 0.03720742
Iteration 50, loss = 0.03606442
Iteration 51, loss = 0.03498048
Iteration 52, loss = 0.03395391
Iteration 53, loss = 0.03306469
Iteration 54, loss = 0.03214288
Iteration 55, loss = 0.03135560
Iteration 56, loss = 0.03052659
Iteration 57, loss = 0.02973053
Iteration 58, loss = 0.02903133
Iteration 59, loss = 0.02837709
Iteration 60, loss = 0.02772382
Iteration 61, loss = 0.02719152
Iteration 62, loss = 0.02658331
Iteration 63, loss = 0.02604857
Iteration 64, loss = 0.02553570
Iteration 65, loss = 0.02504022
Iteration 66, loss = 0.02460053
Iteration 67, loss = 0.02411478
Iteration 68, loss = 0.02373643
Iteration 69, loss = 0.02326994
Iteration 70, loss = 0.02286048
Iteration 71, loss = 0.02252525
Iteration 72, loss = 0.02219594
Iteration 73, loss = 0.02179195
Iteration 74, loss = 0.02147048
Iteration 75, loss = 0.02118642
Iteration 76, loss = 0.02089636
Iteration 77, loss = 0.02058608
Iteration 78, loss = 0.02027570
Iteration 79, loss = 0.02007288
Iteration 80, loss = 0.01979286
Iteration 81, loss = 0.01949021
Iteration 82, loss = 0.01936079
Iteration 83, loss = 0.01907316
Iteration 84, loss = 0.01888986
Iteration 85, loss = 0.01864534
Iteration 86, loss = 0.01843497
Iteration 87, loss = 0.01824096
Iteration 88, loss = 0.01804234
Iteration 89, loss = 0.01787873
Iteration 90, loss = 0.01777009
Iteration 91, loss = 0.01749547
Iteration 92, loss = 0.01735410
Iteration 93, loss = 0.01719646
Iteration 94, loss = 0.01705457
Iteration 95, loss = 0.01687724
Iteration 96, loss = 0.01674036
Iteration 97, loss = 0.01661028
Iteration 98, loss = 0.01648177
Iteration 99, loss = 0.01637799
Iteration 100, loss = 0.01629649
Iteration 101, loss = 0.01608365
Iteration 102, loss = 0.01597408
Iteration 103, loss = 0.01591019
Iteration 104, loss = 0.01584615
Iteration 105, loss = 0.01564231
Iteration 106, loss = 0.01557528
Iteration 107, loss = 0.01551042
Iteration 108, loss = 0.01535110
Iteration 109, loss = 0.01529287
Iteration 110, loss = 0.01517748
Iteration 111, loss = 0.01506398
Iteration 112, loss = 0.01499085
Iteration 113, loss = 0.01491621
Iteration 114, loss = 0.01481936
Iteration 115, loss = 0.01474230
Iteration 116, loss = 0.01464670
Iteration 117, loss = 0.01458747
Iteration 118, loss = 0.01454646
Iteration 119, loss = 0.01448126
Iteration 120, loss = 0.01432861
Iteration 121, loss = 0.01430907
Iteration 122, loss = 0.01425365
Iteration 123, loss = 0.01420241
Iteration 124, loss = 0.01411169
Iteration 125, loss = 0.01403941
Iteration 126, loss = 0.01399681
Iteration 127, loss = 0.01389995
Iteration 128, loss = 0.01390799
Iteration 129, loss = 0.01381223
Iteration 130, loss = 0.01372237
Iteration 131, loss = 0.01375902
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71492068
Iteration 2, loss = 0.67892020
Iteration 3, loss = 0.63092794
Iteration 4, loss = 0.57026388
Iteration 5, loss = 0.49898087
Iteration 6, loss = 0.42484903
Iteration 7, loss = 0.35633089
Iteration 8, loss = 0.29655803
Iteration 9, loss = 0.24714453
Iteration 10, loss = 0.20639409
Iteration 11, loss = 0.17415305
Iteration 12, loss = 0.14810589
Iteration 13, loss = 0.12752653
Iteration 14, loss = 0.11095681
Iteration 15, loss = 0.09770124
Iteration 16, loss = 0.08681081
Iteration 17, loss = 0.07780421
Iteration 18, loss = 0.07041938
Iteration 19, loss = 0.06417601
Iteration 20, loss = 0.05886889
Iteration 21, loss = 0.05433472
Iteration 22, loss = 0.05050270
Iteration 23, loss = 0.04702598
Iteration 24, loss = 0.04411958
Iteration 25, loss = 0.04160558
Iteration 26, loss = 0.03931916
Iteration 27, loss = 0.03733556
Iteration 28, loss = 0.03572026
Iteration 29, loss = 0.03409057
Iteration 30, loss = 0.03256920
Iteration 31, loss = 0.03133189
Iteration 32, loss = 0.03028423
Iteration 33, loss = 0.02921594
Iteration 34, loss = 0.02829004
Iteration 35, loss = 0.02724758
Iteration 36, loss = 0.02646024
Iteration 37, loss = 0.02595351
Iteration 38, loss = 0.02515367
Iteration 39, loss = 0.02458494
Iteration 40, loss = 0.02400495
Iteration 41, loss = 0.02352972
Iteration 42, loss = 0.02300605
Iteration 43, loss = 0.02276199
Iteration 44, loss = 0.02220777
Iteration 45, loss = 0.02177847
Iteration 46, loss = 0.02165479
Iteration 47, loss = 0.02100469
Iteration 48, loss = 0.02076324
Iteration 49, loss = 0.02051563
Iteration 50, loss = 0.02031189
Iteration 51, loss = 0.01999222
Iteration 52, loss = 0.01974922
Iteration 53, loss = 0.01970640
Iteration 54, loss = 0.01927301
Iteration 55, loss = 0.01901931
Iteration 56, loss = 0.01890034
Iteration 57, loss = 0.01880765
Iteration 58, loss = 0.01856950
Iteration 59, loss = 0.01840176
Iteration 60, loss = 0.01818639
Iteration 61, loss = 0.01824194
Iteration 62, loss = 0.01804361
Iteration 63, loss = 0.01780251
Iteration 64, loss = 0.01776573
Iteration 65, loss = 0.01761743
Iteration 66, loss = 0.01750101
Iteration 67, loss = 0.01734815
Iteration 68, loss = 0.01728791
Iteration 69, loss = 0.01734112
Iteration 70, loss = 0.01709579
Iteration 71, loss = 0.01706663
Iteration 72, loss = 0.01696424
Iteration 73, loss = 0.01679665
Iteration 74, loss = 0.01670390
Iteration 75, loss = 0.01659957
Iteration 76, loss = 0.01670382
Iteration 77, loss = 0.01665567
Iteration 78, loss = 0.01645894
Iteration 79, loss = 0.01634487
Iteration 80, loss = 0.01631163
Iteration 81, loss = 0.01639217
Iteration 82, loss = 0.01628532
Iteration 83, loss = 0.01620737
Iteration 84, loss = 0.01626671
Iteration 85, loss = 0.01598314
Iteration 86, loss = 0.01600027
Iteration 87, loss = 0.01599264
Iteration 88, loss = 0.01611862
Iteration 89, loss = 0.01591190
Iteration 90, loss = 0.01581510
Iteration 91, loss = 0.01592243
Iteration 92, loss = 0.01589554
Iteration 93, loss = 0.01583514
Iteration 94, loss = 0.01568966
Iteration 95, loss = 0.01574642
Iteration 96, loss = 0.01566407
Iteration 97, loss = 0.01583064
Iteration 98, loss = 0.01564935
Iteration 99, loss = 0.01561937
Iteration 100, loss = 0.01539592
Iteration 101, loss = 0.01539865
Iteration 102, loss = 0.01553501
Iteration 103, loss = 0.01549485
Iteration 104, loss = 0.01545312
Iteration 105, loss = 0.01533768
Iteration 106, loss = 0.01543897
Iteration 107, loss = 0.01535922
Iteration 108, loss = 0.01536099
Iteration 109, loss = 0.01536606
Iteration 110, loss = 0.01529260
Iteration 111, loss = 0.01531255
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68643988
Iteration 2, loss = 0.64085836
Iteration 3, loss = 0.57623810
Iteration 4, loss = 0.49665350
Iteration 5, loss = 0.41687775
Iteration 6, loss = 0.34286300
Iteration 7, loss = 0.27952419
Iteration 8, loss = 0.22777085
Iteration 9, loss = 0.18685751
Iteration 10, loss = 0.15575804
Iteration 11, loss = 0.13150541
Iteration 12, loss = 0.11287133
Iteration 13, loss = 0.09822021
Iteration 14, loss = 0.08665652
Iteration 15, loss = 0.07754193
Iteration 16, loss = 0.06979675
Iteration 17, loss = 0.06385201
Iteration 18, loss = 0.05866326
Iteration 19, loss = 0.05436224
Iteration 20, loss = 0.05083327
Iteration 21, loss = 0.04769673
Iteration 22, loss = 0.04490814
Iteration 23, loss = 0.04255751
Iteration 24, loss = 0.04050933
Iteration 25, loss = 0.03872073
Iteration 26, loss = 0.03736519
Iteration 27, loss = 0.03580043
Iteration 28, loss = 0.03446118
Iteration 29, loss = 0.03369071
Iteration 30, loss = 0.03226053
Iteration 31, loss = 0.03165179
Iteration 32, loss = 0.03085862
Iteration 33, loss = 0.03025500
Iteration 34, loss = 0.02959714
Iteration 35, loss = 0.02888245
Iteration 36, loss = 0.02825188
Iteration 37, loss = 0.02789724
Iteration 38, loss = 0.02764883
Iteration 39, loss = 0.02680595
Iteration 40, loss = 0.02673967
Iteration 41, loss = 0.02620468
Iteration 42, loss = 0.02596297
Iteration 43, loss = 0.02561107
Iteration 44, loss = 0.02524627
Iteration 45, loss = 0.02483747
Iteration 46, loss = 0.02474129
Iteration 47, loss = 0.02425699
Iteration 48, loss = 0.02435134
Iteration 49, loss = 0.02416166
Iteration 50, loss = 0.02380507
Iteration 51, loss = 0.02367353
Iteration 52, loss = 0.02363815
Iteration 53, loss = 0.02323851
Iteration 54, loss = 0.02316091
Iteration 55, loss = 0.02313765
Iteration 56, loss = 0.02292152
Iteration 57, loss = 0.02254859
Iteration 58, loss = 0.02269330
Iteration 59, loss = 0.02268811
Iteration 60, loss = 0.02274729
Iteration 61, loss = 0.02256772
Iteration 62, loss = 0.02224745
Iteration 63, loss = 0.02210420
Iteration 64, loss = 0.02223369
Iteration 65, loss = 0.02193288
Iteration 66, loss = 0.02193432
Iteration 67, loss = 0.02169968
Iteration 68, loss = 0.02167026
Iteration 69, loss = 0.02218875
Iteration 70, loss = 0.02183038
Iteration 71, loss = 0.02157211
Iteration 72, loss = 0.02141395
Iteration 73, loss = 0.02140463
Iteration 74, loss = 0.02133593
Iteration 75, loss = 0.02125059
Iteration 76, loss = 0.02116547
Iteration 77, loss = 0.02134858
Iteration 78, loss = 0.02117229
Iteration 79, loss = 0.02120996
Iteration 80, loss = 0.02109580
Iteration 81, loss = 0.02091395
Iteration 82, loss = 0.02096335
Iteration 83, loss = 0.02081567
Iteration 84, loss = 0.02095328
Iteration 85, loss = 0.02083500
Iteration 86, loss = 0.02071965
Iteration 87, loss = 0.02068245
Iteration 88, loss = 0.02080390
Iteration 89, loss = 0.02073545
Iteration 90, loss = 0.02050527
Iteration 91, loss = 0.02063920
Iteration 92, loss = 0.02070725
Iteration 93, loss = 0.02051789
Iteration 94, loss = 0.02084378
Iteration 95, loss = 0.02063031
Iteration 96, loss = 0.02062476
Iteration 97, loss = 0.02057714
Iteration 98, loss = 0.02065014
Iteration 99, loss = 0.02069368
Iteration 100, loss = 0.02071317
Iteration 101, loss = 0.02047830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69830253
Iteration 2, loss = 0.64769870
Iteration 3, loss = 0.58205310
Iteration 4, loss = 0.50219107
Iteration 5, loss = 0.42083723
Iteration 6, loss = 0.34580609
Iteration 7, loss = 0.28125618
Iteration 8, loss = 0.22882041
Iteration 9, loss = 0.18771848
Iteration 10, loss = 0.15574848
Iteration 11, loss = 0.13112038
Iteration 12, loss = 0.11242295
Iteration 13, loss = 0.09752588
Iteration 14, loss = 0.08585124
Iteration 15, loss = 0.07626643
Iteration 16, loss = 0.06861977
Iteration 17, loss = 0.06252382
Iteration 18, loss = 0.05731118
Iteration 19, loss = 0.05301269
Iteration 20, loss = 0.04933405
Iteration 21, loss = 0.04634969
Iteration 22, loss = 0.04339806
Iteration 23, loss = 0.04101548
Iteration 24, loss = 0.03888156
Iteration 25, loss = 0.03704296
Iteration 26, loss = 0.03562981
Iteration 27, loss = 0.03409351
Iteration 28, loss = 0.03287078
Iteration 29, loss = 0.03171185
Iteration 30, loss = 0.03078504
Iteration 31, loss = 0.02990831
Iteration 32, loss = 0.02945886
Iteration 33, loss = 0.02845508
Iteration 34, loss = 0.02786008
Iteration 35, loss = 0.02732419
Iteration 36, loss = 0.02662356
Iteration 37, loss = 0.02588245
Iteration 38, loss = 0.02555986
Iteration 39, loss = 0.02491927
Iteration 40, loss = 0.02471436
Iteration 41, loss = 0.02421358
Iteration 42, loss = 0.02411510
Iteration 43, loss = 0.02375615
Iteration 44, loss = 0.02338613
Iteration 45, loss = 0.02321242
Iteration 46, loss = 0.02270627
Iteration 47, loss = 0.02249836
Iteration 48, loss = 0.02233936
Iteration 49, loss = 0.02207285
Iteration 50, loss = 0.02175161
Iteration 51, loss = 0.02168369
Iteration 52, loss = 0.02145895
Iteration 53, loss = 0.02159409
Iteration 54, loss = 0.02148048
Iteration 55, loss = 0.02119271
Iteration 56, loss = 0.02098230
Iteration 57, loss = 0.02096947
Iteration 58, loss = 0.02068013
Iteration 59, loss = 0.02052632
Iteration 60, loss = 0.02053536
Iteration 61, loss = 0.02038596
Iteration 62, loss = 0.02057608
Iteration 63, loss = 0.02037631
Iteration 64, loss = 0.02000465
Iteration 65, loss = 0.01984862
Iteration 66, loss = 0.01979114
Iteration 67, loss = 0.01973298
Iteration 68, loss = 0.01957100
Iteration 69, loss = 0.01951515
Iteration 70, loss = 0.01959375
Iteration 71, loss = 0.01986628
Iteration 72, loss = 0.01961634
Iteration 73, loss = 0.01951748
Iteration 74, loss = 0.01959250
Iteration 75, loss = 0.01933997
Iteration 76, loss = 0.01923368
Iteration 77, loss = 0.01944676
Iteration 78, loss = 0.01915434
Iteration 79, loss = 0.01919353
Iteration 80, loss = 0.01916673
Iteration 81, loss = 0.01956546
Iteration 82, loss = 0.01923418
Iteration 83, loss = 0.01899826
Iteration 84, loss = 0.01878123
Iteration 85, loss = 0.01894825
Iteration 86, loss = 0.01897392
Iteration 87, loss = 0.01909872
Iteration 88, loss = 0.01869108
Iteration 89, loss = 0.01872343
Iteration 90, loss = 0.01870456
Iteration 91, loss = 0.01866314
Iteration 92, loss = 0.01851130
Iteration 93, loss = 0.01845235
Iteration 94, loss = 0.01868668
Iteration 95, loss = 0.01843835
Iteration 96, loss = 0.01858157
Iteration 97, loss = 0.01830183
Iteration 98, loss = 0.01843173
Iteration 99, loss = 0.01827113
Iteration 100, loss = 0.01905441
Iteration 101, loss = 0.01853934
Iteration 102, loss = 0.01864072
Iteration 103, loss = 0.01834290
Iteration 104, loss = 0.01826017
Iteration 105, loss = 0.01821514
Iteration 106, loss = 0.01841686
Iteration 107, loss = 0.01830388
Iteration 108, loss = 0.01824231
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70913052
Iteration 2, loss = 0.68095197
Iteration 3, loss = 0.64852056
Iteration 4, loss = 0.60704404
Iteration 5, loss = 0.55946187
Iteration 6, loss = 0.50805502
Iteration 7, loss = 0.45589253
Iteration 8, loss = 0.40407906
Iteration 9, loss = 0.35569294
Iteration 10, loss = 0.31182468
Iteration 11, loss = 0.27307326
Iteration 12, loss = 0.23921684
Iteration 13, loss = 0.21024421
Iteration 14, loss = 0.18547336
Iteration 15, loss = 0.16460080
Iteration 16, loss = 0.14675887
Iteration 17, loss = 0.13172328
Iteration 18, loss = 0.11867908
Iteration 19, loss = 0.10788540
Iteration 20, loss = 0.09843174
Iteration 21, loss = 0.09020576
Iteration 22, loss = 0.08318965
Iteration 23, loss = 0.07689501
Iteration 24, loss = 0.07146994
Iteration 25, loss = 0.06673039
Iteration 26, loss = 0.06242225
Iteration 27, loss = 0.05874211
Iteration 28, loss = 0.05530697
Iteration 29, loss = 0.05259017
Iteration 30, loss = 0.04978715
Iteration 31, loss = 0.04739793
Iteration 32, loss = 0.04532046
Iteration 33, loss = 0.04353558
Iteration 34, loss = 0.04170459
Iteration 35, loss = 0.04004862
Iteration 36, loss = 0.03845113
Iteration 37, loss = 0.03711095
Iteration 38, loss = 0.03578766
Iteration 39, loss = 0.03445064
Iteration 40, loss = 0.03334761
Iteration 41, loss = 0.03234489
Iteration 42, loss = 0.03140278
Iteration 43, loss = 0.03052101
Iteration 44, loss = 0.02970544
Iteration 45, loss = 0.02892991
Iteration 46, loss = 0.02812006
Iteration 47, loss = 0.02761437
Iteration 48, loss = 0.02706206
Iteration 49, loss = 0.02652384
Iteration 50, loss = 0.02598371
Iteration 51, loss = 0.02544149
Iteration 52, loss = 0.02512873
Iteration 53, loss = 0.02454411
Iteration 54, loss = 0.02436418
Iteration 55, loss = 0.02399869
Iteration 56, loss = 0.02360883
Iteration 57, loss = 0.02313640
Iteration 58, loss = 0.02266635
Iteration 59, loss = 0.02229997
Iteration 60, loss = 0.02199068
Iteration 61, loss = 0.02166558
Iteration 62, loss = 0.02135183
Iteration 63, loss = 0.02104468
Iteration 64, loss = 0.02101127
Iteration 65, loss = 0.02074616
Iteration 66, loss = 0.02060953
Iteration 67, loss = 0.02028627
Iteration 68, loss = 0.02010011
Iteration 69, loss = 0.01975551
Iteration 70, loss = 0.01961527
Iteration 71, loss = 0.01939180
Iteration 72, loss = 0.01919193
Iteration 73, loss = 0.01901924
Iteration 74, loss = 0.01883545
Iteration 75, loss = 0.01874141
Iteration 76, loss = 0.01863464
Iteration 77, loss = 0.01848417
Iteration 78, loss = 0.01862770
Iteration 79, loss = 0.01839110
Iteration 80, loss = 0.01836208
Iteration 81, loss = 0.01820397
Iteration 82, loss = 0.01804790
Iteration 83, loss = 0.01779660
Iteration 84, loss = 0.01770434
Iteration 85, loss = 0.01761267
Iteration 86, loss = 0.01762318
Iteration 87, loss = 0.01734593
Iteration 88, loss = 0.01723793
Iteration 89, loss = 0.01750423
Iteration 90, loss = 0.01736331
Iteration 91, loss = 0.01723164
Iteration 92, loss = 0.01690145
Iteration 93, loss = 0.01674544
Iteration 94, loss = 0.01668279
Iteration 95, loss = 0.01653357
Iteration 96, loss = 0.01657132
Iteration 97, loss = 0.01640851
Iteration 98, loss = 0.01634882
Iteration 99, loss = 0.01651253
Iteration 100, loss = 0.01637491
Iteration 101, loss = 0.01632204
Iteration 102, loss = 0.01629579
Iteration 103, loss = 0.01611848
Iteration 104, loss = 0.01630937
Iteration 105, loss = 0.01611626
Iteration 106, loss = 0.01602052
Iteration 107, loss = 0.01589836
Iteration 108, loss = 0.01582843
Iteration 109, loss = 0.01581624
Iteration 110, loss = 0.01578113
Iteration 111, loss = 0.01613865
Iteration 112, loss = 0.01625611
Iteration 113, loss = 0.01613148
Iteration 114, loss = 0.01594442
Iteration 115, loss = 0.01585311
Iteration 116, loss = 0.01564928
Iteration 117, loss = 0.01563088
Iteration 118, loss = 0.01544211
Iteration 119, loss = 0.01549413
Iteration 120, loss = 0.01543667
Iteration 121, loss = 0.01515179
Iteration 122, loss = 0.01565989
Iteration 123, loss = 0.01580425
Iteration 124, loss = 0.01596014
Iteration 125, loss = 0.01584177
Iteration 126, loss = 0.01556601
Iteration 127, loss = 0.01531319
Iteration 128, loss = 0.01517283
Iteration 129, loss = 0.01511046
Iteration 130, loss = 0.01504513
Iteration 131, loss = 0.01499300
Iteration 132, loss = 0.01493233
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68637584
Iteration 2, loss = 0.63591360
Iteration 3, loss = 0.56872037
Iteration 4, loss = 0.48758909
Iteration 5, loss = 0.40638177
Iteration 6, loss = 0.33329282
Iteration 7, loss = 0.27156667
Iteration 8, loss = 0.22138626
Iteration 9, loss = 0.18222357
Iteration 10, loss = 0.15170800
Iteration 11, loss = 0.12792085
Iteration 12, loss = 0.10960080
Iteration 13, loss = 0.09524249
Iteration 14, loss = 0.08380342
Iteration 15, loss = 0.07456472
Iteration 16, loss = 0.06697503
Iteration 17, loss = 0.06090152
Iteration 18, loss = 0.05597058
Iteration 19, loss = 0.05163063
Iteration 20, loss = 0.04811043
Iteration 21, loss = 0.04482626
Iteration 22, loss = 0.04203140
Iteration 23, loss = 0.03978310
Iteration 24, loss = 0.03773611
Iteration 25, loss = 0.03613130
Iteration 26, loss = 0.03439829
Iteration 27, loss = 0.03297962
Iteration 28, loss = 0.03178851
Iteration 29, loss = 0.03066409
Iteration 30, loss = 0.02950863
Iteration 31, loss = 0.02871970
Iteration 32, loss = 0.02776188
Iteration 33, loss = 0.02711843
Iteration 34, loss = 0.02641677
Iteration 35, loss = 0.02584954
Iteration 36, loss = 0.02548119
Iteration 37, loss = 0.02472781
Iteration 38, loss = 0.02425571
Iteration 39, loss = 0.02412768
Iteration 40, loss = 0.02373907
Iteration 41, loss = 0.02322736
Iteration 42, loss = 0.02275733
Iteration 43, loss = 0.02246712
Iteration 44, loss = 0.02221168
Iteration 45, loss = 0.02193520
Iteration 46, loss = 0.02153827
Iteration 47, loss = 0.02154191
Iteration 48, loss = 0.02100016
Iteration 49, loss = 0.02109862
Iteration 50, loss = 0.02065081
Iteration 51, loss = 0.02045496
Iteration 52, loss = 0.02024238
Iteration 53, loss = 0.02010493
Iteration 54, loss = 0.01991999
Iteration 55, loss = 0.01995013
Iteration 56, loss = 0.01968792
Iteration 57, loss = 0.01976023
Iteration 58, loss = 0.01967753
Iteration 59, loss = 0.01934504
Iteration 60, loss = 0.01927703
Iteration 61, loss = 0.01906074
Iteration 62, loss = 0.01904034
Iteration 63, loss = 0.01889820
Iteration 64, loss = 0.01889081
Iteration 65, loss = 0.01867964
Iteration 66, loss = 0.01864150
Iteration 67, loss = 0.01857279
Iteration 68, loss = 0.01857947
Iteration 69, loss = 0.01841858
Iteration 70, loss = 0.01842327
Iteration 71, loss = 0.01827372
Iteration 72, loss = 0.01821603
Iteration 73, loss = 0.01820359
Iteration 74, loss = 0.01817800
Iteration 75, loss = 0.01810490
Iteration 76, loss = 0.01822992
Iteration 77, loss = 0.01797909
Iteration 78, loss = 0.01803568
Iteration 79, loss = 0.01797816
Iteration 80, loss = 0.01771266
Iteration 81, loss = 0.01779158
Iteration 82, loss = 0.01784712
Iteration 83, loss = 0.01764918
Iteration 84, loss = 0.01765555
Iteration 85, loss = 0.01762187
Iteration 86, loss = 0.01761956
Iteration 87, loss = 0.01751972
Iteration 88, loss = 0.01750559
Iteration 89, loss = 0.01748708
Iteration 90, loss = 0.01745782
Iteration 91, loss = 0.01739302
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70293964
Iteration 2, loss = 0.66431903
Iteration 3, loss = 0.61610593
Iteration 4, loss = 0.56106477
Iteration 5, loss = 0.50345458
Iteration 6, loss = 0.44584864
Iteration 7, loss = 0.39051533
Iteration 8, loss = 0.33997641
Iteration 9, loss = 0.29482005
Iteration 10, loss = 0.25543592
Iteration 11, loss = 0.22149649
Iteration 12, loss = 0.19244242
Iteration 13, loss = 0.16797602
Iteration 14, loss = 0.14718601
Iteration 15, loss = 0.12972632
Iteration 16, loss = 0.11514776
Iteration 17, loss = 0.10289373
Iteration 18, loss = 0.09240949
Iteration 19, loss = 0.08366723
Iteration 20, loss = 0.07628611
Iteration 21, loss = 0.06988560
Iteration 22, loss = 0.06430895
Iteration 23, loss = 0.05953010
Iteration 24, loss = 0.05550719
Iteration 25, loss = 0.05190072
Iteration 26, loss = 0.04856864
Iteration 27, loss = 0.04575476
Iteration 28, loss = 0.04332742
Iteration 29, loss = 0.04103885
Iteration 30, loss = 0.03914828
Iteration 31, loss = 0.03733697
Iteration 32, loss = 0.03566348
Iteration 33, loss = 0.03417298
Iteration 34, loss = 0.03298358
Iteration 35, loss = 0.03179808
Iteration 36, loss = 0.03075438
Iteration 37, loss = 0.02963827
Iteration 38, loss = 0.02863615
Iteration 39, loss = 0.02780965
Iteration 40, loss = 0.02719944
Iteration 41, loss = 0.02649840
Iteration 42, loss = 0.02563883
Iteration 43, loss = 0.02514273
Iteration 44, loss = 0.02456171
Iteration 45, loss = 0.02400034
Iteration 46, loss = 0.02353463
Iteration 47, loss = 0.02304205
Iteration 48, loss = 0.02263619
Iteration 49, loss = 0.02217431
Iteration 50, loss = 0.02196320
Iteration 51, loss = 0.02155875
Iteration 52, loss = 0.02109528
Iteration 53, loss = 0.02093836
Iteration 54, loss = 0.02055216
Iteration 55, loss = 0.02036179
Iteration 56, loss = 0.02001721
Iteration 57, loss = 0.01977916
Iteration 58, loss = 0.01969470
Iteration 59, loss = 0.01937124
Iteration 60, loss = 0.01926683
Iteration 61, loss = 0.01895769
Iteration 62, loss = 0.01883276
Iteration 63, loss = 0.01859336
Iteration 64, loss = 0.01848591
Iteration 65, loss = 0.01821381
Iteration 66, loss = 0.01816143
Iteration 67, loss = 0.01788803
Iteration 68, loss = 0.01782825
Iteration 69, loss = 0.01773328
Iteration 70, loss = 0.01755962
Iteration 71, loss = 0.01739415
Iteration 72, loss = 0.01730060
Iteration 73, loss = 0.01728714
Iteration 74, loss = 0.01706576
Iteration 75, loss = 0.01711815
Iteration 76, loss = 0.01693346
Iteration 77, loss = 0.01680265
Iteration 78, loss = 0.01675417
Iteration 79, loss = 0.01663393
Iteration 80, loss = 0.01666280
Iteration 81, loss = 0.01659932
Iteration 82, loss = 0.01642893
Iteration 83, loss = 0.01630938
Iteration 84, loss = 0.01634480
Iteration 85, loss = 0.01622626
Iteration 86, loss = 0.01609520
Iteration 87, loss = 0.01608660
Iteration 88, loss = 0.01600328
Iteration 89, loss = 0.01606242
Iteration 90, loss = 0.01598768
Iteration 91, loss = 0.01584486
Iteration 92, loss = 0.01584990
Iteration 93, loss = 0.01589165
Iteration 94, loss = 0.01599517
Iteration 95, loss = 0.01578220
Iteration 96, loss = 0.01557640
Iteration 97, loss = 0.01559811
Iteration 98, loss = 0.01544224
Iteration 99, loss = 0.01556854
Iteration 100, loss = 0.01555399
Iteration 101, loss = 0.01553722
Iteration 102, loss = 0.01542877
Iteration 103, loss = 0.01534798
Iteration 104, loss = 0.01536284
Iteration 105, loss = 0.01517584
Iteration 106, loss = 0.01527630
Iteration 107, loss = 0.01516453
Iteration 108, loss = 0.01513128
Iteration 109, loss = 0.01531245
Iteration 110, loss = 0.01507787
Iteration 111, loss = 0.01512131
Iteration 112, loss = 0.01517065
Iteration 113, loss = 0.01516931
Iteration 114, loss = 0.01495315
Iteration 115, loss = 0.01509659
Iteration 116, loss = 0.01493495
Iteration 117, loss = 0.01497240
Iteration 118, loss = 0.01489636
Iteration 119, loss = 0.01490894
Iteration 120, loss = 0.01491660
Iteration 121, loss = 0.01493965
Iteration 122, loss = 0.01477928
Iteration 123, loss = 0.01490219
Iteration 124, loss = 0.01481669
Iteration 125, loss = 0.01485127
Iteration 126, loss = 0.01472007
Iteration 127, loss = 0.01484027
Iteration 128, loss = 0.01469369
Iteration 129, loss = 0.01465174
Iteration 130, loss = 0.01493761
Iteration 131, loss = 0.01474148
Iteration 132, loss = 0.01472230
Iteration 133, loss = 0.01472198
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70055227
Iteration 2, loss = 0.67138164
Iteration 3, loss = 0.63397292
Iteration 4, loss = 0.58680542
Iteration 5, loss = 0.53040469
Iteration 6, loss = 0.47066535
Iteration 7, loss = 0.41154922
Iteration 8, loss = 0.35587625
Iteration 9, loss = 0.30535421
Iteration 10, loss = 0.26105782
Iteration 11, loss = 0.22307218
Iteration 12, loss = 0.19114368
Iteration 13, loss = 0.16484416
Iteration 14, loss = 0.14301277
Iteration 15, loss = 0.12478064
Iteration 16, loss = 0.11000860
Iteration 17, loss = 0.09754967
Iteration 18, loss = 0.08733117
Iteration 19, loss = 0.07856945
Iteration 20, loss = 0.07130010
Iteration 21, loss = 0.06504048
Iteration 22, loss = 0.05968755
Iteration 23, loss = 0.05506808
Iteration 24, loss = 0.05106383
Iteration 25, loss = 0.04761799
Iteration 26, loss = 0.04449262
Iteration 27, loss = 0.04179216
Iteration 28, loss = 0.03943295
Iteration 29, loss = 0.03721331
Iteration 30, loss = 0.03532158
Iteration 31, loss = 0.03365276
Iteration 32, loss = 0.03204766
Iteration 33, loss = 0.03063488
Iteration 34, loss = 0.02939192
Iteration 35, loss = 0.02819946
Iteration 36, loss = 0.02720136
Iteration 37, loss = 0.02620013
Iteration 38, loss = 0.02529627
Iteration 39, loss = 0.02445210
Iteration 40, loss = 0.02375632
Iteration 41, loss = 0.02303385
Iteration 42, loss = 0.02237550
Iteration 43, loss = 0.02184657
Iteration 44, loss = 0.02123966
Iteration 45, loss = 0.02072108
Iteration 46, loss = 0.02025195
Iteration 47, loss = 0.01979739
Iteration 48, loss = 0.01941146
Iteration 49, loss = 0.01906081
Iteration 50, loss = 0.01861698
Iteration 51, loss = 0.01831438
Iteration 52, loss = 0.01797837
Iteration 53, loss = 0.01770242
Iteration 54, loss = 0.01741570
Iteration 55, loss = 0.01714041
Iteration 56, loss = 0.01695445
Iteration 57, loss = 0.01662026
Iteration 58, loss = 0.01648020
Iteration 59, loss = 0.01628585
Iteration 60, loss = 0.01613085
Iteration 61, loss = 0.01577850
Iteration 62, loss = 0.01559798
Iteration 63, loss = 0.01542776
Iteration 64, loss = 0.01532922
Iteration 65, loss = 0.01514016
Iteration 66, loss = 0.01500747
Iteration 67, loss = 0.01479964
Iteration 68, loss = 0.01464983
Iteration 69, loss = 0.01456565
Iteration 70, loss = 0.01443127
Iteration 71, loss = 0.01439182
Iteration 72, loss = 0.01419416
Iteration 73, loss = 0.01414130
Iteration 74, loss = 0.01399624
Iteration 75, loss = 0.01399419
Iteration 76, loss = 0.01384431
Iteration 77, loss = 0.01377340
Iteration 78, loss = 0.01359830
Iteration 79, loss = 0.01353487
Iteration 80, loss = 0.01356754
Iteration 81, loss = 0.01341779
Iteration 82, loss = 0.01335582
Iteration 83, loss = 0.01323547
Iteration 84, loss = 0.01322911
Iteration 85, loss = 0.01318366
Iteration 86, loss = 0.01304043
Iteration 87, loss = 0.01302527
Iteration 88, loss = 0.01295855
Iteration 89, loss = 0.01304208
Iteration 90, loss = 0.01282580
Iteration 91, loss = 0.01283221
Iteration 92, loss = 0.01268419
Iteration 93, loss = 0.01276521
Iteration 94, loss = 0.01268687
Iteration 95, loss = 0.01267448
Iteration 96, loss = 0.01262296
Iteration 97, loss = 0.01250563
Iteration 98, loss = 0.01247369
Iteration 99, loss = 0.01231947
Iteration 100, loss = 0.01249711
Iteration 101, loss = 0.01232327
Iteration 102, loss = 0.01241436
Iteration 103, loss = 0.01221333
Iteration 104, loss = 0.01225352
Iteration 105, loss = 0.01220744
Iteration 106, loss = 0.01207493
Iteration 107, loss = 0.01213460
Iteration 108, loss = 0.01207806
Iteration 109, loss = 0.01208990
Iteration 110, loss = 0.01208014
Iteration 111, loss = 0.01205466
Iteration 112, loss = 0.01199255
Iteration 113, loss = 0.01196644
Iteration 114, loss = 0.01188169
Iteration 115, loss = 0.01192160
Iteration 116, loss = 0.01187566
Iteration 117, loss = 0.01187187
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67998484
Iteration 2, loss = 0.63701823
Iteration 3, loss = 0.56990974
Iteration 4, loss = 0.48414875
Iteration 5, loss = 0.39809231
Iteration 6, loss = 0.31946091
Iteration 7, loss = 0.25388427
Iteration 8, loss = 0.20276246
Iteration 9, loss = 0.16421860
Iteration 10, loss = 0.13563031
Iteration 11, loss = 0.11421904
Iteration 12, loss = 0.09790179
Iteration 13, loss = 0.08564945
Iteration 14, loss = 0.07581327
Iteration 15, loss = 0.06796739
Iteration 16, loss = 0.06168351
Iteration 17, loss = 0.05675272
Iteration 18, loss = 0.05254701
Iteration 19, loss = 0.04919238
Iteration 20, loss = 0.04604350
Iteration 21, loss = 0.04356706
Iteration 22, loss = 0.04143814
Iteration 23, loss = 0.03936708
Iteration 24, loss = 0.03805930
Iteration 25, loss = 0.03634416
Iteration 26, loss = 0.03527261
Iteration 27, loss = 0.03414518
Iteration 28, loss = 0.03308684
Iteration 29, loss = 0.03211354
Iteration 30, loss = 0.03131537
Iteration 31, loss = 0.03081921
Iteration 32, loss = 0.03023982
Iteration 33, loss = 0.02960514
Iteration 34, loss = 0.02906565
Iteration 35, loss = 0.02849411
Iteration 36, loss = 0.02812946
Iteration 37, loss = 0.02777507
Iteration 38, loss = 0.02721463
Iteration 39, loss = 0.02702366
Iteration 40, loss = 0.02659388
Iteration 41, loss = 0.02633965
Iteration 42, loss = 0.02612875
Iteration 43, loss = 0.02583891
Iteration 44, loss = 0.02554666
Iteration 45, loss = 0.02533056
Iteration 46, loss = 0.02514313
Iteration 47, loss = 0.02480069
Iteration 48, loss = 0.02491021
Iteration 49, loss = 0.02464423
Iteration 50, loss = 0.02419229
Iteration 51, loss = 0.02451545
Iteration 52, loss = 0.02420756
Iteration 53, loss = 0.02396847
Iteration 54, loss = 0.02398077
Iteration 55, loss = 0.02393867
Iteration 56, loss = 0.02363616
Iteration 57, loss = 0.02355014
Iteration 58, loss = 0.02342109
Iteration 59, loss = 0.02337815
Iteration 60, loss = 0.02350591
Iteration 61, loss = 0.02312053
Iteration 62, loss = 0.02324327
Iteration 63, loss = 0.02306215
Iteration 64, loss = 0.02307497
Iteration 65, loss = 0.02300116
Iteration 66, loss = 0.02282603
Iteration 67, loss = 0.02295277
Iteration 68, loss = 0.02271892
Iteration 69, loss = 0.02288504
Iteration 70, loss = 0.02265192
Iteration 71, loss = 0.02273768
Iteration 72, loss = 0.02261853
Iteration 73, loss = 0.02264054
Iteration 74, loss = 0.02245808
Iteration 75, loss = 0.02253447
Iteration 76, loss = 0.02243467
Iteration 77, loss = 0.02279479
Iteration 78, loss = 0.02233399
Iteration 79, loss = 0.02234014
Iteration 80, loss = 0.02226431
Iteration 81, loss = 0.02219580
Iteration 82, loss = 0.02220305
Iteration 83, loss = 0.02209083
Iteration 84, loss = 0.02219556
Iteration 85, loss = 0.02225017
Iteration 86, loss = 0.02216862
Iteration 87, loss = 0.02203188
Iteration 88, loss = 0.02196499
Iteration 89, loss = 0.02201295
Iteration 90, loss = 0.02203387
Iteration 91, loss = 0.02200468
Iteration 92, loss = 0.02213221
Iteration 93, loss = 0.02201607
Iteration 94, loss = 0.02213700
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67851247
Iteration 2, loss = 0.62209799
Iteration 3, loss = 0.53218417
Iteration 4, loss = 0.43599080
Iteration 5, loss = 0.34591908
Iteration 6, loss = 0.27185654
Iteration 7, loss = 0.21499326
Iteration 8, loss = 0.17282031
Iteration 9, loss = 0.14165765
Iteration 10, loss = 0.11848472
Iteration 11, loss = 0.10136926
Iteration 12, loss = 0.08831770
Iteration 13, loss = 0.07821548
Iteration 14, loss = 0.07019525
Iteration 15, loss = 0.06389252
Iteration 16, loss = 0.05850032
Iteration 17, loss = 0.05443146
Iteration 18, loss = 0.05076019
Iteration 19, loss = 0.04739629
Iteration 20, loss = 0.04499619
Iteration 21, loss = 0.04264371
Iteration 22, loss = 0.04073604
Iteration 23, loss = 0.03915741
Iteration 24, loss = 0.03773731
Iteration 25, loss = 0.03643306
Iteration 26, loss = 0.03517450
Iteration 27, loss = 0.03419041
Iteration 28, loss = 0.03368486
Iteration 29, loss = 0.03276035
Iteration 30, loss = 0.03191136
Iteration 31, loss = 0.03137941
Iteration 32, loss = 0.03050705
Iteration 33, loss = 0.03033117
Iteration 34, loss = 0.02998881
Iteration 35, loss = 0.02912940
Iteration 36, loss = 0.02872312
Iteration 37, loss = 0.02846269
Iteration 38, loss = 0.02811697
Iteration 39, loss = 0.02771361
Iteration 40, loss = 0.02735306
Iteration 41, loss = 0.02735762
Iteration 42, loss = 0.02720697
Iteration 43, loss = 0.02855583
Iteration 44, loss = 0.02837579
Iteration 45, loss = 0.02771528
Iteration 46, loss = 0.02712647
Iteration 47, loss = 0.02678541
Iteration 48, loss = 0.02653705
Iteration 49, loss = 0.02639772
Iteration 50, loss = 0.02627935
Iteration 51, loss = 0.02634319
Iteration 52, loss = 0.02655393
Iteration 53, loss = 0.02622619
Iteration 54, loss = 0.02597490
Iteration 55, loss = 0.02581032
Iteration 56, loss = 0.02581630
Iteration 57, loss = 0.02557853
Iteration 58, loss = 0.02541249
Iteration 59, loss = 0.02548350
Iteration 60, loss = 0.02521074
Iteration 61, loss = 0.02508874
Iteration 62, loss = 0.02511040
Iteration 63, loss = 0.02488958
Iteration 64, loss = 0.02470318
Iteration 65, loss = 0.02506209
Iteration 66, loss = 0.02573251
Iteration 67, loss = 0.02535313
Iteration 68, loss = 0.02539726
Iteration 69, loss = 0.02530161
Iteration 70, loss = 0.02556972
Iteration 71, loss = 0.02625438
Iteration 72, loss = 0.02603623
Iteration 73, loss = 0.02532018
Iteration 74, loss = 0.02512924
Iteration 75, loss = 0.02500547
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67673355
Iteration 2, loss = 0.62297071
Iteration 3, loss = 0.54375268
Iteration 4, loss = 0.45355902
Iteration 5, loss = 0.36896957
Iteration 6, loss = 0.29549554
Iteration 7, loss = 0.23601329
Iteration 8, loss = 0.19071075
Iteration 9, loss = 0.15601084
Iteration 10, loss = 0.13017723
Iteration 11, loss = 0.11035413
Iteration 12, loss = 0.09535931
Iteration 13, loss = 0.08374742
Iteration 14, loss = 0.07463086
Iteration 15, loss = 0.06716177
Iteration 16, loss = 0.06111424
Iteration 17, loss = 0.05621356
Iteration 18, loss = 0.05204646
Iteration 19, loss = 0.04878793
Iteration 20, loss = 0.04569494
Iteration 21, loss = 0.04318569
Iteration 22, loss = 0.04108345
Iteration 23, loss = 0.03916635
Iteration 24, loss = 0.03773643
Iteration 25, loss = 0.03634208
Iteration 26, loss = 0.03489105
Iteration 27, loss = 0.03388764
Iteration 28, loss = 0.03289878
Iteration 29, loss = 0.03199190
Iteration 30, loss = 0.03143823
Iteration 31, loss = 0.03064428
Iteration 32, loss = 0.02971040
Iteration 33, loss = 0.02929139
Iteration 34, loss = 0.02886497
Iteration 35, loss = 0.02833518
Iteration 36, loss = 0.02770823
Iteration 37, loss = 0.02728415
Iteration 38, loss = 0.02692610
Iteration 39, loss = 0.02674836
Iteration 40, loss = 0.02646304
Iteration 41, loss = 0.02596652
Iteration 42, loss = 0.02587671
Iteration 43, loss = 0.02559946
Iteration 44, loss = 0.02543942
Iteration 45, loss = 0.02503073
Iteration 46, loss = 0.02486647
Iteration 47, loss = 0.02456308
Iteration 48, loss = 0.02440120
Iteration 49, loss = 0.02448785
Iteration 50, loss = 0.02407270
Iteration 51, loss = 0.02385767
Iteration 52, loss = 0.02389181
Iteration 53, loss = 0.02394981
Iteration 54, loss = 0.02352754
Iteration 55, loss = 0.02334286
Iteration 56, loss = 0.02328923
Iteration 57, loss = 0.02334774
Iteration 58, loss = 0.02318967
Iteration 59, loss = 0.02290854
Iteration 60, loss = 0.02293617
Iteration 61, loss = 0.02296494
Iteration 62, loss = 0.02290695
Iteration 63, loss = 0.02283051
Iteration 64, loss = 0.02265047
Iteration 65, loss = 0.02261003
Iteration 66, loss = 0.02268958
Iteration 67, loss = 0.02253461
Iteration 68, loss = 0.02259684
Iteration 69, loss = 0.02248266
Iteration 70, loss = 0.02219563
Iteration 71, loss = 0.02233440
Iteration 72, loss = 0.02209764
Iteration 73, loss = 0.02202557
Iteration 74, loss = 0.02217596
Iteration 75, loss = 0.02211646
Iteration 76, loss = 0.02200548
Iteration 77, loss = 0.02205433
Iteration 78, loss = 0.02199985
Iteration 79, loss = 0.02183960
Iteration 80, loss = 0.02182217
Iteration 81, loss = 0.02203719
Iteration 82, loss = 0.02216207
Iteration 83, loss = 0.02185934
Iteration 84, loss = 0.02196287
Iteration 85, loss = 0.02176331
Iteration 86, loss = 0.02175140
Iteration 87, loss = 0.02182985
Iteration 88, loss = 0.02160660
Iteration 89, loss = 0.02152301
Iteration 90, loss = 0.02149160
Iteration 91, loss = 0.02168576
Iteration 92, loss = 0.02133815
Iteration 93, loss = 0.02147785
Iteration 94, loss = 0.02156883
Iteration 95, loss = 0.02133632
Iteration 96, loss = 0.02159085
Iteration 97, loss = 0.02155173
Iteration 98, loss = 0.02144406
Iteration 99, loss = 0.02148269
Iteration 100, loss = 0.02135849
Iteration 101, loss = 0.02176663
Iteration 102, loss = 0.02125323
Iteration 103, loss = 0.02128871
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69235440
Iteration 2, loss = 0.65096337
Iteration 3, loss = 0.58524244
Iteration 4, loss = 0.49839863
Iteration 5, loss = 0.41271441
Iteration 6, loss = 0.33520319
Iteration 7, loss = 0.27081860
Iteration 8, loss = 0.21913824
Iteration 9, loss = 0.17932496
Iteration 10, loss = 0.14849228
Iteration 11, loss = 0.12505142
Iteration 12, loss = 0.10710662
Iteration 13, loss = 0.09298829
Iteration 14, loss = 0.08202708
Iteration 15, loss = 0.07320502
Iteration 16, loss = 0.06598731
Iteration 17, loss = 0.06005083
Iteration 18, loss = 0.05532686
Iteration 19, loss = 0.05117274
Iteration 20, loss = 0.04777800
Iteration 21, loss = 0.04473193
Iteration 22, loss = 0.04248361
Iteration 23, loss = 0.04003304
Iteration 24, loss = 0.03825694
Iteration 25, loss = 0.03661675
Iteration 26, loss = 0.03530218
Iteration 27, loss = 0.03371794
Iteration 28, loss = 0.03253838
Iteration 29, loss = 0.03176614
Iteration 30, loss = 0.03091716
Iteration 31, loss = 0.02988329
Iteration 32, loss = 0.02925032
Iteration 33, loss = 0.02839990
Iteration 34, loss = 0.02789512
Iteration 35, loss = 0.02720551
Iteration 36, loss = 0.02673616
Iteration 37, loss = 0.02624893
Iteration 38, loss = 0.02575023
Iteration 39, loss = 0.02551781
Iteration 40, loss = 0.02496149
Iteration 41, loss = 0.02464129
Iteration 42, loss = 0.02446069
Iteration 43, loss = 0.02410148
Iteration 44, loss = 0.02407646
Iteration 45, loss = 0.02348686
Iteration 46, loss = 0.02330113
Iteration 47, loss = 0.02310547
Iteration 48, loss = 0.02305429
Iteration 49, loss = 0.02292947
Iteration 50, loss = 0.02245866
Iteration 51, loss = 0.02245393
Iteration 52, loss = 0.02225750
Iteration 53, loss = 0.02216086
Iteration 54, loss = 0.02200336
Iteration 55, loss = 0.02188155
Iteration 56, loss = 0.02166722
Iteration 57, loss = 0.02140480
Iteration 58, loss = 0.02144655
Iteration 59, loss = 0.02146205
Iteration 60, loss = 0.02128436
Iteration 61, loss = 0.02136801
Iteration 62, loss = 0.02106649
Iteration 63, loss = 0.02101987
Iteration 64, loss = 0.02107140
Iteration 65, loss = 0.02076277
Iteration 66, loss = 0.02065175
Iteration 67, loss = 0.02077103
Iteration 68, loss = 0.02039133
Iteration 69, loss = 0.02067950
Iteration 70, loss = 0.02045302
Iteration 71, loss = 0.02039940
Iteration 72, loss = 0.02057774
Iteration 73, loss = 0.02018631
Iteration 74, loss = 0.02036850
Iteration 75, loss = 0.02017415
Iteration 76, loss = 0.02027449
Iteration 77, loss = 0.02004364
Iteration 78, loss = 0.02012812
Iteration 79, loss = 0.01997025
Iteration 80, loss = 0.01998576
Iteration 81, loss = 0.01992017
Iteration 82, loss = 0.02003232
Iteration 83, loss = 0.01976646
Iteration 84, loss = 0.01994741
Iteration 85, loss = 0.02002550
Iteration 86, loss = 0.01971314
Iteration 87, loss = 0.01968277
Iteration 88, loss = 0.01969123
Iteration 89, loss = 0.01949299
Iteration 90, loss = 0.01961040
Iteration 91, loss = 0.01938669
Iteration 92, loss = 0.01963679
Iteration 93, loss = 0.01978279
Iteration 94, loss = 0.01971824
Iteration 95, loss = 0.01964757
Iteration 96, loss = 0.01937005
Iteration 97, loss = 0.01947156
Iteration 98, loss = 0.01948152
Iteration 99, loss = 0.01939712
Iteration 100, loss = 0.01960987
Iteration 101, loss = 0.01942434
Iteration 102, loss = 0.01947259
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67742678
Iteration 2, loss = 0.66109627
Iteration 3, loss = 0.64795693
Iteration 4, loss = 0.63512883
Iteration 5, loss = 0.62174574
Iteration 6, loss = 0.60722699
Iteration 7, loss = 0.59129461
Iteration 8, loss = 0.57374857
Iteration 9, loss = 0.55472151
Iteration 10, loss = 0.53403700
Iteration 11, loss = 0.51246584
Iteration 12, loss = 0.48964797
Iteration 13, loss = 0.46616065
Iteration 14, loss = 0.44237011
Iteration 15, loss = 0.41842381
Iteration 16, loss = 0.39459303
Iteration 17, loss = 0.37152068
Iteration 18, loss = 0.34911618
Iteration 19, loss = 0.32735763
Iteration 20, loss = 0.30684109
Iteration 21, loss = 0.28740995
Iteration 22, loss = 0.26895197
Iteration 23, loss = 0.25167681
Iteration 24, loss = 0.23546801
Iteration 25, loss = 0.22037399
Iteration 26, loss = 0.20634166
Iteration 27, loss = 0.19323627
Iteration 28, loss = 0.18104500
Iteration 29, loss = 0.16975638
Iteration 30, loss = 0.15929482
Iteration 31, loss = 0.14966176
Iteration 32, loss = 0.14068542
Iteration 33, loss = 0.13242619
Iteration 34, loss = 0.12479453
Iteration 35, loss = 0.11772363
Iteration 36, loss = 0.11119071
Iteration 37, loss = 0.10514887
Iteration 38, loss = 0.09954622
Iteration 39, loss = 0.09436462
Iteration 40, loss = 0.08959316
Iteration 41, loss = 0.08510660
Iteration 42, loss = 0.08101546
Iteration 43, loss = 0.07710742
Iteration 44, loss = 0.07358976
Iteration 45, loss = 0.07020119
Iteration 46, loss = 0.06705806
Iteration 47, loss = 0.06417025
Iteration 48, loss = 0.06146841
Iteration 49, loss = 0.05895438
Iteration 50, loss = 0.05656845
Iteration 51, loss = 0.05431483
Iteration 52, loss = 0.05220905
Iteration 53, loss = 0.05023178
Iteration 54, loss = 0.04839660
Iteration 55, loss = 0.04664818
Iteration 56, loss = 0.04499164
Iteration 57, loss = 0.04344608
Iteration 58, loss = 0.04195606
Iteration 59, loss = 0.04058736
Iteration 60, loss = 0.03926171
Iteration 61, loss = 0.03801431
Iteration 62, loss = 0.03682211
Iteration 63, loss = 0.03570384
Iteration 64, loss = 0.03463977
Iteration 65, loss = 0.03361146
Iteration 66, loss = 0.03266757
Iteration 67, loss = 0.03172456
Iteration 68, loss = 0.03084203
Iteration 69, loss = 0.03004712
Iteration 70, loss = 0.02920369
Iteration 71, loss = 0.02844716
Iteration 72, loss = 0.02771666
Iteration 73, loss = 0.02701284
Iteration 74, loss = 0.02634336
Iteration 75, loss = 0.02571399
Iteration 76, loss = 0.02509855
Iteration 77, loss = 0.02451635
Iteration 78, loss = 0.02394989
Iteration 79, loss = 0.02344544
Iteration 80, loss = 0.02290122
Iteration 81, loss = 0.02240479
Iteration 82, loss = 0.02191761
Iteration 83, loss = 0.02146068
Iteration 84, loss = 0.02102694
Iteration 85, loss = 0.02060081
Iteration 86, loss = 0.02018228
Iteration 87, loss = 0.01979614
Iteration 88, loss = 0.01941965
Iteration 89, loss = 0.01902889
Iteration 90, loss = 0.01868272
Iteration 91, loss = 0.01833934
Iteration 92, loss = 0.01801194
Iteration 93, loss = 0.01769590
Iteration 94, loss = 0.01737616
Iteration 95, loss = 0.01708980
Iteration 96, loss = 0.01678443
Iteration 97, loss = 0.01651464
Iteration 98, loss = 0.01626801
Iteration 99, loss = 0.01597810
Iteration 100, loss = 0.01571305
Iteration 101, loss = 0.01546822
Iteration 102, loss = 0.01522127
Iteration 103, loss = 0.01500656
Iteration 104, loss = 0.01477374
Iteration 105, loss = 0.01454547
Iteration 106, loss = 0.01434613
Iteration 107, loss = 0.01413704
Iteration 108, loss = 0.01392888
Iteration 109, loss = 0.01374534
Iteration 110, loss = 0.01354326
Iteration 111, loss = 0.01335508
Iteration 112, loss = 0.01319047
Iteration 113, loss = 0.01299928
Iteration 114, loss = 0.01283130
Iteration 115, loss = 0.01266411
Iteration 116, loss = 0.01251855
Iteration 117, loss = 0.01235123
Iteration 118, loss = 0.01219688
Iteration 119, loss = 0.01207001
Iteration 120, loss = 0.01191256
Iteration 121, loss = 0.01178028
Iteration 122, loss = 0.01164417
Iteration 123, loss = 0.01150260
Iteration 124, loss = 0.01137445
Iteration 125, loss = 0.01124575
Iteration 126, loss = 0.01113466
Iteration 127, loss = 0.01100480
Iteration 128, loss = 0.01088161
Iteration 129, loss = 0.01076261
Iteration 130, loss = 0.01065044
Iteration 131, loss = 0.01053876
Iteration 132, loss = 0.01043969
Iteration 133, loss = 0.01034002
Iteration 134, loss = 0.01024762
Iteration 135, loss = 0.01013108
Iteration 136, loss = 0.01003548
Iteration 137, loss = 0.00994132
Iteration 138, loss = 0.00986100
Iteration 139, loss = 0.00977109
Iteration 140, loss = 0.00968226
Iteration 141, loss = 0.00959394
Iteration 142, loss = 0.00952370
Iteration 143, loss = 0.00942305
Iteration 144, loss = 0.00936772
Iteration 145, loss = 0.00927981
Iteration 146, loss = 0.00917700
Iteration 147, loss = 0.00910121
Iteration 148, loss = 0.00904147
Iteration 149, loss = 0.00895610
Iteration 150, loss = 0.00889171
Iteration 151, loss = 0.00882379
Iteration 152, loss = 0.00877290
Iteration 153, loss = 0.00868380
Iteration 154, loss = 0.00864434
Iteration 155, loss = 0.00855232
Iteration 156, loss = 0.00850778
Iteration 157, loss = 0.00843326
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69484576
Iteration 2, loss = 0.68105278
Iteration 3, loss = 0.66791140
Iteration 4, loss = 0.65264890
Iteration 5, loss = 0.63441438
Iteration 6, loss = 0.61271149
Iteration 7, loss = 0.58718491
Iteration 8, loss = 0.55858728
Iteration 9, loss = 0.52761000
Iteration 10, loss = 0.49501579
Iteration 11, loss = 0.46189357
Iteration 12, loss = 0.42880018
Iteration 13, loss = 0.39664815
Iteration 14, loss = 0.36575906
Iteration 15, loss = 0.33619497
Iteration 16, loss = 0.30855009
Iteration 17, loss = 0.28279920
Iteration 18, loss = 0.25904012
Iteration 19, loss = 0.23728611
Iteration 20, loss = 0.21752811
Iteration 21, loss = 0.19966799
Iteration 22, loss = 0.18346067
Iteration 23, loss = 0.16894272
Iteration 24, loss = 0.15590055
Iteration 25, loss = 0.14415365
Iteration 26, loss = 0.13360239
Iteration 27, loss = 0.12417648
Iteration 28, loss = 0.11566943
Iteration 29, loss = 0.10809463
Iteration 30, loss = 0.10119033
Iteration 31, loss = 0.09492789
Iteration 32, loss = 0.08933399
Iteration 33, loss = 0.08426840
Iteration 34, loss = 0.07961531
Iteration 35, loss = 0.07539373
Iteration 36, loss = 0.07155399
Iteration 37, loss = 0.06804844
Iteration 38, loss = 0.06477299
Iteration 39, loss = 0.06182172
Iteration 40, loss = 0.05908790
Iteration 41, loss = 0.05658180
Iteration 42, loss = 0.05428146
Iteration 43, loss = 0.05207937
Iteration 44, loss = 0.05020216
Iteration 45, loss = 0.04823714
Iteration 46, loss = 0.04660769
Iteration 47, loss = 0.04494953
Iteration 48, loss = 0.04349290
Iteration 49, loss = 0.04211191
Iteration 50, loss = 0.04079949
Iteration 51, loss = 0.03957282
Iteration 52, loss = 0.03839382
Iteration 53, loss = 0.03735981
Iteration 54, loss = 0.03634909
Iteration 55, loss = 0.03540987
Iteration 56, loss = 0.03450655
Iteration 57, loss = 0.03365695
Iteration 58, loss = 0.03281849
Iteration 59, loss = 0.03208676
Iteration 60, loss = 0.03133966
Iteration 61, loss = 0.03067172
Iteration 62, loss = 0.03006892
Iteration 63, loss = 0.02942775
Iteration 64, loss = 0.02886879
Iteration 65, loss = 0.02830506
Iteration 66, loss = 0.02777974
Iteration 67, loss = 0.02725207
Iteration 68, loss = 0.02682151
Iteration 69, loss = 0.02632991
Iteration 70, loss = 0.02594067
Iteration 71, loss = 0.02550762
Iteration 72, loss = 0.02508214
Iteration 73, loss = 0.02471717
Iteration 74, loss = 0.02434705
Iteration 75, loss = 0.02400947
Iteration 76, loss = 0.02365613
Iteration 77, loss = 0.02336538
Iteration 78, loss = 0.02306945
Iteration 79, loss = 0.02274944
Iteration 80, loss = 0.02252563
Iteration 81, loss = 0.02217332
Iteration 82, loss = 0.02193126
Iteration 83, loss = 0.02169302
Iteration 84, loss = 0.02148468
Iteration 85, loss = 0.02122594
Iteration 86, loss = 0.02098700
Iteration 87, loss = 0.02078777
Iteration 88, loss = 0.02056837
Iteration 89, loss = 0.02037897
Iteration 90, loss = 0.02019333
Iteration 91, loss = 0.02002904
Iteration 92, loss = 0.01979680
Iteration 93, loss = 0.01963493
Iteration 94, loss = 0.01948998
Iteration 95, loss = 0.01932347
Iteration 96, loss = 0.01918985
Iteration 97, loss = 0.01902246
Iteration 98, loss = 0.01883673
Iteration 99, loss = 0.01869239
Iteration 100, loss = 0.01858032
Iteration 101, loss = 0.01846039
Iteration 102, loss = 0.01833131
Iteration 103, loss = 0.01820621
Iteration 104, loss = 0.01806031
Iteration 105, loss = 0.01794598
Iteration 106, loss = 0.01786646
Iteration 107, loss = 0.01770873
Iteration 108, loss = 0.01761959
Iteration 109, loss = 0.01752735
Iteration 110, loss = 0.01742544
Iteration 111, loss = 0.01733205
Iteration 112, loss = 0.01723244
Iteration 113, loss = 0.01711210
Iteration 114, loss = 0.01701530
Iteration 115, loss = 0.01700543
Iteration 116, loss = 0.01683582
Iteration 117, loss = 0.01679495
Iteration 118, loss = 0.01671702
Iteration 119, loss = 0.01663100
Iteration 120, loss = 0.01658938
Iteration 121, loss = 0.01651893
Iteration 122, loss = 0.01636210
Iteration 123, loss = 0.01635608
Iteration 124, loss = 0.01624015
Iteration 125, loss = 0.01615596
Iteration 126, loss = 0.01614461
Iteration 127, loss = 0.01608841
Iteration 128, loss = 0.01599889
Iteration 129, loss = 0.01594392
Iteration 130, loss = 0.01589374
Iteration 131, loss = 0.01584932
Iteration 132, loss = 0.01575676
Iteration 133, loss = 0.01573695
Iteration 134, loss = 0.01563189
Iteration 135, loss = 0.01559370
Iteration 136, loss = 0.01552597
Iteration 137, loss = 0.01550945
Iteration 138, loss = 0.01541678
Iteration 139, loss = 0.01545831
Iteration 140, loss = 0.01535761
Iteration 141, loss = 0.01528808
Iteration 142, loss = 0.01525692
Iteration 143, loss = 0.01523290
Iteration 144, loss = 0.01516612
Iteration 145, loss = 0.01512803
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68065880
Iteration 2, loss = 0.63163755
Iteration 3, loss = 0.55578157
Iteration 4, loss = 0.46587184
Iteration 5, loss = 0.37872133
Iteration 6, loss = 0.30222085
Iteration 7, loss = 0.24071435
Iteration 8, loss = 0.19303519
Iteration 9, loss = 0.15729098
Iteration 10, loss = 0.13039197
Iteration 11, loss = 0.11012897
Iteration 12, loss = 0.09497076
Iteration 13, loss = 0.08281982
Iteration 14, loss = 0.07337558
Iteration 15, loss = 0.06586031
Iteration 16, loss = 0.05967878
Iteration 17, loss = 0.05477725
Iteration 18, loss = 0.05046145
Iteration 19, loss = 0.04700250
Iteration 20, loss = 0.04396044
Iteration 21, loss = 0.04134841
Iteration 22, loss = 0.03916805
Iteration 23, loss = 0.03746035
Iteration 24, loss = 0.03571183
Iteration 25, loss = 0.03424660
Iteration 26, loss = 0.03293019
Iteration 27, loss = 0.03195273
Iteration 28, loss = 0.03086550
Iteration 29, loss = 0.02981831
Iteration 30, loss = 0.02900156
Iteration 31, loss = 0.02856438
Iteration 32, loss = 0.02764591
Iteration 33, loss = 0.02708498
Iteration 34, loss = 0.02655506
Iteration 35, loss = 0.02628893
Iteration 36, loss = 0.02560542
Iteration 37, loss = 0.02522143
Iteration 38, loss = 0.02484813
Iteration 39, loss = 0.02447129
Iteration 40, loss = 0.02413800
Iteration 41, loss = 0.02389170
Iteration 42, loss = 0.02342911
Iteration 43, loss = 0.02328769
Iteration 44, loss = 0.02309007
Iteration 45, loss = 0.02292237
Iteration 46, loss = 0.02271368
Iteration 47, loss = 0.02252298
Iteration 48, loss = 0.02229149
Iteration 49, loss = 0.02229359
Iteration 50, loss = 0.02215803
Iteration 51, loss = 0.02188996
Iteration 52, loss = 0.02180468
Iteration 53, loss = 0.02167776
Iteration 54, loss = 0.02131575
Iteration 55, loss = 0.02125939
Iteration 56, loss = 0.02131211
Iteration 57, loss = 0.02116175
Iteration 58, loss = 0.02130109
Iteration 59, loss = 0.02104291
Iteration 60, loss = 0.02092616
Iteration 61, loss = 0.02099127
Iteration 62, loss = 0.02061161
Iteration 63, loss = 0.02069535
Iteration 64, loss = 0.02042477
Iteration 65, loss = 0.02048019
Iteration 66, loss = 0.02051617
Iteration 67, loss = 0.02034400
Iteration 68, loss = 0.02039129
Iteration 69, loss = 0.02039931
Iteration 70, loss = 0.02012804
Iteration 71, loss = 0.02013921
Iteration 72, loss = 0.02002107
Iteration 73, loss = 0.02002278
Iteration 74, loss = 0.02013598
Iteration 75, loss = 0.02001081
Iteration 76, loss = 0.01990569
Iteration 77, loss = 0.01979277
Iteration 78, loss = 0.01994646
Iteration 79, loss = 0.01974567
Iteration 80, loss = 0.01970251
Iteration 81, loss = 0.01977152
Iteration 82, loss = 0.01980304
Iteration 83, loss = 0.01994964
Iteration 84, loss = 0.01990736
Iteration 85, loss = 0.01962421
Iteration 86, loss = 0.01970674
Iteration 87, loss = 0.01957119
Iteration 88, loss = 0.01947248
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69641549
Iteration 2, loss = 0.67780882
Iteration 3, loss = 0.65962681
Iteration 4, loss = 0.63879244
Iteration 5, loss = 0.61446474
Iteration 6, loss = 0.58558634
Iteration 7, loss = 0.55276728
Iteration 8, loss = 0.51761750
Iteration 9, loss = 0.48086966
Iteration 10, loss = 0.44409625
Iteration 11, loss = 0.40790622
Iteration 12, loss = 0.37337076
Iteration 13, loss = 0.34064921
Iteration 14, loss = 0.30996625
Iteration 15, loss = 0.28165420
Iteration 16, loss = 0.25588555
Iteration 17, loss = 0.23238832
Iteration 18, loss = 0.21117124
Iteration 19, loss = 0.19218401
Iteration 20, loss = 0.17524838
Iteration 21, loss = 0.16003076
Iteration 22, loss = 0.14663690
Iteration 23, loss = 0.13463974
Iteration 24, loss = 0.12398767
Iteration 25, loss = 0.11454996
Iteration 26, loss = 0.10612942
Iteration 27, loss = 0.09860033
Iteration 28, loss = 0.09186771
Iteration 29, loss = 0.08582149
Iteration 30, loss = 0.08046630
Iteration 31, loss = 0.07561936
Iteration 32, loss = 0.07121201
Iteration 33, loss = 0.06726340
Iteration 34, loss = 0.06366568
Iteration 35, loss = 0.06040043
Iteration 36, loss = 0.05743417
Iteration 37, loss = 0.05470489
Iteration 38, loss = 0.05221718
Iteration 39, loss = 0.04998407
Iteration 40, loss = 0.04786474
Iteration 41, loss = 0.04589377
Iteration 42, loss = 0.04409869
Iteration 43, loss = 0.04244077
Iteration 44, loss = 0.04089807
Iteration 45, loss = 0.03948867
Iteration 46, loss = 0.03817093
Iteration 47, loss = 0.03694263
Iteration 48, loss = 0.03578576
Iteration 49, loss = 0.03469266
Iteration 50, loss = 0.03372957
Iteration 51, loss = 0.03278572
Iteration 52, loss = 0.03190684
Iteration 53, loss = 0.03111050
Iteration 54, loss = 0.03031991
Iteration 55, loss = 0.02950279
Iteration 56, loss = 0.02878414
Iteration 57, loss = 0.02820903
Iteration 58, loss = 0.02759421
Iteration 59, loss = 0.02701946
Iteration 60, loss = 0.02641289
Iteration 61, loss = 0.02588098
Iteration 62, loss = 0.02541148
Iteration 63, loss = 0.02490695
Iteration 64, loss = 0.02445841
Iteration 65, loss = 0.02403766
Iteration 66, loss = 0.02365265
Iteration 67, loss = 0.02325431
Iteration 68, loss = 0.02287344
Iteration 69, loss = 0.02248784
Iteration 70, loss = 0.02218028
Iteration 71, loss = 0.02184687
Iteration 72, loss = 0.02159910
Iteration 73, loss = 0.02123280
Iteration 74, loss = 0.02097052
Iteration 75, loss = 0.02071104
Iteration 76, loss = 0.02041417
Iteration 77, loss = 0.02016894
Iteration 78, loss = 0.01996054
Iteration 79, loss = 0.01972681
Iteration 80, loss = 0.01953214
Iteration 81, loss = 0.01937855
Iteration 82, loss = 0.01916543
Iteration 83, loss = 0.01889097
Iteration 84, loss = 0.01877319
Iteration 85, loss = 0.01852944
Iteration 86, loss = 0.01839645
Iteration 87, loss = 0.01818587
Iteration 88, loss = 0.01802875
Iteration 89, loss = 0.01789709
Iteration 90, loss = 0.01770920
Iteration 91, loss = 0.01757976
Iteration 92, loss = 0.01742114
Iteration 93, loss = 0.01726968
Iteration 94, loss = 0.01720935
Iteration 95, loss = 0.01705057
Iteration 96, loss = 0.01691522
Iteration 97, loss = 0.01680273
Iteration 98, loss = 0.01672253
Iteration 99, loss = 0.01657324
Iteration 100, loss = 0.01647404
Iteration 101, loss = 0.01636059
Iteration 102, loss = 0.01627882
Iteration 103, loss = 0.01618154
Iteration 104, loss = 0.01606843
Iteration 105, loss = 0.01601084
Iteration 106, loss = 0.01595402
Iteration 107, loss = 0.01585405
Iteration 108, loss = 0.01576899
Iteration 109, loss = 0.01566735
Iteration 110, loss = 0.01554176
Iteration 111, loss = 0.01547334
Iteration 112, loss = 0.01540159
Iteration 113, loss = 0.01537116
Iteration 114, loss = 0.01526561
Iteration 115, loss = 0.01519762
Iteration 116, loss = 0.01511752
Iteration 117, loss = 0.01504797
Iteration 118, loss = 0.01500696
Iteration 119, loss = 0.01493469
Iteration 120, loss = 0.01484211
Iteration 121, loss = 0.01486643
Iteration 122, loss = 0.01473239
Iteration 123, loss = 0.01472317
Iteration 124, loss = 0.01461664
Iteration 125, loss = 0.01458516
Iteration 126, loss = 0.01452572
Iteration 127, loss = 0.01449780
Iteration 128, loss = 0.01439663
Iteration 129, loss = 0.01439733
Iteration 130, loss = 0.01433493
Iteration 131, loss = 0.01428375
Iteration 132, loss = 0.01428034
Iteration 133, loss = 0.01425819
Iteration 134, loss = 0.01416133
Iteration 135, loss = 0.01409947
Iteration 136, loss = 0.01411758
Iteration 137, loss = 0.01402745
Iteration 138, loss = 0.01404726
Iteration 139, loss = 0.01397398
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72098671
Iteration 2, loss = 0.68656726
Iteration 3, loss = 0.64740842
Iteration 4, loss = 0.59922122
Iteration 5, loss = 0.54077366
Iteration 6, loss = 0.47784807
Iteration 7, loss = 0.41514772
Iteration 8, loss = 0.35750041
Iteration 9, loss = 0.30605948
Iteration 10, loss = 0.26180098
Iteration 11, loss = 0.22424914
Iteration 12, loss = 0.19273278
Iteration 13, loss = 0.16667517
Iteration 14, loss = 0.14520748
Iteration 15, loss = 0.12731028
Iteration 16, loss = 0.11266934
Iteration 17, loss = 0.10020485
Iteration 18, loss = 0.08996827
Iteration 19, loss = 0.08130709
Iteration 20, loss = 0.07387123
Iteration 21, loss = 0.06768698
Iteration 22, loss = 0.06227276
Iteration 23, loss = 0.05755917
Iteration 24, loss = 0.05351729
Iteration 25, loss = 0.04998589
Iteration 26, loss = 0.04688163
Iteration 27, loss = 0.04407039
Iteration 28, loss = 0.04163390
Iteration 29, loss = 0.03948063
Iteration 30, loss = 0.03758970
Iteration 31, loss = 0.03572906
Iteration 32, loss = 0.03417162
Iteration 33, loss = 0.03277431
Iteration 34, loss = 0.03150141
Iteration 35, loss = 0.03035593
Iteration 36, loss = 0.02924018
Iteration 37, loss = 0.02830157
Iteration 38, loss = 0.02731511
Iteration 39, loss = 0.02653380
Iteration 40, loss = 0.02579969
Iteration 41, loss = 0.02509674
Iteration 42, loss = 0.02450775
Iteration 43, loss = 0.02381876
Iteration 44, loss = 0.02327060
Iteration 45, loss = 0.02280993
Iteration 46, loss = 0.02219014
Iteration 47, loss = 0.02178559
Iteration 48, loss = 0.02135616
Iteration 49, loss = 0.02110033
Iteration 50, loss = 0.02065971
Iteration 51, loss = 0.02025051
Iteration 52, loss = 0.01997303
Iteration 53, loss = 0.01958642
Iteration 54, loss = 0.01928463
Iteration 55, loss = 0.01911149
Iteration 56, loss = 0.01883863
Iteration 57, loss = 0.01850830
Iteration 58, loss = 0.01834305
Iteration 59, loss = 0.01819077
Iteration 60, loss = 0.01803089
Iteration 61, loss = 0.01773909
Iteration 62, loss = 0.01770825
Iteration 63, loss = 0.01731094
Iteration 64, loss = 0.01727431
Iteration 65, loss = 0.01701077
Iteration 66, loss = 0.01698223
Iteration 67, loss = 0.01672696
Iteration 68, loss = 0.01656208
Iteration 69, loss = 0.01655747
Iteration 70, loss = 0.01634841
Iteration 71, loss = 0.01628080
Iteration 72, loss = 0.01613967
Iteration 73, loss = 0.01608889
Iteration 74, loss = 0.01582663
Iteration 75, loss = 0.01591313
Iteration 76, loss = 0.01588448
Iteration 77, loss = 0.01560208
Iteration 78, loss = 0.01548947
Iteration 79, loss = 0.01545816
Iteration 80, loss = 0.01521892
Iteration 81, loss = 0.01518805
Iteration 82, loss = 0.01517047
Iteration 83, loss = 0.01512103
Iteration 84, loss = 0.01510171
Iteration 85, loss = 0.01500174
Iteration 86, loss = 0.01491779
Iteration 87, loss = 0.01490733
Iteration 88, loss = 0.01477326
Iteration 89, loss = 0.01482625
Iteration 90, loss = 0.01474576
Iteration 91, loss = 0.01468615
Iteration 92, loss = 0.01473017
Iteration 93, loss = 0.01447396
Iteration 94, loss = 0.01453359
Iteration 95, loss = 0.01441183
Iteration 96, loss = 0.01432066
Iteration 97, loss = 0.01432985
Iteration 98, loss = 0.01428371
Iteration 99, loss = 0.01421870
Iteration 100, loss = 0.01422242
Iteration 101, loss = 0.01418059
Iteration 102, loss = 0.01414532
Iteration 103, loss = 0.01417030
Iteration 104, loss = 0.01414514
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67710302
Iteration 2, loss = 0.64422341
Iteration 3, loss = 0.59757007
Iteration 4, loss = 0.53663312
Iteration 5, loss = 0.47139142
Iteration 6, loss = 0.40619361
Iteration 7, loss = 0.34484237
Iteration 8, loss = 0.29071127
Iteration 9, loss = 0.24379004
Iteration 10, loss = 0.20500591
Iteration 11, loss = 0.17336133
Iteration 12, loss = 0.14790482
Iteration 13, loss = 0.12771509
Iteration 14, loss = 0.11116148
Iteration 15, loss = 0.09783086
Iteration 16, loss = 0.08694491
Iteration 17, loss = 0.07806574
Iteration 18, loss = 0.07049087
Iteration 19, loss = 0.06442447
Iteration 20, loss = 0.05900061
Iteration 21, loss = 0.05456031
Iteration 22, loss = 0.05070569
Iteration 23, loss = 0.04726301
Iteration 24, loss = 0.04439634
Iteration 25, loss = 0.04178014
Iteration 26, loss = 0.03939582
Iteration 27, loss = 0.03747806
Iteration 28, loss = 0.03565773
Iteration 29, loss = 0.03415810
Iteration 30, loss = 0.03271084
Iteration 31, loss = 0.03149132
Iteration 32, loss = 0.03025062
Iteration 33, loss = 0.02940385
Iteration 34, loss = 0.02825078
Iteration 35, loss = 0.02735944
Iteration 36, loss = 0.02670882
Iteration 37, loss = 0.02585293
Iteration 38, loss = 0.02521943
Iteration 39, loss = 0.02456910
Iteration 40, loss = 0.02400657
Iteration 41, loss = 0.02348516
Iteration 42, loss = 0.02304247
Iteration 43, loss = 0.02256550
Iteration 44, loss = 0.02203919
Iteration 45, loss = 0.02180319
Iteration 46, loss = 0.02144812
Iteration 47, loss = 0.02106087
Iteration 48, loss = 0.02072786
Iteration 49, loss = 0.02059865
Iteration 50, loss = 0.02009610
Iteration 51, loss = 0.01990374
Iteration 52, loss = 0.01957901
Iteration 53, loss = 0.01937705
Iteration 54, loss = 0.01915329
Iteration 55, loss = 0.01898706
Iteration 56, loss = 0.01881413
Iteration 57, loss = 0.01857352
Iteration 58, loss = 0.01848068
Iteration 59, loss = 0.01832786
Iteration 60, loss = 0.01827644
Iteration 61, loss = 0.01801510
Iteration 62, loss = 0.01782989
Iteration 63, loss = 0.01768866
Iteration 64, loss = 0.01753210
Iteration 65, loss = 0.01762355
Iteration 66, loss = 0.01737605
Iteration 67, loss = 0.01720541
Iteration 68, loss = 0.01715070
Iteration 69, loss = 0.01713485
Iteration 70, loss = 0.01693476
Iteration 71, loss = 0.01691969
Iteration 72, loss = 0.01672645
Iteration 73, loss = 0.01675295
Iteration 74, loss = 0.01651498
Iteration 75, loss = 0.01658256
Iteration 76, loss = 0.01645650
Iteration 77, loss = 0.01639917
Iteration 78, loss = 0.01629754
Iteration 79, loss = 0.01619992
Iteration 80, loss = 0.01604283
Iteration 81, loss = 0.01605893
Iteration 82, loss = 0.01600881
Iteration 83, loss = 0.01596159
Iteration 84, loss = 0.01614408
Iteration 85, loss = 0.01592002
Iteration 86, loss = 0.01585390
Iteration 87, loss = 0.01599379
Iteration 88, loss = 0.01574646
Iteration 89, loss = 0.01568956
Iteration 90, loss = 0.01559122
Iteration 91, loss = 0.01562517
Iteration 92, loss = 0.01555941
Iteration 93, loss = 0.01560920
Iteration 94, loss = 0.01544904
Iteration 95, loss = 0.01544300
Iteration 96, loss = 0.01548688
Iteration 97, loss = 0.01535939
Iteration 98, loss = 0.01538796
Iteration 99, loss = 0.01537179
Iteration 100, loss = 0.01529699
Iteration 101, loss = 0.01519592
Iteration 102, loss = 0.01528369
Iteration 103, loss = 0.01512554
Iteration 104, loss = 0.01514226
Iteration 105, loss = 0.01523323
Iteration 106, loss = 0.01513212
Iteration 107, loss = 0.01524794
Iteration 108, loss = 0.01508019
Iteration 109, loss = 0.01495128
Iteration 110, loss = 0.01509062
Iteration 111, loss = 0.01499000
Iteration 112, loss = 0.01501401
Iteration 113, loss = 0.01499348
Iteration 114, loss = 0.01490296
Iteration 115, loss = 0.01485676
Iteration 116, loss = 0.01499089
Iteration 117, loss = 0.01502857
Iteration 118, loss = 0.01479224
Iteration 119, loss = 0.01482236
Iteration 120, loss = 0.01493902
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68986400
Iteration 2, loss = 0.65147571
Iteration 3, loss = 0.60340453
Iteration 4, loss = 0.54316849
Iteration 5, loss = 0.47735208
Iteration 6, loss = 0.41141158
Iteration 7, loss = 0.34971965
Iteration 8, loss = 0.29524390
Iteration 9, loss = 0.24858456
Iteration 10, loss = 0.20995752
Iteration 11, loss = 0.17825081
Iteration 12, loss = 0.15268459
Iteration 13, loss = 0.13215945
Iteration 14, loss = 0.11549270
Iteration 15, loss = 0.10208562
Iteration 16, loss = 0.09099124
Iteration 17, loss = 0.08148454
Iteration 18, loss = 0.07397491
Iteration 19, loss = 0.06755352
Iteration 20, loss = 0.06203748
Iteration 21, loss = 0.05747343
Iteration 22, loss = 0.05345940
Iteration 23, loss = 0.04992516
Iteration 24, loss = 0.04687898
Iteration 25, loss = 0.04436046
Iteration 26, loss = 0.04200091
Iteration 27, loss = 0.03997473
Iteration 28, loss = 0.03824405
Iteration 29, loss = 0.03648727
Iteration 30, loss = 0.03513507
Iteration 31, loss = 0.03377745
Iteration 32, loss = 0.03279962
Iteration 33, loss = 0.03162343
Iteration 34, loss = 0.03067769
Iteration 35, loss = 0.02964033
Iteration 36, loss = 0.02889907
Iteration 37, loss = 0.02834738
Iteration 38, loss = 0.02758830
Iteration 39, loss = 0.02690956
Iteration 40, loss = 0.02643387
Iteration 41, loss = 0.02602076
Iteration 42, loss = 0.02546975
Iteration 43, loss = 0.02485153
Iteration 44, loss = 0.02443214
Iteration 45, loss = 0.02399016
Iteration 46, loss = 0.02367713
Iteration 47, loss = 0.02343633
Iteration 48, loss = 0.02313895
Iteration 49, loss = 0.02271854
Iteration 50, loss = 0.02251361
Iteration 51, loss = 0.02228028
Iteration 52, loss = 0.02198510
Iteration 53, loss = 0.02191526
Iteration 54, loss = 0.02150737
Iteration 55, loss = 0.02129653
Iteration 56, loss = 0.02138766
Iteration 57, loss = 0.02098407
Iteration 58, loss = 0.02077221
Iteration 59, loss = 0.02072773
Iteration 60, loss = 0.02057158
Iteration 61, loss = 0.02032457
Iteration 62, loss = 0.02032513
Iteration 63, loss = 0.02007905
Iteration 64, loss = 0.01993580
Iteration 65, loss = 0.01993021
Iteration 66, loss = 0.01974814
Iteration 67, loss = 0.01964865
Iteration 68, loss = 0.01955889
Iteration 69, loss = 0.01940440
Iteration 70, loss = 0.01933173
Iteration 71, loss = 0.01943698
Iteration 72, loss = 0.01920706
Iteration 73, loss = 0.01896969
Iteration 74, loss = 0.01904976
Iteration 75, loss = 0.01904162
Iteration 76, loss = 0.01884343
Iteration 77, loss = 0.01879072
Iteration 78, loss = 0.01870167
Iteration 79, loss = 0.01867021
Iteration 80, loss = 0.01866733
Iteration 81, loss = 0.01861948
Iteration 82, loss = 0.01841732
Iteration 83, loss = 0.01841021
Iteration 84, loss = 0.01836782
Iteration 85, loss = 0.01822570
Iteration 86, loss = 0.01829531
Iteration 87, loss = 0.01819412
Iteration 88, loss = 0.01839553
Iteration 89, loss = 0.01817906
Iteration 90, loss = 0.01810661
Iteration 91, loss = 0.01799325
Iteration 92, loss = 0.01789745
Iteration 93, loss = 0.01784443
Iteration 94, loss = 0.01790202
Iteration 95, loss = 0.01793615
Iteration 96, loss = 0.01796931
Iteration 97, loss = 0.01781727
Iteration 98, loss = 0.01791403
Iteration 99, loss = 0.01770189
Iteration 100, loss = 0.01776273
Iteration 101, loss = 0.01771219
Iteration 102, loss = 0.01784151
Iteration 103, loss = 0.01773877
Iteration 104, loss = 0.01787428
Iteration 105, loss = 0.01756269
Iteration 106, loss = 0.01775477
Iteration 107, loss = 0.01763003
Iteration 108, loss = 0.01762094
Iteration 109, loss = 0.01754693
Iteration 110, loss = 0.01762028
Iteration 111, loss = 0.01747685
Iteration 112, loss = 0.01755898
Iteration 113, loss = 0.01749349
Iteration 114, loss = 0.01737899
Iteration 115, loss = 0.01746035
Iteration 116, loss = 0.01745691
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67750849
Iteration 2, loss = 0.64288301
Iteration 3, loss = 0.58447242
Iteration 4, loss = 0.50511923
Iteration 5, loss = 0.42419472
Iteration 6, loss = 0.34907598
Iteration 7, loss = 0.28406425
Iteration 8, loss = 0.23103026
Iteration 9, loss = 0.18908640
Iteration 10, loss = 0.15684136
Iteration 11, loss = 0.13183053
Iteration 12, loss = 0.11252301
Iteration 13, loss = 0.09758979
Iteration 14, loss = 0.08568605
Iteration 15, loss = 0.07634315
Iteration 16, loss = 0.06867700
Iteration 17, loss = 0.06249806
Iteration 18, loss = 0.05716875
Iteration 19, loss = 0.05271745
Iteration 20, loss = 0.04911342
Iteration 21, loss = 0.04578336
Iteration 22, loss = 0.04304149
Iteration 23, loss = 0.04067991
Iteration 24, loss = 0.03861313
Iteration 25, loss = 0.03689555
Iteration 26, loss = 0.03520999
Iteration 27, loss = 0.03366922
Iteration 28, loss = 0.03256193
Iteration 29, loss = 0.03136816
Iteration 30, loss = 0.03033509
Iteration 31, loss = 0.02962361
Iteration 32, loss = 0.02880309
Iteration 33, loss = 0.02783774
Iteration 34, loss = 0.02736159
Iteration 35, loss = 0.02671304
Iteration 36, loss = 0.02598176
Iteration 37, loss = 0.02543961
Iteration 38, loss = 0.02496526
Iteration 39, loss = 0.02448043
Iteration 40, loss = 0.02417846
Iteration 41, loss = 0.02373700
Iteration 42, loss = 0.02337402
Iteration 43, loss = 0.02315153
Iteration 44, loss = 0.02263046
Iteration 45, loss = 0.02236370
Iteration 46, loss = 0.02240424
Iteration 47, loss = 0.02195251
Iteration 48, loss = 0.02173873
Iteration 49, loss = 0.02152220
Iteration 50, loss = 0.02130370
Iteration 51, loss = 0.02101043
Iteration 52, loss = 0.02090241
Iteration 53, loss = 0.02076877
Iteration 54, loss = 0.02054996
Iteration 55, loss = 0.02057607
Iteration 56, loss = 0.02041570
Iteration 57, loss = 0.02015968
Iteration 58, loss = 0.01995043
Iteration 59, loss = 0.01995780
Iteration 60, loss = 0.01973299
Iteration 61, loss = 0.01971961
Iteration 62, loss = 0.01956998
Iteration 63, loss = 0.01940687
Iteration 64, loss = 0.01948008
Iteration 65, loss = 0.01926673
Iteration 66, loss = 0.01925914
Iteration 67, loss = 0.01924297
Iteration 68, loss = 0.01921787
Iteration 69, loss = 0.01908805
Iteration 70, loss = 0.01903301
Iteration 71, loss = 0.01904314
Iteration 72, loss = 0.01875876
Iteration 73, loss = 0.01883934
Iteration 74, loss = 0.01890802
Iteration 75, loss = 0.01862912
Iteration 76, loss = 0.01861186
Iteration 77, loss = 0.01881649
Iteration 78, loss = 0.01854467
Iteration 79, loss = 0.01834808
Iteration 80, loss = 0.01832159
Iteration 81, loss = 0.01848315
Iteration 82, loss = 0.01826982
Iteration 83, loss = 0.01838324
Iteration 84, loss = 0.01822219
Iteration 85, loss = 0.01807496
Iteration 86, loss = 0.01800984
Iteration 87, loss = 0.01818287
Iteration 88, loss = 0.01802964
Iteration 89, loss = 0.01807594
Iteration 90, loss = 0.01805838
Iteration 91, loss = 0.01792297
Iteration 92, loss = 0.01787982
Iteration 93, loss = 0.01811275
Iteration 94, loss = 0.01799285
Iteration 95, loss = 0.01788655
Iteration 96, loss = 0.01803012
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72059496
Iteration 2, loss = 0.69078743
Iteration 3, loss = 0.65437290
Iteration 4, loss = 0.60965117
Iteration 5, loss = 0.55935111
Iteration 6, loss = 0.50599951
Iteration 7, loss = 0.45172461
Iteration 8, loss = 0.39876144
Iteration 9, loss = 0.34920352
Iteration 10, loss = 0.30449355
Iteration 11, loss = 0.26531252
Iteration 12, loss = 0.23147734
Iteration 13, loss = 0.20249542
Iteration 14, loss = 0.17801346
Iteration 15, loss = 0.15729201
Iteration 16, loss = 0.13983311
Iteration 17, loss = 0.12498971
Iteration 18, loss = 0.11234406
Iteration 19, loss = 0.10160522
Iteration 20, loss = 0.09251803
Iteration 21, loss = 0.08472338
Iteration 22, loss = 0.07803130
Iteration 23, loss = 0.07212459
Iteration 24, loss = 0.06689203
Iteration 25, loss = 0.06236759
Iteration 26, loss = 0.05837300
Iteration 27, loss = 0.05485918
Iteration 28, loss = 0.05169438
Iteration 29, loss = 0.04890653
Iteration 30, loss = 0.04638874
Iteration 31, loss = 0.04405377
Iteration 32, loss = 0.04223498
Iteration 33, loss = 0.04042563
Iteration 34, loss = 0.03894899
Iteration 35, loss = 0.03724620
Iteration 36, loss = 0.03577843
Iteration 37, loss = 0.03443115
Iteration 38, loss = 0.03326459
Iteration 39, loss = 0.03228728
Iteration 40, loss = 0.03108786
Iteration 41, loss = 0.03043769
Iteration 42, loss = 0.02968267
Iteration 43, loss = 0.02891606
Iteration 44, loss = 0.02809589
Iteration 45, loss = 0.02746564
Iteration 46, loss = 0.02677978
Iteration 47, loss = 0.02617471
Iteration 48, loss = 0.02549291
Iteration 49, loss = 0.02493664
Iteration 50, loss = 0.02436512
Iteration 51, loss = 0.02387860
Iteration 52, loss = 0.02346126
Iteration 53, loss = 0.02294512
Iteration 54, loss = 0.02268401
Iteration 55, loss = 0.02236689
Iteration 56, loss = 0.02211866
Iteration 57, loss = 0.02181016
Iteration 58, loss = 0.02150806
Iteration 59, loss = 0.02120806
Iteration 60, loss = 0.02076356
Iteration 61, loss = 0.02060916
Iteration 62, loss = 0.02017606
Iteration 63, loss = 0.02003249
Iteration 64, loss = 0.01986994
Iteration 65, loss = 0.01950815
Iteration 66, loss = 0.01921817
Iteration 67, loss = 0.01923613
Iteration 68, loss = 0.01906257
Iteration 69, loss = 0.01895477
Iteration 70, loss = 0.01878309
Iteration 71, loss = 0.01849295
Iteration 72, loss = 0.01840786
Iteration 73, loss = 0.01832275
Iteration 74, loss = 0.01818484
Iteration 75, loss = 0.01803212
Iteration 76, loss = 0.01780424
Iteration 77, loss = 0.01756599
Iteration 78, loss = 0.01741014
Iteration 79, loss = 0.01730535
Iteration 80, loss = 0.01714567
Iteration 81, loss = 0.01708466
Iteration 82, loss = 0.01693711
Iteration 83, loss = 0.01677716
Iteration 84, loss = 0.01682538
Iteration 85, loss = 0.01682472
Iteration 86, loss = 0.01685909
Iteration 87, loss = 0.01671418
Iteration 88, loss = 0.01645107
Iteration 89, loss = 0.01636945
Iteration 90, loss = 0.01624901
Iteration 91, loss = 0.01609067
Iteration 92, loss = 0.01604863
Iteration 93, loss = 0.01601225
Iteration 94, loss = 0.01597142
Iteration 95, loss = 0.01592905
Iteration 96, loss = 0.01590316
Iteration 97, loss = 0.01580340
Iteration 98, loss = 0.01597472
Iteration 99, loss = 0.01588164
Iteration 100, loss = 0.01561373
Iteration 101, loss = 0.01561307
Iteration 102, loss = 0.01548767
Iteration 103, loss = 0.01543123
Iteration 104, loss = 0.01536245
Iteration 105, loss = 0.01536964
Iteration 106, loss = 0.01515853
Iteration 107, loss = 0.01513608
Iteration 108, loss = 0.01507842
Iteration 109, loss = 0.01510406
Iteration 110, loss = 0.01514006
Iteration 111, loss = 0.01552291
Iteration 112, loss = 0.01532806
Iteration 113, loss = 0.01525614
Iteration 114, loss = 0.01503860
Iteration 115, loss = 0.01515513
Iteration 116, loss = 0.01499477
Iteration 117, loss = 0.01492570
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68126044
Iteration 2, loss = 0.61603018
Iteration 3, loss = 0.52716834
Iteration 4, loss = 0.43160947
Iteration 5, loss = 0.34229140
Iteration 6, loss = 0.26853109
Iteration 7, loss = 0.21175618
Iteration 8, loss = 0.17042898
Iteration 9, loss = 0.13961576
Iteration 10, loss = 0.11667102
Iteration 11, loss = 0.09920922
Iteration 12, loss = 0.08656669
Iteration 13, loss = 0.07650647
Iteration 14, loss = 0.06840499
Iteration 15, loss = 0.06191124
Iteration 16, loss = 0.05675173
Iteration 17, loss = 0.05250320
Iteration 18, loss = 0.04892499
Iteration 19, loss = 0.04608984
Iteration 20, loss = 0.04347385
Iteration 21, loss = 0.04117265
Iteration 22, loss = 0.03914360
Iteration 23, loss = 0.03771067
Iteration 24, loss = 0.03645438
Iteration 25, loss = 0.03524020
Iteration 26, loss = 0.03409033
Iteration 27, loss = 0.03297767
Iteration 28, loss = 0.03200284
Iteration 29, loss = 0.03117641
Iteration 30, loss = 0.03057524
Iteration 31, loss = 0.03048674
Iteration 32, loss = 0.03041958
Iteration 33, loss = 0.02945022
Iteration 34, loss = 0.02900487
Iteration 35, loss = 0.02845586
Iteration 36, loss = 0.02804907
Iteration 37, loss = 0.02758350
Iteration 38, loss = 0.02714537
Iteration 39, loss = 0.02687677
Iteration 40, loss = 0.02640821
Iteration 41, loss = 0.02617035
Iteration 42, loss = 0.02602147
Iteration 43, loss = 0.02560563
Iteration 44, loss = 0.02538729
Iteration 45, loss = 0.02516931
Iteration 46, loss = 0.02494025
Iteration 47, loss = 0.02494653
Iteration 48, loss = 0.02510684
Iteration 49, loss = 0.02506485
Iteration 50, loss = 0.02475463
Iteration 51, loss = 0.02594981
Iteration 52, loss = 0.02664533
Iteration 53, loss = 0.02582935
Iteration 54, loss = 0.02559677
Iteration 55, loss = 0.02527157
Iteration 56, loss = 0.02468200
Iteration 57, loss = 0.02441173
Iteration 58, loss = 0.02407684
Iteration 59, loss = 0.02412120
Iteration 60, loss = 0.02371276
Iteration 61, loss = 0.02342027
Iteration 62, loss = 0.02342479
Iteration 63, loss = 0.02336400
Iteration 64, loss = 0.02292886
Iteration 65, loss = 0.02308923
Iteration 66, loss = 0.02288033
Iteration 67, loss = 0.02256737
Iteration 68, loss = 0.02291816
Iteration 69, loss = 0.02286534
Iteration 70, loss = 0.02253173
Iteration 71, loss = 0.02281554
Iteration 72, loss = 0.02245612
Iteration 73, loss = 0.02309609
Iteration 74, loss = 0.02310388
Iteration 75, loss = 0.02281136
Iteration 76, loss = 0.02268885
Iteration 77, loss = 0.02303932
Iteration 78, loss = 0.02253990
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69806304
Iteration 2, loss = 0.62341225
Iteration 3, loss = 0.51904178
Iteration 4, loss = 0.40897383
Iteration 5, loss = 0.31385863
Iteration 6, loss = 0.24013813
Iteration 7, loss = 0.18631050
Iteration 8, loss = 0.14815715
Iteration 9, loss = 0.12099987
Iteration 10, loss = 0.10145397
Iteration 11, loss = 0.08710740
Iteration 12, loss = 0.07631268
Iteration 13, loss = 0.06785359
Iteration 14, loss = 0.06145763
Iteration 15, loss = 0.05617699
Iteration 16, loss = 0.05198642
Iteration 17, loss = 0.04853925
Iteration 18, loss = 0.04558904
Iteration 19, loss = 0.04313955
Iteration 20, loss = 0.04100456
Iteration 21, loss = 0.03932754
Iteration 22, loss = 0.03754265
Iteration 23, loss = 0.03649135
Iteration 24, loss = 0.03542294
Iteration 25, loss = 0.03414235
Iteration 26, loss = 0.03331516
Iteration 27, loss = 0.03218800
Iteration 28, loss = 0.03187745
Iteration 29, loss = 0.03110436
Iteration 30, loss = 0.03033865
Iteration 31, loss = 0.02988115
Iteration 32, loss = 0.02924860
Iteration 33, loss = 0.02900140
Iteration 34, loss = 0.02860982
Iteration 35, loss = 0.02805696
Iteration 36, loss = 0.02803352
Iteration 37, loss = 0.02762050
Iteration 38, loss = 0.02740214
Iteration 39, loss = 0.02714941
Iteration 40, loss = 0.02723328
Iteration 41, loss = 0.02653109
Iteration 42, loss = 0.02662097
Iteration 43, loss = 0.02624844
Iteration 44, loss = 0.02611304
Iteration 45, loss = 0.02564748
Iteration 46, loss = 0.02605901
Iteration 47, loss = 0.02567701
Iteration 48, loss = 0.02571706
Iteration 49, loss = 0.02545152
Iteration 50, loss = 0.02534709
Iteration 51, loss = 0.02496831
Iteration 52, loss = 0.02523116
Iteration 53, loss = 0.02482750
Iteration 54, loss = 0.02521141
Iteration 55, loss = 0.02491054
Iteration 56, loss = 0.02489525
Iteration 57, loss = 0.02469560
Iteration 58, loss = 0.02465296
Iteration 59, loss = 0.02441125
Iteration 60, loss = 0.02438871
Iteration 61, loss = 0.02440237
Iteration 62, loss = 0.02435166
Iteration 63, loss = 0.02445433
Iteration 64, loss = 0.02435398
Iteration 65, loss = 0.02411946
Iteration 66, loss = 0.02467536
Iteration 67, loss = 0.02427130
Iteration 68, loss = 0.02429284
Iteration 69, loss = 0.02436711
Iteration 70, loss = 0.02385785
Iteration 71, loss = 0.02380034
Iteration 72, loss = 0.02412440
Iteration 73, loss = 0.02381339
Iteration 74, loss = 0.02397097
Iteration 75, loss = 0.02358354
Iteration 76, loss = 0.02359030
Iteration 77, loss = 0.02387686
Iteration 78, loss = 0.02387675
Iteration 79, loss = 0.02369196
Iteration 80, loss = 0.02367049
Iteration 81, loss = 0.02374042
Iteration 82, loss = 0.02374424
Iteration 83, loss = 0.02369137
Iteration 84, loss = 0.02362517
Iteration 85, loss = 0.02365501
Iteration 86, loss = 0.02379522
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73468470
Iteration 2, loss = 0.71873759
Iteration 3, loss = 0.70615028
Iteration 4, loss = 0.69457669
Iteration 5, loss = 0.68273342
Iteration 6, loss = 0.66975139
Iteration 7, loss = 0.65574942
Iteration 8, loss = 0.64050389
Iteration 9, loss = 0.62367108
Iteration 10, loss = 0.60540814
Iteration 11, loss = 0.58584980
Iteration 12, loss = 0.56499696
Iteration 13, loss = 0.54338579
Iteration 14, loss = 0.52077051
Iteration 15, loss = 0.49768973
Iteration 16, loss = 0.47421452
Iteration 17, loss = 0.45058132
Iteration 18, loss = 0.42726894
Iteration 19, loss = 0.40410169
Iteration 20, loss = 0.38138872
Iteration 21, loss = 0.35931932
Iteration 22, loss = 0.33819565
Iteration 23, loss = 0.31792156
Iteration 24, loss = 0.29868134
Iteration 25, loss = 0.28046881
Iteration 26, loss = 0.26329201
Iteration 27, loss = 0.24719692
Iteration 28, loss = 0.23215527
Iteration 29, loss = 0.21804854
Iteration 30, loss = 0.20492411
Iteration 31, loss = 0.19273023
Iteration 32, loss = 0.18142390
Iteration 33, loss = 0.17093486
Iteration 34, loss = 0.16114198
Iteration 35, loss = 0.15208225
Iteration 36, loss = 0.14376905
Iteration 37, loss = 0.13597286
Iteration 38, loss = 0.12876512
Iteration 39, loss = 0.12206874
Iteration 40, loss = 0.11589910
Iteration 41, loss = 0.11014257
Iteration 42, loss = 0.10481146
Iteration 43, loss = 0.09993804
Iteration 44, loss = 0.09524258
Iteration 45, loss = 0.09100743
Iteration 46, loss = 0.08702681
Iteration 47, loss = 0.08325306
Iteration 48, loss = 0.07979600
Iteration 49, loss = 0.07650188
Iteration 50, loss = 0.07343777
Iteration 51, loss = 0.07059396
Iteration 52, loss = 0.06791724
Iteration 53, loss = 0.06539726
Iteration 54, loss = 0.06302987
Iteration 55, loss = 0.06075781
Iteration 56, loss = 0.05862684
Iteration 57, loss = 0.05665418
Iteration 58, loss = 0.05474453
Iteration 59, loss = 0.05296288
Iteration 60, loss = 0.05130225
Iteration 61, loss = 0.04971387
Iteration 62, loss = 0.04819477
Iteration 63, loss = 0.04676238
Iteration 64, loss = 0.04541830
Iteration 65, loss = 0.04412045
Iteration 66, loss = 0.04291361
Iteration 67, loss = 0.04176504
Iteration 68, loss = 0.04063929
Iteration 69, loss = 0.03954810
Iteration 70, loss = 0.03859302
Iteration 71, loss = 0.03762567
Iteration 72, loss = 0.03668108
Iteration 73, loss = 0.03581694
Iteration 74, loss = 0.03499107
Iteration 75, loss = 0.03419497
Iteration 76, loss = 0.03345986
Iteration 77, loss = 0.03270705
Iteration 78, loss = 0.03198025
Iteration 79, loss = 0.03129947
Iteration 80, loss = 0.03063903
Iteration 81, loss = 0.03005087
Iteration 82, loss = 0.02940584
Iteration 83, loss = 0.02881263
Iteration 84, loss = 0.02827532
Iteration 85, loss = 0.02775639
Iteration 86, loss = 0.02721843
Iteration 87, loss = 0.02675456
Iteration 88, loss = 0.02626861
Iteration 89, loss = 0.02576208
Iteration 90, loss = 0.02536849
Iteration 91, loss = 0.02489545
Iteration 92, loss = 0.02448227
Iteration 93, loss = 0.02409414
Iteration 94, loss = 0.02369438
Iteration 95, loss = 0.02329819
Iteration 96, loss = 0.02297400
Iteration 97, loss = 0.02260500
Iteration 98, loss = 0.02230303
Iteration 99, loss = 0.02195758
Iteration 100, loss = 0.02163947
Iteration 101, loss = 0.02133790
Iteration 102, loss = 0.02103206
Iteration 103, loss = 0.02073525
Iteration 104, loss = 0.02046990
Iteration 105, loss = 0.02019508
Iteration 106, loss = 0.01990889
Iteration 107, loss = 0.01964759
Iteration 108, loss = 0.01939221
Iteration 109, loss = 0.01912011
Iteration 110, loss = 0.01898588
Iteration 111, loss = 0.01871496
Iteration 112, loss = 0.01845727
Iteration 113, loss = 0.01827676
Iteration 114, loss = 0.01803151
Iteration 115, loss = 0.01783672
Iteration 116, loss = 0.01761780
Iteration 117, loss = 0.01743593
Iteration 118, loss = 0.01721547
Iteration 119, loss = 0.01703754
Iteration 120, loss = 0.01684838
Iteration 121, loss = 0.01667922
Iteration 122, loss = 0.01651257
Iteration 123, loss = 0.01638526
Iteration 124, loss = 0.01618644
Iteration 125, loss = 0.01602691
Iteration 126, loss = 0.01585900
Iteration 127, loss = 0.01571557
Iteration 128, loss = 0.01557252
Iteration 129, loss = 0.01541835
Iteration 130, loss = 0.01529388
Iteration 131, loss = 0.01513636
Iteration 132, loss = 0.01500918
Iteration 133, loss = 0.01487687
Iteration 134, loss = 0.01476308
Iteration 135, loss = 0.01462960
Iteration 136, loss = 0.01451517
Iteration 137, loss = 0.01439889
Iteration 138, loss = 0.01428335
Iteration 139, loss = 0.01415409
Iteration 140, loss = 0.01407391
Iteration 141, loss = 0.01398252
Iteration 142, loss = 0.01384126
Iteration 143, loss = 0.01375939
Iteration 144, loss = 0.01363624
Iteration 145, loss = 0.01355825
Iteration 146, loss = 0.01344529
Iteration 147, loss = 0.01336942
Iteration 148, loss = 0.01325116
Iteration 149, loss = 0.01316723
Iteration 150, loss = 0.01314528
Iteration 151, loss = 0.01300260
Iteration 152, loss = 0.01292594
Iteration 153, loss = 0.01281501
Iteration 154, loss = 0.01278914
Iteration 155, loss = 0.01267085
Iteration 156, loss = 0.01259675
Iteration 157, loss = 0.01252065
Iteration 158, loss = 0.01245490
Iteration 159, loss = 0.01238570
Iteration 160, loss = 0.01230111
Iteration 161, loss = 0.01224638
Iteration 162, loss = 0.01217257
Iteration 163, loss = 0.01210359
Iteration 164, loss = 0.01207327
Iteration 165, loss = 0.01200993
Iteration 166, loss = 0.01197673
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69213518
Iteration 2, loss = 0.67357213
Iteration 3, loss = 0.65853276
Iteration 4, loss = 0.64433666
Iteration 5, loss = 0.62973920
Iteration 6, loss = 0.61446426
Iteration 7, loss = 0.59778731
Iteration 8, loss = 0.58001043
Iteration 9, loss = 0.56079540
Iteration 10, loss = 0.54027633
Iteration 11, loss = 0.51880871
Iteration 12, loss = 0.49638886
Iteration 13, loss = 0.47351722
Iteration 14, loss = 0.45030787
Iteration 15, loss = 0.42707764
Iteration 16, loss = 0.40419500
Iteration 17, loss = 0.38177736
Iteration 18, loss = 0.36000383
Iteration 19, loss = 0.33903496
Iteration 20, loss = 0.31882875
Iteration 21, loss = 0.29968175
Iteration 22, loss = 0.28137121
Iteration 23, loss = 0.26422312
Iteration 24, loss = 0.24793479
Iteration 25, loss = 0.23265983
Iteration 26, loss = 0.21829438
Iteration 27, loss = 0.20500152
Iteration 28, loss = 0.19256565
Iteration 29, loss = 0.18099825
Iteration 30, loss = 0.17019658
Iteration 31, loss = 0.16018244
Iteration 32, loss = 0.15089827
Iteration 33, loss = 0.14227997
Iteration 34, loss = 0.13430543
Iteration 35, loss = 0.12691025
Iteration 36, loss = 0.12001592
Iteration 37, loss = 0.11362558
Iteration 38, loss = 0.10773103
Iteration 39, loss = 0.10226772
Iteration 40, loss = 0.09717140
Iteration 41, loss = 0.09241238
Iteration 42, loss = 0.08801629
Iteration 43, loss = 0.08395009
Iteration 44, loss = 0.08009677
Iteration 45, loss = 0.07653655
Iteration 46, loss = 0.07316363
Iteration 47, loss = 0.07007264
Iteration 48, loss = 0.06713684
Iteration 49, loss = 0.06441162
Iteration 50, loss = 0.06187644
Iteration 51, loss = 0.05943950
Iteration 52, loss = 0.05721198
Iteration 53, loss = 0.05507946
Iteration 54, loss = 0.05306122
Iteration 55, loss = 0.05115456
Iteration 56, loss = 0.04937615
Iteration 57, loss = 0.04767926
Iteration 58, loss = 0.04608250
Iteration 59, loss = 0.04459473
Iteration 60, loss = 0.04318180
Iteration 61, loss = 0.04184358
Iteration 62, loss = 0.04055325
Iteration 63, loss = 0.03934542
Iteration 64, loss = 0.03817824
Iteration 65, loss = 0.03707159
Iteration 66, loss = 0.03600818
Iteration 67, loss = 0.03499378
Iteration 68, loss = 0.03403091
Iteration 69, loss = 0.03316330
Iteration 70, loss = 0.03224150
Iteration 71, loss = 0.03146912
Iteration 72, loss = 0.03065150
Iteration 73, loss = 0.02984310
Iteration 74, loss = 0.02911534
Iteration 75, loss = 0.02840449
Iteration 76, loss = 0.02774988
Iteration 77, loss = 0.02708538
Iteration 78, loss = 0.02646084
Iteration 79, loss = 0.02586820
Iteration 80, loss = 0.02530069
Iteration 81, loss = 0.02474054
Iteration 82, loss = 0.02424204
Iteration 83, loss = 0.02372878
Iteration 84, loss = 0.02325767
Iteration 85, loss = 0.02277032
Iteration 86, loss = 0.02232698
Iteration 87, loss = 0.02188796
Iteration 88, loss = 0.02146367
Iteration 89, loss = 0.02103989
Iteration 90, loss = 0.02063440
Iteration 91, loss = 0.02025177
Iteration 92, loss = 0.01989066
Iteration 93, loss = 0.01952299
Iteration 94, loss = 0.01916528
Iteration 95, loss = 0.01883584
Iteration 96, loss = 0.01850415
Iteration 97, loss = 0.01819373
Iteration 98, loss = 0.01789290
Iteration 99, loss = 0.01761028
Iteration 100, loss = 0.01733153
Iteration 101, loss = 0.01707327
Iteration 102, loss = 0.01680514
Iteration 103, loss = 0.01656569
Iteration 104, loss = 0.01629480
Iteration 105, loss = 0.01605071
Iteration 106, loss = 0.01579122
Iteration 107, loss = 0.01556027
Iteration 108, loss = 0.01533106
Iteration 109, loss = 0.01511467
Iteration 110, loss = 0.01487359
Iteration 111, loss = 0.01470700
Iteration 112, loss = 0.01447243
Iteration 113, loss = 0.01427652
Iteration 114, loss = 0.01409114
Iteration 115, loss = 0.01392780
Iteration 116, loss = 0.01374891
Iteration 117, loss = 0.01355656
Iteration 118, loss = 0.01338925
Iteration 119, loss = 0.01322810
Iteration 120, loss = 0.01307896
Iteration 121, loss = 0.01291850
Iteration 122, loss = 0.01275836
Iteration 123, loss = 0.01261556
Iteration 124, loss = 0.01247166
Iteration 125, loss = 0.01232804
Iteration 126, loss = 0.01219971
Iteration 127, loss = 0.01205288
Iteration 128, loss = 0.01192875
Iteration 129, loss = 0.01178450
Iteration 130, loss = 0.01166632
Iteration 131, loss = 0.01152527
Iteration 132, loss = 0.01140770
Iteration 133, loss = 0.01129217
Iteration 134, loss = 0.01118570
Iteration 135, loss = 0.01106367
Iteration 136, loss = 0.01095620
Iteration 137, loss = 0.01086956
Iteration 138, loss = 0.01075111
Iteration 139, loss = 0.01065743
Iteration 140, loss = 0.01057006
Iteration 141, loss = 0.01044494
Iteration 142, loss = 0.01035054
Iteration 143, loss = 0.01025593
Iteration 144, loss = 0.01016914
Iteration 145, loss = 0.01007438
Iteration 146, loss = 0.00999252
Iteration 147, loss = 0.00994519
Iteration 148, loss = 0.00982145
Iteration 149, loss = 0.00973662
Iteration 150, loss = 0.00965644
Iteration 151, loss = 0.00956710
Iteration 152, loss = 0.00949164
Iteration 153, loss = 0.00941610
Iteration 154, loss = 0.00934441
Iteration 155, loss = 0.00927133
Iteration 156, loss = 0.00920670
Iteration 157, loss = 0.00917013
Iteration 158, loss = 0.00907460
Iteration 159, loss = 0.00902823
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72558322
Iteration 2, loss = 0.69154216
Iteration 3, loss = 0.65393360
Iteration 4, loss = 0.60955065
Iteration 5, loss = 0.56064645
Iteration 6, loss = 0.50806401
Iteration 7, loss = 0.45536033
Iteration 8, loss = 0.40409691
Iteration 9, loss = 0.35644097
Iteration 10, loss = 0.31313729
Iteration 11, loss = 0.27492235
Iteration 12, loss = 0.24134801
Iteration 13, loss = 0.21259580
Iteration 14, loss = 0.18774856
Iteration 15, loss = 0.16671006
Iteration 16, loss = 0.14872616
Iteration 17, loss = 0.13343130
Iteration 18, loss = 0.12027929
Iteration 19, loss = 0.10908466
Iteration 20, loss = 0.09952790
Iteration 21, loss = 0.09126844
Iteration 22, loss = 0.08414191
Iteration 23, loss = 0.07792486
Iteration 24, loss = 0.07235581
Iteration 25, loss = 0.06759644
Iteration 26, loss = 0.06324785
Iteration 27, loss = 0.05948477
Iteration 28, loss = 0.05619628
Iteration 29, loss = 0.05329823
Iteration 30, loss = 0.05064172
Iteration 31, loss = 0.04830131
Iteration 32, loss = 0.04615641
Iteration 33, loss = 0.04417313
Iteration 34, loss = 0.04239464
Iteration 35, loss = 0.04055708
Iteration 36, loss = 0.03896383
Iteration 37, loss = 0.03746249
Iteration 38, loss = 0.03620550
Iteration 39, loss = 0.03497336
Iteration 40, loss = 0.03382336
Iteration 41, loss = 0.03281549
Iteration 42, loss = 0.03191185
Iteration 43, loss = 0.03099274
Iteration 44, loss = 0.03014673
Iteration 45, loss = 0.02932389
Iteration 46, loss = 0.02869449
Iteration 47, loss = 0.02796060
Iteration 48, loss = 0.02742243
Iteration 49, loss = 0.02680354
Iteration 50, loss = 0.02645906
Iteration 51, loss = 0.02599104
Iteration 52, loss = 0.02543475
Iteration 53, loss = 0.02487778
Iteration 54, loss = 0.02453591
Iteration 55, loss = 0.02401547
Iteration 56, loss = 0.02367969
Iteration 57, loss = 0.02323903
Iteration 58, loss = 0.02290892
Iteration 59, loss = 0.02253925
Iteration 60, loss = 0.02221542
Iteration 61, loss = 0.02179661
Iteration 62, loss = 0.02151100
Iteration 63, loss = 0.02131142
Iteration 64, loss = 0.02105595
Iteration 65, loss = 0.02084842
Iteration 66, loss = 0.02067934
Iteration 67, loss = 0.02043293
Iteration 68, loss = 0.02023340
Iteration 69, loss = 0.02004043
Iteration 70, loss = 0.01972200
Iteration 71, loss = 0.01959461
Iteration 72, loss = 0.01947168
Iteration 73, loss = 0.01938615
Iteration 74, loss = 0.01938143
Iteration 75, loss = 0.01912102
Iteration 76, loss = 0.01894498
Iteration 77, loss = 0.01876105
Iteration 78, loss = 0.01859905
Iteration 79, loss = 0.01851015
Iteration 80, loss = 0.01839132
Iteration 81, loss = 0.01833750
Iteration 82, loss = 0.01823688
Iteration 83, loss = 0.01809749
Iteration 84, loss = 0.01784835
Iteration 85, loss = 0.01775455
Iteration 86, loss = 0.01749927
Iteration 87, loss = 0.01744431
Iteration 88, loss = 0.01743988
Iteration 89, loss = 0.01748794
Iteration 90, loss = 0.01739301
Iteration 91, loss = 0.01751175
Iteration 92, loss = 0.01750909
Iteration 93, loss = 0.01738406
Iteration 94, loss = 0.01718024
Iteration 95, loss = 0.01712232
Iteration 96, loss = 0.01701432
Iteration 97, loss = 0.01689422
Iteration 98, loss = 0.01667994
Iteration 99, loss = 0.01660442
Iteration 100, loss = 0.01673607
Iteration 101, loss = 0.01703824
Iteration 102, loss = 0.01688313
Iteration 103, loss = 0.01668519
Iteration 104, loss = 0.01659651
Iteration 105, loss = 0.01676436
Iteration 106, loss = 0.01659653
Iteration 107, loss = 0.01655020
Iteration 108, loss = 0.01616603
Iteration 109, loss = 0.01606236
Iteration 110, loss = 0.01600022
Iteration 111, loss = 0.01595107
Iteration 112, loss = 0.01582869
Iteration 113, loss = 0.01577151
Iteration 114, loss = 0.01579875
Iteration 115, loss = 0.01580766
Iteration 116, loss = 0.01569960
Iteration 117, loss = 0.01554206
Iteration 118, loss = 0.01572387
Iteration 119, loss = 0.01574454
Iteration 120, loss = 0.01570375
Iteration 121, loss = 0.01564594
Iteration 122, loss = 0.01569098
Iteration 123, loss = 0.01581958
Iteration 124, loss = 0.01576469
Iteration 125, loss = 0.01562528
Iteration 126, loss = 0.01553527
Iteration 127, loss = 0.01542728
Iteration 128, loss = 0.01537242
Iteration 129, loss = 0.01530345
Iteration 130, loss = 0.01529725
Iteration 131, loss = 0.01528554
Iteration 132, loss = 0.01518781
Iteration 133, loss = 0.01513188
Iteration 134, loss = 0.01515057
Iteration 135, loss = 0.01524004
Iteration 136, loss = 0.01517219
Iteration 137, loss = 0.01518656
Iteration 138, loss = 0.01527261
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67767529
Iteration 2, loss = 0.65328857
Iteration 3, loss = 0.63079162
Iteration 4, loss = 0.60558760
Iteration 5, loss = 0.57641214
Iteration 6, loss = 0.54420070
Iteration 7, loss = 0.50911546
Iteration 8, loss = 0.47269139
Iteration 9, loss = 0.43569680
Iteration 10, loss = 0.39937618
Iteration 11, loss = 0.36487300
Iteration 12, loss = 0.33238727
Iteration 13, loss = 0.30221244
Iteration 14, loss = 0.27472301
Iteration 15, loss = 0.24985143
Iteration 16, loss = 0.22738302
Iteration 17, loss = 0.20709311
Iteration 18, loss = 0.18909655
Iteration 19, loss = 0.17288118
Iteration 20, loss = 0.15858404
Iteration 21, loss = 0.14582390
Iteration 22, loss = 0.13440049
Iteration 23, loss = 0.12432683
Iteration 24, loss = 0.11523536
Iteration 25, loss = 0.10716904
Iteration 26, loss = 0.09990585
Iteration 27, loss = 0.09348118
Iteration 28, loss = 0.08758946
Iteration 29, loss = 0.08232256
Iteration 30, loss = 0.07761775
Iteration 31, loss = 0.07324880
Iteration 32, loss = 0.06928685
Iteration 33, loss = 0.06572816
Iteration 34, loss = 0.06250642
Iteration 35, loss = 0.05950172
Iteration 36, loss = 0.05680139
Iteration 37, loss = 0.05427210
Iteration 38, loss = 0.05196725
Iteration 39, loss = 0.04978276
Iteration 40, loss = 0.04783877
Iteration 41, loss = 0.04598372
Iteration 42, loss = 0.04434811
Iteration 43, loss = 0.04272361
Iteration 44, loss = 0.04129251
Iteration 45, loss = 0.03998796
Iteration 46, loss = 0.03867878
Iteration 47, loss = 0.03746337
Iteration 48, loss = 0.03640397
Iteration 49, loss = 0.03535199
Iteration 50, loss = 0.03435181
Iteration 51, loss = 0.03342613
Iteration 52, loss = 0.03261382
Iteration 53, loss = 0.03180141
Iteration 54, loss = 0.03097641
Iteration 55, loss = 0.03025034
Iteration 56, loss = 0.02970486
Iteration 57, loss = 0.02897204
Iteration 58, loss = 0.02831281
Iteration 59, loss = 0.02780073
Iteration 60, loss = 0.02721501
Iteration 61, loss = 0.02667337
Iteration 62, loss = 0.02616912
Iteration 63, loss = 0.02571488
Iteration 64, loss = 0.02532318
Iteration 65, loss = 0.02483336
Iteration 66, loss = 0.02446302
Iteration 67, loss = 0.02403003
Iteration 68, loss = 0.02368166
Iteration 69, loss = 0.02333055
Iteration 70, loss = 0.02295645
Iteration 71, loss = 0.02264145
Iteration 72, loss = 0.02233578
Iteration 73, loss = 0.02202836
Iteration 74, loss = 0.02176116
Iteration 75, loss = 0.02149892
Iteration 76, loss = 0.02121459
Iteration 77, loss = 0.02096980
Iteration 78, loss = 0.02071039
Iteration 79, loss = 0.02047961
Iteration 80, loss = 0.02023313
Iteration 81, loss = 0.02004655
Iteration 82, loss = 0.01982683
Iteration 83, loss = 0.01963864
Iteration 84, loss = 0.01941494
Iteration 85, loss = 0.01925190
Iteration 86, loss = 0.01910529
Iteration 87, loss = 0.01889709
Iteration 88, loss = 0.01875395
Iteration 89, loss = 0.01857559
Iteration 90, loss = 0.01840645
Iteration 91, loss = 0.01833991
Iteration 92, loss = 0.01811849
Iteration 93, loss = 0.01801501
Iteration 94, loss = 0.01785371
Iteration 95, loss = 0.01770480
Iteration 96, loss = 0.01755724
Iteration 97, loss = 0.01746333
Iteration 98, loss = 0.01735242
Iteration 99, loss = 0.01726441
Iteration 100, loss = 0.01713360
Iteration 101, loss = 0.01702957
Iteration 102, loss = 0.01694892
Iteration 103, loss = 0.01683689
Iteration 104, loss = 0.01669805
Iteration 105, loss = 0.01661280
Iteration 106, loss = 0.01653464
Iteration 107, loss = 0.01643338
Iteration 108, loss = 0.01638079
Iteration 109, loss = 0.01624365
Iteration 110, loss = 0.01622301
Iteration 111, loss = 0.01614558
Iteration 112, loss = 0.01604545
Iteration 113, loss = 0.01593075
Iteration 114, loss = 0.01585458
Iteration 115, loss = 0.01581400
Iteration 116, loss = 0.01573423
Iteration 117, loss = 0.01565444
Iteration 118, loss = 0.01561704
Iteration 119, loss = 0.01555562
Iteration 120, loss = 0.01544073
Iteration 121, loss = 0.01543612
Iteration 122, loss = 0.01539197
Iteration 123, loss = 0.01529112
Iteration 124, loss = 0.01522943
Iteration 125, loss = 0.01525078
Iteration 126, loss = 0.01512805
Iteration 127, loss = 0.01507498
Iteration 128, loss = 0.01498415
Iteration 129, loss = 0.01498905
Iteration 130, loss = 0.01493178
Iteration 131, loss = 0.01484689
Iteration 132, loss = 0.01481041
Iteration 133, loss = 0.01486074
Iteration 134, loss = 0.01478537
Iteration 135, loss = 0.01468795
Iteration 136, loss = 0.01461700
Iteration 137, loss = 0.01459611
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67490049
Iteration 2, loss = 0.61635467
Iteration 3, loss = 0.53644789
Iteration 4, loss = 0.44805250
Iteration 5, loss = 0.36468466
Iteration 6, loss = 0.29250107
Iteration 7, loss = 0.23357990
Iteration 8, loss = 0.18829510
Iteration 9, loss = 0.15407123
Iteration 10, loss = 0.12805597
Iteration 11, loss = 0.10862476
Iteration 12, loss = 0.09372592
Iteration 13, loss = 0.08216545
Iteration 14, loss = 0.07280969
Iteration 15, loss = 0.06561559
Iteration 16, loss = 0.05956272
Iteration 17, loss = 0.05463741
Iteration 18, loss = 0.05077977
Iteration 19, loss = 0.04715076
Iteration 20, loss = 0.04411746
Iteration 21, loss = 0.04160508
Iteration 22, loss = 0.03951459
Iteration 23, loss = 0.03753048
Iteration 24, loss = 0.03601142
Iteration 25, loss = 0.03436545
Iteration 26, loss = 0.03320448
Iteration 27, loss = 0.03202449
Iteration 28, loss = 0.03107476
Iteration 29, loss = 0.03018138
Iteration 30, loss = 0.02922335
Iteration 31, loss = 0.02894285
Iteration 32, loss = 0.02806757
Iteration 33, loss = 0.02745342
Iteration 34, loss = 0.02682839
Iteration 35, loss = 0.02621003
Iteration 36, loss = 0.02578150
Iteration 37, loss = 0.02564136
Iteration 38, loss = 0.02527088
Iteration 39, loss = 0.02464921
Iteration 40, loss = 0.02447182
Iteration 41, loss = 0.02415466
Iteration 42, loss = 0.02391207
Iteration 43, loss = 0.02353882
Iteration 44, loss = 0.02338183
Iteration 45, loss = 0.02308490
Iteration 46, loss = 0.02305941
Iteration 47, loss = 0.02283155
Iteration 48, loss = 0.02240182
Iteration 49, loss = 0.02243740
Iteration 50, loss = 0.02221102
Iteration 51, loss = 0.02207813
Iteration 52, loss = 0.02194976
Iteration 53, loss = 0.02194898
Iteration 54, loss = 0.02163478
Iteration 55, loss = 0.02168380
Iteration 56, loss = 0.02160852
Iteration 57, loss = 0.02142784
Iteration 58, loss = 0.02138969
Iteration 59, loss = 0.02105529
Iteration 60, loss = 0.02098267
Iteration 61, loss = 0.02096577
Iteration 62, loss = 0.02094140
Iteration 63, loss = 0.02081260
Iteration 64, loss = 0.02066072
Iteration 65, loss = 0.02071192
Iteration 66, loss = 0.02045881
Iteration 67, loss = 0.02044557
Iteration 68, loss = 0.02033594
Iteration 69, loss = 0.02052759
Iteration 70, loss = 0.02045911
Iteration 71, loss = 0.02025325
Iteration 72, loss = 0.02039649
Iteration 73, loss = 0.02031410
Iteration 74, loss = 0.01998936
Iteration 75, loss = 0.02025027
Iteration 76, loss = 0.02023529
Iteration 77, loss = 0.02003004
Iteration 78, loss = 0.02022304
Iteration 79, loss = 0.01993291
Iteration 80, loss = 0.01983340
Iteration 81, loss = 0.01997072
Iteration 82, loss = 0.01988955
Iteration 83, loss = 0.01980111
Iteration 84, loss = 0.01974950
Iteration 85, loss = 0.01978396
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71788687
Iteration 2, loss = 0.65834047
Iteration 3, loss = 0.57468564
Iteration 4, loss = 0.47148689
Iteration 5, loss = 0.37530737
Iteration 6, loss = 0.29229131
Iteration 7, loss = 0.22706204
Iteration 8, loss = 0.17934262
Iteration 9, loss = 0.14458009
Iteration 10, loss = 0.11961553
Iteration 11, loss = 0.10114938
Iteration 12, loss = 0.08728436
Iteration 13, loss = 0.07655655
Iteration 14, loss = 0.06835238
Iteration 15, loss = 0.06157581
Iteration 16, loss = 0.05655744
Iteration 17, loss = 0.05218474
Iteration 18, loss = 0.04836086
Iteration 19, loss = 0.04537509
Iteration 20, loss = 0.04282336
Iteration 21, loss = 0.04143021
Iteration 22, loss = 0.03948926
Iteration 23, loss = 0.03782031
Iteration 24, loss = 0.03634312
Iteration 25, loss = 0.03500554
Iteration 26, loss = 0.03362060
Iteration 27, loss = 0.03259193
Iteration 28, loss = 0.03164858
Iteration 29, loss = 0.03125090
Iteration 30, loss = 0.03061116
Iteration 31, loss = 0.02966197
Iteration 32, loss = 0.02922358
Iteration 33, loss = 0.02848486
Iteration 34, loss = 0.02791121
Iteration 35, loss = 0.02738257
Iteration 36, loss = 0.02683024
Iteration 37, loss = 0.02663351
Iteration 38, loss = 0.02636782
Iteration 39, loss = 0.02646952
Iteration 40, loss = 0.02600496
Iteration 41, loss = 0.02556415
Iteration 42, loss = 0.02524874
Iteration 43, loss = 0.02507012
Iteration 44, loss = 0.02486958
Iteration 45, loss = 0.02456946
Iteration 46, loss = 0.02442262
Iteration 47, loss = 0.02414493
Iteration 48, loss = 0.02383325
Iteration 49, loss = 0.02380427
Iteration 50, loss = 0.02346644
Iteration 51, loss = 0.02326588
Iteration 52, loss = 0.02317303
Iteration 53, loss = 0.02435125
Iteration 54, loss = 0.02435643
Iteration 55, loss = 0.02367391
Iteration 56, loss = 0.02296719
Iteration 57, loss = 0.02286826
Iteration 58, loss = 0.02352138
Iteration 59, loss = 0.02395856
Iteration 60, loss = 0.02340706
Iteration 61, loss = 0.02326835
Iteration 62, loss = 0.02313639
Iteration 63, loss = 0.02284617
Iteration 64, loss = 0.02298091
Iteration 65, loss = 0.02272504
Iteration 66, loss = 0.02250016
Iteration 67, loss = 0.02235036
Iteration 68, loss = 0.02236095
Iteration 69, loss = 0.02221831
Iteration 70, loss = 0.02210067
Iteration 71, loss = 0.02201373
Iteration 72, loss = 0.02179440
Iteration 73, loss = 0.02186724
Iteration 74, loss = 0.02164830
Iteration 75, loss = 0.02195647
Iteration 76, loss = 0.02152263
Iteration 77, loss = 0.02150877
Iteration 78, loss = 0.02145963
Iteration 79, loss = 0.02148961
Iteration 80, loss = 0.02135069
Iteration 81, loss = 0.02135996
Iteration 82, loss = 0.02122845
Iteration 83, loss = 0.02117168
Iteration 84, loss = 0.02103820
Iteration 85, loss = 0.02107043
Iteration 86, loss = 0.02097443
Iteration 87, loss = 0.02068397
Iteration 88, loss = 0.02091070
Iteration 89, loss = 0.02097793
Iteration 90, loss = 0.02057903
Iteration 91, loss = 0.02055166
Iteration 92, loss = 0.02066944
Iteration 93, loss = 0.02056305
Iteration 94, loss = 0.02062634
Iteration 95, loss = 0.02043073
Iteration 96, loss = 0.02057511
Iteration 97, loss = 0.02115697
Iteration 98, loss = 0.02088728
Iteration 99, loss = 0.02097650
Iteration 100, loss = 0.02079430
Iteration 101, loss = 0.02096067
Iteration 102, loss = 0.02104193
Iteration 103, loss = 0.02082204
Iteration 104, loss = 0.02068776
Iteration 105, loss = 0.02068173
Iteration 106, loss = 0.02048429
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71996338
Iteration 2, loss = 0.65753534
Iteration 3, loss = 0.57344644
Iteration 4, loss = 0.47145199
Iteration 5, loss = 0.37301394
Iteration 6, loss = 0.29190740
Iteration 7, loss = 0.22862512
Iteration 8, loss = 0.18172839
Iteration 9, loss = 0.14738183
Iteration 10, loss = 0.12253065
Iteration 11, loss = 0.10406646
Iteration 12, loss = 0.09012108
Iteration 13, loss = 0.07961010
Iteration 14, loss = 0.07139655
Iteration 15, loss = 0.06529188
Iteration 16, loss = 0.05977534
Iteration 17, loss = 0.05521786
Iteration 18, loss = 0.05162223
Iteration 19, loss = 0.04871551
Iteration 20, loss = 0.04619483
Iteration 21, loss = 0.04395079
Iteration 22, loss = 0.04200022
Iteration 23, loss = 0.04032256
Iteration 24, loss = 0.03918568
Iteration 25, loss = 0.03861528
Iteration 26, loss = 0.03743558
Iteration 27, loss = 0.03620824
Iteration 28, loss = 0.03527277
Iteration 29, loss = 0.03471030
Iteration 30, loss = 0.03359297
Iteration 31, loss = 0.03302192
Iteration 32, loss = 0.03243385
Iteration 33, loss = 0.03198562
Iteration 34, loss = 0.03146132
Iteration 35, loss = 0.03097064
Iteration 36, loss = 0.03056970
Iteration 37, loss = 0.03028642
Iteration 38, loss = 0.03002673
Iteration 39, loss = 0.02941985
Iteration 40, loss = 0.02920140
Iteration 41, loss = 0.02891426
Iteration 42, loss = 0.02856672
Iteration 43, loss = 0.02844272
Iteration 44, loss = 0.02794535
Iteration 45, loss = 0.02789597
Iteration 46, loss = 0.02834773
Iteration 47, loss = 0.02847669
Iteration 48, loss = 0.02821738
Iteration 49, loss = 0.02800688
Iteration 50, loss = 0.02767825
Iteration 51, loss = 0.02733369
Iteration 52, loss = 0.02761266
Iteration 53, loss = 0.02743512
Iteration 54, loss = 0.02714684
Iteration 55, loss = 0.02695008
Iteration 56, loss = 0.02662841
Iteration 57, loss = 0.02642839
Iteration 58, loss = 0.02646782
Iteration 59, loss = 0.02612998
Iteration 60, loss = 0.02634249
Iteration 61, loss = 0.02722394
Iteration 62, loss = 0.02715717
Iteration 63, loss = 0.02672920
Iteration 64, loss = 0.02672345
Iteration 65, loss = 0.02648040
Iteration 66, loss = 0.02621210
Iteration 67, loss = 0.02596571
Iteration 68, loss = 0.02662137
Iteration 69, loss = 0.02678093
Iteration 70, loss = 0.02608664
Iteration 71, loss = 0.02582613
Iteration 72, loss = 0.02566786
Iteration 73, loss = 0.02540838
Iteration 74, loss = 0.02534932
Iteration 75, loss = 0.02563385
Iteration 76, loss = 0.02583926
Iteration 77, loss = 0.02551547
Iteration 78, loss = 0.02533784
Iteration 79, loss = 0.02528867
Iteration 80, loss = 0.02518213
Iteration 81, loss = 0.02499488
Iteration 82, loss = 0.02488484
Iteration 83, loss = 0.02509269
Iteration 84, loss = 0.02453109
Iteration 85, loss = 0.02505202
Iteration 86, loss = 0.02438993
Iteration 87, loss = 0.02437047
Iteration 88, loss = 0.02435253
Iteration 89, loss = 0.02437564
Iteration 90, loss = 0.02427611
Iteration 91, loss = 0.02422414
Iteration 92, loss = 0.02437448
Iteration 93, loss = 0.02440935
Iteration 94, loss = 0.02411220
Iteration 95, loss = 0.02405361
Iteration 96, loss = 0.02393699
Iteration 97, loss = 0.02379655
Iteration 98, loss = 0.02393480
Iteration 99, loss = 0.02362579
Iteration 100, loss = 0.02382228
Iteration 101, loss = 0.02381392
Iteration 102, loss = 0.02383860
Iteration 103, loss = 0.02377234
Iteration 104, loss = 0.02378645
Iteration 105, loss = 0.02380473
Iteration 106, loss = 0.02452513
Iteration 107, loss = 0.02496981
Iteration 108, loss = 0.02593140
Iteration 109, loss = 0.02512406
Iteration 110, loss = 0.02496297
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68121473
Iteration 2, loss = 0.60396773
Iteration 3, loss = 0.48985034
Iteration 4, loss = 0.37544482
Iteration 5, loss = 0.28198057
Iteration 6, loss = 0.21317216
Iteration 7, loss = 0.16489450
Iteration 8, loss = 0.13188143
Iteration 9, loss = 0.10849903
Iteration 10, loss = 0.09157957
Iteration 11, loss = 0.07941752
Iteration 12, loss = 0.07012897
Iteration 13, loss = 0.06324367
Iteration 14, loss = 0.05714970
Iteration 15, loss = 0.05274277
Iteration 16, loss = 0.04884358
Iteration 17, loss = 0.04567308
Iteration 18, loss = 0.04311494
Iteration 19, loss = 0.04109327
Iteration 20, loss = 0.03936790
Iteration 21, loss = 0.03787991
Iteration 22, loss = 0.03646691
Iteration 23, loss = 0.03531517
Iteration 24, loss = 0.03426756
Iteration 25, loss = 0.03356153
Iteration 26, loss = 0.03270015
Iteration 27, loss = 0.03177429
Iteration 28, loss = 0.03099979
Iteration 29, loss = 0.03039614
Iteration 30, loss = 0.03008102
Iteration 31, loss = 0.02937623
Iteration 32, loss = 0.02907501
Iteration 33, loss = 0.02906027
Iteration 34, loss = 0.02827608
Iteration 35, loss = 0.02833474
Iteration 36, loss = 0.02773834
Iteration 37, loss = 0.02767299
Iteration 38, loss = 0.02739531
Iteration 39, loss = 0.02713881
Iteration 40, loss = 0.02693179
Iteration 41, loss = 0.02670900
Iteration 42, loss = 0.02680636
Iteration 43, loss = 0.02628650
Iteration 44, loss = 0.02643971
Iteration 45, loss = 0.02611369
Iteration 46, loss = 0.02621826
Iteration 47, loss = 0.02594823
Iteration 48, loss = 0.02604745
Iteration 49, loss = 0.02569822
Iteration 50, loss = 0.02570631
Iteration 51, loss = 0.02568778
Iteration 52, loss = 0.02569406
Iteration 53, loss = 0.02544013
Iteration 54, loss = 0.02534658
Iteration 55, loss = 0.02547375
Iteration 56, loss = 0.02526828
Iteration 57, loss = 0.02508828
Iteration 58, loss = 0.02486194
Iteration 59, loss = 0.02499614
Iteration 60, loss = 0.02489140
Iteration 61, loss = 0.02474873
Iteration 62, loss = 0.02467115
Iteration 63, loss = 0.02469079
Iteration 64, loss = 0.02487932
Iteration 65, loss = 0.02472980
Iteration 66, loss = 0.02460344
Iteration 67, loss = 0.02453219
Iteration 68, loss = 0.02433212
Iteration 69, loss = 0.02427611
Iteration 70, loss = 0.02477911
Iteration 71, loss = 0.02472978
Iteration 72, loss = 0.02471423
Iteration 73, loss = 0.02432977
Iteration 74, loss = 0.02443794
Iteration 75, loss = 0.02427472
Iteration 76, loss = 0.02464284
Iteration 77, loss = 0.02453383
Iteration 78, loss = 0.02424356
Iteration 79, loss = 0.02445233
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68210994
Iteration 2, loss = 0.62051452
Iteration 3, loss = 0.51986582
Iteration 4, loss = 0.41123736
Iteration 5, loss = 0.31604008
Iteration 6, loss = 0.24235215
Iteration 7, loss = 0.18764690
Iteration 8, loss = 0.14934481
Iteration 9, loss = 0.12207095
Iteration 10, loss = 0.10205404
Iteration 11, loss = 0.08753800
Iteration 12, loss = 0.07654732
Iteration 13, loss = 0.06793521
Iteration 14, loss = 0.06150817
Iteration 15, loss = 0.05621168
Iteration 16, loss = 0.05204538
Iteration 17, loss = 0.04843022
Iteration 18, loss = 0.04538297
Iteration 19, loss = 0.04263216
Iteration 20, loss = 0.04049679
Iteration 21, loss = 0.03885125
Iteration 22, loss = 0.03724448
Iteration 23, loss = 0.03582749
Iteration 24, loss = 0.03467251
Iteration 25, loss = 0.03357532
Iteration 26, loss = 0.03286806
Iteration 27, loss = 0.03188748
Iteration 28, loss = 0.03110659
Iteration 29, loss = 0.03021864
Iteration 30, loss = 0.02974234
Iteration 31, loss = 0.02975426
Iteration 32, loss = 0.02878490
Iteration 33, loss = 0.02857298
Iteration 34, loss = 0.02816196
Iteration 35, loss = 0.02784741
Iteration 36, loss = 0.02762362
Iteration 37, loss = 0.02741117
Iteration 38, loss = 0.02702248
Iteration 39, loss = 0.02691287
Iteration 40, loss = 0.02630979
Iteration 41, loss = 0.02629815
Iteration 42, loss = 0.02600189
Iteration 43, loss = 0.02597293
Iteration 44, loss = 0.02588607
Iteration 45, loss = 0.02580231
Iteration 46, loss = 0.02562399
Iteration 47, loss = 0.02537592
Iteration 48, loss = 0.02537738
Iteration 49, loss = 0.02522702
Iteration 50, loss = 0.02498772
Iteration 51, loss = 0.02488587
Iteration 52, loss = 0.02496402
Iteration 53, loss = 0.02453930
Iteration 54, loss = 0.02475144
Iteration 55, loss = 0.02459606
Iteration 56, loss = 0.02427750
Iteration 57, loss = 0.02414324
Iteration 58, loss = 0.02422965
Iteration 59, loss = 0.02434026
Iteration 60, loss = 0.02449870
Iteration 61, loss = 0.02403513
Iteration 62, loss = 0.02397096
Iteration 63, loss = 0.02390718
Iteration 64, loss = 0.02363504
Iteration 65, loss = 0.02396124
Iteration 66, loss = 0.02388939
Iteration 67, loss = 0.02388809
Iteration 68, loss = 0.02395780
Iteration 69, loss = 0.02372152
Iteration 70, loss = 0.02358196
Iteration 71, loss = 0.02342145
Iteration 72, loss = 0.02370917
Iteration 73, loss = 0.02346815
Iteration 74, loss = 0.02373907
Iteration 75, loss = 0.02383310
Iteration 76, loss = 0.02367611
Iteration 77, loss = 0.02338718
Iteration 78, loss = 0.02371779
Iteration 79, loss = 0.02370281
Iteration 80, loss = 0.02327381
Iteration 81, loss = 0.02350034
Iteration 82, loss = 0.02328370
Iteration 83, loss = 0.02352782
Iteration 84, loss = 0.02370998
Iteration 85, loss = 0.02387256
Iteration 86, loss = 0.02313836
Iteration 87, loss = 0.02302913
Iteration 88, loss = 0.02358475
Iteration 89, loss = 0.02315442
Iteration 90, loss = 0.02339044
Iteration 91, loss = 0.02326938
Iteration 92, loss = 0.02337869
Iteration 93, loss = 0.02302586
Iteration 94, loss = 0.02278624
Iteration 95, loss = 0.02299289
Iteration 96, loss = 0.02308465
Iteration 97, loss = 0.02300405
Iteration 98, loss = 0.02299246
Iteration 99, loss = 0.02280480
Iteration 100, loss = 0.02306903
Iteration 101, loss = 0.02283272
Iteration 102, loss = 0.02290394
Iteration 103, loss = 0.02290453
Iteration 104, loss = 0.02289436
Iteration 105, loss = 0.02308632
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70756943
Iteration 2, loss = 0.65322021
Iteration 3, loss = 0.56952469
Iteration 4, loss = 0.46352594
Iteration 5, loss = 0.36238482
Iteration 6, loss = 0.27740086
Iteration 7, loss = 0.21337481
Iteration 8, loss = 0.16707375
Iteration 9, loss = 0.13395731
Iteration 10, loss = 0.11017724
Iteration 11, loss = 0.09278085
Iteration 12, loss = 0.07978221
Iteration 13, loss = 0.07006249
Iteration 14, loss = 0.06246953
Iteration 15, loss = 0.05629444
Iteration 16, loss = 0.05164948
Iteration 17, loss = 0.04753968
Iteration 18, loss = 0.04436370
Iteration 19, loss = 0.04146906
Iteration 20, loss = 0.03906971
Iteration 21, loss = 0.03720230
Iteration 22, loss = 0.03525617
Iteration 23, loss = 0.03371286
Iteration 24, loss = 0.03265753
Iteration 25, loss = 0.03125507
Iteration 26, loss = 0.03018329
Iteration 27, loss = 0.02949008
Iteration 28, loss = 0.02860900
Iteration 29, loss = 0.02782069
Iteration 30, loss = 0.02745780
Iteration 31, loss = 0.02697066
Iteration 32, loss = 0.02591880
Iteration 33, loss = 0.02570767
Iteration 34, loss = 0.02516504
Iteration 35, loss = 0.02483168
Iteration 36, loss = 0.02444480
Iteration 37, loss = 0.02401592
Iteration 38, loss = 0.02393413
Iteration 39, loss = 0.02356589
Iteration 40, loss = 0.02320349
Iteration 41, loss = 0.02307238
Iteration 42, loss = 0.02271997
Iteration 43, loss = 0.02286024
Iteration 44, loss = 0.02243034
Iteration 45, loss = 0.02216828
Iteration 46, loss = 0.02222299
Iteration 47, loss = 0.02201040
Iteration 48, loss = 0.02166387
Iteration 49, loss = 0.02179961
Iteration 50, loss = 0.02125223
Iteration 51, loss = 0.02160281
Iteration 52, loss = 0.02130497
Iteration 53, loss = 0.02138428
Iteration 54, loss = 0.02129661
Iteration 55, loss = 0.02100785
Iteration 56, loss = 0.02081169
Iteration 57, loss = 0.02102154
Iteration 58, loss = 0.02094066
Iteration 59, loss = 0.02076664
Iteration 60, loss = 0.02072454
Iteration 61, loss = 0.02068831
Iteration 62, loss = 0.02070953
Iteration 63, loss = 0.02043025
Iteration 64, loss = 0.02052040
Iteration 65, loss = 0.02052642
Iteration 66, loss = 0.02043972
Iteration 67, loss = 0.02025964
Iteration 68, loss = 0.02016748
Iteration 69, loss = 0.02001368
Iteration 70, loss = 0.02011624
Iteration 71, loss = 0.02017741
Iteration 72, loss = 0.02017489
Iteration 73, loss = 0.02001072
Iteration 74, loss = 0.01998710
Iteration 75, loss = 0.02005772
Iteration 76, loss = 0.01997752
Iteration 77, loss = 0.02029949
Iteration 78, loss = 0.02006655
Iteration 79, loss = 0.02008225
Iteration 80, loss = 0.01974080
Iteration 81, loss = 0.01968310
Iteration 82, loss = 0.02004089
Iteration 83, loss = 0.01978712
Iteration 84, loss = 0.01985038
Iteration 85, loss = 0.01971708
Iteration 86, loss = 0.01957306
Iteration 87, loss = 0.01952802
Iteration 88, loss = 0.01960264
Iteration 89, loss = 0.01962008
Iteration 90, loss = 0.01944180
Iteration 91, loss = 0.01926606
Iteration 92, loss = 0.01960799
Iteration 93, loss = 0.01944697
Iteration 94, loss = 0.01973407
Iteration 95, loss = 0.01978043
Iteration 96, loss = 0.01967933
Iteration 97, loss = 0.01935788
Iteration 98, loss = 0.01939733
Iteration 99, loss = 0.01923115
Iteration 100, loss = 0.01932339
Iteration 101, loss = 0.01955466
Iteration 102, loss = 0.01938198
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69308831
Iteration 2, loss = 0.66350541
Iteration 3, loss = 0.63022984
Iteration 4, loss = 0.59002506
Iteration 5, loss = 0.54340214
Iteration 6, loss = 0.49365351
Iteration 7, loss = 0.44379994
Iteration 8, loss = 0.39569588
Iteration 9, loss = 0.35066559
Iteration 10, loss = 0.30959293
Iteration 11, loss = 0.27287741
Iteration 12, loss = 0.24058484
Iteration 13, loss = 0.21254108
Iteration 14, loss = 0.18829341
Iteration 15, loss = 0.16748202
Iteration 16, loss = 0.14955027
Iteration 17, loss = 0.13435412
Iteration 18, loss = 0.12134139
Iteration 19, loss = 0.11003345
Iteration 20, loss = 0.10035183
Iteration 21, loss = 0.09206914
Iteration 22, loss = 0.08479992
Iteration 23, loss = 0.07853425
Iteration 24, loss = 0.07313796
Iteration 25, loss = 0.06830725
Iteration 26, loss = 0.06392929
Iteration 27, loss = 0.06008716
Iteration 28, loss = 0.05667002
Iteration 29, loss = 0.05357685
Iteration 30, loss = 0.05095272
Iteration 31, loss = 0.04847889
Iteration 32, loss = 0.04612288
Iteration 33, loss = 0.04415757
Iteration 34, loss = 0.04221358
Iteration 35, loss = 0.04046293
Iteration 36, loss = 0.03889993
Iteration 37, loss = 0.03736305
Iteration 38, loss = 0.03606499
Iteration 39, loss = 0.03494511
Iteration 40, loss = 0.03380387
Iteration 41, loss = 0.03278946
Iteration 42, loss = 0.03190680
Iteration 43, loss = 0.03097080
Iteration 44, loss = 0.03018281
Iteration 45, loss = 0.02925259
Iteration 46, loss = 0.02873215
Iteration 47, loss = 0.02818604
Iteration 48, loss = 0.02751875
Iteration 49, loss = 0.02709486
Iteration 50, loss = 0.02636195
Iteration 51, loss = 0.02603750
Iteration 52, loss = 0.02540323
Iteration 53, loss = 0.02486571
Iteration 54, loss = 0.02441855
Iteration 55, loss = 0.02386764
Iteration 56, loss = 0.02349247
Iteration 57, loss = 0.02315252
Iteration 58, loss = 0.02284332
Iteration 59, loss = 0.02264117
Iteration 60, loss = 0.02237913
Iteration 61, loss = 0.02208367
Iteration 62, loss = 0.02174038
Iteration 63, loss = 0.02139408
Iteration 64, loss = 0.02117793
Iteration 65, loss = 0.02092025
Iteration 66, loss = 0.02080410
Iteration 67, loss = 0.02049494
Iteration 68, loss = 0.02025593
Iteration 69, loss = 0.02026584
Iteration 70, loss = 0.01988097
Iteration 71, loss = 0.01982315
Iteration 72, loss = 0.01959352
Iteration 73, loss = 0.01939962
Iteration 74, loss = 0.01905948
Iteration 75, loss = 0.01928584
Iteration 76, loss = 0.01911585
Iteration 77, loss = 0.01901500
Iteration 78, loss = 0.01881103
Iteration 79, loss = 0.01861160
Iteration 80, loss = 0.01836123
Iteration 81, loss = 0.01821643
Iteration 82, loss = 0.01816978
Iteration 83, loss = 0.01791828
Iteration 84, loss = 0.01789604
Iteration 85, loss = 0.01798244
Iteration 86, loss = 0.01776758
Iteration 87, loss = 0.01764024
Iteration 88, loss = 0.01749094
Iteration 89, loss = 0.01751978
Iteration 90, loss = 0.01740536
Iteration 91, loss = 0.01746551
Iteration 92, loss = 0.01795927
Iteration 93, loss = 0.01784210
Iteration 94, loss = 0.01802897
Iteration 95, loss = 0.01743568
Iteration 96, loss = 0.01726773
Iteration 97, loss = 0.01704189
Iteration 98, loss = 0.01696254
Iteration 99, loss = 0.01679213
Iteration 100, loss = 0.01661726
Iteration 101, loss = 0.01655943
Iteration 102, loss = 0.01661283
Iteration 103, loss = 0.01661667
Iteration 104, loss = 0.01662482
Iteration 105, loss = 0.01613578
Iteration 106, loss = 0.01638191
Iteration 107, loss = 0.01631853
Iteration 108, loss = 0.01661352
Iteration 109, loss = 0.01655390
Iteration 110, loss = 0.01623620
Iteration 111, loss = 0.01604779
Iteration 112, loss = 0.01600239
Iteration 113, loss = 0.01606485
Iteration 114, loss = 0.01619038
Iteration 115, loss = 0.01594134
Iteration 116, loss = 0.01589372
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68617216
Iteration 2, loss = 0.62715150
Iteration 3, loss = 0.53427147
Iteration 4, loss = 0.42702153
Iteration 5, loss = 0.33004388
Iteration 6, loss = 0.25199323
Iteration 7, loss = 0.19423220
Iteration 8, loss = 0.15294621
Iteration 9, loss = 0.12365818
Iteration 10, loss = 0.10281925
Iteration 11, loss = 0.08754284
Iteration 12, loss = 0.07577255
Iteration 13, loss = 0.06704297
Iteration 14, loss = 0.06008115
Iteration 15, loss = 0.05467163
Iteration 16, loss = 0.04989980
Iteration 17, loss = 0.04645485
Iteration 18, loss = 0.04315255
Iteration 19, loss = 0.04095612
Iteration 20, loss = 0.03867507
Iteration 21, loss = 0.03652150
Iteration 22, loss = 0.03480787
Iteration 23, loss = 0.03359756
Iteration 24, loss = 0.03267491
Iteration 25, loss = 0.03131533
Iteration 26, loss = 0.03034767
Iteration 27, loss = 0.02958358
Iteration 28, loss = 0.02869267
Iteration 29, loss = 0.02811491
Iteration 30, loss = 0.02761721
Iteration 31, loss = 0.02702249
Iteration 32, loss = 0.02658488
Iteration 33, loss = 0.02593616
Iteration 34, loss = 0.02563710
Iteration 35, loss = 0.02554869
Iteration 36, loss = 0.02498778
Iteration 37, loss = 0.02476028
Iteration 38, loss = 0.02437510
Iteration 39, loss = 0.02448773
Iteration 40, loss = 0.02392210
Iteration 41, loss = 0.02369906
Iteration 42, loss = 0.02377967
Iteration 43, loss = 0.02318891
Iteration 44, loss = 0.02327600
Iteration 45, loss = 0.02309897
Iteration 46, loss = 0.02285050
Iteration 47, loss = 0.02252254
Iteration 48, loss = 0.02271732
Iteration 49, loss = 0.02240458
Iteration 50, loss = 0.02201510
Iteration 51, loss = 0.02216213
Iteration 52, loss = 0.02207112
Iteration 53, loss = 0.02200224
Iteration 54, loss = 0.02200957
Iteration 55, loss = 0.02190025
Iteration 56, loss = 0.02179884
Iteration 57, loss = 0.02160377
Iteration 58, loss = 0.02147708
Iteration 59, loss = 0.02160309
Iteration 60, loss = 0.02184456
Iteration 61, loss = 0.02150121
Iteration 62, loss = 0.02156205
Iteration 63, loss = 0.02111633
Iteration 64, loss = 0.02125599
Iteration 65, loss = 0.02122851
Iteration 66, loss = 0.02108277
Iteration 67, loss = 0.02100647
Iteration 68, loss = 0.02087872
Iteration 69, loss = 0.02096084
Iteration 70, loss = 0.02091065
Iteration 71, loss = 0.02082296
Iteration 72, loss = 0.02075357
Iteration 73, loss = 0.02103297
Iteration 74, loss = 0.02085197
Iteration 75, loss = 0.02113543
Iteration 76, loss = 0.02078569
Iteration 77, loss = 0.02099630
Iteration 78, loss = 0.02062269
Iteration 79, loss = 0.02041830
Iteration 80, loss = 0.02066333
Iteration 81, loss = 0.02046402
Iteration 82, loss = 0.02052564
Iteration 83, loss = 0.02050944
Iteration 84, loss = 0.02033518
Iteration 85, loss = 0.02050678
Iteration 86, loss = 0.02039386
Iteration 87, loss = 0.02083276
Iteration 88, loss = 0.02056751
Iteration 89, loss = 0.02044275
Iteration 90, loss = 0.02032264
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67272670
Iteration 2, loss = 0.59869920
Iteration 3, loss = 0.48984010
Iteration 4, loss = 0.37957189
Iteration 5, loss = 0.28619274
Iteration 6, loss = 0.21608892
Iteration 7, loss = 0.16668911
Iteration 8, loss = 0.13264888
Iteration 9, loss = 0.10866832
Iteration 10, loss = 0.09145869
Iteration 11, loss = 0.07911790
Iteration 12, loss = 0.06944883
Iteration 13, loss = 0.06193350
Iteration 14, loss = 0.05633801
Iteration 15, loss = 0.05203322
Iteration 16, loss = 0.04822823
Iteration 17, loss = 0.04505959
Iteration 18, loss = 0.04238091
Iteration 19, loss = 0.04028623
Iteration 20, loss = 0.03890080
Iteration 21, loss = 0.03710760
Iteration 22, loss = 0.03570707
Iteration 23, loss = 0.03448959
Iteration 24, loss = 0.03345809
Iteration 25, loss = 0.03284641
Iteration 26, loss = 0.03200425
Iteration 27, loss = 0.03131915
Iteration 28, loss = 0.03065541
Iteration 29, loss = 0.03030128
Iteration 30, loss = 0.02970765
Iteration 31, loss = 0.02901332
Iteration 32, loss = 0.02872459
Iteration 33, loss = 0.02831070
Iteration 34, loss = 0.02806868
Iteration 35, loss = 0.02773631
Iteration 36, loss = 0.02737583
Iteration 37, loss = 0.02705805
Iteration 38, loss = 0.02711907
Iteration 39, loss = 0.02680445
Iteration 40, loss = 0.02706597
Iteration 41, loss = 0.02652629
Iteration 42, loss = 0.02635824
Iteration 43, loss = 0.02590809
Iteration 44, loss = 0.02608095
Iteration 45, loss = 0.02606979
Iteration 46, loss = 0.02563244
Iteration 47, loss = 0.02588684
Iteration 48, loss = 0.02559091
Iteration 49, loss = 0.02557111
Iteration 50, loss = 0.02545198
Iteration 51, loss = 0.02536751
Iteration 52, loss = 0.02533068
Iteration 53, loss = 0.02507896
Iteration 54, loss = 0.02499773
Iteration 55, loss = 0.02492108
Iteration 56, loss = 0.02478462
Iteration 57, loss = 0.02466629
Iteration 58, loss = 0.02473724
Iteration 59, loss = 0.02460047
Iteration 60, loss = 0.02483509
Iteration 61, loss = 0.02457020
Iteration 62, loss = 0.02451677
Iteration 63, loss = 0.02457407
Iteration 64, loss = 0.02417746
Iteration 65, loss = 0.02436558
Iteration 66, loss = 0.02449101
Iteration 67, loss = 0.02428684
Iteration 68, loss = 0.02408831
Iteration 69, loss = 0.02453952
Iteration 70, loss = 0.02469866
Iteration 71, loss = 0.02391994
Iteration 72, loss = 0.02415684
Iteration 73, loss = 0.02410643
Iteration 74, loss = 0.02422496
Iteration 75, loss = 0.02408904
Iteration 76, loss = 0.02409412
Iteration 77, loss = 0.02418594
Iteration 78, loss = 0.02395913
Iteration 79, loss = 0.02374868
Iteration 80, loss = 0.02400418
Iteration 81, loss = 0.02377190
Iteration 82, loss = 0.02400026
Iteration 83, loss = 0.02370090
Iteration 84, loss = 0.02397781
Iteration 85, loss = 0.02374672
Iteration 86, loss = 0.02379395
Iteration 87, loss = 0.02387222
Iteration 88, loss = 0.02381462
Iteration 89, loss = 0.02347736
Iteration 90, loss = 0.02412359
Iteration 91, loss = 0.02393235
Iteration 92, loss = 0.02361552
Iteration 93, loss = 0.02353071
Iteration 94, loss = 0.02358065
Iteration 95, loss = 0.02369095
Iteration 96, loss = 0.02359444
Iteration 97, loss = 0.02387681
Iteration 98, loss = 0.02342086
Iteration 99, loss = 0.02348487
Iteration 100, loss = 0.02387494
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68394555
Iteration 2, loss = 0.66531193
Iteration 3, loss = 0.64673492
Iteration 4, loss = 0.62536381
Iteration 5, loss = 0.60033599
Iteration 6, loss = 0.57166707
Iteration 7, loss = 0.54022366
Iteration 8, loss = 0.50648583
Iteration 9, loss = 0.47158411
Iteration 10, loss = 0.43618572
Iteration 11, loss = 0.40146023
Iteration 12, loss = 0.36791531
Iteration 13, loss = 0.33623783
Iteration 14, loss = 0.30658097
Iteration 15, loss = 0.27919179
Iteration 16, loss = 0.25410298
Iteration 17, loss = 0.23138047
Iteration 18, loss = 0.21070775
Iteration 19, loss = 0.19217410
Iteration 20, loss = 0.17561995
Iteration 21, loss = 0.16085885
Iteration 22, loss = 0.14751673
Iteration 23, loss = 0.13577764
Iteration 24, loss = 0.12527803
Iteration 25, loss = 0.11588057
Iteration 26, loss = 0.10757059
Iteration 27, loss = 0.10010826
Iteration 28, loss = 0.09335945
Iteration 29, loss = 0.08738081
Iteration 30, loss = 0.08193980
Iteration 31, loss = 0.07709727
Iteration 32, loss = 0.07260092
Iteration 33, loss = 0.06860348
Iteration 34, loss = 0.06499677
Iteration 35, loss = 0.06164540
Iteration 36, loss = 0.05866772
Iteration 37, loss = 0.05579527
Iteration 38, loss = 0.05325936
Iteration 39, loss = 0.05089094
Iteration 40, loss = 0.04870422
Iteration 41, loss = 0.04672458
Iteration 42, loss = 0.04486799
Iteration 43, loss = 0.04313523
Iteration 44, loss = 0.04153770
Iteration 45, loss = 0.04001511
Iteration 46, loss = 0.03861826
Iteration 47, loss = 0.03732416
Iteration 48, loss = 0.03613861
Iteration 49, loss = 0.03494270
Iteration 50, loss = 0.03391640
Iteration 51, loss = 0.03290368
Iteration 52, loss = 0.03198185
Iteration 53, loss = 0.03108298
Iteration 54, loss = 0.03027474
Iteration 55, loss = 0.02943456
Iteration 56, loss = 0.02873515
Iteration 57, loss = 0.02803482
Iteration 58, loss = 0.02729543
Iteration 59, loss = 0.02668413
Iteration 60, loss = 0.02611136
Iteration 61, loss = 0.02548623
Iteration 62, loss = 0.02500611
Iteration 63, loss = 0.02447327
Iteration 64, loss = 0.02396995
Iteration 65, loss = 0.02352421
Iteration 66, loss = 0.02305166
Iteration 67, loss = 0.02263956
Iteration 68, loss = 0.02225421
Iteration 69, loss = 0.02188636
Iteration 70, loss = 0.02147948
Iteration 71, loss = 0.02113544
Iteration 72, loss = 0.02080398
Iteration 73, loss = 0.02048225
Iteration 74, loss = 0.02017436
Iteration 75, loss = 0.01987047
Iteration 76, loss = 0.01958722
Iteration 77, loss = 0.01931599
Iteration 78, loss = 0.01906681
Iteration 79, loss = 0.01889305
Iteration 80, loss = 0.01859747
Iteration 81, loss = 0.01832413
Iteration 82, loss = 0.01815365
Iteration 83, loss = 0.01787542
Iteration 84, loss = 0.01771064
Iteration 85, loss = 0.01749981
Iteration 86, loss = 0.01731782
Iteration 87, loss = 0.01718364
Iteration 88, loss = 0.01692249
Iteration 89, loss = 0.01679967
Iteration 90, loss = 0.01665286
Iteration 91, loss = 0.01646367
Iteration 92, loss = 0.01627225
Iteration 93, loss = 0.01612477
Iteration 94, loss = 0.01599967
Iteration 95, loss = 0.01583826
Iteration 96, loss = 0.01572332
Iteration 97, loss = 0.01560067
Iteration 98, loss = 0.01544644
Iteration 99, loss = 0.01533117
Iteration 100, loss = 0.01529452
Iteration 101, loss = 0.01507887
Iteration 102, loss = 0.01509585
Iteration 103, loss = 0.01490623
Iteration 104, loss = 0.01478762
Iteration 105, loss = 0.01467846
Iteration 106, loss = 0.01459307
Iteration 107, loss = 0.01447649
Iteration 108, loss = 0.01436936
Iteration 109, loss = 0.01432812
Iteration 110, loss = 0.01424080
Iteration 111, loss = 0.01412076
Iteration 112, loss = 0.01411806
Iteration 113, loss = 0.01395257
Iteration 114, loss = 0.01388843
Iteration 115, loss = 0.01379947
Iteration 116, loss = 0.01387086
Iteration 117, loss = 0.01370411
Iteration 118, loss = 0.01359109
Iteration 119, loss = 0.01351549
Iteration 120, loss = 0.01344975
Iteration 121, loss = 0.01341623
Iteration 122, loss = 0.01331400
Iteration 123, loss = 0.01327062
Iteration 124, loss = 0.01321217
Iteration 125, loss = 0.01317166
Iteration 126, loss = 0.01308073
Iteration 127, loss = 0.01305819
Iteration 128, loss = 0.01299126
Iteration 129, loss = 0.01291795
Iteration 130, loss = 0.01286634
Iteration 131, loss = 0.01281487
Iteration 132, loss = 0.01277571
Iteration 133, loss = 0.01271298
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67875976
Iteration 2, loss = 0.60276003
Iteration 3, loss = 0.49033551
Iteration 4, loss = 0.37742192
Iteration 5, loss = 0.28349332
Iteration 6, loss = 0.21355092
Iteration 7, loss = 0.16431588
Iteration 8, loss = 0.13019747
Iteration 9, loss = 0.10655311
Iteration 10, loss = 0.08921773
Iteration 11, loss = 0.07672616
Iteration 12, loss = 0.06728255
Iteration 13, loss = 0.05992585
Iteration 14, loss = 0.05422928
Iteration 15, loss = 0.04976481
Iteration 16, loss = 0.04606728
Iteration 17, loss = 0.04289498
Iteration 18, loss = 0.04030150
Iteration 19, loss = 0.03823583
Iteration 20, loss = 0.03641397
Iteration 21, loss = 0.03476348
Iteration 22, loss = 0.03348552
Iteration 23, loss = 0.03224204
Iteration 24, loss = 0.03113893
Iteration 25, loss = 0.03024920
Iteration 26, loss = 0.02955762
Iteration 27, loss = 0.02903530
Iteration 28, loss = 0.02845801
Iteration 29, loss = 0.02779685
Iteration 30, loss = 0.02722904
Iteration 31, loss = 0.02689555
Iteration 32, loss = 0.02639673
Iteration 33, loss = 0.02583912
Iteration 34, loss = 0.02566040
Iteration 35, loss = 0.02555561
Iteration 36, loss = 0.02517858
Iteration 37, loss = 0.02492828
Iteration 38, loss = 0.02474899
Iteration 39, loss = 0.02417437
Iteration 40, loss = 0.02428498
Iteration 41, loss = 0.02406197
Iteration 42, loss = 0.02383012
Iteration 43, loss = 0.02383673
Iteration 44, loss = 0.02337304
Iteration 45, loss = 0.02351399
Iteration 46, loss = 0.02360122
Iteration 47, loss = 0.02337336
Iteration 48, loss = 0.02290010
Iteration 49, loss = 0.02286263
Iteration 50, loss = 0.02274812
Iteration 51, loss = 0.02277222
Iteration 52, loss = 0.02262534
Iteration 53, loss = 0.02266329
Iteration 54, loss = 0.02232686
Iteration 55, loss = 0.02232397
Iteration 56, loss = 0.02250154
Iteration 57, loss = 0.02259907
Iteration 58, loss = 0.02287374
Iteration 59, loss = 0.02217002
Iteration 60, loss = 0.02214952
Iteration 61, loss = 0.02213687
Iteration 62, loss = 0.02197656
Iteration 63, loss = 0.02165454
Iteration 64, loss = 0.02214437
Iteration 65, loss = 0.02164899
Iteration 66, loss = 0.02186807
Iteration 67, loss = 0.02187049
Iteration 68, loss = 0.02195280
Iteration 69, loss = 0.02143247
Iteration 70, loss = 0.02173378
Iteration 71, loss = 0.02162094
Iteration 72, loss = 0.02147789
Iteration 73, loss = 0.02139155
Iteration 74, loss = 0.02195453
Iteration 75, loss = 0.02148437
Iteration 76, loss = 0.02153531
Iteration 77, loss = 0.02145934
Iteration 78, loss = 0.02150315
Iteration 79, loss = 0.02126758
Iteration 80, loss = 0.02138729
Iteration 81, loss = 0.02160395
Iteration 82, loss = 0.02130275
Iteration 83, loss = 0.02114343
Iteration 84, loss = 0.02134363
Iteration 85, loss = 0.02112681
Iteration 86, loss = 0.02105708
Iteration 87, loss = 0.02132891
Iteration 88, loss = 0.02111357
Iteration 89, loss = 0.02122147
Iteration 90, loss = 0.02094275
Iteration 91, loss = 0.02124248
Iteration 92, loss = 0.02152765
Iteration 93, loss = 0.02085963
Iteration 94, loss = 0.02105496
Iteration 95, loss = 0.02106498
Iteration 96, loss = 0.02123100
Iteration 97, loss = 0.02112253
Iteration 98, loss = 0.02098253
Iteration 99, loss = 0.02115071
Iteration 100, loss = 0.02118982
Iteration 101, loss = 0.02081374
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68668410
Iteration 2, loss = 0.65465225
Iteration 3, loss = 0.60896074
Iteration 4, loss = 0.54740638
Iteration 5, loss = 0.47703536
Iteration 6, loss = 0.40773815
Iteration 7, loss = 0.34345366
Iteration 8, loss = 0.28704147
Iteration 9, loss = 0.23961744
Iteration 10, loss = 0.20070559
Iteration 11, loss = 0.16930219
Iteration 12, loss = 0.14421786
Iteration 13, loss = 0.12433599
Iteration 14, loss = 0.10823520
Iteration 15, loss = 0.09535150
Iteration 16, loss = 0.08476063
Iteration 17, loss = 0.07612812
Iteration 18, loss = 0.06886307
Iteration 19, loss = 0.06271434
Iteration 20, loss = 0.05769085
Iteration 21, loss = 0.05337512
Iteration 22, loss = 0.04956999
Iteration 23, loss = 0.04640733
Iteration 24, loss = 0.04352601
Iteration 25, loss = 0.04104487
Iteration 26, loss = 0.03897518
Iteration 27, loss = 0.03708797
Iteration 28, loss = 0.03536257
Iteration 29, loss = 0.03387667
Iteration 30, loss = 0.03242590
Iteration 31, loss = 0.03117981
Iteration 32, loss = 0.03023259
Iteration 33, loss = 0.02913282
Iteration 34, loss = 0.02826214
Iteration 35, loss = 0.02738831
Iteration 36, loss = 0.02661728
Iteration 37, loss = 0.02590975
Iteration 38, loss = 0.02527549
Iteration 39, loss = 0.02473703
Iteration 40, loss = 0.02415784
Iteration 41, loss = 0.02374623
Iteration 42, loss = 0.02315040
Iteration 43, loss = 0.02272880
Iteration 44, loss = 0.02250123
Iteration 45, loss = 0.02195239
Iteration 46, loss = 0.02170905
Iteration 47, loss = 0.02126892
Iteration 48, loss = 0.02099779
Iteration 49, loss = 0.02103111
Iteration 50, loss = 0.02054010
Iteration 51, loss = 0.02032852
Iteration 52, loss = 0.02001878
Iteration 53, loss = 0.01972553
Iteration 54, loss = 0.01962170
Iteration 55, loss = 0.01937560
Iteration 56, loss = 0.01922221
Iteration 57, loss = 0.01907791
Iteration 58, loss = 0.01887395
Iteration 59, loss = 0.01878059
Iteration 60, loss = 0.01851444
Iteration 61, loss = 0.01829753
Iteration 62, loss = 0.01823166
Iteration 63, loss = 0.01821483
Iteration 64, loss = 0.01816331
Iteration 65, loss = 0.01782125
Iteration 66, loss = 0.01801582
Iteration 67, loss = 0.01775711
Iteration 68, loss = 0.01751779
Iteration 69, loss = 0.01740974
Iteration 70, loss = 0.01737078
Iteration 71, loss = 0.01740825
Iteration 72, loss = 0.01730562
Iteration 73, loss = 0.01727660
Iteration 74, loss = 0.01709123
Iteration 75, loss = 0.01690626
Iteration 76, loss = 0.01693119
Iteration 77, loss = 0.01686477
Iteration 78, loss = 0.01664738
Iteration 79, loss = 0.01671496
Iteration 80, loss = 0.01690669
Iteration 81, loss = 0.01665927
Iteration 82, loss = 0.01666968
Iteration 83, loss = 0.01648590
Iteration 84, loss = 0.01651124
Iteration 85, loss = 0.01649737
Iteration 86, loss = 0.01636932
Iteration 87, loss = 0.01632888
Iteration 88, loss = 0.01622156
Iteration 89, loss = 0.01616546
Iteration 90, loss = 0.01612366
Iteration 91, loss = 0.01616442
Iteration 92, loss = 0.01631389
Iteration 93, loss = 0.01629219
Iteration 94, loss = 0.01610366
Iteration 95, loss = 0.01594652
Iteration 96, loss = 0.01596053
Iteration 97, loss = 0.01606822
Iteration 98, loss = 0.01587219
Iteration 99, loss = 0.01577146
Iteration 100, loss = 0.01588613
Iteration 101, loss = 0.01573461
Iteration 102, loss = 0.01588599
Iteration 103, loss = 0.01569072
Iteration 104, loss = 0.01578732
Iteration 105, loss = 0.01550022
Iteration 106, loss = 0.01567357
Iteration 107, loss = 0.01560974
Iteration 108, loss = 0.01545407
Iteration 109, loss = 0.01554884
Iteration 110, loss = 0.01562301
Iteration 111, loss = 0.01546147
Iteration 112, loss = 0.01549751
Iteration 113, loss = 0.01542177
Iteration 114, loss = 0.01549612
Iteration 115, loss = 0.01546269
Iteration 116, loss = 0.01538996
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69282200
Iteration 2, loss = 0.62751092
Iteration 3, loss = 0.52922855
Iteration 4, loss = 0.42047979
Iteration 5, loss = 0.32494885
Iteration 6, loss = 0.24886422
Iteration 7, loss = 0.19248229
Iteration 8, loss = 0.15223291
Iteration 9, loss = 0.12354715
Iteration 10, loss = 0.10290629
Iteration 11, loss = 0.08787245
Iteration 12, loss = 0.07629254
Iteration 13, loss = 0.06740891
Iteration 14, loss = 0.06047324
Iteration 15, loss = 0.05481476
Iteration 16, loss = 0.05051408
Iteration 17, loss = 0.04692585
Iteration 18, loss = 0.04349581
Iteration 19, loss = 0.04098393
Iteration 20, loss = 0.03878214
Iteration 21, loss = 0.03681560
Iteration 22, loss = 0.03524937
Iteration 23, loss = 0.03383642
Iteration 24, loss = 0.03280735
Iteration 25, loss = 0.03149201
Iteration 26, loss = 0.03056339
Iteration 27, loss = 0.02990669
Iteration 28, loss = 0.02920003
Iteration 29, loss = 0.02854698
Iteration 30, loss = 0.02789046
Iteration 31, loss = 0.02739141
Iteration 32, loss = 0.02663102
Iteration 33, loss = 0.02634034
Iteration 34, loss = 0.02584253
Iteration 35, loss = 0.02558976
Iteration 36, loss = 0.02508210
Iteration 37, loss = 0.02510510
Iteration 38, loss = 0.02461840
Iteration 39, loss = 0.02436579
Iteration 40, loss = 0.02387389
Iteration 41, loss = 0.02385955
Iteration 42, loss = 0.02378599
Iteration 43, loss = 0.02355100
Iteration 44, loss = 0.02358227
Iteration 45, loss = 0.02329830
Iteration 46, loss = 0.02304679
Iteration 47, loss = 0.02295260
Iteration 48, loss = 0.02256343
Iteration 49, loss = 0.02264666
Iteration 50, loss = 0.02250518
Iteration 51, loss = 0.02250255
Iteration 52, loss = 0.02231061
Iteration 53, loss = 0.02225626
Iteration 54, loss = 0.02207505
Iteration 55, loss = 0.02197852
Iteration 56, loss = 0.02189124
Iteration 57, loss = 0.02175173
Iteration 58, loss = 0.02155097
Iteration 59, loss = 0.02195367
Iteration 60, loss = 0.02154734
Iteration 61, loss = 0.02169770
Iteration 62, loss = 0.02140946
Iteration 63, loss = 0.02152452
Iteration 64, loss = 0.02143592
Iteration 65, loss = 0.02149584
Iteration 66, loss = 0.02151576
Iteration 67, loss = 0.02114979
Iteration 68, loss = 0.02128946
Iteration 69, loss = 0.02097565
Iteration 70, loss = 0.02114164
Iteration 71, loss = 0.02104597
Iteration 72, loss = 0.02082224
Iteration 73, loss = 0.02113322
Iteration 74, loss = 0.02086388
Iteration 75, loss = 0.02097874
Iteration 76, loss = 0.02096851
Iteration 77, loss = 0.02067344
Iteration 78, loss = 0.02072811
Iteration 79, loss = 0.02101695
Iteration 80, loss = 0.02151043
Iteration 81, loss = 0.02101504
Iteration 82, loss = 0.02080810
Iteration 83, loss = 0.02071952
Iteration 84, loss = 0.02051184
Iteration 85, loss = 0.02062376
Iteration 86, loss = 0.02056128
Iteration 87, loss = 0.02056294
Iteration 88, loss = 0.02079162
Iteration 89, loss = 0.02044300
Iteration 90, loss = 0.02035975
Iteration 91, loss = 0.02068375
Iteration 92, loss = 0.02070329
Iteration 93, loss = 0.02034978
Iteration 94, loss = 0.02081669
Iteration 95, loss = 0.02049524
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68180877
Iteration 2, loss = 0.64587310
Iteration 3, loss = 0.59899360
Iteration 4, loss = 0.53613085
Iteration 5, loss = 0.46646619
Iteration 6, loss = 0.39888840
Iteration 7, loss = 0.33750918
Iteration 8, loss = 0.28352637
Iteration 9, loss = 0.23802894
Iteration 10, loss = 0.20061372
Iteration 11, loss = 0.17022543
Iteration 12, loss = 0.14598554
Iteration 13, loss = 0.12633287
Iteration 14, loss = 0.11040819
Iteration 15, loss = 0.09771715
Iteration 16, loss = 0.08712310
Iteration 17, loss = 0.07841602
Iteration 18, loss = 0.07117155
Iteration 19, loss = 0.06505229
Iteration 20, loss = 0.05986169
Iteration 21, loss = 0.05542171
Iteration 22, loss = 0.05172754
Iteration 23, loss = 0.04839193
Iteration 24, loss = 0.04550768
Iteration 25, loss = 0.04310403
Iteration 26, loss = 0.04085166
Iteration 27, loss = 0.03889667
Iteration 28, loss = 0.03712419
Iteration 29, loss = 0.03561588
Iteration 30, loss = 0.03425406
Iteration 31, loss = 0.03292259
Iteration 32, loss = 0.03185984
Iteration 33, loss = 0.03107715
Iteration 34, loss = 0.02996452
Iteration 35, loss = 0.02896861
Iteration 36, loss = 0.02849389
Iteration 37, loss = 0.02768561
Iteration 38, loss = 0.02696596
Iteration 39, loss = 0.02628085
Iteration 40, loss = 0.02587855
Iteration 41, loss = 0.02521284
Iteration 42, loss = 0.02478850
Iteration 43, loss = 0.02448932
Iteration 44, loss = 0.02394770
Iteration 45, loss = 0.02365849
Iteration 46, loss = 0.02319097
Iteration 47, loss = 0.02293488
Iteration 48, loss = 0.02298327
Iteration 49, loss = 0.02238618
Iteration 50, loss = 0.02220593
Iteration 51, loss = 0.02189283
Iteration 52, loss = 0.02171301
Iteration 53, loss = 0.02137288
Iteration 54, loss = 0.02125017
Iteration 55, loss = 0.02108094
Iteration 56, loss = 0.02096419
Iteration 57, loss = 0.02072600
Iteration 58, loss = 0.02052156
Iteration 59, loss = 0.02030788
Iteration 60, loss = 0.02028686
Iteration 61, loss = 0.02010289
Iteration 62, loss = 0.02000335
Iteration 63, loss = 0.01974055
Iteration 64, loss = 0.01972334
Iteration 65, loss = 0.01948677
Iteration 66, loss = 0.01963332
Iteration 67, loss = 0.01955681
Iteration 68, loss = 0.01937265
Iteration 69, loss = 0.01914393
Iteration 70, loss = 0.01914794
Iteration 71, loss = 0.01904456
Iteration 72, loss = 0.01895377
Iteration 73, loss = 0.01880580
Iteration 74, loss = 0.01877211
Iteration 75, loss = 0.01859251
Iteration 76, loss = 0.01870627
Iteration 77, loss = 0.01850398
Iteration 78, loss = 0.01843131
Iteration 79, loss = 0.01837995
Iteration 80, loss = 0.01843334
Iteration 81, loss = 0.01813370
Iteration 82, loss = 0.01810593
Iteration 83, loss = 0.01815621
Iteration 84, loss = 0.01803461
Iteration 85, loss = 0.01805710
Iteration 86, loss = 0.01810775
Iteration 87, loss = 0.01797174
Iteration 88, loss = 0.01779981
Iteration 89, loss = 0.01790832
Iteration 90, loss = 0.01794824
Iteration 91, loss = 0.01772198
Iteration 92, loss = 0.01773015
Iteration 93, loss = 0.01763865
Iteration 94, loss = 0.01766042
Iteration 95, loss = 0.01766506
Iteration 96, loss = 0.01764132
Iteration 97, loss = 0.01745300
Iteration 98, loss = 0.01760883
Iteration 99, loss = 0.01747308
Iteration 100, loss = 0.01756621
Iteration 101, loss = 0.01751831
Iteration 102, loss = 0.01738421
Iteration 103, loss = 0.01740774
Iteration 104, loss = 0.01747128
Iteration 105, loss = 0.01728238
Iteration 106, loss = 0.01736770
Iteration 107, loss = 0.01727837
Iteration 108, loss = 0.01742449
Iteration 109, loss = 0.01738849
Iteration 110, loss = 0.01720484
Iteration 111, loss = 0.01710845
Iteration 112, loss = 0.01737814
Iteration 113, loss = 0.01713400
Iteration 114, loss = 0.01714278
Iteration 115, loss = 0.01714510
Iteration 116, loss = 0.01719359
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67771890
Iteration 2, loss = 0.61941526
Iteration 3, loss = 0.53239340
Iteration 4, loss = 0.43471473
Iteration 5, loss = 0.34562474
Iteration 6, loss = 0.27132901
Iteration 7, loss = 0.21364542
Iteration 8, loss = 0.17048256
Iteration 9, loss = 0.13921184
Iteration 10, loss = 0.11619635
Iteration 11, loss = 0.09902668
Iteration 12, loss = 0.08597941
Iteration 13, loss = 0.07581260
Iteration 14, loss = 0.06808691
Iteration 15, loss = 0.06159958
Iteration 16, loss = 0.05621617
Iteration 17, loss = 0.05209598
Iteration 18, loss = 0.04857572
Iteration 19, loss = 0.04558687
Iteration 20, loss = 0.04391844
Iteration 21, loss = 0.04181619
Iteration 22, loss = 0.03969385
Iteration 23, loss = 0.03777838
Iteration 24, loss = 0.03630543
Iteration 25, loss = 0.03486409
Iteration 26, loss = 0.03401893
Iteration 27, loss = 0.03297054
Iteration 28, loss = 0.03195644
Iteration 29, loss = 0.03099943
Iteration 30, loss = 0.03047125
Iteration 31, loss = 0.02978228
Iteration 32, loss = 0.02910856
Iteration 33, loss = 0.02856726
Iteration 34, loss = 0.02815910
Iteration 35, loss = 0.02745357
Iteration 36, loss = 0.02725515
Iteration 37, loss = 0.02685974
Iteration 38, loss = 0.02638965
Iteration 39, loss = 0.02607224
Iteration 40, loss = 0.02591479
Iteration 41, loss = 0.02552470
Iteration 42, loss = 0.02525140
Iteration 43, loss = 0.02510658
Iteration 44, loss = 0.02503771
Iteration 45, loss = 0.02499301
Iteration 46, loss = 0.02882154
Iteration 47, loss = 0.02894136
Iteration 48, loss = 0.02764108
Iteration 49, loss = 0.02645807
Iteration 50, loss = 0.02579165
Iteration 51, loss = 0.02510751
Iteration 52, loss = 0.02459392
Iteration 53, loss = 0.02425299
Iteration 54, loss = 0.02370593
Iteration 55, loss = 0.02432781
Iteration 56, loss = 0.02486088
Iteration 57, loss = 0.02457806
Iteration 58, loss = 0.02402642
Iteration 59, loss = 0.02366324
Iteration 60, loss = 0.02361168
Iteration 61, loss = 0.02341572
Iteration 62, loss = 0.02318583
Iteration 63, loss = 0.02299052
Iteration 64, loss = 0.02300491
Iteration 65, loss = 0.02259345
Iteration 66, loss = 0.02315296
Iteration 67, loss = 0.02342661
Iteration 68, loss = 0.02319913
Iteration 69, loss = 0.02301164
Iteration 70, loss = 0.02267427
Iteration 71, loss = 0.02253326
Iteration 72, loss = 0.02245259
Iteration 73, loss = 0.02252120
Iteration 74, loss = 0.02232926
Iteration 75, loss = 0.02224425
Iteration 76, loss = 0.02216594
Iteration 77, loss = 0.02213573
Iteration 78, loss = 0.02197308
Iteration 79, loss = 0.02221746
Iteration 80, loss = 0.02197604
Iteration 81, loss = 0.02185884
Iteration 82, loss = 0.02165519
Iteration 83, loss = 0.02166190
Iteration 84, loss = 0.02142725
Iteration 85, loss = 0.02147099
Iteration 86, loss = 0.02159750
Iteration 87, loss = 0.02144628
Iteration 88, loss = 0.02120321
Iteration 89, loss = 0.02151141
Iteration 90, loss = 0.02125067
Iteration 91, loss = 0.02121130
Iteration 92, loss = 0.02112269
Iteration 93, loss = 0.02109445
Iteration 94, loss = 0.02138037
Iteration 95, loss = 0.02095823
Iteration 96, loss = 0.02114909
Iteration 97, loss = 0.02106963
Iteration 98, loss = 0.02101534
Iteration 99, loss = 0.02100098
Iteration 100, loss = 0.02084942
Iteration 101, loss = 0.02088433
Iteration 102, loss = 0.02094580
Iteration 103, loss = 0.02113332
Iteration 104, loss = 0.02099652
Iteration 105, loss = 0.02091138
Iteration 106, loss = 0.02079373
Iteration 107, loss = 0.02047160
Iteration 108, loss = 0.02090053
Iteration 109, loss = 0.02068609
Iteration 110, loss = 0.02066201
Iteration 111, loss = 0.02093166
Iteration 112, loss = 0.02053363
Iteration 113, loss = 0.02059049
Iteration 114, loss = 0.02060626
Iteration 115, loss = 0.02151229
Iteration 116, loss = 0.02224755
Iteration 117, loss = 0.02172769
Iteration 118, loss = 0.02186550
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68214836
Iteration 2, loss = 0.64909572
Iteration 3, loss = 0.60790739
Iteration 4, loss = 0.55580669
Iteration 5, loss = 0.49747468
Iteration 6, loss = 0.43828194
Iteration 7, loss = 0.38136853
Iteration 8, loss = 0.32891074
Iteration 9, loss = 0.28210081
Iteration 10, loss = 0.24162000
Iteration 11, loss = 0.20704542
Iteration 12, loss = 0.17849413
Iteration 13, loss = 0.15465941
Iteration 14, loss = 0.13496278
Iteration 15, loss = 0.11877238
Iteration 16, loss = 0.10541634
Iteration 17, loss = 0.09417769
Iteration 18, loss = 0.08498841
Iteration 19, loss = 0.07692505
Iteration 20, loss = 0.07027138
Iteration 21, loss = 0.06455625
Iteration 22, loss = 0.05954361
Iteration 23, loss = 0.05536823
Iteration 24, loss = 0.05163500
Iteration 25, loss = 0.04837724
Iteration 26, loss = 0.04538666
Iteration 27, loss = 0.04287408
Iteration 28, loss = 0.04064689
Iteration 29, loss = 0.03869141
Iteration 30, loss = 0.03675797
Iteration 31, loss = 0.03524444
Iteration 32, loss = 0.03376626
Iteration 33, loss = 0.03237531
Iteration 34, loss = 0.03121977
Iteration 35, loss = 0.03013288
Iteration 36, loss = 0.02915599
Iteration 37, loss = 0.02830542
Iteration 38, loss = 0.02744670
Iteration 39, loss = 0.02673442
Iteration 40, loss = 0.02603672
Iteration 41, loss = 0.02525485
Iteration 42, loss = 0.02459577
Iteration 43, loss = 0.02416791
Iteration 44, loss = 0.02353011
Iteration 45, loss = 0.02312177
Iteration 46, loss = 0.02265393
Iteration 47, loss = 0.02241609
Iteration 48, loss = 0.02185744
Iteration 49, loss = 0.02143817
Iteration 50, loss = 0.02117430
Iteration 51, loss = 0.02090861
Iteration 52, loss = 0.02063073
Iteration 53, loss = 0.02026098
Iteration 54, loss = 0.01990786
Iteration 55, loss = 0.01966705
Iteration 56, loss = 0.01944753
Iteration 57, loss = 0.01928918
Iteration 58, loss = 0.01903813
Iteration 59, loss = 0.01881334
Iteration 60, loss = 0.01857496
Iteration 61, loss = 0.01842105
Iteration 62, loss = 0.01832904
Iteration 63, loss = 0.01805894
Iteration 64, loss = 0.01797412
Iteration 65, loss = 0.01771767
Iteration 66, loss = 0.01759937
Iteration 67, loss = 0.01747105
Iteration 68, loss = 0.01736608
Iteration 69, loss = 0.01729163
Iteration 70, loss = 0.01715417
Iteration 71, loss = 0.01694310
Iteration 72, loss = 0.01687834
Iteration 73, loss = 0.01673529
Iteration 74, loss = 0.01674083
Iteration 75, loss = 0.01665402
Iteration 76, loss = 0.01667933
Iteration 77, loss = 0.01639174
Iteration 78, loss = 0.01633960
Iteration 79, loss = 0.01621087
Iteration 80, loss = 0.01622868
Iteration 81, loss = 0.01602066
Iteration 82, loss = 0.01593884
Iteration 83, loss = 0.01599569
Iteration 84, loss = 0.01583974
Iteration 85, loss = 0.01579462
Iteration 86, loss = 0.01580469
Iteration 87, loss = 0.01561472
Iteration 88, loss = 0.01572197
Iteration 89, loss = 0.01560598
Iteration 90, loss = 0.01558772
Iteration 91, loss = 0.01560188
Iteration 92, loss = 0.01544155
Iteration 93, loss = 0.01556492
Iteration 94, loss = 0.01529760
Iteration 95, loss = 0.01527988
Iteration 96, loss = 0.01527767
Iteration 97, loss = 0.01520731
Iteration 98, loss = 0.01526485
Iteration 99, loss = 0.01521898
Iteration 100, loss = 0.01495904
Iteration 101, loss = 0.01506934
Iteration 102, loss = 0.01499800
Iteration 103, loss = 0.01500047
Iteration 104, loss = 0.01505466
Iteration 105, loss = 0.01492700
Iteration 106, loss = 0.01494778
Iteration 107, loss = 0.01480757
Iteration 108, loss = 0.01488270
Iteration 109, loss = 0.01493176
Iteration 110, loss = 0.01485761
Iteration 111, loss = 0.01483335
Iteration 112, loss = 0.01477997
Iteration 113, loss = 0.01473891
Iteration 114, loss = 0.01480784
Iteration 115, loss = 0.01472179
Iteration 116, loss = 0.01472020
Iteration 117, loss = 0.01486582
Iteration 118, loss = 0.01461554
Iteration 119, loss = 0.01466511
Iteration 120, loss = 0.01450321
Iteration 121, loss = 0.01457594
Iteration 122, loss = 0.01458836
Iteration 123, loss = 0.01464341
Iteration 124, loss = 0.01443716
Iteration 125, loss = 0.01450780
Iteration 126, loss = 0.01473693
Iteration 127, loss = 0.01451631
Iteration 128, loss = 0.01443777
Iteration 129, loss = 0.01449153
Iteration 130, loss = 0.01451602
Iteration 131, loss = 0.01446109
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68745667
Iteration 2, loss = 0.61782029
Iteration 3, loss = 0.51454765
Iteration 4, loss = 0.40430753
Iteration 5, loss = 0.30747446
Iteration 6, loss = 0.23192497
Iteration 7, loss = 0.17762781
Iteration 8, loss = 0.13971569
Iteration 9, loss = 0.11317576
Iteration 10, loss = 0.09442975
Iteration 11, loss = 0.08051959
Iteration 12, loss = 0.07014648
Iteration 13, loss = 0.06227747
Iteration 14, loss = 0.05600693
Iteration 15, loss = 0.05112783
Iteration 16, loss = 0.04722352
Iteration 17, loss = 0.04405142
Iteration 18, loss = 0.04137147
Iteration 19, loss = 0.03889935
Iteration 20, loss = 0.03668129
Iteration 21, loss = 0.03531627
Iteration 22, loss = 0.03399521
Iteration 23, loss = 0.03277507
Iteration 24, loss = 0.03167253
Iteration 25, loss = 0.03057020
Iteration 26, loss = 0.02997927
Iteration 27, loss = 0.02907484
Iteration 28, loss = 0.02849785
Iteration 29, loss = 0.02792350
Iteration 30, loss = 0.02734477
Iteration 31, loss = 0.02711472
Iteration 32, loss = 0.02656426
Iteration 33, loss = 0.02614274
Iteration 34, loss = 0.02589610
Iteration 35, loss = 0.02561154
Iteration 36, loss = 0.02513186
Iteration 37, loss = 0.02501798
Iteration 38, loss = 0.02460829
Iteration 39, loss = 0.02452754
Iteration 40, loss = 0.02423128
Iteration 41, loss = 0.02417589
Iteration 42, loss = 0.02377160
Iteration 43, loss = 0.02390361
Iteration 44, loss = 0.02380011
Iteration 45, loss = 0.02364654
Iteration 46, loss = 0.02340029
Iteration 47, loss = 0.02337827
Iteration 48, loss = 0.02305464
Iteration 49, loss = 0.02303921
Iteration 50, loss = 0.02271201
Iteration 51, loss = 0.02265311
Iteration 52, loss = 0.02255341
Iteration 53, loss = 0.02251251
Iteration 54, loss = 0.02255881
Iteration 55, loss = 0.02233413
Iteration 56, loss = 0.02255812
Iteration 57, loss = 0.02240364
Iteration 58, loss = 0.02240808
Iteration 59, loss = 0.02219431
Iteration 60, loss = 0.02239513
Iteration 61, loss = 0.02222382
Iteration 62, loss = 0.02207639
Iteration 63, loss = 0.02214736
Iteration 64, loss = 0.02178647
Iteration 65, loss = 0.02190869
Iteration 66, loss = 0.02157726
Iteration 67, loss = 0.02174941
Iteration 68, loss = 0.02185986
Iteration 69, loss = 0.02174635
Iteration 70, loss = 0.02144323
Iteration 71, loss = 0.02169641
Iteration 72, loss = 0.02158206
Iteration 73, loss = 0.02155100
Iteration 74, loss = 0.02167588
Iteration 75, loss = 0.02160550
Iteration 76, loss = 0.02153654
Iteration 77, loss = 0.02135799
Iteration 78, loss = 0.02139215
Iteration 79, loss = 0.02165454
Iteration 80, loss = 0.02118283
Iteration 81, loss = 0.02118778
Iteration 82, loss = 0.02143008
Iteration 83, loss = 0.02147977
Iteration 84, loss = 0.02117841
Iteration 85, loss = 0.02114020
Iteration 86, loss = 0.02120431
Iteration 87, loss = 0.02137228
Iteration 88, loss = 0.02126291
Iteration 89, loss = 0.02150182
Iteration 90, loss = 0.02118454
Iteration 91, loss = 0.02097784
Iteration 92, loss = 0.02118145
Iteration 93, loss = 0.02122895
Iteration 94, loss = 0.02123438
Iteration 95, loss = 0.02109027
Iteration 96, loss = 0.02092036
Iteration 97, loss = 0.02101273
Iteration 98, loss = 0.02098968
Iteration 99, loss = 0.02111676
Iteration 100, loss = 0.02106764
Iteration 101, loss = 0.02099310
Iteration 102, loss = 0.02089076
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68204319
Iteration 2, loss = 0.64985555
Iteration 3, loss = 0.60895085
Iteration 4, loss = 0.55513553
Iteration 5, loss = 0.49252106
Iteration 6, loss = 0.42907423
Iteration 7, loss = 0.36934391
Iteration 8, loss = 0.31618280
Iteration 9, loss = 0.26971124
Iteration 10, loss = 0.23020414
Iteration 11, loss = 0.19714327
Iteration 12, loss = 0.16955319
Iteration 13, loss = 0.14690856
Iteration 14, loss = 0.12805936
Iteration 15, loss = 0.11259703
Iteration 16, loss = 0.09976695
Iteration 17, loss = 0.08888388
Iteration 18, loss = 0.07990664
Iteration 19, loss = 0.07230386
Iteration 20, loss = 0.06577355
Iteration 21, loss = 0.06017897
Iteration 22, loss = 0.05530639
Iteration 23, loss = 0.05124831
Iteration 24, loss = 0.04757379
Iteration 25, loss = 0.04444129
Iteration 26, loss = 0.04161804
Iteration 27, loss = 0.03916944
Iteration 28, loss = 0.03703213
Iteration 29, loss = 0.03509234
Iteration 30, loss = 0.03335309
Iteration 31, loss = 0.03166687
Iteration 32, loss = 0.03034329
Iteration 33, loss = 0.02901459
Iteration 34, loss = 0.02783436
Iteration 35, loss = 0.02673776
Iteration 36, loss = 0.02583966
Iteration 37, loss = 0.02486994
Iteration 38, loss = 0.02418569
Iteration 39, loss = 0.02339783
Iteration 40, loss = 0.02272925
Iteration 41, loss = 0.02208428
Iteration 42, loss = 0.02145871
Iteration 43, loss = 0.02087604
Iteration 44, loss = 0.02033356
Iteration 45, loss = 0.01991865
Iteration 46, loss = 0.01947366
Iteration 47, loss = 0.01906457
Iteration 48, loss = 0.01868786
Iteration 49, loss = 0.01836973
Iteration 50, loss = 0.01800101
Iteration 51, loss = 0.01769120
Iteration 52, loss = 0.01736323
Iteration 53, loss = 0.01722629
Iteration 54, loss = 0.01684286
Iteration 55, loss = 0.01662210
Iteration 56, loss = 0.01640863
Iteration 57, loss = 0.01619423
Iteration 58, loss = 0.01597650
Iteration 59, loss = 0.01577840
Iteration 60, loss = 0.01547248
Iteration 61, loss = 0.01537230
Iteration 62, loss = 0.01545762
Iteration 63, loss = 0.01503484
Iteration 64, loss = 0.01496590
Iteration 65, loss = 0.01476260
Iteration 66, loss = 0.01459081
Iteration 67, loss = 0.01458964
Iteration 68, loss = 0.01434749
Iteration 69, loss = 0.01431680
Iteration 70, loss = 0.01420402
Iteration 71, loss = 0.01407091
Iteration 72, loss = 0.01406664
Iteration 73, loss = 0.01390944
Iteration 74, loss = 0.01373847
Iteration 75, loss = 0.01368452
Iteration 76, loss = 0.01358032
Iteration 77, loss = 0.01345226
Iteration 78, loss = 0.01344937
Iteration 79, loss = 0.01331426
Iteration 80, loss = 0.01325515
Iteration 81, loss = 0.01317412
Iteration 82, loss = 0.01316720
Iteration 83, loss = 0.01315222
Iteration 84, loss = 0.01306798
Iteration 85, loss = 0.01311894
Iteration 86, loss = 0.01281312
Iteration 87, loss = 0.01281702
Iteration 88, loss = 0.01279655
Iteration 89, loss = 0.01278176
Iteration 90, loss = 0.01279509
Iteration 91, loss = 0.01258098
Iteration 92, loss = 0.01266651
Iteration 93, loss = 0.01251589
Iteration 94, loss = 0.01249580
Iteration 95, loss = 0.01238477
Iteration 96, loss = 0.01234145
Iteration 97, loss = 0.01241769
Iteration 98, loss = 0.01226431
Iteration 99, loss = 0.01227681
Iteration 100, loss = 0.01215849
Iteration 101, loss = 0.01220747
Iteration 102, loss = 0.01221502
Iteration 103, loss = 0.01218278
Iteration 104, loss = 0.01210104
Iteration 105, loss = 0.01201056
Iteration 106, loss = 0.01200673
Iteration 107, loss = 0.01198268
Iteration 108, loss = 0.01204073
Iteration 109, loss = 0.01193160
Iteration 110, loss = 0.01189945
Iteration 111, loss = 0.01200388
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.66983391
Iteration 2, loss = 0.65163356
Iteration 3, loss = 0.63553221
Iteration 4, loss = 0.61992871
Iteration 5, loss = 0.60384937
Iteration 6, loss = 0.58653724
Iteration 7, loss = 0.56821990
Iteration 8, loss = 0.54896680
Iteration 9, loss = 0.52865669
Iteration 10, loss = 0.50768284
Iteration 11, loss = 0.48596278
Iteration 12, loss = 0.46416829
Iteration 13, loss = 0.44217212
Iteration 14, loss = 0.42043528
Iteration 15, loss = 0.39906728
Iteration 16, loss = 0.37826309
Iteration 17, loss = 0.35806389
Iteration 18, loss = 0.33849699
Iteration 19, loss = 0.31983217
Iteration 20, loss = 0.30205004
Iteration 21, loss = 0.28494799
Iteration 22, loss = 0.26875782
Iteration 23, loss = 0.25322487
Iteration 24, loss = 0.23872377
Iteration 25, loss = 0.22500037
Iteration 26, loss = 0.21201005
Iteration 27, loss = 0.19979482
Iteration 28, loss = 0.18828273
Iteration 29, loss = 0.17763495
Iteration 30, loss = 0.16747990
Iteration 31, loss = 0.15811306
Iteration 32, loss = 0.14929013
Iteration 33, loss = 0.14098887
Iteration 34, loss = 0.13348756
Iteration 35, loss = 0.12627876
Iteration 36, loss = 0.11963011
Iteration 37, loss = 0.11346881
Iteration 38, loss = 0.10781573
Iteration 39, loss = 0.10244447
Iteration 40, loss = 0.09750759
Iteration 41, loss = 0.09287275
Iteration 42, loss = 0.08858966
Iteration 43, loss = 0.08459373
Iteration 44, loss = 0.08081105
Iteration 45, loss = 0.07734824
Iteration 46, loss = 0.07408156
Iteration 47, loss = 0.07103321
Iteration 48, loss = 0.06806047
Iteration 49, loss = 0.06543733
Iteration 50, loss = 0.06285744
Iteration 51, loss = 0.06047845
Iteration 52, loss = 0.05822216
Iteration 53, loss = 0.05611180
Iteration 54, loss = 0.05413788
Iteration 55, loss = 0.05222306
Iteration 56, loss = 0.05047273
Iteration 57, loss = 0.04880394
Iteration 58, loss = 0.04721085
Iteration 59, loss = 0.04574033
Iteration 60, loss = 0.04433459
Iteration 61, loss = 0.04294462
Iteration 62, loss = 0.04169409
Iteration 63, loss = 0.04052416
Iteration 64, loss = 0.03937292
Iteration 65, loss = 0.03829191
Iteration 66, loss = 0.03724423
Iteration 67, loss = 0.03627649
Iteration 68, loss = 0.03532902
Iteration 69, loss = 0.03443838
Iteration 70, loss = 0.03356921
Iteration 71, loss = 0.03278716
Iteration 72, loss = 0.03198243
Iteration 73, loss = 0.03123258
Iteration 74, loss = 0.03053555
Iteration 75, loss = 0.02985457
Iteration 76, loss = 0.02922228
Iteration 77, loss = 0.02862292
Iteration 78, loss = 0.02799799
Iteration 79, loss = 0.02739287
Iteration 80, loss = 0.02684696
Iteration 81, loss = 0.02636083
Iteration 82, loss = 0.02581548
Iteration 83, loss = 0.02535089
Iteration 84, loss = 0.02485246
Iteration 85, loss = 0.02436666
Iteration 86, loss = 0.02392512
Iteration 87, loss = 0.02350679
Iteration 88, loss = 0.02310599
Iteration 89, loss = 0.02273138
Iteration 90, loss = 0.02234627
Iteration 91, loss = 0.02198743
Iteration 92, loss = 0.02164083
Iteration 93, loss = 0.02131553
Iteration 94, loss = 0.02100555
Iteration 95, loss = 0.02067797
Iteration 96, loss = 0.02037169
Iteration 97, loss = 0.02006303
Iteration 98, loss = 0.01975198
Iteration 99, loss = 0.01953308
Iteration 100, loss = 0.01919082
Iteration 101, loss = 0.01891947
Iteration 102, loss = 0.01865365
Iteration 103, loss = 0.01840999
Iteration 104, loss = 0.01816252
Iteration 105, loss = 0.01793995
Iteration 106, loss = 0.01771879
Iteration 107, loss = 0.01748912
Iteration 108, loss = 0.01729961
Iteration 109, loss = 0.01707358
Iteration 110, loss = 0.01688866
Iteration 111, loss = 0.01673886
Iteration 112, loss = 0.01652022
Iteration 113, loss = 0.01630195
Iteration 114, loss = 0.01615486
Iteration 115, loss = 0.01598761
Iteration 116, loss = 0.01581643
Iteration 117, loss = 0.01564254
Iteration 118, loss = 0.01550360
Iteration 119, loss = 0.01534884
Iteration 120, loss = 0.01518375
Iteration 121, loss = 0.01505064
Iteration 122, loss = 0.01490186
Iteration 123, loss = 0.01474991
Iteration 124, loss = 0.01461272
Iteration 125, loss = 0.01447929
Iteration 126, loss = 0.01434398
Iteration 127, loss = 0.01421578
Iteration 128, loss = 0.01410349
Iteration 129, loss = 0.01397919
Iteration 130, loss = 0.01386487
Iteration 131, loss = 0.01375539
Iteration 132, loss = 0.01364110
Iteration 133, loss = 0.01354611
Iteration 134, loss = 0.01342477
Iteration 135, loss = 0.01330300
Iteration 136, loss = 0.01321359
Iteration 137, loss = 0.01312628
Iteration 138, loss = 0.01300605
Iteration 139, loss = 0.01292780
Iteration 140, loss = 0.01283389
Iteration 141, loss = 0.01274230
Iteration 142, loss = 0.01264731
Iteration 143, loss = 0.01256767
Iteration 144, loss = 0.01246686
Iteration 145, loss = 0.01238556
Iteration 146, loss = 0.01229601
Iteration 147, loss = 0.01221919
Iteration 148, loss = 0.01216478
Iteration 149, loss = 0.01207740
Iteration 150, loss = 0.01201585
Iteration 151, loss = 0.01192259
Iteration 152, loss = 0.01186926
Iteration 153, loss = 0.01179401
Iteration 154, loss = 0.01173715
Iteration 155, loss = 0.01167010
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68734342
Iteration 2, loss = 0.65917583
Iteration 3, loss = 0.62694725
Iteration 4, loss = 0.58863213
Iteration 5, loss = 0.54394824
Iteration 6, loss = 0.49520284
Iteration 7, loss = 0.44467961
Iteration 8, loss = 0.39504694
Iteration 9, loss = 0.34868375
Iteration 10, loss = 0.30659867
Iteration 11, loss = 0.26925835
Iteration 12, loss = 0.23658620
Iteration 13, loss = 0.20823389
Iteration 14, loss = 0.18395040
Iteration 15, loss = 0.16314318
Iteration 16, loss = 0.14539517
Iteration 17, loss = 0.13033776
Iteration 18, loss = 0.11748143
Iteration 19, loss = 0.10639880
Iteration 20, loss = 0.09702164
Iteration 21, loss = 0.08873093
Iteration 22, loss = 0.08155570
Iteration 23, loss = 0.07541012
Iteration 24, loss = 0.06983004
Iteration 25, loss = 0.06502532
Iteration 26, loss = 0.06077003
Iteration 27, loss = 0.05696376
Iteration 28, loss = 0.05365408
Iteration 29, loss = 0.05051640
Iteration 30, loss = 0.04781126
Iteration 31, loss = 0.04537654
Iteration 32, loss = 0.04311953
Iteration 33, loss = 0.04116087
Iteration 34, loss = 0.03931674
Iteration 35, loss = 0.03761252
Iteration 36, loss = 0.03627579
Iteration 37, loss = 0.03480195
Iteration 38, loss = 0.03357681
Iteration 39, loss = 0.03240109
Iteration 40, loss = 0.03125030
Iteration 41, loss = 0.03048665
Iteration 42, loss = 0.02967489
Iteration 43, loss = 0.02867649
Iteration 44, loss = 0.02777466
Iteration 45, loss = 0.02687953
Iteration 46, loss = 0.02622700
Iteration 47, loss = 0.02558664
Iteration 48, loss = 0.02494406
Iteration 49, loss = 0.02461775
Iteration 50, loss = 0.02412155
Iteration 51, loss = 0.02360047
Iteration 52, loss = 0.02314931
Iteration 53, loss = 0.02266177
Iteration 54, loss = 0.02221357
Iteration 55, loss = 0.02178113
Iteration 56, loss = 0.02140939
Iteration 57, loss = 0.02105892
Iteration 58, loss = 0.02071931
Iteration 59, loss = 0.02034551
Iteration 60, loss = 0.01998038
Iteration 61, loss = 0.01966706
Iteration 62, loss = 0.01938189
Iteration 63, loss = 0.01903989
Iteration 64, loss = 0.01890275
Iteration 65, loss = 0.01892343
Iteration 66, loss = 0.01850802
Iteration 67, loss = 0.01851300
Iteration 68, loss = 0.01814635
Iteration 69, loss = 0.01796416
Iteration 70, loss = 0.01771554
Iteration 71, loss = 0.01750789
Iteration 72, loss = 0.01737897
Iteration 73, loss = 0.01710706
Iteration 74, loss = 0.01694830
Iteration 75, loss = 0.01678965
Iteration 76, loss = 0.01663893
Iteration 77, loss = 0.01648598
Iteration 78, loss = 0.01635395
Iteration 79, loss = 0.01620517
Iteration 80, loss = 0.01608499
Iteration 81, loss = 0.01600689
Iteration 82, loss = 0.01583134
Iteration 83, loss = 0.01573804
Iteration 84, loss = 0.01562249
Iteration 85, loss = 0.01548466
Iteration 86, loss = 0.01541193
Iteration 87, loss = 0.01527123
Iteration 88, loss = 0.01535831
Iteration 89, loss = 0.01522673
Iteration 90, loss = 0.01514094
Iteration 91, loss = 0.01499625
Iteration 92, loss = 0.01485970
Iteration 93, loss = 0.01478924
Iteration 94, loss = 0.01475397
Iteration 95, loss = 0.01451870
Iteration 96, loss = 0.01510268
Iteration 97, loss = 0.01488278
Iteration 98, loss = 0.01508791
Iteration 99, loss = 0.01500476
Iteration 100, loss = 0.01478826
Iteration 101, loss = 0.01463738
Iteration 102, loss = 0.01440663
Iteration 103, loss = 0.01427504
Iteration 104, loss = 0.01411961
Iteration 105, loss = 0.01409595
Iteration 106, loss = 0.01408273
Iteration 107, loss = 0.01400267
Iteration 108, loss = 0.01395077
Iteration 109, loss = 0.01393222
Iteration 110, loss = 0.01399620
Iteration 111, loss = 0.01387460
Iteration 112, loss = 0.01394942
Iteration 113, loss = 0.01398261
Iteration 114, loss = 0.01389374
Iteration 115, loss = 0.01416924
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67167949
Iteration 2, loss = 0.59241243
Iteration 3, loss = 0.48224309
Iteration 4, loss = 0.37391077
Iteration 5, loss = 0.28367614
Iteration 6, loss = 0.21581737
Iteration 7, loss = 0.16736736
Iteration 8, loss = 0.13359807
Iteration 9, loss = 0.10969642
Iteration 10, loss = 0.09228089
Iteration 11, loss = 0.07959828
Iteration 12, loss = 0.06982502
Iteration 13, loss = 0.06236410
Iteration 14, loss = 0.05654980
Iteration 15, loss = 0.05144056
Iteration 16, loss = 0.04780884
Iteration 17, loss = 0.04454236
Iteration 18, loss = 0.04173401
Iteration 19, loss = 0.03969891
Iteration 20, loss = 0.03752312
Iteration 21, loss = 0.03591639
Iteration 22, loss = 0.03464274
Iteration 23, loss = 0.03333638
Iteration 24, loss = 0.03210097
Iteration 25, loss = 0.03140525
Iteration 26, loss = 0.03062326
Iteration 27, loss = 0.02950614
Iteration 28, loss = 0.02902242
Iteration 29, loss = 0.02839790
Iteration 30, loss = 0.02793736
Iteration 31, loss = 0.02773218
Iteration 32, loss = 0.02695876
Iteration 33, loss = 0.02654928
Iteration 34, loss = 0.02625558
Iteration 35, loss = 0.02580527
Iteration 36, loss = 0.02563741
Iteration 37, loss = 0.02530244
Iteration 38, loss = 0.02529129
Iteration 39, loss = 0.02478286
Iteration 40, loss = 0.02470134
Iteration 41, loss = 0.02465012
Iteration 42, loss = 0.02448500
Iteration 43, loss = 0.02431656
Iteration 44, loss = 0.02399753
Iteration 45, loss = 0.02369502
Iteration 46, loss = 0.02364412
Iteration 47, loss = 0.02364196
Iteration 48, loss = 0.02338356
Iteration 49, loss = 0.02358090
Iteration 50, loss = 0.02321450
Iteration 51, loss = 0.02319531
Iteration 52, loss = 0.02292733
Iteration 53, loss = 0.02296144
Iteration 54, loss = 0.02306123
Iteration 55, loss = 0.02302969
Iteration 56, loss = 0.02263708
Iteration 57, loss = 0.02267546
Iteration 58, loss = 0.02261779
Iteration 59, loss = 0.02245132
Iteration 60, loss = 0.02246052
Iteration 61, loss = 0.02254049
Iteration 62, loss = 0.02249191
Iteration 63, loss = 0.02237195
Iteration 64, loss = 0.02213597
Iteration 65, loss = 0.02202500
Iteration 66, loss = 0.02219043
Iteration 67, loss = 0.02206235
Iteration 68, loss = 0.02202424
Iteration 69, loss = 0.02215087
Iteration 70, loss = 0.02178622
Iteration 71, loss = 0.02190854
Iteration 72, loss = 0.02235791
Iteration 73, loss = 0.02206005
Iteration 74, loss = 0.02178630
Iteration 75, loss = 0.02175542
Iteration 76, loss = 0.02162498
Iteration 77, loss = 0.02183262
Iteration 78, loss = 0.02180486
Iteration 79, loss = 0.02189076
Iteration 80, loss = 0.02169794
Iteration 81, loss = 0.02177855
Iteration 82, loss = 0.02148690
Iteration 83, loss = 0.02155395
Iteration 84, loss = 0.02148607
Iteration 85, loss = 0.02154970
Iteration 86, loss = 0.02162522
Iteration 87, loss = 0.02148316
Iteration 88, loss = 0.02155477
Iteration 89, loss = 0.02158818
Iteration 90, loss = 0.02134060
Iteration 91, loss = 0.02147797
Iteration 92, loss = 0.02154096
Iteration 93, loss = 0.02157764
Iteration 94, loss = 0.02135332
Iteration 95, loss = 0.02158812
Iteration 96, loss = 0.02133110
Iteration 97, loss = 0.02139307
Iteration 98, loss = 0.02134109
Iteration 99, loss = 0.02122825
Iteration 100, loss = 0.02151260
Iteration 101, loss = 0.02116001
Iteration 102, loss = 0.02127487
Iteration 103, loss = 0.02149230
Iteration 104, loss = 0.02163626
Iteration 105, loss = 0.02137346
Iteration 106, loss = 0.02109337
Iteration 107, loss = 0.02133593
Iteration 108, loss = 0.02124773
Iteration 109, loss = 0.02136395
Iteration 110, loss = 0.02101663
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	80m20.154s
user	342m59.931s
sys	35m7.138s
Elapsed time: 4821 seconds
