Processing dataset: dropped_v9_minimal_processing

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: dropped_v9_minimal_processing (7423 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on dropped_v9_minimal_processing...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on dropped_v9_minimal_processing...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on dropped_v9_minimal_processing...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on dropped_v9_minimal_processing...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on dropped_v9_minimal_processing...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67881151
Iteration 2, loss = 0.60471584
Iteration 3, loss = 0.49092188
Iteration 4, loss = 0.37554028
Iteration 5, loss = 0.27846706
Iteration 6, loss = 0.20632214
Iteration 7, loss = 0.15606845
Iteration 8, loss = 0.12140494
Iteration 9, loss = 0.09751731
Iteration 10, loss = 0.08074352
Iteration 11, loss = 0.06842071
Iteration 12, loss = 0.05917864
Iteration 13, loss = 0.05223344
Iteration 14, loss = 0.04669077
Iteration 15, loss = 0.04231049
Iteration 16, loss = 0.03892984
Iteration 17, loss = 0.03582549
Iteration 18, loss = 0.03352017
Iteration 19, loss = 0.03142966
Iteration 20, loss = 0.02963018
Iteration 21, loss = 0.02825559
Iteration 22, loss = 0.02689145
Iteration 23, loss = 0.02584806
Iteration 24, loss = 0.02492755
Iteration 25, loss = 0.02389247
Iteration 26, loss = 0.02322001
Iteration 27, loss = 0.02248081
Iteration 28, loss = 0.02192914
Iteration 29, loss = 0.02138298
Iteration 30, loss = 0.02091104
Iteration 31, loss = 0.02062797
Iteration 32, loss = 0.02017858
Iteration 33, loss = 0.01983474
Iteration 34, loss = 0.01948492
Iteration 35, loss = 0.01919305
Iteration 36, loss = 0.01899734
Iteration 37, loss = 0.01852153
Iteration 38, loss = 0.01830259
Iteration 39, loss = 0.01817116
Iteration 40, loss = 0.01795707
Iteration 41, loss = 0.01782633
Iteration 42, loss = 0.01781325
Iteration 43, loss = 0.01744891
Iteration 44, loss = 0.01735780
Iteration 45, loss = 0.01746317
Iteration 46, loss = 0.01707142
Iteration 47, loss = 0.01717065
Iteration 48, loss = 0.01681137
Iteration 49, loss = 0.01675178
Iteration 50, loss = 0.01680326
Iteration 51, loss = 0.01652406
Iteration 52, loss = 0.01637541
Iteration 53, loss = 0.01628582
Iteration 54, loss = 0.01643194
Iteration 55, loss = 0.01631005
Iteration 56, loss = 0.01613915
Iteration 57, loss = 0.01592190
Iteration 58, loss = 0.01619224
Iteration 59, loss = 0.01601579
Iteration 60, loss = 0.01608435
Iteration 61, loss = 0.01594136
Iteration 62, loss = 0.01595276
Iteration 63, loss = 0.01589486
Iteration 64, loss = 0.01561644
Iteration 65, loss = 0.01555685
Iteration 66, loss = 0.01555485
Iteration 67, loss = 0.01570948
Iteration 68, loss = 0.01572961
Iteration 69, loss = 0.01559894
Iteration 70, loss = 0.01540700
Iteration 71, loss = 0.01536119
Iteration 72, loss = 0.01555666
Iteration 73, loss = 0.01525030
Iteration 74, loss = 0.01516497
Iteration 75, loss = 0.01522298
Iteration 76, loss = 0.01534127
Iteration 77, loss = 0.01513248
Iteration 78, loss = 0.01516378
Iteration 79, loss = 0.01510524
Iteration 80, loss = 0.01515163
Iteration 81, loss = 0.01543440
Iteration 82, loss = 0.01507641
Iteration 83, loss = 0.01497756
Iteration 84, loss = 0.01496116
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68224994
Iteration 2, loss = 0.60427402
Iteration 3, loss = 0.48810838
Iteration 4, loss = 0.37306006
Iteration 5, loss = 0.27640694
Iteration 6, loss = 0.20455322
Iteration 7, loss = 0.15430598
Iteration 8, loss = 0.12016348
Iteration 9, loss = 0.09655501
Iteration 10, loss = 0.07967325
Iteration 11, loss = 0.06761844
Iteration 12, loss = 0.05863707
Iteration 13, loss = 0.05158622
Iteration 14, loss = 0.04602476
Iteration 15, loss = 0.04176632
Iteration 16, loss = 0.03816763
Iteration 17, loss = 0.03519615
Iteration 18, loss = 0.03292312
Iteration 19, loss = 0.03078569
Iteration 20, loss = 0.02907526
Iteration 21, loss = 0.02760849
Iteration 22, loss = 0.02625800
Iteration 23, loss = 0.02526232
Iteration 24, loss = 0.02429905
Iteration 25, loss = 0.02333954
Iteration 26, loss = 0.02258020
Iteration 27, loss = 0.02192514
Iteration 28, loss = 0.02128234
Iteration 29, loss = 0.02110595
Iteration 30, loss = 0.02042667
Iteration 31, loss = 0.01989648
Iteration 32, loss = 0.01935498
Iteration 33, loss = 0.01917547
Iteration 34, loss = 0.01864750
Iteration 35, loss = 0.01845732
Iteration 36, loss = 0.01813082
Iteration 37, loss = 0.01794353
Iteration 38, loss = 0.01762063
Iteration 39, loss = 0.01760309
Iteration 40, loss = 0.01716377
Iteration 41, loss = 0.01705696
Iteration 42, loss = 0.01689310
Iteration 43, loss = 0.01670259
Iteration 44, loss = 0.01644809
Iteration 45, loss = 0.01634951
Iteration 46, loss = 0.01618967
Iteration 47, loss = 0.01640501
Iteration 48, loss = 0.01612720
Iteration 49, loss = 0.01581805
Iteration 50, loss = 0.01589953
Iteration 51, loss = 0.01569058
Iteration 52, loss = 0.01564824
Iteration 53, loss = 0.01549870
Iteration 54, loss = 0.01535147
Iteration 55, loss = 0.01513372
Iteration 56, loss = 0.01530958
Iteration 57, loss = 0.01537766
Iteration 58, loss = 0.01514903
Iteration 59, loss = 0.01513768
Iteration 60, loss = 0.01497988
Iteration 61, loss = 0.01485562
Iteration 62, loss = 0.01519696
Iteration 63, loss = 0.01498780
Iteration 64, loss = 0.01461615
Iteration 65, loss = 0.01458456
Iteration 66, loss = 0.01461687
Iteration 67, loss = 0.01455461
Iteration 68, loss = 0.01458732
Iteration 69, loss = 0.01430666
Iteration 70, loss = 0.01470113
Iteration 71, loss = 0.01461111
Iteration 72, loss = 0.01437899
Iteration 73, loss = 0.01434830
Iteration 74, loss = 0.01420991
Iteration 75, loss = 0.01424351
Iteration 76, loss = 0.01440742
Iteration 77, loss = 0.01411081
Iteration 78, loss = 0.01413017
Iteration 79, loss = 0.01415860
Iteration 80, loss = 0.01411532
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69401083
Iteration 2, loss = 0.63060414
Iteration 3, loss = 0.52581750
Iteration 4, loss = 0.40417909
Iteration 5, loss = 0.29837068
Iteration 6, loss = 0.21973733
Iteration 7, loss = 0.16480908
Iteration 8, loss = 0.12781812
Iteration 9, loss = 0.10231347
Iteration 10, loss = 0.08426318
Iteration 11, loss = 0.07175619
Iteration 12, loss = 0.06198398
Iteration 13, loss = 0.05482810
Iteration 14, loss = 0.04922683
Iteration 15, loss = 0.04474525
Iteration 16, loss = 0.04120224
Iteration 17, loss = 0.03832806
Iteration 18, loss = 0.03566218
Iteration 19, loss = 0.03371702
Iteration 20, loss = 0.03198801
Iteration 21, loss = 0.03034117
Iteration 22, loss = 0.02901877
Iteration 23, loss = 0.02783818
Iteration 24, loss = 0.02724528
Iteration 25, loss = 0.02616644
Iteration 26, loss = 0.02525337
Iteration 27, loss = 0.02465991
Iteration 28, loss = 0.02423853
Iteration 29, loss = 0.02340160
Iteration 30, loss = 0.02295640
Iteration 31, loss = 0.02240912
Iteration 32, loss = 0.02230797
Iteration 33, loss = 0.02180052
Iteration 34, loss = 0.02134678
Iteration 35, loss = 0.02114477
Iteration 36, loss = 0.02068767
Iteration 37, loss = 0.02055192
Iteration 38, loss = 0.02023156
Iteration 39, loss = 0.02021523
Iteration 40, loss = 0.01987222
Iteration 41, loss = 0.01968943
Iteration 42, loss = 0.01940097
Iteration 43, loss = 0.01937670
Iteration 44, loss = 0.01934405
Iteration 45, loss = 0.01908866
Iteration 46, loss = 0.01902012
Iteration 47, loss = 0.01894018
Iteration 48, loss = 0.01862553
Iteration 49, loss = 0.01857512
Iteration 50, loss = 0.01850714
Iteration 51, loss = 0.01838611
Iteration 52, loss = 0.01838414
Iteration 53, loss = 0.01826244
Iteration 54, loss = 0.01824093
Iteration 55, loss = 0.01838761
Iteration 56, loss = 0.01805568
Iteration 57, loss = 0.01791575
Iteration 58, loss = 0.01819573
Iteration 59, loss = 0.01775816
Iteration 60, loss = 0.01784511
Iteration 61, loss = 0.01782547
Iteration 62, loss = 0.01766641
Iteration 63, loss = 0.01777344
Iteration 64, loss = 0.01739428
Iteration 65, loss = 0.01752288
Iteration 66, loss = 0.01740247
Iteration 67, loss = 0.01754859
Iteration 68, loss = 0.01754743
Iteration 69, loss = 0.01766569
Iteration 70, loss = 0.01725486
Iteration 71, loss = 0.01731770
Iteration 72, loss = 0.01727801
Iteration 73, loss = 0.01736053
Iteration 74, loss = 0.01722414
Iteration 75, loss = 0.01711610
Iteration 76, loss = 0.01701727
Iteration 77, loss = 0.01720706
Iteration 78, loss = 0.01712720
Iteration 79, loss = 0.01716162
Iteration 80, loss = 0.01727433
Iteration 81, loss = 0.01680725
Iteration 82, loss = 0.01713320
Iteration 83, loss = 0.01729013
Iteration 84, loss = 0.01695458
Iteration 85, loss = 0.01701731
Iteration 86, loss = 0.01690402
Iteration 87, loss = 0.01668783
Iteration 88, loss = 0.01686667
Iteration 89, loss = 0.01690415
Iteration 90, loss = 0.01682637
Iteration 91, loss = 0.01677495
Iteration 92, loss = 0.01691548
Iteration 93, loss = 0.01682730
Iteration 94, loss = 0.01687318
Iteration 95, loss = 0.01653820
Iteration 96, loss = 0.01673239
Iteration 97, loss = 0.01683764
Iteration 98, loss = 0.01682618
Iteration 99, loss = 0.01676407
Iteration 100, loss = 0.01671260
Iteration 101, loss = 0.01650782
Iteration 102, loss = 0.01664787
Iteration 103, loss = 0.01650516
Iteration 104, loss = 0.01677718
Iteration 105, loss = 0.01663376
Iteration 106, loss = 0.01652839
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67412444
Iteration 2, loss = 0.60163508
Iteration 3, loss = 0.49217210
Iteration 4, loss = 0.37821174
Iteration 5, loss = 0.28127883
Iteration 6, loss = 0.20829956
Iteration 7, loss = 0.15725677
Iteration 8, loss = 0.12264694
Iteration 9, loss = 0.09894681
Iteration 10, loss = 0.08214007
Iteration 11, loss = 0.07021740
Iteration 12, loss = 0.06095265
Iteration 13, loss = 0.05411476
Iteration 14, loss = 0.04882467
Iteration 15, loss = 0.04442309
Iteration 16, loss = 0.04100422
Iteration 17, loss = 0.03824716
Iteration 18, loss = 0.03590784
Iteration 19, loss = 0.03377774
Iteration 20, loss = 0.03239713
Iteration 21, loss = 0.03085302
Iteration 22, loss = 0.02951850
Iteration 23, loss = 0.02852727
Iteration 24, loss = 0.02747100
Iteration 25, loss = 0.02695738
Iteration 26, loss = 0.02609261
Iteration 27, loss = 0.02539824
Iteration 28, loss = 0.02495799
Iteration 29, loss = 0.02434046
Iteration 30, loss = 0.02401735
Iteration 31, loss = 0.02370591
Iteration 32, loss = 0.02291264
Iteration 33, loss = 0.02293918
Iteration 34, loss = 0.02267294
Iteration 35, loss = 0.02208159
Iteration 36, loss = 0.02201433
Iteration 37, loss = 0.02167214
Iteration 38, loss = 0.02135715
Iteration 39, loss = 0.02140515
Iteration 40, loss = 0.02108880
Iteration 41, loss = 0.02112052
Iteration 42, loss = 0.02083799
Iteration 43, loss = 0.02066658
Iteration 44, loss = 0.02054936
Iteration 45, loss = 0.02021304
Iteration 46, loss = 0.02020347
Iteration 47, loss = 0.02008435
Iteration 48, loss = 0.02003100
Iteration 49, loss = 0.01991607
Iteration 50, loss = 0.01979234
Iteration 51, loss = 0.01968777
Iteration 52, loss = 0.01961920
Iteration 53, loss = 0.01964344
Iteration 54, loss = 0.01958408
Iteration 55, loss = 0.01926496
Iteration 56, loss = 0.01952087
Iteration 57, loss = 0.01918919
Iteration 58, loss = 0.01910700
Iteration 59, loss = 0.01923314
Iteration 60, loss = 0.01921018
Iteration 61, loss = 0.01907953
Iteration 62, loss = 0.01921575
Iteration 63, loss = 0.01891689
Iteration 64, loss = 0.01894149
Iteration 65, loss = 0.01888074
Iteration 66, loss = 0.01886500
Iteration 67, loss = 0.01876612
Iteration 68, loss = 0.01883702
Iteration 69, loss = 0.01869769
Iteration 70, loss = 0.01882073
Iteration 71, loss = 0.01873869
Iteration 72, loss = 0.01889587
Iteration 73, loss = 0.01867208
Iteration 74, loss = 0.01844980
Iteration 75, loss = 0.01869394
Iteration 76, loss = 0.01843452
Iteration 77, loss = 0.01841471
Iteration 78, loss = 0.01852169
Iteration 79, loss = 0.01837471
Iteration 80, loss = 0.01838881
Iteration 81, loss = 0.01817151
Iteration 82, loss = 0.01843313
Iteration 83, loss = 0.01817966
Iteration 84, loss = 0.01820396
Iteration 85, loss = 0.01822998
Iteration 86, loss = 0.01853816
Iteration 87, loss = 0.01815975
Iteration 88, loss = 0.01845597
Iteration 89, loss = 0.01811972
Iteration 90, loss = 0.01819822
Iteration 91, loss = 0.01829504
Iteration 92, loss = 0.01827291
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68294759
Iteration 2, loss = 0.60882801
Iteration 3, loss = 0.49757317
Iteration 4, loss = 0.37976612
Iteration 5, loss = 0.28016835
Iteration 6, loss = 0.20651044
Iteration 7, loss = 0.15513583
Iteration 8, loss = 0.12062791
Iteration 9, loss = 0.09703143
Iteration 10, loss = 0.08018264
Iteration 11, loss = 0.06807485
Iteration 12, loss = 0.05920994
Iteration 13, loss = 0.05236235
Iteration 14, loss = 0.04702976
Iteration 15, loss = 0.04290492
Iteration 16, loss = 0.03932677
Iteration 17, loss = 0.03673448
Iteration 18, loss = 0.03434620
Iteration 19, loss = 0.03234751
Iteration 20, loss = 0.03085375
Iteration 21, loss = 0.02943645
Iteration 22, loss = 0.02813142
Iteration 23, loss = 0.02713438
Iteration 24, loss = 0.02630421
Iteration 25, loss = 0.02546451
Iteration 26, loss = 0.02478844
Iteration 27, loss = 0.02414334
Iteration 28, loss = 0.02361310
Iteration 29, loss = 0.02290231
Iteration 30, loss = 0.02272725
Iteration 31, loss = 0.02217854
Iteration 32, loss = 0.02177112
Iteration 33, loss = 0.02179050
Iteration 34, loss = 0.02122721
Iteration 35, loss = 0.02102174
Iteration 36, loss = 0.02094749
Iteration 37, loss = 0.02052783
Iteration 38, loss = 0.02061011
Iteration 39, loss = 0.01982963
Iteration 40, loss = 0.01992750
Iteration 41, loss = 0.01960741
Iteration 42, loss = 0.01963984
Iteration 43, loss = 0.01947809
Iteration 44, loss = 0.01920313
Iteration 45, loss = 0.01914916
Iteration 46, loss = 0.01896509
Iteration 47, loss = 0.01884134
Iteration 48, loss = 0.01875330
Iteration 49, loss = 0.01887449
Iteration 50, loss = 0.01874217
Iteration 51, loss = 0.01834959
Iteration 52, loss = 0.01855855
Iteration 53, loss = 0.01835906
Iteration 54, loss = 0.01829528
Iteration 55, loss = 0.01827629
Iteration 56, loss = 0.01824199
Iteration 57, loss = 0.01818737
Iteration 58, loss = 0.01832179
Iteration 59, loss = 0.01786738
Iteration 60, loss = 0.01796318
Iteration 61, loss = 0.01786701
Iteration 62, loss = 0.01777047
Iteration 63, loss = 0.01769711
Iteration 64, loss = 0.01767515
Iteration 65, loss = 0.01761303
Iteration 66, loss = 0.01758037
Iteration 67, loss = 0.01767066
Iteration 68, loss = 0.01765102
Iteration 69, loss = 0.01764444
Iteration 70, loss = 0.01771372
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67881151
Iteration 2, loss = 0.60471584
Iteration 3, loss = 0.49092188
Iteration 4, loss = 0.37554028
Iteration 5, loss = 0.27846706
Iteration 6, loss = 0.20632214
Iteration 7, loss = 0.15606845
Iteration 8, loss = 0.12140494
Iteration 9, loss = 0.09751731
Iteration 10, loss = 0.08074352
Iteration 11, loss = 0.06842071
Iteration 12, loss = 0.05917864
Iteration 13, loss = 0.05223344
Iteration 14, loss = 0.04669077
Iteration 15, loss = 0.04231049
Iteration 16, loss = 0.03892984
Iteration 17, loss = 0.03582549
Iteration 18, loss = 0.03352017
Iteration 19, loss = 0.03142966
Iteration 20, loss = 0.02963018
Iteration 21, loss = 0.02825559
Iteration 22, loss = 0.02689145
Iteration 23, loss = 0.02584806
Iteration 24, loss = 0.02492755
Iteration 25, loss = 0.02389247
Iteration 26, loss = 0.02322001
Iteration 27, loss = 0.02248081
Iteration 28, loss = 0.02192914
Iteration 29, loss = 0.02138298
Iteration 30, loss = 0.02091104
Iteration 31, loss = 0.02062797
Iteration 32, loss = 0.02017858
Iteration 33, loss = 0.01983474
Iteration 34, loss = 0.01948492
Iteration 35, loss = 0.01919305
Iteration 36, loss = 0.01899734
Iteration 37, loss = 0.01852153
Iteration 38, loss = 0.01830259
Iteration 39, loss = 0.01817116
Iteration 40, loss = 0.01795707
Iteration 41, loss = 0.01782633
Iteration 42, loss = 0.01781325
Iteration 43, loss = 0.01744891
Iteration 44, loss = 0.01735780
Iteration 45, loss = 0.01746317
Iteration 46, loss = 0.01707142
Iteration 47, loss = 0.01717065
Iteration 48, loss = 0.01681137
Iteration 49, loss = 0.01675178
Iteration 50, loss = 0.01680326
Iteration 51, loss = 0.01652406
Iteration 52, loss = 0.01637541
Iteration 53, loss = 0.01628582
Iteration 54, loss = 0.01643194
Iteration 55, loss = 0.01631005
Iteration 56, loss = 0.01613915
Iteration 57, loss = 0.01592190
Iteration 58, loss = 0.01619224
Iteration 59, loss = 0.01601579
Iteration 60, loss = 0.01608435
Iteration 61, loss = 0.01594136
Iteration 62, loss = 0.01595276
Iteration 63, loss = 0.01589486
Iteration 64, loss = 0.01561644
Iteration 65, loss = 0.01555685
Iteration 66, loss = 0.01555485
Iteration 67, loss = 0.01570948
Iteration 68, loss = 0.01572961
Iteration 69, loss = 0.01559894
Iteration 70, loss = 0.01540700
Iteration 71, loss = 0.01536119
Iteration 72, loss = 0.01555666
Iteration 73, loss = 0.01525030
Iteration 74, loss = 0.01516497
Iteration 75, loss = 0.01522298
Iteration 76, loss = 0.01534127
Iteration 77, loss = 0.01513248
Iteration 78, loss = 0.01516378
Iteration 79, loss = 0.01510524
Iteration 80, loss = 0.01515163
Iteration 81, loss = 0.01543440
Iteration 82, loss = 0.01507641
Iteration 83, loss = 0.01497756
Iteration 84, loss = 0.01496116
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68224994
Iteration 2, loss = 0.60427402
Iteration 3, loss = 0.48810838
Iteration 4, loss = 0.37306006
Iteration 5, loss = 0.27640694
Iteration 6, loss = 0.20455322
Iteration 7, loss = 0.15430598
Iteration 8, loss = 0.12016348
Iteration 9, loss = 0.09655501
Iteration 10, loss = 0.07967325
Iteration 11, loss = 0.06761844
Iteration 12, loss = 0.05863707
Iteration 13, loss = 0.05158622
Iteration 14, loss = 0.04602476
Iteration 15, loss = 0.04176632
Iteration 16, loss = 0.03816763
Iteration 17, loss = 0.03519615
Iteration 18, loss = 0.03292312
Iteration 19, loss = 0.03078569
Iteration 20, loss = 0.02907526
Iteration 21, loss = 0.02760849
Iteration 22, loss = 0.02625800
Iteration 23, loss = 0.02526232
Iteration 24, loss = 0.02429905
Iteration 25, loss = 0.02333954
Iteration 26, loss = 0.02258020
Iteration 27, loss = 0.02192514
Iteration 28, loss = 0.02128234
Iteration 29, loss = 0.02110595
Iteration 30, loss = 0.02042667
Iteration 31, loss = 0.01989648
Iteration 32, loss = 0.01935498
Iteration 33, loss = 0.01917547
Iteration 34, loss = 0.01864750
Iteration 35, loss = 0.01845732
Iteration 36, loss = 0.01813082
Iteration 37, loss = 0.01794353
Iteration 38, loss = 0.01762063
Iteration 39, loss = 0.01760309
Iteration 40, loss = 0.01716377
Iteration 41, loss = 0.01705696
Iteration 42, loss = 0.01689310
Iteration 43, loss = 0.01670259
Iteration 44, loss = 0.01644809
Iteration 45, loss = 0.01634951
Iteration 46, loss = 0.01618967
Iteration 47, loss = 0.01640501
Iteration 48, loss = 0.01612720
Iteration 49, loss = 0.01581805
Iteration 50, loss = 0.01589953
Iteration 51, loss = 0.01569058
Iteration 52, loss = 0.01564824
Iteration 53, loss = 0.01549870
Iteration 54, loss = 0.01535147
Iteration 55, loss = 0.01513372
Iteration 56, loss = 0.01530958
Iteration 57, loss = 0.01537766
Iteration 58, loss = 0.01514903
Iteration 59, loss = 0.01513768
Iteration 60, loss = 0.01497988
Iteration 61, loss = 0.01485562
Iteration 62, loss = 0.01519696
Iteration 63, loss = 0.01498780
Iteration 64, loss = 0.01461615
Iteration 65, loss = 0.01458456
Iteration 66, loss = 0.01461687
Iteration 67, loss = 0.01455461
Iteration 68, loss = 0.01458732
Iteration 69, loss = 0.01430666
Iteration 70, loss = 0.01470113
Iteration 71, loss = 0.01461111
Iteration 72, loss = 0.01437899
Iteration 73, loss = 0.01434830
Iteration 74, loss = 0.01420991
Iteration 75, loss = 0.01424351
Iteration 76, loss = 0.01440742
Iteration 77, loss = 0.01411081
Iteration 78, loss = 0.01413017
Iteration 79, loss = 0.01415860
Iteration 80, loss = 0.01411532
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69401083
Iteration 2, loss = 0.63060414
Iteration 3, loss = 0.52581750
Iteration 4, loss = 0.40417909
Iteration 5, loss = 0.29837068
Iteration 6, loss = 0.21973733
Iteration 7, loss = 0.16480908
Iteration 8, loss = 0.12781812
Iteration 9, loss = 0.10231347
Iteration 10, loss = 0.08426318
Iteration 11, loss = 0.07175619
Iteration 12, loss = 0.06198398
Iteration 13, loss = 0.05482810
Iteration 14, loss = 0.04922683
Iteration 15, loss = 0.04474525
Iteration 16, loss = 0.04120224
Iteration 17, loss = 0.03832806
Iteration 18, loss = 0.03566218
Iteration 19, loss = 0.03371702
Iteration 20, loss = 0.03198801
Iteration 21, loss = 0.03034117
Iteration 22, loss = 0.02901877
Iteration 23, loss = 0.02783818
Iteration 24, loss = 0.02724528
Iteration 25, loss = 0.02616644
Iteration 26, loss = 0.02525337
Iteration 27, loss = 0.02465991
Iteration 28, loss = 0.02423853
Iteration 29, loss = 0.02340160
Iteration 30, loss = 0.02295640
Iteration 31, loss = 0.02240912
Iteration 32, loss = 0.02230797
Iteration 33, loss = 0.02180052
Iteration 34, loss = 0.02134678
Iteration 35, loss = 0.02114477
Iteration 36, loss = 0.02068767
Iteration 37, loss = 0.02055192
Iteration 38, loss = 0.02023156
Iteration 39, loss = 0.02021523
Iteration 40, loss = 0.01987222
Iteration 41, loss = 0.01968943
Iteration 42, loss = 0.01940097
Iteration 43, loss = 0.01937670
Iteration 44, loss = 0.01934405
Iteration 45, loss = 0.01908866
Iteration 46, loss = 0.01902012
Iteration 47, loss = 0.01894018
Iteration 48, loss = 0.01862553
Iteration 49, loss = 0.01857512
Iteration 50, loss = 0.01850714
Iteration 51, loss = 0.01838611
Iteration 52, loss = 0.01838414
Iteration 53, loss = 0.01826244
Iteration 54, loss = 0.01824093
Iteration 55, loss = 0.01838761
Iteration 56, loss = 0.01805568
Iteration 57, loss = 0.01791575
Iteration 58, loss = 0.01819573
Iteration 59, loss = 0.01775816
Iteration 60, loss = 0.01784511
Iteration 61, loss = 0.01782547
Iteration 62, loss = 0.01766641
Iteration 63, loss = 0.01777344
Iteration 64, loss = 0.01739428
Iteration 65, loss = 0.01752288
Iteration 66, loss = 0.01740247
Iteration 67, loss = 0.01754859
Iteration 68, loss = 0.01754743
Iteration 69, loss = 0.01766569
Iteration 70, loss = 0.01725486
Iteration 71, loss = 0.01731770
Iteration 72, loss = 0.01727801
Iteration 73, loss = 0.01736053
Iteration 74, loss = 0.01722414
Iteration 75, loss = 0.01711610
Iteration 76, loss = 0.01701727
Iteration 77, loss = 0.01720706
Iteration 78, loss = 0.01712720
Iteration 79, loss = 0.01716162
Iteration 80, loss = 0.01727433
Iteration 81, loss = 0.01680725
Iteration 82, loss = 0.01713320
Iteration 83, loss = 0.01729013
Iteration 84, loss = 0.01695458
Iteration 85, loss = 0.01701731
Iteration 86, loss = 0.01690402
Iteration 87, loss = 0.01668783
Iteration 88, loss = 0.01686667
Iteration 89, loss = 0.01690415
Iteration 90, loss = 0.01682637
Iteration 91, loss = 0.01677495
Iteration 92, loss = 0.01691548
Iteration 93, loss = 0.01682730
Iteration 94, loss = 0.01687318
Iteration 95, loss = 0.01653820
Iteration 96, loss = 0.01673239
Iteration 97, loss = 0.01683764
Iteration 98, loss = 0.01682618
Iteration 99, loss = 0.01676407
Iteration 100, loss = 0.01671260
Iteration 101, loss = 0.01650782
Iteration 102, loss = 0.01664787
Iteration 103, loss = 0.01650516
Iteration 104, loss = 0.01677718
Iteration 105, loss = 0.01663376
Iteration 106, loss = 0.01652839
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67412444
Iteration 2, loss = 0.60163508
Iteration 3, loss = 0.49217210
Iteration 4, loss = 0.37821174
Iteration 5, loss = 0.28127883
Iteration 6, loss = 0.20829956
Iteration 7, loss = 0.15725677
Iteration 8, loss = 0.12264694
Iteration 9, loss = 0.09894681
Iteration 10, loss = 0.08214007
Iteration 11, loss = 0.07021740
Iteration 12, loss = 0.06095265
Iteration 13, loss = 0.05411476
Iteration 14, loss = 0.04882467
Iteration 15, loss = 0.04442309
Iteration 16, loss = 0.04100422
Iteration 17, loss = 0.03824716
Iteration 18, loss = 0.03590784
Iteration 19, loss = 0.03377774
Iteration 20, loss = 0.03239713
Iteration 21, loss = 0.03085302
Iteration 22, loss = 0.02951850
Iteration 23, loss = 0.02852727
Iteration 24, loss = 0.02747100
Iteration 25, loss = 0.02695738
Iteration 26, loss = 0.02609261
Iteration 27, loss = 0.02539824
Iteration 28, loss = 0.02495799
Iteration 29, loss = 0.02434046
Iteration 30, loss = 0.02401735
Iteration 31, loss = 0.02370591
Iteration 32, loss = 0.02291264
Iteration 33, loss = 0.02293918
Iteration 34, loss = 0.02267294
Iteration 35, loss = 0.02208159
Iteration 36, loss = 0.02201433
Iteration 37, loss = 0.02167214
Iteration 38, loss = 0.02135715
Iteration 39, loss = 0.02140515
Iteration 40, loss = 0.02108880
Iteration 41, loss = 0.02112052
Iteration 42, loss = 0.02083799
Iteration 43, loss = 0.02066658
Iteration 44, loss = 0.02054936
Iteration 45, loss = 0.02021304
Iteration 46, loss = 0.02020347
Iteration 47, loss = 0.02008435
Iteration 48, loss = 0.02003100
Iteration 49, loss = 0.01991607
Iteration 50, loss = 0.01979234
Iteration 51, loss = 0.01968777
Iteration 52, loss = 0.01961920
Iteration 53, loss = 0.01964344
Iteration 54, loss = 0.01958408
Iteration 55, loss = 0.01926496
Iteration 56, loss = 0.01952087
Iteration 57, loss = 0.01918919
Iteration 58, loss = 0.01910700
Iteration 59, loss = 0.01923314
Iteration 60, loss = 0.01921018
Iteration 61, loss = 0.01907953
Iteration 62, loss = 0.01921575
Iteration 63, loss = 0.01891689
Iteration 64, loss = 0.01894149
Iteration 65, loss = 0.01888074
Iteration 66, loss = 0.01886500
Iteration 67, loss = 0.01876612
Iteration 68, loss = 0.01883702
Iteration 69, loss = 0.01869769
Iteration 70, loss = 0.01882073
Iteration 71, loss = 0.01873869
Iteration 72, loss = 0.01889587
Iteration 73, loss = 0.01867208
Iteration 74, loss = 0.01844980
Iteration 75, loss = 0.01869394
Iteration 76, loss = 0.01843452
Iteration 77, loss = 0.01841471
Iteration 78, loss = 0.01852169
Iteration 79, loss = 0.01837471
Iteration 80, loss = 0.01838881
Iteration 81, loss = 0.01817151
Iteration 82, loss = 0.01843313
Iteration 83, loss = 0.01817966
Iteration 84, loss = 0.01820396
Iteration 85, loss = 0.01822998
Iteration 86, loss = 0.01853816
Iteration 87, loss = 0.01815975
Iteration 88, loss = 0.01845597
Iteration 89, loss = 0.01811972
Iteration 90, loss = 0.01819822
Iteration 91, loss = 0.01829504
Iteration 92, loss = 0.01827291
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68294759
Iteration 2, loss = 0.60882801
Iteration 3, loss = 0.49757317
Iteration 4, loss = 0.37976612
Iteration 5, loss = 0.28016835
Iteration 6, loss = 0.20651044
Iteration 7, loss = 0.15513583
Iteration 8, loss = 0.12062791
Iteration 9, loss = 0.09703143
Iteration 10, loss = 0.08018264
Iteration 11, loss = 0.06807485
Iteration 12, loss = 0.05920994
Iteration 13, loss = 0.05236235
Iteration 14, loss = 0.04702976
Iteration 15, loss = 0.04290492
Iteration 16, loss = 0.03932677
Iteration 17, loss = 0.03673448
Iteration 18, loss = 0.03434620
Iteration 19, loss = 0.03234751
Iteration 20, loss = 0.03085375
Iteration 21, loss = 0.02943645
Iteration 22, loss = 0.02813142
Iteration 23, loss = 0.02713438
Iteration 24, loss = 0.02630421
Iteration 25, loss = 0.02546451
Iteration 26, loss = 0.02478844
Iteration 27, loss = 0.02414334
Iteration 28, loss = 0.02361310
Iteration 29, loss = 0.02290231
Iteration 30, loss = 0.02272725
Iteration 31, loss = 0.02217854
Iteration 32, loss = 0.02177112
Iteration 33, loss = 0.02179050
Iteration 34, loss = 0.02122721
Iteration 35, loss = 0.02102174
Iteration 36, loss = 0.02094749
Iteration 37, loss = 0.02052783
Iteration 38, loss = 0.02061011
Iteration 39, loss = 0.01982963
Iteration 40, loss = 0.01992750
Iteration 41, loss = 0.01960741
Iteration 42, loss = 0.01963984
Iteration 43, loss = 0.01947809
Iteration 44, loss = 0.01920313
Iteration 45, loss = 0.01914916
Iteration 46, loss = 0.01896509
Iteration 47, loss = 0.01884134
Iteration 48, loss = 0.01875330
Iteration 49, loss = 0.01887449
Iteration 50, loss = 0.01874217
Iteration 51, loss = 0.01834959
Iteration 52, loss = 0.01855855
Iteration 53, loss = 0.01835906
Iteration 54, loss = 0.01829528
Iteration 55, loss = 0.01827629
Iteration 56, loss = 0.01824199
Iteration 57, loss = 0.01818737
Iteration 58, loss = 0.01832179
Iteration 59, loss = 0.01786738
Iteration 60, loss = 0.01796318
Iteration 61, loss = 0.01786701
Iteration 62, loss = 0.01777047
Iteration 63, loss = 0.01769711
Iteration 64, loss = 0.01767515
Iteration 65, loss = 0.01761303
Iteration 66, loss = 0.01758037
Iteration 67, loss = 0.01767066
Iteration 68, loss = 0.01765102
Iteration 69, loss = 0.01764444
Iteration 70, loss = 0.01771372
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67881151
Iteration 2, loss = 0.60471584
Iteration 3, loss = 0.49092188
Iteration 4, loss = 0.37554028
Iteration 5, loss = 0.27846706
Iteration 6, loss = 0.20632214
Iteration 7, loss = 0.15606845
Iteration 8, loss = 0.12140494
Iteration 9, loss = 0.09751731
Iteration 10, loss = 0.08074352
Iteration 11, loss = 0.06842071
Iteration 12, loss = 0.05917864
Iteration 13, loss = 0.05223344
Iteration 14, loss = 0.04669077
Iteration 15, loss = 0.04231049
Iteration 16, loss = 0.03892984
Iteration 17, loss = 0.03582549
Iteration 18, loss = 0.03352017
Iteration 19, loss = 0.03142966
Iteration 20, loss = 0.02963018
Iteration 21, loss = 0.02825559
Iteration 22, loss = 0.02689145
Iteration 23, loss = 0.02584806
Iteration 24, loss = 0.02492755
Iteration 25, loss = 0.02389247
Iteration 26, loss = 0.02322001
Iteration 27, loss = 0.02248081
Iteration 28, loss = 0.02192914
Iteration 29, loss = 0.02138298
Iteration 30, loss = 0.02091104
Iteration 31, loss = 0.02062797
Iteration 32, loss = 0.02017858
Iteration 33, loss = 0.01983474
Iteration 34, loss = 0.01948492
Iteration 35, loss = 0.01919305
Iteration 36, loss = 0.01899734
Iteration 37, loss = 0.01852153
Iteration 38, loss = 0.01830259
Iteration 39, loss = 0.01817116
Iteration 40, loss = 0.01795707
Iteration 41, loss = 0.01782633
Iteration 42, loss = 0.01781325
Iteration 43, loss = 0.01744891
Iteration 44, loss = 0.01735780
Iteration 45, loss = 0.01746317
Iteration 46, loss = 0.01707142
Iteration 47, loss = 0.01717065
Iteration 48, loss = 0.01681137
Iteration 49, loss = 0.01675178
Iteration 50, loss = 0.01680326
Iteration 51, loss = 0.01652406
Iteration 52, loss = 0.01637541
Iteration 53, loss = 0.01628582
Iteration 54, loss = 0.01643194
Iteration 55, loss = 0.01631005
Iteration 56, loss = 0.01613915
Iteration 57, loss = 0.01592190
Iteration 58, loss = 0.01619224
Iteration 59, loss = 0.01601579
Iteration 60, loss = 0.01608435
Iteration 61, loss = 0.01594136
Iteration 62, loss = 0.01595276
Iteration 63, loss = 0.01589486
Iteration 64, loss = 0.01561644
Iteration 65, loss = 0.01555685
Iteration 66, loss = 0.01555485
Iteration 67, loss = 0.01570948
Iteration 68, loss = 0.01572961
Iteration 69, loss = 0.01559894
Iteration 70, loss = 0.01540700
Iteration 71, loss = 0.01536119
Iteration 72, loss = 0.01555666
Iteration 73, loss = 0.01525030
Iteration 74, loss = 0.01516497
Iteration 75, loss = 0.01522298
Iteration 76, loss = 0.01534127
Iteration 77, loss = 0.01513248
Iteration 78, loss = 0.01516378
Iteration 79, loss = 0.01510524
Iteration 80, loss = 0.01515163
Iteration 81, loss = 0.01543440
Iteration 82, loss = 0.01507641
Iteration 83, loss = 0.01497756
Iteration 84, loss = 0.01496116
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68224994
Iteration 2, loss = 0.60427402
Iteration 3, loss = 0.48810838
Iteration 4, loss = 0.37306006
Iteration 5, loss = 0.27640694
Iteration 6, loss = 0.20455322
Iteration 7, loss = 0.15430598
Iteration 8, loss = 0.12016348
Iteration 9, loss = 0.09655501
Iteration 10, loss = 0.07967325
Iteration 11, loss = 0.06761844
Iteration 12, loss = 0.05863707
Iteration 13, loss = 0.05158622
Iteration 14, loss = 0.04602476
Iteration 15, loss = 0.04176632
Iteration 16, loss = 0.03816763
Iteration 17, loss = 0.03519615
Iteration 18, loss = 0.03292312
Iteration 19, loss = 0.03078569
Iteration 20, loss = 0.02907526
Iteration 21, loss = 0.02760849
Iteration 22, loss = 0.02625800
Iteration 23, loss = 0.02526232
Iteration 24, loss = 0.02429905
Iteration 25, loss = 0.02333954
Iteration 26, loss = 0.02258020
Iteration 27, loss = 0.02192514
Iteration 28, loss = 0.02128234
Iteration 29, loss = 0.02110595
Iteration 30, loss = 0.02042667
Iteration 31, loss = 0.01989648
Iteration 32, loss = 0.01935498
Iteration 33, loss = 0.01917547
Iteration 34, loss = 0.01864750
Iteration 35, loss = 0.01845732
Iteration 36, loss = 0.01813082
Iteration 37, loss = 0.01794353
Iteration 38, loss = 0.01762063
Iteration 39, loss = 0.01760309
Iteration 40, loss = 0.01716377
Iteration 41, loss = 0.01705696
Iteration 42, loss = 0.01689310
Iteration 43, loss = 0.01670259
Iteration 44, loss = 0.01644809
Iteration 45, loss = 0.01634951
Iteration 46, loss = 0.01618967
Iteration 47, loss = 0.01640501
Iteration 48, loss = 0.01612720
Iteration 49, loss = 0.01581805
Iteration 50, loss = 0.01589953
Iteration 51, loss = 0.01569058
Iteration 52, loss = 0.01564824
Iteration 53, loss = 0.01549870
Iteration 54, loss = 0.01535147
Iteration 55, loss = 0.01513372
Iteration 56, loss = 0.01530958
Iteration 57, loss = 0.01537766
Iteration 58, loss = 0.01514903
Iteration 59, loss = 0.01513768
Iteration 60, loss = 0.01497988
Iteration 61, loss = 0.01485562
Iteration 62, loss = 0.01519696
Iteration 63, loss = 0.01498780
Iteration 64, loss = 0.01461615
Iteration 65, loss = 0.01458456
Iteration 66, loss = 0.01461687
Iteration 67, loss = 0.01455461
Iteration 68, loss = 0.01458732
Iteration 69, loss = 0.01430666
Iteration 70, loss = 0.01470113
Iteration 71, loss = 0.01461111
Iteration 72, loss = 0.01437899
Iteration 73, loss = 0.01434830
Iteration 74, loss = 0.01420991
Iteration 75, loss = 0.01424351
Iteration 76, loss = 0.01440742
Iteration 77, loss = 0.01411081
Iteration 78, loss = 0.01413017
Iteration 79, loss = 0.01415860
Iteration 80, loss = 0.01411532
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69401083
Iteration 2, loss = 0.63060414
Iteration 3, loss = 0.52581750
Iteration 4, loss = 0.40417909
Iteration 5, loss = 0.29837068
Iteration 6, loss = 0.21973733
Iteration 7, loss = 0.16480908
Iteration 8, loss = 0.12781812
Iteration 9, loss = 0.10231347
Iteration 10, loss = 0.08426318
Iteration 11, loss = 0.07175619
Iteration 12, loss = 0.06198398
Iteration 13, loss = 0.05482810
Iteration 14, loss = 0.04922683
Iteration 15, loss = 0.04474525
Iteration 16, loss = 0.04120224
Iteration 17, loss = 0.03832806
Iteration 18, loss = 0.03566218
Iteration 19, loss = 0.03371702
Iteration 20, loss = 0.03198801
Iteration 21, loss = 0.03034117
Iteration 22, loss = 0.02901877
Iteration 23, loss = 0.02783818
Iteration 24, loss = 0.02724528
Iteration 25, loss = 0.02616644
Iteration 26, loss = 0.02525337
Iteration 27, loss = 0.02465991
Iteration 28, loss = 0.02423853
Iteration 29, loss = 0.02340160
Iteration 30, loss = 0.02295640
Iteration 31, loss = 0.02240912
Iteration 32, loss = 0.02230797
Iteration 33, loss = 0.02180052
Iteration 34, loss = 0.02134678
Iteration 35, loss = 0.02114477
Iteration 36, loss = 0.02068767
Iteration 37, loss = 0.02055192
Iteration 38, loss = 0.02023156
Iteration 39, loss = 0.02021523
Iteration 40, loss = 0.01987222
Iteration 41, loss = 0.01968943
Iteration 42, loss = 0.01940097
Iteration 43, loss = 0.01937670
Iteration 44, loss = 0.01934405
Iteration 45, loss = 0.01908866
Iteration 46, loss = 0.01902012
Iteration 47, loss = 0.01894018
Iteration 48, loss = 0.01862553
Iteration 49, loss = 0.01857512
Iteration 50, loss = 0.01850714
Iteration 51, loss = 0.01838611
Iteration 52, loss = 0.01838414
Iteration 53, loss = 0.01826244
Iteration 54, loss = 0.01824093
Iteration 55, loss = 0.01838761
Iteration 56, loss = 0.01805568
Iteration 57, loss = 0.01791575
Iteration 58, loss = 0.01819573
Iteration 59, loss = 0.01775816
Iteration 60, loss = 0.01784511
Iteration 61, loss = 0.01782547
Iteration 62, loss = 0.01766641
Iteration 63, loss = 0.01777344
Iteration 64, loss = 0.01739428
Iteration 65, loss = 0.01752288
Iteration 66, loss = 0.01740247
Iteration 67, loss = 0.01754859
Iteration 68, loss = 0.01754743
Iteration 69, loss = 0.01766569
Iteration 70, loss = 0.01725486
Iteration 71, loss = 0.01731770
Iteration 72, loss = 0.01727801
Iteration 73, loss = 0.01736053
Iteration 74, loss = 0.01722414
Iteration 75, loss = 0.01711610
Iteration 76, loss = 0.01701727
Iteration 77, loss = 0.01720706
Iteration 78, loss = 0.01712720
Iteration 79, loss = 0.01716162
Iteration 80, loss = 0.01727433
Iteration 81, loss = 0.01680725
Iteration 82, loss = 0.01713320
Iteration 83, loss = 0.01729013
Iteration 84, loss = 0.01695458
Iteration 85, loss = 0.01701731
Iteration 86, loss = 0.01690402
Iteration 87, loss = 0.01668783
Iteration 88, loss = 0.01686667
Iteration 89, loss = 0.01690415
Iteration 90, loss = 0.01682637
Iteration 91, loss = 0.01677495
Iteration 92, loss = 0.01691548
Iteration 93, loss = 0.01682730
Iteration 94, loss = 0.01687318
Iteration 95, loss = 0.01653820
Iteration 96, loss = 0.01673239
Iteration 97, loss = 0.01683764
Iteration 98, loss = 0.01682618
Iteration 99, loss = 0.01676407
Iteration 100, loss = 0.01671260
Iteration 101, loss = 0.01650782
Iteration 102, loss = 0.01664787
Iteration 103, loss = 0.01650516
Iteration 104, loss = 0.01677718
Iteration 105, loss = 0.01663376
Iteration 106, loss = 0.01652839
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67412444
Iteration 2, loss = 0.60163508
Iteration 3, loss = 0.49217210
Iteration 4, loss = 0.37821174
Iteration 5, loss = 0.28127883
Iteration 6, loss = 0.20829956
Iteration 7, loss = 0.15725677
Iteration 8, loss = 0.12264694
Iteration 9, loss = 0.09894681
Iteration 10, loss = 0.08214007
Iteration 11, loss = 0.07021740
Iteration 12, loss = 0.06095265
Iteration 13, loss = 0.05411476
Iteration 14, loss = 0.04882467
Iteration 15, loss = 0.04442309
Iteration 16, loss = 0.04100422
Iteration 17, loss = 0.03824716
Iteration 18, loss = 0.03590784
Iteration 19, loss = 0.03377774
Iteration 20, loss = 0.03239713
Iteration 21, loss = 0.03085302
Iteration 22, loss = 0.02951850
Iteration 23, loss = 0.02852727
Iteration 24, loss = 0.02747100
Iteration 25, loss = 0.02695738
Iteration 26, loss = 0.02609261
Iteration 27, loss = 0.02539824
Iteration 28, loss = 0.02495799
Iteration 29, loss = 0.02434046
Iteration 30, loss = 0.02401735
Iteration 31, loss = 0.02370591
Iteration 32, loss = 0.02291264
Iteration 33, loss = 0.02293918
Iteration 34, loss = 0.02267294
Iteration 35, loss = 0.02208159
Iteration 36, loss = 0.02201433
Iteration 37, loss = 0.02167214
Iteration 38, loss = 0.02135715
Iteration 39, loss = 0.02140515
Iteration 40, loss = 0.02108880
Iteration 41, loss = 0.02112052
Iteration 42, loss = 0.02083799
Iteration 43, loss = 0.02066658
Iteration 44, loss = 0.02054936
Iteration 45, loss = 0.02021304
Iteration 46, loss = 0.02020347
Iteration 47, loss = 0.02008435
Iteration 48, loss = 0.02003100
Iteration 49, loss = 0.01991607
Iteration 50, loss = 0.01979234
Iteration 51, loss = 0.01968777
Iteration 52, loss = 0.01961920
Iteration 53, loss = 0.01964344
Iteration 54, loss = 0.01958408
Iteration 55, loss = 0.01926496
Iteration 56, loss = 0.01952087
Iteration 57, loss = 0.01918919
Iteration 58, loss = 0.01910700
Iteration 59, loss = 0.01923314
Iteration 60, loss = 0.01921018
Iteration 61, loss = 0.01907953
Iteration 62, loss = 0.01921575
Iteration 63, loss = 0.01891689
Iteration 64, loss = 0.01894149
Iteration 65, loss = 0.01888074
Iteration 66, loss = 0.01886500
Iteration 67, loss = 0.01876612
Iteration 68, loss = 0.01883702
Iteration 69, loss = 0.01869769
Iteration 70, loss = 0.01882073
Iteration 71, loss = 0.01873869
Iteration 72, loss = 0.01889587
Iteration 73, loss = 0.01867208
Iteration 74, loss = 0.01844980
Iteration 75, loss = 0.01869394
Iteration 76, loss = 0.01843452
Iteration 77, loss = 0.01841471
Iteration 78, loss = 0.01852169
Iteration 79, loss = 0.01837471
Iteration 80, loss = 0.01838881
Iteration 81, loss = 0.01817151
Iteration 82, loss = 0.01843313
Iteration 83, loss = 0.01817966
Iteration 84, loss = 0.01820396
Iteration 85, loss = 0.01822998
Iteration 86, loss = 0.01853816
Iteration 87, loss = 0.01815975
Iteration 88, loss = 0.01845597
Iteration 89, loss = 0.01811972
Iteration 90, loss = 0.01819822
Iteration 91, loss = 0.01829504
Iteration 92, loss = 0.01827291
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68294759
Iteration 2, loss = 0.60882801
Iteration 3, loss = 0.49757317
Iteration 4, loss = 0.37976612
Iteration 5, loss = 0.28016835
Iteration 6, loss = 0.20651044
Iteration 7, loss = 0.15513583
Iteration 8, loss = 0.12062791
Iteration 9, loss = 0.09703143
Iteration 10, loss = 0.08018264
Iteration 11, loss = 0.06807485
Iteration 12, loss = 0.05920994
Iteration 13, loss = 0.05236235
Iteration 14, loss = 0.04702976
Iteration 15, loss = 0.04290492
Iteration 16, loss = 0.03932677
Iteration 17, loss = 0.03673448
Iteration 18, loss = 0.03434620
Iteration 19, loss = 0.03234751
Iteration 20, loss = 0.03085375
Iteration 21, loss = 0.02943645
Iteration 22, loss = 0.02813142
Iteration 23, loss = 0.02713438
Iteration 24, loss = 0.02630421
Iteration 25, loss = 0.02546451
Iteration 26, loss = 0.02478844
Iteration 27, loss = 0.02414334
Iteration 28, loss = 0.02361310
Iteration 29, loss = 0.02290231
Iteration 30, loss = 0.02272725
Iteration 31, loss = 0.02217854
Iteration 32, loss = 0.02177112
Iteration 33, loss = 0.02179050
Iteration 34, loss = 0.02122721
Iteration 35, loss = 0.02102174
Iteration 36, loss = 0.02094749
Iteration 37, loss = 0.02052783
Iteration 38, loss = 0.02061011
Iteration 39, loss = 0.01982963
Iteration 40, loss = 0.01992750
Iteration 41, loss = 0.01960741
Iteration 42, loss = 0.01963984
Iteration 43, loss = 0.01947809
Iteration 44, loss = 0.01920313
Iteration 45, loss = 0.01914916
Iteration 46, loss = 0.01896509
Iteration 47, loss = 0.01884134
Iteration 48, loss = 0.01875330
Iteration 49, loss = 0.01887449
Iteration 50, loss = 0.01874217
Iteration 51, loss = 0.01834959
Iteration 52, loss = 0.01855855
Iteration 53, loss = 0.01835906
Iteration 54, loss = 0.01829528
Iteration 55, loss = 0.01827629
Iteration 56, loss = 0.01824199
Iteration 57, loss = 0.01818737
Iteration 58, loss = 0.01832179
Iteration 59, loss = 0.01786738
Iteration 60, loss = 0.01796318
Iteration 61, loss = 0.01786701
Iteration 62, loss = 0.01777047
Iteration 63, loss = 0.01769711
Iteration 64, loss = 0.01767515
Iteration 65, loss = 0.01761303
Iteration 66, loss = 0.01758037
Iteration 67, loss = 0.01767066
Iteration 68, loss = 0.01765102
Iteration 69, loss = 0.01764444
Iteration 70, loss = 0.01771372
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67881151
Iteration 2, loss = 0.60471584
Iteration 3, loss = 0.49092188
Iteration 4, loss = 0.37554028
Iteration 5, loss = 0.27846706
Iteration 6, loss = 0.20632214
Iteration 7, loss = 0.15606845
Iteration 8, loss = 0.12140494
Iteration 9, loss = 0.09751731
Iteration 10, loss = 0.08074352
Iteration 11, loss = 0.06842071
Iteration 12, loss = 0.05917864
Iteration 13, loss = 0.05223344
Iteration 14, loss = 0.04669077
Iteration 15, loss = 0.04231049
Iteration 16, loss = 0.03892984
Iteration 17, loss = 0.03582549
Iteration 18, loss = 0.03352017
Iteration 19, loss = 0.03142966
Iteration 20, loss = 0.02963018
Iteration 21, loss = 0.02825559
Iteration 22, loss = 0.02689145
Iteration 23, loss = 0.02584806
Iteration 24, loss = 0.02492755
Iteration 25, loss = 0.02389247
Iteration 26, loss = 0.02322001
Iteration 27, loss = 0.02248081
Iteration 28, loss = 0.02192914
Iteration 29, loss = 0.02138298
Iteration 30, loss = 0.02091104
Iteration 31, loss = 0.02062797
Iteration 32, loss = 0.02017858
Iteration 33, loss = 0.01983474
Iteration 34, loss = 0.01948492
Iteration 35, loss = 0.01919305
Iteration 36, loss = 0.01899734
Iteration 37, loss = 0.01852153
Iteration 38, loss = 0.01830259
Iteration 39, loss = 0.01817116
Iteration 40, loss = 0.01795707
Iteration 41, loss = 0.01782633
Iteration 42, loss = 0.01781325
Iteration 43, loss = 0.01744891
Iteration 44, loss = 0.01735780
Iteration 45, loss = 0.01746317
Iteration 46, loss = 0.01707142
Iteration 47, loss = 0.01717065
Iteration 48, loss = 0.01681137
Iteration 49, loss = 0.01675178
Iteration 50, loss = 0.01680326
Iteration 51, loss = 0.01652406
Iteration 52, loss = 0.01637541
Iteration 53, loss = 0.01628582
Iteration 54, loss = 0.01643194
Iteration 55, loss = 0.01631005
Iteration 56, loss = 0.01613915
Iteration 57, loss = 0.01592190
Iteration 58, loss = 0.01619224
Iteration 59, loss = 0.01601579
Iteration 60, loss = 0.01608435
Iteration 61, loss = 0.01594136
Iteration 62, loss = 0.01595276
Iteration 63, loss = 0.01589486
Iteration 64, loss = 0.01561644
Iteration 65, loss = 0.01555685
Iteration 66, loss = 0.01555485
Iteration 67, loss = 0.01570948
Iteration 68, loss = 0.01572961
Iteration 69, loss = 0.01559894
Iteration 70, loss = 0.01540700
Iteration 71, loss = 0.01536119
Iteration 72, loss = 0.01555666
Iteration 73, loss = 0.01525030
Iteration 74, loss = 0.01516497
Iteration 75, loss = 0.01522298
Iteration 76, loss = 0.01534127
Iteration 77, loss = 0.01513248
Iteration 78, loss = 0.01516378
Iteration 79, loss = 0.01510524
Iteration 80, loss = 0.01515163
Iteration 81, loss = 0.01543440
Iteration 82, loss = 0.01507641
Iteration 83, loss = 0.01497756
Iteration 84, loss = 0.01496116
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68224994
Iteration 2, loss = 0.60427402
Iteration 3, loss = 0.48810838
Iteration 4, loss = 0.37306006
Iteration 5, loss = 0.27640694
Iteration 6, loss = 0.20455322
Iteration 7, loss = 0.15430598
Iteration 8, loss = 0.12016348
Iteration 9, loss = 0.09655501
Iteration 10, loss = 0.07967325
Iteration 11, loss = 0.06761844
Iteration 12, loss = 0.05863707
Iteration 13, loss = 0.05158622
Iteration 14, loss = 0.04602476
Iteration 15, loss = 0.04176632
Iteration 16, loss = 0.03816763
Iteration 17, loss = 0.03519615
Iteration 18, loss = 0.03292312
Iteration 19, loss = 0.03078569
Iteration 20, loss = 0.02907526
Iteration 21, loss = 0.02760849
Iteration 22, loss = 0.02625800
Iteration 23, loss = 0.02526232
Iteration 24, loss = 0.02429905
Iteration 25, loss = 0.02333954
Iteration 26, loss = 0.02258020
Iteration 27, loss = 0.02192514
Iteration 28, loss = 0.02128234
Iteration 29, loss = 0.02110595
Iteration 30, loss = 0.02042667
Iteration 31, loss = 0.01989648
Iteration 32, loss = 0.01935498
Iteration 33, loss = 0.01917547
Iteration 34, loss = 0.01864750
Iteration 35, loss = 0.01845732
Iteration 36, loss = 0.01813082
Iteration 37, loss = 0.01794353
Iteration 38, loss = 0.01762063
Iteration 39, loss = 0.01760309
Iteration 40, loss = 0.01716377
Iteration 41, loss = 0.01705696
Iteration 42, loss = 0.01689310
Iteration 43, loss = 0.01670259
Iteration 44, loss = 0.01644809
Iteration 45, loss = 0.01634951
Iteration 46, loss = 0.01618967
Iteration 47, loss = 0.01640501
Iteration 48, loss = 0.01612720
Iteration 49, loss = 0.01581805
Iteration 50, loss = 0.01589953
Iteration 51, loss = 0.01569058
Iteration 52, loss = 0.01564824
Iteration 53, loss = 0.01549870
Iteration 54, loss = 0.01535147
Iteration 55, loss = 0.01513372
Iteration 56, loss = 0.01530958
Iteration 57, loss = 0.01537766
Iteration 58, loss = 0.01514903
Iteration 59, loss = 0.01513768
Iteration 60, loss = 0.01497988
Iteration 61, loss = 0.01485562
Iteration 62, loss = 0.01519696
Iteration 63, loss = 0.01498780
Iteration 64, loss = 0.01461615
Iteration 65, loss = 0.01458456
Iteration 66, loss = 0.01461687
Iteration 67, loss = 0.01455461
Iteration 68, loss = 0.01458732
Iteration 69, loss = 0.01430666
Iteration 70, loss = 0.01470113
Iteration 71, loss = 0.01461111
Iteration 72, loss = 0.01437899
Iteration 73, loss = 0.01434830
Iteration 74, loss = 0.01420991
Iteration 75, loss = 0.01424351
Iteration 76, loss = 0.01440742
Iteration 77, loss = 0.01411081
Iteration 78, loss = 0.01413017
Iteration 79, loss = 0.01415860
Iteration 80, loss = 0.01411532
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69401083
Iteration 2, loss = 0.63060414
Iteration 3, loss = 0.52581750
Iteration 4, loss = 0.40417909
Iteration 5, loss = 0.29837068
Iteration 6, loss = 0.21973733
Iteration 7, loss = 0.16480908
Iteration 8, loss = 0.12781812
Iteration 9, loss = 0.10231347
Iteration 10, loss = 0.08426318
Iteration 11, loss = 0.07175619
Iteration 12, loss = 0.06198398
Iteration 13, loss = 0.05482810
Iteration 14, loss = 0.04922683
Iteration 15, loss = 0.04474525
Iteration 16, loss = 0.04120224
Iteration 17, loss = 0.03832806
Iteration 18, loss = 0.03566218
Iteration 19, loss = 0.03371702
Iteration 20, loss = 0.03198801
Iteration 21, loss = 0.03034117
Iteration 22, loss = 0.02901877
Iteration 23, loss = 0.02783818
Iteration 24, loss = 0.02724528
Iteration 25, loss = 0.02616644
Iteration 26, loss = 0.02525337
Iteration 27, loss = 0.02465991
Iteration 28, loss = 0.02423853
Iteration 29, loss = 0.02340160
Iteration 30, loss = 0.02295640
Iteration 31, loss = 0.02240912
Iteration 32, loss = 0.02230797
Iteration 33, loss = 0.02180052
Iteration 34, loss = 0.02134678
Iteration 35, loss = 0.02114477
Iteration 36, loss = 0.02068767
Iteration 37, loss = 0.02055192
Iteration 38, loss = 0.02023156
Iteration 39, loss = 0.02021523
Iteration 40, loss = 0.01987222
Iteration 41, loss = 0.01968943
Iteration 42, loss = 0.01940097
Iteration 43, loss = 0.01937670
Iteration 44, loss = 0.01934405
Iteration 45, loss = 0.01908866
Iteration 46, loss = 0.01902012
Iteration 47, loss = 0.01894018
Iteration 48, loss = 0.01862553
Iteration 49, loss = 0.01857512
Iteration 50, loss = 0.01850714
Iteration 51, loss = 0.01838611
Iteration 52, loss = 0.01838414
Iteration 53, loss = 0.01826244
Iteration 54, loss = 0.01824093
Iteration 55, loss = 0.01838761
Iteration 56, loss = 0.01805568
Iteration 57, loss = 0.01791575
Iteration 58, loss = 0.01819573
Iteration 59, loss = 0.01775816
Iteration 60, loss = 0.01784511
Iteration 61, loss = 0.01782547
Iteration 62, loss = 0.01766641
Iteration 63, loss = 0.01777344
Iteration 64, loss = 0.01739428
Iteration 65, loss = 0.01752288
Iteration 66, loss = 0.01740247
Iteration 67, loss = 0.01754859
Iteration 68, loss = 0.01754743
Iteration 69, loss = 0.01766569
Iteration 70, loss = 0.01725486
Iteration 71, loss = 0.01731770
Iteration 72, loss = 0.01727801
Iteration 73, loss = 0.01736053
Iteration 74, loss = 0.01722414
Iteration 75, loss = 0.01711610
Iteration 76, loss = 0.01701727
Iteration 77, loss = 0.01720706
Iteration 78, loss = 0.01712720
Iteration 79, loss = 0.01716162
Iteration 80, loss = 0.01727433
Iteration 81, loss = 0.01680725
Iteration 82, loss = 0.01713320
Iteration 83, loss = 0.01729013
Iteration 84, loss = 0.01695458
Iteration 85, loss = 0.01701731
Iteration 86, loss = 0.01690402
Iteration 87, loss = 0.01668783
Iteration 88, loss = 0.01686667
Iteration 89, loss = 0.01690415
Iteration 90, loss = 0.01682637
Iteration 91, loss = 0.01677495
Iteration 92, loss = 0.01691548
Iteration 93, loss = 0.01682730
Iteration 94, loss = 0.01687318
Iteration 95, loss = 0.01653820
Iteration 96, loss = 0.01673239
Iteration 97, loss = 0.01683764
Iteration 98, loss = 0.01682618
Iteration 99, loss = 0.01676407
Iteration 100, loss = 0.01671260
Iteration 101, loss = 0.01650782
Iteration 102, loss = 0.01664787
Iteration 103, loss = 0.01650516
Iteration 104, loss = 0.01677718
Iteration 105, loss = 0.01663376
Iteration 106, loss = 0.01652839
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67412444
Iteration 2, loss = 0.60163508
Iteration 3, loss = 0.49217210
Iteration 4, loss = 0.37821174
Iteration 5, loss = 0.28127883
Iteration 6, loss = 0.20829956
Iteration 7, loss = 0.15725677
Iteration 8, loss = 0.12264694
Iteration 9, loss = 0.09894681
Iteration 10, loss = 0.08214007
Iteration 11, loss = 0.07021740
Iteration 12, loss = 0.06095265
Iteration 13, loss = 0.05411476
Iteration 14, loss = 0.04882467
Iteration 15, loss = 0.04442309
Iteration 16, loss = 0.04100422
Iteration 17, loss = 0.03824716
Iteration 18, loss = 0.03590784
Iteration 19, loss = 0.03377774
Iteration 20, loss = 0.03239713
Iteration 21, loss = 0.03085302
Iteration 22, loss = 0.02951850
Iteration 23, loss = 0.02852727
Iteration 24, loss = 0.02747100
Iteration 25, loss = 0.02695738
Iteration 26, loss = 0.02609261
Iteration 27, loss = 0.02539824
Iteration 28, loss = 0.02495799
Iteration 29, loss = 0.02434046
Iteration 30, loss = 0.02401735
Iteration 31, loss = 0.02370591
Iteration 32, loss = 0.02291264
Iteration 33, loss = 0.02293918
Iteration 34, loss = 0.02267294
Iteration 35, loss = 0.02208159
Iteration 36, loss = 0.02201433
Iteration 37, loss = 0.02167214
Iteration 38, loss = 0.02135715
Iteration 39, loss = 0.02140515
Iteration 40, loss = 0.02108880
Iteration 41, loss = 0.02112052
Iteration 42, loss = 0.02083799
Iteration 43, loss = 0.02066658
Iteration 44, loss = 0.02054936
Iteration 45, loss = 0.02021304
Iteration 46, loss = 0.02020347
Iteration 47, loss = 0.02008435
Iteration 48, loss = 0.02003100
Iteration 49, loss = 0.01991607
Iteration 50, loss = 0.01979234
Iteration 51, loss = 0.01968777
Iteration 52, loss = 0.01961920
Iteration 53, loss = 0.01964344
Iteration 54, loss = 0.01958408
Iteration 55, loss = 0.01926496
Iteration 56, loss = 0.01952087
Iteration 57, loss = 0.01918919
Iteration 58, loss = 0.01910700
Iteration 59, loss = 0.01923314
Iteration 60, loss = 0.01921018
Iteration 61, loss = 0.01907953
Iteration 62, loss = 0.01921575
Iteration 63, loss = 0.01891689
Iteration 64, loss = 0.01894149
Iteration 65, loss = 0.01888074
Iteration 66, loss = 0.01886500
Iteration 67, loss = 0.01876612
Iteration 68, loss = 0.01883702
Iteration 69, loss = 0.01869769
Iteration 70, loss = 0.01882073
Iteration 71, loss = 0.01873869
Iteration 72, loss = 0.01889587
Iteration 73, loss = 0.01867208
Iteration 74, loss = 0.01844980
Iteration 75, loss = 0.01869394
Iteration 76, loss = 0.01843452
Iteration 77, loss = 0.01841471
Iteration 78, loss = 0.01852169
Iteration 79, loss = 0.01837471
Iteration 80, loss = 0.01838881
Iteration 81, loss = 0.01817151
Iteration 82, loss = 0.01843313
Iteration 83, loss = 0.01817966
Iteration 84, loss = 0.01820396
Iteration 85, loss = 0.01822998
Iteration 86, loss = 0.01853816
Iteration 87, loss = 0.01815975
Iteration 88, loss = 0.01845597
Iteration 89, loss = 0.01811972
Iteration 90, loss = 0.01819822
Iteration 91, loss = 0.01829504
Iteration 92, loss = 0.01827291
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68294759
Iteration 2, loss = 0.60882801
Iteration 3, loss = 0.49757317
Iteration 4, loss = 0.37976612
Iteration 5, loss = 0.28016835
Iteration 6, loss = 0.20651044
Iteration 7, loss = 0.15513583
Iteration 8, loss = 0.12062791
Iteration 9, loss = 0.09703143
Iteration 10, loss = 0.08018264
Iteration 11, loss = 0.06807485
Iteration 12, loss = 0.05920994
Iteration 13, loss = 0.05236235
Iteration 14, loss = 0.04702976
Iteration 15, loss = 0.04290492
Iteration 16, loss = 0.03932677
Iteration 17, loss = 0.03673448
Iteration 18, loss = 0.03434620
Iteration 19, loss = 0.03234751
Iteration 20, loss = 0.03085375
Iteration 21, loss = 0.02943645
Iteration 22, loss = 0.02813142
Iteration 23, loss = 0.02713438
Iteration 24, loss = 0.02630421
Iteration 25, loss = 0.02546451
Iteration 26, loss = 0.02478844
Iteration 27, loss = 0.02414334
Iteration 28, loss = 0.02361310
Iteration 29, loss = 0.02290231
Iteration 30, loss = 0.02272725
Iteration 31, loss = 0.02217854
Iteration 32, loss = 0.02177112
Iteration 33, loss = 0.02179050
Iteration 34, loss = 0.02122721
Iteration 35, loss = 0.02102174
Iteration 36, loss = 0.02094749
Iteration 37, loss = 0.02052783
Iteration 38, loss = 0.02061011
Iteration 39, loss = 0.01982963
Iteration 40, loss = 0.01992750
Iteration 41, loss = 0.01960741
Iteration 42, loss = 0.01963984
Iteration 43, loss = 0.01947809
Iteration 44, loss = 0.01920313
Iteration 45, loss = 0.01914916
Iteration 46, loss = 0.01896509
Iteration 47, loss = 0.01884134
Iteration 48, loss = 0.01875330
Iteration 49, loss = 0.01887449
Iteration 50, loss = 0.01874217
Iteration 51, loss = 0.01834959
Iteration 52, loss = 0.01855855
Iteration 53, loss = 0.01835906
Iteration 54, loss = 0.01829528
Iteration 55, loss = 0.01827629
Iteration 56, loss = 0.01824199
Iteration 57, loss = 0.01818737
Iteration 58, loss = 0.01832179
Iteration 59, loss = 0.01786738
Iteration 60, loss = 0.01796318
Iteration 61, loss = 0.01786701
Iteration 62, loss = 0.01777047
Iteration 63, loss = 0.01769711
Iteration 64, loss = 0.01767515
Iteration 65, loss = 0.01761303
Iteration 66, loss = 0.01758037
Iteration 67, loss = 0.01767066
Iteration 68, loss = 0.01765102
Iteration 69, loss = 0.01764444
Iteration 70, loss = 0.01771372
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67719242
Iteration 2, loss = 0.57685585
Iteration 3, loss = 0.44440379
Iteration 4, loss = 0.31962824
Iteration 5, loss = 0.22498115
Iteration 6, loss = 0.16193353
Iteration 7, loss = 0.12130239
Iteration 8, loss = 0.09511148
Iteration 9, loss = 0.07736425
Iteration 10, loss = 0.06529816
Iteration 11, loss = 0.05637454
Iteration 12, loss = 0.04967840
Iteration 13, loss = 0.04474885
Iteration 14, loss = 0.04080267
Iteration 15, loss = 0.03762379
Iteration 16, loss = 0.03518715
Iteration 17, loss = 0.03292993
Iteration 18, loss = 0.03134461
Iteration 19, loss = 0.02976206
Iteration 20, loss = 0.02855821
Iteration 21, loss = 0.02744954
Iteration 22, loss = 0.02659875
Iteration 23, loss = 0.02575236
Iteration 24, loss = 0.02516455
Iteration 25, loss = 0.02455061
Iteration 26, loss = 0.02412876
Iteration 27, loss = 0.02370069
Iteration 28, loss = 0.02286919
Iteration 29, loss = 0.02275770
Iteration 30, loss = 0.02243410
Iteration 31, loss = 0.02203817
Iteration 32, loss = 0.02202390
Iteration 33, loss = 0.02152481
Iteration 34, loss = 0.02140736
Iteration 35, loss = 0.02119620
Iteration 36, loss = 0.02115050
Iteration 37, loss = 0.02081466
Iteration 38, loss = 0.02045396
Iteration 39, loss = 0.02073798
Iteration 40, loss = 0.02019001
Iteration 41, loss = 0.02017630
Iteration 42, loss = 0.02001395
Iteration 43, loss = 0.01986285
Iteration 44, loss = 0.01981741
Iteration 45, loss = 0.01969018
Iteration 46, loss = 0.01965949
Iteration 47, loss = 0.01963805
Iteration 48, loss = 0.01949028
Iteration 49, loss = 0.01946256
Iteration 50, loss = 0.01932875
Iteration 51, loss = 0.01939722
Iteration 52, loss = 0.01913223
Iteration 53, loss = 0.01942931
Iteration 54, loss = 0.01904769
Iteration 55, loss = 0.01919421
Iteration 56, loss = 0.01897409
Iteration 57, loss = 0.01918586
Iteration 58, loss = 0.01899004
Iteration 59, loss = 0.01912429
Iteration 60, loss = 0.01857548
Iteration 61, loss = 0.01867171
Iteration 62, loss = 0.01842411
Iteration 63, loss = 0.01865349
Iteration 64, loss = 0.01854582
Iteration 65, loss = 0.01885913
Iteration 66, loss = 0.01881671
Iteration 67, loss = 0.01856226
Iteration 68, loss = 0.01847842
Iteration 69, loss = 0.01843264
Iteration 70, loss = 0.01871042
Iteration 71, loss = 0.01830178
Iteration 72, loss = 0.01820891
Iteration 73, loss = 0.01836554
Iteration 74, loss = 0.01858217
Iteration 75, loss = 0.01812355
Iteration 76, loss = 0.01865572
Iteration 77, loss = 0.01843789
Iteration 78, loss = 0.01837144
Iteration 79, loss = 0.01819301
Iteration 80, loss = 0.01837530
Iteration 81, loss = 0.01864151
Iteration 82, loss = 0.01840074
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on dropped_v9_minimal_processing...
Iteration 1, loss = 0.69054611
Iteration 2, loss = 0.66484455
Iteration 3, loss = 0.64056678
Iteration 4, loss = 0.61398977
Iteration 5, loss = 0.58515222
Iteration 6, loss = 0.55303076
Iteration 7, loss = 0.51886108
Iteration 8, loss = 0.48257041
Iteration 9, loss = 0.44536690
Iteration 10, loss = 0.40863422
Iteration 11, loss = 0.37280357
Iteration 12, loss = 0.33871605
Iteration 13, loss = 0.30685359
Iteration 14, loss = 0.27743106
Iteration 15, loss = 0.25060240
Iteration 16, loss = 0.22609446
Iteration 17, loss = 0.20426395
Iteration 18, loss = 0.18478792
Iteration 19, loss = 0.16738027
Iteration 20, loss = 0.15201814
Iteration 21, loss = 0.13841459
Iteration 22, loss = 0.12635554
Iteration 23, loss = 0.11573098
Iteration 24, loss = 0.10640038
Iteration 25, loss = 0.09807249
Iteration 26, loss = 0.09081557
Iteration 27, loss = 0.08426661
Iteration 28, loss = 0.07843476
Iteration 29, loss = 0.07327695
Iteration 30, loss = 0.06863315
Iteration 31, loss = 0.06447689
Iteration 32, loss = 0.06074869
Iteration 33, loss = 0.05726600
Iteration 34, loss = 0.05423732
Iteration 35, loss = 0.05148068
Iteration 36, loss = 0.04891315
Iteration 37, loss = 0.04661758
Iteration 38, loss = 0.04447743
Iteration 39, loss = 0.04247802
Iteration 40, loss = 0.04071729
Iteration 41, loss = 0.03902914
Iteration 42, loss = 0.03750494
Iteration 43, loss = 0.03613319
Iteration 44, loss = 0.03482797
Iteration 45, loss = 0.03355133
Iteration 46, loss = 0.03243672
Iteration 47, loss = 0.03140947
Iteration 48, loss = 0.03038604
Iteration 49, loss = 0.02945778
Iteration 50, loss = 0.02861902
Iteration 51, loss = 0.02778632
Iteration 52, loss = 0.02703773
Iteration 53, loss = 0.02628199
Iteration 54, loss = 0.02567243
Iteration 55, loss = 0.02505847
Iteration 56, loss = 0.02443904
Iteration 57, loss = 0.02379953
Iteration 58, loss = 0.02327113
Iteration 59, loss = 0.02275812
Iteration 60, loss = 0.02230249
Iteration 61, loss = 0.02183236
Iteration 62, loss = 0.02135249
Iteration 63, loss = 0.02093927
Iteration 64, loss = 0.02063926
Iteration 65, loss = 0.02017052
Iteration 66, loss = 0.01981224
Iteration 67, loss = 0.01941889
Iteration 68, loss = 0.01910977
Iteration 69, loss = 0.01878567
Iteration 70, loss = 0.01846785
Iteration 71, loss = 0.01820968
Iteration 72, loss = 0.01791258
Iteration 73, loss = 0.01766845
Iteration 74, loss = 0.01743193
Iteration 75, loss = 0.01723925
Iteration 76, loss = 0.01694492
Iteration 77, loss = 0.01672012
Iteration 78, loss = 0.01652682
Iteration 79, loss = 0.01637971
Iteration 80, loss = 0.01614696
Iteration 81, loss = 0.01589109
Iteration 82, loss = 0.01575522
Iteration 83, loss = 0.01562062
Iteration 84, loss = 0.01543260
Iteration 85, loss = 0.01522883
Iteration 86, loss = 0.01508419
Iteration 87, loss = 0.01492310
Iteration 88, loss = 0.01474011
Iteration 89, loss = 0.01469986
Iteration 90, loss = 0.01443702
Iteration 91, loss = 0.01439337
Iteration 92, loss = 0.01425250
Iteration 93, loss = 0.01410791
Iteration 94, loss = 0.01397538
Iteration 95, loss = 0.01384839
Iteration 96, loss = 0.01370368
Iteration 97, loss = 0.01357624
Iteration 98, loss = 0.01353900
Iteration 99, loss = 0.01340482
Iteration 100, loss = 0.01332118
Iteration 101, loss = 0.01321642
Iteration 102, loss = 0.01319040
Iteration 103, loss = 0.01304570
Iteration 104, loss = 0.01294087
Iteration 105, loss = 0.01288446
Iteration 106, loss = 0.01279625
Iteration 107, loss = 0.01267781
Iteration 108, loss = 0.01262208
Iteration 109, loss = 0.01251343
Iteration 110, loss = 0.01245453
Iteration 111, loss = 0.01238917
Iteration 112, loss = 0.01236364
Iteration 113, loss = 0.01230557
Iteration 114, loss = 0.01218438
Iteration 115, loss = 0.01212490
Iteration 116, loss = 0.01205497
Iteration 117, loss = 0.01194371
Iteration 118, loss = 0.01190530
Iteration 119, loss = 0.01186644
Iteration 120, loss = 0.01176516
Iteration 121, loss = 0.01170709
Iteration 122, loss = 0.01170283
Iteration 123, loss = 0.01167009
Iteration 124, loss = 0.01154393
Iteration 125, loss = 0.01160535
Iteration 126, loss = 0.01148435
Iteration 127, loss = 0.01152154
Iteration 128, loss = 0.01143898
Iteration 129, loss = 0.01137341
Iteration 130, loss = 0.01128194
Iteration 131, loss = 0.01127775
Iteration 132, loss = 0.01125440
Iteration 133, loss = 0.01123855
Iteration 134, loss = 0.01109914
Iteration 135, loss = 0.01108098
Iteration 136, loss = 0.01103884
Iteration 137, loss = 0.01100297
Iteration 138, loss = 0.01096388
Iteration 139, loss = 0.01089953
Iteration 140, loss = 0.01097179
Iteration 141, loss = 0.01089644
Iteration 142, loss = 0.01086754
Iteration 143, loss = 0.01079001
Iteration 144, loss = 0.01077016
Iteration 145, loss = 0.01072788
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69915789
Iteration 2, loss = 0.66087754
Iteration 3, loss = 0.61032559
Iteration 4, loss = 0.54582793
Iteration 5, loss = 0.47132807
Iteration 6, loss = 0.39701716
Iteration 7, loss = 0.32911457
Iteration 8, loss = 0.27085811
Iteration 9, loss = 0.22258528
Iteration 10, loss = 0.18389674
Iteration 11, loss = 0.15333454
Iteration 12, loss = 0.12912900
Iteration 13, loss = 0.11006880
Iteration 14, loss = 0.09515293
Iteration 15, loss = 0.08314928
Iteration 16, loss = 0.07338823
Iteration 17, loss = 0.06538123
Iteration 18, loss = 0.05903956
Iteration 19, loss = 0.05346767
Iteration 20, loss = 0.04885338
Iteration 21, loss = 0.04499352
Iteration 22, loss = 0.04162505
Iteration 23, loss = 0.03891511
Iteration 24, loss = 0.03635786
Iteration 25, loss = 0.03414038
Iteration 26, loss = 0.03227341
Iteration 27, loss = 0.03063202
Iteration 28, loss = 0.02928850
Iteration 29, loss = 0.02769930
Iteration 30, loss = 0.02659170
Iteration 31, loss = 0.02552294
Iteration 32, loss = 0.02452274
Iteration 33, loss = 0.02361058
Iteration 34, loss = 0.02288587
Iteration 35, loss = 0.02225471
Iteration 36, loss = 0.02162193
Iteration 37, loss = 0.02092652
Iteration 38, loss = 0.02040201
Iteration 39, loss = 0.01986064
Iteration 40, loss = 0.01946407
Iteration 41, loss = 0.01891011
Iteration 42, loss = 0.01862485
Iteration 43, loss = 0.01825174
Iteration 44, loss = 0.01798466
Iteration 45, loss = 0.01751791
Iteration 46, loss = 0.01726851
Iteration 47, loss = 0.01696176
Iteration 48, loss = 0.01668793
Iteration 49, loss = 0.01647206
Iteration 50, loss = 0.01618526
Iteration 51, loss = 0.01604922
Iteration 52, loss = 0.01588559
Iteration 53, loss = 0.01557531
Iteration 54, loss = 0.01542334
Iteration 55, loss = 0.01531039
Iteration 56, loss = 0.01508056
Iteration 57, loss = 0.01504936
Iteration 58, loss = 0.01479451
Iteration 59, loss = 0.01462921
Iteration 60, loss = 0.01452117
Iteration 61, loss = 0.01453176
Iteration 62, loss = 0.01440755
Iteration 63, loss = 0.01419627
Iteration 64, loss = 0.01418468
Iteration 65, loss = 0.01402371
Iteration 66, loss = 0.01397277
Iteration 67, loss = 0.01382565
Iteration 68, loss = 0.01381361
Iteration 69, loss = 0.01369504
Iteration 70, loss = 0.01355702
Iteration 71, loss = 0.01343388
Iteration 72, loss = 0.01344600
Iteration 73, loss = 0.01333822
Iteration 74, loss = 0.01330695
Iteration 75, loss = 0.01325824
Iteration 76, loss = 0.01323311
Iteration 77, loss = 0.01316469
Iteration 78, loss = 0.01308355
Iteration 79, loss = 0.01305870
Iteration 80, loss = 0.01295463
Iteration 81, loss = 0.01291807
Iteration 82, loss = 0.01280722
Iteration 83, loss = 0.01294145
Iteration 84, loss = 0.01293455
Iteration 85, loss = 0.01277938
Iteration 86, loss = 0.01263523
Iteration 87, loss = 0.01267772
Iteration 88, loss = 0.01263741
Iteration 89, loss = 0.01263103
Iteration 90, loss = 0.01272468
Iteration 91, loss = 0.01257583
Iteration 92, loss = 0.01250187
Iteration 93, loss = 0.01257351
Iteration 94, loss = 0.01244227
Iteration 95, loss = 0.01254052
Iteration 96, loss = 0.01240925
Iteration 97, loss = 0.01255416
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70504001
Iteration 2, loss = 0.65509663
Iteration 3, loss = 0.58913446
Iteration 4, loss = 0.50655136
Iteration 5, loss = 0.41898424
Iteration 6, loss = 0.33807958
Iteration 7, loss = 0.27030971
Iteration 8, loss = 0.21698835
Iteration 9, loss = 0.17561028
Iteration 10, loss = 0.14411210
Iteration 11, loss = 0.12036394
Iteration 12, loss = 0.10180342
Iteration 13, loss = 0.08753844
Iteration 14, loss = 0.07638956
Iteration 15, loss = 0.06757536
Iteration 16, loss = 0.06019098
Iteration 17, loss = 0.05426697
Iteration 18, loss = 0.04936406
Iteration 19, loss = 0.04533953
Iteration 20, loss = 0.04183841
Iteration 21, loss = 0.03891550
Iteration 22, loss = 0.03634739
Iteration 23, loss = 0.03400418
Iteration 24, loss = 0.03213894
Iteration 25, loss = 0.03040801
Iteration 26, loss = 0.02902765
Iteration 27, loss = 0.02774782
Iteration 28, loss = 0.02657334
Iteration 29, loss = 0.02545059
Iteration 30, loss = 0.02464153
Iteration 31, loss = 0.02371285
Iteration 32, loss = 0.02291420
Iteration 33, loss = 0.02221775
Iteration 34, loss = 0.02171695
Iteration 35, loss = 0.02097855
Iteration 36, loss = 0.02045019
Iteration 37, loss = 0.02007396
Iteration 38, loss = 0.01965483
Iteration 39, loss = 0.01908799
Iteration 40, loss = 0.01882451
Iteration 41, loss = 0.01839769
Iteration 42, loss = 0.01804357
Iteration 43, loss = 0.01772025
Iteration 44, loss = 0.01750417
Iteration 45, loss = 0.01727575
Iteration 46, loss = 0.01696101
Iteration 47, loss = 0.01687060
Iteration 48, loss = 0.01651724
Iteration 49, loss = 0.01649282
Iteration 50, loss = 0.01657481
Iteration 51, loss = 0.01605926
Iteration 52, loss = 0.01595486
Iteration 53, loss = 0.01588732
Iteration 54, loss = 0.01559365
Iteration 55, loss = 0.01527915
Iteration 56, loss = 0.01539856
Iteration 57, loss = 0.01534805
Iteration 58, loss = 0.01514034
Iteration 59, loss = 0.01496183
Iteration 60, loss = 0.01478966
Iteration 61, loss = 0.01455626
Iteration 62, loss = 0.01468428
Iteration 63, loss = 0.01454687
Iteration 64, loss = 0.01439177
Iteration 65, loss = 0.01430979
Iteration 66, loss = 0.01434689
Iteration 67, loss = 0.01418130
Iteration 68, loss = 0.01406304
Iteration 69, loss = 0.01397057
Iteration 70, loss = 0.01393502
Iteration 71, loss = 0.01389269
Iteration 72, loss = 0.01386265
Iteration 73, loss = 0.01384899
Iteration 74, loss = 0.01363191
Iteration 75, loss = 0.01362686
Iteration 76, loss = 0.01394817
Iteration 77, loss = 0.01366233
Iteration 78, loss = 0.01355433
Iteration 79, loss = 0.01354475
Iteration 80, loss = 0.01329866
Iteration 81, loss = 0.01323046
Iteration 82, loss = 0.01322369
Iteration 83, loss = 0.01317160
Iteration 84, loss = 0.01315706
Iteration 85, loss = 0.01310740
Iteration 86, loss = 0.01320165
Iteration 87, loss = 0.01337623
Iteration 88, loss = 0.01341700
Iteration 89, loss = 0.01302343
Iteration 90, loss = 0.01300262
Iteration 91, loss = 0.01295960
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69332652
Iteration 2, loss = 0.67775118
Iteration 3, loss = 0.66469632
Iteration 4, loss = 0.65180216
Iteration 5, loss = 0.63841522
Iteration 6, loss = 0.62415847
Iteration 7, loss = 0.60861079
Iteration 8, loss = 0.59181301
Iteration 9, loss = 0.57355718
Iteration 10, loss = 0.55401942
Iteration 11, loss = 0.53303935
Iteration 12, loss = 0.51111397
Iteration 13, loss = 0.48814376
Iteration 14, loss = 0.46454859
Iteration 15, loss = 0.44075990
Iteration 16, loss = 0.41685860
Iteration 17, loss = 0.39332916
Iteration 18, loss = 0.37023009
Iteration 19, loss = 0.34787779
Iteration 20, loss = 0.32639641
Iteration 21, loss = 0.30580036
Iteration 22, loss = 0.28630385
Iteration 23, loss = 0.26790651
Iteration 24, loss = 0.25056348
Iteration 25, loss = 0.23435205
Iteration 26, loss = 0.21912672
Iteration 27, loss = 0.20489096
Iteration 28, loss = 0.19176049
Iteration 29, loss = 0.17949966
Iteration 30, loss = 0.16826851
Iteration 31, loss = 0.15774729
Iteration 32, loss = 0.14807564
Iteration 33, loss = 0.13918639
Iteration 34, loss = 0.13090818
Iteration 35, loss = 0.12328469
Iteration 36, loss = 0.11621549
Iteration 37, loss = 0.10977351
Iteration 38, loss = 0.10372263
Iteration 39, loss = 0.09815442
Iteration 40, loss = 0.09310295
Iteration 41, loss = 0.08834722
Iteration 42, loss = 0.08391481
Iteration 43, loss = 0.07984206
Iteration 44, loss = 0.07605852
Iteration 45, loss = 0.07254020
Iteration 46, loss = 0.06926769
Iteration 47, loss = 0.06618823
Iteration 48, loss = 0.06333837
Iteration 49, loss = 0.06065752
Iteration 50, loss = 0.05820584
Iteration 51, loss = 0.05589724
Iteration 52, loss = 0.05370371
Iteration 53, loss = 0.05160012
Iteration 54, loss = 0.04964850
Iteration 55, loss = 0.04784653
Iteration 56, loss = 0.04613101
Iteration 57, loss = 0.04449515
Iteration 58, loss = 0.04297981
Iteration 59, loss = 0.04156398
Iteration 60, loss = 0.04021368
Iteration 61, loss = 0.03893956
Iteration 62, loss = 0.03770802
Iteration 63, loss = 0.03659486
Iteration 64, loss = 0.03546030
Iteration 65, loss = 0.03443724
Iteration 66, loss = 0.03343587
Iteration 67, loss = 0.03249669
Iteration 68, loss = 0.03161417
Iteration 69, loss = 0.03070848
Iteration 70, loss = 0.02990415
Iteration 71, loss = 0.02910271
Iteration 72, loss = 0.02836837
Iteration 73, loss = 0.02765887
Iteration 74, loss = 0.02697541
Iteration 75, loss = 0.02634003
Iteration 76, loss = 0.02575111
Iteration 77, loss = 0.02510703
Iteration 78, loss = 0.02453138
Iteration 79, loss = 0.02398214
Iteration 80, loss = 0.02342546
Iteration 81, loss = 0.02288479
Iteration 82, loss = 0.02240772
Iteration 83, loss = 0.02192598
Iteration 84, loss = 0.02147955
Iteration 85, loss = 0.02107191
Iteration 86, loss = 0.02066975
Iteration 87, loss = 0.02028606
Iteration 88, loss = 0.01990857
Iteration 89, loss = 0.01953762
Iteration 90, loss = 0.01917033
Iteration 91, loss = 0.01883730
Iteration 92, loss = 0.01842731
Iteration 93, loss = 0.01808012
Iteration 94, loss = 0.01777455
Iteration 95, loss = 0.01759135
Iteration 96, loss = 0.01724091
Iteration 97, loss = 0.01693998
Iteration 98, loss = 0.01666462
Iteration 99, loss = 0.01642342
Iteration 100, loss = 0.01613292
Iteration 101, loss = 0.01591423
Iteration 102, loss = 0.01566682
Iteration 103, loss = 0.01540455
Iteration 104, loss = 0.01515978
Iteration 105, loss = 0.01496011
Iteration 106, loss = 0.01479158
Iteration 107, loss = 0.01458984
Iteration 108, loss = 0.01433855
Iteration 109, loss = 0.01417880
Iteration 110, loss = 0.01394797
Iteration 111, loss = 0.01376289
Iteration 112, loss = 0.01362471
Iteration 113, loss = 0.01340907
Iteration 114, loss = 0.01324182
Iteration 115, loss = 0.01309029
Iteration 116, loss = 0.01291757
Iteration 117, loss = 0.01279595
Iteration 118, loss = 0.01265413
Iteration 119, loss = 0.01253640
Iteration 120, loss = 0.01242228
Iteration 121, loss = 0.01227187
Iteration 122, loss = 0.01212677
Iteration 123, loss = 0.01202122
Iteration 124, loss = 0.01191339
Iteration 125, loss = 0.01178690
Iteration 126, loss = 0.01162567
Iteration 127, loss = 0.01148763
Iteration 128, loss = 0.01135192
Iteration 129, loss = 0.01120908
Iteration 130, loss = 0.01107493
Iteration 131, loss = 0.01094891
Iteration 132, loss = 0.01081209
Iteration 133, loss = 0.01069634
Iteration 134, loss = 0.01062445
Iteration 135, loss = 0.01049221
Iteration 136, loss = 0.01042006
Iteration 137, loss = 0.01030231
Iteration 138, loss = 0.01021837
Iteration 139, loss = 0.01014716
Iteration 140, loss = 0.01001578
Iteration 141, loss = 0.00997702
Iteration 142, loss = 0.00987395
Iteration 143, loss = 0.00982537
Iteration 144, loss = 0.00978425
Iteration 145, loss = 0.00970249
Iteration 146, loss = 0.00954807
Iteration 147, loss = 0.00948209
Iteration 148, loss = 0.00940098
Iteration 149, loss = 0.00933328
Iteration 150, loss = 0.00925170
Iteration 151, loss = 0.00915552
Iteration 152, loss = 0.00909362
Iteration 153, loss = 0.00902283
Iteration 154, loss = 0.00897163
Iteration 155, loss = 0.00890759
Iteration 156, loss = 0.00882976
Iteration 157, loss = 0.00877583
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70196156
Iteration 2, loss = 0.68406958
Iteration 3, loss = 0.66986280
Iteration 4, loss = 0.65615049
Iteration 5, loss = 0.64176746
Iteration 6, loss = 0.62622877
Iteration 7, loss = 0.60934200
Iteration 8, loss = 0.59116613
Iteration 9, loss = 0.57150261
Iteration 10, loss = 0.55061589
Iteration 11, loss = 0.52874986
Iteration 12, loss = 0.50602884
Iteration 13, loss = 0.48294472
Iteration 14, loss = 0.45965368
Iteration 15, loss = 0.43647208
Iteration 16, loss = 0.41360052
Iteration 17, loss = 0.39123477
Iteration 18, loss = 0.36954437
Iteration 19, loss = 0.34847970
Iteration 20, loss = 0.32825404
Iteration 21, loss = 0.30901695
Iteration 22, loss = 0.29075397
Iteration 23, loss = 0.27336050
Iteration 24, loss = 0.25684237
Iteration 25, loss = 0.24139018
Iteration 26, loss = 0.22680699
Iteration 27, loss = 0.21318825
Iteration 28, loss = 0.20035213
Iteration 29, loss = 0.18847615
Iteration 30, loss = 0.17739867
Iteration 31, loss = 0.16698306
Iteration 32, loss = 0.15738422
Iteration 33, loss = 0.14849490
Iteration 34, loss = 0.14016093
Iteration 35, loss = 0.13247485
Iteration 36, loss = 0.12535338
Iteration 37, loss = 0.11874086
Iteration 38, loss = 0.11262136
Iteration 39, loss = 0.10690957
Iteration 40, loss = 0.10160486
Iteration 41, loss = 0.09677328
Iteration 42, loss = 0.09213407
Iteration 43, loss = 0.08784519
Iteration 44, loss = 0.08389674
Iteration 45, loss = 0.08015356
Iteration 46, loss = 0.07673999
Iteration 47, loss = 0.07351354
Iteration 48, loss = 0.07047796
Iteration 49, loss = 0.06767721
Iteration 50, loss = 0.06503302
Iteration 51, loss = 0.06256198
Iteration 52, loss = 0.06020056
Iteration 53, loss = 0.05803448
Iteration 54, loss = 0.05591144
Iteration 55, loss = 0.05403198
Iteration 56, loss = 0.05216818
Iteration 57, loss = 0.05048429
Iteration 58, loss = 0.04887564
Iteration 59, loss = 0.04734609
Iteration 60, loss = 0.04589192
Iteration 61, loss = 0.04449461
Iteration 62, loss = 0.04321318
Iteration 63, loss = 0.04196667
Iteration 64, loss = 0.04078043
Iteration 65, loss = 0.03965068
Iteration 66, loss = 0.03859097
Iteration 67, loss = 0.03757006
Iteration 68, loss = 0.03657529
Iteration 69, loss = 0.03567922
Iteration 70, loss = 0.03477994
Iteration 71, loss = 0.03396250
Iteration 72, loss = 0.03313821
Iteration 73, loss = 0.03235387
Iteration 74, loss = 0.03163917
Iteration 75, loss = 0.03091635
Iteration 76, loss = 0.03022904
Iteration 77, loss = 0.02963561
Iteration 78, loss = 0.02904983
Iteration 79, loss = 0.02849859
Iteration 80, loss = 0.02794529
Iteration 81, loss = 0.02736312
Iteration 82, loss = 0.02685048
Iteration 83, loss = 0.02634019
Iteration 84, loss = 0.02583412
Iteration 85, loss = 0.02534035
Iteration 86, loss = 0.02485058
Iteration 87, loss = 0.02441002
Iteration 88, loss = 0.02398783
Iteration 89, loss = 0.02361217
Iteration 90, loss = 0.02317410
Iteration 91, loss = 0.02284024
Iteration 92, loss = 0.02251054
Iteration 93, loss = 0.02212552
Iteration 94, loss = 0.02179071
Iteration 95, loss = 0.02150158
Iteration 96, loss = 0.02113266
Iteration 97, loss = 0.02083826
Iteration 98, loss = 0.02055304
Iteration 99, loss = 0.02027188
Iteration 100, loss = 0.01996124
Iteration 101, loss = 0.01969439
Iteration 102, loss = 0.01944345
Iteration 103, loss = 0.01916959
Iteration 104, loss = 0.01893448
Iteration 105, loss = 0.01869243
Iteration 106, loss = 0.01842739
Iteration 107, loss = 0.01825330
Iteration 108, loss = 0.01802341
Iteration 109, loss = 0.01778071
Iteration 110, loss = 0.01758503
Iteration 111, loss = 0.01740560
Iteration 112, loss = 0.01719481
Iteration 113, loss = 0.01705001
Iteration 114, loss = 0.01684657
Iteration 115, loss = 0.01665319
Iteration 116, loss = 0.01650568
Iteration 117, loss = 0.01632295
Iteration 118, loss = 0.01612206
Iteration 119, loss = 0.01598009
Iteration 120, loss = 0.01585311
Iteration 121, loss = 0.01568334
Iteration 122, loss = 0.01552102
Iteration 123, loss = 0.01543003
Iteration 124, loss = 0.01531562
Iteration 125, loss = 0.01512945
Iteration 126, loss = 0.01500134
Iteration 127, loss = 0.01487764
Iteration 128, loss = 0.01473404
Iteration 129, loss = 0.01460211
Iteration 130, loss = 0.01450454
Iteration 131, loss = 0.01437596
Iteration 132, loss = 0.01423888
Iteration 133, loss = 0.01414832
Iteration 134, loss = 0.01402215
Iteration 135, loss = 0.01389682
Iteration 136, loss = 0.01375940
Iteration 137, loss = 0.01370628
Iteration 138, loss = 0.01355832
Iteration 139, loss = 0.01352404
Iteration 140, loss = 0.01337625
Iteration 141, loss = 0.01328848
Iteration 142, loss = 0.01316581
Iteration 143, loss = 0.01308325
Iteration 144, loss = 0.01299077
Iteration 145, loss = 0.01293263
Iteration 146, loss = 0.01284850
Iteration 147, loss = 0.01276818
Iteration 148, loss = 0.01268139
Iteration 149, loss = 0.01261384
Iteration 150, loss = 0.01253889
Iteration 151, loss = 0.01245232
Iteration 152, loss = 0.01244935
Iteration 153, loss = 0.01234377
Iteration 154, loss = 0.01228550
Iteration 155, loss = 0.01217216
Iteration 156, loss = 0.01209164
Iteration 157, loss = 0.01204666
Iteration 158, loss = 0.01196301
Iteration 159, loss = 0.01186174
Iteration 160, loss = 0.01183190
Iteration 161, loss = 0.01174048
Iteration 162, loss = 0.01168316
Iteration 163, loss = 0.01162836
Iteration 164, loss = 0.01159914
Iteration 165, loss = 0.01155472
Iteration 166, loss = 0.01148549
Iteration 167, loss = 0.01147579
Iteration 168, loss = 0.01138541
Iteration 169, loss = 0.01133025
Iteration 170, loss = 0.01125862
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67928542
Iteration 2, loss = 0.65308320
Iteration 3, loss = 0.62231232
Iteration 4, loss = 0.58266299
Iteration 5, loss = 0.53588384
Iteration 6, loss = 0.48504661
Iteration 7, loss = 0.43421326
Iteration 8, loss = 0.38464510
Iteration 9, loss = 0.33859430
Iteration 10, loss = 0.29668860
Iteration 11, loss = 0.25911242
Iteration 12, loss = 0.22600207
Iteration 13, loss = 0.19759947
Iteration 14, loss = 0.17317091
Iteration 15, loss = 0.15265111
Iteration 16, loss = 0.13513872
Iteration 17, loss = 0.12042993
Iteration 18, loss = 0.10772668
Iteration 19, loss = 0.09710928
Iteration 20, loss = 0.08785681
Iteration 21, loss = 0.08011957
Iteration 22, loss = 0.07334803
Iteration 23, loss = 0.06747241
Iteration 24, loss = 0.06235412
Iteration 25, loss = 0.05790681
Iteration 26, loss = 0.05396827
Iteration 27, loss = 0.05060201
Iteration 28, loss = 0.04745437
Iteration 29, loss = 0.04470976
Iteration 30, loss = 0.04229311
Iteration 31, loss = 0.04012507
Iteration 32, loss = 0.03818518
Iteration 33, loss = 0.03642605
Iteration 34, loss = 0.03481874
Iteration 35, loss = 0.03336476
Iteration 36, loss = 0.03204437
Iteration 37, loss = 0.03077497
Iteration 38, loss = 0.02963211
Iteration 39, loss = 0.02861281
Iteration 40, loss = 0.02754610
Iteration 41, loss = 0.02669712
Iteration 42, loss = 0.02589105
Iteration 43, loss = 0.02505509
Iteration 44, loss = 0.02442087
Iteration 45, loss = 0.02374716
Iteration 46, loss = 0.02318803
Iteration 47, loss = 0.02266146
Iteration 48, loss = 0.02204342
Iteration 49, loss = 0.02168097
Iteration 50, loss = 0.02141605
Iteration 51, loss = 0.02099796
Iteration 52, loss = 0.02057682
Iteration 53, loss = 0.02010465
Iteration 54, loss = 0.01966286
Iteration 55, loss = 0.01932463
Iteration 56, loss = 0.01900667
Iteration 57, loss = 0.01868885
Iteration 58, loss = 0.01859005
Iteration 59, loss = 0.01838802
Iteration 60, loss = 0.01833135
Iteration 61, loss = 0.01809367
Iteration 62, loss = 0.01769615
Iteration 63, loss = 0.01738723
Iteration 64, loss = 0.01721198
Iteration 65, loss = 0.01712982
Iteration 66, loss = 0.01686642
Iteration 67, loss = 0.01661623
Iteration 68, loss = 0.01625861
Iteration 69, loss = 0.01603700
Iteration 70, loss = 0.01575521
Iteration 71, loss = 0.01563650
Iteration 72, loss = 0.01556822
Iteration 73, loss = 0.01543968
Iteration 74, loss = 0.01520869
Iteration 75, loss = 0.01507823
Iteration 76, loss = 0.01492282
Iteration 77, loss = 0.01472108
Iteration 78, loss = 0.01464740
Iteration 79, loss = 0.01455038
Iteration 80, loss = 0.01444966
Iteration 81, loss = 0.01438014
Iteration 82, loss = 0.01432352
Iteration 83, loss = 0.01425639
Iteration 84, loss = 0.01424153
Iteration 85, loss = 0.01425406
Iteration 86, loss = 0.01409318
Iteration 87, loss = 0.01397334
Iteration 88, loss = 0.01405141
Iteration 89, loss = 0.01411237
Iteration 90, loss = 0.01405904
Iteration 91, loss = 0.01383448
Iteration 92, loss = 0.01368389
Iteration 93, loss = 0.01353755
Iteration 94, loss = 0.01339002
Iteration 95, loss = 0.01338474
Iteration 96, loss = 0.01316227
Iteration 97, loss = 0.01325093
Iteration 98, loss = 0.01319711
Iteration 99, loss = 0.01317096
Iteration 100, loss = 0.01305211
Iteration 101, loss = 0.01353736
Iteration 102, loss = 0.01354223
Iteration 103, loss = 0.01343073
Iteration 104, loss = 0.01324937
Iteration 105, loss = 0.01306200
Iteration 106, loss = 0.01293138
Iteration 107, loss = 0.01279889
Iteration 108, loss = 0.01274256
Iteration 109, loss = 0.01260124
Iteration 110, loss = 0.01258365
Iteration 111, loss = 0.01248057
Iteration 112, loss = 0.01258878
Iteration 113, loss = 0.01269331
Iteration 114, loss = 0.01263942
Iteration 115, loss = 0.01271583
Iteration 116, loss = 0.01266240
Iteration 117, loss = 0.01254092
Iteration 118, loss = 0.01236001
Iteration 119, loss = 0.01238633
Iteration 120, loss = 0.01252792
Iteration 121, loss = 0.01230180
Iteration 122, loss = 0.01236633
Iteration 123, loss = 0.01218873
Iteration 124, loss = 0.01239410
Iteration 125, loss = 0.01209129
Iteration 126, loss = 0.01214111
Iteration 127, loss = 0.01204666
Iteration 128, loss = 0.01217351
Iteration 129, loss = 0.01229278
Iteration 130, loss = 0.01218229
Iteration 131, loss = 0.01213329
Iteration 132, loss = 0.01207939
Iteration 133, loss = 0.01201269
Iteration 134, loss = 0.01191086
Iteration 135, loss = 0.01187190
Iteration 136, loss = 0.01187933
Iteration 137, loss = 0.01179041
Iteration 138, loss = 0.01176495
Iteration 139, loss = 0.01158771
Iteration 140, loss = 0.01194492
Iteration 141, loss = 0.01187961
Iteration 142, loss = 0.01190279
Iteration 143, loss = 0.01194457
Iteration 144, loss = 0.01179819
Iteration 145, loss = 0.01177265
Iteration 146, loss = 0.01168589
Iteration 147, loss = 0.01164969
Iteration 148, loss = 0.01162567
Iteration 149, loss = 0.01177762
Iteration 150, loss = 0.01175096
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68655834
Iteration 2, loss = 0.66437201
Iteration 3, loss = 0.63763119
Iteration 4, loss = 0.60222353
Iteration 5, loss = 0.55865933
Iteration 6, loss = 0.51004264
Iteration 7, loss = 0.45945351
Iteration 8, loss = 0.40907589
Iteration 9, loss = 0.36124134
Iteration 10, loss = 0.31720043
Iteration 11, loss = 0.27748986
Iteration 12, loss = 0.24244740
Iteration 13, loss = 0.21180812
Iteration 14, loss = 0.18557790
Iteration 15, loss = 0.16321789
Iteration 16, loss = 0.14422275
Iteration 17, loss = 0.12822375
Iteration 18, loss = 0.11455323
Iteration 19, loss = 0.10309501
Iteration 20, loss = 0.09313132
Iteration 21, loss = 0.08472211
Iteration 22, loss = 0.07736253
Iteration 23, loss = 0.07116528
Iteration 24, loss = 0.06574554
Iteration 25, loss = 0.06089756
Iteration 26, loss = 0.05687257
Iteration 27, loss = 0.05304252
Iteration 28, loss = 0.04983096
Iteration 29, loss = 0.04681805
Iteration 30, loss = 0.04423967
Iteration 31, loss = 0.04198390
Iteration 32, loss = 0.03987833
Iteration 33, loss = 0.03799426
Iteration 34, loss = 0.03634125
Iteration 35, loss = 0.03479171
Iteration 36, loss = 0.03343178
Iteration 37, loss = 0.03204384
Iteration 38, loss = 0.03088177
Iteration 39, loss = 0.02987879
Iteration 40, loss = 0.02881450
Iteration 41, loss = 0.02784576
Iteration 42, loss = 0.02700935
Iteration 43, loss = 0.02614052
Iteration 44, loss = 0.02536103
Iteration 45, loss = 0.02468938
Iteration 46, loss = 0.02420697
Iteration 47, loss = 0.02352637
Iteration 48, loss = 0.02298378
Iteration 49, loss = 0.02245464
Iteration 50, loss = 0.02195140
Iteration 51, loss = 0.02141592
Iteration 52, loss = 0.02096367
Iteration 53, loss = 0.02057073
Iteration 54, loss = 0.02028475
Iteration 55, loss = 0.01975897
Iteration 56, loss = 0.01973338
Iteration 57, loss = 0.01932848
Iteration 58, loss = 0.01898626
Iteration 59, loss = 0.01867700
Iteration 60, loss = 0.01833473
Iteration 61, loss = 0.01808208
Iteration 62, loss = 0.01775129
Iteration 63, loss = 0.01751373
Iteration 64, loss = 0.01725721
Iteration 65, loss = 0.01722739
Iteration 66, loss = 0.01693884
Iteration 67, loss = 0.01686464
Iteration 68, loss = 0.01666356
Iteration 69, loss = 0.01637203
Iteration 70, loss = 0.01640644
Iteration 71, loss = 0.01629590
Iteration 72, loss = 0.01622908
Iteration 73, loss = 0.01579148
Iteration 74, loss = 0.01570241
Iteration 75, loss = 0.01553773
Iteration 76, loss = 0.01528102
Iteration 77, loss = 0.01520974
Iteration 78, loss = 0.01519355
Iteration 79, loss = 0.01518487
Iteration 80, loss = 0.01505042
Iteration 81, loss = 0.01482396
Iteration 82, loss = 0.01507972
Iteration 83, loss = 0.01494125
Iteration 84, loss = 0.01483775
Iteration 85, loss = 0.01468387
Iteration 86, loss = 0.01450957
Iteration 87, loss = 0.01439945
Iteration 88, loss = 0.01416199
Iteration 89, loss = 0.01404171
Iteration 90, loss = 0.01399250
Iteration 91, loss = 0.01385006
Iteration 92, loss = 0.01385128
Iteration 93, loss = 0.01374214
Iteration 94, loss = 0.01361415
Iteration 95, loss = 0.01369516
Iteration 96, loss = 0.01355696
Iteration 97, loss = 0.01357619
Iteration 98, loss = 0.01358732
Iteration 99, loss = 0.01341281
Iteration 100, loss = 0.01371439
Iteration 101, loss = 0.01379727
Iteration 102, loss = 0.01373625
Iteration 103, loss = 0.01366101
Iteration 104, loss = 0.01341519
Iteration 105, loss = 0.01330702
Iteration 106, loss = 0.01316728
Iteration 107, loss = 0.01307123
Iteration 108, loss = 0.01297558
Iteration 109, loss = 0.01288117
Iteration 110, loss = 0.01303839
Iteration 111, loss = 0.01285778
Iteration 112, loss = 0.01292302
Iteration 113, loss = 0.01297231
Iteration 114, loss = 0.01272030
Iteration 115, loss = 0.01266046
Iteration 116, loss = 0.01260680
Iteration 117, loss = 0.01256018
Iteration 118, loss = 0.01246227
Iteration 119, loss = 0.01234841
Iteration 120, loss = 0.01255659
Iteration 121, loss = 0.01246795
Iteration 122, loss = 0.01280462
Iteration 123, loss = 0.01272297
Iteration 124, loss = 0.01256513
Iteration 125, loss = 0.01239261
Iteration 126, loss = 0.01231096
Iteration 127, loss = 0.01248674
Iteration 128, loss = 0.01239823
Iteration 129, loss = 0.01235530
Iteration 130, loss = 0.01241085
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70115710
Iteration 2, loss = 0.66083963
Iteration 3, loss = 0.60323986
Iteration 4, loss = 0.52385372
Iteration 5, loss = 0.43676572
Iteration 6, loss = 0.35495103
Iteration 7, loss = 0.28611448
Iteration 8, loss = 0.23022423
Iteration 9, loss = 0.18658047
Iteration 10, loss = 0.15296769
Iteration 11, loss = 0.12699917
Iteration 12, loss = 0.10723614
Iteration 13, loss = 0.09182542
Iteration 14, loss = 0.07961082
Iteration 15, loss = 0.06978617
Iteration 16, loss = 0.06194867
Iteration 17, loss = 0.05552847
Iteration 18, loss = 0.05023774
Iteration 19, loss = 0.04583182
Iteration 20, loss = 0.04206322
Iteration 21, loss = 0.03895317
Iteration 22, loss = 0.03627571
Iteration 23, loss = 0.03387257
Iteration 24, loss = 0.03181242
Iteration 25, loss = 0.03014842
Iteration 26, loss = 0.02842443
Iteration 27, loss = 0.02725982
Iteration 28, loss = 0.02595148
Iteration 29, loss = 0.02488612
Iteration 30, loss = 0.02385170
Iteration 31, loss = 0.02298413
Iteration 32, loss = 0.02210324
Iteration 33, loss = 0.02147350
Iteration 34, loss = 0.02082333
Iteration 35, loss = 0.02032539
Iteration 36, loss = 0.01989555
Iteration 37, loss = 0.01907802
Iteration 38, loss = 0.01857154
Iteration 39, loss = 0.01823482
Iteration 40, loss = 0.01787059
Iteration 41, loss = 0.01741556
Iteration 42, loss = 0.01705728
Iteration 43, loss = 0.01699023
Iteration 44, loss = 0.01670536
Iteration 45, loss = 0.01625193
Iteration 46, loss = 0.01604585
Iteration 47, loss = 0.01584514
Iteration 48, loss = 0.01571526
Iteration 49, loss = 0.01548976
Iteration 50, loss = 0.01527778
Iteration 51, loss = 0.01502434
Iteration 52, loss = 0.01470719
Iteration 53, loss = 0.01466521
Iteration 54, loss = 0.01449398
Iteration 55, loss = 0.01425102
Iteration 56, loss = 0.01418137
Iteration 57, loss = 0.01405159
Iteration 58, loss = 0.01392116
Iteration 59, loss = 0.01384523
Iteration 60, loss = 0.01388419
Iteration 61, loss = 0.01370743
Iteration 62, loss = 0.01348801
Iteration 63, loss = 0.01351624
Iteration 64, loss = 0.01326143
Iteration 65, loss = 0.01313355
Iteration 66, loss = 0.01321255
Iteration 67, loss = 0.01331629
Iteration 68, loss = 0.01309828
Iteration 69, loss = 0.01293769
Iteration 70, loss = 0.01304714
Iteration 71, loss = 0.01278022
Iteration 72, loss = 0.01289073
Iteration 73, loss = 0.01280727
Iteration 74, loss = 0.01280317
Iteration 75, loss = 0.01254081
Iteration 76, loss = 0.01265863
Iteration 77, loss = 0.01269515
Iteration 78, loss = 0.01230686
Iteration 79, loss = 0.01243619
Iteration 80, loss = 0.01242613
Iteration 81, loss = 0.01241793
Iteration 82, loss = 0.01253540
Iteration 83, loss = 0.01249255
Iteration 84, loss = 0.01235588
Iteration 85, loss = 0.01229342
Iteration 86, loss = 0.01215539
Iteration 87, loss = 0.01216760
Iteration 88, loss = 0.01198289
Iteration 89, loss = 0.01207431
Iteration 90, loss = 0.01207352
Iteration 91, loss = 0.01192802
Iteration 92, loss = 0.01207951
Iteration 93, loss = 0.01194798
Iteration 94, loss = 0.01178620
Iteration 95, loss = 0.01184782
Iteration 96, loss = 0.01180709
Iteration 97, loss = 0.01177470
Iteration 98, loss = 0.01167928
Iteration 99, loss = 0.01175725
Iteration 100, loss = 0.01168152
Iteration 101, loss = 0.01168543
Iteration 102, loss = 0.01163428
Iteration 103, loss = 0.01173607
Iteration 104, loss = 0.01157508
Iteration 105, loss = 0.01172058
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69574266
Iteration 2, loss = 0.64737751
Iteration 3, loss = 0.57236013
Iteration 4, loss = 0.47826030
Iteration 5, loss = 0.38626103
Iteration 6, loss = 0.30451643
Iteration 7, loss = 0.23848670
Iteration 8, loss = 0.18733367
Iteration 9, loss = 0.14943795
Iteration 10, loss = 0.12129422
Iteration 11, loss = 0.10038268
Iteration 12, loss = 0.08474716
Iteration 13, loss = 0.07281053
Iteration 14, loss = 0.06343671
Iteration 15, loss = 0.05614059
Iteration 16, loss = 0.05010442
Iteration 17, loss = 0.04542663
Iteration 18, loss = 0.04151525
Iteration 19, loss = 0.03811176
Iteration 20, loss = 0.03530372
Iteration 21, loss = 0.03297456
Iteration 22, loss = 0.03083981
Iteration 23, loss = 0.02899992
Iteration 24, loss = 0.02747305
Iteration 25, loss = 0.02613179
Iteration 26, loss = 0.02488333
Iteration 27, loss = 0.02371910
Iteration 28, loss = 0.02275080
Iteration 29, loss = 0.02200659
Iteration 30, loss = 0.02118993
Iteration 31, loss = 0.02060551
Iteration 32, loss = 0.01975863
Iteration 33, loss = 0.01932006
Iteration 34, loss = 0.01885820
Iteration 35, loss = 0.01822793
Iteration 36, loss = 0.01785419
Iteration 37, loss = 0.01745668
Iteration 38, loss = 0.01696293
Iteration 39, loss = 0.01681100
Iteration 40, loss = 0.01642847
Iteration 41, loss = 0.01630660
Iteration 42, loss = 0.01603850
Iteration 43, loss = 0.01558320
Iteration 44, loss = 0.01540718
Iteration 45, loss = 0.01518079
Iteration 46, loss = 0.01500781
Iteration 47, loss = 0.01478110
Iteration 48, loss = 0.01463121
Iteration 49, loss = 0.01452547
Iteration 50, loss = 0.01423391
Iteration 51, loss = 0.01419802
Iteration 52, loss = 0.01402504
Iteration 53, loss = 0.01390782
Iteration 54, loss = 0.01366642
Iteration 55, loss = 0.01373483
Iteration 56, loss = 0.01355168
Iteration 57, loss = 0.01349002
Iteration 58, loss = 0.01344639
Iteration 59, loss = 0.01311561
Iteration 60, loss = 0.01316439
Iteration 61, loss = 0.01312096
Iteration 62, loss = 0.01291463
Iteration 63, loss = 0.01291265
Iteration 64, loss = 0.01271764
Iteration 65, loss = 0.01268412
Iteration 66, loss = 0.01282805
Iteration 67, loss = 0.01272449
Iteration 68, loss = 0.01249753
Iteration 69, loss = 0.01255697
Iteration 70, loss = 0.01247767
Iteration 71, loss = 0.01231020
Iteration 72, loss = 0.01234341
Iteration 73, loss = 0.01232119
Iteration 74, loss = 0.01233576
Iteration 75, loss = 0.01221516
Iteration 76, loss = 0.01214913
Iteration 77, loss = 0.01208697
Iteration 78, loss = 0.01202354
Iteration 79, loss = 0.01207551
Iteration 80, loss = 0.01198130
Iteration 81, loss = 0.01198678
Iteration 82, loss = 0.01190908
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68139753
Iteration 2, loss = 0.62932450
Iteration 3, loss = 0.54793634
Iteration 4, loss = 0.44786901
Iteration 5, loss = 0.35737526
Iteration 6, loss = 0.28071617
Iteration 7, loss = 0.22140258
Iteration 8, loss = 0.17551738
Iteration 9, loss = 0.14146605
Iteration 10, loss = 0.11594313
Iteration 11, loss = 0.09694878
Iteration 12, loss = 0.08233076
Iteration 13, loss = 0.07120129
Iteration 14, loss = 0.06224891
Iteration 15, loss = 0.05505363
Iteration 16, loss = 0.04945612
Iteration 17, loss = 0.04467156
Iteration 18, loss = 0.04096098
Iteration 19, loss = 0.03776198
Iteration 20, loss = 0.03478443
Iteration 21, loss = 0.03245188
Iteration 22, loss = 0.03058081
Iteration 23, loss = 0.02890604
Iteration 24, loss = 0.02729271
Iteration 25, loss = 0.02589812
Iteration 26, loss = 0.02484993
Iteration 27, loss = 0.02380147
Iteration 28, loss = 0.02275369
Iteration 29, loss = 0.02188775
Iteration 30, loss = 0.02125049
Iteration 31, loss = 0.02057454
Iteration 32, loss = 0.02002770
Iteration 33, loss = 0.01953032
Iteration 34, loss = 0.01886312
Iteration 35, loss = 0.01840045
Iteration 36, loss = 0.01801126
Iteration 37, loss = 0.01781493
Iteration 38, loss = 0.01727809
Iteration 39, loss = 0.01696111
Iteration 40, loss = 0.01687423
Iteration 41, loss = 0.01648503
Iteration 42, loss = 0.01613316
Iteration 43, loss = 0.01598604
Iteration 44, loss = 0.01577708
Iteration 45, loss = 0.01550539
Iteration 46, loss = 0.01541163
Iteration 47, loss = 0.01519019
Iteration 48, loss = 0.01493799
Iteration 49, loss = 0.01484517
Iteration 50, loss = 0.01489264
Iteration 51, loss = 0.01469860
Iteration 52, loss = 0.01454802
Iteration 53, loss = 0.01421109
Iteration 54, loss = 0.01407398
Iteration 55, loss = 0.01396616
Iteration 56, loss = 0.01388864
Iteration 57, loss = 0.01396248
Iteration 58, loss = 0.01371393
Iteration 59, loss = 0.01350367
Iteration 60, loss = 0.01367458
Iteration 61, loss = 0.01356028
Iteration 62, loss = 0.01335367
Iteration 63, loss = 0.01332351
Iteration 64, loss = 0.01326753
Iteration 65, loss = 0.01335163
Iteration 66, loss = 0.01308711
Iteration 67, loss = 0.01321235
Iteration 68, loss = 0.01293778
Iteration 69, loss = 0.01303852
Iteration 70, loss = 0.01288337
Iteration 71, loss = 0.01293843
Iteration 72, loss = 0.01312803
Iteration 73, loss = 0.01285761
Iteration 74, loss = 0.01271329
Iteration 75, loss = 0.01270097
Iteration 76, loss = 0.01264945
Iteration 77, loss = 0.01257558
Iteration 78, loss = 0.01277095
Iteration 79, loss = 0.01264902
Iteration 80, loss = 0.01277289
Iteration 81, loss = 0.01248867
Iteration 82, loss = 0.01244647
Iteration 83, loss = 0.01254513
Iteration 84, loss = 0.01249991
Iteration 85, loss = 0.01238994
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68742242
Iteration 2, loss = 0.67195067
Iteration 3, loss = 0.65848121
Iteration 4, loss = 0.64502065
Iteration 5, loss = 0.63083157
Iteration 6, loss = 0.61573411
Iteration 7, loss = 0.59933416
Iteration 8, loss = 0.58160204
Iteration 9, loss = 0.56258772
Iteration 10, loss = 0.54228775
Iteration 11, loss = 0.52075012
Iteration 12, loss = 0.49846648
Iteration 13, loss = 0.47543792
Iteration 14, loss = 0.45216004
Iteration 15, loss = 0.42888106
Iteration 16, loss = 0.40590646
Iteration 17, loss = 0.38340805
Iteration 18, loss = 0.36146535
Iteration 19, loss = 0.34018776
Iteration 20, loss = 0.31986404
Iteration 21, loss = 0.30022002
Iteration 22, loss = 0.28169980
Iteration 23, loss = 0.26402829
Iteration 24, loss = 0.24717176
Iteration 25, loss = 0.23140548
Iteration 26, loss = 0.21650821
Iteration 27, loss = 0.20267408
Iteration 28, loss = 0.18970232
Iteration 29, loss = 0.17762016
Iteration 30, loss = 0.16639474
Iteration 31, loss = 0.15595797
Iteration 32, loss = 0.14631422
Iteration 33, loss = 0.13740178
Iteration 34, loss = 0.12913919
Iteration 35, loss = 0.12150496
Iteration 36, loss = 0.11441936
Iteration 37, loss = 0.10791735
Iteration 38, loss = 0.10189526
Iteration 39, loss = 0.09631098
Iteration 40, loss = 0.09117654
Iteration 41, loss = 0.08641686
Iteration 42, loss = 0.08200941
Iteration 43, loss = 0.07789944
Iteration 44, loss = 0.07414459
Iteration 45, loss = 0.07061900
Iteration 46, loss = 0.06735905
Iteration 47, loss = 0.06428290
Iteration 48, loss = 0.06144956
Iteration 49, loss = 0.05880532
Iteration 50, loss = 0.05629025
Iteration 51, loss = 0.05398643
Iteration 52, loss = 0.05181937
Iteration 53, loss = 0.04978636
Iteration 54, loss = 0.04785174
Iteration 55, loss = 0.04607848
Iteration 56, loss = 0.04437989
Iteration 57, loss = 0.04282351
Iteration 58, loss = 0.04130401
Iteration 59, loss = 0.03988559
Iteration 60, loss = 0.03856943
Iteration 61, loss = 0.03728463
Iteration 62, loss = 0.03609300
Iteration 63, loss = 0.03495844
Iteration 64, loss = 0.03388672
Iteration 65, loss = 0.03285303
Iteration 66, loss = 0.03187683
Iteration 67, loss = 0.03095329
Iteration 68, loss = 0.03010352
Iteration 69, loss = 0.02923848
Iteration 70, loss = 0.02843265
Iteration 71, loss = 0.02768666
Iteration 72, loss = 0.02693074
Iteration 73, loss = 0.02623270
Iteration 74, loss = 0.02557655
Iteration 75, loss = 0.02494416
Iteration 76, loss = 0.02434579
Iteration 77, loss = 0.02375115
Iteration 78, loss = 0.02320291
Iteration 79, loss = 0.02267923
Iteration 80, loss = 0.02214798
Iteration 81, loss = 0.02167732
Iteration 82, loss = 0.02118176
Iteration 83, loss = 0.02078166
Iteration 84, loss = 0.02030896
Iteration 85, loss = 0.01989398
Iteration 86, loss = 0.01950023
Iteration 87, loss = 0.01910981
Iteration 88, loss = 0.01874957
Iteration 89, loss = 0.01837337
Iteration 90, loss = 0.01802618
Iteration 91, loss = 0.01768075
Iteration 92, loss = 0.01738101
Iteration 93, loss = 0.01705095
Iteration 94, loss = 0.01675195
Iteration 95, loss = 0.01647684
Iteration 96, loss = 0.01619250
Iteration 97, loss = 0.01592509
Iteration 98, loss = 0.01566111
Iteration 99, loss = 0.01541107
Iteration 100, loss = 0.01516263
Iteration 101, loss = 0.01496785
Iteration 102, loss = 0.01470838
Iteration 103, loss = 0.01450900
Iteration 104, loss = 0.01426162
Iteration 105, loss = 0.01404828
Iteration 106, loss = 0.01384564
Iteration 107, loss = 0.01365197
Iteration 108, loss = 0.01347427
Iteration 109, loss = 0.01329619
Iteration 110, loss = 0.01314456
Iteration 111, loss = 0.01292779
Iteration 112, loss = 0.01273361
Iteration 113, loss = 0.01259168
Iteration 114, loss = 0.01246439
Iteration 115, loss = 0.01231075
Iteration 116, loss = 0.01215485
Iteration 117, loss = 0.01197924
Iteration 118, loss = 0.01182791
Iteration 119, loss = 0.01169974
Iteration 120, loss = 0.01154131
Iteration 121, loss = 0.01141268
Iteration 122, loss = 0.01128003
Iteration 123, loss = 0.01115456
Iteration 124, loss = 0.01100440
Iteration 125, loss = 0.01088511
Iteration 126, loss = 0.01076360
Iteration 127, loss = 0.01069250
Iteration 128, loss = 0.01059026
Iteration 129, loss = 0.01053462
Iteration 130, loss = 0.01043196
Iteration 131, loss = 0.01029393
Iteration 132, loss = 0.01022343
Iteration 133, loss = 0.01006939
Iteration 134, loss = 0.00997802
Iteration 135, loss = 0.00988790
Iteration 136, loss = 0.00978208
Iteration 137, loss = 0.00970112
Iteration 138, loss = 0.00962748
Iteration 139, loss = 0.00950606
Iteration 140, loss = 0.00942033
Iteration 141, loss = 0.00933824
Iteration 142, loss = 0.00925854
Iteration 143, loss = 0.00917838
Iteration 144, loss = 0.00910810
Iteration 145, loss = 0.00904152
Iteration 146, loss = 0.00897659
Iteration 147, loss = 0.00891933
Iteration 148, loss = 0.00888571
Iteration 149, loss = 0.00879410
Iteration 150, loss = 0.00872256
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67966997
Iteration 2, loss = 0.64154818
Iteration 3, loss = 0.58659138
Iteration 4, loss = 0.51219862
Iteration 5, loss = 0.43396528
Iteration 6, loss = 0.36117692
Iteration 7, loss = 0.29766873
Iteration 8, loss = 0.24431728
Iteration 9, loss = 0.20089795
Iteration 10, loss = 0.16641112
Iteration 11, loss = 0.13940884
Iteration 12, loss = 0.11789649
Iteration 13, loss = 0.10101440
Iteration 14, loss = 0.08734189
Iteration 15, loss = 0.07661252
Iteration 16, loss = 0.06780664
Iteration 17, loss = 0.06040176
Iteration 18, loss = 0.05443618
Iteration 19, loss = 0.04933703
Iteration 20, loss = 0.04505317
Iteration 21, loss = 0.04136814
Iteration 22, loss = 0.03824155
Iteration 23, loss = 0.03549597
Iteration 24, loss = 0.03309173
Iteration 25, loss = 0.03095611
Iteration 26, loss = 0.02917965
Iteration 27, loss = 0.02757981
Iteration 28, loss = 0.02607006
Iteration 29, loss = 0.02473375
Iteration 30, loss = 0.02353129
Iteration 31, loss = 0.02248694
Iteration 32, loss = 0.02152257
Iteration 33, loss = 0.02082044
Iteration 34, loss = 0.01989869
Iteration 35, loss = 0.01912879
Iteration 36, loss = 0.01850162
Iteration 37, loss = 0.01796902
Iteration 38, loss = 0.01734708
Iteration 39, loss = 0.01683326
Iteration 40, loss = 0.01627536
Iteration 41, loss = 0.01586386
Iteration 42, loss = 0.01546789
Iteration 43, loss = 0.01512133
Iteration 44, loss = 0.01470104
Iteration 45, loss = 0.01442487
Iteration 46, loss = 0.01411099
Iteration 47, loss = 0.01391109
Iteration 48, loss = 0.01354083
Iteration 49, loss = 0.01332549
Iteration 50, loss = 0.01303281
Iteration 51, loss = 0.01286613
Iteration 52, loss = 0.01264520
Iteration 53, loss = 0.01236721
Iteration 54, loss = 0.01224609
Iteration 55, loss = 0.01205198
Iteration 56, loss = 0.01200797
Iteration 57, loss = 0.01170038
Iteration 58, loss = 0.01153276
Iteration 59, loss = 0.01137177
Iteration 60, loss = 0.01125972
Iteration 61, loss = 0.01113097
Iteration 62, loss = 0.01104883
Iteration 63, loss = 0.01095006
Iteration 64, loss = 0.01079849
Iteration 65, loss = 0.01070297
Iteration 66, loss = 0.01060508
Iteration 67, loss = 0.01051873
Iteration 68, loss = 0.01036629
Iteration 69, loss = 0.01032538
Iteration 70, loss = 0.01022871
Iteration 71, loss = 0.01005290
Iteration 72, loss = 0.01006927
Iteration 73, loss = 0.00998684
Iteration 74, loss = 0.00994139
Iteration 75, loss = 0.00988332
Iteration 76, loss = 0.00978202
Iteration 77, loss = 0.00977231
Iteration 78, loss = 0.00971810
Iteration 79, loss = 0.00972215
Iteration 80, loss = 0.00952335
Iteration 81, loss = 0.00949127
Iteration 82, loss = 0.00952792
Iteration 83, loss = 0.00941493
Iteration 84, loss = 0.00937925
Iteration 85, loss = 0.00925191
Iteration 86, loss = 0.00924981
Iteration 87, loss = 0.00917202
Iteration 88, loss = 0.00906280
Iteration 89, loss = 0.00913995
Iteration 90, loss = 0.00908614
Iteration 91, loss = 0.00909079
Iteration 92, loss = 0.00896614
Iteration 93, loss = 0.00896176
Iteration 94, loss = 0.00891735
Iteration 95, loss = 0.00885776
Iteration 96, loss = 0.00880996
Iteration 97, loss = 0.00888996
Iteration 98, loss = 0.00878339
Iteration 99, loss = 0.00877855
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68665698
Iteration 2, loss = 0.65588862
Iteration 3, loss = 0.61640484
Iteration 4, loss = 0.56436434
Iteration 5, loss = 0.50285129
Iteration 6, loss = 0.43992004
Iteration 7, loss = 0.37995581
Iteration 8, loss = 0.32480320
Iteration 9, loss = 0.27602570
Iteration 10, loss = 0.23391366
Iteration 11, loss = 0.19762524
Iteration 12, loss = 0.16734649
Iteration 13, loss = 0.14286965
Iteration 14, loss = 0.12306323
Iteration 15, loss = 0.10721663
Iteration 16, loss = 0.09405675
Iteration 17, loss = 0.08344321
Iteration 18, loss = 0.07462567
Iteration 19, loss = 0.06719080
Iteration 20, loss = 0.06109782
Iteration 21, loss = 0.05577827
Iteration 22, loss = 0.05144536
Iteration 23, loss = 0.04747036
Iteration 24, loss = 0.04414587
Iteration 25, loss = 0.04115383
Iteration 26, loss = 0.03876453
Iteration 27, loss = 0.03656269
Iteration 28, loss = 0.03451206
Iteration 29, loss = 0.03265903
Iteration 30, loss = 0.03110816
Iteration 31, loss = 0.02965534
Iteration 32, loss = 0.02840006
Iteration 33, loss = 0.02727738
Iteration 34, loss = 0.02615889
Iteration 35, loss = 0.02523026
Iteration 36, loss = 0.02430065
Iteration 37, loss = 0.02352587
Iteration 38, loss = 0.02271280
Iteration 39, loss = 0.02206198
Iteration 40, loss = 0.02152557
Iteration 41, loss = 0.02085997
Iteration 42, loss = 0.02034872
Iteration 43, loss = 0.02002029
Iteration 44, loss = 0.01940690
Iteration 45, loss = 0.01900464
Iteration 46, loss = 0.01857782
Iteration 47, loss = 0.01815558
Iteration 48, loss = 0.01802966
Iteration 49, loss = 0.01753915
Iteration 50, loss = 0.01722109
Iteration 51, loss = 0.01691523
Iteration 52, loss = 0.01654207
Iteration 53, loss = 0.01642262
Iteration 54, loss = 0.01615289
Iteration 55, loss = 0.01592798
Iteration 56, loss = 0.01574123
Iteration 57, loss = 0.01552172
Iteration 58, loss = 0.01532560
Iteration 59, loss = 0.01509777
Iteration 60, loss = 0.01496733
Iteration 61, loss = 0.01476233
Iteration 62, loss = 0.01471711
Iteration 63, loss = 0.01446018
Iteration 64, loss = 0.01438394
Iteration 65, loss = 0.01438738
Iteration 66, loss = 0.01411960
Iteration 67, loss = 0.01401839
Iteration 68, loss = 0.01379333
Iteration 69, loss = 0.01378901
Iteration 70, loss = 0.01365668
Iteration 71, loss = 0.01350754
Iteration 72, loss = 0.01352976
Iteration 73, loss = 0.01337457
Iteration 74, loss = 0.01346779
Iteration 75, loss = 0.01328244
Iteration 76, loss = 0.01311827
Iteration 77, loss = 0.01297464
Iteration 78, loss = 0.01292149
Iteration 79, loss = 0.01281671
Iteration 80, loss = 0.01275760
Iteration 81, loss = 0.01268452
Iteration 82, loss = 0.01284467
Iteration 83, loss = 0.01253719
Iteration 84, loss = 0.01264771
Iteration 85, loss = 0.01258516
Iteration 86, loss = 0.01243957
Iteration 87, loss = 0.01243856
Iteration 88, loss = 0.01227334
Iteration 89, loss = 0.01234178
Iteration 90, loss = 0.01221946
Iteration 91, loss = 0.01212407
Iteration 92, loss = 0.01206268
Iteration 93, loss = 0.01210118
Iteration 94, loss = 0.01205565
Iteration 95, loss = 0.01195076
Iteration 96, loss = 0.01194783
Iteration 97, loss = 0.01186213
Iteration 98, loss = 0.01185126
Iteration 99, loss = 0.01178805
Iteration 100, loss = 0.01182561
Iteration 101, loss = 0.01191236
Iteration 102, loss = 0.01174378
Iteration 103, loss = 0.01184294
Iteration 104, loss = 0.01188803
Iteration 105, loss = 0.01179245
Iteration 106, loss = 0.01161151
Iteration 107, loss = 0.01161238
Iteration 108, loss = 0.01159506
Iteration 109, loss = 0.01162977
Iteration 110, loss = 0.01154208
Iteration 111, loss = 0.01144505
Iteration 112, loss = 0.01152160
Iteration 113, loss = 0.01139770
Iteration 114, loss = 0.01142198
Iteration 115, loss = 0.01144874
Iteration 116, loss = 0.01139616
Iteration 117, loss = 0.01129275
Iteration 118, loss = 0.01161239
Iteration 119, loss = 0.01143963
Iteration 120, loss = 0.01124820
Iteration 121, loss = 0.01134545
Iteration 122, loss = 0.01123499
Iteration 123, loss = 0.01133028
Iteration 124, loss = 0.01118288
Iteration 125, loss = 0.01120886
Iteration 126, loss = 0.01134488
Iteration 127, loss = 0.01117780
Iteration 128, loss = 0.01111252
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67578046
Iteration 2, loss = 0.62065592
Iteration 3, loss = 0.54067649
Iteration 4, loss = 0.44935482
Iteration 5, loss = 0.36216108
Iteration 6, loss = 0.28607731
Iteration 7, loss = 0.22492750
Iteration 8, loss = 0.17800450
Iteration 9, loss = 0.14314453
Iteration 10, loss = 0.11735364
Iteration 11, loss = 0.09819696
Iteration 12, loss = 0.08354734
Iteration 13, loss = 0.07244383
Iteration 14, loss = 0.06373727
Iteration 15, loss = 0.05669937
Iteration 16, loss = 0.05120257
Iteration 17, loss = 0.04657435
Iteration 18, loss = 0.04280581
Iteration 19, loss = 0.03957192
Iteration 20, loss = 0.03696888
Iteration 21, loss = 0.03458562
Iteration 22, loss = 0.03259117
Iteration 23, loss = 0.03089876
Iteration 24, loss = 0.02926679
Iteration 25, loss = 0.02804346
Iteration 26, loss = 0.02686725
Iteration 27, loss = 0.02599003
Iteration 28, loss = 0.02507795
Iteration 29, loss = 0.02412622
Iteration 30, loss = 0.02336297
Iteration 31, loss = 0.02277118
Iteration 32, loss = 0.02211110
Iteration 33, loss = 0.02178527
Iteration 34, loss = 0.02114121
Iteration 35, loss = 0.02067928
Iteration 36, loss = 0.02031431
Iteration 37, loss = 0.01981964
Iteration 38, loss = 0.01965647
Iteration 39, loss = 0.01899572
Iteration 40, loss = 0.01891997
Iteration 41, loss = 0.01854529
Iteration 42, loss = 0.01836787
Iteration 43, loss = 0.01821141
Iteration 44, loss = 0.01782946
Iteration 45, loss = 0.01767708
Iteration 46, loss = 0.01763922
Iteration 47, loss = 0.01737862
Iteration 48, loss = 0.01723205
Iteration 49, loss = 0.01703583
Iteration 50, loss = 0.01682409
Iteration 51, loss = 0.01649895
Iteration 52, loss = 0.01664462
Iteration 53, loss = 0.01638444
Iteration 54, loss = 0.01628739
Iteration 55, loss = 0.01615360
Iteration 56, loss = 0.01598437
Iteration 57, loss = 0.01599493
Iteration 58, loss = 0.01587417
Iteration 59, loss = 0.01573860
Iteration 60, loss = 0.01581368
Iteration 61, loss = 0.01569129
Iteration 62, loss = 0.01554305
Iteration 63, loss = 0.01551124
Iteration 64, loss = 0.01541809
Iteration 65, loss = 0.01528280
Iteration 66, loss = 0.01516598
Iteration 67, loss = 0.01521416
Iteration 68, loss = 0.01521879
Iteration 69, loss = 0.01509807
Iteration 70, loss = 0.01500032
Iteration 71, loss = 0.01494090
Iteration 72, loss = 0.01527212
Iteration 73, loss = 0.01507024
Iteration 74, loss = 0.01472893
Iteration 75, loss = 0.01478851
Iteration 76, loss = 0.01471219
Iteration 77, loss = 0.01471682
Iteration 78, loss = 0.01460566
Iteration 79, loss = 0.01467008
Iteration 80, loss = 0.01456068
Iteration 81, loss = 0.01464355
Iteration 82, loss = 0.01444448
Iteration 83, loss = 0.01456808
Iteration 84, loss = 0.01456806
Iteration 85, loss = 0.01434037
Iteration 86, loss = 0.01445191
Iteration 87, loss = 0.01450265
Iteration 88, loss = 0.01436751
Iteration 89, loss = 0.01417277
Iteration 90, loss = 0.01431934
Iteration 91, loss = 0.01426064
Iteration 92, loss = 0.01426497
Iteration 93, loss = 0.01421252
Iteration 94, loss = 0.01435182
Iteration 95, loss = 0.01413057
Iteration 96, loss = 0.01405082
Iteration 97, loss = 0.01432533
Iteration 98, loss = 0.01414253
Iteration 99, loss = 0.01408514
Iteration 100, loss = 0.01398304
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71419279
Iteration 2, loss = 0.68041968
Iteration 3, loss = 0.64148984
Iteration 4, loss = 0.59356806
Iteration 5, loss = 0.53398387
Iteration 6, loss = 0.46865704
Iteration 7, loss = 0.40237594
Iteration 8, loss = 0.34177166
Iteration 9, loss = 0.28837991
Iteration 10, loss = 0.24257020
Iteration 11, loss = 0.20470930
Iteration 12, loss = 0.17342360
Iteration 13, loss = 0.14799556
Iteration 14, loss = 0.12738854
Iteration 15, loss = 0.11059583
Iteration 16, loss = 0.09689941
Iteration 17, loss = 0.08576371
Iteration 18, loss = 0.07650260
Iteration 19, loss = 0.06871195
Iteration 20, loss = 0.06219973
Iteration 21, loss = 0.05674896
Iteration 22, loss = 0.05192835
Iteration 23, loss = 0.04795136
Iteration 24, loss = 0.04437007
Iteration 25, loss = 0.04131015
Iteration 26, loss = 0.03861502
Iteration 27, loss = 0.03627117
Iteration 28, loss = 0.03410501
Iteration 29, loss = 0.03224980
Iteration 30, loss = 0.03060966
Iteration 31, loss = 0.02899306
Iteration 32, loss = 0.02769914
Iteration 33, loss = 0.02647433
Iteration 34, loss = 0.02542508
Iteration 35, loss = 0.02431251
Iteration 36, loss = 0.02344549
Iteration 37, loss = 0.02260410
Iteration 38, loss = 0.02187779
Iteration 39, loss = 0.02106826
Iteration 40, loss = 0.02045674
Iteration 41, loss = 0.01983097
Iteration 42, loss = 0.01926981
Iteration 43, loss = 0.01871405
Iteration 44, loss = 0.01821005
Iteration 45, loss = 0.01775988
Iteration 46, loss = 0.01730933
Iteration 47, loss = 0.01695253
Iteration 48, loss = 0.01666458
Iteration 49, loss = 0.01623157
Iteration 50, loss = 0.01589510
Iteration 51, loss = 0.01567989
Iteration 52, loss = 0.01543308
Iteration 53, loss = 0.01504215
Iteration 54, loss = 0.01484875
Iteration 55, loss = 0.01454707
Iteration 56, loss = 0.01432962
Iteration 57, loss = 0.01409349
Iteration 58, loss = 0.01395486
Iteration 59, loss = 0.01372886
Iteration 60, loss = 0.01351642
Iteration 61, loss = 0.01330923
Iteration 62, loss = 0.01324568
Iteration 63, loss = 0.01302416
Iteration 64, loss = 0.01286513
Iteration 65, loss = 0.01276274
Iteration 66, loss = 0.01256596
Iteration 67, loss = 0.01242717
Iteration 68, loss = 0.01236012
Iteration 69, loss = 0.01218258
Iteration 70, loss = 0.01206381
Iteration 71, loss = 0.01199225
Iteration 72, loss = 0.01188519
Iteration 73, loss = 0.01175214
Iteration 74, loss = 0.01164013
Iteration 75, loss = 0.01159809
Iteration 76, loss = 0.01150331
Iteration 77, loss = 0.01133986
Iteration 78, loss = 0.01135733
Iteration 79, loss = 0.01119452
Iteration 80, loss = 0.01123158
Iteration 81, loss = 0.01102606
Iteration 82, loss = 0.01105273
Iteration 83, loss = 0.01088899
Iteration 84, loss = 0.01093047
Iteration 85, loss = 0.01077479
Iteration 86, loss = 0.01068729
Iteration 87, loss = 0.01062035
Iteration 88, loss = 0.01058435
Iteration 89, loss = 0.01070264
Iteration 90, loss = 0.01057759
Iteration 91, loss = 0.01038564
Iteration 92, loss = 0.01033027
Iteration 93, loss = 0.01026233
Iteration 94, loss = 0.01026741
Iteration 95, loss = 0.01018817
Iteration 96, loss = 0.01019734
Iteration 97, loss = 0.01015546
Iteration 98, loss = 0.01004683
Iteration 99, loss = 0.01002896
Iteration 100, loss = 0.00996500
Iteration 101, loss = 0.00992900
Iteration 102, loss = 0.00990040
Iteration 103, loss = 0.00995167
Iteration 104, loss = 0.00978630
Iteration 105, loss = 0.00979762
Iteration 106, loss = 0.00975916
Iteration 107, loss = 0.00976010
Iteration 108, loss = 0.00967298
Iteration 109, loss = 0.00967499
Iteration 110, loss = 0.00960508
Iteration 111, loss = 0.00961381
Iteration 112, loss = 0.00960367
Iteration 113, loss = 0.00954812
Iteration 114, loss = 0.00949460
Iteration 115, loss = 0.00949379
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68230237
Iteration 2, loss = 0.66394940
Iteration 3, loss = 0.64840307
Iteration 4, loss = 0.63294817
Iteration 5, loss = 0.61712007
Iteration 6, loss = 0.59983096
Iteration 7, loss = 0.58132828
Iteration 8, loss = 0.56160783
Iteration 9, loss = 0.54083899
Iteration 10, loss = 0.51892094
Iteration 11, loss = 0.49625952
Iteration 12, loss = 0.47288334
Iteration 13, loss = 0.44942936
Iteration 14, loss = 0.42585429
Iteration 15, loss = 0.40250311
Iteration 16, loss = 0.37986377
Iteration 17, loss = 0.35776419
Iteration 18, loss = 0.33643210
Iteration 19, loss = 0.31589011
Iteration 20, loss = 0.29651709
Iteration 21, loss = 0.27803723
Iteration 22, loss = 0.26065793
Iteration 23, loss = 0.24428046
Iteration 24, loss = 0.22888439
Iteration 25, loss = 0.21445202
Iteration 26, loss = 0.20114488
Iteration 27, loss = 0.18868551
Iteration 28, loss = 0.17717492
Iteration 29, loss = 0.16647615
Iteration 30, loss = 0.15657710
Iteration 31, loss = 0.14736802
Iteration 32, loss = 0.13895175
Iteration 33, loss = 0.13114268
Iteration 34, loss = 0.12389820
Iteration 35, loss = 0.11727265
Iteration 36, loss = 0.11112357
Iteration 37, loss = 0.10542267
Iteration 38, loss = 0.10013004
Iteration 39, loss = 0.09520431
Iteration 40, loss = 0.09069445
Iteration 41, loss = 0.08649625
Iteration 42, loss = 0.08262758
Iteration 43, loss = 0.07894555
Iteration 44, loss = 0.07559802
Iteration 45, loss = 0.07250175
Iteration 46, loss = 0.06953964
Iteration 47, loss = 0.06678200
Iteration 48, loss = 0.06425553
Iteration 49, loss = 0.06186964
Iteration 50, loss = 0.05964791
Iteration 51, loss = 0.05753284
Iteration 52, loss = 0.05553796
Iteration 53, loss = 0.05374262
Iteration 54, loss = 0.05206135
Iteration 55, loss = 0.05035262
Iteration 56, loss = 0.04879235
Iteration 57, loss = 0.04726717
Iteration 58, loss = 0.04591075
Iteration 59, loss = 0.04459603
Iteration 60, loss = 0.04338373
Iteration 61, loss = 0.04222951
Iteration 62, loss = 0.04113095
Iteration 63, loss = 0.04006044
Iteration 64, loss = 0.03914563
Iteration 65, loss = 0.03819145
Iteration 66, loss = 0.03725329
Iteration 67, loss = 0.03636947
Iteration 68, loss = 0.03560287
Iteration 69, loss = 0.03472366
Iteration 70, loss = 0.03396059
Iteration 71, loss = 0.03327328
Iteration 72, loss = 0.03260092
Iteration 73, loss = 0.03189950
Iteration 74, loss = 0.03136018
Iteration 75, loss = 0.03071879
Iteration 76, loss = 0.03008315
Iteration 77, loss = 0.02960863
Iteration 78, loss = 0.02900802
Iteration 79, loss = 0.02853610
Iteration 80, loss = 0.02805990
Iteration 81, loss = 0.02758712
Iteration 82, loss = 0.02711708
Iteration 83, loss = 0.02666370
Iteration 84, loss = 0.02630620
Iteration 85, loss = 0.02585941
Iteration 86, loss = 0.02555208
Iteration 87, loss = 0.02515273
Iteration 88, loss = 0.02480608
Iteration 89, loss = 0.02452971
Iteration 90, loss = 0.02409300
Iteration 91, loss = 0.02377401
Iteration 92, loss = 0.02346419
Iteration 93, loss = 0.02316896
Iteration 94, loss = 0.02286777
Iteration 95, loss = 0.02258204
Iteration 96, loss = 0.02237381
Iteration 97, loss = 0.02206422
Iteration 98, loss = 0.02181248
Iteration 99, loss = 0.02164837
Iteration 100, loss = 0.02134309
Iteration 101, loss = 0.02117158
Iteration 102, loss = 0.02091343
Iteration 103, loss = 0.02067904
Iteration 104, loss = 0.02048390
Iteration 105, loss = 0.02024276
Iteration 106, loss = 0.02005385
Iteration 107, loss = 0.01986237
Iteration 108, loss = 0.01970865
Iteration 109, loss = 0.01946420
Iteration 110, loss = 0.01926080
Iteration 111, loss = 0.01910003
Iteration 112, loss = 0.01895414
Iteration 113, loss = 0.01877587
Iteration 114, loss = 0.01860556
Iteration 115, loss = 0.01845838
Iteration 116, loss = 0.01828324
Iteration 117, loss = 0.01814243
Iteration 118, loss = 0.01799762
Iteration 119, loss = 0.01785313
Iteration 120, loss = 0.01772506
Iteration 121, loss = 0.01760394
Iteration 122, loss = 0.01747803
Iteration 123, loss = 0.01736393
Iteration 124, loss = 0.01726652
Iteration 125, loss = 0.01718559
Iteration 126, loss = 0.01701918
Iteration 127, loss = 0.01692168
Iteration 128, loss = 0.01681323
Iteration 129, loss = 0.01671251
Iteration 130, loss = 0.01666551
Iteration 131, loss = 0.01650849
Iteration 132, loss = 0.01648754
Iteration 133, loss = 0.01634867
Iteration 134, loss = 0.01623844
Iteration 135, loss = 0.01616794
Iteration 136, loss = 0.01604894
Iteration 137, loss = 0.01599391
Iteration 138, loss = 0.01596425
Iteration 139, loss = 0.01581477
Iteration 140, loss = 0.01570521
Iteration 141, loss = 0.01563063
Iteration 142, loss = 0.01554424
Iteration 143, loss = 0.01549944
Iteration 144, loss = 0.01536280
Iteration 145, loss = 0.01529505
Iteration 146, loss = 0.01524795
Iteration 147, loss = 0.01506833
Iteration 148, loss = 0.01497480
Iteration 149, loss = 0.01487456
Iteration 150, loss = 0.01478860
Iteration 151, loss = 0.01471107
Iteration 152, loss = 0.01480832
Iteration 153, loss = 0.01473566
Iteration 154, loss = 0.01468776
Iteration 155, loss = 0.01465475
Iteration 156, loss = 0.01457888
Iteration 157, loss = 0.01452117
Iteration 158, loss = 0.01445232
Iteration 159, loss = 0.01434503
Iteration 160, loss = 0.01420435
Iteration 161, loss = 0.01419609
Iteration 162, loss = 0.01415496
Iteration 163, loss = 0.01405096
Iteration 164, loss = 0.01399492
Iteration 165, loss = 0.01395035
Iteration 166, loss = 0.01390600
Iteration 167, loss = 0.01395212
Iteration 168, loss = 0.01386440
Iteration 169, loss = 0.01379880
Iteration 170, loss = 0.01377015
Iteration 171, loss = 0.01372515
Iteration 172, loss = 0.01361407
Iteration 173, loss = 0.01362464
Iteration 174, loss = 0.01354352
Iteration 175, loss = 0.01349679
Iteration 176, loss = 0.01340314
Iteration 177, loss = 0.01333550
Iteration 178, loss = 0.01328739
Iteration 179, loss = 0.01327773
Iteration 180, loss = 0.01324725
Iteration 181, loss = 0.01324310
Iteration 182, loss = 0.01319238
Iteration 183, loss = 0.01319633
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72871298
Iteration 2, loss = 0.69341549
Iteration 3, loss = 0.64269357
Iteration 4, loss = 0.57954727
Iteration 5, loss = 0.50632958
Iteration 6, loss = 0.43181595
Iteration 7, loss = 0.36087018
Iteration 8, loss = 0.29770626
Iteration 9, loss = 0.24469953
Iteration 10, loss = 0.20169568
Iteration 11, loss = 0.16753341
Iteration 12, loss = 0.14050299
Iteration 13, loss = 0.11942251
Iteration 14, loss = 0.10265880
Iteration 15, loss = 0.08919287
Iteration 16, loss = 0.07841054
Iteration 17, loss = 0.06956772
Iteration 18, loss = 0.06238449
Iteration 19, loss = 0.05637437
Iteration 20, loss = 0.05132390
Iteration 21, loss = 0.04699876
Iteration 22, loss = 0.04338425
Iteration 23, loss = 0.04020725
Iteration 24, loss = 0.03757495
Iteration 25, loss = 0.03517368
Iteration 26, loss = 0.03307384
Iteration 27, loss = 0.03123617
Iteration 28, loss = 0.02972982
Iteration 29, loss = 0.02817584
Iteration 30, loss = 0.02700648
Iteration 31, loss = 0.02576468
Iteration 32, loss = 0.02473589
Iteration 33, loss = 0.02390916
Iteration 34, loss = 0.02290722
Iteration 35, loss = 0.02218993
Iteration 36, loss = 0.02151301
Iteration 37, loss = 0.02084692
Iteration 38, loss = 0.02013534
Iteration 39, loss = 0.01964790
Iteration 40, loss = 0.01917726
Iteration 41, loss = 0.01867818
Iteration 42, loss = 0.01837522
Iteration 43, loss = 0.01785232
Iteration 44, loss = 0.01765762
Iteration 45, loss = 0.01710179
Iteration 46, loss = 0.01684533
Iteration 47, loss = 0.01644958
Iteration 48, loss = 0.01625376
Iteration 49, loss = 0.01603685
Iteration 50, loss = 0.01579398
Iteration 51, loss = 0.01566951
Iteration 52, loss = 0.01557788
Iteration 53, loss = 0.01517204
Iteration 54, loss = 0.01481270
Iteration 55, loss = 0.01479294
Iteration 56, loss = 0.01459826
Iteration 57, loss = 0.01435612
Iteration 58, loss = 0.01437382
Iteration 59, loss = 0.01409578
Iteration 60, loss = 0.01398316
Iteration 61, loss = 0.01382624
Iteration 62, loss = 0.01369146
Iteration 63, loss = 0.01352863
Iteration 64, loss = 0.01351606
Iteration 65, loss = 0.01332246
Iteration 66, loss = 0.01332047
Iteration 67, loss = 0.01324323
Iteration 68, loss = 0.01314977
Iteration 69, loss = 0.01291508
Iteration 70, loss = 0.01296840
Iteration 71, loss = 0.01284875
Iteration 72, loss = 0.01273202
Iteration 73, loss = 0.01277923
Iteration 74, loss = 0.01285125
Iteration 75, loss = 0.01271768
Iteration 76, loss = 0.01254077
Iteration 77, loss = 0.01247836
Iteration 78, loss = 0.01250959
Iteration 79, loss = 0.01235843
Iteration 80, loss = 0.01226296
Iteration 81, loss = 0.01224230
Iteration 82, loss = 0.01223021
Iteration 83, loss = 0.01202213
Iteration 84, loss = 0.01206364
Iteration 85, loss = 0.01205093
Iteration 86, loss = 0.01186786
Iteration 87, loss = 0.01196259
Iteration 88, loss = 0.01181043
Iteration 89, loss = 0.01182269
Iteration 90, loss = 0.01175719
Iteration 91, loss = 0.01180076
Iteration 92, loss = 0.01175540
Iteration 93, loss = 0.01164748
Iteration 94, loss = 0.01160799
Iteration 95, loss = 0.01156055
Iteration 96, loss = 0.01163077
Iteration 97, loss = 0.01151896
Iteration 98, loss = 0.01152397
Iteration 99, loss = 0.01146924
Iteration 100, loss = 0.01157133
Iteration 101, loss = 0.01147309
Iteration 102, loss = 0.01137844
Iteration 103, loss = 0.01148171
Iteration 104, loss = 0.01134887
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69376468
Iteration 2, loss = 0.67068088
Iteration 3, loss = 0.65136727
Iteration 4, loss = 0.63305056
Iteration 5, loss = 0.61436893
Iteration 6, loss = 0.59489659
Iteration 7, loss = 0.57451332
Iteration 8, loss = 0.55315950
Iteration 9, loss = 0.53102434
Iteration 10, loss = 0.50803177
Iteration 11, loss = 0.48489484
Iteration 12, loss = 0.46132318
Iteration 13, loss = 0.43757822
Iteration 14, loss = 0.41396373
Iteration 15, loss = 0.39072054
Iteration 16, loss = 0.36781550
Iteration 17, loss = 0.34557924
Iteration 18, loss = 0.32423274
Iteration 19, loss = 0.30378991
Iteration 20, loss = 0.28428368
Iteration 21, loss = 0.26593017
Iteration 22, loss = 0.24868797
Iteration 23, loss = 0.23259743
Iteration 24, loss = 0.21754143
Iteration 25, loss = 0.20354718
Iteration 26, loss = 0.19051306
Iteration 27, loss = 0.17857007
Iteration 28, loss = 0.16737445
Iteration 29, loss = 0.15709788
Iteration 30, loss = 0.14769879
Iteration 31, loss = 0.13888251
Iteration 32, loss = 0.13083963
Iteration 33, loss = 0.12339734
Iteration 34, loss = 0.11659187
Iteration 35, loss = 0.11025542
Iteration 36, loss = 0.10445080
Iteration 37, loss = 0.09903305
Iteration 38, loss = 0.09396510
Iteration 39, loss = 0.08939491
Iteration 40, loss = 0.08517554
Iteration 41, loss = 0.08112722
Iteration 42, loss = 0.07746823
Iteration 43, loss = 0.07404814
Iteration 44, loss = 0.07082346
Iteration 45, loss = 0.06785733
Iteration 46, loss = 0.06509449
Iteration 47, loss = 0.06244561
Iteration 48, loss = 0.06004334
Iteration 49, loss = 0.05783825
Iteration 50, loss = 0.05571671
Iteration 51, loss = 0.05370615
Iteration 52, loss = 0.05176401
Iteration 53, loss = 0.05001905
Iteration 54, loss = 0.04839473
Iteration 55, loss = 0.04677553
Iteration 56, loss = 0.04537729
Iteration 57, loss = 0.04390500
Iteration 58, loss = 0.04256713
Iteration 59, loss = 0.04132686
Iteration 60, loss = 0.04012392
Iteration 61, loss = 0.03904040
Iteration 62, loss = 0.03791056
Iteration 63, loss = 0.03691345
Iteration 64, loss = 0.03599410
Iteration 65, loss = 0.03503413
Iteration 66, loss = 0.03428253
Iteration 67, loss = 0.03338060
Iteration 68, loss = 0.03258513
Iteration 69, loss = 0.03181897
Iteration 70, loss = 0.03108298
Iteration 71, loss = 0.03042001
Iteration 72, loss = 0.02972103
Iteration 73, loss = 0.02913206
Iteration 74, loss = 0.02847636
Iteration 75, loss = 0.02789174
Iteration 76, loss = 0.02738839
Iteration 77, loss = 0.02682610
Iteration 78, loss = 0.02624425
Iteration 79, loss = 0.02576572
Iteration 80, loss = 0.02527098
Iteration 81, loss = 0.02477221
Iteration 82, loss = 0.02430807
Iteration 83, loss = 0.02384883
Iteration 84, loss = 0.02343016
Iteration 85, loss = 0.02305291
Iteration 86, loss = 0.02270685
Iteration 87, loss = 0.02235834
Iteration 88, loss = 0.02204204
Iteration 89, loss = 0.02174738
Iteration 90, loss = 0.02140715
Iteration 91, loss = 0.02106611
Iteration 92, loss = 0.02073406
Iteration 93, loss = 0.02042448
Iteration 94, loss = 0.02013001
Iteration 95, loss = 0.01987180
Iteration 96, loss = 0.01956962
Iteration 97, loss = 0.01929508
Iteration 98, loss = 0.01901328
Iteration 99, loss = 0.01875890
Iteration 100, loss = 0.01860453
Iteration 101, loss = 0.01831403
Iteration 102, loss = 0.01808781
Iteration 103, loss = 0.01783970
Iteration 104, loss = 0.01764819
Iteration 105, loss = 0.01744360
Iteration 106, loss = 0.01722718
Iteration 107, loss = 0.01708361
Iteration 108, loss = 0.01686041
Iteration 109, loss = 0.01669238
Iteration 110, loss = 0.01654216
Iteration 111, loss = 0.01636171
Iteration 112, loss = 0.01616380
Iteration 113, loss = 0.01604139
Iteration 114, loss = 0.01584364
Iteration 115, loss = 0.01568795
Iteration 116, loss = 0.01554858
Iteration 117, loss = 0.01534437
Iteration 118, loss = 0.01522540
Iteration 119, loss = 0.01505334
Iteration 120, loss = 0.01488559
Iteration 121, loss = 0.01474870
Iteration 122, loss = 0.01458837
Iteration 123, loss = 0.01448284
Iteration 124, loss = 0.01434123
Iteration 125, loss = 0.01424365
Iteration 126, loss = 0.01413497
Iteration 127, loss = 0.01399227
Iteration 128, loss = 0.01388248
Iteration 129, loss = 0.01379644
Iteration 130, loss = 0.01368769
Iteration 131, loss = 0.01360177
Iteration 132, loss = 0.01344513
Iteration 133, loss = 0.01341111
Iteration 134, loss = 0.01325205
Iteration 135, loss = 0.01316703
Iteration 136, loss = 0.01303824
Iteration 137, loss = 0.01296878
Iteration 138, loss = 0.01293424
Iteration 139, loss = 0.01282840
Iteration 140, loss = 0.01275111
Iteration 141, loss = 0.01268237
Iteration 142, loss = 0.01259188
Iteration 143, loss = 0.01250212
Iteration 144, loss = 0.01241029
Iteration 145, loss = 0.01232744
Iteration 146, loss = 0.01224175
Iteration 147, loss = 0.01224913
Iteration 148, loss = 0.01206354
Iteration 149, loss = 0.01199797
Iteration 150, loss = 0.01193357
Iteration 151, loss = 0.01185816
Iteration 152, loss = 0.01178190
Iteration 153, loss = 0.01173024
Iteration 154, loss = 0.01167318
Iteration 155, loss = 0.01164448
Iteration 156, loss = 0.01159811
Iteration 157, loss = 0.01147260
Iteration 158, loss = 0.01141535
Iteration 159, loss = 0.01138267
Iteration 160, loss = 0.01129396
Iteration 161, loss = 0.01124909
Iteration 162, loss = 0.01121601
Iteration 163, loss = 0.01112634
Iteration 164, loss = 0.01107399
Iteration 165, loss = 0.01102196
Iteration 166, loss = 0.01097293
Iteration 167, loss = 0.01095363
Iteration 168, loss = 0.01087288
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70585088
Iteration 2, loss = 0.68415076
Iteration 3, loss = 0.66295230
Iteration 4, loss = 0.63862714
Iteration 5, loss = 0.60958139
Iteration 6, loss = 0.57669403
Iteration 7, loss = 0.54061984
Iteration 8, loss = 0.50275578
Iteration 9, loss = 0.46468033
Iteration 10, loss = 0.42702988
Iteration 11, loss = 0.39069036
Iteration 12, loss = 0.35621488
Iteration 13, loss = 0.32365445
Iteration 14, loss = 0.29372251
Iteration 15, loss = 0.26602235
Iteration 16, loss = 0.24089389
Iteration 17, loss = 0.21823508
Iteration 18, loss = 0.19784054
Iteration 19, loss = 0.17970748
Iteration 20, loss = 0.16350871
Iteration 21, loss = 0.14912616
Iteration 22, loss = 0.13654685
Iteration 23, loss = 0.12523372
Iteration 24, loss = 0.11523212
Iteration 25, loss = 0.10640665
Iteration 26, loss = 0.09863761
Iteration 27, loss = 0.09162779
Iteration 28, loss = 0.08543729
Iteration 29, loss = 0.07994838
Iteration 30, loss = 0.07491477
Iteration 31, loss = 0.07043807
Iteration 32, loss = 0.06646013
Iteration 33, loss = 0.06278715
Iteration 34, loss = 0.05957875
Iteration 35, loss = 0.05663133
Iteration 36, loss = 0.05382426
Iteration 37, loss = 0.05143339
Iteration 38, loss = 0.04907615
Iteration 39, loss = 0.04709041
Iteration 40, loss = 0.04510613
Iteration 41, loss = 0.04335318
Iteration 42, loss = 0.04175092
Iteration 43, loss = 0.04027051
Iteration 44, loss = 0.03886454
Iteration 45, loss = 0.03751035
Iteration 46, loss = 0.03631129
Iteration 47, loss = 0.03521882
Iteration 48, loss = 0.03417746
Iteration 49, loss = 0.03322676
Iteration 50, loss = 0.03226873
Iteration 51, loss = 0.03139771
Iteration 52, loss = 0.03063777
Iteration 53, loss = 0.02990979
Iteration 54, loss = 0.02913172
Iteration 55, loss = 0.02851774
Iteration 56, loss = 0.02793187
Iteration 57, loss = 0.02732861
Iteration 58, loss = 0.02672125
Iteration 59, loss = 0.02624138
Iteration 60, loss = 0.02561633
Iteration 61, loss = 0.02518359
Iteration 62, loss = 0.02470697
Iteration 63, loss = 0.02430774
Iteration 64, loss = 0.02388555
Iteration 65, loss = 0.02347880
Iteration 66, loss = 0.02308759
Iteration 67, loss = 0.02272972
Iteration 68, loss = 0.02244948
Iteration 69, loss = 0.02207493
Iteration 70, loss = 0.02173617
Iteration 71, loss = 0.02146096
Iteration 72, loss = 0.02126196
Iteration 73, loss = 0.02097219
Iteration 74, loss = 0.02067729
Iteration 75, loss = 0.02044119
Iteration 76, loss = 0.02013067
Iteration 77, loss = 0.01994303
Iteration 78, loss = 0.01967589
Iteration 79, loss = 0.01949639
Iteration 80, loss = 0.01933080
Iteration 81, loss = 0.01914667
Iteration 82, loss = 0.01893968
Iteration 83, loss = 0.01880149
Iteration 84, loss = 0.01858328
Iteration 85, loss = 0.01838894
Iteration 86, loss = 0.01826966
Iteration 87, loss = 0.01808835
Iteration 88, loss = 0.01791201
Iteration 89, loss = 0.01778498
Iteration 90, loss = 0.01766371
Iteration 91, loss = 0.01753404
Iteration 92, loss = 0.01732599
Iteration 93, loss = 0.01727366
Iteration 94, loss = 0.01715124
Iteration 95, loss = 0.01699808
Iteration 96, loss = 0.01689627
Iteration 97, loss = 0.01676373
Iteration 98, loss = 0.01674004
Iteration 99, loss = 0.01661123
Iteration 100, loss = 0.01653400
Iteration 101, loss = 0.01640441
Iteration 102, loss = 0.01625289
Iteration 103, loss = 0.01620706
Iteration 104, loss = 0.01610081
Iteration 105, loss = 0.01605386
Iteration 106, loss = 0.01595862
Iteration 107, loss = 0.01585306
Iteration 108, loss = 0.01582409
Iteration 109, loss = 0.01572397
Iteration 110, loss = 0.01561721
Iteration 111, loss = 0.01551320
Iteration 112, loss = 0.01543053
Iteration 113, loss = 0.01546604
Iteration 114, loss = 0.01533510
Iteration 115, loss = 0.01518238
Iteration 116, loss = 0.01523886
Iteration 117, loss = 0.01517222
Iteration 118, loss = 0.01503828
Iteration 119, loss = 0.01496656
Iteration 120, loss = 0.01497776
Iteration 121, loss = 0.01496028
Iteration 122, loss = 0.01485641
Iteration 123, loss = 0.01480313
Iteration 124, loss = 0.01474864
Iteration 125, loss = 0.01466782
Iteration 126, loss = 0.01467651
Iteration 127, loss = 0.01456705
Iteration 128, loss = 0.01454806
Iteration 129, loss = 0.01453433
Iteration 130, loss = 0.01450981
Iteration 131, loss = 0.01449527
Iteration 132, loss = 0.01436163
Iteration 133, loss = 0.01434006
Iteration 134, loss = 0.01433905
Iteration 135, loss = 0.01428269
Iteration 136, loss = 0.01430367
Iteration 137, loss = 0.01417760
Iteration 138, loss = 0.01415777
Iteration 139, loss = 0.01410593
Iteration 140, loss = 0.01404450
Iteration 141, loss = 0.01402089
Iteration 142, loss = 0.01399809
Iteration 143, loss = 0.01402043
Iteration 144, loss = 0.01398475
Iteration 145, loss = 0.01389423
Iteration 146, loss = 0.01384890
Iteration 147, loss = 0.01387069
Iteration 148, loss = 0.01387568
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67620420
Iteration 2, loss = 0.63016406
Iteration 3, loss = 0.56270834
Iteration 4, loss = 0.48100385
Iteration 5, loss = 0.39839769
Iteration 6, loss = 0.32282010
Iteration 7, loss = 0.25851738
Iteration 8, loss = 0.20708756
Iteration 9, loss = 0.16682911
Iteration 10, loss = 0.13666634
Iteration 11, loss = 0.11366598
Iteration 12, loss = 0.09607355
Iteration 13, loss = 0.08242863
Iteration 14, loss = 0.07180561
Iteration 15, loss = 0.06348673
Iteration 16, loss = 0.05664982
Iteration 17, loss = 0.05124848
Iteration 18, loss = 0.04659757
Iteration 19, loss = 0.04302238
Iteration 20, loss = 0.03984357
Iteration 21, loss = 0.03699904
Iteration 22, loss = 0.03488692
Iteration 23, loss = 0.03287583
Iteration 24, loss = 0.03116495
Iteration 25, loss = 0.02976112
Iteration 26, loss = 0.02822664
Iteration 27, loss = 0.02697154
Iteration 28, loss = 0.02600945
Iteration 29, loss = 0.02524021
Iteration 30, loss = 0.02423382
Iteration 31, loss = 0.02346785
Iteration 32, loss = 0.02303918
Iteration 33, loss = 0.02238215
Iteration 34, loss = 0.02148029
Iteration 35, loss = 0.02119258
Iteration 36, loss = 0.02070618
Iteration 37, loss = 0.02035315
Iteration 38, loss = 0.01987008
Iteration 39, loss = 0.01955429
Iteration 40, loss = 0.01927617
Iteration 41, loss = 0.01902091
Iteration 42, loss = 0.01869624
Iteration 43, loss = 0.01837482
Iteration 44, loss = 0.01813888
Iteration 45, loss = 0.01787848
Iteration 46, loss = 0.01801918
Iteration 47, loss = 0.01777768
Iteration 48, loss = 0.01750057
Iteration 49, loss = 0.01717884
Iteration 50, loss = 0.01706856
Iteration 51, loss = 0.01688781
Iteration 52, loss = 0.01665475
Iteration 53, loss = 0.01659671
Iteration 54, loss = 0.01653758
Iteration 55, loss = 0.01613202
Iteration 56, loss = 0.01633677
Iteration 57, loss = 0.01605438
Iteration 58, loss = 0.01630813
Iteration 59, loss = 0.01616482
Iteration 60, loss = 0.01602555
Iteration 61, loss = 0.01585844
Iteration 62, loss = 0.01564305
Iteration 63, loss = 0.01573353
Iteration 64, loss = 0.01560850
Iteration 65, loss = 0.01543283
Iteration 66, loss = 0.01550117
Iteration 67, loss = 0.01532520
Iteration 68, loss = 0.01520724
Iteration 69, loss = 0.01529974
Iteration 70, loss = 0.01541694
Iteration 71, loss = 0.01518899
Iteration 72, loss = 0.01511335
Iteration 73, loss = 0.01521097
Iteration 74, loss = 0.01511834
Iteration 75, loss = 0.01490511
Iteration 76, loss = 0.01491523
Iteration 77, loss = 0.01484420
Iteration 78, loss = 0.01485327
Iteration 79, loss = 0.01471857
Iteration 80, loss = 0.01490784
Iteration 81, loss = 0.01483384
Iteration 82, loss = 0.01489526
Iteration 83, loss = 0.01478640
Iteration 84, loss = 0.01466466
Iteration 85, loss = 0.01466532
Iteration 86, loss = 0.01460856
Iteration 87, loss = 0.01447054
Iteration 88, loss = 0.01436708
Iteration 89, loss = 0.01434262
Iteration 90, loss = 0.01457821
Iteration 91, loss = 0.01448269
Iteration 92, loss = 0.01426845
Iteration 93, loss = 0.01465892
Iteration 94, loss = 0.01464567
Iteration 95, loss = 0.01444927
Iteration 96, loss = 0.01424682
Iteration 97, loss = 0.01434001
Iteration 98, loss = 0.01412876
Iteration 99, loss = 0.01429595
Iteration 100, loss = 0.01427245
Iteration 101, loss = 0.01448853
Iteration 102, loss = 0.01438065
Iteration 103, loss = 0.01430353
Iteration 104, loss = 0.01425378
Iteration 105, loss = 0.01402157
Iteration 106, loss = 0.01428794
Iteration 107, loss = 0.01418078
Iteration 108, loss = 0.01411445
Iteration 109, loss = 0.01410280
Iteration 110, loss = 0.01408226
Iteration 111, loss = 0.01427350
Iteration 112, loss = 0.01406197
Iteration 113, loss = 0.01403653
Iteration 114, loss = 0.01393027
Iteration 115, loss = 0.01412093
Iteration 116, loss = 0.01399106
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68114722
Iteration 2, loss = 0.65525849
Iteration 3, loss = 0.62516163
Iteration 4, loss = 0.58514402
Iteration 5, loss = 0.53577823
Iteration 6, loss = 0.48230025
Iteration 7, loss = 0.42923752
Iteration 8, loss = 0.37909407
Iteration 9, loss = 0.33251898
Iteration 10, loss = 0.29066657
Iteration 11, loss = 0.25384693
Iteration 12, loss = 0.22170123
Iteration 13, loss = 0.19391489
Iteration 14, loss = 0.17025188
Iteration 15, loss = 0.15024389
Iteration 16, loss = 0.13327692
Iteration 17, loss = 0.11883092
Iteration 18, loss = 0.10656736
Iteration 19, loss = 0.09613664
Iteration 20, loss = 0.08709300
Iteration 21, loss = 0.07953405
Iteration 22, loss = 0.07285963
Iteration 23, loss = 0.06710185
Iteration 24, loss = 0.06213358
Iteration 25, loss = 0.05754458
Iteration 26, loss = 0.05371076
Iteration 27, loss = 0.05022755
Iteration 28, loss = 0.04710460
Iteration 29, loss = 0.04448907
Iteration 30, loss = 0.04201505
Iteration 31, loss = 0.03980968
Iteration 32, loss = 0.03778810
Iteration 33, loss = 0.03595217
Iteration 34, loss = 0.03432619
Iteration 35, loss = 0.03282805
Iteration 36, loss = 0.03151253
Iteration 37, loss = 0.03011867
Iteration 38, loss = 0.02901174
Iteration 39, loss = 0.02799904
Iteration 40, loss = 0.02698681
Iteration 41, loss = 0.02612747
Iteration 42, loss = 0.02526452
Iteration 43, loss = 0.02442966
Iteration 44, loss = 0.02367243
Iteration 45, loss = 0.02309125
Iteration 46, loss = 0.02231800
Iteration 47, loss = 0.02186550
Iteration 48, loss = 0.02121605
Iteration 49, loss = 0.02103168
Iteration 50, loss = 0.02044148
Iteration 51, loss = 0.01999684
Iteration 52, loss = 0.01947672
Iteration 53, loss = 0.01902870
Iteration 54, loss = 0.01855231
Iteration 55, loss = 0.01813487
Iteration 56, loss = 0.01787468
Iteration 57, loss = 0.01742040
Iteration 58, loss = 0.01708915
Iteration 59, loss = 0.01681644
Iteration 60, loss = 0.01646985
Iteration 61, loss = 0.01621795
Iteration 62, loss = 0.01599651
Iteration 63, loss = 0.01583510
Iteration 64, loss = 0.01560578
Iteration 65, loss = 0.01538260
Iteration 66, loss = 0.01522706
Iteration 67, loss = 0.01511123
Iteration 68, loss = 0.01487331
Iteration 69, loss = 0.01475769
Iteration 70, loss = 0.01450418
Iteration 71, loss = 0.01437424
Iteration 72, loss = 0.01443267
Iteration 73, loss = 0.01415906
Iteration 74, loss = 0.01390237
Iteration 75, loss = 0.01377542
Iteration 76, loss = 0.01364980
Iteration 77, loss = 0.01366196
Iteration 78, loss = 0.01340166
Iteration 79, loss = 0.01329853
Iteration 80, loss = 0.01317259
Iteration 81, loss = 0.01301387
Iteration 82, loss = 0.01285365
Iteration 83, loss = 0.01263161
Iteration 84, loss = 0.01248845
Iteration 85, loss = 0.01239254
Iteration 86, loss = 0.01228921
Iteration 87, loss = 0.01226851
Iteration 88, loss = 0.01226550
Iteration 89, loss = 0.01208162
Iteration 90, loss = 0.01197843
Iteration 91, loss = 0.01185880
Iteration 92, loss = 0.01181078
Iteration 93, loss = 0.01184068
Iteration 94, loss = 0.01177523
Iteration 95, loss = 0.01169041
Iteration 96, loss = 0.01210457
Iteration 97, loss = 0.01226217
Iteration 98, loss = 0.01181227
Iteration 99, loss = 0.01164195
Iteration 100, loss = 0.01155457
Iteration 101, loss = 0.01151409
Iteration 102, loss = 0.01149482
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68255954
Iteration 2, loss = 0.63387844
Iteration 3, loss = 0.55674515
Iteration 4, loss = 0.46253278
Iteration 5, loss = 0.37290036
Iteration 6, loss = 0.29468116
Iteration 7, loss = 0.23161389
Iteration 8, loss = 0.18315038
Iteration 9, loss = 0.14718662
Iteration 10, loss = 0.12048473
Iteration 11, loss = 0.10065806
Iteration 12, loss = 0.08553580
Iteration 13, loss = 0.07406179
Iteration 14, loss = 0.06498299
Iteration 15, loss = 0.05791351
Iteration 16, loss = 0.05217888
Iteration 17, loss = 0.04762540
Iteration 18, loss = 0.04377595
Iteration 19, loss = 0.04048350
Iteration 20, loss = 0.03762334
Iteration 21, loss = 0.03552080
Iteration 22, loss = 0.03348036
Iteration 23, loss = 0.03173770
Iteration 24, loss = 0.03012832
Iteration 25, loss = 0.02889248
Iteration 26, loss = 0.02773335
Iteration 27, loss = 0.02677789
Iteration 28, loss = 0.02581171
Iteration 29, loss = 0.02501988
Iteration 30, loss = 0.02425300
Iteration 31, loss = 0.02341067
Iteration 32, loss = 0.02296289
Iteration 33, loss = 0.02246353
Iteration 34, loss = 0.02196920
Iteration 35, loss = 0.02157201
Iteration 36, loss = 0.02111249
Iteration 37, loss = 0.02073633
Iteration 38, loss = 0.02033262
Iteration 39, loss = 0.02016423
Iteration 40, loss = 0.01977078
Iteration 41, loss = 0.01945941
Iteration 42, loss = 0.01922786
Iteration 43, loss = 0.01897785
Iteration 44, loss = 0.01869651
Iteration 45, loss = 0.01857657
Iteration 46, loss = 0.01845400
Iteration 47, loss = 0.01824594
Iteration 48, loss = 0.01818817
Iteration 49, loss = 0.01793666
Iteration 50, loss = 0.01791007
Iteration 51, loss = 0.01770259
Iteration 52, loss = 0.01747835
Iteration 53, loss = 0.01738457
Iteration 54, loss = 0.01732980
Iteration 55, loss = 0.01725710
Iteration 56, loss = 0.01719548
Iteration 57, loss = 0.01712716
Iteration 58, loss = 0.01688103
Iteration 59, loss = 0.01672853
Iteration 60, loss = 0.01675539
Iteration 61, loss = 0.01665906
Iteration 62, loss = 0.01667483
Iteration 63, loss = 0.01639551
Iteration 64, loss = 0.01668603
Iteration 65, loss = 0.01637521
Iteration 66, loss = 0.01634561
Iteration 67, loss = 0.01616570
Iteration 68, loss = 0.01613722
Iteration 69, loss = 0.01616641
Iteration 70, loss = 0.01606152
Iteration 71, loss = 0.01605163
Iteration 72, loss = 0.01580185
Iteration 73, loss = 0.01607737
Iteration 74, loss = 0.01612512
Iteration 75, loss = 0.01583995
Iteration 76, loss = 0.01584719
Iteration 77, loss = 0.01575609
Iteration 78, loss = 0.01573648
Iteration 79, loss = 0.01589109
Iteration 80, loss = 0.01579190
Iteration 81, loss = 0.01561026
Iteration 82, loss = 0.01552069
Iteration 83, loss = 0.01568088
Iteration 84, loss = 0.01572011
Iteration 85, loss = 0.01560932
Iteration 86, loss = 0.01548054
Iteration 87, loss = 0.01538034
Iteration 88, loss = 0.01540177
Iteration 89, loss = 0.01544302
Iteration 90, loss = 0.01529790
Iteration 91, loss = 0.01551259
Iteration 92, loss = 0.01551761
Iteration 93, loss = 0.01522441
Iteration 94, loss = 0.01544296
Iteration 95, loss = 0.01518450
Iteration 96, loss = 0.01529938
Iteration 97, loss = 0.01514441
Iteration 98, loss = 0.01513654
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68614345
Iteration 2, loss = 0.66682355
Iteration 3, loss = 0.64783448
Iteration 4, loss = 0.62513312
Iteration 5, loss = 0.59745999
Iteration 6, loss = 0.56479962
Iteration 7, loss = 0.52871666
Iteration 8, loss = 0.49068035
Iteration 9, loss = 0.45253914
Iteration 10, loss = 0.41512229
Iteration 11, loss = 0.37928385
Iteration 12, loss = 0.34535380
Iteration 13, loss = 0.31356250
Iteration 14, loss = 0.28428388
Iteration 15, loss = 0.25748838
Iteration 16, loss = 0.23290282
Iteration 17, loss = 0.21092444
Iteration 18, loss = 0.19117671
Iteration 19, loss = 0.17351494
Iteration 20, loss = 0.15777038
Iteration 21, loss = 0.14396835
Iteration 22, loss = 0.13161415
Iteration 23, loss = 0.12077784
Iteration 24, loss = 0.11105696
Iteration 25, loss = 0.10238846
Iteration 26, loss = 0.09481106
Iteration 27, loss = 0.08803919
Iteration 28, loss = 0.08200136
Iteration 29, loss = 0.07657343
Iteration 30, loss = 0.07173782
Iteration 31, loss = 0.06745765
Iteration 32, loss = 0.06353621
Iteration 33, loss = 0.05992128
Iteration 34, loss = 0.05668201
Iteration 35, loss = 0.05378058
Iteration 36, loss = 0.05112333
Iteration 37, loss = 0.04872345
Iteration 38, loss = 0.04647871
Iteration 39, loss = 0.04443631
Iteration 40, loss = 0.04255319
Iteration 41, loss = 0.04088192
Iteration 42, loss = 0.03926525
Iteration 43, loss = 0.03774111
Iteration 44, loss = 0.03643611
Iteration 45, loss = 0.03515547
Iteration 46, loss = 0.03403794
Iteration 47, loss = 0.03289661
Iteration 48, loss = 0.03182393
Iteration 49, loss = 0.03083606
Iteration 50, loss = 0.02995631
Iteration 51, loss = 0.02917912
Iteration 52, loss = 0.02836528
Iteration 53, loss = 0.02761063
Iteration 54, loss = 0.02688418
Iteration 55, loss = 0.02627034
Iteration 56, loss = 0.02562337
Iteration 57, loss = 0.02504307
Iteration 58, loss = 0.02446928
Iteration 59, loss = 0.02394249
Iteration 60, loss = 0.02342978
Iteration 61, loss = 0.02298555
Iteration 62, loss = 0.02254650
Iteration 63, loss = 0.02207361
Iteration 64, loss = 0.02174290
Iteration 65, loss = 0.02132220
Iteration 66, loss = 0.02093815
Iteration 67, loss = 0.02063500
Iteration 68, loss = 0.02028674
Iteration 69, loss = 0.01993548
Iteration 70, loss = 0.01961655
Iteration 71, loss = 0.01934027
Iteration 72, loss = 0.01901533
Iteration 73, loss = 0.01874597
Iteration 74, loss = 0.01855769
Iteration 75, loss = 0.01827546
Iteration 76, loss = 0.01805654
Iteration 77, loss = 0.01775455
Iteration 78, loss = 0.01757518
Iteration 79, loss = 0.01738347
Iteration 80, loss = 0.01721962
Iteration 81, loss = 0.01694643
Iteration 82, loss = 0.01679592
Iteration 83, loss = 0.01657616
Iteration 84, loss = 0.01638112
Iteration 85, loss = 0.01623233
Iteration 86, loss = 0.01610850
Iteration 87, loss = 0.01589948
Iteration 88, loss = 0.01575934
Iteration 89, loss = 0.01564236
Iteration 90, loss = 0.01547183
Iteration 91, loss = 0.01530649
Iteration 92, loss = 0.01525464
Iteration 93, loss = 0.01509775
Iteration 94, loss = 0.01495904
Iteration 95, loss = 0.01476813
Iteration 96, loss = 0.01474037
Iteration 97, loss = 0.01462241
Iteration 98, loss = 0.01448949
Iteration 99, loss = 0.01433199
Iteration 100, loss = 0.01431628
Iteration 101, loss = 0.01413476
Iteration 102, loss = 0.01408428
Iteration 103, loss = 0.01397038
Iteration 104, loss = 0.01386700
Iteration 105, loss = 0.01380481
Iteration 106, loss = 0.01370302
Iteration 107, loss = 0.01359029
Iteration 108, loss = 0.01354672
Iteration 109, loss = 0.01342872
Iteration 110, loss = 0.01338215
Iteration 111, loss = 0.01330278
Iteration 112, loss = 0.01326016
Iteration 113, loss = 0.01315573
Iteration 114, loss = 0.01306870
Iteration 115, loss = 0.01305178
Iteration 116, loss = 0.01294625
Iteration 117, loss = 0.01289475
Iteration 118, loss = 0.01280496
Iteration 119, loss = 0.01275864
Iteration 120, loss = 0.01269384
Iteration 121, loss = 0.01260226
Iteration 122, loss = 0.01258274
Iteration 123, loss = 0.01249392
Iteration 124, loss = 0.01247444
Iteration 125, loss = 0.01244269
Iteration 126, loss = 0.01237449
Iteration 127, loss = 0.01232304
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68030970
Iteration 2, loss = 0.64463747
Iteration 3, loss = 0.60106006
Iteration 4, loss = 0.54714066
Iteration 5, loss = 0.48618229
Iteration 6, loss = 0.42391134
Iteration 7, loss = 0.36519973
Iteration 8, loss = 0.31154165
Iteration 9, loss = 0.26443792
Iteration 10, loss = 0.22406257
Iteration 11, loss = 0.19053939
Iteration 12, loss = 0.16266493
Iteration 13, loss = 0.14008268
Iteration 14, loss = 0.12144286
Iteration 15, loss = 0.10630397
Iteration 16, loss = 0.09365499
Iteration 17, loss = 0.08334659
Iteration 18, loss = 0.07480479
Iteration 19, loss = 0.06756453
Iteration 20, loss = 0.06143592
Iteration 21, loss = 0.05629987
Iteration 22, loss = 0.05182752
Iteration 23, loss = 0.04796208
Iteration 24, loss = 0.04463699
Iteration 25, loss = 0.04176927
Iteration 26, loss = 0.03910135
Iteration 27, loss = 0.03688492
Iteration 28, loss = 0.03483052
Iteration 29, loss = 0.03309962
Iteration 30, loss = 0.03156374
Iteration 31, loss = 0.03013591
Iteration 32, loss = 0.02881720
Iteration 33, loss = 0.02768762
Iteration 34, loss = 0.02658042
Iteration 35, loss = 0.02554654
Iteration 36, loss = 0.02476653
Iteration 37, loss = 0.02389051
Iteration 38, loss = 0.02308017
Iteration 39, loss = 0.02249323
Iteration 40, loss = 0.02190687
Iteration 41, loss = 0.02129722
Iteration 42, loss = 0.02081007
Iteration 43, loss = 0.02031736
Iteration 44, loss = 0.01976755
Iteration 45, loss = 0.01935192
Iteration 46, loss = 0.01901743
Iteration 47, loss = 0.01857800
Iteration 48, loss = 0.01816197
Iteration 49, loss = 0.01801548
Iteration 50, loss = 0.01756339
Iteration 51, loss = 0.01730479
Iteration 52, loss = 0.01705170
Iteration 53, loss = 0.01677571
Iteration 54, loss = 0.01647497
Iteration 55, loss = 0.01636137
Iteration 56, loss = 0.01603602
Iteration 57, loss = 0.01595269
Iteration 58, loss = 0.01572264
Iteration 59, loss = 0.01555059
Iteration 60, loss = 0.01531177
Iteration 61, loss = 0.01522378
Iteration 62, loss = 0.01498125
Iteration 63, loss = 0.01490926
Iteration 64, loss = 0.01475412
Iteration 65, loss = 0.01459667
Iteration 66, loss = 0.01461938
Iteration 67, loss = 0.01435270
Iteration 68, loss = 0.01421455
Iteration 69, loss = 0.01413346
Iteration 70, loss = 0.01398817
Iteration 71, loss = 0.01398042
Iteration 72, loss = 0.01370588
Iteration 73, loss = 0.01378785
Iteration 74, loss = 0.01362190
Iteration 75, loss = 0.01353799
Iteration 76, loss = 0.01351234
Iteration 77, loss = 0.01342895
Iteration 78, loss = 0.01332952
Iteration 79, loss = 0.01331792
Iteration 80, loss = 0.01315303
Iteration 81, loss = 0.01314395
Iteration 82, loss = 0.01309155
Iteration 83, loss = 0.01301910
Iteration 84, loss = 0.01286808
Iteration 85, loss = 0.01286578
Iteration 86, loss = 0.01274924
Iteration 87, loss = 0.01276314
Iteration 88, loss = 0.01271689
Iteration 89, loss = 0.01273849
Iteration 90, loss = 0.01259264
Iteration 91, loss = 0.01245051
Iteration 92, loss = 0.01265829
Iteration 93, loss = 0.01245984
Iteration 94, loss = 0.01240261
Iteration 95, loss = 0.01233199
Iteration 96, loss = 0.01229642
Iteration 97, loss = 0.01247566
Iteration 98, loss = 0.01222928
Iteration 99, loss = 0.01221033
Iteration 100, loss = 0.01222808
Iteration 101, loss = 0.01212191
Iteration 102, loss = 0.01223742
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70423170
Iteration 2, loss = 0.65210491
Iteration 3, loss = 0.58197909
Iteration 4, loss = 0.49666362
Iteration 5, loss = 0.40890983
Iteration 6, loss = 0.32928974
Iteration 7, loss = 0.26241435
Iteration 8, loss = 0.20933154
Iteration 9, loss = 0.16852725
Iteration 10, loss = 0.13771804
Iteration 11, loss = 0.11415280
Iteration 12, loss = 0.09654087
Iteration 13, loss = 0.08271068
Iteration 14, loss = 0.07209020
Iteration 15, loss = 0.06366041
Iteration 16, loss = 0.05660204
Iteration 17, loss = 0.05118743
Iteration 18, loss = 0.04653607
Iteration 19, loss = 0.04279309
Iteration 20, loss = 0.03932615
Iteration 21, loss = 0.03666695
Iteration 22, loss = 0.03446626
Iteration 23, loss = 0.03243377
Iteration 24, loss = 0.03069243
Iteration 25, loss = 0.02902408
Iteration 26, loss = 0.02762078
Iteration 27, loss = 0.02650297
Iteration 28, loss = 0.02536757
Iteration 29, loss = 0.02452673
Iteration 30, loss = 0.02375099
Iteration 31, loss = 0.02297750
Iteration 32, loss = 0.02212522
Iteration 33, loss = 0.02162349
Iteration 34, loss = 0.02098282
Iteration 35, loss = 0.02036971
Iteration 36, loss = 0.01998919
Iteration 37, loss = 0.01956448
Iteration 38, loss = 0.01925373
Iteration 39, loss = 0.01886973
Iteration 40, loss = 0.01849283
Iteration 41, loss = 0.01851697
Iteration 42, loss = 0.01818885
Iteration 43, loss = 0.01797321
Iteration 44, loss = 0.01760844
Iteration 45, loss = 0.01714493
Iteration 46, loss = 0.01685170
Iteration 47, loss = 0.01683856
Iteration 48, loss = 0.01641063
Iteration 49, loss = 0.01638698
Iteration 50, loss = 0.01629496
Iteration 51, loss = 0.01621245
Iteration 52, loss = 0.01592525
Iteration 53, loss = 0.01574559
Iteration 54, loss = 0.01575526
Iteration 55, loss = 0.01541240
Iteration 56, loss = 0.01524180
Iteration 57, loss = 0.01537036
Iteration 58, loss = 0.01521296
Iteration 59, loss = 0.01527061
Iteration 60, loss = 0.01494880
Iteration 61, loss = 0.01487038
Iteration 62, loss = 0.01483998
Iteration 63, loss = 0.01469753
Iteration 64, loss = 0.01464782
Iteration 65, loss = 0.01468990
Iteration 66, loss = 0.01475594
Iteration 67, loss = 0.01471624
Iteration 68, loss = 0.01453975
Iteration 69, loss = 0.01457716
Iteration 70, loss = 0.01448802
Iteration 71, loss = 0.01426979
Iteration 72, loss = 0.01420130
Iteration 73, loss = 0.01421011
Iteration 74, loss = 0.01403271
Iteration 75, loss = 0.01424958
Iteration 76, loss = 0.01437773
Iteration 77, loss = 0.01407668
Iteration 78, loss = 0.01394076
Iteration 79, loss = 0.01401639
Iteration 80, loss = 0.01394501
Iteration 81, loss = 0.01391075
Iteration 82, loss = 0.01394271
Iteration 83, loss = 0.01361637
Iteration 84, loss = 0.01382369
Iteration 85, loss = 0.01379991
Iteration 86, loss = 0.01365328
Iteration 87, loss = 0.01360052
Iteration 88, loss = 0.01341172
Iteration 89, loss = 0.01363843
Iteration 90, loss = 0.01361634
Iteration 91, loss = 0.01361753
Iteration 92, loss = 0.01350649
Iteration 93, loss = 0.01345533
Iteration 94, loss = 0.01356523
Iteration 95, loss = 0.01358968
Iteration 96, loss = 0.01332797
Iteration 97, loss = 0.01354707
Iteration 98, loss = 0.01359233
Iteration 99, loss = 0.01332778
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71418389
Iteration 2, loss = 0.65964129
Iteration 3, loss = 0.57903070
Iteration 4, loss = 0.47518292
Iteration 5, loss = 0.37305719
Iteration 6, loss = 0.28619812
Iteration 7, loss = 0.21947948
Iteration 8, loss = 0.17040303
Iteration 9, loss = 0.13561082
Iteration 10, loss = 0.11067163
Iteration 11, loss = 0.09232982
Iteration 12, loss = 0.07886609
Iteration 13, loss = 0.06851568
Iteration 14, loss = 0.06049307
Iteration 15, loss = 0.05426508
Iteration 16, loss = 0.04911831
Iteration 17, loss = 0.04487383
Iteration 18, loss = 0.04169497
Iteration 19, loss = 0.03865281
Iteration 20, loss = 0.03606340
Iteration 21, loss = 0.03399552
Iteration 22, loss = 0.03223053
Iteration 23, loss = 0.03065863
Iteration 24, loss = 0.02943453
Iteration 25, loss = 0.02819618
Iteration 26, loss = 0.02700173
Iteration 27, loss = 0.02618739
Iteration 28, loss = 0.02543245
Iteration 29, loss = 0.02473997
Iteration 30, loss = 0.02385969
Iteration 31, loss = 0.02307233
Iteration 32, loss = 0.02269597
Iteration 33, loss = 0.02216252
Iteration 34, loss = 0.02160471
Iteration 35, loss = 0.02129947
Iteration 36, loss = 0.02092354
Iteration 37, loss = 0.02055355
Iteration 38, loss = 0.02040076
Iteration 39, loss = 0.01978797
Iteration 40, loss = 0.01962985
Iteration 41, loss = 0.01929728
Iteration 42, loss = 0.01917210
Iteration 43, loss = 0.01903996
Iteration 44, loss = 0.01864802
Iteration 45, loss = 0.01852686
Iteration 46, loss = 0.01834383
Iteration 47, loss = 0.01795019
Iteration 48, loss = 0.01821206
Iteration 49, loss = 0.01771147
Iteration 50, loss = 0.01786452
Iteration 51, loss = 0.01755557
Iteration 52, loss = 0.01746781
Iteration 53, loss = 0.01739918
Iteration 54, loss = 0.01728603
Iteration 55, loss = 0.01706645
Iteration 56, loss = 0.01695941
Iteration 57, loss = 0.01716786
Iteration 58, loss = 0.01689395
Iteration 59, loss = 0.01679574
Iteration 60, loss = 0.01656808
Iteration 61, loss = 0.01673145
Iteration 62, loss = 0.01647469
Iteration 63, loss = 0.01639574
Iteration 64, loss = 0.01638665
Iteration 65, loss = 0.01651034
Iteration 66, loss = 0.01638301
Iteration 67, loss = 0.01626766
Iteration 68, loss = 0.01629945
Iteration 69, loss = 0.01622771
Iteration 70, loss = 0.01613088
Iteration 71, loss = 0.01608884
Iteration 72, loss = 0.01597337
Iteration 73, loss = 0.01582086
Iteration 74, loss = 0.01581864
Iteration 75, loss = 0.01584310
Iteration 76, loss = 0.01581462
Iteration 77, loss = 0.01570965
Iteration 78, loss = 0.01581314
Iteration 79, loss = 0.01564258
Iteration 80, loss = 0.01557849
Iteration 81, loss = 0.01578074
Iteration 82, loss = 0.01565415
Iteration 83, loss = 0.01565275
Iteration 84, loss = 0.01542467
Iteration 85, loss = 0.01573679
Iteration 86, loss = 0.01556656
Iteration 87, loss = 0.01554670
Iteration 88, loss = 0.01544209
Iteration 89, loss = 0.01579031
Iteration 90, loss = 0.01559415
Iteration 91, loss = 0.01507493
Iteration 92, loss = 0.01544647
Iteration 93, loss = 0.01537693
Iteration 94, loss = 0.01524948
Iteration 95, loss = 0.01523543
Iteration 96, loss = 0.01520349
Iteration 97, loss = 0.01536831
Iteration 98, loss = 0.01532197
Iteration 99, loss = 0.01513042
Iteration 100, loss = 0.01515014
Iteration 101, loss = 0.01522414
Iteration 102, loss = 0.01517177
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67556824
Iteration 2, loss = 0.61210405
Iteration 3, loss = 0.51931214
Iteration 4, loss = 0.41919003
Iteration 5, loss = 0.32710353
Iteration 6, loss = 0.25146535
Iteration 7, loss = 0.19382416
Iteration 8, loss = 0.15194587
Iteration 9, loss = 0.12176866
Iteration 10, loss = 0.09979063
Iteration 11, loss = 0.08364148
Iteration 12, loss = 0.07160567
Iteration 13, loss = 0.06236520
Iteration 14, loss = 0.05491815
Iteration 15, loss = 0.04911299
Iteration 16, loss = 0.04460772
Iteration 17, loss = 0.04058372
Iteration 18, loss = 0.03745349
Iteration 19, loss = 0.03473459
Iteration 20, loss = 0.03263801
Iteration 21, loss = 0.03057457
Iteration 22, loss = 0.02902271
Iteration 23, loss = 0.02744929
Iteration 24, loss = 0.02616710
Iteration 25, loss = 0.02506707
Iteration 26, loss = 0.02394529
Iteration 27, loss = 0.02320639
Iteration 28, loss = 0.02230667
Iteration 29, loss = 0.02157613
Iteration 30, loss = 0.02097544
Iteration 31, loss = 0.02048570
Iteration 32, loss = 0.02003320
Iteration 33, loss = 0.01960444
Iteration 34, loss = 0.01894715
Iteration 35, loss = 0.01861239
Iteration 36, loss = 0.01819599
Iteration 37, loss = 0.01789298
Iteration 38, loss = 0.01776722
Iteration 39, loss = 0.01745187
Iteration 40, loss = 0.01719249
Iteration 41, loss = 0.01692552
Iteration 42, loss = 0.01671273
Iteration 43, loss = 0.01651503
Iteration 44, loss = 0.01624140
Iteration 45, loss = 0.01618489
Iteration 46, loss = 0.01595379
Iteration 47, loss = 0.01568709
Iteration 48, loss = 0.01551647
Iteration 49, loss = 0.01555558
Iteration 50, loss = 0.01542611
Iteration 51, loss = 0.01527341
Iteration 52, loss = 0.01487078
Iteration 53, loss = 0.01497849
Iteration 54, loss = 0.01471817
Iteration 55, loss = 0.01485810
Iteration 56, loss = 0.01473065
Iteration 57, loss = 0.01445554
Iteration 58, loss = 0.01459647
Iteration 59, loss = 0.01448007
Iteration 60, loss = 0.01444503
Iteration 61, loss = 0.01421214
Iteration 62, loss = 0.01398553
Iteration 63, loss = 0.01414922
Iteration 64, loss = 0.01405131
Iteration 65, loss = 0.01402880
Iteration 66, loss = 0.01403432
Iteration 67, loss = 0.01396464
Iteration 68, loss = 0.01373777
Iteration 69, loss = 0.01365779
Iteration 70, loss = 0.01358924
Iteration 71, loss = 0.01367403
Iteration 72, loss = 0.01347090
Iteration 73, loss = 0.01358806
Iteration 74, loss = 0.01363630
Iteration 75, loss = 0.01352257
Iteration 76, loss = 0.01359287
Iteration 77, loss = 0.01332543
Iteration 78, loss = 0.01346765
Iteration 79, loss = 0.01334936
Iteration 80, loss = 0.01335265
Iteration 81, loss = 0.01331368
Iteration 82, loss = 0.01322578
Iteration 83, loss = 0.01306283
Iteration 84, loss = 0.01315071
Iteration 85, loss = 0.01312259
Iteration 86, loss = 0.01313911
Iteration 87, loss = 0.01310137
Iteration 88, loss = 0.01302168
Iteration 89, loss = 0.01309887
Iteration 90, loss = 0.01290323
Iteration 91, loss = 0.01287518
Iteration 92, loss = 0.01277206
Iteration 93, loss = 0.01297601
Iteration 94, loss = 0.01294832
Iteration 95, loss = 0.01305889
Iteration 96, loss = 0.01295157
Iteration 97, loss = 0.01292976
Iteration 98, loss = 0.01291867
Iteration 99, loss = 0.01297788
Iteration 100, loss = 0.01292068
Iteration 101, loss = 0.01287428
Iteration 102, loss = 0.01274380
Iteration 103, loss = 0.01281366
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68682785
Iteration 2, loss = 0.63345798
Iteration 3, loss = 0.55341821
Iteration 4, loss = 0.45375129
Iteration 5, loss = 0.35493012
Iteration 6, loss = 0.27109274
Iteration 7, loss = 0.20721592
Iteration 8, loss = 0.16109611
Iteration 9, loss = 0.12834318
Iteration 10, loss = 0.10454485
Iteration 11, loss = 0.08745236
Iteration 12, loss = 0.07457418
Iteration 13, loss = 0.06478644
Iteration 14, loss = 0.05716710
Iteration 15, loss = 0.05122284
Iteration 16, loss = 0.04654732
Iteration 17, loss = 0.04218052
Iteration 18, loss = 0.03895583
Iteration 19, loss = 0.03618973
Iteration 20, loss = 0.03375756
Iteration 21, loss = 0.03184371
Iteration 22, loss = 0.02983791
Iteration 23, loss = 0.02838811
Iteration 24, loss = 0.02720073
Iteration 25, loss = 0.02575162
Iteration 26, loss = 0.02483095
Iteration 27, loss = 0.02388201
Iteration 28, loss = 0.02302260
Iteration 29, loss = 0.02234303
Iteration 30, loss = 0.02180834
Iteration 31, loss = 0.02120778
Iteration 32, loss = 0.02045272
Iteration 33, loss = 0.01991705
Iteration 34, loss = 0.01959280
Iteration 35, loss = 0.01916423
Iteration 36, loss = 0.01871184
Iteration 37, loss = 0.01837416
Iteration 38, loss = 0.01807342
Iteration 39, loss = 0.01765914
Iteration 40, loss = 0.01746181
Iteration 41, loss = 0.01717473
Iteration 42, loss = 0.01691024
Iteration 43, loss = 0.01663017
Iteration 44, loss = 0.01643928
Iteration 45, loss = 0.01635335
Iteration 46, loss = 0.01603194
Iteration 47, loss = 0.01585788
Iteration 48, loss = 0.01561461
Iteration 49, loss = 0.01549148
Iteration 50, loss = 0.01530984
Iteration 51, loss = 0.01532209
Iteration 52, loss = 0.01513241
Iteration 53, loss = 0.01502539
Iteration 54, loss = 0.01491853
Iteration 55, loss = 0.01493804
Iteration 56, loss = 0.01473985
Iteration 57, loss = 0.01458991
Iteration 58, loss = 0.01457511
Iteration 59, loss = 0.01435478
Iteration 60, loss = 0.01447514
Iteration 61, loss = 0.01446308
Iteration 62, loss = 0.01410573
Iteration 63, loss = 0.01410804
Iteration 64, loss = 0.01407148
Iteration 65, loss = 0.01403729
Iteration 66, loss = 0.01389719
Iteration 67, loss = 0.01382416
Iteration 68, loss = 0.01389744
Iteration 69, loss = 0.01362953
Iteration 70, loss = 0.01369632
Iteration 71, loss = 0.01354579
Iteration 72, loss = 0.01343116
Iteration 73, loss = 0.01343462
Iteration 74, loss = 0.01332610
Iteration 75, loss = 0.01339523
Iteration 76, loss = 0.01330704
Iteration 77, loss = 0.01326665
Iteration 78, loss = 0.01333799
Iteration 79, loss = 0.01323253
Iteration 80, loss = 0.01335807
Iteration 81, loss = 0.01341920
Iteration 82, loss = 0.01312158
Iteration 83, loss = 0.01305454
Iteration 84, loss = 0.01307897
Iteration 85, loss = 0.01288771
Iteration 86, loss = 0.01299066
Iteration 87, loss = 0.01305974
Iteration 88, loss = 0.01297128
Iteration 89, loss = 0.01292744
Iteration 90, loss = 0.01284113
Iteration 91, loss = 0.01286006
Iteration 92, loss = 0.01270003
Iteration 93, loss = 0.01292123
Iteration 94, loss = 0.01279914
Iteration 95, loss = 0.01273309
Iteration 96, loss = 0.01273951
Iteration 97, loss = 0.01273165
Iteration 98, loss = 0.01275204
Iteration 99, loss = 0.01250988
Iteration 100, loss = 0.01261378
Iteration 101, loss = 0.01262770
Iteration 102, loss = 0.01268392
Iteration 103, loss = 0.01273535
Iteration 104, loss = 0.01254545
Iteration 105, loss = 0.01267153
Iteration 106, loss = 0.01241434
Iteration 107, loss = 0.01249770
Iteration 108, loss = 0.01252160
Iteration 109, loss = 0.01247359
Iteration 110, loss = 0.01251893
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67891112
Iteration 2, loss = 0.65660450
Iteration 3, loss = 0.62963615
Iteration 4, loss = 0.59258732
Iteration 5, loss = 0.54538826
Iteration 6, loss = 0.49335431
Iteration 7, loss = 0.44086475
Iteration 8, loss = 0.38996076
Iteration 9, loss = 0.34311093
Iteration 10, loss = 0.30030768
Iteration 11, loss = 0.26259906
Iteration 12, loss = 0.22985601
Iteration 13, loss = 0.20159679
Iteration 14, loss = 0.17743707
Iteration 15, loss = 0.15685854
Iteration 16, loss = 0.13936065
Iteration 17, loss = 0.12449284
Iteration 18, loss = 0.11167593
Iteration 19, loss = 0.10082243
Iteration 20, loss = 0.09142923
Iteration 21, loss = 0.08333715
Iteration 22, loss = 0.07640312
Iteration 23, loss = 0.07029106
Iteration 24, loss = 0.06503803
Iteration 25, loss = 0.06030210
Iteration 26, loss = 0.05623313
Iteration 27, loss = 0.05256735
Iteration 28, loss = 0.04928831
Iteration 29, loss = 0.04636061
Iteration 30, loss = 0.04370769
Iteration 31, loss = 0.04144400
Iteration 32, loss = 0.03930316
Iteration 33, loss = 0.03734327
Iteration 34, loss = 0.03567159
Iteration 35, loss = 0.03404984
Iteration 36, loss = 0.03276517
Iteration 37, loss = 0.03140190
Iteration 38, loss = 0.03011373
Iteration 39, loss = 0.02910342
Iteration 40, loss = 0.02800046
Iteration 41, loss = 0.02702669
Iteration 42, loss = 0.02611287
Iteration 43, loss = 0.02521645
Iteration 44, loss = 0.02444624
Iteration 45, loss = 0.02380406
Iteration 46, loss = 0.02316330
Iteration 47, loss = 0.02260583
Iteration 48, loss = 0.02195729
Iteration 49, loss = 0.02123290
Iteration 50, loss = 0.02091288
Iteration 51, loss = 0.02042890
Iteration 52, loss = 0.01991692
Iteration 53, loss = 0.01951031
Iteration 54, loss = 0.01908421
Iteration 55, loss = 0.01865630
Iteration 56, loss = 0.01828725
Iteration 57, loss = 0.01808944
Iteration 58, loss = 0.01758103
Iteration 59, loss = 0.01729343
Iteration 60, loss = 0.01701921
Iteration 61, loss = 0.01671787
Iteration 62, loss = 0.01647306
Iteration 63, loss = 0.01629083
Iteration 64, loss = 0.01622704
Iteration 65, loss = 0.01594793
Iteration 66, loss = 0.01565767
Iteration 67, loss = 0.01548210
Iteration 68, loss = 0.01516214
Iteration 69, loss = 0.01499183
Iteration 70, loss = 0.01473294
Iteration 71, loss = 0.01446798
Iteration 72, loss = 0.01425501
Iteration 73, loss = 0.01409233
Iteration 74, loss = 0.01393267
Iteration 75, loss = 0.01375788
Iteration 76, loss = 0.01372284
Iteration 77, loss = 0.01369161
Iteration 78, loss = 0.01363548
Iteration 79, loss = 0.01368501
Iteration 80, loss = 0.01350830
Iteration 81, loss = 0.01329541
Iteration 82, loss = 0.01314249
Iteration 83, loss = 0.01274628
Iteration 84, loss = 0.01290795
Iteration 85, loss = 0.01274867
Iteration 86, loss = 0.01260861
Iteration 87, loss = 0.01272246
Iteration 88, loss = 0.01264266
Iteration 89, loss = 0.01248541
Iteration 90, loss = 0.01226117
Iteration 91, loss = 0.01219528
Iteration 92, loss = 0.01195544
Iteration 93, loss = 0.01198800
Iteration 94, loss = 0.01186289
Iteration 95, loss = 0.01194416
Iteration 96, loss = 0.01178570
Iteration 97, loss = 0.01166790
Iteration 98, loss = 0.01171354
Iteration 99, loss = 0.01161277
Iteration 100, loss = 0.01145185
Iteration 101, loss = 0.01134959
Iteration 102, loss = 0.01127784
Iteration 103, loss = 0.01112838
Iteration 104, loss = 0.01100115
Iteration 105, loss = 0.01106742
Iteration 106, loss = 0.01121538
Iteration 107, loss = 0.01112351
Iteration 108, loss = 0.01105192
Iteration 109, loss = 0.01090406
Iteration 110, loss = 0.01079601
Iteration 111, loss = 0.01068228
Iteration 112, loss = 0.01067803
Iteration 113, loss = 0.01055700
Iteration 114, loss = 0.01047822
Iteration 115, loss = 0.01047370
Iteration 116, loss = 0.01041839
Iteration 117, loss = 0.01034261
Iteration 118, loss = 0.01033240
Iteration 119, loss = 0.01031665
Iteration 120, loss = 0.01026740
Iteration 121, loss = 0.01020373
Iteration 122, loss = 0.01022068
Iteration 123, loss = 0.01019542
Iteration 124, loss = 0.01021549
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68144021
Iteration 2, loss = 0.61958759
Iteration 3, loss = 0.52645564
Iteration 4, loss = 0.42069814
Iteration 5, loss = 0.32563347
Iteration 6, loss = 0.24864885
Iteration 7, loss = 0.19185103
Iteration 8, loss = 0.15107910
Iteration 9, loss = 0.12174345
Iteration 10, loss = 0.10061839
Iteration 11, loss = 0.08500838
Iteration 12, loss = 0.07325810
Iteration 13, loss = 0.06436306
Iteration 14, loss = 0.05722168
Iteration 15, loss = 0.05149923
Iteration 16, loss = 0.04685356
Iteration 17, loss = 0.04332886
Iteration 18, loss = 0.04012041
Iteration 19, loss = 0.03758307
Iteration 20, loss = 0.03546501
Iteration 21, loss = 0.03340527
Iteration 22, loss = 0.03195198
Iteration 23, loss = 0.03030449
Iteration 24, loss = 0.02893649
Iteration 25, loss = 0.02790761
Iteration 26, loss = 0.02717389
Iteration 27, loss = 0.02610141
Iteration 28, loss = 0.02540056
Iteration 29, loss = 0.02459754
Iteration 30, loss = 0.02401512
Iteration 31, loss = 0.02329757
Iteration 32, loss = 0.02302078
Iteration 33, loss = 0.02252496
Iteration 34, loss = 0.02222098
Iteration 35, loss = 0.02179367
Iteration 36, loss = 0.02161656
Iteration 37, loss = 0.02099501
Iteration 38, loss = 0.02077054
Iteration 39, loss = 0.02039624
Iteration 40, loss = 0.02033208
Iteration 41, loss = 0.02014233
Iteration 42, loss = 0.01992295
Iteration 43, loss = 0.01946067
Iteration 44, loss = 0.01967103
Iteration 45, loss = 0.01924057
Iteration 46, loss = 0.01916060
Iteration 47, loss = 0.01880048
Iteration 48, loss = 0.01878901
Iteration 49, loss = 0.01858988
Iteration 50, loss = 0.01831676
Iteration 51, loss = 0.01818428
Iteration 52, loss = 0.01831090
Iteration 53, loss = 0.01819510
Iteration 54, loss = 0.01795022
Iteration 55, loss = 0.01803113
Iteration 56, loss = 0.01786052
Iteration 57, loss = 0.01775157
Iteration 58, loss = 0.01773473
Iteration 59, loss = 0.01760550
Iteration 60, loss = 0.01756650
Iteration 61, loss = 0.01753362
Iteration 62, loss = 0.01740073
Iteration 63, loss = 0.01725099
Iteration 64, loss = 0.01723018
Iteration 65, loss = 0.01720220
Iteration 66, loss = 0.01716512
Iteration 67, loss = 0.01711875
Iteration 68, loss = 0.01716123
Iteration 69, loss = 0.01693742
Iteration 70, loss = 0.01710603
Iteration 71, loss = 0.01687090
Iteration 72, loss = 0.01692952
Iteration 73, loss = 0.01701977
Iteration 74, loss = 0.01680450
Iteration 75, loss = 0.01691724
Iteration 76, loss = 0.01675475
Iteration 77, loss = 0.01659956
Iteration 78, loss = 0.01660751
Iteration 79, loss = 0.01640108
Iteration 80, loss = 0.01647464
Iteration 81, loss = 0.01660064
Iteration 82, loss = 0.01657303
Iteration 83, loss = 0.01652813
Iteration 84, loss = 0.01633777
Iteration 85, loss = 0.01660766
Iteration 86, loss = 0.01636624
Iteration 87, loss = 0.01657928
Iteration 88, loss = 0.01638728
Iteration 89, loss = 0.01655625
Iteration 90, loss = 0.01631291
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67816547
Iteration 2, loss = 0.64909636
Iteration 3, loss = 0.60342506
Iteration 4, loss = 0.53882877
Iteration 5, loss = 0.46604135
Iteration 6, loss = 0.39438663
Iteration 7, loss = 0.32833867
Iteration 8, loss = 0.27077974
Iteration 9, loss = 0.22257870
Iteration 10, loss = 0.18355488
Iteration 11, loss = 0.15271513
Iteration 12, loss = 0.12831454
Iteration 13, loss = 0.10924045
Iteration 14, loss = 0.09404300
Iteration 15, loss = 0.08195162
Iteration 16, loss = 0.07221440
Iteration 17, loss = 0.06423801
Iteration 18, loss = 0.05772445
Iteration 19, loss = 0.05219298
Iteration 20, loss = 0.04767802
Iteration 21, loss = 0.04387821
Iteration 22, loss = 0.04042838
Iteration 23, loss = 0.03769829
Iteration 24, loss = 0.03501233
Iteration 25, loss = 0.03297934
Iteration 26, loss = 0.03095122
Iteration 27, loss = 0.02930855
Iteration 28, loss = 0.02781953
Iteration 29, loss = 0.02650743
Iteration 30, loss = 0.02530016
Iteration 31, loss = 0.02426320
Iteration 32, loss = 0.02328103
Iteration 33, loss = 0.02242571
Iteration 34, loss = 0.02161511
Iteration 35, loss = 0.02084299
Iteration 36, loss = 0.02012937
Iteration 37, loss = 0.01958250
Iteration 38, loss = 0.01896411
Iteration 39, loss = 0.01850131
Iteration 40, loss = 0.01798455
Iteration 41, loss = 0.01760052
Iteration 42, loss = 0.01728283
Iteration 43, loss = 0.01686184
Iteration 44, loss = 0.01647494
Iteration 45, loss = 0.01608876
Iteration 46, loss = 0.01582427
Iteration 47, loss = 0.01563261
Iteration 48, loss = 0.01529602
Iteration 49, loss = 0.01494165
Iteration 50, loss = 0.01476287
Iteration 51, loss = 0.01462188
Iteration 52, loss = 0.01436936
Iteration 53, loss = 0.01423883
Iteration 54, loss = 0.01405455
Iteration 55, loss = 0.01379346
Iteration 56, loss = 0.01367767
Iteration 57, loss = 0.01347770
Iteration 58, loss = 0.01342880
Iteration 59, loss = 0.01326046
Iteration 60, loss = 0.01304721
Iteration 61, loss = 0.01297963
Iteration 62, loss = 0.01278202
Iteration 63, loss = 0.01273401
Iteration 64, loss = 0.01256565
Iteration 65, loss = 0.01248834
Iteration 66, loss = 0.01243358
Iteration 67, loss = 0.01228778
Iteration 68, loss = 0.01228952
Iteration 69, loss = 0.01205256
Iteration 70, loss = 0.01206041
Iteration 71, loss = 0.01199253
Iteration 72, loss = 0.01192109
Iteration 73, loss = 0.01181645
Iteration 74, loss = 0.01170086
Iteration 75, loss = 0.01167324
Iteration 76, loss = 0.01162768
Iteration 77, loss = 0.01158844
Iteration 78, loss = 0.01140410
Iteration 79, loss = 0.01142739
Iteration 80, loss = 0.01142902
Iteration 81, loss = 0.01132366
Iteration 82, loss = 0.01113735
Iteration 83, loss = 0.01122662
Iteration 84, loss = 0.01111522
Iteration 85, loss = 0.01107142
Iteration 86, loss = 0.01105561
Iteration 87, loss = 0.01101567
Iteration 88, loss = 0.01097101
Iteration 89, loss = 0.01096658
Iteration 90, loss = 0.01093435
Iteration 91, loss = 0.01091546
Iteration 92, loss = 0.01088935
Iteration 93, loss = 0.01088245
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67841564
Iteration 2, loss = 0.63203642
Iteration 3, loss = 0.56137954
Iteration 4, loss = 0.47340456
Iteration 5, loss = 0.38470650
Iteration 6, loss = 0.30535408
Iteration 7, loss = 0.24046000
Iteration 8, loss = 0.19012479
Iteration 9, loss = 0.15245203
Iteration 10, loss = 0.12434160
Iteration 11, loss = 0.10342036
Iteration 12, loss = 0.08793611
Iteration 13, loss = 0.07580758
Iteration 14, loss = 0.06643722
Iteration 15, loss = 0.05902091
Iteration 16, loss = 0.05314552
Iteration 17, loss = 0.04827838
Iteration 18, loss = 0.04425814
Iteration 19, loss = 0.04097070
Iteration 20, loss = 0.03813814
Iteration 21, loss = 0.03573969
Iteration 22, loss = 0.03369236
Iteration 23, loss = 0.03203525
Iteration 24, loss = 0.03048109
Iteration 25, loss = 0.02902726
Iteration 26, loss = 0.02789388
Iteration 27, loss = 0.02692605
Iteration 28, loss = 0.02603767
Iteration 29, loss = 0.02514222
Iteration 30, loss = 0.02465338
Iteration 31, loss = 0.02382307
Iteration 32, loss = 0.02329190
Iteration 33, loss = 0.02289940
Iteration 34, loss = 0.02191405
Iteration 35, loss = 0.02154893
Iteration 36, loss = 0.02136482
Iteration 37, loss = 0.02094714
Iteration 38, loss = 0.02072554
Iteration 39, loss = 0.02025278
Iteration 40, loss = 0.01984513
Iteration 41, loss = 0.01948225
Iteration 42, loss = 0.01948779
Iteration 43, loss = 0.01929274
Iteration 44, loss = 0.01900882
Iteration 45, loss = 0.01865352
Iteration 46, loss = 0.01865931
Iteration 47, loss = 0.01844559
Iteration 48, loss = 0.01818433
Iteration 49, loss = 0.01806061
Iteration 50, loss = 0.01808171
Iteration 51, loss = 0.01764266
Iteration 52, loss = 0.01766569
Iteration 53, loss = 0.01753453
Iteration 54, loss = 0.01744519
Iteration 55, loss = 0.01731484
Iteration 56, loss = 0.01714867
Iteration 57, loss = 0.01706651
Iteration 58, loss = 0.01695827
Iteration 59, loss = 0.01699810
Iteration 60, loss = 0.01671626
Iteration 61, loss = 0.01688015
Iteration 62, loss = 0.01660725
Iteration 63, loss = 0.01685504
Iteration 64, loss = 0.01638593
Iteration 65, loss = 0.01664346
Iteration 66, loss = 0.01656876
Iteration 67, loss = 0.01628998
Iteration 68, loss = 0.01631825
Iteration 69, loss = 0.01628493
Iteration 70, loss = 0.01620277
Iteration 71, loss = 0.01608683
Iteration 72, loss = 0.01637370
Iteration 73, loss = 0.01617054
Iteration 74, loss = 0.01630080
Iteration 75, loss = 0.01609203
Iteration 76, loss = 0.01619600
Iteration 77, loss = 0.01591396
Iteration 78, loss = 0.01586514
Iteration 79, loss = 0.01595126
Iteration 80, loss = 0.01586038
Iteration 81, loss = 0.01603205
Iteration 82, loss = 0.01599875
Iteration 83, loss = 0.01569058
Iteration 84, loss = 0.01585023
Iteration 85, loss = 0.01568241
Iteration 86, loss = 0.01579433
Iteration 87, loss = 0.01569380
Iteration 88, loss = 0.01571381
Iteration 89, loss = 0.01559556
Iteration 90, loss = 0.01550646
Iteration 91, loss = 0.01558366
Iteration 92, loss = 0.01558445
Iteration 93, loss = 0.01544914
Iteration 94, loss = 0.01555326
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68224994
Iteration 2, loss = 0.60427402
Iteration 3, loss = 0.48810838
Iteration 4, loss = 0.37306006
Iteration 5, loss = 0.27640694
Iteration 6, loss = 0.20455322
Iteration 7, loss = 0.15430598
Iteration 8, loss = 0.12016348
Iteration 9, loss = 0.09655501
Iteration 10, loss = 0.07967325
Iteration 11, loss = 0.06761844
Iteration 12, loss = 0.05863707
Iteration 13, loss = 0.05158622
Iteration 14, loss = 0.04602476
Iteration 15, loss = 0.04176632
Iteration 16, loss = 0.03816763
Iteration 17, loss = 0.03519615
Iteration 18, loss = 0.03292312
Iteration 19, loss = 0.03078569
Iteration 20, loss = 0.02907526
Iteration 21, loss = 0.02760849
Iteration 22, loss = 0.02625800
Iteration 23, loss = 0.02526232
Iteration 24, loss = 0.02429905
Iteration 25, loss = 0.02333954
Iteration 26, loss = 0.02258020
Iteration 27, loss = 0.02192514
Iteration 28, loss = 0.02128234
Iteration 29, loss = 0.02110595
Iteration 30, loss = 0.02042667
Iteration 31, loss = 0.01989648
Iteration 32, loss = 0.01935498
Iteration 33, loss = 0.01917547
Iteration 34, loss = 0.01864750
Iteration 35, loss = 0.01845732
Iteration 36, loss = 0.01813082
Iteration 37, loss = 0.01794353
Iteration 38, loss = 0.01762063
Iteration 39, loss = 0.01760309
Iteration 40, loss = 0.01716377
Iteration 41, loss = 0.01705696
Iteration 42, loss = 0.01689310
Iteration 43, loss = 0.01670259
Iteration 44, loss = 0.01644809
Iteration 45, loss = 0.01634951
Iteration 46, loss = 0.01618967
Iteration 47, loss = 0.01640501
Iteration 48, loss = 0.01612720
Iteration 49, loss = 0.01581805
Iteration 50, loss = 0.01589953
Iteration 51, loss = 0.01569058
Iteration 52, loss = 0.01564824
Iteration 53, loss = 0.01549870
Iteration 54, loss = 0.01535147
Iteration 55, loss = 0.01513372
Iteration 56, loss = 0.01530958
Iteration 57, loss = 0.01537766
Iteration 58, loss = 0.01514903
Iteration 59, loss = 0.01513768
Iteration 60, loss = 0.01497988
Iteration 61, loss = 0.01485562
Iteration 62, loss = 0.01519696
Iteration 63, loss = 0.01498780
Iteration 64, loss = 0.01461615
Iteration 65, loss = 0.01458456
Iteration 66, loss = 0.01461687
Iteration 67, loss = 0.01455461
Iteration 68, loss = 0.01458732
Iteration 69, loss = 0.01430666
Iteration 70, loss = 0.01470113
Iteration 71, loss = 0.01461111
Iteration 72, loss = 0.01437899
Iteration 73, loss = 0.01434830
Iteration 74, loss = 0.01420991
Iteration 75, loss = 0.01424351
Iteration 76, loss = 0.01440742
Iteration 77, loss = 0.01411081
Iteration 78, loss = 0.01413017
Iteration 79, loss = 0.01415860
Iteration 80, loss = 0.01411532
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70052806
Iteration 2, loss = 0.62591925
Iteration 3, loss = 0.52103571
Iteration 4, loss = 0.40381485
Iteration 5, loss = 0.30420606
Iteration 6, loss = 0.22760366
Iteration 7, loss = 0.17296112
Iteration 8, loss = 0.13490983
Iteration 9, loss = 0.10799859
Iteration 10, loss = 0.08899399
Iteration 11, loss = 0.07512360
Iteration 12, loss = 0.06493263
Iteration 13, loss = 0.05670603
Iteration 14, loss = 0.05063321
Iteration 15, loss = 0.04570779
Iteration 16, loss = 0.04158194
Iteration 17, loss = 0.03826087
Iteration 18, loss = 0.03540397
Iteration 19, loss = 0.03318748
Iteration 20, loss = 0.03112679
Iteration 21, loss = 0.02942467
Iteration 22, loss = 0.02807965
Iteration 23, loss = 0.02683620
Iteration 24, loss = 0.02543298
Iteration 25, loss = 0.02454436
Iteration 26, loss = 0.02376974
Iteration 27, loss = 0.02293518
Iteration 28, loss = 0.02219460
Iteration 29, loss = 0.02167183
Iteration 30, loss = 0.02102034
Iteration 31, loss = 0.02071606
Iteration 32, loss = 0.02007053
Iteration 33, loss = 0.01953907
Iteration 34, loss = 0.01920753
Iteration 35, loss = 0.01918943
Iteration 36, loss = 0.01870171
Iteration 37, loss = 0.01814065
Iteration 38, loss = 0.01812863
Iteration 39, loss = 0.01761415
Iteration 40, loss = 0.01759051
Iteration 41, loss = 0.01718697
Iteration 42, loss = 0.01703427
Iteration 43, loss = 0.01707278
Iteration 44, loss = 0.01678673
Iteration 45, loss = 0.01638662
Iteration 46, loss = 0.01625830
Iteration 47, loss = 0.01634092
Iteration 48, loss = 0.01606921
Iteration 49, loss = 0.01621835
Iteration 50, loss = 0.01586728
Iteration 51, loss = 0.01559874
Iteration 52, loss = 0.01543683
Iteration 53, loss = 0.01556005
Iteration 54, loss = 0.01525893
Iteration 55, loss = 0.01520969
Iteration 56, loss = 0.01527616
Iteration 57, loss = 0.01520870
Iteration 58, loss = 0.01500260
Iteration 59, loss = 0.01482079
Iteration 60, loss = 0.01484429
Iteration 61, loss = 0.01483334
Iteration 62, loss = 0.01469878
Iteration 63, loss = 0.01494879
Iteration 64, loss = 0.01471092
Iteration 65, loss = 0.01448578
Iteration 66, loss = 0.01447002
Iteration 67, loss = 0.01427905
Iteration 68, loss = 0.01434220
Iteration 69, loss = 0.01429099
Iteration 70, loss = 0.01427267
Iteration 71, loss = 0.01419276
Iteration 72, loss = 0.01434141
Iteration 73, loss = 0.01404661
Iteration 74, loss = 0.01428055
Iteration 75, loss = 0.01388279
Iteration 76, loss = 0.01403285
Iteration 77, loss = 0.01404674
Iteration 78, loss = 0.01416780
Iteration 79, loss = 0.01383755
Iteration 80, loss = 0.01399899
Iteration 81, loss = 0.01393258
Iteration 82, loss = 0.01374784
Iteration 83, loss = 0.01385150
Iteration 84, loss = 0.01392912
Iteration 85, loss = 0.01376092
Iteration 86, loss = 0.01364055
Iteration 87, loss = 0.01364158
Iteration 88, loss = 0.01377116
Iteration 89, loss = 0.01363587
Iteration 90, loss = 0.01367776
Iteration 91, loss = 0.01362590
Iteration 92, loss = 0.01375116
Iteration 93, loss = 0.01357707
Iteration 94, loss = 0.01349807
Iteration 95, loss = 0.01352436
Iteration 96, loss = 0.01341555
Iteration 97, loss = 0.01355377
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70591096
Iteration 2, loss = 0.62879884
Iteration 3, loss = 0.52036755
Iteration 4, loss = 0.40392973
Iteration 5, loss = 0.30535987
Iteration 6, loss = 0.22921560
Iteration 7, loss = 0.17488127
Iteration 8, loss = 0.13666503
Iteration 9, loss = 0.10990882
Iteration 10, loss = 0.09081323
Iteration 11, loss = 0.07707089
Iteration 12, loss = 0.06664228
Iteration 13, loss = 0.05879067
Iteration 14, loss = 0.05245152
Iteration 15, loss = 0.04745657
Iteration 16, loss = 0.04358781
Iteration 17, loss = 0.04040309
Iteration 18, loss = 0.03755281
Iteration 19, loss = 0.03549810
Iteration 20, loss = 0.03337175
Iteration 21, loss = 0.03171054
Iteration 22, loss = 0.03055451
Iteration 23, loss = 0.02904616
Iteration 24, loss = 0.02794007
Iteration 25, loss = 0.02710049
Iteration 26, loss = 0.02618525
Iteration 27, loss = 0.02533211
Iteration 28, loss = 0.02472941
Iteration 29, loss = 0.02416036
Iteration 30, loss = 0.02372358
Iteration 31, loss = 0.02337554
Iteration 32, loss = 0.02259044
Iteration 33, loss = 0.02247836
Iteration 34, loss = 0.02202215
Iteration 35, loss = 0.02172421
Iteration 36, loss = 0.02129638
Iteration 37, loss = 0.02090036
Iteration 38, loss = 0.02082474
Iteration 39, loss = 0.02084210
Iteration 40, loss = 0.02028768
Iteration 41, loss = 0.02018505
Iteration 42, loss = 0.01977834
Iteration 43, loss = 0.01968350
Iteration 44, loss = 0.01963116
Iteration 45, loss = 0.01946776
Iteration 46, loss = 0.01918732
Iteration 47, loss = 0.01919728
Iteration 48, loss = 0.01907307
Iteration 49, loss = 0.01900694
Iteration 50, loss = 0.01878888
Iteration 51, loss = 0.01872056
Iteration 52, loss = 0.01874869
Iteration 53, loss = 0.01872854
Iteration 54, loss = 0.01869462
Iteration 55, loss = 0.01835718
Iteration 56, loss = 0.01858870
Iteration 57, loss = 0.01819424
Iteration 58, loss = 0.01815284
Iteration 59, loss = 0.01818147
Iteration 60, loss = 0.01798717
Iteration 61, loss = 0.01798386
Iteration 62, loss = 0.01777457
Iteration 63, loss = 0.01792572
Iteration 64, loss = 0.01775191
Iteration 65, loss = 0.01768063
Iteration 66, loss = 0.01769429
Iteration 67, loss = 0.01794401
Iteration 68, loss = 0.01807191
Iteration 69, loss = 0.01763233
Iteration 70, loss = 0.01775065
Iteration 71, loss = 0.01730117
Iteration 72, loss = 0.01783068
Iteration 73, loss = 0.01763753
Iteration 74, loss = 0.01742321
Iteration 75, loss = 0.01733661
Iteration 76, loss = 0.01742993
Iteration 77, loss = 0.01736836
Iteration 78, loss = 0.01734131
Iteration 79, loss = 0.01724041
Iteration 80, loss = 0.01713515
Iteration 81, loss = 0.01713632
Iteration 82, loss = 0.01716021
Iteration 83, loss = 0.01708059
Iteration 84, loss = 0.01731191
Iteration 85, loss = 0.01695540
Iteration 86, loss = 0.01708550
Iteration 87, loss = 0.01706219
Iteration 88, loss = 0.01686030
Iteration 89, loss = 0.01692449
Iteration 90, loss = 0.01696857
Iteration 91, loss = 0.01700519
Iteration 92, loss = 0.01695585
Iteration 93, loss = 0.01730576
Iteration 94, loss = 0.01703284
Iteration 95, loss = 0.01685706
Iteration 96, loss = 0.01698755
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69244730
Iteration 2, loss = 0.61392030
Iteration 3, loss = 0.50860571
Iteration 4, loss = 0.39201831
Iteration 5, loss = 0.29370166
Iteration 6, loss = 0.21969863
Iteration 7, loss = 0.16703465
Iteration 8, loss = 0.13052167
Iteration 9, loss = 0.10518779
Iteration 10, loss = 0.08693143
Iteration 11, loss = 0.07365972
Iteration 12, loss = 0.06358654
Iteration 13, loss = 0.05598112
Iteration 14, loss = 0.05004630
Iteration 15, loss = 0.04521912
Iteration 16, loss = 0.04135984
Iteration 17, loss = 0.03806931
Iteration 18, loss = 0.03560138
Iteration 19, loss = 0.03320652
Iteration 20, loss = 0.03131686
Iteration 21, loss = 0.02974747
Iteration 22, loss = 0.02827321
Iteration 23, loss = 0.02714421
Iteration 24, loss = 0.02596694
Iteration 25, loss = 0.02491577
Iteration 26, loss = 0.02411029
Iteration 27, loss = 0.02342064
Iteration 28, loss = 0.02281299
Iteration 29, loss = 0.02232210
Iteration 30, loss = 0.02161040
Iteration 31, loss = 0.02116368
Iteration 32, loss = 0.02070081
Iteration 33, loss = 0.02036369
Iteration 34, loss = 0.01990460
Iteration 35, loss = 0.01980915
Iteration 36, loss = 0.01962931
Iteration 37, loss = 0.01925799
Iteration 38, loss = 0.01893968
Iteration 39, loss = 0.01850055
Iteration 40, loss = 0.01849005
Iteration 41, loss = 0.01826924
Iteration 42, loss = 0.01809167
Iteration 43, loss = 0.01797053
Iteration 44, loss = 0.01772229
Iteration 45, loss = 0.01760313
Iteration 46, loss = 0.01733013
Iteration 47, loss = 0.01712987
Iteration 48, loss = 0.01714251
Iteration 49, loss = 0.01683011
Iteration 50, loss = 0.01676534
Iteration 51, loss = 0.01672750
Iteration 52, loss = 0.01673116
Iteration 53, loss = 0.01651342
Iteration 54, loss = 0.01628736
Iteration 55, loss = 0.01637404
Iteration 56, loss = 0.01620340
Iteration 57, loss = 0.01635543
Iteration 58, loss = 0.01612385
Iteration 59, loss = 0.01607096
Iteration 60, loss = 0.01607226
Iteration 61, loss = 0.01608063
Iteration 62, loss = 0.01588700
Iteration 63, loss = 0.01605285
Iteration 64, loss = 0.01587226
Iteration 65, loss = 0.01569421
Iteration 66, loss = 0.01566466
Iteration 67, loss = 0.01568363
Iteration 68, loss = 0.01561400
Iteration 69, loss = 0.01559674
Iteration 70, loss = 0.01565742
Iteration 71, loss = 0.01549040
Iteration 72, loss = 0.01561684
Iteration 73, loss = 0.01555459
Iteration 74, loss = 0.01529909
Iteration 75, loss = 0.01533303
Iteration 76, loss = 0.01534584
Iteration 77, loss = 0.01535574
Iteration 78, loss = 0.01520760
Iteration 79, loss = 0.01506696
Iteration 80, loss = 0.01515571
Iteration 81, loss = 0.01513802
Iteration 82, loss = 0.01518445
Iteration 83, loss = 0.01509923
Iteration 84, loss = 0.01490762
Iteration 85, loss = 0.01506870
Iteration 86, loss = 0.01503484
Iteration 87, loss = 0.01499938
Iteration 88, loss = 0.01514966
Iteration 89, loss = 0.01478411
Iteration 90, loss = 0.01489654
Iteration 91, loss = 0.01491213
Iteration 92, loss = 0.01486538
Iteration 93, loss = 0.01496867
Iteration 94, loss = 0.01474771
Iteration 95, loss = 0.01495537
Iteration 96, loss = 0.01486308
Iteration 97, loss = 0.01476732
Iteration 98, loss = 0.01497187
Iteration 99, loss = 0.01511825
Iteration 100, loss = 0.01478076
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68311707
Iteration 2, loss = 0.65643353
Iteration 3, loss = 0.61782609
Iteration 4, loss = 0.56311643
Iteration 5, loss = 0.49883417
Iteration 6, loss = 0.43366760
Iteration 7, loss = 0.37163460
Iteration 8, loss = 0.31573306
Iteration 9, loss = 0.26694031
Iteration 10, loss = 0.22537016
Iteration 11, loss = 0.19091900
Iteration 12, loss = 0.16248833
Iteration 13, loss = 0.13940970
Iteration 14, loss = 0.12062132
Iteration 15, loss = 0.10529825
Iteration 16, loss = 0.09272644
Iteration 17, loss = 0.08230380
Iteration 18, loss = 0.07361813
Iteration 19, loss = 0.06636566
Iteration 20, loss = 0.06034495
Iteration 21, loss = 0.05509581
Iteration 22, loss = 0.05062838
Iteration 23, loss = 0.04676747
Iteration 24, loss = 0.04342147
Iteration 25, loss = 0.04049379
Iteration 26, loss = 0.03798040
Iteration 27, loss = 0.03571208
Iteration 28, loss = 0.03386113
Iteration 29, loss = 0.03184887
Iteration 30, loss = 0.03029810
Iteration 31, loss = 0.02888264
Iteration 32, loss = 0.02750172
Iteration 33, loss = 0.02641370
Iteration 34, loss = 0.02529760
Iteration 35, loss = 0.02432690
Iteration 36, loss = 0.02335826
Iteration 37, loss = 0.02262546
Iteration 38, loss = 0.02182893
Iteration 39, loss = 0.02113548
Iteration 40, loss = 0.02049469
Iteration 41, loss = 0.01985394
Iteration 42, loss = 0.01935343
Iteration 43, loss = 0.01876886
Iteration 44, loss = 0.01832911
Iteration 45, loss = 0.01787617
Iteration 46, loss = 0.01741551
Iteration 47, loss = 0.01706791
Iteration 48, loss = 0.01666725
Iteration 49, loss = 0.01631234
Iteration 50, loss = 0.01607601
Iteration 51, loss = 0.01580263
Iteration 52, loss = 0.01552600
Iteration 53, loss = 0.01517128
Iteration 54, loss = 0.01486252
Iteration 55, loss = 0.01475413
Iteration 56, loss = 0.01444951
Iteration 57, loss = 0.01428539
Iteration 58, loss = 0.01407722
Iteration 59, loss = 0.01378784
Iteration 60, loss = 0.01373556
Iteration 61, loss = 0.01352130
Iteration 62, loss = 0.01338438
Iteration 63, loss = 0.01314046
Iteration 64, loss = 0.01296700
Iteration 65, loss = 0.01289548
Iteration 66, loss = 0.01283706
Iteration 67, loss = 0.01260592
Iteration 68, loss = 0.01253143
Iteration 69, loss = 0.01237658
Iteration 70, loss = 0.01230345
Iteration 71, loss = 0.01224413
Iteration 72, loss = 0.01222738
Iteration 73, loss = 0.01200614
Iteration 74, loss = 0.01192996
Iteration 75, loss = 0.01184819
Iteration 76, loss = 0.01170800
Iteration 77, loss = 0.01158918
Iteration 78, loss = 0.01153811
Iteration 79, loss = 0.01140513
Iteration 80, loss = 0.01139899
Iteration 81, loss = 0.01133684
Iteration 82, loss = 0.01121846
Iteration 83, loss = 0.01115856
Iteration 84, loss = 0.01115751
Iteration 85, loss = 0.01106296
Iteration 86, loss = 0.01103339
Iteration 87, loss = 0.01093046
Iteration 88, loss = 0.01081053
Iteration 89, loss = 0.01079687
Iteration 90, loss = 0.01074527
Iteration 91, loss = 0.01068786
Iteration 92, loss = 0.01069903
Iteration 93, loss = 0.01070790
Iteration 94, loss = 0.01064549
Iteration 95, loss = 0.01066523
Iteration 96, loss = 0.01046082
Iteration 97, loss = 0.01041510
Iteration 98, loss = 0.01050230
Iteration 99, loss = 0.01033117
Iteration 100, loss = 0.01030614
Iteration 101, loss = 0.01031313
Iteration 102, loss = 0.01019029
Iteration 103, loss = 0.01016014
Iteration 104, loss = 0.01016551
Iteration 105, loss = 0.01015367
Iteration 106, loss = 0.01013520
Iteration 107, loss = 0.01001434
Iteration 108, loss = 0.00998135
Iteration 109, loss = 0.00997992
Iteration 110, loss = 0.00993220
Iteration 111, loss = 0.01000497
Iteration 112, loss = 0.01001618
Iteration 113, loss = 0.00986081
Iteration 114, loss = 0.01008312
Iteration 115, loss = 0.01001888
Iteration 116, loss = 0.00986845
Iteration 117, loss = 0.00984954
Iteration 118, loss = 0.00983809
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68375973
Iteration 2, loss = 0.65657132
Iteration 3, loss = 0.63030638
Iteration 4, loss = 0.60058002
Iteration 5, loss = 0.56774147
Iteration 6, loss = 0.53200910
Iteration 7, loss = 0.49485907
Iteration 8, loss = 0.45718554
Iteration 9, loss = 0.42014207
Iteration 10, loss = 0.38462400
Iteration 11, loss = 0.35117660
Iteration 12, loss = 0.31964260
Iteration 13, loss = 0.29076236
Iteration 14, loss = 0.26417888
Iteration 15, loss = 0.24003262
Iteration 16, loss = 0.21825138
Iteration 17, loss = 0.19866972
Iteration 18, loss = 0.18110845
Iteration 19, loss = 0.16556842
Iteration 20, loss = 0.15156706
Iteration 21, loss = 0.13918117
Iteration 22, loss = 0.12813632
Iteration 23, loss = 0.11829791
Iteration 24, loss = 0.10958551
Iteration 25, loss = 0.10169953
Iteration 26, loss = 0.09474524
Iteration 27, loss = 0.08841300
Iteration 28, loss = 0.08289508
Iteration 29, loss = 0.07781676
Iteration 30, loss = 0.07326631
Iteration 31, loss = 0.06922216
Iteration 32, loss = 0.06545676
Iteration 33, loss = 0.06208548
Iteration 34, loss = 0.05899650
Iteration 35, loss = 0.05615526
Iteration 36, loss = 0.05364730
Iteration 37, loss = 0.05127664
Iteration 38, loss = 0.04914053
Iteration 39, loss = 0.04712561
Iteration 40, loss = 0.04532860
Iteration 41, loss = 0.04356146
Iteration 42, loss = 0.04204814
Iteration 43, loss = 0.04060705
Iteration 44, loss = 0.03924949
Iteration 45, loss = 0.03802360
Iteration 46, loss = 0.03677810
Iteration 47, loss = 0.03573463
Iteration 48, loss = 0.03468222
Iteration 49, loss = 0.03384090
Iteration 50, loss = 0.03285994
Iteration 51, loss = 0.03198688
Iteration 52, loss = 0.03118469
Iteration 53, loss = 0.03044325
Iteration 54, loss = 0.02967547
Iteration 55, loss = 0.02911586
Iteration 56, loss = 0.02839151
Iteration 57, loss = 0.02789213
Iteration 58, loss = 0.02730164
Iteration 59, loss = 0.02680263
Iteration 60, loss = 0.02622807
Iteration 61, loss = 0.02574535
Iteration 62, loss = 0.02529712
Iteration 63, loss = 0.02491243
Iteration 64, loss = 0.02451465
Iteration 65, loss = 0.02404874
Iteration 66, loss = 0.02366453
Iteration 67, loss = 0.02332876
Iteration 68, loss = 0.02300895
Iteration 69, loss = 0.02267088
Iteration 70, loss = 0.02235537
Iteration 71, loss = 0.02212788
Iteration 72, loss = 0.02174744
Iteration 73, loss = 0.02149547
Iteration 74, loss = 0.02121861
Iteration 75, loss = 0.02104675
Iteration 76, loss = 0.02072408
Iteration 77, loss = 0.02050689
Iteration 78, loss = 0.02029491
Iteration 79, loss = 0.02008216
Iteration 80, loss = 0.01983645
Iteration 81, loss = 0.01964725
Iteration 82, loss = 0.01944759
Iteration 83, loss = 0.01935587
Iteration 84, loss = 0.01910868
Iteration 85, loss = 0.01900182
Iteration 86, loss = 0.01875969
Iteration 87, loss = 0.01861130
Iteration 88, loss = 0.01846424
Iteration 89, loss = 0.01831699
Iteration 90, loss = 0.01821517
Iteration 91, loss = 0.01806009
Iteration 92, loss = 0.01797198
Iteration 93, loss = 0.01776616
Iteration 94, loss = 0.01770311
Iteration 95, loss = 0.01749216
Iteration 96, loss = 0.01744737
Iteration 97, loss = 0.01733749
Iteration 98, loss = 0.01723481
Iteration 99, loss = 0.01708675
Iteration 100, loss = 0.01693503
Iteration 101, loss = 0.01692164
Iteration 102, loss = 0.01683416
Iteration 103, loss = 0.01670058
Iteration 104, loss = 0.01660082
Iteration 105, loss = 0.01647327
Iteration 106, loss = 0.01646110
Iteration 107, loss = 0.01636524
Iteration 108, loss = 0.01625922
Iteration 109, loss = 0.01618025
Iteration 110, loss = 0.01604293
Iteration 111, loss = 0.01598220
Iteration 112, loss = 0.01606798
Iteration 113, loss = 0.01586310
Iteration 114, loss = 0.01586509
Iteration 115, loss = 0.01567996
Iteration 116, loss = 0.01567375
Iteration 117, loss = 0.01564791
Iteration 118, loss = 0.01552942
Iteration 119, loss = 0.01548446
Iteration 120, loss = 0.01543041
Iteration 121, loss = 0.01531610
Iteration 122, loss = 0.01531071
Iteration 123, loss = 0.01523470
Iteration 124, loss = 0.01525548
Iteration 125, loss = 0.01513187
Iteration 126, loss = 0.01512885
Iteration 127, loss = 0.01503372
Iteration 128, loss = 0.01508506
Iteration 129, loss = 0.01494362
Iteration 130, loss = 0.01490805
Iteration 131, loss = 0.01489570
Iteration 132, loss = 0.01479560
Iteration 133, loss = 0.01468879
Iteration 134, loss = 0.01476554
Iteration 135, loss = 0.01472884
Iteration 136, loss = 0.01462910
Iteration 137, loss = 0.01456660
Iteration 138, loss = 0.01457700
Iteration 139, loss = 0.01454774
Iteration 140, loss = 0.01445253
Iteration 141, loss = 0.01442082
Iteration 142, loss = 0.01448730
Iteration 143, loss = 0.01443092
Iteration 144, loss = 0.01434208
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68077970
Iteration 2, loss = 0.62645007
Iteration 3, loss = 0.53974029
Iteration 4, loss = 0.44049659
Iteration 5, loss = 0.34800461
Iteration 6, loss = 0.26893244
Iteration 7, loss = 0.20819786
Iteration 8, loss = 0.16313765
Iteration 9, loss = 0.13083280
Iteration 10, loss = 0.10732715
Iteration 11, loss = 0.08991124
Iteration 12, loss = 0.07697832
Iteration 13, loss = 0.06713351
Iteration 14, loss = 0.05926556
Iteration 15, loss = 0.05314855
Iteration 16, loss = 0.04822696
Iteration 17, loss = 0.04412957
Iteration 18, loss = 0.04070193
Iteration 19, loss = 0.03800784
Iteration 20, loss = 0.03554810
Iteration 21, loss = 0.03346703
Iteration 22, loss = 0.03195987
Iteration 23, loss = 0.03034696
Iteration 24, loss = 0.02900742
Iteration 25, loss = 0.02797164
Iteration 26, loss = 0.02691375
Iteration 27, loss = 0.02593979
Iteration 28, loss = 0.02502085
Iteration 29, loss = 0.02463355
Iteration 30, loss = 0.02387199
Iteration 31, loss = 0.02347966
Iteration 32, loss = 0.02264156
Iteration 33, loss = 0.02225143
Iteration 34, loss = 0.02176811
Iteration 35, loss = 0.02129052
Iteration 36, loss = 0.02100317
Iteration 37, loss = 0.02067418
Iteration 38, loss = 0.02027941
Iteration 39, loss = 0.02007082
Iteration 40, loss = 0.01982945
Iteration 41, loss = 0.01948520
Iteration 42, loss = 0.01929873
Iteration 43, loss = 0.01906969
Iteration 44, loss = 0.01905130
Iteration 45, loss = 0.01871653
Iteration 46, loss = 0.01864869
Iteration 47, loss = 0.01840483
Iteration 48, loss = 0.01835230
Iteration 49, loss = 0.01811341
Iteration 50, loss = 0.01799581
Iteration 51, loss = 0.01769234
Iteration 52, loss = 0.01764525
Iteration 53, loss = 0.01758608
Iteration 54, loss = 0.01773406
Iteration 55, loss = 0.01762718
Iteration 56, loss = 0.01722705
Iteration 57, loss = 0.01711624
Iteration 58, loss = 0.01728576
Iteration 59, loss = 0.01707671
Iteration 60, loss = 0.01718964
Iteration 61, loss = 0.01701171
Iteration 62, loss = 0.01690357
Iteration 63, loss = 0.01678606
Iteration 64, loss = 0.01703884
Iteration 65, loss = 0.01697368
Iteration 66, loss = 0.01653391
Iteration 67, loss = 0.01676991
Iteration 68, loss = 0.01654447
Iteration 69, loss = 0.01642614
Iteration 70, loss = 0.01657491
Iteration 71, loss = 0.01649920
Iteration 72, loss = 0.01638706
Iteration 73, loss = 0.01636211
Iteration 74, loss = 0.01620334
Iteration 75, loss = 0.01611839
Iteration 76, loss = 0.01622013
Iteration 77, loss = 0.01604244
Iteration 78, loss = 0.01601437
Iteration 79, loss = 0.01601927
Iteration 80, loss = 0.01622108
Iteration 81, loss = 0.01629784
Iteration 82, loss = 0.01599896
Iteration 83, loss = 0.01609545
Iteration 84, loss = 0.01594504
Iteration 85, loss = 0.01593814
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67881151
Iteration 2, loss = 0.60471584
Iteration 3, loss = 0.49092188
Iteration 4, loss = 0.37554028
Iteration 5, loss = 0.27846706
Iteration 6, loss = 0.20632214
Iteration 7, loss = 0.15606845
Iteration 8, loss = 0.12140494
Iteration 9, loss = 0.09751731
Iteration 10, loss = 0.08074352
Iteration 11, loss = 0.06842071
Iteration 12, loss = 0.05917864
Iteration 13, loss = 0.05223344
Iteration 14, loss = 0.04669077
Iteration 15, loss = 0.04231049
Iteration 16, loss = 0.03892984
Iteration 17, loss = 0.03582549
Iteration 18, loss = 0.03352017
Iteration 19, loss = 0.03142966
Iteration 20, loss = 0.02963018
Iteration 21, loss = 0.02825559
Iteration 22, loss = 0.02689145
Iteration 23, loss = 0.02584806
Iteration 24, loss = 0.02492755
Iteration 25, loss = 0.02389247
Iteration 26, loss = 0.02322001
Iteration 27, loss = 0.02248081
Iteration 28, loss = 0.02192914
Iteration 29, loss = 0.02138298
Iteration 30, loss = 0.02091104
Iteration 31, loss = 0.02062797
Iteration 32, loss = 0.02017858
Iteration 33, loss = 0.01983474
Iteration 34, loss = 0.01948492
Iteration 35, loss = 0.01919305
Iteration 36, loss = 0.01899734
Iteration 37, loss = 0.01852153
Iteration 38, loss = 0.01830259
Iteration 39, loss = 0.01817116
Iteration 40, loss = 0.01795707
Iteration 41, loss = 0.01782633
Iteration 42, loss = 0.01781325
Iteration 43, loss = 0.01744891
Iteration 44, loss = 0.01735780
Iteration 45, loss = 0.01746317
Iteration 46, loss = 0.01707142
Iteration 47, loss = 0.01717065
Iteration 48, loss = 0.01681137
Iteration 49, loss = 0.01675178
Iteration 50, loss = 0.01680326
Iteration 51, loss = 0.01652406
Iteration 52, loss = 0.01637541
Iteration 53, loss = 0.01628582
Iteration 54, loss = 0.01643194
Iteration 55, loss = 0.01631005
Iteration 56, loss = 0.01613915
Iteration 57, loss = 0.01592190
Iteration 58, loss = 0.01619224
Iteration 59, loss = 0.01601579
Iteration 60, loss = 0.01608435
Iteration 61, loss = 0.01594136
Iteration 62, loss = 0.01595276
Iteration 63, loss = 0.01589486
Iteration 64, loss = 0.01561644
Iteration 65, loss = 0.01555685
Iteration 66, loss = 0.01555485
Iteration 67, loss = 0.01570948
Iteration 68, loss = 0.01572961
Iteration 69, loss = 0.01559894
Iteration 70, loss = 0.01540700
Iteration 71, loss = 0.01536119
Iteration 72, loss = 0.01555666
Iteration 73, loss = 0.01525030
Iteration 74, loss = 0.01516497
Iteration 75, loss = 0.01522298
Iteration 76, loss = 0.01534127
Iteration 77, loss = 0.01513248
Iteration 78, loss = 0.01516378
Iteration 79, loss = 0.01510524
Iteration 80, loss = 0.01515163
Iteration 81, loss = 0.01543440
Iteration 82, loss = 0.01507641
Iteration 83, loss = 0.01497756
Iteration 84, loss = 0.01496116
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70730038
Iteration 2, loss = 0.67379633
Iteration 3, loss = 0.63460516
Iteration 4, loss = 0.58854388
Iteration 5, loss = 0.53703693
Iteration 6, loss = 0.48403900
Iteration 7, loss = 0.43153844
Iteration 8, loss = 0.38281340
Iteration 9, loss = 0.33749531
Iteration 10, loss = 0.29635101
Iteration 11, loss = 0.25963453
Iteration 12, loss = 0.22748752
Iteration 13, loss = 0.19946887
Iteration 14, loss = 0.17556030
Iteration 15, loss = 0.15506514
Iteration 16, loss = 0.13766444
Iteration 17, loss = 0.12275271
Iteration 18, loss = 0.11023766
Iteration 19, loss = 0.09935877
Iteration 20, loss = 0.08998825
Iteration 21, loss = 0.08209911
Iteration 22, loss = 0.07523049
Iteration 23, loss = 0.06936918
Iteration 24, loss = 0.06402572
Iteration 25, loss = 0.05949825
Iteration 26, loss = 0.05558009
Iteration 27, loss = 0.05185303
Iteration 28, loss = 0.04869125
Iteration 29, loss = 0.04588104
Iteration 30, loss = 0.04330692
Iteration 31, loss = 0.04093174
Iteration 32, loss = 0.03885532
Iteration 33, loss = 0.03685541
Iteration 34, loss = 0.03506147
Iteration 35, loss = 0.03360672
Iteration 36, loss = 0.03214542
Iteration 37, loss = 0.03079075
Iteration 38, loss = 0.02966595
Iteration 39, loss = 0.02860878
Iteration 40, loss = 0.02757453
Iteration 41, loss = 0.02687147
Iteration 42, loss = 0.02606825
Iteration 43, loss = 0.02507781
Iteration 44, loss = 0.02421480
Iteration 45, loss = 0.02339980
Iteration 46, loss = 0.02265707
Iteration 47, loss = 0.02203629
Iteration 48, loss = 0.02158037
Iteration 49, loss = 0.02109378
Iteration 50, loss = 0.02055299
Iteration 51, loss = 0.01999697
Iteration 52, loss = 0.01948078
Iteration 53, loss = 0.01887964
Iteration 54, loss = 0.01854128
Iteration 55, loss = 0.01819195
Iteration 56, loss = 0.01782996
Iteration 57, loss = 0.01752617
Iteration 58, loss = 0.01723051
Iteration 59, loss = 0.01699698
Iteration 60, loss = 0.01681378
Iteration 61, loss = 0.01647764
Iteration 62, loss = 0.01615323
Iteration 63, loss = 0.01588589
Iteration 64, loss = 0.01579292
Iteration 65, loss = 0.01551899
Iteration 66, loss = 0.01523655
Iteration 67, loss = 0.01495055
Iteration 68, loss = 0.01471839
Iteration 69, loss = 0.01447486
Iteration 70, loss = 0.01433101
Iteration 71, loss = 0.01409185
Iteration 72, loss = 0.01393444
Iteration 73, loss = 0.01385223
Iteration 74, loss = 0.01366418
Iteration 75, loss = 0.01353938
Iteration 76, loss = 0.01351943
Iteration 77, loss = 0.01331857
Iteration 78, loss = 0.01305233
Iteration 79, loss = 0.01334940
Iteration 80, loss = 0.01315975
Iteration 81, loss = 0.01299288
Iteration 82, loss = 0.01293147
Iteration 83, loss = 0.01271125
Iteration 84, loss = 0.01255138
Iteration 85, loss = 0.01240159
Iteration 86, loss = 0.01225605
Iteration 87, loss = 0.01216298
Iteration 88, loss = 0.01208133
Iteration 89, loss = 0.01191745
Iteration 90, loss = 0.01184610
Iteration 91, loss = 0.01168327
Iteration 92, loss = 0.01157245
Iteration 93, loss = 0.01148198
Iteration 94, loss = 0.01142326
Iteration 95, loss = 0.01135060
Iteration 96, loss = 0.01124467
Iteration 97, loss = 0.01118224
Iteration 98, loss = 0.01114813
Iteration 99, loss = 0.01125844
Iteration 100, loss = 0.01126448
Iteration 101, loss = 0.01134066
Iteration 102, loss = 0.01134665
Iteration 103, loss = 0.01131392
Iteration 104, loss = 0.01113622
Iteration 105, loss = 0.01102126
Iteration 106, loss = 0.01079212
Iteration 107, loss = 0.01113362
Iteration 108, loss = 0.01093580
Iteration 109, loss = 0.01094473
Iteration 110, loss = 0.01076842
Iteration 111, loss = 0.01088337
Iteration 112, loss = 0.01080589
Iteration 113, loss = 0.01061899
Iteration 114, loss = 0.01052401
Iteration 115, loss = 0.01033824
Iteration 116, loss = 0.01026337
Iteration 117, loss = 0.01014907
Iteration 118, loss = 0.01014000
Iteration 119, loss = 0.01014251
Iteration 120, loss = 0.01007131
Iteration 121, loss = 0.01007384
Iteration 122, loss = 0.01005167
Iteration 123, loss = 0.01000159
Iteration 124, loss = 0.01003951
Iteration 125, loss = 0.01000523
Iteration 126, loss = 0.00993602
Iteration 127, loss = 0.00986995
Iteration 128, loss = 0.00978933
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67481812
Iteration 2, loss = 0.60944773
Iteration 3, loss = 0.51246915
Iteration 4, loss = 0.40372553
Iteration 5, loss = 0.30693802
Iteration 6, loss = 0.23043716
Iteration 7, loss = 0.17551961
Iteration 8, loss = 0.13711892
Iteration 9, loss = 0.11021026
Iteration 10, loss = 0.09100707
Iteration 11, loss = 0.07710745
Iteration 12, loss = 0.06679837
Iteration 13, loss = 0.05889033
Iteration 14, loss = 0.05268207
Iteration 15, loss = 0.04797221
Iteration 16, loss = 0.04371447
Iteration 17, loss = 0.04060279
Iteration 18, loss = 0.03783816
Iteration 19, loss = 0.03557522
Iteration 20, loss = 0.03372529
Iteration 21, loss = 0.03212077
Iteration 22, loss = 0.03080614
Iteration 23, loss = 0.02929345
Iteration 24, loss = 0.02843050
Iteration 25, loss = 0.02742021
Iteration 26, loss = 0.02660771
Iteration 27, loss = 0.02564386
Iteration 28, loss = 0.02511803
Iteration 29, loss = 0.02475634
Iteration 30, loss = 0.02428469
Iteration 31, loss = 0.02381411
Iteration 32, loss = 0.02326872
Iteration 33, loss = 0.02270701
Iteration 34, loss = 0.02233007
Iteration 35, loss = 0.02212483
Iteration 36, loss = 0.02190736
Iteration 37, loss = 0.02156335
Iteration 38, loss = 0.02131557
Iteration 39, loss = 0.02115242
Iteration 40, loss = 0.02075892
Iteration 41, loss = 0.02071396
Iteration 42, loss = 0.02037977
Iteration 43, loss = 0.02036113
Iteration 44, loss = 0.02014826
Iteration 45, loss = 0.01965458
Iteration 46, loss = 0.01968524
Iteration 47, loss = 0.01978089
Iteration 48, loss = 0.01931617
Iteration 49, loss = 0.01956305
Iteration 50, loss = 0.01926994
Iteration 51, loss = 0.01918446
Iteration 52, loss = 0.01894436
Iteration 53, loss = 0.01884508
Iteration 54, loss = 0.01926635
Iteration 55, loss = 0.01901315
Iteration 56, loss = 0.01878698
Iteration 57, loss = 0.01873629
Iteration 58, loss = 0.01882692
Iteration 59, loss = 0.01857430
Iteration 60, loss = 0.01835275
Iteration 61, loss = 0.01825571
Iteration 62, loss = 0.01824136
Iteration 63, loss = 0.01816169
Iteration 64, loss = 0.01807276
Iteration 65, loss = 0.01827681
Iteration 66, loss = 0.01817679
Iteration 67, loss = 0.01846776
Iteration 68, loss = 0.01860488
Iteration 69, loss = 0.01826311
Iteration 70, loss = 0.01782790
Iteration 71, loss = 0.01799850
Iteration 72, loss = 0.01796066
Iteration 73, loss = 0.01777347
Iteration 74, loss = 0.01808253
Iteration 75, loss = 0.01793015
Iteration 76, loss = 0.01787683
Iteration 77, loss = 0.01766654
Iteration 78, loss = 0.01773671
Iteration 79, loss = 0.01739138
Iteration 80, loss = 0.01751027
Iteration 81, loss = 0.01761518
Iteration 82, loss = 0.01749088
Iteration 83, loss = 0.01763661
Iteration 84, loss = 0.01760661
Iteration 85, loss = 0.01766805
Iteration 86, loss = 0.01741264
Iteration 87, loss = 0.01749058
Iteration 88, loss = 0.01765875
Iteration 89, loss = 0.01760462
Iteration 90, loss = 0.01744820
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69768917
Iteration 2, loss = 0.66939736
Iteration 3, loss = 0.63012309
Iteration 4, loss = 0.57950830
Iteration 5, loss = 0.51836263
Iteration 6, loss = 0.45305438
Iteration 7, loss = 0.38931485
Iteration 8, loss = 0.33094850
Iteration 9, loss = 0.27953615
Iteration 10, loss = 0.23581082
Iteration 11, loss = 0.19920108
Iteration 12, loss = 0.16930799
Iteration 13, loss = 0.14476839
Iteration 14, loss = 0.12477232
Iteration 15, loss = 0.10864713
Iteration 16, loss = 0.09554065
Iteration 17, loss = 0.08452260
Iteration 18, loss = 0.07557039
Iteration 19, loss = 0.06785884
Iteration 20, loss = 0.06160065
Iteration 21, loss = 0.05610831
Iteration 22, loss = 0.05160177
Iteration 23, loss = 0.04748210
Iteration 24, loss = 0.04404928
Iteration 25, loss = 0.04102311
Iteration 26, loss = 0.03838573
Iteration 27, loss = 0.03605829
Iteration 28, loss = 0.03400042
Iteration 29, loss = 0.03200238
Iteration 30, loss = 0.03039815
Iteration 31, loss = 0.02896056
Iteration 32, loss = 0.02754640
Iteration 33, loss = 0.02633354
Iteration 34, loss = 0.02523279
Iteration 35, loss = 0.02422471
Iteration 36, loss = 0.02328923
Iteration 37, loss = 0.02249596
Iteration 38, loss = 0.02175374
Iteration 39, loss = 0.02110087
Iteration 40, loss = 0.02037919
Iteration 41, loss = 0.01971164
Iteration 42, loss = 0.01914967
Iteration 43, loss = 0.01864385
Iteration 44, loss = 0.01817840
Iteration 45, loss = 0.01764841
Iteration 46, loss = 0.01725826
Iteration 47, loss = 0.01683168
Iteration 48, loss = 0.01642989
Iteration 49, loss = 0.01610844
Iteration 50, loss = 0.01577866
Iteration 51, loss = 0.01546017
Iteration 52, loss = 0.01516713
Iteration 53, loss = 0.01485362
Iteration 54, loss = 0.01468798
Iteration 55, loss = 0.01444890
Iteration 56, loss = 0.01414539
Iteration 57, loss = 0.01400126
Iteration 58, loss = 0.01376925
Iteration 59, loss = 0.01360815
Iteration 60, loss = 0.01346313
Iteration 61, loss = 0.01315897
Iteration 62, loss = 0.01298849
Iteration 63, loss = 0.01286908
Iteration 64, loss = 0.01270796
Iteration 65, loss = 0.01253201
Iteration 66, loss = 0.01246268
Iteration 67, loss = 0.01227520
Iteration 68, loss = 0.01214145
Iteration 69, loss = 0.01205581
Iteration 70, loss = 0.01189125
Iteration 71, loss = 0.01175055
Iteration 72, loss = 0.01173596
Iteration 73, loss = 0.01161012
Iteration 74, loss = 0.01140126
Iteration 75, loss = 0.01140742
Iteration 76, loss = 0.01132252
Iteration 77, loss = 0.01130261
Iteration 78, loss = 0.01109329
Iteration 79, loss = 0.01100923
Iteration 80, loss = 0.01100561
Iteration 81, loss = 0.01088578
Iteration 82, loss = 0.01088418
Iteration 83, loss = 0.01076105
Iteration 84, loss = 0.01070174
Iteration 85, loss = 0.01063537
Iteration 86, loss = 0.01054038
Iteration 87, loss = 0.01048060
Iteration 88, loss = 0.01045005
Iteration 89, loss = 0.01037186
Iteration 90, loss = 0.01032861
Iteration 91, loss = 0.01029132
Iteration 92, loss = 0.01024710
Iteration 93, loss = 0.01017148
Iteration 94, loss = 0.01015864
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67463265
Iteration 2, loss = 0.59907569
Iteration 3, loss = 0.48554560
Iteration 4, loss = 0.37014664
Iteration 5, loss = 0.27369514
Iteration 6, loss = 0.20249122
Iteration 7, loss = 0.15303595
Iteration 8, loss = 0.11952594
Iteration 9, loss = 0.09658892
Iteration 10, loss = 0.08036021
Iteration 11, loss = 0.06862693
Iteration 12, loss = 0.05993179
Iteration 13, loss = 0.05314934
Iteration 14, loss = 0.04780400
Iteration 15, loss = 0.04402055
Iteration 16, loss = 0.04023130
Iteration 17, loss = 0.03750082
Iteration 18, loss = 0.03512388
Iteration 19, loss = 0.03330066
Iteration 20, loss = 0.03173952
Iteration 21, loss = 0.03031566
Iteration 22, loss = 0.02940068
Iteration 23, loss = 0.02825736
Iteration 24, loss = 0.02739909
Iteration 25, loss = 0.02639008
Iteration 26, loss = 0.02571548
Iteration 27, loss = 0.02507565
Iteration 28, loss = 0.02467545
Iteration 29, loss = 0.02401006
Iteration 30, loss = 0.02371186
Iteration 31, loss = 0.02336699
Iteration 32, loss = 0.02294242
Iteration 33, loss = 0.02247937
Iteration 34, loss = 0.02235663
Iteration 35, loss = 0.02206917
Iteration 36, loss = 0.02165237
Iteration 37, loss = 0.02140694
Iteration 38, loss = 0.02117368
Iteration 39, loss = 0.02113170
Iteration 40, loss = 0.02095165
Iteration 41, loss = 0.02071103
Iteration 42, loss = 0.02059720
Iteration 43, loss = 0.02013996
Iteration 44, loss = 0.02041040
Iteration 45, loss = 0.02028326
Iteration 46, loss = 0.01997069
Iteration 47, loss = 0.02011871
Iteration 48, loss = 0.01978479
Iteration 49, loss = 0.02000835
Iteration 50, loss = 0.01970786
Iteration 51, loss = 0.01959362
Iteration 52, loss = 0.01973194
Iteration 53, loss = 0.01953743
Iteration 54, loss = 0.01938131
Iteration 55, loss = 0.01957750
Iteration 56, loss = 0.01901633
Iteration 57, loss = 0.01941058
Iteration 58, loss = 0.01908940
Iteration 59, loss = 0.01884663
Iteration 60, loss = 0.01917655
Iteration 61, loss = 0.01893465
Iteration 62, loss = 0.01887249
Iteration 63, loss = 0.01897953
Iteration 64, loss = 0.01886631
Iteration 65, loss = 0.01868023
Iteration 66, loss = 0.01845929
Iteration 67, loss = 0.01844835
Iteration 68, loss = 0.01885607
Iteration 69, loss = 0.01848190
Iteration 70, loss = 0.01855795
Iteration 71, loss = 0.01858445
Iteration 72, loss = 0.01872950
Iteration 73, loss = 0.01841341
Iteration 74, loss = 0.01858579
Iteration 75, loss = 0.01836525
Iteration 76, loss = 0.01834258
Iteration 77, loss = 0.01827901
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72035992
Iteration 2, loss = 0.67943222
Iteration 3, loss = 0.62374653
Iteration 4, loss = 0.55709115
Iteration 5, loss = 0.48312310
Iteration 6, loss = 0.41000051
Iteration 7, loss = 0.34188053
Iteration 8, loss = 0.28274151
Iteration 9, loss = 0.23269982
Iteration 10, loss = 0.19206191
Iteration 11, loss = 0.15973751
Iteration 12, loss = 0.13413482
Iteration 13, loss = 0.11406299
Iteration 14, loss = 0.09804817
Iteration 15, loss = 0.08530377
Iteration 16, loss = 0.07519497
Iteration 17, loss = 0.06679287
Iteration 18, loss = 0.05977245
Iteration 19, loss = 0.05410980
Iteration 20, loss = 0.04927478
Iteration 21, loss = 0.04511105
Iteration 22, loss = 0.04169157
Iteration 23, loss = 0.03864700
Iteration 24, loss = 0.03600624
Iteration 25, loss = 0.03375737
Iteration 26, loss = 0.03174894
Iteration 27, loss = 0.03003223
Iteration 28, loss = 0.02849509
Iteration 29, loss = 0.02707456
Iteration 30, loss = 0.02582967
Iteration 31, loss = 0.02464224
Iteration 32, loss = 0.02360262
Iteration 33, loss = 0.02276942
Iteration 34, loss = 0.02188987
Iteration 35, loss = 0.02110447
Iteration 36, loss = 0.02052916
Iteration 37, loss = 0.01978553
Iteration 38, loss = 0.01918595
Iteration 39, loss = 0.01859915
Iteration 40, loss = 0.01816834
Iteration 41, loss = 0.01772020
Iteration 42, loss = 0.01726958
Iteration 43, loss = 0.01684355
Iteration 44, loss = 0.01649672
Iteration 45, loss = 0.01617258
Iteration 46, loss = 0.01586102
Iteration 47, loss = 0.01553370
Iteration 48, loss = 0.01524604
Iteration 49, loss = 0.01503475
Iteration 50, loss = 0.01476830
Iteration 51, loss = 0.01459809
Iteration 52, loss = 0.01428266
Iteration 53, loss = 0.01407777
Iteration 54, loss = 0.01396499
Iteration 55, loss = 0.01372311
Iteration 56, loss = 0.01362893
Iteration 57, loss = 0.01336101
Iteration 58, loss = 0.01330192
Iteration 59, loss = 0.01313486
Iteration 60, loss = 0.01290140
Iteration 61, loss = 0.01285362
Iteration 62, loss = 0.01269668
Iteration 63, loss = 0.01256202
Iteration 64, loss = 0.01239294
Iteration 65, loss = 0.01236090
Iteration 66, loss = 0.01221343
Iteration 67, loss = 0.01216190
Iteration 68, loss = 0.01205983
Iteration 69, loss = 0.01188175
Iteration 70, loss = 0.01196335
Iteration 71, loss = 0.01189630
Iteration 72, loss = 0.01165008
Iteration 73, loss = 0.01173265
Iteration 74, loss = 0.01151977
Iteration 75, loss = 0.01147055
Iteration 76, loss = 0.01141333
Iteration 77, loss = 0.01140376
Iteration 78, loss = 0.01133449
Iteration 79, loss = 0.01132713
Iteration 80, loss = 0.01116576
Iteration 81, loss = 0.01119648
Iteration 82, loss = 0.01114673
Iteration 83, loss = 0.01114155
Iteration 84, loss = 0.01099244
Iteration 85, loss = 0.01097695
Iteration 86, loss = 0.01086864
Iteration 87, loss = 0.01084656
Iteration 88, loss = 0.01079864
Iteration 89, loss = 0.01069458
Iteration 90, loss = 0.01077759
Iteration 91, loss = 0.01065687
Iteration 92, loss = 0.01072064
Iteration 93, loss = 0.01055658
Iteration 94, loss = 0.01057407
Iteration 95, loss = 0.01047168
Iteration 96, loss = 0.01042489
Iteration 97, loss = 0.01036477
Iteration 98, loss = 0.01032582
Iteration 99, loss = 0.01046171
Iteration 100, loss = 0.01037588
Iteration 101, loss = 0.01034090
Iteration 102, loss = 0.01040153
Iteration 103, loss = 0.01030219
Iteration 104, loss = 0.01020339
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69695314
Iteration 2, loss = 0.63334951
Iteration 3, loss = 0.53670959
Iteration 4, loss = 0.42130276
Iteration 5, loss = 0.31695026
Iteration 6, loss = 0.23598826
Iteration 7, loss = 0.17809962
Iteration 8, loss = 0.13862561
Iteration 9, loss = 0.11097143
Iteration 10, loss = 0.09143799
Iteration 11, loss = 0.07691360
Iteration 12, loss = 0.06630424
Iteration 13, loss = 0.05845031
Iteration 14, loss = 0.05211218
Iteration 15, loss = 0.04711798
Iteration 16, loss = 0.04308954
Iteration 17, loss = 0.03998234
Iteration 18, loss = 0.03724307
Iteration 19, loss = 0.03478479
Iteration 20, loss = 0.03292795
Iteration 21, loss = 0.03127488
Iteration 22, loss = 0.02982571
Iteration 23, loss = 0.02869457
Iteration 24, loss = 0.02757150
Iteration 25, loss = 0.02689904
Iteration 26, loss = 0.02584317
Iteration 27, loss = 0.02517794
Iteration 28, loss = 0.02447846
Iteration 29, loss = 0.02397063
Iteration 30, loss = 0.02334653
Iteration 31, loss = 0.02285746
Iteration 32, loss = 0.02246290
Iteration 33, loss = 0.02216879
Iteration 34, loss = 0.02189435
Iteration 35, loss = 0.02144488
Iteration 36, loss = 0.02109348
Iteration 37, loss = 0.02087560
Iteration 38, loss = 0.02072876
Iteration 39, loss = 0.02028820
Iteration 40, loss = 0.02012747
Iteration 41, loss = 0.02012715
Iteration 42, loss = 0.01968698
Iteration 43, loss = 0.01960766
Iteration 44, loss = 0.01938497
Iteration 45, loss = 0.01943181
Iteration 46, loss = 0.01934274
Iteration 47, loss = 0.01904476
Iteration 48, loss = 0.01898393
Iteration 49, loss = 0.01903652
Iteration 50, loss = 0.01884624
Iteration 51, loss = 0.01865874
Iteration 52, loss = 0.01857775
Iteration 53, loss = 0.01839924
Iteration 54, loss = 0.01821253
Iteration 55, loss = 0.01837121
Iteration 56, loss = 0.01833094
Iteration 57, loss = 0.01824609
Iteration 58, loss = 0.01798642
Iteration 59, loss = 0.01805975
Iteration 60, loss = 0.01792487
Iteration 61, loss = 0.01802013
Iteration 62, loss = 0.01802960
Iteration 63, loss = 0.01803877
Iteration 64, loss = 0.01772839
Iteration 65, loss = 0.01751943
Iteration 66, loss = 0.01764588
Iteration 67, loss = 0.01763304
Iteration 68, loss = 0.01745529
Iteration 69, loss = 0.01724669
Iteration 70, loss = 0.01758730
Iteration 71, loss = 0.01756542
Iteration 72, loss = 0.01774050
Iteration 73, loss = 0.01777441
Iteration 74, loss = 0.01738300
Iteration 75, loss = 0.01729179
Iteration 76, loss = 0.01705193
Iteration 77, loss = 0.01713074
Iteration 78, loss = 0.01712969
Iteration 79, loss = 0.01705996
Iteration 80, loss = 0.01705951
Iteration 81, loss = 0.01711966
Iteration 82, loss = 0.01716939
Iteration 83, loss = 0.01698139
Iteration 84, loss = 0.01727018
Iteration 85, loss = 0.01708661
Iteration 86, loss = 0.01690850
Iteration 87, loss = 0.01697238
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69401083
Iteration 2, loss = 0.63060414
Iteration 3, loss = 0.52581750
Iteration 4, loss = 0.40417909
Iteration 5, loss = 0.29837068
Iteration 6, loss = 0.21973733
Iteration 7, loss = 0.16480908
Iteration 8, loss = 0.12781812
Iteration 9, loss = 0.10231347
Iteration 10, loss = 0.08426318
Iteration 11, loss = 0.07175619
Iteration 12, loss = 0.06198398
Iteration 13, loss = 0.05482810
Iteration 14, loss = 0.04922683
Iteration 15, loss = 0.04474525
Iteration 16, loss = 0.04120224
Iteration 17, loss = 0.03832806
Iteration 18, loss = 0.03566218
Iteration 19, loss = 0.03371702
Iteration 20, loss = 0.03198801
Iteration 21, loss = 0.03034117
Iteration 22, loss = 0.02901877
Iteration 23, loss = 0.02783818
Iteration 24, loss = 0.02724528
Iteration 25, loss = 0.02616644
Iteration 26, loss = 0.02525337
Iteration 27, loss = 0.02465991
Iteration 28, loss = 0.02423853
Iteration 29, loss = 0.02340160
Iteration 30, loss = 0.02295640
Iteration 31, loss = 0.02240912
Iteration 32, loss = 0.02230797
Iteration 33, loss = 0.02180052
Iteration 34, loss = 0.02134678
Iteration 35, loss = 0.02114477
Iteration 36, loss = 0.02068767
Iteration 37, loss = 0.02055192
Iteration 38, loss = 0.02023156
Iteration 39, loss = 0.02021523
Iteration 40, loss = 0.01987222
Iteration 41, loss = 0.01968943
Iteration 42, loss = 0.01940097
Iteration 43, loss = 0.01937670
Iteration 44, loss = 0.01934405
Iteration 45, loss = 0.01908866
Iteration 46, loss = 0.01902012
Iteration 47, loss = 0.01894018
Iteration 48, loss = 0.01862553
Iteration 49, loss = 0.01857512
Iteration 50, loss = 0.01850714
Iteration 51, loss = 0.01838611
Iteration 52, loss = 0.01838414
Iteration 53, loss = 0.01826244
Iteration 54, loss = 0.01824093
Iteration 55, loss = 0.01838761
Iteration 56, loss = 0.01805568
Iteration 57, loss = 0.01791575
Iteration 58, loss = 0.01819573
Iteration 59, loss = 0.01775816
Iteration 60, loss = 0.01784511
Iteration 61, loss = 0.01782547
Iteration 62, loss = 0.01766641
Iteration 63, loss = 0.01777344
Iteration 64, loss = 0.01739428
Iteration 65, loss = 0.01752288
Iteration 66, loss = 0.01740247
Iteration 67, loss = 0.01754859
Iteration 68, loss = 0.01754743
Iteration 69, loss = 0.01766569
Iteration 70, loss = 0.01725486
Iteration 71, loss = 0.01731770
Iteration 72, loss = 0.01727801
Iteration 73, loss = 0.01736053
Iteration 74, loss = 0.01722414
Iteration 75, loss = 0.01711610
Iteration 76, loss = 0.01701727
Iteration 77, loss = 0.01720706
Iteration 78, loss = 0.01712720
Iteration 79, loss = 0.01716162
Iteration 80, loss = 0.01727433
Iteration 81, loss = 0.01680725
Iteration 82, loss = 0.01713320
Iteration 83, loss = 0.01729013
Iteration 84, loss = 0.01695458
Iteration 85, loss = 0.01701731
Iteration 86, loss = 0.01690402
Iteration 87, loss = 0.01668783
Iteration 88, loss = 0.01686667
Iteration 89, loss = 0.01690415
Iteration 90, loss = 0.01682637
Iteration 91, loss = 0.01677495
Iteration 92, loss = 0.01691548
Iteration 93, loss = 0.01682730
Iteration 94, loss = 0.01687318
Iteration 95, loss = 0.01653820
Iteration 96, loss = 0.01673239
Iteration 97, loss = 0.01683764
Iteration 98, loss = 0.01682618
Iteration 99, loss = 0.01676407
Iteration 100, loss = 0.01671260
Iteration 101, loss = 0.01650782
Iteration 102, loss = 0.01664787
Iteration 103, loss = 0.01650516
Iteration 104, loss = 0.01677718
Iteration 105, loss = 0.01663376
Iteration 106, loss = 0.01652839
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68703310
Iteration 2, loss = 0.66470602
Iteration 3, loss = 0.64246163
Iteration 4, loss = 0.61634907
Iteration 5, loss = 0.58503262
Iteration 6, loss = 0.55024872
Iteration 7, loss = 0.51243352
Iteration 8, loss = 0.47334112
Iteration 9, loss = 0.43436649
Iteration 10, loss = 0.39669347
Iteration 11, loss = 0.36064638
Iteration 12, loss = 0.32710938
Iteration 13, loss = 0.29594107
Iteration 14, loss = 0.26727507
Iteration 15, loss = 0.24135318
Iteration 16, loss = 0.21781699
Iteration 17, loss = 0.19682712
Iteration 18, loss = 0.17813538
Iteration 19, loss = 0.16149222
Iteration 20, loss = 0.14666332
Iteration 21, loss = 0.13360372
Iteration 22, loss = 0.12209487
Iteration 23, loss = 0.11186995
Iteration 24, loss = 0.10281640
Iteration 25, loss = 0.09477472
Iteration 26, loss = 0.08763304
Iteration 27, loss = 0.08130962
Iteration 28, loss = 0.07564319
Iteration 29, loss = 0.07050748
Iteration 30, loss = 0.06600566
Iteration 31, loss = 0.06187416
Iteration 32, loss = 0.05815627
Iteration 33, loss = 0.05485871
Iteration 34, loss = 0.05177555
Iteration 35, loss = 0.04900006
Iteration 36, loss = 0.04647988
Iteration 37, loss = 0.04419646
Iteration 38, loss = 0.04207663
Iteration 39, loss = 0.04007996
Iteration 40, loss = 0.03836122
Iteration 41, loss = 0.03667924
Iteration 42, loss = 0.03510090
Iteration 43, loss = 0.03372843
Iteration 44, loss = 0.03236923
Iteration 45, loss = 0.03116244
Iteration 46, loss = 0.03002042
Iteration 47, loss = 0.02896958
Iteration 48, loss = 0.02796094
Iteration 49, loss = 0.02701506
Iteration 50, loss = 0.02617532
Iteration 51, loss = 0.02532010
Iteration 52, loss = 0.02456766
Iteration 53, loss = 0.02388506
Iteration 54, loss = 0.02314511
Iteration 55, loss = 0.02254125
Iteration 56, loss = 0.02189691
Iteration 57, loss = 0.02133092
Iteration 58, loss = 0.02080600
Iteration 59, loss = 0.02028998
Iteration 60, loss = 0.01976570
Iteration 61, loss = 0.01930097
Iteration 62, loss = 0.01887192
Iteration 63, loss = 0.01844198
Iteration 64, loss = 0.01805355
Iteration 65, loss = 0.01771263
Iteration 66, loss = 0.01734192
Iteration 67, loss = 0.01697953
Iteration 68, loss = 0.01664236
Iteration 69, loss = 0.01634435
Iteration 70, loss = 0.01605270
Iteration 71, loss = 0.01574005
Iteration 72, loss = 0.01546396
Iteration 73, loss = 0.01523768
Iteration 74, loss = 0.01492258
Iteration 75, loss = 0.01467104
Iteration 76, loss = 0.01449022
Iteration 77, loss = 0.01419495
Iteration 78, loss = 0.01403315
Iteration 79, loss = 0.01381721
Iteration 80, loss = 0.01361622
Iteration 81, loss = 0.01339643
Iteration 82, loss = 0.01324214
Iteration 83, loss = 0.01302207
Iteration 84, loss = 0.01289631
Iteration 85, loss = 0.01274237
Iteration 86, loss = 0.01254404
Iteration 87, loss = 0.01240562
Iteration 88, loss = 0.01227115
Iteration 89, loss = 0.01214836
Iteration 90, loss = 0.01200310
Iteration 91, loss = 0.01182230
Iteration 92, loss = 0.01172291
Iteration 93, loss = 0.01157472
Iteration 94, loss = 0.01148706
Iteration 95, loss = 0.01137017
Iteration 96, loss = 0.01122935
Iteration 97, loss = 0.01110476
Iteration 98, loss = 0.01099139
Iteration 99, loss = 0.01092349
Iteration 100, loss = 0.01078269
Iteration 101, loss = 0.01074449
Iteration 102, loss = 0.01062476
Iteration 103, loss = 0.01055838
Iteration 104, loss = 0.01044112
Iteration 105, loss = 0.01035842
Iteration 106, loss = 0.01033135
Iteration 107, loss = 0.01020910
Iteration 108, loss = 0.01009559
Iteration 109, loss = 0.01003196
Iteration 110, loss = 0.00999884
Iteration 111, loss = 0.00987775
Iteration 112, loss = 0.00981940
Iteration 113, loss = 0.00974584
Iteration 114, loss = 0.00971157
Iteration 115, loss = 0.00962998
Iteration 116, loss = 0.00957398
Iteration 117, loss = 0.00952287
Iteration 118, loss = 0.00941292
Iteration 119, loss = 0.00938504
Iteration 120, loss = 0.00932627
Iteration 121, loss = 0.00923694
Iteration 122, loss = 0.00923425
Iteration 123, loss = 0.00915122
Iteration 124, loss = 0.00912313
Iteration 125, loss = 0.00903285
Iteration 126, loss = 0.00899841
Iteration 127, loss = 0.00892586
Iteration 128, loss = 0.00887962
Iteration 129, loss = 0.00886211
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67866553
Iteration 2, loss = 0.63486248
Iteration 3, loss = 0.56815225
Iteration 4, loss = 0.48431997
Iteration 5, loss = 0.39892052
Iteration 6, loss = 0.32149991
Iteration 7, loss = 0.25675395
Iteration 8, loss = 0.20556093
Iteration 9, loss = 0.16583051
Iteration 10, loss = 0.13563503
Iteration 11, loss = 0.11258760
Iteration 12, loss = 0.09519991
Iteration 13, loss = 0.08146821
Iteration 14, loss = 0.07087330
Iteration 15, loss = 0.06247342
Iteration 16, loss = 0.05546477
Iteration 17, loss = 0.04981827
Iteration 18, loss = 0.04512202
Iteration 19, loss = 0.04125011
Iteration 20, loss = 0.03794591
Iteration 21, loss = 0.03514700
Iteration 22, loss = 0.03275456
Iteration 23, loss = 0.03071447
Iteration 24, loss = 0.02898168
Iteration 25, loss = 0.02732681
Iteration 26, loss = 0.02589378
Iteration 27, loss = 0.02454397
Iteration 28, loss = 0.02370822
Iteration 29, loss = 0.02253285
Iteration 30, loss = 0.02158443
Iteration 31, loss = 0.02065002
Iteration 32, loss = 0.02000640
Iteration 33, loss = 0.01926952
Iteration 34, loss = 0.01876817
Iteration 35, loss = 0.01810369
Iteration 36, loss = 0.01763016
Iteration 37, loss = 0.01720292
Iteration 38, loss = 0.01674112
Iteration 39, loss = 0.01644687
Iteration 40, loss = 0.01614539
Iteration 41, loss = 0.01577985
Iteration 42, loss = 0.01552790
Iteration 43, loss = 0.01508910
Iteration 44, loss = 0.01497017
Iteration 45, loss = 0.01468208
Iteration 46, loss = 0.01435718
Iteration 47, loss = 0.01408542
Iteration 48, loss = 0.01388750
Iteration 49, loss = 0.01382306
Iteration 50, loss = 0.01363721
Iteration 51, loss = 0.01333007
Iteration 52, loss = 0.01315595
Iteration 53, loss = 0.01310284
Iteration 54, loss = 0.01303333
Iteration 55, loss = 0.01281672
Iteration 56, loss = 0.01262797
Iteration 57, loss = 0.01266995
Iteration 58, loss = 0.01253282
Iteration 59, loss = 0.01242350
Iteration 60, loss = 0.01225357
Iteration 61, loss = 0.01219971
Iteration 62, loss = 0.01201625
Iteration 63, loss = 0.01190872
Iteration 64, loss = 0.01189354
Iteration 65, loss = 0.01191045
Iteration 66, loss = 0.01179007
Iteration 67, loss = 0.01156539
Iteration 68, loss = 0.01173040
Iteration 69, loss = 0.01140010
Iteration 70, loss = 0.01154568
Iteration 71, loss = 0.01130681
Iteration 72, loss = 0.01130651
Iteration 73, loss = 0.01131204
Iteration 74, loss = 0.01109701
Iteration 75, loss = 0.01103430
Iteration 76, loss = 0.01108407
Iteration 77, loss = 0.01094883
Iteration 78, loss = 0.01103895
Iteration 79, loss = 0.01095682
Iteration 80, loss = 0.01096640
Iteration 81, loss = 0.01089070
Iteration 82, loss = 0.01072006
Iteration 83, loss = 0.01073530
Iteration 84, loss = 0.01079012
Iteration 85, loss = 0.01072754
Iteration 86, loss = 0.01069731
Iteration 87, loss = 0.01062978
Iteration 88, loss = 0.01056133
Iteration 89, loss = 0.01054517
Iteration 90, loss = 0.01046229
Iteration 91, loss = 0.01039060
Iteration 92, loss = 0.01047666
Iteration 93, loss = 0.01068896
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70109955
Iteration 2, loss = 0.62613694
Iteration 3, loss = 0.52078628
Iteration 4, loss = 0.40310459
Iteration 5, loss = 0.30037879
Iteration 6, loss = 0.22199462
Iteration 7, loss = 0.16655553
Iteration 8, loss = 0.12895246
Iteration 9, loss = 0.10292453
Iteration 10, loss = 0.08462605
Iteration 11, loss = 0.07155195
Iteration 12, loss = 0.06195671
Iteration 13, loss = 0.05457830
Iteration 14, loss = 0.04883678
Iteration 15, loss = 0.04426643
Iteration 16, loss = 0.04068155
Iteration 17, loss = 0.03776189
Iteration 18, loss = 0.03529112
Iteration 19, loss = 0.03335671
Iteration 20, loss = 0.03147564
Iteration 21, loss = 0.03010589
Iteration 22, loss = 0.02886099
Iteration 23, loss = 0.02760484
Iteration 24, loss = 0.02671384
Iteration 25, loss = 0.02605331
Iteration 26, loss = 0.02529051
Iteration 27, loss = 0.02467788
Iteration 28, loss = 0.02403277
Iteration 29, loss = 0.02362933
Iteration 30, loss = 0.02310033
Iteration 31, loss = 0.02255019
Iteration 32, loss = 0.02222049
Iteration 33, loss = 0.02188183
Iteration 34, loss = 0.02170665
Iteration 35, loss = 0.02123474
Iteration 36, loss = 0.02104802
Iteration 37, loss = 0.02080425
Iteration 38, loss = 0.02046579
Iteration 39, loss = 0.02044135
Iteration 40, loss = 0.02034973
Iteration 41, loss = 0.01989266
Iteration 42, loss = 0.01995622
Iteration 43, loss = 0.01961991
Iteration 44, loss = 0.01935905
Iteration 45, loss = 0.01948205
Iteration 46, loss = 0.01938586
Iteration 47, loss = 0.01920847
Iteration 48, loss = 0.01917442
Iteration 49, loss = 0.01907955
Iteration 50, loss = 0.01900255
Iteration 51, loss = 0.01878709
Iteration 52, loss = 0.01879018
Iteration 53, loss = 0.01874838
Iteration 54, loss = 0.01868270
Iteration 55, loss = 0.01844628
Iteration 56, loss = 0.01832065
Iteration 57, loss = 0.01827222
Iteration 58, loss = 0.01837175
Iteration 59, loss = 0.01823999
Iteration 60, loss = 0.01829902
Iteration 61, loss = 0.01819813
Iteration 62, loss = 0.01810948
Iteration 63, loss = 0.01799018
Iteration 64, loss = 0.01828053
Iteration 65, loss = 0.01810676
Iteration 66, loss = 0.01808390
Iteration 67, loss = 0.01813210
Iteration 68, loss = 0.01795498
Iteration 69, loss = 0.01770909
Iteration 70, loss = 0.01761766
Iteration 71, loss = 0.01790515
Iteration 72, loss = 0.01798757
Iteration 73, loss = 0.01790785
Iteration 74, loss = 0.01769015
Iteration 75, loss = 0.01755976
Iteration 76, loss = 0.01756623
Iteration 77, loss = 0.01735864
Iteration 78, loss = 0.01749533
Iteration 79, loss = 0.01756312
Iteration 80, loss = 0.01722193
Iteration 81, loss = 0.01735795
Iteration 82, loss = 0.01745407
Iteration 83, loss = 0.01753106
Iteration 84, loss = 0.01748222
Iteration 85, loss = 0.01747536
Iteration 86, loss = 0.01741118
Iteration 87, loss = 0.01736125
Iteration 88, loss = 0.01726040
Iteration 89, loss = 0.01737529
Iteration 90, loss = 0.01713332
Iteration 91, loss = 0.01723259
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	91m16.792s
user	385m30.302s
sys	39m56.632s
Elapsed time: 5477 seconds
