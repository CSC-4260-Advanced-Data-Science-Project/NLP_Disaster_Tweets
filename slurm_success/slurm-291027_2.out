Processing dataset: prepended_v10_lemma_stem_custom_stopwords

üì• Loaded 30 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - prepended_v10_lemma_stem_custom_stopwords: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - dropped_v10_lemma_stem_custom_stopwords: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - kept_v10_lemma_stem_custom_stopwords: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: prepended_v10_lemma_stem_custom_stopwords (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on prepended_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on prepended_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on prepended_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on prepended_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on prepended_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67444688
Iteration 2, loss = 0.59822130
Iteration 3, loss = 0.48247445
Iteration 4, loss = 0.36913793
Iteration 5, loss = 0.27454340
Iteration 6, loss = 0.20448237
Iteration 7, loss = 0.15604932
Iteration 8, loss = 0.12292308
Iteration 9, loss = 0.09976920
Iteration 10, loss = 0.08345546
Iteration 11, loss = 0.07161669
Iteration 12, loss = 0.06245978
Iteration 13, loss = 0.05595028
Iteration 14, loss = 0.05035780
Iteration 15, loss = 0.04631598
Iteration 16, loss = 0.04279641
Iteration 17, loss = 0.04013755
Iteration 18, loss = 0.03768342
Iteration 19, loss = 0.03589691
Iteration 20, loss = 0.03458620
Iteration 21, loss = 0.03285230
Iteration 22, loss = 0.03159748
Iteration 23, loss = 0.03076676
Iteration 24, loss = 0.02986960
Iteration 25, loss = 0.02910371
Iteration 26, loss = 0.02851176
Iteration 27, loss = 0.02790070
Iteration 28, loss = 0.02721095
Iteration 29, loss = 0.02683314
Iteration 30, loss = 0.02611865
Iteration 31, loss = 0.02616807
Iteration 32, loss = 0.02577558
Iteration 33, loss = 0.02542743
Iteration 34, loss = 0.02522120
Iteration 35, loss = 0.02492782
Iteration 36, loss = 0.02471106
Iteration 37, loss = 0.02454406
Iteration 38, loss = 0.02425056
Iteration 39, loss = 0.02399826
Iteration 40, loss = 0.02374003
Iteration 41, loss = 0.02352312
Iteration 42, loss = 0.02358491
Iteration 43, loss = 0.02363261
Iteration 44, loss = 0.02327910
Iteration 45, loss = 0.02335829
Iteration 46, loss = 0.02312040
Iteration 47, loss = 0.02303568
Iteration 48, loss = 0.02288141
Iteration 49, loss = 0.02300501
Iteration 50, loss = 0.02282751
Iteration 51, loss = 0.02282086
Iteration 52, loss = 0.02291186
Iteration 53, loss = 0.02259839
Iteration 54, loss = 0.02248208
Iteration 55, loss = 0.02252038
Iteration 56, loss = 0.02210944
Iteration 57, loss = 0.02231692
Iteration 58, loss = 0.02219912
Iteration 59, loss = 0.02205258
Iteration 60, loss = 0.02224643
Iteration 61, loss = 0.02212624
Iteration 62, loss = 0.02199575
Iteration 63, loss = 0.02199673
Iteration 64, loss = 0.02216556
Iteration 65, loss = 0.02231846
Iteration 66, loss = 0.02164307
Iteration 67, loss = 0.02214686
Iteration 68, loss = 0.02207190
Iteration 69, loss = 0.02187796
Iteration 70, loss = 0.02161339
Iteration 71, loss = 0.02189982
Iteration 72, loss = 0.02183765
Iteration 73, loss = 0.02185522
Iteration 74, loss = 0.02195778
Iteration 75, loss = 0.02173707
Iteration 76, loss = 0.02184420
Iteration 77, loss = 0.02152633
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68080260
Iteration 2, loss = 0.60646073
Iteration 3, loss = 0.49653315
Iteration 4, loss = 0.38292485
Iteration 5, loss = 0.28609168
Iteration 6, loss = 0.21275238
Iteration 7, loss = 0.16167392
Iteration 8, loss = 0.12671468
Iteration 9, loss = 0.10287345
Iteration 10, loss = 0.08562907
Iteration 11, loss = 0.07343106
Iteration 12, loss = 0.06435694
Iteration 13, loss = 0.05744825
Iteration 14, loss = 0.05224079
Iteration 15, loss = 0.04776823
Iteration 16, loss = 0.04450835
Iteration 17, loss = 0.04182906
Iteration 18, loss = 0.03937981
Iteration 19, loss = 0.03772223
Iteration 20, loss = 0.03617707
Iteration 21, loss = 0.03475236
Iteration 22, loss = 0.03356368
Iteration 23, loss = 0.03242600
Iteration 24, loss = 0.03159702
Iteration 25, loss = 0.03086474
Iteration 26, loss = 0.03028787
Iteration 27, loss = 0.02995004
Iteration 28, loss = 0.02892366
Iteration 29, loss = 0.02877518
Iteration 30, loss = 0.02815576
Iteration 31, loss = 0.02799577
Iteration 32, loss = 0.02754655
Iteration 33, loss = 0.02721037
Iteration 34, loss = 0.02692532
Iteration 35, loss = 0.02660719
Iteration 36, loss = 0.02678607
Iteration 37, loss = 0.02597171
Iteration 38, loss = 0.02610118
Iteration 39, loss = 0.02583300
Iteration 40, loss = 0.02555420
Iteration 41, loss = 0.02559210
Iteration 42, loss = 0.02506295
Iteration 43, loss = 0.02536681
Iteration 44, loss = 0.02522841
Iteration 45, loss = 0.02493598
Iteration 46, loss = 0.02496163
Iteration 47, loss = 0.02497459
Iteration 48, loss = 0.02475344
Iteration 49, loss = 0.02482840
Iteration 50, loss = 0.02467853
Iteration 51, loss = 0.02442434
Iteration 52, loss = 0.02459710
Iteration 53, loss = 0.02435824
Iteration 54, loss = 0.02453458
Iteration 55, loss = 0.02426069
Iteration 56, loss = 0.02445901
Iteration 57, loss = 0.02421261
Iteration 58, loss = 0.02402334
Iteration 59, loss = 0.02394081
Iteration 60, loss = 0.02399824
Iteration 61, loss = 0.02395372
Iteration 62, loss = 0.02383748
Iteration 63, loss = 0.02403758
Iteration 64, loss = 0.02394348
Iteration 65, loss = 0.02364763
Iteration 66, loss = 0.02412247
Iteration 67, loss = 0.02354648
Iteration 68, loss = 0.02385798
Iteration 69, loss = 0.02386643
Iteration 70, loss = 0.02345004
Iteration 71, loss = 0.02369536
Iteration 72, loss = 0.02362138
Iteration 73, loss = 0.02358905
Iteration 74, loss = 0.02341063
Iteration 75, loss = 0.02351855
Iteration 76, loss = 0.02352730
Iteration 77, loss = 0.02331979
Iteration 78, loss = 0.02363078
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67621383
Iteration 2, loss = 0.59951254
Iteration 3, loss = 0.47965686
Iteration 4, loss = 0.36268204
Iteration 5, loss = 0.26681168
Iteration 6, loss = 0.19810779
Iteration 7, loss = 0.15038698
Iteration 8, loss = 0.11775309
Iteration 9, loss = 0.09554368
Iteration 10, loss = 0.07993186
Iteration 11, loss = 0.06843974
Iteration 12, loss = 0.05999608
Iteration 13, loss = 0.05305755
Iteration 14, loss = 0.04798413
Iteration 15, loss = 0.04404239
Iteration 16, loss = 0.04087923
Iteration 17, loss = 0.03794399
Iteration 18, loss = 0.03569290
Iteration 19, loss = 0.03381717
Iteration 20, loss = 0.03235197
Iteration 21, loss = 0.03094047
Iteration 22, loss = 0.02982779
Iteration 23, loss = 0.02889177
Iteration 24, loss = 0.02792162
Iteration 25, loss = 0.02708358
Iteration 26, loss = 0.02652307
Iteration 27, loss = 0.02568790
Iteration 28, loss = 0.02524666
Iteration 29, loss = 0.02472787
Iteration 30, loss = 0.02438224
Iteration 31, loss = 0.02398573
Iteration 32, loss = 0.02372194
Iteration 33, loss = 0.02335452
Iteration 34, loss = 0.02321705
Iteration 35, loss = 0.02308626
Iteration 36, loss = 0.02251747
Iteration 37, loss = 0.02235926
Iteration 38, loss = 0.02227244
Iteration 39, loss = 0.02202870
Iteration 40, loss = 0.02186939
Iteration 41, loss = 0.02170386
Iteration 42, loss = 0.02181590
Iteration 43, loss = 0.02143763
Iteration 44, loss = 0.02128630
Iteration 45, loss = 0.02111278
Iteration 46, loss = 0.02120699
Iteration 47, loss = 0.02132191
Iteration 48, loss = 0.02076367
Iteration 49, loss = 0.02081197
Iteration 50, loss = 0.02082352
Iteration 51, loss = 0.02050275
Iteration 52, loss = 0.02062900
Iteration 53, loss = 0.02038091
Iteration 54, loss = 0.02055537
Iteration 55, loss = 0.02045917
Iteration 56, loss = 0.02021327
Iteration 57, loss = 0.02028928
Iteration 58, loss = 0.02020591
Iteration 59, loss = 0.02004976
Iteration 60, loss = 0.02032999
Iteration 61, loss = 0.01993823
Iteration 62, loss = 0.02003979
Iteration 63, loss = 0.01976790
Iteration 64, loss = 0.01983441
Iteration 65, loss = 0.01965620
Iteration 66, loss = 0.01995125
Iteration 67, loss = 0.01960601
Iteration 68, loss = 0.01962907
Iteration 69, loss = 0.01979302
Iteration 70, loss = 0.01989970
Iteration 71, loss = 0.01928979
Iteration 72, loss = 0.01984898
Iteration 73, loss = 0.01981152
Iteration 74, loss = 0.01945244
Iteration 75, loss = 0.01955751
Iteration 76, loss = 0.01974649
Iteration 77, loss = 0.01961967
Iteration 78, loss = 0.01956276
Iteration 79, loss = 0.01972851
Iteration 80, loss = 0.01940439
Iteration 81, loss = 0.01918140
Iteration 82, loss = 0.01916543
Iteration 83, loss = 0.01954267
Iteration 84, loss = 0.01967277
Iteration 85, loss = 0.01901106
Iteration 86, loss = 0.01920728
Iteration 87, loss = 0.01942761
Iteration 88, loss = 0.01920641
Iteration 89, loss = 0.01897406
Iteration 90, loss = 0.01924809
Iteration 91, loss = 0.01904916
Iteration 92, loss = 0.01914104
Iteration 93, loss = 0.01905182
Iteration 94, loss = 0.01931245
Iteration 95, loss = 0.01951970
Iteration 96, loss = 0.01899535
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68054056
Iteration 2, loss = 0.60545977
Iteration 3, loss = 0.49673147
Iteration 4, loss = 0.38354187
Iteration 5, loss = 0.28594942
Iteration 6, loss = 0.21214678
Iteration 7, loss = 0.16020352
Iteration 8, loss = 0.12465164
Iteration 9, loss = 0.10037099
Iteration 10, loss = 0.08308249
Iteration 11, loss = 0.07065548
Iteration 12, loss = 0.06131048
Iteration 13, loss = 0.05400466
Iteration 14, loss = 0.04865890
Iteration 15, loss = 0.04448092
Iteration 16, loss = 0.04082148
Iteration 17, loss = 0.03813392
Iteration 18, loss = 0.03586048
Iteration 19, loss = 0.03394741
Iteration 20, loss = 0.03222393
Iteration 21, loss = 0.03105213
Iteration 22, loss = 0.02980169
Iteration 23, loss = 0.02878571
Iteration 24, loss = 0.02779022
Iteration 25, loss = 0.02700039
Iteration 26, loss = 0.02644685
Iteration 27, loss = 0.02560350
Iteration 28, loss = 0.02526589
Iteration 29, loss = 0.02501854
Iteration 30, loss = 0.02441325
Iteration 31, loss = 0.02401498
Iteration 32, loss = 0.02360861
Iteration 33, loss = 0.02340236
Iteration 34, loss = 0.02318800
Iteration 35, loss = 0.02292386
Iteration 36, loss = 0.02266968
Iteration 37, loss = 0.02239800
Iteration 38, loss = 0.02236654
Iteration 39, loss = 0.02220281
Iteration 40, loss = 0.02167001
Iteration 41, loss = 0.02162798
Iteration 42, loss = 0.02162673
Iteration 43, loss = 0.02118816
Iteration 44, loss = 0.02135500
Iteration 45, loss = 0.02112759
Iteration 46, loss = 0.02098427
Iteration 47, loss = 0.02075282
Iteration 48, loss = 0.02090947
Iteration 49, loss = 0.02083483
Iteration 50, loss = 0.02037324
Iteration 51, loss = 0.02080393
Iteration 52, loss = 0.02059224
Iteration 53, loss = 0.02029545
Iteration 54, loss = 0.02012706
Iteration 55, loss = 0.02036361
Iteration 56, loss = 0.02012600
Iteration 57, loss = 0.02015071
Iteration 58, loss = 0.02015651
Iteration 59, loss = 0.01980214
Iteration 60, loss = 0.02004570
Iteration 61, loss = 0.01980637
Iteration 62, loss = 0.01984693
Iteration 63, loss = 0.01994298
Iteration 64, loss = 0.01978383
Iteration 65, loss = 0.01980879
Iteration 66, loss = 0.01968998
Iteration 67, loss = 0.01955209
Iteration 68, loss = 0.01977137
Iteration 69, loss = 0.02001352
Iteration 70, loss = 0.01967408
Iteration 71, loss = 0.01935767
Iteration 72, loss = 0.01950678
Iteration 73, loss = 0.01934745
Iteration 74, loss = 0.01932118
Iteration 75, loss = 0.01955488
Iteration 76, loss = 0.01940157
Iteration 77, loss = 0.01942206
Iteration 78, loss = 0.01936762
Iteration 79, loss = 0.01930423
Iteration 80, loss = 0.01928290
Iteration 81, loss = 0.01969918
Iteration 82, loss = 0.01954757
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68528676
Iteration 2, loss = 0.61129048
Iteration 3, loss = 0.50030402
Iteration 4, loss = 0.38084747
Iteration 5, loss = 0.28041713
Iteration 6, loss = 0.20708413
Iteration 7, loss = 0.15670743
Iteration 8, loss = 0.12215272
Iteration 9, loss = 0.09887547
Iteration 10, loss = 0.08211579
Iteration 11, loss = 0.06998824
Iteration 12, loss = 0.06121564
Iteration 13, loss = 0.05417696
Iteration 14, loss = 0.04883368
Iteration 15, loss = 0.04475953
Iteration 16, loss = 0.04115638
Iteration 17, loss = 0.03862464
Iteration 18, loss = 0.03653673
Iteration 19, loss = 0.03430525
Iteration 20, loss = 0.03267738
Iteration 21, loss = 0.03128562
Iteration 22, loss = 0.02995484
Iteration 23, loss = 0.02892633
Iteration 24, loss = 0.02836385
Iteration 25, loss = 0.02722467
Iteration 26, loss = 0.02669285
Iteration 27, loss = 0.02612211
Iteration 28, loss = 0.02558092
Iteration 29, loss = 0.02489302
Iteration 30, loss = 0.02460728
Iteration 31, loss = 0.02417513
Iteration 32, loss = 0.02401103
Iteration 33, loss = 0.02356565
Iteration 34, loss = 0.02339460
Iteration 35, loss = 0.02314571
Iteration 36, loss = 0.02272182
Iteration 37, loss = 0.02247668
Iteration 38, loss = 0.02243840
Iteration 39, loss = 0.02257453
Iteration 40, loss = 0.02202754
Iteration 41, loss = 0.02207647
Iteration 42, loss = 0.02181954
Iteration 43, loss = 0.02159914
Iteration 44, loss = 0.02134441
Iteration 45, loss = 0.02141774
Iteration 46, loss = 0.02127103
Iteration 47, loss = 0.02110724
Iteration 48, loss = 0.02097609
Iteration 49, loss = 0.02083313
Iteration 50, loss = 0.02092036
Iteration 51, loss = 0.02071453
Iteration 52, loss = 0.02079715
Iteration 53, loss = 0.02061987
Iteration 54, loss = 0.02071879
Iteration 55, loss = 0.02043106
Iteration 56, loss = 0.02077797
Iteration 57, loss = 0.02061626
Iteration 58, loss = 0.02034157
Iteration 59, loss = 0.02044390
Iteration 60, loss = 0.02069394
Iteration 61, loss = 0.02033637
Iteration 62, loss = 0.02027866
Iteration 63, loss = 0.02031612
Iteration 64, loss = 0.02040706
Iteration 65, loss = 0.02013651
Iteration 66, loss = 0.02026142
Iteration 67, loss = 0.02003015
Iteration 68, loss = 0.01993213
Iteration 69, loss = 0.02003249
Iteration 70, loss = 0.01988487
Iteration 71, loss = 0.01992877
Iteration 72, loss = 0.01979803
Iteration 73, loss = 0.01988500
Iteration 74, loss = 0.01958649
Iteration 75, loss = 0.02000146
Iteration 76, loss = 0.01984654
Iteration 77, loss = 0.01963486
Iteration 78, loss = 0.01958201
Iteration 79, loss = 0.01972001
Iteration 80, loss = 0.01965181
Iteration 81, loss = 0.01959879
Iteration 82, loss = 0.01946945
Iteration 83, loss = 0.01978653
Iteration 84, loss = 0.01975035
Iteration 85, loss = 0.01947601
Iteration 86, loss = 0.01982342
Iteration 87, loss = 0.01981996
Iteration 88, loss = 0.01947151
Iteration 89, loss = 0.01957660
Iteration 90, loss = 0.01949606
Iteration 91, loss = 0.01949320
Iteration 92, loss = 0.01950525
Iteration 93, loss = 0.01929059
Iteration 94, loss = 0.01914145
Iteration 95, loss = 0.01940342
Iteration 96, loss = 0.01937834
Iteration 97, loss = 0.01952888
Iteration 98, loss = 0.01911843
Iteration 99, loss = 0.01934144
Iteration 100, loss = 0.01945185
Iteration 101, loss = 0.01924875
Iteration 102, loss = 0.01920425
Iteration 103, loss = 0.01945126
Iteration 104, loss = 0.01919568
Iteration 105, loss = 0.01933361
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67444688
Iteration 2, loss = 0.59822130
Iteration 3, loss = 0.48247445
Iteration 4, loss = 0.36913793
Iteration 5, loss = 0.27454340
Iteration 6, loss = 0.20448237
Iteration 7, loss = 0.15604932
Iteration 8, loss = 0.12292308
Iteration 9, loss = 0.09976920
Iteration 10, loss = 0.08345546
Iteration 11, loss = 0.07161669
Iteration 12, loss = 0.06245978
Iteration 13, loss = 0.05595028
Iteration 14, loss = 0.05035780
Iteration 15, loss = 0.04631598
Iteration 16, loss = 0.04279641
Iteration 17, loss = 0.04013755
Iteration 18, loss = 0.03768342
Iteration 19, loss = 0.03589691
Iteration 20, loss = 0.03458620
Iteration 21, loss = 0.03285230
Iteration 22, loss = 0.03159748
Iteration 23, loss = 0.03076676
Iteration 24, loss = 0.02986960
Iteration 25, loss = 0.02910371
Iteration 26, loss = 0.02851176
Iteration 27, loss = 0.02790070
Iteration 28, loss = 0.02721095
Iteration 29, loss = 0.02683314
Iteration 30, loss = 0.02611865
Iteration 31, loss = 0.02616807
Iteration 32, loss = 0.02577558
Iteration 33, loss = 0.02542743
Iteration 34, loss = 0.02522120
Iteration 35, loss = 0.02492782
Iteration 36, loss = 0.02471106
Iteration 37, loss = 0.02454406
Iteration 38, loss = 0.02425056
Iteration 39, loss = 0.02399826
Iteration 40, loss = 0.02374003
Iteration 41, loss = 0.02352312
Iteration 42, loss = 0.02358491
Iteration 43, loss = 0.02363261
Iteration 44, loss = 0.02327910
Iteration 45, loss = 0.02335829
Iteration 46, loss = 0.02312040
Iteration 47, loss = 0.02303568
Iteration 48, loss = 0.02288141
Iteration 49, loss = 0.02300501
Iteration 50, loss = 0.02282751
Iteration 51, loss = 0.02282086
Iteration 52, loss = 0.02291186
Iteration 53, loss = 0.02259839
Iteration 54, loss = 0.02248208
Iteration 55, loss = 0.02252038
Iteration 56, loss = 0.02210944
Iteration 57, loss = 0.02231692
Iteration 58, loss = 0.02219912
Iteration 59, loss = 0.02205258
Iteration 60, loss = 0.02224643
Iteration 61, loss = 0.02212624
Iteration 62, loss = 0.02199575
Iteration 63, loss = 0.02199673
Iteration 64, loss = 0.02216556
Iteration 65, loss = 0.02231846
Iteration 66, loss = 0.02164307
Iteration 67, loss = 0.02214686
Iteration 68, loss = 0.02207190
Iteration 69, loss = 0.02187796
Iteration 70, loss = 0.02161339
Iteration 71, loss = 0.02189982
Iteration 72, loss = 0.02183765
Iteration 73, loss = 0.02185522
Iteration 74, loss = 0.02195778
Iteration 75, loss = 0.02173707
Iteration 76, loss = 0.02184420
Iteration 77, loss = 0.02152633
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68080260
Iteration 2, loss = 0.60646073
Iteration 3, loss = 0.49653315
Iteration 4, loss = 0.38292485
Iteration 5, loss = 0.28609168
Iteration 6, loss = 0.21275238
Iteration 7, loss = 0.16167392
Iteration 8, loss = 0.12671468
Iteration 9, loss = 0.10287345
Iteration 10, loss = 0.08562907
Iteration 11, loss = 0.07343106
Iteration 12, loss = 0.06435694
Iteration 13, loss = 0.05744825
Iteration 14, loss = 0.05224079
Iteration 15, loss = 0.04776823
Iteration 16, loss = 0.04450835
Iteration 17, loss = 0.04182906
Iteration 18, loss = 0.03937981
Iteration 19, loss = 0.03772223
Iteration 20, loss = 0.03617707
Iteration 21, loss = 0.03475236
Iteration 22, loss = 0.03356368
Iteration 23, loss = 0.03242600
Iteration 24, loss = 0.03159702
Iteration 25, loss = 0.03086474
Iteration 26, loss = 0.03028787
Iteration 27, loss = 0.02995004
Iteration 28, loss = 0.02892366
Iteration 29, loss = 0.02877518
Iteration 30, loss = 0.02815576
Iteration 31, loss = 0.02799577
Iteration 32, loss = 0.02754655
Iteration 33, loss = 0.02721037
Iteration 34, loss = 0.02692532
Iteration 35, loss = 0.02660719
Iteration 36, loss = 0.02678607
Iteration 37, loss = 0.02597171
Iteration 38, loss = 0.02610118
Iteration 39, loss = 0.02583300
Iteration 40, loss = 0.02555420
Iteration 41, loss = 0.02559210
Iteration 42, loss = 0.02506295
Iteration 43, loss = 0.02536681
Iteration 44, loss = 0.02522841
Iteration 45, loss = 0.02493598
Iteration 46, loss = 0.02496163
Iteration 47, loss = 0.02497459
Iteration 48, loss = 0.02475344
Iteration 49, loss = 0.02482840
Iteration 50, loss = 0.02467853
Iteration 51, loss = 0.02442434
Iteration 52, loss = 0.02459710
Iteration 53, loss = 0.02435824
Iteration 54, loss = 0.02453458
Iteration 55, loss = 0.02426069
Iteration 56, loss = 0.02445901
Iteration 57, loss = 0.02421261
Iteration 58, loss = 0.02402334
Iteration 59, loss = 0.02394081
Iteration 60, loss = 0.02399824
Iteration 61, loss = 0.02395372
Iteration 62, loss = 0.02383748
Iteration 63, loss = 0.02403758
Iteration 64, loss = 0.02394348
Iteration 65, loss = 0.02364763
Iteration 66, loss = 0.02412247
Iteration 67, loss = 0.02354648
Iteration 68, loss = 0.02385798
Iteration 69, loss = 0.02386643
Iteration 70, loss = 0.02345004
Iteration 71, loss = 0.02369536
Iteration 72, loss = 0.02362138
Iteration 73, loss = 0.02358905
Iteration 74, loss = 0.02341063
Iteration 75, loss = 0.02351855
Iteration 76, loss = 0.02352730
Iteration 77, loss = 0.02331979
Iteration 78, loss = 0.02363078
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67621383
Iteration 2, loss = 0.59951254
Iteration 3, loss = 0.47965686
Iteration 4, loss = 0.36268204
Iteration 5, loss = 0.26681168
Iteration 6, loss = 0.19810779
Iteration 7, loss = 0.15038698
Iteration 8, loss = 0.11775309
Iteration 9, loss = 0.09554368
Iteration 10, loss = 0.07993186
Iteration 11, loss = 0.06843974
Iteration 12, loss = 0.05999608
Iteration 13, loss = 0.05305755
Iteration 14, loss = 0.04798413
Iteration 15, loss = 0.04404239
Iteration 16, loss = 0.04087923
Iteration 17, loss = 0.03794399
Iteration 18, loss = 0.03569290
Iteration 19, loss = 0.03381717
Iteration 20, loss = 0.03235197
Iteration 21, loss = 0.03094047
Iteration 22, loss = 0.02982779
Iteration 23, loss = 0.02889177
Iteration 24, loss = 0.02792162
Iteration 25, loss = 0.02708358
Iteration 26, loss = 0.02652307
Iteration 27, loss = 0.02568790
Iteration 28, loss = 0.02524666
Iteration 29, loss = 0.02472787
Iteration 30, loss = 0.02438224
Iteration 31, loss = 0.02398573
Iteration 32, loss = 0.02372194
Iteration 33, loss = 0.02335452
Iteration 34, loss = 0.02321705
Iteration 35, loss = 0.02308626
Iteration 36, loss = 0.02251747
Iteration 37, loss = 0.02235926
Iteration 38, loss = 0.02227244
Iteration 39, loss = 0.02202870
Iteration 40, loss = 0.02186939
Iteration 41, loss = 0.02170386
Iteration 42, loss = 0.02181590
Iteration 43, loss = 0.02143763
Iteration 44, loss = 0.02128630
Iteration 45, loss = 0.02111278
Iteration 46, loss = 0.02120699
Iteration 47, loss = 0.02132191
Iteration 48, loss = 0.02076367
Iteration 49, loss = 0.02081197
Iteration 50, loss = 0.02082352
Iteration 51, loss = 0.02050275
Iteration 52, loss = 0.02062900
Iteration 53, loss = 0.02038091
Iteration 54, loss = 0.02055537
Iteration 55, loss = 0.02045917
Iteration 56, loss = 0.02021327
Iteration 57, loss = 0.02028928
Iteration 58, loss = 0.02020591
Iteration 59, loss = 0.02004976
Iteration 60, loss = 0.02032999
Iteration 61, loss = 0.01993823
Iteration 62, loss = 0.02003979
Iteration 63, loss = 0.01976790
Iteration 64, loss = 0.01983441
Iteration 65, loss = 0.01965620
Iteration 66, loss = 0.01995125
Iteration 67, loss = 0.01960601
Iteration 68, loss = 0.01962907
Iteration 69, loss = 0.01979302
Iteration 70, loss = 0.01989970
Iteration 71, loss = 0.01928979
Iteration 72, loss = 0.01984898
Iteration 73, loss = 0.01981152
Iteration 74, loss = 0.01945244
Iteration 75, loss = 0.01955751
Iteration 76, loss = 0.01974649
Iteration 77, loss = 0.01961967
Iteration 78, loss = 0.01956276
Iteration 79, loss = 0.01972851
Iteration 80, loss = 0.01940439
Iteration 81, loss = 0.01918140
Iteration 82, loss = 0.01916543
Iteration 83, loss = 0.01954267
Iteration 84, loss = 0.01967277
Iteration 85, loss = 0.01901106
Iteration 86, loss = 0.01920728
Iteration 87, loss = 0.01942761
Iteration 88, loss = 0.01920641
Iteration 89, loss = 0.01897406
Iteration 90, loss = 0.01924809
Iteration 91, loss = 0.01904916
Iteration 92, loss = 0.01914104
Iteration 93, loss = 0.01905182
Iteration 94, loss = 0.01931245
Iteration 95, loss = 0.01951970
Iteration 96, loss = 0.01899535
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68054056
Iteration 2, loss = 0.60545977
Iteration 3, loss = 0.49673147
Iteration 4, loss = 0.38354187
Iteration 5, loss = 0.28594942
Iteration 6, loss = 0.21214678
Iteration 7, loss = 0.16020352
Iteration 8, loss = 0.12465164
Iteration 9, loss = 0.10037099
Iteration 10, loss = 0.08308249
Iteration 11, loss = 0.07065548
Iteration 12, loss = 0.06131048
Iteration 13, loss = 0.05400466
Iteration 14, loss = 0.04865890
Iteration 15, loss = 0.04448092
Iteration 16, loss = 0.04082148
Iteration 17, loss = 0.03813392
Iteration 18, loss = 0.03586048
Iteration 19, loss = 0.03394741
Iteration 20, loss = 0.03222393
Iteration 21, loss = 0.03105213
Iteration 22, loss = 0.02980169
Iteration 23, loss = 0.02878571
Iteration 24, loss = 0.02779022
Iteration 25, loss = 0.02700039
Iteration 26, loss = 0.02644685
Iteration 27, loss = 0.02560350
Iteration 28, loss = 0.02526589
Iteration 29, loss = 0.02501854
Iteration 30, loss = 0.02441325
Iteration 31, loss = 0.02401498
Iteration 32, loss = 0.02360861
Iteration 33, loss = 0.02340236
Iteration 34, loss = 0.02318800
Iteration 35, loss = 0.02292386
Iteration 36, loss = 0.02266968
Iteration 37, loss = 0.02239800
Iteration 38, loss = 0.02236654
Iteration 39, loss = 0.02220281
Iteration 40, loss = 0.02167001
Iteration 41, loss = 0.02162798
Iteration 42, loss = 0.02162673
Iteration 43, loss = 0.02118816
Iteration 44, loss = 0.02135500
Iteration 45, loss = 0.02112759
Iteration 46, loss = 0.02098427
Iteration 47, loss = 0.02075282
Iteration 48, loss = 0.02090947
Iteration 49, loss = 0.02083483
Iteration 50, loss = 0.02037324
Iteration 51, loss = 0.02080393
Iteration 52, loss = 0.02059224
Iteration 53, loss = 0.02029545
Iteration 54, loss = 0.02012706
Iteration 55, loss = 0.02036361
Iteration 56, loss = 0.02012600
Iteration 57, loss = 0.02015071
Iteration 58, loss = 0.02015651
Iteration 59, loss = 0.01980214
Iteration 60, loss = 0.02004570
Iteration 61, loss = 0.01980637
Iteration 62, loss = 0.01984693
Iteration 63, loss = 0.01994298
Iteration 64, loss = 0.01978383
Iteration 65, loss = 0.01980879
Iteration 66, loss = 0.01968998
Iteration 67, loss = 0.01955209
Iteration 68, loss = 0.01977137
Iteration 69, loss = 0.02001352
Iteration 70, loss = 0.01967408
Iteration 71, loss = 0.01935767
Iteration 72, loss = 0.01950678
Iteration 73, loss = 0.01934745
Iteration 74, loss = 0.01932118
Iteration 75, loss = 0.01955488
Iteration 76, loss = 0.01940157
Iteration 77, loss = 0.01942206
Iteration 78, loss = 0.01936762
Iteration 79, loss = 0.01930423
Iteration 80, loss = 0.01928290
Iteration 81, loss = 0.01969918
Iteration 82, loss = 0.01954757
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68528676
Iteration 2, loss = 0.61129048
Iteration 3, loss = 0.50030402
Iteration 4, loss = 0.38084747
Iteration 5, loss = 0.28041713
Iteration 6, loss = 0.20708413
Iteration 7, loss = 0.15670743
Iteration 8, loss = 0.12215272
Iteration 9, loss = 0.09887547
Iteration 10, loss = 0.08211579
Iteration 11, loss = 0.06998824
Iteration 12, loss = 0.06121564
Iteration 13, loss = 0.05417696
Iteration 14, loss = 0.04883368
Iteration 15, loss = 0.04475953
Iteration 16, loss = 0.04115638
Iteration 17, loss = 0.03862464
Iteration 18, loss = 0.03653673
Iteration 19, loss = 0.03430525
Iteration 20, loss = 0.03267738
Iteration 21, loss = 0.03128562
Iteration 22, loss = 0.02995484
Iteration 23, loss = 0.02892633
Iteration 24, loss = 0.02836385
Iteration 25, loss = 0.02722467
Iteration 26, loss = 0.02669285
Iteration 27, loss = 0.02612211
Iteration 28, loss = 0.02558092
Iteration 29, loss = 0.02489302
Iteration 30, loss = 0.02460728
Iteration 31, loss = 0.02417513
Iteration 32, loss = 0.02401103
Iteration 33, loss = 0.02356565
Iteration 34, loss = 0.02339460
Iteration 35, loss = 0.02314571
Iteration 36, loss = 0.02272182
Iteration 37, loss = 0.02247668
Iteration 38, loss = 0.02243840
Iteration 39, loss = 0.02257453
Iteration 40, loss = 0.02202754
Iteration 41, loss = 0.02207647
Iteration 42, loss = 0.02181954
Iteration 43, loss = 0.02159914
Iteration 44, loss = 0.02134441
Iteration 45, loss = 0.02141774
Iteration 46, loss = 0.02127103
Iteration 47, loss = 0.02110724
Iteration 48, loss = 0.02097609
Iteration 49, loss = 0.02083313
Iteration 50, loss = 0.02092036
Iteration 51, loss = 0.02071453
Iteration 52, loss = 0.02079715
Iteration 53, loss = 0.02061987
Iteration 54, loss = 0.02071879
Iteration 55, loss = 0.02043106
Iteration 56, loss = 0.02077797
Iteration 57, loss = 0.02061626
Iteration 58, loss = 0.02034157
Iteration 59, loss = 0.02044390
Iteration 60, loss = 0.02069394
Iteration 61, loss = 0.02033637
Iteration 62, loss = 0.02027866
Iteration 63, loss = 0.02031612
Iteration 64, loss = 0.02040706
Iteration 65, loss = 0.02013651
Iteration 66, loss = 0.02026142
Iteration 67, loss = 0.02003015
Iteration 68, loss = 0.01993213
Iteration 69, loss = 0.02003249
Iteration 70, loss = 0.01988487
Iteration 71, loss = 0.01992877
Iteration 72, loss = 0.01979803
Iteration 73, loss = 0.01988500
Iteration 74, loss = 0.01958649
Iteration 75, loss = 0.02000146
Iteration 76, loss = 0.01984654
Iteration 77, loss = 0.01963486
Iteration 78, loss = 0.01958201
Iteration 79, loss = 0.01972001
Iteration 80, loss = 0.01965181
Iteration 81, loss = 0.01959879
Iteration 82, loss = 0.01946945
Iteration 83, loss = 0.01978653
Iteration 84, loss = 0.01975035
Iteration 85, loss = 0.01947601
Iteration 86, loss = 0.01982342
Iteration 87, loss = 0.01981996
Iteration 88, loss = 0.01947151
Iteration 89, loss = 0.01957660
Iteration 90, loss = 0.01949606
Iteration 91, loss = 0.01949320
Iteration 92, loss = 0.01950525
Iteration 93, loss = 0.01929059
Iteration 94, loss = 0.01914145
Iteration 95, loss = 0.01940342
Iteration 96, loss = 0.01937834
Iteration 97, loss = 0.01952888
Iteration 98, loss = 0.01911843
Iteration 99, loss = 0.01934144
Iteration 100, loss = 0.01945185
Iteration 101, loss = 0.01924875
Iteration 102, loss = 0.01920425
Iteration 103, loss = 0.01945126
Iteration 104, loss = 0.01919568
Iteration 105, loss = 0.01933361
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67444688
Iteration 2, loss = 0.59822130
Iteration 3, loss = 0.48247445
Iteration 4, loss = 0.36913793
Iteration 5, loss = 0.27454340
Iteration 6, loss = 0.20448237
Iteration 7, loss = 0.15604932
Iteration 8, loss = 0.12292308
Iteration 9, loss = 0.09976920
Iteration 10, loss = 0.08345546
Iteration 11, loss = 0.07161669
Iteration 12, loss = 0.06245978
Iteration 13, loss = 0.05595028
Iteration 14, loss = 0.05035780
Iteration 15, loss = 0.04631598
Iteration 16, loss = 0.04279641
Iteration 17, loss = 0.04013755
Iteration 18, loss = 0.03768342
Iteration 19, loss = 0.03589691
Iteration 20, loss = 0.03458620
Iteration 21, loss = 0.03285230
Iteration 22, loss = 0.03159748
Iteration 23, loss = 0.03076676
Iteration 24, loss = 0.02986960
Iteration 25, loss = 0.02910371
Iteration 26, loss = 0.02851176
Iteration 27, loss = 0.02790070
Iteration 28, loss = 0.02721095
Iteration 29, loss = 0.02683314
Iteration 30, loss = 0.02611865
Iteration 31, loss = 0.02616807
Iteration 32, loss = 0.02577558
Iteration 33, loss = 0.02542743
Iteration 34, loss = 0.02522120
Iteration 35, loss = 0.02492782
Iteration 36, loss = 0.02471106
Iteration 37, loss = 0.02454406
Iteration 38, loss = 0.02425056
Iteration 39, loss = 0.02399826
Iteration 40, loss = 0.02374003
Iteration 41, loss = 0.02352312
Iteration 42, loss = 0.02358491
Iteration 43, loss = 0.02363261
Iteration 44, loss = 0.02327910
Iteration 45, loss = 0.02335829
Iteration 46, loss = 0.02312040
Iteration 47, loss = 0.02303568
Iteration 48, loss = 0.02288141
Iteration 49, loss = 0.02300501
Iteration 50, loss = 0.02282751
Iteration 51, loss = 0.02282086
Iteration 52, loss = 0.02291186
Iteration 53, loss = 0.02259839
Iteration 54, loss = 0.02248208
Iteration 55, loss = 0.02252038
Iteration 56, loss = 0.02210944
Iteration 57, loss = 0.02231692
Iteration 58, loss = 0.02219912
Iteration 59, loss = 0.02205258
Iteration 60, loss = 0.02224643
Iteration 61, loss = 0.02212624
Iteration 62, loss = 0.02199575
Iteration 63, loss = 0.02199673
Iteration 64, loss = 0.02216556
Iteration 65, loss = 0.02231846
Iteration 66, loss = 0.02164307
Iteration 67, loss = 0.02214686
Iteration 68, loss = 0.02207190
Iteration 69, loss = 0.02187796
Iteration 70, loss = 0.02161339
Iteration 71, loss = 0.02189982
Iteration 72, loss = 0.02183765
Iteration 73, loss = 0.02185522
Iteration 74, loss = 0.02195778
Iteration 75, loss = 0.02173707
Iteration 76, loss = 0.02184420
Iteration 77, loss = 0.02152633
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68080260
Iteration 2, loss = 0.60646073
Iteration 3, loss = 0.49653315
Iteration 4, loss = 0.38292485
Iteration 5, loss = 0.28609168
Iteration 6, loss = 0.21275238
Iteration 7, loss = 0.16167392
Iteration 8, loss = 0.12671468
Iteration 9, loss = 0.10287345
Iteration 10, loss = 0.08562907
Iteration 11, loss = 0.07343106
Iteration 12, loss = 0.06435694
Iteration 13, loss = 0.05744825
Iteration 14, loss = 0.05224079
Iteration 15, loss = 0.04776823
Iteration 16, loss = 0.04450835
Iteration 17, loss = 0.04182906
Iteration 18, loss = 0.03937981
Iteration 19, loss = 0.03772223
Iteration 20, loss = 0.03617707
Iteration 21, loss = 0.03475236
Iteration 22, loss = 0.03356368
Iteration 23, loss = 0.03242600
Iteration 24, loss = 0.03159702
Iteration 25, loss = 0.03086474
Iteration 26, loss = 0.03028787
Iteration 27, loss = 0.02995004
Iteration 28, loss = 0.02892366
Iteration 29, loss = 0.02877518
Iteration 30, loss = 0.02815576
Iteration 31, loss = 0.02799577
Iteration 32, loss = 0.02754655
Iteration 33, loss = 0.02721037
Iteration 34, loss = 0.02692532
Iteration 35, loss = 0.02660719
Iteration 36, loss = 0.02678607
Iteration 37, loss = 0.02597171
Iteration 38, loss = 0.02610118
Iteration 39, loss = 0.02583300
Iteration 40, loss = 0.02555420
Iteration 41, loss = 0.02559210
Iteration 42, loss = 0.02506295
Iteration 43, loss = 0.02536681
Iteration 44, loss = 0.02522841
Iteration 45, loss = 0.02493598
Iteration 46, loss = 0.02496163
Iteration 47, loss = 0.02497459
Iteration 48, loss = 0.02475344
Iteration 49, loss = 0.02482840
Iteration 50, loss = 0.02467853
Iteration 51, loss = 0.02442434
Iteration 52, loss = 0.02459710
Iteration 53, loss = 0.02435824
Iteration 54, loss = 0.02453458
Iteration 55, loss = 0.02426069
Iteration 56, loss = 0.02445901
Iteration 57, loss = 0.02421261
Iteration 58, loss = 0.02402334
Iteration 59, loss = 0.02394081
Iteration 60, loss = 0.02399824
Iteration 61, loss = 0.02395372
Iteration 62, loss = 0.02383748
Iteration 63, loss = 0.02403758
Iteration 64, loss = 0.02394348
Iteration 65, loss = 0.02364763
Iteration 66, loss = 0.02412247
Iteration 67, loss = 0.02354648
Iteration 68, loss = 0.02385798
Iteration 69, loss = 0.02386643
Iteration 70, loss = 0.02345004
Iteration 71, loss = 0.02369536
Iteration 72, loss = 0.02362138
Iteration 73, loss = 0.02358905
Iteration 74, loss = 0.02341063
Iteration 75, loss = 0.02351855
Iteration 76, loss = 0.02352730
Iteration 77, loss = 0.02331979
Iteration 78, loss = 0.02363078
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67621383
Iteration 2, loss = 0.59951254
Iteration 3, loss = 0.47965686
Iteration 4, loss = 0.36268204
Iteration 5, loss = 0.26681168
Iteration 6, loss = 0.19810779
Iteration 7, loss = 0.15038698
Iteration 8, loss = 0.11775309
Iteration 9, loss = 0.09554368
Iteration 10, loss = 0.07993186
Iteration 11, loss = 0.06843974
Iteration 12, loss = 0.05999608
Iteration 13, loss = 0.05305755
Iteration 14, loss = 0.04798413
Iteration 15, loss = 0.04404239
Iteration 16, loss = 0.04087923
Iteration 17, loss = 0.03794399
Iteration 18, loss = 0.03569290
Iteration 19, loss = 0.03381717
Iteration 20, loss = 0.03235197
Iteration 21, loss = 0.03094047
Iteration 22, loss = 0.02982779
Iteration 23, loss = 0.02889177
Iteration 24, loss = 0.02792162
Iteration 25, loss = 0.02708358
Iteration 26, loss = 0.02652307
Iteration 27, loss = 0.02568790
Iteration 28, loss = 0.02524666
Iteration 29, loss = 0.02472787
Iteration 30, loss = 0.02438224
Iteration 31, loss = 0.02398573
Iteration 32, loss = 0.02372194
Iteration 33, loss = 0.02335452
Iteration 34, loss = 0.02321705
Iteration 35, loss = 0.02308626
Iteration 36, loss = 0.02251747
Iteration 37, loss = 0.02235926
Iteration 38, loss = 0.02227244
Iteration 39, loss = 0.02202870
Iteration 40, loss = 0.02186939
Iteration 41, loss = 0.02170386
Iteration 42, loss = 0.02181590
Iteration 43, loss = 0.02143763
Iteration 44, loss = 0.02128630
Iteration 45, loss = 0.02111278
Iteration 46, loss = 0.02120699
Iteration 47, loss = 0.02132191
Iteration 48, loss = 0.02076367
Iteration 49, loss = 0.02081197
Iteration 50, loss = 0.02082352
Iteration 51, loss = 0.02050275
Iteration 52, loss = 0.02062900
Iteration 53, loss = 0.02038091
Iteration 54, loss = 0.02055537
Iteration 55, loss = 0.02045917
Iteration 56, loss = 0.02021327
Iteration 57, loss = 0.02028928
Iteration 58, loss = 0.02020591
Iteration 59, loss = 0.02004976
Iteration 60, loss = 0.02032999
Iteration 61, loss = 0.01993823
Iteration 62, loss = 0.02003979
Iteration 63, loss = 0.01976790
Iteration 64, loss = 0.01983441
Iteration 65, loss = 0.01965620
Iteration 66, loss = 0.01995125
Iteration 67, loss = 0.01960601
Iteration 68, loss = 0.01962907
Iteration 69, loss = 0.01979302
Iteration 70, loss = 0.01989970
Iteration 71, loss = 0.01928979
Iteration 72, loss = 0.01984898
Iteration 73, loss = 0.01981152
Iteration 74, loss = 0.01945244
Iteration 75, loss = 0.01955751
Iteration 76, loss = 0.01974649
Iteration 77, loss = 0.01961967
Iteration 78, loss = 0.01956276
Iteration 79, loss = 0.01972851
Iteration 80, loss = 0.01940439
Iteration 81, loss = 0.01918140
Iteration 82, loss = 0.01916543
Iteration 83, loss = 0.01954267
Iteration 84, loss = 0.01967277
Iteration 85, loss = 0.01901106
Iteration 86, loss = 0.01920728
Iteration 87, loss = 0.01942761
Iteration 88, loss = 0.01920641
Iteration 89, loss = 0.01897406
Iteration 90, loss = 0.01924809
Iteration 91, loss = 0.01904916
Iteration 92, loss = 0.01914104
Iteration 93, loss = 0.01905182
Iteration 94, loss = 0.01931245
Iteration 95, loss = 0.01951970
Iteration 96, loss = 0.01899535
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68054056
Iteration 2, loss = 0.60545977
Iteration 3, loss = 0.49673147
Iteration 4, loss = 0.38354187
Iteration 5, loss = 0.28594942
Iteration 6, loss = 0.21214678
Iteration 7, loss = 0.16020352
Iteration 8, loss = 0.12465164
Iteration 9, loss = 0.10037099
Iteration 10, loss = 0.08308249
Iteration 11, loss = 0.07065548
Iteration 12, loss = 0.06131048
Iteration 13, loss = 0.05400466
Iteration 14, loss = 0.04865890
Iteration 15, loss = 0.04448092
Iteration 16, loss = 0.04082148
Iteration 17, loss = 0.03813392
Iteration 18, loss = 0.03586048
Iteration 19, loss = 0.03394741
Iteration 20, loss = 0.03222393
Iteration 21, loss = 0.03105213
Iteration 22, loss = 0.02980169
Iteration 23, loss = 0.02878571
Iteration 24, loss = 0.02779022
Iteration 25, loss = 0.02700039
Iteration 26, loss = 0.02644685
Iteration 27, loss = 0.02560350
Iteration 28, loss = 0.02526589
Iteration 29, loss = 0.02501854
Iteration 30, loss = 0.02441325
Iteration 31, loss = 0.02401498
Iteration 32, loss = 0.02360861
Iteration 33, loss = 0.02340236
Iteration 34, loss = 0.02318800
Iteration 35, loss = 0.02292386
Iteration 36, loss = 0.02266968
Iteration 37, loss = 0.02239800
Iteration 38, loss = 0.02236654
Iteration 39, loss = 0.02220281
Iteration 40, loss = 0.02167001
Iteration 41, loss = 0.02162798
Iteration 42, loss = 0.02162673
Iteration 43, loss = 0.02118816
Iteration 44, loss = 0.02135500
Iteration 45, loss = 0.02112759
Iteration 46, loss = 0.02098427
Iteration 47, loss = 0.02075282
Iteration 48, loss = 0.02090947
Iteration 49, loss = 0.02083483
Iteration 50, loss = 0.02037324
Iteration 51, loss = 0.02080393
Iteration 52, loss = 0.02059224
Iteration 53, loss = 0.02029545
Iteration 54, loss = 0.02012706
Iteration 55, loss = 0.02036361
Iteration 56, loss = 0.02012600
Iteration 57, loss = 0.02015071
Iteration 58, loss = 0.02015651
Iteration 59, loss = 0.01980214
Iteration 60, loss = 0.02004570
Iteration 61, loss = 0.01980637
Iteration 62, loss = 0.01984693
Iteration 63, loss = 0.01994298
Iteration 64, loss = 0.01978383
Iteration 65, loss = 0.01980879
Iteration 66, loss = 0.01968998
Iteration 67, loss = 0.01955209
Iteration 68, loss = 0.01977137
Iteration 69, loss = 0.02001352
Iteration 70, loss = 0.01967408
Iteration 71, loss = 0.01935767
Iteration 72, loss = 0.01950678
Iteration 73, loss = 0.01934745
Iteration 74, loss = 0.01932118
Iteration 75, loss = 0.01955488
Iteration 76, loss = 0.01940157
Iteration 77, loss = 0.01942206
Iteration 78, loss = 0.01936762
Iteration 79, loss = 0.01930423
Iteration 80, loss = 0.01928290
Iteration 81, loss = 0.01969918
Iteration 82, loss = 0.01954757
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68528676
Iteration 2, loss = 0.61129048
Iteration 3, loss = 0.50030402
Iteration 4, loss = 0.38084747
Iteration 5, loss = 0.28041713
Iteration 6, loss = 0.20708413
Iteration 7, loss = 0.15670743
Iteration 8, loss = 0.12215272
Iteration 9, loss = 0.09887547
Iteration 10, loss = 0.08211579
Iteration 11, loss = 0.06998824
Iteration 12, loss = 0.06121564
Iteration 13, loss = 0.05417696
Iteration 14, loss = 0.04883368
Iteration 15, loss = 0.04475953
Iteration 16, loss = 0.04115638
Iteration 17, loss = 0.03862464
Iteration 18, loss = 0.03653673
Iteration 19, loss = 0.03430525
Iteration 20, loss = 0.03267738
Iteration 21, loss = 0.03128562
Iteration 22, loss = 0.02995484
Iteration 23, loss = 0.02892633
Iteration 24, loss = 0.02836385
Iteration 25, loss = 0.02722467
Iteration 26, loss = 0.02669285
Iteration 27, loss = 0.02612211
Iteration 28, loss = 0.02558092
Iteration 29, loss = 0.02489302
Iteration 30, loss = 0.02460728
Iteration 31, loss = 0.02417513
Iteration 32, loss = 0.02401103
Iteration 33, loss = 0.02356565
Iteration 34, loss = 0.02339460
Iteration 35, loss = 0.02314571
Iteration 36, loss = 0.02272182
Iteration 37, loss = 0.02247668
Iteration 38, loss = 0.02243840
Iteration 39, loss = 0.02257453
Iteration 40, loss = 0.02202754
Iteration 41, loss = 0.02207647
Iteration 42, loss = 0.02181954
Iteration 43, loss = 0.02159914
Iteration 44, loss = 0.02134441
Iteration 45, loss = 0.02141774
Iteration 46, loss = 0.02127103
Iteration 47, loss = 0.02110724
Iteration 48, loss = 0.02097609
Iteration 49, loss = 0.02083313
Iteration 50, loss = 0.02092036
Iteration 51, loss = 0.02071453
Iteration 52, loss = 0.02079715
Iteration 53, loss = 0.02061987
Iteration 54, loss = 0.02071879
Iteration 55, loss = 0.02043106
Iteration 56, loss = 0.02077797
Iteration 57, loss = 0.02061626
Iteration 58, loss = 0.02034157
Iteration 59, loss = 0.02044390
Iteration 60, loss = 0.02069394
Iteration 61, loss = 0.02033637
Iteration 62, loss = 0.02027866
Iteration 63, loss = 0.02031612
Iteration 64, loss = 0.02040706
Iteration 65, loss = 0.02013651
Iteration 66, loss = 0.02026142
Iteration 67, loss = 0.02003015
Iteration 68, loss = 0.01993213
Iteration 69, loss = 0.02003249
Iteration 70, loss = 0.01988487
Iteration 71, loss = 0.01992877
Iteration 72, loss = 0.01979803
Iteration 73, loss = 0.01988500
Iteration 74, loss = 0.01958649
Iteration 75, loss = 0.02000146
Iteration 76, loss = 0.01984654
Iteration 77, loss = 0.01963486
Iteration 78, loss = 0.01958201
Iteration 79, loss = 0.01972001
Iteration 80, loss = 0.01965181
Iteration 81, loss = 0.01959879
Iteration 82, loss = 0.01946945
Iteration 83, loss = 0.01978653
Iteration 84, loss = 0.01975035
Iteration 85, loss = 0.01947601
Iteration 86, loss = 0.01982342
Iteration 87, loss = 0.01981996
Iteration 88, loss = 0.01947151
Iteration 89, loss = 0.01957660
Iteration 90, loss = 0.01949606
Iteration 91, loss = 0.01949320
Iteration 92, loss = 0.01950525
Iteration 93, loss = 0.01929059
Iteration 94, loss = 0.01914145
Iteration 95, loss = 0.01940342
Iteration 96, loss = 0.01937834
Iteration 97, loss = 0.01952888
Iteration 98, loss = 0.01911843
Iteration 99, loss = 0.01934144
Iteration 100, loss = 0.01945185
Iteration 101, loss = 0.01924875
Iteration 102, loss = 0.01920425
Iteration 103, loss = 0.01945126
Iteration 104, loss = 0.01919568
Iteration 105, loss = 0.01933361
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67444688
Iteration 2, loss = 0.59822130
Iteration 3, loss = 0.48247445
Iteration 4, loss = 0.36913793
Iteration 5, loss = 0.27454340
Iteration 6, loss = 0.20448237
Iteration 7, loss = 0.15604932
Iteration 8, loss = 0.12292308
Iteration 9, loss = 0.09976920
Iteration 10, loss = 0.08345546
Iteration 11, loss = 0.07161669
Iteration 12, loss = 0.06245978
Iteration 13, loss = 0.05595028
Iteration 14, loss = 0.05035780
Iteration 15, loss = 0.04631598
Iteration 16, loss = 0.04279641
Iteration 17, loss = 0.04013755
Iteration 18, loss = 0.03768342
Iteration 19, loss = 0.03589691
Iteration 20, loss = 0.03458620
Iteration 21, loss = 0.03285230
Iteration 22, loss = 0.03159748
Iteration 23, loss = 0.03076676
Iteration 24, loss = 0.02986960
Iteration 25, loss = 0.02910371
Iteration 26, loss = 0.02851176
Iteration 27, loss = 0.02790070
Iteration 28, loss = 0.02721095
Iteration 29, loss = 0.02683314
Iteration 30, loss = 0.02611865
Iteration 31, loss = 0.02616807
Iteration 32, loss = 0.02577558
Iteration 33, loss = 0.02542743
Iteration 34, loss = 0.02522120
Iteration 35, loss = 0.02492782
Iteration 36, loss = 0.02471106
Iteration 37, loss = 0.02454406
Iteration 38, loss = 0.02425056
Iteration 39, loss = 0.02399826
Iteration 40, loss = 0.02374003
Iteration 41, loss = 0.02352312
Iteration 42, loss = 0.02358491
Iteration 43, loss = 0.02363261
Iteration 44, loss = 0.02327910
Iteration 45, loss = 0.02335829
Iteration 46, loss = 0.02312040
Iteration 47, loss = 0.02303568
Iteration 48, loss = 0.02288141
Iteration 49, loss = 0.02300501
Iteration 50, loss = 0.02282751
Iteration 51, loss = 0.02282086
Iteration 52, loss = 0.02291186
Iteration 53, loss = 0.02259839
Iteration 54, loss = 0.02248208
Iteration 55, loss = 0.02252038
Iteration 56, loss = 0.02210944
Iteration 57, loss = 0.02231692
Iteration 58, loss = 0.02219912
Iteration 59, loss = 0.02205258
Iteration 60, loss = 0.02224643
Iteration 61, loss = 0.02212624
Iteration 62, loss = 0.02199575
Iteration 63, loss = 0.02199673
Iteration 64, loss = 0.02216556
Iteration 65, loss = 0.02231846
Iteration 66, loss = 0.02164307
Iteration 67, loss = 0.02214686
Iteration 68, loss = 0.02207190
Iteration 69, loss = 0.02187796
Iteration 70, loss = 0.02161339
Iteration 71, loss = 0.02189982
Iteration 72, loss = 0.02183765
Iteration 73, loss = 0.02185522
Iteration 74, loss = 0.02195778
Iteration 75, loss = 0.02173707
Iteration 76, loss = 0.02184420
Iteration 77, loss = 0.02152633
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68080260
Iteration 2, loss = 0.60646073
Iteration 3, loss = 0.49653315
Iteration 4, loss = 0.38292485
Iteration 5, loss = 0.28609168
Iteration 6, loss = 0.21275238
Iteration 7, loss = 0.16167392
Iteration 8, loss = 0.12671468
Iteration 9, loss = 0.10287345
Iteration 10, loss = 0.08562907
Iteration 11, loss = 0.07343106
Iteration 12, loss = 0.06435694
Iteration 13, loss = 0.05744825
Iteration 14, loss = 0.05224079
Iteration 15, loss = 0.04776823
Iteration 16, loss = 0.04450835
Iteration 17, loss = 0.04182906
Iteration 18, loss = 0.03937981
Iteration 19, loss = 0.03772223
Iteration 20, loss = 0.03617707
Iteration 21, loss = 0.03475236
Iteration 22, loss = 0.03356368
Iteration 23, loss = 0.03242600
Iteration 24, loss = 0.03159702
Iteration 25, loss = 0.03086474
Iteration 26, loss = 0.03028787
Iteration 27, loss = 0.02995004
Iteration 28, loss = 0.02892366
Iteration 29, loss = 0.02877518
Iteration 30, loss = 0.02815576
Iteration 31, loss = 0.02799577
Iteration 32, loss = 0.02754655
Iteration 33, loss = 0.02721037
Iteration 34, loss = 0.02692532
Iteration 35, loss = 0.02660719
Iteration 36, loss = 0.02678607
Iteration 37, loss = 0.02597171
Iteration 38, loss = 0.02610118
Iteration 39, loss = 0.02583300
Iteration 40, loss = 0.02555420
Iteration 41, loss = 0.02559210
Iteration 42, loss = 0.02506295
Iteration 43, loss = 0.02536681
Iteration 44, loss = 0.02522841
Iteration 45, loss = 0.02493598
Iteration 46, loss = 0.02496163
Iteration 47, loss = 0.02497459
Iteration 48, loss = 0.02475344
Iteration 49, loss = 0.02482840
Iteration 50, loss = 0.02467853
Iteration 51, loss = 0.02442434
Iteration 52, loss = 0.02459710
Iteration 53, loss = 0.02435824
Iteration 54, loss = 0.02453458
Iteration 55, loss = 0.02426069
Iteration 56, loss = 0.02445901
Iteration 57, loss = 0.02421261
Iteration 58, loss = 0.02402334
Iteration 59, loss = 0.02394081
Iteration 60, loss = 0.02399824
Iteration 61, loss = 0.02395372
Iteration 62, loss = 0.02383748
Iteration 63, loss = 0.02403758
Iteration 64, loss = 0.02394348
Iteration 65, loss = 0.02364763
Iteration 66, loss = 0.02412247
Iteration 67, loss = 0.02354648
Iteration 68, loss = 0.02385798
Iteration 69, loss = 0.02386643
Iteration 70, loss = 0.02345004
Iteration 71, loss = 0.02369536
Iteration 72, loss = 0.02362138
Iteration 73, loss = 0.02358905
Iteration 74, loss = 0.02341063
Iteration 75, loss = 0.02351855
Iteration 76, loss = 0.02352730
Iteration 77, loss = 0.02331979
Iteration 78, loss = 0.02363078
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67621383
Iteration 2, loss = 0.59951254
Iteration 3, loss = 0.47965686
Iteration 4, loss = 0.36268204
Iteration 5, loss = 0.26681168
Iteration 6, loss = 0.19810779
Iteration 7, loss = 0.15038698
Iteration 8, loss = 0.11775309
Iteration 9, loss = 0.09554368
Iteration 10, loss = 0.07993186
Iteration 11, loss = 0.06843974
Iteration 12, loss = 0.05999608
Iteration 13, loss = 0.05305755
Iteration 14, loss = 0.04798413
Iteration 15, loss = 0.04404239
Iteration 16, loss = 0.04087923
Iteration 17, loss = 0.03794399
Iteration 18, loss = 0.03569290
Iteration 19, loss = 0.03381717
Iteration 20, loss = 0.03235197
Iteration 21, loss = 0.03094047
Iteration 22, loss = 0.02982779
Iteration 23, loss = 0.02889177
Iteration 24, loss = 0.02792162
Iteration 25, loss = 0.02708358
Iteration 26, loss = 0.02652307
Iteration 27, loss = 0.02568790
Iteration 28, loss = 0.02524666
Iteration 29, loss = 0.02472787
Iteration 30, loss = 0.02438224
Iteration 31, loss = 0.02398573
Iteration 32, loss = 0.02372194
Iteration 33, loss = 0.02335452
Iteration 34, loss = 0.02321705
Iteration 35, loss = 0.02308626
Iteration 36, loss = 0.02251747
Iteration 37, loss = 0.02235926
Iteration 38, loss = 0.02227244
Iteration 39, loss = 0.02202870
Iteration 40, loss = 0.02186939
Iteration 41, loss = 0.02170386
Iteration 42, loss = 0.02181590
Iteration 43, loss = 0.02143763
Iteration 44, loss = 0.02128630
Iteration 45, loss = 0.02111278
Iteration 46, loss = 0.02120699
Iteration 47, loss = 0.02132191
Iteration 48, loss = 0.02076367
Iteration 49, loss = 0.02081197
Iteration 50, loss = 0.02082352
Iteration 51, loss = 0.02050275
Iteration 52, loss = 0.02062900
Iteration 53, loss = 0.02038091
Iteration 54, loss = 0.02055537
Iteration 55, loss = 0.02045917
Iteration 56, loss = 0.02021327
Iteration 57, loss = 0.02028928
Iteration 58, loss = 0.02020591
Iteration 59, loss = 0.02004976
Iteration 60, loss = 0.02032999
Iteration 61, loss = 0.01993823
Iteration 62, loss = 0.02003979
Iteration 63, loss = 0.01976790
Iteration 64, loss = 0.01983441
Iteration 65, loss = 0.01965620
Iteration 66, loss = 0.01995125
Iteration 67, loss = 0.01960601
Iteration 68, loss = 0.01962907
Iteration 69, loss = 0.01979302
Iteration 70, loss = 0.01989970
Iteration 71, loss = 0.01928979
Iteration 72, loss = 0.01984898
Iteration 73, loss = 0.01981152
Iteration 74, loss = 0.01945244
Iteration 75, loss = 0.01955751
Iteration 76, loss = 0.01974649
Iteration 77, loss = 0.01961967
Iteration 78, loss = 0.01956276
Iteration 79, loss = 0.01972851
Iteration 80, loss = 0.01940439
Iteration 81, loss = 0.01918140
Iteration 82, loss = 0.01916543
Iteration 83, loss = 0.01954267
Iteration 84, loss = 0.01967277
Iteration 85, loss = 0.01901106
Iteration 86, loss = 0.01920728
Iteration 87, loss = 0.01942761
Iteration 88, loss = 0.01920641
Iteration 89, loss = 0.01897406
Iteration 90, loss = 0.01924809
Iteration 91, loss = 0.01904916
Iteration 92, loss = 0.01914104
Iteration 93, loss = 0.01905182
Iteration 94, loss = 0.01931245
Iteration 95, loss = 0.01951970
Iteration 96, loss = 0.01899535
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68054056
Iteration 2, loss = 0.60545977
Iteration 3, loss = 0.49673147
Iteration 4, loss = 0.38354187
Iteration 5, loss = 0.28594942
Iteration 6, loss = 0.21214678
Iteration 7, loss = 0.16020352
Iteration 8, loss = 0.12465164
Iteration 9, loss = 0.10037099
Iteration 10, loss = 0.08308249
Iteration 11, loss = 0.07065548
Iteration 12, loss = 0.06131048
Iteration 13, loss = 0.05400466
Iteration 14, loss = 0.04865890
Iteration 15, loss = 0.04448092
Iteration 16, loss = 0.04082148
Iteration 17, loss = 0.03813392
Iteration 18, loss = 0.03586048
Iteration 19, loss = 0.03394741
Iteration 20, loss = 0.03222393
Iteration 21, loss = 0.03105213
Iteration 22, loss = 0.02980169
Iteration 23, loss = 0.02878571
Iteration 24, loss = 0.02779022
Iteration 25, loss = 0.02700039
Iteration 26, loss = 0.02644685
Iteration 27, loss = 0.02560350
Iteration 28, loss = 0.02526589
Iteration 29, loss = 0.02501854
Iteration 30, loss = 0.02441325
Iteration 31, loss = 0.02401498
Iteration 32, loss = 0.02360861
Iteration 33, loss = 0.02340236
Iteration 34, loss = 0.02318800
Iteration 35, loss = 0.02292386
Iteration 36, loss = 0.02266968
Iteration 37, loss = 0.02239800
Iteration 38, loss = 0.02236654
Iteration 39, loss = 0.02220281
Iteration 40, loss = 0.02167001
Iteration 41, loss = 0.02162798
Iteration 42, loss = 0.02162673
Iteration 43, loss = 0.02118816
Iteration 44, loss = 0.02135500
Iteration 45, loss = 0.02112759
Iteration 46, loss = 0.02098427
Iteration 47, loss = 0.02075282
Iteration 48, loss = 0.02090947
Iteration 49, loss = 0.02083483
Iteration 50, loss = 0.02037324
Iteration 51, loss = 0.02080393
Iteration 52, loss = 0.02059224
Iteration 53, loss = 0.02029545
Iteration 54, loss = 0.02012706
Iteration 55, loss = 0.02036361
Iteration 56, loss = 0.02012600
Iteration 57, loss = 0.02015071
Iteration 58, loss = 0.02015651
Iteration 59, loss = 0.01980214
Iteration 60, loss = 0.02004570
Iteration 61, loss = 0.01980637
Iteration 62, loss = 0.01984693
Iteration 63, loss = 0.01994298
Iteration 64, loss = 0.01978383
Iteration 65, loss = 0.01980879
Iteration 66, loss = 0.01968998
Iteration 67, loss = 0.01955209
Iteration 68, loss = 0.01977137
Iteration 69, loss = 0.02001352
Iteration 70, loss = 0.01967408
Iteration 71, loss = 0.01935767
Iteration 72, loss = 0.01950678
Iteration 73, loss = 0.01934745
Iteration 74, loss = 0.01932118
Iteration 75, loss = 0.01955488
Iteration 76, loss = 0.01940157
Iteration 77, loss = 0.01942206
Iteration 78, loss = 0.01936762
Iteration 79, loss = 0.01930423
Iteration 80, loss = 0.01928290
Iteration 81, loss = 0.01969918
Iteration 82, loss = 0.01954757
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68528676
Iteration 2, loss = 0.61129048
Iteration 3, loss = 0.50030402
Iteration 4, loss = 0.38084747
Iteration 5, loss = 0.28041713
Iteration 6, loss = 0.20708413
Iteration 7, loss = 0.15670743
Iteration 8, loss = 0.12215272
Iteration 9, loss = 0.09887547
Iteration 10, loss = 0.08211579
Iteration 11, loss = 0.06998824
Iteration 12, loss = 0.06121564
Iteration 13, loss = 0.05417696
Iteration 14, loss = 0.04883368
Iteration 15, loss = 0.04475953
Iteration 16, loss = 0.04115638
Iteration 17, loss = 0.03862464
Iteration 18, loss = 0.03653673
Iteration 19, loss = 0.03430525
Iteration 20, loss = 0.03267738
Iteration 21, loss = 0.03128562
Iteration 22, loss = 0.02995484
Iteration 23, loss = 0.02892633
Iteration 24, loss = 0.02836385
Iteration 25, loss = 0.02722467
Iteration 26, loss = 0.02669285
Iteration 27, loss = 0.02612211
Iteration 28, loss = 0.02558092
Iteration 29, loss = 0.02489302
Iteration 30, loss = 0.02460728
Iteration 31, loss = 0.02417513
Iteration 32, loss = 0.02401103
Iteration 33, loss = 0.02356565
Iteration 34, loss = 0.02339460
Iteration 35, loss = 0.02314571
Iteration 36, loss = 0.02272182
Iteration 37, loss = 0.02247668
Iteration 38, loss = 0.02243840
Iteration 39, loss = 0.02257453
Iteration 40, loss = 0.02202754
Iteration 41, loss = 0.02207647
Iteration 42, loss = 0.02181954
Iteration 43, loss = 0.02159914
Iteration 44, loss = 0.02134441
Iteration 45, loss = 0.02141774
Iteration 46, loss = 0.02127103
Iteration 47, loss = 0.02110724
Iteration 48, loss = 0.02097609
Iteration 49, loss = 0.02083313
Iteration 50, loss = 0.02092036
Iteration 51, loss = 0.02071453
Iteration 52, loss = 0.02079715
Iteration 53, loss = 0.02061987
Iteration 54, loss = 0.02071879
Iteration 55, loss = 0.02043106
Iteration 56, loss = 0.02077797
Iteration 57, loss = 0.02061626
Iteration 58, loss = 0.02034157
Iteration 59, loss = 0.02044390
Iteration 60, loss = 0.02069394
Iteration 61, loss = 0.02033637
Iteration 62, loss = 0.02027866
Iteration 63, loss = 0.02031612
Iteration 64, loss = 0.02040706
Iteration 65, loss = 0.02013651
Iteration 66, loss = 0.02026142
Iteration 67, loss = 0.02003015
Iteration 68, loss = 0.01993213
Iteration 69, loss = 0.02003249
Iteration 70, loss = 0.01988487
Iteration 71, loss = 0.01992877
Iteration 72, loss = 0.01979803
Iteration 73, loss = 0.01988500
Iteration 74, loss = 0.01958649
Iteration 75, loss = 0.02000146
Iteration 76, loss = 0.01984654
Iteration 77, loss = 0.01963486
Iteration 78, loss = 0.01958201
Iteration 79, loss = 0.01972001
Iteration 80, loss = 0.01965181
Iteration 81, loss = 0.01959879
Iteration 82, loss = 0.01946945
Iteration 83, loss = 0.01978653
Iteration 84, loss = 0.01975035
Iteration 85, loss = 0.01947601
Iteration 86, loss = 0.01982342
Iteration 87, loss = 0.01981996
Iteration 88, loss = 0.01947151
Iteration 89, loss = 0.01957660
Iteration 90, loss = 0.01949606
Iteration 91, loss = 0.01949320
Iteration 92, loss = 0.01950525
Iteration 93, loss = 0.01929059
Iteration 94, loss = 0.01914145
Iteration 95, loss = 0.01940342
Iteration 96, loss = 0.01937834
Iteration 97, loss = 0.01952888
Iteration 98, loss = 0.01911843
Iteration 99, loss = 0.01934144
Iteration 100, loss = 0.01945185
Iteration 101, loss = 0.01924875
Iteration 102, loss = 0.01920425
Iteration 103, loss = 0.01945126
Iteration 104, loss = 0.01919568
Iteration 105, loss = 0.01933361
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68686318
Iteration 2, loss = 0.59115761
Iteration 3, loss = 0.44925934
Iteration 4, loss = 0.32051737
Iteration 5, loss = 0.22581611
Iteration 6, loss = 0.16339428
Iteration 7, loss = 0.12340588
Iteration 8, loss = 0.09749903
Iteration 9, loss = 0.08017078
Iteration 10, loss = 0.06821927
Iteration 11, loss = 0.05957186
Iteration 12, loss = 0.05275984
Iteration 13, loss = 0.04792529
Iteration 14, loss = 0.04429932
Iteration 15, loss = 0.04119918
Iteration 16, loss = 0.03864787
Iteration 17, loss = 0.03675898
Iteration 18, loss = 0.03499398
Iteration 19, loss = 0.03370493
Iteration 20, loss = 0.03226059
Iteration 21, loss = 0.03139787
Iteration 22, loss = 0.03027210
Iteration 23, loss = 0.02980130
Iteration 24, loss = 0.02916782
Iteration 25, loss = 0.02868890
Iteration 26, loss = 0.02797085
Iteration 27, loss = 0.02761849
Iteration 28, loss = 0.02701012
Iteration 29, loss = 0.02704360
Iteration 30, loss = 0.02650709
Iteration 31, loss = 0.02635084
Iteration 32, loss = 0.02601911
Iteration 33, loss = 0.02598807
Iteration 34, loss = 0.02588093
Iteration 35, loss = 0.02536049
Iteration 36, loss = 0.02529956
Iteration 37, loss = 0.02538120
Iteration 38, loss = 0.02506868
Iteration 39, loss = 0.02486404
Iteration 40, loss = 0.02483179
Iteration 41, loss = 0.02467321
Iteration 42, loss = 0.02456439
Iteration 43, loss = 0.02456805
Iteration 44, loss = 0.02431541
Iteration 45, loss = 0.02423882
Iteration 46, loss = 0.02477134
Iteration 47, loss = 0.02442331
Iteration 48, loss = 0.02451742
Iteration 49, loss = 0.02383546
Iteration 50, loss = 0.02391574
Iteration 51, loss = 0.02377357
Iteration 52, loss = 0.02373350
Iteration 53, loss = 0.02395590
Iteration 54, loss = 0.02356806
Iteration 55, loss = 0.02363044
Iteration 56, loss = 0.02341980
Iteration 57, loss = 0.02378992
Iteration 58, loss = 0.02369307
Iteration 59, loss = 0.02371547
Iteration 60, loss = 0.02351927
Iteration 61, loss = 0.02342553
Iteration 62, loss = 0.02346706
Iteration 63, loss = 0.02376104
Iteration 64, loss = 0.02334470
Iteration 65, loss = 0.02356276
Iteration 66, loss = 0.02325943
Iteration 67, loss = 0.02351124
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on prepended_v10_lemma_stem_custom_stopwords...
Iteration 1, loss = 0.73849604
Iteration 2, loss = 0.71743983
Iteration 3, loss = 0.70094206
Iteration 4, loss = 0.68552995
Iteration 5, loss = 0.66975811
Iteration 6, loss = 0.65328769
Iteration 7, loss = 0.63587250
Iteration 8, loss = 0.61739420
Iteration 9, loss = 0.59789166
Iteration 10, loss = 0.57727622
Iteration 11, loss = 0.55576792
Iteration 12, loss = 0.53323830
Iteration 13, loss = 0.51033179
Iteration 14, loss = 0.48671362
Iteration 15, loss = 0.46302453
Iteration 16, loss = 0.43933085
Iteration 17, loss = 0.41611591
Iteration 18, loss = 0.39305607
Iteration 19, loss = 0.37081806
Iteration 20, loss = 0.34928581
Iteration 21, loss = 0.32861373
Iteration 22, loss = 0.30889834
Iteration 23, loss = 0.29015799
Iteration 24, loss = 0.27238794
Iteration 25, loss = 0.25566796
Iteration 26, loss = 0.23986305
Iteration 27, loss = 0.22509652
Iteration 28, loss = 0.21120479
Iteration 29, loss = 0.19825809
Iteration 30, loss = 0.18625160
Iteration 31, loss = 0.17496973
Iteration 32, loss = 0.16455033
Iteration 33, loss = 0.15487765
Iteration 34, loss = 0.14592017
Iteration 35, loss = 0.13767382
Iteration 36, loss = 0.12995717
Iteration 37, loss = 0.12281899
Iteration 38, loss = 0.11621616
Iteration 39, loss = 0.11008041
Iteration 40, loss = 0.10438291
Iteration 41, loss = 0.09906662
Iteration 42, loss = 0.09424021
Iteration 43, loss = 0.08959528
Iteration 44, loss = 0.08535949
Iteration 45, loss = 0.08142262
Iteration 46, loss = 0.07769699
Iteration 47, loss = 0.07426905
Iteration 48, loss = 0.07100814
Iteration 49, loss = 0.06802288
Iteration 50, loss = 0.06518280
Iteration 51, loss = 0.06252800
Iteration 52, loss = 0.06004387
Iteration 53, loss = 0.05774216
Iteration 54, loss = 0.05552786
Iteration 55, loss = 0.05348394
Iteration 56, loss = 0.05153869
Iteration 57, loss = 0.04973935
Iteration 58, loss = 0.04807677
Iteration 59, loss = 0.04644329
Iteration 60, loss = 0.04492750
Iteration 61, loss = 0.04344559
Iteration 62, loss = 0.04206617
Iteration 63, loss = 0.04076448
Iteration 64, loss = 0.03954723
Iteration 65, loss = 0.03835862
Iteration 66, loss = 0.03721239
Iteration 67, loss = 0.03612714
Iteration 68, loss = 0.03511639
Iteration 69, loss = 0.03416335
Iteration 70, loss = 0.03320784
Iteration 71, loss = 0.03232122
Iteration 72, loss = 0.03149964
Iteration 73, loss = 0.03069388
Iteration 74, loss = 0.02990633
Iteration 75, loss = 0.02919777
Iteration 76, loss = 0.02846438
Iteration 77, loss = 0.02778049
Iteration 78, loss = 0.02715319
Iteration 79, loss = 0.02649444
Iteration 80, loss = 0.02591574
Iteration 81, loss = 0.02533213
Iteration 82, loss = 0.02478256
Iteration 83, loss = 0.02430626
Iteration 84, loss = 0.02371967
Iteration 85, loss = 0.02323316
Iteration 86, loss = 0.02276512
Iteration 87, loss = 0.02229671
Iteration 88, loss = 0.02184990
Iteration 89, loss = 0.02142142
Iteration 90, loss = 0.02100486
Iteration 91, loss = 0.02059138
Iteration 92, loss = 0.02022384
Iteration 93, loss = 0.01988830
Iteration 94, loss = 0.01950159
Iteration 95, loss = 0.01917184
Iteration 96, loss = 0.01884313
Iteration 97, loss = 0.01852336
Iteration 98, loss = 0.01820578
Iteration 99, loss = 0.01792639
Iteration 100, loss = 0.01758989
Iteration 101, loss = 0.01731166
Iteration 102, loss = 0.01705120
Iteration 103, loss = 0.01678840
Iteration 104, loss = 0.01649659
Iteration 105, loss = 0.01630026
Iteration 106, loss = 0.01600940
Iteration 107, loss = 0.01579108
Iteration 108, loss = 0.01554040
Iteration 109, loss = 0.01533174
Iteration 110, loss = 0.01512179
Iteration 111, loss = 0.01489924
Iteration 112, loss = 0.01470227
Iteration 113, loss = 0.01451553
Iteration 114, loss = 0.01432202
Iteration 115, loss = 0.01414857
Iteration 116, loss = 0.01397691
Iteration 117, loss = 0.01380918
Iteration 118, loss = 0.01362736
Iteration 119, loss = 0.01344371
Iteration 120, loss = 0.01331085
Iteration 121, loss = 0.01312337
Iteration 122, loss = 0.01296748
Iteration 123, loss = 0.01280757
Iteration 124, loss = 0.01269816
Iteration 125, loss = 0.01255957
Iteration 126, loss = 0.01238520
Iteration 127, loss = 0.01224027
Iteration 128, loss = 0.01211253
Iteration 129, loss = 0.01198977
Iteration 130, loss = 0.01185525
Iteration 131, loss = 0.01171073
Iteration 132, loss = 0.01158445
Iteration 133, loss = 0.01147474
Iteration 134, loss = 0.01133730
Iteration 135, loss = 0.01120999
Iteration 136, loss = 0.01110018
Iteration 137, loss = 0.01104610
Iteration 138, loss = 0.01096385
Iteration 139, loss = 0.01087164
Iteration 140, loss = 0.01077898
Iteration 141, loss = 0.01069991
Iteration 142, loss = 0.01062480
Iteration 143, loss = 0.01056209
Iteration 144, loss = 0.01046337
Iteration 145, loss = 0.01037463
Iteration 146, loss = 0.01027256
Iteration 147, loss = 0.01018780
Iteration 148, loss = 0.01009281
Iteration 149, loss = 0.00998995
Iteration 150, loss = 0.00992354
Iteration 151, loss = 0.00978027
Iteration 152, loss = 0.00969131
Iteration 153, loss = 0.00960799
Iteration 154, loss = 0.00957023
Iteration 155, loss = 0.00946234
Iteration 156, loss = 0.00937927
Iteration 157, loss = 0.00930833
Iteration 158, loss = 0.00923291
Iteration 159, loss = 0.00917510
Iteration 160, loss = 0.00912078
Iteration 161, loss = 0.00902182
Iteration 162, loss = 0.00895519
Iteration 163, loss = 0.00888884
Iteration 164, loss = 0.00883165
Iteration 165, loss = 0.00876695
Iteration 166, loss = 0.00870663
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72309035
Iteration 2, loss = 0.69526843
Iteration 3, loss = 0.66222796
Iteration 4, loss = 0.62292524
Iteration 5, loss = 0.57785057
Iteration 6, loss = 0.52873847
Iteration 7, loss = 0.47717429
Iteration 8, loss = 0.42548875
Iteration 9, loss = 0.37552662
Iteration 10, loss = 0.32935199
Iteration 11, loss = 0.28813570
Iteration 12, loss = 0.25160199
Iteration 13, loss = 0.22004320
Iteration 14, loss = 0.19291306
Iteration 15, loss = 0.16976556
Iteration 16, loss = 0.15014282
Iteration 17, loss = 0.13345763
Iteration 18, loss = 0.11935671
Iteration 19, loss = 0.10747130
Iteration 20, loss = 0.09732134
Iteration 21, loss = 0.08862332
Iteration 22, loss = 0.08103108
Iteration 23, loss = 0.07455712
Iteration 24, loss = 0.06896598
Iteration 25, loss = 0.06391807
Iteration 26, loss = 0.05963187
Iteration 27, loss = 0.05577609
Iteration 28, loss = 0.05233698
Iteration 29, loss = 0.04933977
Iteration 30, loss = 0.04661683
Iteration 31, loss = 0.04409549
Iteration 32, loss = 0.04194631
Iteration 33, loss = 0.03991573
Iteration 34, loss = 0.03813298
Iteration 35, loss = 0.03642614
Iteration 36, loss = 0.03487456
Iteration 37, loss = 0.03345640
Iteration 38, loss = 0.03237075
Iteration 39, loss = 0.03121976
Iteration 40, loss = 0.03015404
Iteration 41, loss = 0.02909412
Iteration 42, loss = 0.02808111
Iteration 43, loss = 0.02721185
Iteration 44, loss = 0.02640331
Iteration 45, loss = 0.02566960
Iteration 46, loss = 0.02505306
Iteration 47, loss = 0.02449807
Iteration 48, loss = 0.02383524
Iteration 49, loss = 0.02328993
Iteration 50, loss = 0.02281781
Iteration 51, loss = 0.02239389
Iteration 52, loss = 0.02186027
Iteration 53, loss = 0.02154586
Iteration 54, loss = 0.02103966
Iteration 55, loss = 0.02072440
Iteration 56, loss = 0.02031947
Iteration 57, loss = 0.01998162
Iteration 58, loss = 0.01954926
Iteration 59, loss = 0.01958057
Iteration 60, loss = 0.01930672
Iteration 61, loss = 0.01914001
Iteration 62, loss = 0.01885445
Iteration 63, loss = 0.01844552
Iteration 64, loss = 0.01813924
Iteration 65, loss = 0.01779957
Iteration 66, loss = 0.01750182
Iteration 67, loss = 0.01726431
Iteration 68, loss = 0.01700834
Iteration 69, loss = 0.01698955
Iteration 70, loss = 0.01679777
Iteration 71, loss = 0.01658007
Iteration 72, loss = 0.01641082
Iteration 73, loss = 0.01631345
Iteration 74, loss = 0.01603879
Iteration 75, loss = 0.01610328
Iteration 76, loss = 0.01603967
Iteration 77, loss = 0.01565534
Iteration 78, loss = 0.01558159
Iteration 79, loss = 0.01547973
Iteration 80, loss = 0.01543073
Iteration 81, loss = 0.01539424
Iteration 82, loss = 0.01525725
Iteration 83, loss = 0.01518472
Iteration 84, loss = 0.01492875
Iteration 85, loss = 0.01483918
Iteration 86, loss = 0.01483562
Iteration 87, loss = 0.01473116
Iteration 88, loss = 0.01452402
Iteration 89, loss = 0.01442201
Iteration 90, loss = 0.01440315
Iteration 91, loss = 0.01432554
Iteration 92, loss = 0.01424915
Iteration 93, loss = 0.01413789
Iteration 94, loss = 0.01409860
Iteration 95, loss = 0.01398792
Iteration 96, loss = 0.01389838
Iteration 97, loss = 0.01380549
Iteration 98, loss = 0.01377059
Iteration 99, loss = 0.01383233
Iteration 100, loss = 0.01378034
Iteration 101, loss = 0.01371202
Iteration 102, loss = 0.01360904
Iteration 103, loss = 0.01363816
Iteration 104, loss = 0.01377770
Iteration 105, loss = 0.01357361
Iteration 106, loss = 0.01365927
Iteration 107, loss = 0.01359452
Iteration 108, loss = 0.01354359
Iteration 109, loss = 0.01346123
Iteration 110, loss = 0.01341882
Iteration 111, loss = 0.01334637
Iteration 112, loss = 0.01325824
Iteration 113, loss = 0.01315338
Iteration 114, loss = 0.01308338
Iteration 115, loss = 0.01319683
Iteration 116, loss = 0.01332412
Iteration 117, loss = 0.01357235
Iteration 118, loss = 0.01355147
Iteration 119, loss = 0.01323395
Iteration 120, loss = 0.01304653
Iteration 121, loss = 0.01292232
Iteration 122, loss = 0.01278333
Iteration 123, loss = 0.01278959
Iteration 124, loss = 0.01272907
Iteration 125, loss = 0.01271308
Iteration 126, loss = 0.01272321
Iteration 127, loss = 0.01272589
Iteration 128, loss = 0.01274627
Iteration 129, loss = 0.01266431
Iteration 130, loss = 0.01249944
Iteration 131, loss = 0.01265459
Iteration 132, loss = 0.01258047
Iteration 133, loss = 0.01277127
Iteration 134, loss = 0.01275901
Iteration 135, loss = 0.01253560
Iteration 136, loss = 0.01261023
Iteration 137, loss = 0.01258891
Iteration 138, loss = 0.01256132
Iteration 139, loss = 0.01246452
Iteration 140, loss = 0.01240553
Iteration 141, loss = 0.01231430
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67704931
Iteration 2, loss = 0.65074682
Iteration 3, loss = 0.61958666
Iteration 4, loss = 0.57932912
Iteration 5, loss = 0.53175952
Iteration 6, loss = 0.48062418
Iteration 7, loss = 0.42963870
Iteration 8, loss = 0.38109160
Iteration 9, loss = 0.33549241
Iteration 10, loss = 0.29420644
Iteration 11, loss = 0.25737741
Iteration 12, loss = 0.22503431
Iteration 13, loss = 0.19719861
Iteration 14, loss = 0.17336736
Iteration 15, loss = 0.15304093
Iteration 16, loss = 0.13574289
Iteration 17, loss = 0.12110812
Iteration 18, loss = 0.10853782
Iteration 19, loss = 0.09781606
Iteration 20, loss = 0.08866261
Iteration 21, loss = 0.08073057
Iteration 22, loss = 0.07395240
Iteration 23, loss = 0.06783446
Iteration 24, loss = 0.06266634
Iteration 25, loss = 0.05802874
Iteration 26, loss = 0.05404133
Iteration 27, loss = 0.05055700
Iteration 28, loss = 0.04731894
Iteration 29, loss = 0.04454040
Iteration 30, loss = 0.04197762
Iteration 31, loss = 0.03980003
Iteration 32, loss = 0.03770343
Iteration 33, loss = 0.03584071
Iteration 34, loss = 0.03413318
Iteration 35, loss = 0.03262279
Iteration 36, loss = 0.03123325
Iteration 37, loss = 0.03001111
Iteration 38, loss = 0.02879659
Iteration 39, loss = 0.02773606
Iteration 40, loss = 0.02673635
Iteration 41, loss = 0.02587419
Iteration 42, loss = 0.02493227
Iteration 43, loss = 0.02421099
Iteration 44, loss = 0.02345844
Iteration 45, loss = 0.02276286
Iteration 46, loss = 0.02216775
Iteration 47, loss = 0.02157010
Iteration 48, loss = 0.02099700
Iteration 49, loss = 0.02043924
Iteration 50, loss = 0.01994108
Iteration 51, loss = 0.01949911
Iteration 52, loss = 0.01915279
Iteration 53, loss = 0.01885412
Iteration 54, loss = 0.01837065
Iteration 55, loss = 0.01824131
Iteration 56, loss = 0.01778352
Iteration 57, loss = 0.01743451
Iteration 58, loss = 0.01704686
Iteration 59, loss = 0.01675936
Iteration 60, loss = 0.01650751
Iteration 61, loss = 0.01625210
Iteration 62, loss = 0.01599912
Iteration 63, loss = 0.01593986
Iteration 64, loss = 0.01587263
Iteration 65, loss = 0.01567888
Iteration 66, loss = 0.01530612
Iteration 67, loss = 0.01506868
Iteration 68, loss = 0.01485164
Iteration 69, loss = 0.01459001
Iteration 70, loss = 0.01442799
Iteration 71, loss = 0.01427076
Iteration 72, loss = 0.01406234
Iteration 73, loss = 0.01398213
Iteration 74, loss = 0.01388670
Iteration 75, loss = 0.01377455
Iteration 76, loss = 0.01366369
Iteration 77, loss = 0.01348009
Iteration 78, loss = 0.01336353
Iteration 79, loss = 0.01320162
Iteration 80, loss = 0.01312430
Iteration 81, loss = 0.01302826
Iteration 82, loss = 0.01291371
Iteration 83, loss = 0.01284163
Iteration 84, loss = 0.01280705
Iteration 85, loss = 0.01266021
Iteration 86, loss = 0.01259961
Iteration 87, loss = 0.01248708
Iteration 88, loss = 0.01245037
Iteration 89, loss = 0.01238405
Iteration 90, loss = 0.01220904
Iteration 91, loss = 0.01219317
Iteration 92, loss = 0.01207414
Iteration 93, loss = 0.01199032
Iteration 94, loss = 0.01191118
Iteration 95, loss = 0.01190421
Iteration 96, loss = 0.01186395
Iteration 97, loss = 0.01176987
Iteration 98, loss = 0.01185237
Iteration 99, loss = 0.01166786
Iteration 100, loss = 0.01160040
Iteration 101, loss = 0.01164147
Iteration 102, loss = 0.01149636
Iteration 103, loss = 0.01151653
Iteration 104, loss = 0.01145639
Iteration 105, loss = 0.01135970
Iteration 106, loss = 0.01140206
Iteration 107, loss = 0.01136583
Iteration 108, loss = 0.01124536
Iteration 109, loss = 0.01125801
Iteration 110, loss = 0.01116479
Iteration 111, loss = 0.01114465
Iteration 112, loss = 0.01112987
Iteration 113, loss = 0.01111285
Iteration 114, loss = 0.01105893
Iteration 115, loss = 0.01108128
Iteration 116, loss = 0.01106417
Iteration 117, loss = 0.01102160
Iteration 118, loss = 0.01094546
Iteration 119, loss = 0.01088865
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68413809
Iteration 2, loss = 0.63781019
Iteration 3, loss = 0.57301143
Iteration 4, loss = 0.49112492
Iteration 5, loss = 0.40739785
Iteration 6, loss = 0.33093773
Iteration 7, loss = 0.26629841
Iteration 8, loss = 0.21408372
Iteration 9, loss = 0.17387918
Iteration 10, loss = 0.14313218
Iteration 11, loss = 0.11977843
Iteration 12, loss = 0.10228539
Iteration 13, loss = 0.08841321
Iteration 14, loss = 0.07763678
Iteration 15, loss = 0.06913281
Iteration 16, loss = 0.06230633
Iteration 17, loss = 0.05665626
Iteration 18, loss = 0.05211301
Iteration 19, loss = 0.04826943
Iteration 20, loss = 0.04514709
Iteration 21, loss = 0.04241106
Iteration 22, loss = 0.04014964
Iteration 23, loss = 0.03815252
Iteration 24, loss = 0.03654645
Iteration 25, loss = 0.03476771
Iteration 26, loss = 0.03361690
Iteration 27, loss = 0.03251939
Iteration 28, loss = 0.03133863
Iteration 29, loss = 0.03045362
Iteration 30, loss = 0.02953968
Iteration 31, loss = 0.02895861
Iteration 32, loss = 0.02826045
Iteration 33, loss = 0.02751408
Iteration 34, loss = 0.02727034
Iteration 35, loss = 0.02658456
Iteration 36, loss = 0.02623186
Iteration 37, loss = 0.02558968
Iteration 38, loss = 0.02539853
Iteration 39, loss = 0.02504125
Iteration 40, loss = 0.02481877
Iteration 41, loss = 0.02446830
Iteration 42, loss = 0.02430708
Iteration 43, loss = 0.02399414
Iteration 44, loss = 0.02368157
Iteration 45, loss = 0.02341790
Iteration 46, loss = 0.02343089
Iteration 47, loss = 0.02312088
Iteration 48, loss = 0.02279574
Iteration 49, loss = 0.02275811
Iteration 50, loss = 0.02283240
Iteration 51, loss = 0.02242744
Iteration 52, loss = 0.02226434
Iteration 53, loss = 0.02213832
Iteration 54, loss = 0.02206650
Iteration 55, loss = 0.02205767
Iteration 56, loss = 0.02183570
Iteration 57, loss = 0.02155170
Iteration 58, loss = 0.02149051
Iteration 59, loss = 0.02156807
Iteration 60, loss = 0.02145368
Iteration 61, loss = 0.02130320
Iteration 62, loss = 0.02116497
Iteration 63, loss = 0.02128096
Iteration 64, loss = 0.02132282
Iteration 65, loss = 0.02107344
Iteration 66, loss = 0.02137134
Iteration 67, loss = 0.02124727
Iteration 68, loss = 0.02078011
Iteration 69, loss = 0.02081218
Iteration 70, loss = 0.02082293
Iteration 71, loss = 0.02068431
Iteration 72, loss = 0.02048970
Iteration 73, loss = 0.02050955
Iteration 74, loss = 0.02044067
Iteration 75, loss = 0.02073189
Iteration 76, loss = 0.02037559
Iteration 77, loss = 0.02042193
Iteration 78, loss = 0.02039324
Iteration 79, loss = 0.02033969
Iteration 80, loss = 0.02020541
Iteration 81, loss = 0.02023048
Iteration 82, loss = 0.02035603
Iteration 83, loss = 0.02041504
Iteration 84, loss = 0.02012673
Iteration 85, loss = 0.02000037
Iteration 86, loss = 0.02027423
Iteration 87, loss = 0.02012230
Iteration 88, loss = 0.02004886
Iteration 89, loss = 0.02007492
Iteration 90, loss = 0.02008447
Iteration 91, loss = 0.01990615
Iteration 92, loss = 0.02009681
Iteration 93, loss = 0.02009982
Iteration 94, loss = 0.01985261
Iteration 95, loss = 0.01994101
Iteration 96, loss = 0.01977558
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69015043
Iteration 2, loss = 0.64052236
Iteration 3, loss = 0.57210526
Iteration 4, loss = 0.48459675
Iteration 5, loss = 0.39893285
Iteration 6, loss = 0.32236020
Iteration 7, loss = 0.25867418
Iteration 8, loss = 0.20825049
Iteration 9, loss = 0.16960851
Iteration 10, loss = 0.13973423
Iteration 11, loss = 0.11727372
Iteration 12, loss = 0.10003071
Iteration 13, loss = 0.08644724
Iteration 14, loss = 0.07581177
Iteration 15, loss = 0.06743174
Iteration 16, loss = 0.06049099
Iteration 17, loss = 0.05499280
Iteration 18, loss = 0.05035691
Iteration 19, loss = 0.04657981
Iteration 20, loss = 0.04323216
Iteration 21, loss = 0.04027571
Iteration 22, loss = 0.03797112
Iteration 23, loss = 0.03618224
Iteration 24, loss = 0.03415182
Iteration 25, loss = 0.03273588
Iteration 26, loss = 0.03129560
Iteration 27, loss = 0.03014627
Iteration 28, loss = 0.02878397
Iteration 29, loss = 0.02789328
Iteration 30, loss = 0.02707906
Iteration 31, loss = 0.02618017
Iteration 32, loss = 0.02545124
Iteration 33, loss = 0.02495484
Iteration 34, loss = 0.02419643
Iteration 35, loss = 0.02375527
Iteration 36, loss = 0.02335299
Iteration 37, loss = 0.02282980
Iteration 38, loss = 0.02235531
Iteration 39, loss = 0.02205720
Iteration 40, loss = 0.02167046
Iteration 41, loss = 0.02158738
Iteration 42, loss = 0.02136024
Iteration 43, loss = 0.02086008
Iteration 44, loss = 0.02049136
Iteration 45, loss = 0.02046533
Iteration 46, loss = 0.02014437
Iteration 47, loss = 0.02007591
Iteration 48, loss = 0.01987554
Iteration 49, loss = 0.01968557
Iteration 50, loss = 0.01962788
Iteration 51, loss = 0.01921193
Iteration 52, loss = 0.01918719
Iteration 53, loss = 0.01871712
Iteration 54, loss = 0.01890566
Iteration 55, loss = 0.01885956
Iteration 56, loss = 0.01865623
Iteration 57, loss = 0.01859348
Iteration 58, loss = 0.01822081
Iteration 59, loss = 0.01830979
Iteration 60, loss = 0.01820365
Iteration 61, loss = 0.01807932
Iteration 62, loss = 0.01802072
Iteration 63, loss = 0.01788171
Iteration 64, loss = 0.01784369
Iteration 65, loss = 0.01769317
Iteration 66, loss = 0.01745647
Iteration 67, loss = 0.01771757
Iteration 68, loss = 0.01778126
Iteration 69, loss = 0.01752403
Iteration 70, loss = 0.01706671
Iteration 71, loss = 0.01723733
Iteration 72, loss = 0.01713396
Iteration 73, loss = 0.01720805
Iteration 74, loss = 0.01699542
Iteration 75, loss = 0.01693856
Iteration 76, loss = 0.01694710
Iteration 77, loss = 0.01683016
Iteration 78, loss = 0.01691485
Iteration 79, loss = 0.01688544
Iteration 80, loss = 0.01672329
Iteration 81, loss = 0.01674586
Iteration 82, loss = 0.01675627
Iteration 83, loss = 0.01687832
Iteration 84, loss = 0.01690205
Iteration 85, loss = 0.01692332
Iteration 86, loss = 0.01673445
Iteration 87, loss = 0.01663792
Iteration 88, loss = 0.01660769
Iteration 89, loss = 0.01665048
Iteration 90, loss = 0.01658003
Iteration 91, loss = 0.01656432
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70232327
Iteration 2, loss = 0.68023894
Iteration 3, loss = 0.65990709
Iteration 4, loss = 0.63718721
Iteration 5, loss = 0.61146897
Iteration 6, loss = 0.58214479
Iteration 7, loss = 0.54933180
Iteration 8, loss = 0.51345763
Iteration 9, loss = 0.47576566
Iteration 10, loss = 0.43703528
Iteration 11, loss = 0.39927589
Iteration 12, loss = 0.36289813
Iteration 13, loss = 0.32906064
Iteration 14, loss = 0.29781578
Iteration 15, loss = 0.26922587
Iteration 16, loss = 0.24356416
Iteration 17, loss = 0.22043049
Iteration 18, loss = 0.19973554
Iteration 19, loss = 0.18123639
Iteration 20, loss = 0.16485965
Iteration 21, loss = 0.15028520
Iteration 22, loss = 0.13736767
Iteration 23, loss = 0.12590167
Iteration 24, loss = 0.11579996
Iteration 25, loss = 0.10674344
Iteration 26, loss = 0.09857186
Iteration 27, loss = 0.09153824
Iteration 28, loss = 0.08515353
Iteration 29, loss = 0.07932888
Iteration 30, loss = 0.07420528
Iteration 31, loss = 0.06952974
Iteration 32, loss = 0.06540808
Iteration 33, loss = 0.06157031
Iteration 34, loss = 0.05813964
Iteration 35, loss = 0.05511004
Iteration 36, loss = 0.05221657
Iteration 37, loss = 0.04967134
Iteration 38, loss = 0.04728239
Iteration 39, loss = 0.04512140
Iteration 40, loss = 0.04315136
Iteration 41, loss = 0.04132274
Iteration 42, loss = 0.03962458
Iteration 43, loss = 0.03806751
Iteration 44, loss = 0.03663642
Iteration 45, loss = 0.03525121
Iteration 46, loss = 0.03399620
Iteration 47, loss = 0.03282727
Iteration 48, loss = 0.03179097
Iteration 49, loss = 0.03074600
Iteration 50, loss = 0.02987310
Iteration 51, loss = 0.02895399
Iteration 52, loss = 0.02809530
Iteration 53, loss = 0.02736791
Iteration 54, loss = 0.02661548
Iteration 55, loss = 0.02590053
Iteration 56, loss = 0.02527730
Iteration 57, loss = 0.02463905
Iteration 58, loss = 0.02404912
Iteration 59, loss = 0.02348760
Iteration 60, loss = 0.02297118
Iteration 61, loss = 0.02249316
Iteration 62, loss = 0.02200466
Iteration 63, loss = 0.02157977
Iteration 64, loss = 0.02111626
Iteration 65, loss = 0.02071186
Iteration 66, loss = 0.02032542
Iteration 67, loss = 0.01995930
Iteration 68, loss = 0.01961709
Iteration 69, loss = 0.01930833
Iteration 70, loss = 0.01898623
Iteration 71, loss = 0.01870952
Iteration 72, loss = 0.01838097
Iteration 73, loss = 0.01812729
Iteration 74, loss = 0.01782284
Iteration 75, loss = 0.01759989
Iteration 76, loss = 0.01738683
Iteration 77, loss = 0.01711155
Iteration 78, loss = 0.01689117
Iteration 79, loss = 0.01667378
Iteration 80, loss = 0.01646802
Iteration 81, loss = 0.01628663
Iteration 82, loss = 0.01613780
Iteration 83, loss = 0.01589767
Iteration 84, loss = 0.01573228
Iteration 85, loss = 0.01557099
Iteration 86, loss = 0.01537113
Iteration 87, loss = 0.01525072
Iteration 88, loss = 0.01509602
Iteration 89, loss = 0.01491064
Iteration 90, loss = 0.01483014
Iteration 91, loss = 0.01463844
Iteration 92, loss = 0.01450240
Iteration 93, loss = 0.01439419
Iteration 94, loss = 0.01429346
Iteration 95, loss = 0.01417704
Iteration 96, loss = 0.01404909
Iteration 97, loss = 0.01395751
Iteration 98, loss = 0.01384187
Iteration 99, loss = 0.01369326
Iteration 100, loss = 0.01357253
Iteration 101, loss = 0.01349817
Iteration 102, loss = 0.01338405
Iteration 103, loss = 0.01328797
Iteration 104, loss = 0.01323901
Iteration 105, loss = 0.01313843
Iteration 106, loss = 0.01306353
Iteration 107, loss = 0.01297145
Iteration 108, loss = 0.01291878
Iteration 109, loss = 0.01285720
Iteration 110, loss = 0.01278058
Iteration 111, loss = 0.01266588
Iteration 112, loss = 0.01261988
Iteration 113, loss = 0.01250669
Iteration 114, loss = 0.01243355
Iteration 115, loss = 0.01238726
Iteration 116, loss = 0.01232191
Iteration 117, loss = 0.01221452
Iteration 118, loss = 0.01215525
Iteration 119, loss = 0.01213688
Iteration 120, loss = 0.01214913
Iteration 121, loss = 0.01196470
Iteration 122, loss = 0.01198149
Iteration 123, loss = 0.01187042
Iteration 124, loss = 0.01184590
Iteration 125, loss = 0.01177852
Iteration 126, loss = 0.01175475
Iteration 127, loss = 0.01173786
Iteration 128, loss = 0.01162016
Iteration 129, loss = 0.01165352
Iteration 130, loss = 0.01157241
Iteration 131, loss = 0.01149624
Iteration 132, loss = 0.01148165
Iteration 133, loss = 0.01141141
Iteration 134, loss = 0.01137005
Iteration 135, loss = 0.01135742
Iteration 136, loss = 0.01136083
Iteration 137, loss = 0.01123462
Iteration 138, loss = 0.01123661
Iteration 139, loss = 0.01121220
Iteration 140, loss = 0.01116995
Iteration 141, loss = 0.01116670
Iteration 142, loss = 0.01108403
Iteration 143, loss = 0.01103952
Iteration 144, loss = 0.01102552
Iteration 145, loss = 0.01098279
Iteration 146, loss = 0.01094723
Iteration 147, loss = 0.01093122
Iteration 148, loss = 0.01093354
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69804999
Iteration 2, loss = 0.66428034
Iteration 3, loss = 0.61479114
Iteration 4, loss = 0.54812167
Iteration 5, loss = 0.47362971
Iteration 6, loss = 0.40098388
Iteration 7, loss = 0.33483759
Iteration 8, loss = 0.27765058
Iteration 9, loss = 0.22994293
Iteration 10, loss = 0.19105400
Iteration 11, loss = 0.16005219
Iteration 12, loss = 0.13537141
Iteration 13, loss = 0.11577086
Iteration 14, loss = 0.10024937
Iteration 15, loss = 0.08777699
Iteration 16, loss = 0.07768341
Iteration 17, loss = 0.06945783
Iteration 18, loss = 0.06263038
Iteration 19, loss = 0.05697826
Iteration 20, loss = 0.05207347
Iteration 21, loss = 0.04811513
Iteration 22, loss = 0.04461055
Iteration 23, loss = 0.04161470
Iteration 24, loss = 0.03906436
Iteration 25, loss = 0.03672888
Iteration 26, loss = 0.03467984
Iteration 27, loss = 0.03305385
Iteration 28, loss = 0.03159866
Iteration 29, loss = 0.03008676
Iteration 30, loss = 0.02888626
Iteration 31, loss = 0.02773216
Iteration 32, loss = 0.02670841
Iteration 33, loss = 0.02584680
Iteration 34, loss = 0.02500537
Iteration 35, loss = 0.02441557
Iteration 36, loss = 0.02361528
Iteration 37, loss = 0.02314187
Iteration 38, loss = 0.02247254
Iteration 39, loss = 0.02191080
Iteration 40, loss = 0.02136996
Iteration 41, loss = 0.02107045
Iteration 42, loss = 0.02054980
Iteration 43, loss = 0.02019330
Iteration 44, loss = 0.01983347
Iteration 45, loss = 0.01951978
Iteration 46, loss = 0.01931067
Iteration 47, loss = 0.01900804
Iteration 48, loss = 0.01862402
Iteration 49, loss = 0.01834359
Iteration 50, loss = 0.01814020
Iteration 51, loss = 0.01808622
Iteration 52, loss = 0.01772880
Iteration 53, loss = 0.01762543
Iteration 54, loss = 0.01732568
Iteration 55, loss = 0.01710482
Iteration 56, loss = 0.01707996
Iteration 57, loss = 0.01687917
Iteration 58, loss = 0.01669491
Iteration 59, loss = 0.01665059
Iteration 60, loss = 0.01643456
Iteration 61, loss = 0.01634718
Iteration 62, loss = 0.01622124
Iteration 63, loss = 0.01619651
Iteration 64, loss = 0.01615499
Iteration 65, loss = 0.01596585
Iteration 66, loss = 0.01574836
Iteration 67, loss = 0.01563004
Iteration 68, loss = 0.01569326
Iteration 69, loss = 0.01549649
Iteration 70, loss = 0.01541852
Iteration 71, loss = 0.01549021
Iteration 72, loss = 0.01527056
Iteration 73, loss = 0.01522876
Iteration 74, loss = 0.01515287
Iteration 75, loss = 0.01514442
Iteration 76, loss = 0.01511302
Iteration 77, loss = 0.01505686
Iteration 78, loss = 0.01490195
Iteration 79, loss = 0.01483718
Iteration 80, loss = 0.01483341
Iteration 81, loss = 0.01470861
Iteration 82, loss = 0.01482756
Iteration 83, loss = 0.01467869
Iteration 84, loss = 0.01473579
Iteration 85, loss = 0.01473899
Iteration 86, loss = 0.01459988
Iteration 87, loss = 0.01456401
Iteration 88, loss = 0.01437898
Iteration 89, loss = 0.01444974
Iteration 90, loss = 0.01437747
Iteration 91, loss = 0.01444656
Iteration 92, loss = 0.01450056
Iteration 93, loss = 0.01439167
Iteration 94, loss = 0.01429739
Iteration 95, loss = 0.01434632
Iteration 96, loss = 0.01431835
Iteration 97, loss = 0.01413770
Iteration 98, loss = 0.01409202
Iteration 99, loss = 0.01414274
Iteration 100, loss = 0.01420030
Iteration 101, loss = 0.01405130
Iteration 102, loss = 0.01406235
Iteration 103, loss = 0.01407961
Iteration 104, loss = 0.01402629
Iteration 105, loss = 0.01391548
Iteration 106, loss = 0.01388807
Iteration 107, loss = 0.01389707
Iteration 108, loss = 0.01394483
Iteration 109, loss = 0.01402398
Iteration 110, loss = 0.01380972
Iteration 111, loss = 0.01405250
Iteration 112, loss = 0.01395268
Iteration 113, loss = 0.01380441
Iteration 114, loss = 0.01387600
Iteration 115, loss = 0.01381607
Iteration 116, loss = 0.01383179
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67724847
Iteration 2, loss = 0.64634758
Iteration 3, loss = 0.60718856
Iteration 4, loss = 0.55703873
Iteration 5, loss = 0.50014754
Iteration 6, loss = 0.44203742
Iteration 7, loss = 0.38612653
Iteration 8, loss = 0.33432543
Iteration 9, loss = 0.28792692
Iteration 10, loss = 0.24749099
Iteration 11, loss = 0.21266869
Iteration 12, loss = 0.18338836
Iteration 13, loss = 0.15867150
Iteration 14, loss = 0.13806046
Iteration 15, loss = 0.12091794
Iteration 16, loss = 0.10670765
Iteration 17, loss = 0.09465758
Iteration 18, loss = 0.08469390
Iteration 19, loss = 0.07606603
Iteration 20, loss = 0.06885098
Iteration 21, loss = 0.06261792
Iteration 22, loss = 0.05733782
Iteration 23, loss = 0.05278825
Iteration 24, loss = 0.04878116
Iteration 25, loss = 0.04537801
Iteration 26, loss = 0.04226544
Iteration 27, loss = 0.03969441
Iteration 28, loss = 0.03729707
Iteration 29, loss = 0.03517493
Iteration 30, loss = 0.03330960
Iteration 31, loss = 0.03167660
Iteration 32, loss = 0.03016943
Iteration 33, loss = 0.02877886
Iteration 34, loss = 0.02760233
Iteration 35, loss = 0.02639386
Iteration 36, loss = 0.02539620
Iteration 37, loss = 0.02443053
Iteration 38, loss = 0.02358727
Iteration 39, loss = 0.02275421
Iteration 40, loss = 0.02212391
Iteration 41, loss = 0.02149294
Iteration 42, loss = 0.02093024
Iteration 43, loss = 0.02037664
Iteration 44, loss = 0.01978504
Iteration 45, loss = 0.01934252
Iteration 46, loss = 0.01877303
Iteration 47, loss = 0.01839452
Iteration 48, loss = 0.01799209
Iteration 49, loss = 0.01773297
Iteration 50, loss = 0.01729983
Iteration 51, loss = 0.01695423
Iteration 52, loss = 0.01677071
Iteration 53, loss = 0.01636951
Iteration 54, loss = 0.01618032
Iteration 55, loss = 0.01586869
Iteration 56, loss = 0.01574400
Iteration 57, loss = 0.01545117
Iteration 58, loss = 0.01506559
Iteration 59, loss = 0.01493158
Iteration 60, loss = 0.01486218
Iteration 61, loss = 0.01471671
Iteration 62, loss = 0.01441903
Iteration 63, loss = 0.01424465
Iteration 64, loss = 0.01417905
Iteration 65, loss = 0.01390831
Iteration 66, loss = 0.01404279
Iteration 67, loss = 0.01368854
Iteration 68, loss = 0.01356401
Iteration 69, loss = 0.01349414
Iteration 70, loss = 0.01335850
Iteration 71, loss = 0.01323294
Iteration 72, loss = 0.01320721
Iteration 73, loss = 0.01301237
Iteration 74, loss = 0.01298349
Iteration 75, loss = 0.01280311
Iteration 76, loss = 0.01264824
Iteration 77, loss = 0.01263136
Iteration 78, loss = 0.01258660
Iteration 79, loss = 0.01257467
Iteration 80, loss = 0.01257987
Iteration 81, loss = 0.01241009
Iteration 82, loss = 0.01223492
Iteration 83, loss = 0.01216580
Iteration 84, loss = 0.01219773
Iteration 85, loss = 0.01206833
Iteration 86, loss = 0.01198577
Iteration 87, loss = 0.01207619
Iteration 88, loss = 0.01193957
Iteration 89, loss = 0.01194655
Iteration 90, loss = 0.01186230
Iteration 91, loss = 0.01175782
Iteration 92, loss = 0.01175767
Iteration 93, loss = 0.01165111
Iteration 94, loss = 0.01159506
Iteration 95, loss = 0.01161397
Iteration 96, loss = 0.01154072
Iteration 97, loss = 0.01153681
Iteration 98, loss = 0.01145822
Iteration 99, loss = 0.01143111
Iteration 100, loss = 0.01143762
Iteration 101, loss = 0.01128620
Iteration 102, loss = 0.01129377
Iteration 103, loss = 0.01125233
Iteration 104, loss = 0.01124553
Iteration 105, loss = 0.01121611
Iteration 106, loss = 0.01112573
Iteration 107, loss = 0.01111160
Iteration 108, loss = 0.01121620
Iteration 109, loss = 0.01107575
Iteration 110, loss = 0.01109469
Iteration 111, loss = 0.01092643
Iteration 112, loss = 0.01106531
Iteration 113, loss = 0.01099005
Iteration 114, loss = 0.01100335
Iteration 115, loss = 0.01087188
Iteration 116, loss = 0.01086524
Iteration 117, loss = 0.01089775
Iteration 118, loss = 0.01087491
Iteration 119, loss = 0.01083517
Iteration 120, loss = 0.01082543
Iteration 121, loss = 0.01078738
Iteration 122, loss = 0.01083559
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67666529
Iteration 2, loss = 0.64750129
Iteration 3, loss = 0.60824722
Iteration 4, loss = 0.55516396
Iteration 5, loss = 0.49444845
Iteration 6, loss = 0.43241128
Iteration 7, loss = 0.37331536
Iteration 8, loss = 0.31903420
Iteration 9, loss = 0.27055255
Iteration 10, loss = 0.22896892
Iteration 11, loss = 0.19407262
Iteration 12, loss = 0.16506551
Iteration 13, loss = 0.14132912
Iteration 14, loss = 0.12200928
Iteration 15, loss = 0.10614506
Iteration 16, loss = 0.09326859
Iteration 17, loss = 0.08248780
Iteration 18, loss = 0.07354544
Iteration 19, loss = 0.06623031
Iteration 20, loss = 0.05990351
Iteration 21, loss = 0.05465132
Iteration 22, loss = 0.05013979
Iteration 23, loss = 0.04624071
Iteration 24, loss = 0.04285703
Iteration 25, loss = 0.03988766
Iteration 26, loss = 0.03738012
Iteration 27, loss = 0.03509081
Iteration 28, loss = 0.03302883
Iteration 29, loss = 0.03123271
Iteration 30, loss = 0.02975834
Iteration 31, loss = 0.02817767
Iteration 32, loss = 0.02694317
Iteration 33, loss = 0.02576565
Iteration 34, loss = 0.02463737
Iteration 35, loss = 0.02368499
Iteration 36, loss = 0.02281018
Iteration 37, loss = 0.02198219
Iteration 38, loss = 0.02124058
Iteration 39, loss = 0.02065776
Iteration 40, loss = 0.01993751
Iteration 41, loss = 0.01936614
Iteration 42, loss = 0.01884713
Iteration 43, loss = 0.01841389
Iteration 44, loss = 0.01791390
Iteration 45, loss = 0.01754319
Iteration 46, loss = 0.01707872
Iteration 47, loss = 0.01676977
Iteration 48, loss = 0.01632990
Iteration 49, loss = 0.01603273
Iteration 50, loss = 0.01572112
Iteration 51, loss = 0.01542680
Iteration 52, loss = 0.01521393
Iteration 53, loss = 0.01490434
Iteration 54, loss = 0.01472896
Iteration 55, loss = 0.01451690
Iteration 56, loss = 0.01440227
Iteration 57, loss = 0.01409044
Iteration 58, loss = 0.01386921
Iteration 59, loss = 0.01380413
Iteration 60, loss = 0.01352566
Iteration 61, loss = 0.01338560
Iteration 62, loss = 0.01331590
Iteration 63, loss = 0.01307788
Iteration 64, loss = 0.01292284
Iteration 65, loss = 0.01278091
Iteration 66, loss = 0.01265010
Iteration 67, loss = 0.01264476
Iteration 68, loss = 0.01242521
Iteration 69, loss = 0.01233999
Iteration 70, loss = 0.01225263
Iteration 71, loss = 0.01216678
Iteration 72, loss = 0.01201120
Iteration 73, loss = 0.01189213
Iteration 74, loss = 0.01194654
Iteration 75, loss = 0.01178407
Iteration 76, loss = 0.01170266
Iteration 77, loss = 0.01161806
Iteration 78, loss = 0.01151987
Iteration 79, loss = 0.01146129
Iteration 80, loss = 0.01143070
Iteration 81, loss = 0.01138810
Iteration 82, loss = 0.01132061
Iteration 83, loss = 0.01120508
Iteration 84, loss = 0.01117185
Iteration 85, loss = 0.01112597
Iteration 86, loss = 0.01116109
Iteration 87, loss = 0.01099720
Iteration 88, loss = 0.01094514
Iteration 89, loss = 0.01088086
Iteration 90, loss = 0.01085032
Iteration 91, loss = 0.01083179
Iteration 92, loss = 0.01075210
Iteration 93, loss = 0.01083047
Iteration 94, loss = 0.01063074
Iteration 95, loss = 0.01061516
Iteration 96, loss = 0.01061325
Iteration 97, loss = 0.01057710
Iteration 98, loss = 0.01047283
Iteration 99, loss = 0.01059403
Iteration 100, loss = 0.01047966
Iteration 101, loss = 0.01039794
Iteration 102, loss = 0.01043508
Iteration 103, loss = 0.01040767
Iteration 104, loss = 0.01034720
Iteration 105, loss = 0.01031945
Iteration 106, loss = 0.01026530
Iteration 107, loss = 0.01029194
Iteration 108, loss = 0.01033636
Iteration 109, loss = 0.01017434
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68397667
Iteration 2, loss = 0.63256721
Iteration 3, loss = 0.55466074
Iteration 4, loss = 0.46149163
Iteration 5, loss = 0.37144241
Iteration 6, loss = 0.29349695
Iteration 7, loss = 0.23082241
Iteration 8, loss = 0.18297675
Iteration 9, loss = 0.14728602
Iteration 10, loss = 0.12107341
Iteration 11, loss = 0.10165008
Iteration 12, loss = 0.08697470
Iteration 13, loss = 0.07584246
Iteration 14, loss = 0.06717128
Iteration 15, loss = 0.06048399
Iteration 16, loss = 0.05473871
Iteration 17, loss = 0.05034328
Iteration 18, loss = 0.04670844
Iteration 19, loss = 0.04352228
Iteration 20, loss = 0.04111182
Iteration 21, loss = 0.03902656
Iteration 22, loss = 0.03708229
Iteration 23, loss = 0.03567423
Iteration 24, loss = 0.03419117
Iteration 25, loss = 0.03291883
Iteration 26, loss = 0.03191350
Iteration 27, loss = 0.03102098
Iteration 28, loss = 0.03035523
Iteration 29, loss = 0.02957545
Iteration 30, loss = 0.02878738
Iteration 31, loss = 0.02813221
Iteration 32, loss = 0.02790072
Iteration 33, loss = 0.02731865
Iteration 34, loss = 0.02674945
Iteration 35, loss = 0.02648418
Iteration 36, loss = 0.02608349
Iteration 37, loss = 0.02589227
Iteration 38, loss = 0.02561868
Iteration 39, loss = 0.02535892
Iteration 40, loss = 0.02493219
Iteration 41, loss = 0.02477366
Iteration 42, loss = 0.02473219
Iteration 43, loss = 0.02424309
Iteration 44, loss = 0.02425880
Iteration 45, loss = 0.02379312
Iteration 46, loss = 0.02416959
Iteration 47, loss = 0.02368069
Iteration 48, loss = 0.02358396
Iteration 49, loss = 0.02335388
Iteration 50, loss = 0.02343691
Iteration 51, loss = 0.02311194
Iteration 52, loss = 0.02316612
Iteration 53, loss = 0.02306628
Iteration 54, loss = 0.02298759
Iteration 55, loss = 0.02302351
Iteration 56, loss = 0.02273286
Iteration 57, loss = 0.02275645
Iteration 58, loss = 0.02250071
Iteration 59, loss = 0.02258656
Iteration 60, loss = 0.02245797
Iteration 61, loss = 0.02234089
Iteration 62, loss = 0.02225006
Iteration 63, loss = 0.02263442
Iteration 64, loss = 0.02236080
Iteration 65, loss = 0.02235115
Iteration 66, loss = 0.02231064
Iteration 67, loss = 0.02199185
Iteration 68, loss = 0.02203020
Iteration 69, loss = 0.02175131
Iteration 70, loss = 0.02209507
Iteration 71, loss = 0.02184849
Iteration 72, loss = 0.02175960
Iteration 73, loss = 0.02179345
Iteration 74, loss = 0.02172302
Iteration 75, loss = 0.02166279
Iteration 76, loss = 0.02178055
Iteration 77, loss = 0.02138722
Iteration 78, loss = 0.02166368
Iteration 79, loss = 0.02150639
Iteration 80, loss = 0.02156090
Iteration 81, loss = 0.02156210
Iteration 82, loss = 0.02152035
Iteration 83, loss = 0.02162799
Iteration 84, loss = 0.02141806
Iteration 85, loss = 0.02145602
Iteration 86, loss = 0.02146478
Iteration 87, loss = 0.02145919
Iteration 88, loss = 0.02162840
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69254775
Iteration 2, loss = 0.63806757
Iteration 3, loss = 0.54856329
Iteration 4, loss = 0.44656150
Iteration 5, loss = 0.35165105
Iteration 6, loss = 0.27271262
Iteration 7, loss = 0.21120824
Iteration 8, loss = 0.16635826
Iteration 9, loss = 0.13396353
Iteration 10, loss = 0.11048758
Iteration 11, loss = 0.09315394
Iteration 12, loss = 0.08032035
Iteration 13, loss = 0.07065196
Iteration 14, loss = 0.06308894
Iteration 15, loss = 0.05687795
Iteration 16, loss = 0.05228362
Iteration 17, loss = 0.04788683
Iteration 18, loss = 0.04491072
Iteration 19, loss = 0.04188227
Iteration 20, loss = 0.03965029
Iteration 21, loss = 0.03759646
Iteration 22, loss = 0.03578404
Iteration 23, loss = 0.03430037
Iteration 24, loss = 0.03304629
Iteration 25, loss = 0.03209892
Iteration 26, loss = 0.03110727
Iteration 27, loss = 0.03018591
Iteration 28, loss = 0.02928193
Iteration 29, loss = 0.02868785
Iteration 30, loss = 0.02801891
Iteration 31, loss = 0.02770933
Iteration 32, loss = 0.02707940
Iteration 33, loss = 0.02686075
Iteration 34, loss = 0.02657052
Iteration 35, loss = 0.02615618
Iteration 36, loss = 0.02569156
Iteration 37, loss = 0.02537525
Iteration 38, loss = 0.02493798
Iteration 39, loss = 0.02486191
Iteration 40, loss = 0.02464709
Iteration 41, loss = 0.02430204
Iteration 42, loss = 0.02434624
Iteration 43, loss = 0.02401524
Iteration 44, loss = 0.02381447
Iteration 45, loss = 0.02376338
Iteration 46, loss = 0.02340364
Iteration 47, loss = 0.02315051
Iteration 48, loss = 0.02284219
Iteration 49, loss = 0.02279310
Iteration 50, loss = 0.02272626
Iteration 51, loss = 0.02249493
Iteration 52, loss = 0.02260530
Iteration 53, loss = 0.02207473
Iteration 54, loss = 0.02195107
Iteration 55, loss = 0.02197473
Iteration 56, loss = 0.02177766
Iteration 57, loss = 0.02175333
Iteration 58, loss = 0.02157003
Iteration 59, loss = 0.02144026
Iteration 60, loss = 0.02163605
Iteration 61, loss = 0.02279804
Iteration 62, loss = 0.02259580
Iteration 63, loss = 0.02202199
Iteration 64, loss = 0.02184744
Iteration 65, loss = 0.02178233
Iteration 66, loss = 0.02171851
Iteration 67, loss = 0.02175130
Iteration 68, loss = 0.02163414
Iteration 69, loss = 0.02138324
Iteration 70, loss = 0.02142628
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67816959
Iteration 2, loss = 0.65526209
Iteration 3, loss = 0.62759304
Iteration 4, loss = 0.58981604
Iteration 5, loss = 0.54333239
Iteration 6, loss = 0.49166537
Iteration 7, loss = 0.43851309
Iteration 8, loss = 0.38759866
Iteration 9, loss = 0.34036355
Iteration 10, loss = 0.29756882
Iteration 11, loss = 0.26021311
Iteration 12, loss = 0.22718960
Iteration 13, loss = 0.19874109
Iteration 14, loss = 0.17453673
Iteration 15, loss = 0.15404743
Iteration 16, loss = 0.13676482
Iteration 17, loss = 0.12212208
Iteration 18, loss = 0.10967150
Iteration 19, loss = 0.09896137
Iteration 20, loss = 0.08996417
Iteration 21, loss = 0.08223571
Iteration 22, loss = 0.07569990
Iteration 23, loss = 0.06980559
Iteration 24, loss = 0.06488240
Iteration 25, loss = 0.06049659
Iteration 26, loss = 0.05660566
Iteration 27, loss = 0.05320037
Iteration 28, loss = 0.05018287
Iteration 29, loss = 0.04754369
Iteration 30, loss = 0.04510190
Iteration 31, loss = 0.04289011
Iteration 32, loss = 0.04092788
Iteration 33, loss = 0.03912989
Iteration 34, loss = 0.03765383
Iteration 35, loss = 0.03617466
Iteration 36, loss = 0.03510486
Iteration 37, loss = 0.03394319
Iteration 38, loss = 0.03270277
Iteration 39, loss = 0.03158598
Iteration 40, loss = 0.03049459
Iteration 41, loss = 0.02953219
Iteration 42, loss = 0.02880689
Iteration 43, loss = 0.02804534
Iteration 44, loss = 0.02738480
Iteration 45, loss = 0.02670699
Iteration 46, loss = 0.02613704
Iteration 47, loss = 0.02555880
Iteration 48, loss = 0.02496179
Iteration 49, loss = 0.02448278
Iteration 50, loss = 0.02402648
Iteration 51, loss = 0.02384685
Iteration 52, loss = 0.02339968
Iteration 53, loss = 0.02287000
Iteration 54, loss = 0.02267869
Iteration 55, loss = 0.02226112
Iteration 56, loss = 0.02222180
Iteration 57, loss = 0.02195481
Iteration 58, loss = 0.02164517
Iteration 59, loss = 0.02135712
Iteration 60, loss = 0.02122447
Iteration 61, loss = 0.02101838
Iteration 62, loss = 0.02065069
Iteration 63, loss = 0.02040984
Iteration 64, loss = 0.02013249
Iteration 65, loss = 0.01984844
Iteration 66, loss = 0.01975872
Iteration 67, loss = 0.01947067
Iteration 68, loss = 0.01960236
Iteration 69, loss = 0.01941671
Iteration 70, loss = 0.01924965
Iteration 71, loss = 0.01901263
Iteration 72, loss = 0.01878117
Iteration 73, loss = 0.01873154
Iteration 74, loss = 0.01852065
Iteration 75, loss = 0.01838366
Iteration 76, loss = 0.01821826
Iteration 77, loss = 0.01796822
Iteration 78, loss = 0.01786561
Iteration 79, loss = 0.01763624
Iteration 80, loss = 0.01752659
Iteration 81, loss = 0.01760167
Iteration 82, loss = 0.01749620
Iteration 83, loss = 0.01738551
Iteration 84, loss = 0.01729006
Iteration 85, loss = 0.01713757
Iteration 86, loss = 0.01699187
Iteration 87, loss = 0.01689017
Iteration 88, loss = 0.01681447
Iteration 89, loss = 0.01671848
Iteration 90, loss = 0.01672615
Iteration 91, loss = 0.01663215
Iteration 92, loss = 0.01657610
Iteration 93, loss = 0.01663106
Iteration 94, loss = 0.01651532
Iteration 95, loss = 0.01656482
Iteration 96, loss = 0.01643051
Iteration 97, loss = 0.01650711
Iteration 98, loss = 0.01654823
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68787896
Iteration 2, loss = 0.63269221
Iteration 3, loss = 0.56073440
Iteration 4, loss = 0.47420962
Iteration 5, loss = 0.38953038
Iteration 6, loss = 0.31499731
Iteration 7, loss = 0.25271502
Iteration 8, loss = 0.20340536
Iteration 9, loss = 0.16526636
Iteration 10, loss = 0.13613810
Iteration 11, loss = 0.11390626
Iteration 12, loss = 0.09673303
Iteration 13, loss = 0.08353376
Iteration 14, loss = 0.07306377
Iteration 15, loss = 0.06497728
Iteration 16, loss = 0.05824089
Iteration 17, loss = 0.05279813
Iteration 18, loss = 0.04806290
Iteration 19, loss = 0.04450911
Iteration 20, loss = 0.04105426
Iteration 21, loss = 0.03837106
Iteration 22, loss = 0.03598414
Iteration 23, loss = 0.03410323
Iteration 24, loss = 0.03227666
Iteration 25, loss = 0.03061804
Iteration 26, loss = 0.02943696
Iteration 27, loss = 0.02819941
Iteration 28, loss = 0.02701892
Iteration 29, loss = 0.02617939
Iteration 30, loss = 0.02528166
Iteration 31, loss = 0.02462206
Iteration 32, loss = 0.02398169
Iteration 33, loss = 0.02321428
Iteration 34, loss = 0.02259443
Iteration 35, loss = 0.02209076
Iteration 36, loss = 0.02155386
Iteration 37, loss = 0.02131611
Iteration 38, loss = 0.02089424
Iteration 39, loss = 0.02033133
Iteration 40, loss = 0.02018813
Iteration 41, loss = 0.01978008
Iteration 42, loss = 0.01960240
Iteration 43, loss = 0.01937088
Iteration 44, loss = 0.01904124
Iteration 45, loss = 0.01887874
Iteration 46, loss = 0.01854897
Iteration 47, loss = 0.01842869
Iteration 48, loss = 0.01838980
Iteration 49, loss = 0.01795617
Iteration 50, loss = 0.01802910
Iteration 51, loss = 0.01762477
Iteration 52, loss = 0.01767821
Iteration 53, loss = 0.01761595
Iteration 54, loss = 0.01733268
Iteration 55, loss = 0.01713708
Iteration 56, loss = 0.01710417
Iteration 57, loss = 0.01721635
Iteration 58, loss = 0.01697089
Iteration 59, loss = 0.01700600
Iteration 60, loss = 0.01674527
Iteration 61, loss = 0.01659891
Iteration 62, loss = 0.01653266
Iteration 63, loss = 0.01642778
Iteration 64, loss = 0.01633890
Iteration 65, loss = 0.01626157
Iteration 66, loss = 0.01612983
Iteration 67, loss = 0.01626228
Iteration 68, loss = 0.01630007
Iteration 69, loss = 0.01619609
Iteration 70, loss = 0.01615105
Iteration 71, loss = 0.01601893
Iteration 72, loss = 0.01597541
Iteration 73, loss = 0.01593591
Iteration 74, loss = 0.01572977
Iteration 75, loss = 0.01575067
Iteration 76, loss = 0.01568702
Iteration 77, loss = 0.01574396
Iteration 78, loss = 0.01573656
Iteration 79, loss = 0.01566119
Iteration 80, loss = 0.01561591
Iteration 81, loss = 0.01565448
Iteration 82, loss = 0.01563920
Iteration 83, loss = 0.01541743
Iteration 84, loss = 0.01555001
Iteration 85, loss = 0.01537196
Iteration 86, loss = 0.01527105
Iteration 87, loss = 0.01533713
Iteration 88, loss = 0.01531462
Iteration 89, loss = 0.01514029
Iteration 90, loss = 0.01517472
Iteration 91, loss = 0.01519626
Iteration 92, loss = 0.01503163
Iteration 93, loss = 0.01495191
Iteration 94, loss = 0.01522742
Iteration 95, loss = 0.01531671
Iteration 96, loss = 0.01520031
Iteration 97, loss = 0.01518852
Iteration 98, loss = 0.01519825
Iteration 99, loss = 0.01513700
Iteration 100, loss = 0.01509630
Iteration 101, loss = 0.01499237
Iteration 102, loss = 0.01510029
Iteration 103, loss = 0.01516054
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67914754
Iteration 2, loss = 0.65830324
Iteration 3, loss = 0.63849728
Iteration 4, loss = 0.61595842
Iteration 5, loss = 0.58964846
Iteration 6, loss = 0.55983236
Iteration 7, loss = 0.52636318
Iteration 8, loss = 0.49069128
Iteration 9, loss = 0.45407911
Iteration 10, loss = 0.41797642
Iteration 11, loss = 0.38283686
Iteration 12, loss = 0.34977864
Iteration 13, loss = 0.31853912
Iteration 14, loss = 0.28947176
Iteration 15, loss = 0.26300340
Iteration 16, loss = 0.23886723
Iteration 17, loss = 0.21699917
Iteration 18, loss = 0.19737765
Iteration 19, loss = 0.17990931
Iteration 20, loss = 0.16419489
Iteration 21, loss = 0.15019043
Iteration 22, loss = 0.13787706
Iteration 23, loss = 0.12684792
Iteration 24, loss = 0.11700416
Iteration 25, loss = 0.10822573
Iteration 26, loss = 0.10048334
Iteration 27, loss = 0.09354439
Iteration 28, loss = 0.08731125
Iteration 29, loss = 0.08168171
Iteration 30, loss = 0.07669831
Iteration 31, loss = 0.07208311
Iteration 32, loss = 0.06802145
Iteration 33, loss = 0.06430196
Iteration 34, loss = 0.06090168
Iteration 35, loss = 0.05779736
Iteration 36, loss = 0.05497418
Iteration 37, loss = 0.05245059
Iteration 38, loss = 0.05007181
Iteration 39, loss = 0.04788666
Iteration 40, loss = 0.04585332
Iteration 41, loss = 0.04398786
Iteration 42, loss = 0.04238853
Iteration 43, loss = 0.04076656
Iteration 44, loss = 0.03926787
Iteration 45, loss = 0.03789353
Iteration 46, loss = 0.03661815
Iteration 47, loss = 0.03542780
Iteration 48, loss = 0.03429978
Iteration 49, loss = 0.03329056
Iteration 50, loss = 0.03232267
Iteration 51, loss = 0.03139767
Iteration 52, loss = 0.03055318
Iteration 53, loss = 0.02974372
Iteration 54, loss = 0.02899418
Iteration 55, loss = 0.02823828
Iteration 56, loss = 0.02758938
Iteration 57, loss = 0.02690224
Iteration 58, loss = 0.02633009
Iteration 59, loss = 0.02572110
Iteration 60, loss = 0.02515807
Iteration 61, loss = 0.02467911
Iteration 62, loss = 0.02417554
Iteration 63, loss = 0.02376686
Iteration 64, loss = 0.02330992
Iteration 65, loss = 0.02287527
Iteration 66, loss = 0.02244605
Iteration 67, loss = 0.02206066
Iteration 68, loss = 0.02174005
Iteration 69, loss = 0.02143015
Iteration 70, loss = 0.02119579
Iteration 71, loss = 0.02075926
Iteration 72, loss = 0.02050421
Iteration 73, loss = 0.02014105
Iteration 74, loss = 0.01982010
Iteration 75, loss = 0.01958155
Iteration 76, loss = 0.01936368
Iteration 77, loss = 0.01912916
Iteration 78, loss = 0.01883620
Iteration 79, loss = 0.01859577
Iteration 80, loss = 0.01839546
Iteration 81, loss = 0.01817382
Iteration 82, loss = 0.01793870
Iteration 83, loss = 0.01779993
Iteration 84, loss = 0.01764033
Iteration 85, loss = 0.01747090
Iteration 86, loss = 0.01724743
Iteration 87, loss = 0.01709920
Iteration 88, loss = 0.01693700
Iteration 89, loss = 0.01673984
Iteration 90, loss = 0.01665069
Iteration 91, loss = 0.01653777
Iteration 92, loss = 0.01632506
Iteration 93, loss = 0.01621251
Iteration 94, loss = 0.01604861
Iteration 95, loss = 0.01603598
Iteration 96, loss = 0.01582402
Iteration 97, loss = 0.01575564
Iteration 98, loss = 0.01558672
Iteration 99, loss = 0.01554842
Iteration 100, loss = 0.01534996
Iteration 101, loss = 0.01527386
Iteration 102, loss = 0.01512409
Iteration 103, loss = 0.01505562
Iteration 104, loss = 0.01496785
Iteration 105, loss = 0.01492024
Iteration 106, loss = 0.01477879
Iteration 107, loss = 0.01468685
Iteration 108, loss = 0.01462133
Iteration 109, loss = 0.01450990
Iteration 110, loss = 0.01448750
Iteration 111, loss = 0.01437364
Iteration 112, loss = 0.01434317
Iteration 113, loss = 0.01427622
Iteration 114, loss = 0.01418280
Iteration 115, loss = 0.01409541
Iteration 116, loss = 0.01402305
Iteration 117, loss = 0.01397731
Iteration 118, loss = 0.01385130
Iteration 119, loss = 0.01394483
Iteration 120, loss = 0.01383434
Iteration 121, loss = 0.01371087
Iteration 122, loss = 0.01363113
Iteration 123, loss = 0.01363739
Iteration 124, loss = 0.01353424
Iteration 125, loss = 0.01346409
Iteration 126, loss = 0.01339540
Iteration 127, loss = 0.01341112
Iteration 128, loss = 0.01329845
Iteration 129, loss = 0.01327657
Iteration 130, loss = 0.01320840
Iteration 131, loss = 0.01321955
Iteration 132, loss = 0.01317389
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72767903
Iteration 2, loss = 0.69343116
Iteration 3, loss = 0.65246562
Iteration 4, loss = 0.60563265
Iteration 5, loss = 0.55144528
Iteration 6, loss = 0.49211429
Iteration 7, loss = 0.43096529
Iteration 8, loss = 0.37188182
Iteration 9, loss = 0.31816919
Iteration 10, loss = 0.27066889
Iteration 11, loss = 0.23002743
Iteration 12, loss = 0.19583931
Iteration 13, loss = 0.16741971
Iteration 14, loss = 0.14403441
Iteration 15, loss = 0.12489562
Iteration 16, loss = 0.10933283
Iteration 17, loss = 0.09645566
Iteration 18, loss = 0.08580525
Iteration 19, loss = 0.07683869
Iteration 20, loss = 0.06936602
Iteration 21, loss = 0.06315983
Iteration 22, loss = 0.05790654
Iteration 23, loss = 0.05318064
Iteration 24, loss = 0.04915429
Iteration 25, loss = 0.04570218
Iteration 26, loss = 0.04267432
Iteration 27, loss = 0.04007539
Iteration 28, loss = 0.03775310
Iteration 29, loss = 0.03554190
Iteration 30, loss = 0.03384344
Iteration 31, loss = 0.03209697
Iteration 32, loss = 0.03067650
Iteration 33, loss = 0.02933811
Iteration 34, loss = 0.02804850
Iteration 35, loss = 0.02706386
Iteration 36, loss = 0.02600205
Iteration 37, loss = 0.02509096
Iteration 38, loss = 0.02429757
Iteration 39, loss = 0.02355098
Iteration 40, loss = 0.02278742
Iteration 41, loss = 0.02218745
Iteration 42, loss = 0.02152534
Iteration 43, loss = 0.02104040
Iteration 44, loss = 0.02052101
Iteration 45, loss = 0.02007757
Iteration 46, loss = 0.01956100
Iteration 47, loss = 0.01916530
Iteration 48, loss = 0.01889322
Iteration 49, loss = 0.01851774
Iteration 50, loss = 0.01820404
Iteration 51, loss = 0.01784989
Iteration 52, loss = 0.01742983
Iteration 53, loss = 0.01735977
Iteration 54, loss = 0.01699993
Iteration 55, loss = 0.01671550
Iteration 56, loss = 0.01651900
Iteration 57, loss = 0.01624740
Iteration 58, loss = 0.01605501
Iteration 59, loss = 0.01584449
Iteration 60, loss = 0.01567240
Iteration 61, loss = 0.01548249
Iteration 62, loss = 0.01542081
Iteration 63, loss = 0.01525782
Iteration 64, loss = 0.01503160
Iteration 65, loss = 0.01493055
Iteration 66, loss = 0.01475056
Iteration 67, loss = 0.01467015
Iteration 68, loss = 0.01453460
Iteration 69, loss = 0.01446049
Iteration 70, loss = 0.01435419
Iteration 71, loss = 0.01424045
Iteration 72, loss = 0.01408323
Iteration 73, loss = 0.01400270
Iteration 74, loss = 0.01392785
Iteration 75, loss = 0.01383375
Iteration 76, loss = 0.01371993
Iteration 77, loss = 0.01367600
Iteration 78, loss = 0.01356935
Iteration 79, loss = 0.01350938
Iteration 80, loss = 0.01344692
Iteration 81, loss = 0.01341397
Iteration 82, loss = 0.01331006
Iteration 83, loss = 0.01326457
Iteration 84, loss = 0.01320741
Iteration 85, loss = 0.01311365
Iteration 86, loss = 0.01302867
Iteration 87, loss = 0.01302427
Iteration 88, loss = 0.01304427
Iteration 89, loss = 0.01281278
Iteration 90, loss = 0.01277349
Iteration 91, loss = 0.01274437
Iteration 92, loss = 0.01270788
Iteration 93, loss = 0.01268454
Iteration 94, loss = 0.01264139
Iteration 95, loss = 0.01263567
Iteration 96, loss = 0.01253841
Iteration 97, loss = 0.01251990
Iteration 98, loss = 0.01251977
Iteration 99, loss = 0.01261655
Iteration 100, loss = 0.01232618
Iteration 101, loss = 0.01244240
Iteration 102, loss = 0.01246541
Iteration 103, loss = 0.01236300
Iteration 104, loss = 0.01228051
Iteration 105, loss = 0.01229528
Iteration 106, loss = 0.01218184
Iteration 107, loss = 0.01219686
Iteration 108, loss = 0.01215592
Iteration 109, loss = 0.01211920
Iteration 110, loss = 0.01208933
Iteration 111, loss = 0.01207195
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68265811
Iteration 2, loss = 0.63980695
Iteration 3, loss = 0.58274891
Iteration 4, loss = 0.51515426
Iteration 5, loss = 0.44550991
Iteration 6, loss = 0.37907247
Iteration 7, loss = 0.31876760
Iteration 8, loss = 0.26612384
Iteration 9, loss = 0.22202041
Iteration 10, loss = 0.18559945
Iteration 11, loss = 0.15637842
Iteration 12, loss = 0.13274256
Iteration 13, loss = 0.11391904
Iteration 14, loss = 0.09879242
Iteration 15, loss = 0.08675199
Iteration 16, loss = 0.07674561
Iteration 17, loss = 0.06863659
Iteration 18, loss = 0.06197374
Iteration 19, loss = 0.05629393
Iteration 20, loss = 0.05144215
Iteration 21, loss = 0.04739502
Iteration 22, loss = 0.04399192
Iteration 23, loss = 0.04107133
Iteration 24, loss = 0.03858987
Iteration 25, loss = 0.03616531
Iteration 26, loss = 0.03419538
Iteration 27, loss = 0.03269486
Iteration 28, loss = 0.03092005
Iteration 29, loss = 0.02960935
Iteration 30, loss = 0.02835280
Iteration 31, loss = 0.02718707
Iteration 32, loss = 0.02631276
Iteration 33, loss = 0.02532787
Iteration 34, loss = 0.02461210
Iteration 35, loss = 0.02373361
Iteration 36, loss = 0.02304688
Iteration 37, loss = 0.02249492
Iteration 38, loss = 0.02198652
Iteration 39, loss = 0.02142529
Iteration 40, loss = 0.02107061
Iteration 41, loss = 0.02057087
Iteration 42, loss = 0.02001333
Iteration 43, loss = 0.01980864
Iteration 44, loss = 0.01933150
Iteration 45, loss = 0.01892753
Iteration 46, loss = 0.01871648
Iteration 47, loss = 0.01837501
Iteration 48, loss = 0.01821257
Iteration 49, loss = 0.01799762
Iteration 50, loss = 0.01763489
Iteration 51, loss = 0.01741579
Iteration 52, loss = 0.01729259
Iteration 53, loss = 0.01705332
Iteration 54, loss = 0.01678471
Iteration 55, loss = 0.01664274
Iteration 56, loss = 0.01646962
Iteration 57, loss = 0.01631869
Iteration 58, loss = 0.01626663
Iteration 59, loss = 0.01613893
Iteration 60, loss = 0.01595424
Iteration 61, loss = 0.01589533
Iteration 62, loss = 0.01570120
Iteration 63, loss = 0.01561695
Iteration 64, loss = 0.01556037
Iteration 65, loss = 0.01549099
Iteration 66, loss = 0.01533223
Iteration 67, loss = 0.01541566
Iteration 68, loss = 0.01515678
Iteration 69, loss = 0.01497256
Iteration 70, loss = 0.01510923
Iteration 71, loss = 0.01496048
Iteration 72, loss = 0.01479700
Iteration 73, loss = 0.01484063
Iteration 74, loss = 0.01474431
Iteration 75, loss = 0.01460837
Iteration 76, loss = 0.01459711
Iteration 77, loss = 0.01460606
Iteration 78, loss = 0.01448475
Iteration 79, loss = 0.01434640
Iteration 80, loss = 0.01436751
Iteration 81, loss = 0.01440640
Iteration 82, loss = 0.01426213
Iteration 83, loss = 0.01418779
Iteration 84, loss = 0.01429243
Iteration 85, loss = 0.01406972
Iteration 86, loss = 0.01407522
Iteration 87, loss = 0.01401292
Iteration 88, loss = 0.01405421
Iteration 89, loss = 0.01394355
Iteration 90, loss = 0.01387544
Iteration 91, loss = 0.01381811
Iteration 92, loss = 0.01399967
Iteration 93, loss = 0.01399966
Iteration 94, loss = 0.01372895
Iteration 95, loss = 0.01375365
Iteration 96, loss = 0.01376991
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70545594
Iteration 2, loss = 0.63916247
Iteration 3, loss = 0.55638599
Iteration 4, loss = 0.46067984
Iteration 5, loss = 0.36873413
Iteration 6, loss = 0.29029259
Iteration 7, loss = 0.22787381
Iteration 8, loss = 0.18072692
Iteration 9, loss = 0.14557241
Iteration 10, loss = 0.11953434
Iteration 11, loss = 0.10003323
Iteration 12, loss = 0.08556900
Iteration 13, loss = 0.07424870
Iteration 14, loss = 0.06544296
Iteration 15, loss = 0.05835504
Iteration 16, loss = 0.05273719
Iteration 17, loss = 0.04816097
Iteration 18, loss = 0.04455503
Iteration 19, loss = 0.04123103
Iteration 20, loss = 0.03869355
Iteration 21, loss = 0.03636344
Iteration 22, loss = 0.03426531
Iteration 23, loss = 0.03286332
Iteration 24, loss = 0.03144102
Iteration 25, loss = 0.02994190
Iteration 26, loss = 0.02909695
Iteration 27, loss = 0.02794769
Iteration 28, loss = 0.02710864
Iteration 29, loss = 0.02624962
Iteration 30, loss = 0.02560664
Iteration 31, loss = 0.02502119
Iteration 32, loss = 0.02434790
Iteration 33, loss = 0.02373561
Iteration 34, loss = 0.02343782
Iteration 35, loss = 0.02287931
Iteration 36, loss = 0.02266617
Iteration 37, loss = 0.02215706
Iteration 38, loss = 0.02192326
Iteration 39, loss = 0.02165368
Iteration 40, loss = 0.02115063
Iteration 41, loss = 0.02098821
Iteration 42, loss = 0.02082876
Iteration 43, loss = 0.02060625
Iteration 44, loss = 0.02048851
Iteration 45, loss = 0.02012634
Iteration 46, loss = 0.02002699
Iteration 47, loss = 0.01999568
Iteration 48, loss = 0.01959092
Iteration 49, loss = 0.01943041
Iteration 50, loss = 0.01940950
Iteration 51, loss = 0.01937853
Iteration 52, loss = 0.01919333
Iteration 53, loss = 0.01914764
Iteration 54, loss = 0.01885159
Iteration 55, loss = 0.01877746
Iteration 56, loss = 0.01875567
Iteration 57, loss = 0.01862451
Iteration 58, loss = 0.01857608
Iteration 59, loss = 0.01856308
Iteration 60, loss = 0.01830538
Iteration 61, loss = 0.01840644
Iteration 62, loss = 0.01800613
Iteration 63, loss = 0.01829624
Iteration 64, loss = 0.01821622
Iteration 65, loss = 0.01813193
Iteration 66, loss = 0.01788900
Iteration 67, loss = 0.01802274
Iteration 68, loss = 0.01789178
Iteration 69, loss = 0.01790662
Iteration 70, loss = 0.01780269
Iteration 71, loss = 0.01768273
Iteration 72, loss = 0.01767173
Iteration 73, loss = 0.01755689
Iteration 74, loss = 0.01770753
Iteration 75, loss = 0.01749458
Iteration 76, loss = 0.01771692
Iteration 77, loss = 0.01752360
Iteration 78, loss = 0.01760577
Iteration 79, loss = 0.01751049
Iteration 80, loss = 0.01753972
Iteration 81, loss = 0.01730925
Iteration 82, loss = 0.01729058
Iteration 83, loss = 0.01749577
Iteration 84, loss = 0.01748583
Iteration 85, loss = 0.01733838
Iteration 86, loss = 0.01733040
Iteration 87, loss = 0.01713179
Iteration 88, loss = 0.01735993
Iteration 89, loss = 0.01724427
Iteration 90, loss = 0.01730535
Iteration 91, loss = 0.01719401
Iteration 92, loss = 0.01713646
Iteration 93, loss = 0.01711669
Iteration 94, loss = 0.01728323
Iteration 95, loss = 0.01719178
Iteration 96, loss = 0.01746496
Iteration 97, loss = 0.01716093
Iteration 98, loss = 0.01725837
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69467001
Iteration 2, loss = 0.61907478
Iteration 3, loss = 0.52236289
Iteration 4, loss = 0.41517792
Iteration 5, loss = 0.32063554
Iteration 6, loss = 0.24555652
Iteration 7, loss = 0.19036480
Iteration 8, loss = 0.15083849
Iteration 9, loss = 0.12263635
Iteration 10, loss = 0.10209094
Iteration 11, loss = 0.08721135
Iteration 12, loss = 0.07566396
Iteration 13, loss = 0.06715722
Iteration 14, loss = 0.06060590
Iteration 15, loss = 0.05506814
Iteration 16, loss = 0.05052062
Iteration 17, loss = 0.04718782
Iteration 18, loss = 0.04424053
Iteration 19, loss = 0.04194530
Iteration 20, loss = 0.03974426
Iteration 21, loss = 0.03839451
Iteration 22, loss = 0.03763847
Iteration 23, loss = 0.03663764
Iteration 24, loss = 0.03509833
Iteration 25, loss = 0.03400674
Iteration 26, loss = 0.03283190
Iteration 27, loss = 0.03196544
Iteration 28, loss = 0.03130336
Iteration 29, loss = 0.03046128
Iteration 30, loss = 0.03002634
Iteration 31, loss = 0.02954866
Iteration 32, loss = 0.02906650
Iteration 33, loss = 0.02843822
Iteration 34, loss = 0.02810943
Iteration 35, loss = 0.02816386
Iteration 36, loss = 0.02813521
Iteration 37, loss = 0.02756819
Iteration 38, loss = 0.02751008
Iteration 39, loss = 0.02806108
Iteration 40, loss = 0.02729824
Iteration 41, loss = 0.02687605
Iteration 42, loss = 0.02694364
Iteration 43, loss = 0.02684859
Iteration 44, loss = 0.02640674
Iteration 45, loss = 0.02615330
Iteration 46, loss = 0.02594598
Iteration 47, loss = 0.02579449
Iteration 48, loss = 0.02549454
Iteration 49, loss = 0.02538144
Iteration 50, loss = 0.02576690
Iteration 51, loss = 0.02565716
Iteration 52, loss = 0.02560810
Iteration 53, loss = 0.02484498
Iteration 54, loss = 0.02509728
Iteration 55, loss = 0.02483544
Iteration 56, loss = 0.02471800
Iteration 57, loss = 0.02460444
Iteration 58, loss = 0.02449471
Iteration 59, loss = 0.02442423
Iteration 60, loss = 0.02419194
Iteration 61, loss = 0.02411211
Iteration 62, loss = 0.02402065
Iteration 63, loss = 0.02387893
Iteration 64, loss = 0.02396804
Iteration 65, loss = 0.02352477
Iteration 66, loss = 0.02351264
Iteration 67, loss = 0.02370764
Iteration 68, loss = 0.02339286
Iteration 69, loss = 0.02364024
Iteration 70, loss = 0.02367723
Iteration 71, loss = 0.02357580
Iteration 72, loss = 0.02351655
Iteration 73, loss = 0.02418722
Iteration 74, loss = 0.02391480
Iteration 75, loss = 0.02406024
Iteration 76, loss = 0.02391888
Iteration 77, loss = 0.02365834
Iteration 78, loss = 0.02348809
Iteration 79, loss = 0.02368189
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70748641
Iteration 2, loss = 0.65011370
Iteration 3, loss = 0.57096976
Iteration 4, loss = 0.47692094
Iteration 5, loss = 0.38335354
Iteration 6, loss = 0.30169440
Iteration 7, loss = 0.23649964
Iteration 8, loss = 0.18705792
Iteration 9, loss = 0.15051571
Iteration 10, loss = 0.12347075
Iteration 11, loss = 0.10329626
Iteration 12, loss = 0.08823617
Iteration 13, loss = 0.07655380
Iteration 14, loss = 0.06757812
Iteration 15, loss = 0.06035532
Iteration 16, loss = 0.05462243
Iteration 17, loss = 0.04993951
Iteration 18, loss = 0.04599063
Iteration 19, loss = 0.04274323
Iteration 20, loss = 0.04011775
Iteration 21, loss = 0.03799871
Iteration 22, loss = 0.03597140
Iteration 23, loss = 0.03419002
Iteration 24, loss = 0.03276709
Iteration 25, loss = 0.03144876
Iteration 26, loss = 0.03058183
Iteration 27, loss = 0.02928261
Iteration 28, loss = 0.02852625
Iteration 29, loss = 0.02771527
Iteration 30, loss = 0.02718266
Iteration 31, loss = 0.02643389
Iteration 32, loss = 0.02573229
Iteration 33, loss = 0.02521488
Iteration 34, loss = 0.02462769
Iteration 35, loss = 0.02447034
Iteration 36, loss = 0.02400430
Iteration 37, loss = 0.02395662
Iteration 38, loss = 0.02326886
Iteration 39, loss = 0.02298137
Iteration 40, loss = 0.02254785
Iteration 41, loss = 0.02232211
Iteration 42, loss = 0.02216244
Iteration 43, loss = 0.02206898
Iteration 44, loss = 0.02173282
Iteration 45, loss = 0.02158572
Iteration 46, loss = 0.02150305
Iteration 47, loss = 0.02123759
Iteration 48, loss = 0.02110761
Iteration 49, loss = 0.02093670
Iteration 50, loss = 0.02061275
Iteration 51, loss = 0.02059836
Iteration 52, loss = 0.02049917
Iteration 53, loss = 0.02051501
Iteration 54, loss = 0.02022486
Iteration 55, loss = 0.02012304
Iteration 56, loss = 0.01995100
Iteration 57, loss = 0.01986084
Iteration 58, loss = 0.01983260
Iteration 59, loss = 0.01969103
Iteration 60, loss = 0.01971095
Iteration 61, loss = 0.01972518
Iteration 62, loss = 0.01957389
Iteration 63, loss = 0.01957634
Iteration 64, loss = 0.01981325
Iteration 65, loss = 0.01943133
Iteration 66, loss = 0.01939275
Iteration 67, loss = 0.01933040
Iteration 68, loss = 0.01909767
Iteration 69, loss = 0.01931154
Iteration 70, loss = 0.01912544
Iteration 71, loss = 0.01894498
Iteration 72, loss = 0.01906741
Iteration 73, loss = 0.01892215
Iteration 74, loss = 0.01906920
Iteration 75, loss = 0.01900139
Iteration 76, loss = 0.01898479
Iteration 77, loss = 0.01878991
Iteration 78, loss = 0.01894957
Iteration 79, loss = 0.01889386
Iteration 80, loss = 0.01883405
Iteration 81, loss = 0.01863374
Iteration 82, loss = 0.01852394
Iteration 83, loss = 0.01884438
Iteration 84, loss = 0.01916036
Iteration 85, loss = 0.01870988
Iteration 86, loss = 0.01853316
Iteration 87, loss = 0.01861330
Iteration 88, loss = 0.01842173
Iteration 89, loss = 0.01860301
Iteration 90, loss = 0.01848170
Iteration 91, loss = 0.01860329
Iteration 92, loss = 0.01845966
Iteration 93, loss = 0.01845216
Iteration 94, loss = 0.01825918
Iteration 95, loss = 0.01840610
Iteration 96, loss = 0.01847437
Iteration 97, loss = 0.01839653
Iteration 98, loss = 0.01833173
Iteration 99, loss = 0.01826366
Iteration 100, loss = 0.01833540
Iteration 101, loss = 0.01830250
Iteration 102, loss = 0.01858665
Iteration 103, loss = 0.01848863
Iteration 104, loss = 0.01842590
Iteration 105, loss = 0.01816106
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73671989
Iteration 2, loss = 0.71319471
Iteration 3, loss = 0.69439456
Iteration 4, loss = 0.67626937
Iteration 5, loss = 0.65823373
Iteration 6, loss = 0.63988423
Iteration 7, loss = 0.62039111
Iteration 8, loss = 0.60038818
Iteration 9, loss = 0.57936979
Iteration 10, loss = 0.55746378
Iteration 11, loss = 0.53488622
Iteration 12, loss = 0.51164389
Iteration 13, loss = 0.48802286
Iteration 14, loss = 0.46427369
Iteration 15, loss = 0.44060358
Iteration 16, loss = 0.41710999
Iteration 17, loss = 0.39419677
Iteration 18, loss = 0.37179134
Iteration 19, loss = 0.35020025
Iteration 20, loss = 0.32952003
Iteration 21, loss = 0.30962970
Iteration 22, loss = 0.29089255
Iteration 23, loss = 0.27306412
Iteration 24, loss = 0.25613793
Iteration 25, loss = 0.24032700
Iteration 26, loss = 0.22547569
Iteration 27, loss = 0.21157999
Iteration 28, loss = 0.19855964
Iteration 29, loss = 0.18652294
Iteration 30, loss = 0.17530448
Iteration 31, loss = 0.16481016
Iteration 32, loss = 0.15521830
Iteration 33, loss = 0.14616591
Iteration 34, loss = 0.13782421
Iteration 35, loss = 0.13012325
Iteration 36, loss = 0.12297472
Iteration 37, loss = 0.11637585
Iteration 38, loss = 0.11022737
Iteration 39, loss = 0.10458074
Iteration 40, loss = 0.09932806
Iteration 41, loss = 0.09441117
Iteration 42, loss = 0.08986265
Iteration 43, loss = 0.08570926
Iteration 44, loss = 0.08180014
Iteration 45, loss = 0.07820543
Iteration 46, loss = 0.07479503
Iteration 47, loss = 0.07164978
Iteration 48, loss = 0.06871089
Iteration 49, loss = 0.06593313
Iteration 50, loss = 0.06335274
Iteration 51, loss = 0.06089567
Iteration 52, loss = 0.05865604
Iteration 53, loss = 0.05654158
Iteration 54, loss = 0.05448984
Iteration 55, loss = 0.05258821
Iteration 56, loss = 0.05082098
Iteration 57, loss = 0.04915857
Iteration 58, loss = 0.04756034
Iteration 59, loss = 0.04608065
Iteration 60, loss = 0.04464796
Iteration 61, loss = 0.04332676
Iteration 62, loss = 0.04204581
Iteration 63, loss = 0.04088189
Iteration 64, loss = 0.03970990
Iteration 65, loss = 0.03862327
Iteration 66, loss = 0.03758357
Iteration 67, loss = 0.03660283
Iteration 68, loss = 0.03567916
Iteration 69, loss = 0.03474854
Iteration 70, loss = 0.03390359
Iteration 71, loss = 0.03308572
Iteration 72, loss = 0.03231884
Iteration 73, loss = 0.03159409
Iteration 74, loss = 0.03089256
Iteration 75, loss = 0.03019033
Iteration 76, loss = 0.02960847
Iteration 77, loss = 0.02893444
Iteration 78, loss = 0.02838207
Iteration 79, loss = 0.02782850
Iteration 80, loss = 0.02722722
Iteration 81, loss = 0.02670602
Iteration 82, loss = 0.02618752
Iteration 83, loss = 0.02567550
Iteration 84, loss = 0.02521142
Iteration 85, loss = 0.02476493
Iteration 86, loss = 0.02433873
Iteration 87, loss = 0.02394896
Iteration 88, loss = 0.02353586
Iteration 89, loss = 0.02313179
Iteration 90, loss = 0.02277144
Iteration 91, loss = 0.02241495
Iteration 92, loss = 0.02207231
Iteration 93, loss = 0.02171935
Iteration 94, loss = 0.02140819
Iteration 95, loss = 0.02103713
Iteration 96, loss = 0.02077479
Iteration 97, loss = 0.02042144
Iteration 98, loss = 0.02012933
Iteration 99, loss = 0.01986380
Iteration 100, loss = 0.01959342
Iteration 101, loss = 0.01933938
Iteration 102, loss = 0.01907954
Iteration 103, loss = 0.01884116
Iteration 104, loss = 0.01860845
Iteration 105, loss = 0.01839612
Iteration 106, loss = 0.01813347
Iteration 107, loss = 0.01792552
Iteration 108, loss = 0.01774621
Iteration 109, loss = 0.01751094
Iteration 110, loss = 0.01733356
Iteration 111, loss = 0.01713729
Iteration 112, loss = 0.01695974
Iteration 113, loss = 0.01675826
Iteration 114, loss = 0.01662041
Iteration 115, loss = 0.01642399
Iteration 116, loss = 0.01623296
Iteration 117, loss = 0.01608783
Iteration 118, loss = 0.01590909
Iteration 119, loss = 0.01576106
Iteration 120, loss = 0.01557160
Iteration 121, loss = 0.01542521
Iteration 122, loss = 0.01529664
Iteration 123, loss = 0.01512694
Iteration 124, loss = 0.01502620
Iteration 125, loss = 0.01489791
Iteration 126, loss = 0.01474294
Iteration 127, loss = 0.01460700
Iteration 128, loss = 0.01450398
Iteration 129, loss = 0.01438630
Iteration 130, loss = 0.01427556
Iteration 131, loss = 0.01414074
Iteration 132, loss = 0.01402692
Iteration 133, loss = 0.01390618
Iteration 134, loss = 0.01381617
Iteration 135, loss = 0.01367849
Iteration 136, loss = 0.01359156
Iteration 137, loss = 0.01348235
Iteration 138, loss = 0.01340919
Iteration 139, loss = 0.01328598
Iteration 140, loss = 0.01319389
Iteration 141, loss = 0.01310422
Iteration 142, loss = 0.01304699
Iteration 143, loss = 0.01294605
Iteration 144, loss = 0.01286439
Iteration 145, loss = 0.01281137
Iteration 146, loss = 0.01270119
Iteration 147, loss = 0.01261982
Iteration 148, loss = 0.01252615
Iteration 149, loss = 0.01250488
Iteration 150, loss = 0.01236188
Iteration 151, loss = 0.01228506
Iteration 152, loss = 0.01225168
Iteration 153, loss = 0.01216442
Iteration 154, loss = 0.01208964
Iteration 155, loss = 0.01201494
Iteration 156, loss = 0.01197481
Iteration 157, loss = 0.01191377
Iteration 158, loss = 0.01183209
Iteration 159, loss = 0.01176489
Iteration 160, loss = 0.01168431
Iteration 161, loss = 0.01164139
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71700199
Iteration 2, loss = 0.64381335
Iteration 3, loss = 0.55001331
Iteration 4, loss = 0.43978358
Iteration 5, loss = 0.33992865
Iteration 6, loss = 0.25974308
Iteration 7, loss = 0.19954196
Iteration 8, loss = 0.15610247
Iteration 9, loss = 0.12539852
Iteration 10, loss = 0.10315110
Iteration 11, loss = 0.08709624
Iteration 12, loss = 0.07491971
Iteration 13, loss = 0.06596476
Iteration 14, loss = 0.05865150
Iteration 15, loss = 0.05312721
Iteration 16, loss = 0.04851643
Iteration 17, loss = 0.04477076
Iteration 18, loss = 0.04167646
Iteration 19, loss = 0.03909188
Iteration 20, loss = 0.03691882
Iteration 21, loss = 0.03522159
Iteration 22, loss = 0.03353358
Iteration 23, loss = 0.03206475
Iteration 24, loss = 0.03089869
Iteration 25, loss = 0.02952995
Iteration 26, loss = 0.02879089
Iteration 27, loss = 0.02794088
Iteration 28, loss = 0.02734100
Iteration 29, loss = 0.02714263
Iteration 30, loss = 0.02682757
Iteration 31, loss = 0.02585118
Iteration 32, loss = 0.02528099
Iteration 33, loss = 0.02506218
Iteration 34, loss = 0.02580512
Iteration 35, loss = 0.02590443
Iteration 36, loss = 0.02633186
Iteration 37, loss = 0.02566524
Iteration 38, loss = 0.02494664
Iteration 39, loss = 0.02419051
Iteration 40, loss = 0.02376605
Iteration 41, loss = 0.02335616
Iteration 42, loss = 0.02298844
Iteration 43, loss = 0.02265499
Iteration 44, loss = 0.02242828
Iteration 45, loss = 0.02207386
Iteration 46, loss = 0.02210794
Iteration 47, loss = 0.02161599
Iteration 48, loss = 0.02242189
Iteration 49, loss = 0.02231396
Iteration 50, loss = 0.02143705
Iteration 51, loss = 0.02090984
Iteration 52, loss = 0.02077000
Iteration 53, loss = 0.02049905
Iteration 54, loss = 0.02051182
Iteration 55, loss = 0.02027064
Iteration 56, loss = 0.02031542
Iteration 57, loss = 0.02021445
Iteration 58, loss = 0.02002998
Iteration 59, loss = 0.02019847
Iteration 60, loss = 0.01984803
Iteration 61, loss = 0.01987481
Iteration 62, loss = 0.01956632
Iteration 63, loss = 0.01947649
Iteration 64, loss = 0.01938576
Iteration 65, loss = 0.01939711
Iteration 66, loss = 0.01925754
Iteration 67, loss = 0.01911223
Iteration 68, loss = 0.01912754
Iteration 69, loss = 0.01923401
Iteration 70, loss = 0.01892569
Iteration 71, loss = 0.01904839
Iteration 72, loss = 0.02106087
Iteration 73, loss = 0.02280586
Iteration 74, loss = 0.02210467
Iteration 75, loss = 0.02172958
Iteration 76, loss = 0.02118552
Iteration 77, loss = 0.02063363
Iteration 78, loss = 0.02034486
Iteration 79, loss = 0.01990333
Iteration 80, loss = 0.01959356
Iteration 81, loss = 0.01929726
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67568273
Iteration 2, loss = 0.59724096
Iteration 3, loss = 0.48491123
Iteration 4, loss = 0.37588436
Iteration 5, loss = 0.28471609
Iteration 6, loss = 0.21519488
Iteration 7, loss = 0.16540679
Iteration 8, loss = 0.13095315
Iteration 9, loss = 0.10648484
Iteration 10, loss = 0.08898723
Iteration 11, loss = 0.07630507
Iteration 12, loss = 0.06677674
Iteration 13, loss = 0.05907566
Iteration 14, loss = 0.05368138
Iteration 15, loss = 0.04897350
Iteration 16, loss = 0.04530644
Iteration 17, loss = 0.04237852
Iteration 18, loss = 0.04010607
Iteration 19, loss = 0.03802808
Iteration 20, loss = 0.03614823
Iteration 21, loss = 0.03496540
Iteration 22, loss = 0.03359319
Iteration 23, loss = 0.03233465
Iteration 24, loss = 0.03150495
Iteration 25, loss = 0.03061745
Iteration 26, loss = 0.03015042
Iteration 27, loss = 0.02945918
Iteration 28, loss = 0.02881180
Iteration 29, loss = 0.02831963
Iteration 30, loss = 0.02764557
Iteration 31, loss = 0.02722240
Iteration 32, loss = 0.02683570
Iteration 33, loss = 0.02653279
Iteration 34, loss = 0.02602774
Iteration 35, loss = 0.02586967
Iteration 36, loss = 0.02558466
Iteration 37, loss = 0.02574280
Iteration 38, loss = 0.02526876
Iteration 39, loss = 0.02495876
Iteration 40, loss = 0.02478111
Iteration 41, loss = 0.02461348
Iteration 42, loss = 0.02472658
Iteration 43, loss = 0.02425482
Iteration 44, loss = 0.02447684
Iteration 45, loss = 0.02427983
Iteration 46, loss = 0.02391993
Iteration 47, loss = 0.02403705
Iteration 48, loss = 0.02386132
Iteration 49, loss = 0.02387399
Iteration 50, loss = 0.02351095
Iteration 51, loss = 0.02353638
Iteration 52, loss = 0.02340244
Iteration 53, loss = 0.02370760
Iteration 54, loss = 0.02368965
Iteration 55, loss = 0.02349124
Iteration 56, loss = 0.02344452
Iteration 57, loss = 0.02315033
Iteration 58, loss = 0.02310131
Iteration 59, loss = 0.02287999
Iteration 60, loss = 0.02301547
Iteration 61, loss = 0.02314093
Iteration 62, loss = 0.02300586
Iteration 63, loss = 0.02280359
Iteration 64, loss = 0.02281491
Iteration 65, loss = 0.02296120
Iteration 66, loss = 0.02281441
Iteration 67, loss = 0.02273418
Iteration 68, loss = 0.02299445
Iteration 69, loss = 0.02264100
Iteration 70, loss = 0.02256847
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70991278
Iteration 2, loss = 0.62734887
Iteration 3, loss = 0.52044586
Iteration 4, loss = 0.40344781
Iteration 5, loss = 0.30427205
Iteration 6, loss = 0.22850070
Iteration 7, loss = 0.17404551
Iteration 8, loss = 0.13640512
Iteration 9, loss = 0.10992009
Iteration 10, loss = 0.09128726
Iteration 11, loss = 0.07779282
Iteration 12, loss = 0.06753810
Iteration 13, loss = 0.05994572
Iteration 14, loss = 0.05378236
Iteration 15, loss = 0.04881178
Iteration 16, loss = 0.04494282
Iteration 17, loss = 0.04169890
Iteration 18, loss = 0.03915558
Iteration 19, loss = 0.03710975
Iteration 20, loss = 0.03502059
Iteration 21, loss = 0.03338800
Iteration 22, loss = 0.03220248
Iteration 23, loss = 0.03103934
Iteration 24, loss = 0.03012164
Iteration 25, loss = 0.02932952
Iteration 26, loss = 0.02855832
Iteration 27, loss = 0.02778762
Iteration 28, loss = 0.02696068
Iteration 29, loss = 0.02640453
Iteration 30, loss = 0.02588545
Iteration 31, loss = 0.02556062
Iteration 32, loss = 0.02526117
Iteration 33, loss = 0.02495579
Iteration 34, loss = 0.02457395
Iteration 35, loss = 0.02428024
Iteration 36, loss = 0.02396994
Iteration 37, loss = 0.02361359
Iteration 38, loss = 0.02374749
Iteration 39, loss = 0.02316666
Iteration 40, loss = 0.02325729
Iteration 41, loss = 0.02260989
Iteration 42, loss = 0.02264307
Iteration 43, loss = 0.02247904
Iteration 44, loss = 0.02248945
Iteration 45, loss = 0.02215422
Iteration 46, loss = 0.02212697
Iteration 47, loss = 0.02217851
Iteration 48, loss = 0.02181390
Iteration 49, loss = 0.02227083
Iteration 50, loss = 0.02176434
Iteration 51, loss = 0.02172838
Iteration 52, loss = 0.02156941
Iteration 53, loss = 0.02156619
Iteration 54, loss = 0.02145679
Iteration 55, loss = 0.02141661
Iteration 56, loss = 0.02116683
Iteration 57, loss = 0.02130357
Iteration 58, loss = 0.02134276
Iteration 59, loss = 0.02107002
Iteration 60, loss = 0.02140951
Iteration 61, loss = 0.02134463
Iteration 62, loss = 0.02111270
Iteration 63, loss = 0.02105293
Iteration 64, loss = 0.02090894
Iteration 65, loss = 0.02106748
Iteration 66, loss = 0.02077497
Iteration 67, loss = 0.02083271
Iteration 68, loss = 0.02055969
Iteration 69, loss = 0.02084960
Iteration 70, loss = 0.02060843
Iteration 71, loss = 0.02053189
Iteration 72, loss = 0.02078518
Iteration 73, loss = 0.02062782
Iteration 74, loss = 0.02055968
Iteration 75, loss = 0.02056965
Iteration 76, loss = 0.02045004
Iteration 77, loss = 0.02052469
Iteration 78, loss = 0.02052072
Iteration 79, loss = 0.02040794
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.66990088
Iteration 2, loss = 0.65595220
Iteration 3, loss = 0.64403034
Iteration 4, loss = 0.63204992
Iteration 5, loss = 0.61925003
Iteration 6, loss = 0.60510892
Iteration 7, loss = 0.58951007
Iteration 8, loss = 0.57245502
Iteration 9, loss = 0.55409136
Iteration 10, loss = 0.53440261
Iteration 11, loss = 0.51374252
Iteration 12, loss = 0.49206092
Iteration 13, loss = 0.46981120
Iteration 14, loss = 0.44714315
Iteration 15, loss = 0.42438814
Iteration 16, loss = 0.40181689
Iteration 17, loss = 0.37966150
Iteration 18, loss = 0.35809272
Iteration 19, loss = 0.33712505
Iteration 20, loss = 0.31700331
Iteration 21, loss = 0.29771230
Iteration 22, loss = 0.27925285
Iteration 23, loss = 0.26173912
Iteration 24, loss = 0.24521743
Iteration 25, loss = 0.22963669
Iteration 26, loss = 0.21480276
Iteration 27, loss = 0.20100882
Iteration 28, loss = 0.18817576
Iteration 29, loss = 0.17610530
Iteration 30, loss = 0.16480378
Iteration 31, loss = 0.15441870
Iteration 32, loss = 0.14472008
Iteration 33, loss = 0.13566500
Iteration 34, loss = 0.12733948
Iteration 35, loss = 0.11972267
Iteration 36, loss = 0.11259405
Iteration 37, loss = 0.10599395
Iteration 38, loss = 0.09997800
Iteration 39, loss = 0.09436438
Iteration 40, loss = 0.08919441
Iteration 41, loss = 0.08447264
Iteration 42, loss = 0.08008221
Iteration 43, loss = 0.07599536
Iteration 44, loss = 0.07217259
Iteration 45, loss = 0.06865867
Iteration 46, loss = 0.06538750
Iteration 47, loss = 0.06239907
Iteration 48, loss = 0.05952943
Iteration 49, loss = 0.05691839
Iteration 50, loss = 0.05448995
Iteration 51, loss = 0.05218013
Iteration 52, loss = 0.05004263
Iteration 53, loss = 0.04804696
Iteration 54, loss = 0.04615030
Iteration 55, loss = 0.04439647
Iteration 56, loss = 0.04273036
Iteration 57, loss = 0.04116895
Iteration 58, loss = 0.03970480
Iteration 59, loss = 0.03834735
Iteration 60, loss = 0.03701466
Iteration 61, loss = 0.03579306
Iteration 62, loss = 0.03464486
Iteration 63, loss = 0.03350946
Iteration 64, loss = 0.03247602
Iteration 65, loss = 0.03147649
Iteration 66, loss = 0.03053574
Iteration 67, loss = 0.02966317
Iteration 68, loss = 0.02880298
Iteration 69, loss = 0.02798488
Iteration 70, loss = 0.02721603
Iteration 71, loss = 0.02651767
Iteration 72, loss = 0.02580346
Iteration 73, loss = 0.02516127
Iteration 74, loss = 0.02451476
Iteration 75, loss = 0.02391197
Iteration 76, loss = 0.02333243
Iteration 77, loss = 0.02278794
Iteration 78, loss = 0.02223051
Iteration 79, loss = 0.02176977
Iteration 80, loss = 0.02123157
Iteration 81, loss = 0.02078592
Iteration 82, loss = 0.02032084
Iteration 83, loss = 0.01989691
Iteration 84, loss = 0.01947786
Iteration 85, loss = 0.01907276
Iteration 86, loss = 0.01868774
Iteration 87, loss = 0.01832129
Iteration 88, loss = 0.01795953
Iteration 89, loss = 0.01762147
Iteration 90, loss = 0.01729693
Iteration 91, loss = 0.01699690
Iteration 92, loss = 0.01667597
Iteration 93, loss = 0.01638250
Iteration 94, loss = 0.01609010
Iteration 95, loss = 0.01580498
Iteration 96, loss = 0.01554316
Iteration 97, loss = 0.01531203
Iteration 98, loss = 0.01503719
Iteration 99, loss = 0.01479160
Iteration 100, loss = 0.01456041
Iteration 101, loss = 0.01434304
Iteration 102, loss = 0.01412910
Iteration 103, loss = 0.01389866
Iteration 104, loss = 0.01370306
Iteration 105, loss = 0.01350872
Iteration 106, loss = 0.01333660
Iteration 107, loss = 0.01314552
Iteration 108, loss = 0.01294842
Iteration 109, loss = 0.01277255
Iteration 110, loss = 0.01260757
Iteration 111, loss = 0.01244531
Iteration 112, loss = 0.01227655
Iteration 113, loss = 0.01212495
Iteration 114, loss = 0.01199663
Iteration 115, loss = 0.01183263
Iteration 116, loss = 0.01168809
Iteration 117, loss = 0.01156742
Iteration 118, loss = 0.01140831
Iteration 119, loss = 0.01127688
Iteration 120, loss = 0.01114929
Iteration 121, loss = 0.01102334
Iteration 122, loss = 0.01090952
Iteration 123, loss = 0.01078234
Iteration 124, loss = 0.01067777
Iteration 125, loss = 0.01054118
Iteration 126, loss = 0.01043970
Iteration 127, loss = 0.01033894
Iteration 128, loss = 0.01021900
Iteration 129, loss = 0.01011892
Iteration 130, loss = 0.01003389
Iteration 131, loss = 0.00992352
Iteration 132, loss = 0.00982659
Iteration 133, loss = 0.00973130
Iteration 134, loss = 0.00966444
Iteration 135, loss = 0.00954912
Iteration 136, loss = 0.00946650
Iteration 137, loss = 0.00939305
Iteration 138, loss = 0.00931431
Iteration 139, loss = 0.00922677
Iteration 140, loss = 0.00914288
Iteration 141, loss = 0.00906209
Iteration 142, loss = 0.00898916
Iteration 143, loss = 0.00892988
Iteration 144, loss = 0.00884866
Iteration 145, loss = 0.00878482
Iteration 146, loss = 0.00871585
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68047645
Iteration 2, loss = 0.65699182
Iteration 3, loss = 0.63428372
Iteration 4, loss = 0.60842220
Iteration 5, loss = 0.57829171
Iteration 6, loss = 0.54449317
Iteration 7, loss = 0.50758136
Iteration 8, loss = 0.46940042
Iteration 9, loss = 0.43109882
Iteration 10, loss = 0.39404166
Iteration 11, loss = 0.35880767
Iteration 12, loss = 0.32595653
Iteration 13, loss = 0.29553850
Iteration 14, loss = 0.26758518
Iteration 15, loss = 0.24228336
Iteration 16, loss = 0.21942660
Iteration 17, loss = 0.19891871
Iteration 18, loss = 0.18060692
Iteration 19, loss = 0.16443468
Iteration 20, loss = 0.14988093
Iteration 21, loss = 0.13715137
Iteration 22, loss = 0.12580567
Iteration 23, loss = 0.11574266
Iteration 24, loss = 0.10680869
Iteration 25, loss = 0.09893837
Iteration 26, loss = 0.09181373
Iteration 27, loss = 0.08552491
Iteration 28, loss = 0.07986841
Iteration 29, loss = 0.07481297
Iteration 30, loss = 0.07032800
Iteration 31, loss = 0.06620346
Iteration 32, loss = 0.06245024
Iteration 33, loss = 0.05912921
Iteration 34, loss = 0.05605393
Iteration 35, loss = 0.05326406
Iteration 36, loss = 0.05066836
Iteration 37, loss = 0.04840959
Iteration 38, loss = 0.04624443
Iteration 39, loss = 0.04428392
Iteration 40, loss = 0.04243636
Iteration 41, loss = 0.04078758
Iteration 42, loss = 0.03918589
Iteration 43, loss = 0.03773638
Iteration 44, loss = 0.03643558
Iteration 45, loss = 0.03519056
Iteration 46, loss = 0.03400539
Iteration 47, loss = 0.03290495
Iteration 48, loss = 0.03192584
Iteration 49, loss = 0.03096654
Iteration 50, loss = 0.03008081
Iteration 51, loss = 0.02917612
Iteration 52, loss = 0.02838603
Iteration 53, loss = 0.02767657
Iteration 54, loss = 0.02695411
Iteration 55, loss = 0.02631624
Iteration 56, loss = 0.02565457
Iteration 57, loss = 0.02509394
Iteration 58, loss = 0.02453023
Iteration 59, loss = 0.02404627
Iteration 60, loss = 0.02348912
Iteration 61, loss = 0.02309986
Iteration 62, loss = 0.02260126
Iteration 63, loss = 0.02215764
Iteration 64, loss = 0.02179308
Iteration 65, loss = 0.02136211
Iteration 66, loss = 0.02096089
Iteration 67, loss = 0.02064144
Iteration 68, loss = 0.02029194
Iteration 69, loss = 0.01992622
Iteration 70, loss = 0.01961829
Iteration 71, loss = 0.01934868
Iteration 72, loss = 0.01906884
Iteration 73, loss = 0.01878236
Iteration 74, loss = 0.01848688
Iteration 75, loss = 0.01828801
Iteration 76, loss = 0.01805568
Iteration 77, loss = 0.01782639
Iteration 78, loss = 0.01760168
Iteration 79, loss = 0.01736794
Iteration 80, loss = 0.01715822
Iteration 81, loss = 0.01704500
Iteration 82, loss = 0.01680844
Iteration 83, loss = 0.01659436
Iteration 84, loss = 0.01642272
Iteration 85, loss = 0.01622140
Iteration 86, loss = 0.01607142
Iteration 87, loss = 0.01592006
Iteration 88, loss = 0.01581845
Iteration 89, loss = 0.01563410
Iteration 90, loss = 0.01552016
Iteration 91, loss = 0.01533984
Iteration 92, loss = 0.01525452
Iteration 93, loss = 0.01510978
Iteration 94, loss = 0.01501911
Iteration 95, loss = 0.01484259
Iteration 96, loss = 0.01472824
Iteration 97, loss = 0.01460105
Iteration 98, loss = 0.01453463
Iteration 99, loss = 0.01444168
Iteration 100, loss = 0.01430770
Iteration 101, loss = 0.01424265
Iteration 102, loss = 0.01409895
Iteration 103, loss = 0.01404020
Iteration 104, loss = 0.01392877
Iteration 105, loss = 0.01379130
Iteration 106, loss = 0.01374099
Iteration 107, loss = 0.01365564
Iteration 108, loss = 0.01361481
Iteration 109, loss = 0.01351138
Iteration 110, loss = 0.01344794
Iteration 111, loss = 0.01333639
Iteration 112, loss = 0.01328354
Iteration 113, loss = 0.01322469
Iteration 114, loss = 0.01315720
Iteration 115, loss = 0.01312836
Iteration 116, loss = 0.01301316
Iteration 117, loss = 0.01298005
Iteration 118, loss = 0.01289583
Iteration 119, loss = 0.01283519
Iteration 120, loss = 0.01280300
Iteration 121, loss = 0.01273352
Iteration 122, loss = 0.01273583
Iteration 123, loss = 0.01260858
Iteration 124, loss = 0.01260838
Iteration 125, loss = 0.01249328
Iteration 126, loss = 0.01247051
Iteration 127, loss = 0.01238994
Iteration 128, loss = 0.01240254
Iteration 129, loss = 0.01231837
Iteration 130, loss = 0.01228804
Iteration 131, loss = 0.01224346
Iteration 132, loss = 0.01218655
Iteration 133, loss = 0.01213757
Iteration 134, loss = 0.01207050
Iteration 135, loss = 0.01208843
Iteration 136, loss = 0.01204150
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68809010
Iteration 2, loss = 0.63069538
Iteration 3, loss = 0.55592448
Iteration 4, loss = 0.46776911
Iteration 5, loss = 0.37903534
Iteration 6, loss = 0.29902706
Iteration 7, loss = 0.23440185
Iteration 8, loss = 0.18483633
Iteration 9, loss = 0.14791886
Iteration 10, loss = 0.12080607
Iteration 11, loss = 0.10074426
Iteration 12, loss = 0.08563084
Iteration 13, loss = 0.07408905
Iteration 14, loss = 0.06526642
Iteration 15, loss = 0.05800298
Iteration 16, loss = 0.05221020
Iteration 17, loss = 0.04776342
Iteration 18, loss = 0.04395089
Iteration 19, loss = 0.04071287
Iteration 20, loss = 0.03782288
Iteration 21, loss = 0.03588061
Iteration 22, loss = 0.03377436
Iteration 23, loss = 0.03214481
Iteration 24, loss = 0.03054158
Iteration 25, loss = 0.02962578
Iteration 26, loss = 0.02841847
Iteration 27, loss = 0.02728082
Iteration 28, loss = 0.02629221
Iteration 29, loss = 0.02557586
Iteration 30, loss = 0.02512655
Iteration 31, loss = 0.02439401
Iteration 32, loss = 0.02377575
Iteration 33, loss = 0.02350600
Iteration 34, loss = 0.02278765
Iteration 35, loss = 0.02245148
Iteration 36, loss = 0.02192902
Iteration 37, loss = 0.02169809
Iteration 38, loss = 0.02139002
Iteration 39, loss = 0.02119521
Iteration 40, loss = 0.02096586
Iteration 41, loss = 0.02074181
Iteration 42, loss = 0.02053530
Iteration 43, loss = 0.02027564
Iteration 44, loss = 0.02013581
Iteration 45, loss = 0.01986586
Iteration 46, loss = 0.01985477
Iteration 47, loss = 0.01951344
Iteration 48, loss = 0.01945195
Iteration 49, loss = 0.01927393
Iteration 50, loss = 0.01908625
Iteration 51, loss = 0.01897375
Iteration 52, loss = 0.01885064
Iteration 53, loss = 0.01886803
Iteration 54, loss = 0.01864168
Iteration 55, loss = 0.01858245
Iteration 56, loss = 0.01865652
Iteration 57, loss = 0.01836605
Iteration 58, loss = 0.01838827
Iteration 59, loss = 0.01839867
Iteration 60, loss = 0.01821123
Iteration 61, loss = 0.01813735
Iteration 62, loss = 0.01804502
Iteration 63, loss = 0.01799407
Iteration 64, loss = 0.01803972
Iteration 65, loss = 0.01780164
Iteration 66, loss = 0.01775380
Iteration 67, loss = 0.01773097
Iteration 68, loss = 0.01754677
Iteration 69, loss = 0.01791050
Iteration 70, loss = 0.01756668
Iteration 71, loss = 0.01764354
Iteration 72, loss = 0.01767560
Iteration 73, loss = 0.01754346
Iteration 74, loss = 0.01749841
Iteration 75, loss = 0.01743180
Iteration 76, loss = 0.01737518
Iteration 77, loss = 0.01735974
Iteration 78, loss = 0.01741242
Iteration 79, loss = 0.01721304
Iteration 80, loss = 0.01719458
Iteration 81, loss = 0.01721879
Iteration 82, loss = 0.01717150
Iteration 83, loss = 0.01719148
Iteration 84, loss = 0.01730655
Iteration 85, loss = 0.01706237
Iteration 86, loss = 0.01715324
Iteration 87, loss = 0.01726579
Iteration 88, loss = 0.01708717
Iteration 89, loss = 0.01700357
Iteration 90, loss = 0.01701937
Iteration 91, loss = 0.01684193
Iteration 92, loss = 0.01720966
Iteration 93, loss = 0.01683802
Iteration 94, loss = 0.01687697
Iteration 95, loss = 0.01721562
Iteration 96, loss = 0.01694582
Iteration 97, loss = 0.01693060
Iteration 98, loss = 0.01700849
Iteration 99, loss = 0.01696406
Iteration 100, loss = 0.01671331
Iteration 101, loss = 0.01684283
Iteration 102, loss = 0.01686766
Iteration 103, loss = 0.01668018
Iteration 104, loss = 0.01676638
Iteration 105, loss = 0.01659807
Iteration 106, loss = 0.01665831
Iteration 107, loss = 0.01661569
Iteration 108, loss = 0.01680066
Iteration 109, loss = 0.01675428
Iteration 110, loss = 0.01682438
Iteration 111, loss = 0.01677039
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69257403
Iteration 2, loss = 0.66867640
Iteration 3, loss = 0.63537488
Iteration 4, loss = 0.58987948
Iteration 5, loss = 0.53380783
Iteration 6, loss = 0.47146756
Iteration 7, loss = 0.40857529
Iteration 8, loss = 0.34951000
Iteration 9, loss = 0.29675837
Iteration 10, loss = 0.25144378
Iteration 11, loss = 0.21309647
Iteration 12, loss = 0.18156079
Iteration 13, loss = 0.15553005
Iteration 14, loss = 0.13446686
Iteration 15, loss = 0.11716381
Iteration 16, loss = 0.10310181
Iteration 17, loss = 0.09134677
Iteration 18, loss = 0.08176712
Iteration 19, loss = 0.07384799
Iteration 20, loss = 0.06700557
Iteration 21, loss = 0.06141094
Iteration 22, loss = 0.05647809
Iteration 23, loss = 0.05245235
Iteration 24, loss = 0.04858535
Iteration 25, loss = 0.04562071
Iteration 26, loss = 0.04297886
Iteration 27, loss = 0.04038196
Iteration 28, loss = 0.03836322
Iteration 29, loss = 0.03651901
Iteration 30, loss = 0.03469743
Iteration 31, loss = 0.03324151
Iteration 32, loss = 0.03188788
Iteration 33, loss = 0.03063946
Iteration 34, loss = 0.02964969
Iteration 35, loss = 0.02863626
Iteration 36, loss = 0.02773389
Iteration 37, loss = 0.02693708
Iteration 38, loss = 0.02612317
Iteration 39, loss = 0.02541504
Iteration 40, loss = 0.02479931
Iteration 41, loss = 0.02413634
Iteration 42, loss = 0.02370962
Iteration 43, loss = 0.02318997
Iteration 44, loss = 0.02272377
Iteration 45, loss = 0.02226080
Iteration 46, loss = 0.02181675
Iteration 47, loss = 0.02153159
Iteration 48, loss = 0.02118052
Iteration 49, loss = 0.02089588
Iteration 50, loss = 0.02050247
Iteration 51, loss = 0.02021959
Iteration 52, loss = 0.02001966
Iteration 53, loss = 0.01972856
Iteration 54, loss = 0.01958168
Iteration 55, loss = 0.01915971
Iteration 56, loss = 0.01902531
Iteration 57, loss = 0.01882648
Iteration 58, loss = 0.01869868
Iteration 59, loss = 0.01851746
Iteration 60, loss = 0.01829517
Iteration 61, loss = 0.01826647
Iteration 62, loss = 0.01816420
Iteration 63, loss = 0.01784509
Iteration 64, loss = 0.01776043
Iteration 65, loss = 0.01766653
Iteration 66, loss = 0.01748875
Iteration 67, loss = 0.01735170
Iteration 68, loss = 0.01719163
Iteration 69, loss = 0.01709581
Iteration 70, loss = 0.01703851
Iteration 71, loss = 0.01692454
Iteration 72, loss = 0.01676960
Iteration 73, loss = 0.01677672
Iteration 74, loss = 0.01665705
Iteration 75, loss = 0.01652488
Iteration 76, loss = 0.01646919
Iteration 77, loss = 0.01649766
Iteration 78, loss = 0.01634380
Iteration 79, loss = 0.01621045
Iteration 80, loss = 0.01621563
Iteration 81, loss = 0.01616797
Iteration 82, loss = 0.01624659
Iteration 83, loss = 0.01598463
Iteration 84, loss = 0.01603387
Iteration 85, loss = 0.01601643
Iteration 86, loss = 0.01583111
Iteration 87, loss = 0.01583952
Iteration 88, loss = 0.01573472
Iteration 89, loss = 0.01588209
Iteration 90, loss = 0.01584725
Iteration 91, loss = 0.01551722
Iteration 92, loss = 0.01554364
Iteration 93, loss = 0.01563164
Iteration 94, loss = 0.01552148
Iteration 95, loss = 0.01540510
Iteration 96, loss = 0.01552445
Iteration 97, loss = 0.01549029
Iteration 98, loss = 0.01545067
Iteration 99, loss = 0.01531176
Iteration 100, loss = 0.01531960
Iteration 101, loss = 0.01535975
Iteration 102, loss = 0.01526313
Iteration 103, loss = 0.01519790
Iteration 104, loss = 0.01522566
Iteration 105, loss = 0.01529281
Iteration 106, loss = 0.01523655
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70943923
Iteration 2, loss = 0.68658983
Iteration 3, loss = 0.66853289
Iteration 4, loss = 0.65196354
Iteration 5, loss = 0.63541111
Iteration 6, loss = 0.61828227
Iteration 7, loss = 0.60004881
Iteration 8, loss = 0.58064669
Iteration 9, loss = 0.56033850
Iteration 10, loss = 0.53899919
Iteration 11, loss = 0.51697610
Iteration 12, loss = 0.49437031
Iteration 13, loss = 0.47154745
Iteration 14, loss = 0.44861758
Iteration 15, loss = 0.42576428
Iteration 16, loss = 0.40318289
Iteration 17, loss = 0.38104984
Iteration 18, loss = 0.35943614
Iteration 19, loss = 0.33842398
Iteration 20, loss = 0.31838027
Iteration 21, loss = 0.29919087
Iteration 22, loss = 0.28098135
Iteration 23, loss = 0.26373594
Iteration 24, loss = 0.24751138
Iteration 25, loss = 0.23222443
Iteration 26, loss = 0.21797201
Iteration 27, loss = 0.20453171
Iteration 28, loss = 0.19213291
Iteration 29, loss = 0.18051205
Iteration 30, loss = 0.16974652
Iteration 31, loss = 0.15975109
Iteration 32, loss = 0.15043294
Iteration 33, loss = 0.14185918
Iteration 34, loss = 0.13385124
Iteration 35, loss = 0.12645573
Iteration 36, loss = 0.11961403
Iteration 37, loss = 0.11327374
Iteration 38, loss = 0.10736537
Iteration 39, loss = 0.10185242
Iteration 40, loss = 0.09679740
Iteration 41, loss = 0.09208237
Iteration 42, loss = 0.08763810
Iteration 43, loss = 0.08357437
Iteration 44, loss = 0.07975315
Iteration 45, loss = 0.07617920
Iteration 46, loss = 0.07290005
Iteration 47, loss = 0.06977194
Iteration 48, loss = 0.06696637
Iteration 49, loss = 0.06419929
Iteration 50, loss = 0.06166940
Iteration 51, loss = 0.05928090
Iteration 52, loss = 0.05708679
Iteration 53, loss = 0.05495292
Iteration 54, loss = 0.05299558
Iteration 55, loss = 0.05115166
Iteration 56, loss = 0.04940548
Iteration 57, loss = 0.04774475
Iteration 58, loss = 0.04620316
Iteration 59, loss = 0.04472616
Iteration 60, loss = 0.04333226
Iteration 61, loss = 0.04202333
Iteration 62, loss = 0.04076732
Iteration 63, loss = 0.03959886
Iteration 64, loss = 0.03845353
Iteration 65, loss = 0.03741975
Iteration 66, loss = 0.03639946
Iteration 67, loss = 0.03541691
Iteration 68, loss = 0.03451871
Iteration 69, loss = 0.03362286
Iteration 70, loss = 0.03281161
Iteration 71, loss = 0.03200229
Iteration 72, loss = 0.03127452
Iteration 73, loss = 0.03054501
Iteration 74, loss = 0.02985617
Iteration 75, loss = 0.02920764
Iteration 76, loss = 0.02856715
Iteration 77, loss = 0.02795769
Iteration 78, loss = 0.02738484
Iteration 79, loss = 0.02684871
Iteration 80, loss = 0.02629423
Iteration 81, loss = 0.02577774
Iteration 82, loss = 0.02530369
Iteration 83, loss = 0.02478835
Iteration 84, loss = 0.02433227
Iteration 85, loss = 0.02389199
Iteration 86, loss = 0.02345410
Iteration 87, loss = 0.02304361
Iteration 88, loss = 0.02267815
Iteration 89, loss = 0.02228047
Iteration 90, loss = 0.02192542
Iteration 91, loss = 0.02155326
Iteration 92, loss = 0.02121784
Iteration 93, loss = 0.02089166
Iteration 94, loss = 0.02056944
Iteration 95, loss = 0.02026237
Iteration 96, loss = 0.01996149
Iteration 97, loss = 0.01966608
Iteration 98, loss = 0.01935144
Iteration 99, loss = 0.01904803
Iteration 100, loss = 0.01880573
Iteration 101, loss = 0.01858185
Iteration 102, loss = 0.01830501
Iteration 103, loss = 0.01812684
Iteration 104, loss = 0.01786151
Iteration 105, loss = 0.01765124
Iteration 106, loss = 0.01743318
Iteration 107, loss = 0.01723150
Iteration 108, loss = 0.01701936
Iteration 109, loss = 0.01688161
Iteration 110, loss = 0.01665117
Iteration 111, loss = 0.01644662
Iteration 112, loss = 0.01630228
Iteration 113, loss = 0.01612216
Iteration 114, loss = 0.01594923
Iteration 115, loss = 0.01575810
Iteration 116, loss = 0.01559187
Iteration 117, loss = 0.01540471
Iteration 118, loss = 0.01525360
Iteration 119, loss = 0.01510218
Iteration 120, loss = 0.01493983
Iteration 121, loss = 0.01484546
Iteration 122, loss = 0.01475465
Iteration 123, loss = 0.01460382
Iteration 124, loss = 0.01448606
Iteration 125, loss = 0.01438693
Iteration 126, loss = 0.01426009
Iteration 127, loss = 0.01409992
Iteration 128, loss = 0.01400098
Iteration 129, loss = 0.01390229
Iteration 130, loss = 0.01375802
Iteration 131, loss = 0.01362182
Iteration 132, loss = 0.01350209
Iteration 133, loss = 0.01342523
Iteration 134, loss = 0.01330372
Iteration 135, loss = 0.01320054
Iteration 136, loss = 0.01314881
Iteration 137, loss = 0.01301084
Iteration 138, loss = 0.01292701
Iteration 139, loss = 0.01281480
Iteration 140, loss = 0.01272309
Iteration 141, loss = 0.01262626
Iteration 142, loss = 0.01256380
Iteration 143, loss = 0.01246972
Iteration 144, loss = 0.01239615
Iteration 145, loss = 0.01229389
Iteration 146, loss = 0.01223245
Iteration 147, loss = 0.01217277
Iteration 148, loss = 0.01209740
Iteration 149, loss = 0.01201251
Iteration 150, loss = 0.01192930
Iteration 151, loss = 0.01190225
Iteration 152, loss = 0.01182034
Iteration 153, loss = 0.01175001
Iteration 154, loss = 0.01168346
Iteration 155, loss = 0.01162937
Iteration 156, loss = 0.01157342
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71286201
Iteration 2, loss = 0.68906548
Iteration 3, loss = 0.66712734
Iteration 4, loss = 0.64178615
Iteration 5, loss = 0.61303547
Iteration 6, loss = 0.58089054
Iteration 7, loss = 0.54638698
Iteration 8, loss = 0.51026404
Iteration 9, loss = 0.47319685
Iteration 10, loss = 0.43597569
Iteration 11, loss = 0.39951146
Iteration 12, loss = 0.36462861
Iteration 13, loss = 0.33179678
Iteration 14, loss = 0.30131270
Iteration 15, loss = 0.27323451
Iteration 16, loss = 0.24771094
Iteration 17, loss = 0.22469956
Iteration 18, loss = 0.20400761
Iteration 19, loss = 0.18541606
Iteration 20, loss = 0.16894383
Iteration 21, loss = 0.15426392
Iteration 22, loss = 0.14112776
Iteration 23, loss = 0.12959760
Iteration 24, loss = 0.11924606
Iteration 25, loss = 0.11013227
Iteration 26, loss = 0.10196324
Iteration 27, loss = 0.09467966
Iteration 28, loss = 0.08818572
Iteration 29, loss = 0.08230596
Iteration 30, loss = 0.07709420
Iteration 31, loss = 0.07237430
Iteration 32, loss = 0.06809941
Iteration 33, loss = 0.06428986
Iteration 34, loss = 0.06078175
Iteration 35, loss = 0.05758491
Iteration 36, loss = 0.05476368
Iteration 37, loss = 0.05202525
Iteration 38, loss = 0.04966691
Iteration 39, loss = 0.04736160
Iteration 40, loss = 0.04535264
Iteration 41, loss = 0.04346548
Iteration 42, loss = 0.04169194
Iteration 43, loss = 0.04009745
Iteration 44, loss = 0.03861805
Iteration 45, loss = 0.03728352
Iteration 46, loss = 0.03590751
Iteration 47, loss = 0.03473904
Iteration 48, loss = 0.03359482
Iteration 49, loss = 0.03251321
Iteration 50, loss = 0.03156514
Iteration 51, loss = 0.03063667
Iteration 52, loss = 0.02976009
Iteration 53, loss = 0.02900658
Iteration 54, loss = 0.02818221
Iteration 55, loss = 0.02744719
Iteration 56, loss = 0.02680895
Iteration 57, loss = 0.02618356
Iteration 58, loss = 0.02557132
Iteration 59, loss = 0.02496939
Iteration 60, loss = 0.02441689
Iteration 61, loss = 0.02394881
Iteration 62, loss = 0.02342842
Iteration 63, loss = 0.02299397
Iteration 64, loss = 0.02248537
Iteration 65, loss = 0.02210036
Iteration 66, loss = 0.02169688
Iteration 67, loss = 0.02135609
Iteration 68, loss = 0.02101976
Iteration 69, loss = 0.02064088
Iteration 70, loss = 0.02028460
Iteration 71, loss = 0.01996001
Iteration 72, loss = 0.01967041
Iteration 73, loss = 0.01942645
Iteration 74, loss = 0.01909098
Iteration 75, loss = 0.01887136
Iteration 76, loss = 0.01859999
Iteration 77, loss = 0.01836709
Iteration 78, loss = 0.01810793
Iteration 79, loss = 0.01785526
Iteration 80, loss = 0.01765156
Iteration 81, loss = 0.01743370
Iteration 82, loss = 0.01723401
Iteration 83, loss = 0.01710017
Iteration 84, loss = 0.01691109
Iteration 85, loss = 0.01669849
Iteration 86, loss = 0.01656101
Iteration 87, loss = 0.01636474
Iteration 88, loss = 0.01622167
Iteration 89, loss = 0.01604732
Iteration 90, loss = 0.01588452
Iteration 91, loss = 0.01575140
Iteration 92, loss = 0.01561724
Iteration 93, loss = 0.01551611
Iteration 94, loss = 0.01534719
Iteration 95, loss = 0.01523236
Iteration 96, loss = 0.01514347
Iteration 97, loss = 0.01502947
Iteration 98, loss = 0.01491166
Iteration 99, loss = 0.01477023
Iteration 100, loss = 0.01467732
Iteration 101, loss = 0.01457843
Iteration 102, loss = 0.01446208
Iteration 103, loss = 0.01437227
Iteration 104, loss = 0.01431733
Iteration 105, loss = 0.01417272
Iteration 106, loss = 0.01411663
Iteration 107, loss = 0.01402325
Iteration 108, loss = 0.01396596
Iteration 109, loss = 0.01387573
Iteration 110, loss = 0.01380511
Iteration 111, loss = 0.01371950
Iteration 112, loss = 0.01367642
Iteration 113, loss = 0.01351843
Iteration 114, loss = 0.01349437
Iteration 115, loss = 0.01350011
Iteration 116, loss = 0.01343038
Iteration 117, loss = 0.01335221
Iteration 118, loss = 0.01323794
Iteration 119, loss = 0.01315541
Iteration 120, loss = 0.01309425
Iteration 121, loss = 0.01303844
Iteration 122, loss = 0.01301332
Iteration 123, loss = 0.01292875
Iteration 124, loss = 0.01290352
Iteration 125, loss = 0.01280786
Iteration 126, loss = 0.01278821
Iteration 127, loss = 0.01270129
Iteration 128, loss = 0.01264581
Iteration 129, loss = 0.01259041
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67905958
Iteration 2, loss = 0.63402321
Iteration 3, loss = 0.56802551
Iteration 4, loss = 0.48427354
Iteration 5, loss = 0.39895002
Iteration 6, loss = 0.32213182
Iteration 7, loss = 0.25809313
Iteration 8, loss = 0.20730922
Iteration 9, loss = 0.16816571
Iteration 10, loss = 0.13838949
Iteration 11, loss = 0.11574536
Iteration 12, loss = 0.09843327
Iteration 13, loss = 0.08495579
Iteration 14, loss = 0.07436676
Iteration 15, loss = 0.06590556
Iteration 16, loss = 0.05908934
Iteration 17, loss = 0.05354758
Iteration 18, loss = 0.04897532
Iteration 19, loss = 0.04513845
Iteration 20, loss = 0.04177023
Iteration 21, loss = 0.03901888
Iteration 22, loss = 0.03676611
Iteration 23, loss = 0.03456515
Iteration 24, loss = 0.03278842
Iteration 25, loss = 0.03136380
Iteration 26, loss = 0.02977544
Iteration 27, loss = 0.02856510
Iteration 28, loss = 0.02745921
Iteration 29, loss = 0.02663539
Iteration 30, loss = 0.02569666
Iteration 31, loss = 0.02486912
Iteration 32, loss = 0.02419230
Iteration 33, loss = 0.02362563
Iteration 34, loss = 0.02298616
Iteration 35, loss = 0.02278641
Iteration 36, loss = 0.02204104
Iteration 37, loss = 0.02169334
Iteration 38, loss = 0.02123172
Iteration 39, loss = 0.02095269
Iteration 40, loss = 0.02049001
Iteration 41, loss = 0.01998593
Iteration 42, loss = 0.01988328
Iteration 43, loss = 0.01957168
Iteration 44, loss = 0.01924087
Iteration 45, loss = 0.01903588
Iteration 46, loss = 0.01867339
Iteration 47, loss = 0.01862082
Iteration 48, loss = 0.01845709
Iteration 49, loss = 0.01830387
Iteration 50, loss = 0.01806349
Iteration 51, loss = 0.01806341
Iteration 52, loss = 0.01773950
Iteration 53, loss = 0.01768099
Iteration 54, loss = 0.01747961
Iteration 55, loss = 0.01733713
Iteration 56, loss = 0.01710649
Iteration 57, loss = 0.01708692
Iteration 58, loss = 0.01695774
Iteration 59, loss = 0.01693588
Iteration 60, loss = 0.01677750
Iteration 61, loss = 0.01681812
Iteration 62, loss = 0.01666568
Iteration 63, loss = 0.01654725
Iteration 64, loss = 0.01650313
Iteration 65, loss = 0.01632851
Iteration 66, loss = 0.01650421
Iteration 67, loss = 0.01610647
Iteration 68, loss = 0.01665037
Iteration 69, loss = 0.01626310
Iteration 70, loss = 0.01633828
Iteration 71, loss = 0.01611155
Iteration 72, loss = 0.01582559
Iteration 73, loss = 0.01593174
Iteration 74, loss = 0.01601713
Iteration 75, loss = 0.01580877
Iteration 76, loss = 0.01592221
Iteration 77, loss = 0.01589326
Iteration 78, loss = 0.01575239
Iteration 79, loss = 0.01626612
Iteration 80, loss = 0.01591754
Iteration 81, loss = 0.01564202
Iteration 82, loss = 0.01566003
Iteration 83, loss = 0.01546734
Iteration 84, loss = 0.01547468
Iteration 85, loss = 0.01539897
Iteration 86, loss = 0.01537302
Iteration 87, loss = 0.01537134
Iteration 88, loss = 0.01549225
Iteration 89, loss = 0.01532909
Iteration 90, loss = 0.01548581
Iteration 91, loss = 0.01538593
Iteration 92, loss = 0.01519308
Iteration 93, loss = 0.01508248
Iteration 94, loss = 0.01515003
Iteration 95, loss = 0.01559358
Iteration 96, loss = 0.01505933
Iteration 97, loss = 0.01514415
Iteration 98, loss = 0.01512043
Iteration 99, loss = 0.01505766
Iteration 100, loss = 0.01505527
Iteration 101, loss = 0.01516927
Iteration 102, loss = 0.01506841
Iteration 103, loss = 0.01500317
Iteration 104, loss = 0.01502692
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67444688
Iteration 2, loss = 0.59822130
Iteration 3, loss = 0.48247445
Iteration 4, loss = 0.36913793
Iteration 5, loss = 0.27454340
Iteration 6, loss = 0.20448237
Iteration 7, loss = 0.15604932
Iteration 8, loss = 0.12292308
Iteration 9, loss = 0.09976920
Iteration 10, loss = 0.08345546
Iteration 11, loss = 0.07161669
Iteration 12, loss = 0.06245978
Iteration 13, loss = 0.05595028
Iteration 14, loss = 0.05035780
Iteration 15, loss = 0.04631598
Iteration 16, loss = 0.04279641
Iteration 17, loss = 0.04013755
Iteration 18, loss = 0.03768342
Iteration 19, loss = 0.03589691
Iteration 20, loss = 0.03458620
Iteration 21, loss = 0.03285230
Iteration 22, loss = 0.03159748
Iteration 23, loss = 0.03076676
Iteration 24, loss = 0.02986960
Iteration 25, loss = 0.02910371
Iteration 26, loss = 0.02851176
Iteration 27, loss = 0.02790070
Iteration 28, loss = 0.02721095
Iteration 29, loss = 0.02683314
Iteration 30, loss = 0.02611865
Iteration 31, loss = 0.02616807
Iteration 32, loss = 0.02577558
Iteration 33, loss = 0.02542743
Iteration 34, loss = 0.02522120
Iteration 35, loss = 0.02492782
Iteration 36, loss = 0.02471106
Iteration 37, loss = 0.02454406
Iteration 38, loss = 0.02425056
Iteration 39, loss = 0.02399826
Iteration 40, loss = 0.02374003
Iteration 41, loss = 0.02352312
Iteration 42, loss = 0.02358491
Iteration 43, loss = 0.02363261
Iteration 44, loss = 0.02327910
Iteration 45, loss = 0.02335829
Iteration 46, loss = 0.02312040
Iteration 47, loss = 0.02303568
Iteration 48, loss = 0.02288141
Iteration 49, loss = 0.02300501
Iteration 50, loss = 0.02282751
Iteration 51, loss = 0.02282086
Iteration 52, loss = 0.02291186
Iteration 53, loss = 0.02259839
Iteration 54, loss = 0.02248208
Iteration 55, loss = 0.02252038
Iteration 56, loss = 0.02210944
Iteration 57, loss = 0.02231692
Iteration 58, loss = 0.02219912
Iteration 59, loss = 0.02205258
Iteration 60, loss = 0.02224643
Iteration 61, loss = 0.02212624
Iteration 62, loss = 0.02199575
Iteration 63, loss = 0.02199673
Iteration 64, loss = 0.02216556
Iteration 65, loss = 0.02231846
Iteration 66, loss = 0.02164307
Iteration 67, loss = 0.02214686
Iteration 68, loss = 0.02207190
Iteration 69, loss = 0.02187796
Iteration 70, loss = 0.02161339
Iteration 71, loss = 0.02189982
Iteration 72, loss = 0.02183765
Iteration 73, loss = 0.02185522
Iteration 74, loss = 0.02195778
Iteration 75, loss = 0.02173707
Iteration 76, loss = 0.02184420
Iteration 77, loss = 0.02152633
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67496243
Iteration 2, loss = 0.65861759
Iteration 3, loss = 0.64474860
Iteration 4, loss = 0.63102508
Iteration 5, loss = 0.61658237
Iteration 6, loss = 0.60144511
Iteration 7, loss = 0.58467983
Iteration 8, loss = 0.56659718
Iteration 9, loss = 0.54711939
Iteration 10, loss = 0.52646168
Iteration 11, loss = 0.50476794
Iteration 12, loss = 0.48201004
Iteration 13, loss = 0.45899809
Iteration 14, loss = 0.43554145
Iteration 15, loss = 0.41200147
Iteration 16, loss = 0.38884155
Iteration 17, loss = 0.36590582
Iteration 18, loss = 0.34368897
Iteration 19, loss = 0.32228219
Iteration 20, loss = 0.30185934
Iteration 21, loss = 0.28253503
Iteration 22, loss = 0.26401726
Iteration 23, loss = 0.24668884
Iteration 24, loss = 0.23039881
Iteration 25, loss = 0.21515159
Iteration 26, loss = 0.20104368
Iteration 27, loss = 0.18775377
Iteration 28, loss = 0.17553169
Iteration 29, loss = 0.16415822
Iteration 30, loss = 0.15364846
Iteration 31, loss = 0.14395478
Iteration 32, loss = 0.13502680
Iteration 33, loss = 0.12683303
Iteration 34, loss = 0.11917738
Iteration 35, loss = 0.11208305
Iteration 36, loss = 0.10563385
Iteration 37, loss = 0.09957229
Iteration 38, loss = 0.09405215
Iteration 39, loss = 0.08896048
Iteration 40, loss = 0.08420417
Iteration 41, loss = 0.07982633
Iteration 42, loss = 0.07574210
Iteration 43, loss = 0.07201300
Iteration 44, loss = 0.06853142
Iteration 45, loss = 0.06522089
Iteration 46, loss = 0.06223048
Iteration 47, loss = 0.05939585
Iteration 48, loss = 0.05673906
Iteration 49, loss = 0.05429382
Iteration 50, loss = 0.05196572
Iteration 51, loss = 0.04983167
Iteration 52, loss = 0.04778343
Iteration 53, loss = 0.04588435
Iteration 54, loss = 0.04410098
Iteration 55, loss = 0.04240820
Iteration 56, loss = 0.04082965
Iteration 57, loss = 0.03932781
Iteration 58, loss = 0.03791907
Iteration 59, loss = 0.03659923
Iteration 60, loss = 0.03531735
Iteration 61, loss = 0.03411856
Iteration 62, loss = 0.03301084
Iteration 63, loss = 0.03191475
Iteration 64, loss = 0.03089872
Iteration 65, loss = 0.02992068
Iteration 66, loss = 0.02900177
Iteration 67, loss = 0.02812075
Iteration 68, loss = 0.02727170
Iteration 69, loss = 0.02648523
Iteration 70, loss = 0.02570056
Iteration 71, loss = 0.02498183
Iteration 72, loss = 0.02427893
Iteration 73, loss = 0.02359908
Iteration 74, loss = 0.02294742
Iteration 75, loss = 0.02234135
Iteration 76, loss = 0.02174683
Iteration 77, loss = 0.02120036
Iteration 78, loss = 0.02065707
Iteration 79, loss = 0.02014020
Iteration 80, loss = 0.01964641
Iteration 81, loss = 0.01917626
Iteration 82, loss = 0.01870783
Iteration 83, loss = 0.01826649
Iteration 84, loss = 0.01785617
Iteration 85, loss = 0.01745365
Iteration 86, loss = 0.01704849
Iteration 87, loss = 0.01665740
Iteration 88, loss = 0.01628587
Iteration 89, loss = 0.01594576
Iteration 90, loss = 0.01558776
Iteration 91, loss = 0.01526699
Iteration 92, loss = 0.01493849
Iteration 93, loss = 0.01463201
Iteration 94, loss = 0.01434099
Iteration 95, loss = 0.01403964
Iteration 96, loss = 0.01376240
Iteration 97, loss = 0.01348128
Iteration 98, loss = 0.01322349
Iteration 99, loss = 0.01296534
Iteration 100, loss = 0.01272158
Iteration 101, loss = 0.01248281
Iteration 102, loss = 0.01225044
Iteration 103, loss = 0.01203343
Iteration 104, loss = 0.01180667
Iteration 105, loss = 0.01159637
Iteration 106, loss = 0.01139046
Iteration 107, loss = 0.01119958
Iteration 108, loss = 0.01100221
Iteration 109, loss = 0.01080967
Iteration 110, loss = 0.01063435
Iteration 111, loss = 0.01044761
Iteration 112, loss = 0.01026438
Iteration 113, loss = 0.01010035
Iteration 114, loss = 0.00993157
Iteration 115, loss = 0.00976923
Iteration 116, loss = 0.00961635
Iteration 117, loss = 0.00946570
Iteration 118, loss = 0.00931897
Iteration 119, loss = 0.00917729
Iteration 120, loss = 0.00904069
Iteration 121, loss = 0.00890260
Iteration 122, loss = 0.00877005
Iteration 123, loss = 0.00863866
Iteration 124, loss = 0.00851499
Iteration 125, loss = 0.00839512
Iteration 126, loss = 0.00827591
Iteration 127, loss = 0.00815506
Iteration 128, loss = 0.00804720
Iteration 129, loss = 0.00793539
Iteration 130, loss = 0.00782470
Iteration 131, loss = 0.00772515
Iteration 132, loss = 0.00761739
Iteration 133, loss = 0.00751766
Iteration 134, loss = 0.00741849
Iteration 135, loss = 0.00732106
Iteration 136, loss = 0.00722511
Iteration 137, loss = 0.00713338
Iteration 138, loss = 0.00704380
Iteration 139, loss = 0.00695605
Iteration 140, loss = 0.00686972
Iteration 141, loss = 0.00678662
Iteration 142, loss = 0.00670838
Iteration 143, loss = 0.00662201
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67848432
Iteration 2, loss = 0.63126195
Iteration 3, loss = 0.56878771
Iteration 4, loss = 0.49416223
Iteration 5, loss = 0.41815184
Iteration 6, loss = 0.34981809
Iteration 7, loss = 0.29061565
Iteration 8, loss = 0.24108337
Iteration 9, loss = 0.20080741
Iteration 10, loss = 0.16836881
Iteration 11, loss = 0.14284556
Iteration 12, loss = 0.12227054
Iteration 13, loss = 0.10590173
Iteration 14, loss = 0.09263784
Iteration 15, loss = 0.08202417
Iteration 16, loss = 0.07334289
Iteration 17, loss = 0.06624752
Iteration 18, loss = 0.06022350
Iteration 19, loss = 0.05531914
Iteration 20, loss = 0.05110331
Iteration 21, loss = 0.04746804
Iteration 22, loss = 0.04442697
Iteration 23, loss = 0.04167430
Iteration 24, loss = 0.03933494
Iteration 25, loss = 0.03734806
Iteration 26, loss = 0.03546584
Iteration 27, loss = 0.03381434
Iteration 28, loss = 0.03262211
Iteration 29, loss = 0.03132659
Iteration 30, loss = 0.03010104
Iteration 31, loss = 0.02915924
Iteration 32, loss = 0.02826100
Iteration 33, loss = 0.02744615
Iteration 34, loss = 0.02663322
Iteration 35, loss = 0.02591782
Iteration 36, loss = 0.02531056
Iteration 37, loss = 0.02506499
Iteration 38, loss = 0.02416071
Iteration 39, loss = 0.02376304
Iteration 40, loss = 0.02322154
Iteration 41, loss = 0.02293658
Iteration 42, loss = 0.02247507
Iteration 43, loss = 0.02221666
Iteration 44, loss = 0.02188823
Iteration 45, loss = 0.02149205
Iteration 46, loss = 0.02115301
Iteration 47, loss = 0.02091262
Iteration 48, loss = 0.02062547
Iteration 49, loss = 0.02043276
Iteration 50, loss = 0.02035064
Iteration 51, loss = 0.01998879
Iteration 52, loss = 0.01993541
Iteration 53, loss = 0.01952435
Iteration 54, loss = 0.01956441
Iteration 55, loss = 0.01953254
Iteration 56, loss = 0.01913969
Iteration 57, loss = 0.01901976
Iteration 58, loss = 0.01882325
Iteration 59, loss = 0.01880188
Iteration 60, loss = 0.01852775
Iteration 61, loss = 0.01863401
Iteration 62, loss = 0.01838995
Iteration 63, loss = 0.01826431
Iteration 64, loss = 0.01825875
Iteration 65, loss = 0.01806489
Iteration 66, loss = 0.01796416
Iteration 67, loss = 0.01790740
Iteration 68, loss = 0.01788716
Iteration 69, loss = 0.01794210
Iteration 70, loss = 0.01768603
Iteration 71, loss = 0.01757477
Iteration 72, loss = 0.01767890
Iteration 73, loss = 0.01742570
Iteration 74, loss = 0.01739171
Iteration 75, loss = 0.01739914
Iteration 76, loss = 0.01723430
Iteration 77, loss = 0.01723287
Iteration 78, loss = 0.01723030
Iteration 79, loss = 0.01710957
Iteration 80, loss = 0.01707042
Iteration 81, loss = 0.01715793
Iteration 82, loss = 0.01700319
Iteration 83, loss = 0.01690857
Iteration 84, loss = 0.01696635
Iteration 85, loss = 0.01695459
Iteration 86, loss = 0.01675051
Iteration 87, loss = 0.01676940
Iteration 88, loss = 0.01682586
Iteration 89, loss = 0.01673101
Iteration 90, loss = 0.01657962
Iteration 91, loss = 0.01657329
Iteration 92, loss = 0.01645311
Iteration 93, loss = 0.01671314
Iteration 94, loss = 0.01657192
Iteration 95, loss = 0.01653624
Iteration 96, loss = 0.01634228
Iteration 97, loss = 0.01630742
Iteration 98, loss = 0.01638237
Iteration 99, loss = 0.01654073
Iteration 100, loss = 0.01691350
Iteration 101, loss = 0.01629251
Iteration 102, loss = 0.01626468
Iteration 103, loss = 0.01628055
Iteration 104, loss = 0.01644170
Iteration 105, loss = 0.01651617
Iteration 106, loss = 0.01634815
Iteration 107, loss = 0.01615747
Iteration 108, loss = 0.01624327
Iteration 109, loss = 0.01642011
Iteration 110, loss = 0.01616702
Iteration 111, loss = 0.01612939
Iteration 112, loss = 0.01615771
Iteration 113, loss = 0.01605405
Iteration 114, loss = 0.01603807
Iteration 115, loss = 0.01607632
Iteration 116, loss = 0.01605253
Iteration 117, loss = 0.01586644
Iteration 118, loss = 0.01616448
Iteration 119, loss = 0.01607479
Iteration 120, loss = 0.01606971
Iteration 121, loss = 0.01594993
Iteration 122, loss = 0.01592264
Iteration 123, loss = 0.01579833
Iteration 124, loss = 0.01594288
Iteration 125, loss = 0.01591619
Iteration 126, loss = 0.01583959
Iteration 127, loss = 0.01602906
Iteration 128, loss = 0.01573353
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69268789
Iteration 2, loss = 0.64761704
Iteration 3, loss = 0.58022572
Iteration 4, loss = 0.49035018
Iteration 5, loss = 0.40021577
Iteration 6, loss = 0.31960329
Iteration 7, loss = 0.25308845
Iteration 8, loss = 0.20088160
Iteration 9, loss = 0.16166714
Iteration 10, loss = 0.13217233
Iteration 11, loss = 0.11024591
Iteration 12, loss = 0.09344387
Iteration 13, loss = 0.08065362
Iteration 14, loss = 0.07064524
Iteration 15, loss = 0.06292792
Iteration 16, loss = 0.05652613
Iteration 17, loss = 0.05136541
Iteration 18, loss = 0.04709544
Iteration 19, loss = 0.04355550
Iteration 20, loss = 0.04060401
Iteration 21, loss = 0.03806375
Iteration 22, loss = 0.03592998
Iteration 23, loss = 0.03401566
Iteration 24, loss = 0.03245880
Iteration 25, loss = 0.03101392
Iteration 26, loss = 0.02970585
Iteration 27, loss = 0.02864680
Iteration 28, loss = 0.02769758
Iteration 29, loss = 0.02679614
Iteration 30, loss = 0.02613141
Iteration 31, loss = 0.02524818
Iteration 32, loss = 0.02466494
Iteration 33, loss = 0.02418163
Iteration 34, loss = 0.02369006
Iteration 35, loss = 0.02316991
Iteration 36, loss = 0.02275528
Iteration 37, loss = 0.02254614
Iteration 38, loss = 0.02195238
Iteration 39, loss = 0.02178715
Iteration 40, loss = 0.02152632
Iteration 41, loss = 0.02120786
Iteration 42, loss = 0.02085008
Iteration 43, loss = 0.02069743
Iteration 44, loss = 0.02038391
Iteration 45, loss = 0.02043069
Iteration 46, loss = 0.02002884
Iteration 47, loss = 0.01997708
Iteration 48, loss = 0.01968510
Iteration 49, loss = 0.01964231
Iteration 50, loss = 0.01943997
Iteration 51, loss = 0.01925615
Iteration 52, loss = 0.01914522
Iteration 53, loss = 0.01903464
Iteration 54, loss = 0.01886449
Iteration 55, loss = 0.01875112
Iteration 56, loss = 0.01865657
Iteration 57, loss = 0.01857858
Iteration 58, loss = 0.01852020
Iteration 59, loss = 0.01866904
Iteration 60, loss = 0.01838871
Iteration 61, loss = 0.01832579
Iteration 62, loss = 0.01821267
Iteration 63, loss = 0.01828084
Iteration 64, loss = 0.01808470
Iteration 65, loss = 0.01791860
Iteration 66, loss = 0.01796079
Iteration 67, loss = 0.01810390
Iteration 68, loss = 0.01792727
Iteration 69, loss = 0.01784204
Iteration 70, loss = 0.01800452
Iteration 71, loss = 0.01771634
Iteration 72, loss = 0.01777595
Iteration 73, loss = 0.01766436
Iteration 74, loss = 0.01755860
Iteration 75, loss = 0.01748213
Iteration 76, loss = 0.01748013
Iteration 77, loss = 0.01741376
Iteration 78, loss = 0.01734606
Iteration 79, loss = 0.01730805
Iteration 80, loss = 0.01734210
Iteration 81, loss = 0.01725094
Iteration 82, loss = 0.01726359
Iteration 83, loss = 0.01751922
Iteration 84, loss = 0.01727394
Iteration 85, loss = 0.01717142
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70693291
Iteration 2, loss = 0.67561253
Iteration 3, loss = 0.64015095
Iteration 4, loss = 0.59787481
Iteration 5, loss = 0.55080780
Iteration 6, loss = 0.49971488
Iteration 7, loss = 0.44748303
Iteration 8, loss = 0.39653096
Iteration 9, loss = 0.34882635
Iteration 10, loss = 0.30540278
Iteration 11, loss = 0.26688240
Iteration 12, loss = 0.23308386
Iteration 13, loss = 0.20390095
Iteration 14, loss = 0.17898948
Iteration 15, loss = 0.15781730
Iteration 16, loss = 0.14001554
Iteration 17, loss = 0.12478128
Iteration 18, loss = 0.11193152
Iteration 19, loss = 0.10093807
Iteration 20, loss = 0.09154541
Iteration 21, loss = 0.08357405
Iteration 22, loss = 0.07665023
Iteration 23, loss = 0.07068546
Iteration 24, loss = 0.06557660
Iteration 25, loss = 0.06099146
Iteration 26, loss = 0.05692910
Iteration 27, loss = 0.05337400
Iteration 28, loss = 0.05019359
Iteration 29, loss = 0.04737084
Iteration 30, loss = 0.04483193
Iteration 31, loss = 0.04267484
Iteration 32, loss = 0.04075559
Iteration 33, loss = 0.03893347
Iteration 34, loss = 0.03717443
Iteration 35, loss = 0.03562103
Iteration 36, loss = 0.03405224
Iteration 37, loss = 0.03296680
Iteration 38, loss = 0.03178607
Iteration 39, loss = 0.03068700
Iteration 40, loss = 0.02973592
Iteration 41, loss = 0.02879233
Iteration 42, loss = 0.02794835
Iteration 43, loss = 0.02707983
Iteration 44, loss = 0.02637004
Iteration 45, loss = 0.02572821
Iteration 46, loss = 0.02506205
Iteration 47, loss = 0.02456167
Iteration 48, loss = 0.02397326
Iteration 49, loss = 0.02343951
Iteration 50, loss = 0.02296940
Iteration 51, loss = 0.02257982
Iteration 52, loss = 0.02228778
Iteration 53, loss = 0.02188475
Iteration 54, loss = 0.02147533
Iteration 55, loss = 0.02102247
Iteration 56, loss = 0.02068832
Iteration 57, loss = 0.02047690
Iteration 58, loss = 0.02020090
Iteration 59, loss = 0.01966000
Iteration 60, loss = 0.01940703
Iteration 61, loss = 0.01921139
Iteration 62, loss = 0.01896919
Iteration 63, loss = 0.01864224
Iteration 64, loss = 0.01873144
Iteration 65, loss = 0.01863472
Iteration 66, loss = 0.01836671
Iteration 67, loss = 0.01808790
Iteration 68, loss = 0.01792654
Iteration 69, loss = 0.01761763
Iteration 70, loss = 0.01731747
Iteration 71, loss = 0.01713704
Iteration 72, loss = 0.01699709
Iteration 73, loss = 0.01687930
Iteration 74, loss = 0.01670270
Iteration 75, loss = 0.01654397
Iteration 76, loss = 0.01648338
Iteration 77, loss = 0.01632546
Iteration 78, loss = 0.01609301
Iteration 79, loss = 0.01600735
Iteration 80, loss = 0.01601168
Iteration 81, loss = 0.01604356
Iteration 82, loss = 0.01593536
Iteration 83, loss = 0.01583087
Iteration 84, loss = 0.01581181
Iteration 85, loss = 0.01580850
Iteration 86, loss = 0.01572159
Iteration 87, loss = 0.01568163
Iteration 88, loss = 0.01556047
Iteration 89, loss = 0.01574653
Iteration 90, loss = 0.01568239
Iteration 91, loss = 0.01558439
Iteration 92, loss = 0.01534113
Iteration 93, loss = 0.01520610
Iteration 94, loss = 0.01499793
Iteration 95, loss = 0.01513741
Iteration 96, loss = 0.01508906
Iteration 97, loss = 0.01482768
Iteration 98, loss = 0.01483766
Iteration 99, loss = 0.01470159
Iteration 100, loss = 0.01455245
Iteration 101, loss = 0.01434967
Iteration 102, loss = 0.01453462
Iteration 103, loss = 0.01440411
Iteration 104, loss = 0.01436844
Iteration 105, loss = 0.01424494
Iteration 106, loss = 0.01417692
Iteration 107, loss = 0.01419686
Iteration 108, loss = 0.01418689
Iteration 109, loss = 0.01417825
Iteration 110, loss = 0.01407560
Iteration 111, loss = 0.01393387
Iteration 112, loss = 0.01394489
Iteration 113, loss = 0.01386632
Iteration 114, loss = 0.01374993
Iteration 115, loss = 0.01370866
Iteration 116, loss = 0.01363056
Iteration 117, loss = 0.01364777
Iteration 118, loss = 0.01359149
Iteration 119, loss = 0.01342607
Iteration 120, loss = 0.01363930
Iteration 121, loss = 0.01370439
Iteration 122, loss = 0.01355006
Iteration 123, loss = 0.01351068
Iteration 124, loss = 0.01344952
Iteration 125, loss = 0.01341822
Iteration 126, loss = 0.01365001
Iteration 127, loss = 0.01359446
Iteration 128, loss = 0.01357227
Iteration 129, loss = 0.01353078
Iteration 130, loss = 0.01346137
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68949173
Iteration 2, loss = 0.62222949
Iteration 3, loss = 0.52944445
Iteration 4, loss = 0.42483394
Iteration 5, loss = 0.33042817
Iteration 6, loss = 0.25430666
Iteration 7, loss = 0.19665656
Iteration 8, loss = 0.15480077
Iteration 9, loss = 0.12468872
Iteration 10, loss = 0.10286272
Iteration 11, loss = 0.08670113
Iteration 12, loss = 0.07476662
Iteration 13, loss = 0.06566080
Iteration 14, loss = 0.05861905
Iteration 15, loss = 0.05332753
Iteration 16, loss = 0.04857388
Iteration 17, loss = 0.04479248
Iteration 18, loss = 0.04167819
Iteration 19, loss = 0.03908058
Iteration 20, loss = 0.03677760
Iteration 21, loss = 0.03505392
Iteration 22, loss = 0.03348156
Iteration 23, loss = 0.03184505
Iteration 24, loss = 0.03074853
Iteration 25, loss = 0.02952564
Iteration 26, loss = 0.02904807
Iteration 27, loss = 0.02788827
Iteration 28, loss = 0.02713609
Iteration 29, loss = 0.02655311
Iteration 30, loss = 0.02586034
Iteration 31, loss = 0.02537741
Iteration 32, loss = 0.02504884
Iteration 33, loss = 0.02458380
Iteration 34, loss = 0.02403452
Iteration 35, loss = 0.02440437
Iteration 36, loss = 0.02435473
Iteration 37, loss = 0.02381780
Iteration 38, loss = 0.02328742
Iteration 39, loss = 0.02313921
Iteration 40, loss = 0.02279243
Iteration 41, loss = 0.02261377
Iteration 42, loss = 0.02241219
Iteration 43, loss = 0.02198081
Iteration 44, loss = 0.02187417
Iteration 45, loss = 0.02188141
Iteration 46, loss = 0.02142494
Iteration 47, loss = 0.02136074
Iteration 48, loss = 0.02119515
Iteration 49, loss = 0.02110476
Iteration 50, loss = 0.02112347
Iteration 51, loss = 0.02112053
Iteration 52, loss = 0.02081594
Iteration 53, loss = 0.02060531
Iteration 54, loss = 0.02053873
Iteration 55, loss = 0.02029442
Iteration 56, loss = 0.02034720
Iteration 57, loss = 0.02044519
Iteration 58, loss = 0.02019379
Iteration 59, loss = 0.02001319
Iteration 60, loss = 0.01990024
Iteration 61, loss = 0.01989176
Iteration 62, loss = 0.01974368
Iteration 63, loss = 0.01971029
Iteration 64, loss = 0.01950710
Iteration 65, loss = 0.01971478
Iteration 66, loss = 0.01952038
Iteration 67, loss = 0.01951109
Iteration 68, loss = 0.01941914
Iteration 69, loss = 0.01941920
Iteration 70, loss = 0.01927786
Iteration 71, loss = 0.01916234
Iteration 72, loss = 0.01916300
Iteration 73, loss = 0.01924091
Iteration 74, loss = 0.01904040
Iteration 75, loss = 0.01911564
Iteration 76, loss = 0.01915155
Iteration 77, loss = 0.01915429
Iteration 78, loss = 0.01888014
Iteration 79, loss = 0.01877036
Iteration 80, loss = 0.01872970
Iteration 81, loss = 0.01872901
Iteration 82, loss = 0.01888902
Iteration 83, loss = 0.01860815
Iteration 84, loss = 0.01868453
Iteration 85, loss = 0.01859234
Iteration 86, loss = 0.01841930
Iteration 87, loss = 0.01859727
Iteration 88, loss = 0.01943122
Iteration 89, loss = 0.02042294
Iteration 90, loss = 0.02039183
Iteration 91, loss = 0.01996194
Iteration 92, loss = 0.01974274
Iteration 93, loss = 0.01950113
Iteration 94, loss = 0.01940381
Iteration 95, loss = 0.01931676
Iteration 96, loss = 0.01909107
Iteration 97, loss = 0.01899129
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68080260
Iteration 2, loss = 0.60646073
Iteration 3, loss = 0.49653315
Iteration 4, loss = 0.38292485
Iteration 5, loss = 0.28609168
Iteration 6, loss = 0.21275238
Iteration 7, loss = 0.16167392
Iteration 8, loss = 0.12671468
Iteration 9, loss = 0.10287345
Iteration 10, loss = 0.08562907
Iteration 11, loss = 0.07343106
Iteration 12, loss = 0.06435694
Iteration 13, loss = 0.05744825
Iteration 14, loss = 0.05224079
Iteration 15, loss = 0.04776823
Iteration 16, loss = 0.04450835
Iteration 17, loss = 0.04182906
Iteration 18, loss = 0.03937981
Iteration 19, loss = 0.03772223
Iteration 20, loss = 0.03617707
Iteration 21, loss = 0.03475236
Iteration 22, loss = 0.03356368
Iteration 23, loss = 0.03242600
Iteration 24, loss = 0.03159702
Iteration 25, loss = 0.03086474
Iteration 26, loss = 0.03028787
Iteration 27, loss = 0.02995004
Iteration 28, loss = 0.02892366
Iteration 29, loss = 0.02877518
Iteration 30, loss = 0.02815576
Iteration 31, loss = 0.02799577
Iteration 32, loss = 0.02754655
Iteration 33, loss = 0.02721037
Iteration 34, loss = 0.02692532
Iteration 35, loss = 0.02660719
Iteration 36, loss = 0.02678607
Iteration 37, loss = 0.02597171
Iteration 38, loss = 0.02610118
Iteration 39, loss = 0.02583300
Iteration 40, loss = 0.02555420
Iteration 41, loss = 0.02559210
Iteration 42, loss = 0.02506295
Iteration 43, loss = 0.02536681
Iteration 44, loss = 0.02522841
Iteration 45, loss = 0.02493598
Iteration 46, loss = 0.02496163
Iteration 47, loss = 0.02497459
Iteration 48, loss = 0.02475344
Iteration 49, loss = 0.02482840
Iteration 50, loss = 0.02467853
Iteration 51, loss = 0.02442434
Iteration 52, loss = 0.02459710
Iteration 53, loss = 0.02435824
Iteration 54, loss = 0.02453458
Iteration 55, loss = 0.02426069
Iteration 56, loss = 0.02445901
Iteration 57, loss = 0.02421261
Iteration 58, loss = 0.02402334
Iteration 59, loss = 0.02394081
Iteration 60, loss = 0.02399824
Iteration 61, loss = 0.02395372
Iteration 62, loss = 0.02383748
Iteration 63, loss = 0.02403758
Iteration 64, loss = 0.02394348
Iteration 65, loss = 0.02364763
Iteration 66, loss = 0.02412247
Iteration 67, loss = 0.02354648
Iteration 68, loss = 0.02385798
Iteration 69, loss = 0.02386643
Iteration 70, loss = 0.02345004
Iteration 71, loss = 0.02369536
Iteration 72, loss = 0.02362138
Iteration 73, loss = 0.02358905
Iteration 74, loss = 0.02341063
Iteration 75, loss = 0.02351855
Iteration 76, loss = 0.02352730
Iteration 77, loss = 0.02331979
Iteration 78, loss = 0.02363078
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67623418
Iteration 2, loss = 0.60017217
Iteration 3, loss = 0.48058794
Iteration 4, loss = 0.36314187
Iteration 5, loss = 0.26724828
Iteration 6, loss = 0.19793696
Iteration 7, loss = 0.15025407
Iteration 8, loss = 0.11782601
Iteration 9, loss = 0.09557971
Iteration 10, loss = 0.07995196
Iteration 11, loss = 0.06808333
Iteration 12, loss = 0.05994175
Iteration 13, loss = 0.05283834
Iteration 14, loss = 0.04785622
Iteration 15, loss = 0.04381011
Iteration 16, loss = 0.04069256
Iteration 17, loss = 0.03790915
Iteration 18, loss = 0.03547747
Iteration 19, loss = 0.03371066
Iteration 20, loss = 0.03217199
Iteration 21, loss = 0.03078138
Iteration 22, loss = 0.02954559
Iteration 23, loss = 0.02860202
Iteration 24, loss = 0.02792259
Iteration 25, loss = 0.02695583
Iteration 26, loss = 0.02633072
Iteration 27, loss = 0.02558342
Iteration 28, loss = 0.02535725
Iteration 29, loss = 0.02462348
Iteration 30, loss = 0.02418239
Iteration 31, loss = 0.02372706
Iteration 32, loss = 0.02359010
Iteration 33, loss = 0.02357918
Iteration 34, loss = 0.02299134
Iteration 35, loss = 0.02273703
Iteration 36, loss = 0.02262042
Iteration 37, loss = 0.02228943
Iteration 38, loss = 0.02204308
Iteration 39, loss = 0.02196297
Iteration 40, loss = 0.02164930
Iteration 41, loss = 0.02153362
Iteration 42, loss = 0.02150818
Iteration 43, loss = 0.02139813
Iteration 44, loss = 0.02110379
Iteration 45, loss = 0.02107985
Iteration 46, loss = 0.02085471
Iteration 47, loss = 0.02107167
Iteration 48, loss = 0.02064630
Iteration 49, loss = 0.02065222
Iteration 50, loss = 0.02067765
Iteration 51, loss = 0.02064254
Iteration 52, loss = 0.02021154
Iteration 53, loss = 0.02009914
Iteration 54, loss = 0.02014530
Iteration 55, loss = 0.02011479
Iteration 56, loss = 0.02015207
Iteration 57, loss = 0.02023975
Iteration 58, loss = 0.01982512
Iteration 59, loss = 0.02005220
Iteration 60, loss = 0.01991047
Iteration 61, loss = 0.01998351
Iteration 62, loss = 0.01982148
Iteration 63, loss = 0.01992184
Iteration 64, loss = 0.01956824
Iteration 65, loss = 0.01972847
Iteration 66, loss = 0.01949127
Iteration 67, loss = 0.01960130
Iteration 68, loss = 0.01967597
Iteration 69, loss = 0.01964849
Iteration 70, loss = 0.01947214
Iteration 71, loss = 0.01946217
Iteration 72, loss = 0.01934738
Iteration 73, loss = 0.01951773
Iteration 74, loss = 0.01931689
Iteration 75, loss = 0.01947269
Iteration 76, loss = 0.01920380
Iteration 77, loss = 0.01953639
Iteration 78, loss = 0.01896231
Iteration 79, loss = 0.01955062
Iteration 80, loss = 0.01944883
Iteration 81, loss = 0.01922674
Iteration 82, loss = 0.01902236
Iteration 83, loss = 0.01944464
Iteration 84, loss = 0.01913967
Iteration 85, loss = 0.01926494
Iteration 86, loss = 0.01911471
Iteration 87, loss = 0.01936650
Iteration 88, loss = 0.01930961
Iteration 89, loss = 0.01910033
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68638863
Iteration 2, loss = 0.66171435
Iteration 3, loss = 0.63141838
Iteration 4, loss = 0.59179433
Iteration 5, loss = 0.54289328
Iteration 6, loss = 0.48914769
Iteration 7, loss = 0.43488761
Iteration 8, loss = 0.38276598
Iteration 9, loss = 0.33462590
Iteration 10, loss = 0.29154289
Iteration 11, loss = 0.25354634
Iteration 12, loss = 0.22071873
Iteration 13, loss = 0.19253501
Iteration 14, loss = 0.16864098
Iteration 15, loss = 0.14841317
Iteration 16, loss = 0.13142765
Iteration 17, loss = 0.11706123
Iteration 18, loss = 0.10497256
Iteration 19, loss = 0.09467603
Iteration 20, loss = 0.08594292
Iteration 21, loss = 0.07844020
Iteration 22, loss = 0.07196665
Iteration 23, loss = 0.06641187
Iteration 24, loss = 0.06156366
Iteration 25, loss = 0.05740377
Iteration 26, loss = 0.05349702
Iteration 27, loss = 0.05013604
Iteration 28, loss = 0.04715639
Iteration 29, loss = 0.04448845
Iteration 30, loss = 0.04210580
Iteration 31, loss = 0.04002267
Iteration 32, loss = 0.03815177
Iteration 33, loss = 0.03642424
Iteration 34, loss = 0.03482425
Iteration 35, loss = 0.03340408
Iteration 36, loss = 0.03212443
Iteration 37, loss = 0.03106227
Iteration 38, loss = 0.03004847
Iteration 39, loss = 0.02912783
Iteration 40, loss = 0.02806485
Iteration 41, loss = 0.02722915
Iteration 42, loss = 0.02640833
Iteration 43, loss = 0.02581612
Iteration 44, loss = 0.02502662
Iteration 45, loss = 0.02438937
Iteration 46, loss = 0.02384421
Iteration 47, loss = 0.02321688
Iteration 48, loss = 0.02278895
Iteration 49, loss = 0.02220566
Iteration 50, loss = 0.02175046
Iteration 51, loss = 0.02127825
Iteration 52, loss = 0.02105479
Iteration 53, loss = 0.02083913
Iteration 54, loss = 0.02036434
Iteration 55, loss = 0.02004912
Iteration 56, loss = 0.01964691
Iteration 57, loss = 0.01926719
Iteration 58, loss = 0.01904547
Iteration 59, loss = 0.01898169
Iteration 60, loss = 0.01870816
Iteration 61, loss = 0.01842012
Iteration 62, loss = 0.01805430
Iteration 63, loss = 0.01799336
Iteration 64, loss = 0.01759489
Iteration 65, loss = 0.01739801
Iteration 66, loss = 0.01719998
Iteration 67, loss = 0.01707227
Iteration 68, loss = 0.01700898
Iteration 69, loss = 0.01659938
Iteration 70, loss = 0.01665813
Iteration 71, loss = 0.01650637
Iteration 72, loss = 0.01629803
Iteration 73, loss = 0.01610266
Iteration 74, loss = 0.01605867
Iteration 75, loss = 0.01580848
Iteration 76, loss = 0.01566703
Iteration 77, loss = 0.01544272
Iteration 78, loss = 0.01560091
Iteration 79, loss = 0.01556819
Iteration 80, loss = 0.01536659
Iteration 81, loss = 0.01525705
Iteration 82, loss = 0.01518644
Iteration 83, loss = 0.01500947
Iteration 84, loss = 0.01488836
Iteration 85, loss = 0.01496535
Iteration 86, loss = 0.01471563
Iteration 87, loss = 0.01467851
Iteration 88, loss = 0.01454114
Iteration 89, loss = 0.01448274
Iteration 90, loss = 0.01447407
Iteration 91, loss = 0.01442990
Iteration 92, loss = 0.01440117
Iteration 93, loss = 0.01445432
Iteration 94, loss = 0.01438871
Iteration 95, loss = 0.01430124
Iteration 96, loss = 0.01408557
Iteration 97, loss = 0.01402157
Iteration 98, loss = 0.01420487
Iteration 99, loss = 0.01420130
Iteration 100, loss = 0.01447424
Iteration 101, loss = 0.01432996
Iteration 102, loss = 0.01426590
Iteration 103, loss = 0.01416461
Iteration 104, loss = 0.01385506
Iteration 105, loss = 0.01383960
Iteration 106, loss = 0.01363515
Iteration 107, loss = 0.01369331
Iteration 108, loss = 0.01377276
Iteration 109, loss = 0.01354608
Iteration 110, loss = 0.01353687
Iteration 111, loss = 0.01354313
Iteration 112, loss = 0.01334516
Iteration 113, loss = 0.01344475
Iteration 114, loss = 0.01359011
Iteration 115, loss = 0.01395769
Iteration 116, loss = 0.01355617
Iteration 117, loss = 0.01363196
Iteration 118, loss = 0.01350835
Iteration 119, loss = 0.01330924
Iteration 120, loss = 0.01339061
Iteration 121, loss = 0.01326801
Iteration 122, loss = 0.01335170
Iteration 123, loss = 0.01323601
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71951205
Iteration 2, loss = 0.65220979
Iteration 3, loss = 0.55087475
Iteration 4, loss = 0.43225772
Iteration 5, loss = 0.32517326
Iteration 6, loss = 0.24188112
Iteration 7, loss = 0.18283064
Iteration 8, loss = 0.14162749
Iteration 9, loss = 0.11304807
Iteration 10, loss = 0.09252896
Iteration 11, loss = 0.07827320
Iteration 12, loss = 0.06720471
Iteration 13, loss = 0.05901544
Iteration 14, loss = 0.05272666
Iteration 15, loss = 0.04763613
Iteration 16, loss = 0.04354794
Iteration 17, loss = 0.04029904
Iteration 18, loss = 0.03762699
Iteration 19, loss = 0.03521849
Iteration 20, loss = 0.03356942
Iteration 21, loss = 0.03173408
Iteration 22, loss = 0.03024426
Iteration 23, loss = 0.02920085
Iteration 24, loss = 0.02795309
Iteration 25, loss = 0.02715397
Iteration 26, loss = 0.02643836
Iteration 27, loss = 0.02566711
Iteration 28, loss = 0.02488285
Iteration 29, loss = 0.02439476
Iteration 30, loss = 0.02397813
Iteration 31, loss = 0.02363458
Iteration 32, loss = 0.02324563
Iteration 33, loss = 0.02277196
Iteration 34, loss = 0.02226773
Iteration 35, loss = 0.02202856
Iteration 36, loss = 0.02194420
Iteration 37, loss = 0.02172938
Iteration 38, loss = 0.02147454
Iteration 39, loss = 0.02121238
Iteration 40, loss = 0.02106448
Iteration 41, loss = 0.02070268
Iteration 42, loss = 0.02052585
Iteration 43, loss = 0.02051542
Iteration 44, loss = 0.02027691
Iteration 45, loss = 0.02019732
Iteration 46, loss = 0.02013997
Iteration 47, loss = 0.02010437
Iteration 48, loss = 0.01987946
Iteration 49, loss = 0.01953855
Iteration 50, loss = 0.01966545
Iteration 51, loss = 0.01965269
Iteration 52, loss = 0.01939383
Iteration 53, loss = 0.01940299
Iteration 54, loss = 0.01936870
Iteration 55, loss = 0.01921107
Iteration 56, loss = 0.01916530
Iteration 57, loss = 0.01901355
Iteration 58, loss = 0.01892235
Iteration 59, loss = 0.01904803
Iteration 60, loss = 0.01886274
Iteration 61, loss = 0.01875298
Iteration 62, loss = 0.01880828
Iteration 63, loss = 0.01876472
Iteration 64, loss = 0.01878560
Iteration 65, loss = 0.01872591
Iteration 66, loss = 0.01889739
Iteration 67, loss = 0.01868402
Iteration 68, loss = 0.01852360
Iteration 69, loss = 0.01848141
Iteration 70, loss = 0.01841179
Iteration 71, loss = 0.01819224
Iteration 72, loss = 0.01836633
Iteration 73, loss = 0.01841406
Iteration 74, loss = 0.01869826
Iteration 75, loss = 0.01812385
Iteration 76, loss = 0.01842922
Iteration 77, loss = 0.01808261
Iteration 78, loss = 0.01835016
Iteration 79, loss = 0.01850434
Iteration 80, loss = 0.01820841
Iteration 81, loss = 0.01796581
Iteration 82, loss = 0.01807798
Iteration 83, loss = 0.01801601
Iteration 84, loss = 0.01822830
Iteration 85, loss = 0.01793239
Iteration 86, loss = 0.01786692
Iteration 87, loss = 0.01811675
Iteration 88, loss = 0.01808201
Iteration 89, loss = 0.01781912
Iteration 90, loss = 0.01815520
Iteration 91, loss = 0.01793871
Iteration 92, loss = 0.01790771
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69901177
Iteration 2, loss = 0.66430631
Iteration 3, loss = 0.61947203
Iteration 4, loss = 0.56338715
Iteration 5, loss = 0.49991633
Iteration 6, loss = 0.43669771
Iteration 7, loss = 0.37621352
Iteration 8, loss = 0.32136829
Iteration 9, loss = 0.27317418
Iteration 10, loss = 0.23170139
Iteration 11, loss = 0.19699597
Iteration 12, loss = 0.16828587
Iteration 13, loss = 0.14480580
Iteration 14, loss = 0.12560089
Iteration 15, loss = 0.10989590
Iteration 16, loss = 0.09696805
Iteration 17, loss = 0.08638430
Iteration 18, loss = 0.07748455
Iteration 19, loss = 0.07009053
Iteration 20, loss = 0.06383131
Iteration 21, loss = 0.05847329
Iteration 22, loss = 0.05385995
Iteration 23, loss = 0.04985829
Iteration 24, loss = 0.04649823
Iteration 25, loss = 0.04357676
Iteration 26, loss = 0.04089284
Iteration 27, loss = 0.03873712
Iteration 28, loss = 0.03659925
Iteration 29, loss = 0.03469906
Iteration 30, loss = 0.03312563
Iteration 31, loss = 0.03166221
Iteration 32, loss = 0.03046229
Iteration 33, loss = 0.02923130
Iteration 34, loss = 0.02813861
Iteration 35, loss = 0.02720537
Iteration 36, loss = 0.02631505
Iteration 37, loss = 0.02552900
Iteration 38, loss = 0.02473486
Iteration 39, loss = 0.02398241
Iteration 40, loss = 0.02331819
Iteration 41, loss = 0.02296560
Iteration 42, loss = 0.02224686
Iteration 43, loss = 0.02171009
Iteration 44, loss = 0.02128979
Iteration 45, loss = 0.02089601
Iteration 46, loss = 0.02053117
Iteration 47, loss = 0.02018560
Iteration 48, loss = 0.01981453
Iteration 49, loss = 0.01951747
Iteration 50, loss = 0.01919592
Iteration 51, loss = 0.01891526
Iteration 52, loss = 0.01856152
Iteration 53, loss = 0.01828033
Iteration 54, loss = 0.01807945
Iteration 55, loss = 0.01770729
Iteration 56, loss = 0.01750851
Iteration 57, loss = 0.01744921
Iteration 58, loss = 0.01721138
Iteration 59, loss = 0.01701005
Iteration 60, loss = 0.01689544
Iteration 61, loss = 0.01676795
Iteration 62, loss = 0.01666723
Iteration 63, loss = 0.01651917
Iteration 64, loss = 0.01630616
Iteration 65, loss = 0.01619566
Iteration 66, loss = 0.01612958
Iteration 67, loss = 0.01592027
Iteration 68, loss = 0.01573170
Iteration 69, loss = 0.01579293
Iteration 70, loss = 0.01563392
Iteration 71, loss = 0.01540282
Iteration 72, loss = 0.01535109
Iteration 73, loss = 0.01522617
Iteration 74, loss = 0.01510545
Iteration 75, loss = 0.01507607
Iteration 76, loss = 0.01502112
Iteration 77, loss = 0.01500171
Iteration 78, loss = 0.01491723
Iteration 79, loss = 0.01476179
Iteration 80, loss = 0.01472764
Iteration 81, loss = 0.01460922
Iteration 82, loss = 0.01468990
Iteration 83, loss = 0.01451018
Iteration 84, loss = 0.01447201
Iteration 85, loss = 0.01454809
Iteration 86, loss = 0.01452309
Iteration 87, loss = 0.01426514
Iteration 88, loss = 0.01430335
Iteration 89, loss = 0.01419751
Iteration 90, loss = 0.01415540
Iteration 91, loss = 0.01408501
Iteration 92, loss = 0.01405450
Iteration 93, loss = 0.01396178
Iteration 94, loss = 0.01405669
Iteration 95, loss = 0.01392310
Iteration 96, loss = 0.01398612
Iteration 97, loss = 0.01398041
Iteration 98, loss = 0.01385631
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68023989
Iteration 2, loss = 0.60365578
Iteration 3, loss = 0.49667289
Iteration 4, loss = 0.38536249
Iteration 5, loss = 0.28700914
Iteration 6, loss = 0.21265870
Iteration 7, loss = 0.16021760
Iteration 8, loss = 0.12456478
Iteration 9, loss = 0.10007301
Iteration 10, loss = 0.08268458
Iteration 11, loss = 0.07043819
Iteration 12, loss = 0.06124004
Iteration 13, loss = 0.05431432
Iteration 14, loss = 0.04866504
Iteration 15, loss = 0.04440034
Iteration 16, loss = 0.04094097
Iteration 17, loss = 0.03814329
Iteration 18, loss = 0.03573037
Iteration 19, loss = 0.03398840
Iteration 20, loss = 0.03235552
Iteration 21, loss = 0.03086764
Iteration 22, loss = 0.02966633
Iteration 23, loss = 0.02868835
Iteration 24, loss = 0.02781847
Iteration 25, loss = 0.02707191
Iteration 26, loss = 0.02630504
Iteration 27, loss = 0.02564077
Iteration 28, loss = 0.02522330
Iteration 29, loss = 0.02459311
Iteration 30, loss = 0.02448360
Iteration 31, loss = 0.02395536
Iteration 32, loss = 0.02363813
Iteration 33, loss = 0.02335768
Iteration 34, loss = 0.02287396
Iteration 35, loss = 0.02249310
Iteration 36, loss = 0.02253896
Iteration 37, loss = 0.02225057
Iteration 38, loss = 0.02255350
Iteration 39, loss = 0.02172311
Iteration 40, loss = 0.02167670
Iteration 41, loss = 0.02158721
Iteration 42, loss = 0.02127199
Iteration 43, loss = 0.02116122
Iteration 44, loss = 0.02107808
Iteration 45, loss = 0.02101114
Iteration 46, loss = 0.02086869
Iteration 47, loss = 0.02058826
Iteration 48, loss = 0.02087189
Iteration 49, loss = 0.02030714
Iteration 50, loss = 0.02026990
Iteration 51, loss = 0.02028050
Iteration 52, loss = 0.02046420
Iteration 53, loss = 0.02045106
Iteration 54, loss = 0.02014978
Iteration 55, loss = 0.02030934
Iteration 56, loss = 0.02023470
Iteration 57, loss = 0.02020201
Iteration 58, loss = 0.01992314
Iteration 59, loss = 0.01972429
Iteration 60, loss = 0.02011710
Iteration 61, loss = 0.01990023
Iteration 62, loss = 0.02014184
Iteration 63, loss = 0.02009105
Iteration 64, loss = 0.01992609
Iteration 65, loss = 0.01972531
Iteration 66, loss = 0.01997763
Iteration 67, loss = 0.01955266
Iteration 68, loss = 0.01978509
Iteration 69, loss = 0.01934043
Iteration 70, loss = 0.01953120
Iteration 71, loss = 0.01957285
Iteration 72, loss = 0.01975632
Iteration 73, loss = 0.01944506
Iteration 74, loss = 0.01975233
Iteration 75, loss = 0.01933751
Iteration 76, loss = 0.01924462
Iteration 77, loss = 0.01945476
Iteration 78, loss = 0.01926457
Iteration 79, loss = 0.01928836
Iteration 80, loss = 0.01944731
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68228482
Iteration 2, loss = 0.61719404
Iteration 3, loss = 0.51379328
Iteration 4, loss = 0.40444133
Iteration 5, loss = 0.30639424
Iteration 6, loss = 0.22983742
Iteration 7, loss = 0.17510253
Iteration 8, loss = 0.13646782
Iteration 9, loss = 0.10932356
Iteration 10, loss = 0.09001764
Iteration 11, loss = 0.07608384
Iteration 12, loss = 0.06570847
Iteration 13, loss = 0.05771503
Iteration 14, loss = 0.05161978
Iteration 15, loss = 0.04657823
Iteration 16, loss = 0.04262200
Iteration 17, loss = 0.03964908
Iteration 18, loss = 0.03692416
Iteration 19, loss = 0.03488787
Iteration 20, loss = 0.03274170
Iteration 21, loss = 0.03112758
Iteration 22, loss = 0.02965942
Iteration 23, loss = 0.02862952
Iteration 24, loss = 0.02732225
Iteration 25, loss = 0.02642232
Iteration 26, loss = 0.02596107
Iteration 27, loss = 0.02496606
Iteration 28, loss = 0.02428941
Iteration 29, loss = 0.02391525
Iteration 30, loss = 0.02330130
Iteration 31, loss = 0.02299715
Iteration 32, loss = 0.02239958
Iteration 33, loss = 0.02225134
Iteration 34, loss = 0.02172579
Iteration 35, loss = 0.02132536
Iteration 36, loss = 0.02120722
Iteration 37, loss = 0.02098580
Iteration 38, loss = 0.02076430
Iteration 39, loss = 0.02045450
Iteration 40, loss = 0.02025628
Iteration 41, loss = 0.02003075
Iteration 42, loss = 0.02006558
Iteration 43, loss = 0.01990918
Iteration 44, loss = 0.01953020
Iteration 45, loss = 0.01939003
Iteration 46, loss = 0.01951182
Iteration 47, loss = 0.01937697
Iteration 48, loss = 0.01916246
Iteration 49, loss = 0.01896980
Iteration 50, loss = 0.01899803
Iteration 51, loss = 0.01914905
Iteration 52, loss = 0.01854884
Iteration 53, loss = 0.01862662
Iteration 54, loss = 0.01862309
Iteration 55, loss = 0.01869864
Iteration 56, loss = 0.01838054
Iteration 57, loss = 0.01832816
Iteration 58, loss = 0.01868135
Iteration 59, loss = 0.01812200
Iteration 60, loss = 0.01815804
Iteration 61, loss = 0.01812391
Iteration 62, loss = 0.01847334
Iteration 63, loss = 0.01829937
Iteration 64, loss = 0.01796630
Iteration 65, loss = 0.01813669
Iteration 66, loss = 0.01828350
Iteration 67, loss = 0.01808927
Iteration 68, loss = 0.01773866
Iteration 69, loss = 0.01792036
Iteration 70, loss = 0.01791691
Iteration 71, loss = 0.01787917
Iteration 72, loss = 0.01780971
Iteration 73, loss = 0.01769301
Iteration 74, loss = 0.01773248
Iteration 75, loss = 0.01757767
Iteration 76, loss = 0.01768841
Iteration 77, loss = 0.01765579
Iteration 78, loss = 0.01789657
Iteration 79, loss = 0.01769228
Iteration 80, loss = 0.01788001
Iteration 81, loss = 0.01762555
Iteration 82, loss = 0.01755325
Iteration 83, loss = 0.01775261
Iteration 84, loss = 0.01744335
Iteration 85, loss = 0.01733990
Iteration 86, loss = 0.01746867
Iteration 87, loss = 0.01733277
Iteration 88, loss = 0.01743861
Iteration 89, loss = 0.01725678
Iteration 90, loss = 0.01733598
Iteration 91, loss = 0.01723103
Iteration 92, loss = 0.01717170
Iteration 93, loss = 0.01715734
Iteration 94, loss = 0.01715153
Iteration 95, loss = 0.01727693
Iteration 96, loss = 0.01704090
Iteration 97, loss = 0.01751872
Iteration 98, loss = 0.01731022
Iteration 99, loss = 0.01728900
Iteration 100, loss = 0.01724999
Iteration 101, loss = 0.01704944
Iteration 102, loss = 0.01710756
Iteration 103, loss = 0.01719413
Iteration 104, loss = 0.01689039
Iteration 105, loss = 0.01699701
Iteration 106, loss = 0.01698425
Iteration 107, loss = 0.01700164
Iteration 108, loss = 0.01684530
Iteration 109, loss = 0.01696809
Iteration 110, loss = 0.01703744
Iteration 111, loss = 0.01692527
Iteration 112, loss = 0.01721345
Iteration 113, loss = 0.01700849
Iteration 114, loss = 0.01724570
Iteration 115, loss = 0.01686416
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68130491
Iteration 2, loss = 0.64468192
Iteration 3, loss = 0.59320101
Iteration 4, loss = 0.52540947
Iteration 5, loss = 0.45271734
Iteration 6, loss = 0.38204484
Iteration 7, loss = 0.31799973
Iteration 8, loss = 0.26328640
Iteration 9, loss = 0.21786143
Iteration 10, loss = 0.18155763
Iteration 11, loss = 0.15246906
Iteration 12, loss = 0.12949654
Iteration 13, loss = 0.11146525
Iteration 14, loss = 0.09704446
Iteration 15, loss = 0.08554796
Iteration 16, loss = 0.07627760
Iteration 17, loss = 0.06858389
Iteration 18, loss = 0.06231486
Iteration 19, loss = 0.05694247
Iteration 20, loss = 0.05264265
Iteration 21, loss = 0.04883512
Iteration 22, loss = 0.04578166
Iteration 23, loss = 0.04289667
Iteration 24, loss = 0.04037197
Iteration 25, loss = 0.03835721
Iteration 26, loss = 0.03644462
Iteration 27, loss = 0.03492788
Iteration 28, loss = 0.03348952
Iteration 29, loss = 0.03221213
Iteration 30, loss = 0.03103363
Iteration 31, loss = 0.02998701
Iteration 32, loss = 0.02919381
Iteration 33, loss = 0.02833041
Iteration 34, loss = 0.02755087
Iteration 35, loss = 0.02690143
Iteration 36, loss = 0.02631808
Iteration 37, loss = 0.02559821
Iteration 38, loss = 0.02515199
Iteration 39, loss = 0.02469301
Iteration 40, loss = 0.02413149
Iteration 41, loss = 0.02382653
Iteration 42, loss = 0.02360386
Iteration 43, loss = 0.02297675
Iteration 44, loss = 0.02271832
Iteration 45, loss = 0.02246793
Iteration 46, loss = 0.02206241
Iteration 47, loss = 0.02190454
Iteration 48, loss = 0.02167644
Iteration 49, loss = 0.02156488
Iteration 50, loss = 0.02124817
Iteration 51, loss = 0.02102929
Iteration 52, loss = 0.02064704
Iteration 53, loss = 0.02060701
Iteration 54, loss = 0.02057317
Iteration 55, loss = 0.02032682
Iteration 56, loss = 0.02009494
Iteration 57, loss = 0.02020360
Iteration 58, loss = 0.01988609
Iteration 59, loss = 0.01963197
Iteration 60, loss = 0.01962335
Iteration 61, loss = 0.01944201
Iteration 62, loss = 0.01935444
Iteration 63, loss = 0.01928361
Iteration 64, loss = 0.01917727
Iteration 65, loss = 0.01925393
Iteration 66, loss = 0.01913889
Iteration 67, loss = 0.01883878
Iteration 68, loss = 0.01890107
Iteration 69, loss = 0.01872058
Iteration 70, loss = 0.01858939
Iteration 71, loss = 0.01862442
Iteration 72, loss = 0.01848714
Iteration 73, loss = 0.01842544
Iteration 74, loss = 0.01837812
Iteration 75, loss = 0.01838996
Iteration 76, loss = 0.01823370
Iteration 77, loss = 0.01841228
Iteration 78, loss = 0.01813327
Iteration 79, loss = 0.01814985
Iteration 80, loss = 0.01807339
Iteration 81, loss = 0.01810630
Iteration 82, loss = 0.01800734
Iteration 83, loss = 0.01788566
Iteration 84, loss = 0.01802898
Iteration 85, loss = 0.01779217
Iteration 86, loss = 0.01783958
Iteration 87, loss = 0.01782614
Iteration 88, loss = 0.01766960
Iteration 89, loss = 0.01766420
Iteration 90, loss = 0.01779277
Iteration 91, loss = 0.01757230
Iteration 92, loss = 0.01766086
Iteration 93, loss = 0.01766152
Iteration 94, loss = 0.01748657
Iteration 95, loss = 0.01770137
Iteration 96, loss = 0.01778299
Iteration 97, loss = 0.01750749
Iteration 98, loss = 0.01759786
Iteration 99, loss = 0.01736378
Iteration 100, loss = 0.01737027
Iteration 101, loss = 0.01730556
Iteration 102, loss = 0.01741375
Iteration 103, loss = 0.01728068
Iteration 104, loss = 0.01740329
Iteration 105, loss = 0.01723075
Iteration 106, loss = 0.01740145
Iteration 107, loss = 0.01716433
Iteration 108, loss = 0.01723902
Iteration 109, loss = 0.01732415
Iteration 110, loss = 0.01706331
Iteration 111, loss = 0.01711145
Iteration 112, loss = 0.01725219
Iteration 113, loss = 0.01713002
Iteration 114, loss = 0.01714205
Iteration 115, loss = 0.01721987
Iteration 116, loss = 0.01708842
Iteration 117, loss = 0.01700916
Iteration 118, loss = 0.01695248
Iteration 119, loss = 0.01698644
Iteration 120, loss = 0.01709641
Iteration 121, loss = 0.01714983
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70142818
Iteration 2, loss = 0.64699417
Iteration 3, loss = 0.56093520
Iteration 4, loss = 0.45516407
Iteration 5, loss = 0.35756594
Iteration 6, loss = 0.27528462
Iteration 7, loss = 0.21183136
Iteration 8, loss = 0.16539054
Iteration 9, loss = 0.13240472
Iteration 10, loss = 0.10846367
Iteration 11, loss = 0.09231569
Iteration 12, loss = 0.08006355
Iteration 13, loss = 0.07054667
Iteration 14, loss = 0.06257339
Iteration 15, loss = 0.05641771
Iteration 16, loss = 0.05134129
Iteration 17, loss = 0.04721445
Iteration 18, loss = 0.04389201
Iteration 19, loss = 0.04118658
Iteration 20, loss = 0.03864659
Iteration 21, loss = 0.03694405
Iteration 22, loss = 0.03487549
Iteration 23, loss = 0.03374015
Iteration 24, loss = 0.03198078
Iteration 25, loss = 0.03078541
Iteration 26, loss = 0.02976241
Iteration 27, loss = 0.02877248
Iteration 28, loss = 0.02807762
Iteration 29, loss = 0.02748838
Iteration 30, loss = 0.02705711
Iteration 31, loss = 0.02664122
Iteration 32, loss = 0.02623375
Iteration 33, loss = 0.02530077
Iteration 34, loss = 0.02509437
Iteration 35, loss = 0.02470210
Iteration 36, loss = 0.02428525
Iteration 37, loss = 0.02380204
Iteration 38, loss = 0.02374498
Iteration 39, loss = 0.02321954
Iteration 40, loss = 0.02291294
Iteration 41, loss = 0.02284365
Iteration 42, loss = 0.02243949
Iteration 43, loss = 0.02227202
Iteration 44, loss = 0.02200942
Iteration 45, loss = 0.02202557
Iteration 46, loss = 0.02169569
Iteration 47, loss = 0.02151707
Iteration 48, loss = 0.02159703
Iteration 49, loss = 0.02127553
Iteration 50, loss = 0.02104937
Iteration 51, loss = 0.02094421
Iteration 52, loss = 0.02074454
Iteration 53, loss = 0.02097551
Iteration 54, loss = 0.02143780
Iteration 55, loss = 0.02122162
Iteration 56, loss = 0.02094679
Iteration 57, loss = 0.02090991
Iteration 58, loss = 0.02081204
Iteration 59, loss = 0.02058624
Iteration 60, loss = 0.02051082
Iteration 61, loss = 0.02028418
Iteration 62, loss = 0.02006305
Iteration 63, loss = 0.02009045
Iteration 64, loss = 0.02004220
Iteration 65, loss = 0.02011771
Iteration 66, loss = 0.02128453
Iteration 67, loss = 0.02037240
Iteration 68, loss = 0.02049489
Iteration 69, loss = 0.02006073
Iteration 70, loss = 0.01996509
Iteration 71, loss = 0.01974500
Iteration 72, loss = 0.01948079
Iteration 73, loss = 0.01956540
Iteration 74, loss = 0.01964162
Iteration 75, loss = 0.01958783
Iteration 76, loss = 0.01938683
Iteration 77, loss = 0.01920552
Iteration 78, loss = 0.02001271
Iteration 79, loss = 0.02012959
Iteration 80, loss = 0.01988706
Iteration 81, loss = 0.01972028
Iteration 82, loss = 0.01983965
Iteration 83, loss = 0.01942956
Iteration 84, loss = 0.01961728
Iteration 85, loss = 0.01918044
Iteration 86, loss = 0.01919612
Iteration 87, loss = 0.01918387
Iteration 88, loss = 0.01902250
Iteration 89, loss = 0.01918275
Iteration 90, loss = 0.01888198
Iteration 91, loss = 0.01889529
Iteration 92, loss = 0.01882024
Iteration 93, loss = 0.01882380
Iteration 94, loss = 0.01855901
Iteration 95, loss = 0.02019758
Iteration 96, loss = 0.02157424
Iteration 97, loss = 0.02078797
Iteration 98, loss = 0.02032526
Iteration 99, loss = 0.01984498
Iteration 100, loss = 0.01975888
Iteration 101, loss = 0.01961723
Iteration 102, loss = 0.02119218
Iteration 103, loss = 0.02089230
Iteration 104, loss = 0.02068191
Iteration 105, loss = 0.02030805
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70855908
Iteration 2, loss = 0.68650916
Iteration 3, loss = 0.66530805
Iteration 4, loss = 0.64080725
Iteration 5, loss = 0.61296746
Iteration 6, loss = 0.58143795
Iteration 7, loss = 0.54674253
Iteration 8, loss = 0.51034622
Iteration 9, loss = 0.47251557
Iteration 10, loss = 0.43463169
Iteration 11, loss = 0.39749348
Iteration 12, loss = 0.36201184
Iteration 13, loss = 0.32890338
Iteration 14, loss = 0.29801379
Iteration 15, loss = 0.26969337
Iteration 16, loss = 0.24394802
Iteration 17, loss = 0.22070116
Iteration 18, loss = 0.19987362
Iteration 19, loss = 0.18119769
Iteration 20, loss = 0.16478262
Iteration 21, loss = 0.15007865
Iteration 22, loss = 0.13706437
Iteration 23, loss = 0.12563933
Iteration 24, loss = 0.11543375
Iteration 25, loss = 0.10637340
Iteration 26, loss = 0.09841980
Iteration 27, loss = 0.09126882
Iteration 28, loss = 0.08488885
Iteration 29, loss = 0.07924489
Iteration 30, loss = 0.07413122
Iteration 31, loss = 0.06951503
Iteration 32, loss = 0.06541344
Iteration 33, loss = 0.06160547
Iteration 34, loss = 0.05822167
Iteration 35, loss = 0.05517734
Iteration 36, loss = 0.05236741
Iteration 37, loss = 0.04978084
Iteration 38, loss = 0.04740645
Iteration 39, loss = 0.04527172
Iteration 40, loss = 0.04327081
Iteration 41, loss = 0.04141357
Iteration 42, loss = 0.03975453
Iteration 43, loss = 0.03818675
Iteration 44, loss = 0.03674267
Iteration 45, loss = 0.03533660
Iteration 46, loss = 0.03416338
Iteration 47, loss = 0.03299985
Iteration 48, loss = 0.03186996
Iteration 49, loss = 0.03085633
Iteration 50, loss = 0.02989006
Iteration 51, loss = 0.02900114
Iteration 52, loss = 0.02814627
Iteration 53, loss = 0.02734860
Iteration 54, loss = 0.02658258
Iteration 55, loss = 0.02587824
Iteration 56, loss = 0.02519501
Iteration 57, loss = 0.02457595
Iteration 58, loss = 0.02398851
Iteration 59, loss = 0.02348033
Iteration 60, loss = 0.02291341
Iteration 61, loss = 0.02237549
Iteration 62, loss = 0.02188346
Iteration 63, loss = 0.02146860
Iteration 64, loss = 0.02101841
Iteration 65, loss = 0.02067394
Iteration 66, loss = 0.02021356
Iteration 67, loss = 0.01984983
Iteration 68, loss = 0.01951207
Iteration 69, loss = 0.01913836
Iteration 70, loss = 0.01882846
Iteration 71, loss = 0.01850597
Iteration 72, loss = 0.01821557
Iteration 73, loss = 0.01792287
Iteration 74, loss = 0.01764684
Iteration 75, loss = 0.01736872
Iteration 76, loss = 0.01715248
Iteration 77, loss = 0.01697126
Iteration 78, loss = 0.01670093
Iteration 79, loss = 0.01649387
Iteration 80, loss = 0.01621862
Iteration 81, loss = 0.01602690
Iteration 82, loss = 0.01585041
Iteration 83, loss = 0.01563898
Iteration 84, loss = 0.01550114
Iteration 85, loss = 0.01526561
Iteration 86, loss = 0.01509824
Iteration 87, loss = 0.01494142
Iteration 88, loss = 0.01478242
Iteration 89, loss = 0.01472463
Iteration 90, loss = 0.01449588
Iteration 91, loss = 0.01437080
Iteration 92, loss = 0.01424589
Iteration 93, loss = 0.01408240
Iteration 94, loss = 0.01396629
Iteration 95, loss = 0.01390246
Iteration 96, loss = 0.01369478
Iteration 97, loss = 0.01357580
Iteration 98, loss = 0.01348605
Iteration 99, loss = 0.01337781
Iteration 100, loss = 0.01325419
Iteration 101, loss = 0.01312754
Iteration 102, loss = 0.01308578
Iteration 103, loss = 0.01299605
Iteration 104, loss = 0.01292845
Iteration 105, loss = 0.01276584
Iteration 106, loss = 0.01265487
Iteration 107, loss = 0.01258781
Iteration 108, loss = 0.01252006
Iteration 109, loss = 0.01243498
Iteration 110, loss = 0.01235372
Iteration 111, loss = 0.01227893
Iteration 112, loss = 0.01220209
Iteration 113, loss = 0.01214389
Iteration 114, loss = 0.01207898
Iteration 115, loss = 0.01202186
Iteration 116, loss = 0.01194305
Iteration 117, loss = 0.01191585
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68169891
Iteration 2, loss = 0.62984323
Iteration 3, loss = 0.55551259
Iteration 4, loss = 0.46802501
Iteration 5, loss = 0.38377232
Iteration 6, loss = 0.30975052
Iteration 7, loss = 0.24830605
Iteration 8, loss = 0.20020531
Iteration 9, loss = 0.16329676
Iteration 10, loss = 0.13537816
Iteration 11, loss = 0.11369534
Iteration 12, loss = 0.09726925
Iteration 13, loss = 0.08459636
Iteration 14, loss = 0.07463459
Iteration 15, loss = 0.06659393
Iteration 16, loss = 0.06002089
Iteration 17, loss = 0.05487984
Iteration 18, loss = 0.05039130
Iteration 19, loss = 0.04662819
Iteration 20, loss = 0.04350194
Iteration 21, loss = 0.04114347
Iteration 22, loss = 0.03892421
Iteration 23, loss = 0.03675053
Iteration 24, loss = 0.03491254
Iteration 25, loss = 0.03351826
Iteration 26, loss = 0.03213477
Iteration 27, loss = 0.03109603
Iteration 28, loss = 0.03006953
Iteration 29, loss = 0.02916633
Iteration 30, loss = 0.02832604
Iteration 31, loss = 0.02754687
Iteration 32, loss = 0.02684383
Iteration 33, loss = 0.02618795
Iteration 34, loss = 0.02570744
Iteration 35, loss = 0.02527189
Iteration 36, loss = 0.02474077
Iteration 37, loss = 0.02418180
Iteration 38, loss = 0.02388238
Iteration 39, loss = 0.02374498
Iteration 40, loss = 0.02324721
Iteration 41, loss = 0.02310587
Iteration 42, loss = 0.02269774
Iteration 43, loss = 0.02234913
Iteration 44, loss = 0.02197361
Iteration 45, loss = 0.02186612
Iteration 46, loss = 0.02157644
Iteration 47, loss = 0.02128414
Iteration 48, loss = 0.02132338
Iteration 49, loss = 0.02118214
Iteration 50, loss = 0.02096496
Iteration 51, loss = 0.02072692
Iteration 52, loss = 0.02060761
Iteration 53, loss = 0.02056151
Iteration 54, loss = 0.02055787
Iteration 55, loss = 0.02031069
Iteration 56, loss = 0.01987710
Iteration 57, loss = 0.01991740
Iteration 58, loss = 0.01990562
Iteration 59, loss = 0.02002832
Iteration 60, loss = 0.01967418
Iteration 61, loss = 0.01990134
Iteration 62, loss = 0.01964296
Iteration 63, loss = 0.01941472
Iteration 64, loss = 0.01937207
Iteration 65, loss = 0.01940200
Iteration 66, loss = 0.01917416
Iteration 67, loss = 0.01928373
Iteration 68, loss = 0.01914601
Iteration 69, loss = 0.01924248
Iteration 70, loss = 0.01906591
Iteration 71, loss = 0.01907094
Iteration 72, loss = 0.01915324
Iteration 73, loss = 0.01880444
Iteration 74, loss = 0.01892856
Iteration 75, loss = 0.01887787
Iteration 76, loss = 0.01862587
Iteration 77, loss = 0.01851869
Iteration 78, loss = 0.01873960
Iteration 79, loss = 0.01862167
Iteration 80, loss = 0.01848773
Iteration 81, loss = 0.01860753
Iteration 82, loss = 0.01841961
Iteration 83, loss = 0.01854521
Iteration 84, loss = 0.01836029
Iteration 85, loss = 0.01855863
Iteration 86, loss = 0.01829174
Iteration 87, loss = 0.01824001
Iteration 88, loss = 0.01815428
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70034929
Iteration 2, loss = 0.63039173
Iteration 3, loss = 0.53029333
Iteration 4, loss = 0.41407325
Iteration 5, loss = 0.31173023
Iteration 6, loss = 0.23265404
Iteration 7, loss = 0.17641803
Iteration 8, loss = 0.13713565
Iteration 9, loss = 0.10971759
Iteration 10, loss = 0.09073228
Iteration 11, loss = 0.07652483
Iteration 12, loss = 0.06600609
Iteration 13, loss = 0.05806646
Iteration 14, loss = 0.05190390
Iteration 15, loss = 0.04708571
Iteration 16, loss = 0.04322675
Iteration 17, loss = 0.03983295
Iteration 18, loss = 0.03729247
Iteration 19, loss = 0.03507248
Iteration 20, loss = 0.03314854
Iteration 21, loss = 0.03150421
Iteration 22, loss = 0.03035477
Iteration 23, loss = 0.02893513
Iteration 24, loss = 0.02801234
Iteration 25, loss = 0.02705938
Iteration 26, loss = 0.02638848
Iteration 27, loss = 0.02578819
Iteration 28, loss = 0.02507369
Iteration 29, loss = 0.02471348
Iteration 30, loss = 0.02430080
Iteration 31, loss = 0.02362921
Iteration 32, loss = 0.02320057
Iteration 33, loss = 0.02293948
Iteration 34, loss = 0.02271892
Iteration 35, loss = 0.02243772
Iteration 36, loss = 0.02215460
Iteration 37, loss = 0.02159746
Iteration 38, loss = 0.02147135
Iteration 39, loss = 0.02141938
Iteration 40, loss = 0.02111763
Iteration 41, loss = 0.02082039
Iteration 42, loss = 0.02097303
Iteration 43, loss = 0.02064497
Iteration 44, loss = 0.02027612
Iteration 45, loss = 0.02035277
Iteration 46, loss = 0.02016468
Iteration 47, loss = 0.01997563
Iteration 48, loss = 0.02012173
Iteration 49, loss = 0.01980607
Iteration 50, loss = 0.01978099
Iteration 51, loss = 0.01947608
Iteration 52, loss = 0.01983065
Iteration 53, loss = 0.01948105
Iteration 54, loss = 0.01939154
Iteration 55, loss = 0.01967874
Iteration 56, loss = 0.01921054
Iteration 57, loss = 0.01937658
Iteration 58, loss = 0.01918719
Iteration 59, loss = 0.01939309
Iteration 60, loss = 0.01922183
Iteration 61, loss = 0.01891687
Iteration 62, loss = 0.01887036
Iteration 63, loss = 0.01885202
Iteration 64, loss = 0.01866378
Iteration 65, loss = 0.01895102
Iteration 66, loss = 0.01877736
Iteration 67, loss = 0.01888770
Iteration 68, loss = 0.01862074
Iteration 69, loss = 0.01861172
Iteration 70, loss = 0.01844151
Iteration 71, loss = 0.01858857
Iteration 72, loss = 0.01865697
Iteration 73, loss = 0.01893479
Iteration 74, loss = 0.01848193
Iteration 75, loss = 0.01856736
Iteration 76, loss = 0.01861728
Iteration 77, loss = 0.01858028
Iteration 78, loss = 0.01835795
Iteration 79, loss = 0.01838315
Iteration 80, loss = 0.01813981
Iteration 81, loss = 0.01843878
Iteration 82, loss = 0.01820742
Iteration 83, loss = 0.01817573
Iteration 84, loss = 0.01834262
Iteration 85, loss = 0.01826129
Iteration 86, loss = 0.01817040
Iteration 87, loss = 0.01811059
Iteration 88, loss = 0.01791980
Iteration 89, loss = 0.01821042
Iteration 90, loss = 0.01809920
Iteration 91, loss = 0.01822397
Iteration 92, loss = 0.01809119
Iteration 93, loss = 0.01831102
Iteration 94, loss = 0.01814469
Iteration 95, loss = 0.01864064
Iteration 96, loss = 0.01810879
Iteration 97, loss = 0.01818933
Iteration 98, loss = 0.01794953
Iteration 99, loss = 0.01792508
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67764898
Iteration 2, loss = 0.64678521
Iteration 3, loss = 0.59704127
Iteration 4, loss = 0.52545984
Iteration 5, loss = 0.44806186
Iteration 6, loss = 0.37400111
Iteration 7, loss = 0.30897771
Iteration 8, loss = 0.25349440
Iteration 9, loss = 0.20842408
Iteration 10, loss = 0.17262662
Iteration 11, loss = 0.14434615
Iteration 12, loss = 0.12225412
Iteration 13, loss = 0.10503561
Iteration 14, loss = 0.09106417
Iteration 15, loss = 0.08008416
Iteration 16, loss = 0.07110000
Iteration 17, loss = 0.06378414
Iteration 18, loss = 0.05779613
Iteration 19, loss = 0.05279562
Iteration 20, loss = 0.04858011
Iteration 21, loss = 0.04494129
Iteration 22, loss = 0.04180754
Iteration 23, loss = 0.03914060
Iteration 24, loss = 0.03689740
Iteration 25, loss = 0.03494749
Iteration 26, loss = 0.03319717
Iteration 27, loss = 0.03159599
Iteration 28, loss = 0.03030856
Iteration 29, loss = 0.02905352
Iteration 30, loss = 0.02786828
Iteration 31, loss = 0.02691213
Iteration 32, loss = 0.02603808
Iteration 33, loss = 0.02518623
Iteration 34, loss = 0.02454753
Iteration 35, loss = 0.02388039
Iteration 36, loss = 0.02327835
Iteration 37, loss = 0.02273568
Iteration 38, loss = 0.02216385
Iteration 39, loss = 0.02181643
Iteration 40, loss = 0.02133167
Iteration 41, loss = 0.02086202
Iteration 42, loss = 0.02060236
Iteration 43, loss = 0.02012212
Iteration 44, loss = 0.01988371
Iteration 45, loss = 0.01951998
Iteration 46, loss = 0.01913916
Iteration 47, loss = 0.01900239
Iteration 48, loss = 0.01883258
Iteration 49, loss = 0.01858010
Iteration 50, loss = 0.01825154
Iteration 51, loss = 0.01817710
Iteration 52, loss = 0.01792151
Iteration 53, loss = 0.01775318
Iteration 54, loss = 0.01774886
Iteration 55, loss = 0.01740830
Iteration 56, loss = 0.01759253
Iteration 57, loss = 0.01739964
Iteration 58, loss = 0.01717032
Iteration 59, loss = 0.01687752
Iteration 60, loss = 0.01685622
Iteration 61, loss = 0.01665958
Iteration 62, loss = 0.01653250
Iteration 63, loss = 0.01651200
Iteration 64, loss = 0.01650568
Iteration 65, loss = 0.01630370
Iteration 66, loss = 0.01611311
Iteration 67, loss = 0.01616349
Iteration 68, loss = 0.01598221
Iteration 69, loss = 0.01613833
Iteration 70, loss = 0.01580210
Iteration 71, loss = 0.01588446
Iteration 72, loss = 0.01578929
Iteration 73, loss = 0.01568030
Iteration 74, loss = 0.01554435
Iteration 75, loss = 0.01547242
Iteration 76, loss = 0.01538344
Iteration 77, loss = 0.01540302
Iteration 78, loss = 0.01531889
Iteration 79, loss = 0.01527536
Iteration 80, loss = 0.01530737
Iteration 81, loss = 0.01519562
Iteration 82, loss = 0.01503923
Iteration 83, loss = 0.01512116
Iteration 84, loss = 0.01506985
Iteration 85, loss = 0.01519333
Iteration 86, loss = 0.01498800
Iteration 87, loss = 0.01485282
Iteration 88, loss = 0.01499337
Iteration 89, loss = 0.01485836
Iteration 90, loss = 0.01467415
Iteration 91, loss = 0.01483166
Iteration 92, loss = 0.01486808
Iteration 93, loss = 0.01471348
Iteration 94, loss = 0.01470753
Iteration 95, loss = 0.01480505
Iteration 96, loss = 0.01455097
Iteration 97, loss = 0.01504370
Iteration 98, loss = 0.01458489
Iteration 99, loss = 0.01455141
Iteration 100, loss = 0.01457332
Iteration 101, loss = 0.01462093
Iteration 102, loss = 0.01441463
Iteration 103, loss = 0.01455162
Iteration 104, loss = 0.01452358
Iteration 105, loss = 0.01452793
Iteration 106, loss = 0.01428588
Iteration 107, loss = 0.01425082
Iteration 108, loss = 0.01437885
Iteration 109, loss = 0.01426209
Iteration 110, loss = 0.01436232
Iteration 111, loss = 0.01442344
Iteration 112, loss = 0.01425291
Iteration 113, loss = 0.01440414
Iteration 114, loss = 0.01416781
Iteration 115, loss = 0.01418914
Iteration 116, loss = 0.01417120
Iteration 117, loss = 0.01421938
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68509446
Iteration 2, loss = 0.60973174
Iteration 3, loss = 0.49588816
Iteration 4, loss = 0.37676764
Iteration 5, loss = 0.27823751
Iteration 6, loss = 0.20577835
Iteration 7, loss = 0.15587803
Iteration 8, loss = 0.12198220
Iteration 9, loss = 0.09863293
Iteration 10, loss = 0.08195148
Iteration 11, loss = 0.07013465
Iteration 12, loss = 0.06102862
Iteration 13, loss = 0.05403535
Iteration 14, loss = 0.04878665
Iteration 15, loss = 0.04460472
Iteration 16, loss = 0.04097782
Iteration 17, loss = 0.03849148
Iteration 18, loss = 0.03601787
Iteration 19, loss = 0.03413161
Iteration 20, loss = 0.03240265
Iteration 21, loss = 0.03123789
Iteration 22, loss = 0.02999337
Iteration 23, loss = 0.02864424
Iteration 24, loss = 0.02794603
Iteration 25, loss = 0.02740703
Iteration 26, loss = 0.02640801
Iteration 27, loss = 0.02609483
Iteration 28, loss = 0.02527647
Iteration 29, loss = 0.02496634
Iteration 30, loss = 0.02448568
Iteration 31, loss = 0.02391978
Iteration 32, loss = 0.02354930
Iteration 33, loss = 0.02325365
Iteration 34, loss = 0.02322560
Iteration 35, loss = 0.02266424
Iteration 36, loss = 0.02276472
Iteration 37, loss = 0.02248834
Iteration 38, loss = 0.02214303
Iteration 39, loss = 0.02221527
Iteration 40, loss = 0.02192175
Iteration 41, loss = 0.02159453
Iteration 42, loss = 0.02135419
Iteration 43, loss = 0.02143871
Iteration 44, loss = 0.02138002
Iteration 45, loss = 0.02111897
Iteration 46, loss = 0.02098547
Iteration 47, loss = 0.02085474
Iteration 48, loss = 0.02056865
Iteration 49, loss = 0.02064550
Iteration 50, loss = 0.02078069
Iteration 51, loss = 0.02058432
Iteration 52, loss = 0.02053383
Iteration 53, loss = 0.02053505
Iteration 54, loss = 0.02042954
Iteration 55, loss = 0.02037320
Iteration 56, loss = 0.02024216
Iteration 57, loss = 0.01995184
Iteration 58, loss = 0.02019969
Iteration 59, loss = 0.02006453
Iteration 60, loss = 0.02018175
Iteration 61, loss = 0.02002545
Iteration 62, loss = 0.01997760
Iteration 63, loss = 0.01980476
Iteration 64, loss = 0.02015872
Iteration 65, loss = 0.01976412
Iteration 66, loss = 0.01980625
Iteration 67, loss = 0.01976799
Iteration 68, loss = 0.01983560
Iteration 69, loss = 0.01965204
Iteration 70, loss = 0.01972932
Iteration 71, loss = 0.01961126
Iteration 72, loss = 0.01964081
Iteration 73, loss = 0.01954851
Iteration 74, loss = 0.01973524
Iteration 75, loss = 0.01944387
Iteration 76, loss = 0.01951256
Iteration 77, loss = 0.01961972
Iteration 78, loss = 0.01950097
Iteration 79, loss = 0.01949770
Iteration 80, loss = 0.01948695
Iteration 81, loss = 0.01944684
Iteration 82, loss = 0.01973789
Iteration 83, loss = 0.01933531
Iteration 84, loss = 0.01942255
Iteration 85, loss = 0.01911965
Iteration 86, loss = 0.01932969
Iteration 87, loss = 0.01922087
Iteration 88, loss = 0.01933452
Iteration 89, loss = 0.01935056
Iteration 90, loss = 0.01915204
Iteration 91, loss = 0.01926547
Iteration 92, loss = 0.01963268
Iteration 93, loss = 0.01922047
Iteration 94, loss = 0.01909672
Iteration 95, loss = 0.01902538
Iteration 96, loss = 0.01933369
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	91m9.108s
user	372m16.297s
sys	37m58.445s
Elapsed time: 5469 seconds
