Processing dataset: kept_v7_lowercase_words_only

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: kept_v7_lowercase_words_only (7443 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on kept_v7_lowercase_words_only...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on kept_v7_lowercase_words_only...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on kept_v7_lowercase_words_only...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on kept_v7_lowercase_words_only...
/opt/ohpc/pub/spack/v0.21.1/opt/spack/linux-rocky8-x86_64_v3/gcc-8.5.0/py-joblib-1.2.0-g6vh2usiiivwucfr57aoxwqvljmoxokc/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on kept_v7_lowercase_words_only...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67684882
Iteration 2, loss = 0.60378921
Iteration 3, loss = 0.49900124
Iteration 4, loss = 0.38721228
Iteration 5, loss = 0.29151154
Iteration 6, loss = 0.21928635
Iteration 7, loss = 0.16864806
Iteration 8, loss = 0.13372942
Iteration 9, loss = 0.10937838
Iteration 10, loss = 0.09256203
Iteration 11, loss = 0.08015860
Iteration 12, loss = 0.07056758
Iteration 13, loss = 0.06345882
Iteration 14, loss = 0.05770870
Iteration 15, loss = 0.05364084
Iteration 16, loss = 0.04981111
Iteration 17, loss = 0.04714310
Iteration 18, loss = 0.04457970
Iteration 19, loss = 0.04265716
Iteration 20, loss = 0.04080083
Iteration 21, loss = 0.03935526
Iteration 22, loss = 0.03786756
Iteration 23, loss = 0.03684457
Iteration 24, loss = 0.03604277
Iteration 25, loss = 0.03504364
Iteration 26, loss = 0.03444374
Iteration 27, loss = 0.03372101
Iteration 28, loss = 0.03313095
Iteration 29, loss = 0.03275678
Iteration 30, loss = 0.03221999
Iteration 31, loss = 0.03170372
Iteration 32, loss = 0.03140754
Iteration 33, loss = 0.03108587
Iteration 34, loss = 0.03046422
Iteration 35, loss = 0.03024429
Iteration 36, loss = 0.03033198
Iteration 37, loss = 0.02982470
Iteration 38, loss = 0.02962770
Iteration 39, loss = 0.02933395
Iteration 40, loss = 0.02931462
Iteration 41, loss = 0.02903571
Iteration 42, loss = 0.02876611
Iteration 43, loss = 0.02867494
Iteration 44, loss = 0.02865188
Iteration 45, loss = 0.02854113
Iteration 46, loss = 0.02863565
Iteration 47, loss = 0.02859737
Iteration 48, loss = 0.02804606
Iteration 49, loss = 0.02792149
Iteration 50, loss = 0.02828418
Iteration 51, loss = 0.02817367
Iteration 52, loss = 0.02749471
Iteration 53, loss = 0.02783317
Iteration 54, loss = 0.02757551
Iteration 55, loss = 0.02778556
Iteration 56, loss = 0.02709425
Iteration 57, loss = 0.02761796
Iteration 58, loss = 0.02736444
Iteration 59, loss = 0.02733262
Iteration 60, loss = 0.02728029
Iteration 61, loss = 0.02731956
Iteration 62, loss = 0.02731743
Iteration 63, loss = 0.02723793
Iteration 64, loss = 0.02708087
Iteration 65, loss = 0.02681677
Iteration 66, loss = 0.02701149
Iteration 67, loss = 0.02693851
Iteration 68, loss = 0.02677560
Iteration 69, loss = 0.02700076
Iteration 70, loss = 0.02663329
Iteration 71, loss = 0.02664561
Iteration 72, loss = 0.02687468
Iteration 73, loss = 0.02655934
Iteration 74, loss = 0.02658164
Iteration 75, loss = 0.02645208
Iteration 76, loss = 0.02706557
Iteration 77, loss = 0.02684477
Iteration 78, loss = 0.02670394
Iteration 79, loss = 0.02663579
Iteration 80, loss = 0.02690369
Iteration 81, loss = 0.02678258
Iteration 82, loss = 0.02677326
Iteration 83, loss = 0.02656204
Iteration 84, loss = 0.02650761
Iteration 85, loss = 0.02676857
Iteration 86, loss = 0.02656812
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70119582
Iteration 2, loss = 0.62410220
Iteration 3, loss = 0.51816012
Iteration 4, loss = 0.40184205
Iteration 5, loss = 0.30134970
Iteration 6, loss = 0.22518525
Iteration 7, loss = 0.17174459
Iteration 8, loss = 0.13471669
Iteration 9, loss = 0.10956575
Iteration 10, loss = 0.09177886
Iteration 11, loss = 0.07885055
Iteration 12, loss = 0.06906626
Iteration 13, loss = 0.06193342
Iteration 14, loss = 0.05607658
Iteration 15, loss = 0.05145454
Iteration 16, loss = 0.04759458
Iteration 17, loss = 0.04491621
Iteration 18, loss = 0.04221795
Iteration 19, loss = 0.04003340
Iteration 20, loss = 0.03843604
Iteration 21, loss = 0.03657286
Iteration 22, loss = 0.03533198
Iteration 23, loss = 0.03428144
Iteration 24, loss = 0.03337127
Iteration 25, loss = 0.03260878
Iteration 26, loss = 0.03165106
Iteration 27, loss = 0.03130695
Iteration 28, loss = 0.02998420
Iteration 29, loss = 0.02983048
Iteration 30, loss = 0.02906603
Iteration 31, loss = 0.02885972
Iteration 32, loss = 0.02820318
Iteration 33, loss = 0.02796675
Iteration 34, loss = 0.02796024
Iteration 35, loss = 0.02728083
Iteration 36, loss = 0.02714691
Iteration 37, loss = 0.02679946
Iteration 38, loss = 0.02646969
Iteration 39, loss = 0.02636548
Iteration 40, loss = 0.02621456
Iteration 41, loss = 0.02602272
Iteration 42, loss = 0.02585937
Iteration 43, loss = 0.02573470
Iteration 44, loss = 0.02583682
Iteration 45, loss = 0.02552410
Iteration 46, loss = 0.02526588
Iteration 47, loss = 0.02527309
Iteration 48, loss = 0.02522178
Iteration 49, loss = 0.02504939
Iteration 50, loss = 0.02484165
Iteration 51, loss = 0.02483691
Iteration 52, loss = 0.02476657
Iteration 53, loss = 0.02456481
Iteration 54, loss = 0.02504516
Iteration 55, loss = 0.02431812
Iteration 56, loss = 0.02436574
Iteration 57, loss = 0.02414500
Iteration 58, loss = 0.02441832
Iteration 59, loss = 0.02421299
Iteration 60, loss = 0.02436255
Iteration 61, loss = 0.02403597
Iteration 62, loss = 0.02419040
Iteration 63, loss = 0.02403754
Iteration 64, loss = 0.02403178
Iteration 65, loss = 0.02406818
Iteration 66, loss = 0.02381387
Iteration 67, loss = 0.02395179
Iteration 68, loss = 0.02407785
Iteration 69, loss = 0.02378868
Iteration 70, loss = 0.02356766
Iteration 71, loss = 0.02366346
Iteration 72, loss = 0.02375752
Iteration 73, loss = 0.02354171
Iteration 74, loss = 0.02376239
Iteration 75, loss = 0.02373081
Iteration 76, loss = 0.02341321
Iteration 77, loss = 0.02383902
Iteration 78, loss = 0.02368147
Iteration 79, loss = 0.02339749
Iteration 80, loss = 0.02351684
Iteration 81, loss = 0.02362378
Iteration 82, loss = 0.02372029
Iteration 83, loss = 0.02328482
Iteration 84, loss = 0.02344275
Iteration 85, loss = 0.02330548
Iteration 86, loss = 0.02310632
Iteration 87, loss = 0.02356782
Iteration 88, loss = 0.02359820
Iteration 89, loss = 0.02349149
Iteration 90, loss = 0.02346104
Iteration 91, loss = 0.02324200
Iteration 92, loss = 0.02310540
Iteration 93, loss = 0.02321883
Iteration 94, loss = 0.02325852
Iteration 95, loss = 0.02326587
Iteration 96, loss = 0.02325787
Iteration 97, loss = 0.02322841
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69689033
Iteration 2, loss = 0.62539923
Iteration 3, loss = 0.51880363
Iteration 4, loss = 0.40172948
Iteration 5, loss = 0.30117541
Iteration 6, loss = 0.22461571
Iteration 7, loss = 0.17096658
Iteration 8, loss = 0.13408466
Iteration 9, loss = 0.10864531
Iteration 10, loss = 0.09053586
Iteration 11, loss = 0.07755942
Iteration 12, loss = 0.06774442
Iteration 13, loss = 0.06025529
Iteration 14, loss = 0.05469161
Iteration 15, loss = 0.04987410
Iteration 16, loss = 0.04615173
Iteration 17, loss = 0.04301970
Iteration 18, loss = 0.04037362
Iteration 19, loss = 0.03829174
Iteration 20, loss = 0.03634809
Iteration 21, loss = 0.03484867
Iteration 22, loss = 0.03330748
Iteration 23, loss = 0.03237610
Iteration 24, loss = 0.03128217
Iteration 25, loss = 0.03034941
Iteration 26, loss = 0.02966024
Iteration 27, loss = 0.02905845
Iteration 28, loss = 0.02829753
Iteration 29, loss = 0.02766168
Iteration 30, loss = 0.02695157
Iteration 31, loss = 0.02680058
Iteration 32, loss = 0.02643725
Iteration 33, loss = 0.02587737
Iteration 34, loss = 0.02567107
Iteration 35, loss = 0.02540189
Iteration 36, loss = 0.02520898
Iteration 37, loss = 0.02470833
Iteration 38, loss = 0.02478087
Iteration 39, loss = 0.02447119
Iteration 40, loss = 0.02406699
Iteration 41, loss = 0.02361671
Iteration 42, loss = 0.02392453
Iteration 43, loss = 0.02341867
Iteration 44, loss = 0.02358467
Iteration 45, loss = 0.02323811
Iteration 46, loss = 0.02295628
Iteration 47, loss = 0.02324748
Iteration 48, loss = 0.02332549
Iteration 49, loss = 0.02282925
Iteration 50, loss = 0.02312321
Iteration 51, loss = 0.02287660
Iteration 52, loss = 0.02252462
Iteration 53, loss = 0.02250394
Iteration 54, loss = 0.02267626
Iteration 55, loss = 0.02230659
Iteration 56, loss = 0.02244849
Iteration 57, loss = 0.02202308
Iteration 58, loss = 0.02209476
Iteration 59, loss = 0.02217651
Iteration 60, loss = 0.02223558
Iteration 61, loss = 0.02187968
Iteration 62, loss = 0.02203895
Iteration 63, loss = 0.02185569
Iteration 64, loss = 0.02198381
Iteration 65, loss = 0.02191913
Iteration 66, loss = 0.02206207
Iteration 67, loss = 0.02187136
Iteration 68, loss = 0.02176652
Iteration 69, loss = 0.02159035
Iteration 70, loss = 0.02166934
Iteration 71, loss = 0.02164781
Iteration 72, loss = 0.02154386
Iteration 73, loss = 0.02165897
Iteration 74, loss = 0.02171142
Iteration 75, loss = 0.02123855
Iteration 76, loss = 0.02142599
Iteration 77, loss = 0.02143094
Iteration 78, loss = 0.02130901
Iteration 79, loss = 0.02119684
Iteration 80, loss = 0.02149969
Iteration 81, loss = 0.02094328
Iteration 82, loss = 0.02138093
Iteration 83, loss = 0.02123339
Iteration 84, loss = 0.02133102
Iteration 85, loss = 0.02099844
Iteration 86, loss = 0.02142811
Iteration 87, loss = 0.02114829
Iteration 88, loss = 0.02107159
Iteration 89, loss = 0.02112392
Iteration 90, loss = 0.02142502
Iteration 91, loss = 0.02125838
Iteration 92, loss = 0.02111842
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69249511
Iteration 2, loss = 0.61928946
Iteration 3, loss = 0.51367578
Iteration 4, loss = 0.39546432
Iteration 5, loss = 0.29612531
Iteration 6, loss = 0.22195550
Iteration 7, loss = 0.17019659
Iteration 8, loss = 0.13502581
Iteration 9, loss = 0.11049234
Iteration 10, loss = 0.09324065
Iteration 11, loss = 0.08059627
Iteration 12, loss = 0.07158199
Iteration 13, loss = 0.06422444
Iteration 14, loss = 0.05851361
Iteration 15, loss = 0.05426330
Iteration 16, loss = 0.05053118
Iteration 17, loss = 0.04763620
Iteration 18, loss = 0.04516781
Iteration 19, loss = 0.04302059
Iteration 20, loss = 0.04115790
Iteration 21, loss = 0.03977110
Iteration 22, loss = 0.03855569
Iteration 23, loss = 0.03763367
Iteration 24, loss = 0.03622871
Iteration 25, loss = 0.03547588
Iteration 26, loss = 0.03479803
Iteration 27, loss = 0.03399944
Iteration 28, loss = 0.03343654
Iteration 29, loss = 0.03262086
Iteration 30, loss = 0.03234747
Iteration 31, loss = 0.03182965
Iteration 32, loss = 0.03160513
Iteration 33, loss = 0.03131637
Iteration 34, loss = 0.03070102
Iteration 35, loss = 0.03045296
Iteration 36, loss = 0.03013083
Iteration 37, loss = 0.02992771
Iteration 38, loss = 0.02975524
Iteration 39, loss = 0.02963963
Iteration 40, loss = 0.02967330
Iteration 41, loss = 0.02904414
Iteration 42, loss = 0.02939166
Iteration 43, loss = 0.02916417
Iteration 44, loss = 0.02852572
Iteration 45, loss = 0.02869485
Iteration 46, loss = 0.02839921
Iteration 47, loss = 0.02832336
Iteration 48, loss = 0.02828751
Iteration 49, loss = 0.02816957
Iteration 50, loss = 0.02812840
Iteration 51, loss = 0.02808937
Iteration 52, loss = 0.02791774
Iteration 53, loss = 0.02767040
Iteration 54, loss = 0.02753456
Iteration 55, loss = 0.02769564
Iteration 56, loss = 0.02788414
Iteration 57, loss = 0.02769003
Iteration 58, loss = 0.02793582
Iteration 59, loss = 0.02714002
Iteration 60, loss = 0.02742346
Iteration 61, loss = 0.02731731
Iteration 62, loss = 0.02728900
Iteration 63, loss = 0.02704156
Iteration 64, loss = 0.02729487
Iteration 65, loss = 0.02707699
Iteration 66, loss = 0.02712848
Iteration 67, loss = 0.02705512
Iteration 68, loss = 0.02699215
Iteration 69, loss = 0.02674585
Iteration 70, loss = 0.02713490
Iteration 71, loss = 0.02686294
Iteration 72, loss = 0.02664781
Iteration 73, loss = 0.02671120
Iteration 74, loss = 0.02679264
Iteration 75, loss = 0.02700637
Iteration 76, loss = 0.02686518
Iteration 77, loss = 0.02682355
Iteration 78, loss = 0.02658816
Iteration 79, loss = 0.02663477
Iteration 80, loss = 0.02676720
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70794459
Iteration 2, loss = 0.62072796
Iteration 3, loss = 0.50677750
Iteration 4, loss = 0.38685988
Iteration 5, loss = 0.28867716
Iteration 6, loss = 0.21631223
Iteration 7, loss = 0.16572898
Iteration 8, loss = 0.13157811
Iteration 9, loss = 0.10738028
Iteration 10, loss = 0.09030527
Iteration 11, loss = 0.07804686
Iteration 12, loss = 0.06886377
Iteration 13, loss = 0.06157010
Iteration 14, loss = 0.05603731
Iteration 15, loss = 0.05145379
Iteration 16, loss = 0.04809335
Iteration 17, loss = 0.04510154
Iteration 18, loss = 0.04278874
Iteration 19, loss = 0.04065365
Iteration 20, loss = 0.03906281
Iteration 21, loss = 0.03736735
Iteration 22, loss = 0.03608724
Iteration 23, loss = 0.03512361
Iteration 24, loss = 0.03410162
Iteration 25, loss = 0.03315698
Iteration 26, loss = 0.03262788
Iteration 27, loss = 0.03187203
Iteration 28, loss = 0.03092343
Iteration 29, loss = 0.03069658
Iteration 30, loss = 0.02995122
Iteration 31, loss = 0.02966552
Iteration 32, loss = 0.02970890
Iteration 33, loss = 0.02931820
Iteration 34, loss = 0.02863229
Iteration 35, loss = 0.02856420
Iteration 36, loss = 0.02795177
Iteration 37, loss = 0.02798935
Iteration 38, loss = 0.02757816
Iteration 39, loss = 0.02754411
Iteration 40, loss = 0.02724666
Iteration 41, loss = 0.02730036
Iteration 42, loss = 0.02695744
Iteration 43, loss = 0.02665689
Iteration 44, loss = 0.02689546
Iteration 45, loss = 0.02666497
Iteration 46, loss = 0.02644789
Iteration 47, loss = 0.02654100
Iteration 48, loss = 0.02611991
Iteration 49, loss = 0.02599460
Iteration 50, loss = 0.02582146
Iteration 51, loss = 0.02586458
Iteration 52, loss = 0.02597935
Iteration 53, loss = 0.02582596
Iteration 54, loss = 0.02542275
Iteration 55, loss = 0.02555447
Iteration 56, loss = 0.02532779
Iteration 57, loss = 0.02548823
Iteration 58, loss = 0.02545137
Iteration 59, loss = 0.02563227
Iteration 60, loss = 0.02577031
Iteration 61, loss = 0.02528673
Iteration 62, loss = 0.02543479
Iteration 63, loss = 0.02510692
Iteration 64, loss = 0.02515482
Iteration 65, loss = 0.02498761
Iteration 66, loss = 0.02556583
Iteration 67, loss = 0.02470568
Iteration 68, loss = 0.02490023
Iteration 69, loss = 0.02478654
Iteration 70, loss = 0.02534187
Iteration 71, loss = 0.02481027
Iteration 72, loss = 0.02501586
Iteration 73, loss = 0.02502077
Iteration 74, loss = 0.02493940
Iteration 75, loss = 0.02469737
Iteration 76, loss = 0.02472411
Iteration 77, loss = 0.02459462
Iteration 78, loss = 0.02469794
Iteration 79, loss = 0.02452762
Iteration 80, loss = 0.02440704
Iteration 81, loss = 0.02442267
Iteration 82, loss = 0.02450098
Iteration 83, loss = 0.02464526
Iteration 84, loss = 0.02459605
Iteration 85, loss = 0.02435568
Iteration 86, loss = 0.02458510
Iteration 87, loss = 0.02451471
Iteration 88, loss = 0.02439453
Iteration 89, loss = 0.02423843
Iteration 90, loss = 0.02421981
Iteration 91, loss = 0.02459110
Iteration 92, loss = 0.02451551
Iteration 93, loss = 0.02436661
Iteration 94, loss = 0.02443183
Iteration 95, loss = 0.02463870
Iteration 96, loss = 0.02408589
Iteration 97, loss = 0.02472817
Iteration 98, loss = 0.02418232
Iteration 99, loss = 0.02431705
Iteration 100, loss = 0.02433125
Iteration 101, loss = 0.02407712
Iteration 102, loss = 0.02426595
Iteration 103, loss = 0.02414853
Iteration 104, loss = 0.02432333
Iteration 105, loss = 0.02439758
Iteration 106, loss = 0.02432250
Iteration 107, loss = 0.02425443
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67684882
Iteration 2, loss = 0.60378921
Iteration 3, loss = 0.49900124
Iteration 4, loss = 0.38721228
Iteration 5, loss = 0.29151154
Iteration 6, loss = 0.21928635
Iteration 7, loss = 0.16864806
Iteration 8, loss = 0.13372942
Iteration 9, loss = 0.10937838
Iteration 10, loss = 0.09256203
Iteration 11, loss = 0.08015860
Iteration 12, loss = 0.07056758
Iteration 13, loss = 0.06345882
Iteration 14, loss = 0.05770870
Iteration 15, loss = 0.05364084
Iteration 16, loss = 0.04981111
Iteration 17, loss = 0.04714310
Iteration 18, loss = 0.04457970
Iteration 19, loss = 0.04265716
Iteration 20, loss = 0.04080083
Iteration 21, loss = 0.03935526
Iteration 22, loss = 0.03786756
Iteration 23, loss = 0.03684457
Iteration 24, loss = 0.03604277
Iteration 25, loss = 0.03504364
Iteration 26, loss = 0.03444374
Iteration 27, loss = 0.03372101
Iteration 28, loss = 0.03313095
Iteration 29, loss = 0.03275678
Iteration 30, loss = 0.03221999
Iteration 31, loss = 0.03170372
Iteration 32, loss = 0.03140754
Iteration 33, loss = 0.03108587
Iteration 34, loss = 0.03046422
Iteration 35, loss = 0.03024429
Iteration 36, loss = 0.03033198
Iteration 37, loss = 0.02982470
Iteration 38, loss = 0.02962770
Iteration 39, loss = 0.02933395
Iteration 40, loss = 0.02931462
Iteration 41, loss = 0.02903571
Iteration 42, loss = 0.02876611
Iteration 43, loss = 0.02867494
Iteration 44, loss = 0.02865188
Iteration 45, loss = 0.02854113
Iteration 46, loss = 0.02863565
Iteration 47, loss = 0.02859737
Iteration 48, loss = 0.02804606
Iteration 49, loss = 0.02792149
Iteration 50, loss = 0.02828418
Iteration 51, loss = 0.02817367
Iteration 52, loss = 0.02749471
Iteration 53, loss = 0.02783317
Iteration 54, loss = 0.02757551
Iteration 55, loss = 0.02778556
Iteration 56, loss = 0.02709425
Iteration 57, loss = 0.02761796
Iteration 58, loss = 0.02736444
Iteration 59, loss = 0.02733262
Iteration 60, loss = 0.02728029
Iteration 61, loss = 0.02731956
Iteration 62, loss = 0.02731743
Iteration 63, loss = 0.02723793
Iteration 64, loss = 0.02708087
Iteration 65, loss = 0.02681677
Iteration 66, loss = 0.02701149
Iteration 67, loss = 0.02693851
Iteration 68, loss = 0.02677560
Iteration 69, loss = 0.02700076
Iteration 70, loss = 0.02663329
Iteration 71, loss = 0.02664561
Iteration 72, loss = 0.02687468
Iteration 73, loss = 0.02655934
Iteration 74, loss = 0.02658164
Iteration 75, loss = 0.02645208
Iteration 76, loss = 0.02706557
Iteration 77, loss = 0.02684477
Iteration 78, loss = 0.02670394
Iteration 79, loss = 0.02663579
Iteration 80, loss = 0.02690369
Iteration 81, loss = 0.02678258
Iteration 82, loss = 0.02677326
Iteration 83, loss = 0.02656204
Iteration 84, loss = 0.02650761
Iteration 85, loss = 0.02676857
Iteration 86, loss = 0.02656812
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70119582
Iteration 2, loss = 0.62410220
Iteration 3, loss = 0.51816012
Iteration 4, loss = 0.40184205
Iteration 5, loss = 0.30134970
Iteration 6, loss = 0.22518525
Iteration 7, loss = 0.17174459
Iteration 8, loss = 0.13471669
Iteration 9, loss = 0.10956575
Iteration 10, loss = 0.09177886
Iteration 11, loss = 0.07885055
Iteration 12, loss = 0.06906626
Iteration 13, loss = 0.06193342
Iteration 14, loss = 0.05607658
Iteration 15, loss = 0.05145454
Iteration 16, loss = 0.04759458
Iteration 17, loss = 0.04491621
Iteration 18, loss = 0.04221795
Iteration 19, loss = 0.04003340
Iteration 20, loss = 0.03843604
Iteration 21, loss = 0.03657286
Iteration 22, loss = 0.03533198
Iteration 23, loss = 0.03428144
Iteration 24, loss = 0.03337127
Iteration 25, loss = 0.03260878
Iteration 26, loss = 0.03165106
Iteration 27, loss = 0.03130695
Iteration 28, loss = 0.02998420
Iteration 29, loss = 0.02983048
Iteration 30, loss = 0.02906603
Iteration 31, loss = 0.02885972
Iteration 32, loss = 0.02820318
Iteration 33, loss = 0.02796675
Iteration 34, loss = 0.02796024
Iteration 35, loss = 0.02728083
Iteration 36, loss = 0.02714691
Iteration 37, loss = 0.02679946
Iteration 38, loss = 0.02646969
Iteration 39, loss = 0.02636548
Iteration 40, loss = 0.02621456
Iteration 41, loss = 0.02602272
Iteration 42, loss = 0.02585937
Iteration 43, loss = 0.02573470
Iteration 44, loss = 0.02583682
Iteration 45, loss = 0.02552410
Iteration 46, loss = 0.02526588
Iteration 47, loss = 0.02527309
Iteration 48, loss = 0.02522178
Iteration 49, loss = 0.02504939
Iteration 50, loss = 0.02484165
Iteration 51, loss = 0.02483691
Iteration 52, loss = 0.02476657
Iteration 53, loss = 0.02456481
Iteration 54, loss = 0.02504516
Iteration 55, loss = 0.02431812
Iteration 56, loss = 0.02436574
Iteration 57, loss = 0.02414500
Iteration 58, loss = 0.02441832
Iteration 59, loss = 0.02421299
Iteration 60, loss = 0.02436255
Iteration 61, loss = 0.02403597
Iteration 62, loss = 0.02419040
Iteration 63, loss = 0.02403754
Iteration 64, loss = 0.02403178
Iteration 65, loss = 0.02406818
Iteration 66, loss = 0.02381387
Iteration 67, loss = 0.02395179
Iteration 68, loss = 0.02407785
Iteration 69, loss = 0.02378868
Iteration 70, loss = 0.02356766
Iteration 71, loss = 0.02366346
Iteration 72, loss = 0.02375752
Iteration 73, loss = 0.02354171
Iteration 74, loss = 0.02376239
Iteration 75, loss = 0.02373081
Iteration 76, loss = 0.02341321
Iteration 77, loss = 0.02383902
Iteration 78, loss = 0.02368147
Iteration 79, loss = 0.02339749
Iteration 80, loss = 0.02351684
Iteration 81, loss = 0.02362378
Iteration 82, loss = 0.02372029
Iteration 83, loss = 0.02328482
Iteration 84, loss = 0.02344275
Iteration 85, loss = 0.02330548
Iteration 86, loss = 0.02310632
Iteration 87, loss = 0.02356782
Iteration 88, loss = 0.02359820
Iteration 89, loss = 0.02349149
Iteration 90, loss = 0.02346104
Iteration 91, loss = 0.02324200
Iteration 92, loss = 0.02310540
Iteration 93, loss = 0.02321883
Iteration 94, loss = 0.02325852
Iteration 95, loss = 0.02326587
Iteration 96, loss = 0.02325787
Iteration 97, loss = 0.02322841
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69689033
Iteration 2, loss = 0.62539923
Iteration 3, loss = 0.51880363
Iteration 4, loss = 0.40172948
Iteration 5, loss = 0.30117541
Iteration 6, loss = 0.22461571
Iteration 7, loss = 0.17096658
Iteration 8, loss = 0.13408466
Iteration 9, loss = 0.10864531
Iteration 10, loss = 0.09053586
Iteration 11, loss = 0.07755942
Iteration 12, loss = 0.06774442
Iteration 13, loss = 0.06025529
Iteration 14, loss = 0.05469161
Iteration 15, loss = 0.04987410
Iteration 16, loss = 0.04615173
Iteration 17, loss = 0.04301970
Iteration 18, loss = 0.04037362
Iteration 19, loss = 0.03829174
Iteration 20, loss = 0.03634809
Iteration 21, loss = 0.03484867
Iteration 22, loss = 0.03330748
Iteration 23, loss = 0.03237610
Iteration 24, loss = 0.03128217
Iteration 25, loss = 0.03034941
Iteration 26, loss = 0.02966024
Iteration 27, loss = 0.02905845
Iteration 28, loss = 0.02829753
Iteration 29, loss = 0.02766168
Iteration 30, loss = 0.02695157
Iteration 31, loss = 0.02680058
Iteration 32, loss = 0.02643725
Iteration 33, loss = 0.02587737
Iteration 34, loss = 0.02567107
Iteration 35, loss = 0.02540189
Iteration 36, loss = 0.02520898
Iteration 37, loss = 0.02470833
Iteration 38, loss = 0.02478087
Iteration 39, loss = 0.02447119
Iteration 40, loss = 0.02406699
Iteration 41, loss = 0.02361671
Iteration 42, loss = 0.02392453
Iteration 43, loss = 0.02341867
Iteration 44, loss = 0.02358467
Iteration 45, loss = 0.02323811
Iteration 46, loss = 0.02295628
Iteration 47, loss = 0.02324748
Iteration 48, loss = 0.02332549
Iteration 49, loss = 0.02282925
Iteration 50, loss = 0.02312321
Iteration 51, loss = 0.02287660
Iteration 52, loss = 0.02252462
Iteration 53, loss = 0.02250394
Iteration 54, loss = 0.02267626
Iteration 55, loss = 0.02230659
Iteration 56, loss = 0.02244849
Iteration 57, loss = 0.02202308
Iteration 58, loss = 0.02209476
Iteration 59, loss = 0.02217651
Iteration 60, loss = 0.02223558
Iteration 61, loss = 0.02187968
Iteration 62, loss = 0.02203895
Iteration 63, loss = 0.02185569
Iteration 64, loss = 0.02198381
Iteration 65, loss = 0.02191913
Iteration 66, loss = 0.02206207
Iteration 67, loss = 0.02187136
Iteration 68, loss = 0.02176652
Iteration 69, loss = 0.02159035
Iteration 70, loss = 0.02166934
Iteration 71, loss = 0.02164781
Iteration 72, loss = 0.02154386
Iteration 73, loss = 0.02165897
Iteration 74, loss = 0.02171142
Iteration 75, loss = 0.02123855
Iteration 76, loss = 0.02142599
Iteration 77, loss = 0.02143094
Iteration 78, loss = 0.02130901
Iteration 79, loss = 0.02119684
Iteration 80, loss = 0.02149969
Iteration 81, loss = 0.02094328
Iteration 82, loss = 0.02138093
Iteration 83, loss = 0.02123339
Iteration 84, loss = 0.02133102
Iteration 85, loss = 0.02099844
Iteration 86, loss = 0.02142811
Iteration 87, loss = 0.02114829
Iteration 88, loss = 0.02107159
Iteration 89, loss = 0.02112392
Iteration 90, loss = 0.02142502
Iteration 91, loss = 0.02125838
Iteration 92, loss = 0.02111842
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69249511
Iteration 2, loss = 0.61928946
Iteration 3, loss = 0.51367578
Iteration 4, loss = 0.39546432
Iteration 5, loss = 0.29612531
Iteration 6, loss = 0.22195550
Iteration 7, loss = 0.17019659
Iteration 8, loss = 0.13502581
Iteration 9, loss = 0.11049234
Iteration 10, loss = 0.09324065
Iteration 11, loss = 0.08059627
Iteration 12, loss = 0.07158199
Iteration 13, loss = 0.06422444
Iteration 14, loss = 0.05851361
Iteration 15, loss = 0.05426330
Iteration 16, loss = 0.05053118
Iteration 17, loss = 0.04763620
Iteration 18, loss = 0.04516781
Iteration 19, loss = 0.04302059
Iteration 20, loss = 0.04115790
Iteration 21, loss = 0.03977110
Iteration 22, loss = 0.03855569
Iteration 23, loss = 0.03763367
Iteration 24, loss = 0.03622871
Iteration 25, loss = 0.03547588
Iteration 26, loss = 0.03479803
Iteration 27, loss = 0.03399944
Iteration 28, loss = 0.03343654
Iteration 29, loss = 0.03262086
Iteration 30, loss = 0.03234747
Iteration 31, loss = 0.03182965
Iteration 32, loss = 0.03160513
Iteration 33, loss = 0.03131637
Iteration 34, loss = 0.03070102
Iteration 35, loss = 0.03045296
Iteration 36, loss = 0.03013083
Iteration 37, loss = 0.02992771
Iteration 38, loss = 0.02975524
Iteration 39, loss = 0.02963963
Iteration 40, loss = 0.02967330
Iteration 41, loss = 0.02904414
Iteration 42, loss = 0.02939166
Iteration 43, loss = 0.02916417
Iteration 44, loss = 0.02852572
Iteration 45, loss = 0.02869485
Iteration 46, loss = 0.02839921
Iteration 47, loss = 0.02832336
Iteration 48, loss = 0.02828751
Iteration 49, loss = 0.02816957
Iteration 50, loss = 0.02812840
Iteration 51, loss = 0.02808937
Iteration 52, loss = 0.02791774
Iteration 53, loss = 0.02767040
Iteration 54, loss = 0.02753456
Iteration 55, loss = 0.02769564
Iteration 56, loss = 0.02788414
Iteration 57, loss = 0.02769003
Iteration 58, loss = 0.02793582
Iteration 59, loss = 0.02714002
Iteration 60, loss = 0.02742346
Iteration 61, loss = 0.02731731
Iteration 62, loss = 0.02728900
Iteration 63, loss = 0.02704156
Iteration 64, loss = 0.02729487
Iteration 65, loss = 0.02707699
Iteration 66, loss = 0.02712848
Iteration 67, loss = 0.02705512
Iteration 68, loss = 0.02699215
Iteration 69, loss = 0.02674585
Iteration 70, loss = 0.02713490
Iteration 71, loss = 0.02686294
Iteration 72, loss = 0.02664781
Iteration 73, loss = 0.02671120
Iteration 74, loss = 0.02679264
Iteration 75, loss = 0.02700637
Iteration 76, loss = 0.02686518
Iteration 77, loss = 0.02682355
Iteration 78, loss = 0.02658816
Iteration 79, loss = 0.02663477
Iteration 80, loss = 0.02676720
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70794459
Iteration 2, loss = 0.62072796
Iteration 3, loss = 0.50677750
Iteration 4, loss = 0.38685988
Iteration 5, loss = 0.28867716
Iteration 6, loss = 0.21631223
Iteration 7, loss = 0.16572898
Iteration 8, loss = 0.13157811
Iteration 9, loss = 0.10738028
Iteration 10, loss = 0.09030527
Iteration 11, loss = 0.07804686
Iteration 12, loss = 0.06886377
Iteration 13, loss = 0.06157010
Iteration 14, loss = 0.05603731
Iteration 15, loss = 0.05145379
Iteration 16, loss = 0.04809335
Iteration 17, loss = 0.04510154
Iteration 18, loss = 0.04278874
Iteration 19, loss = 0.04065365
Iteration 20, loss = 0.03906281
Iteration 21, loss = 0.03736735
Iteration 22, loss = 0.03608724
Iteration 23, loss = 0.03512361
Iteration 24, loss = 0.03410162
Iteration 25, loss = 0.03315698
Iteration 26, loss = 0.03262788
Iteration 27, loss = 0.03187203
Iteration 28, loss = 0.03092343
Iteration 29, loss = 0.03069658
Iteration 30, loss = 0.02995122
Iteration 31, loss = 0.02966552
Iteration 32, loss = 0.02970890
Iteration 33, loss = 0.02931820
Iteration 34, loss = 0.02863229
Iteration 35, loss = 0.02856420
Iteration 36, loss = 0.02795177
Iteration 37, loss = 0.02798935
Iteration 38, loss = 0.02757816
Iteration 39, loss = 0.02754411
Iteration 40, loss = 0.02724666
Iteration 41, loss = 0.02730036
Iteration 42, loss = 0.02695744
Iteration 43, loss = 0.02665689
Iteration 44, loss = 0.02689546
Iteration 45, loss = 0.02666497
Iteration 46, loss = 0.02644789
Iteration 47, loss = 0.02654100
Iteration 48, loss = 0.02611991
Iteration 49, loss = 0.02599460
Iteration 50, loss = 0.02582146
Iteration 51, loss = 0.02586458
Iteration 52, loss = 0.02597935
Iteration 53, loss = 0.02582596
Iteration 54, loss = 0.02542275
Iteration 55, loss = 0.02555447
Iteration 56, loss = 0.02532779
Iteration 57, loss = 0.02548823
Iteration 58, loss = 0.02545137
Iteration 59, loss = 0.02563227
Iteration 60, loss = 0.02577031
Iteration 61, loss = 0.02528673
Iteration 62, loss = 0.02543479
Iteration 63, loss = 0.02510692
Iteration 64, loss = 0.02515482
Iteration 65, loss = 0.02498761
Iteration 66, loss = 0.02556583
Iteration 67, loss = 0.02470568
Iteration 68, loss = 0.02490023
Iteration 69, loss = 0.02478654
Iteration 70, loss = 0.02534187
Iteration 71, loss = 0.02481027
Iteration 72, loss = 0.02501586
Iteration 73, loss = 0.02502077
Iteration 74, loss = 0.02493940
Iteration 75, loss = 0.02469737
Iteration 76, loss = 0.02472411
Iteration 77, loss = 0.02459462
Iteration 78, loss = 0.02469794
Iteration 79, loss = 0.02452762
Iteration 80, loss = 0.02440704
Iteration 81, loss = 0.02442267
Iteration 82, loss = 0.02450098
Iteration 83, loss = 0.02464526
Iteration 84, loss = 0.02459605
Iteration 85, loss = 0.02435568
Iteration 86, loss = 0.02458510
Iteration 87, loss = 0.02451471
Iteration 88, loss = 0.02439453
Iteration 89, loss = 0.02423843
Iteration 90, loss = 0.02421981
Iteration 91, loss = 0.02459110
Iteration 92, loss = 0.02451551
Iteration 93, loss = 0.02436661
Iteration 94, loss = 0.02443183
Iteration 95, loss = 0.02463870
Iteration 96, loss = 0.02408589
Iteration 97, loss = 0.02472817
Iteration 98, loss = 0.02418232
Iteration 99, loss = 0.02431705
Iteration 100, loss = 0.02433125
Iteration 101, loss = 0.02407712
Iteration 102, loss = 0.02426595
Iteration 103, loss = 0.02414853
Iteration 104, loss = 0.02432333
Iteration 105, loss = 0.02439758
Iteration 106, loss = 0.02432250
Iteration 107, loss = 0.02425443
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67684882
Iteration 2, loss = 0.60378921
Iteration 3, loss = 0.49900124
Iteration 4, loss = 0.38721228
Iteration 5, loss = 0.29151154
Iteration 6, loss = 0.21928635
Iteration 7, loss = 0.16864806
Iteration 8, loss = 0.13372942
Iteration 9, loss = 0.10937838
Iteration 10, loss = 0.09256203
Iteration 11, loss = 0.08015860
Iteration 12, loss = 0.07056758
Iteration 13, loss = 0.06345882
Iteration 14, loss = 0.05770870
Iteration 15, loss = 0.05364084
Iteration 16, loss = 0.04981111
Iteration 17, loss = 0.04714310
Iteration 18, loss = 0.04457970
Iteration 19, loss = 0.04265716
Iteration 20, loss = 0.04080083
Iteration 21, loss = 0.03935526
Iteration 22, loss = 0.03786756
Iteration 23, loss = 0.03684457
Iteration 24, loss = 0.03604277
Iteration 25, loss = 0.03504364
Iteration 26, loss = 0.03444374
Iteration 27, loss = 0.03372101
Iteration 28, loss = 0.03313095
Iteration 29, loss = 0.03275678
Iteration 30, loss = 0.03221999
Iteration 31, loss = 0.03170372
Iteration 32, loss = 0.03140754
Iteration 33, loss = 0.03108587
Iteration 34, loss = 0.03046422
Iteration 35, loss = 0.03024429
Iteration 36, loss = 0.03033198
Iteration 37, loss = 0.02982470
Iteration 38, loss = 0.02962770
Iteration 39, loss = 0.02933395
Iteration 40, loss = 0.02931462
Iteration 41, loss = 0.02903571
Iteration 42, loss = 0.02876611
Iteration 43, loss = 0.02867494
Iteration 44, loss = 0.02865188
Iteration 45, loss = 0.02854113
Iteration 46, loss = 0.02863565
Iteration 47, loss = 0.02859737
Iteration 48, loss = 0.02804606
Iteration 49, loss = 0.02792149
Iteration 50, loss = 0.02828418
Iteration 51, loss = 0.02817367
Iteration 52, loss = 0.02749471
Iteration 53, loss = 0.02783317
Iteration 54, loss = 0.02757551
Iteration 55, loss = 0.02778556
Iteration 56, loss = 0.02709425
Iteration 57, loss = 0.02761796
Iteration 58, loss = 0.02736444
Iteration 59, loss = 0.02733262
Iteration 60, loss = 0.02728029
Iteration 61, loss = 0.02731956
Iteration 62, loss = 0.02731743
Iteration 63, loss = 0.02723793
Iteration 64, loss = 0.02708087
Iteration 65, loss = 0.02681677
Iteration 66, loss = 0.02701149
Iteration 67, loss = 0.02693851
Iteration 68, loss = 0.02677560
Iteration 69, loss = 0.02700076
Iteration 70, loss = 0.02663329
Iteration 71, loss = 0.02664561
Iteration 72, loss = 0.02687468
Iteration 73, loss = 0.02655934
Iteration 74, loss = 0.02658164
Iteration 75, loss = 0.02645208
Iteration 76, loss = 0.02706557
Iteration 77, loss = 0.02684477
Iteration 78, loss = 0.02670394
Iteration 79, loss = 0.02663579
Iteration 80, loss = 0.02690369
Iteration 81, loss = 0.02678258
Iteration 82, loss = 0.02677326
Iteration 83, loss = 0.02656204
Iteration 84, loss = 0.02650761
Iteration 85, loss = 0.02676857
Iteration 86, loss = 0.02656812
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70119582
Iteration 2, loss = 0.62410220
Iteration 3, loss = 0.51816012
Iteration 4, loss = 0.40184205
Iteration 5, loss = 0.30134970
Iteration 6, loss = 0.22518525
Iteration 7, loss = 0.17174459
Iteration 8, loss = 0.13471669
Iteration 9, loss = 0.10956575
Iteration 10, loss = 0.09177886
Iteration 11, loss = 0.07885055
Iteration 12, loss = 0.06906626
Iteration 13, loss = 0.06193342
Iteration 14, loss = 0.05607658
Iteration 15, loss = 0.05145454
Iteration 16, loss = 0.04759458
Iteration 17, loss = 0.04491621
Iteration 18, loss = 0.04221795
Iteration 19, loss = 0.04003340
Iteration 20, loss = 0.03843604
Iteration 21, loss = 0.03657286
Iteration 22, loss = 0.03533198
Iteration 23, loss = 0.03428144
Iteration 24, loss = 0.03337127
Iteration 25, loss = 0.03260878
Iteration 26, loss = 0.03165106
Iteration 27, loss = 0.03130695
Iteration 28, loss = 0.02998420
Iteration 29, loss = 0.02983048
Iteration 30, loss = 0.02906603
Iteration 31, loss = 0.02885972
Iteration 32, loss = 0.02820318
Iteration 33, loss = 0.02796675
Iteration 34, loss = 0.02796024
Iteration 35, loss = 0.02728083
Iteration 36, loss = 0.02714691
Iteration 37, loss = 0.02679946
Iteration 38, loss = 0.02646969
Iteration 39, loss = 0.02636548
Iteration 40, loss = 0.02621456
Iteration 41, loss = 0.02602272
Iteration 42, loss = 0.02585937
Iteration 43, loss = 0.02573470
Iteration 44, loss = 0.02583682
Iteration 45, loss = 0.02552410
Iteration 46, loss = 0.02526588
Iteration 47, loss = 0.02527309
Iteration 48, loss = 0.02522178
Iteration 49, loss = 0.02504939
Iteration 50, loss = 0.02484165
Iteration 51, loss = 0.02483691
Iteration 52, loss = 0.02476657
Iteration 53, loss = 0.02456481
Iteration 54, loss = 0.02504516
Iteration 55, loss = 0.02431812
Iteration 56, loss = 0.02436574
Iteration 57, loss = 0.02414500
Iteration 58, loss = 0.02441832
Iteration 59, loss = 0.02421299
Iteration 60, loss = 0.02436255
Iteration 61, loss = 0.02403597
Iteration 62, loss = 0.02419040
Iteration 63, loss = 0.02403754
Iteration 64, loss = 0.02403178
Iteration 65, loss = 0.02406818
Iteration 66, loss = 0.02381387
Iteration 67, loss = 0.02395179
Iteration 68, loss = 0.02407785
Iteration 69, loss = 0.02378868
Iteration 70, loss = 0.02356766
Iteration 71, loss = 0.02366346
Iteration 72, loss = 0.02375752
Iteration 73, loss = 0.02354171
Iteration 74, loss = 0.02376239
Iteration 75, loss = 0.02373081
Iteration 76, loss = 0.02341321
Iteration 77, loss = 0.02383902
Iteration 78, loss = 0.02368147
Iteration 79, loss = 0.02339749
Iteration 80, loss = 0.02351684
Iteration 81, loss = 0.02362378
Iteration 82, loss = 0.02372029
Iteration 83, loss = 0.02328482
Iteration 84, loss = 0.02344275
Iteration 85, loss = 0.02330548
Iteration 86, loss = 0.02310632
Iteration 87, loss = 0.02356782
Iteration 88, loss = 0.02359820
Iteration 89, loss = 0.02349149
Iteration 90, loss = 0.02346104
Iteration 91, loss = 0.02324200
Iteration 92, loss = 0.02310540
Iteration 93, loss = 0.02321883
Iteration 94, loss = 0.02325852
Iteration 95, loss = 0.02326587
Iteration 96, loss = 0.02325787
Iteration 97, loss = 0.02322841
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69689033
Iteration 2, loss = 0.62539923
Iteration 3, loss = 0.51880363
Iteration 4, loss = 0.40172948
Iteration 5, loss = 0.30117541
Iteration 6, loss = 0.22461571
Iteration 7, loss = 0.17096658
Iteration 8, loss = 0.13408466
Iteration 9, loss = 0.10864531
Iteration 10, loss = 0.09053586
Iteration 11, loss = 0.07755942
Iteration 12, loss = 0.06774442
Iteration 13, loss = 0.06025529
Iteration 14, loss = 0.05469161
Iteration 15, loss = 0.04987410
Iteration 16, loss = 0.04615173
Iteration 17, loss = 0.04301970
Iteration 18, loss = 0.04037362
Iteration 19, loss = 0.03829174
Iteration 20, loss = 0.03634809
Iteration 21, loss = 0.03484867
Iteration 22, loss = 0.03330748
Iteration 23, loss = 0.03237610
Iteration 24, loss = 0.03128217
Iteration 25, loss = 0.03034941
Iteration 26, loss = 0.02966024
Iteration 27, loss = 0.02905845
Iteration 28, loss = 0.02829753
Iteration 29, loss = 0.02766168
Iteration 30, loss = 0.02695157
Iteration 31, loss = 0.02680058
Iteration 32, loss = 0.02643725
Iteration 33, loss = 0.02587737
Iteration 34, loss = 0.02567107
Iteration 35, loss = 0.02540189
Iteration 36, loss = 0.02520898
Iteration 37, loss = 0.02470833
Iteration 38, loss = 0.02478087
Iteration 39, loss = 0.02447119
Iteration 40, loss = 0.02406699
Iteration 41, loss = 0.02361671
Iteration 42, loss = 0.02392453
Iteration 43, loss = 0.02341867
Iteration 44, loss = 0.02358467
Iteration 45, loss = 0.02323811
Iteration 46, loss = 0.02295628
Iteration 47, loss = 0.02324748
Iteration 48, loss = 0.02332549
Iteration 49, loss = 0.02282925
Iteration 50, loss = 0.02312321
Iteration 51, loss = 0.02287660
Iteration 52, loss = 0.02252462
Iteration 53, loss = 0.02250394
Iteration 54, loss = 0.02267626
Iteration 55, loss = 0.02230659
Iteration 56, loss = 0.02244849
Iteration 57, loss = 0.02202308
Iteration 58, loss = 0.02209476
Iteration 59, loss = 0.02217651
Iteration 60, loss = 0.02223558
Iteration 61, loss = 0.02187968
Iteration 62, loss = 0.02203895
Iteration 63, loss = 0.02185569
Iteration 64, loss = 0.02198381
Iteration 65, loss = 0.02191913
Iteration 66, loss = 0.02206207
Iteration 67, loss = 0.02187136
Iteration 68, loss = 0.02176652
Iteration 69, loss = 0.02159035
Iteration 70, loss = 0.02166934
Iteration 71, loss = 0.02164781
Iteration 72, loss = 0.02154386
Iteration 73, loss = 0.02165897
Iteration 74, loss = 0.02171142
Iteration 75, loss = 0.02123855
Iteration 76, loss = 0.02142599
Iteration 77, loss = 0.02143094
Iteration 78, loss = 0.02130901
Iteration 79, loss = 0.02119684
Iteration 80, loss = 0.02149969
Iteration 81, loss = 0.02094328
Iteration 82, loss = 0.02138093
Iteration 83, loss = 0.02123339
Iteration 84, loss = 0.02133102
Iteration 85, loss = 0.02099844
Iteration 86, loss = 0.02142811
Iteration 87, loss = 0.02114829
Iteration 88, loss = 0.02107159
Iteration 89, loss = 0.02112392
Iteration 90, loss = 0.02142502
Iteration 91, loss = 0.02125838
Iteration 92, loss = 0.02111842
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69249511
Iteration 2, loss = 0.61928946
Iteration 3, loss = 0.51367578
Iteration 4, loss = 0.39546432
Iteration 5, loss = 0.29612531
Iteration 6, loss = 0.22195550
Iteration 7, loss = 0.17019659
Iteration 8, loss = 0.13502581
Iteration 9, loss = 0.11049234
Iteration 10, loss = 0.09324065
Iteration 11, loss = 0.08059627
Iteration 12, loss = 0.07158199
Iteration 13, loss = 0.06422444
Iteration 14, loss = 0.05851361
Iteration 15, loss = 0.05426330
Iteration 16, loss = 0.05053118
Iteration 17, loss = 0.04763620
Iteration 18, loss = 0.04516781
Iteration 19, loss = 0.04302059
Iteration 20, loss = 0.04115790
Iteration 21, loss = 0.03977110
Iteration 22, loss = 0.03855569
Iteration 23, loss = 0.03763367
Iteration 24, loss = 0.03622871
Iteration 25, loss = 0.03547588
Iteration 26, loss = 0.03479803
Iteration 27, loss = 0.03399944
Iteration 28, loss = 0.03343654
Iteration 29, loss = 0.03262086
Iteration 30, loss = 0.03234747
Iteration 31, loss = 0.03182965
Iteration 32, loss = 0.03160513
Iteration 33, loss = 0.03131637
Iteration 34, loss = 0.03070102
Iteration 35, loss = 0.03045296
Iteration 36, loss = 0.03013083
Iteration 37, loss = 0.02992771
Iteration 38, loss = 0.02975524
Iteration 39, loss = 0.02963963
Iteration 40, loss = 0.02967330
Iteration 41, loss = 0.02904414
Iteration 42, loss = 0.02939166
Iteration 43, loss = 0.02916417
Iteration 44, loss = 0.02852572
Iteration 45, loss = 0.02869485
Iteration 46, loss = 0.02839921
Iteration 47, loss = 0.02832336
Iteration 48, loss = 0.02828751
Iteration 49, loss = 0.02816957
Iteration 50, loss = 0.02812840
Iteration 51, loss = 0.02808937
Iteration 52, loss = 0.02791774
Iteration 53, loss = 0.02767040
Iteration 54, loss = 0.02753456
Iteration 55, loss = 0.02769564
Iteration 56, loss = 0.02788414
Iteration 57, loss = 0.02769003
Iteration 58, loss = 0.02793582
Iteration 59, loss = 0.02714002
Iteration 60, loss = 0.02742346
Iteration 61, loss = 0.02731731
Iteration 62, loss = 0.02728900
Iteration 63, loss = 0.02704156
Iteration 64, loss = 0.02729487
Iteration 65, loss = 0.02707699
Iteration 66, loss = 0.02712848
Iteration 67, loss = 0.02705512
Iteration 68, loss = 0.02699215
Iteration 69, loss = 0.02674585
Iteration 70, loss = 0.02713490
Iteration 71, loss = 0.02686294
Iteration 72, loss = 0.02664781
Iteration 73, loss = 0.02671120
Iteration 74, loss = 0.02679264
Iteration 75, loss = 0.02700637
Iteration 76, loss = 0.02686518
Iteration 77, loss = 0.02682355
Iteration 78, loss = 0.02658816
Iteration 79, loss = 0.02663477
Iteration 80, loss = 0.02676720
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70794459
Iteration 2, loss = 0.62072796
Iteration 3, loss = 0.50677750
Iteration 4, loss = 0.38685988
Iteration 5, loss = 0.28867716
Iteration 6, loss = 0.21631223
Iteration 7, loss = 0.16572898
Iteration 8, loss = 0.13157811
Iteration 9, loss = 0.10738028
Iteration 10, loss = 0.09030527
Iteration 11, loss = 0.07804686
Iteration 12, loss = 0.06886377
Iteration 13, loss = 0.06157010
Iteration 14, loss = 0.05603731
Iteration 15, loss = 0.05145379
Iteration 16, loss = 0.04809335
Iteration 17, loss = 0.04510154
Iteration 18, loss = 0.04278874
Iteration 19, loss = 0.04065365
Iteration 20, loss = 0.03906281
Iteration 21, loss = 0.03736735
Iteration 22, loss = 0.03608724
Iteration 23, loss = 0.03512361
Iteration 24, loss = 0.03410162
Iteration 25, loss = 0.03315698
Iteration 26, loss = 0.03262788
Iteration 27, loss = 0.03187203
Iteration 28, loss = 0.03092343
Iteration 29, loss = 0.03069658
Iteration 30, loss = 0.02995122
Iteration 31, loss = 0.02966552
Iteration 32, loss = 0.02970890
Iteration 33, loss = 0.02931820
Iteration 34, loss = 0.02863229
Iteration 35, loss = 0.02856420
Iteration 36, loss = 0.02795177
Iteration 37, loss = 0.02798935
Iteration 38, loss = 0.02757816
Iteration 39, loss = 0.02754411
Iteration 40, loss = 0.02724666
Iteration 41, loss = 0.02730036
Iteration 42, loss = 0.02695744
Iteration 43, loss = 0.02665689
Iteration 44, loss = 0.02689546
Iteration 45, loss = 0.02666497
Iteration 46, loss = 0.02644789
Iteration 47, loss = 0.02654100
Iteration 48, loss = 0.02611991
Iteration 49, loss = 0.02599460
Iteration 50, loss = 0.02582146
Iteration 51, loss = 0.02586458
Iteration 52, loss = 0.02597935
Iteration 53, loss = 0.02582596
Iteration 54, loss = 0.02542275
Iteration 55, loss = 0.02555447
Iteration 56, loss = 0.02532779
Iteration 57, loss = 0.02548823
Iteration 58, loss = 0.02545137
Iteration 59, loss = 0.02563227
Iteration 60, loss = 0.02577031
Iteration 61, loss = 0.02528673
Iteration 62, loss = 0.02543479
Iteration 63, loss = 0.02510692
Iteration 64, loss = 0.02515482
Iteration 65, loss = 0.02498761
Iteration 66, loss = 0.02556583
Iteration 67, loss = 0.02470568
Iteration 68, loss = 0.02490023
Iteration 69, loss = 0.02478654
Iteration 70, loss = 0.02534187
Iteration 71, loss = 0.02481027
Iteration 72, loss = 0.02501586
Iteration 73, loss = 0.02502077
Iteration 74, loss = 0.02493940
Iteration 75, loss = 0.02469737
Iteration 76, loss = 0.02472411
Iteration 77, loss = 0.02459462
Iteration 78, loss = 0.02469794
Iteration 79, loss = 0.02452762
Iteration 80, loss = 0.02440704
Iteration 81, loss = 0.02442267
Iteration 82, loss = 0.02450098
Iteration 83, loss = 0.02464526
Iteration 84, loss = 0.02459605
Iteration 85, loss = 0.02435568
Iteration 86, loss = 0.02458510
Iteration 87, loss = 0.02451471
Iteration 88, loss = 0.02439453
Iteration 89, loss = 0.02423843
Iteration 90, loss = 0.02421981
Iteration 91, loss = 0.02459110
Iteration 92, loss = 0.02451551
Iteration 93, loss = 0.02436661
Iteration 94, loss = 0.02443183
Iteration 95, loss = 0.02463870
Iteration 96, loss = 0.02408589
Iteration 97, loss = 0.02472817
Iteration 98, loss = 0.02418232
Iteration 99, loss = 0.02431705
Iteration 100, loss = 0.02433125
Iteration 101, loss = 0.02407712
Iteration 102, loss = 0.02426595
Iteration 103, loss = 0.02414853
Iteration 104, loss = 0.02432333
Iteration 105, loss = 0.02439758
Iteration 106, loss = 0.02432250
Iteration 107, loss = 0.02425443
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67684882
Iteration 2, loss = 0.60378921
Iteration 3, loss = 0.49900124
Iteration 4, loss = 0.38721228
Iteration 5, loss = 0.29151154
Iteration 6, loss = 0.21928635
Iteration 7, loss = 0.16864806
Iteration 8, loss = 0.13372942
Iteration 9, loss = 0.10937838
Iteration 10, loss = 0.09256203
Iteration 11, loss = 0.08015860
Iteration 12, loss = 0.07056758
Iteration 13, loss = 0.06345882
Iteration 14, loss = 0.05770870
Iteration 15, loss = 0.05364084
Iteration 16, loss = 0.04981111
Iteration 17, loss = 0.04714310
Iteration 18, loss = 0.04457970
Iteration 19, loss = 0.04265716
Iteration 20, loss = 0.04080083
Iteration 21, loss = 0.03935526
Iteration 22, loss = 0.03786756
Iteration 23, loss = 0.03684457
Iteration 24, loss = 0.03604277
Iteration 25, loss = 0.03504364
Iteration 26, loss = 0.03444374
Iteration 27, loss = 0.03372101
Iteration 28, loss = 0.03313095
Iteration 29, loss = 0.03275678
Iteration 30, loss = 0.03221999
Iteration 31, loss = 0.03170372
Iteration 32, loss = 0.03140754
Iteration 33, loss = 0.03108587
Iteration 34, loss = 0.03046422
Iteration 35, loss = 0.03024429
Iteration 36, loss = 0.03033198
Iteration 37, loss = 0.02982470
Iteration 38, loss = 0.02962770
Iteration 39, loss = 0.02933395
Iteration 40, loss = 0.02931462
Iteration 41, loss = 0.02903571
Iteration 42, loss = 0.02876611
Iteration 43, loss = 0.02867494
Iteration 44, loss = 0.02865188
Iteration 45, loss = 0.02854113
Iteration 46, loss = 0.02863565
Iteration 47, loss = 0.02859737
Iteration 48, loss = 0.02804606
Iteration 49, loss = 0.02792149
Iteration 50, loss = 0.02828418
Iteration 51, loss = 0.02817367
Iteration 52, loss = 0.02749471
Iteration 53, loss = 0.02783317
Iteration 54, loss = 0.02757551
Iteration 55, loss = 0.02778556
Iteration 56, loss = 0.02709425
Iteration 57, loss = 0.02761796
Iteration 58, loss = 0.02736444
Iteration 59, loss = 0.02733262
Iteration 60, loss = 0.02728029
Iteration 61, loss = 0.02731956
Iteration 62, loss = 0.02731743
Iteration 63, loss = 0.02723793
Iteration 64, loss = 0.02708087
Iteration 65, loss = 0.02681677
Iteration 66, loss = 0.02701149
Iteration 67, loss = 0.02693851
Iteration 68, loss = 0.02677560
Iteration 69, loss = 0.02700076
Iteration 70, loss = 0.02663329
Iteration 71, loss = 0.02664561
Iteration 72, loss = 0.02687468
Iteration 73, loss = 0.02655934
Iteration 74, loss = 0.02658164
Iteration 75, loss = 0.02645208
Iteration 76, loss = 0.02706557
Iteration 77, loss = 0.02684477
Iteration 78, loss = 0.02670394
Iteration 79, loss = 0.02663579
Iteration 80, loss = 0.02690369
Iteration 81, loss = 0.02678258
Iteration 82, loss = 0.02677326
Iteration 83, loss = 0.02656204
Iteration 84, loss = 0.02650761
Iteration 85, loss = 0.02676857
Iteration 86, loss = 0.02656812
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70119582
Iteration 2, loss = 0.62410220
Iteration 3, loss = 0.51816012
Iteration 4, loss = 0.40184205
Iteration 5, loss = 0.30134970
Iteration 6, loss = 0.22518525
Iteration 7, loss = 0.17174459
Iteration 8, loss = 0.13471669
Iteration 9, loss = 0.10956575
Iteration 10, loss = 0.09177886
Iteration 11, loss = 0.07885055
Iteration 12, loss = 0.06906626
Iteration 13, loss = 0.06193342
Iteration 14, loss = 0.05607658
Iteration 15, loss = 0.05145454
Iteration 16, loss = 0.04759458
Iteration 17, loss = 0.04491621
Iteration 18, loss = 0.04221795
Iteration 19, loss = 0.04003340
Iteration 20, loss = 0.03843604
Iteration 21, loss = 0.03657286
Iteration 22, loss = 0.03533198
Iteration 23, loss = 0.03428144
Iteration 24, loss = 0.03337127
Iteration 25, loss = 0.03260878
Iteration 26, loss = 0.03165106
Iteration 27, loss = 0.03130695
Iteration 28, loss = 0.02998420
Iteration 29, loss = 0.02983048
Iteration 30, loss = 0.02906603
Iteration 31, loss = 0.02885972
Iteration 32, loss = 0.02820318
Iteration 33, loss = 0.02796675
Iteration 34, loss = 0.02796024
Iteration 35, loss = 0.02728083
Iteration 36, loss = 0.02714691
Iteration 37, loss = 0.02679946
Iteration 38, loss = 0.02646969
Iteration 39, loss = 0.02636548
Iteration 40, loss = 0.02621456
Iteration 41, loss = 0.02602272
Iteration 42, loss = 0.02585937
Iteration 43, loss = 0.02573470
Iteration 44, loss = 0.02583682
Iteration 45, loss = 0.02552410
Iteration 46, loss = 0.02526588
Iteration 47, loss = 0.02527309
Iteration 48, loss = 0.02522178
Iteration 49, loss = 0.02504939
Iteration 50, loss = 0.02484165
Iteration 51, loss = 0.02483691
Iteration 52, loss = 0.02476657
Iteration 53, loss = 0.02456481
Iteration 54, loss = 0.02504516
Iteration 55, loss = 0.02431812
Iteration 56, loss = 0.02436574
Iteration 57, loss = 0.02414500
Iteration 58, loss = 0.02441832
Iteration 59, loss = 0.02421299
Iteration 60, loss = 0.02436255
Iteration 61, loss = 0.02403597
Iteration 62, loss = 0.02419040
Iteration 63, loss = 0.02403754
Iteration 64, loss = 0.02403178
Iteration 65, loss = 0.02406818
Iteration 66, loss = 0.02381387
Iteration 67, loss = 0.02395179
Iteration 68, loss = 0.02407785
Iteration 69, loss = 0.02378868
Iteration 70, loss = 0.02356766
Iteration 71, loss = 0.02366346
Iteration 72, loss = 0.02375752
Iteration 73, loss = 0.02354171
Iteration 74, loss = 0.02376239
Iteration 75, loss = 0.02373081
Iteration 76, loss = 0.02341321
Iteration 77, loss = 0.02383902
Iteration 78, loss = 0.02368147
Iteration 79, loss = 0.02339749
Iteration 80, loss = 0.02351684
Iteration 81, loss = 0.02362378
Iteration 82, loss = 0.02372029
Iteration 83, loss = 0.02328482
Iteration 84, loss = 0.02344275
Iteration 85, loss = 0.02330548
Iteration 86, loss = 0.02310632
Iteration 87, loss = 0.02356782
Iteration 88, loss = 0.02359820
Iteration 89, loss = 0.02349149
Iteration 90, loss = 0.02346104
Iteration 91, loss = 0.02324200
Iteration 92, loss = 0.02310540
Iteration 93, loss = 0.02321883
Iteration 94, loss = 0.02325852
Iteration 95, loss = 0.02326587
Iteration 96, loss = 0.02325787
Iteration 97, loss = 0.02322841
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69689033
Iteration 2, loss = 0.62539923
Iteration 3, loss = 0.51880363
Iteration 4, loss = 0.40172948
Iteration 5, loss = 0.30117541
Iteration 6, loss = 0.22461571
Iteration 7, loss = 0.17096658
Iteration 8, loss = 0.13408466
Iteration 9, loss = 0.10864531
Iteration 10, loss = 0.09053586
Iteration 11, loss = 0.07755942
Iteration 12, loss = 0.06774442
Iteration 13, loss = 0.06025529
Iteration 14, loss = 0.05469161
Iteration 15, loss = 0.04987410
Iteration 16, loss = 0.04615173
Iteration 17, loss = 0.04301970
Iteration 18, loss = 0.04037362
Iteration 19, loss = 0.03829174
Iteration 20, loss = 0.03634809
Iteration 21, loss = 0.03484867
Iteration 22, loss = 0.03330748
Iteration 23, loss = 0.03237610
Iteration 24, loss = 0.03128217
Iteration 25, loss = 0.03034941
Iteration 26, loss = 0.02966024
Iteration 27, loss = 0.02905845
Iteration 28, loss = 0.02829753
Iteration 29, loss = 0.02766168
Iteration 30, loss = 0.02695157
Iteration 31, loss = 0.02680058
Iteration 32, loss = 0.02643725
Iteration 33, loss = 0.02587737
Iteration 34, loss = 0.02567107
Iteration 35, loss = 0.02540189
Iteration 36, loss = 0.02520898
Iteration 37, loss = 0.02470833
Iteration 38, loss = 0.02478087
Iteration 39, loss = 0.02447119
Iteration 40, loss = 0.02406699
Iteration 41, loss = 0.02361671
Iteration 42, loss = 0.02392453
Iteration 43, loss = 0.02341867
Iteration 44, loss = 0.02358467
Iteration 45, loss = 0.02323811
Iteration 46, loss = 0.02295628
Iteration 47, loss = 0.02324748
Iteration 48, loss = 0.02332549
Iteration 49, loss = 0.02282925
Iteration 50, loss = 0.02312321
Iteration 51, loss = 0.02287660
Iteration 52, loss = 0.02252462
Iteration 53, loss = 0.02250394
Iteration 54, loss = 0.02267626
Iteration 55, loss = 0.02230659
Iteration 56, loss = 0.02244849
Iteration 57, loss = 0.02202308
Iteration 58, loss = 0.02209476
Iteration 59, loss = 0.02217651
Iteration 60, loss = 0.02223558
Iteration 61, loss = 0.02187968
Iteration 62, loss = 0.02203895
Iteration 63, loss = 0.02185569
Iteration 64, loss = 0.02198381
Iteration 65, loss = 0.02191913
Iteration 66, loss = 0.02206207
Iteration 67, loss = 0.02187136
Iteration 68, loss = 0.02176652
Iteration 69, loss = 0.02159035
Iteration 70, loss = 0.02166934
Iteration 71, loss = 0.02164781
Iteration 72, loss = 0.02154386
Iteration 73, loss = 0.02165897
Iteration 74, loss = 0.02171142
Iteration 75, loss = 0.02123855
Iteration 76, loss = 0.02142599
Iteration 77, loss = 0.02143094
Iteration 78, loss = 0.02130901
Iteration 79, loss = 0.02119684
Iteration 80, loss = 0.02149969
Iteration 81, loss = 0.02094328
Iteration 82, loss = 0.02138093
Iteration 83, loss = 0.02123339
Iteration 84, loss = 0.02133102
Iteration 85, loss = 0.02099844
Iteration 86, loss = 0.02142811
Iteration 87, loss = 0.02114829
Iteration 88, loss = 0.02107159
Iteration 89, loss = 0.02112392
Iteration 90, loss = 0.02142502
Iteration 91, loss = 0.02125838
Iteration 92, loss = 0.02111842
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69249511
Iteration 2, loss = 0.61928946
Iteration 3, loss = 0.51367578
Iteration 4, loss = 0.39546432
Iteration 5, loss = 0.29612531
Iteration 6, loss = 0.22195550
Iteration 7, loss = 0.17019659
Iteration 8, loss = 0.13502581
Iteration 9, loss = 0.11049234
Iteration 10, loss = 0.09324065
Iteration 11, loss = 0.08059627
Iteration 12, loss = 0.07158199
Iteration 13, loss = 0.06422444
Iteration 14, loss = 0.05851361
Iteration 15, loss = 0.05426330
Iteration 16, loss = 0.05053118
Iteration 17, loss = 0.04763620
Iteration 18, loss = 0.04516781
Iteration 19, loss = 0.04302059
Iteration 20, loss = 0.04115790
Iteration 21, loss = 0.03977110
Iteration 22, loss = 0.03855569
Iteration 23, loss = 0.03763367
Iteration 24, loss = 0.03622871
Iteration 25, loss = 0.03547588
Iteration 26, loss = 0.03479803
Iteration 27, loss = 0.03399944
Iteration 28, loss = 0.03343654
Iteration 29, loss = 0.03262086
Iteration 30, loss = 0.03234747
Iteration 31, loss = 0.03182965
Iteration 32, loss = 0.03160513
Iteration 33, loss = 0.03131637
Iteration 34, loss = 0.03070102
Iteration 35, loss = 0.03045296
Iteration 36, loss = 0.03013083
Iteration 37, loss = 0.02992771
Iteration 38, loss = 0.02975524
Iteration 39, loss = 0.02963963
Iteration 40, loss = 0.02967330
Iteration 41, loss = 0.02904414
Iteration 42, loss = 0.02939166
Iteration 43, loss = 0.02916417
Iteration 44, loss = 0.02852572
Iteration 45, loss = 0.02869485
Iteration 46, loss = 0.02839921
Iteration 47, loss = 0.02832336
Iteration 48, loss = 0.02828751
Iteration 49, loss = 0.02816957
Iteration 50, loss = 0.02812840
Iteration 51, loss = 0.02808937
Iteration 52, loss = 0.02791774
Iteration 53, loss = 0.02767040
Iteration 54, loss = 0.02753456
Iteration 55, loss = 0.02769564
Iteration 56, loss = 0.02788414
Iteration 57, loss = 0.02769003
Iteration 58, loss = 0.02793582
Iteration 59, loss = 0.02714002
Iteration 60, loss = 0.02742346
Iteration 61, loss = 0.02731731
Iteration 62, loss = 0.02728900
Iteration 63, loss = 0.02704156
Iteration 64, loss = 0.02729487
Iteration 65, loss = 0.02707699
Iteration 66, loss = 0.02712848
Iteration 67, loss = 0.02705512
Iteration 68, loss = 0.02699215
Iteration 69, loss = 0.02674585
Iteration 70, loss = 0.02713490
Iteration 71, loss = 0.02686294
Iteration 72, loss = 0.02664781
Iteration 73, loss = 0.02671120
Iteration 74, loss = 0.02679264
Iteration 75, loss = 0.02700637
Iteration 76, loss = 0.02686518
Iteration 77, loss = 0.02682355
Iteration 78, loss = 0.02658816
Iteration 79, loss = 0.02663477
Iteration 80, loss = 0.02676720
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70794459
Iteration 2, loss = 0.62072796
Iteration 3, loss = 0.50677750
Iteration 4, loss = 0.38685988
Iteration 5, loss = 0.28867716
Iteration 6, loss = 0.21631223
Iteration 7, loss = 0.16572898
Iteration 8, loss = 0.13157811
Iteration 9, loss = 0.10738028
Iteration 10, loss = 0.09030527
Iteration 11, loss = 0.07804686
Iteration 12, loss = 0.06886377
Iteration 13, loss = 0.06157010
Iteration 14, loss = 0.05603731
Iteration 15, loss = 0.05145379
Iteration 16, loss = 0.04809335
Iteration 17, loss = 0.04510154
Iteration 18, loss = 0.04278874
Iteration 19, loss = 0.04065365
Iteration 20, loss = 0.03906281
Iteration 21, loss = 0.03736735
Iteration 22, loss = 0.03608724
Iteration 23, loss = 0.03512361
Iteration 24, loss = 0.03410162
Iteration 25, loss = 0.03315698
Iteration 26, loss = 0.03262788
Iteration 27, loss = 0.03187203
Iteration 28, loss = 0.03092343
Iteration 29, loss = 0.03069658
Iteration 30, loss = 0.02995122
Iteration 31, loss = 0.02966552
Iteration 32, loss = 0.02970890
Iteration 33, loss = 0.02931820
Iteration 34, loss = 0.02863229
Iteration 35, loss = 0.02856420
Iteration 36, loss = 0.02795177
Iteration 37, loss = 0.02798935
Iteration 38, loss = 0.02757816
Iteration 39, loss = 0.02754411
Iteration 40, loss = 0.02724666
Iteration 41, loss = 0.02730036
Iteration 42, loss = 0.02695744
Iteration 43, loss = 0.02665689
Iteration 44, loss = 0.02689546
Iteration 45, loss = 0.02666497
Iteration 46, loss = 0.02644789
Iteration 47, loss = 0.02654100
Iteration 48, loss = 0.02611991
Iteration 49, loss = 0.02599460
Iteration 50, loss = 0.02582146
Iteration 51, loss = 0.02586458
Iteration 52, loss = 0.02597935
Iteration 53, loss = 0.02582596
Iteration 54, loss = 0.02542275
Iteration 55, loss = 0.02555447
Iteration 56, loss = 0.02532779
Iteration 57, loss = 0.02548823
Iteration 58, loss = 0.02545137
Iteration 59, loss = 0.02563227
Iteration 60, loss = 0.02577031
Iteration 61, loss = 0.02528673
Iteration 62, loss = 0.02543479
Iteration 63, loss = 0.02510692
Iteration 64, loss = 0.02515482
Iteration 65, loss = 0.02498761
Iteration 66, loss = 0.02556583
Iteration 67, loss = 0.02470568
Iteration 68, loss = 0.02490023
Iteration 69, loss = 0.02478654
Iteration 70, loss = 0.02534187
Iteration 71, loss = 0.02481027
Iteration 72, loss = 0.02501586
Iteration 73, loss = 0.02502077
Iteration 74, loss = 0.02493940
Iteration 75, loss = 0.02469737
Iteration 76, loss = 0.02472411
Iteration 77, loss = 0.02459462
Iteration 78, loss = 0.02469794
Iteration 79, loss = 0.02452762
Iteration 80, loss = 0.02440704
Iteration 81, loss = 0.02442267
Iteration 82, loss = 0.02450098
Iteration 83, loss = 0.02464526
Iteration 84, loss = 0.02459605
Iteration 85, loss = 0.02435568
Iteration 86, loss = 0.02458510
Iteration 87, loss = 0.02451471
Iteration 88, loss = 0.02439453
Iteration 89, loss = 0.02423843
Iteration 90, loss = 0.02421981
Iteration 91, loss = 0.02459110
Iteration 92, loss = 0.02451551
Iteration 93, loss = 0.02436661
Iteration 94, loss = 0.02443183
Iteration 95, loss = 0.02463870
Iteration 96, loss = 0.02408589
Iteration 97, loss = 0.02472817
Iteration 98, loss = 0.02418232
Iteration 99, loss = 0.02431705
Iteration 100, loss = 0.02433125
Iteration 101, loss = 0.02407712
Iteration 102, loss = 0.02426595
Iteration 103, loss = 0.02414853
Iteration 104, loss = 0.02432333
Iteration 105, loss = 0.02439758
Iteration 106, loss = 0.02432250
Iteration 107, loss = 0.02425443
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69373012
Iteration 2, loss = 0.59740595
Iteration 3, loss = 0.46018152
Iteration 4, loss = 0.33124614
Iteration 5, loss = 0.23624691
Iteration 6, loss = 0.17304086
Iteration 7, loss = 0.13217223
Iteration 8, loss = 0.10644193
Iteration 9, loss = 0.08858014
Iteration 10, loss = 0.07578345
Iteration 11, loss = 0.06683712
Iteration 12, loss = 0.06013709
Iteration 13, loss = 0.05494054
Iteration 14, loss = 0.05067281
Iteration 15, loss = 0.04744724
Iteration 16, loss = 0.04496828
Iteration 17, loss = 0.04288144
Iteration 18, loss = 0.04112133
Iteration 19, loss = 0.03927683
Iteration 20, loss = 0.03806438
Iteration 21, loss = 0.03717977
Iteration 22, loss = 0.03601844
Iteration 23, loss = 0.03507660
Iteration 24, loss = 0.03459992
Iteration 25, loss = 0.03377780
Iteration 26, loss = 0.03347179
Iteration 27, loss = 0.03264393
Iteration 28, loss = 0.03236272
Iteration 29, loss = 0.03217961
Iteration 30, loss = 0.03217736
Iteration 31, loss = 0.03146546
Iteration 32, loss = 0.03115166
Iteration 33, loss = 0.03057441
Iteration 34, loss = 0.03052435
Iteration 35, loss = 0.03001754
Iteration 36, loss = 0.03015419
Iteration 37, loss = 0.02983477
Iteration 38, loss = 0.02981920
Iteration 39, loss = 0.02962137
Iteration 40, loss = 0.02938424
Iteration 41, loss = 0.02938307
Iteration 42, loss = 0.02942464
Iteration 43, loss = 0.02910039
Iteration 44, loss = 0.02880817
Iteration 45, loss = 0.02905006
Iteration 46, loss = 0.02905926
Iteration 47, loss = 0.02892552
Iteration 48, loss = 0.02853927
Iteration 49, loss = 0.02857357
Iteration 50, loss = 0.02877717
Iteration 51, loss = 0.02874732
Iteration 52, loss = 0.02830917
Iteration 53, loss = 0.02825399
Iteration 54, loss = 0.02815346
Iteration 55, loss = 0.02842325
Iteration 56, loss = 0.02803279
Iteration 57, loss = 0.02842670
Iteration 58, loss = 0.02847772
Iteration 59, loss = 0.02818830
Iteration 60, loss = 0.02817325
Iteration 61, loss = 0.02816147
Iteration 62, loss = 0.02788909
Iteration 63, loss = 0.02802478
Iteration 64, loss = 0.02809748
Iteration 65, loss = 0.02755604
Iteration 66, loss = 0.02773263
Iteration 67, loss = 0.02769863
Iteration 68, loss = 0.02784581
Iteration 69, loss = 0.02753854
Iteration 70, loss = 0.02744302
Iteration 71, loss = 0.02733769
Iteration 72, loss = 0.02774722
Iteration 73, loss = 0.02752337
Iteration 74, loss = 0.02752074
Iteration 75, loss = 0.02745168
Iteration 76, loss = 0.02742969
Iteration 77, loss = 0.02771088
Iteration 78, loss = 0.02771956
Iteration 79, loss = 0.02726002
Iteration 80, loss = 0.02737594
Iteration 81, loss = 0.02708554
Iteration 82, loss = 0.02715476
Iteration 83, loss = 0.02738760
Iteration 84, loss = 0.02741047
Iteration 85, loss = 0.02735589
Iteration 86, loss = 0.02708715
Iteration 87, loss = 0.02723746
Iteration 88, loss = 0.02728788
Iteration 89, loss = 0.02701185
Iteration 90, loss = 0.02741263
Iteration 91, loss = 0.02698491
Iteration 92, loss = 0.02727731
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on kept_v7_lowercase_words_only...
Iteration 1, loss = 0.69859622
Iteration 2, loss = 0.65505395
Iteration 3, loss = 0.59170193
Iteration 4, loss = 0.51121142
Iteration 5, loss = 0.42621277
Iteration 6, loss = 0.34800664
Iteration 7, loss = 0.28164844
Iteration 8, loss = 0.22836608
Iteration 9, loss = 0.18709219
Iteration 10, loss = 0.15566435
Iteration 11, loss = 0.13147352
Iteration 12, loss = 0.11314206
Iteration 13, loss = 0.09883927
Iteration 14, loss = 0.08752301
Iteration 15, loss = 0.07846305
Iteration 16, loss = 0.07128454
Iteration 17, loss = 0.06526534
Iteration 18, loss = 0.06027629
Iteration 19, loss = 0.05621025
Iteration 20, loss = 0.05240640
Iteration 21, loss = 0.04969399
Iteration 22, loss = 0.04697123
Iteration 23, loss = 0.04481200
Iteration 24, loss = 0.04283505
Iteration 25, loss = 0.04076385
Iteration 26, loss = 0.03929483
Iteration 27, loss = 0.03799649
Iteration 28, loss = 0.03665716
Iteration 29, loss = 0.03570866
Iteration 30, loss = 0.03483954
Iteration 31, loss = 0.03380280
Iteration 32, loss = 0.03299079
Iteration 33, loss = 0.03220078
Iteration 34, loss = 0.03173659
Iteration 35, loss = 0.03123280
Iteration 36, loss = 0.03044137
Iteration 37, loss = 0.02986092
Iteration 38, loss = 0.02955553
Iteration 39, loss = 0.02897855
Iteration 40, loss = 0.02872107
Iteration 41, loss = 0.02806903
Iteration 42, loss = 0.02804621
Iteration 43, loss = 0.02750161
Iteration 44, loss = 0.02737193
Iteration 45, loss = 0.02704557
Iteration 46, loss = 0.02668383
Iteration 47, loss = 0.02639123
Iteration 48, loss = 0.02638227
Iteration 49, loss = 0.02602147
Iteration 50, loss = 0.02567337
Iteration 51, loss = 0.02541939
Iteration 52, loss = 0.02538274
Iteration 53, loss = 0.02535558
Iteration 54, loss = 0.02529446
Iteration 55, loss = 0.02479086
Iteration 56, loss = 0.02505436
Iteration 57, loss = 0.02495039
Iteration 58, loss = 0.02441953
Iteration 59, loss = 0.02437570
Iteration 60, loss = 0.02448326
Iteration 61, loss = 0.02433034
Iteration 62, loss = 0.02416361
Iteration 63, loss = 0.02392986
Iteration 64, loss = 0.02370593
Iteration 65, loss = 0.02378047
Iteration 66, loss = 0.02397002
Iteration 67, loss = 0.02350749
Iteration 68, loss = 0.02342758
Iteration 69, loss = 0.02346781
Iteration 70, loss = 0.02346520
Iteration 71, loss = 0.02329243
Iteration 72, loss = 0.02316933
Iteration 73, loss = 0.02292783
Iteration 74, loss = 0.02295777
Iteration 75, loss = 0.02290360
Iteration 76, loss = 0.02290717
Iteration 77, loss = 0.02269344
Iteration 78, loss = 0.02276176
Iteration 79, loss = 0.02254664
Iteration 80, loss = 0.02262765
Iteration 81, loss = 0.02277355
Iteration 82, loss = 0.02274596
Iteration 83, loss = 0.02248166
Iteration 84, loss = 0.02249684
Iteration 85, loss = 0.02236217
Iteration 86, loss = 0.02215459
Iteration 87, loss = 0.02256167
Iteration 88, loss = 0.02237266
Iteration 89, loss = 0.02222688
Iteration 90, loss = 0.02234710
Iteration 91, loss = 0.02214321
Iteration 92, loss = 0.02202933
Iteration 93, loss = 0.02215385
Iteration 94, loss = 0.02213427
Iteration 95, loss = 0.02196484
Iteration 96, loss = 0.02195796
Iteration 97, loss = 0.02215300
Iteration 98, loss = 0.02208578
Iteration 99, loss = 0.02204201
Iteration 100, loss = 0.02221212
Iteration 101, loss = 0.02206628
Iteration 102, loss = 0.02197878
Iteration 103, loss = 0.02197144
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68293650
Iteration 2, loss = 0.66453053
Iteration 3, loss = 0.64634875
Iteration 4, loss = 0.62453455
Iteration 5, loss = 0.59933733
Iteration 6, loss = 0.57055725
Iteration 7, loss = 0.53842107
Iteration 8, loss = 0.50429237
Iteration 9, loss = 0.46934572
Iteration 10, loss = 0.43440161
Iteration 11, loss = 0.40004842
Iteration 12, loss = 0.36720506
Iteration 13, loss = 0.33590466
Iteration 14, loss = 0.30660219
Iteration 15, loss = 0.27937807
Iteration 16, loss = 0.25429269
Iteration 17, loss = 0.23138134
Iteration 18, loss = 0.21051403
Iteration 19, loss = 0.19186794
Iteration 20, loss = 0.17489137
Iteration 21, loss = 0.15965922
Iteration 22, loss = 0.14607071
Iteration 23, loss = 0.13399854
Iteration 24, loss = 0.12320953
Iteration 25, loss = 0.11355479
Iteration 26, loss = 0.10490948
Iteration 27, loss = 0.09720316
Iteration 28, loss = 0.09030087
Iteration 29, loss = 0.08415852
Iteration 30, loss = 0.07856881
Iteration 31, loss = 0.07350992
Iteration 32, loss = 0.06903285
Iteration 33, loss = 0.06489644
Iteration 34, loss = 0.06117162
Iteration 35, loss = 0.05781806
Iteration 36, loss = 0.05472303
Iteration 37, loss = 0.05186891
Iteration 38, loss = 0.04926886
Iteration 39, loss = 0.04689257
Iteration 40, loss = 0.04472417
Iteration 41, loss = 0.04270373
Iteration 42, loss = 0.04082893
Iteration 43, loss = 0.03911466
Iteration 44, loss = 0.03752191
Iteration 45, loss = 0.03600947
Iteration 46, loss = 0.03464114
Iteration 47, loss = 0.03336606
Iteration 48, loss = 0.03213924
Iteration 49, loss = 0.03098743
Iteration 50, loss = 0.02994257
Iteration 51, loss = 0.02894375
Iteration 52, loss = 0.02800378
Iteration 53, loss = 0.02713157
Iteration 54, loss = 0.02636389
Iteration 55, loss = 0.02551072
Iteration 56, loss = 0.02479985
Iteration 57, loss = 0.02410083
Iteration 58, loss = 0.02346864
Iteration 59, loss = 0.02285629
Iteration 60, loss = 0.02223026
Iteration 61, loss = 0.02165823
Iteration 62, loss = 0.02110816
Iteration 63, loss = 0.02061375
Iteration 64, loss = 0.02015916
Iteration 65, loss = 0.01969634
Iteration 66, loss = 0.01926143
Iteration 67, loss = 0.01888312
Iteration 68, loss = 0.01845688
Iteration 69, loss = 0.01808510
Iteration 70, loss = 0.01774225
Iteration 71, loss = 0.01738822
Iteration 72, loss = 0.01702793
Iteration 73, loss = 0.01672468
Iteration 74, loss = 0.01640457
Iteration 75, loss = 0.01614355
Iteration 76, loss = 0.01586961
Iteration 77, loss = 0.01556434
Iteration 78, loss = 0.01531658
Iteration 79, loss = 0.01510068
Iteration 80, loss = 0.01482363
Iteration 81, loss = 0.01461397
Iteration 82, loss = 0.01440071
Iteration 83, loss = 0.01420472
Iteration 84, loss = 0.01399996
Iteration 85, loss = 0.01379323
Iteration 86, loss = 0.01364586
Iteration 87, loss = 0.01340252
Iteration 88, loss = 0.01326075
Iteration 89, loss = 0.01311839
Iteration 90, loss = 0.01287925
Iteration 91, loss = 0.01277281
Iteration 92, loss = 0.01262706
Iteration 93, loss = 0.01248236
Iteration 94, loss = 0.01232884
Iteration 95, loss = 0.01218978
Iteration 96, loss = 0.01202453
Iteration 97, loss = 0.01192872
Iteration 98, loss = 0.01182569
Iteration 99, loss = 0.01168747
Iteration 100, loss = 0.01157596
Iteration 101, loss = 0.01144525
Iteration 102, loss = 0.01133376
Iteration 103, loss = 0.01123998
Iteration 104, loss = 0.01110347
Iteration 105, loss = 0.01101664
Iteration 106, loss = 0.01090591
Iteration 107, loss = 0.01081374
Iteration 108, loss = 0.01072635
Iteration 109, loss = 0.01066902
Iteration 110, loss = 0.01053139
Iteration 111, loss = 0.01049741
Iteration 112, loss = 0.01039238
Iteration 113, loss = 0.01036928
Iteration 114, loss = 0.01024779
Iteration 115, loss = 0.01014637
Iteration 116, loss = 0.01009055
Iteration 117, loss = 0.00999704
Iteration 118, loss = 0.00995616
Iteration 119, loss = 0.00995030
Iteration 120, loss = 0.00983595
Iteration 121, loss = 0.00975595
Iteration 122, loss = 0.00967340
Iteration 123, loss = 0.00967346
Iteration 124, loss = 0.00957370
Iteration 125, loss = 0.00956507
Iteration 126, loss = 0.00941523
Iteration 127, loss = 0.00941166
Iteration 128, loss = 0.00936461
Iteration 129, loss = 0.00929608
Iteration 130, loss = 0.00924101
Iteration 131, loss = 0.00917715
Iteration 132, loss = 0.00917842
Iteration 133, loss = 0.00910310
Iteration 134, loss = 0.00902791
Iteration 135, loss = 0.00900876
Iteration 136, loss = 0.00897672
Iteration 137, loss = 0.00887328
Iteration 138, loss = 0.00888174
Iteration 139, loss = 0.00885864
Iteration 140, loss = 0.00873573
Iteration 141, loss = 0.00871686
Iteration 142, loss = 0.00868331
Iteration 143, loss = 0.00867007
Iteration 144, loss = 0.00862200
Iteration 145, loss = 0.00854495
Iteration 146, loss = 0.00859618
Iteration 147, loss = 0.00851750
Iteration 148, loss = 0.00848816
Iteration 149, loss = 0.00844425
Iteration 150, loss = 0.00842579
Iteration 151, loss = 0.00841012
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67859419
Iteration 2, loss = 0.65146160
Iteration 3, loss = 0.61118697
Iteration 4, loss = 0.55294413
Iteration 5, loss = 0.48533123
Iteration 6, loss = 0.41741232
Iteration 7, loss = 0.35307263
Iteration 8, loss = 0.29580573
Iteration 9, loss = 0.24647092
Iteration 10, loss = 0.20617448
Iteration 11, loss = 0.17345231
Iteration 12, loss = 0.14755353
Iteration 13, loss = 0.12692674
Iteration 14, loss = 0.11035185
Iteration 15, loss = 0.09710664
Iteration 16, loss = 0.08638938
Iteration 17, loss = 0.07775987
Iteration 18, loss = 0.07055133
Iteration 19, loss = 0.06445617
Iteration 20, loss = 0.05930858
Iteration 21, loss = 0.05517492
Iteration 22, loss = 0.05150564
Iteration 23, loss = 0.04844524
Iteration 24, loss = 0.04564368
Iteration 25, loss = 0.04319633
Iteration 26, loss = 0.04100709
Iteration 27, loss = 0.03924202
Iteration 28, loss = 0.03750798
Iteration 29, loss = 0.03605759
Iteration 30, loss = 0.03481918
Iteration 31, loss = 0.03352575
Iteration 32, loss = 0.03265133
Iteration 33, loss = 0.03164806
Iteration 34, loss = 0.03053731
Iteration 35, loss = 0.02972964
Iteration 36, loss = 0.02912424
Iteration 37, loss = 0.02840989
Iteration 38, loss = 0.02782383
Iteration 39, loss = 0.02712607
Iteration 40, loss = 0.02680017
Iteration 41, loss = 0.02620800
Iteration 42, loss = 0.02575664
Iteration 43, loss = 0.02537903
Iteration 44, loss = 0.02496609
Iteration 45, loss = 0.02470923
Iteration 46, loss = 0.02421724
Iteration 47, loss = 0.02396388
Iteration 48, loss = 0.02381068
Iteration 49, loss = 0.02342638
Iteration 50, loss = 0.02309475
Iteration 51, loss = 0.02292671
Iteration 52, loss = 0.02274428
Iteration 53, loss = 0.02245272
Iteration 54, loss = 0.02226084
Iteration 55, loss = 0.02195066
Iteration 56, loss = 0.02204821
Iteration 57, loss = 0.02164731
Iteration 58, loss = 0.02145049
Iteration 59, loss = 0.02140017
Iteration 60, loss = 0.02130074
Iteration 61, loss = 0.02096527
Iteration 62, loss = 0.02089739
Iteration 63, loss = 0.02085030
Iteration 64, loss = 0.02062961
Iteration 65, loss = 0.02059133
Iteration 66, loss = 0.02036877
Iteration 67, loss = 0.02029434
Iteration 68, loss = 0.02036985
Iteration 69, loss = 0.02021002
Iteration 70, loss = 0.02019532
Iteration 71, loss = 0.02011443
Iteration 72, loss = 0.01986890
Iteration 73, loss = 0.01981226
Iteration 74, loss = 0.01975234
Iteration 75, loss = 0.01959722
Iteration 76, loss = 0.01964942
Iteration 77, loss = 0.01964788
Iteration 78, loss = 0.01957124
Iteration 79, loss = 0.01949073
Iteration 80, loss = 0.01942472
Iteration 81, loss = 0.01912512
Iteration 82, loss = 0.01921278
Iteration 83, loss = 0.01909162
Iteration 84, loss = 0.01903489
Iteration 85, loss = 0.01894909
Iteration 86, loss = 0.01911909
Iteration 87, loss = 0.01886901
Iteration 88, loss = 0.01893213
Iteration 89, loss = 0.01876050
Iteration 90, loss = 0.01875974
Iteration 91, loss = 0.01883527
Iteration 92, loss = 0.01875363
Iteration 93, loss = 0.01862445
Iteration 94, loss = 0.01864311
Iteration 95, loss = 0.01868427
Iteration 96, loss = 0.01868696
Iteration 97, loss = 0.01854592
Iteration 98, loss = 0.01871125
Iteration 99, loss = 0.01840093
Iteration 100, loss = 0.01850219
Iteration 101, loss = 0.01844421
Iteration 102, loss = 0.01848119
Iteration 103, loss = 0.01850982
Iteration 104, loss = 0.01839809
Iteration 105, loss = 0.01834589
Iteration 106, loss = 0.01810086
Iteration 107, loss = 0.01826673
Iteration 108, loss = 0.01829337
Iteration 109, loss = 0.01818985
Iteration 110, loss = 0.01817904
Iteration 111, loss = 0.01825842
Iteration 112, loss = 0.01816268
Iteration 113, loss = 0.01808507
Iteration 114, loss = 0.01800923
Iteration 115, loss = 0.01808655
Iteration 116, loss = 0.01804829
Iteration 117, loss = 0.01788922
Iteration 118, loss = 0.01809513
Iteration 119, loss = 0.01797670
Iteration 120, loss = 0.01787851
Iteration 121, loss = 0.01779076
Iteration 122, loss = 0.01788870
Iteration 123, loss = 0.01781944
Iteration 124, loss = 0.01792679
Iteration 125, loss = 0.01781560
Iteration 126, loss = 0.01784288
Iteration 127, loss = 0.01774810
Iteration 128, loss = 0.01789663
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67678683
Iteration 2, loss = 0.64906883
Iteration 3, loss = 0.61407167
Iteration 4, loss = 0.56587212
Iteration 5, loss = 0.50932103
Iteration 6, loss = 0.45004295
Iteration 7, loss = 0.39165931
Iteration 8, loss = 0.33729592
Iteration 9, loss = 0.28859084
Iteration 10, loss = 0.24603695
Iteration 11, loss = 0.21012773
Iteration 12, loss = 0.18028262
Iteration 13, loss = 0.15551807
Iteration 14, loss = 0.13531636
Iteration 15, loss = 0.11870799
Iteration 16, loss = 0.10524612
Iteration 17, loss = 0.09393475
Iteration 18, loss = 0.08430901
Iteration 19, loss = 0.07655089
Iteration 20, loss = 0.06983892
Iteration 21, loss = 0.06419083
Iteration 22, loss = 0.05924987
Iteration 23, loss = 0.05514079
Iteration 24, loss = 0.05140143
Iteration 25, loss = 0.04821034
Iteration 26, loss = 0.04545222
Iteration 27, loss = 0.04286236
Iteration 28, loss = 0.04075329
Iteration 29, loss = 0.03881132
Iteration 30, loss = 0.03698546
Iteration 31, loss = 0.03534712
Iteration 32, loss = 0.03402692
Iteration 33, loss = 0.03264297
Iteration 34, loss = 0.03145818
Iteration 35, loss = 0.03047038
Iteration 36, loss = 0.02943730
Iteration 37, loss = 0.02853542
Iteration 38, loss = 0.02768186
Iteration 39, loss = 0.02692950
Iteration 40, loss = 0.02613481
Iteration 41, loss = 0.02566208
Iteration 42, loss = 0.02495242
Iteration 43, loss = 0.02448462
Iteration 44, loss = 0.02381598
Iteration 45, loss = 0.02337886
Iteration 46, loss = 0.02291461
Iteration 47, loss = 0.02253739
Iteration 48, loss = 0.02206261
Iteration 49, loss = 0.02174171
Iteration 50, loss = 0.02150999
Iteration 51, loss = 0.02095538
Iteration 52, loss = 0.02073808
Iteration 53, loss = 0.02050953
Iteration 54, loss = 0.02024820
Iteration 55, loss = 0.01986266
Iteration 56, loss = 0.01964913
Iteration 57, loss = 0.01939398
Iteration 58, loss = 0.01915441
Iteration 59, loss = 0.01904299
Iteration 60, loss = 0.01881677
Iteration 61, loss = 0.01856204
Iteration 62, loss = 0.01837885
Iteration 63, loss = 0.01821419
Iteration 64, loss = 0.01807492
Iteration 65, loss = 0.01806445
Iteration 66, loss = 0.01784677
Iteration 67, loss = 0.01766929
Iteration 68, loss = 0.01761152
Iteration 69, loss = 0.01760280
Iteration 70, loss = 0.01740296
Iteration 71, loss = 0.01701596
Iteration 72, loss = 0.01700280
Iteration 73, loss = 0.01679671
Iteration 74, loss = 0.01676876
Iteration 75, loss = 0.01664953
Iteration 76, loss = 0.01656388
Iteration 77, loss = 0.01652739
Iteration 78, loss = 0.01629572
Iteration 79, loss = 0.01631719
Iteration 80, loss = 0.01620887
Iteration 81, loss = 0.01621140
Iteration 82, loss = 0.01620836
Iteration 83, loss = 0.01608570
Iteration 84, loss = 0.01601052
Iteration 85, loss = 0.01590425
Iteration 86, loss = 0.01580851
Iteration 87, loss = 0.01583984
Iteration 88, loss = 0.01565737
Iteration 89, loss = 0.01560302
Iteration 90, loss = 0.01551809
Iteration 91, loss = 0.01547775
Iteration 92, loss = 0.01537096
Iteration 93, loss = 0.01568411
Iteration 94, loss = 0.01543021
Iteration 95, loss = 0.01538831
Iteration 96, loss = 0.01528443
Iteration 97, loss = 0.01541250
Iteration 98, loss = 0.01513741
Iteration 99, loss = 0.01537901
Iteration 100, loss = 0.01516408
Iteration 101, loss = 0.01504736
Iteration 102, loss = 0.01515266
Iteration 103, loss = 0.01494309
Iteration 104, loss = 0.01491527
Iteration 105, loss = 0.01506133
Iteration 106, loss = 0.01483244
Iteration 107, loss = 0.01489022
Iteration 108, loss = 0.01493406
Iteration 109, loss = 0.01476406
Iteration 110, loss = 0.01478891
Iteration 111, loss = 0.01471165
Iteration 112, loss = 0.01463325
Iteration 113, loss = 0.01471884
Iteration 114, loss = 0.01471203
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68538115
Iteration 2, loss = 0.65010202
Iteration 3, loss = 0.60589265
Iteration 4, loss = 0.54914125
Iteration 5, loss = 0.48691065
Iteration 6, loss = 0.42612325
Iteration 7, loss = 0.36908941
Iteration 8, loss = 0.31786169
Iteration 9, loss = 0.27266229
Iteration 10, loss = 0.23382028
Iteration 11, loss = 0.20059010
Iteration 12, loss = 0.17310952
Iteration 13, loss = 0.15029281
Iteration 14, loss = 0.13148797
Iteration 15, loss = 0.11604020
Iteration 16, loss = 0.10299480
Iteration 17, loss = 0.09232325
Iteration 18, loss = 0.08332735
Iteration 19, loss = 0.07585057
Iteration 20, loss = 0.06937500
Iteration 21, loss = 0.06391744
Iteration 22, loss = 0.05908045
Iteration 23, loss = 0.05500732
Iteration 24, loss = 0.05141869
Iteration 25, loss = 0.04828334
Iteration 26, loss = 0.04549720
Iteration 27, loss = 0.04310423
Iteration 28, loss = 0.04103078
Iteration 29, loss = 0.03897951
Iteration 30, loss = 0.03732722
Iteration 31, loss = 0.03574394
Iteration 32, loss = 0.03423354
Iteration 33, loss = 0.03304872
Iteration 34, loss = 0.03181562
Iteration 35, loss = 0.03074205
Iteration 36, loss = 0.02980044
Iteration 37, loss = 0.02892167
Iteration 38, loss = 0.02801662
Iteration 39, loss = 0.02737873
Iteration 40, loss = 0.02657065
Iteration 41, loss = 0.02608349
Iteration 42, loss = 0.02545971
Iteration 43, loss = 0.02481232
Iteration 44, loss = 0.02441615
Iteration 45, loss = 0.02381041
Iteration 46, loss = 0.02334776
Iteration 47, loss = 0.02304454
Iteration 48, loss = 0.02253727
Iteration 49, loss = 0.02227420
Iteration 50, loss = 0.02193003
Iteration 51, loss = 0.02153148
Iteration 52, loss = 0.02133420
Iteration 53, loss = 0.02097688
Iteration 54, loss = 0.02071362
Iteration 55, loss = 0.02045281
Iteration 56, loss = 0.02011379
Iteration 57, loss = 0.01996820
Iteration 58, loss = 0.01979493
Iteration 59, loss = 0.01953762
Iteration 60, loss = 0.01943500
Iteration 61, loss = 0.01930808
Iteration 62, loss = 0.01904831
Iteration 63, loss = 0.01886206
Iteration 64, loss = 0.01874018
Iteration 65, loss = 0.01847687
Iteration 66, loss = 0.01840478
Iteration 67, loss = 0.01818250
Iteration 68, loss = 0.01804368
Iteration 69, loss = 0.01795116
Iteration 70, loss = 0.01788298
Iteration 71, loss = 0.01778991
Iteration 72, loss = 0.01761238
Iteration 73, loss = 0.01746840
Iteration 74, loss = 0.01743374
Iteration 75, loss = 0.01731032
Iteration 76, loss = 0.01727915
Iteration 77, loss = 0.01718626
Iteration 78, loss = 0.01717190
Iteration 79, loss = 0.01697549
Iteration 80, loss = 0.01705295
Iteration 81, loss = 0.01684821
Iteration 82, loss = 0.01671811
Iteration 83, loss = 0.01680660
Iteration 84, loss = 0.01654731
Iteration 85, loss = 0.01658512
Iteration 86, loss = 0.01654882
Iteration 87, loss = 0.01632267
Iteration 88, loss = 0.01652051
Iteration 89, loss = 0.01639670
Iteration 90, loss = 0.01616389
Iteration 91, loss = 0.01638503
Iteration 92, loss = 0.01604021
Iteration 93, loss = 0.01611975
Iteration 94, loss = 0.01598384
Iteration 95, loss = 0.01593006
Iteration 96, loss = 0.01593942
Iteration 97, loss = 0.01598775
Iteration 98, loss = 0.01591099
Iteration 99, loss = 0.01603708
Iteration 100, loss = 0.01589251
Iteration 101, loss = 0.01579978
Iteration 102, loss = 0.01564375
Iteration 103, loss = 0.01563787
Iteration 104, loss = 0.01564228
Iteration 105, loss = 0.01554353
Iteration 106, loss = 0.01560022
Iteration 107, loss = 0.01552565
Iteration 108, loss = 0.01550178
Iteration 109, loss = 0.01549099
Iteration 110, loss = 0.01532483
Iteration 111, loss = 0.01534658
Iteration 112, loss = 0.01534242
Iteration 113, loss = 0.01534265
Iteration 114, loss = 0.01536637
Iteration 115, loss = 0.01531648
Iteration 116, loss = 0.01524539
Iteration 117, loss = 0.01521997
Iteration 118, loss = 0.01519036
Iteration 119, loss = 0.01530755
Iteration 120, loss = 0.01530182
Iteration 121, loss = 0.01526701
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68495590
Iteration 2, loss = 0.63753598
Iteration 3, loss = 0.56497124
Iteration 4, loss = 0.48048366
Iteration 5, loss = 0.39778447
Iteration 6, loss = 0.32485822
Iteration 7, loss = 0.26368925
Iteration 8, loss = 0.21458039
Iteration 9, loss = 0.17587122
Iteration 10, loss = 0.14615878
Iteration 11, loss = 0.12333015
Iteration 12, loss = 0.10568949
Iteration 13, loss = 0.09166190
Iteration 14, loss = 0.08060938
Iteration 15, loss = 0.07173904
Iteration 16, loss = 0.06470835
Iteration 17, loss = 0.05872826
Iteration 18, loss = 0.05385417
Iteration 19, loss = 0.04989529
Iteration 20, loss = 0.04632440
Iteration 21, loss = 0.04344051
Iteration 22, loss = 0.04076071
Iteration 23, loss = 0.03842104
Iteration 24, loss = 0.03662721
Iteration 25, loss = 0.03488055
Iteration 26, loss = 0.03351616
Iteration 27, loss = 0.03213259
Iteration 28, loss = 0.03088155
Iteration 29, loss = 0.03003111
Iteration 30, loss = 0.02890759
Iteration 31, loss = 0.02816994
Iteration 32, loss = 0.02744656
Iteration 33, loss = 0.02685499
Iteration 34, loss = 0.02589631
Iteration 35, loss = 0.02540867
Iteration 36, loss = 0.02503952
Iteration 37, loss = 0.02426174
Iteration 38, loss = 0.02419325
Iteration 39, loss = 0.02361408
Iteration 40, loss = 0.02312893
Iteration 41, loss = 0.02271922
Iteration 42, loss = 0.02230029
Iteration 43, loss = 0.02196983
Iteration 44, loss = 0.02191841
Iteration 45, loss = 0.02159599
Iteration 46, loss = 0.02132347
Iteration 47, loss = 0.02103045
Iteration 48, loss = 0.02085110
Iteration 49, loss = 0.02061992
Iteration 50, loss = 0.02037055
Iteration 51, loss = 0.02017795
Iteration 52, loss = 0.02006877
Iteration 53, loss = 0.01980874
Iteration 54, loss = 0.01974150
Iteration 55, loss = 0.01949066
Iteration 56, loss = 0.01936007
Iteration 57, loss = 0.01925925
Iteration 58, loss = 0.01912789
Iteration 59, loss = 0.01913442
Iteration 60, loss = 0.01888791
Iteration 61, loss = 0.01897852
Iteration 62, loss = 0.01884925
Iteration 63, loss = 0.01901435
Iteration 64, loss = 0.01847972
Iteration 65, loss = 0.01839569
Iteration 66, loss = 0.01814835
Iteration 67, loss = 0.01827122
Iteration 68, loss = 0.01830241
Iteration 69, loss = 0.01835477
Iteration 70, loss = 0.01795523
Iteration 71, loss = 0.01785972
Iteration 72, loss = 0.01781823
Iteration 73, loss = 0.01766670
Iteration 74, loss = 0.01789193
Iteration 75, loss = 0.01775872
Iteration 76, loss = 0.01778988
Iteration 77, loss = 0.01781562
Iteration 78, loss = 0.01773278
Iteration 79, loss = 0.01750973
Iteration 80, loss = 0.01770204
Iteration 81, loss = 0.01758341
Iteration 82, loss = 0.01742350
Iteration 83, loss = 0.01745158
Iteration 84, loss = 0.01739429
Iteration 85, loss = 0.01733100
Iteration 86, loss = 0.01743840
Iteration 87, loss = 0.01742730
Iteration 88, loss = 0.01717509
Iteration 89, loss = 0.01730484
Iteration 90, loss = 0.01736860
Iteration 91, loss = 0.01708868
Iteration 92, loss = 0.01718877
Iteration 93, loss = 0.01719361
Iteration 94, loss = 0.01697307
Iteration 95, loss = 0.01687571
Iteration 96, loss = 0.01697725
Iteration 97, loss = 0.01716025
Iteration 98, loss = 0.01691637
Iteration 99, loss = 0.01687134
Iteration 100, loss = 0.01697875
Iteration 101, loss = 0.01713860
Iteration 102, loss = 0.01697594
Iteration 103, loss = 0.01679468
Iteration 104, loss = 0.01672725
Iteration 105, loss = 0.01656075
Iteration 106, loss = 0.01666746
Iteration 107, loss = 0.01669985
Iteration 108, loss = 0.01672981
Iteration 109, loss = 0.01677001
Iteration 110, loss = 0.01695539
Iteration 111, loss = 0.01679212
Iteration 112, loss = 0.01672446
Iteration 113, loss = 0.01653522
Iteration 114, loss = 0.01655458
Iteration 115, loss = 0.01658140
Iteration 116, loss = 0.01664428
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68642178
Iteration 2, loss = 0.64210106
Iteration 3, loss = 0.58000413
Iteration 4, loss = 0.50267899
Iteration 5, loss = 0.42321026
Iteration 6, loss = 0.34894703
Iteration 7, loss = 0.28510067
Iteration 8, loss = 0.23274856
Iteration 9, loss = 0.19142055
Iteration 10, loss = 0.15943368
Iteration 11, loss = 0.13496268
Iteration 12, loss = 0.11628209
Iteration 13, loss = 0.10142468
Iteration 14, loss = 0.08983416
Iteration 15, loss = 0.08056359
Iteration 16, loss = 0.07326328
Iteration 17, loss = 0.06725262
Iteration 18, loss = 0.06185929
Iteration 19, loss = 0.05767764
Iteration 20, loss = 0.05410545
Iteration 21, loss = 0.05097992
Iteration 22, loss = 0.04866679
Iteration 23, loss = 0.04628637
Iteration 24, loss = 0.04433697
Iteration 25, loss = 0.04249016
Iteration 26, loss = 0.04100822
Iteration 27, loss = 0.03955004
Iteration 28, loss = 0.03842868
Iteration 29, loss = 0.03732822
Iteration 30, loss = 0.03636630
Iteration 31, loss = 0.03542823
Iteration 32, loss = 0.03471238
Iteration 33, loss = 0.03412620
Iteration 34, loss = 0.03359940
Iteration 35, loss = 0.03285470
Iteration 36, loss = 0.03222445
Iteration 37, loss = 0.03181712
Iteration 38, loss = 0.03134913
Iteration 39, loss = 0.03091737
Iteration 40, loss = 0.03058811
Iteration 41, loss = 0.03000475
Iteration 42, loss = 0.02985291
Iteration 43, loss = 0.02970820
Iteration 44, loss = 0.02907598
Iteration 45, loss = 0.02910491
Iteration 46, loss = 0.02896448
Iteration 47, loss = 0.02867957
Iteration 48, loss = 0.02831526
Iteration 49, loss = 0.02818790
Iteration 50, loss = 0.02783839
Iteration 51, loss = 0.02772295
Iteration 52, loss = 0.02738438
Iteration 53, loss = 0.02730893
Iteration 54, loss = 0.02718995
Iteration 55, loss = 0.02713132
Iteration 56, loss = 0.02692444
Iteration 57, loss = 0.02683555
Iteration 58, loss = 0.02669638
Iteration 59, loss = 0.02664453
Iteration 60, loss = 0.02631528
Iteration 61, loss = 0.02612591
Iteration 62, loss = 0.02615477
Iteration 63, loss = 0.02606464
Iteration 64, loss = 0.02594086
Iteration 65, loss = 0.02592212
Iteration 66, loss = 0.02606822
Iteration 67, loss = 0.02574554
Iteration 68, loss = 0.02602877
Iteration 69, loss = 0.02566154
Iteration 70, loss = 0.02551965
Iteration 71, loss = 0.02546574
Iteration 72, loss = 0.02555422
Iteration 73, loss = 0.02537803
Iteration 74, loss = 0.02529993
Iteration 75, loss = 0.02512847
Iteration 76, loss = 0.02537898
Iteration 77, loss = 0.02519591
Iteration 78, loss = 0.02524707
Iteration 79, loss = 0.02512431
Iteration 80, loss = 0.02502028
Iteration 81, loss = 0.02485188
Iteration 82, loss = 0.02469273
Iteration 83, loss = 0.02470570
Iteration 84, loss = 0.02488886
Iteration 85, loss = 0.02486667
Iteration 86, loss = 0.02490473
Iteration 87, loss = 0.02482926
Iteration 88, loss = 0.02480102
Iteration 89, loss = 0.02476404
Iteration 90, loss = 0.02471261
Iteration 91, loss = 0.02452796
Iteration 92, loss = 0.02454319
Iteration 93, loss = 0.02447480
Iteration 94, loss = 0.02464912
Iteration 95, loss = 0.02463098
Iteration 96, loss = 0.02473306
Iteration 97, loss = 0.02452448
Iteration 98, loss = 0.02428536
Iteration 99, loss = 0.02443080
Iteration 100, loss = 0.02442939
Iteration 101, loss = 0.02429445
Iteration 102, loss = 0.02448031
Iteration 103, loss = 0.02443199
Iteration 104, loss = 0.02413803
Iteration 105, loss = 0.02434560
Iteration 106, loss = 0.02415123
Iteration 107, loss = 0.02439290
Iteration 108, loss = 0.02408298
Iteration 109, loss = 0.02392107
Iteration 110, loss = 0.02459558
Iteration 111, loss = 0.02420697
Iteration 112, loss = 0.02402108
Iteration 113, loss = 0.02387175
Iteration 114, loss = 0.02400546
Iteration 115, loss = 0.02406139
Iteration 116, loss = 0.02377939
Iteration 117, loss = 0.02406085
Iteration 118, loss = 0.02407636
Iteration 119, loss = 0.02400643
Iteration 120, loss = 0.02428746
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68396739
Iteration 2, loss = 0.62853871
Iteration 3, loss = 0.55591531
Iteration 4, loss = 0.47297885
Iteration 5, loss = 0.39295961
Iteration 6, loss = 0.32154718
Iteration 7, loss = 0.26126315
Iteration 8, loss = 0.21279180
Iteration 9, loss = 0.17491245
Iteration 10, loss = 0.14561572
Iteration 11, loss = 0.12325317
Iteration 12, loss = 0.10585432
Iteration 13, loss = 0.09243102
Iteration 14, loss = 0.08172149
Iteration 15, loss = 0.07350787
Iteration 16, loss = 0.06652283
Iteration 17, loss = 0.06093414
Iteration 18, loss = 0.05644748
Iteration 19, loss = 0.05238473
Iteration 20, loss = 0.04911249
Iteration 21, loss = 0.04651165
Iteration 22, loss = 0.04400891
Iteration 23, loss = 0.04198437
Iteration 24, loss = 0.04016234
Iteration 25, loss = 0.03865585
Iteration 26, loss = 0.03719901
Iteration 27, loss = 0.03611648
Iteration 28, loss = 0.03489223
Iteration 29, loss = 0.03381396
Iteration 30, loss = 0.03286552
Iteration 31, loss = 0.03233008
Iteration 32, loss = 0.03145098
Iteration 33, loss = 0.03068716
Iteration 34, loss = 0.03016941
Iteration 35, loss = 0.02987492
Iteration 36, loss = 0.02919280
Iteration 37, loss = 0.02901191
Iteration 38, loss = 0.02826115
Iteration 39, loss = 0.02784233
Iteration 40, loss = 0.02749159
Iteration 41, loss = 0.02721303
Iteration 42, loss = 0.02696122
Iteration 43, loss = 0.02667742
Iteration 44, loss = 0.02639188
Iteration 45, loss = 0.02605597
Iteration 46, loss = 0.02602189
Iteration 47, loss = 0.02571661
Iteration 48, loss = 0.02541760
Iteration 49, loss = 0.02525850
Iteration 50, loss = 0.02521465
Iteration 51, loss = 0.02478354
Iteration 52, loss = 0.02485974
Iteration 53, loss = 0.02469850
Iteration 54, loss = 0.02489113
Iteration 55, loss = 0.02420503
Iteration 56, loss = 0.02414249
Iteration 57, loss = 0.02404069
Iteration 58, loss = 0.02391187
Iteration 59, loss = 0.02393708
Iteration 60, loss = 0.02391938
Iteration 61, loss = 0.02364498
Iteration 62, loss = 0.02375537
Iteration 63, loss = 0.02371271
Iteration 64, loss = 0.02346003
Iteration 65, loss = 0.02338774
Iteration 66, loss = 0.02327582
Iteration 67, loss = 0.02326791
Iteration 68, loss = 0.02342936
Iteration 69, loss = 0.02326440
Iteration 70, loss = 0.02301155
Iteration 71, loss = 0.02290085
Iteration 72, loss = 0.02285212
Iteration 73, loss = 0.02278868
Iteration 74, loss = 0.02292388
Iteration 75, loss = 0.02255001
Iteration 76, loss = 0.02272933
Iteration 77, loss = 0.02267847
Iteration 78, loss = 0.02259205
Iteration 79, loss = 0.02241218
Iteration 80, loss = 0.02259363
Iteration 81, loss = 0.02257149
Iteration 82, loss = 0.02227030
Iteration 83, loss = 0.02249223
Iteration 84, loss = 0.02231982
Iteration 85, loss = 0.02229216
Iteration 86, loss = 0.02223095
Iteration 87, loss = 0.02211898
Iteration 88, loss = 0.02240990
Iteration 89, loss = 0.02227405
Iteration 90, loss = 0.02214394
Iteration 91, loss = 0.02226694
Iteration 92, loss = 0.02210738
Iteration 93, loss = 0.02212544
Iteration 94, loss = 0.02205771
Iteration 95, loss = 0.02190748
Iteration 96, loss = 0.02209324
Iteration 97, loss = 0.02206633
Iteration 98, loss = 0.02200617
Iteration 99, loss = 0.02206754
Iteration 100, loss = 0.02181311
Iteration 101, loss = 0.02210989
Iteration 102, loss = 0.02211966
Iteration 103, loss = 0.02180757
Iteration 104, loss = 0.02180015
Iteration 105, loss = 0.02189056
Iteration 106, loss = 0.02181586
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68220574
Iteration 2, loss = 0.64144961
Iteration 3, loss = 0.57715504
Iteration 4, loss = 0.49294822
Iteration 5, loss = 0.40535513
Iteration 6, loss = 0.32572209
Iteration 7, loss = 0.25930177
Iteration 8, loss = 0.20695248
Iteration 9, loss = 0.16762649
Iteration 10, loss = 0.13836377
Iteration 11, loss = 0.11659321
Iteration 12, loss = 0.09991855
Iteration 13, loss = 0.08728826
Iteration 14, loss = 0.07743644
Iteration 15, loss = 0.06944231
Iteration 16, loss = 0.06326244
Iteration 17, loss = 0.05802802
Iteration 18, loss = 0.05389096
Iteration 19, loss = 0.05036030
Iteration 20, loss = 0.04726383
Iteration 21, loss = 0.04492998
Iteration 22, loss = 0.04256996
Iteration 23, loss = 0.04061338
Iteration 24, loss = 0.03923093
Iteration 25, loss = 0.03765744
Iteration 26, loss = 0.03666467
Iteration 27, loss = 0.03548478
Iteration 28, loss = 0.03443948
Iteration 29, loss = 0.03343955
Iteration 30, loss = 0.03289631
Iteration 31, loss = 0.03190919
Iteration 32, loss = 0.03124265
Iteration 33, loss = 0.03056417
Iteration 34, loss = 0.03021987
Iteration 35, loss = 0.02959113
Iteration 36, loss = 0.02926253
Iteration 37, loss = 0.02893722
Iteration 38, loss = 0.02848123
Iteration 39, loss = 0.02825166
Iteration 40, loss = 0.02756207
Iteration 41, loss = 0.02764515
Iteration 42, loss = 0.02747402
Iteration 43, loss = 0.02731821
Iteration 44, loss = 0.02683699
Iteration 45, loss = 0.02661939
Iteration 46, loss = 0.02658098
Iteration 47, loss = 0.02617055
Iteration 48, loss = 0.02608482
Iteration 49, loss = 0.02575262
Iteration 50, loss = 0.02570528
Iteration 51, loss = 0.02537473
Iteration 52, loss = 0.02520118
Iteration 53, loss = 0.02521354
Iteration 54, loss = 0.02505003
Iteration 55, loss = 0.02505514
Iteration 56, loss = 0.02488548
Iteration 57, loss = 0.02479694
Iteration 58, loss = 0.02455460
Iteration 59, loss = 0.02447110
Iteration 60, loss = 0.02446873
Iteration 61, loss = 0.02448605
Iteration 62, loss = 0.02423480
Iteration 63, loss = 0.02425851
Iteration 64, loss = 0.02400339
Iteration 65, loss = 0.02423455
Iteration 66, loss = 0.02421711
Iteration 67, loss = 0.02405588
Iteration 68, loss = 0.02385933
Iteration 69, loss = 0.02392184
Iteration 70, loss = 0.02392368
Iteration 71, loss = 0.02356538
Iteration 72, loss = 0.02368140
Iteration 73, loss = 0.02397075
Iteration 74, loss = 0.02349439
Iteration 75, loss = 0.02346656
Iteration 76, loss = 0.02335154
Iteration 77, loss = 0.02335641
Iteration 78, loss = 0.02344058
Iteration 79, loss = 0.02349316
Iteration 80, loss = 0.02333377
Iteration 81, loss = 0.02335159
Iteration 82, loss = 0.02334762
Iteration 83, loss = 0.02339640
Iteration 84, loss = 0.02320439
Iteration 85, loss = 0.02316812
Iteration 86, loss = 0.02325823
Iteration 87, loss = 0.02315870
Iteration 88, loss = 0.02320693
Iteration 89, loss = 0.02306457
Iteration 90, loss = 0.02305651
Iteration 91, loss = 0.02322228
Iteration 92, loss = 0.02282471
Iteration 93, loss = 0.02310041
Iteration 94, loss = 0.02309053
Iteration 95, loss = 0.02302803
Iteration 96, loss = 0.02297818
Iteration 97, loss = 0.02295850
Iteration 98, loss = 0.02306537
Iteration 99, loss = 0.02271756
Iteration 100, loss = 0.02296342
Iteration 101, loss = 0.02291324
Iteration 102, loss = 0.02297485
Iteration 103, loss = 0.02288477
Iteration 104, loss = 0.02262685
Iteration 105, loss = 0.02254738
Iteration 106, loss = 0.02285585
Iteration 107, loss = 0.02286046
Iteration 108, loss = 0.02270758
Iteration 109, loss = 0.02282799
Iteration 110, loss = 0.02263553
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67949547
Iteration 2, loss = 0.64676714
Iteration 3, loss = 0.60223011
Iteration 4, loss = 0.54212799
Iteration 5, loss = 0.47187192
Iteration 6, loss = 0.40205685
Iteration 7, loss = 0.33823985
Iteration 8, loss = 0.28260683
Iteration 9, loss = 0.23625711
Iteration 10, loss = 0.19852850
Iteration 11, loss = 0.16818580
Iteration 12, loss = 0.14420320
Iteration 13, loss = 0.12496012
Iteration 14, loss = 0.10953375
Iteration 15, loss = 0.09692423
Iteration 16, loss = 0.08688908
Iteration 17, loss = 0.07852915
Iteration 18, loss = 0.07159248
Iteration 19, loss = 0.06571913
Iteration 20, loss = 0.06080544
Iteration 21, loss = 0.05659985
Iteration 22, loss = 0.05297474
Iteration 23, loss = 0.04976664
Iteration 24, loss = 0.04705112
Iteration 25, loss = 0.04459653
Iteration 26, loss = 0.04249415
Iteration 27, loss = 0.04068026
Iteration 28, loss = 0.03898289
Iteration 29, loss = 0.03756907
Iteration 30, loss = 0.03612826
Iteration 31, loss = 0.03480684
Iteration 32, loss = 0.03383378
Iteration 33, loss = 0.03279063
Iteration 34, loss = 0.03197351
Iteration 35, loss = 0.03117622
Iteration 36, loss = 0.03026516
Iteration 37, loss = 0.02975698
Iteration 38, loss = 0.02888394
Iteration 39, loss = 0.02852395
Iteration 40, loss = 0.02781451
Iteration 41, loss = 0.02733807
Iteration 42, loss = 0.02684412
Iteration 43, loss = 0.02645384
Iteration 44, loss = 0.02610251
Iteration 45, loss = 0.02557540
Iteration 46, loss = 0.02518904
Iteration 47, loss = 0.02491429
Iteration 48, loss = 0.02465064
Iteration 49, loss = 0.02416554
Iteration 50, loss = 0.02386449
Iteration 51, loss = 0.02375535
Iteration 52, loss = 0.02361059
Iteration 53, loss = 0.02328495
Iteration 54, loss = 0.02290824
Iteration 55, loss = 0.02286153
Iteration 56, loss = 0.02252498
Iteration 57, loss = 0.02240319
Iteration 58, loss = 0.02228433
Iteration 59, loss = 0.02211503
Iteration 60, loss = 0.02197372
Iteration 61, loss = 0.02181771
Iteration 62, loss = 0.02159539
Iteration 63, loss = 0.02144024
Iteration 64, loss = 0.02131721
Iteration 65, loss = 0.02132309
Iteration 66, loss = 0.02108665
Iteration 67, loss = 0.02106016
Iteration 68, loss = 0.02082484
Iteration 69, loss = 0.02076116
Iteration 70, loss = 0.02075486
Iteration 71, loss = 0.02070577
Iteration 72, loss = 0.02054233
Iteration 73, loss = 0.02049007
Iteration 74, loss = 0.02054324
Iteration 75, loss = 0.02030236
Iteration 76, loss = 0.02025817
Iteration 77, loss = 0.02007817
Iteration 78, loss = 0.01997816
Iteration 79, loss = 0.01984126
Iteration 80, loss = 0.01990978
Iteration 81, loss = 0.01980395
Iteration 82, loss = 0.01963504
Iteration 83, loss = 0.01976661
Iteration 84, loss = 0.01960448
Iteration 85, loss = 0.01960090
Iteration 86, loss = 0.01952229
Iteration 87, loss = 0.01940201
Iteration 88, loss = 0.01948479
Iteration 89, loss = 0.01952246
Iteration 90, loss = 0.01933931
Iteration 91, loss = 0.01938749
Iteration 92, loss = 0.01908667
Iteration 93, loss = 0.01930102
Iteration 94, loss = 0.01917855
Iteration 95, loss = 0.01904416
Iteration 96, loss = 0.01908662
Iteration 97, loss = 0.01894516
Iteration 98, loss = 0.01901551
Iteration 99, loss = 0.01896634
Iteration 100, loss = 0.01899946
Iteration 101, loss = 0.01886769
Iteration 102, loss = 0.01890438
Iteration 103, loss = 0.01894924
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69393876
Iteration 2, loss = 0.66006817
Iteration 3, loss = 0.61513513
Iteration 4, loss = 0.55394449
Iteration 5, loss = 0.48160302
Iteration 6, loss = 0.41137768
Iteration 7, loss = 0.34740977
Iteration 8, loss = 0.29258255
Iteration 9, loss = 0.24615311
Iteration 10, loss = 0.20837843
Iteration 11, loss = 0.17724370
Iteration 12, loss = 0.15245031
Iteration 13, loss = 0.13214375
Iteration 14, loss = 0.11600294
Iteration 15, loss = 0.10248678
Iteration 16, loss = 0.09172831
Iteration 17, loss = 0.08276115
Iteration 18, loss = 0.07499351
Iteration 19, loss = 0.06884771
Iteration 20, loss = 0.06331825
Iteration 21, loss = 0.05894646
Iteration 22, loss = 0.05483680
Iteration 23, loss = 0.05149560
Iteration 24, loss = 0.04852899
Iteration 25, loss = 0.04586320
Iteration 26, loss = 0.04352351
Iteration 27, loss = 0.04141878
Iteration 28, loss = 0.03968214
Iteration 29, loss = 0.03816806
Iteration 30, loss = 0.03670476
Iteration 31, loss = 0.03540980
Iteration 32, loss = 0.03411194
Iteration 33, loss = 0.03311580
Iteration 34, loss = 0.03217771
Iteration 35, loss = 0.03138282
Iteration 36, loss = 0.03052391
Iteration 37, loss = 0.02969776
Iteration 38, loss = 0.02906740
Iteration 39, loss = 0.02858470
Iteration 40, loss = 0.02785752
Iteration 41, loss = 0.02723736
Iteration 42, loss = 0.02682437
Iteration 43, loss = 0.02627368
Iteration 44, loss = 0.02609650
Iteration 45, loss = 0.02555796
Iteration 46, loss = 0.02506645
Iteration 47, loss = 0.02467180
Iteration 48, loss = 0.02448082
Iteration 49, loss = 0.02417853
Iteration 50, loss = 0.02392131
Iteration 51, loss = 0.02367570
Iteration 52, loss = 0.02334146
Iteration 53, loss = 0.02310231
Iteration 54, loss = 0.02274009
Iteration 55, loss = 0.02259438
Iteration 56, loss = 0.02252902
Iteration 57, loss = 0.02219809
Iteration 58, loss = 0.02203873
Iteration 59, loss = 0.02185437
Iteration 60, loss = 0.02178610
Iteration 61, loss = 0.02152929
Iteration 62, loss = 0.02117793
Iteration 63, loss = 0.02114057
Iteration 64, loss = 0.02107783
Iteration 65, loss = 0.02107659
Iteration 66, loss = 0.02083609
Iteration 67, loss = 0.02076314
Iteration 68, loss = 0.02050613
Iteration 69, loss = 0.02052421
Iteration 70, loss = 0.02039384
Iteration 71, loss = 0.02022547
Iteration 72, loss = 0.02027971
Iteration 73, loss = 0.02009897
Iteration 74, loss = 0.02028577
Iteration 75, loss = 0.01999568
Iteration 76, loss = 0.01976524
Iteration 77, loss = 0.01980605
Iteration 78, loss = 0.01978101
Iteration 79, loss = 0.01980008
Iteration 80, loss = 0.01961208
Iteration 81, loss = 0.01943552
Iteration 82, loss = 0.01949025
Iteration 83, loss = 0.01931646
Iteration 84, loss = 0.01943988
Iteration 85, loss = 0.01934929
Iteration 86, loss = 0.01918812
Iteration 87, loss = 0.01933177
Iteration 88, loss = 0.01928540
Iteration 89, loss = 0.01905689
Iteration 90, loss = 0.01923762
Iteration 91, loss = 0.01914953
Iteration 92, loss = 0.01895007
Iteration 93, loss = 0.01895525
Iteration 94, loss = 0.01884661
Iteration 95, loss = 0.01880814
Iteration 96, loss = 0.01909144
Iteration 97, loss = 0.01885965
Iteration 98, loss = 0.01881343
Iteration 99, loss = 0.01875349
Iteration 100, loss = 0.01856323
Iteration 101, loss = 0.01854821
Iteration 102, loss = 0.01855641
Iteration 103, loss = 0.01854028
Iteration 104, loss = 0.01848864
Iteration 105, loss = 0.01849101
Iteration 106, loss = 0.01843777
Iteration 107, loss = 0.01847937
Iteration 108, loss = 0.01850779
Iteration 109, loss = 0.01835862
Iteration 110, loss = 0.01835906
Iteration 111, loss = 0.01833073
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71020320
Iteration 2, loss = 0.64260421
Iteration 3, loss = 0.54533455
Iteration 4, loss = 0.43237950
Iteration 5, loss = 0.33028877
Iteration 6, loss = 0.24961963
Iteration 7, loss = 0.19091850
Iteration 8, loss = 0.14987391
Iteration 9, loss = 0.12104757
Iteration 10, loss = 0.10064193
Iteration 11, loss = 0.08578415
Iteration 12, loss = 0.07463013
Iteration 13, loss = 0.06625117
Iteration 14, loss = 0.05961995
Iteration 15, loss = 0.05451105
Iteration 16, loss = 0.05010804
Iteration 17, loss = 0.04660999
Iteration 18, loss = 0.04347848
Iteration 19, loss = 0.04120142
Iteration 20, loss = 0.03905573
Iteration 21, loss = 0.03729500
Iteration 22, loss = 0.03597528
Iteration 23, loss = 0.03468764
Iteration 24, loss = 0.03312256
Iteration 25, loss = 0.03234835
Iteration 26, loss = 0.03161235
Iteration 27, loss = 0.03051876
Iteration 28, loss = 0.02992549
Iteration 29, loss = 0.02900601
Iteration 30, loss = 0.02841447
Iteration 31, loss = 0.02807405
Iteration 32, loss = 0.02764300
Iteration 33, loss = 0.02701765
Iteration 34, loss = 0.02662536
Iteration 35, loss = 0.02646937
Iteration 36, loss = 0.02587080
Iteration 37, loss = 0.02568810
Iteration 38, loss = 0.02560496
Iteration 39, loss = 0.02541136
Iteration 40, loss = 0.02508864
Iteration 41, loss = 0.02473444
Iteration 42, loss = 0.02463525
Iteration 43, loss = 0.02472856
Iteration 44, loss = 0.02421514
Iteration 45, loss = 0.02428317
Iteration 46, loss = 0.02417750
Iteration 47, loss = 0.02388802
Iteration 48, loss = 0.02343001
Iteration 49, loss = 0.02363261
Iteration 50, loss = 0.02345361
Iteration 51, loss = 0.02326450
Iteration 52, loss = 0.02320757
Iteration 53, loss = 0.02306289
Iteration 54, loss = 0.02270339
Iteration 55, loss = 0.02283042
Iteration 56, loss = 0.02264151
Iteration 57, loss = 0.02286978
Iteration 58, loss = 0.02244359
Iteration 59, loss = 0.02283281
Iteration 60, loss = 0.02219215
Iteration 61, loss = 0.02237891
Iteration 62, loss = 0.02237003
Iteration 63, loss = 0.02216681
Iteration 64, loss = 0.02247730
Iteration 65, loss = 0.02253038
Iteration 66, loss = 0.02208799
Iteration 67, loss = 0.02219925
Iteration 68, loss = 0.02216773
Iteration 69, loss = 0.02214901
Iteration 70, loss = 0.02209673
Iteration 71, loss = 0.02226000
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68000843
Iteration 2, loss = 0.62495785
Iteration 3, loss = 0.53045953
Iteration 4, loss = 0.42441415
Iteration 5, loss = 0.32785948
Iteration 6, loss = 0.25043229
Iteration 7, loss = 0.19357261
Iteration 8, loss = 0.15327896
Iteration 9, loss = 0.12468317
Iteration 10, loss = 0.10437965
Iteration 11, loss = 0.08969092
Iteration 12, loss = 0.07855547
Iteration 13, loss = 0.07020935
Iteration 14, loss = 0.06351643
Iteration 15, loss = 0.05851827
Iteration 16, loss = 0.05414333
Iteration 17, loss = 0.05084391
Iteration 18, loss = 0.04806228
Iteration 19, loss = 0.04563141
Iteration 20, loss = 0.04343418
Iteration 21, loss = 0.04197006
Iteration 22, loss = 0.04041088
Iteration 23, loss = 0.03905493
Iteration 24, loss = 0.03756895
Iteration 25, loss = 0.03685753
Iteration 26, loss = 0.03596333
Iteration 27, loss = 0.03490962
Iteration 28, loss = 0.03441194
Iteration 29, loss = 0.03385135
Iteration 30, loss = 0.03317445
Iteration 31, loss = 0.03283426
Iteration 32, loss = 0.03247391
Iteration 33, loss = 0.03167416
Iteration 34, loss = 0.03138116
Iteration 35, loss = 0.03143003
Iteration 36, loss = 0.03101204
Iteration 37, loss = 0.03069202
Iteration 38, loss = 0.03011122
Iteration 39, loss = 0.03020736
Iteration 40, loss = 0.02966936
Iteration 41, loss = 0.02967342
Iteration 42, loss = 0.02939895
Iteration 43, loss = 0.02911884
Iteration 44, loss = 0.02901733
Iteration 45, loss = 0.02880610
Iteration 46, loss = 0.02882884
Iteration 47, loss = 0.02851146
Iteration 48, loss = 0.02850472
Iteration 49, loss = 0.02802733
Iteration 50, loss = 0.02834576
Iteration 51, loss = 0.02826904
Iteration 52, loss = 0.02814171
Iteration 53, loss = 0.02792464
Iteration 54, loss = 0.02792102
Iteration 55, loss = 0.02786423
Iteration 56, loss = 0.02773274
Iteration 57, loss = 0.02778758
Iteration 58, loss = 0.02751676
Iteration 59, loss = 0.02762024
Iteration 60, loss = 0.02722140
Iteration 61, loss = 0.02739545
Iteration 62, loss = 0.02712639
Iteration 63, loss = 0.02745452
Iteration 64, loss = 0.02729686
Iteration 65, loss = 0.02720501
Iteration 66, loss = 0.02701708
Iteration 67, loss = 0.02713369
Iteration 68, loss = 0.02670825
Iteration 69, loss = 0.02697209
Iteration 70, loss = 0.02684392
Iteration 71, loss = 0.02675525
Iteration 72, loss = 0.02685067
Iteration 73, loss = 0.02675025
Iteration 74, loss = 0.02684716
Iteration 75, loss = 0.02667039
Iteration 76, loss = 0.02671276
Iteration 77, loss = 0.02661086
Iteration 78, loss = 0.02660639
Iteration 79, loss = 0.02678128
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67658266
Iteration 2, loss = 0.62716463
Iteration 3, loss = 0.54851632
Iteration 4, loss = 0.45690449
Iteration 5, loss = 0.36975838
Iteration 6, loss = 0.29405732
Iteration 7, loss = 0.23289394
Iteration 8, loss = 0.18554861
Iteration 9, loss = 0.15040303
Iteration 10, loss = 0.12413227
Iteration 11, loss = 0.10452749
Iteration 12, loss = 0.08936373
Iteration 13, loss = 0.07796708
Iteration 14, loss = 0.06889793
Iteration 15, loss = 0.06164190
Iteration 16, loss = 0.05586515
Iteration 17, loss = 0.05104093
Iteration 18, loss = 0.04713065
Iteration 19, loss = 0.04389604
Iteration 20, loss = 0.04098192
Iteration 21, loss = 0.03883098
Iteration 22, loss = 0.03637113
Iteration 23, loss = 0.03462982
Iteration 24, loss = 0.03313547
Iteration 25, loss = 0.03176389
Iteration 26, loss = 0.03056703
Iteration 27, loss = 0.02944804
Iteration 28, loss = 0.02854475
Iteration 29, loss = 0.02746869
Iteration 30, loss = 0.02708139
Iteration 31, loss = 0.02615909
Iteration 32, loss = 0.02553363
Iteration 33, loss = 0.02502181
Iteration 34, loss = 0.02437190
Iteration 35, loss = 0.02393994
Iteration 36, loss = 0.02367085
Iteration 37, loss = 0.02318620
Iteration 38, loss = 0.02272875
Iteration 39, loss = 0.02244864
Iteration 40, loss = 0.02202757
Iteration 41, loss = 0.02186204
Iteration 42, loss = 0.02144587
Iteration 43, loss = 0.02125012
Iteration 44, loss = 0.02137831
Iteration 45, loss = 0.02107665
Iteration 46, loss = 0.02071601
Iteration 47, loss = 0.02051013
Iteration 48, loss = 0.02022338
Iteration 49, loss = 0.02014723
Iteration 50, loss = 0.01974131
Iteration 51, loss = 0.01978877
Iteration 52, loss = 0.01959748
Iteration 53, loss = 0.01948443
Iteration 54, loss = 0.01943913
Iteration 55, loss = 0.01941765
Iteration 56, loss = 0.01921526
Iteration 57, loss = 0.01893737
Iteration 58, loss = 0.01900034
Iteration 59, loss = 0.01921252
Iteration 60, loss = 0.01901898
Iteration 61, loss = 0.01867365
Iteration 62, loss = 0.01866517
Iteration 63, loss = 0.01854694
Iteration 64, loss = 0.01850585
Iteration 65, loss = 0.01847873
Iteration 66, loss = 0.01836146
Iteration 67, loss = 0.01810523
Iteration 68, loss = 0.01838505
Iteration 69, loss = 0.01819347
Iteration 70, loss = 0.01801125
Iteration 71, loss = 0.01823853
Iteration 72, loss = 0.01804221
Iteration 73, loss = 0.01796977
Iteration 74, loss = 0.01777772
Iteration 75, loss = 0.01789480
Iteration 76, loss = 0.01777575
Iteration 77, loss = 0.01775906
Iteration 78, loss = 0.01781854
Iteration 79, loss = 0.01782518
Iteration 80, loss = 0.01760200
Iteration 81, loss = 0.01759555
Iteration 82, loss = 0.01768216
Iteration 83, loss = 0.01765854
Iteration 84, loss = 0.01759640
Iteration 85, loss = 0.01768652
Iteration 86, loss = 0.01739201
Iteration 87, loss = 0.01744358
Iteration 88, loss = 0.01727595
Iteration 89, loss = 0.01727779
Iteration 90, loss = 0.01737519
Iteration 91, loss = 0.01729929
Iteration 92, loss = 0.01740912
Iteration 93, loss = 0.01737965
Iteration 94, loss = 0.01733325
Iteration 95, loss = 0.01730860
Iteration 96, loss = 0.01723026
Iteration 97, loss = 0.01715560
Iteration 98, loss = 0.01692007
Iteration 99, loss = 0.01714014
Iteration 100, loss = 0.01702699
Iteration 101, loss = 0.01732532
Iteration 102, loss = 0.01696744
Iteration 103, loss = 0.01704070
Iteration 104, loss = 0.01706925
Iteration 105, loss = 0.01689688
Iteration 106, loss = 0.01700966
Iteration 107, loss = 0.01674207
Iteration 108, loss = 0.01700247
Iteration 109, loss = 0.01692559
Iteration 110, loss = 0.01685703
Iteration 111, loss = 0.01687420
Iteration 112, loss = 0.01681959
Iteration 113, loss = 0.01701153
Iteration 114, loss = 0.01663035
Iteration 115, loss = 0.01680759
Iteration 116, loss = 0.01682295
Iteration 117, loss = 0.01679631
Iteration 118, loss = 0.01682014
Iteration 119, loss = 0.01676968
Iteration 120, loss = 0.01659639
Iteration 121, loss = 0.01660309
Iteration 122, loss = 0.01681972
Iteration 123, loss = 0.01712876
Iteration 124, loss = 0.01676406
Iteration 125, loss = 0.01653138
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68744665
Iteration 2, loss = 0.67526741
Iteration 3, loss = 0.66486208
Iteration 4, loss = 0.65438567
Iteration 5, loss = 0.64300663
Iteration 6, loss = 0.63065087
Iteration 7, loss = 0.61682410
Iteration 8, loss = 0.60158125
Iteration 9, loss = 0.58474061
Iteration 10, loss = 0.56618723
Iteration 11, loss = 0.54615876
Iteration 12, loss = 0.52482981
Iteration 13, loss = 0.50212620
Iteration 14, loss = 0.47847102
Iteration 15, loss = 0.45435821
Iteration 16, loss = 0.43010175
Iteration 17, loss = 0.40598657
Iteration 18, loss = 0.38240670
Iteration 19, loss = 0.35952105
Iteration 20, loss = 0.33763316
Iteration 21, loss = 0.31666441
Iteration 22, loss = 0.29675813
Iteration 23, loss = 0.27791612
Iteration 24, loss = 0.26021850
Iteration 25, loss = 0.24345896
Iteration 26, loss = 0.22784729
Iteration 27, loss = 0.21338195
Iteration 28, loss = 0.19982440
Iteration 29, loss = 0.18726377
Iteration 30, loss = 0.17559416
Iteration 31, loss = 0.16481016
Iteration 32, loss = 0.15484832
Iteration 33, loss = 0.14561396
Iteration 34, loss = 0.13705072
Iteration 35, loss = 0.12916006
Iteration 36, loss = 0.12195780
Iteration 37, loss = 0.11521553
Iteration 38, loss = 0.10899418
Iteration 39, loss = 0.10323131
Iteration 40, loss = 0.09792785
Iteration 41, loss = 0.09302963
Iteration 42, loss = 0.08843045
Iteration 43, loss = 0.08415291
Iteration 44, loss = 0.08023934
Iteration 45, loss = 0.07659779
Iteration 46, loss = 0.07313740
Iteration 47, loss = 0.07001693
Iteration 48, loss = 0.06704795
Iteration 49, loss = 0.06425240
Iteration 50, loss = 0.06167673
Iteration 51, loss = 0.05922384
Iteration 52, loss = 0.05694918
Iteration 53, loss = 0.05479339
Iteration 54, loss = 0.05281648
Iteration 55, loss = 0.05087867
Iteration 56, loss = 0.04910365
Iteration 57, loss = 0.04742425
Iteration 58, loss = 0.04584640
Iteration 59, loss = 0.04433205
Iteration 60, loss = 0.04289899
Iteration 61, loss = 0.04159972
Iteration 62, loss = 0.04027854
Iteration 63, loss = 0.03905494
Iteration 64, loss = 0.03793185
Iteration 65, loss = 0.03681475
Iteration 66, loss = 0.03576414
Iteration 67, loss = 0.03478418
Iteration 68, loss = 0.03383090
Iteration 69, loss = 0.03292352
Iteration 70, loss = 0.03209496
Iteration 71, loss = 0.03125882
Iteration 72, loss = 0.03052560
Iteration 73, loss = 0.02973722
Iteration 74, loss = 0.02902777
Iteration 75, loss = 0.02835594
Iteration 76, loss = 0.02773352
Iteration 77, loss = 0.02707330
Iteration 78, loss = 0.02647490
Iteration 79, loss = 0.02590798
Iteration 80, loss = 0.02533444
Iteration 81, loss = 0.02480690
Iteration 82, loss = 0.02430942
Iteration 83, loss = 0.02381436
Iteration 84, loss = 0.02335046
Iteration 85, loss = 0.02290657
Iteration 86, loss = 0.02243580
Iteration 87, loss = 0.02197657
Iteration 88, loss = 0.02158595
Iteration 89, loss = 0.02117812
Iteration 90, loss = 0.02083620
Iteration 91, loss = 0.02044642
Iteration 92, loss = 0.02012538
Iteration 93, loss = 0.01975015
Iteration 94, loss = 0.01945305
Iteration 95, loss = 0.01909973
Iteration 96, loss = 0.01882126
Iteration 97, loss = 0.01851649
Iteration 98, loss = 0.01822490
Iteration 99, loss = 0.01795219
Iteration 100, loss = 0.01767037
Iteration 101, loss = 0.01741526
Iteration 102, loss = 0.01716501
Iteration 103, loss = 0.01694527
Iteration 104, loss = 0.01668283
Iteration 105, loss = 0.01646987
Iteration 106, loss = 0.01622215
Iteration 107, loss = 0.01602556
Iteration 108, loss = 0.01576756
Iteration 109, loss = 0.01568862
Iteration 110, loss = 0.01539550
Iteration 111, loss = 0.01517883
Iteration 112, loss = 0.01500168
Iteration 113, loss = 0.01480682
Iteration 114, loss = 0.01463927
Iteration 115, loss = 0.01446213
Iteration 116, loss = 0.01429410
Iteration 117, loss = 0.01414647
Iteration 118, loss = 0.01398460
Iteration 119, loss = 0.01382421
Iteration 120, loss = 0.01363809
Iteration 121, loss = 0.01348952
Iteration 122, loss = 0.01334827
Iteration 123, loss = 0.01322611
Iteration 124, loss = 0.01305534
Iteration 125, loss = 0.01296610
Iteration 126, loss = 0.01283113
Iteration 127, loss = 0.01268235
Iteration 128, loss = 0.01257735
Iteration 129, loss = 0.01244618
Iteration 130, loss = 0.01231926
Iteration 131, loss = 0.01218189
Iteration 132, loss = 0.01210620
Iteration 133, loss = 0.01198031
Iteration 134, loss = 0.01188642
Iteration 135, loss = 0.01175378
Iteration 136, loss = 0.01166096
Iteration 137, loss = 0.01157993
Iteration 138, loss = 0.01146698
Iteration 139, loss = 0.01140955
Iteration 140, loss = 0.01130275
Iteration 141, loss = 0.01120662
Iteration 142, loss = 0.01110514
Iteration 143, loss = 0.01100875
Iteration 144, loss = 0.01091087
Iteration 145, loss = 0.01086632
Iteration 146, loss = 0.01072005
Iteration 147, loss = 0.01065826
Iteration 148, loss = 0.01058038
Iteration 149, loss = 0.01048899
Iteration 150, loss = 0.01042964
Iteration 151, loss = 0.01031377
Iteration 152, loss = 0.01022053
Iteration 153, loss = 0.01021800
Iteration 154, loss = 0.01008456
Iteration 155, loss = 0.01000117
Iteration 156, loss = 0.00993872
Iteration 157, loss = 0.00987514
Iteration 158, loss = 0.00982080
Iteration 159, loss = 0.00977182
Iteration 160, loss = 0.00969069
Iteration 161, loss = 0.00962526
Iteration 162, loss = 0.00958186
Iteration 163, loss = 0.00954248
Iteration 164, loss = 0.00946432
Iteration 165, loss = 0.00942303
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68100120
Iteration 2, loss = 0.65818903
Iteration 3, loss = 0.63690317
Iteration 4, loss = 0.61324888
Iteration 5, loss = 0.58592890
Iteration 6, loss = 0.55517108
Iteration 7, loss = 0.52145531
Iteration 8, loss = 0.48588326
Iteration 9, loss = 0.44998315
Iteration 10, loss = 0.41492165
Iteration 11, loss = 0.38100340
Iteration 12, loss = 0.34878941
Iteration 13, loss = 0.31847662
Iteration 14, loss = 0.29033927
Iteration 15, loss = 0.26430207
Iteration 16, loss = 0.24060793
Iteration 17, loss = 0.21902003
Iteration 18, loss = 0.19947341
Iteration 19, loss = 0.18190216
Iteration 20, loss = 0.16621317
Iteration 21, loss = 0.15224051
Iteration 22, loss = 0.13969308
Iteration 23, loss = 0.12848020
Iteration 24, loss = 0.11857096
Iteration 25, loss = 0.10960885
Iteration 26, loss = 0.10172590
Iteration 27, loss = 0.09459038
Iteration 28, loss = 0.08814379
Iteration 29, loss = 0.08239794
Iteration 30, loss = 0.07729564
Iteration 31, loss = 0.07254776
Iteration 32, loss = 0.06838026
Iteration 33, loss = 0.06453936
Iteration 34, loss = 0.06102873
Iteration 35, loss = 0.05788906
Iteration 36, loss = 0.05505723
Iteration 37, loss = 0.05225402
Iteration 38, loss = 0.04984051
Iteration 39, loss = 0.04763200
Iteration 40, loss = 0.04554529
Iteration 41, loss = 0.04363725
Iteration 42, loss = 0.04184226
Iteration 43, loss = 0.04019508
Iteration 44, loss = 0.03867090
Iteration 45, loss = 0.03724482
Iteration 46, loss = 0.03593255
Iteration 47, loss = 0.03468188
Iteration 48, loss = 0.03353230
Iteration 49, loss = 0.03245795
Iteration 50, loss = 0.03146653
Iteration 51, loss = 0.03044782
Iteration 52, loss = 0.02957640
Iteration 53, loss = 0.02871069
Iteration 54, loss = 0.02791569
Iteration 55, loss = 0.02718969
Iteration 56, loss = 0.02654214
Iteration 57, loss = 0.02584307
Iteration 58, loss = 0.02516677
Iteration 59, loss = 0.02454880
Iteration 60, loss = 0.02401621
Iteration 61, loss = 0.02343773
Iteration 62, loss = 0.02293426
Iteration 63, loss = 0.02245234
Iteration 64, loss = 0.02197865
Iteration 65, loss = 0.02159370
Iteration 66, loss = 0.02115783
Iteration 67, loss = 0.02074625
Iteration 68, loss = 0.02032601
Iteration 69, loss = 0.01995969
Iteration 70, loss = 0.01962585
Iteration 71, loss = 0.01928400
Iteration 72, loss = 0.01898752
Iteration 73, loss = 0.01869097
Iteration 74, loss = 0.01835480
Iteration 75, loss = 0.01815782
Iteration 76, loss = 0.01783771
Iteration 77, loss = 0.01753039
Iteration 78, loss = 0.01729713
Iteration 79, loss = 0.01706667
Iteration 80, loss = 0.01680809
Iteration 81, loss = 0.01659528
Iteration 82, loss = 0.01638214
Iteration 83, loss = 0.01617498
Iteration 84, loss = 0.01596944
Iteration 85, loss = 0.01579489
Iteration 86, loss = 0.01561014
Iteration 87, loss = 0.01540995
Iteration 88, loss = 0.01525154
Iteration 89, loss = 0.01504858
Iteration 90, loss = 0.01491072
Iteration 91, loss = 0.01475415
Iteration 92, loss = 0.01460533
Iteration 93, loss = 0.01447495
Iteration 94, loss = 0.01434658
Iteration 95, loss = 0.01417783
Iteration 96, loss = 0.01404414
Iteration 97, loss = 0.01393475
Iteration 98, loss = 0.01383826
Iteration 99, loss = 0.01367558
Iteration 100, loss = 0.01360257
Iteration 101, loss = 0.01344227
Iteration 102, loss = 0.01331672
Iteration 103, loss = 0.01323546
Iteration 104, loss = 0.01314927
Iteration 105, loss = 0.01305000
Iteration 106, loss = 0.01293161
Iteration 107, loss = 0.01285849
Iteration 108, loss = 0.01275201
Iteration 109, loss = 0.01264336
Iteration 110, loss = 0.01258344
Iteration 111, loss = 0.01249863
Iteration 112, loss = 0.01244568
Iteration 113, loss = 0.01232595
Iteration 114, loss = 0.01223999
Iteration 115, loss = 0.01214806
Iteration 116, loss = 0.01212787
Iteration 117, loss = 0.01201333
Iteration 118, loss = 0.01194049
Iteration 119, loss = 0.01192836
Iteration 120, loss = 0.01181222
Iteration 121, loss = 0.01176528
Iteration 122, loss = 0.01167831
Iteration 123, loss = 0.01160506
Iteration 124, loss = 0.01159101
Iteration 125, loss = 0.01152296
Iteration 126, loss = 0.01145600
Iteration 127, loss = 0.01141821
Iteration 128, loss = 0.01133870
Iteration 129, loss = 0.01127112
Iteration 130, loss = 0.01125125
Iteration 131, loss = 0.01119760
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68015482
Iteration 2, loss = 0.63866872
Iteration 3, loss = 0.57372439
Iteration 4, loss = 0.49131921
Iteration 5, loss = 0.40932108
Iteration 6, loss = 0.33531039
Iteration 7, loss = 0.27246402
Iteration 8, loss = 0.22160193
Iteration 9, loss = 0.18209769
Iteration 10, loss = 0.15196631
Iteration 11, loss = 0.12863870
Iteration 12, loss = 0.11069788
Iteration 13, loss = 0.09694763
Iteration 14, loss = 0.08576209
Iteration 15, loss = 0.07703236
Iteration 16, loss = 0.07025599
Iteration 17, loss = 0.06412794
Iteration 18, loss = 0.05938907
Iteration 19, loss = 0.05523700
Iteration 20, loss = 0.05191134
Iteration 21, loss = 0.04902684
Iteration 22, loss = 0.04669590
Iteration 23, loss = 0.04408314
Iteration 24, loss = 0.04230727
Iteration 25, loss = 0.04070995
Iteration 26, loss = 0.03916141
Iteration 27, loss = 0.03796787
Iteration 28, loss = 0.03674793
Iteration 29, loss = 0.03556188
Iteration 30, loss = 0.03486391
Iteration 31, loss = 0.03417039
Iteration 32, loss = 0.03321576
Iteration 33, loss = 0.03259606
Iteration 34, loss = 0.03195024
Iteration 35, loss = 0.03148452
Iteration 36, loss = 0.03056847
Iteration 37, loss = 0.03010564
Iteration 38, loss = 0.03006849
Iteration 39, loss = 0.02938058
Iteration 40, loss = 0.02908167
Iteration 41, loss = 0.02871616
Iteration 42, loss = 0.02849206
Iteration 43, loss = 0.02814253
Iteration 44, loss = 0.02781969
Iteration 45, loss = 0.02753819
Iteration 46, loss = 0.02752563
Iteration 47, loss = 0.02720080
Iteration 48, loss = 0.02683844
Iteration 49, loss = 0.02673840
Iteration 50, loss = 0.02641957
Iteration 51, loss = 0.02626839
Iteration 52, loss = 0.02632318
Iteration 53, loss = 0.02596953
Iteration 54, loss = 0.02593255
Iteration 55, loss = 0.02588510
Iteration 56, loss = 0.02561256
Iteration 57, loss = 0.02542084
Iteration 58, loss = 0.02543316
Iteration 59, loss = 0.02550302
Iteration 60, loss = 0.02526913
Iteration 61, loss = 0.02497741
Iteration 62, loss = 0.02489546
Iteration 63, loss = 0.02488708
Iteration 64, loss = 0.02485536
Iteration 65, loss = 0.02450407
Iteration 66, loss = 0.02464714
Iteration 67, loss = 0.02460473
Iteration 68, loss = 0.02423202
Iteration 69, loss = 0.02440127
Iteration 70, loss = 0.02426602
Iteration 71, loss = 0.02415319
Iteration 72, loss = 0.02416496
Iteration 73, loss = 0.02406942
Iteration 74, loss = 0.02418648
Iteration 75, loss = 0.02399464
Iteration 76, loss = 0.02374701
Iteration 77, loss = 0.02388968
Iteration 78, loss = 0.02386778
Iteration 79, loss = 0.02395298
Iteration 80, loss = 0.02400574
Iteration 81, loss = 0.02369113
Iteration 82, loss = 0.02372251
Iteration 83, loss = 0.02342648
Iteration 84, loss = 0.02367689
Iteration 85, loss = 0.02369189
Iteration 86, loss = 0.02382766
Iteration 87, loss = 0.02360738
Iteration 88, loss = 0.02338967
Iteration 89, loss = 0.02366501
Iteration 90, loss = 0.02345600
Iteration 91, loss = 0.02323258
Iteration 92, loss = 0.02332598
Iteration 93, loss = 0.02330159
Iteration 94, loss = 0.02325417
Iteration 95, loss = 0.02342937
Iteration 96, loss = 0.02322549
Iteration 97, loss = 0.02311534
Iteration 98, loss = 0.02319371
Iteration 99, loss = 0.02328750
Iteration 100, loss = 0.02353214
Iteration 101, loss = 0.02378108
Iteration 102, loss = 0.02303347
Iteration 103, loss = 0.02312209
Iteration 104, loss = 0.02323588
Iteration 105, loss = 0.02301071
Iteration 106, loss = 0.02316263
Iteration 107, loss = 0.02326602
Iteration 108, loss = 0.02312942
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68723446
Iteration 2, loss = 0.62555169
Iteration 3, loss = 0.53402398
Iteration 4, loss = 0.42924627
Iteration 5, loss = 0.33533255
Iteration 6, loss = 0.25972051
Iteration 7, loss = 0.20283512
Iteration 8, loss = 0.16162994
Iteration 9, loss = 0.13160059
Iteration 10, loss = 0.11052916
Iteration 11, loss = 0.09468517
Iteration 12, loss = 0.08295792
Iteration 13, loss = 0.07377645
Iteration 14, loss = 0.06655022
Iteration 15, loss = 0.06058953
Iteration 16, loss = 0.05613206
Iteration 17, loss = 0.05228433
Iteration 18, loss = 0.04906076
Iteration 19, loss = 0.04649296
Iteration 20, loss = 0.04416398
Iteration 21, loss = 0.04250396
Iteration 22, loss = 0.04066518
Iteration 23, loss = 0.03953418
Iteration 24, loss = 0.03805436
Iteration 25, loss = 0.03676554
Iteration 26, loss = 0.03599758
Iteration 27, loss = 0.03548149
Iteration 28, loss = 0.03501139
Iteration 29, loss = 0.03389907
Iteration 30, loss = 0.03346688
Iteration 31, loss = 0.03269314
Iteration 32, loss = 0.03212699
Iteration 33, loss = 0.03162503
Iteration 34, loss = 0.03134286
Iteration 35, loss = 0.03063530
Iteration 36, loss = 0.03026438
Iteration 37, loss = 0.02995781
Iteration 38, loss = 0.02961262
Iteration 39, loss = 0.03074533
Iteration 40, loss = 0.03055859
Iteration 41, loss = 0.02925775
Iteration 42, loss = 0.02881861
Iteration 43, loss = 0.02836606
Iteration 44, loss = 0.02826532
Iteration 45, loss = 0.02784356
Iteration 46, loss = 0.02856865
Iteration 47, loss = 0.02884648
Iteration 48, loss = 0.02810660
Iteration 49, loss = 0.02808008
Iteration 50, loss = 0.02809301
Iteration 51, loss = 0.02749472
Iteration 52, loss = 0.02743562
Iteration 53, loss = 0.02736786
Iteration 54, loss = 0.02696518
Iteration 55, loss = 0.02705238
Iteration 56, loss = 0.02683315
Iteration 57, loss = 0.02695414
Iteration 58, loss = 0.02702440
Iteration 59, loss = 0.02658696
Iteration 60, loss = 0.02671566
Iteration 61, loss = 0.02642801
Iteration 62, loss = 0.02721061
Iteration 63, loss = 0.02716891
Iteration 64, loss = 0.02708274
Iteration 65, loss = 0.02705563
Iteration 66, loss = 0.02715522
Iteration 67, loss = 0.02635003
Iteration 68, loss = 0.02632440
Iteration 69, loss = 0.02615751
Iteration 70, loss = 0.02568370
Iteration 71, loss = 0.02537419
Iteration 72, loss = 0.02547829
Iteration 73, loss = 0.02555336
Iteration 74, loss = 0.02574431
Iteration 75, loss = 0.02538834
Iteration 76, loss = 0.02573626
Iteration 77, loss = 0.02571432
Iteration 78, loss = 0.02535612
Iteration 79, loss = 0.02533059
Iteration 80, loss = 0.02546737
Iteration 81, loss = 0.02513218
Iteration 82, loss = 0.02522656
Iteration 83, loss = 0.02534162
Iteration 84, loss = 0.02589423
Iteration 85, loss = 0.02568928
Iteration 86, loss = 0.02516327
Iteration 87, loss = 0.02502798
Iteration 88, loss = 0.02491404
Iteration 89, loss = 0.02489950
Iteration 90, loss = 0.02484258
Iteration 91, loss = 0.02467383
Iteration 92, loss = 0.02467421
Iteration 93, loss = 0.02478459
Iteration 94, loss = 0.02546640
Iteration 95, loss = 0.02588838
Iteration 96, loss = 0.02595749
Iteration 97, loss = 0.02590730
Iteration 98, loss = 0.02552894
Iteration 99, loss = 0.02504088
Iteration 100, loss = 0.02501754
Iteration 101, loss = 0.02479567
Iteration 102, loss = 0.02463723
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67611943
Iteration 2, loss = 0.61359968
Iteration 3, loss = 0.52329553
Iteration 4, loss = 0.42219143
Iteration 5, loss = 0.33114016
Iteration 6, loss = 0.25668440
Iteration 7, loss = 0.20012645
Iteration 8, loss = 0.15878156
Iteration 9, loss = 0.12886589
Iteration 10, loss = 0.10724534
Iteration 11, loss = 0.09108192
Iteration 12, loss = 0.07899171
Iteration 13, loss = 0.06967250
Iteration 14, loss = 0.06246515
Iteration 15, loss = 0.05685620
Iteration 16, loss = 0.05190586
Iteration 17, loss = 0.04799565
Iteration 18, loss = 0.04494291
Iteration 19, loss = 0.04214320
Iteration 20, loss = 0.03981955
Iteration 21, loss = 0.03786788
Iteration 22, loss = 0.03625368
Iteration 23, loss = 0.03472330
Iteration 24, loss = 0.03369918
Iteration 25, loss = 0.03235898
Iteration 26, loss = 0.03122594
Iteration 27, loss = 0.03033716
Iteration 28, loss = 0.02943159
Iteration 29, loss = 0.02914782
Iteration 30, loss = 0.02855716
Iteration 31, loss = 0.02769652
Iteration 32, loss = 0.02703428
Iteration 33, loss = 0.02657006
Iteration 34, loss = 0.02639014
Iteration 35, loss = 0.02594567
Iteration 36, loss = 0.02537882
Iteration 37, loss = 0.02569469
Iteration 38, loss = 0.02587251
Iteration 39, loss = 0.02451075
Iteration 40, loss = 0.02406968
Iteration 41, loss = 0.02381501
Iteration 42, loss = 0.02360019
Iteration 43, loss = 0.02331895
Iteration 44, loss = 0.02332859
Iteration 45, loss = 0.02327305
Iteration 46, loss = 0.02278789
Iteration 47, loss = 0.02261634
Iteration 48, loss = 0.02262902
Iteration 49, loss = 0.02251715
Iteration 50, loss = 0.02215282
Iteration 51, loss = 0.02214657
Iteration 52, loss = 0.02208998
Iteration 53, loss = 0.02174437
Iteration 54, loss = 0.02165163
Iteration 55, loss = 0.02154658
Iteration 56, loss = 0.02169112
Iteration 57, loss = 0.02151591
Iteration 58, loss = 0.02135314
Iteration 59, loss = 0.02131993
Iteration 60, loss = 0.02125582
Iteration 61, loss = 0.02105421
Iteration 62, loss = 0.02080074
Iteration 63, loss = 0.02079719
Iteration 64, loss = 0.02063565
Iteration 65, loss = 0.02058105
Iteration 66, loss = 0.02121600
Iteration 67, loss = 0.02101428
Iteration 68, loss = 0.02132830
Iteration 69, loss = 0.02103750
Iteration 70, loss = 0.02245341
Iteration 71, loss = 0.02274970
Iteration 72, loss = 0.02141106
Iteration 73, loss = 0.02078449
Iteration 74, loss = 0.02053999
Iteration 75, loss = 0.02032966
Iteration 76, loss = 0.02079818
Iteration 77, loss = 0.02070073
Iteration 78, loss = 0.02041662
Iteration 79, loss = 0.02002590
Iteration 80, loss = 0.02024094
Iteration 81, loss = 0.01994428
Iteration 82, loss = 0.02030372
Iteration 83, loss = 0.02029087
Iteration 84, loss = 0.01997738
Iteration 85, loss = 0.01979997
Iteration 86, loss = 0.01990725
Iteration 87, loss = 0.01977348
Iteration 88, loss = 0.01992809
Iteration 89, loss = 0.01983507
Iteration 90, loss = 0.01967469
Iteration 91, loss = 0.01941394
Iteration 92, loss = 0.02034514
Iteration 93, loss = 0.02157280
Iteration 94, loss = 0.02150716
Iteration 95, loss = 0.02101904
Iteration 96, loss = 0.02104363
Iteration 97, loss = 0.02125334
Iteration 98, loss = 0.02069489
Iteration 99, loss = 0.02061551
Iteration 100, loss = 0.02063258
Iteration 101, loss = 0.02020672
Iteration 102, loss = 0.02007786
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68196970
Iteration 2, loss = 0.65815961
Iteration 3, loss = 0.62623569
Iteration 4, loss = 0.58133523
Iteration 5, loss = 0.52513607
Iteration 6, loss = 0.46372220
Iteration 7, loss = 0.40381281
Iteration 8, loss = 0.34750603
Iteration 9, loss = 0.29716363
Iteration 10, loss = 0.25340925
Iteration 11, loss = 0.21621106
Iteration 12, loss = 0.18560118
Iteration 13, loss = 0.16022444
Iteration 14, loss = 0.13946318
Iteration 15, loss = 0.12233085
Iteration 16, loss = 0.10838001
Iteration 17, loss = 0.09672012
Iteration 18, loss = 0.08717198
Iteration 19, loss = 0.07914709
Iteration 20, loss = 0.07230611
Iteration 21, loss = 0.06647842
Iteration 22, loss = 0.06148627
Iteration 23, loss = 0.05716291
Iteration 24, loss = 0.05334952
Iteration 25, loss = 0.05007694
Iteration 26, loss = 0.04715329
Iteration 27, loss = 0.04468550
Iteration 28, loss = 0.04252178
Iteration 29, loss = 0.04029568
Iteration 30, loss = 0.03864610
Iteration 31, loss = 0.03692800
Iteration 32, loss = 0.03540849
Iteration 33, loss = 0.03419826
Iteration 34, loss = 0.03301042
Iteration 35, loss = 0.03187420
Iteration 36, loss = 0.03081762
Iteration 37, loss = 0.02996154
Iteration 38, loss = 0.02903684
Iteration 39, loss = 0.02830222
Iteration 40, loss = 0.02753211
Iteration 41, loss = 0.02702681
Iteration 42, loss = 0.02625324
Iteration 43, loss = 0.02585991
Iteration 44, loss = 0.02527590
Iteration 45, loss = 0.02471472
Iteration 46, loss = 0.02435910
Iteration 47, loss = 0.02384319
Iteration 48, loss = 0.02343603
Iteration 49, loss = 0.02303095
Iteration 50, loss = 0.02270981
Iteration 51, loss = 0.02219429
Iteration 52, loss = 0.02206362
Iteration 53, loss = 0.02177297
Iteration 54, loss = 0.02148121
Iteration 55, loss = 0.02120246
Iteration 56, loss = 0.02097322
Iteration 57, loss = 0.02069131
Iteration 58, loss = 0.02051629
Iteration 59, loss = 0.02021007
Iteration 60, loss = 0.02008732
Iteration 61, loss = 0.01989607
Iteration 62, loss = 0.01966308
Iteration 63, loss = 0.01957224
Iteration 64, loss = 0.01944598
Iteration 65, loss = 0.01915509
Iteration 66, loss = 0.01909492
Iteration 67, loss = 0.01897345
Iteration 68, loss = 0.01884550
Iteration 69, loss = 0.01863196
Iteration 70, loss = 0.01851359
Iteration 71, loss = 0.01857940
Iteration 72, loss = 0.01826952
Iteration 73, loss = 0.01812242
Iteration 74, loss = 0.01810677
Iteration 75, loss = 0.01802699
Iteration 76, loss = 0.01797192
Iteration 77, loss = 0.01765845
Iteration 78, loss = 0.01757406
Iteration 79, loss = 0.01763645
Iteration 80, loss = 0.01741178
Iteration 81, loss = 0.01751100
Iteration 82, loss = 0.01748275
Iteration 83, loss = 0.01742998
Iteration 84, loss = 0.01729407
Iteration 85, loss = 0.01719779
Iteration 86, loss = 0.01717822
Iteration 87, loss = 0.01684154
Iteration 88, loss = 0.01690729
Iteration 89, loss = 0.01702452
Iteration 90, loss = 0.01698871
Iteration 91, loss = 0.01687186
Iteration 92, loss = 0.01671208
Iteration 93, loss = 0.01677257
Iteration 94, loss = 0.01659676
Iteration 95, loss = 0.01660953
Iteration 96, loss = 0.01652827
Iteration 97, loss = 0.01652776
Iteration 98, loss = 0.01642629
Iteration 99, loss = 0.01637460
Iteration 100, loss = 0.01634703
Iteration 101, loss = 0.01641652
Iteration 102, loss = 0.01615049
Iteration 103, loss = 0.01632895
Iteration 104, loss = 0.01618551
Iteration 105, loss = 0.01612834
Iteration 106, loss = 0.01620990
Iteration 107, loss = 0.01605943
Iteration 108, loss = 0.01598947
Iteration 109, loss = 0.01596546
Iteration 110, loss = 0.01597143
Iteration 111, loss = 0.01596403
Iteration 112, loss = 0.01607707
Iteration 113, loss = 0.01594813
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68556832
Iteration 2, loss = 0.67137828
Iteration 3, loss = 0.65756853
Iteration 4, loss = 0.64078853
Iteration 5, loss = 0.61978930
Iteration 6, loss = 0.59424178
Iteration 7, loss = 0.56433148
Iteration 8, loss = 0.53136084
Iteration 9, loss = 0.49630012
Iteration 10, loss = 0.46108574
Iteration 11, loss = 0.42641766
Iteration 12, loss = 0.39278314
Iteration 13, loss = 0.36025897
Iteration 14, loss = 0.32988995
Iteration 15, loss = 0.30133448
Iteration 16, loss = 0.27474656
Iteration 17, loss = 0.25028273
Iteration 18, loss = 0.22799333
Iteration 19, loss = 0.20764798
Iteration 20, loss = 0.18938945
Iteration 21, loss = 0.17298565
Iteration 22, loss = 0.15813373
Iteration 23, loss = 0.14504020
Iteration 24, loss = 0.13313231
Iteration 25, loss = 0.12263353
Iteration 26, loss = 0.11323380
Iteration 27, loss = 0.10484829
Iteration 28, loss = 0.09735783
Iteration 29, loss = 0.09062756
Iteration 30, loss = 0.08469219
Iteration 31, loss = 0.07925095
Iteration 32, loss = 0.07442459
Iteration 33, loss = 0.06997658
Iteration 34, loss = 0.06598263
Iteration 35, loss = 0.06241215
Iteration 36, loss = 0.05906335
Iteration 37, loss = 0.05603854
Iteration 38, loss = 0.05334575
Iteration 39, loss = 0.05084001
Iteration 40, loss = 0.04854725
Iteration 41, loss = 0.04636638
Iteration 42, loss = 0.04434797
Iteration 43, loss = 0.04261177
Iteration 44, loss = 0.04086395
Iteration 45, loss = 0.03930561
Iteration 46, loss = 0.03781643
Iteration 47, loss = 0.03646947
Iteration 48, loss = 0.03524397
Iteration 49, loss = 0.03403687
Iteration 50, loss = 0.03292707
Iteration 51, loss = 0.03187628
Iteration 52, loss = 0.03095075
Iteration 53, loss = 0.02999379
Iteration 54, loss = 0.02913160
Iteration 55, loss = 0.02831853
Iteration 56, loss = 0.02760154
Iteration 57, loss = 0.02684572
Iteration 58, loss = 0.02614505
Iteration 59, loss = 0.02547002
Iteration 60, loss = 0.02489711
Iteration 61, loss = 0.02429199
Iteration 62, loss = 0.02373552
Iteration 63, loss = 0.02321229
Iteration 64, loss = 0.02273060
Iteration 65, loss = 0.02224570
Iteration 66, loss = 0.02179958
Iteration 67, loss = 0.02135452
Iteration 68, loss = 0.02093067
Iteration 69, loss = 0.02057626
Iteration 70, loss = 0.02013574
Iteration 71, loss = 0.01980744
Iteration 72, loss = 0.01943421
Iteration 73, loss = 0.01915276
Iteration 74, loss = 0.01876176
Iteration 75, loss = 0.01851593
Iteration 76, loss = 0.01819611
Iteration 77, loss = 0.01793221
Iteration 78, loss = 0.01765916
Iteration 79, loss = 0.01738206
Iteration 80, loss = 0.01716114
Iteration 81, loss = 0.01694240
Iteration 82, loss = 0.01665278
Iteration 83, loss = 0.01644985
Iteration 84, loss = 0.01625712
Iteration 85, loss = 0.01606939
Iteration 86, loss = 0.01582061
Iteration 87, loss = 0.01572772
Iteration 88, loss = 0.01548524
Iteration 89, loss = 0.01528510
Iteration 90, loss = 0.01508397
Iteration 91, loss = 0.01497757
Iteration 92, loss = 0.01482325
Iteration 93, loss = 0.01465042
Iteration 94, loss = 0.01446650
Iteration 95, loss = 0.01433788
Iteration 96, loss = 0.01419958
Iteration 97, loss = 0.01405265
Iteration 98, loss = 0.01390894
Iteration 99, loss = 0.01382959
Iteration 100, loss = 0.01365092
Iteration 101, loss = 0.01349867
Iteration 102, loss = 0.01348305
Iteration 103, loss = 0.01330799
Iteration 104, loss = 0.01318716
Iteration 105, loss = 0.01310244
Iteration 106, loss = 0.01301347
Iteration 107, loss = 0.01289393
Iteration 108, loss = 0.01284605
Iteration 109, loss = 0.01270073
Iteration 110, loss = 0.01259909
Iteration 111, loss = 0.01250660
Iteration 112, loss = 0.01242197
Iteration 113, loss = 0.01233852
Iteration 114, loss = 0.01225169
Iteration 115, loss = 0.01220350
Iteration 116, loss = 0.01207769
Iteration 117, loss = 0.01202988
Iteration 118, loss = 0.01192810
Iteration 119, loss = 0.01192466
Iteration 120, loss = 0.01180481
Iteration 121, loss = 0.01176444
Iteration 122, loss = 0.01172818
Iteration 123, loss = 0.01160633
Iteration 124, loss = 0.01153474
Iteration 125, loss = 0.01148767
Iteration 126, loss = 0.01139639
Iteration 127, loss = 0.01132787
Iteration 128, loss = 0.01127180
Iteration 129, loss = 0.01123096
Iteration 130, loss = 0.01120418
Iteration 131, loss = 0.01111075
Iteration 132, loss = 0.01106928
Iteration 133, loss = 0.01109180
Iteration 134, loss = 0.01091371
Iteration 135, loss = 0.01089022
Iteration 136, loss = 0.01087825
Iteration 137, loss = 0.01079760
Iteration 138, loss = 0.01077691
Iteration 139, loss = 0.01074385
Iteration 140, loss = 0.01069498
Iteration 141, loss = 0.01064241
Iteration 142, loss = 0.01064208
Iteration 143, loss = 0.01053988
Iteration 144, loss = 0.01052676
Iteration 145, loss = 0.01045834
Iteration 146, loss = 0.01054442
Iteration 147, loss = 0.01040092
Iteration 148, loss = 0.01032571
Iteration 149, loss = 0.01028312
Iteration 150, loss = 0.01026558
Iteration 151, loss = 0.01026329
Iteration 152, loss = 0.01019270
Iteration 153, loss = 0.01016332
Iteration 154, loss = 0.01017305
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68265077
Iteration 2, loss = 0.64108339
Iteration 3, loss = 0.57752583
Iteration 4, loss = 0.49617942
Iteration 5, loss = 0.41449641
Iteration 6, loss = 0.34029225
Iteration 7, loss = 0.27652969
Iteration 8, loss = 0.22476887
Iteration 9, loss = 0.18407517
Iteration 10, loss = 0.15296302
Iteration 11, loss = 0.12923774
Iteration 12, loss = 0.11079593
Iteration 13, loss = 0.09656977
Iteration 14, loss = 0.08552006
Iteration 15, loss = 0.07657130
Iteration 16, loss = 0.06923258
Iteration 17, loss = 0.06327866
Iteration 18, loss = 0.05846816
Iteration 19, loss = 0.05439643
Iteration 20, loss = 0.05099612
Iteration 21, loss = 0.04818890
Iteration 22, loss = 0.04554593
Iteration 23, loss = 0.04336442
Iteration 24, loss = 0.04129765
Iteration 25, loss = 0.03973622
Iteration 26, loss = 0.03831519
Iteration 27, loss = 0.03692686
Iteration 28, loss = 0.03610159
Iteration 29, loss = 0.03458042
Iteration 30, loss = 0.03393017
Iteration 31, loss = 0.03282512
Iteration 32, loss = 0.03222126
Iteration 33, loss = 0.03154696
Iteration 34, loss = 0.03098661
Iteration 35, loss = 0.03027997
Iteration 36, loss = 0.02983510
Iteration 37, loss = 0.02937365
Iteration 38, loss = 0.02876240
Iteration 39, loss = 0.02852764
Iteration 40, loss = 0.02799208
Iteration 41, loss = 0.02743888
Iteration 42, loss = 0.02735278
Iteration 43, loss = 0.02712537
Iteration 44, loss = 0.02659751
Iteration 45, loss = 0.02648722
Iteration 46, loss = 0.02619964
Iteration 47, loss = 0.02585890
Iteration 48, loss = 0.02578016
Iteration 49, loss = 0.02549262
Iteration 50, loss = 0.02530297
Iteration 51, loss = 0.02505589
Iteration 52, loss = 0.02504102
Iteration 53, loss = 0.02503828
Iteration 54, loss = 0.02448626
Iteration 55, loss = 0.02451700
Iteration 56, loss = 0.02432382
Iteration 57, loss = 0.02419714
Iteration 58, loss = 0.02425305
Iteration 59, loss = 0.02384937
Iteration 60, loss = 0.02408466
Iteration 61, loss = 0.02381956
Iteration 62, loss = 0.02375916
Iteration 63, loss = 0.02362864
Iteration 64, loss = 0.02376089
Iteration 65, loss = 0.02351812
Iteration 66, loss = 0.02356588
Iteration 67, loss = 0.02324367
Iteration 68, loss = 0.02343451
Iteration 69, loss = 0.02307326
Iteration 70, loss = 0.02336142
Iteration 71, loss = 0.02324810
Iteration 72, loss = 0.02321109
Iteration 73, loss = 0.02302205
Iteration 74, loss = 0.02301118
Iteration 75, loss = 0.02296471
Iteration 76, loss = 0.02306762
Iteration 77, loss = 0.02280883
Iteration 78, loss = 0.02295033
Iteration 79, loss = 0.02280932
Iteration 80, loss = 0.02283155
Iteration 81, loss = 0.02293432
Iteration 82, loss = 0.02254046
Iteration 83, loss = 0.02230902
Iteration 84, loss = 0.02222706
Iteration 85, loss = 0.02242245
Iteration 86, loss = 0.02222034
Iteration 87, loss = 0.02261019
Iteration 88, loss = 0.02225756
Iteration 89, loss = 0.02236899
Iteration 90, loss = 0.02221203
Iteration 91, loss = 0.02250868
Iteration 92, loss = 0.02242842
Iteration 93, loss = 0.02235926
Iteration 94, loss = 0.02204345
Iteration 95, loss = 0.02227610
Iteration 96, loss = 0.02193299
Iteration 97, loss = 0.02203467
Iteration 98, loss = 0.02180969
Iteration 99, loss = 0.02217192
Iteration 100, loss = 0.02181590
Iteration 101, loss = 0.02210185
Iteration 102, loss = 0.02197049
Iteration 103, loss = 0.02255281
Iteration 104, loss = 0.02204438
Iteration 105, loss = 0.02210195
Iteration 106, loss = 0.02229254
Iteration 107, loss = 0.02189596
Iteration 108, loss = 0.02200105
Iteration 109, loss = 0.02177390
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69995931
Iteration 2, loss = 0.67416031
Iteration 3, loss = 0.64477556
Iteration 4, loss = 0.60890273
Iteration 5, loss = 0.56688938
Iteration 6, loss = 0.52023305
Iteration 7, loss = 0.47140971
Iteration 8, loss = 0.42300718
Iteration 9, loss = 0.37668086
Iteration 10, loss = 0.33330175
Iteration 11, loss = 0.29393650
Iteration 12, loss = 0.25880633
Iteration 13, loss = 0.22788045
Iteration 14, loss = 0.20110262
Iteration 15, loss = 0.17802818
Iteration 16, loss = 0.15832201
Iteration 17, loss = 0.14162404
Iteration 18, loss = 0.12725648
Iteration 19, loss = 0.11521993
Iteration 20, loss = 0.10467548
Iteration 21, loss = 0.09568021
Iteration 22, loss = 0.08781802
Iteration 23, loss = 0.08109369
Iteration 24, loss = 0.07528318
Iteration 25, loss = 0.07019175
Iteration 26, loss = 0.06566003
Iteration 27, loss = 0.06163138
Iteration 28, loss = 0.05794466
Iteration 29, loss = 0.05491457
Iteration 30, loss = 0.05212792
Iteration 31, loss = 0.04947123
Iteration 32, loss = 0.04713332
Iteration 33, loss = 0.04506612
Iteration 34, loss = 0.04319542
Iteration 35, loss = 0.04129733
Iteration 36, loss = 0.03994100
Iteration 37, loss = 0.03855602
Iteration 38, loss = 0.03722195
Iteration 39, loss = 0.03586624
Iteration 40, loss = 0.03464167
Iteration 41, loss = 0.03348912
Iteration 42, loss = 0.03236777
Iteration 43, loss = 0.03168401
Iteration 44, loss = 0.03080245
Iteration 45, loss = 0.02987875
Iteration 46, loss = 0.02933565
Iteration 47, loss = 0.02834053
Iteration 48, loss = 0.02787186
Iteration 49, loss = 0.02718974
Iteration 50, loss = 0.02670845
Iteration 51, loss = 0.02624336
Iteration 52, loss = 0.02532441
Iteration 53, loss = 0.02537091
Iteration 54, loss = 0.02484521
Iteration 55, loss = 0.02437509
Iteration 56, loss = 0.02387986
Iteration 57, loss = 0.02318792
Iteration 58, loss = 0.02315037
Iteration 59, loss = 0.02266337
Iteration 60, loss = 0.02249897
Iteration 61, loss = 0.02214746
Iteration 62, loss = 0.02186580
Iteration 63, loss = 0.02145030
Iteration 64, loss = 0.02120073
Iteration 65, loss = 0.02087874
Iteration 66, loss = 0.02066948
Iteration 67, loss = 0.02053463
Iteration 68, loss = 0.02039191
Iteration 69, loss = 0.02008807
Iteration 70, loss = 0.01980743
Iteration 71, loss = 0.01942715
Iteration 72, loss = 0.01953677
Iteration 73, loss = 0.01938431
Iteration 74, loss = 0.01911874
Iteration 75, loss = 0.01903570
Iteration 76, loss = 0.01861232
Iteration 77, loss = 0.01847979
Iteration 78, loss = 0.01847574
Iteration 79, loss = 0.01829521
Iteration 80, loss = 0.01813705
Iteration 81, loss = 0.01794070
Iteration 82, loss = 0.01785420
Iteration 83, loss = 0.01777979
Iteration 84, loss = 0.01765984
Iteration 85, loss = 0.01770829
Iteration 86, loss = 0.01762657
Iteration 87, loss = 0.01745275
Iteration 88, loss = 0.01702044
Iteration 89, loss = 0.01705005
Iteration 90, loss = 0.01705758
Iteration 91, loss = 0.01697316
Iteration 92, loss = 0.01690511
Iteration 93, loss = 0.01694868
Iteration 94, loss = 0.01694630
Iteration 95, loss = 0.01681529
Iteration 96, loss = 0.01681677
Iteration 97, loss = 0.01668940
Iteration 98, loss = 0.01652198
Iteration 99, loss = 0.01629762
Iteration 100, loss = 0.01618798
Iteration 101, loss = 0.01602351
Iteration 102, loss = 0.01589291
Iteration 103, loss = 0.01587320
Iteration 104, loss = 0.01581103
Iteration 105, loss = 0.01574667
Iteration 106, loss = 0.01585701
Iteration 107, loss = 0.01591157
Iteration 108, loss = 0.01566941
Iteration 109, loss = 0.01585822
Iteration 110, loss = 0.01555810
Iteration 111, loss = 0.01582376
Iteration 112, loss = 0.01558955
Iteration 113, loss = 0.01566162
Iteration 114, loss = 0.01572311
Iteration 115, loss = 0.01555082
Iteration 116, loss = 0.01551895
Iteration 117, loss = 0.01548097
Iteration 118, loss = 0.01541772
Iteration 119, loss = 0.01531263
Iteration 120, loss = 0.01515880
Iteration 121, loss = 0.01500457
Iteration 122, loss = 0.01501984
Iteration 123, loss = 0.01489854
Iteration 124, loss = 0.01480049
Iteration 125, loss = 0.01479336
Iteration 126, loss = 0.01475276
Iteration 127, loss = 0.01474294
Iteration 128, loss = 0.01468289
Iteration 129, loss = 0.01466745
Iteration 130, loss = 0.01467800
Iteration 131, loss = 0.01452290
Iteration 132, loss = 0.01485342
Iteration 133, loss = 0.01484095
Iteration 134, loss = 0.01479508
Iteration 135, loss = 0.01470167
Iteration 136, loss = 0.01467613
Iteration 137, loss = 0.01456080
Iteration 138, loss = 0.01461840
Iteration 139, loss = 0.01471831
Iteration 140, loss = 0.01474188
Iteration 141, loss = 0.01468559
Iteration 142, loss = 0.01456091
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69592305
Iteration 2, loss = 0.64987667
Iteration 3, loss = 0.58291154
Iteration 4, loss = 0.49595563
Iteration 5, loss = 0.40605213
Iteration 6, loss = 0.32531736
Iteration 7, loss = 0.25888128
Iteration 8, loss = 0.20780271
Iteration 9, loss = 0.16854093
Iteration 10, loss = 0.13966004
Iteration 11, loss = 0.11799142
Iteration 12, loss = 0.10167611
Iteration 13, loss = 0.08894992
Iteration 14, loss = 0.07894286
Iteration 15, loss = 0.07116178
Iteration 16, loss = 0.06457339
Iteration 17, loss = 0.05947431
Iteration 18, loss = 0.05515391
Iteration 19, loss = 0.05134390
Iteration 20, loss = 0.04849974
Iteration 21, loss = 0.04583898
Iteration 22, loss = 0.04366390
Iteration 23, loss = 0.04156584
Iteration 24, loss = 0.04016401
Iteration 25, loss = 0.03836016
Iteration 26, loss = 0.03721485
Iteration 27, loss = 0.03582107
Iteration 28, loss = 0.03512564
Iteration 29, loss = 0.03397415
Iteration 30, loss = 0.03303797
Iteration 31, loss = 0.03234878
Iteration 32, loss = 0.03147711
Iteration 33, loss = 0.03114471
Iteration 34, loss = 0.03070054
Iteration 35, loss = 0.03006355
Iteration 36, loss = 0.02949357
Iteration 37, loss = 0.02909613
Iteration 38, loss = 0.02866433
Iteration 39, loss = 0.02824920
Iteration 40, loss = 0.02795614
Iteration 41, loss = 0.02774428
Iteration 42, loss = 0.02764902
Iteration 43, loss = 0.02699129
Iteration 44, loss = 0.02660417
Iteration 45, loss = 0.02652779
Iteration 46, loss = 0.02631718
Iteration 47, loss = 0.02615726
Iteration 48, loss = 0.02587575
Iteration 49, loss = 0.02565920
Iteration 50, loss = 0.02584388
Iteration 51, loss = 0.02576757
Iteration 52, loss = 0.02522031
Iteration 53, loss = 0.02519925
Iteration 54, loss = 0.02485856
Iteration 55, loss = 0.02491816
Iteration 56, loss = 0.02462972
Iteration 57, loss = 0.02456048
Iteration 58, loss = 0.02448010
Iteration 59, loss = 0.02445727
Iteration 60, loss = 0.02429597
Iteration 61, loss = 0.02426651
Iteration 62, loss = 0.02409305
Iteration 63, loss = 0.02410292
Iteration 64, loss = 0.02395385
Iteration 65, loss = 0.02408483
Iteration 66, loss = 0.02390610
Iteration 67, loss = 0.02392416
Iteration 68, loss = 0.02370035
Iteration 69, loss = 0.02350909
Iteration 70, loss = 0.02361593
Iteration 71, loss = 0.02338536
Iteration 72, loss = 0.02365599
Iteration 73, loss = 0.02330502
Iteration 74, loss = 0.02334999
Iteration 75, loss = 0.02323963
Iteration 76, loss = 0.02337947
Iteration 77, loss = 0.02315323
Iteration 78, loss = 0.02308176
Iteration 79, loss = 0.02315196
Iteration 80, loss = 0.02309068
Iteration 81, loss = 0.02311413
Iteration 82, loss = 0.02290293
Iteration 83, loss = 0.02293632
Iteration 84, loss = 0.02286253
Iteration 85, loss = 0.02279480
Iteration 86, loss = 0.02276871
Iteration 87, loss = 0.02286946
Iteration 88, loss = 0.02263741
Iteration 89, loss = 0.02266047
Iteration 90, loss = 0.02255754
Iteration 91, loss = 0.02258874
Iteration 92, loss = 0.02272594
Iteration 93, loss = 0.02269646
Iteration 94, loss = 0.02241702
Iteration 95, loss = 0.02295450
Iteration 96, loss = 0.02260074
Iteration 97, loss = 0.02256108
Iteration 98, loss = 0.02243605
Iteration 99, loss = 0.02231563
Iteration 100, loss = 0.02262535
Iteration 101, loss = 0.02243006
Iteration 102, loss = 0.02264921
Iteration 103, loss = 0.02248578
Iteration 104, loss = 0.02236907
Iteration 105, loss = 0.02252931
Iteration 106, loss = 0.02221565
Iteration 107, loss = 0.02233227
Iteration 108, loss = 0.02206390
Iteration 109, loss = 0.02228305
Iteration 110, loss = 0.02232162
Iteration 111, loss = 0.02237687
Iteration 112, loss = 0.02226415
Iteration 113, loss = 0.02229079
Iteration 114, loss = 0.02215026
Iteration 115, loss = 0.02230834
Iteration 116, loss = 0.02220377
Iteration 117, loss = 0.02212588
Iteration 118, loss = 0.02195532
Iteration 119, loss = 0.02204974
Iteration 120, loss = 0.02198096
Iteration 121, loss = 0.02209375
Iteration 122, loss = 0.02240245
Iteration 123, loss = 0.02199546
Iteration 124, loss = 0.02229528
Iteration 125, loss = 0.02222712
Iteration 126, loss = 0.02212288
Iteration 127, loss = 0.02218523
Iteration 128, loss = 0.02213313
Iteration 129, loss = 0.02189100
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69872339
Iteration 2, loss = 0.62661928
Iteration 3, loss = 0.52469370
Iteration 4, loss = 0.41260817
Iteration 5, loss = 0.31556832
Iteration 6, loss = 0.23980035
Iteration 7, loss = 0.18530872
Iteration 8, loss = 0.14668137
Iteration 9, loss = 0.11973646
Iteration 10, loss = 0.10023840
Iteration 11, loss = 0.08584189
Iteration 12, loss = 0.07542045
Iteration 13, loss = 0.06716155
Iteration 14, loss = 0.06084442
Iteration 15, loss = 0.05580941
Iteration 16, loss = 0.05171205
Iteration 17, loss = 0.04812516
Iteration 18, loss = 0.04514611
Iteration 19, loss = 0.04282929
Iteration 20, loss = 0.04081299
Iteration 21, loss = 0.03911171
Iteration 22, loss = 0.03749020
Iteration 23, loss = 0.03633243
Iteration 24, loss = 0.03493063
Iteration 25, loss = 0.03422508
Iteration 26, loss = 0.03304591
Iteration 27, loss = 0.03238081
Iteration 28, loss = 0.03146982
Iteration 29, loss = 0.03093016
Iteration 30, loss = 0.03041543
Iteration 31, loss = 0.02986674
Iteration 32, loss = 0.02964869
Iteration 33, loss = 0.02912283
Iteration 34, loss = 0.02880339
Iteration 35, loss = 0.02822146
Iteration 36, loss = 0.02830741
Iteration 37, loss = 0.02797745
Iteration 38, loss = 0.02751166
Iteration 39, loss = 0.02718292
Iteration 40, loss = 0.02693927
Iteration 41, loss = 0.02674261
Iteration 42, loss = 0.02655164
Iteration 43, loss = 0.02655085
Iteration 44, loss = 0.02631898
Iteration 45, loss = 0.02610901
Iteration 46, loss = 0.02609094
Iteration 47, loss = 0.02588379
Iteration 48, loss = 0.02572088
Iteration 49, loss = 0.02568753
Iteration 50, loss = 0.02550889
Iteration 51, loss = 0.02550705
Iteration 52, loss = 0.02519114
Iteration 53, loss = 0.02519103
Iteration 54, loss = 0.02488551
Iteration 55, loss = 0.02499118
Iteration 56, loss = 0.02480898
Iteration 57, loss = 0.02490643
Iteration 58, loss = 0.02463891
Iteration 59, loss = 0.02462454
Iteration 60, loss = 0.02450432
Iteration 61, loss = 0.02464097
Iteration 62, loss = 0.02455735
Iteration 63, loss = 0.02423950
Iteration 64, loss = 0.02432444
Iteration 65, loss = 0.02420247
Iteration 66, loss = 0.02425224
Iteration 67, loss = 0.02438788
Iteration 68, loss = 0.02422417
Iteration 69, loss = 0.02407338
Iteration 70, loss = 0.02411501
Iteration 71, loss = 0.02412256
Iteration 72, loss = 0.02442798
Iteration 73, loss = 0.02411479
Iteration 74, loss = 0.02437260
Iteration 75, loss = 0.02417245
Iteration 76, loss = 0.02381291
Iteration 77, loss = 0.02379667
Iteration 78, loss = 0.02371971
Iteration 79, loss = 0.02407436
Iteration 80, loss = 0.02390637
Iteration 81, loss = 0.02373220
Iteration 82, loss = 0.02378101
Iteration 83, loss = 0.02386450
Iteration 84, loss = 0.02345904
Iteration 85, loss = 0.02364695
Iteration 86, loss = 0.02353178
Iteration 87, loss = 0.02369851
Iteration 88, loss = 0.02349250
Iteration 89, loss = 0.02354032
Iteration 90, loss = 0.02385349
Iteration 91, loss = 0.02385016
Iteration 92, loss = 0.02376146
Iteration 93, loss = 0.02365585
Iteration 94, loss = 0.02342243
Iteration 95, loss = 0.02385799
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68396804
Iteration 2, loss = 0.66391810
Iteration 3, loss = 0.64459106
Iteration 4, loss = 0.62257174
Iteration 5, loss = 0.59650828
Iteration 6, loss = 0.56631091
Iteration 7, loss = 0.53217222
Iteration 8, loss = 0.49618696
Iteration 9, loss = 0.45930130
Iteration 10, loss = 0.42314418
Iteration 11, loss = 0.38831078
Iteration 12, loss = 0.35509878
Iteration 13, loss = 0.32390173
Iteration 14, loss = 0.29500225
Iteration 15, loss = 0.26838536
Iteration 16, loss = 0.24407640
Iteration 17, loss = 0.22202502
Iteration 18, loss = 0.20228467
Iteration 19, loss = 0.18459329
Iteration 20, loss = 0.16878214
Iteration 21, loss = 0.15466417
Iteration 22, loss = 0.14205619
Iteration 23, loss = 0.13074193
Iteration 24, loss = 0.12076020
Iteration 25, loss = 0.11186325
Iteration 26, loss = 0.10387257
Iteration 27, loss = 0.09673854
Iteration 28, loss = 0.09039776
Iteration 29, loss = 0.08465349
Iteration 30, loss = 0.07945696
Iteration 31, loss = 0.07481184
Iteration 32, loss = 0.07060194
Iteration 33, loss = 0.06679401
Iteration 34, loss = 0.06335640
Iteration 35, loss = 0.06014762
Iteration 36, loss = 0.05728362
Iteration 37, loss = 0.05459004
Iteration 38, loss = 0.05215029
Iteration 39, loss = 0.04993453
Iteration 40, loss = 0.04787403
Iteration 41, loss = 0.04595648
Iteration 42, loss = 0.04415353
Iteration 43, loss = 0.04255976
Iteration 44, loss = 0.04107798
Iteration 45, loss = 0.03962030
Iteration 46, loss = 0.03828292
Iteration 47, loss = 0.03705856
Iteration 48, loss = 0.03585338
Iteration 49, loss = 0.03490709
Iteration 50, loss = 0.03381846
Iteration 51, loss = 0.03287923
Iteration 52, loss = 0.03192836
Iteration 53, loss = 0.03109637
Iteration 54, loss = 0.03028711
Iteration 55, loss = 0.02955961
Iteration 56, loss = 0.02883178
Iteration 57, loss = 0.02819181
Iteration 58, loss = 0.02757663
Iteration 59, loss = 0.02700007
Iteration 60, loss = 0.02644185
Iteration 61, loss = 0.02583142
Iteration 62, loss = 0.02532418
Iteration 63, loss = 0.02483025
Iteration 64, loss = 0.02438722
Iteration 65, loss = 0.02393316
Iteration 66, loss = 0.02349351
Iteration 67, loss = 0.02309816
Iteration 68, loss = 0.02266597
Iteration 69, loss = 0.02233842
Iteration 70, loss = 0.02196980
Iteration 71, loss = 0.02163985
Iteration 72, loss = 0.02136995
Iteration 73, loss = 0.02101979
Iteration 74, loss = 0.02074758
Iteration 75, loss = 0.02040935
Iteration 76, loss = 0.02012919
Iteration 77, loss = 0.01988875
Iteration 78, loss = 0.01962363
Iteration 79, loss = 0.01944103
Iteration 80, loss = 0.01911299
Iteration 81, loss = 0.01892878
Iteration 82, loss = 0.01871834
Iteration 83, loss = 0.01850470
Iteration 84, loss = 0.01832382
Iteration 85, loss = 0.01818058
Iteration 86, loss = 0.01794319
Iteration 87, loss = 0.01774051
Iteration 88, loss = 0.01756893
Iteration 89, loss = 0.01742548
Iteration 90, loss = 0.01721640
Iteration 91, loss = 0.01710385
Iteration 92, loss = 0.01695571
Iteration 93, loss = 0.01679204
Iteration 94, loss = 0.01666446
Iteration 95, loss = 0.01650336
Iteration 96, loss = 0.01637704
Iteration 97, loss = 0.01623393
Iteration 98, loss = 0.01617088
Iteration 99, loss = 0.01602958
Iteration 100, loss = 0.01587727
Iteration 101, loss = 0.01575526
Iteration 102, loss = 0.01567822
Iteration 103, loss = 0.01551296
Iteration 104, loss = 0.01544165
Iteration 105, loss = 0.01530118
Iteration 106, loss = 0.01523589
Iteration 107, loss = 0.01511312
Iteration 108, loss = 0.01505970
Iteration 109, loss = 0.01502262
Iteration 110, loss = 0.01489159
Iteration 111, loss = 0.01484597
Iteration 112, loss = 0.01473897
Iteration 113, loss = 0.01461431
Iteration 114, loss = 0.01452105
Iteration 115, loss = 0.01446598
Iteration 116, loss = 0.01439858
Iteration 117, loss = 0.01429227
Iteration 118, loss = 0.01420696
Iteration 119, loss = 0.01420545
Iteration 120, loss = 0.01416345
Iteration 121, loss = 0.01407733
Iteration 122, loss = 0.01395108
Iteration 123, loss = 0.01389930
Iteration 124, loss = 0.01386741
Iteration 125, loss = 0.01376180
Iteration 126, loss = 0.01373086
Iteration 127, loss = 0.01369923
Iteration 128, loss = 0.01364941
Iteration 129, loss = 0.01362462
Iteration 130, loss = 0.01351282
Iteration 131, loss = 0.01350624
Iteration 132, loss = 0.01341608
Iteration 133, loss = 0.01337186
Iteration 134, loss = 0.01333996
Iteration 135, loss = 0.01325590
Iteration 136, loss = 0.01326577
Iteration 137, loss = 0.01317485
Iteration 138, loss = 0.01310366
Iteration 139, loss = 0.01307451
Iteration 140, loss = 0.01302990
Iteration 141, loss = 0.01301339
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68751173
Iteration 2, loss = 0.65577167
Iteration 3, loss = 0.61296952
Iteration 4, loss = 0.55701225
Iteration 5, loss = 0.49356698
Iteration 6, loss = 0.43054086
Iteration 7, loss = 0.37106407
Iteration 8, loss = 0.31781456
Iteration 9, loss = 0.27099336
Iteration 10, loss = 0.23121271
Iteration 11, loss = 0.19782119
Iteration 12, loss = 0.17023901
Iteration 13, loss = 0.14758178
Iteration 14, loss = 0.12901434
Iteration 15, loss = 0.11363872
Iteration 16, loss = 0.10094554
Iteration 17, loss = 0.09054628
Iteration 18, loss = 0.08188532
Iteration 19, loss = 0.07435156
Iteration 20, loss = 0.06812752
Iteration 21, loss = 0.06297030
Iteration 22, loss = 0.05806858
Iteration 23, loss = 0.05412719
Iteration 24, loss = 0.05061515
Iteration 25, loss = 0.04759923
Iteration 26, loss = 0.04483997
Iteration 27, loss = 0.04235346
Iteration 28, loss = 0.04030008
Iteration 29, loss = 0.03838507
Iteration 30, loss = 0.03659244
Iteration 31, loss = 0.03516318
Iteration 32, loss = 0.03374147
Iteration 33, loss = 0.03250449
Iteration 34, loss = 0.03129735
Iteration 35, loss = 0.03015379
Iteration 36, loss = 0.02925380
Iteration 37, loss = 0.02839521
Iteration 38, loss = 0.02758900
Iteration 39, loss = 0.02683448
Iteration 40, loss = 0.02613396
Iteration 41, loss = 0.02557586
Iteration 42, loss = 0.02494450
Iteration 43, loss = 0.02445181
Iteration 44, loss = 0.02402085
Iteration 45, loss = 0.02328159
Iteration 46, loss = 0.02297224
Iteration 47, loss = 0.02264705
Iteration 48, loss = 0.02228296
Iteration 49, loss = 0.02180925
Iteration 50, loss = 0.02138140
Iteration 51, loss = 0.02110540
Iteration 52, loss = 0.02089908
Iteration 53, loss = 0.02069108
Iteration 54, loss = 0.02039016
Iteration 55, loss = 0.02004275
Iteration 56, loss = 0.01982926
Iteration 57, loss = 0.01957492
Iteration 58, loss = 0.01932861
Iteration 59, loss = 0.01907053
Iteration 60, loss = 0.01890245
Iteration 61, loss = 0.01873713
Iteration 62, loss = 0.01854636
Iteration 63, loss = 0.01840556
Iteration 64, loss = 0.01835113
Iteration 65, loss = 0.01828523
Iteration 66, loss = 0.01793914
Iteration 67, loss = 0.01781250
Iteration 68, loss = 0.01769397
Iteration 69, loss = 0.01760448
Iteration 70, loss = 0.01759130
Iteration 71, loss = 0.01729780
Iteration 72, loss = 0.01722361
Iteration 73, loss = 0.01707903
Iteration 74, loss = 0.01713531
Iteration 75, loss = 0.01698422
Iteration 76, loss = 0.01676572
Iteration 77, loss = 0.01661145
Iteration 78, loss = 0.01660155
Iteration 79, loss = 0.01657560
Iteration 80, loss = 0.01641548
Iteration 81, loss = 0.01630726
Iteration 82, loss = 0.01632317
Iteration 83, loss = 0.01621360
Iteration 84, loss = 0.01618972
Iteration 85, loss = 0.01607934
Iteration 86, loss = 0.01598527
Iteration 87, loss = 0.01616131
Iteration 88, loss = 0.01592630
Iteration 89, loss = 0.01593207
Iteration 90, loss = 0.01588780
Iteration 91, loss = 0.01577042
Iteration 92, loss = 0.01572530
Iteration 93, loss = 0.01567903
Iteration 94, loss = 0.01565551
Iteration 95, loss = 0.01561482
Iteration 96, loss = 0.01552696
Iteration 97, loss = 0.01551797
Iteration 98, loss = 0.01550919
Iteration 99, loss = 0.01541269
Iteration 100, loss = 0.01542875
Iteration 101, loss = 0.01530908
Iteration 102, loss = 0.01533437
Iteration 103, loss = 0.01538103
Iteration 104, loss = 0.01530111
Iteration 105, loss = 0.01518282
Iteration 106, loss = 0.01513730
Iteration 107, loss = 0.01513338
Iteration 108, loss = 0.01501430
Iteration 109, loss = 0.01503106
Iteration 110, loss = 0.01503534
Iteration 111, loss = 0.01505686
Iteration 112, loss = 0.01499194
Iteration 113, loss = 0.01487533
Iteration 114, loss = 0.01479349
Iteration 115, loss = 0.01497416
Iteration 116, loss = 0.01488613
Iteration 117, loss = 0.01485805
Iteration 118, loss = 0.01483475
Iteration 119, loss = 0.01473396
Iteration 120, loss = 0.01477656
Iteration 121, loss = 0.01474861
Iteration 122, loss = 0.01477001
Iteration 123, loss = 0.01466338
Iteration 124, loss = 0.01472251
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67815223
Iteration 2, loss = 0.62306634
Iteration 3, loss = 0.54116590
Iteration 4, loss = 0.44717937
Iteration 5, loss = 0.35794811
Iteration 6, loss = 0.28236208
Iteration 7, loss = 0.22339606
Iteration 8, loss = 0.17881332
Iteration 9, loss = 0.14600234
Iteration 10, loss = 0.12179919
Iteration 11, loss = 0.10363022
Iteration 12, loss = 0.08966768
Iteration 13, loss = 0.07904401
Iteration 14, loss = 0.07044152
Iteration 15, loss = 0.06377163
Iteration 16, loss = 0.05824670
Iteration 17, loss = 0.05380478
Iteration 18, loss = 0.05017094
Iteration 19, loss = 0.04681454
Iteration 20, loss = 0.04405520
Iteration 21, loss = 0.04173149
Iteration 22, loss = 0.03991499
Iteration 23, loss = 0.03822049
Iteration 24, loss = 0.03670368
Iteration 25, loss = 0.03541094
Iteration 26, loss = 0.03406135
Iteration 27, loss = 0.03337072
Iteration 28, loss = 0.03232669
Iteration 29, loss = 0.03141614
Iteration 30, loss = 0.03059141
Iteration 31, loss = 0.02999257
Iteration 32, loss = 0.02955201
Iteration 33, loss = 0.02896372
Iteration 34, loss = 0.02816898
Iteration 35, loss = 0.02792881
Iteration 36, loss = 0.02742491
Iteration 37, loss = 0.02717232
Iteration 38, loss = 0.02684940
Iteration 39, loss = 0.02649134
Iteration 40, loss = 0.02603569
Iteration 41, loss = 0.02594344
Iteration 42, loss = 0.02572575
Iteration 43, loss = 0.02528188
Iteration 44, loss = 0.02536287
Iteration 45, loss = 0.02510539
Iteration 46, loss = 0.02470195
Iteration 47, loss = 0.02463404
Iteration 48, loss = 0.02438339
Iteration 49, loss = 0.02415764
Iteration 50, loss = 0.02409504
Iteration 51, loss = 0.02395501
Iteration 52, loss = 0.02393312
Iteration 53, loss = 0.02367067
Iteration 54, loss = 0.02351931
Iteration 55, loss = 0.02344663
Iteration 56, loss = 0.02338254
Iteration 57, loss = 0.02340341
Iteration 58, loss = 0.02307816
Iteration 59, loss = 0.02321374
Iteration 60, loss = 0.02293100
Iteration 61, loss = 0.02282944
Iteration 62, loss = 0.02295806
Iteration 63, loss = 0.02290703
Iteration 64, loss = 0.02281318
Iteration 65, loss = 0.02263284
Iteration 66, loss = 0.02244266
Iteration 67, loss = 0.02254196
Iteration 68, loss = 0.02266455
Iteration 69, loss = 0.02241781
Iteration 70, loss = 0.02211855
Iteration 71, loss = 0.02231088
Iteration 72, loss = 0.02226696
Iteration 73, loss = 0.02229378
Iteration 74, loss = 0.02199373
Iteration 75, loss = 0.02218453
Iteration 76, loss = 0.02208651
Iteration 77, loss = 0.02204814
Iteration 78, loss = 0.02171659
Iteration 79, loss = 0.02219141
Iteration 80, loss = 0.02195166
Iteration 81, loss = 0.02210573
Iteration 82, loss = 0.02181024
Iteration 83, loss = 0.02170774
Iteration 84, loss = 0.02182457
Iteration 85, loss = 0.02167233
Iteration 86, loss = 0.02160382
Iteration 87, loss = 0.02178086
Iteration 88, loss = 0.02171627
Iteration 89, loss = 0.02155293
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71318588
Iteration 2, loss = 0.67030235
Iteration 3, loss = 0.60921812
Iteration 4, loss = 0.54000063
Iteration 5, loss = 0.46652864
Iteration 6, loss = 0.39581694
Iteration 7, loss = 0.33183816
Iteration 8, loss = 0.27667396
Iteration 9, loss = 0.23086738
Iteration 10, loss = 0.19337825
Iteration 11, loss = 0.16318409
Iteration 12, loss = 0.13928944
Iteration 13, loss = 0.12007996
Iteration 14, loss = 0.10456798
Iteration 15, loss = 0.09221822
Iteration 16, loss = 0.08189482
Iteration 17, loss = 0.07370159
Iteration 18, loss = 0.06659405
Iteration 19, loss = 0.06089300
Iteration 20, loss = 0.05591755
Iteration 21, loss = 0.05176887
Iteration 22, loss = 0.04824792
Iteration 23, loss = 0.04521222
Iteration 24, loss = 0.04229439
Iteration 25, loss = 0.03991977
Iteration 26, loss = 0.03791863
Iteration 27, loss = 0.03598316
Iteration 28, loss = 0.03431327
Iteration 29, loss = 0.03296055
Iteration 30, loss = 0.03142434
Iteration 31, loss = 0.03063036
Iteration 32, loss = 0.02936704
Iteration 33, loss = 0.02830497
Iteration 34, loss = 0.02737145
Iteration 35, loss = 0.02668609
Iteration 36, loss = 0.02588117
Iteration 37, loss = 0.02516597
Iteration 38, loss = 0.02461306
Iteration 39, loss = 0.02397249
Iteration 40, loss = 0.02344794
Iteration 41, loss = 0.02298407
Iteration 42, loss = 0.02250365
Iteration 43, loss = 0.02203421
Iteration 44, loss = 0.02154173
Iteration 45, loss = 0.02125157
Iteration 46, loss = 0.02086048
Iteration 47, loss = 0.02054981
Iteration 48, loss = 0.02029243
Iteration 49, loss = 0.01997171
Iteration 50, loss = 0.01987841
Iteration 51, loss = 0.01948040
Iteration 52, loss = 0.01916384
Iteration 53, loss = 0.01902220
Iteration 54, loss = 0.01888164
Iteration 55, loss = 0.01865553
Iteration 56, loss = 0.01849163
Iteration 57, loss = 0.01830173
Iteration 58, loss = 0.01808035
Iteration 59, loss = 0.01787363
Iteration 60, loss = 0.01767772
Iteration 61, loss = 0.01756543
Iteration 62, loss = 0.01741523
Iteration 63, loss = 0.01738406
Iteration 64, loss = 0.01722784
Iteration 65, loss = 0.01707182
Iteration 66, loss = 0.01692597
Iteration 67, loss = 0.01686149
Iteration 68, loss = 0.01666431
Iteration 69, loss = 0.01666836
Iteration 70, loss = 0.01652247
Iteration 71, loss = 0.01655531
Iteration 72, loss = 0.01645448
Iteration 73, loss = 0.01629447
Iteration 74, loss = 0.01606738
Iteration 75, loss = 0.01613344
Iteration 76, loss = 0.01616649
Iteration 77, loss = 0.01603033
Iteration 78, loss = 0.01592498
Iteration 79, loss = 0.01573721
Iteration 80, loss = 0.01588570
Iteration 81, loss = 0.01592225
Iteration 82, loss = 0.01559489
Iteration 83, loss = 0.01546589
Iteration 84, loss = 0.01553108
Iteration 85, loss = 0.01559647
Iteration 86, loss = 0.01560319
Iteration 87, loss = 0.01545074
Iteration 88, loss = 0.01531954
Iteration 89, loss = 0.01526394
Iteration 90, loss = 0.01529327
Iteration 91, loss = 0.01543285
Iteration 92, loss = 0.01522691
Iteration 93, loss = 0.01516272
Iteration 94, loss = 0.01502933
Iteration 95, loss = 0.01501841
Iteration 96, loss = 0.01513508
Iteration 97, loss = 0.01491410
Iteration 98, loss = 0.01513091
Iteration 99, loss = 0.01490115
Iteration 100, loss = 0.01489295
Iteration 101, loss = 0.01486812
Iteration 102, loss = 0.01479858
Iteration 103, loss = 0.01487884
Iteration 104, loss = 0.01465918
Iteration 105, loss = 0.01483361
Iteration 106, loss = 0.01473668
Iteration 107, loss = 0.01454718
Iteration 108, loss = 0.01473518
Iteration 109, loss = 0.01464432
Iteration 110, loss = 0.01461825
Iteration 111, loss = 0.01457782
Iteration 112, loss = 0.01460298
Iteration 113, loss = 0.01466009
Iteration 114, loss = 0.01455449
Iteration 115, loss = 0.01445086
Iteration 116, loss = 0.01449205
Iteration 117, loss = 0.01442601
Iteration 118, loss = 0.01460215
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67920426
Iteration 2, loss = 0.62387254
Iteration 3, loss = 0.53339010
Iteration 4, loss = 0.43213403
Iteration 5, loss = 0.33976218
Iteration 6, loss = 0.26392818
Iteration 7, loss = 0.20628980
Iteration 8, loss = 0.16437728
Iteration 9, loss = 0.13396362
Iteration 10, loss = 0.11195799
Iteration 11, loss = 0.09577190
Iteration 12, loss = 0.08347321
Iteration 13, loss = 0.07410360
Iteration 14, loss = 0.06653590
Iteration 15, loss = 0.06050675
Iteration 16, loss = 0.05588152
Iteration 17, loss = 0.05172500
Iteration 18, loss = 0.04841632
Iteration 19, loss = 0.04587019
Iteration 20, loss = 0.04373636
Iteration 21, loss = 0.04170278
Iteration 22, loss = 0.03987431
Iteration 23, loss = 0.03836189
Iteration 24, loss = 0.03699367
Iteration 25, loss = 0.03600230
Iteration 26, loss = 0.03564938
Iteration 27, loss = 0.03465636
Iteration 28, loss = 0.03347293
Iteration 29, loss = 0.03256905
Iteration 30, loss = 0.03182445
Iteration 31, loss = 0.03119925
Iteration 32, loss = 0.03035020
Iteration 33, loss = 0.03033256
Iteration 34, loss = 0.03024816
Iteration 35, loss = 0.02955527
Iteration 36, loss = 0.02908533
Iteration 37, loss = 0.02875877
Iteration 38, loss = 0.02836200
Iteration 39, loss = 0.02798426
Iteration 40, loss = 0.02764609
Iteration 41, loss = 0.02728211
Iteration 42, loss = 0.02736180
Iteration 43, loss = 0.02724658
Iteration 44, loss = 0.02676837
Iteration 45, loss = 0.02660932
Iteration 46, loss = 0.02660787
Iteration 47, loss = 0.02610162
Iteration 48, loss = 0.02616452
Iteration 49, loss = 0.02579290
Iteration 50, loss = 0.02593676
Iteration 51, loss = 0.02581789
Iteration 52, loss = 0.02569211
Iteration 53, loss = 0.02540186
Iteration 54, loss = 0.02544287
Iteration 55, loss = 0.02532731
Iteration 56, loss = 0.02516038
Iteration 57, loss = 0.02504418
Iteration 58, loss = 0.02578009
Iteration 59, loss = 0.02556539
Iteration 60, loss = 0.02506927
Iteration 61, loss = 0.02502714
Iteration 62, loss = 0.02500131
Iteration 63, loss = 0.02553957
Iteration 64, loss = 0.02537304
Iteration 65, loss = 0.02506274
Iteration 66, loss = 0.02490690
Iteration 67, loss = 0.02545380
Iteration 68, loss = 0.02548679
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69247350
Iteration 2, loss = 0.66041371
Iteration 3, loss = 0.62500770
Iteration 4, loss = 0.58363205
Iteration 5, loss = 0.53787434
Iteration 6, loss = 0.48901704
Iteration 7, loss = 0.43987089
Iteration 8, loss = 0.39225353
Iteration 9, loss = 0.34769415
Iteration 10, loss = 0.30695390
Iteration 11, loss = 0.27033231
Iteration 12, loss = 0.23787813
Iteration 13, loss = 0.20960482
Iteration 14, loss = 0.18517421
Iteration 15, loss = 0.16414233
Iteration 16, loss = 0.14635240
Iteration 17, loss = 0.13095285
Iteration 18, loss = 0.11795456
Iteration 19, loss = 0.10679488
Iteration 20, loss = 0.09720667
Iteration 21, loss = 0.08893177
Iteration 22, loss = 0.08181554
Iteration 23, loss = 0.07566759
Iteration 24, loss = 0.07012863
Iteration 25, loss = 0.06535470
Iteration 26, loss = 0.06112149
Iteration 27, loss = 0.05739257
Iteration 28, loss = 0.05391885
Iteration 29, loss = 0.05092007
Iteration 30, loss = 0.04816606
Iteration 31, loss = 0.04580870
Iteration 32, loss = 0.04367473
Iteration 33, loss = 0.04174205
Iteration 34, loss = 0.03998542
Iteration 35, loss = 0.03831422
Iteration 36, loss = 0.03680362
Iteration 37, loss = 0.03544639
Iteration 38, loss = 0.03403210
Iteration 39, loss = 0.03285553
Iteration 40, loss = 0.03171602
Iteration 41, loss = 0.03074105
Iteration 42, loss = 0.02988077
Iteration 43, loss = 0.02905456
Iteration 44, loss = 0.02812646
Iteration 45, loss = 0.02729123
Iteration 46, loss = 0.02660776
Iteration 47, loss = 0.02596841
Iteration 48, loss = 0.02537403
Iteration 49, loss = 0.02471826
Iteration 50, loss = 0.02417792
Iteration 51, loss = 0.02360775
Iteration 52, loss = 0.02307242
Iteration 53, loss = 0.02260387
Iteration 54, loss = 0.02209239
Iteration 55, loss = 0.02163260
Iteration 56, loss = 0.02131433
Iteration 57, loss = 0.02090355
Iteration 58, loss = 0.02058106
Iteration 59, loss = 0.02018650
Iteration 60, loss = 0.01981134
Iteration 61, loss = 0.01957122
Iteration 62, loss = 0.01935949
Iteration 63, loss = 0.01907163
Iteration 64, loss = 0.01884144
Iteration 65, loss = 0.01858922
Iteration 66, loss = 0.01828027
Iteration 67, loss = 0.01807845
Iteration 68, loss = 0.01776622
Iteration 69, loss = 0.01763013
Iteration 70, loss = 0.01766733
Iteration 71, loss = 0.01758436
Iteration 72, loss = 0.01736957
Iteration 73, loss = 0.01710142
Iteration 74, loss = 0.01688105
Iteration 75, loss = 0.01665978
Iteration 76, loss = 0.01631298
Iteration 77, loss = 0.01614664
Iteration 78, loss = 0.01626889
Iteration 79, loss = 0.01626519
Iteration 80, loss = 0.01612541
Iteration 81, loss = 0.01597938
Iteration 82, loss = 0.01569103
Iteration 83, loss = 0.01559342
Iteration 84, loss = 0.01531782
Iteration 85, loss = 0.01536842
Iteration 86, loss = 0.01511267
Iteration 87, loss = 0.01521783
Iteration 88, loss = 0.01518874
Iteration 89, loss = 0.01503696
Iteration 90, loss = 0.01496586
Iteration 91, loss = 0.01480120
Iteration 92, loss = 0.01464595
Iteration 93, loss = 0.01452088
Iteration 94, loss = 0.01453156
Iteration 95, loss = 0.01440527
Iteration 96, loss = 0.01434517
Iteration 97, loss = 0.01429235
Iteration 98, loss = 0.01447595
Iteration 99, loss = 0.01378281
Iteration 100, loss = 0.01391159
Iteration 101, loss = 0.01382937
Iteration 102, loss = 0.01376507
Iteration 103, loss = 0.01366968
Iteration 104, loss = 0.01358845
Iteration 105, loss = 0.01338181
Iteration 106, loss = 0.01358519
Iteration 107, loss = 0.01367604
Iteration 108, loss = 0.01366775
Iteration 109, loss = 0.01341175
Iteration 110, loss = 0.01333548
Iteration 111, loss = 0.01314965
Iteration 112, loss = 0.01314091
Iteration 113, loss = 0.01312072
Iteration 114, loss = 0.01301879
Iteration 115, loss = 0.01301635
Iteration 116, loss = 0.01309347
Iteration 117, loss = 0.01313885
Iteration 118, loss = 0.01320434
Iteration 119, loss = 0.01324943
Iteration 120, loss = 0.01311546
Iteration 121, loss = 0.01296752
Iteration 122, loss = 0.01278587
Iteration 123, loss = 0.01274238
Iteration 124, loss = 0.01269259
Iteration 125, loss = 0.01255315
Iteration 126, loss = 0.01249920
Iteration 127, loss = 0.01257123
Iteration 128, loss = 0.01241373
Iteration 129, loss = 0.01238425
Iteration 130, loss = 0.01240236
Iteration 131, loss = 0.01237392
Iteration 132, loss = 0.01232714
Iteration 133, loss = 0.01223397
Iteration 134, loss = 0.01222073
Iteration 135, loss = 0.01217439
Iteration 136, loss = 0.01214108
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69711091
Iteration 2, loss = 0.63812880
Iteration 3, loss = 0.55098582
Iteration 4, loss = 0.44426952
Iteration 5, loss = 0.34812790
Iteration 6, loss = 0.26960918
Iteration 7, loss = 0.21020464
Iteration 8, loss = 0.16760683
Iteration 9, loss = 0.13672577
Iteration 10, loss = 0.11432392
Iteration 11, loss = 0.09806803
Iteration 12, loss = 0.08599784
Iteration 13, loss = 0.07649296
Iteration 14, loss = 0.06897153
Iteration 15, loss = 0.06319805
Iteration 16, loss = 0.05830698
Iteration 17, loss = 0.05443866
Iteration 18, loss = 0.05109220
Iteration 19, loss = 0.04827555
Iteration 20, loss = 0.04581223
Iteration 21, loss = 0.04359058
Iteration 22, loss = 0.04222904
Iteration 23, loss = 0.04069059
Iteration 24, loss = 0.03929979
Iteration 25, loss = 0.03819585
Iteration 26, loss = 0.03688292
Iteration 27, loss = 0.03605206
Iteration 28, loss = 0.03499667
Iteration 29, loss = 0.03475256
Iteration 30, loss = 0.03377783
Iteration 31, loss = 0.03297314
Iteration 32, loss = 0.03293024
Iteration 33, loss = 0.03230992
Iteration 34, loss = 0.03180619
Iteration 35, loss = 0.03141113
Iteration 36, loss = 0.03157331
Iteration 37, loss = 0.03107223
Iteration 38, loss = 0.03072475
Iteration 39, loss = 0.03012797
Iteration 40, loss = 0.02970035
Iteration 41, loss = 0.02959106
Iteration 42, loss = 0.02939220
Iteration 43, loss = 0.02924213
Iteration 44, loss = 0.02885223
Iteration 45, loss = 0.02864626
Iteration 46, loss = 0.02822491
Iteration 47, loss = 0.02820696
Iteration 48, loss = 0.02811511
Iteration 49, loss = 0.02885371
Iteration 50, loss = 0.02825633
Iteration 51, loss = 0.02806381
Iteration 52, loss = 0.02762251
Iteration 53, loss = 0.02790132
Iteration 54, loss = 0.02740295
Iteration 55, loss = 0.02740771
Iteration 56, loss = 0.02683846
Iteration 57, loss = 0.02699673
Iteration 58, loss = 0.02664819
Iteration 59, loss = 0.02656688
Iteration 60, loss = 0.02661717
Iteration 61, loss = 0.02631614
Iteration 62, loss = 0.02651683
Iteration 63, loss = 0.02626557
Iteration 64, loss = 0.02646835
Iteration 65, loss = 0.02672166
Iteration 66, loss = 0.02619119
Iteration 67, loss = 0.02589962
Iteration 68, loss = 0.02566937
Iteration 69, loss = 0.02546809
Iteration 70, loss = 0.02608326
Iteration 71, loss = 0.02614344
Iteration 72, loss = 0.02615540
Iteration 73, loss = 0.02592596
Iteration 74, loss = 0.02581408
Iteration 75, loss = 0.02559737
Iteration 76, loss = 0.02549023
Iteration 77, loss = 0.02556124
Iteration 78, loss = 0.02537200
Iteration 79, loss = 0.02521070
Iteration 80, loss = 0.02533809
Iteration 81, loss = 0.02519549
Iteration 82, loss = 0.02501140
Iteration 83, loss = 0.02515418
Iteration 84, loss = 0.02508723
Iteration 85, loss = 0.02490713
Iteration 86, loss = 0.02517116
Iteration 87, loss = 0.02487974
Iteration 88, loss = 0.02487513
Iteration 89, loss = 0.02492843
Iteration 90, loss = 0.02505356
Iteration 91, loss = 0.02489376
Iteration 92, loss = 0.02514448
Iteration 93, loss = 0.02535151
Iteration 94, loss = 0.02515863
Iteration 95, loss = 0.02519827
Iteration 96, loss = 0.02504939
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71984829
Iteration 2, loss = 0.69890949
Iteration 3, loss = 0.67828159
Iteration 4, loss = 0.65431538
Iteration 5, loss = 0.62588356
Iteration 6, loss = 0.59314619
Iteration 7, loss = 0.55709211
Iteration 8, loss = 0.51866995
Iteration 9, loss = 0.47986344
Iteration 10, loss = 0.44143187
Iteration 11, loss = 0.40397557
Iteration 12, loss = 0.36854532
Iteration 13, loss = 0.33529517
Iteration 14, loss = 0.30450869
Iteration 15, loss = 0.27620756
Iteration 16, loss = 0.25050552
Iteration 17, loss = 0.22722555
Iteration 18, loss = 0.20650119
Iteration 19, loss = 0.18780985
Iteration 20, loss = 0.17114255
Iteration 21, loss = 0.15635838
Iteration 22, loss = 0.14326953
Iteration 23, loss = 0.13157980
Iteration 24, loss = 0.12128367
Iteration 25, loss = 0.11203289
Iteration 26, loss = 0.10381499
Iteration 27, loss = 0.09647416
Iteration 28, loss = 0.08989656
Iteration 29, loss = 0.08406496
Iteration 30, loss = 0.07871782
Iteration 31, loss = 0.07394223
Iteration 32, loss = 0.06967111
Iteration 33, loss = 0.06575894
Iteration 34, loss = 0.06219951
Iteration 35, loss = 0.05895532
Iteration 36, loss = 0.05597096
Iteration 37, loss = 0.05331896
Iteration 38, loss = 0.05077627
Iteration 39, loss = 0.04849065
Iteration 40, loss = 0.04643324
Iteration 41, loss = 0.04444135
Iteration 42, loss = 0.04262680
Iteration 43, loss = 0.04103977
Iteration 44, loss = 0.03944527
Iteration 45, loss = 0.03804013
Iteration 46, loss = 0.03661741
Iteration 47, loss = 0.03542943
Iteration 48, loss = 0.03423027
Iteration 49, loss = 0.03319420
Iteration 50, loss = 0.03214652
Iteration 51, loss = 0.03112753
Iteration 52, loss = 0.03021868
Iteration 53, loss = 0.02939277
Iteration 54, loss = 0.02860510
Iteration 55, loss = 0.02785634
Iteration 56, loss = 0.02709599
Iteration 57, loss = 0.02641280
Iteration 58, loss = 0.02578216
Iteration 59, loss = 0.02513317
Iteration 60, loss = 0.02453094
Iteration 61, loss = 0.02407400
Iteration 62, loss = 0.02353471
Iteration 63, loss = 0.02303519
Iteration 64, loss = 0.02251300
Iteration 65, loss = 0.02209043
Iteration 66, loss = 0.02162542
Iteration 67, loss = 0.02126082
Iteration 68, loss = 0.02087451
Iteration 69, loss = 0.02051360
Iteration 70, loss = 0.02011096
Iteration 71, loss = 0.01976961
Iteration 72, loss = 0.01945122
Iteration 73, loss = 0.01913984
Iteration 74, loss = 0.01880453
Iteration 75, loss = 0.01858837
Iteration 76, loss = 0.01829827
Iteration 77, loss = 0.01801365
Iteration 78, loss = 0.01769644
Iteration 79, loss = 0.01750860
Iteration 80, loss = 0.01723689
Iteration 81, loss = 0.01708765
Iteration 82, loss = 0.01679787
Iteration 83, loss = 0.01661503
Iteration 84, loss = 0.01637087
Iteration 85, loss = 0.01620367
Iteration 86, loss = 0.01599331
Iteration 87, loss = 0.01584011
Iteration 88, loss = 0.01566419
Iteration 89, loss = 0.01548077
Iteration 90, loss = 0.01529898
Iteration 91, loss = 0.01512686
Iteration 92, loss = 0.01500845
Iteration 93, loss = 0.01481918
Iteration 94, loss = 0.01471921
Iteration 95, loss = 0.01452500
Iteration 96, loss = 0.01440881
Iteration 97, loss = 0.01427527
Iteration 98, loss = 0.01420543
Iteration 99, loss = 0.01402946
Iteration 100, loss = 0.01395154
Iteration 101, loss = 0.01376674
Iteration 102, loss = 0.01368975
Iteration 103, loss = 0.01357531
Iteration 104, loss = 0.01349243
Iteration 105, loss = 0.01338921
Iteration 106, loss = 0.01328959
Iteration 107, loss = 0.01324559
Iteration 108, loss = 0.01310332
Iteration 109, loss = 0.01304114
Iteration 110, loss = 0.01287901
Iteration 111, loss = 0.01290234
Iteration 112, loss = 0.01276489
Iteration 113, loss = 0.01267256
Iteration 114, loss = 0.01258492
Iteration 115, loss = 0.01249072
Iteration 116, loss = 0.01242872
Iteration 117, loss = 0.01233054
Iteration 118, loss = 0.01227179
Iteration 119, loss = 0.01218920
Iteration 120, loss = 0.01212398
Iteration 121, loss = 0.01205917
Iteration 122, loss = 0.01202138
Iteration 123, loss = 0.01200574
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68113329
Iteration 2, loss = 0.62424861
Iteration 3, loss = 0.53949472
Iteration 4, loss = 0.43663901
Iteration 5, loss = 0.34116843
Iteration 6, loss = 0.26333712
Iteration 7, loss = 0.20463056
Iteration 8, loss = 0.16274499
Iteration 9, loss = 0.13261815
Iteration 10, loss = 0.11069161
Iteration 11, loss = 0.09463640
Iteration 12, loss = 0.08240079
Iteration 13, loss = 0.07342912
Iteration 14, loss = 0.06590508
Iteration 15, loss = 0.05976285
Iteration 16, loss = 0.05510964
Iteration 17, loss = 0.05119094
Iteration 18, loss = 0.04786193
Iteration 19, loss = 0.04506537
Iteration 20, loss = 0.04277886
Iteration 21, loss = 0.04059962
Iteration 22, loss = 0.03914863
Iteration 23, loss = 0.03748806
Iteration 24, loss = 0.03611532
Iteration 25, loss = 0.03493567
Iteration 26, loss = 0.03430586
Iteration 27, loss = 0.03336617
Iteration 28, loss = 0.03264382
Iteration 29, loss = 0.03182578
Iteration 30, loss = 0.03096661
Iteration 31, loss = 0.03024247
Iteration 32, loss = 0.02970807
Iteration 33, loss = 0.02911706
Iteration 34, loss = 0.02889268
Iteration 35, loss = 0.02879770
Iteration 36, loss = 0.02815298
Iteration 37, loss = 0.02785990
Iteration 38, loss = 0.02759044
Iteration 39, loss = 0.02759398
Iteration 40, loss = 0.02739971
Iteration 41, loss = 0.02695756
Iteration 42, loss = 0.02652262
Iteration 43, loss = 0.02614894
Iteration 44, loss = 0.02600240
Iteration 45, loss = 0.02621963
Iteration 46, loss = 0.02616553
Iteration 47, loss = 0.02586918
Iteration 48, loss = 0.02641924
Iteration 49, loss = 0.02617891
Iteration 50, loss = 0.02556385
Iteration 51, loss = 0.02504674
Iteration 52, loss = 0.02479247
Iteration 53, loss = 0.02467032
Iteration 54, loss = 0.02443736
Iteration 55, loss = 0.02420849
Iteration 56, loss = 0.02417854
Iteration 57, loss = 0.02427107
Iteration 58, loss = 0.02437852
Iteration 59, loss = 0.02416731
Iteration 60, loss = 0.02399843
Iteration 61, loss = 0.02381735
Iteration 62, loss = 0.02376418
Iteration 63, loss = 0.02354392
Iteration 64, loss = 0.02554615
Iteration 65, loss = 0.02578045
Iteration 66, loss = 0.02447554
Iteration 67, loss = 0.02376072
Iteration 68, loss = 0.02359788
Iteration 69, loss = 0.02335788
Iteration 70, loss = 0.02356892
Iteration 71, loss = 0.02378593
Iteration 72, loss = 0.02330614
Iteration 73, loss = 0.02321794
Iteration 74, loss = 0.02314490
Iteration 75, loss = 0.02297613
Iteration 76, loss = 0.02308625
Iteration 77, loss = 0.02277017
Iteration 78, loss = 0.02302981
Iteration 79, loss = 0.02276788
Iteration 80, loss = 0.02268528
Iteration 81, loss = 0.02291992
Iteration 82, loss = 0.02303300
Iteration 83, loss = 0.02299773
Iteration 84, loss = 0.02283093
Iteration 85, loss = 0.02284968
Iteration 86, loss = 0.02262787
Iteration 87, loss = 0.02246231
Iteration 88, loss = 0.02372676
Iteration 89, loss = 0.02359413
Iteration 90, loss = 0.02305195
Iteration 91, loss = 0.02261867
Iteration 92, loss = 0.02228526
Iteration 93, loss = 0.02237598
Iteration 94, loss = 0.02227483
Iteration 95, loss = 0.02232225
Iteration 96, loss = 0.02222584
Iteration 97, loss = 0.02223212
Iteration 98, loss = 0.02237343
Iteration 99, loss = 0.02211849
Iteration 100, loss = 0.02226627
Iteration 101, loss = 0.02204406
Iteration 102, loss = 0.02232210
Iteration 103, loss = 0.02209352
Iteration 104, loss = 0.02203860
Iteration 105, loss = 0.02225423
Iteration 106, loss = 0.02204640
Iteration 107, loss = 0.02206564
Iteration 108, loss = 0.02207082
Iteration 109, loss = 0.02201380
Iteration 110, loss = 0.02183366
Iteration 111, loss = 0.02191854
Iteration 112, loss = 0.02210471
Iteration 113, loss = 0.02216854
Iteration 114, loss = 0.02203600
Iteration 115, loss = 0.02307102
Iteration 116, loss = 0.02293299
Iteration 117, loss = 0.02259504
Iteration 118, loss = 0.02242901
Iteration 119, loss = 0.02266668
Iteration 120, loss = 0.02287720
Iteration 121, loss = 0.02255318
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67684882
Iteration 2, loss = 0.60378921
Iteration 3, loss = 0.49900124
Iteration 4, loss = 0.38721228
Iteration 5, loss = 0.29151154
Iteration 6, loss = 0.21928635
Iteration 7, loss = 0.16864806
Iteration 8, loss = 0.13372942
Iteration 9, loss = 0.10937838
Iteration 10, loss = 0.09256203
Iteration 11, loss = 0.08015860
Iteration 12, loss = 0.07056758
Iteration 13, loss = 0.06345882
Iteration 14, loss = 0.05770870
Iteration 15, loss = 0.05364084
Iteration 16, loss = 0.04981111
Iteration 17, loss = 0.04714310
Iteration 18, loss = 0.04457970
Iteration 19, loss = 0.04265716
Iteration 20, loss = 0.04080083
Iteration 21, loss = 0.03935526
Iteration 22, loss = 0.03786756
Iteration 23, loss = 0.03684457
Iteration 24, loss = 0.03604277
Iteration 25, loss = 0.03504364
Iteration 26, loss = 0.03444374
Iteration 27, loss = 0.03372101
Iteration 28, loss = 0.03313095
Iteration 29, loss = 0.03275678
Iteration 30, loss = 0.03221999
Iteration 31, loss = 0.03170372
Iteration 32, loss = 0.03140754
Iteration 33, loss = 0.03108587
Iteration 34, loss = 0.03046422
Iteration 35, loss = 0.03024429
Iteration 36, loss = 0.03033198
Iteration 37, loss = 0.02982470
Iteration 38, loss = 0.02962770
Iteration 39, loss = 0.02933395
Iteration 40, loss = 0.02931462
Iteration 41, loss = 0.02903571
Iteration 42, loss = 0.02876611
Iteration 43, loss = 0.02867494
Iteration 44, loss = 0.02865188
Iteration 45, loss = 0.02854113
Iteration 46, loss = 0.02863565
Iteration 47, loss = 0.02859737
Iteration 48, loss = 0.02804606
Iteration 49, loss = 0.02792149
Iteration 50, loss = 0.02828418
Iteration 51, loss = 0.02817367
Iteration 52, loss = 0.02749471
Iteration 53, loss = 0.02783317
Iteration 54, loss = 0.02757551
Iteration 55, loss = 0.02778556
Iteration 56, loss = 0.02709425
Iteration 57, loss = 0.02761796
Iteration 58, loss = 0.02736444
Iteration 59, loss = 0.02733262
Iteration 60, loss = 0.02728029
Iteration 61, loss = 0.02731956
Iteration 62, loss = 0.02731743
Iteration 63, loss = 0.02723793
Iteration 64, loss = 0.02708087
Iteration 65, loss = 0.02681677
Iteration 66, loss = 0.02701149
Iteration 67, loss = 0.02693851
Iteration 68, loss = 0.02677560
Iteration 69, loss = 0.02700076
Iteration 70, loss = 0.02663329
Iteration 71, loss = 0.02664561
Iteration 72, loss = 0.02687468
Iteration 73, loss = 0.02655934
Iteration 74, loss = 0.02658164
Iteration 75, loss = 0.02645208
Iteration 76, loss = 0.02706557
Iteration 77, loss = 0.02684477
Iteration 78, loss = 0.02670394
Iteration 79, loss = 0.02663579
Iteration 80, loss = 0.02690369
Iteration 81, loss = 0.02678258
Iteration 82, loss = 0.02677326
Iteration 83, loss = 0.02656204
Iteration 84, loss = 0.02650761
Iteration 85, loss = 0.02676857
Iteration 86, loss = 0.02656812
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69359307
Iteration 2, loss = 0.67864175
Iteration 3, loss = 0.66596918
Iteration 4, loss = 0.65332788
Iteration 5, loss = 0.64030302
Iteration 6, loss = 0.62598849
Iteration 7, loss = 0.61046279
Iteration 8, loss = 0.59371212
Iteration 9, loss = 0.57553399
Iteration 10, loss = 0.55604271
Iteration 11, loss = 0.53526524
Iteration 12, loss = 0.51354724
Iteration 13, loss = 0.49064066
Iteration 14, loss = 0.46718615
Iteration 15, loss = 0.44334161
Iteration 16, loss = 0.41921728
Iteration 17, loss = 0.39538607
Iteration 18, loss = 0.37209497
Iteration 19, loss = 0.34940955
Iteration 20, loss = 0.32771444
Iteration 21, loss = 0.30702232
Iteration 22, loss = 0.28741266
Iteration 23, loss = 0.26897001
Iteration 24, loss = 0.25158703
Iteration 25, loss = 0.23538916
Iteration 26, loss = 0.22023650
Iteration 27, loss = 0.20613566
Iteration 28, loss = 0.19304412
Iteration 29, loss = 0.18090202
Iteration 30, loss = 0.16967476
Iteration 31, loss = 0.15921608
Iteration 32, loss = 0.14961857
Iteration 33, loss = 0.14079292
Iteration 34, loss = 0.13260875
Iteration 35, loss = 0.12503210
Iteration 36, loss = 0.11806216
Iteration 37, loss = 0.11161287
Iteration 38, loss = 0.10566597
Iteration 39, loss = 0.10012483
Iteration 40, loss = 0.09506723
Iteration 41, loss = 0.09029603
Iteration 42, loss = 0.08592230
Iteration 43, loss = 0.08183791
Iteration 44, loss = 0.07806064
Iteration 45, loss = 0.07455855
Iteration 46, loss = 0.07126216
Iteration 47, loss = 0.06819556
Iteration 48, loss = 0.06542798
Iteration 49, loss = 0.06274037
Iteration 50, loss = 0.06028932
Iteration 51, loss = 0.05798229
Iteration 52, loss = 0.05577476
Iteration 53, loss = 0.05374146
Iteration 54, loss = 0.05181360
Iteration 55, loss = 0.05000966
Iteration 56, loss = 0.04834371
Iteration 57, loss = 0.04672856
Iteration 58, loss = 0.04520519
Iteration 59, loss = 0.04377954
Iteration 60, loss = 0.04242951
Iteration 61, loss = 0.04116211
Iteration 62, loss = 0.03996346
Iteration 63, loss = 0.03881549
Iteration 64, loss = 0.03774204
Iteration 65, loss = 0.03668671
Iteration 66, loss = 0.03570625
Iteration 67, loss = 0.03474330
Iteration 68, loss = 0.03382513
Iteration 69, loss = 0.03297805
Iteration 70, loss = 0.03214056
Iteration 71, loss = 0.03136839
Iteration 72, loss = 0.03067458
Iteration 73, loss = 0.02994313
Iteration 74, loss = 0.02927602
Iteration 75, loss = 0.02863359
Iteration 76, loss = 0.02798238
Iteration 77, loss = 0.02739615
Iteration 78, loss = 0.02685837
Iteration 79, loss = 0.02625925
Iteration 80, loss = 0.02573913
Iteration 81, loss = 0.02530798
Iteration 82, loss = 0.02483375
Iteration 83, loss = 0.02435100
Iteration 84, loss = 0.02392316
Iteration 85, loss = 0.02348263
Iteration 86, loss = 0.02311620
Iteration 87, loss = 0.02267968
Iteration 88, loss = 0.02229788
Iteration 89, loss = 0.02191553
Iteration 90, loss = 0.02153155
Iteration 91, loss = 0.02118789
Iteration 92, loss = 0.02087875
Iteration 93, loss = 0.02054383
Iteration 94, loss = 0.02022754
Iteration 95, loss = 0.01993386
Iteration 96, loss = 0.01963212
Iteration 97, loss = 0.01935079
Iteration 98, loss = 0.01908847
Iteration 99, loss = 0.01881844
Iteration 100, loss = 0.01856025
Iteration 101, loss = 0.01834226
Iteration 102, loss = 0.01812796
Iteration 103, loss = 0.01790661
Iteration 104, loss = 0.01768739
Iteration 105, loss = 0.01747794
Iteration 106, loss = 0.01724460
Iteration 107, loss = 0.01701878
Iteration 108, loss = 0.01682346
Iteration 109, loss = 0.01660548
Iteration 110, loss = 0.01639342
Iteration 111, loss = 0.01627856
Iteration 112, loss = 0.01603789
Iteration 113, loss = 0.01587471
Iteration 114, loss = 0.01568912
Iteration 115, loss = 0.01554053
Iteration 116, loss = 0.01540241
Iteration 117, loss = 0.01526047
Iteration 118, loss = 0.01511822
Iteration 119, loss = 0.01495455
Iteration 120, loss = 0.01481539
Iteration 121, loss = 0.01466489
Iteration 122, loss = 0.01454169
Iteration 123, loss = 0.01439256
Iteration 124, loss = 0.01425196
Iteration 125, loss = 0.01414092
Iteration 126, loss = 0.01405903
Iteration 127, loss = 0.01393337
Iteration 128, loss = 0.01382141
Iteration 129, loss = 0.01370490
Iteration 130, loss = 0.01359242
Iteration 131, loss = 0.01348757
Iteration 132, loss = 0.01336614
Iteration 133, loss = 0.01325712
Iteration 134, loss = 0.01325051
Iteration 135, loss = 0.01312520
Iteration 136, loss = 0.01300940
Iteration 137, loss = 0.01286175
Iteration 138, loss = 0.01278686
Iteration 139, loss = 0.01267238
Iteration 140, loss = 0.01259132
Iteration 141, loss = 0.01251394
Iteration 142, loss = 0.01241550
Iteration 143, loss = 0.01233589
Iteration 144, loss = 0.01226207
Iteration 145, loss = 0.01224603
Iteration 146, loss = 0.01207291
Iteration 147, loss = 0.01200683
Iteration 148, loss = 0.01193826
Iteration 149, loss = 0.01186102
Iteration 150, loss = 0.01178399
Iteration 151, loss = 0.01172049
Iteration 152, loss = 0.01164299
Iteration 153, loss = 0.01160277
Iteration 154, loss = 0.01152489
Iteration 155, loss = 0.01149834
Iteration 156, loss = 0.01142319
Iteration 157, loss = 0.01135812
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69689033
Iteration 2, loss = 0.62539923
Iteration 3, loss = 0.51880363
Iteration 4, loss = 0.40172948
Iteration 5, loss = 0.30117541
Iteration 6, loss = 0.22461571
Iteration 7, loss = 0.17096658
Iteration 8, loss = 0.13408466
Iteration 9, loss = 0.10864531
Iteration 10, loss = 0.09053586
Iteration 11, loss = 0.07755942
Iteration 12, loss = 0.06774442
Iteration 13, loss = 0.06025529
Iteration 14, loss = 0.05469161
Iteration 15, loss = 0.04987410
Iteration 16, loss = 0.04615173
Iteration 17, loss = 0.04301970
Iteration 18, loss = 0.04037362
Iteration 19, loss = 0.03829174
Iteration 20, loss = 0.03634809
Iteration 21, loss = 0.03484867
Iteration 22, loss = 0.03330748
Iteration 23, loss = 0.03237610
Iteration 24, loss = 0.03128217
Iteration 25, loss = 0.03034941
Iteration 26, loss = 0.02966024
Iteration 27, loss = 0.02905845
Iteration 28, loss = 0.02829753
Iteration 29, loss = 0.02766168
Iteration 30, loss = 0.02695157
Iteration 31, loss = 0.02680058
Iteration 32, loss = 0.02643725
Iteration 33, loss = 0.02587737
Iteration 34, loss = 0.02567107
Iteration 35, loss = 0.02540189
Iteration 36, loss = 0.02520898
Iteration 37, loss = 0.02470833
Iteration 38, loss = 0.02478087
Iteration 39, loss = 0.02447119
Iteration 40, loss = 0.02406699
Iteration 41, loss = 0.02361671
Iteration 42, loss = 0.02392453
Iteration 43, loss = 0.02341867
Iteration 44, loss = 0.02358467
Iteration 45, loss = 0.02323811
Iteration 46, loss = 0.02295628
Iteration 47, loss = 0.02324748
Iteration 48, loss = 0.02332549
Iteration 49, loss = 0.02282925
Iteration 50, loss = 0.02312321
Iteration 51, loss = 0.02287660
Iteration 52, loss = 0.02252462
Iteration 53, loss = 0.02250394
Iteration 54, loss = 0.02267626
Iteration 55, loss = 0.02230659
Iteration 56, loss = 0.02244849
Iteration 57, loss = 0.02202308
Iteration 58, loss = 0.02209476
Iteration 59, loss = 0.02217651
Iteration 60, loss = 0.02223558
Iteration 61, loss = 0.02187968
Iteration 62, loss = 0.02203895
Iteration 63, loss = 0.02185569
Iteration 64, loss = 0.02198381
Iteration 65, loss = 0.02191913
Iteration 66, loss = 0.02206207
Iteration 67, loss = 0.02187136
Iteration 68, loss = 0.02176652
Iteration 69, loss = 0.02159035
Iteration 70, loss = 0.02166934
Iteration 71, loss = 0.02164781
Iteration 72, loss = 0.02154386
Iteration 73, loss = 0.02165897
Iteration 74, loss = 0.02171142
Iteration 75, loss = 0.02123855
Iteration 76, loss = 0.02142599
Iteration 77, loss = 0.02143094
Iteration 78, loss = 0.02130901
Iteration 79, loss = 0.02119684
Iteration 80, loss = 0.02149969
Iteration 81, loss = 0.02094328
Iteration 82, loss = 0.02138093
Iteration 83, loss = 0.02123339
Iteration 84, loss = 0.02133102
Iteration 85, loss = 0.02099844
Iteration 86, loss = 0.02142811
Iteration 87, loss = 0.02114829
Iteration 88, loss = 0.02107159
Iteration 89, loss = 0.02112392
Iteration 90, loss = 0.02142502
Iteration 91, loss = 0.02125838
Iteration 92, loss = 0.02111842
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67958618
Iteration 2, loss = 0.64807863
Iteration 3, loss = 0.60896110
Iteration 4, loss = 0.55737945
Iteration 5, loss = 0.49741587
Iteration 6, loss = 0.43617224
Iteration 7, loss = 0.37770075
Iteration 8, loss = 0.32439240
Iteration 9, loss = 0.27761274
Iteration 10, loss = 0.23735556
Iteration 11, loss = 0.20330505
Iteration 12, loss = 0.17518443
Iteration 13, loss = 0.15192580
Iteration 14, loss = 0.13289492
Iteration 15, loss = 0.11720024
Iteration 16, loss = 0.10427079
Iteration 17, loss = 0.09343547
Iteration 18, loss = 0.08436498
Iteration 19, loss = 0.07676243
Iteration 20, loss = 0.07025318
Iteration 21, loss = 0.06478759
Iteration 22, loss = 0.06020048
Iteration 23, loss = 0.05577736
Iteration 24, loss = 0.05228138
Iteration 25, loss = 0.04905989
Iteration 26, loss = 0.04619886
Iteration 27, loss = 0.04378862
Iteration 28, loss = 0.04157920
Iteration 29, loss = 0.03959166
Iteration 30, loss = 0.03795971
Iteration 31, loss = 0.03633173
Iteration 32, loss = 0.03483023
Iteration 33, loss = 0.03352018
Iteration 34, loss = 0.03239358
Iteration 35, loss = 0.03136053
Iteration 36, loss = 0.03024767
Iteration 37, loss = 0.02937375
Iteration 38, loss = 0.02859732
Iteration 39, loss = 0.02784012
Iteration 40, loss = 0.02709430
Iteration 41, loss = 0.02638300
Iteration 42, loss = 0.02571873
Iteration 43, loss = 0.02516371
Iteration 44, loss = 0.02469231
Iteration 45, loss = 0.02412144
Iteration 46, loss = 0.02369444
Iteration 47, loss = 0.02328285
Iteration 48, loss = 0.02285680
Iteration 49, loss = 0.02247593
Iteration 50, loss = 0.02204756
Iteration 51, loss = 0.02178354
Iteration 52, loss = 0.02149961
Iteration 53, loss = 0.02127670
Iteration 54, loss = 0.02098717
Iteration 55, loss = 0.02060886
Iteration 56, loss = 0.02048315
Iteration 57, loss = 0.02006548
Iteration 58, loss = 0.01994908
Iteration 59, loss = 0.01978522
Iteration 60, loss = 0.01948248
Iteration 61, loss = 0.01935774
Iteration 62, loss = 0.01914560
Iteration 63, loss = 0.01915385
Iteration 64, loss = 0.01892157
Iteration 65, loss = 0.01880338
Iteration 66, loss = 0.01861168
Iteration 67, loss = 0.01839337
Iteration 68, loss = 0.01825800
Iteration 69, loss = 0.01810674
Iteration 70, loss = 0.01797934
Iteration 71, loss = 0.01788770
Iteration 72, loss = 0.01762388
Iteration 73, loss = 0.01772816
Iteration 74, loss = 0.01747305
Iteration 75, loss = 0.01740232
Iteration 76, loss = 0.01735669
Iteration 77, loss = 0.01726657
Iteration 78, loss = 0.01715848
Iteration 79, loss = 0.01703702
Iteration 80, loss = 0.01693673
Iteration 81, loss = 0.01699088
Iteration 82, loss = 0.01686862
Iteration 83, loss = 0.01692071
Iteration 84, loss = 0.01668377
Iteration 85, loss = 0.01665645
Iteration 86, loss = 0.01658623
Iteration 87, loss = 0.01645633
Iteration 88, loss = 0.01645208
Iteration 89, loss = 0.01634635
Iteration 90, loss = 0.01631866
Iteration 91, loss = 0.01626004
Iteration 92, loss = 0.01621936
Iteration 93, loss = 0.01618821
Iteration 94, loss = 0.01619218
Iteration 95, loss = 0.01598083
Iteration 96, loss = 0.01603718
Iteration 97, loss = 0.01601124
Iteration 98, loss = 0.01600286
Iteration 99, loss = 0.01585446
Iteration 100, loss = 0.01596296
Iteration 101, loss = 0.01572833
Iteration 102, loss = 0.01575606
Iteration 103, loss = 0.01566817
Iteration 104, loss = 0.01567987
Iteration 105, loss = 0.01576148
Iteration 106, loss = 0.01566214
Iteration 107, loss = 0.01562912
Iteration 108, loss = 0.01565382
Iteration 109, loss = 0.01557272
Iteration 110, loss = 0.01574498
Iteration 111, loss = 0.01550404
Iteration 112, loss = 0.01550888
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68434453
Iteration 2, loss = 0.61272973
Iteration 3, loss = 0.51155977
Iteration 4, loss = 0.40401212
Iteration 5, loss = 0.31033569
Iteration 6, loss = 0.23739040
Iteration 7, loss = 0.18489335
Iteration 8, loss = 0.14746304
Iteration 9, loss = 0.12105827
Iteration 10, loss = 0.10224763
Iteration 11, loss = 0.08818626
Iteration 12, loss = 0.07782932
Iteration 13, loss = 0.06966674
Iteration 14, loss = 0.06346418
Iteration 15, loss = 0.05851629
Iteration 16, loss = 0.05432005
Iteration 17, loss = 0.05097141
Iteration 18, loss = 0.04791611
Iteration 19, loss = 0.04553771
Iteration 20, loss = 0.04361577
Iteration 21, loss = 0.04189650
Iteration 22, loss = 0.04024744
Iteration 23, loss = 0.03924400
Iteration 24, loss = 0.03816301
Iteration 25, loss = 0.03713654
Iteration 26, loss = 0.03628197
Iteration 27, loss = 0.03537368
Iteration 28, loss = 0.03489196
Iteration 29, loss = 0.03381108
Iteration 30, loss = 0.03350327
Iteration 31, loss = 0.03300820
Iteration 32, loss = 0.03265010
Iteration 33, loss = 0.03182652
Iteration 34, loss = 0.03167634
Iteration 35, loss = 0.03134061
Iteration 36, loss = 0.03119112
Iteration 37, loss = 0.03083197
Iteration 38, loss = 0.03054063
Iteration 39, loss = 0.03021577
Iteration 40, loss = 0.02994348
Iteration 41, loss = 0.02988000
Iteration 42, loss = 0.02957503
Iteration 43, loss = 0.02948821
Iteration 44, loss = 0.02941528
Iteration 45, loss = 0.02918805
Iteration 46, loss = 0.02892300
Iteration 47, loss = 0.02900391
Iteration 48, loss = 0.02850350
Iteration 49, loss = 0.02864627
Iteration 50, loss = 0.02854799
Iteration 51, loss = 0.02858115
Iteration 52, loss = 0.02832245
Iteration 53, loss = 0.02816864
Iteration 54, loss = 0.02795349
Iteration 55, loss = 0.02783679
Iteration 56, loss = 0.02803681
Iteration 57, loss = 0.02802962
Iteration 58, loss = 0.02816403
Iteration 59, loss = 0.02737856
Iteration 60, loss = 0.02774843
Iteration 61, loss = 0.02771671
Iteration 62, loss = 0.02747538
Iteration 63, loss = 0.02751545
Iteration 64, loss = 0.02740211
Iteration 65, loss = 0.02727298
Iteration 66, loss = 0.02742592
Iteration 67, loss = 0.02744526
Iteration 68, loss = 0.02707827
Iteration 69, loss = 0.02721814
Iteration 70, loss = 0.02716711
Iteration 71, loss = 0.02711539
Iteration 72, loss = 0.02715999
Iteration 73, loss = 0.02715797
Iteration 74, loss = 0.02708913
Iteration 75, loss = 0.02719499
Iteration 76, loss = 0.02707470
Iteration 77, loss = 0.02682139
Iteration 78, loss = 0.02671011
Iteration 79, loss = 0.02703040
Iteration 80, loss = 0.02682444
Iteration 81, loss = 0.02713140
Iteration 82, loss = 0.02675010
Iteration 83, loss = 0.02694580
Iteration 84, loss = 0.02660800
Iteration 85, loss = 0.02680697
Iteration 86, loss = 0.02626827
Iteration 87, loss = 0.02686654
Iteration 88, loss = 0.02664691
Iteration 89, loss = 0.02665963
Iteration 90, loss = 0.02640660
Iteration 91, loss = 0.02658232
Iteration 92, loss = 0.02653279
Iteration 93, loss = 0.02640474
Iteration 94, loss = 0.02675979
Iteration 95, loss = 0.02656924
Iteration 96, loss = 0.02638246
Iteration 97, loss = 0.02643938
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70119582
Iteration 2, loss = 0.62410220
Iteration 3, loss = 0.51816012
Iteration 4, loss = 0.40184205
Iteration 5, loss = 0.30134970
Iteration 6, loss = 0.22518525
Iteration 7, loss = 0.17174459
Iteration 8, loss = 0.13471669
Iteration 9, loss = 0.10956575
Iteration 10, loss = 0.09177886
Iteration 11, loss = 0.07885055
Iteration 12, loss = 0.06906626
Iteration 13, loss = 0.06193342
Iteration 14, loss = 0.05607658
Iteration 15, loss = 0.05145454
Iteration 16, loss = 0.04759458
Iteration 17, loss = 0.04491621
Iteration 18, loss = 0.04221795
Iteration 19, loss = 0.04003340
Iteration 20, loss = 0.03843604
Iteration 21, loss = 0.03657286
Iteration 22, loss = 0.03533198
Iteration 23, loss = 0.03428144
Iteration 24, loss = 0.03337127
Iteration 25, loss = 0.03260878
Iteration 26, loss = 0.03165106
Iteration 27, loss = 0.03130695
Iteration 28, loss = 0.02998420
Iteration 29, loss = 0.02983048
Iteration 30, loss = 0.02906603
Iteration 31, loss = 0.02885972
Iteration 32, loss = 0.02820318
Iteration 33, loss = 0.02796675
Iteration 34, loss = 0.02796024
Iteration 35, loss = 0.02728083
Iteration 36, loss = 0.02714691
Iteration 37, loss = 0.02679946
Iteration 38, loss = 0.02646969
Iteration 39, loss = 0.02636548
Iteration 40, loss = 0.02621456
Iteration 41, loss = 0.02602272
Iteration 42, loss = 0.02585937
Iteration 43, loss = 0.02573470
Iteration 44, loss = 0.02583682
Iteration 45, loss = 0.02552410
Iteration 46, loss = 0.02526588
Iteration 47, loss = 0.02527309
Iteration 48, loss = 0.02522178
Iteration 49, loss = 0.02504939
Iteration 50, loss = 0.02484165
Iteration 51, loss = 0.02483691
Iteration 52, loss = 0.02476657
Iteration 53, loss = 0.02456481
Iteration 54, loss = 0.02504516
Iteration 55, loss = 0.02431812
Iteration 56, loss = 0.02436574
Iteration 57, loss = 0.02414500
Iteration 58, loss = 0.02441832
Iteration 59, loss = 0.02421299
Iteration 60, loss = 0.02436255
Iteration 61, loss = 0.02403597
Iteration 62, loss = 0.02419040
Iteration 63, loss = 0.02403754
Iteration 64, loss = 0.02403178
Iteration 65, loss = 0.02406818
Iteration 66, loss = 0.02381387
Iteration 67, loss = 0.02395179
Iteration 68, loss = 0.02407785
Iteration 69, loss = 0.02378868
Iteration 70, loss = 0.02356766
Iteration 71, loss = 0.02366346
Iteration 72, loss = 0.02375752
Iteration 73, loss = 0.02354171
Iteration 74, loss = 0.02376239
Iteration 75, loss = 0.02373081
Iteration 76, loss = 0.02341321
Iteration 77, loss = 0.02383902
Iteration 78, loss = 0.02368147
Iteration 79, loss = 0.02339749
Iteration 80, loss = 0.02351684
Iteration 81, loss = 0.02362378
Iteration 82, loss = 0.02372029
Iteration 83, loss = 0.02328482
Iteration 84, loss = 0.02344275
Iteration 85, loss = 0.02330548
Iteration 86, loss = 0.02310632
Iteration 87, loss = 0.02356782
Iteration 88, loss = 0.02359820
Iteration 89, loss = 0.02349149
Iteration 90, loss = 0.02346104
Iteration 91, loss = 0.02324200
Iteration 92, loss = 0.02310540
Iteration 93, loss = 0.02321883
Iteration 94, loss = 0.02325852
Iteration 95, loss = 0.02326587
Iteration 96, loss = 0.02325787
Iteration 97, loss = 0.02322841
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70571648
Iteration 2, loss = 0.67979689
Iteration 3, loss = 0.64834606
Iteration 4, loss = 0.60771095
Iteration 5, loss = 0.55998390
Iteration 6, loss = 0.50816513
Iteration 7, loss = 0.45545411
Iteration 8, loss = 0.40385337
Iteration 9, loss = 0.35486809
Iteration 10, loss = 0.31051545
Iteration 11, loss = 0.27137119
Iteration 12, loss = 0.23721488
Iteration 13, loss = 0.20771436
Iteration 14, loss = 0.18272308
Iteration 15, loss = 0.16128704
Iteration 16, loss = 0.14330987
Iteration 17, loss = 0.12791875
Iteration 18, loss = 0.11487193
Iteration 19, loss = 0.10382986
Iteration 20, loss = 0.09444773
Iteration 21, loss = 0.08629693
Iteration 22, loss = 0.07920935
Iteration 23, loss = 0.07306483
Iteration 24, loss = 0.06753457
Iteration 25, loss = 0.06312460
Iteration 26, loss = 0.05892293
Iteration 27, loss = 0.05531988
Iteration 28, loss = 0.05195117
Iteration 29, loss = 0.04890402
Iteration 30, loss = 0.04633636
Iteration 31, loss = 0.04395173
Iteration 32, loss = 0.04181969
Iteration 33, loss = 0.03976326
Iteration 34, loss = 0.03798111
Iteration 35, loss = 0.03634138
Iteration 36, loss = 0.03487307
Iteration 37, loss = 0.03358891
Iteration 38, loss = 0.03237695
Iteration 39, loss = 0.03118148
Iteration 40, loss = 0.03021987
Iteration 41, loss = 0.02922493
Iteration 42, loss = 0.02839699
Iteration 43, loss = 0.02754570
Iteration 44, loss = 0.02679979
Iteration 45, loss = 0.02606684
Iteration 46, loss = 0.02530199
Iteration 47, loss = 0.02466681
Iteration 48, loss = 0.02414758
Iteration 49, loss = 0.02349426
Iteration 50, loss = 0.02295921
Iteration 51, loss = 0.02248544
Iteration 52, loss = 0.02193120
Iteration 53, loss = 0.02141059
Iteration 54, loss = 0.02092657
Iteration 55, loss = 0.02061689
Iteration 56, loss = 0.02023965
Iteration 57, loss = 0.01991681
Iteration 58, loss = 0.01960253
Iteration 59, loss = 0.01967292
Iteration 60, loss = 0.01926118
Iteration 61, loss = 0.01899340
Iteration 62, loss = 0.01861877
Iteration 63, loss = 0.01827322
Iteration 64, loss = 0.01782114
Iteration 65, loss = 0.01765643
Iteration 66, loss = 0.01735839
Iteration 67, loss = 0.01713742
Iteration 68, loss = 0.01710745
Iteration 69, loss = 0.01683130
Iteration 70, loss = 0.01651364
Iteration 71, loss = 0.01642868
Iteration 72, loss = 0.01610281
Iteration 73, loss = 0.01590696
Iteration 74, loss = 0.01569252
Iteration 75, loss = 0.01549607
Iteration 76, loss = 0.01535511
Iteration 77, loss = 0.01514567
Iteration 78, loss = 0.01491825
Iteration 79, loss = 0.01480985
Iteration 80, loss = 0.01464863
Iteration 81, loss = 0.01465492
Iteration 82, loss = 0.01447456
Iteration 83, loss = 0.01442267
Iteration 84, loss = 0.01422846
Iteration 85, loss = 0.01413457
Iteration 86, loss = 0.01415299
Iteration 87, loss = 0.01409995
Iteration 88, loss = 0.01396641
Iteration 89, loss = 0.01386899
Iteration 90, loss = 0.01348595
Iteration 91, loss = 0.01376877
Iteration 92, loss = 0.01366981
Iteration 93, loss = 0.01363344
Iteration 94, loss = 0.01366390
Iteration 95, loss = 0.01345066
Iteration 96, loss = 0.01352314
Iteration 97, loss = 0.01342076
Iteration 98, loss = 0.01329189
Iteration 99, loss = 0.01310037
Iteration 100, loss = 0.01297810
Iteration 101, loss = 0.01281851
Iteration 102, loss = 0.01268837
Iteration 103, loss = 0.01261720
Iteration 104, loss = 0.01264272
Iteration 105, loss = 0.01262730
Iteration 106, loss = 0.01256611
Iteration 107, loss = 0.01242878
Iteration 108, loss = 0.01248507
Iteration 109, loss = 0.01233454
Iteration 110, loss = 0.01224266
Iteration 111, loss = 0.01226676
Iteration 112, loss = 0.01235476
Iteration 113, loss = 0.01222495
Iteration 114, loss = 0.01196974
Iteration 115, loss = 0.01194435
Iteration 116, loss = 0.01200439
Iteration 117, loss = 0.01195211
Iteration 118, loss = 0.01197152
Iteration 119, loss = 0.01182122
Iteration 120, loss = 0.01173911
Iteration 121, loss = 0.01174154
Iteration 122, loss = 0.01171679
Iteration 123, loss = 0.01170581
Iteration 124, loss = 0.01156837
Iteration 125, loss = 0.01158060
Iteration 126, loss = 0.01146921
Iteration 127, loss = 0.01156290
Iteration 128, loss = 0.01154856
Iteration 129, loss = 0.01145105
Iteration 130, loss = 0.01147648
Iteration 131, loss = 0.01139035
Iteration 132, loss = 0.01128279
Iteration 133, loss = 0.01133323
Iteration 134, loss = 0.01135133
Iteration 135, loss = 0.01129062
Iteration 136, loss = 0.01121938
Iteration 137, loss = 0.01134198
Iteration 138, loss = 0.01130834
Iteration 139, loss = 0.01122116
Iteration 140, loss = 0.01149241
Iteration 141, loss = 0.01147452
Iteration 142, loss = 0.01150099
Iteration 143, loss = 0.01137741
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69249511
Iteration 2, loss = 0.61928946
Iteration 3, loss = 0.51367578
Iteration 4, loss = 0.39546432
Iteration 5, loss = 0.29612531
Iteration 6, loss = 0.22195550
Iteration 7, loss = 0.17019659
Iteration 8, loss = 0.13502581
Iteration 9, loss = 0.11049234
Iteration 10, loss = 0.09324065
Iteration 11, loss = 0.08059627
Iteration 12, loss = 0.07158199
Iteration 13, loss = 0.06422444
Iteration 14, loss = 0.05851361
Iteration 15, loss = 0.05426330
Iteration 16, loss = 0.05053118
Iteration 17, loss = 0.04763620
Iteration 18, loss = 0.04516781
Iteration 19, loss = 0.04302059
Iteration 20, loss = 0.04115790
Iteration 21, loss = 0.03977110
Iteration 22, loss = 0.03855569
Iteration 23, loss = 0.03763367
Iteration 24, loss = 0.03622871
Iteration 25, loss = 0.03547588
Iteration 26, loss = 0.03479803
Iteration 27, loss = 0.03399944
Iteration 28, loss = 0.03343654
Iteration 29, loss = 0.03262086
Iteration 30, loss = 0.03234747
Iteration 31, loss = 0.03182965
Iteration 32, loss = 0.03160513
Iteration 33, loss = 0.03131637
Iteration 34, loss = 0.03070102
Iteration 35, loss = 0.03045296
Iteration 36, loss = 0.03013083
Iteration 37, loss = 0.02992771
Iteration 38, loss = 0.02975524
Iteration 39, loss = 0.02963963
Iteration 40, loss = 0.02967330
Iteration 41, loss = 0.02904414
Iteration 42, loss = 0.02939166
Iteration 43, loss = 0.02916417
Iteration 44, loss = 0.02852572
Iteration 45, loss = 0.02869485
Iteration 46, loss = 0.02839921
Iteration 47, loss = 0.02832336
Iteration 48, loss = 0.02828751
Iteration 49, loss = 0.02816957
Iteration 50, loss = 0.02812840
Iteration 51, loss = 0.02808937
Iteration 52, loss = 0.02791774
Iteration 53, loss = 0.02767040
Iteration 54, loss = 0.02753456
Iteration 55, loss = 0.02769564
Iteration 56, loss = 0.02788414
Iteration 57, loss = 0.02769003
Iteration 58, loss = 0.02793582
Iteration 59, loss = 0.02714002
Iteration 60, loss = 0.02742346
Iteration 61, loss = 0.02731731
Iteration 62, loss = 0.02728900
Iteration 63, loss = 0.02704156
Iteration 64, loss = 0.02729487
Iteration 65, loss = 0.02707699
Iteration 66, loss = 0.02712848
Iteration 67, loss = 0.02705512
Iteration 68, loss = 0.02699215
Iteration 69, loss = 0.02674585
Iteration 70, loss = 0.02713490
Iteration 71, loss = 0.02686294
Iteration 72, loss = 0.02664781
Iteration 73, loss = 0.02671120
Iteration 74, loss = 0.02679264
Iteration 75, loss = 0.02700637
Iteration 76, loss = 0.02686518
Iteration 77, loss = 0.02682355
Iteration 78, loss = 0.02658816
Iteration 79, loss = 0.02663477
Iteration 80, loss = 0.02676720
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72252097
Iteration 2, loss = 0.67906902
Iteration 3, loss = 0.62238682
Iteration 4, loss = 0.55722951
Iteration 5, loss = 0.48741808
Iteration 6, loss = 0.41817136
Iteration 7, loss = 0.35404299
Iteration 8, loss = 0.29714867
Iteration 9, loss = 0.24832529
Iteration 10, loss = 0.20722290
Iteration 11, loss = 0.17451731
Iteration 12, loss = 0.14855097
Iteration 13, loss = 0.12787818
Iteration 14, loss = 0.11145783
Iteration 15, loss = 0.09843020
Iteration 16, loss = 0.08771426
Iteration 17, loss = 0.07922512
Iteration 18, loss = 0.07188359
Iteration 19, loss = 0.06597366
Iteration 20, loss = 0.06091866
Iteration 21, loss = 0.05665260
Iteration 22, loss = 0.05301615
Iteration 23, loss = 0.04990280
Iteration 24, loss = 0.04714778
Iteration 25, loss = 0.04463733
Iteration 26, loss = 0.04246980
Iteration 27, loss = 0.04060135
Iteration 28, loss = 0.03896095
Iteration 29, loss = 0.03753672
Iteration 30, loss = 0.03624296
Iteration 31, loss = 0.03498288
Iteration 32, loss = 0.03386970
Iteration 33, loss = 0.03274335
Iteration 34, loss = 0.03196739
Iteration 35, loss = 0.03103364
Iteration 36, loss = 0.03033604
Iteration 37, loss = 0.02955949
Iteration 38, loss = 0.02901099
Iteration 39, loss = 0.02840320
Iteration 40, loss = 0.02788778
Iteration 41, loss = 0.02739300
Iteration 42, loss = 0.02675513
Iteration 43, loss = 0.02643029
Iteration 44, loss = 0.02599711
Iteration 45, loss = 0.02567657
Iteration 46, loss = 0.02553908
Iteration 47, loss = 0.02498052
Iteration 48, loss = 0.02465178
Iteration 49, loss = 0.02441906
Iteration 50, loss = 0.02412281
Iteration 51, loss = 0.02382021
Iteration 52, loss = 0.02374042
Iteration 53, loss = 0.02343317
Iteration 54, loss = 0.02311744
Iteration 55, loss = 0.02300958
Iteration 56, loss = 0.02275282
Iteration 57, loss = 0.02256923
Iteration 58, loss = 0.02241999
Iteration 59, loss = 0.02210188
Iteration 60, loss = 0.02210347
Iteration 61, loss = 0.02205723
Iteration 62, loss = 0.02171361
Iteration 63, loss = 0.02168259
Iteration 64, loss = 0.02144301
Iteration 65, loss = 0.02157972
Iteration 66, loss = 0.02137733
Iteration 67, loss = 0.02121920
Iteration 68, loss = 0.02093240
Iteration 69, loss = 0.02091926
Iteration 70, loss = 0.02109785
Iteration 71, loss = 0.02077312
Iteration 72, loss = 0.02079885
Iteration 73, loss = 0.02053297
Iteration 74, loss = 0.02040619
Iteration 75, loss = 0.02040795
Iteration 76, loss = 0.02024398
Iteration 77, loss = 0.02040630
Iteration 78, loss = 0.02033937
Iteration 79, loss = 0.02011243
Iteration 80, loss = 0.02021061
Iteration 81, loss = 0.02005109
Iteration 82, loss = 0.01995765
Iteration 83, loss = 0.01982104
Iteration 84, loss = 0.01987569
Iteration 85, loss = 0.01973074
Iteration 86, loss = 0.01966590
Iteration 87, loss = 0.01958282
Iteration 88, loss = 0.01967366
Iteration 89, loss = 0.01967693
Iteration 90, loss = 0.01959165
Iteration 91, loss = 0.01944440
Iteration 92, loss = 0.01959077
Iteration 93, loss = 0.01943269
Iteration 94, loss = 0.01944758
Iteration 95, loss = 0.01923342
Iteration 96, loss = 0.01928156
Iteration 97, loss = 0.01917516
Iteration 98, loss = 0.01926738
Iteration 99, loss = 0.01917322
Iteration 100, loss = 0.01913210
Iteration 101, loss = 0.01905621
Iteration 102, loss = 0.01902632
Iteration 103, loss = 0.01912793
Iteration 104, loss = 0.01901557
Iteration 105, loss = 0.01909169
Iteration 106, loss = 0.01890533
Iteration 107, loss = 0.01901404
Iteration 108, loss = 0.01892889
Iteration 109, loss = 0.01897419
Iteration 110, loss = 0.01887627
Iteration 111, loss = 0.01868622
Iteration 112, loss = 0.01863313
Iteration 113, loss = 0.01881042
Iteration 114, loss = 0.01869354
Iteration 115, loss = 0.01883472
Iteration 116, loss = 0.01880827
Iteration 117, loss = 0.01869657
Iteration 118, loss = 0.01873274
Iteration 119, loss = 0.01851454
Iteration 120, loss = 0.01870304
Iteration 121, loss = 0.01861384
Iteration 122, loss = 0.01876561
Iteration 123, loss = 0.01870289
Iteration 124, loss = 0.01853586
Iteration 125, loss = 0.01841891
Iteration 126, loss = 0.01855594
Iteration 127, loss = 0.01846336
Iteration 128, loss = 0.01852002
Iteration 129, loss = 0.01843687
Iteration 130, loss = 0.01855931
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69173092
Iteration 2, loss = 0.62220908
Iteration 3, loss = 0.52294411
Iteration 4, loss = 0.41111127
Iteration 5, loss = 0.31352011
Iteration 6, loss = 0.23761754
Iteration 7, loss = 0.18299124
Iteration 8, loss = 0.14466319
Iteration 9, loss = 0.11799814
Iteration 10, loss = 0.09854669
Iteration 11, loss = 0.08495388
Iteration 12, loss = 0.07432800
Iteration 13, loss = 0.06647611
Iteration 14, loss = 0.06022465
Iteration 15, loss = 0.05532000
Iteration 16, loss = 0.05113581
Iteration 17, loss = 0.04792459
Iteration 18, loss = 0.04519256
Iteration 19, loss = 0.04315395
Iteration 20, loss = 0.04126809
Iteration 21, loss = 0.03922558
Iteration 22, loss = 0.03786875
Iteration 23, loss = 0.03631966
Iteration 24, loss = 0.03538852
Iteration 25, loss = 0.03439771
Iteration 26, loss = 0.03359831
Iteration 27, loss = 0.03301346
Iteration 28, loss = 0.03209646
Iteration 29, loss = 0.03178159
Iteration 30, loss = 0.03121784
Iteration 31, loss = 0.03058101
Iteration 32, loss = 0.03012183
Iteration 33, loss = 0.02980290
Iteration 34, loss = 0.02941280
Iteration 35, loss = 0.02902650
Iteration 36, loss = 0.02876974
Iteration 37, loss = 0.02854912
Iteration 38, loss = 0.02830450
Iteration 39, loss = 0.02801407
Iteration 40, loss = 0.02792516
Iteration 41, loss = 0.02759873
Iteration 42, loss = 0.02722267
Iteration 43, loss = 0.02741867
Iteration 44, loss = 0.02716903
Iteration 45, loss = 0.02689797
Iteration 46, loss = 0.02655754
Iteration 47, loss = 0.02653714
Iteration 48, loss = 0.02678000
Iteration 49, loss = 0.02623099
Iteration 50, loss = 0.02628838
Iteration 51, loss = 0.02606251
Iteration 52, loss = 0.02632424
Iteration 53, loss = 0.02626355
Iteration 54, loss = 0.02587719
Iteration 55, loss = 0.02609505
Iteration 56, loss = 0.02574627
Iteration 57, loss = 0.02563409
Iteration 58, loss = 0.02576748
Iteration 59, loss = 0.02564201
Iteration 60, loss = 0.02536007
Iteration 61, loss = 0.02577707
Iteration 62, loss = 0.02581355
Iteration 63, loss = 0.02549432
Iteration 64, loss = 0.02535650
Iteration 65, loss = 0.02557529
Iteration 66, loss = 0.02528833
Iteration 67, loss = 0.02520974
Iteration 68, loss = 0.02514220
Iteration 69, loss = 0.02514881
Iteration 70, loss = 0.02483386
Iteration 71, loss = 0.02496029
Iteration 72, loss = 0.02502348
Iteration 73, loss = 0.02500641
Iteration 74, loss = 0.02517205
Iteration 75, loss = 0.02503055
Iteration 76, loss = 0.02469261
Iteration 77, loss = 0.02432943
Iteration 78, loss = 0.02505355
Iteration 79, loss = 0.02486969
Iteration 80, loss = 0.02472846
Iteration 81, loss = 0.02436370
Iteration 82, loss = 0.02460103
Iteration 83, loss = 0.02463493
Iteration 84, loss = 0.02447980
Iteration 85, loss = 0.02459949
Iteration 86, loss = 0.02456777
Iteration 87, loss = 0.02449224
Iteration 88, loss = 0.02459911
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69970648
Iteration 2, loss = 0.68943645
Iteration 3, loss = 0.68070594
Iteration 4, loss = 0.67196324
Iteration 5, loss = 0.66238631
Iteration 6, loss = 0.65159758
Iteration 7, loss = 0.63925104
Iteration 8, loss = 0.62527026
Iteration 9, loss = 0.60951488
Iteration 10, loss = 0.59191483
Iteration 11, loss = 0.57239271
Iteration 12, loss = 0.55123905
Iteration 13, loss = 0.52871431
Iteration 14, loss = 0.50521760
Iteration 15, loss = 0.48105522
Iteration 16, loss = 0.45642043
Iteration 17, loss = 0.43202874
Iteration 18, loss = 0.40804637
Iteration 19, loss = 0.38459784
Iteration 20, loss = 0.36196645
Iteration 21, loss = 0.34023231
Iteration 22, loss = 0.31943632
Iteration 23, loss = 0.29971174
Iteration 24, loss = 0.28099608
Iteration 25, loss = 0.26336894
Iteration 26, loss = 0.24688880
Iteration 27, loss = 0.23134884
Iteration 28, loss = 0.21687725
Iteration 29, loss = 0.20341178
Iteration 30, loss = 0.19091057
Iteration 31, loss = 0.17928417
Iteration 32, loss = 0.16849109
Iteration 33, loss = 0.15849660
Iteration 34, loss = 0.14933257
Iteration 35, loss = 0.14072036
Iteration 36, loss = 0.13279901
Iteration 37, loss = 0.12548406
Iteration 38, loss = 0.11866031
Iteration 39, loss = 0.11235541
Iteration 40, loss = 0.10656949
Iteration 41, loss = 0.10116050
Iteration 42, loss = 0.09619835
Iteration 43, loss = 0.09151024
Iteration 44, loss = 0.08724065
Iteration 45, loss = 0.08323886
Iteration 46, loss = 0.07956634
Iteration 47, loss = 0.07608043
Iteration 48, loss = 0.07286978
Iteration 49, loss = 0.06986086
Iteration 50, loss = 0.06709558
Iteration 51, loss = 0.06441544
Iteration 52, loss = 0.06194847
Iteration 53, loss = 0.05964734
Iteration 54, loss = 0.05755081
Iteration 55, loss = 0.05545062
Iteration 56, loss = 0.05358608
Iteration 57, loss = 0.05180891
Iteration 58, loss = 0.05007419
Iteration 59, loss = 0.04848898
Iteration 60, loss = 0.04698646
Iteration 61, loss = 0.04559141
Iteration 62, loss = 0.04423130
Iteration 63, loss = 0.04296086
Iteration 64, loss = 0.04174651
Iteration 65, loss = 0.04057734
Iteration 66, loss = 0.03949313
Iteration 67, loss = 0.03848350
Iteration 68, loss = 0.03744861
Iteration 69, loss = 0.03652416
Iteration 70, loss = 0.03560826
Iteration 71, loss = 0.03477888
Iteration 72, loss = 0.03394755
Iteration 73, loss = 0.03325348
Iteration 74, loss = 0.03246551
Iteration 75, loss = 0.03171494
Iteration 76, loss = 0.03101960
Iteration 77, loss = 0.03030724
Iteration 78, loss = 0.02972072
Iteration 79, loss = 0.02911744
Iteration 80, loss = 0.02868462
Iteration 81, loss = 0.02804465
Iteration 82, loss = 0.02752772
Iteration 83, loss = 0.02703025
Iteration 84, loss = 0.02655871
Iteration 85, loss = 0.02608353
Iteration 86, loss = 0.02561578
Iteration 87, loss = 0.02515808
Iteration 88, loss = 0.02469772
Iteration 89, loss = 0.02430674
Iteration 90, loss = 0.02392201
Iteration 91, loss = 0.02350820
Iteration 92, loss = 0.02315520
Iteration 93, loss = 0.02279599
Iteration 94, loss = 0.02253417
Iteration 95, loss = 0.02215385
Iteration 96, loss = 0.02191333
Iteration 97, loss = 0.02162097
Iteration 98, loss = 0.02134554
Iteration 99, loss = 0.02105981
Iteration 100, loss = 0.02079226
Iteration 101, loss = 0.02053818
Iteration 102, loss = 0.02023535
Iteration 103, loss = 0.01998460
Iteration 104, loss = 0.01974543
Iteration 105, loss = 0.01949415
Iteration 106, loss = 0.01928610
Iteration 107, loss = 0.01900358
Iteration 108, loss = 0.01883577
Iteration 109, loss = 0.01855888
Iteration 110, loss = 0.01836909
Iteration 111, loss = 0.01816330
Iteration 112, loss = 0.01796583
Iteration 113, loss = 0.01783573
Iteration 114, loss = 0.01757419
Iteration 115, loss = 0.01741837
Iteration 116, loss = 0.01723096
Iteration 117, loss = 0.01704775
Iteration 118, loss = 0.01688954
Iteration 119, loss = 0.01674678
Iteration 120, loss = 0.01667370
Iteration 121, loss = 0.01645877
Iteration 122, loss = 0.01630933
Iteration 123, loss = 0.01615603
Iteration 124, loss = 0.01602296
Iteration 125, loss = 0.01584413
Iteration 126, loss = 0.01572913
Iteration 127, loss = 0.01560212
Iteration 128, loss = 0.01548029
Iteration 129, loss = 0.01537447
Iteration 130, loss = 0.01525822
Iteration 131, loss = 0.01514822
Iteration 132, loss = 0.01503094
Iteration 133, loss = 0.01493902
Iteration 134, loss = 0.01481122
Iteration 135, loss = 0.01470235
Iteration 136, loss = 0.01462768
Iteration 137, loss = 0.01456076
Iteration 138, loss = 0.01444904
Iteration 139, loss = 0.01434352
Iteration 140, loss = 0.01424596
Iteration 141, loss = 0.01413966
Iteration 142, loss = 0.01405777
Iteration 143, loss = 0.01394987
Iteration 144, loss = 0.01384687
Iteration 145, loss = 0.01378160
Iteration 146, loss = 0.01370057
Iteration 147, loss = 0.01361782
Iteration 148, loss = 0.01354922
Iteration 149, loss = 0.01346202
Iteration 150, loss = 0.01337079
Iteration 151, loss = 0.01328417
Iteration 152, loss = 0.01320644
Iteration 153, loss = 0.01312493
Iteration 154, loss = 0.01305262
Iteration 155, loss = 0.01297595
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69310895
Iteration 2, loss = 0.67893129
Iteration 3, loss = 0.66691460
Iteration 4, loss = 0.65500546
Iteration 5, loss = 0.64253439
Iteration 6, loss = 0.62898085
Iteration 7, loss = 0.61379192
Iteration 8, loss = 0.59712964
Iteration 9, loss = 0.57875186
Iteration 10, loss = 0.55904357
Iteration 11, loss = 0.53800749
Iteration 12, loss = 0.51591575
Iteration 13, loss = 0.49308984
Iteration 14, loss = 0.46975916
Iteration 15, loss = 0.44635371
Iteration 16, loss = 0.42306698
Iteration 17, loss = 0.40002982
Iteration 18, loss = 0.37752766
Iteration 19, loss = 0.35574944
Iteration 20, loss = 0.33471278
Iteration 21, loss = 0.31453944
Iteration 22, loss = 0.29533201
Iteration 23, loss = 0.27704212
Iteration 24, loss = 0.25982614
Iteration 25, loss = 0.24359889
Iteration 26, loss = 0.22837236
Iteration 27, loss = 0.21405138
Iteration 28, loss = 0.20068988
Iteration 29, loss = 0.18826484
Iteration 30, loss = 0.17669958
Iteration 31, loss = 0.16600173
Iteration 32, loss = 0.15602089
Iteration 33, loss = 0.14677035
Iteration 34, loss = 0.13819517
Iteration 35, loss = 0.13026800
Iteration 36, loss = 0.12291748
Iteration 37, loss = 0.11616728
Iteration 38, loss = 0.10990520
Iteration 39, loss = 0.10409430
Iteration 40, loss = 0.09871591
Iteration 41, loss = 0.09370722
Iteration 42, loss = 0.08911332
Iteration 43, loss = 0.08478649
Iteration 44, loss = 0.08076932
Iteration 45, loss = 0.07707793
Iteration 46, loss = 0.07361095
Iteration 47, loss = 0.07038938
Iteration 48, loss = 0.06738150
Iteration 49, loss = 0.06455609
Iteration 50, loss = 0.06192253
Iteration 51, loss = 0.05945840
Iteration 52, loss = 0.05713544
Iteration 53, loss = 0.05496673
Iteration 54, loss = 0.05293365
Iteration 55, loss = 0.05101074
Iteration 56, loss = 0.04919695
Iteration 57, loss = 0.04748953
Iteration 58, loss = 0.04587621
Iteration 59, loss = 0.04436004
Iteration 60, loss = 0.04294302
Iteration 61, loss = 0.04157985
Iteration 62, loss = 0.04029660
Iteration 63, loss = 0.03907784
Iteration 64, loss = 0.03792472
Iteration 65, loss = 0.03683464
Iteration 66, loss = 0.03580134
Iteration 67, loss = 0.03481150
Iteration 68, loss = 0.03386977
Iteration 69, loss = 0.03296511
Iteration 70, loss = 0.03212769
Iteration 71, loss = 0.03130103
Iteration 72, loss = 0.03051720
Iteration 73, loss = 0.02977877
Iteration 74, loss = 0.02906364
Iteration 75, loss = 0.02838344
Iteration 76, loss = 0.02772654
Iteration 77, loss = 0.02708368
Iteration 78, loss = 0.02649001
Iteration 79, loss = 0.02591434
Iteration 80, loss = 0.02535983
Iteration 81, loss = 0.02483275
Iteration 82, loss = 0.02431899
Iteration 83, loss = 0.02383278
Iteration 84, loss = 0.02336055
Iteration 85, loss = 0.02290254
Iteration 86, loss = 0.02246357
Iteration 87, loss = 0.02203541
Iteration 88, loss = 0.02162702
Iteration 89, loss = 0.02124578
Iteration 90, loss = 0.02085551
Iteration 91, loss = 0.02048503
Iteration 92, loss = 0.02014170
Iteration 93, loss = 0.01978951
Iteration 94, loss = 0.01947486
Iteration 95, loss = 0.01913725
Iteration 96, loss = 0.01883463
Iteration 97, loss = 0.01853260
Iteration 98, loss = 0.01824379
Iteration 99, loss = 0.01797580
Iteration 100, loss = 0.01769549
Iteration 101, loss = 0.01743837
Iteration 102, loss = 0.01719085
Iteration 103, loss = 0.01693491
Iteration 104, loss = 0.01669773
Iteration 105, loss = 0.01646535
Iteration 106, loss = 0.01624765
Iteration 107, loss = 0.01602455
Iteration 108, loss = 0.01580967
Iteration 109, loss = 0.01559815
Iteration 110, loss = 0.01541389
Iteration 111, loss = 0.01520691
Iteration 112, loss = 0.01501892
Iteration 113, loss = 0.01484524
Iteration 114, loss = 0.01465738
Iteration 115, loss = 0.01449551
Iteration 116, loss = 0.01431492
Iteration 117, loss = 0.01415056
Iteration 118, loss = 0.01399673
Iteration 119, loss = 0.01383364
Iteration 120, loss = 0.01369085
Iteration 121, loss = 0.01353089
Iteration 122, loss = 0.01338603
Iteration 123, loss = 0.01325877
Iteration 124, loss = 0.01310830
Iteration 125, loss = 0.01297272
Iteration 126, loss = 0.01284113
Iteration 127, loss = 0.01271962
Iteration 128, loss = 0.01258869
Iteration 129, loss = 0.01246906
Iteration 130, loss = 0.01235136
Iteration 131, loss = 0.01223546
Iteration 132, loss = 0.01212328
Iteration 133, loss = 0.01201645
Iteration 134, loss = 0.01190187
Iteration 135, loss = 0.01180038
Iteration 136, loss = 0.01169364
Iteration 137, loss = 0.01160076
Iteration 138, loss = 0.01149693
Iteration 139, loss = 0.01139864
Iteration 140, loss = 0.01130797
Iteration 141, loss = 0.01121897
Iteration 142, loss = 0.01111904
Iteration 143, loss = 0.01103700
Iteration 144, loss = 0.01094706
Iteration 145, loss = 0.01086007
Iteration 146, loss = 0.01078381
Iteration 147, loss = 0.01069046
Iteration 148, loss = 0.01061579
Iteration 149, loss = 0.01053295
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72170201
Iteration 2, loss = 0.68484863
Iteration 3, loss = 0.64682756
Iteration 4, loss = 0.60389416
Iteration 5, loss = 0.55696578
Iteration 6, loss = 0.50736871
Iteration 7, loss = 0.45718893
Iteration 8, loss = 0.40756671
Iteration 9, loss = 0.36037125
Iteration 10, loss = 0.31699261
Iteration 11, loss = 0.27803983
Iteration 12, loss = 0.24413817
Iteration 13, loss = 0.21457891
Iteration 14, loss = 0.18933602
Iteration 15, loss = 0.16775093
Iteration 16, loss = 0.14939305
Iteration 17, loss = 0.13370666
Iteration 18, loss = 0.12034384
Iteration 19, loss = 0.10890317
Iteration 20, loss = 0.09901636
Iteration 21, loss = 0.09053981
Iteration 22, loss = 0.08317202
Iteration 23, loss = 0.07664848
Iteration 24, loss = 0.07101265
Iteration 25, loss = 0.06608651
Iteration 26, loss = 0.06169471
Iteration 27, loss = 0.05789404
Iteration 28, loss = 0.05441304
Iteration 29, loss = 0.05123899
Iteration 30, loss = 0.04834528
Iteration 31, loss = 0.04582364
Iteration 32, loss = 0.04346251
Iteration 33, loss = 0.04139072
Iteration 34, loss = 0.03944330
Iteration 35, loss = 0.03770364
Iteration 36, loss = 0.03610311
Iteration 37, loss = 0.03462216
Iteration 38, loss = 0.03323809
Iteration 39, loss = 0.03193850
Iteration 40, loss = 0.03092527
Iteration 41, loss = 0.02984905
Iteration 42, loss = 0.02892123
Iteration 43, loss = 0.02805823
Iteration 44, loss = 0.02713635
Iteration 45, loss = 0.02643388
Iteration 46, loss = 0.02562119
Iteration 47, loss = 0.02487003
Iteration 48, loss = 0.02437168
Iteration 49, loss = 0.02380145
Iteration 50, loss = 0.02311108
Iteration 51, loss = 0.02266536
Iteration 52, loss = 0.02214414
Iteration 53, loss = 0.02154430
Iteration 54, loss = 0.02117132
Iteration 55, loss = 0.02061278
Iteration 56, loss = 0.02004819
Iteration 57, loss = 0.01972141
Iteration 58, loss = 0.01932905
Iteration 59, loss = 0.01899605
Iteration 60, loss = 0.01863689
Iteration 61, loss = 0.01835126
Iteration 62, loss = 0.01803453
Iteration 63, loss = 0.01773618
Iteration 64, loss = 0.01746441
Iteration 65, loss = 0.01709282
Iteration 66, loss = 0.01701868
Iteration 67, loss = 0.01690991
Iteration 68, loss = 0.01663621
Iteration 69, loss = 0.01640958
Iteration 70, loss = 0.01612493
Iteration 71, loss = 0.01590240
Iteration 72, loss = 0.01563870
Iteration 73, loss = 0.01558671
Iteration 74, loss = 0.01534050
Iteration 75, loss = 0.01551623
Iteration 76, loss = 0.01497839
Iteration 77, loss = 0.01513437
Iteration 78, loss = 0.01486037
Iteration 79, loss = 0.01473150
Iteration 80, loss = 0.01452267
Iteration 81, loss = 0.01440720
Iteration 82, loss = 0.01430942
Iteration 83, loss = 0.01419568
Iteration 84, loss = 0.01399278
Iteration 85, loss = 0.01405131
Iteration 86, loss = 0.01391820
Iteration 87, loss = 0.01382532
Iteration 88, loss = 0.01366048
Iteration 89, loss = 0.01348858
Iteration 90, loss = 0.01339540
Iteration 91, loss = 0.01322642
Iteration 92, loss = 0.01316934
Iteration 93, loss = 0.01301488
Iteration 94, loss = 0.01290968
Iteration 95, loss = 0.01268268
Iteration 96, loss = 0.01280846
Iteration 97, loss = 0.01282526
Iteration 98, loss = 0.01275093
Iteration 99, loss = 0.01270184
Iteration 100, loss = 0.01272544
Iteration 101, loss = 0.01269753
Iteration 102, loss = 0.01251840
Iteration 103, loss = 0.01255153
Iteration 104, loss = 0.01240146
Iteration 105, loss = 0.01226809
Iteration 106, loss = 0.01224038
Iteration 107, loss = 0.01216857
Iteration 108, loss = 0.01207611
Iteration 109, loss = 0.01195451
Iteration 110, loss = 0.01186068
Iteration 111, loss = 0.01178353
Iteration 112, loss = 0.01171105
Iteration 113, loss = 0.01170495
Iteration 114, loss = 0.01170401
Iteration 115, loss = 0.01175790
Iteration 116, loss = 0.01168635
Iteration 117, loss = 0.01158778
Iteration 118, loss = 0.01151380
Iteration 119, loss = 0.01144398
Iteration 120, loss = 0.01148715
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69686706
Iteration 2, loss = 0.68300194
Iteration 3, loss = 0.67091890
Iteration 4, loss = 0.65894748
Iteration 5, loss = 0.64634701
Iteration 6, loss = 0.63246867
Iteration 7, loss = 0.61711145
Iteration 8, loss = 0.60006358
Iteration 9, loss = 0.58148087
Iteration 10, loss = 0.56134846
Iteration 11, loss = 0.53967329
Iteration 12, loss = 0.51710601
Iteration 13, loss = 0.49331075
Iteration 14, loss = 0.46929094
Iteration 15, loss = 0.44502118
Iteration 16, loss = 0.42085389
Iteration 17, loss = 0.39726003
Iteration 18, loss = 0.37417654
Iteration 19, loss = 0.35207931
Iteration 20, loss = 0.33078185
Iteration 21, loss = 0.31048574
Iteration 22, loss = 0.29124459
Iteration 23, loss = 0.27300267
Iteration 24, loss = 0.25579774
Iteration 25, loss = 0.23962444
Iteration 26, loss = 0.22449525
Iteration 27, loss = 0.21033105
Iteration 28, loss = 0.19712526
Iteration 29, loss = 0.18488220
Iteration 30, loss = 0.17349664
Iteration 31, loss = 0.16287264
Iteration 32, loss = 0.15304603
Iteration 33, loss = 0.14399505
Iteration 34, loss = 0.13555957
Iteration 35, loss = 0.12779955
Iteration 36, loss = 0.12062698
Iteration 37, loss = 0.11397211
Iteration 38, loss = 0.10781114
Iteration 39, loss = 0.10209122
Iteration 40, loss = 0.09681197
Iteration 41, loss = 0.09185621
Iteration 42, loss = 0.08727766
Iteration 43, loss = 0.08303723
Iteration 44, loss = 0.07904595
Iteration 45, loss = 0.07535621
Iteration 46, loss = 0.07193990
Iteration 47, loss = 0.06872280
Iteration 48, loss = 0.06572877
Iteration 49, loss = 0.06292705
Iteration 50, loss = 0.06028973
Iteration 51, loss = 0.05779151
Iteration 52, loss = 0.05548816
Iteration 53, loss = 0.05328868
Iteration 54, loss = 0.05124702
Iteration 55, loss = 0.04931449
Iteration 56, loss = 0.04749859
Iteration 57, loss = 0.04578479
Iteration 58, loss = 0.04417542
Iteration 59, loss = 0.04263093
Iteration 60, loss = 0.04119490
Iteration 61, loss = 0.03980527
Iteration 62, loss = 0.03850978
Iteration 63, loss = 0.03727407
Iteration 64, loss = 0.03611076
Iteration 65, loss = 0.03499000
Iteration 66, loss = 0.03393939
Iteration 67, loss = 0.03293802
Iteration 68, loss = 0.03196680
Iteration 69, loss = 0.03107725
Iteration 70, loss = 0.03018863
Iteration 71, loss = 0.02936891
Iteration 72, loss = 0.02857265
Iteration 73, loss = 0.02780564
Iteration 74, loss = 0.02707959
Iteration 75, loss = 0.02638406
Iteration 76, loss = 0.02570919
Iteration 77, loss = 0.02506294
Iteration 78, loss = 0.02445670
Iteration 79, loss = 0.02385683
Iteration 80, loss = 0.02328966
Iteration 81, loss = 0.02274234
Iteration 82, loss = 0.02222626
Iteration 83, loss = 0.02172076
Iteration 84, loss = 0.02123913
Iteration 85, loss = 0.02077207
Iteration 86, loss = 0.02031962
Iteration 87, loss = 0.01987537
Iteration 88, loss = 0.01945509
Iteration 89, loss = 0.01905499
Iteration 90, loss = 0.01865582
Iteration 91, loss = 0.01826965
Iteration 92, loss = 0.01790638
Iteration 93, loss = 0.01754859
Iteration 94, loss = 0.01720464
Iteration 95, loss = 0.01687746
Iteration 96, loss = 0.01655411
Iteration 97, loss = 0.01623447
Iteration 98, loss = 0.01593789
Iteration 99, loss = 0.01564598
Iteration 100, loss = 0.01535768
Iteration 101, loss = 0.01508012
Iteration 102, loss = 0.01481394
Iteration 103, loss = 0.01455517
Iteration 104, loss = 0.01430424
Iteration 105, loss = 0.01406699
Iteration 106, loss = 0.01383242
Iteration 107, loss = 0.01359839
Iteration 108, loss = 0.01337425
Iteration 109, loss = 0.01315662
Iteration 110, loss = 0.01294896
Iteration 111, loss = 0.01274342
Iteration 112, loss = 0.01254727
Iteration 113, loss = 0.01235435
Iteration 114, loss = 0.01216773
Iteration 115, loss = 0.01198753
Iteration 116, loss = 0.01180904
Iteration 117, loss = 0.01164118
Iteration 118, loss = 0.01147606
Iteration 119, loss = 0.01131228
Iteration 120, loss = 0.01115600
Iteration 121, loss = 0.01099988
Iteration 122, loss = 0.01085103
Iteration 123, loss = 0.01070776
Iteration 124, loss = 0.01056012
Iteration 125, loss = 0.01042203
Iteration 126, loss = 0.01028517
Iteration 127, loss = 0.01015265
Iteration 128, loss = 0.01001943
Iteration 129, loss = 0.00989242
Iteration 130, loss = 0.00976959
Iteration 131, loss = 0.00964753
Iteration 132, loss = 0.00953265
Iteration 133, loss = 0.00942139
Iteration 134, loss = 0.00930711
Iteration 135, loss = 0.00919349
Iteration 136, loss = 0.00907969
Iteration 137, loss = 0.00897691
Iteration 138, loss = 0.00887269
Iteration 139, loss = 0.00877200
Iteration 140, loss = 0.00867294
Iteration 141, loss = 0.00857678
Iteration 142, loss = 0.00848273
Iteration 143, loss = 0.00838747
Iteration 144, loss = 0.00829658
Iteration 145, loss = 0.00821257
Iteration 146, loss = 0.00812192
Iteration 147, loss = 0.00803472
Iteration 148, loss = 0.00794808
Iteration 149, loss = 0.00786418
Iteration 150, loss = 0.00778243
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68033024
Iteration 2, loss = 0.65786224
Iteration 3, loss = 0.63265967
Iteration 4, loss = 0.59942867
Iteration 5, loss = 0.55760768
Iteration 6, loss = 0.50970570
Iteration 7, loss = 0.45948818
Iteration 8, loss = 0.41059554
Iteration 9, loss = 0.36364217
Iteration 10, loss = 0.32032606
Iteration 11, loss = 0.28108938
Iteration 12, loss = 0.24668539
Iteration 13, loss = 0.21646752
Iteration 14, loss = 0.19070333
Iteration 15, loss = 0.16858214
Iteration 16, loss = 0.14972494
Iteration 17, loss = 0.13370469
Iteration 18, loss = 0.12004333
Iteration 19, loss = 0.10846849
Iteration 20, loss = 0.09848358
Iteration 21, loss = 0.08992624
Iteration 22, loss = 0.08254782
Iteration 23, loss = 0.07609168
Iteration 24, loss = 0.07050437
Iteration 25, loss = 0.06554999
Iteration 26, loss = 0.06113693
Iteration 27, loss = 0.05730110
Iteration 28, loss = 0.05387560
Iteration 29, loss = 0.05081123
Iteration 30, loss = 0.04808314
Iteration 31, loss = 0.04565261
Iteration 32, loss = 0.04349246
Iteration 33, loss = 0.04140500
Iteration 34, loss = 0.03958300
Iteration 35, loss = 0.03782049
Iteration 36, loss = 0.03637787
Iteration 37, loss = 0.03492055
Iteration 38, loss = 0.03368447
Iteration 39, loss = 0.03243349
Iteration 40, loss = 0.03124266
Iteration 41, loss = 0.03036133
Iteration 42, loss = 0.02950219
Iteration 43, loss = 0.02858259
Iteration 44, loss = 0.02773543
Iteration 45, loss = 0.02691161
Iteration 46, loss = 0.02627845
Iteration 47, loss = 0.02542970
Iteration 48, loss = 0.02498786
Iteration 49, loss = 0.02445710
Iteration 50, loss = 0.02386411
Iteration 51, loss = 0.02334219
Iteration 52, loss = 0.02284807
Iteration 53, loss = 0.02233303
Iteration 54, loss = 0.02203288
Iteration 55, loss = 0.02164059
Iteration 56, loss = 0.02110880
Iteration 57, loss = 0.02106207
Iteration 58, loss = 0.02058971
Iteration 59, loss = 0.02022196
Iteration 60, loss = 0.01983802
Iteration 61, loss = 0.01949858
Iteration 62, loss = 0.01912274
Iteration 63, loss = 0.01888947
Iteration 64, loss = 0.01854752
Iteration 65, loss = 0.01837411
Iteration 66, loss = 0.01811235
Iteration 67, loss = 0.01798200
Iteration 68, loss = 0.01771004
Iteration 69, loss = 0.01752233
Iteration 70, loss = 0.01720340
Iteration 71, loss = 0.01709654
Iteration 72, loss = 0.01695705
Iteration 73, loss = 0.01687262
Iteration 74, loss = 0.01668081
Iteration 75, loss = 0.01647060
Iteration 76, loss = 0.01624288
Iteration 77, loss = 0.01608986
Iteration 78, loss = 0.01584620
Iteration 79, loss = 0.01579702
Iteration 80, loss = 0.01550449
Iteration 81, loss = 0.01545554
Iteration 82, loss = 0.01519029
Iteration 83, loss = 0.01535449
Iteration 84, loss = 0.01519036
Iteration 85, loss = 0.01505351
Iteration 86, loss = 0.01518786
Iteration 87, loss = 0.01499081
Iteration 88, loss = 0.01486278
Iteration 89, loss = 0.01468216
Iteration 90, loss = 0.01462012
Iteration 91, loss = 0.01449421
Iteration 92, loss = 0.01440267
Iteration 93, loss = 0.01435689
Iteration 94, loss = 0.01446472
Iteration 95, loss = 0.01446868
Iteration 96, loss = 0.01422909
Iteration 97, loss = 0.01418302
Iteration 98, loss = 0.01405138
Iteration 99, loss = 0.01390273
Iteration 100, loss = 0.01359963
Iteration 101, loss = 0.01353930
Iteration 102, loss = 0.01347037
Iteration 103, loss = 0.01374252
Iteration 104, loss = 0.01367488
Iteration 105, loss = 0.01348880
Iteration 106, loss = 0.01339390
Iteration 107, loss = 0.01329938
Iteration 108, loss = 0.01320474
Iteration 109, loss = 0.01308967
Iteration 110, loss = 0.01304642
Iteration 111, loss = 0.01299765
Iteration 112, loss = 0.01293016
Iteration 113, loss = 0.01292794
Iteration 114, loss = 0.01294927
Iteration 115, loss = 0.01287930
Iteration 116, loss = 0.01290239
Iteration 117, loss = 0.01276764
Iteration 118, loss = 0.01287449
Iteration 119, loss = 0.01279030
Iteration 120, loss = 0.01269369
Iteration 121, loss = 0.01270954
Iteration 122, loss = 0.01266213
Iteration 123, loss = 0.01275364
Iteration 124, loss = 0.01259484
Iteration 125, loss = 0.01251643
Iteration 126, loss = 0.01243573
Iteration 127, loss = 0.01234231
Iteration 128, loss = 0.01230906
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67376259
Iteration 2, loss = 0.59837926
Iteration 3, loss = 0.48549504
Iteration 4, loss = 0.37330408
Iteration 5, loss = 0.28040166
Iteration 6, loss = 0.21076654
Iteration 7, loss = 0.16212732
Iteration 8, loss = 0.12866919
Iteration 9, loss = 0.10536362
Iteration 10, loss = 0.08897251
Iteration 11, loss = 0.07690335
Iteration 12, loss = 0.06776915
Iteration 13, loss = 0.06081663
Iteration 14, loss = 0.05524987
Iteration 15, loss = 0.05113311
Iteration 16, loss = 0.04759730
Iteration 17, loss = 0.04461864
Iteration 18, loss = 0.04238267
Iteration 19, loss = 0.04056491
Iteration 20, loss = 0.03867149
Iteration 21, loss = 0.03712758
Iteration 22, loss = 0.03600687
Iteration 23, loss = 0.03503010
Iteration 24, loss = 0.03388824
Iteration 25, loss = 0.03301453
Iteration 26, loss = 0.03220490
Iteration 27, loss = 0.03174760
Iteration 28, loss = 0.03104372
Iteration 29, loss = 0.03086638
Iteration 30, loss = 0.02997245
Iteration 31, loss = 0.02964497
Iteration 32, loss = 0.02965267
Iteration 33, loss = 0.02910387
Iteration 34, loss = 0.02884529
Iteration 35, loss = 0.02859725
Iteration 36, loss = 0.02844683
Iteration 37, loss = 0.02802040
Iteration 38, loss = 0.02768227
Iteration 39, loss = 0.02756833
Iteration 40, loss = 0.02730805
Iteration 41, loss = 0.02722201
Iteration 42, loss = 0.02720290
Iteration 43, loss = 0.02705721
Iteration 44, loss = 0.02689241
Iteration 45, loss = 0.02648719
Iteration 46, loss = 0.02652853
Iteration 47, loss = 0.02670575
Iteration 48, loss = 0.02614621
Iteration 49, loss = 0.02617205
Iteration 50, loss = 0.02613959
Iteration 51, loss = 0.02619463
Iteration 52, loss = 0.02598796
Iteration 53, loss = 0.02612679
Iteration 54, loss = 0.02581134
Iteration 55, loss = 0.02566247
Iteration 56, loss = 0.02566330
Iteration 57, loss = 0.02565728
Iteration 58, loss = 0.02554258
Iteration 59, loss = 0.02544262
Iteration 60, loss = 0.02552501
Iteration 61, loss = 0.02535084
Iteration 62, loss = 0.02526497
Iteration 63, loss = 0.02550845
Iteration 64, loss = 0.02542017
Iteration 65, loss = 0.02524018
Iteration 66, loss = 0.02547255
Iteration 67, loss = 0.02496812
Iteration 68, loss = 0.02511128
Iteration 69, loss = 0.02533515
Iteration 70, loss = 0.02495016
Iteration 71, loss = 0.02485531
Iteration 72, loss = 0.02511771
Iteration 73, loss = 0.02488105
Iteration 74, loss = 0.02476542
Iteration 75, loss = 0.02481097
Iteration 76, loss = 0.02498944
Iteration 77, loss = 0.02488817
Iteration 78, loss = 0.02498311
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	86m30.949s
user	381m36.967s
sys	38m23.162s
Elapsed time: 5191 seconds
