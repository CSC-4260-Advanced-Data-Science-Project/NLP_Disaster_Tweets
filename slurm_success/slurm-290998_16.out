Processing dataset: kept_v8_keep_hashtags

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: kept_v8_keep_hashtags (7443 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on kept_v8_keep_hashtags...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on kept_v8_keep_hashtags...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on kept_v8_keep_hashtags...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on kept_v8_keep_hashtags...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on kept_v8_keep_hashtags...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.70461987
Iteration 2, loss = 0.62397434
Iteration 3, loss = 0.51682810
Iteration 4, loss = 0.39803372
Iteration 5, loss = 0.29771185
Iteration 6, loss = 0.22309736
Iteration 7, loss = 0.17110628
Iteration 8, loss = 0.13549273
Iteration 9, loss = 0.11086799
Iteration 10, loss = 0.09338260
Iteration 11, loss = 0.08086058
Iteration 12, loss = 0.07132206
Iteration 13, loss = 0.06388446
Iteration 14, loss = 0.05816694
Iteration 15, loss = 0.05388609
Iteration 16, loss = 0.04993822
Iteration 17, loss = 0.04718649
Iteration 18, loss = 0.04455180
Iteration 19, loss = 0.04277993
Iteration 20, loss = 0.04065229
Iteration 21, loss = 0.03920072
Iteration 22, loss = 0.03805455
Iteration 23, loss = 0.03699912
Iteration 24, loss = 0.03606060
Iteration 25, loss = 0.03489016
Iteration 26, loss = 0.03460921
Iteration 27, loss = 0.03364889
Iteration 28, loss = 0.03283695
Iteration 29, loss = 0.03269541
Iteration 30, loss = 0.03213014
Iteration 31, loss = 0.03178574
Iteration 32, loss = 0.03118113
Iteration 33, loss = 0.03091026
Iteration 34, loss = 0.03074409
Iteration 35, loss = 0.03019153
Iteration 36, loss = 0.03001810
Iteration 37, loss = 0.02958295
Iteration 38, loss = 0.02967476
Iteration 39, loss = 0.02926682
Iteration 40, loss = 0.02918109
Iteration 41, loss = 0.02878196
Iteration 42, loss = 0.02876533
Iteration 43, loss = 0.02844033
Iteration 44, loss = 0.02805595
Iteration 45, loss = 0.02855890
Iteration 46, loss = 0.02821410
Iteration 47, loss = 0.02816119
Iteration 48, loss = 0.02812133
Iteration 49, loss = 0.02815213
Iteration 50, loss = 0.02770774
Iteration 51, loss = 0.02811392
Iteration 52, loss = 0.02747642
Iteration 53, loss = 0.02773040
Iteration 54, loss = 0.02764406
Iteration 55, loss = 0.02743088
Iteration 56, loss = 0.02750707
Iteration 57, loss = 0.02745552
Iteration 58, loss = 0.02744289
Iteration 59, loss = 0.02713930
Iteration 60, loss = 0.02712895
Iteration 61, loss = 0.02706230
Iteration 62, loss = 0.02719302
Iteration 63, loss = 0.02714276
Iteration 64, loss = 0.02671589
Iteration 65, loss = 0.02700343
Iteration 66, loss = 0.02667739
Iteration 67, loss = 0.02695506
Iteration 68, loss = 0.02683747
Iteration 69, loss = 0.02692313
Iteration 70, loss = 0.02706211
Iteration 71, loss = 0.02659106
Iteration 72, loss = 0.02676417
Iteration 73, loss = 0.02642717
Iteration 74, loss = 0.02681540
Iteration 75, loss = 0.02691715
Iteration 76, loss = 0.02657779
Iteration 77, loss = 0.02654643
Iteration 78, loss = 0.02649137
Iteration 79, loss = 0.02651843
Iteration 80, loss = 0.02631473
Iteration 81, loss = 0.02638475
Iteration 82, loss = 0.02623952
Iteration 83, loss = 0.02652680
Iteration 84, loss = 0.02644352
Iteration 85, loss = 0.02634635
Iteration 86, loss = 0.02663465
Iteration 87, loss = 0.02648001
Iteration 88, loss = 0.02622102
Iteration 89, loss = 0.02616158
Iteration 90, loss = 0.02621268
Iteration 91, loss = 0.02668852
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71194494
Iteration 2, loss = 0.63902591
Iteration 3, loss = 0.53483972
Iteration 4, loss = 0.41434458
Iteration 5, loss = 0.31027718
Iteration 6, loss = 0.23124662
Iteration 7, loss = 0.17599634
Iteration 8, loss = 0.13824366
Iteration 9, loss = 0.11229118
Iteration 10, loss = 0.09393097
Iteration 11, loss = 0.08070765
Iteration 12, loss = 0.07049534
Iteration 13, loss = 0.06324414
Iteration 14, loss = 0.05743047
Iteration 15, loss = 0.05241036
Iteration 16, loss = 0.04858767
Iteration 17, loss = 0.04547988
Iteration 18, loss = 0.04291796
Iteration 19, loss = 0.04060969
Iteration 20, loss = 0.03869806
Iteration 21, loss = 0.03724984
Iteration 22, loss = 0.03560813
Iteration 23, loss = 0.03445235
Iteration 24, loss = 0.03338961
Iteration 25, loss = 0.03265393
Iteration 26, loss = 0.03156471
Iteration 27, loss = 0.03111817
Iteration 28, loss = 0.03053871
Iteration 29, loss = 0.02983588
Iteration 30, loss = 0.02954530
Iteration 31, loss = 0.02898053
Iteration 32, loss = 0.02859418
Iteration 33, loss = 0.02815124
Iteration 34, loss = 0.02781557
Iteration 35, loss = 0.02755912
Iteration 36, loss = 0.02714707
Iteration 37, loss = 0.02696033
Iteration 38, loss = 0.02663330
Iteration 39, loss = 0.02623897
Iteration 40, loss = 0.02614035
Iteration 41, loss = 0.02608353
Iteration 42, loss = 0.02574195
Iteration 43, loss = 0.02603555
Iteration 44, loss = 0.02538181
Iteration 45, loss = 0.02550012
Iteration 46, loss = 0.02522750
Iteration 47, loss = 0.02509911
Iteration 48, loss = 0.02531983
Iteration 49, loss = 0.02507082
Iteration 50, loss = 0.02490148
Iteration 51, loss = 0.02480220
Iteration 52, loss = 0.02481086
Iteration 53, loss = 0.02467328
Iteration 54, loss = 0.02435619
Iteration 55, loss = 0.02475128
Iteration 56, loss = 0.02440454
Iteration 57, loss = 0.02433656
Iteration 58, loss = 0.02453995
Iteration 59, loss = 0.02433533
Iteration 60, loss = 0.02421523
Iteration 61, loss = 0.02430745
Iteration 62, loss = 0.02413646
Iteration 63, loss = 0.02416284
Iteration 64, loss = 0.02394277
Iteration 65, loss = 0.02385362
Iteration 66, loss = 0.02395791
Iteration 67, loss = 0.02383168
Iteration 68, loss = 0.02379078
Iteration 69, loss = 0.02352122
Iteration 70, loss = 0.02378056
Iteration 71, loss = 0.02369058
Iteration 72, loss = 0.02377006
Iteration 73, loss = 0.02388359
Iteration 74, loss = 0.02374188
Iteration 75, loss = 0.02363055
Iteration 76, loss = 0.02397735
Iteration 77, loss = 0.02364423
Iteration 78, loss = 0.02371522
Iteration 79, loss = 0.02345714
Iteration 80, loss = 0.02322666
Iteration 81, loss = 0.02335757
Iteration 82, loss = 0.02327068
Iteration 83, loss = 0.02350278
Iteration 84, loss = 0.02359802
Iteration 85, loss = 0.02373784
Iteration 86, loss = 0.02318464
Iteration 87, loss = 0.02329183
Iteration 88, loss = 0.02322280
Iteration 89, loss = 0.02329027
Iteration 90, loss = 0.02309162
Iteration 91, loss = 0.02308025
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67560542
Iteration 2, loss = 0.59963297
Iteration 3, loss = 0.49133376
Iteration 4, loss = 0.38172890
Iteration 5, loss = 0.28883938
Iteration 6, loss = 0.21845970
Iteration 7, loss = 0.16863484
Iteration 8, loss = 0.13358776
Iteration 9, loss = 0.10930261
Iteration 10, loss = 0.09141338
Iteration 11, loss = 0.07872045
Iteration 12, loss = 0.06896408
Iteration 13, loss = 0.06135229
Iteration 14, loss = 0.05545140
Iteration 15, loss = 0.05059749
Iteration 16, loss = 0.04678274
Iteration 17, loss = 0.04378426
Iteration 18, loss = 0.04091965
Iteration 19, loss = 0.03874866
Iteration 20, loss = 0.03669838
Iteration 21, loss = 0.03532501
Iteration 22, loss = 0.03392534
Iteration 23, loss = 0.03258017
Iteration 24, loss = 0.03163262
Iteration 25, loss = 0.03072537
Iteration 26, loss = 0.02962987
Iteration 27, loss = 0.02931158
Iteration 28, loss = 0.02850082
Iteration 29, loss = 0.02782726
Iteration 30, loss = 0.02704560
Iteration 31, loss = 0.02685406
Iteration 32, loss = 0.02624791
Iteration 33, loss = 0.02592622
Iteration 34, loss = 0.02566484
Iteration 35, loss = 0.02533577
Iteration 36, loss = 0.02493784
Iteration 37, loss = 0.02476048
Iteration 38, loss = 0.02437730
Iteration 39, loss = 0.02419073
Iteration 40, loss = 0.02403220
Iteration 41, loss = 0.02385184
Iteration 42, loss = 0.02369577
Iteration 43, loss = 0.02344662
Iteration 44, loss = 0.02353898
Iteration 45, loss = 0.02321125
Iteration 46, loss = 0.02336335
Iteration 47, loss = 0.02312324
Iteration 48, loss = 0.02289857
Iteration 49, loss = 0.02295430
Iteration 50, loss = 0.02263113
Iteration 51, loss = 0.02266373
Iteration 52, loss = 0.02281731
Iteration 53, loss = 0.02222577
Iteration 54, loss = 0.02206043
Iteration 55, loss = 0.02236778
Iteration 56, loss = 0.02231889
Iteration 57, loss = 0.02216312
Iteration 58, loss = 0.02219011
Iteration 59, loss = 0.02190854
Iteration 60, loss = 0.02193792
Iteration 61, loss = 0.02181301
Iteration 62, loss = 0.02176089
Iteration 63, loss = 0.02187281
Iteration 64, loss = 0.02160783
Iteration 65, loss = 0.02170528
Iteration 66, loss = 0.02162503
Iteration 67, loss = 0.02156600
Iteration 68, loss = 0.02163836
Iteration 69, loss = 0.02139421
Iteration 70, loss = 0.02152631
Iteration 71, loss = 0.02157862
Iteration 72, loss = 0.02138508
Iteration 73, loss = 0.02151320
Iteration 74, loss = 0.02140605
Iteration 75, loss = 0.02123877
Iteration 76, loss = 0.02143444
Iteration 77, loss = 0.02128154
Iteration 78, loss = 0.02162984
Iteration 79, loss = 0.02141464
Iteration 80, loss = 0.02136642
Iteration 81, loss = 0.02155690
Iteration 82, loss = 0.02133487
Iteration 83, loss = 0.02105499
Iteration 84, loss = 0.02145933
Iteration 85, loss = 0.02128175
Iteration 86, loss = 0.02100696
Iteration 87, loss = 0.02130399
Iteration 88, loss = 0.02134249
Iteration 89, loss = 0.02073742
Iteration 90, loss = 0.02105019
Iteration 91, loss = 0.02111258
Iteration 92, loss = 0.02120651
Iteration 93, loss = 0.02113475
Iteration 94, loss = 0.02100947
Iteration 95, loss = 0.02127790
Iteration 96, loss = 0.02117292
Iteration 97, loss = 0.02127383
Iteration 98, loss = 0.02105813
Iteration 99, loss = 0.02113216
Iteration 100, loss = 0.02118045
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71344479
Iteration 2, loss = 0.62648296
Iteration 3, loss = 0.51290552
Iteration 4, loss = 0.39493412
Iteration 5, loss = 0.29728626
Iteration 6, loss = 0.22402509
Iteration 7, loss = 0.17270177
Iteration 8, loss = 0.13755096
Iteration 9, loss = 0.11259784
Iteration 10, loss = 0.09543671
Iteration 11, loss = 0.08263671
Iteration 12, loss = 0.07299232
Iteration 13, loss = 0.06563726
Iteration 14, loss = 0.05983752
Iteration 15, loss = 0.05533936
Iteration 16, loss = 0.05139907
Iteration 17, loss = 0.04821121
Iteration 18, loss = 0.04583730
Iteration 19, loss = 0.04354330
Iteration 20, loss = 0.04163684
Iteration 21, loss = 0.04003100
Iteration 22, loss = 0.03904911
Iteration 23, loss = 0.03775501
Iteration 24, loss = 0.03696980
Iteration 25, loss = 0.03586805
Iteration 26, loss = 0.03491596
Iteration 27, loss = 0.03407363
Iteration 28, loss = 0.03361383
Iteration 29, loss = 0.03310955
Iteration 30, loss = 0.03251767
Iteration 31, loss = 0.03219447
Iteration 32, loss = 0.03180208
Iteration 33, loss = 0.03145719
Iteration 34, loss = 0.03101235
Iteration 35, loss = 0.03060613
Iteration 36, loss = 0.03028541
Iteration 37, loss = 0.03008028
Iteration 38, loss = 0.02994084
Iteration 39, loss = 0.02961472
Iteration 40, loss = 0.02934174
Iteration 41, loss = 0.02920989
Iteration 42, loss = 0.02916601
Iteration 43, loss = 0.02883346
Iteration 44, loss = 0.02885238
Iteration 45, loss = 0.02893382
Iteration 46, loss = 0.02863434
Iteration 47, loss = 0.02853885
Iteration 48, loss = 0.02830956
Iteration 49, loss = 0.02822888
Iteration 50, loss = 0.02796273
Iteration 51, loss = 0.02803896
Iteration 52, loss = 0.02800117
Iteration 53, loss = 0.02768750
Iteration 54, loss = 0.02780547
Iteration 55, loss = 0.02740464
Iteration 56, loss = 0.02750472
Iteration 57, loss = 0.02768158
Iteration 58, loss = 0.02743406
Iteration 59, loss = 0.02741749
Iteration 60, loss = 0.02743552
Iteration 61, loss = 0.02717237
Iteration 62, loss = 0.02714974
Iteration 63, loss = 0.02701191
Iteration 64, loss = 0.02703559
Iteration 65, loss = 0.02689852
Iteration 66, loss = 0.02708949
Iteration 67, loss = 0.02718772
Iteration 68, loss = 0.02722811
Iteration 69, loss = 0.02676906
Iteration 70, loss = 0.02675273
Iteration 71, loss = 0.02675816
Iteration 72, loss = 0.02689609
Iteration 73, loss = 0.02663109
Iteration 74, loss = 0.02660471
Iteration 75, loss = 0.02678444
Iteration 76, loss = 0.02669482
Iteration 77, loss = 0.02689941
Iteration 78, loss = 0.02665303
Iteration 79, loss = 0.02673315
Iteration 80, loss = 0.02670330
Iteration 81, loss = 0.02671420
Iteration 82, loss = 0.02666277
Iteration 83, loss = 0.02659300
Iteration 84, loss = 0.02649669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69693282
Iteration 2, loss = 0.60886980
Iteration 3, loss = 0.48897016
Iteration 4, loss = 0.36932731
Iteration 5, loss = 0.27381766
Iteration 6, loss = 0.20502104
Iteration 7, loss = 0.15782826
Iteration 8, loss = 0.12587906
Iteration 9, loss = 0.10299411
Iteration 10, loss = 0.08726542
Iteration 11, loss = 0.07571965
Iteration 12, loss = 0.06699644
Iteration 13, loss = 0.05991255
Iteration 14, loss = 0.05447793
Iteration 15, loss = 0.05039104
Iteration 16, loss = 0.04718012
Iteration 17, loss = 0.04429864
Iteration 18, loss = 0.04209039
Iteration 19, loss = 0.03976505
Iteration 20, loss = 0.03820236
Iteration 21, loss = 0.03681341
Iteration 22, loss = 0.03555932
Iteration 23, loss = 0.03444120
Iteration 24, loss = 0.03352924
Iteration 25, loss = 0.03281723
Iteration 26, loss = 0.03199228
Iteration 27, loss = 0.03164711
Iteration 28, loss = 0.03091931
Iteration 29, loss = 0.03018900
Iteration 30, loss = 0.02991166
Iteration 31, loss = 0.02948416
Iteration 32, loss = 0.02909192
Iteration 33, loss = 0.02861979
Iteration 34, loss = 0.02852512
Iteration 35, loss = 0.02830011
Iteration 36, loss = 0.02805735
Iteration 37, loss = 0.02767600
Iteration 38, loss = 0.02726080
Iteration 39, loss = 0.02733185
Iteration 40, loss = 0.02693883
Iteration 41, loss = 0.02691448
Iteration 42, loss = 0.02689899
Iteration 43, loss = 0.02664622
Iteration 44, loss = 0.02664576
Iteration 45, loss = 0.02640883
Iteration 46, loss = 0.02650654
Iteration 47, loss = 0.02630917
Iteration 48, loss = 0.02594803
Iteration 49, loss = 0.02599657
Iteration 50, loss = 0.02573883
Iteration 51, loss = 0.02583623
Iteration 52, loss = 0.02569206
Iteration 53, loss = 0.02548101
Iteration 54, loss = 0.02560036
Iteration 55, loss = 0.02551505
Iteration 56, loss = 0.02561594
Iteration 57, loss = 0.02545463
Iteration 58, loss = 0.02526823
Iteration 59, loss = 0.02560965
Iteration 60, loss = 0.02548682
Iteration 61, loss = 0.02509950
Iteration 62, loss = 0.02548854
Iteration 63, loss = 0.02507274
Iteration 64, loss = 0.02520526
Iteration 65, loss = 0.02501156
Iteration 66, loss = 0.02488615
Iteration 67, loss = 0.02485140
Iteration 68, loss = 0.02507758
Iteration 69, loss = 0.02509241
Iteration 70, loss = 0.02472586
Iteration 71, loss = 0.02496794
Iteration 72, loss = 0.02475523
Iteration 73, loss = 0.02483295
Iteration 74, loss = 0.02457760
Iteration 75, loss = 0.02479437
Iteration 76, loss = 0.02462773
Iteration 77, loss = 0.02468366
Iteration 78, loss = 0.02497714
Iteration 79, loss = 0.02458356
Iteration 80, loss = 0.02474140
Iteration 81, loss = 0.02473185
Iteration 82, loss = 0.02433627
Iteration 83, loss = 0.02433555
Iteration 84, loss = 0.02455158
Iteration 85, loss = 0.02436937
Iteration 86, loss = 0.02430246
Iteration 87, loss = 0.02444184
Iteration 88, loss = 0.02484791
Iteration 89, loss = 0.02438129
Iteration 90, loss = 0.02434981
Iteration 91, loss = 0.02422726
Iteration 92, loss = 0.02426158
Iteration 93, loss = 0.02408948
Iteration 94, loss = 0.02432645
Iteration 95, loss = 0.02421509
Iteration 96, loss = 0.02426889
Iteration 97, loss = 0.02424314
Iteration 98, loss = 0.02426807
Iteration 99, loss = 0.02415452
Iteration 100, loss = 0.02408629
Iteration 101, loss = 0.02450365
Iteration 102, loss = 0.02433805
Iteration 103, loss = 0.02430728
Iteration 104, loss = 0.02441342
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70461987
Iteration 2, loss = 0.62397434
Iteration 3, loss = 0.51682810
Iteration 4, loss = 0.39803372
Iteration 5, loss = 0.29771185
Iteration 6, loss = 0.22309736
Iteration 7, loss = 0.17110628
Iteration 8, loss = 0.13549273
Iteration 9, loss = 0.11086799
Iteration 10, loss = 0.09338260
Iteration 11, loss = 0.08086058
Iteration 12, loss = 0.07132206
Iteration 13, loss = 0.06388446
Iteration 14, loss = 0.05816694
Iteration 15, loss = 0.05388609
Iteration 16, loss = 0.04993822
Iteration 17, loss = 0.04718649
Iteration 18, loss = 0.04455180
Iteration 19, loss = 0.04277993
Iteration 20, loss = 0.04065229
Iteration 21, loss = 0.03920072
Iteration 22, loss = 0.03805455
Iteration 23, loss = 0.03699912
Iteration 24, loss = 0.03606060
Iteration 25, loss = 0.03489016
Iteration 26, loss = 0.03460921
Iteration 27, loss = 0.03364889
Iteration 28, loss = 0.03283695
Iteration 29, loss = 0.03269541
Iteration 30, loss = 0.03213014
Iteration 31, loss = 0.03178574
Iteration 32, loss = 0.03118113
Iteration 33, loss = 0.03091026
Iteration 34, loss = 0.03074409
Iteration 35, loss = 0.03019153
Iteration 36, loss = 0.03001810
Iteration 37, loss = 0.02958295
Iteration 38, loss = 0.02967476
Iteration 39, loss = 0.02926682
Iteration 40, loss = 0.02918109
Iteration 41, loss = 0.02878196
Iteration 42, loss = 0.02876533
Iteration 43, loss = 0.02844033
Iteration 44, loss = 0.02805595
Iteration 45, loss = 0.02855890
Iteration 46, loss = 0.02821410
Iteration 47, loss = 0.02816119
Iteration 48, loss = 0.02812133
Iteration 49, loss = 0.02815213
Iteration 50, loss = 0.02770774
Iteration 51, loss = 0.02811392
Iteration 52, loss = 0.02747642
Iteration 53, loss = 0.02773040
Iteration 54, loss = 0.02764406
Iteration 55, loss = 0.02743088
Iteration 56, loss = 0.02750707
Iteration 57, loss = 0.02745552
Iteration 58, loss = 0.02744289
Iteration 59, loss = 0.02713930
Iteration 60, loss = 0.02712895
Iteration 61, loss = 0.02706230
Iteration 62, loss = 0.02719302
Iteration 63, loss = 0.02714276
Iteration 64, loss = 0.02671589
Iteration 65, loss = 0.02700343
Iteration 66, loss = 0.02667739
Iteration 67, loss = 0.02695506
Iteration 68, loss = 0.02683747
Iteration 69, loss = 0.02692313
Iteration 70, loss = 0.02706211
Iteration 71, loss = 0.02659106
Iteration 72, loss = 0.02676417
Iteration 73, loss = 0.02642717
Iteration 74, loss = 0.02681540
Iteration 75, loss = 0.02691715
Iteration 76, loss = 0.02657779
Iteration 77, loss = 0.02654643
Iteration 78, loss = 0.02649137
Iteration 79, loss = 0.02651843
Iteration 80, loss = 0.02631473
Iteration 81, loss = 0.02638475
Iteration 82, loss = 0.02623952
Iteration 83, loss = 0.02652680
Iteration 84, loss = 0.02644352
Iteration 85, loss = 0.02634635
Iteration 86, loss = 0.02663465
Iteration 87, loss = 0.02648001
Iteration 88, loss = 0.02622102
Iteration 89, loss = 0.02616158
Iteration 90, loss = 0.02621268
Iteration 91, loss = 0.02668852
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71194494
Iteration 2, loss = 0.63902591
Iteration 3, loss = 0.53483972
Iteration 4, loss = 0.41434458
Iteration 5, loss = 0.31027718
Iteration 6, loss = 0.23124662
Iteration 7, loss = 0.17599634
Iteration 8, loss = 0.13824366
Iteration 9, loss = 0.11229118
Iteration 10, loss = 0.09393097
Iteration 11, loss = 0.08070765
Iteration 12, loss = 0.07049534
Iteration 13, loss = 0.06324414
Iteration 14, loss = 0.05743047
Iteration 15, loss = 0.05241036
Iteration 16, loss = 0.04858767
Iteration 17, loss = 0.04547988
Iteration 18, loss = 0.04291796
Iteration 19, loss = 0.04060969
Iteration 20, loss = 0.03869806
Iteration 21, loss = 0.03724984
Iteration 22, loss = 0.03560813
Iteration 23, loss = 0.03445235
Iteration 24, loss = 0.03338961
Iteration 25, loss = 0.03265393
Iteration 26, loss = 0.03156471
Iteration 27, loss = 0.03111817
Iteration 28, loss = 0.03053871
Iteration 29, loss = 0.02983588
Iteration 30, loss = 0.02954530
Iteration 31, loss = 0.02898053
Iteration 32, loss = 0.02859418
Iteration 33, loss = 0.02815124
Iteration 34, loss = 0.02781557
Iteration 35, loss = 0.02755912
Iteration 36, loss = 0.02714707
Iteration 37, loss = 0.02696033
Iteration 38, loss = 0.02663330
Iteration 39, loss = 0.02623897
Iteration 40, loss = 0.02614035
Iteration 41, loss = 0.02608353
Iteration 42, loss = 0.02574195
Iteration 43, loss = 0.02603555
Iteration 44, loss = 0.02538181
Iteration 45, loss = 0.02550012
Iteration 46, loss = 0.02522750
Iteration 47, loss = 0.02509911
Iteration 48, loss = 0.02531983
Iteration 49, loss = 0.02507082
Iteration 50, loss = 0.02490148
Iteration 51, loss = 0.02480220
Iteration 52, loss = 0.02481086
Iteration 53, loss = 0.02467328
Iteration 54, loss = 0.02435619
Iteration 55, loss = 0.02475128
Iteration 56, loss = 0.02440454
Iteration 57, loss = 0.02433656
Iteration 58, loss = 0.02453995
Iteration 59, loss = 0.02433533
Iteration 60, loss = 0.02421523
Iteration 61, loss = 0.02430745
Iteration 62, loss = 0.02413646
Iteration 63, loss = 0.02416284
Iteration 64, loss = 0.02394277
Iteration 65, loss = 0.02385362
Iteration 66, loss = 0.02395791
Iteration 67, loss = 0.02383168
Iteration 68, loss = 0.02379078
Iteration 69, loss = 0.02352122
Iteration 70, loss = 0.02378056
Iteration 71, loss = 0.02369058
Iteration 72, loss = 0.02377006
Iteration 73, loss = 0.02388359
Iteration 74, loss = 0.02374188
Iteration 75, loss = 0.02363055
Iteration 76, loss = 0.02397735
Iteration 77, loss = 0.02364423
Iteration 78, loss = 0.02371522
Iteration 79, loss = 0.02345714
Iteration 80, loss = 0.02322666
Iteration 81, loss = 0.02335757
Iteration 82, loss = 0.02327068
Iteration 83, loss = 0.02350278
Iteration 84, loss = 0.02359802
Iteration 85, loss = 0.02373784
Iteration 86, loss = 0.02318464
Iteration 87, loss = 0.02329183
Iteration 88, loss = 0.02322280
Iteration 89, loss = 0.02329027
Iteration 90, loss = 0.02309162
Iteration 91, loss = 0.02308025
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67560542
Iteration 2, loss = 0.59963297
Iteration 3, loss = 0.49133376
Iteration 4, loss = 0.38172890
Iteration 5, loss = 0.28883938
Iteration 6, loss = 0.21845970
Iteration 7, loss = 0.16863484
Iteration 8, loss = 0.13358776
Iteration 9, loss = 0.10930261
Iteration 10, loss = 0.09141338
Iteration 11, loss = 0.07872045
Iteration 12, loss = 0.06896408
Iteration 13, loss = 0.06135229
Iteration 14, loss = 0.05545140
Iteration 15, loss = 0.05059749
Iteration 16, loss = 0.04678274
Iteration 17, loss = 0.04378426
Iteration 18, loss = 0.04091965
Iteration 19, loss = 0.03874866
Iteration 20, loss = 0.03669838
Iteration 21, loss = 0.03532501
Iteration 22, loss = 0.03392534
Iteration 23, loss = 0.03258017
Iteration 24, loss = 0.03163262
Iteration 25, loss = 0.03072537
Iteration 26, loss = 0.02962987
Iteration 27, loss = 0.02931158
Iteration 28, loss = 0.02850082
Iteration 29, loss = 0.02782726
Iteration 30, loss = 0.02704560
Iteration 31, loss = 0.02685406
Iteration 32, loss = 0.02624791
Iteration 33, loss = 0.02592622
Iteration 34, loss = 0.02566484
Iteration 35, loss = 0.02533577
Iteration 36, loss = 0.02493784
Iteration 37, loss = 0.02476048
Iteration 38, loss = 0.02437730
Iteration 39, loss = 0.02419073
Iteration 40, loss = 0.02403220
Iteration 41, loss = 0.02385184
Iteration 42, loss = 0.02369577
Iteration 43, loss = 0.02344662
Iteration 44, loss = 0.02353898
Iteration 45, loss = 0.02321125
Iteration 46, loss = 0.02336335
Iteration 47, loss = 0.02312324
Iteration 48, loss = 0.02289857
Iteration 49, loss = 0.02295430
Iteration 50, loss = 0.02263113
Iteration 51, loss = 0.02266373
Iteration 52, loss = 0.02281731
Iteration 53, loss = 0.02222577
Iteration 54, loss = 0.02206043
Iteration 55, loss = 0.02236778
Iteration 56, loss = 0.02231889
Iteration 57, loss = 0.02216312
Iteration 58, loss = 0.02219011
Iteration 59, loss = 0.02190854
Iteration 60, loss = 0.02193792
Iteration 61, loss = 0.02181301
Iteration 62, loss = 0.02176089
Iteration 63, loss = 0.02187281
Iteration 64, loss = 0.02160783
Iteration 65, loss = 0.02170528
Iteration 66, loss = 0.02162503
Iteration 67, loss = 0.02156600
Iteration 68, loss = 0.02163836
Iteration 69, loss = 0.02139421
Iteration 70, loss = 0.02152631
Iteration 71, loss = 0.02157862
Iteration 72, loss = 0.02138508
Iteration 73, loss = 0.02151320
Iteration 74, loss = 0.02140605
Iteration 75, loss = 0.02123877
Iteration 76, loss = 0.02143444
Iteration 77, loss = 0.02128154
Iteration 78, loss = 0.02162984
Iteration 79, loss = 0.02141464
Iteration 80, loss = 0.02136642
Iteration 81, loss = 0.02155690
Iteration 82, loss = 0.02133487
Iteration 83, loss = 0.02105499
Iteration 84, loss = 0.02145933
Iteration 85, loss = 0.02128175
Iteration 86, loss = 0.02100696
Iteration 87, loss = 0.02130399
Iteration 88, loss = 0.02134249
Iteration 89, loss = 0.02073742
Iteration 90, loss = 0.02105019
Iteration 91, loss = 0.02111258
Iteration 92, loss = 0.02120651
Iteration 93, loss = 0.02113475
Iteration 94, loss = 0.02100947
Iteration 95, loss = 0.02127790
Iteration 96, loss = 0.02117292
Iteration 97, loss = 0.02127383
Iteration 98, loss = 0.02105813
Iteration 99, loss = 0.02113216
Iteration 100, loss = 0.02118045
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71344479
Iteration 2, loss = 0.62648296
Iteration 3, loss = 0.51290552
Iteration 4, loss = 0.39493412
Iteration 5, loss = 0.29728626
Iteration 6, loss = 0.22402509
Iteration 7, loss = 0.17270177
Iteration 8, loss = 0.13755096
Iteration 9, loss = 0.11259784
Iteration 10, loss = 0.09543671
Iteration 11, loss = 0.08263671
Iteration 12, loss = 0.07299232
Iteration 13, loss = 0.06563726
Iteration 14, loss = 0.05983752
Iteration 15, loss = 0.05533936
Iteration 16, loss = 0.05139907
Iteration 17, loss = 0.04821121
Iteration 18, loss = 0.04583730
Iteration 19, loss = 0.04354330
Iteration 20, loss = 0.04163684
Iteration 21, loss = 0.04003100
Iteration 22, loss = 0.03904911
Iteration 23, loss = 0.03775501
Iteration 24, loss = 0.03696980
Iteration 25, loss = 0.03586805
Iteration 26, loss = 0.03491596
Iteration 27, loss = 0.03407363
Iteration 28, loss = 0.03361383
Iteration 29, loss = 0.03310955
Iteration 30, loss = 0.03251767
Iteration 31, loss = 0.03219447
Iteration 32, loss = 0.03180208
Iteration 33, loss = 0.03145719
Iteration 34, loss = 0.03101235
Iteration 35, loss = 0.03060613
Iteration 36, loss = 0.03028541
Iteration 37, loss = 0.03008028
Iteration 38, loss = 0.02994084
Iteration 39, loss = 0.02961472
Iteration 40, loss = 0.02934174
Iteration 41, loss = 0.02920989
Iteration 42, loss = 0.02916601
Iteration 43, loss = 0.02883346
Iteration 44, loss = 0.02885238
Iteration 45, loss = 0.02893382
Iteration 46, loss = 0.02863434
Iteration 47, loss = 0.02853885
Iteration 48, loss = 0.02830956
Iteration 49, loss = 0.02822888
Iteration 50, loss = 0.02796273
Iteration 51, loss = 0.02803896
Iteration 52, loss = 0.02800117
Iteration 53, loss = 0.02768750
Iteration 54, loss = 0.02780547
Iteration 55, loss = 0.02740464
Iteration 56, loss = 0.02750472
Iteration 57, loss = 0.02768158
Iteration 58, loss = 0.02743406
Iteration 59, loss = 0.02741749
Iteration 60, loss = 0.02743552
Iteration 61, loss = 0.02717237
Iteration 62, loss = 0.02714974
Iteration 63, loss = 0.02701191
Iteration 64, loss = 0.02703559
Iteration 65, loss = 0.02689852
Iteration 66, loss = 0.02708949
Iteration 67, loss = 0.02718772
Iteration 68, loss = 0.02722811
Iteration 69, loss = 0.02676906
Iteration 70, loss = 0.02675273
Iteration 71, loss = 0.02675816
Iteration 72, loss = 0.02689609
Iteration 73, loss = 0.02663109
Iteration 74, loss = 0.02660471
Iteration 75, loss = 0.02678444
Iteration 76, loss = 0.02669482
Iteration 77, loss = 0.02689941
Iteration 78, loss = 0.02665303
Iteration 79, loss = 0.02673315
Iteration 80, loss = 0.02670330
Iteration 81, loss = 0.02671420
Iteration 82, loss = 0.02666277
Iteration 83, loss = 0.02659300
Iteration 84, loss = 0.02649669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69693282
Iteration 2, loss = 0.60886980
Iteration 3, loss = 0.48897016
Iteration 4, loss = 0.36932731
Iteration 5, loss = 0.27381766
Iteration 6, loss = 0.20502104
Iteration 7, loss = 0.15782826
Iteration 8, loss = 0.12587906
Iteration 9, loss = 0.10299411
Iteration 10, loss = 0.08726542
Iteration 11, loss = 0.07571965
Iteration 12, loss = 0.06699644
Iteration 13, loss = 0.05991255
Iteration 14, loss = 0.05447793
Iteration 15, loss = 0.05039104
Iteration 16, loss = 0.04718012
Iteration 17, loss = 0.04429864
Iteration 18, loss = 0.04209039
Iteration 19, loss = 0.03976505
Iteration 20, loss = 0.03820236
Iteration 21, loss = 0.03681341
Iteration 22, loss = 0.03555932
Iteration 23, loss = 0.03444120
Iteration 24, loss = 0.03352924
Iteration 25, loss = 0.03281723
Iteration 26, loss = 0.03199228
Iteration 27, loss = 0.03164711
Iteration 28, loss = 0.03091931
Iteration 29, loss = 0.03018900
Iteration 30, loss = 0.02991166
Iteration 31, loss = 0.02948416
Iteration 32, loss = 0.02909192
Iteration 33, loss = 0.02861979
Iteration 34, loss = 0.02852512
Iteration 35, loss = 0.02830011
Iteration 36, loss = 0.02805735
Iteration 37, loss = 0.02767600
Iteration 38, loss = 0.02726080
Iteration 39, loss = 0.02733185
Iteration 40, loss = 0.02693883
Iteration 41, loss = 0.02691448
Iteration 42, loss = 0.02689899
Iteration 43, loss = 0.02664622
Iteration 44, loss = 0.02664576
Iteration 45, loss = 0.02640883
Iteration 46, loss = 0.02650654
Iteration 47, loss = 0.02630917
Iteration 48, loss = 0.02594803
Iteration 49, loss = 0.02599657
Iteration 50, loss = 0.02573883
Iteration 51, loss = 0.02583623
Iteration 52, loss = 0.02569206
Iteration 53, loss = 0.02548101
Iteration 54, loss = 0.02560036
Iteration 55, loss = 0.02551505
Iteration 56, loss = 0.02561594
Iteration 57, loss = 0.02545463
Iteration 58, loss = 0.02526823
Iteration 59, loss = 0.02560965
Iteration 60, loss = 0.02548682
Iteration 61, loss = 0.02509950
Iteration 62, loss = 0.02548854
Iteration 63, loss = 0.02507274
Iteration 64, loss = 0.02520526
Iteration 65, loss = 0.02501156
Iteration 66, loss = 0.02488615
Iteration 67, loss = 0.02485140
Iteration 68, loss = 0.02507758
Iteration 69, loss = 0.02509241
Iteration 70, loss = 0.02472586
Iteration 71, loss = 0.02496794
Iteration 72, loss = 0.02475523
Iteration 73, loss = 0.02483295
Iteration 74, loss = 0.02457760
Iteration 75, loss = 0.02479437
Iteration 76, loss = 0.02462773
Iteration 77, loss = 0.02468366
Iteration 78, loss = 0.02497714
Iteration 79, loss = 0.02458356
Iteration 80, loss = 0.02474140
Iteration 81, loss = 0.02473185
Iteration 82, loss = 0.02433627
Iteration 83, loss = 0.02433555
Iteration 84, loss = 0.02455158
Iteration 85, loss = 0.02436937
Iteration 86, loss = 0.02430246
Iteration 87, loss = 0.02444184
Iteration 88, loss = 0.02484791
Iteration 89, loss = 0.02438129
Iteration 90, loss = 0.02434981
Iteration 91, loss = 0.02422726
Iteration 92, loss = 0.02426158
Iteration 93, loss = 0.02408948
Iteration 94, loss = 0.02432645
Iteration 95, loss = 0.02421509
Iteration 96, loss = 0.02426889
Iteration 97, loss = 0.02424314
Iteration 98, loss = 0.02426807
Iteration 99, loss = 0.02415452
Iteration 100, loss = 0.02408629
Iteration 101, loss = 0.02450365
Iteration 102, loss = 0.02433805
Iteration 103, loss = 0.02430728
Iteration 104, loss = 0.02441342
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70461987
Iteration 2, loss = 0.62397434
Iteration 3, loss = 0.51682810
Iteration 4, loss = 0.39803372
Iteration 5, loss = 0.29771185
Iteration 6, loss = 0.22309736
Iteration 7, loss = 0.17110628
Iteration 8, loss = 0.13549273
Iteration 9, loss = 0.11086799
Iteration 10, loss = 0.09338260
Iteration 11, loss = 0.08086058
Iteration 12, loss = 0.07132206
Iteration 13, loss = 0.06388446
Iteration 14, loss = 0.05816694
Iteration 15, loss = 0.05388609
Iteration 16, loss = 0.04993822
Iteration 17, loss = 0.04718649
Iteration 18, loss = 0.04455180
Iteration 19, loss = 0.04277993
Iteration 20, loss = 0.04065229
Iteration 21, loss = 0.03920072
Iteration 22, loss = 0.03805455
Iteration 23, loss = 0.03699912
Iteration 24, loss = 0.03606060
Iteration 25, loss = 0.03489016
Iteration 26, loss = 0.03460921
Iteration 27, loss = 0.03364889
Iteration 28, loss = 0.03283695
Iteration 29, loss = 0.03269541
Iteration 30, loss = 0.03213014
Iteration 31, loss = 0.03178574
Iteration 32, loss = 0.03118113
Iteration 33, loss = 0.03091026
Iteration 34, loss = 0.03074409
Iteration 35, loss = 0.03019153
Iteration 36, loss = 0.03001810
Iteration 37, loss = 0.02958295
Iteration 38, loss = 0.02967476
Iteration 39, loss = 0.02926682
Iteration 40, loss = 0.02918109
Iteration 41, loss = 0.02878196
Iteration 42, loss = 0.02876533
Iteration 43, loss = 0.02844033
Iteration 44, loss = 0.02805595
Iteration 45, loss = 0.02855890
Iteration 46, loss = 0.02821410
Iteration 47, loss = 0.02816119
Iteration 48, loss = 0.02812133
Iteration 49, loss = 0.02815213
Iteration 50, loss = 0.02770774
Iteration 51, loss = 0.02811392
Iteration 52, loss = 0.02747642
Iteration 53, loss = 0.02773040
Iteration 54, loss = 0.02764406
Iteration 55, loss = 0.02743088
Iteration 56, loss = 0.02750707
Iteration 57, loss = 0.02745552
Iteration 58, loss = 0.02744289
Iteration 59, loss = 0.02713930
Iteration 60, loss = 0.02712895
Iteration 61, loss = 0.02706230
Iteration 62, loss = 0.02719302
Iteration 63, loss = 0.02714276
Iteration 64, loss = 0.02671589
Iteration 65, loss = 0.02700343
Iteration 66, loss = 0.02667739
Iteration 67, loss = 0.02695506
Iteration 68, loss = 0.02683747
Iteration 69, loss = 0.02692313
Iteration 70, loss = 0.02706211
Iteration 71, loss = 0.02659106
Iteration 72, loss = 0.02676417
Iteration 73, loss = 0.02642717
Iteration 74, loss = 0.02681540
Iteration 75, loss = 0.02691715
Iteration 76, loss = 0.02657779
Iteration 77, loss = 0.02654643
Iteration 78, loss = 0.02649137
Iteration 79, loss = 0.02651843
Iteration 80, loss = 0.02631473
Iteration 81, loss = 0.02638475
Iteration 82, loss = 0.02623952
Iteration 83, loss = 0.02652680
Iteration 84, loss = 0.02644352
Iteration 85, loss = 0.02634635
Iteration 86, loss = 0.02663465
Iteration 87, loss = 0.02648001
Iteration 88, loss = 0.02622102
Iteration 89, loss = 0.02616158
Iteration 90, loss = 0.02621268
Iteration 91, loss = 0.02668852
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71194494
Iteration 2, loss = 0.63902591
Iteration 3, loss = 0.53483972
Iteration 4, loss = 0.41434458
Iteration 5, loss = 0.31027718
Iteration 6, loss = 0.23124662
Iteration 7, loss = 0.17599634
Iteration 8, loss = 0.13824366
Iteration 9, loss = 0.11229118
Iteration 10, loss = 0.09393097
Iteration 11, loss = 0.08070765
Iteration 12, loss = 0.07049534
Iteration 13, loss = 0.06324414
Iteration 14, loss = 0.05743047
Iteration 15, loss = 0.05241036
Iteration 16, loss = 0.04858767
Iteration 17, loss = 0.04547988
Iteration 18, loss = 0.04291796
Iteration 19, loss = 0.04060969
Iteration 20, loss = 0.03869806
Iteration 21, loss = 0.03724984
Iteration 22, loss = 0.03560813
Iteration 23, loss = 0.03445235
Iteration 24, loss = 0.03338961
Iteration 25, loss = 0.03265393
Iteration 26, loss = 0.03156471
Iteration 27, loss = 0.03111817
Iteration 28, loss = 0.03053871
Iteration 29, loss = 0.02983588
Iteration 30, loss = 0.02954530
Iteration 31, loss = 0.02898053
Iteration 32, loss = 0.02859418
Iteration 33, loss = 0.02815124
Iteration 34, loss = 0.02781557
Iteration 35, loss = 0.02755912
Iteration 36, loss = 0.02714707
Iteration 37, loss = 0.02696033
Iteration 38, loss = 0.02663330
Iteration 39, loss = 0.02623897
Iteration 40, loss = 0.02614035
Iteration 41, loss = 0.02608353
Iteration 42, loss = 0.02574195
Iteration 43, loss = 0.02603555
Iteration 44, loss = 0.02538181
Iteration 45, loss = 0.02550012
Iteration 46, loss = 0.02522750
Iteration 47, loss = 0.02509911
Iteration 48, loss = 0.02531983
Iteration 49, loss = 0.02507082
Iteration 50, loss = 0.02490148
Iteration 51, loss = 0.02480220
Iteration 52, loss = 0.02481086
Iteration 53, loss = 0.02467328
Iteration 54, loss = 0.02435619
Iteration 55, loss = 0.02475128
Iteration 56, loss = 0.02440454
Iteration 57, loss = 0.02433656
Iteration 58, loss = 0.02453995
Iteration 59, loss = 0.02433533
Iteration 60, loss = 0.02421523
Iteration 61, loss = 0.02430745
Iteration 62, loss = 0.02413646
Iteration 63, loss = 0.02416284
Iteration 64, loss = 0.02394277
Iteration 65, loss = 0.02385362
Iteration 66, loss = 0.02395791
Iteration 67, loss = 0.02383168
Iteration 68, loss = 0.02379078
Iteration 69, loss = 0.02352122
Iteration 70, loss = 0.02378056
Iteration 71, loss = 0.02369058
Iteration 72, loss = 0.02377006
Iteration 73, loss = 0.02388359
Iteration 74, loss = 0.02374188
Iteration 75, loss = 0.02363055
Iteration 76, loss = 0.02397735
Iteration 77, loss = 0.02364423
Iteration 78, loss = 0.02371522
Iteration 79, loss = 0.02345714
Iteration 80, loss = 0.02322666
Iteration 81, loss = 0.02335757
Iteration 82, loss = 0.02327068
Iteration 83, loss = 0.02350278
Iteration 84, loss = 0.02359802
Iteration 85, loss = 0.02373784
Iteration 86, loss = 0.02318464
Iteration 87, loss = 0.02329183
Iteration 88, loss = 0.02322280
Iteration 89, loss = 0.02329027
Iteration 90, loss = 0.02309162
Iteration 91, loss = 0.02308025
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67560542
Iteration 2, loss = 0.59963297
Iteration 3, loss = 0.49133376
Iteration 4, loss = 0.38172890
Iteration 5, loss = 0.28883938
Iteration 6, loss = 0.21845970
Iteration 7, loss = 0.16863484
Iteration 8, loss = 0.13358776
Iteration 9, loss = 0.10930261
Iteration 10, loss = 0.09141338
Iteration 11, loss = 0.07872045
Iteration 12, loss = 0.06896408
Iteration 13, loss = 0.06135229
Iteration 14, loss = 0.05545140
Iteration 15, loss = 0.05059749
Iteration 16, loss = 0.04678274
Iteration 17, loss = 0.04378426
Iteration 18, loss = 0.04091965
Iteration 19, loss = 0.03874866
Iteration 20, loss = 0.03669838
Iteration 21, loss = 0.03532501
Iteration 22, loss = 0.03392534
Iteration 23, loss = 0.03258017
Iteration 24, loss = 0.03163262
Iteration 25, loss = 0.03072537
Iteration 26, loss = 0.02962987
Iteration 27, loss = 0.02931158
Iteration 28, loss = 0.02850082
Iteration 29, loss = 0.02782726
Iteration 30, loss = 0.02704560
Iteration 31, loss = 0.02685406
Iteration 32, loss = 0.02624791
Iteration 33, loss = 0.02592622
Iteration 34, loss = 0.02566484
Iteration 35, loss = 0.02533577
Iteration 36, loss = 0.02493784
Iteration 37, loss = 0.02476048
Iteration 38, loss = 0.02437730
Iteration 39, loss = 0.02419073
Iteration 40, loss = 0.02403220
Iteration 41, loss = 0.02385184
Iteration 42, loss = 0.02369577
Iteration 43, loss = 0.02344662
Iteration 44, loss = 0.02353898
Iteration 45, loss = 0.02321125
Iteration 46, loss = 0.02336335
Iteration 47, loss = 0.02312324
Iteration 48, loss = 0.02289857
Iteration 49, loss = 0.02295430
Iteration 50, loss = 0.02263113
Iteration 51, loss = 0.02266373
Iteration 52, loss = 0.02281731
Iteration 53, loss = 0.02222577
Iteration 54, loss = 0.02206043
Iteration 55, loss = 0.02236778
Iteration 56, loss = 0.02231889
Iteration 57, loss = 0.02216312
Iteration 58, loss = 0.02219011
Iteration 59, loss = 0.02190854
Iteration 60, loss = 0.02193792
Iteration 61, loss = 0.02181301
Iteration 62, loss = 0.02176089
Iteration 63, loss = 0.02187281
Iteration 64, loss = 0.02160783
Iteration 65, loss = 0.02170528
Iteration 66, loss = 0.02162503
Iteration 67, loss = 0.02156600
Iteration 68, loss = 0.02163836
Iteration 69, loss = 0.02139421
Iteration 70, loss = 0.02152631
Iteration 71, loss = 0.02157862
Iteration 72, loss = 0.02138508
Iteration 73, loss = 0.02151320
Iteration 74, loss = 0.02140605
Iteration 75, loss = 0.02123877
Iteration 76, loss = 0.02143444
Iteration 77, loss = 0.02128154
Iteration 78, loss = 0.02162984
Iteration 79, loss = 0.02141464
Iteration 80, loss = 0.02136642
Iteration 81, loss = 0.02155690
Iteration 82, loss = 0.02133487
Iteration 83, loss = 0.02105499
Iteration 84, loss = 0.02145933
Iteration 85, loss = 0.02128175
Iteration 86, loss = 0.02100696
Iteration 87, loss = 0.02130399
Iteration 88, loss = 0.02134249
Iteration 89, loss = 0.02073742
Iteration 90, loss = 0.02105019
Iteration 91, loss = 0.02111258
Iteration 92, loss = 0.02120651
Iteration 93, loss = 0.02113475
Iteration 94, loss = 0.02100947
Iteration 95, loss = 0.02127790
Iteration 96, loss = 0.02117292
Iteration 97, loss = 0.02127383
Iteration 98, loss = 0.02105813
Iteration 99, loss = 0.02113216
Iteration 100, loss = 0.02118045
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71344479
Iteration 2, loss = 0.62648296
Iteration 3, loss = 0.51290552
Iteration 4, loss = 0.39493412
Iteration 5, loss = 0.29728626
Iteration 6, loss = 0.22402509
Iteration 7, loss = 0.17270177
Iteration 8, loss = 0.13755096
Iteration 9, loss = 0.11259784
Iteration 10, loss = 0.09543671
Iteration 11, loss = 0.08263671
Iteration 12, loss = 0.07299232
Iteration 13, loss = 0.06563726
Iteration 14, loss = 0.05983752
Iteration 15, loss = 0.05533936
Iteration 16, loss = 0.05139907
Iteration 17, loss = 0.04821121
Iteration 18, loss = 0.04583730
Iteration 19, loss = 0.04354330
Iteration 20, loss = 0.04163684
Iteration 21, loss = 0.04003100
Iteration 22, loss = 0.03904911
Iteration 23, loss = 0.03775501
Iteration 24, loss = 0.03696980
Iteration 25, loss = 0.03586805
Iteration 26, loss = 0.03491596
Iteration 27, loss = 0.03407363
Iteration 28, loss = 0.03361383
Iteration 29, loss = 0.03310955
Iteration 30, loss = 0.03251767
Iteration 31, loss = 0.03219447
Iteration 32, loss = 0.03180208
Iteration 33, loss = 0.03145719
Iteration 34, loss = 0.03101235
Iteration 35, loss = 0.03060613
Iteration 36, loss = 0.03028541
Iteration 37, loss = 0.03008028
Iteration 38, loss = 0.02994084
Iteration 39, loss = 0.02961472
Iteration 40, loss = 0.02934174
Iteration 41, loss = 0.02920989
Iteration 42, loss = 0.02916601
Iteration 43, loss = 0.02883346
Iteration 44, loss = 0.02885238
Iteration 45, loss = 0.02893382
Iteration 46, loss = 0.02863434
Iteration 47, loss = 0.02853885
Iteration 48, loss = 0.02830956
Iteration 49, loss = 0.02822888
Iteration 50, loss = 0.02796273
Iteration 51, loss = 0.02803896
Iteration 52, loss = 0.02800117
Iteration 53, loss = 0.02768750
Iteration 54, loss = 0.02780547
Iteration 55, loss = 0.02740464
Iteration 56, loss = 0.02750472
Iteration 57, loss = 0.02768158
Iteration 58, loss = 0.02743406
Iteration 59, loss = 0.02741749
Iteration 60, loss = 0.02743552
Iteration 61, loss = 0.02717237
Iteration 62, loss = 0.02714974
Iteration 63, loss = 0.02701191
Iteration 64, loss = 0.02703559
Iteration 65, loss = 0.02689852
Iteration 66, loss = 0.02708949
Iteration 67, loss = 0.02718772
Iteration 68, loss = 0.02722811
Iteration 69, loss = 0.02676906
Iteration 70, loss = 0.02675273
Iteration 71, loss = 0.02675816
Iteration 72, loss = 0.02689609
Iteration 73, loss = 0.02663109
Iteration 74, loss = 0.02660471
Iteration 75, loss = 0.02678444
Iteration 76, loss = 0.02669482
Iteration 77, loss = 0.02689941
Iteration 78, loss = 0.02665303
Iteration 79, loss = 0.02673315
Iteration 80, loss = 0.02670330
Iteration 81, loss = 0.02671420
Iteration 82, loss = 0.02666277
Iteration 83, loss = 0.02659300
Iteration 84, loss = 0.02649669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69693282
Iteration 2, loss = 0.60886980
Iteration 3, loss = 0.48897016
Iteration 4, loss = 0.36932731
Iteration 5, loss = 0.27381766
Iteration 6, loss = 0.20502104
Iteration 7, loss = 0.15782826
Iteration 8, loss = 0.12587906
Iteration 9, loss = 0.10299411
Iteration 10, loss = 0.08726542
Iteration 11, loss = 0.07571965
Iteration 12, loss = 0.06699644
Iteration 13, loss = 0.05991255
Iteration 14, loss = 0.05447793
Iteration 15, loss = 0.05039104
Iteration 16, loss = 0.04718012
Iteration 17, loss = 0.04429864
Iteration 18, loss = 0.04209039
Iteration 19, loss = 0.03976505
Iteration 20, loss = 0.03820236
Iteration 21, loss = 0.03681341
Iteration 22, loss = 0.03555932
Iteration 23, loss = 0.03444120
Iteration 24, loss = 0.03352924
Iteration 25, loss = 0.03281723
Iteration 26, loss = 0.03199228
Iteration 27, loss = 0.03164711
Iteration 28, loss = 0.03091931
Iteration 29, loss = 0.03018900
Iteration 30, loss = 0.02991166
Iteration 31, loss = 0.02948416
Iteration 32, loss = 0.02909192
Iteration 33, loss = 0.02861979
Iteration 34, loss = 0.02852512
Iteration 35, loss = 0.02830011
Iteration 36, loss = 0.02805735
Iteration 37, loss = 0.02767600
Iteration 38, loss = 0.02726080
Iteration 39, loss = 0.02733185
Iteration 40, loss = 0.02693883
Iteration 41, loss = 0.02691448
Iteration 42, loss = 0.02689899
Iteration 43, loss = 0.02664622
Iteration 44, loss = 0.02664576
Iteration 45, loss = 0.02640883
Iteration 46, loss = 0.02650654
Iteration 47, loss = 0.02630917
Iteration 48, loss = 0.02594803
Iteration 49, loss = 0.02599657
Iteration 50, loss = 0.02573883
Iteration 51, loss = 0.02583623
Iteration 52, loss = 0.02569206
Iteration 53, loss = 0.02548101
Iteration 54, loss = 0.02560036
Iteration 55, loss = 0.02551505
Iteration 56, loss = 0.02561594
Iteration 57, loss = 0.02545463
Iteration 58, loss = 0.02526823
Iteration 59, loss = 0.02560965
Iteration 60, loss = 0.02548682
Iteration 61, loss = 0.02509950
Iteration 62, loss = 0.02548854
Iteration 63, loss = 0.02507274
Iteration 64, loss = 0.02520526
Iteration 65, loss = 0.02501156
Iteration 66, loss = 0.02488615
Iteration 67, loss = 0.02485140
Iteration 68, loss = 0.02507758
Iteration 69, loss = 0.02509241
Iteration 70, loss = 0.02472586
Iteration 71, loss = 0.02496794
Iteration 72, loss = 0.02475523
Iteration 73, loss = 0.02483295
Iteration 74, loss = 0.02457760
Iteration 75, loss = 0.02479437
Iteration 76, loss = 0.02462773
Iteration 77, loss = 0.02468366
Iteration 78, loss = 0.02497714
Iteration 79, loss = 0.02458356
Iteration 80, loss = 0.02474140
Iteration 81, loss = 0.02473185
Iteration 82, loss = 0.02433627
Iteration 83, loss = 0.02433555
Iteration 84, loss = 0.02455158
Iteration 85, loss = 0.02436937
Iteration 86, loss = 0.02430246
Iteration 87, loss = 0.02444184
Iteration 88, loss = 0.02484791
Iteration 89, loss = 0.02438129
Iteration 90, loss = 0.02434981
Iteration 91, loss = 0.02422726
Iteration 92, loss = 0.02426158
Iteration 93, loss = 0.02408948
Iteration 94, loss = 0.02432645
Iteration 95, loss = 0.02421509
Iteration 96, loss = 0.02426889
Iteration 97, loss = 0.02424314
Iteration 98, loss = 0.02426807
Iteration 99, loss = 0.02415452
Iteration 100, loss = 0.02408629
Iteration 101, loss = 0.02450365
Iteration 102, loss = 0.02433805
Iteration 103, loss = 0.02430728
Iteration 104, loss = 0.02441342
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70461987
Iteration 2, loss = 0.62397434
Iteration 3, loss = 0.51682810
Iteration 4, loss = 0.39803372
Iteration 5, loss = 0.29771185
Iteration 6, loss = 0.22309736
Iteration 7, loss = 0.17110628
Iteration 8, loss = 0.13549273
Iteration 9, loss = 0.11086799
Iteration 10, loss = 0.09338260
Iteration 11, loss = 0.08086058
Iteration 12, loss = 0.07132206
Iteration 13, loss = 0.06388446
Iteration 14, loss = 0.05816694
Iteration 15, loss = 0.05388609
Iteration 16, loss = 0.04993822
Iteration 17, loss = 0.04718649
Iteration 18, loss = 0.04455180
Iteration 19, loss = 0.04277993
Iteration 20, loss = 0.04065229
Iteration 21, loss = 0.03920072
Iteration 22, loss = 0.03805455
Iteration 23, loss = 0.03699912
Iteration 24, loss = 0.03606060
Iteration 25, loss = 0.03489016
Iteration 26, loss = 0.03460921
Iteration 27, loss = 0.03364889
Iteration 28, loss = 0.03283695
Iteration 29, loss = 0.03269541
Iteration 30, loss = 0.03213014
Iteration 31, loss = 0.03178574
Iteration 32, loss = 0.03118113
Iteration 33, loss = 0.03091026
Iteration 34, loss = 0.03074409
Iteration 35, loss = 0.03019153
Iteration 36, loss = 0.03001810
Iteration 37, loss = 0.02958295
Iteration 38, loss = 0.02967476
Iteration 39, loss = 0.02926682
Iteration 40, loss = 0.02918109
Iteration 41, loss = 0.02878196
Iteration 42, loss = 0.02876533
Iteration 43, loss = 0.02844033
Iteration 44, loss = 0.02805595
Iteration 45, loss = 0.02855890
Iteration 46, loss = 0.02821410
Iteration 47, loss = 0.02816119
Iteration 48, loss = 0.02812133
Iteration 49, loss = 0.02815213
Iteration 50, loss = 0.02770774
Iteration 51, loss = 0.02811392
Iteration 52, loss = 0.02747642
Iteration 53, loss = 0.02773040
Iteration 54, loss = 0.02764406
Iteration 55, loss = 0.02743088
Iteration 56, loss = 0.02750707
Iteration 57, loss = 0.02745552
Iteration 58, loss = 0.02744289
Iteration 59, loss = 0.02713930
Iteration 60, loss = 0.02712895
Iteration 61, loss = 0.02706230
Iteration 62, loss = 0.02719302
Iteration 63, loss = 0.02714276
Iteration 64, loss = 0.02671589
Iteration 65, loss = 0.02700343
Iteration 66, loss = 0.02667739
Iteration 67, loss = 0.02695506
Iteration 68, loss = 0.02683747
Iteration 69, loss = 0.02692313
Iteration 70, loss = 0.02706211
Iteration 71, loss = 0.02659106
Iteration 72, loss = 0.02676417
Iteration 73, loss = 0.02642717
Iteration 74, loss = 0.02681540
Iteration 75, loss = 0.02691715
Iteration 76, loss = 0.02657779
Iteration 77, loss = 0.02654643
Iteration 78, loss = 0.02649137
Iteration 79, loss = 0.02651843
Iteration 80, loss = 0.02631473
Iteration 81, loss = 0.02638475
Iteration 82, loss = 0.02623952
Iteration 83, loss = 0.02652680
Iteration 84, loss = 0.02644352
Iteration 85, loss = 0.02634635
Iteration 86, loss = 0.02663465
Iteration 87, loss = 0.02648001
Iteration 88, loss = 0.02622102
Iteration 89, loss = 0.02616158
Iteration 90, loss = 0.02621268
Iteration 91, loss = 0.02668852
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71194494
Iteration 2, loss = 0.63902591
Iteration 3, loss = 0.53483972
Iteration 4, loss = 0.41434458
Iteration 5, loss = 0.31027718
Iteration 6, loss = 0.23124662
Iteration 7, loss = 0.17599634
Iteration 8, loss = 0.13824366
Iteration 9, loss = 0.11229118
Iteration 10, loss = 0.09393097
Iteration 11, loss = 0.08070765
Iteration 12, loss = 0.07049534
Iteration 13, loss = 0.06324414
Iteration 14, loss = 0.05743047
Iteration 15, loss = 0.05241036
Iteration 16, loss = 0.04858767
Iteration 17, loss = 0.04547988
Iteration 18, loss = 0.04291796
Iteration 19, loss = 0.04060969
Iteration 20, loss = 0.03869806
Iteration 21, loss = 0.03724984
Iteration 22, loss = 0.03560813
Iteration 23, loss = 0.03445235
Iteration 24, loss = 0.03338961
Iteration 25, loss = 0.03265393
Iteration 26, loss = 0.03156471
Iteration 27, loss = 0.03111817
Iteration 28, loss = 0.03053871
Iteration 29, loss = 0.02983588
Iteration 30, loss = 0.02954530
Iteration 31, loss = 0.02898053
Iteration 32, loss = 0.02859418
Iteration 33, loss = 0.02815124
Iteration 34, loss = 0.02781557
Iteration 35, loss = 0.02755912
Iteration 36, loss = 0.02714707
Iteration 37, loss = 0.02696033
Iteration 38, loss = 0.02663330
Iteration 39, loss = 0.02623897
Iteration 40, loss = 0.02614035
Iteration 41, loss = 0.02608353
Iteration 42, loss = 0.02574195
Iteration 43, loss = 0.02603555
Iteration 44, loss = 0.02538181
Iteration 45, loss = 0.02550012
Iteration 46, loss = 0.02522750
Iteration 47, loss = 0.02509911
Iteration 48, loss = 0.02531983
Iteration 49, loss = 0.02507082
Iteration 50, loss = 0.02490148
Iteration 51, loss = 0.02480220
Iteration 52, loss = 0.02481086
Iteration 53, loss = 0.02467328
Iteration 54, loss = 0.02435619
Iteration 55, loss = 0.02475128
Iteration 56, loss = 0.02440454
Iteration 57, loss = 0.02433656
Iteration 58, loss = 0.02453995
Iteration 59, loss = 0.02433533
Iteration 60, loss = 0.02421523
Iteration 61, loss = 0.02430745
Iteration 62, loss = 0.02413646
Iteration 63, loss = 0.02416284
Iteration 64, loss = 0.02394277
Iteration 65, loss = 0.02385362
Iteration 66, loss = 0.02395791
Iteration 67, loss = 0.02383168
Iteration 68, loss = 0.02379078
Iteration 69, loss = 0.02352122
Iteration 70, loss = 0.02378056
Iteration 71, loss = 0.02369058
Iteration 72, loss = 0.02377006
Iteration 73, loss = 0.02388359
Iteration 74, loss = 0.02374188
Iteration 75, loss = 0.02363055
Iteration 76, loss = 0.02397735
Iteration 77, loss = 0.02364423
Iteration 78, loss = 0.02371522
Iteration 79, loss = 0.02345714
Iteration 80, loss = 0.02322666
Iteration 81, loss = 0.02335757
Iteration 82, loss = 0.02327068
Iteration 83, loss = 0.02350278
Iteration 84, loss = 0.02359802
Iteration 85, loss = 0.02373784
Iteration 86, loss = 0.02318464
Iteration 87, loss = 0.02329183
Iteration 88, loss = 0.02322280
Iteration 89, loss = 0.02329027
Iteration 90, loss = 0.02309162
Iteration 91, loss = 0.02308025
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67560542
Iteration 2, loss = 0.59963297
Iteration 3, loss = 0.49133376
Iteration 4, loss = 0.38172890
Iteration 5, loss = 0.28883938
Iteration 6, loss = 0.21845970
Iteration 7, loss = 0.16863484
Iteration 8, loss = 0.13358776
Iteration 9, loss = 0.10930261
Iteration 10, loss = 0.09141338
Iteration 11, loss = 0.07872045
Iteration 12, loss = 0.06896408
Iteration 13, loss = 0.06135229
Iteration 14, loss = 0.05545140
Iteration 15, loss = 0.05059749
Iteration 16, loss = 0.04678274
Iteration 17, loss = 0.04378426
Iteration 18, loss = 0.04091965
Iteration 19, loss = 0.03874866
Iteration 20, loss = 0.03669838
Iteration 21, loss = 0.03532501
Iteration 22, loss = 0.03392534
Iteration 23, loss = 0.03258017
Iteration 24, loss = 0.03163262
Iteration 25, loss = 0.03072537
Iteration 26, loss = 0.02962987
Iteration 27, loss = 0.02931158
Iteration 28, loss = 0.02850082
Iteration 29, loss = 0.02782726
Iteration 30, loss = 0.02704560
Iteration 31, loss = 0.02685406
Iteration 32, loss = 0.02624791
Iteration 33, loss = 0.02592622
Iteration 34, loss = 0.02566484
Iteration 35, loss = 0.02533577
Iteration 36, loss = 0.02493784
Iteration 37, loss = 0.02476048
Iteration 38, loss = 0.02437730
Iteration 39, loss = 0.02419073
Iteration 40, loss = 0.02403220
Iteration 41, loss = 0.02385184
Iteration 42, loss = 0.02369577
Iteration 43, loss = 0.02344662
Iteration 44, loss = 0.02353898
Iteration 45, loss = 0.02321125
Iteration 46, loss = 0.02336335
Iteration 47, loss = 0.02312324
Iteration 48, loss = 0.02289857
Iteration 49, loss = 0.02295430
Iteration 50, loss = 0.02263113
Iteration 51, loss = 0.02266373
Iteration 52, loss = 0.02281731
Iteration 53, loss = 0.02222577
Iteration 54, loss = 0.02206043
Iteration 55, loss = 0.02236778
Iteration 56, loss = 0.02231889
Iteration 57, loss = 0.02216312
Iteration 58, loss = 0.02219011
Iteration 59, loss = 0.02190854
Iteration 60, loss = 0.02193792
Iteration 61, loss = 0.02181301
Iteration 62, loss = 0.02176089
Iteration 63, loss = 0.02187281
Iteration 64, loss = 0.02160783
Iteration 65, loss = 0.02170528
Iteration 66, loss = 0.02162503
Iteration 67, loss = 0.02156600
Iteration 68, loss = 0.02163836
Iteration 69, loss = 0.02139421
Iteration 70, loss = 0.02152631
Iteration 71, loss = 0.02157862
Iteration 72, loss = 0.02138508
Iteration 73, loss = 0.02151320
Iteration 74, loss = 0.02140605
Iteration 75, loss = 0.02123877
Iteration 76, loss = 0.02143444
Iteration 77, loss = 0.02128154
Iteration 78, loss = 0.02162984
Iteration 79, loss = 0.02141464
Iteration 80, loss = 0.02136642
Iteration 81, loss = 0.02155690
Iteration 82, loss = 0.02133487
Iteration 83, loss = 0.02105499
Iteration 84, loss = 0.02145933
Iteration 85, loss = 0.02128175
Iteration 86, loss = 0.02100696
Iteration 87, loss = 0.02130399
Iteration 88, loss = 0.02134249
Iteration 89, loss = 0.02073742
Iteration 90, loss = 0.02105019
Iteration 91, loss = 0.02111258
Iteration 92, loss = 0.02120651
Iteration 93, loss = 0.02113475
Iteration 94, loss = 0.02100947
Iteration 95, loss = 0.02127790
Iteration 96, loss = 0.02117292
Iteration 97, loss = 0.02127383
Iteration 98, loss = 0.02105813
Iteration 99, loss = 0.02113216
Iteration 100, loss = 0.02118045
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71344479
Iteration 2, loss = 0.62648296
Iteration 3, loss = 0.51290552
Iteration 4, loss = 0.39493412
Iteration 5, loss = 0.29728626
Iteration 6, loss = 0.22402509
Iteration 7, loss = 0.17270177
Iteration 8, loss = 0.13755096
Iteration 9, loss = 0.11259784
Iteration 10, loss = 0.09543671
Iteration 11, loss = 0.08263671
Iteration 12, loss = 0.07299232
Iteration 13, loss = 0.06563726
Iteration 14, loss = 0.05983752
Iteration 15, loss = 0.05533936
Iteration 16, loss = 0.05139907
Iteration 17, loss = 0.04821121
Iteration 18, loss = 0.04583730
Iteration 19, loss = 0.04354330
Iteration 20, loss = 0.04163684
Iteration 21, loss = 0.04003100
Iteration 22, loss = 0.03904911
Iteration 23, loss = 0.03775501
Iteration 24, loss = 0.03696980
Iteration 25, loss = 0.03586805
Iteration 26, loss = 0.03491596
Iteration 27, loss = 0.03407363
Iteration 28, loss = 0.03361383
Iteration 29, loss = 0.03310955
Iteration 30, loss = 0.03251767
Iteration 31, loss = 0.03219447
Iteration 32, loss = 0.03180208
Iteration 33, loss = 0.03145719
Iteration 34, loss = 0.03101235
Iteration 35, loss = 0.03060613
Iteration 36, loss = 0.03028541
Iteration 37, loss = 0.03008028
Iteration 38, loss = 0.02994084
Iteration 39, loss = 0.02961472
Iteration 40, loss = 0.02934174
Iteration 41, loss = 0.02920989
Iteration 42, loss = 0.02916601
Iteration 43, loss = 0.02883346
Iteration 44, loss = 0.02885238
Iteration 45, loss = 0.02893382
Iteration 46, loss = 0.02863434
Iteration 47, loss = 0.02853885
Iteration 48, loss = 0.02830956
Iteration 49, loss = 0.02822888
Iteration 50, loss = 0.02796273
Iteration 51, loss = 0.02803896
Iteration 52, loss = 0.02800117
Iteration 53, loss = 0.02768750
Iteration 54, loss = 0.02780547
Iteration 55, loss = 0.02740464
Iteration 56, loss = 0.02750472
Iteration 57, loss = 0.02768158
Iteration 58, loss = 0.02743406
Iteration 59, loss = 0.02741749
Iteration 60, loss = 0.02743552
Iteration 61, loss = 0.02717237
Iteration 62, loss = 0.02714974
Iteration 63, loss = 0.02701191
Iteration 64, loss = 0.02703559
Iteration 65, loss = 0.02689852
Iteration 66, loss = 0.02708949
Iteration 67, loss = 0.02718772
Iteration 68, loss = 0.02722811
Iteration 69, loss = 0.02676906
Iteration 70, loss = 0.02675273
Iteration 71, loss = 0.02675816
Iteration 72, loss = 0.02689609
Iteration 73, loss = 0.02663109
Iteration 74, loss = 0.02660471
Iteration 75, loss = 0.02678444
Iteration 76, loss = 0.02669482
Iteration 77, loss = 0.02689941
Iteration 78, loss = 0.02665303
Iteration 79, loss = 0.02673315
Iteration 80, loss = 0.02670330
Iteration 81, loss = 0.02671420
Iteration 82, loss = 0.02666277
Iteration 83, loss = 0.02659300
Iteration 84, loss = 0.02649669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69693282
Iteration 2, loss = 0.60886980
Iteration 3, loss = 0.48897016
Iteration 4, loss = 0.36932731
Iteration 5, loss = 0.27381766
Iteration 6, loss = 0.20502104
Iteration 7, loss = 0.15782826
Iteration 8, loss = 0.12587906
Iteration 9, loss = 0.10299411
Iteration 10, loss = 0.08726542
Iteration 11, loss = 0.07571965
Iteration 12, loss = 0.06699644
Iteration 13, loss = 0.05991255
Iteration 14, loss = 0.05447793
Iteration 15, loss = 0.05039104
Iteration 16, loss = 0.04718012
Iteration 17, loss = 0.04429864
Iteration 18, loss = 0.04209039
Iteration 19, loss = 0.03976505
Iteration 20, loss = 0.03820236
Iteration 21, loss = 0.03681341
Iteration 22, loss = 0.03555932
Iteration 23, loss = 0.03444120
Iteration 24, loss = 0.03352924
Iteration 25, loss = 0.03281723
Iteration 26, loss = 0.03199228
Iteration 27, loss = 0.03164711
Iteration 28, loss = 0.03091931
Iteration 29, loss = 0.03018900
Iteration 30, loss = 0.02991166
Iteration 31, loss = 0.02948416
Iteration 32, loss = 0.02909192
Iteration 33, loss = 0.02861979
Iteration 34, loss = 0.02852512
Iteration 35, loss = 0.02830011
Iteration 36, loss = 0.02805735
Iteration 37, loss = 0.02767600
Iteration 38, loss = 0.02726080
Iteration 39, loss = 0.02733185
Iteration 40, loss = 0.02693883
Iteration 41, loss = 0.02691448
Iteration 42, loss = 0.02689899
Iteration 43, loss = 0.02664622
Iteration 44, loss = 0.02664576
Iteration 45, loss = 0.02640883
Iteration 46, loss = 0.02650654
Iteration 47, loss = 0.02630917
Iteration 48, loss = 0.02594803
Iteration 49, loss = 0.02599657
Iteration 50, loss = 0.02573883
Iteration 51, loss = 0.02583623
Iteration 52, loss = 0.02569206
Iteration 53, loss = 0.02548101
Iteration 54, loss = 0.02560036
Iteration 55, loss = 0.02551505
Iteration 56, loss = 0.02561594
Iteration 57, loss = 0.02545463
Iteration 58, loss = 0.02526823
Iteration 59, loss = 0.02560965
Iteration 60, loss = 0.02548682
Iteration 61, loss = 0.02509950
Iteration 62, loss = 0.02548854
Iteration 63, loss = 0.02507274
Iteration 64, loss = 0.02520526
Iteration 65, loss = 0.02501156
Iteration 66, loss = 0.02488615
Iteration 67, loss = 0.02485140
Iteration 68, loss = 0.02507758
Iteration 69, loss = 0.02509241
Iteration 70, loss = 0.02472586
Iteration 71, loss = 0.02496794
Iteration 72, loss = 0.02475523
Iteration 73, loss = 0.02483295
Iteration 74, loss = 0.02457760
Iteration 75, loss = 0.02479437
Iteration 76, loss = 0.02462773
Iteration 77, loss = 0.02468366
Iteration 78, loss = 0.02497714
Iteration 79, loss = 0.02458356
Iteration 80, loss = 0.02474140
Iteration 81, loss = 0.02473185
Iteration 82, loss = 0.02433627
Iteration 83, loss = 0.02433555
Iteration 84, loss = 0.02455158
Iteration 85, loss = 0.02436937
Iteration 86, loss = 0.02430246
Iteration 87, loss = 0.02444184
Iteration 88, loss = 0.02484791
Iteration 89, loss = 0.02438129
Iteration 90, loss = 0.02434981
Iteration 91, loss = 0.02422726
Iteration 92, loss = 0.02426158
Iteration 93, loss = 0.02408948
Iteration 94, loss = 0.02432645
Iteration 95, loss = 0.02421509
Iteration 96, loss = 0.02426889
Iteration 97, loss = 0.02424314
Iteration 98, loss = 0.02426807
Iteration 99, loss = 0.02415452
Iteration 100, loss = 0.02408629
Iteration 101, loss = 0.02450365
Iteration 102, loss = 0.02433805
Iteration 103, loss = 0.02430728
Iteration 104, loss = 0.02441342
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67627328
Iteration 2, loss = 0.59082394
Iteration 3, loss = 0.46143781
Iteration 4, loss = 0.33599577
Iteration 5, loss = 0.23989805
Iteration 6, loss = 0.17570079
Iteration 7, loss = 0.13469570
Iteration 8, loss = 0.10775046
Iteration 9, loss = 0.08929102
Iteration 10, loss = 0.07659345
Iteration 11, loss = 0.06779452
Iteration 12, loss = 0.06051301
Iteration 13, loss = 0.05541692
Iteration 14, loss = 0.05110317
Iteration 15, loss = 0.04770492
Iteration 16, loss = 0.04504085
Iteration 17, loss = 0.04280118
Iteration 18, loss = 0.04110449
Iteration 19, loss = 0.03949099
Iteration 20, loss = 0.03792610
Iteration 21, loss = 0.03724425
Iteration 22, loss = 0.03590473
Iteration 23, loss = 0.03512124
Iteration 24, loss = 0.03456930
Iteration 25, loss = 0.03393084
Iteration 26, loss = 0.03328965
Iteration 27, loss = 0.03286032
Iteration 28, loss = 0.03221879
Iteration 29, loss = 0.03200370
Iteration 30, loss = 0.03154893
Iteration 31, loss = 0.03112323
Iteration 32, loss = 0.03077072
Iteration 33, loss = 0.03065192
Iteration 34, loss = 0.03048936
Iteration 35, loss = 0.03024750
Iteration 36, loss = 0.02999061
Iteration 37, loss = 0.02981295
Iteration 38, loss = 0.02970018
Iteration 39, loss = 0.02951737
Iteration 40, loss = 0.02943660
Iteration 41, loss = 0.02936884
Iteration 42, loss = 0.02910905
Iteration 43, loss = 0.02879783
Iteration 44, loss = 0.02909042
Iteration 45, loss = 0.02906438
Iteration 46, loss = 0.02894609
Iteration 47, loss = 0.02875822
Iteration 48, loss = 0.02842368
Iteration 49, loss = 0.02851593
Iteration 50, loss = 0.02842409
Iteration 51, loss = 0.02860168
Iteration 52, loss = 0.02829466
Iteration 53, loss = 0.02834623
Iteration 54, loss = 0.02838141
Iteration 55, loss = 0.02820312
Iteration 56, loss = 0.02801680
Iteration 57, loss = 0.02800097
Iteration 58, loss = 0.02811064
Iteration 59, loss = 0.02789348
Iteration 60, loss = 0.02805633
Iteration 61, loss = 0.02806913
Iteration 62, loss = 0.02796150
Iteration 63, loss = 0.02806929
Iteration 64, loss = 0.02774511
Iteration 65, loss = 0.02769380
Iteration 66, loss = 0.02771646
Iteration 67, loss = 0.02787998
Iteration 68, loss = 0.02753791
Iteration 69, loss = 0.02814951
Iteration 70, loss = 0.02797137
Iteration 71, loss = 0.02766703
Iteration 72, loss = 0.02788886
Iteration 73, loss = 0.02746408
Iteration 74, loss = 0.02762320
Iteration 75, loss = 0.02775883
Iteration 76, loss = 0.02797105
Iteration 77, loss = 0.02747224
Iteration 78, loss = 0.02738610
Iteration 79, loss = 0.02748267
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on kept_v8_keep_hashtags...
Iteration 1, loss = 0.69749492
Iteration 2, loss = 0.67014516
Iteration 3, loss = 0.63613883
Iteration 4, loss = 0.59407762
Iteration 5, loss = 0.54211094
Iteration 6, loss = 0.48536688
Iteration 7, loss = 0.42757963
Iteration 8, loss = 0.37199866
Iteration 9, loss = 0.32061605
Iteration 10, loss = 0.27484839
Iteration 11, loss = 0.23506083
Iteration 12, loss = 0.20182170
Iteration 13, loss = 0.17385622
Iteration 14, loss = 0.15103926
Iteration 15, loss = 0.13204960
Iteration 16, loss = 0.11651158
Iteration 17, loss = 0.10350955
Iteration 18, loss = 0.09286038
Iteration 19, loss = 0.08395847
Iteration 20, loss = 0.07655220
Iteration 21, loss = 0.06999865
Iteration 22, loss = 0.06470230
Iteration 23, loss = 0.05980009
Iteration 24, loss = 0.05573249
Iteration 25, loss = 0.05223335
Iteration 26, loss = 0.04906906
Iteration 27, loss = 0.04638204
Iteration 28, loss = 0.04389071
Iteration 29, loss = 0.04180274
Iteration 30, loss = 0.03980353
Iteration 31, loss = 0.03805371
Iteration 32, loss = 0.03655323
Iteration 33, loss = 0.03512151
Iteration 34, loss = 0.03373237
Iteration 35, loss = 0.03268783
Iteration 36, loss = 0.03158597
Iteration 37, loss = 0.03065832
Iteration 38, loss = 0.02982940
Iteration 39, loss = 0.02898199
Iteration 40, loss = 0.02809475
Iteration 41, loss = 0.02745209
Iteration 42, loss = 0.02689424
Iteration 43, loss = 0.02622739
Iteration 44, loss = 0.02577251
Iteration 45, loss = 0.02519602
Iteration 46, loss = 0.02461267
Iteration 47, loss = 0.02420420
Iteration 48, loss = 0.02379639
Iteration 49, loss = 0.02342112
Iteration 50, loss = 0.02306044
Iteration 51, loss = 0.02259213
Iteration 52, loss = 0.02241982
Iteration 53, loss = 0.02199954
Iteration 54, loss = 0.02170389
Iteration 55, loss = 0.02133521
Iteration 56, loss = 0.02121253
Iteration 57, loss = 0.02095627
Iteration 58, loss = 0.02077690
Iteration 59, loss = 0.02050269
Iteration 60, loss = 0.02029634
Iteration 61, loss = 0.02007042
Iteration 62, loss = 0.01982248
Iteration 63, loss = 0.01983495
Iteration 64, loss = 0.01957910
Iteration 65, loss = 0.01947066
Iteration 66, loss = 0.01926971
Iteration 67, loss = 0.01909445
Iteration 68, loss = 0.01899043
Iteration 69, loss = 0.01897895
Iteration 70, loss = 0.01868852
Iteration 71, loss = 0.01860298
Iteration 72, loss = 0.01840918
Iteration 73, loss = 0.01833684
Iteration 74, loss = 0.01824538
Iteration 75, loss = 0.01814385
Iteration 76, loss = 0.01811918
Iteration 77, loss = 0.01791904
Iteration 78, loss = 0.01785128
Iteration 79, loss = 0.01774965
Iteration 80, loss = 0.01760599
Iteration 81, loss = 0.01761041
Iteration 82, loss = 0.01748718
Iteration 83, loss = 0.01746821
Iteration 84, loss = 0.01739689
Iteration 85, loss = 0.01718031
Iteration 86, loss = 0.01721732
Iteration 87, loss = 0.01714555
Iteration 88, loss = 0.01714547
Iteration 89, loss = 0.01700805
Iteration 90, loss = 0.01692447
Iteration 91, loss = 0.01696732
Iteration 92, loss = 0.01676433
Iteration 93, loss = 0.01677920
Iteration 94, loss = 0.01679582
Iteration 95, loss = 0.01670618
Iteration 96, loss = 0.01678845
Iteration 97, loss = 0.01662141
Iteration 98, loss = 0.01657665
Iteration 99, loss = 0.01642097
Iteration 100, loss = 0.01649846
Iteration 101, loss = 0.01645212
Iteration 102, loss = 0.01654666
Iteration 103, loss = 0.01642908
Iteration 104, loss = 0.01631907
Iteration 105, loss = 0.01630338
Iteration 106, loss = 0.01631134
Iteration 107, loss = 0.01613704
Iteration 108, loss = 0.01630071
Iteration 109, loss = 0.01610466
Iteration 110, loss = 0.01614836
Iteration 111, loss = 0.01608600
Iteration 112, loss = 0.01606843
Iteration 113, loss = 0.01609928
Iteration 114, loss = 0.01594276
Iteration 115, loss = 0.01597575
Iteration 116, loss = 0.01611276
Iteration 117, loss = 0.01617581
Iteration 118, loss = 0.01592683
Iteration 119, loss = 0.01586298
Iteration 120, loss = 0.01588723
Iteration 121, loss = 0.01588029
Iteration 122, loss = 0.01588720
Iteration 123, loss = 0.01581617
Iteration 124, loss = 0.01575936
Iteration 125, loss = 0.01594151
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69322626
Iteration 2, loss = 0.67849435
Iteration 3, loss = 0.66540671
Iteration 4, loss = 0.65252278
Iteration 5, loss = 0.63881147
Iteration 6, loss = 0.62427449
Iteration 7, loss = 0.60849608
Iteration 8, loss = 0.59127433
Iteration 9, loss = 0.57269156
Iteration 10, loss = 0.55256949
Iteration 11, loss = 0.53154101
Iteration 12, loss = 0.50950681
Iteration 13, loss = 0.48665336
Iteration 14, loss = 0.46343939
Iteration 15, loss = 0.44024158
Iteration 16, loss = 0.41723501
Iteration 17, loss = 0.39474916
Iteration 18, loss = 0.37295492
Iteration 19, loss = 0.35177843
Iteration 20, loss = 0.33146097
Iteration 21, loss = 0.31203679
Iteration 22, loss = 0.29357217
Iteration 23, loss = 0.27595258
Iteration 24, loss = 0.25944221
Iteration 25, loss = 0.24373518
Iteration 26, loss = 0.22897938
Iteration 27, loss = 0.21517808
Iteration 28, loss = 0.20224216
Iteration 29, loss = 0.19013571
Iteration 30, loss = 0.17890796
Iteration 31, loss = 0.16829984
Iteration 32, loss = 0.15846726
Iteration 33, loss = 0.14939566
Iteration 34, loss = 0.14086059
Iteration 35, loss = 0.13297449
Iteration 36, loss = 0.12566589
Iteration 37, loss = 0.11878471
Iteration 38, loss = 0.11246308
Iteration 39, loss = 0.10659103
Iteration 40, loss = 0.10110307
Iteration 41, loss = 0.09598133
Iteration 42, loss = 0.09125173
Iteration 43, loss = 0.08680030
Iteration 44, loss = 0.08265109
Iteration 45, loss = 0.07878881
Iteration 46, loss = 0.07519852
Iteration 47, loss = 0.07179528
Iteration 48, loss = 0.06862929
Iteration 49, loss = 0.06565173
Iteration 50, loss = 0.06287408
Iteration 51, loss = 0.06025637
Iteration 52, loss = 0.05782622
Iteration 53, loss = 0.05549998
Iteration 54, loss = 0.05334661
Iteration 55, loss = 0.05130123
Iteration 56, loss = 0.04938037
Iteration 57, loss = 0.04758886
Iteration 58, loss = 0.04586525
Iteration 59, loss = 0.04426252
Iteration 60, loss = 0.04270926
Iteration 61, loss = 0.04127265
Iteration 62, loss = 0.03990158
Iteration 63, loss = 0.03858104
Iteration 64, loss = 0.03735335
Iteration 65, loss = 0.03617852
Iteration 66, loss = 0.03507070
Iteration 67, loss = 0.03399740
Iteration 68, loss = 0.03300374
Iteration 69, loss = 0.03203717
Iteration 70, loss = 0.03111021
Iteration 71, loss = 0.03023992
Iteration 72, loss = 0.02941008
Iteration 73, loss = 0.02860830
Iteration 74, loss = 0.02783883
Iteration 75, loss = 0.02710538
Iteration 76, loss = 0.02639576
Iteration 77, loss = 0.02572699
Iteration 78, loss = 0.02507856
Iteration 79, loss = 0.02445491
Iteration 80, loss = 0.02384940
Iteration 81, loss = 0.02328959
Iteration 82, loss = 0.02273379
Iteration 83, loss = 0.02221803
Iteration 84, loss = 0.02170184
Iteration 85, loss = 0.02120268
Iteration 86, loss = 0.02073322
Iteration 87, loss = 0.02028269
Iteration 88, loss = 0.01984617
Iteration 89, loss = 0.01941629
Iteration 90, loss = 0.01901577
Iteration 91, loss = 0.01861846
Iteration 92, loss = 0.01824185
Iteration 93, loss = 0.01787546
Iteration 94, loss = 0.01751686
Iteration 95, loss = 0.01717981
Iteration 96, loss = 0.01685044
Iteration 97, loss = 0.01652896
Iteration 98, loss = 0.01622570
Iteration 99, loss = 0.01592579
Iteration 100, loss = 0.01563759
Iteration 101, loss = 0.01535798
Iteration 102, loss = 0.01509045
Iteration 103, loss = 0.01482404
Iteration 104, loss = 0.01456860
Iteration 105, loss = 0.01431927
Iteration 106, loss = 0.01408183
Iteration 107, loss = 0.01384491
Iteration 108, loss = 0.01361696
Iteration 109, loss = 0.01338551
Iteration 110, loss = 0.01318174
Iteration 111, loss = 0.01295905
Iteration 112, loss = 0.01275978
Iteration 113, loss = 0.01256022
Iteration 114, loss = 0.01236488
Iteration 115, loss = 0.01217691
Iteration 116, loss = 0.01199473
Iteration 117, loss = 0.01181710
Iteration 118, loss = 0.01164731
Iteration 119, loss = 0.01147458
Iteration 120, loss = 0.01131799
Iteration 121, loss = 0.01115695
Iteration 122, loss = 0.01099856
Iteration 123, loss = 0.01084938
Iteration 124, loss = 0.01069511
Iteration 125, loss = 0.01055265
Iteration 126, loss = 0.01040994
Iteration 127, loss = 0.01027006
Iteration 128, loss = 0.01013634
Iteration 129, loss = 0.01000378
Iteration 130, loss = 0.00986778
Iteration 131, loss = 0.00974042
Iteration 132, loss = 0.00961725
Iteration 133, loss = 0.00949759
Iteration 134, loss = 0.00938033
Iteration 135, loss = 0.00926514
Iteration 136, loss = 0.00915316
Iteration 137, loss = 0.00904516
Iteration 138, loss = 0.00893803
Iteration 139, loss = 0.00883564
Iteration 140, loss = 0.00873616
Iteration 141, loss = 0.00863584
Iteration 142, loss = 0.00853932
Iteration 143, loss = 0.00844408
Iteration 144, loss = 0.00835398
Iteration 145, loss = 0.00826052
Iteration 146, loss = 0.00817114
Iteration 147, loss = 0.00808417
Iteration 148, loss = 0.00800203
Iteration 149, loss = 0.00791547
Iteration 150, loss = 0.00783309
Iteration 151, loss = 0.00775548
Iteration 152, loss = 0.00767783
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70340791
Iteration 2, loss = 0.67492654
Iteration 3, loss = 0.64207127
Iteration 4, loss = 0.60267724
Iteration 5, loss = 0.55659347
Iteration 6, loss = 0.50751783
Iteration 7, loss = 0.45770154
Iteration 8, loss = 0.40876344
Iteration 9, loss = 0.36206587
Iteration 10, loss = 0.31923255
Iteration 11, loss = 0.28059255
Iteration 12, loss = 0.24673315
Iteration 13, loss = 0.21695138
Iteration 14, loss = 0.19161940
Iteration 15, loss = 0.16956487
Iteration 16, loss = 0.15096121
Iteration 17, loss = 0.13513864
Iteration 18, loss = 0.12155703
Iteration 19, loss = 0.11007373
Iteration 20, loss = 0.10016918
Iteration 21, loss = 0.09150931
Iteration 22, loss = 0.08408277
Iteration 23, loss = 0.07754186
Iteration 24, loss = 0.07197612
Iteration 25, loss = 0.06694915
Iteration 26, loss = 0.06247906
Iteration 27, loss = 0.05859780
Iteration 28, loss = 0.05510504
Iteration 29, loss = 0.05190376
Iteration 30, loss = 0.04918926
Iteration 31, loss = 0.04659725
Iteration 32, loss = 0.04436500
Iteration 33, loss = 0.04218999
Iteration 34, loss = 0.04048881
Iteration 35, loss = 0.03876509
Iteration 36, loss = 0.03710566
Iteration 37, loss = 0.03545419
Iteration 38, loss = 0.03445934
Iteration 39, loss = 0.03313540
Iteration 40, loss = 0.03196016
Iteration 41, loss = 0.03096775
Iteration 42, loss = 0.02992477
Iteration 43, loss = 0.02896318
Iteration 44, loss = 0.02816480
Iteration 45, loss = 0.02730891
Iteration 46, loss = 0.02676635
Iteration 47, loss = 0.02609148
Iteration 48, loss = 0.02535745
Iteration 49, loss = 0.02483487
Iteration 50, loss = 0.02424043
Iteration 51, loss = 0.02370585
Iteration 52, loss = 0.02310760
Iteration 53, loss = 0.02263749
Iteration 54, loss = 0.02216109
Iteration 55, loss = 0.02162365
Iteration 56, loss = 0.02148688
Iteration 57, loss = 0.02096106
Iteration 58, loss = 0.02068164
Iteration 59, loss = 0.02034091
Iteration 60, loss = 0.01996914
Iteration 61, loss = 0.01956276
Iteration 62, loss = 0.01924245
Iteration 63, loss = 0.01908971
Iteration 64, loss = 0.01877273
Iteration 65, loss = 0.01873180
Iteration 66, loss = 0.01840008
Iteration 67, loss = 0.01799780
Iteration 68, loss = 0.01782998
Iteration 69, loss = 0.01763415
Iteration 70, loss = 0.01736963
Iteration 71, loss = 0.01715088
Iteration 72, loss = 0.01691289
Iteration 73, loss = 0.01666684
Iteration 74, loss = 0.01646666
Iteration 75, loss = 0.01626671
Iteration 76, loss = 0.01608602
Iteration 77, loss = 0.01596285
Iteration 78, loss = 0.01579708
Iteration 79, loss = 0.01569725
Iteration 80, loss = 0.01553772
Iteration 81, loss = 0.01541456
Iteration 82, loss = 0.01526763
Iteration 83, loss = 0.01516242
Iteration 84, loss = 0.01503707
Iteration 85, loss = 0.01494825
Iteration 86, loss = 0.01477104
Iteration 87, loss = 0.01459375
Iteration 88, loss = 0.01477183
Iteration 89, loss = 0.01468689
Iteration 90, loss = 0.01445768
Iteration 91, loss = 0.01431839
Iteration 92, loss = 0.01428687
Iteration 93, loss = 0.01426282
Iteration 94, loss = 0.01415025
Iteration 95, loss = 0.01404262
Iteration 96, loss = 0.01392327
Iteration 97, loss = 0.01379753
Iteration 98, loss = 0.01380527
Iteration 99, loss = 0.01363261
Iteration 100, loss = 0.01355656
Iteration 101, loss = 0.01373793
Iteration 102, loss = 0.01363453
Iteration 103, loss = 0.01352780
Iteration 104, loss = 0.01356906
Iteration 105, loss = 0.01345969
Iteration 106, loss = 0.01323258
Iteration 107, loss = 0.01323111
Iteration 108, loss = 0.01311329
Iteration 109, loss = 0.01297083
Iteration 110, loss = 0.01288242
Iteration 111, loss = 0.01323322
Iteration 112, loss = 0.01323658
Iteration 113, loss = 0.01326945
Iteration 114, loss = 0.01291186
Iteration 115, loss = 0.01305370
Iteration 116, loss = 0.01299809
Iteration 117, loss = 0.01297093
Iteration 118, loss = 0.01281545
Iteration 119, loss = 0.01277143
Iteration 120, loss = 0.01269583
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68701451
Iteration 2, loss = 0.64771806
Iteration 3, loss = 0.59445976
Iteration 4, loss = 0.52718633
Iteration 5, loss = 0.45400512
Iteration 6, loss = 0.38456754
Iteration 7, loss = 0.32226695
Iteration 8, loss = 0.26838101
Iteration 9, loss = 0.22322826
Iteration 10, loss = 0.18686064
Iteration 11, loss = 0.15771039
Iteration 12, loss = 0.13423449
Iteration 13, loss = 0.11597394
Iteration 14, loss = 0.10100797
Iteration 15, loss = 0.08901468
Iteration 16, loss = 0.07922525
Iteration 17, loss = 0.07124640
Iteration 18, loss = 0.06466349
Iteration 19, loss = 0.05922137
Iteration 20, loss = 0.05445324
Iteration 21, loss = 0.05024110
Iteration 22, loss = 0.04688031
Iteration 23, loss = 0.04389960
Iteration 24, loss = 0.04131567
Iteration 25, loss = 0.03892112
Iteration 26, loss = 0.03696620
Iteration 27, loss = 0.03507906
Iteration 28, loss = 0.03377003
Iteration 29, loss = 0.03215899
Iteration 30, loss = 0.03098375
Iteration 31, loss = 0.02983804
Iteration 32, loss = 0.02875466
Iteration 33, loss = 0.02778379
Iteration 34, loss = 0.02690054
Iteration 35, loss = 0.02606199
Iteration 36, loss = 0.02541324
Iteration 37, loss = 0.02479807
Iteration 38, loss = 0.02411342
Iteration 39, loss = 0.02350798
Iteration 40, loss = 0.02308540
Iteration 41, loss = 0.02255812
Iteration 42, loss = 0.02209317
Iteration 43, loss = 0.02179345
Iteration 44, loss = 0.02126998
Iteration 45, loss = 0.02092954
Iteration 46, loss = 0.02060612
Iteration 47, loss = 0.02017150
Iteration 48, loss = 0.02008662
Iteration 49, loss = 0.01979120
Iteration 50, loss = 0.01953409
Iteration 51, loss = 0.01925267
Iteration 52, loss = 0.01895850
Iteration 53, loss = 0.01884557
Iteration 54, loss = 0.01853086
Iteration 55, loss = 0.01841754
Iteration 56, loss = 0.01818908
Iteration 57, loss = 0.01810610
Iteration 58, loss = 0.01788108
Iteration 59, loss = 0.01798591
Iteration 60, loss = 0.01741899
Iteration 61, loss = 0.01734762
Iteration 62, loss = 0.01738155
Iteration 63, loss = 0.01703618
Iteration 64, loss = 0.01700108
Iteration 65, loss = 0.01677188
Iteration 66, loss = 0.01674335
Iteration 67, loss = 0.01674848
Iteration 68, loss = 0.01654343
Iteration 69, loss = 0.01644102
Iteration 70, loss = 0.01656115
Iteration 71, loss = 0.01627754
Iteration 72, loss = 0.01632253
Iteration 73, loss = 0.01616717
Iteration 74, loss = 0.01613339
Iteration 75, loss = 0.01595020
Iteration 76, loss = 0.01599223
Iteration 77, loss = 0.01599119
Iteration 78, loss = 0.01577648
Iteration 79, loss = 0.01568372
Iteration 80, loss = 0.01572513
Iteration 81, loss = 0.01579480
Iteration 82, loss = 0.01546691
Iteration 83, loss = 0.01552720
Iteration 84, loss = 0.01546205
Iteration 85, loss = 0.01540799
Iteration 86, loss = 0.01540790
Iteration 87, loss = 0.01532671
Iteration 88, loss = 0.01520548
Iteration 89, loss = 0.01520315
Iteration 90, loss = 0.01517285
Iteration 91, loss = 0.01519738
Iteration 92, loss = 0.01505242
Iteration 93, loss = 0.01521050
Iteration 94, loss = 0.01506422
Iteration 95, loss = 0.01485013
Iteration 96, loss = 0.01509618
Iteration 97, loss = 0.01503954
Iteration 98, loss = 0.01494651
Iteration 99, loss = 0.01472946
Iteration 100, loss = 0.01470645
Iteration 101, loss = 0.01497547
Iteration 102, loss = 0.01478242
Iteration 103, loss = 0.01470617
Iteration 104, loss = 0.01469150
Iteration 105, loss = 0.01477784
Iteration 106, loss = 0.01468657
Iteration 107, loss = 0.01474500
Iteration 108, loss = 0.01461936
Iteration 109, loss = 0.01455588
Iteration 110, loss = 0.01462131
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72098886
Iteration 2, loss = 0.67405756
Iteration 3, loss = 0.61499663
Iteration 4, loss = 0.54199237
Iteration 5, loss = 0.46201059
Iteration 6, loss = 0.38331596
Iteration 7, loss = 0.31329079
Iteration 8, loss = 0.25485158
Iteration 9, loss = 0.20897479
Iteration 10, loss = 0.17287432
Iteration 11, loss = 0.14546360
Iteration 12, loss = 0.12421353
Iteration 13, loss = 0.10786100
Iteration 14, loss = 0.09516815
Iteration 15, loss = 0.08486004
Iteration 16, loss = 0.07675868
Iteration 17, loss = 0.07006522
Iteration 18, loss = 0.06465979
Iteration 19, loss = 0.05991464
Iteration 20, loss = 0.05594954
Iteration 21, loss = 0.05262852
Iteration 22, loss = 0.04994952
Iteration 23, loss = 0.04762067
Iteration 24, loss = 0.04547675
Iteration 25, loss = 0.04348949
Iteration 26, loss = 0.04192891
Iteration 27, loss = 0.04062897
Iteration 28, loss = 0.03934221
Iteration 29, loss = 0.03815280
Iteration 30, loss = 0.03690295
Iteration 31, loss = 0.03600789
Iteration 32, loss = 0.03529149
Iteration 33, loss = 0.03444565
Iteration 34, loss = 0.03373733
Iteration 35, loss = 0.03327737
Iteration 36, loss = 0.03254020
Iteration 37, loss = 0.03197951
Iteration 38, loss = 0.03144161
Iteration 39, loss = 0.03134868
Iteration 40, loss = 0.03075535
Iteration 41, loss = 0.03028814
Iteration 42, loss = 0.02988178
Iteration 43, loss = 0.02994547
Iteration 44, loss = 0.02941931
Iteration 45, loss = 0.02941405
Iteration 46, loss = 0.02908611
Iteration 47, loss = 0.02865635
Iteration 48, loss = 0.02862923
Iteration 49, loss = 0.02819501
Iteration 50, loss = 0.02791530
Iteration 51, loss = 0.02782968
Iteration 52, loss = 0.02765664
Iteration 53, loss = 0.02728219
Iteration 54, loss = 0.02727697
Iteration 55, loss = 0.02714799
Iteration 56, loss = 0.02700402
Iteration 57, loss = 0.02693990
Iteration 58, loss = 0.02670012
Iteration 59, loss = 0.02692133
Iteration 60, loss = 0.02670035
Iteration 61, loss = 0.02646799
Iteration 62, loss = 0.02620901
Iteration 63, loss = 0.02594046
Iteration 64, loss = 0.02597072
Iteration 65, loss = 0.02599004
Iteration 66, loss = 0.02568538
Iteration 67, loss = 0.02608272
Iteration 68, loss = 0.02583305
Iteration 69, loss = 0.02578836
Iteration 70, loss = 0.02543328
Iteration 71, loss = 0.02567639
Iteration 72, loss = 0.02531720
Iteration 73, loss = 0.02520672
Iteration 74, loss = 0.02536856
Iteration 75, loss = 0.02503727
Iteration 76, loss = 0.02523689
Iteration 77, loss = 0.02516386
Iteration 78, loss = 0.02510096
Iteration 79, loss = 0.02504240
Iteration 80, loss = 0.02497242
Iteration 81, loss = 0.02492140
Iteration 82, loss = 0.02489765
Iteration 83, loss = 0.02496648
Iteration 84, loss = 0.02490145
Iteration 85, loss = 0.02487313
Iteration 86, loss = 0.02492000
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70563871
Iteration 2, loss = 0.68402265
Iteration 3, loss = 0.66219274
Iteration 4, loss = 0.63666532
Iteration 5, loss = 0.60719761
Iteration 6, loss = 0.57416557
Iteration 7, loss = 0.53812132
Iteration 8, loss = 0.50123517
Iteration 9, loss = 0.46392956
Iteration 10, loss = 0.42725662
Iteration 11, loss = 0.39181115
Iteration 12, loss = 0.35778613
Iteration 13, loss = 0.32593054
Iteration 14, loss = 0.29653081
Iteration 15, loss = 0.26957538
Iteration 16, loss = 0.24509127
Iteration 17, loss = 0.22298489
Iteration 18, loss = 0.20295293
Iteration 19, loss = 0.18499369
Iteration 20, loss = 0.16914589
Iteration 21, loss = 0.15484548
Iteration 22, loss = 0.14215300
Iteration 23, loss = 0.13084687
Iteration 24, loss = 0.12083483
Iteration 25, loss = 0.11181888
Iteration 26, loss = 0.10372941
Iteration 27, loss = 0.09660155
Iteration 28, loss = 0.09014977
Iteration 29, loss = 0.08444222
Iteration 30, loss = 0.07922283
Iteration 31, loss = 0.07456359
Iteration 32, loss = 0.07034052
Iteration 33, loss = 0.06648989
Iteration 34, loss = 0.06308309
Iteration 35, loss = 0.05985347
Iteration 36, loss = 0.05695110
Iteration 37, loss = 0.05430929
Iteration 38, loss = 0.05184598
Iteration 39, loss = 0.04956692
Iteration 40, loss = 0.04754129
Iteration 41, loss = 0.04560938
Iteration 42, loss = 0.04390113
Iteration 43, loss = 0.04226143
Iteration 44, loss = 0.04072512
Iteration 45, loss = 0.03925212
Iteration 46, loss = 0.03797126
Iteration 47, loss = 0.03679663
Iteration 48, loss = 0.03563215
Iteration 49, loss = 0.03451744
Iteration 50, loss = 0.03355883
Iteration 51, loss = 0.03254875
Iteration 52, loss = 0.03170063
Iteration 53, loss = 0.03086132
Iteration 54, loss = 0.03004661
Iteration 55, loss = 0.02927477
Iteration 56, loss = 0.02861114
Iteration 57, loss = 0.02795156
Iteration 58, loss = 0.02732169
Iteration 59, loss = 0.02674595
Iteration 60, loss = 0.02609884
Iteration 61, loss = 0.02555675
Iteration 62, loss = 0.02507760
Iteration 63, loss = 0.02460773
Iteration 64, loss = 0.02414073
Iteration 65, loss = 0.02371007
Iteration 66, loss = 0.02325588
Iteration 67, loss = 0.02286106
Iteration 68, loss = 0.02244893
Iteration 69, loss = 0.02212304
Iteration 70, loss = 0.02178267
Iteration 71, loss = 0.02140019
Iteration 72, loss = 0.02112340
Iteration 73, loss = 0.02079468
Iteration 74, loss = 0.02044905
Iteration 75, loss = 0.02024401
Iteration 76, loss = 0.01994658
Iteration 77, loss = 0.01973867
Iteration 78, loss = 0.01942257
Iteration 79, loss = 0.01919776
Iteration 80, loss = 0.01894489
Iteration 81, loss = 0.01873346
Iteration 82, loss = 0.01852968
Iteration 83, loss = 0.01828122
Iteration 84, loss = 0.01808435
Iteration 85, loss = 0.01796865
Iteration 86, loss = 0.01774405
Iteration 87, loss = 0.01757046
Iteration 88, loss = 0.01744328
Iteration 89, loss = 0.01723354
Iteration 90, loss = 0.01703841
Iteration 91, loss = 0.01688577
Iteration 92, loss = 0.01673795
Iteration 93, loss = 0.01662046
Iteration 94, loss = 0.01643900
Iteration 95, loss = 0.01629728
Iteration 96, loss = 0.01614402
Iteration 97, loss = 0.01608483
Iteration 98, loss = 0.01593290
Iteration 99, loss = 0.01582745
Iteration 100, loss = 0.01566505
Iteration 101, loss = 0.01557194
Iteration 102, loss = 0.01540951
Iteration 103, loss = 0.01540548
Iteration 104, loss = 0.01524510
Iteration 105, loss = 0.01517177
Iteration 106, loss = 0.01505021
Iteration 107, loss = 0.01498912
Iteration 108, loss = 0.01486856
Iteration 109, loss = 0.01475153
Iteration 110, loss = 0.01473107
Iteration 111, loss = 0.01463978
Iteration 112, loss = 0.01454457
Iteration 113, loss = 0.01446137
Iteration 114, loss = 0.01433910
Iteration 115, loss = 0.01430109
Iteration 116, loss = 0.01422936
Iteration 117, loss = 0.01417500
Iteration 118, loss = 0.01413788
Iteration 119, loss = 0.01397963
Iteration 120, loss = 0.01391528
Iteration 121, loss = 0.01392289
Iteration 122, loss = 0.01381324
Iteration 123, loss = 0.01386681
Iteration 124, loss = 0.01372273
Iteration 125, loss = 0.01367235
Iteration 126, loss = 0.01359213
Iteration 127, loss = 0.01354291
Iteration 128, loss = 0.01341475
Iteration 129, loss = 0.01340422
Iteration 130, loss = 0.01331000
Iteration 131, loss = 0.01329065
Iteration 132, loss = 0.01329329
Iteration 133, loss = 0.01318669
Iteration 134, loss = 0.01312487
Iteration 135, loss = 0.01309978
Iteration 136, loss = 0.01310041
Iteration 137, loss = 0.01303632
Iteration 138, loss = 0.01302834
Iteration 139, loss = 0.01291994
Iteration 140, loss = 0.01287362
Iteration 141, loss = 0.01286978
Iteration 142, loss = 0.01281800
Iteration 143, loss = 0.01274505
Iteration 144, loss = 0.01269685
Iteration 145, loss = 0.01274498
Iteration 146, loss = 0.01262802
Iteration 147, loss = 0.01257400
Iteration 148, loss = 0.01260220
Iteration 149, loss = 0.01268663
Iteration 150, loss = 0.01249191
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70269717
Iteration 2, loss = 0.67458807
Iteration 3, loss = 0.63733707
Iteration 4, loss = 0.59016810
Iteration 5, loss = 0.53389830
Iteration 6, loss = 0.47206947
Iteration 7, loss = 0.40947594
Iteration 8, loss = 0.35108606
Iteration 9, loss = 0.29957666
Iteration 10, loss = 0.25516333
Iteration 11, loss = 0.21784244
Iteration 12, loss = 0.18679023
Iteration 13, loss = 0.16149338
Iteration 14, loss = 0.14053696
Iteration 15, loss = 0.12330618
Iteration 16, loss = 0.10911808
Iteration 17, loss = 0.09752808
Iteration 18, loss = 0.08776741
Iteration 19, loss = 0.07944106
Iteration 20, loss = 0.07247010
Iteration 21, loss = 0.06662373
Iteration 22, loss = 0.06142061
Iteration 23, loss = 0.05693820
Iteration 24, loss = 0.05315689
Iteration 25, loss = 0.04973319
Iteration 26, loss = 0.04682325
Iteration 27, loss = 0.04426802
Iteration 28, loss = 0.04206162
Iteration 29, loss = 0.03974450
Iteration 30, loss = 0.03798403
Iteration 31, loss = 0.03643331
Iteration 32, loss = 0.03477167
Iteration 33, loss = 0.03338422
Iteration 34, loss = 0.03208690
Iteration 35, loss = 0.03107502
Iteration 36, loss = 0.03001242
Iteration 37, loss = 0.02904584
Iteration 38, loss = 0.02827396
Iteration 39, loss = 0.02747837
Iteration 40, loss = 0.02664723
Iteration 41, loss = 0.02591081
Iteration 42, loss = 0.02537232
Iteration 43, loss = 0.02472241
Iteration 44, loss = 0.02423451
Iteration 45, loss = 0.02376342
Iteration 46, loss = 0.02328576
Iteration 47, loss = 0.02282520
Iteration 48, loss = 0.02241928
Iteration 49, loss = 0.02204598
Iteration 50, loss = 0.02169473
Iteration 51, loss = 0.02126305
Iteration 52, loss = 0.02089445
Iteration 53, loss = 0.02086582
Iteration 54, loss = 0.02053480
Iteration 55, loss = 0.02024434
Iteration 56, loss = 0.01990230
Iteration 57, loss = 0.01967687
Iteration 58, loss = 0.01933047
Iteration 59, loss = 0.01922157
Iteration 60, loss = 0.01906306
Iteration 61, loss = 0.01886608
Iteration 62, loss = 0.01869867
Iteration 63, loss = 0.01857636
Iteration 64, loss = 0.01831143
Iteration 65, loss = 0.01813290
Iteration 66, loss = 0.01814111
Iteration 67, loss = 0.01804292
Iteration 68, loss = 0.01767213
Iteration 69, loss = 0.01752802
Iteration 70, loss = 0.01751010
Iteration 71, loss = 0.01732105
Iteration 72, loss = 0.01721317
Iteration 73, loss = 0.01722951
Iteration 74, loss = 0.01700179
Iteration 75, loss = 0.01697577
Iteration 76, loss = 0.01689644
Iteration 77, loss = 0.01694394
Iteration 78, loss = 0.01659289
Iteration 79, loss = 0.01659919
Iteration 80, loss = 0.01648561
Iteration 81, loss = 0.01635048
Iteration 82, loss = 0.01636318
Iteration 83, loss = 0.01623372
Iteration 84, loss = 0.01618423
Iteration 85, loss = 0.01607526
Iteration 86, loss = 0.01602397
Iteration 87, loss = 0.01600125
Iteration 88, loss = 0.01599608
Iteration 89, loss = 0.01586647
Iteration 90, loss = 0.01587671
Iteration 91, loss = 0.01587146
Iteration 92, loss = 0.01580654
Iteration 93, loss = 0.01568887
Iteration 94, loss = 0.01556080
Iteration 95, loss = 0.01549108
Iteration 96, loss = 0.01550282
Iteration 97, loss = 0.01545389
Iteration 98, loss = 0.01534496
Iteration 99, loss = 0.01547377
Iteration 100, loss = 0.01545602
Iteration 101, loss = 0.01534440
Iteration 102, loss = 0.01540631
Iteration 103, loss = 0.01527742
Iteration 104, loss = 0.01527685
Iteration 105, loss = 0.01514346
Iteration 106, loss = 0.01509848
Iteration 107, loss = 0.01517551
Iteration 108, loss = 0.01504218
Iteration 109, loss = 0.01517422
Iteration 110, loss = 0.01503172
Iteration 111, loss = 0.01496873
Iteration 112, loss = 0.01496194
Iteration 113, loss = 0.01498766
Iteration 114, loss = 0.01496094
Iteration 115, loss = 0.01481901
Iteration 116, loss = 0.01479344
Iteration 117, loss = 0.01485617
Iteration 118, loss = 0.01473335
Iteration 119, loss = 0.01496789
Iteration 120, loss = 0.01476714
Iteration 121, loss = 0.01478677
Iteration 122, loss = 0.01469393
Iteration 123, loss = 0.01465108
Iteration 124, loss = 0.01475759
Iteration 125, loss = 0.01468141
Iteration 126, loss = 0.01462782
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69453392
Iteration 2, loss = 0.64553520
Iteration 3, loss = 0.57757334
Iteration 4, loss = 0.48811615
Iteration 5, loss = 0.39846080
Iteration 6, loss = 0.31820562
Iteration 7, loss = 0.25260106
Iteration 8, loss = 0.20141447
Iteration 9, loss = 0.16278831
Iteration 10, loss = 0.13421997
Iteration 11, loss = 0.11243571
Iteration 12, loss = 0.09613637
Iteration 13, loss = 0.08346270
Iteration 14, loss = 0.07364663
Iteration 15, loss = 0.06564964
Iteration 16, loss = 0.05925544
Iteration 17, loss = 0.05425218
Iteration 18, loss = 0.04971870
Iteration 19, loss = 0.04612167
Iteration 20, loss = 0.04304098
Iteration 21, loss = 0.04038244
Iteration 22, loss = 0.03812422
Iteration 23, loss = 0.03635692
Iteration 24, loss = 0.03449541
Iteration 25, loss = 0.03286811
Iteration 26, loss = 0.03166162
Iteration 27, loss = 0.03047130
Iteration 28, loss = 0.02962297
Iteration 29, loss = 0.02864667
Iteration 30, loss = 0.02759054
Iteration 31, loss = 0.02681654
Iteration 32, loss = 0.02614904
Iteration 33, loss = 0.02561013
Iteration 34, loss = 0.02499128
Iteration 35, loss = 0.02451988
Iteration 36, loss = 0.02400391
Iteration 37, loss = 0.02367840
Iteration 38, loss = 0.02303416
Iteration 39, loss = 0.02305594
Iteration 40, loss = 0.02229169
Iteration 41, loss = 0.02218440
Iteration 42, loss = 0.02213311
Iteration 43, loss = 0.02156939
Iteration 44, loss = 0.02127558
Iteration 45, loss = 0.02097866
Iteration 46, loss = 0.02101616
Iteration 47, loss = 0.02048116
Iteration 48, loss = 0.02056618
Iteration 49, loss = 0.02029649
Iteration 50, loss = 0.02018207
Iteration 51, loss = 0.01993516
Iteration 52, loss = 0.01981260
Iteration 53, loss = 0.01976794
Iteration 54, loss = 0.01957119
Iteration 55, loss = 0.01948597
Iteration 56, loss = 0.01930916
Iteration 57, loss = 0.01917755
Iteration 58, loss = 0.01904685
Iteration 59, loss = 0.01906315
Iteration 60, loss = 0.01883000
Iteration 61, loss = 0.01883005
Iteration 62, loss = 0.01861764
Iteration 63, loss = 0.01835788
Iteration 64, loss = 0.01852327
Iteration 65, loss = 0.01825593
Iteration 66, loss = 0.01851474
Iteration 67, loss = 0.01819915
Iteration 68, loss = 0.01824790
Iteration 69, loss = 0.01811128
Iteration 70, loss = 0.01820625
Iteration 71, loss = 0.01797275
Iteration 72, loss = 0.01809650
Iteration 73, loss = 0.01796155
Iteration 74, loss = 0.01801186
Iteration 75, loss = 0.01784152
Iteration 76, loss = 0.01790272
Iteration 77, loss = 0.01769536
Iteration 78, loss = 0.01758938
Iteration 79, loss = 0.01776558
Iteration 80, loss = 0.01768779
Iteration 81, loss = 0.01751307
Iteration 82, loss = 0.01747606
Iteration 83, loss = 0.01748270
Iteration 84, loss = 0.01746256
Iteration 85, loss = 0.01742737
Iteration 86, loss = 0.01740936
Iteration 87, loss = 0.01753622
Iteration 88, loss = 0.01746320
Iteration 89, loss = 0.01746427
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67965280
Iteration 2, loss = 0.62413739
Iteration 3, loss = 0.54461719
Iteration 4, loss = 0.45274608
Iteration 5, loss = 0.36685131
Iteration 6, loss = 0.29296125
Iteration 7, loss = 0.23360150
Iteration 8, loss = 0.18815098
Iteration 9, loss = 0.15382246
Iteration 10, loss = 0.12844439
Iteration 11, loss = 0.10938034
Iteration 12, loss = 0.09475242
Iteration 13, loss = 0.08347693
Iteration 14, loss = 0.07427541
Iteration 15, loss = 0.06718104
Iteration 16, loss = 0.06147386
Iteration 17, loss = 0.05662822
Iteration 18, loss = 0.05260779
Iteration 19, loss = 0.04905454
Iteration 20, loss = 0.04621395
Iteration 21, loss = 0.04391362
Iteration 22, loss = 0.04162876
Iteration 23, loss = 0.03988310
Iteration 24, loss = 0.03833634
Iteration 25, loss = 0.03674601
Iteration 26, loss = 0.03557804
Iteration 27, loss = 0.03459022
Iteration 28, loss = 0.03329043
Iteration 29, loss = 0.03263200
Iteration 30, loss = 0.03164317
Iteration 31, loss = 0.03099089
Iteration 32, loss = 0.03042448
Iteration 33, loss = 0.02974029
Iteration 34, loss = 0.02915098
Iteration 35, loss = 0.02878069
Iteration 36, loss = 0.02852417
Iteration 37, loss = 0.02790174
Iteration 38, loss = 0.02758783
Iteration 39, loss = 0.02708971
Iteration 40, loss = 0.02681027
Iteration 41, loss = 0.02653146
Iteration 42, loss = 0.02619861
Iteration 43, loss = 0.02631605
Iteration 44, loss = 0.02571803
Iteration 45, loss = 0.02542336
Iteration 46, loss = 0.02521725
Iteration 47, loss = 0.02503633
Iteration 48, loss = 0.02480609
Iteration 49, loss = 0.02466288
Iteration 50, loss = 0.02467829
Iteration 51, loss = 0.02449048
Iteration 52, loss = 0.02416138
Iteration 53, loss = 0.02415379
Iteration 54, loss = 0.02404982
Iteration 55, loss = 0.02382551
Iteration 56, loss = 0.02389278
Iteration 57, loss = 0.02396336
Iteration 58, loss = 0.02337431
Iteration 59, loss = 0.02350587
Iteration 60, loss = 0.02323545
Iteration 61, loss = 0.02325251
Iteration 62, loss = 0.02344015
Iteration 63, loss = 0.02322311
Iteration 64, loss = 0.02298448
Iteration 65, loss = 0.02318659
Iteration 66, loss = 0.02292289
Iteration 67, loss = 0.02279975
Iteration 68, loss = 0.02257532
Iteration 69, loss = 0.02274086
Iteration 70, loss = 0.02272169
Iteration 71, loss = 0.02277821
Iteration 72, loss = 0.02282244
Iteration 73, loss = 0.02234205
Iteration 74, loss = 0.02249809
Iteration 75, loss = 0.02242804
Iteration 76, loss = 0.02246751
Iteration 77, loss = 0.02235095
Iteration 78, loss = 0.02243294
Iteration 79, loss = 0.02220642
Iteration 80, loss = 0.02210839
Iteration 81, loss = 0.02212314
Iteration 82, loss = 0.02211645
Iteration 83, loss = 0.02202224
Iteration 84, loss = 0.02209913
Iteration 85, loss = 0.02187769
Iteration 86, loss = 0.02231314
Iteration 87, loss = 0.02201093
Iteration 88, loss = 0.02205401
Iteration 89, loss = 0.02208649
Iteration 90, loss = 0.02185837
Iteration 91, loss = 0.02191927
Iteration 92, loss = 0.02191064
Iteration 93, loss = 0.02170463
Iteration 94, loss = 0.02156979
Iteration 95, loss = 0.02219864
Iteration 96, loss = 0.02160659
Iteration 97, loss = 0.02178687
Iteration 98, loss = 0.02178470
Iteration 99, loss = 0.02160109
Iteration 100, loss = 0.02157967
Iteration 101, loss = 0.02173358
Iteration 102, loss = 0.02160701
Iteration 103, loss = 0.02161904
Iteration 104, loss = 0.02141177
Iteration 105, loss = 0.02144285
Iteration 106, loss = 0.02195639
Iteration 107, loss = 0.02137429
Iteration 108, loss = 0.02138350
Iteration 109, loss = 0.02154605
Iteration 110, loss = 0.02161734
Iteration 111, loss = 0.02138629
Iteration 112, loss = 0.02134787
Iteration 113, loss = 0.02121370
Iteration 114, loss = 0.02144654
Iteration 115, loss = 0.02133793
Iteration 116, loss = 0.02162713
Iteration 117, loss = 0.02137334
Iteration 118, loss = 0.02132172
Iteration 119, loss = 0.02142422
Iteration 120, loss = 0.02132389
Iteration 121, loss = 0.02143661
Iteration 122, loss = 0.02127745
Iteration 123, loss = 0.02125067
Iteration 124, loss = 0.02115854
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71156076
Iteration 2, loss = 0.66910817
Iteration 3, loss = 0.61280093
Iteration 4, loss = 0.53632906
Iteration 5, loss = 0.45283308
Iteration 6, loss = 0.37356554
Iteration 7, loss = 0.30526184
Iteration 8, loss = 0.24941647
Iteration 9, loss = 0.20509543
Iteration 10, loss = 0.17076158
Iteration 11, loss = 0.14406435
Iteration 12, loss = 0.12340913
Iteration 13, loss = 0.10734327
Iteration 14, loss = 0.09475779
Iteration 15, loss = 0.08454992
Iteration 16, loss = 0.07612611
Iteration 17, loss = 0.06956986
Iteration 18, loss = 0.06398290
Iteration 19, loss = 0.05920120
Iteration 20, loss = 0.05521074
Iteration 21, loss = 0.05200557
Iteration 22, loss = 0.04905663
Iteration 23, loss = 0.04671530
Iteration 24, loss = 0.04466935
Iteration 25, loss = 0.04239890
Iteration 26, loss = 0.04063736
Iteration 27, loss = 0.03919892
Iteration 28, loss = 0.03781120
Iteration 29, loss = 0.03655653
Iteration 30, loss = 0.03560874
Iteration 31, loss = 0.03456482
Iteration 32, loss = 0.03398614
Iteration 33, loss = 0.03302146
Iteration 34, loss = 0.03211003
Iteration 35, loss = 0.03167162
Iteration 36, loss = 0.03096476
Iteration 37, loss = 0.03039321
Iteration 38, loss = 0.02992785
Iteration 39, loss = 0.02922274
Iteration 40, loss = 0.02891560
Iteration 41, loss = 0.02851337
Iteration 42, loss = 0.02809509
Iteration 43, loss = 0.02779085
Iteration 44, loss = 0.02763226
Iteration 45, loss = 0.02716932
Iteration 46, loss = 0.02692756
Iteration 47, loss = 0.02670471
Iteration 48, loss = 0.02632277
Iteration 49, loss = 0.02613422
Iteration 50, loss = 0.02581427
Iteration 51, loss = 0.02562758
Iteration 52, loss = 0.02548763
Iteration 53, loss = 0.02542214
Iteration 54, loss = 0.02534891
Iteration 55, loss = 0.02498236
Iteration 56, loss = 0.02513564
Iteration 57, loss = 0.02494035
Iteration 58, loss = 0.02459932
Iteration 59, loss = 0.02448467
Iteration 60, loss = 0.02453331
Iteration 61, loss = 0.02428108
Iteration 62, loss = 0.02396375
Iteration 63, loss = 0.02406784
Iteration 64, loss = 0.02374964
Iteration 65, loss = 0.02391496
Iteration 66, loss = 0.02351479
Iteration 67, loss = 0.02352871
Iteration 68, loss = 0.02365142
Iteration 69, loss = 0.02356940
Iteration 70, loss = 0.02351661
Iteration 71, loss = 0.02329888
Iteration 72, loss = 0.02324750
Iteration 73, loss = 0.02299433
Iteration 74, loss = 0.02301703
Iteration 75, loss = 0.02284069
Iteration 76, loss = 0.02313506
Iteration 77, loss = 0.02274412
Iteration 78, loss = 0.02275260
Iteration 79, loss = 0.02282707
Iteration 80, loss = 0.02256383
Iteration 81, loss = 0.02260667
Iteration 82, loss = 0.02273271
Iteration 83, loss = 0.02244305
Iteration 84, loss = 0.02252193
Iteration 85, loss = 0.02259487
Iteration 86, loss = 0.02250331
Iteration 87, loss = 0.02235469
Iteration 88, loss = 0.02262644
Iteration 89, loss = 0.02224028
Iteration 90, loss = 0.02215135
Iteration 91, loss = 0.02218998
Iteration 92, loss = 0.02211062
Iteration 93, loss = 0.02203860
Iteration 94, loss = 0.02216109
Iteration 95, loss = 0.02217352
Iteration 96, loss = 0.02195939
Iteration 97, loss = 0.02204860
Iteration 98, loss = 0.02201133
Iteration 99, loss = 0.02192476
Iteration 100, loss = 0.02183135
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69037192
Iteration 2, loss = 0.62663534
Iteration 3, loss = 0.53503333
Iteration 4, loss = 0.43249904
Iteration 5, loss = 0.33950289
Iteration 6, loss = 0.26346592
Iteration 7, loss = 0.20669304
Iteration 8, loss = 0.16468234
Iteration 9, loss = 0.13444224
Iteration 10, loss = 0.11218110
Iteration 11, loss = 0.09596433
Iteration 12, loss = 0.08352994
Iteration 13, loss = 0.07417431
Iteration 14, loss = 0.06654100
Iteration 15, loss = 0.06067767
Iteration 16, loss = 0.05597281
Iteration 17, loss = 0.05186332
Iteration 18, loss = 0.04848103
Iteration 19, loss = 0.04577773
Iteration 20, loss = 0.04336195
Iteration 21, loss = 0.04141648
Iteration 22, loss = 0.03949856
Iteration 23, loss = 0.03820266
Iteration 24, loss = 0.03690564
Iteration 25, loss = 0.03607998
Iteration 26, loss = 0.03497899
Iteration 27, loss = 0.03376175
Iteration 28, loss = 0.03255958
Iteration 29, loss = 0.03189741
Iteration 30, loss = 0.03127422
Iteration 31, loss = 0.03054672
Iteration 32, loss = 0.03010388
Iteration 33, loss = 0.02981231
Iteration 34, loss = 0.02950374
Iteration 35, loss = 0.02930579
Iteration 36, loss = 0.02935454
Iteration 37, loss = 0.02862287
Iteration 38, loss = 0.02832627
Iteration 39, loss = 0.02764940
Iteration 40, loss = 0.02696161
Iteration 41, loss = 0.02664553
Iteration 42, loss = 0.02684871
Iteration 43, loss = 0.02708292
Iteration 44, loss = 0.02675660
Iteration 45, loss = 0.02655220
Iteration 46, loss = 0.02567392
Iteration 47, loss = 0.02564838
Iteration 48, loss = 0.02543656
Iteration 49, loss = 0.02522296
Iteration 50, loss = 0.02499946
Iteration 51, loss = 0.02482769
Iteration 52, loss = 0.02470144
Iteration 53, loss = 0.02448051
Iteration 54, loss = 0.02440440
Iteration 55, loss = 0.02439147
Iteration 56, loss = 0.02412403
Iteration 57, loss = 0.02394500
Iteration 58, loss = 0.02374871
Iteration 59, loss = 0.02377557
Iteration 60, loss = 0.02363174
Iteration 61, loss = 0.02376715
Iteration 62, loss = 0.02431225
Iteration 63, loss = 0.02412191
Iteration 64, loss = 0.02445392
Iteration 65, loss = 0.02400275
Iteration 66, loss = 0.02413876
Iteration 67, loss = 0.02372317
Iteration 68, loss = 0.02348385
Iteration 69, loss = 0.02356386
Iteration 70, loss = 0.02343710
Iteration 71, loss = 0.02346326
Iteration 72, loss = 0.02372129
Iteration 73, loss = 0.02408293
Iteration 74, loss = 0.02372865
Iteration 75, loss = 0.02384350
Iteration 76, loss = 0.02356016
Iteration 77, loss = 0.02308938
Iteration 78, loss = 0.02288093
Iteration 79, loss = 0.02266340
Iteration 80, loss = 0.02251947
Iteration 81, loss = 0.02257671
Iteration 82, loss = 0.02285889
Iteration 83, loss = 0.02280208
Iteration 84, loss = 0.02287147
Iteration 85, loss = 0.02253186
Iteration 86, loss = 0.02241882
Iteration 87, loss = 0.02241268
Iteration 88, loss = 0.02229768
Iteration 89, loss = 0.02320363
Iteration 90, loss = 0.02324829
Iteration 91, loss = 0.02261331
Iteration 92, loss = 0.02324602
Iteration 93, loss = 0.02320661
Iteration 94, loss = 0.02285196
Iteration 95, loss = 0.02269942
Iteration 96, loss = 0.02262035
Iteration 97, loss = 0.02236200
Iteration 98, loss = 0.02277644
Iteration 99, loss = 0.02315295
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70891112
Iteration 2, loss = 0.69051932
Iteration 3, loss = 0.67472036
Iteration 4, loss = 0.65911869
Iteration 5, loss = 0.64275445
Iteration 6, loss = 0.62593711
Iteration 7, loss = 0.60773639
Iteration 8, loss = 0.58879992
Iteration 9, loss = 0.56907156
Iteration 10, loss = 0.54838116
Iteration 11, loss = 0.52703861
Iteration 12, loss = 0.50533429
Iteration 13, loss = 0.48321686
Iteration 14, loss = 0.46078049
Iteration 15, loss = 0.43842578
Iteration 16, loss = 0.41628423
Iteration 17, loss = 0.39440723
Iteration 18, loss = 0.37293528
Iteration 19, loss = 0.35195202
Iteration 20, loss = 0.33173218
Iteration 21, loss = 0.31224404
Iteration 22, loss = 0.29367983
Iteration 23, loss = 0.27597450
Iteration 24, loss = 0.25921351
Iteration 25, loss = 0.24339728
Iteration 26, loss = 0.22847053
Iteration 27, loss = 0.21447570
Iteration 28, loss = 0.20142934
Iteration 29, loss = 0.18919613
Iteration 30, loss = 0.17783704
Iteration 31, loss = 0.16719790
Iteration 32, loss = 0.15738458
Iteration 33, loss = 0.14826971
Iteration 34, loss = 0.13973558
Iteration 35, loss = 0.13191953
Iteration 36, loss = 0.12456759
Iteration 37, loss = 0.11782238
Iteration 38, loss = 0.11158023
Iteration 39, loss = 0.10576665
Iteration 40, loss = 0.10040544
Iteration 41, loss = 0.09538067
Iteration 42, loss = 0.09075127
Iteration 43, loss = 0.08644332
Iteration 44, loss = 0.08241110
Iteration 45, loss = 0.07867344
Iteration 46, loss = 0.07519148
Iteration 47, loss = 0.07194704
Iteration 48, loss = 0.06886367
Iteration 49, loss = 0.06601270
Iteration 50, loss = 0.06339095
Iteration 51, loss = 0.06084528
Iteration 52, loss = 0.05853584
Iteration 53, loss = 0.05631569
Iteration 54, loss = 0.05426344
Iteration 55, loss = 0.05235657
Iteration 56, loss = 0.05047602
Iteration 57, loss = 0.04874118
Iteration 58, loss = 0.04714345
Iteration 59, loss = 0.04561113
Iteration 60, loss = 0.04415906
Iteration 61, loss = 0.04280445
Iteration 62, loss = 0.04146925
Iteration 63, loss = 0.04024437
Iteration 64, loss = 0.03902682
Iteration 65, loss = 0.03799289
Iteration 66, loss = 0.03685063
Iteration 67, loss = 0.03588407
Iteration 68, loss = 0.03483373
Iteration 69, loss = 0.03394427
Iteration 70, loss = 0.03303107
Iteration 71, loss = 0.03222985
Iteration 72, loss = 0.03137400
Iteration 73, loss = 0.03062053
Iteration 74, loss = 0.02987789
Iteration 75, loss = 0.02920606
Iteration 76, loss = 0.02853427
Iteration 77, loss = 0.02794337
Iteration 78, loss = 0.02731583
Iteration 79, loss = 0.02667405
Iteration 80, loss = 0.02610515
Iteration 81, loss = 0.02555732
Iteration 82, loss = 0.02503688
Iteration 83, loss = 0.02451747
Iteration 84, loss = 0.02401894
Iteration 85, loss = 0.02355250
Iteration 86, loss = 0.02310380
Iteration 87, loss = 0.02266790
Iteration 88, loss = 0.02227626
Iteration 89, loss = 0.02186532
Iteration 90, loss = 0.02147183
Iteration 91, loss = 0.02107869
Iteration 92, loss = 0.02071269
Iteration 93, loss = 0.02035671
Iteration 94, loss = 0.02001820
Iteration 95, loss = 0.01969237
Iteration 96, loss = 0.01938856
Iteration 97, loss = 0.01906325
Iteration 98, loss = 0.01879092
Iteration 99, loss = 0.01848403
Iteration 100, loss = 0.01821754
Iteration 101, loss = 0.01791354
Iteration 102, loss = 0.01767014
Iteration 103, loss = 0.01742948
Iteration 104, loss = 0.01726442
Iteration 105, loss = 0.01695821
Iteration 106, loss = 0.01674919
Iteration 107, loss = 0.01648685
Iteration 108, loss = 0.01626114
Iteration 109, loss = 0.01605245
Iteration 110, loss = 0.01583538
Iteration 111, loss = 0.01566602
Iteration 112, loss = 0.01543363
Iteration 113, loss = 0.01524953
Iteration 114, loss = 0.01510577
Iteration 115, loss = 0.01492612
Iteration 116, loss = 0.01469440
Iteration 117, loss = 0.01450456
Iteration 118, loss = 0.01435041
Iteration 119, loss = 0.01420226
Iteration 120, loss = 0.01405801
Iteration 121, loss = 0.01390775
Iteration 122, loss = 0.01377422
Iteration 123, loss = 0.01363850
Iteration 124, loss = 0.01354815
Iteration 125, loss = 0.01334511
Iteration 126, loss = 0.01322274
Iteration 127, loss = 0.01305361
Iteration 128, loss = 0.01294282
Iteration 129, loss = 0.01279370
Iteration 130, loss = 0.01265848
Iteration 131, loss = 0.01256187
Iteration 132, loss = 0.01243777
Iteration 133, loss = 0.01229420
Iteration 134, loss = 0.01219319
Iteration 135, loss = 0.01208234
Iteration 136, loss = 0.01197944
Iteration 137, loss = 0.01187592
Iteration 138, loss = 0.01177360
Iteration 139, loss = 0.01168981
Iteration 140, loss = 0.01158690
Iteration 141, loss = 0.01147755
Iteration 142, loss = 0.01138764
Iteration 143, loss = 0.01131023
Iteration 144, loss = 0.01123170
Iteration 145, loss = 0.01112699
Iteration 146, loss = 0.01103927
Iteration 147, loss = 0.01094790
Iteration 148, loss = 0.01086004
Iteration 149, loss = 0.01082026
Iteration 150, loss = 0.01069140
Iteration 151, loss = 0.01064895
Iteration 152, loss = 0.01057137
Iteration 153, loss = 0.01051176
Iteration 154, loss = 0.01040363
Iteration 155, loss = 0.01030350
Iteration 156, loss = 0.01023757
Iteration 157, loss = 0.01016525
Iteration 158, loss = 0.01010322
Iteration 159, loss = 0.01003027
Iteration 160, loss = 0.00997029
Iteration 161, loss = 0.00995484
Iteration 162, loss = 0.00989196
Iteration 163, loss = 0.00978685
Iteration 164, loss = 0.00972040
Iteration 165, loss = 0.00965715
Iteration 166, loss = 0.00962093
Iteration 167, loss = 0.00954733
Iteration 168, loss = 0.00948226
Iteration 169, loss = 0.00943537
Iteration 170, loss = 0.00936970
Iteration 171, loss = 0.00932907
Iteration 172, loss = 0.00928727
Iteration 173, loss = 0.00918514
Iteration 174, loss = 0.00917053
Iteration 175, loss = 0.00918365
Iteration 176, loss = 0.00917880
Iteration 177, loss = 0.00906865
Iteration 178, loss = 0.00903187
Iteration 179, loss = 0.00896574
Iteration 180, loss = 0.00890452
Iteration 181, loss = 0.00882668
Iteration 182, loss = 0.00882173
Iteration 183, loss = 0.00873191
Iteration 184, loss = 0.00868968
Iteration 185, loss = 0.00866137
Iteration 186, loss = 0.00860835
Iteration 187, loss = 0.00861626
Iteration 188, loss = 0.00854943
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69269341
Iteration 2, loss = 0.67665805
Iteration 3, loss = 0.66258122
Iteration 4, loss = 0.64865087
Iteration 5, loss = 0.63420773
Iteration 6, loss = 0.61913500
Iteration 7, loss = 0.60292704
Iteration 8, loss = 0.58549371
Iteration 9, loss = 0.56672297
Iteration 10, loss = 0.54671130
Iteration 11, loss = 0.52530197
Iteration 12, loss = 0.50263258
Iteration 13, loss = 0.47885147
Iteration 14, loss = 0.45438608
Iteration 15, loss = 0.42958259
Iteration 16, loss = 0.40468580
Iteration 17, loss = 0.38011001
Iteration 18, loss = 0.35633082
Iteration 19, loss = 0.33353663
Iteration 20, loss = 0.31183217
Iteration 21, loss = 0.29118342
Iteration 22, loss = 0.27183548
Iteration 23, loss = 0.25371219
Iteration 24, loss = 0.23672906
Iteration 25, loss = 0.22094477
Iteration 26, loss = 0.20627837
Iteration 27, loss = 0.19269181
Iteration 28, loss = 0.18013514
Iteration 29, loss = 0.16855433
Iteration 30, loss = 0.15788225
Iteration 31, loss = 0.14801787
Iteration 32, loss = 0.13898878
Iteration 33, loss = 0.13062366
Iteration 34, loss = 0.12290874
Iteration 35, loss = 0.11581526
Iteration 36, loss = 0.10930790
Iteration 37, loss = 0.10323673
Iteration 38, loss = 0.09769557
Iteration 39, loss = 0.09256582
Iteration 40, loss = 0.08780035
Iteration 41, loss = 0.08343208
Iteration 42, loss = 0.07936665
Iteration 43, loss = 0.07559177
Iteration 44, loss = 0.07211551
Iteration 45, loss = 0.06882501
Iteration 46, loss = 0.06581060
Iteration 47, loss = 0.06299647
Iteration 48, loss = 0.06036178
Iteration 49, loss = 0.05791877
Iteration 50, loss = 0.05562007
Iteration 51, loss = 0.05346353
Iteration 52, loss = 0.05143336
Iteration 53, loss = 0.04956592
Iteration 54, loss = 0.04776913
Iteration 55, loss = 0.04610197
Iteration 56, loss = 0.04451580
Iteration 57, loss = 0.04303055
Iteration 58, loss = 0.04164285
Iteration 59, loss = 0.04031540
Iteration 60, loss = 0.03905896
Iteration 61, loss = 0.03787242
Iteration 62, loss = 0.03674548
Iteration 63, loss = 0.03566611
Iteration 64, loss = 0.03465255
Iteration 65, loss = 0.03367033
Iteration 66, loss = 0.03274775
Iteration 67, loss = 0.03187081
Iteration 68, loss = 0.03102375
Iteration 69, loss = 0.03021362
Iteration 70, loss = 0.02946160
Iteration 71, loss = 0.02872404
Iteration 72, loss = 0.02805373
Iteration 73, loss = 0.02738379
Iteration 74, loss = 0.02674978
Iteration 75, loss = 0.02615978
Iteration 76, loss = 0.02557226
Iteration 77, loss = 0.02501680
Iteration 78, loss = 0.02447736
Iteration 79, loss = 0.02396993
Iteration 80, loss = 0.02346303
Iteration 81, loss = 0.02299019
Iteration 82, loss = 0.02253602
Iteration 83, loss = 0.02209650
Iteration 84, loss = 0.02167695
Iteration 85, loss = 0.02127568
Iteration 86, loss = 0.02088746
Iteration 87, loss = 0.02049210
Iteration 88, loss = 0.02013339
Iteration 89, loss = 0.01977691
Iteration 90, loss = 0.01943324
Iteration 91, loss = 0.01910917
Iteration 92, loss = 0.01878532
Iteration 93, loss = 0.01847969
Iteration 94, loss = 0.01818145
Iteration 95, loss = 0.01791040
Iteration 96, loss = 0.01761933
Iteration 97, loss = 0.01736101
Iteration 98, loss = 0.01709634
Iteration 99, loss = 0.01684491
Iteration 100, loss = 0.01660394
Iteration 101, loss = 0.01636601
Iteration 102, loss = 0.01614076
Iteration 103, loss = 0.01591618
Iteration 104, loss = 0.01570099
Iteration 105, loss = 0.01549294
Iteration 106, loss = 0.01529507
Iteration 107, loss = 0.01509029
Iteration 108, loss = 0.01489185
Iteration 109, loss = 0.01471531
Iteration 110, loss = 0.01454003
Iteration 111, loss = 0.01437043
Iteration 112, loss = 0.01419347
Iteration 113, loss = 0.01402630
Iteration 114, loss = 0.01387592
Iteration 115, loss = 0.01370438
Iteration 116, loss = 0.01356192
Iteration 117, loss = 0.01340463
Iteration 118, loss = 0.01325289
Iteration 119, loss = 0.01310983
Iteration 120, loss = 0.01296923
Iteration 121, loss = 0.01282220
Iteration 122, loss = 0.01268801
Iteration 123, loss = 0.01257484
Iteration 124, loss = 0.01244275
Iteration 125, loss = 0.01231347
Iteration 126, loss = 0.01219040
Iteration 127, loss = 0.01207257
Iteration 128, loss = 0.01196232
Iteration 129, loss = 0.01184586
Iteration 130, loss = 0.01173227
Iteration 131, loss = 0.01163204
Iteration 132, loss = 0.01151910
Iteration 133, loss = 0.01142317
Iteration 134, loss = 0.01131793
Iteration 135, loss = 0.01122105
Iteration 136, loss = 0.01112929
Iteration 137, loss = 0.01103366
Iteration 138, loss = 0.01094447
Iteration 139, loss = 0.01085132
Iteration 140, loss = 0.01077180
Iteration 141, loss = 0.01068152
Iteration 142, loss = 0.01059713
Iteration 143, loss = 0.01051068
Iteration 144, loss = 0.01043022
Iteration 145, loss = 0.01035266
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69703828
Iteration 2, loss = 0.67746473
Iteration 3, loss = 0.65725008
Iteration 4, loss = 0.63359760
Iteration 5, loss = 0.60722359
Iteration 6, loss = 0.57754103
Iteration 7, loss = 0.54534245
Iteration 8, loss = 0.51130590
Iteration 9, loss = 0.47660622
Iteration 10, loss = 0.44212652
Iteration 11, loss = 0.40830778
Iteration 12, loss = 0.37559788
Iteration 13, loss = 0.34436292
Iteration 14, loss = 0.31485031
Iteration 15, loss = 0.28735595
Iteration 16, loss = 0.26191699
Iteration 17, loss = 0.23837504
Iteration 18, loss = 0.21711972
Iteration 19, loss = 0.19774459
Iteration 20, loss = 0.18033457
Iteration 21, loss = 0.16465455
Iteration 22, loss = 0.15057132
Iteration 23, loss = 0.13817513
Iteration 24, loss = 0.12697328
Iteration 25, loss = 0.11705759
Iteration 26, loss = 0.10818264
Iteration 27, loss = 0.10025516
Iteration 28, loss = 0.09320765
Iteration 29, loss = 0.08690203
Iteration 30, loss = 0.08114610
Iteration 31, loss = 0.07599163
Iteration 32, loss = 0.07144086
Iteration 33, loss = 0.06724237
Iteration 34, loss = 0.06344981
Iteration 35, loss = 0.05999295
Iteration 36, loss = 0.05686360
Iteration 37, loss = 0.05402436
Iteration 38, loss = 0.05140607
Iteration 39, loss = 0.04908099
Iteration 40, loss = 0.04680670
Iteration 41, loss = 0.04476676
Iteration 42, loss = 0.04294967
Iteration 43, loss = 0.04115719
Iteration 44, loss = 0.03957702
Iteration 45, loss = 0.03805585
Iteration 46, loss = 0.03669911
Iteration 47, loss = 0.03536489
Iteration 48, loss = 0.03417364
Iteration 49, loss = 0.03305068
Iteration 50, loss = 0.03200008
Iteration 51, loss = 0.03097012
Iteration 52, loss = 0.03003548
Iteration 53, loss = 0.02918713
Iteration 54, loss = 0.02839104
Iteration 55, loss = 0.02757183
Iteration 56, loss = 0.02685113
Iteration 57, loss = 0.02616704
Iteration 58, loss = 0.02552082
Iteration 59, loss = 0.02488589
Iteration 60, loss = 0.02429969
Iteration 61, loss = 0.02376903
Iteration 62, loss = 0.02324330
Iteration 63, loss = 0.02273592
Iteration 64, loss = 0.02223004
Iteration 65, loss = 0.02178138
Iteration 66, loss = 0.02135784
Iteration 67, loss = 0.02092825
Iteration 68, loss = 0.02058059
Iteration 69, loss = 0.02019048
Iteration 70, loss = 0.01979641
Iteration 71, loss = 0.01947814
Iteration 72, loss = 0.01915734
Iteration 73, loss = 0.01881358
Iteration 74, loss = 0.01852332
Iteration 75, loss = 0.01825657
Iteration 76, loss = 0.01796569
Iteration 77, loss = 0.01767149
Iteration 78, loss = 0.01744446
Iteration 79, loss = 0.01721766
Iteration 80, loss = 0.01694696
Iteration 81, loss = 0.01672929
Iteration 82, loss = 0.01651020
Iteration 83, loss = 0.01630462
Iteration 84, loss = 0.01611385
Iteration 85, loss = 0.01590499
Iteration 86, loss = 0.01572309
Iteration 87, loss = 0.01557542
Iteration 88, loss = 0.01536903
Iteration 89, loss = 0.01520199
Iteration 90, loss = 0.01504627
Iteration 91, loss = 0.01490455
Iteration 92, loss = 0.01471549
Iteration 93, loss = 0.01456830
Iteration 94, loss = 0.01445221
Iteration 95, loss = 0.01430863
Iteration 96, loss = 0.01418086
Iteration 97, loss = 0.01404006
Iteration 98, loss = 0.01395444
Iteration 99, loss = 0.01383250
Iteration 100, loss = 0.01371798
Iteration 101, loss = 0.01356070
Iteration 102, loss = 0.01345579
Iteration 103, loss = 0.01337329
Iteration 104, loss = 0.01327111
Iteration 105, loss = 0.01313142
Iteration 106, loss = 0.01307959
Iteration 107, loss = 0.01298928
Iteration 108, loss = 0.01284481
Iteration 109, loss = 0.01278072
Iteration 110, loss = 0.01274117
Iteration 111, loss = 0.01260254
Iteration 112, loss = 0.01256872
Iteration 113, loss = 0.01243815
Iteration 114, loss = 0.01239141
Iteration 115, loss = 0.01231135
Iteration 116, loss = 0.01222143
Iteration 117, loss = 0.01217887
Iteration 118, loss = 0.01209775
Iteration 119, loss = 0.01199202
Iteration 120, loss = 0.01197199
Iteration 121, loss = 0.01188712
Iteration 122, loss = 0.01184758
Iteration 123, loss = 0.01172581
Iteration 124, loss = 0.01171865
Iteration 125, loss = 0.01167793
Iteration 126, loss = 0.01160370
Iteration 127, loss = 0.01156060
Iteration 128, loss = 0.01149594
Iteration 129, loss = 0.01143002
Iteration 130, loss = 0.01134606
Iteration 131, loss = 0.01132004
Iteration 132, loss = 0.01125763
Iteration 133, loss = 0.01120871
Iteration 134, loss = 0.01116624
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70339572
Iteration 2, loss = 0.65815621
Iteration 3, loss = 0.59218779
Iteration 4, loss = 0.51196047
Iteration 5, loss = 0.43055273
Iteration 6, loss = 0.35566201
Iteration 7, loss = 0.29114847
Iteration 8, loss = 0.23834261
Iteration 9, loss = 0.19634943
Iteration 10, loss = 0.16334844
Iteration 11, loss = 0.13800598
Iteration 12, loss = 0.11819281
Iteration 13, loss = 0.10291324
Iteration 14, loss = 0.09078524
Iteration 15, loss = 0.08129088
Iteration 16, loss = 0.07343490
Iteration 17, loss = 0.06709170
Iteration 18, loss = 0.06188453
Iteration 19, loss = 0.05743136
Iteration 20, loss = 0.05372720
Iteration 21, loss = 0.05063915
Iteration 22, loss = 0.04794708
Iteration 23, loss = 0.04543986
Iteration 24, loss = 0.04344230
Iteration 25, loss = 0.04161236
Iteration 26, loss = 0.04011680
Iteration 27, loss = 0.03865214
Iteration 28, loss = 0.03746029
Iteration 29, loss = 0.03633739
Iteration 30, loss = 0.03539286
Iteration 31, loss = 0.03449504
Iteration 32, loss = 0.03368799
Iteration 33, loss = 0.03300372
Iteration 34, loss = 0.03215650
Iteration 35, loss = 0.03158054
Iteration 36, loss = 0.03110214
Iteration 37, loss = 0.03051669
Iteration 38, loss = 0.03020402
Iteration 39, loss = 0.02960890
Iteration 40, loss = 0.02938569
Iteration 41, loss = 0.02885893
Iteration 42, loss = 0.02874677
Iteration 43, loss = 0.02814171
Iteration 44, loss = 0.02786026
Iteration 45, loss = 0.02791584
Iteration 46, loss = 0.02749091
Iteration 47, loss = 0.02726778
Iteration 48, loss = 0.02710113
Iteration 49, loss = 0.02682350
Iteration 50, loss = 0.02663532
Iteration 51, loss = 0.02647328
Iteration 52, loss = 0.02620826
Iteration 53, loss = 0.02613804
Iteration 54, loss = 0.02593306
Iteration 55, loss = 0.02572693
Iteration 56, loss = 0.02563850
Iteration 57, loss = 0.02563131
Iteration 58, loss = 0.02549565
Iteration 59, loss = 0.02520387
Iteration 60, loss = 0.02533673
Iteration 61, loss = 0.02529294
Iteration 62, loss = 0.02491869
Iteration 63, loss = 0.02498001
Iteration 64, loss = 0.02493864
Iteration 65, loss = 0.02457457
Iteration 66, loss = 0.02489939
Iteration 67, loss = 0.02469782
Iteration 68, loss = 0.02446640
Iteration 69, loss = 0.02433273
Iteration 70, loss = 0.02432104
Iteration 71, loss = 0.02445693
Iteration 72, loss = 0.02464824
Iteration 73, loss = 0.02421137
Iteration 74, loss = 0.02399693
Iteration 75, loss = 0.02405137
Iteration 76, loss = 0.02406098
Iteration 77, loss = 0.02409253
Iteration 78, loss = 0.02385092
Iteration 79, loss = 0.02395854
Iteration 80, loss = 0.02351747
Iteration 81, loss = 0.02402315
Iteration 82, loss = 0.02377760
Iteration 83, loss = 0.02367168
Iteration 84, loss = 0.02353510
Iteration 85, loss = 0.02356803
Iteration 86, loss = 0.02362721
Iteration 87, loss = 0.02353409
Iteration 88, loss = 0.02347491
Iteration 89, loss = 0.02363327
Iteration 90, loss = 0.02368654
Iteration 91, loss = 0.02350862
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68098865
Iteration 2, loss = 0.62639674
Iteration 3, loss = 0.54334109
Iteration 4, loss = 0.44839700
Iteration 5, loss = 0.35863567
Iteration 6, loss = 0.28284827
Iteration 7, loss = 0.22308897
Iteration 8, loss = 0.17870748
Iteration 9, loss = 0.14557322
Iteration 10, loss = 0.12195976
Iteration 11, loss = 0.10399395
Iteration 12, loss = 0.09076360
Iteration 13, loss = 0.08012345
Iteration 14, loss = 0.07189661
Iteration 15, loss = 0.06537465
Iteration 16, loss = 0.06018505
Iteration 17, loss = 0.05554476
Iteration 18, loss = 0.05246812
Iteration 19, loss = 0.04954495
Iteration 20, loss = 0.04668524
Iteration 21, loss = 0.04441168
Iteration 22, loss = 0.04234785
Iteration 23, loss = 0.04068395
Iteration 24, loss = 0.03981903
Iteration 25, loss = 0.03851874
Iteration 26, loss = 0.03758895
Iteration 27, loss = 0.03641684
Iteration 28, loss = 0.03525339
Iteration 29, loss = 0.03439266
Iteration 30, loss = 0.03363624
Iteration 31, loss = 0.03293975
Iteration 32, loss = 0.03243670
Iteration 33, loss = 0.03211433
Iteration 34, loss = 0.03181797
Iteration 35, loss = 0.03139916
Iteration 36, loss = 0.03073856
Iteration 37, loss = 0.03041231
Iteration 38, loss = 0.03034585
Iteration 39, loss = 0.02979125
Iteration 40, loss = 0.02935975
Iteration 41, loss = 0.02894229
Iteration 42, loss = 0.02890216
Iteration 43, loss = 0.02856120
Iteration 44, loss = 0.02829302
Iteration 45, loss = 0.02824020
Iteration 46, loss = 0.02814848
Iteration 47, loss = 0.02780988
Iteration 48, loss = 0.02737467
Iteration 49, loss = 0.02736374
Iteration 50, loss = 0.02725201
Iteration 51, loss = 0.02818947
Iteration 52, loss = 0.02807893
Iteration 53, loss = 0.02720543
Iteration 54, loss = 0.02704958
Iteration 55, loss = 0.02666894
Iteration 56, loss = 0.02686871
Iteration 57, loss = 0.02706846
Iteration 58, loss = 0.02666700
Iteration 59, loss = 0.02651685
Iteration 60, loss = 0.02651263
Iteration 61, loss = 0.02628144
Iteration 62, loss = 0.02704686
Iteration 63, loss = 0.02710087
Iteration 64, loss = 0.02701779
Iteration 65, loss = 0.02665435
Iteration 66, loss = 0.02645369
Iteration 67, loss = 0.02641656
Iteration 68, loss = 0.02642231
Iteration 69, loss = 0.02605651
Iteration 70, loss = 0.02623615
Iteration 71, loss = 0.02642388
Iteration 72, loss = 0.02625742
Iteration 73, loss = 0.02617416
Iteration 74, loss = 0.02556755
Iteration 75, loss = 0.02552656
Iteration 76, loss = 0.02531813
Iteration 77, loss = 0.02681074
Iteration 78, loss = 0.02765245
Iteration 79, loss = 0.02584890
Iteration 80, loss = 0.02592501
Iteration 81, loss = 0.02549142
Iteration 82, loss = 0.02529240
Iteration 83, loss = 0.02512698
Iteration 84, loss = 0.02503882
Iteration 85, loss = 0.02514026
Iteration 86, loss = 0.02520270
Iteration 87, loss = 0.02492583
Iteration 88, loss = 0.02509848
Iteration 89, loss = 0.02472360
Iteration 90, loss = 0.02517207
Iteration 91, loss = 0.02492700
Iteration 92, loss = 0.02461425
Iteration 93, loss = 0.02467964
Iteration 94, loss = 0.02458711
Iteration 95, loss = 0.02468329
Iteration 96, loss = 0.02471091
Iteration 97, loss = 0.02499345
Iteration 98, loss = 0.02510985
Iteration 99, loss = 0.02497718
Iteration 100, loss = 0.02496478
Iteration 101, loss = 0.02492784
Iteration 102, loss = 0.02535873
Iteration 103, loss = 0.02539446
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68036133
Iteration 2, loss = 0.65270539
Iteration 3, loss = 0.61583598
Iteration 4, loss = 0.56688726
Iteration 5, loss = 0.50942178
Iteration 6, loss = 0.44946193
Iteration 7, loss = 0.39081887
Iteration 8, loss = 0.33655340
Iteration 9, loss = 0.28844204
Iteration 10, loss = 0.24645035
Iteration 11, loss = 0.21108011
Iteration 12, loss = 0.18151825
Iteration 13, loss = 0.15718544
Iteration 14, loss = 0.13715722
Iteration 15, loss = 0.12073977
Iteration 16, loss = 0.10709111
Iteration 17, loss = 0.09578023
Iteration 18, loss = 0.08633086
Iteration 19, loss = 0.07837732
Iteration 20, loss = 0.07173494
Iteration 21, loss = 0.06607851
Iteration 22, loss = 0.06094960
Iteration 23, loss = 0.05689719
Iteration 24, loss = 0.05300625
Iteration 25, loss = 0.04975172
Iteration 26, loss = 0.04689948
Iteration 27, loss = 0.04419932
Iteration 28, loss = 0.04207419
Iteration 29, loss = 0.04013799
Iteration 30, loss = 0.03812473
Iteration 31, loss = 0.03648241
Iteration 32, loss = 0.03507728
Iteration 33, loss = 0.03367832
Iteration 34, loss = 0.03259540
Iteration 35, loss = 0.03148283
Iteration 36, loss = 0.03046217
Iteration 37, loss = 0.02949278
Iteration 38, loss = 0.02862885
Iteration 39, loss = 0.02770375
Iteration 40, loss = 0.02707009
Iteration 41, loss = 0.02653186
Iteration 42, loss = 0.02594181
Iteration 43, loss = 0.02526342
Iteration 44, loss = 0.02460532
Iteration 45, loss = 0.02412521
Iteration 46, loss = 0.02378127
Iteration 47, loss = 0.02330851
Iteration 48, loss = 0.02282397
Iteration 49, loss = 0.02244294
Iteration 50, loss = 0.02208859
Iteration 51, loss = 0.02174108
Iteration 52, loss = 0.02139853
Iteration 53, loss = 0.02121089
Iteration 54, loss = 0.02077702
Iteration 55, loss = 0.02068462
Iteration 56, loss = 0.02035276
Iteration 57, loss = 0.02009739
Iteration 58, loss = 0.01974567
Iteration 59, loss = 0.01959633
Iteration 60, loss = 0.01955192
Iteration 61, loss = 0.01936886
Iteration 62, loss = 0.01906139
Iteration 63, loss = 0.01884295
Iteration 64, loss = 0.01870333
Iteration 65, loss = 0.01856654
Iteration 66, loss = 0.01838591
Iteration 67, loss = 0.01817267
Iteration 68, loss = 0.01814479
Iteration 69, loss = 0.01801235
Iteration 70, loss = 0.01791527
Iteration 71, loss = 0.01776668
Iteration 72, loss = 0.01766319
Iteration 73, loss = 0.01758882
Iteration 74, loss = 0.01749068
Iteration 75, loss = 0.01750726
Iteration 76, loss = 0.01725571
Iteration 77, loss = 0.01724424
Iteration 78, loss = 0.01724952
Iteration 79, loss = 0.01694027
Iteration 80, loss = 0.01689181
Iteration 81, loss = 0.01692153
Iteration 82, loss = 0.01683583
Iteration 83, loss = 0.01675957
Iteration 84, loss = 0.01662820
Iteration 85, loss = 0.01661380
Iteration 86, loss = 0.01644491
Iteration 87, loss = 0.01647913
Iteration 88, loss = 0.01644242
Iteration 89, loss = 0.01627589
Iteration 90, loss = 0.01617721
Iteration 91, loss = 0.01615126
Iteration 92, loss = 0.01611782
Iteration 93, loss = 0.01607909
Iteration 94, loss = 0.01608258
Iteration 95, loss = 0.01597325
Iteration 96, loss = 0.01611863
Iteration 97, loss = 0.01594999
Iteration 98, loss = 0.01585281
Iteration 99, loss = 0.01597930
Iteration 100, loss = 0.01579460
Iteration 101, loss = 0.01595881
Iteration 102, loss = 0.01571754
Iteration 103, loss = 0.01566930
Iteration 104, loss = 0.01569377
Iteration 105, loss = 0.01560413
Iteration 106, loss = 0.01569401
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72381803
Iteration 2, loss = 0.69312948
Iteration 3, loss = 0.65278611
Iteration 4, loss = 0.60346369
Iteration 5, loss = 0.54591827
Iteration 6, loss = 0.48335075
Iteration 7, loss = 0.42040038
Iteration 8, loss = 0.36156490
Iteration 9, loss = 0.30878191
Iteration 10, loss = 0.26279264
Iteration 11, loss = 0.22416874
Iteration 12, loss = 0.19200495
Iteration 13, loss = 0.16532625
Iteration 14, loss = 0.14361899
Iteration 15, loss = 0.12582068
Iteration 16, loss = 0.11119313
Iteration 17, loss = 0.09914458
Iteration 18, loss = 0.08895776
Iteration 19, loss = 0.08062109
Iteration 20, loss = 0.07333897
Iteration 21, loss = 0.06742101
Iteration 22, loss = 0.06221578
Iteration 23, loss = 0.05766052
Iteration 24, loss = 0.05371935
Iteration 25, loss = 0.05042348
Iteration 26, loss = 0.04743114
Iteration 27, loss = 0.04478461
Iteration 28, loss = 0.04243842
Iteration 29, loss = 0.04035483
Iteration 30, loss = 0.03855354
Iteration 31, loss = 0.03681679
Iteration 32, loss = 0.03533992
Iteration 33, loss = 0.03384147
Iteration 34, loss = 0.03279182
Iteration 35, loss = 0.03160174
Iteration 36, loss = 0.03059054
Iteration 37, loss = 0.02959953
Iteration 38, loss = 0.02880229
Iteration 39, loss = 0.02816799
Iteration 40, loss = 0.02722391
Iteration 41, loss = 0.02649305
Iteration 42, loss = 0.02584287
Iteration 43, loss = 0.02527918
Iteration 44, loss = 0.02485970
Iteration 45, loss = 0.02418996
Iteration 46, loss = 0.02373139
Iteration 47, loss = 0.02325815
Iteration 48, loss = 0.02293342
Iteration 49, loss = 0.02257018
Iteration 50, loss = 0.02215480
Iteration 51, loss = 0.02179941
Iteration 52, loss = 0.02160192
Iteration 53, loss = 0.02122834
Iteration 54, loss = 0.02082663
Iteration 55, loss = 0.02052774
Iteration 56, loss = 0.02031857
Iteration 57, loss = 0.02022301
Iteration 58, loss = 0.01994650
Iteration 59, loss = 0.01973974
Iteration 60, loss = 0.01942925
Iteration 61, loss = 0.01933804
Iteration 62, loss = 0.01927827
Iteration 63, loss = 0.01897632
Iteration 64, loss = 0.01885871
Iteration 65, loss = 0.01868268
Iteration 66, loss = 0.01844359
Iteration 67, loss = 0.01831189
Iteration 68, loss = 0.01822519
Iteration 69, loss = 0.01830249
Iteration 70, loss = 0.01793601
Iteration 71, loss = 0.01784118
Iteration 72, loss = 0.01773759
Iteration 73, loss = 0.01754318
Iteration 74, loss = 0.01746453
Iteration 75, loss = 0.01757986
Iteration 76, loss = 0.01737618
Iteration 77, loss = 0.01712727
Iteration 78, loss = 0.01704342
Iteration 79, loss = 0.01711871
Iteration 80, loss = 0.01688709
Iteration 81, loss = 0.01679774
Iteration 82, loss = 0.01682107
Iteration 83, loss = 0.01669879
Iteration 84, loss = 0.01660600
Iteration 85, loss = 0.01659731
Iteration 86, loss = 0.01643140
Iteration 87, loss = 0.01640109
Iteration 88, loss = 0.01628420
Iteration 89, loss = 0.01625986
Iteration 90, loss = 0.01620672
Iteration 91, loss = 0.01611013
Iteration 92, loss = 0.01621874
Iteration 93, loss = 0.01598783
Iteration 94, loss = 0.01610789
Iteration 95, loss = 0.01600900
Iteration 96, loss = 0.01601441
Iteration 97, loss = 0.01598612
Iteration 98, loss = 0.01584815
Iteration 99, loss = 0.01580146
Iteration 100, loss = 0.01570851
Iteration 101, loss = 0.01578203
Iteration 102, loss = 0.01564298
Iteration 103, loss = 0.01580562
Iteration 104, loss = 0.01565646
Iteration 105, loss = 0.01554054
Iteration 106, loss = 0.01558275
Iteration 107, loss = 0.01549994
Iteration 108, loss = 0.01555068
Iteration 109, loss = 0.01547916
Iteration 110, loss = 0.01544768
Iteration 111, loss = 0.01546351
Iteration 112, loss = 0.01543100
Iteration 113, loss = 0.01541036
Iteration 114, loss = 0.01530292
Iteration 115, loss = 0.01539242
Iteration 116, loss = 0.01520039
Iteration 117, loss = 0.01537015
Iteration 118, loss = 0.01515836
Iteration 119, loss = 0.01531737
Iteration 120, loss = 0.01513194
Iteration 121, loss = 0.01516749
Iteration 122, loss = 0.01522909
Iteration 123, loss = 0.01510579
Iteration 124, loss = 0.01525022
Iteration 125, loss = 0.01511838
Iteration 126, loss = 0.01509795
Iteration 127, loss = 0.01494118
Iteration 128, loss = 0.01500816
Iteration 129, loss = 0.01507401
Iteration 130, loss = 0.01491538
Iteration 131, loss = 0.01497884
Iteration 132, loss = 0.01490762
Iteration 133, loss = 0.01507759
Iteration 134, loss = 0.01507250
Iteration 135, loss = 0.01487534
Iteration 136, loss = 0.01495488
Iteration 137, loss = 0.01489748
Iteration 138, loss = 0.01492755
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70349725
Iteration 2, loss = 0.66801461
Iteration 3, loss = 0.63107731
Iteration 4, loss = 0.58739759
Iteration 5, loss = 0.53739407
Iteration 6, loss = 0.48427308
Iteration 7, loss = 0.43081006
Iteration 8, loss = 0.38080355
Iteration 9, loss = 0.33460372
Iteration 10, loss = 0.29362074
Iteration 11, loss = 0.25720163
Iteration 12, loss = 0.22574932
Iteration 13, loss = 0.19907181
Iteration 14, loss = 0.17607962
Iteration 15, loss = 0.15668802
Iteration 16, loss = 0.14008831
Iteration 17, loss = 0.12615044
Iteration 18, loss = 0.11408769
Iteration 19, loss = 0.10376550
Iteration 20, loss = 0.09483160
Iteration 21, loss = 0.08721144
Iteration 22, loss = 0.08048923
Iteration 23, loss = 0.07470308
Iteration 24, loss = 0.06957717
Iteration 25, loss = 0.06493876
Iteration 26, loss = 0.06094789
Iteration 27, loss = 0.05740007
Iteration 28, loss = 0.05441120
Iteration 29, loss = 0.05147816
Iteration 30, loss = 0.04891192
Iteration 31, loss = 0.04664970
Iteration 32, loss = 0.04437267
Iteration 33, loss = 0.04252921
Iteration 34, loss = 0.04078084
Iteration 35, loss = 0.03917408
Iteration 36, loss = 0.03761854
Iteration 37, loss = 0.03623368
Iteration 38, loss = 0.03491873
Iteration 39, loss = 0.03372857
Iteration 40, loss = 0.03267784
Iteration 41, loss = 0.03168162
Iteration 42, loss = 0.03071024
Iteration 43, loss = 0.03000083
Iteration 44, loss = 0.02928715
Iteration 45, loss = 0.02856183
Iteration 46, loss = 0.02774322
Iteration 47, loss = 0.02711668
Iteration 48, loss = 0.02654483
Iteration 49, loss = 0.02591319
Iteration 50, loss = 0.02533756
Iteration 51, loss = 0.02490580
Iteration 52, loss = 0.02430428
Iteration 53, loss = 0.02385416
Iteration 54, loss = 0.02345060
Iteration 55, loss = 0.02290097
Iteration 56, loss = 0.02278381
Iteration 57, loss = 0.02254851
Iteration 58, loss = 0.02219765
Iteration 59, loss = 0.02178511
Iteration 60, loss = 0.02146604
Iteration 61, loss = 0.02104722
Iteration 62, loss = 0.02075601
Iteration 63, loss = 0.02048792
Iteration 64, loss = 0.02021462
Iteration 65, loss = 0.01996906
Iteration 66, loss = 0.01991532
Iteration 67, loss = 0.01984109
Iteration 68, loss = 0.01979235
Iteration 69, loss = 0.01939075
Iteration 70, loss = 0.01908724
Iteration 71, loss = 0.01909842
Iteration 72, loss = 0.01886627
Iteration 73, loss = 0.01835702
Iteration 74, loss = 0.01828203
Iteration 75, loss = 0.01820247
Iteration 76, loss = 0.01810825
Iteration 77, loss = 0.01780899
Iteration 78, loss = 0.01763195
Iteration 79, loss = 0.01747007
Iteration 80, loss = 0.01738815
Iteration 81, loss = 0.01730193
Iteration 82, loss = 0.01721848
Iteration 83, loss = 0.01721599
Iteration 84, loss = 0.01703267
Iteration 85, loss = 0.01693522
Iteration 86, loss = 0.01672946
Iteration 87, loss = 0.01663190
Iteration 88, loss = 0.01651786
Iteration 89, loss = 0.01639428
Iteration 90, loss = 0.01631244
Iteration 91, loss = 0.01618311
Iteration 92, loss = 0.01609762
Iteration 93, loss = 0.01614541
Iteration 94, loss = 0.01594817
Iteration 95, loss = 0.01629842
Iteration 96, loss = 0.01621739
Iteration 97, loss = 0.01620621
Iteration 98, loss = 0.01598095
Iteration 99, loss = 0.01572765
Iteration 100, loss = 0.01557769
Iteration 101, loss = 0.01550875
Iteration 102, loss = 0.01543079
Iteration 103, loss = 0.01547015
Iteration 104, loss = 0.01523040
Iteration 105, loss = 0.01528074
Iteration 106, loss = 0.01524339
Iteration 107, loss = 0.01510879
Iteration 108, loss = 0.01537774
Iteration 109, loss = 0.01508493
Iteration 110, loss = 0.01526999
Iteration 111, loss = 0.01540148
Iteration 112, loss = 0.01513941
Iteration 113, loss = 0.01504550
Iteration 114, loss = 0.01496369
Iteration 115, loss = 0.01481696
Iteration 116, loss = 0.01456014
Iteration 117, loss = 0.01468183
Iteration 118, loss = 0.01479199
Iteration 119, loss = 0.01472858
Iteration 120, loss = 0.01460736
Iteration 121, loss = 0.01450911
Iteration 122, loss = 0.01454776
Iteration 123, loss = 0.01439819
Iteration 124, loss = 0.01460273
Iteration 125, loss = 0.01471016
Iteration 126, loss = 0.01455045
Iteration 127, loss = 0.01444591
Iteration 128, loss = 0.01436514
Iteration 129, loss = 0.01437191
Iteration 130, loss = 0.01465350
Iteration 131, loss = 0.01487278
Iteration 132, loss = 0.01479596
Iteration 133, loss = 0.01451375
Iteration 134, loss = 0.01417584
Iteration 135, loss = 0.01419786
Iteration 136, loss = 0.01409558
Iteration 137, loss = 0.01410880
Iteration 138, loss = 0.01408159
Iteration 139, loss = 0.01413660
Iteration 140, loss = 0.01412016
Iteration 141, loss = 0.01400520
Iteration 142, loss = 0.01394520
Iteration 143, loss = 0.01404918
Iteration 144, loss = 0.01406515
Iteration 145, loss = 0.01403449
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69817321
Iteration 2, loss = 0.65439851
Iteration 3, loss = 0.59080471
Iteration 4, loss = 0.50447932
Iteration 5, loss = 0.41652969
Iteration 6, loss = 0.33574872
Iteration 7, loss = 0.26876370
Iteration 8, loss = 0.21537165
Iteration 9, loss = 0.17450617
Iteration 10, loss = 0.14387581
Iteration 11, loss = 0.12087544
Iteration 12, loss = 0.10354513
Iteration 13, loss = 0.09043138
Iteration 14, loss = 0.08000186
Iteration 15, loss = 0.07200130
Iteration 16, loss = 0.06520323
Iteration 17, loss = 0.06001040
Iteration 18, loss = 0.05549864
Iteration 19, loss = 0.05185038
Iteration 20, loss = 0.04864857
Iteration 21, loss = 0.04609582
Iteration 22, loss = 0.04387110
Iteration 23, loss = 0.04182180
Iteration 24, loss = 0.04008887
Iteration 25, loss = 0.03847012
Iteration 26, loss = 0.03719083
Iteration 27, loss = 0.03605643
Iteration 28, loss = 0.03481960
Iteration 29, loss = 0.03411790
Iteration 30, loss = 0.03318570
Iteration 31, loss = 0.03245500
Iteration 32, loss = 0.03168188
Iteration 33, loss = 0.03098770
Iteration 34, loss = 0.03049799
Iteration 35, loss = 0.03003000
Iteration 36, loss = 0.02955264
Iteration 37, loss = 0.02932816
Iteration 38, loss = 0.02858620
Iteration 39, loss = 0.02835205
Iteration 40, loss = 0.02787908
Iteration 41, loss = 0.02773360
Iteration 42, loss = 0.02737936
Iteration 43, loss = 0.02704541
Iteration 44, loss = 0.02692445
Iteration 45, loss = 0.02663455
Iteration 46, loss = 0.02641933
Iteration 47, loss = 0.02606485
Iteration 48, loss = 0.02579910
Iteration 49, loss = 0.02571551
Iteration 50, loss = 0.02568861
Iteration 51, loss = 0.02551194
Iteration 52, loss = 0.02521056
Iteration 53, loss = 0.02505369
Iteration 54, loss = 0.02499780
Iteration 55, loss = 0.02490710
Iteration 56, loss = 0.02466516
Iteration 57, loss = 0.02463302
Iteration 58, loss = 0.02455235
Iteration 59, loss = 0.02449453
Iteration 60, loss = 0.02431270
Iteration 61, loss = 0.02428661
Iteration 62, loss = 0.02412197
Iteration 63, loss = 0.02399701
Iteration 64, loss = 0.02395319
Iteration 65, loss = 0.02384041
Iteration 66, loss = 0.02370377
Iteration 67, loss = 0.02364043
Iteration 68, loss = 0.02374318
Iteration 69, loss = 0.02355407
Iteration 70, loss = 0.02345775
Iteration 71, loss = 0.02328225
Iteration 72, loss = 0.02325217
Iteration 73, loss = 0.02326070
Iteration 74, loss = 0.02337745
Iteration 75, loss = 0.02337028
Iteration 76, loss = 0.02315744
Iteration 77, loss = 0.02323546
Iteration 78, loss = 0.02311730
Iteration 79, loss = 0.02292401
Iteration 80, loss = 0.02314897
Iteration 81, loss = 0.02285768
Iteration 82, loss = 0.02295952
Iteration 83, loss = 0.02286312
Iteration 84, loss = 0.02313095
Iteration 85, loss = 0.02288270
Iteration 86, loss = 0.02260450
Iteration 87, loss = 0.02272082
Iteration 88, loss = 0.02273214
Iteration 89, loss = 0.02255255
Iteration 90, loss = 0.02276779
Iteration 91, loss = 0.02267891
Iteration 92, loss = 0.02265789
Iteration 93, loss = 0.02267760
Iteration 94, loss = 0.02277023
Iteration 95, loss = 0.02282498
Iteration 96, loss = 0.02265346
Iteration 97, loss = 0.02240650
Iteration 98, loss = 0.02260960
Iteration 99, loss = 0.02262258
Iteration 100, loss = 0.02242858
Iteration 101, loss = 0.02252066
Iteration 102, loss = 0.02241923
Iteration 103, loss = 0.02245513
Iteration 104, loss = 0.02223153
Iteration 105, loss = 0.02237137
Iteration 106, loss = 0.02233717
Iteration 107, loss = 0.02233787
Iteration 108, loss = 0.02232778
Iteration 109, loss = 0.02226304
Iteration 110, loss = 0.02222442
Iteration 111, loss = 0.02226423
Iteration 112, loss = 0.02214796
Iteration 113, loss = 0.02240369
Iteration 114, loss = 0.02217367
Iteration 115, loss = 0.02237064
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71502488
Iteration 2, loss = 0.69375068
Iteration 3, loss = 0.67573747
Iteration 4, loss = 0.65791921
Iteration 5, loss = 0.63922770
Iteration 6, loss = 0.61989610
Iteration 7, loss = 0.59937690
Iteration 8, loss = 0.57773060
Iteration 9, loss = 0.55532428
Iteration 10, loss = 0.53223613
Iteration 11, loss = 0.50857471
Iteration 12, loss = 0.48397963
Iteration 13, loss = 0.45969631
Iteration 14, loss = 0.43563256
Iteration 15, loss = 0.41158039
Iteration 16, loss = 0.38791136
Iteration 17, loss = 0.36522981
Iteration 18, loss = 0.34320346
Iteration 19, loss = 0.32219864
Iteration 20, loss = 0.30216638
Iteration 21, loss = 0.28323836
Iteration 22, loss = 0.26533277
Iteration 23, loss = 0.24855963
Iteration 24, loss = 0.23284831
Iteration 25, loss = 0.21817415
Iteration 26, loss = 0.20447023
Iteration 27, loss = 0.19174690
Iteration 28, loss = 0.17998412
Iteration 29, loss = 0.16896018
Iteration 30, loss = 0.15881874
Iteration 31, loss = 0.14934360
Iteration 32, loss = 0.14066000
Iteration 33, loss = 0.13256881
Iteration 34, loss = 0.12508775
Iteration 35, loss = 0.11815791
Iteration 36, loss = 0.11177383
Iteration 37, loss = 0.10585011
Iteration 38, loss = 0.10032828
Iteration 39, loss = 0.09528193
Iteration 40, loss = 0.09054729
Iteration 41, loss = 0.08616030
Iteration 42, loss = 0.08212303
Iteration 43, loss = 0.07828570
Iteration 44, loss = 0.07478052
Iteration 45, loss = 0.07155545
Iteration 46, loss = 0.06854373
Iteration 47, loss = 0.06568068
Iteration 48, loss = 0.06302567
Iteration 49, loss = 0.06052501
Iteration 50, loss = 0.05819375
Iteration 51, loss = 0.05612248
Iteration 52, loss = 0.05396452
Iteration 53, loss = 0.05204154
Iteration 54, loss = 0.05018654
Iteration 55, loss = 0.04843618
Iteration 56, loss = 0.04676033
Iteration 57, loss = 0.04524825
Iteration 58, loss = 0.04386208
Iteration 59, loss = 0.04244132
Iteration 60, loss = 0.04119597
Iteration 61, loss = 0.03993132
Iteration 62, loss = 0.03881114
Iteration 63, loss = 0.03771246
Iteration 64, loss = 0.03664616
Iteration 65, loss = 0.03565475
Iteration 66, loss = 0.03471911
Iteration 67, loss = 0.03384515
Iteration 68, loss = 0.03295874
Iteration 69, loss = 0.03215081
Iteration 70, loss = 0.03136998
Iteration 71, loss = 0.03061846
Iteration 72, loss = 0.02992674
Iteration 73, loss = 0.02926753
Iteration 74, loss = 0.02856457
Iteration 75, loss = 0.02794538
Iteration 76, loss = 0.02735060
Iteration 77, loss = 0.02679051
Iteration 78, loss = 0.02622317
Iteration 79, loss = 0.02569930
Iteration 80, loss = 0.02521078
Iteration 81, loss = 0.02475199
Iteration 82, loss = 0.02428694
Iteration 83, loss = 0.02381333
Iteration 84, loss = 0.02336219
Iteration 85, loss = 0.02297192
Iteration 86, loss = 0.02254008
Iteration 87, loss = 0.02216279
Iteration 88, loss = 0.02179641
Iteration 89, loss = 0.02142370
Iteration 90, loss = 0.02110929
Iteration 91, loss = 0.02074076
Iteration 92, loss = 0.02047519
Iteration 93, loss = 0.02015822
Iteration 94, loss = 0.01981765
Iteration 95, loss = 0.01950015
Iteration 96, loss = 0.01920032
Iteration 97, loss = 0.01896282
Iteration 98, loss = 0.01870392
Iteration 99, loss = 0.01856403
Iteration 100, loss = 0.01828792
Iteration 101, loss = 0.01808711
Iteration 102, loss = 0.01787477
Iteration 103, loss = 0.01765995
Iteration 104, loss = 0.01742445
Iteration 105, loss = 0.01721484
Iteration 106, loss = 0.01703870
Iteration 107, loss = 0.01671980
Iteration 108, loss = 0.01649234
Iteration 109, loss = 0.01630840
Iteration 110, loss = 0.01611268
Iteration 111, loss = 0.01591702
Iteration 112, loss = 0.01575155
Iteration 113, loss = 0.01561223
Iteration 114, loss = 0.01549679
Iteration 115, loss = 0.01536770
Iteration 116, loss = 0.01528686
Iteration 117, loss = 0.01512119
Iteration 118, loss = 0.01489304
Iteration 119, loss = 0.01472444
Iteration 120, loss = 0.01461317
Iteration 121, loss = 0.01444474
Iteration 122, loss = 0.01435262
Iteration 123, loss = 0.01417554
Iteration 124, loss = 0.01404373
Iteration 125, loss = 0.01392867
Iteration 126, loss = 0.01393690
Iteration 127, loss = 0.01373884
Iteration 128, loss = 0.01363740
Iteration 129, loss = 0.01353962
Iteration 130, loss = 0.01338064
Iteration 131, loss = 0.01328045
Iteration 132, loss = 0.01323623
Iteration 133, loss = 0.01312867
Iteration 134, loss = 0.01302616
Iteration 135, loss = 0.01292016
Iteration 136, loss = 0.01286310
Iteration 137, loss = 0.01272998
Iteration 138, loss = 0.01258395
Iteration 139, loss = 0.01253092
Iteration 140, loss = 0.01243904
Iteration 141, loss = 0.01234269
Iteration 142, loss = 0.01233371
Iteration 143, loss = 0.01217952
Iteration 144, loss = 0.01214518
Iteration 145, loss = 0.01202162
Iteration 146, loss = 0.01197248
Iteration 147, loss = 0.01191037
Iteration 148, loss = 0.01181681
Iteration 149, loss = 0.01172628
Iteration 150, loss = 0.01167668
Iteration 151, loss = 0.01160144
Iteration 152, loss = 0.01152161
Iteration 153, loss = 0.01147349
Iteration 154, loss = 0.01141138
Iteration 155, loss = 0.01131809
Iteration 156, loss = 0.01125310
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67374278
Iteration 2, loss = 0.60047300
Iteration 3, loss = 0.49586736
Iteration 4, loss = 0.38604173
Iteration 5, loss = 0.29354286
Iteration 6, loss = 0.22344585
Iteration 7, loss = 0.17313811
Iteration 8, loss = 0.13765708
Iteration 9, loss = 0.11258706
Iteration 10, loss = 0.09465157
Iteration 11, loss = 0.08132313
Iteration 12, loss = 0.07118541
Iteration 13, loss = 0.06344511
Iteration 14, loss = 0.05726340
Iteration 15, loss = 0.05257569
Iteration 16, loss = 0.04810398
Iteration 17, loss = 0.04498829
Iteration 18, loss = 0.04232283
Iteration 19, loss = 0.03979824
Iteration 20, loss = 0.03776653
Iteration 21, loss = 0.03629055
Iteration 22, loss = 0.03464778
Iteration 23, loss = 0.03357868
Iteration 24, loss = 0.03262428
Iteration 25, loss = 0.03140738
Iteration 26, loss = 0.03066795
Iteration 27, loss = 0.02965689
Iteration 28, loss = 0.02907747
Iteration 29, loss = 0.02854050
Iteration 30, loss = 0.02794457
Iteration 31, loss = 0.02757045
Iteration 32, loss = 0.02701259
Iteration 33, loss = 0.02649197
Iteration 34, loss = 0.02623533
Iteration 35, loss = 0.02624965
Iteration 36, loss = 0.02572255
Iteration 37, loss = 0.02526729
Iteration 38, loss = 0.02488293
Iteration 39, loss = 0.02479244
Iteration 40, loss = 0.02456347
Iteration 41, loss = 0.02437041
Iteration 42, loss = 0.02435543
Iteration 43, loss = 0.02398105
Iteration 44, loss = 0.02373244
Iteration 45, loss = 0.02374077
Iteration 46, loss = 0.02337743
Iteration 47, loss = 0.02330056
Iteration 48, loss = 0.02315304
Iteration 49, loss = 0.02305683
Iteration 50, loss = 0.02349466
Iteration 51, loss = 0.02283746
Iteration 52, loss = 0.02279412
Iteration 53, loss = 0.02277105
Iteration 54, loss = 0.02251154
Iteration 55, loss = 0.02263889
Iteration 56, loss = 0.02258725
Iteration 57, loss = 0.02222734
Iteration 58, loss = 0.02216454
Iteration 59, loss = 0.02233671
Iteration 60, loss = 0.02225530
Iteration 61, loss = 0.02233062
Iteration 62, loss = 0.02224000
Iteration 63, loss = 0.02229634
Iteration 64, loss = 0.02194565
Iteration 65, loss = 0.02216131
Iteration 66, loss = 0.02212869
Iteration 67, loss = 0.02171618
Iteration 68, loss = 0.02194978
Iteration 69, loss = 0.02173990
Iteration 70, loss = 0.02174696
Iteration 71, loss = 0.02189397
Iteration 72, loss = 0.02180872
Iteration 73, loss = 0.02142643
Iteration 74, loss = 0.02218376
Iteration 75, loss = 0.02150048
Iteration 76, loss = 0.02177077
Iteration 77, loss = 0.02184766
Iteration 78, loss = 0.02156486
Iteration 79, loss = 0.02148359
Iteration 80, loss = 0.02144179
Iteration 81, loss = 0.02168077
Iteration 82, loss = 0.02128851
Iteration 83, loss = 0.02131216
Iteration 84, loss = 0.02150064
Iteration 85, loss = 0.02164516
Iteration 86, loss = 0.02130910
Iteration 87, loss = 0.02142840
Iteration 88, loss = 0.02112751
Iteration 89, loss = 0.02135624
Iteration 90, loss = 0.02126039
Iteration 91, loss = 0.02120648
Iteration 92, loss = 0.02124718
Iteration 93, loss = 0.02122578
Iteration 94, loss = 0.02111451
Iteration 95, loss = 0.02140924
Iteration 96, loss = 0.02124384
Iteration 97, loss = 0.02125665
Iteration 98, loss = 0.02148860
Iteration 99, loss = 0.02130363
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68243781
Iteration 2, loss = 0.62720714
Iteration 3, loss = 0.53341474
Iteration 4, loss = 0.42553714
Iteration 5, loss = 0.32593706
Iteration 6, loss = 0.24784543
Iteration 7, loss = 0.19062412
Iteration 8, loss = 0.15037978
Iteration 9, loss = 0.12228180
Iteration 10, loss = 0.10234283
Iteration 11, loss = 0.08770324
Iteration 12, loss = 0.07636422
Iteration 13, loss = 0.06819238
Iteration 14, loss = 0.06142017
Iteration 15, loss = 0.05626171
Iteration 16, loss = 0.05210450
Iteration 17, loss = 0.04852704
Iteration 18, loss = 0.04567794
Iteration 19, loss = 0.04320571
Iteration 20, loss = 0.04116206
Iteration 21, loss = 0.03925396
Iteration 22, loss = 0.03794803
Iteration 23, loss = 0.03638496
Iteration 24, loss = 0.03539622
Iteration 25, loss = 0.03439913
Iteration 26, loss = 0.03318082
Iteration 27, loss = 0.03274585
Iteration 28, loss = 0.03178937
Iteration 29, loss = 0.03141495
Iteration 30, loss = 0.03071318
Iteration 31, loss = 0.03014659
Iteration 32, loss = 0.02984721
Iteration 33, loss = 0.02916578
Iteration 34, loss = 0.02887476
Iteration 35, loss = 0.02841430
Iteration 36, loss = 0.02796134
Iteration 37, loss = 0.02785772
Iteration 38, loss = 0.02792552
Iteration 39, loss = 0.02714480
Iteration 40, loss = 0.02714581
Iteration 41, loss = 0.02672143
Iteration 42, loss = 0.02657915
Iteration 43, loss = 0.02611153
Iteration 44, loss = 0.02659318
Iteration 45, loss = 0.02611060
Iteration 46, loss = 0.02595020
Iteration 47, loss = 0.02555620
Iteration 48, loss = 0.02568099
Iteration 49, loss = 0.02537748
Iteration 50, loss = 0.02543902
Iteration 51, loss = 0.02531125
Iteration 52, loss = 0.02512946
Iteration 53, loss = 0.02522476
Iteration 54, loss = 0.02494356
Iteration 55, loss = 0.02516414
Iteration 56, loss = 0.02472024
Iteration 57, loss = 0.02457051
Iteration 58, loss = 0.02481924
Iteration 59, loss = 0.02446812
Iteration 60, loss = 0.02460869
Iteration 61, loss = 0.02441061
Iteration 62, loss = 0.02433035
Iteration 63, loss = 0.02432324
Iteration 64, loss = 0.02403191
Iteration 65, loss = 0.02425142
Iteration 66, loss = 0.02449233
Iteration 67, loss = 0.02410826
Iteration 68, loss = 0.02406800
Iteration 69, loss = 0.02421556
Iteration 70, loss = 0.02407249
Iteration 71, loss = 0.02418662
Iteration 72, loss = 0.02377962
Iteration 73, loss = 0.02402766
Iteration 74, loss = 0.02386681
Iteration 75, loss = 0.02372604
Iteration 76, loss = 0.02390923
Iteration 77, loss = 0.02372345
Iteration 78, loss = 0.02382798
Iteration 79, loss = 0.02387166
Iteration 80, loss = 0.02383313
Iteration 81, loss = 0.02364369
Iteration 82, loss = 0.02372903
Iteration 83, loss = 0.02368870
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67740058
Iteration 2, loss = 0.60845448
Iteration 3, loss = 0.50776636
Iteration 4, loss = 0.40208990
Iteration 5, loss = 0.31096986
Iteration 6, loss = 0.23879675
Iteration 7, loss = 0.18605750
Iteration 8, loss = 0.14820854
Iteration 9, loss = 0.12155740
Iteration 10, loss = 0.10213050
Iteration 11, loss = 0.08802999
Iteration 12, loss = 0.07740767
Iteration 13, loss = 0.06918941
Iteration 14, loss = 0.06278644
Iteration 15, loss = 0.05771155
Iteration 16, loss = 0.05336857
Iteration 17, loss = 0.05023601
Iteration 18, loss = 0.04715015
Iteration 19, loss = 0.04492460
Iteration 20, loss = 0.04322221
Iteration 21, loss = 0.04128215
Iteration 22, loss = 0.03977528
Iteration 23, loss = 0.03823302
Iteration 24, loss = 0.03762972
Iteration 25, loss = 0.03629119
Iteration 26, loss = 0.03565853
Iteration 27, loss = 0.03472478
Iteration 28, loss = 0.03394970
Iteration 29, loss = 0.03328333
Iteration 30, loss = 0.03288997
Iteration 31, loss = 0.03267644
Iteration 32, loss = 0.03211519
Iteration 33, loss = 0.03163192
Iteration 34, loss = 0.03110732
Iteration 35, loss = 0.03077676
Iteration 36, loss = 0.03060583
Iteration 37, loss = 0.03009468
Iteration 38, loss = 0.03012011
Iteration 39, loss = 0.02968243
Iteration 40, loss = 0.02954893
Iteration 41, loss = 0.02952452
Iteration 42, loss = 0.02916539
Iteration 43, loss = 0.02902832
Iteration 44, loss = 0.02902763
Iteration 45, loss = 0.02842768
Iteration 46, loss = 0.02850931
Iteration 47, loss = 0.02839420
Iteration 48, loss = 0.02816441
Iteration 49, loss = 0.02812537
Iteration 50, loss = 0.02806016
Iteration 51, loss = 0.02792593
Iteration 52, loss = 0.02777281
Iteration 53, loss = 0.02760666
Iteration 54, loss = 0.02795012
Iteration 55, loss = 0.02755532
Iteration 56, loss = 0.02755358
Iteration 57, loss = 0.02742217
Iteration 58, loss = 0.02723694
Iteration 59, loss = 0.02714274
Iteration 60, loss = 0.02706385
Iteration 61, loss = 0.02714342
Iteration 62, loss = 0.02747506
Iteration 63, loss = 0.02717079
Iteration 64, loss = 0.02688663
Iteration 65, loss = 0.02678876
Iteration 66, loss = 0.02678447
Iteration 67, loss = 0.02687154
Iteration 68, loss = 0.02693154
Iteration 69, loss = 0.02696132
Iteration 70, loss = 0.02671329
Iteration 71, loss = 0.02659705
Iteration 72, loss = 0.02647245
Iteration 73, loss = 0.02662399
Iteration 74, loss = 0.02668599
Iteration 75, loss = 0.02675848
Iteration 76, loss = 0.02651796
Iteration 77, loss = 0.02659308
Iteration 78, loss = 0.02639821
Iteration 79, loss = 0.02678463
Iteration 80, loss = 0.02647157
Iteration 81, loss = 0.02668549
Iteration 82, loss = 0.02665340
Iteration 83, loss = 0.02667033
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67786813
Iteration 2, loss = 0.64960891
Iteration 3, loss = 0.61174159
Iteration 4, loss = 0.56052410
Iteration 5, loss = 0.50190428
Iteration 6, loss = 0.44170086
Iteration 7, loss = 0.38415522
Iteration 8, loss = 0.33080391
Iteration 9, loss = 0.28365735
Iteration 10, loss = 0.24268296
Iteration 11, loss = 0.20807210
Iteration 12, loss = 0.17922629
Iteration 13, loss = 0.15523483
Iteration 14, loss = 0.13557416
Iteration 15, loss = 0.11932669
Iteration 16, loss = 0.10572298
Iteration 17, loss = 0.09451744
Iteration 18, loss = 0.08528625
Iteration 19, loss = 0.07728977
Iteration 20, loss = 0.07074581
Iteration 21, loss = 0.06487614
Iteration 22, loss = 0.05993026
Iteration 23, loss = 0.05561573
Iteration 24, loss = 0.05209268
Iteration 25, loss = 0.04871785
Iteration 26, loss = 0.04588345
Iteration 27, loss = 0.04331654
Iteration 28, loss = 0.04107091
Iteration 29, loss = 0.03904819
Iteration 30, loss = 0.03725004
Iteration 31, loss = 0.03563689
Iteration 32, loss = 0.03425632
Iteration 33, loss = 0.03292860
Iteration 34, loss = 0.03173665
Iteration 35, loss = 0.03075300
Iteration 36, loss = 0.02979011
Iteration 37, loss = 0.02869080
Iteration 38, loss = 0.02779415
Iteration 39, loss = 0.02705693
Iteration 40, loss = 0.02641357
Iteration 41, loss = 0.02557024
Iteration 42, loss = 0.02501897
Iteration 43, loss = 0.02438088
Iteration 44, loss = 0.02395667
Iteration 45, loss = 0.02341858
Iteration 46, loss = 0.02298215
Iteration 47, loss = 0.02268074
Iteration 48, loss = 0.02206485
Iteration 49, loss = 0.02164709
Iteration 50, loss = 0.02144266
Iteration 51, loss = 0.02109019
Iteration 52, loss = 0.02077513
Iteration 53, loss = 0.02041651
Iteration 54, loss = 0.02014082
Iteration 55, loss = 0.02005955
Iteration 56, loss = 0.01964576
Iteration 57, loss = 0.01934004
Iteration 58, loss = 0.01917495
Iteration 59, loss = 0.01898718
Iteration 60, loss = 0.01865329
Iteration 61, loss = 0.01851716
Iteration 62, loss = 0.01826175
Iteration 63, loss = 0.01809297
Iteration 64, loss = 0.01810411
Iteration 65, loss = 0.01783053
Iteration 66, loss = 0.01773357
Iteration 67, loss = 0.01764989
Iteration 68, loss = 0.01754653
Iteration 69, loss = 0.01721531
Iteration 70, loss = 0.01717326
Iteration 71, loss = 0.01704726
Iteration 72, loss = 0.01684310
Iteration 73, loss = 0.01680971
Iteration 74, loss = 0.01672088
Iteration 75, loss = 0.01654513
Iteration 76, loss = 0.01653580
Iteration 77, loss = 0.01636242
Iteration 78, loss = 0.01622816
Iteration 79, loss = 0.01626440
Iteration 80, loss = 0.01609824
Iteration 81, loss = 0.01617727
Iteration 82, loss = 0.01602901
Iteration 83, loss = 0.01586801
Iteration 84, loss = 0.01589138
Iteration 85, loss = 0.01575288
Iteration 86, loss = 0.01564565
Iteration 87, loss = 0.01569522
Iteration 88, loss = 0.01552222
Iteration 89, loss = 0.01563663
Iteration 90, loss = 0.01538111
Iteration 91, loss = 0.01530837
Iteration 92, loss = 0.01531482
Iteration 93, loss = 0.01538333
Iteration 94, loss = 0.01525402
Iteration 95, loss = 0.01535712
Iteration 96, loss = 0.01517749
Iteration 97, loss = 0.01506479
Iteration 98, loss = 0.01524118
Iteration 99, loss = 0.01506255
Iteration 100, loss = 0.01494993
Iteration 101, loss = 0.01496454
Iteration 102, loss = 0.01495515
Iteration 103, loss = 0.01490273
Iteration 104, loss = 0.01501171
Iteration 105, loss = 0.01481238
Iteration 106, loss = 0.01478000
Iteration 107, loss = 0.01469497
Iteration 108, loss = 0.01471055
Iteration 109, loss = 0.01468787
Iteration 110, loss = 0.01460433
Iteration 111, loss = 0.01466184
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67997288
Iteration 2, loss = 0.64478619
Iteration 3, loss = 0.59890970
Iteration 4, loss = 0.53958765
Iteration 5, loss = 0.47226637
Iteration 6, loss = 0.40415887
Iteration 7, loss = 0.34029143
Iteration 8, loss = 0.28464734
Iteration 9, loss = 0.23775164
Iteration 10, loss = 0.19906058
Iteration 11, loss = 0.16811867
Iteration 12, loss = 0.14340512
Iteration 13, loss = 0.12388742
Iteration 14, loss = 0.10830980
Iteration 15, loss = 0.09570002
Iteration 16, loss = 0.08544664
Iteration 17, loss = 0.07683564
Iteration 18, loss = 0.07000500
Iteration 19, loss = 0.06409961
Iteration 20, loss = 0.05929476
Iteration 21, loss = 0.05491992
Iteration 22, loss = 0.05135776
Iteration 23, loss = 0.04838362
Iteration 24, loss = 0.04561381
Iteration 25, loss = 0.04305994
Iteration 26, loss = 0.04095933
Iteration 27, loss = 0.03928365
Iteration 28, loss = 0.03758822
Iteration 29, loss = 0.03614691
Iteration 30, loss = 0.03491354
Iteration 31, loss = 0.03367994
Iteration 32, loss = 0.03257505
Iteration 33, loss = 0.03180607
Iteration 34, loss = 0.03073056
Iteration 35, loss = 0.02995743
Iteration 36, loss = 0.02913552
Iteration 37, loss = 0.02851612
Iteration 38, loss = 0.02784844
Iteration 39, loss = 0.02736629
Iteration 40, loss = 0.02682344
Iteration 41, loss = 0.02641773
Iteration 42, loss = 0.02585907
Iteration 43, loss = 0.02535891
Iteration 44, loss = 0.02504781
Iteration 45, loss = 0.02465971
Iteration 46, loss = 0.02429354
Iteration 47, loss = 0.02398380
Iteration 48, loss = 0.02374537
Iteration 49, loss = 0.02355376
Iteration 50, loss = 0.02325611
Iteration 51, loss = 0.02312404
Iteration 52, loss = 0.02278839
Iteration 53, loss = 0.02257548
Iteration 54, loss = 0.02239611
Iteration 55, loss = 0.02215999
Iteration 56, loss = 0.02203372
Iteration 57, loss = 0.02189351
Iteration 58, loss = 0.02152848
Iteration 59, loss = 0.02143593
Iteration 60, loss = 0.02132691
Iteration 61, loss = 0.02116472
Iteration 62, loss = 0.02094326
Iteration 63, loss = 0.02102649
Iteration 64, loss = 0.02083638
Iteration 65, loss = 0.02072718
Iteration 66, loss = 0.02055065
Iteration 67, loss = 0.02063135
Iteration 68, loss = 0.02038609
Iteration 69, loss = 0.02016693
Iteration 70, loss = 0.02014720
Iteration 71, loss = 0.02021145
Iteration 72, loss = 0.01987770
Iteration 73, loss = 0.02001130
Iteration 74, loss = 0.01982878
Iteration 75, loss = 0.01970643
Iteration 76, loss = 0.01974363
Iteration 77, loss = 0.01979363
Iteration 78, loss = 0.01971263
Iteration 79, loss = 0.01958594
Iteration 80, loss = 0.01948300
Iteration 81, loss = 0.01930550
Iteration 82, loss = 0.01933402
Iteration 83, loss = 0.01920955
Iteration 84, loss = 0.01923484
Iteration 85, loss = 0.01921455
Iteration 86, loss = 0.01924751
Iteration 87, loss = 0.01911083
Iteration 88, loss = 0.01894093
Iteration 89, loss = 0.01907346
Iteration 90, loss = 0.01918176
Iteration 91, loss = 0.01891555
Iteration 92, loss = 0.01904596
Iteration 93, loss = 0.01879225
Iteration 94, loss = 0.01868008
Iteration 95, loss = 0.01865399
Iteration 96, loss = 0.01883619
Iteration 97, loss = 0.01859160
Iteration 98, loss = 0.01865139
Iteration 99, loss = 0.01866397
Iteration 100, loss = 0.01860647
Iteration 101, loss = 0.01850637
Iteration 102, loss = 0.01839042
Iteration 103, loss = 0.01845978
Iteration 104, loss = 0.01851425
Iteration 105, loss = 0.01849598
Iteration 106, loss = 0.01833072
Iteration 107, loss = 0.01842451
Iteration 108, loss = 0.01828481
Iteration 109, loss = 0.01824676
Iteration 110, loss = 0.01841932
Iteration 111, loss = 0.01826025
Iteration 112, loss = 0.01838480
Iteration 113, loss = 0.01816060
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71442289
Iteration 2, loss = 0.69092618
Iteration 3, loss = 0.66964065
Iteration 4, loss = 0.64609323
Iteration 5, loss = 0.61891080
Iteration 6, loss = 0.58800684
Iteration 7, loss = 0.55374291
Iteration 8, loss = 0.51731193
Iteration 9, loss = 0.47982157
Iteration 10, loss = 0.44268411
Iteration 11, loss = 0.40646722
Iteration 12, loss = 0.37166066
Iteration 13, loss = 0.33878100
Iteration 14, loss = 0.30828349
Iteration 15, loss = 0.28023209
Iteration 16, loss = 0.25458732
Iteration 17, loss = 0.23126117
Iteration 18, loss = 0.21026492
Iteration 19, loss = 0.19142199
Iteration 20, loss = 0.17464020
Iteration 21, loss = 0.15954306
Iteration 22, loss = 0.14629204
Iteration 23, loss = 0.13440155
Iteration 24, loss = 0.12378531
Iteration 25, loss = 0.11436233
Iteration 26, loss = 0.10598461
Iteration 27, loss = 0.09849875
Iteration 28, loss = 0.09176587
Iteration 29, loss = 0.08572595
Iteration 30, loss = 0.08029560
Iteration 31, loss = 0.07542803
Iteration 32, loss = 0.07100227
Iteration 33, loss = 0.06701696
Iteration 34, loss = 0.06330207
Iteration 35, loss = 0.06000347
Iteration 36, loss = 0.05695423
Iteration 37, loss = 0.05418195
Iteration 38, loss = 0.05162810
Iteration 39, loss = 0.04937305
Iteration 40, loss = 0.04715184
Iteration 41, loss = 0.04522893
Iteration 42, loss = 0.04337649
Iteration 43, loss = 0.04162429
Iteration 44, loss = 0.04001917
Iteration 45, loss = 0.03858006
Iteration 46, loss = 0.03714689
Iteration 47, loss = 0.03589997
Iteration 48, loss = 0.03468072
Iteration 49, loss = 0.03354682
Iteration 50, loss = 0.03249472
Iteration 51, loss = 0.03148756
Iteration 52, loss = 0.03057737
Iteration 53, loss = 0.02969302
Iteration 54, loss = 0.02889747
Iteration 55, loss = 0.02812245
Iteration 56, loss = 0.02737528
Iteration 57, loss = 0.02666594
Iteration 58, loss = 0.02597203
Iteration 59, loss = 0.02538701
Iteration 60, loss = 0.02484332
Iteration 61, loss = 0.02425065
Iteration 62, loss = 0.02370029
Iteration 63, loss = 0.02331303
Iteration 64, loss = 0.02278844
Iteration 65, loss = 0.02229056
Iteration 66, loss = 0.02183548
Iteration 67, loss = 0.02142424
Iteration 68, loss = 0.02104156
Iteration 69, loss = 0.02059179
Iteration 70, loss = 0.02027283
Iteration 71, loss = 0.01992675
Iteration 72, loss = 0.01953656
Iteration 73, loss = 0.01924362
Iteration 74, loss = 0.01890969
Iteration 75, loss = 0.01864735
Iteration 76, loss = 0.01835339
Iteration 77, loss = 0.01807244
Iteration 78, loss = 0.01784088
Iteration 79, loss = 0.01755400
Iteration 80, loss = 0.01737539
Iteration 81, loss = 0.01710087
Iteration 82, loss = 0.01683871
Iteration 83, loss = 0.01665555
Iteration 84, loss = 0.01648190
Iteration 85, loss = 0.01629470
Iteration 86, loss = 0.01610137
Iteration 87, loss = 0.01591046
Iteration 88, loss = 0.01567643
Iteration 89, loss = 0.01564720
Iteration 90, loss = 0.01540261
Iteration 91, loss = 0.01524436
Iteration 92, loss = 0.01510249
Iteration 93, loss = 0.01495017
Iteration 94, loss = 0.01479025
Iteration 95, loss = 0.01469674
Iteration 96, loss = 0.01449164
Iteration 97, loss = 0.01436696
Iteration 98, loss = 0.01423505
Iteration 99, loss = 0.01409754
Iteration 100, loss = 0.01404417
Iteration 101, loss = 0.01388792
Iteration 102, loss = 0.01375115
Iteration 103, loss = 0.01363954
Iteration 104, loss = 0.01365603
Iteration 105, loss = 0.01344733
Iteration 106, loss = 0.01337054
Iteration 107, loss = 0.01321135
Iteration 108, loss = 0.01312778
Iteration 109, loss = 0.01304067
Iteration 110, loss = 0.01293533
Iteration 111, loss = 0.01286156
Iteration 112, loss = 0.01284124
Iteration 113, loss = 0.01267875
Iteration 114, loss = 0.01266008
Iteration 115, loss = 0.01255792
Iteration 116, loss = 0.01250760
Iteration 117, loss = 0.01241752
Iteration 118, loss = 0.01231583
Iteration 119, loss = 0.01228014
Iteration 120, loss = 0.01223440
Iteration 121, loss = 0.01203901
Iteration 122, loss = 0.01218236
Iteration 123, loss = 0.01201795
Iteration 124, loss = 0.01191348
Iteration 125, loss = 0.01188533
Iteration 126, loss = 0.01182286
Iteration 127, loss = 0.01169178
Iteration 128, loss = 0.01169301
Iteration 129, loss = 0.01165423
Iteration 130, loss = 0.01156379
Iteration 131, loss = 0.01152615
Iteration 132, loss = 0.01154510
Iteration 133, loss = 0.01138272
Iteration 134, loss = 0.01134192
Iteration 135, loss = 0.01133316
Iteration 136, loss = 0.01127749
Iteration 137, loss = 0.01121171
Iteration 138, loss = 0.01123157
Iteration 139, loss = 0.01117326
Iteration 140, loss = 0.01108531
Iteration 141, loss = 0.01110432
Iteration 142, loss = 0.01100579
Iteration 143, loss = 0.01100696
Iteration 144, loss = 0.01095077
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67891743
Iteration 2, loss = 0.65670928
Iteration 3, loss = 0.63125037
Iteration 4, loss = 0.59817160
Iteration 5, loss = 0.55768388
Iteration 6, loss = 0.51143593
Iteration 7, loss = 0.46358833
Iteration 8, loss = 0.41585470
Iteration 9, loss = 0.37022317
Iteration 10, loss = 0.32760628
Iteration 11, loss = 0.28855297
Iteration 12, loss = 0.25380609
Iteration 13, loss = 0.22338940
Iteration 14, loss = 0.19692837
Iteration 15, loss = 0.17400126
Iteration 16, loss = 0.15449000
Iteration 17, loss = 0.13784207
Iteration 18, loss = 0.12355123
Iteration 19, loss = 0.11138608
Iteration 20, loss = 0.10093317
Iteration 21, loss = 0.09194104
Iteration 22, loss = 0.08412620
Iteration 23, loss = 0.07735675
Iteration 24, loss = 0.07149578
Iteration 25, loss = 0.06633845
Iteration 26, loss = 0.06192705
Iteration 27, loss = 0.05787060
Iteration 28, loss = 0.05419321
Iteration 29, loss = 0.05092557
Iteration 30, loss = 0.04808256
Iteration 31, loss = 0.04553431
Iteration 32, loss = 0.04313605
Iteration 33, loss = 0.04106161
Iteration 34, loss = 0.03906033
Iteration 35, loss = 0.03736205
Iteration 36, loss = 0.03573078
Iteration 37, loss = 0.03425664
Iteration 38, loss = 0.03287934
Iteration 39, loss = 0.03165969
Iteration 40, loss = 0.03044822
Iteration 41, loss = 0.02939611
Iteration 42, loss = 0.02842882
Iteration 43, loss = 0.02751000
Iteration 44, loss = 0.02668080
Iteration 45, loss = 0.02581959
Iteration 46, loss = 0.02510542
Iteration 47, loss = 0.02444759
Iteration 48, loss = 0.02380751
Iteration 49, loss = 0.02316318
Iteration 50, loss = 0.02261587
Iteration 51, loss = 0.02207638
Iteration 52, loss = 0.02157487
Iteration 53, loss = 0.02105888
Iteration 54, loss = 0.02067048
Iteration 55, loss = 0.02020073
Iteration 56, loss = 0.01996147
Iteration 57, loss = 0.01961640
Iteration 58, loss = 0.01909789
Iteration 59, loss = 0.01877147
Iteration 60, loss = 0.01847230
Iteration 61, loss = 0.01816164
Iteration 62, loss = 0.01782874
Iteration 63, loss = 0.01751793
Iteration 64, loss = 0.01724765
Iteration 65, loss = 0.01702414
Iteration 66, loss = 0.01680247
Iteration 67, loss = 0.01654354
Iteration 68, loss = 0.01633162
Iteration 69, loss = 0.01609937
Iteration 70, loss = 0.01594150
Iteration 71, loss = 0.01575487
Iteration 72, loss = 0.01561698
Iteration 73, loss = 0.01541861
Iteration 74, loss = 0.01528878
Iteration 75, loss = 0.01506655
Iteration 76, loss = 0.01492843
Iteration 77, loss = 0.01471382
Iteration 78, loss = 0.01453720
Iteration 79, loss = 0.01447354
Iteration 80, loss = 0.01436216
Iteration 81, loss = 0.01418370
Iteration 82, loss = 0.01404510
Iteration 83, loss = 0.01386640
Iteration 84, loss = 0.01379519
Iteration 85, loss = 0.01365641
Iteration 86, loss = 0.01359777
Iteration 87, loss = 0.01345813
Iteration 88, loss = 0.01344314
Iteration 89, loss = 0.01330294
Iteration 90, loss = 0.01344673
Iteration 91, loss = 0.01331224
Iteration 92, loss = 0.01337743
Iteration 93, loss = 0.01315494
Iteration 94, loss = 0.01304922
Iteration 95, loss = 0.01289928
Iteration 96, loss = 0.01276692
Iteration 97, loss = 0.01261459
Iteration 98, loss = 0.01268431
Iteration 99, loss = 0.01265816
Iteration 100, loss = 0.01257285
Iteration 101, loss = 0.01244591
Iteration 102, loss = 0.01246248
Iteration 103, loss = 0.01234719
Iteration 104, loss = 0.01217783
Iteration 105, loss = 0.01213160
Iteration 106, loss = 0.01206384
Iteration 107, loss = 0.01202796
Iteration 108, loss = 0.01196182
Iteration 109, loss = 0.01192003
Iteration 110, loss = 0.01187665
Iteration 111, loss = 0.01181326
Iteration 112, loss = 0.01182825
Iteration 113, loss = 0.01200144
Iteration 114, loss = 0.01203740
Iteration 115, loss = 0.01189058
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68670445
Iteration 2, loss = 0.63229322
Iteration 3, loss = 0.55113746
Iteration 4, loss = 0.45449074
Iteration 5, loss = 0.36016224
Iteration 6, loss = 0.28040627
Iteration 7, loss = 0.21918884
Iteration 8, loss = 0.17453508
Iteration 9, loss = 0.14184269
Iteration 10, loss = 0.11815603
Iteration 11, loss = 0.10120479
Iteration 12, loss = 0.08809203
Iteration 13, loss = 0.07809102
Iteration 14, loss = 0.07028138
Iteration 15, loss = 0.06403474
Iteration 16, loss = 0.05892646
Iteration 17, loss = 0.05479353
Iteration 18, loss = 0.05140736
Iteration 19, loss = 0.04887421
Iteration 20, loss = 0.04623904
Iteration 21, loss = 0.04410096
Iteration 22, loss = 0.04228496
Iteration 23, loss = 0.04075362
Iteration 24, loss = 0.03925695
Iteration 25, loss = 0.03779728
Iteration 26, loss = 0.03695742
Iteration 27, loss = 0.03586174
Iteration 28, loss = 0.03521879
Iteration 29, loss = 0.03444899
Iteration 30, loss = 0.03374295
Iteration 31, loss = 0.03329417
Iteration 32, loss = 0.03258126
Iteration 33, loss = 0.03247353
Iteration 34, loss = 0.03184619
Iteration 35, loss = 0.03155589
Iteration 36, loss = 0.03094943
Iteration 37, loss = 0.03080460
Iteration 38, loss = 0.03042180
Iteration 39, loss = 0.03006578
Iteration 40, loss = 0.02986734
Iteration 41, loss = 0.02931218
Iteration 42, loss = 0.02896373
Iteration 43, loss = 0.02869038
Iteration 44, loss = 0.02864830
Iteration 45, loss = 0.02840359
Iteration 46, loss = 0.02811955
Iteration 47, loss = 0.02782990
Iteration 48, loss = 0.02783577
Iteration 49, loss = 0.02762456
Iteration 50, loss = 0.02763470
Iteration 51, loss = 0.02728596
Iteration 52, loss = 0.02719939
Iteration 53, loss = 0.02693734
Iteration 54, loss = 0.02700313
Iteration 55, loss = 0.02705408
Iteration 56, loss = 0.02693107
Iteration 57, loss = 0.02681874
Iteration 58, loss = 0.02673207
Iteration 59, loss = 0.02662074
Iteration 60, loss = 0.02651014
Iteration 61, loss = 0.02620997
Iteration 62, loss = 0.02638869
Iteration 63, loss = 0.02600679
Iteration 64, loss = 0.02623474
Iteration 65, loss = 0.02601606
Iteration 66, loss = 0.02616146
Iteration 67, loss = 0.02623549
Iteration 68, loss = 0.02630737
Iteration 69, loss = 0.02621871
Iteration 70, loss = 0.02619487
Iteration 71, loss = 0.02604780
Iteration 72, loss = 0.02591672
Iteration 73, loss = 0.02553049
Iteration 74, loss = 0.02725755
Iteration 75, loss = 0.02742500
Iteration 76, loss = 0.02680603
Iteration 77, loss = 0.02623170
Iteration 78, loss = 0.02580254
Iteration 79, loss = 0.02550514
Iteration 80, loss = 0.02533925
Iteration 81, loss = 0.02512349
Iteration 82, loss = 0.02513819
Iteration 83, loss = 0.02514083
Iteration 84, loss = 0.02516227
Iteration 85, loss = 0.02492502
Iteration 86, loss = 0.02517423
Iteration 87, loss = 0.02527585
Iteration 88, loss = 0.02503396
Iteration 89, loss = 0.02520940
Iteration 90, loss = 0.02504258
Iteration 91, loss = 0.02502674
Iteration 92, loss = 0.02486518
Iteration 93, loss = 0.02475595
Iteration 94, loss = 0.02473657
Iteration 95, loss = 0.02490126
Iteration 96, loss = 0.02479560
Iteration 97, loss = 0.02451031
Iteration 98, loss = 0.02494019
Iteration 99, loss = 0.02478407
Iteration 100, loss = 0.02500902
Iteration 101, loss = 0.02465808
Iteration 102, loss = 0.02496873
Iteration 103, loss = 0.02581691
Iteration 104, loss = 0.02646671
Iteration 105, loss = 0.02583398
Iteration 106, loss = 0.02534308
Iteration 107, loss = 0.02512459
Iteration 108, loss = 0.02465785
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70461987
Iteration 2, loss = 0.62397434
Iteration 3, loss = 0.51682810
Iteration 4, loss = 0.39803372
Iteration 5, loss = 0.29771185
Iteration 6, loss = 0.22309736
Iteration 7, loss = 0.17110628
Iteration 8, loss = 0.13549273
Iteration 9, loss = 0.11086799
Iteration 10, loss = 0.09338260
Iteration 11, loss = 0.08086058
Iteration 12, loss = 0.07132206
Iteration 13, loss = 0.06388446
Iteration 14, loss = 0.05816694
Iteration 15, loss = 0.05388609
Iteration 16, loss = 0.04993822
Iteration 17, loss = 0.04718649
Iteration 18, loss = 0.04455180
Iteration 19, loss = 0.04277993
Iteration 20, loss = 0.04065229
Iteration 21, loss = 0.03920072
Iteration 22, loss = 0.03805455
Iteration 23, loss = 0.03699912
Iteration 24, loss = 0.03606060
Iteration 25, loss = 0.03489016
Iteration 26, loss = 0.03460921
Iteration 27, loss = 0.03364889
Iteration 28, loss = 0.03283695
Iteration 29, loss = 0.03269541
Iteration 30, loss = 0.03213014
Iteration 31, loss = 0.03178574
Iteration 32, loss = 0.03118113
Iteration 33, loss = 0.03091026
Iteration 34, loss = 0.03074409
Iteration 35, loss = 0.03019153
Iteration 36, loss = 0.03001810
Iteration 37, loss = 0.02958295
Iteration 38, loss = 0.02967476
Iteration 39, loss = 0.02926682
Iteration 40, loss = 0.02918109
Iteration 41, loss = 0.02878196
Iteration 42, loss = 0.02876533
Iteration 43, loss = 0.02844033
Iteration 44, loss = 0.02805595
Iteration 45, loss = 0.02855890
Iteration 46, loss = 0.02821410
Iteration 47, loss = 0.02816119
Iteration 48, loss = 0.02812133
Iteration 49, loss = 0.02815213
Iteration 50, loss = 0.02770774
Iteration 51, loss = 0.02811392
Iteration 52, loss = 0.02747642
Iteration 53, loss = 0.02773040
Iteration 54, loss = 0.02764406
Iteration 55, loss = 0.02743088
Iteration 56, loss = 0.02750707
Iteration 57, loss = 0.02745552
Iteration 58, loss = 0.02744289
Iteration 59, loss = 0.02713930
Iteration 60, loss = 0.02712895
Iteration 61, loss = 0.02706230
Iteration 62, loss = 0.02719302
Iteration 63, loss = 0.02714276
Iteration 64, loss = 0.02671589
Iteration 65, loss = 0.02700343
Iteration 66, loss = 0.02667739
Iteration 67, loss = 0.02695506
Iteration 68, loss = 0.02683747
Iteration 69, loss = 0.02692313
Iteration 70, loss = 0.02706211
Iteration 71, loss = 0.02659106
Iteration 72, loss = 0.02676417
Iteration 73, loss = 0.02642717
Iteration 74, loss = 0.02681540
Iteration 75, loss = 0.02691715
Iteration 76, loss = 0.02657779
Iteration 77, loss = 0.02654643
Iteration 78, loss = 0.02649137
Iteration 79, loss = 0.02651843
Iteration 80, loss = 0.02631473
Iteration 81, loss = 0.02638475
Iteration 82, loss = 0.02623952
Iteration 83, loss = 0.02652680
Iteration 84, loss = 0.02644352
Iteration 85, loss = 0.02634635
Iteration 86, loss = 0.02663465
Iteration 87, loss = 0.02648001
Iteration 88, loss = 0.02622102
Iteration 89, loss = 0.02616158
Iteration 90, loss = 0.02621268
Iteration 91, loss = 0.02668852
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70793289
Iteration 2, loss = 0.64679456
Iteration 3, loss = 0.55525175
Iteration 4, loss = 0.44698806
Iteration 5, loss = 0.34875526
Iteration 6, loss = 0.26809268
Iteration 7, loss = 0.20760559
Iteration 8, loss = 0.16421136
Iteration 9, loss = 0.13303425
Iteration 10, loss = 0.11065778
Iteration 11, loss = 0.09400077
Iteration 12, loss = 0.08157117
Iteration 13, loss = 0.07196502
Iteration 14, loss = 0.06448149
Iteration 15, loss = 0.05845188
Iteration 16, loss = 0.05337224
Iteration 17, loss = 0.04959152
Iteration 18, loss = 0.04602690
Iteration 19, loss = 0.04304906
Iteration 20, loss = 0.04078224
Iteration 21, loss = 0.03885190
Iteration 22, loss = 0.03708860
Iteration 23, loss = 0.03533309
Iteration 24, loss = 0.03396741
Iteration 25, loss = 0.03258216
Iteration 26, loss = 0.03162526
Iteration 27, loss = 0.03064545
Iteration 28, loss = 0.02985537
Iteration 29, loss = 0.02898005
Iteration 30, loss = 0.02831893
Iteration 31, loss = 0.02763217
Iteration 32, loss = 0.02704045
Iteration 33, loss = 0.02668052
Iteration 34, loss = 0.02601764
Iteration 35, loss = 0.02563989
Iteration 36, loss = 0.02523227
Iteration 37, loss = 0.02579144
Iteration 38, loss = 0.02552314
Iteration 39, loss = 0.02496755
Iteration 40, loss = 0.02424000
Iteration 41, loss = 0.02379144
Iteration 42, loss = 0.02354592
Iteration 43, loss = 0.02314935
Iteration 44, loss = 0.02297016
Iteration 45, loss = 0.02343912
Iteration 46, loss = 0.02328220
Iteration 47, loss = 0.02321184
Iteration 48, loss = 0.02354956
Iteration 49, loss = 0.02283853
Iteration 50, loss = 0.02268178
Iteration 51, loss = 0.02218197
Iteration 52, loss = 0.02192116
Iteration 53, loss = 0.02170676
Iteration 54, loss = 0.02171025
Iteration 55, loss = 0.02148945
Iteration 56, loss = 0.02146091
Iteration 57, loss = 0.02127416
Iteration 58, loss = 0.02130963
Iteration 59, loss = 0.02105583
Iteration 60, loss = 0.02109661
Iteration 61, loss = 0.02106941
Iteration 62, loss = 0.02081841
Iteration 63, loss = 0.02137987
Iteration 64, loss = 0.02131567
Iteration 65, loss = 0.02117030
Iteration 66, loss = 0.02094336
Iteration 67, loss = 0.02097768
Iteration 68, loss = 0.02073011
Iteration 69, loss = 0.02077048
Iteration 70, loss = 0.02055501
Iteration 71, loss = 0.02034513
Iteration 72, loss = 0.02126026
Iteration 73, loss = 0.02086169
Iteration 74, loss = 0.02031191
Iteration 75, loss = 0.02037992
Iteration 76, loss = 0.02041219
Iteration 77, loss = 0.02023965
Iteration 78, loss = 0.02034752
Iteration 79, loss = 0.02061822
Iteration 80, loss = 0.02049897
Iteration 81, loss = 0.01998170
Iteration 82, loss = 0.02025186
Iteration 83, loss = 0.01996358
Iteration 84, loss = 0.01997513
Iteration 85, loss = 0.01996391
Iteration 86, loss = 0.01986986
Iteration 87, loss = 0.01982105
Iteration 88, loss = 0.02021341
Iteration 89, loss = 0.02000561
Iteration 90, loss = 0.02007834
Iteration 91, loss = 0.02017095
Iteration 92, loss = 0.01978386
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67925720
Iteration 2, loss = 0.64598652
Iteration 3, loss = 0.60122543
Iteration 4, loss = 0.54111794
Iteration 5, loss = 0.47445232
Iteration 6, loss = 0.40821257
Iteration 7, loss = 0.34622018
Iteration 8, loss = 0.29129167
Iteration 9, loss = 0.24466508
Iteration 10, loss = 0.20600119
Iteration 11, loss = 0.17459755
Iteration 12, loss = 0.14962947
Iteration 13, loss = 0.12930144
Iteration 14, loss = 0.11322256
Iteration 15, loss = 0.10016846
Iteration 16, loss = 0.08945216
Iteration 17, loss = 0.08075903
Iteration 18, loss = 0.07326660
Iteration 19, loss = 0.06727671
Iteration 20, loss = 0.06213384
Iteration 21, loss = 0.05765738
Iteration 22, loss = 0.05392624
Iteration 23, loss = 0.05059096
Iteration 24, loss = 0.04765953
Iteration 25, loss = 0.04526606
Iteration 26, loss = 0.04294842
Iteration 27, loss = 0.04112113
Iteration 28, loss = 0.03930250
Iteration 29, loss = 0.03781797
Iteration 30, loss = 0.03634572
Iteration 31, loss = 0.03506708
Iteration 32, loss = 0.03395153
Iteration 33, loss = 0.03296186
Iteration 34, loss = 0.03204536
Iteration 35, loss = 0.03107633
Iteration 36, loss = 0.03024751
Iteration 37, loss = 0.02956463
Iteration 38, loss = 0.02892965
Iteration 39, loss = 0.02836062
Iteration 40, loss = 0.02783106
Iteration 41, loss = 0.02726219
Iteration 42, loss = 0.02676981
Iteration 43, loss = 0.02636981
Iteration 44, loss = 0.02598503
Iteration 45, loss = 0.02554276
Iteration 46, loss = 0.02514400
Iteration 47, loss = 0.02477079
Iteration 48, loss = 0.02447130
Iteration 49, loss = 0.02430507
Iteration 50, loss = 0.02398390
Iteration 51, loss = 0.02353872
Iteration 52, loss = 0.02338597
Iteration 53, loss = 0.02319510
Iteration 54, loss = 0.02297048
Iteration 55, loss = 0.02267860
Iteration 56, loss = 0.02258369
Iteration 57, loss = 0.02238315
Iteration 58, loss = 0.02221657
Iteration 59, loss = 0.02219927
Iteration 60, loss = 0.02176527
Iteration 61, loss = 0.02156514
Iteration 62, loss = 0.02151676
Iteration 63, loss = 0.02124455
Iteration 64, loss = 0.02109708
Iteration 65, loss = 0.02105468
Iteration 66, loss = 0.02100612
Iteration 67, loss = 0.02092645
Iteration 68, loss = 0.02070900
Iteration 69, loss = 0.02067681
Iteration 70, loss = 0.02054187
Iteration 71, loss = 0.02048576
Iteration 72, loss = 0.02043751
Iteration 73, loss = 0.02040430
Iteration 74, loss = 0.02018262
Iteration 75, loss = 0.02014904
Iteration 76, loss = 0.02006841
Iteration 77, loss = 0.01998006
Iteration 78, loss = 0.01991856
Iteration 79, loss = 0.01988586
Iteration 80, loss = 0.01974900
Iteration 81, loss = 0.01966182
Iteration 82, loss = 0.01960647
Iteration 83, loss = 0.01972630
Iteration 84, loss = 0.01971402
Iteration 85, loss = 0.01954333
Iteration 86, loss = 0.01945653
Iteration 87, loss = 0.01921362
Iteration 88, loss = 0.01935794
Iteration 89, loss = 0.01941192
Iteration 90, loss = 0.01912554
Iteration 91, loss = 0.01913101
Iteration 92, loss = 0.01926402
Iteration 93, loss = 0.01915337
Iteration 94, loss = 0.01904463
Iteration 95, loss = 0.01907841
Iteration 96, loss = 0.01923738
Iteration 97, loss = 0.01896747
Iteration 98, loss = 0.01909092
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72519450
Iteration 2, loss = 0.70798261
Iteration 3, loss = 0.69195375
Iteration 4, loss = 0.67319969
Iteration 5, loss = 0.65135476
Iteration 6, loss = 0.62535260
Iteration 7, loss = 0.59514829
Iteration 8, loss = 0.56193941
Iteration 9, loss = 0.52667877
Iteration 10, loss = 0.49061730
Iteration 11, loss = 0.45451670
Iteration 12, loss = 0.41930493
Iteration 13, loss = 0.38534334
Iteration 14, loss = 0.35320595
Iteration 15, loss = 0.32271736
Iteration 16, loss = 0.29462505
Iteration 17, loss = 0.26870468
Iteration 18, loss = 0.24494420
Iteration 19, loss = 0.22338822
Iteration 20, loss = 0.20389473
Iteration 21, loss = 0.18634486
Iteration 22, loss = 0.17065555
Iteration 23, loss = 0.15656869
Iteration 24, loss = 0.14394086
Iteration 25, loss = 0.13273882
Iteration 26, loss = 0.12269529
Iteration 27, loss = 0.11372993
Iteration 28, loss = 0.10568592
Iteration 29, loss = 0.09841837
Iteration 30, loss = 0.09192153
Iteration 31, loss = 0.08606280
Iteration 32, loss = 0.08079294
Iteration 33, loss = 0.07606374
Iteration 34, loss = 0.07171915
Iteration 35, loss = 0.06780415
Iteration 36, loss = 0.06417058
Iteration 37, loss = 0.06089530
Iteration 38, loss = 0.05796418
Iteration 39, loss = 0.05516445
Iteration 40, loss = 0.05261749
Iteration 41, loss = 0.05032776
Iteration 42, loss = 0.04812797
Iteration 43, loss = 0.04621837
Iteration 44, loss = 0.04427758
Iteration 45, loss = 0.04261244
Iteration 46, loss = 0.04095852
Iteration 47, loss = 0.03950504
Iteration 48, loss = 0.03806740
Iteration 49, loss = 0.03675276
Iteration 50, loss = 0.03562114
Iteration 51, loss = 0.03446309
Iteration 52, loss = 0.03342252
Iteration 53, loss = 0.03236382
Iteration 54, loss = 0.03138818
Iteration 55, loss = 0.03051474
Iteration 56, loss = 0.02970506
Iteration 57, loss = 0.02887331
Iteration 58, loss = 0.02813729
Iteration 59, loss = 0.02737986
Iteration 60, loss = 0.02670360
Iteration 61, loss = 0.02606509
Iteration 62, loss = 0.02543885
Iteration 63, loss = 0.02486822
Iteration 64, loss = 0.02432614
Iteration 65, loss = 0.02380203
Iteration 66, loss = 0.02333795
Iteration 67, loss = 0.02281387
Iteration 68, loss = 0.02236368
Iteration 69, loss = 0.02193935
Iteration 70, loss = 0.02148664
Iteration 71, loss = 0.02110788
Iteration 72, loss = 0.02077339
Iteration 73, loss = 0.02035257
Iteration 74, loss = 0.02000526
Iteration 75, loss = 0.01967695
Iteration 76, loss = 0.01935324
Iteration 77, loss = 0.01906832
Iteration 78, loss = 0.01884113
Iteration 79, loss = 0.01850302
Iteration 80, loss = 0.01821502
Iteration 81, loss = 0.01795629
Iteration 82, loss = 0.01766300
Iteration 83, loss = 0.01744838
Iteration 84, loss = 0.01718213
Iteration 85, loss = 0.01692822
Iteration 86, loss = 0.01676758
Iteration 87, loss = 0.01660535
Iteration 88, loss = 0.01634633
Iteration 89, loss = 0.01613349
Iteration 90, loss = 0.01594390
Iteration 91, loss = 0.01574125
Iteration 92, loss = 0.01562747
Iteration 93, loss = 0.01545935
Iteration 94, loss = 0.01522227
Iteration 95, loss = 0.01505803
Iteration 96, loss = 0.01491484
Iteration 97, loss = 0.01473347
Iteration 98, loss = 0.01457775
Iteration 99, loss = 0.01447158
Iteration 100, loss = 0.01437854
Iteration 101, loss = 0.01416933
Iteration 102, loss = 0.01409807
Iteration 103, loss = 0.01395680
Iteration 104, loss = 0.01379508
Iteration 105, loss = 0.01368347
Iteration 106, loss = 0.01357934
Iteration 107, loss = 0.01346199
Iteration 108, loss = 0.01337953
Iteration 109, loss = 0.01322185
Iteration 110, loss = 0.01313618
Iteration 111, loss = 0.01302083
Iteration 112, loss = 0.01296860
Iteration 113, loss = 0.01283824
Iteration 114, loss = 0.01275363
Iteration 115, loss = 0.01265890
Iteration 116, loss = 0.01255371
Iteration 117, loss = 0.01249433
Iteration 118, loss = 0.01241698
Iteration 119, loss = 0.01228626
Iteration 120, loss = 0.01224143
Iteration 121, loss = 0.01218496
Iteration 122, loss = 0.01211516
Iteration 123, loss = 0.01204964
Iteration 124, loss = 0.01202032
Iteration 125, loss = 0.01186420
Iteration 126, loss = 0.01186383
Iteration 127, loss = 0.01179143
Iteration 128, loss = 0.01164959
Iteration 129, loss = 0.01165563
Iteration 130, loss = 0.01160571
Iteration 131, loss = 0.01151853
Iteration 132, loss = 0.01142588
Iteration 133, loss = 0.01137920
Iteration 134, loss = 0.01129325
Iteration 135, loss = 0.01127785
Iteration 136, loss = 0.01117856
Iteration 137, loss = 0.01120077
Iteration 138, loss = 0.01109564
Iteration 139, loss = 0.01108214
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70029572
Iteration 2, loss = 0.65025313
Iteration 3, loss = 0.58569647
Iteration 4, loss = 0.50616341
Iteration 5, loss = 0.42103041
Iteration 6, loss = 0.34345625
Iteration 7, loss = 0.27807425
Iteration 8, loss = 0.22585496
Iteration 9, loss = 0.18515024
Iteration 10, loss = 0.15394985
Iteration 11, loss = 0.12987866
Iteration 12, loss = 0.11158469
Iteration 13, loss = 0.09726111
Iteration 14, loss = 0.08623484
Iteration 15, loss = 0.07727334
Iteration 16, loss = 0.06998690
Iteration 17, loss = 0.06390667
Iteration 18, loss = 0.05896415
Iteration 19, loss = 0.05471388
Iteration 20, loss = 0.05131856
Iteration 21, loss = 0.04833943
Iteration 22, loss = 0.04567215
Iteration 23, loss = 0.04358698
Iteration 24, loss = 0.04129995
Iteration 25, loss = 0.03994171
Iteration 26, loss = 0.03836752
Iteration 27, loss = 0.03695115
Iteration 28, loss = 0.03585701
Iteration 29, loss = 0.03485192
Iteration 30, loss = 0.03387757
Iteration 31, loss = 0.03306690
Iteration 32, loss = 0.03220497
Iteration 33, loss = 0.03140490
Iteration 34, loss = 0.03069370
Iteration 35, loss = 0.03017725
Iteration 36, loss = 0.02991887
Iteration 37, loss = 0.02950899
Iteration 38, loss = 0.02889800
Iteration 39, loss = 0.02855677
Iteration 40, loss = 0.02801590
Iteration 41, loss = 0.02739958
Iteration 42, loss = 0.02728690
Iteration 43, loss = 0.02695237
Iteration 44, loss = 0.02651149
Iteration 45, loss = 0.02637626
Iteration 46, loss = 0.02605440
Iteration 47, loss = 0.02584871
Iteration 48, loss = 0.02565017
Iteration 49, loss = 0.02572059
Iteration 50, loss = 0.02533733
Iteration 51, loss = 0.02529398
Iteration 52, loss = 0.02510522
Iteration 53, loss = 0.02490481
Iteration 54, loss = 0.02491985
Iteration 55, loss = 0.02452278
Iteration 56, loss = 0.02456016
Iteration 57, loss = 0.02422876
Iteration 58, loss = 0.02423830
Iteration 59, loss = 0.02413065
Iteration 60, loss = 0.02384313
Iteration 61, loss = 0.02371920
Iteration 62, loss = 0.02361312
Iteration 63, loss = 0.02358097
Iteration 64, loss = 0.02360402
Iteration 65, loss = 0.02326636
Iteration 66, loss = 0.02316529
Iteration 67, loss = 0.02322491
Iteration 68, loss = 0.02311278
Iteration 69, loss = 0.02317847
Iteration 70, loss = 0.02307501
Iteration 71, loss = 0.02291834
Iteration 72, loss = 0.02309675
Iteration 73, loss = 0.02277458
Iteration 74, loss = 0.02291926
Iteration 75, loss = 0.02288375
Iteration 76, loss = 0.02272076
Iteration 77, loss = 0.02268152
Iteration 78, loss = 0.02243355
Iteration 79, loss = 0.02230222
Iteration 80, loss = 0.02236430
Iteration 81, loss = 0.02238008
Iteration 82, loss = 0.02230763
Iteration 83, loss = 0.02245095
Iteration 84, loss = 0.02243769
Iteration 85, loss = 0.02222716
Iteration 86, loss = 0.02223064
Iteration 87, loss = 0.02244274
Iteration 88, loss = 0.02221641
Iteration 89, loss = 0.02215271
Iteration 90, loss = 0.02211144
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67791223
Iteration 2, loss = 0.63074132
Iteration 3, loss = 0.56117495
Iteration 4, loss = 0.47718506
Iteration 5, loss = 0.39289985
Iteration 6, loss = 0.31631427
Iteration 7, loss = 0.25298882
Iteration 8, loss = 0.20274854
Iteration 9, loss = 0.16508964
Iteration 10, loss = 0.13675213
Iteration 11, loss = 0.11537516
Iteration 12, loss = 0.09943977
Iteration 13, loss = 0.08692519
Iteration 14, loss = 0.07721730
Iteration 15, loss = 0.06934708
Iteration 16, loss = 0.06304041
Iteration 17, loss = 0.05808962
Iteration 18, loss = 0.05400359
Iteration 19, loss = 0.05054652
Iteration 20, loss = 0.04753746
Iteration 21, loss = 0.04479597
Iteration 22, loss = 0.04288697
Iteration 23, loss = 0.04088073
Iteration 24, loss = 0.03897865
Iteration 25, loss = 0.03766949
Iteration 26, loss = 0.03650541
Iteration 27, loss = 0.03526553
Iteration 28, loss = 0.03418037
Iteration 29, loss = 0.03341138
Iteration 30, loss = 0.03256138
Iteration 31, loss = 0.03194708
Iteration 32, loss = 0.03114901
Iteration 33, loss = 0.03082347
Iteration 34, loss = 0.03020917
Iteration 35, loss = 0.02956583
Iteration 36, loss = 0.02942350
Iteration 37, loss = 0.02879976
Iteration 38, loss = 0.02839690
Iteration 39, loss = 0.02797921
Iteration 40, loss = 0.02758327
Iteration 41, loss = 0.02745327
Iteration 42, loss = 0.02733542
Iteration 43, loss = 0.02701306
Iteration 44, loss = 0.02671169
Iteration 45, loss = 0.02643314
Iteration 46, loss = 0.02635109
Iteration 47, loss = 0.02612291
Iteration 48, loss = 0.02610042
Iteration 49, loss = 0.02580839
Iteration 50, loss = 0.02571858
Iteration 51, loss = 0.02553611
Iteration 52, loss = 0.02516242
Iteration 53, loss = 0.02529451
Iteration 54, loss = 0.02484540
Iteration 55, loss = 0.02483879
Iteration 56, loss = 0.02483256
Iteration 57, loss = 0.02487270
Iteration 58, loss = 0.02449057
Iteration 59, loss = 0.02453327
Iteration 60, loss = 0.02450172
Iteration 61, loss = 0.02451208
Iteration 62, loss = 0.02436193
Iteration 63, loss = 0.02407391
Iteration 64, loss = 0.02385508
Iteration 65, loss = 0.02421479
Iteration 66, loss = 0.02407977
Iteration 67, loss = 0.02393232
Iteration 68, loss = 0.02400075
Iteration 69, loss = 0.02371438
Iteration 70, loss = 0.02407567
Iteration 71, loss = 0.02373781
Iteration 72, loss = 0.02355126
Iteration 73, loss = 0.02338483
Iteration 74, loss = 0.02365752
Iteration 75, loss = 0.02343121
Iteration 76, loss = 0.02348285
Iteration 77, loss = 0.02342597
Iteration 78, loss = 0.02325695
Iteration 79, loss = 0.02349184
Iteration 80, loss = 0.02307238
Iteration 81, loss = 0.02346225
Iteration 82, loss = 0.02339136
Iteration 83, loss = 0.02307705
Iteration 84, loss = 0.02310686
Iteration 85, loss = 0.02347318
Iteration 86, loss = 0.02293615
Iteration 87, loss = 0.02303972
Iteration 88, loss = 0.02316558
Iteration 89, loss = 0.02305339
Iteration 90, loss = 0.02304462
Iteration 91, loss = 0.02312564
Iteration 92, loss = 0.02286666
Iteration 93, loss = 0.02305654
Iteration 94, loss = 0.02297338
Iteration 95, loss = 0.02285141
Iteration 96, loss = 0.02282194
Iteration 97, loss = 0.02280045
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67893004
Iteration 2, loss = 0.63252509
Iteration 3, loss = 0.55881364
Iteration 4, loss = 0.47168170
Iteration 5, loss = 0.38685451
Iteration 6, loss = 0.31061259
Iteration 7, loss = 0.24747113
Iteration 8, loss = 0.19839115
Iteration 9, loss = 0.16144631
Iteration 10, loss = 0.13359361
Iteration 11, loss = 0.11295354
Iteration 12, loss = 0.09701440
Iteration 13, loss = 0.08493404
Iteration 14, loss = 0.07529345
Iteration 15, loss = 0.06775464
Iteration 16, loss = 0.06160628
Iteration 17, loss = 0.05647738
Iteration 18, loss = 0.05236827
Iteration 19, loss = 0.04899839
Iteration 20, loss = 0.04611319
Iteration 21, loss = 0.04344985
Iteration 22, loss = 0.04145333
Iteration 23, loss = 0.03960154
Iteration 24, loss = 0.03782720
Iteration 25, loss = 0.03642630
Iteration 26, loss = 0.03522454
Iteration 27, loss = 0.03404715
Iteration 28, loss = 0.03305341
Iteration 29, loss = 0.03209956
Iteration 30, loss = 0.03143968
Iteration 31, loss = 0.03048628
Iteration 32, loss = 0.03003432
Iteration 33, loss = 0.02960829
Iteration 34, loss = 0.02897765
Iteration 35, loss = 0.02859056
Iteration 36, loss = 0.02783453
Iteration 37, loss = 0.02750329
Iteration 38, loss = 0.02729598
Iteration 39, loss = 0.02688877
Iteration 40, loss = 0.02662076
Iteration 41, loss = 0.02631199
Iteration 42, loss = 0.02608616
Iteration 43, loss = 0.02565390
Iteration 44, loss = 0.02549953
Iteration 45, loss = 0.02542275
Iteration 46, loss = 0.02512527
Iteration 47, loss = 0.02506202
Iteration 48, loss = 0.02476516
Iteration 49, loss = 0.02477245
Iteration 50, loss = 0.02440157
Iteration 51, loss = 0.02428014
Iteration 52, loss = 0.02423154
Iteration 53, loss = 0.02407245
Iteration 54, loss = 0.02400061
Iteration 55, loss = 0.02353909
Iteration 56, loss = 0.02380992
Iteration 57, loss = 0.02343863
Iteration 58, loss = 0.02371774
Iteration 59, loss = 0.02356582
Iteration 60, loss = 0.02332285
Iteration 61, loss = 0.02319333
Iteration 62, loss = 0.02295107
Iteration 63, loss = 0.02299259
Iteration 64, loss = 0.02314812
Iteration 65, loss = 0.02294789
Iteration 66, loss = 0.02289436
Iteration 67, loss = 0.02291804
Iteration 68, loss = 0.02279647
Iteration 69, loss = 0.02256489
Iteration 70, loss = 0.02267126
Iteration 71, loss = 0.02260772
Iteration 72, loss = 0.02264005
Iteration 73, loss = 0.02234569
Iteration 74, loss = 0.02232825
Iteration 75, loss = 0.02218447
Iteration 76, loss = 0.02228138
Iteration 77, loss = 0.02233675
Iteration 78, loss = 0.02226725
Iteration 79, loss = 0.02238610
Iteration 80, loss = 0.02223777
Iteration 81, loss = 0.02196121
Iteration 82, loss = 0.02227714
Iteration 83, loss = 0.02217560
Iteration 84, loss = 0.02212823
Iteration 85, loss = 0.02211714
Iteration 86, loss = 0.02186379
Iteration 87, loss = 0.02195051
Iteration 88, loss = 0.02187100
Iteration 89, loss = 0.02195012
Iteration 90, loss = 0.02202460
Iteration 91, loss = 0.02160977
Iteration 92, loss = 0.02195945
Iteration 93, loss = 0.02182854
Iteration 94, loss = 0.02177622
Iteration 95, loss = 0.02161208
Iteration 96, loss = 0.02184112
Iteration 97, loss = 0.02157265
Iteration 98, loss = 0.02170321
Iteration 99, loss = 0.02159689
Iteration 100, loss = 0.02201776
Iteration 101, loss = 0.02171372
Iteration 102, loss = 0.02151261
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67855984
Iteration 2, loss = 0.65748608
Iteration 3, loss = 0.63214193
Iteration 4, loss = 0.59682992
Iteration 5, loss = 0.55220041
Iteration 6, loss = 0.50271742
Iteration 7, loss = 0.45262680
Iteration 8, loss = 0.40407365
Iteration 9, loss = 0.35874080
Iteration 10, loss = 0.31647492
Iteration 11, loss = 0.27853588
Iteration 12, loss = 0.24479298
Iteration 13, loss = 0.21537746
Iteration 14, loss = 0.19016513
Iteration 15, loss = 0.16817880
Iteration 16, loss = 0.14974549
Iteration 17, loss = 0.13384037
Iteration 18, loss = 0.12033794
Iteration 19, loss = 0.10884686
Iteration 20, loss = 0.09879996
Iteration 21, loss = 0.09032936
Iteration 22, loss = 0.08297657
Iteration 23, loss = 0.07639492
Iteration 24, loss = 0.07066839
Iteration 25, loss = 0.06569722
Iteration 26, loss = 0.06135877
Iteration 27, loss = 0.05754800
Iteration 28, loss = 0.05379514
Iteration 29, loss = 0.05105945
Iteration 30, loss = 0.04843546
Iteration 31, loss = 0.04580427
Iteration 32, loss = 0.04352554
Iteration 33, loss = 0.04141578
Iteration 34, loss = 0.03951163
Iteration 35, loss = 0.03770175
Iteration 36, loss = 0.03605362
Iteration 37, loss = 0.03463120
Iteration 38, loss = 0.03328126
Iteration 39, loss = 0.03196649
Iteration 40, loss = 0.03097204
Iteration 41, loss = 0.02989975
Iteration 42, loss = 0.02890019
Iteration 43, loss = 0.02802328
Iteration 44, loss = 0.02715811
Iteration 45, loss = 0.02636296
Iteration 46, loss = 0.02561169
Iteration 47, loss = 0.02487165
Iteration 48, loss = 0.02429745
Iteration 49, loss = 0.02365114
Iteration 50, loss = 0.02301999
Iteration 51, loss = 0.02245883
Iteration 52, loss = 0.02202756
Iteration 53, loss = 0.02153110
Iteration 54, loss = 0.02114566
Iteration 55, loss = 0.02079227
Iteration 56, loss = 0.02038501
Iteration 57, loss = 0.01999333
Iteration 58, loss = 0.01959883
Iteration 59, loss = 0.01925635
Iteration 60, loss = 0.01892325
Iteration 61, loss = 0.01884121
Iteration 62, loss = 0.01845509
Iteration 63, loss = 0.01798174
Iteration 64, loss = 0.01775620
Iteration 65, loss = 0.01751789
Iteration 66, loss = 0.01724506
Iteration 67, loss = 0.01720729
Iteration 68, loss = 0.01697200
Iteration 69, loss = 0.01672433
Iteration 70, loss = 0.01652492
Iteration 71, loss = 0.01631466
Iteration 72, loss = 0.01610598
Iteration 73, loss = 0.01576296
Iteration 74, loss = 0.01574287
Iteration 75, loss = 0.01567415
Iteration 76, loss = 0.01551104
Iteration 77, loss = 0.01523826
Iteration 78, loss = 0.01507924
Iteration 79, loss = 0.01496081
Iteration 80, loss = 0.01478809
Iteration 81, loss = 0.01478610
Iteration 82, loss = 0.01462292
Iteration 83, loss = 0.01452710
Iteration 84, loss = 0.01449830
Iteration 85, loss = 0.01429429
Iteration 86, loss = 0.01413637
Iteration 87, loss = 0.01399610
Iteration 88, loss = 0.01380739
Iteration 89, loss = 0.01375430
Iteration 90, loss = 0.01366332
Iteration 91, loss = 0.01357015
Iteration 92, loss = 0.01345653
Iteration 93, loss = 0.01340239
Iteration 94, loss = 0.01328063
Iteration 95, loss = 0.01325999
Iteration 96, loss = 0.01314426
Iteration 97, loss = 0.01311127
Iteration 98, loss = 0.01295462
Iteration 99, loss = 0.01294004
Iteration 100, loss = 0.01282497
Iteration 101, loss = 0.01275254
Iteration 102, loss = 0.01266380
Iteration 103, loss = 0.01263177
Iteration 104, loss = 0.01261609
Iteration 105, loss = 0.01265419
Iteration 106, loss = 0.01259757
Iteration 107, loss = 0.01248561
Iteration 108, loss = 0.01220040
Iteration 109, loss = 0.01219478
Iteration 110, loss = 0.01223552
Iteration 111, loss = 0.01213630
Iteration 112, loss = 0.01208337
Iteration 113, loss = 0.01188822
Iteration 114, loss = 0.01219391
Iteration 115, loss = 0.01222795
Iteration 116, loss = 0.01210772
Iteration 117, loss = 0.01203810
Iteration 118, loss = 0.01188410
Iteration 119, loss = 0.01181034
Iteration 120, loss = 0.01174198
Iteration 121, loss = 0.01167486
Iteration 122, loss = 0.01162851
Iteration 123, loss = 0.01162006
Iteration 124, loss = 0.01159649
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70170457
Iteration 2, loss = 0.62880203
Iteration 3, loss = 0.53119560
Iteration 4, loss = 0.41867566
Iteration 5, loss = 0.31952752
Iteration 6, loss = 0.24292307
Iteration 7, loss = 0.18853731
Iteration 8, loss = 0.14996635
Iteration 9, loss = 0.12278656
Iteration 10, loss = 0.10343896
Iteration 11, loss = 0.08929817
Iteration 12, loss = 0.07857320
Iteration 13, loss = 0.07040854
Iteration 14, loss = 0.06387144
Iteration 15, loss = 0.05872319
Iteration 16, loss = 0.05473371
Iteration 17, loss = 0.05105971
Iteration 18, loss = 0.04829368
Iteration 19, loss = 0.04594447
Iteration 20, loss = 0.04389629
Iteration 21, loss = 0.04211258
Iteration 22, loss = 0.04077488
Iteration 23, loss = 0.03918384
Iteration 24, loss = 0.03822878
Iteration 25, loss = 0.03745309
Iteration 26, loss = 0.03666187
Iteration 27, loss = 0.03548253
Iteration 28, loss = 0.03475641
Iteration 29, loss = 0.03403055
Iteration 30, loss = 0.03363457
Iteration 31, loss = 0.03295830
Iteration 32, loss = 0.03282170
Iteration 33, loss = 0.03249456
Iteration 34, loss = 0.03211800
Iteration 35, loss = 0.03158657
Iteration 36, loss = 0.03130339
Iteration 37, loss = 0.03087289
Iteration 38, loss = 0.03077448
Iteration 39, loss = 0.03039400
Iteration 40, loss = 0.03001451
Iteration 41, loss = 0.02989506
Iteration 42, loss = 0.02965978
Iteration 43, loss = 0.02944258
Iteration 44, loss = 0.02937034
Iteration 45, loss = 0.02941494
Iteration 46, loss = 0.02889092
Iteration 47, loss = 0.02908942
Iteration 48, loss = 0.02888822
Iteration 49, loss = 0.02870404
Iteration 50, loss = 0.02888202
Iteration 51, loss = 0.02837160
Iteration 52, loss = 0.02830336
Iteration 53, loss = 0.02825382
Iteration 54, loss = 0.02804516
Iteration 55, loss = 0.02824737
Iteration 56, loss = 0.02797254
Iteration 57, loss = 0.02792448
Iteration 58, loss = 0.02788095
Iteration 59, loss = 0.02810941
Iteration 60, loss = 0.02783457
Iteration 61, loss = 0.02779390
Iteration 62, loss = 0.02764117
Iteration 63, loss = 0.02771693
Iteration 64, loss = 0.02782607
Iteration 65, loss = 0.02762424
Iteration 66, loss = 0.02722260
Iteration 67, loss = 0.02745168
Iteration 68, loss = 0.02725393
Iteration 69, loss = 0.02725370
Iteration 70, loss = 0.02726506
Iteration 71, loss = 0.02710851
Iteration 72, loss = 0.02729628
Iteration 73, loss = 0.02733486
Iteration 74, loss = 0.02709200
Iteration 75, loss = 0.02710863
Iteration 76, loss = 0.02680082
Iteration 77, loss = 0.02690837
Iteration 78, loss = 0.02687387
Iteration 79, loss = 0.02676469
Iteration 80, loss = 0.02689678
Iteration 81, loss = 0.02673491
Iteration 82, loss = 0.02681265
Iteration 83, loss = 0.02670154
Iteration 84, loss = 0.02676017
Iteration 85, loss = 0.02665250
Iteration 86, loss = 0.02660300
Iteration 87, loss = 0.02652537
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68828372
Iteration 2, loss = 0.67414910
Iteration 3, loss = 0.66225684
Iteration 4, loss = 0.65041942
Iteration 5, loss = 0.63801695
Iteration 6, loss = 0.62464101
Iteration 7, loss = 0.61007538
Iteration 8, loss = 0.59395119
Iteration 9, loss = 0.57631824
Iteration 10, loss = 0.55768705
Iteration 11, loss = 0.53722657
Iteration 12, loss = 0.51560054
Iteration 13, loss = 0.49329703
Iteration 14, loss = 0.47009371
Iteration 15, loss = 0.44674011
Iteration 16, loss = 0.42342799
Iteration 17, loss = 0.40061521
Iteration 18, loss = 0.37837749
Iteration 19, loss = 0.35683165
Iteration 20, loss = 0.33642362
Iteration 21, loss = 0.31686691
Iteration 22, loss = 0.29828265
Iteration 23, loss = 0.28079774
Iteration 24, loss = 0.26443043
Iteration 25, loss = 0.24901050
Iteration 26, loss = 0.23445897
Iteration 27, loss = 0.22090924
Iteration 28, loss = 0.20828006
Iteration 29, loss = 0.19650095
Iteration 30, loss = 0.18542488
Iteration 31, loss = 0.17522552
Iteration 32, loss = 0.16567914
Iteration 33, loss = 0.15679070
Iteration 34, loss = 0.14851151
Iteration 35, loss = 0.14076004
Iteration 36, loss = 0.13360133
Iteration 37, loss = 0.12688022
Iteration 38, loss = 0.12065125
Iteration 39, loss = 0.11484035
Iteration 40, loss = 0.10946447
Iteration 41, loss = 0.10440106
Iteration 42, loss = 0.09971654
Iteration 43, loss = 0.09531547
Iteration 44, loss = 0.09114460
Iteration 45, loss = 0.08728984
Iteration 46, loss = 0.08361380
Iteration 47, loss = 0.08021218
Iteration 48, loss = 0.07703791
Iteration 49, loss = 0.07398486
Iteration 50, loss = 0.07123591
Iteration 51, loss = 0.06850459
Iteration 52, loss = 0.06602063
Iteration 53, loss = 0.06364272
Iteration 54, loss = 0.06146402
Iteration 55, loss = 0.05930864
Iteration 56, loss = 0.05733658
Iteration 57, loss = 0.05546011
Iteration 58, loss = 0.05369078
Iteration 59, loss = 0.05201200
Iteration 60, loss = 0.05041749
Iteration 61, loss = 0.04891232
Iteration 62, loss = 0.04748227
Iteration 63, loss = 0.04614941
Iteration 64, loss = 0.04485170
Iteration 65, loss = 0.04363932
Iteration 66, loss = 0.04248317
Iteration 67, loss = 0.04138107
Iteration 68, loss = 0.04030118
Iteration 69, loss = 0.03928362
Iteration 70, loss = 0.03831283
Iteration 71, loss = 0.03737992
Iteration 72, loss = 0.03649777
Iteration 73, loss = 0.03562920
Iteration 74, loss = 0.03485205
Iteration 75, loss = 0.03406732
Iteration 76, loss = 0.03338915
Iteration 77, loss = 0.03265490
Iteration 78, loss = 0.03197358
Iteration 79, loss = 0.03132542
Iteration 80, loss = 0.03072586
Iteration 81, loss = 0.03014400
Iteration 82, loss = 0.02954371
Iteration 83, loss = 0.02898250
Iteration 84, loss = 0.02846357
Iteration 85, loss = 0.02790392
Iteration 86, loss = 0.02741193
Iteration 87, loss = 0.02696059
Iteration 88, loss = 0.02647270
Iteration 89, loss = 0.02602176
Iteration 90, loss = 0.02563704
Iteration 91, loss = 0.02520107
Iteration 92, loss = 0.02482124
Iteration 93, loss = 0.02446910
Iteration 94, loss = 0.02407853
Iteration 95, loss = 0.02370099
Iteration 96, loss = 0.02336431
Iteration 97, loss = 0.02304301
Iteration 98, loss = 0.02268858
Iteration 99, loss = 0.02237498
Iteration 100, loss = 0.02207030
Iteration 101, loss = 0.02179186
Iteration 102, loss = 0.02147241
Iteration 103, loss = 0.02117661
Iteration 104, loss = 0.02090795
Iteration 105, loss = 0.02066135
Iteration 106, loss = 0.02038709
Iteration 107, loss = 0.02015020
Iteration 108, loss = 0.01991679
Iteration 109, loss = 0.01969748
Iteration 110, loss = 0.01943263
Iteration 111, loss = 0.01922313
Iteration 112, loss = 0.01899890
Iteration 113, loss = 0.01879581
Iteration 114, loss = 0.01863237
Iteration 115, loss = 0.01845801
Iteration 116, loss = 0.01828226
Iteration 117, loss = 0.01809506
Iteration 118, loss = 0.01792171
Iteration 119, loss = 0.01773543
Iteration 120, loss = 0.01759482
Iteration 121, loss = 0.01751900
Iteration 122, loss = 0.01733183
Iteration 123, loss = 0.01715243
Iteration 124, loss = 0.01701391
Iteration 125, loss = 0.01679457
Iteration 126, loss = 0.01664009
Iteration 127, loss = 0.01649144
Iteration 128, loss = 0.01633537
Iteration 129, loss = 0.01618921
Iteration 130, loss = 0.01604317
Iteration 131, loss = 0.01592995
Iteration 132, loss = 0.01576692
Iteration 133, loss = 0.01565441
Iteration 134, loss = 0.01549766
Iteration 135, loss = 0.01538178
Iteration 136, loss = 0.01526382
Iteration 137, loss = 0.01516968
Iteration 138, loss = 0.01504163
Iteration 139, loss = 0.01491222
Iteration 140, loss = 0.01480909
Iteration 141, loss = 0.01472657
Iteration 142, loss = 0.01465927
Iteration 143, loss = 0.01454144
Iteration 144, loss = 0.01440810
Iteration 145, loss = 0.01432056
Iteration 146, loss = 0.01422492
Iteration 147, loss = 0.01414261
Iteration 148, loss = 0.01409580
Iteration 149, loss = 0.01397365
Iteration 150, loss = 0.01388257
Iteration 151, loss = 0.01375890
Iteration 152, loss = 0.01377550
Iteration 153, loss = 0.01373351
Iteration 154, loss = 0.01356514
Iteration 155, loss = 0.01347704
Iteration 156, loss = 0.01342505
Iteration 157, loss = 0.01332242
Iteration 158, loss = 0.01325941
Iteration 159, loss = 0.01322737
Iteration 160, loss = 0.01310086
Iteration 161, loss = 0.01302630
Iteration 162, loss = 0.01300507
Iteration 163, loss = 0.01291690
Iteration 164, loss = 0.01285409
Iteration 165, loss = 0.01279592
Iteration 166, loss = 0.01271378
Iteration 167, loss = 0.01265897
Iteration 168, loss = 0.01260881
Iteration 169, loss = 0.01256689
Iteration 170, loss = 0.01251630
Iteration 171, loss = 0.01248706
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67560542
Iteration 2, loss = 0.59963297
Iteration 3, loss = 0.49133376
Iteration 4, loss = 0.38172890
Iteration 5, loss = 0.28883938
Iteration 6, loss = 0.21845970
Iteration 7, loss = 0.16863484
Iteration 8, loss = 0.13358776
Iteration 9, loss = 0.10930261
Iteration 10, loss = 0.09141338
Iteration 11, loss = 0.07872045
Iteration 12, loss = 0.06896408
Iteration 13, loss = 0.06135229
Iteration 14, loss = 0.05545140
Iteration 15, loss = 0.05059749
Iteration 16, loss = 0.04678274
Iteration 17, loss = 0.04378426
Iteration 18, loss = 0.04091965
Iteration 19, loss = 0.03874866
Iteration 20, loss = 0.03669838
Iteration 21, loss = 0.03532501
Iteration 22, loss = 0.03392534
Iteration 23, loss = 0.03258017
Iteration 24, loss = 0.03163262
Iteration 25, loss = 0.03072537
Iteration 26, loss = 0.02962987
Iteration 27, loss = 0.02931158
Iteration 28, loss = 0.02850082
Iteration 29, loss = 0.02782726
Iteration 30, loss = 0.02704560
Iteration 31, loss = 0.02685406
Iteration 32, loss = 0.02624791
Iteration 33, loss = 0.02592622
Iteration 34, loss = 0.02566484
Iteration 35, loss = 0.02533577
Iteration 36, loss = 0.02493784
Iteration 37, loss = 0.02476048
Iteration 38, loss = 0.02437730
Iteration 39, loss = 0.02419073
Iteration 40, loss = 0.02403220
Iteration 41, loss = 0.02385184
Iteration 42, loss = 0.02369577
Iteration 43, loss = 0.02344662
Iteration 44, loss = 0.02353898
Iteration 45, loss = 0.02321125
Iteration 46, loss = 0.02336335
Iteration 47, loss = 0.02312324
Iteration 48, loss = 0.02289857
Iteration 49, loss = 0.02295430
Iteration 50, loss = 0.02263113
Iteration 51, loss = 0.02266373
Iteration 52, loss = 0.02281731
Iteration 53, loss = 0.02222577
Iteration 54, loss = 0.02206043
Iteration 55, loss = 0.02236778
Iteration 56, loss = 0.02231889
Iteration 57, loss = 0.02216312
Iteration 58, loss = 0.02219011
Iteration 59, loss = 0.02190854
Iteration 60, loss = 0.02193792
Iteration 61, loss = 0.02181301
Iteration 62, loss = 0.02176089
Iteration 63, loss = 0.02187281
Iteration 64, loss = 0.02160783
Iteration 65, loss = 0.02170528
Iteration 66, loss = 0.02162503
Iteration 67, loss = 0.02156600
Iteration 68, loss = 0.02163836
Iteration 69, loss = 0.02139421
Iteration 70, loss = 0.02152631
Iteration 71, loss = 0.02157862
Iteration 72, loss = 0.02138508
Iteration 73, loss = 0.02151320
Iteration 74, loss = 0.02140605
Iteration 75, loss = 0.02123877
Iteration 76, loss = 0.02143444
Iteration 77, loss = 0.02128154
Iteration 78, loss = 0.02162984
Iteration 79, loss = 0.02141464
Iteration 80, loss = 0.02136642
Iteration 81, loss = 0.02155690
Iteration 82, loss = 0.02133487
Iteration 83, loss = 0.02105499
Iteration 84, loss = 0.02145933
Iteration 85, loss = 0.02128175
Iteration 86, loss = 0.02100696
Iteration 87, loss = 0.02130399
Iteration 88, loss = 0.02134249
Iteration 89, loss = 0.02073742
Iteration 90, loss = 0.02105019
Iteration 91, loss = 0.02111258
Iteration 92, loss = 0.02120651
Iteration 93, loss = 0.02113475
Iteration 94, loss = 0.02100947
Iteration 95, loss = 0.02127790
Iteration 96, loss = 0.02117292
Iteration 97, loss = 0.02127383
Iteration 98, loss = 0.02105813
Iteration 99, loss = 0.02113216
Iteration 100, loss = 0.02118045
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68262982
Iteration 2, loss = 0.65845768
Iteration 3, loss = 0.63279857
Iteration 4, loss = 0.59922137
Iteration 5, loss = 0.55735432
Iteration 6, loss = 0.50916028
Iteration 7, loss = 0.45942976
Iteration 8, loss = 0.41046778
Iteration 9, loss = 0.36392488
Iteration 10, loss = 0.32101277
Iteration 11, loss = 0.28224990
Iteration 12, loss = 0.24785910
Iteration 13, loss = 0.21795453
Iteration 14, loss = 0.19208521
Iteration 15, loss = 0.16989667
Iteration 16, loss = 0.15104438
Iteration 17, loss = 0.13487051
Iteration 18, loss = 0.12122005
Iteration 19, loss = 0.10943747
Iteration 20, loss = 0.09931122
Iteration 21, loss = 0.09077657
Iteration 22, loss = 0.08323419
Iteration 23, loss = 0.07679930
Iteration 24, loss = 0.07107634
Iteration 25, loss = 0.06614420
Iteration 26, loss = 0.06162060
Iteration 27, loss = 0.05781826
Iteration 28, loss = 0.05440485
Iteration 29, loss = 0.05131522
Iteration 30, loss = 0.04850719
Iteration 31, loss = 0.04612566
Iteration 32, loss = 0.04380667
Iteration 33, loss = 0.04172094
Iteration 34, loss = 0.03979618
Iteration 35, loss = 0.03820044
Iteration 36, loss = 0.03661862
Iteration 37, loss = 0.03495440
Iteration 38, loss = 0.03400675
Iteration 39, loss = 0.03285599
Iteration 40, loss = 0.03168900
Iteration 41, loss = 0.03062533
Iteration 42, loss = 0.02966634
Iteration 43, loss = 0.02870599
Iteration 44, loss = 0.02801343
Iteration 45, loss = 0.02718801
Iteration 46, loss = 0.02635552
Iteration 47, loss = 0.02586336
Iteration 48, loss = 0.02526698
Iteration 49, loss = 0.02459768
Iteration 50, loss = 0.02415687
Iteration 51, loss = 0.02356638
Iteration 52, loss = 0.02303974
Iteration 53, loss = 0.02253917
Iteration 54, loss = 0.02217729
Iteration 55, loss = 0.02167033
Iteration 56, loss = 0.02129896
Iteration 57, loss = 0.02086457
Iteration 58, loss = 0.02054831
Iteration 59, loss = 0.02019010
Iteration 60, loss = 0.01988113
Iteration 61, loss = 0.01954454
Iteration 62, loss = 0.01923285
Iteration 63, loss = 0.01901279
Iteration 64, loss = 0.01849995
Iteration 65, loss = 0.01884308
Iteration 66, loss = 0.01859341
Iteration 67, loss = 0.01829860
Iteration 68, loss = 0.01806227
Iteration 69, loss = 0.01778600
Iteration 70, loss = 0.01749075
Iteration 71, loss = 0.01724888
Iteration 72, loss = 0.01704670
Iteration 73, loss = 0.01698570
Iteration 74, loss = 0.01680789
Iteration 75, loss = 0.01668631
Iteration 76, loss = 0.01654724
Iteration 77, loss = 0.01637191
Iteration 78, loss = 0.01618071
Iteration 79, loss = 0.01600383
Iteration 80, loss = 0.01580455
Iteration 81, loss = 0.01562507
Iteration 82, loss = 0.01558248
Iteration 83, loss = 0.01541444
Iteration 84, loss = 0.01524114
Iteration 85, loss = 0.01525907
Iteration 86, loss = 0.01520706
Iteration 87, loss = 0.01541924
Iteration 88, loss = 0.01525144
Iteration 89, loss = 0.01503941
Iteration 90, loss = 0.01509141
Iteration 91, loss = 0.01485844
Iteration 92, loss = 0.01480684
Iteration 93, loss = 0.01473537
Iteration 94, loss = 0.01449044
Iteration 95, loss = 0.01432836
Iteration 96, loss = 0.01426758
Iteration 97, loss = 0.01413289
Iteration 98, loss = 0.01399504
Iteration 99, loss = 0.01389631
Iteration 100, loss = 0.01378212
Iteration 101, loss = 0.01376150
Iteration 102, loss = 0.01365665
Iteration 103, loss = 0.01409662
Iteration 104, loss = 0.01419540
Iteration 105, loss = 0.01411219
Iteration 106, loss = 0.01408371
Iteration 107, loss = 0.01386412
Iteration 108, loss = 0.01370865
Iteration 109, loss = 0.01363602
Iteration 110, loss = 0.01361906
Iteration 111, loss = 0.01346817
Iteration 112, loss = 0.01339411
Iteration 113, loss = 0.01335987
Iteration 114, loss = 0.01331081
Iteration 115, loss = 0.01341033
Iteration 116, loss = 0.01335687
Iteration 117, loss = 0.01329812
Iteration 118, loss = 0.01325090
Iteration 119, loss = 0.01308230
Iteration 120, loss = 0.01297911
Iteration 121, loss = 0.01289515
Iteration 122, loss = 0.01276225
Iteration 123, loss = 0.01272984
Iteration 124, loss = 0.01267098
Iteration 125, loss = 0.01263527
Iteration 126, loss = 0.01241819
Iteration 127, loss = 0.01271943
Iteration 128, loss = 0.01294394
Iteration 129, loss = 0.01290541
Iteration 130, loss = 0.01270654
Iteration 131, loss = 0.01259165
Iteration 132, loss = 0.01242664
Iteration 133, loss = 0.01238710
Iteration 134, loss = 0.01228677
Iteration 135, loss = 0.01235528
Iteration 136, loss = 0.01233030
Iteration 137, loss = 0.01220284
Iteration 138, loss = 0.01218930
Iteration 139, loss = 0.01216065
Iteration 140, loss = 0.01216684
Iteration 141, loss = 0.01219776
Iteration 142, loss = 0.01195254
Iteration 143, loss = 0.01226539
Iteration 144, loss = 0.01222547
Iteration 145, loss = 0.01214153
Iteration 146, loss = 0.01203916
Iteration 147, loss = 0.01200494
Iteration 148, loss = 0.01195788
Iteration 149, loss = 0.01202045
Iteration 150, loss = 0.01220526
Iteration 151, loss = 0.01218367
Iteration 152, loss = 0.01231656
Iteration 153, loss = 0.01231084
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71344479
Iteration 2, loss = 0.62648296
Iteration 3, loss = 0.51290552
Iteration 4, loss = 0.39493412
Iteration 5, loss = 0.29728626
Iteration 6, loss = 0.22402509
Iteration 7, loss = 0.17270177
Iteration 8, loss = 0.13755096
Iteration 9, loss = 0.11259784
Iteration 10, loss = 0.09543671
Iteration 11, loss = 0.08263671
Iteration 12, loss = 0.07299232
Iteration 13, loss = 0.06563726
Iteration 14, loss = 0.05983752
Iteration 15, loss = 0.05533936
Iteration 16, loss = 0.05139907
Iteration 17, loss = 0.04821121
Iteration 18, loss = 0.04583730
Iteration 19, loss = 0.04354330
Iteration 20, loss = 0.04163684
Iteration 21, loss = 0.04003100
Iteration 22, loss = 0.03904911
Iteration 23, loss = 0.03775501
Iteration 24, loss = 0.03696980
Iteration 25, loss = 0.03586805
Iteration 26, loss = 0.03491596
Iteration 27, loss = 0.03407363
Iteration 28, loss = 0.03361383
Iteration 29, loss = 0.03310955
Iteration 30, loss = 0.03251767
Iteration 31, loss = 0.03219447
Iteration 32, loss = 0.03180208
Iteration 33, loss = 0.03145719
Iteration 34, loss = 0.03101235
Iteration 35, loss = 0.03060613
Iteration 36, loss = 0.03028541
Iteration 37, loss = 0.03008028
Iteration 38, loss = 0.02994084
Iteration 39, loss = 0.02961472
Iteration 40, loss = 0.02934174
Iteration 41, loss = 0.02920989
Iteration 42, loss = 0.02916601
Iteration 43, loss = 0.02883346
Iteration 44, loss = 0.02885238
Iteration 45, loss = 0.02893382
Iteration 46, loss = 0.02863434
Iteration 47, loss = 0.02853885
Iteration 48, loss = 0.02830956
Iteration 49, loss = 0.02822888
Iteration 50, loss = 0.02796273
Iteration 51, loss = 0.02803896
Iteration 52, loss = 0.02800117
Iteration 53, loss = 0.02768750
Iteration 54, loss = 0.02780547
Iteration 55, loss = 0.02740464
Iteration 56, loss = 0.02750472
Iteration 57, loss = 0.02768158
Iteration 58, loss = 0.02743406
Iteration 59, loss = 0.02741749
Iteration 60, loss = 0.02743552
Iteration 61, loss = 0.02717237
Iteration 62, loss = 0.02714974
Iteration 63, loss = 0.02701191
Iteration 64, loss = 0.02703559
Iteration 65, loss = 0.02689852
Iteration 66, loss = 0.02708949
Iteration 67, loss = 0.02718772
Iteration 68, loss = 0.02722811
Iteration 69, loss = 0.02676906
Iteration 70, loss = 0.02675273
Iteration 71, loss = 0.02675816
Iteration 72, loss = 0.02689609
Iteration 73, loss = 0.02663109
Iteration 74, loss = 0.02660471
Iteration 75, loss = 0.02678444
Iteration 76, loss = 0.02669482
Iteration 77, loss = 0.02689941
Iteration 78, loss = 0.02665303
Iteration 79, loss = 0.02673315
Iteration 80, loss = 0.02670330
Iteration 81, loss = 0.02671420
Iteration 82, loss = 0.02666277
Iteration 83, loss = 0.02659300
Iteration 84, loss = 0.02649669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68246637
Iteration 2, loss = 0.66535123
Iteration 3, loss = 0.64834733
Iteration 4, loss = 0.62822230
Iteration 5, loss = 0.60377201
Iteration 6, loss = 0.57462709
Iteration 7, loss = 0.54157605
Iteration 8, loss = 0.50575908
Iteration 9, loss = 0.46884218
Iteration 10, loss = 0.43203837
Iteration 11, loss = 0.39623853
Iteration 12, loss = 0.36205130
Iteration 13, loss = 0.32997751
Iteration 14, loss = 0.30011271
Iteration 15, loss = 0.27259723
Iteration 16, loss = 0.24767195
Iteration 17, loss = 0.22471667
Iteration 18, loss = 0.20418801
Iteration 19, loss = 0.18590973
Iteration 20, loss = 0.16935848
Iteration 21, loss = 0.15463164
Iteration 22, loss = 0.14156304
Iteration 23, loss = 0.12992618
Iteration 24, loss = 0.11944675
Iteration 25, loss = 0.11022865
Iteration 26, loss = 0.10189963
Iteration 27, loss = 0.09459422
Iteration 28, loss = 0.08791466
Iteration 29, loss = 0.08206959
Iteration 30, loss = 0.07660836
Iteration 31, loss = 0.07182893
Iteration 32, loss = 0.06741454
Iteration 33, loss = 0.06350026
Iteration 34, loss = 0.05993708
Iteration 35, loss = 0.05662685
Iteration 36, loss = 0.05367921
Iteration 37, loss = 0.05092238
Iteration 38, loss = 0.04837288
Iteration 39, loss = 0.04611184
Iteration 40, loss = 0.04397516
Iteration 41, loss = 0.04197859
Iteration 42, loss = 0.04022843
Iteration 43, loss = 0.03851238
Iteration 44, loss = 0.03696934
Iteration 45, loss = 0.03546530
Iteration 46, loss = 0.03408660
Iteration 47, loss = 0.03289154
Iteration 48, loss = 0.03166568
Iteration 49, loss = 0.03056457
Iteration 50, loss = 0.02952307
Iteration 51, loss = 0.02855112
Iteration 52, loss = 0.02762576
Iteration 53, loss = 0.02679993
Iteration 54, loss = 0.02598390
Iteration 55, loss = 0.02526256
Iteration 56, loss = 0.02450611
Iteration 57, loss = 0.02382219
Iteration 58, loss = 0.02316103
Iteration 59, loss = 0.02253716
Iteration 60, loss = 0.02196407
Iteration 61, loss = 0.02142610
Iteration 62, loss = 0.02093635
Iteration 63, loss = 0.02043686
Iteration 64, loss = 0.01994391
Iteration 65, loss = 0.01952615
Iteration 66, loss = 0.01903704
Iteration 67, loss = 0.01869061
Iteration 68, loss = 0.01826713
Iteration 69, loss = 0.01788022
Iteration 70, loss = 0.01754515
Iteration 71, loss = 0.01717399
Iteration 72, loss = 0.01690590
Iteration 73, loss = 0.01656563
Iteration 74, loss = 0.01626048
Iteration 75, loss = 0.01598587
Iteration 76, loss = 0.01571652
Iteration 77, loss = 0.01543111
Iteration 78, loss = 0.01520200
Iteration 79, loss = 0.01495425
Iteration 80, loss = 0.01472286
Iteration 81, loss = 0.01451010
Iteration 82, loss = 0.01425058
Iteration 83, loss = 0.01412382
Iteration 84, loss = 0.01387385
Iteration 85, loss = 0.01366058
Iteration 86, loss = 0.01347128
Iteration 87, loss = 0.01329498
Iteration 88, loss = 0.01312781
Iteration 89, loss = 0.01298590
Iteration 90, loss = 0.01279308
Iteration 91, loss = 0.01266166
Iteration 92, loss = 0.01248176
Iteration 93, loss = 0.01231234
Iteration 94, loss = 0.01218973
Iteration 95, loss = 0.01207946
Iteration 96, loss = 0.01192118
Iteration 97, loss = 0.01180067
Iteration 98, loss = 0.01168680
Iteration 99, loss = 0.01157059
Iteration 100, loss = 0.01145421
Iteration 101, loss = 0.01134232
Iteration 102, loss = 0.01120588
Iteration 103, loss = 0.01111345
Iteration 104, loss = 0.01100727
Iteration 105, loss = 0.01091007
Iteration 106, loss = 0.01085165
Iteration 107, loss = 0.01071313
Iteration 108, loss = 0.01060995
Iteration 109, loss = 0.01051983
Iteration 110, loss = 0.01046604
Iteration 111, loss = 0.01038432
Iteration 112, loss = 0.01028394
Iteration 113, loss = 0.01023262
Iteration 114, loss = 0.01017087
Iteration 115, loss = 0.01005638
Iteration 116, loss = 0.00998437
Iteration 117, loss = 0.00990144
Iteration 118, loss = 0.00981190
Iteration 119, loss = 0.00976972
Iteration 120, loss = 0.00973729
Iteration 121, loss = 0.00968855
Iteration 122, loss = 0.00956091
Iteration 123, loss = 0.00954303
Iteration 124, loss = 0.00945300
Iteration 125, loss = 0.00939180
Iteration 126, loss = 0.00939746
Iteration 127, loss = 0.00934311
Iteration 128, loss = 0.00923563
Iteration 129, loss = 0.00920103
Iteration 130, loss = 0.00915474
Iteration 131, loss = 0.00909270
Iteration 132, loss = 0.00901158
Iteration 133, loss = 0.00898624
Iteration 134, loss = 0.00896659
Iteration 135, loss = 0.00891450
Iteration 136, loss = 0.00882283
Iteration 137, loss = 0.00883436
Iteration 138, loss = 0.00875379
Iteration 139, loss = 0.00877818
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67772306
Iteration 2, loss = 0.64213295
Iteration 3, loss = 0.59129251
Iteration 4, loss = 0.52632382
Iteration 5, loss = 0.45751196
Iteration 6, loss = 0.39089760
Iteration 7, loss = 0.33005055
Iteration 8, loss = 0.27649704
Iteration 9, loss = 0.23148767
Iteration 10, loss = 0.19452839
Iteration 11, loss = 0.16471620
Iteration 12, loss = 0.14068427
Iteration 13, loss = 0.12196268
Iteration 14, loss = 0.10655501
Iteration 15, loss = 0.09417377
Iteration 16, loss = 0.08416952
Iteration 17, loss = 0.07591792
Iteration 18, loss = 0.06898472
Iteration 19, loss = 0.06315959
Iteration 20, loss = 0.05831600
Iteration 21, loss = 0.05429739
Iteration 22, loss = 0.05060698
Iteration 23, loss = 0.04760447
Iteration 24, loss = 0.04493281
Iteration 25, loss = 0.04248998
Iteration 26, loss = 0.04068383
Iteration 27, loss = 0.03863254
Iteration 28, loss = 0.03702560
Iteration 29, loss = 0.03569270
Iteration 30, loss = 0.03428015
Iteration 31, loss = 0.03307826
Iteration 32, loss = 0.03196324
Iteration 33, loss = 0.03114227
Iteration 34, loss = 0.03018053
Iteration 35, loss = 0.02943050
Iteration 36, loss = 0.02873437
Iteration 37, loss = 0.02802538
Iteration 38, loss = 0.02747908
Iteration 39, loss = 0.02686039
Iteration 40, loss = 0.02639976
Iteration 41, loss = 0.02581681
Iteration 42, loss = 0.02538426
Iteration 43, loss = 0.02489973
Iteration 44, loss = 0.02454430
Iteration 45, loss = 0.02428799
Iteration 46, loss = 0.02385551
Iteration 47, loss = 0.02352083
Iteration 48, loss = 0.02345320
Iteration 49, loss = 0.02289560
Iteration 50, loss = 0.02268696
Iteration 51, loss = 0.02249864
Iteration 52, loss = 0.02235278
Iteration 53, loss = 0.02207610
Iteration 54, loss = 0.02191546
Iteration 55, loss = 0.02162648
Iteration 56, loss = 0.02152951
Iteration 57, loss = 0.02142472
Iteration 58, loss = 0.02126609
Iteration 59, loss = 0.02125870
Iteration 60, loss = 0.02098941
Iteration 61, loss = 0.02078553
Iteration 62, loss = 0.02061778
Iteration 63, loss = 0.02058583
Iteration 64, loss = 0.02028066
Iteration 65, loss = 0.02023383
Iteration 66, loss = 0.02011933
Iteration 67, loss = 0.02008717
Iteration 68, loss = 0.02000001
Iteration 69, loss = 0.01984079
Iteration 70, loss = 0.01984101
Iteration 71, loss = 0.01972980
Iteration 72, loss = 0.01978134
Iteration 73, loss = 0.01953216
Iteration 74, loss = 0.01949960
Iteration 75, loss = 0.01949806
Iteration 76, loss = 0.01935694
Iteration 77, loss = 0.01927818
Iteration 78, loss = 0.01940708
Iteration 79, loss = 0.01908363
Iteration 80, loss = 0.01903596
Iteration 81, loss = 0.01908513
Iteration 82, loss = 0.01897736
Iteration 83, loss = 0.01888754
Iteration 84, loss = 0.01880613
Iteration 85, loss = 0.01880075
Iteration 86, loss = 0.01870879
Iteration 87, loss = 0.01887800
Iteration 88, loss = 0.01869371
Iteration 89, loss = 0.01875781
Iteration 90, loss = 0.01843770
Iteration 91, loss = 0.01849680
Iteration 92, loss = 0.01852790
Iteration 93, loss = 0.01845611
Iteration 94, loss = 0.01857428
Iteration 95, loss = 0.01828860
Iteration 96, loss = 0.01841963
Iteration 97, loss = 0.01832652
Iteration 98, loss = 0.01840404
Iteration 99, loss = 0.01817953
Iteration 100, loss = 0.01813127
Iteration 101, loss = 0.01823915
Iteration 102, loss = 0.01841916
Iteration 103, loss = 0.01821820
Iteration 104, loss = 0.01806480
Iteration 105, loss = 0.01808618
Iteration 106, loss = 0.01798400
Iteration 107, loss = 0.01813406
Iteration 108, loss = 0.01805952
Iteration 109, loss = 0.01805057
Iteration 110, loss = 0.01803297
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67513463
Iteration 2, loss = 0.60649129
Iteration 3, loss = 0.50400238
Iteration 4, loss = 0.39584369
Iteration 5, loss = 0.30244707
Iteration 6, loss = 0.23022176
Iteration 7, loss = 0.17832526
Iteration 8, loss = 0.14162017
Iteration 9, loss = 0.11581622
Iteration 10, loss = 0.09733792
Iteration 11, loss = 0.08400386
Iteration 12, loss = 0.07352927
Iteration 13, loss = 0.06589561
Iteration 14, loss = 0.05950326
Iteration 15, loss = 0.05483232
Iteration 16, loss = 0.05099613
Iteration 17, loss = 0.04759756
Iteration 18, loss = 0.04485260
Iteration 19, loss = 0.04250648
Iteration 20, loss = 0.04053825
Iteration 21, loss = 0.03893466
Iteration 22, loss = 0.03769644
Iteration 23, loss = 0.03633709
Iteration 24, loss = 0.03545694
Iteration 25, loss = 0.03426434
Iteration 26, loss = 0.03338998
Iteration 27, loss = 0.03276698
Iteration 28, loss = 0.03240796
Iteration 29, loss = 0.03144849
Iteration 30, loss = 0.03100039
Iteration 31, loss = 0.03043183
Iteration 32, loss = 0.03014697
Iteration 33, loss = 0.02955576
Iteration 34, loss = 0.02928781
Iteration 35, loss = 0.02890024
Iteration 36, loss = 0.02860473
Iteration 37, loss = 0.02822901
Iteration 38, loss = 0.02805777
Iteration 39, loss = 0.02824472
Iteration 40, loss = 0.02775973
Iteration 41, loss = 0.02763014
Iteration 42, loss = 0.02727501
Iteration 43, loss = 0.02712118
Iteration 44, loss = 0.02726170
Iteration 45, loss = 0.02700070
Iteration 46, loss = 0.02660619
Iteration 47, loss = 0.02691320
Iteration 48, loss = 0.02648890
Iteration 49, loss = 0.02609862
Iteration 50, loss = 0.02634073
Iteration 51, loss = 0.02611192
Iteration 52, loss = 0.02609522
Iteration 53, loss = 0.02575868
Iteration 54, loss = 0.02579932
Iteration 55, loss = 0.02565806
Iteration 56, loss = 0.02569737
Iteration 57, loss = 0.02560578
Iteration 58, loss = 0.02577349
Iteration 59, loss = 0.02567678
Iteration 60, loss = 0.02568197
Iteration 61, loss = 0.02514703
Iteration 62, loss = 0.02548683
Iteration 63, loss = 0.02553075
Iteration 64, loss = 0.02505865
Iteration 65, loss = 0.02526827
Iteration 66, loss = 0.02548858
Iteration 67, loss = 0.02507080
Iteration 68, loss = 0.02501566
Iteration 69, loss = 0.02493669
Iteration 70, loss = 0.02505411
Iteration 71, loss = 0.02480616
Iteration 72, loss = 0.02510045
Iteration 73, loss = 0.02497311
Iteration 74, loss = 0.02466994
Iteration 75, loss = 0.02492981
Iteration 76, loss = 0.02447989
Iteration 77, loss = 0.02491508
Iteration 78, loss = 0.02493346
Iteration 79, loss = 0.02475158
Iteration 80, loss = 0.02475059
Iteration 81, loss = 0.02459035
Iteration 82, loss = 0.02472529
Iteration 83, loss = 0.02447154
Iteration 84, loss = 0.02471894
Iteration 85, loss = 0.02450680
Iteration 86, loss = 0.02453841
Iteration 87, loss = 0.02479067
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68760004
Iteration 2, loss = 0.64413282
Iteration 3, loss = 0.58804265
Iteration 4, loss = 0.51882857
Iteration 5, loss = 0.44636095
Iteration 6, loss = 0.37834997
Iteration 7, loss = 0.31674998
Iteration 8, loss = 0.26374239
Iteration 9, loss = 0.22012276
Iteration 10, loss = 0.18507564
Iteration 11, loss = 0.15728196
Iteration 12, loss = 0.13523385
Iteration 13, loss = 0.11754322
Iteration 14, loss = 0.10361687
Iteration 15, loss = 0.09208744
Iteration 16, loss = 0.08265578
Iteration 17, loss = 0.07504506
Iteration 18, loss = 0.06851948
Iteration 19, loss = 0.06306388
Iteration 20, loss = 0.05848697
Iteration 21, loss = 0.05439040
Iteration 22, loss = 0.05105370
Iteration 23, loss = 0.04820531
Iteration 24, loss = 0.04566860
Iteration 25, loss = 0.04317865
Iteration 26, loss = 0.04123118
Iteration 27, loss = 0.03956847
Iteration 28, loss = 0.03789872
Iteration 29, loss = 0.03649148
Iteration 30, loss = 0.03517663
Iteration 31, loss = 0.03387701
Iteration 32, loss = 0.03302281
Iteration 33, loss = 0.03204926
Iteration 34, loss = 0.03114621
Iteration 35, loss = 0.03038118
Iteration 36, loss = 0.02957468
Iteration 37, loss = 0.02903761
Iteration 38, loss = 0.02831612
Iteration 39, loss = 0.02772689
Iteration 40, loss = 0.02720956
Iteration 41, loss = 0.02693150
Iteration 42, loss = 0.02627048
Iteration 43, loss = 0.02578726
Iteration 44, loss = 0.02544595
Iteration 45, loss = 0.02526577
Iteration 46, loss = 0.02480937
Iteration 47, loss = 0.02463098
Iteration 48, loss = 0.02426805
Iteration 49, loss = 0.02389705
Iteration 50, loss = 0.02372713
Iteration 51, loss = 0.02328687
Iteration 52, loss = 0.02308431
Iteration 53, loss = 0.02293393
Iteration 54, loss = 0.02258046
Iteration 55, loss = 0.02269113
Iteration 56, loss = 0.02232077
Iteration 57, loss = 0.02230299
Iteration 58, loss = 0.02193660
Iteration 59, loss = 0.02185728
Iteration 60, loss = 0.02175081
Iteration 61, loss = 0.02156748
Iteration 62, loss = 0.02139082
Iteration 63, loss = 0.02116523
Iteration 64, loss = 0.02127970
Iteration 65, loss = 0.02102204
Iteration 66, loss = 0.02083714
Iteration 67, loss = 0.02079036
Iteration 68, loss = 0.02064425
Iteration 69, loss = 0.02050072
Iteration 70, loss = 0.02042670
Iteration 71, loss = 0.02026409
Iteration 72, loss = 0.02029408
Iteration 73, loss = 0.02016788
Iteration 74, loss = 0.02012559
Iteration 75, loss = 0.02012593
Iteration 76, loss = 0.02010109
Iteration 77, loss = 0.01988110
Iteration 78, loss = 0.01986147
Iteration 79, loss = 0.01977050
Iteration 80, loss = 0.01974413
Iteration 81, loss = 0.01977723
Iteration 82, loss = 0.01970570
Iteration 83, loss = 0.01961833
Iteration 84, loss = 0.01954243
Iteration 85, loss = 0.01948955
Iteration 86, loss = 0.01934609
Iteration 87, loss = 0.01953455
Iteration 88, loss = 0.01943841
Iteration 89, loss = 0.01912260
Iteration 90, loss = 0.01933144
Iteration 91, loss = 0.01951894
Iteration 92, loss = 0.01911038
Iteration 93, loss = 0.01917363
Iteration 94, loss = 0.01898159
Iteration 95, loss = 0.01893848
Iteration 96, loss = 0.01907885
Iteration 97, loss = 0.01889060
Iteration 98, loss = 0.01898196
Iteration 99, loss = 0.01890028
Iteration 100, loss = 0.01886305
Iteration 101, loss = 0.01881474
Iteration 102, loss = 0.01861717
Iteration 103, loss = 0.01860468
Iteration 104, loss = 0.01870333
Iteration 105, loss = 0.01878045
Iteration 106, loss = 0.01876791
Iteration 107, loss = 0.01868577
Iteration 108, loss = 0.01865195
Iteration 109, loss = 0.01860410
Iteration 110, loss = 0.01870639
Iteration 111, loss = 0.01880816
Iteration 112, loss = 0.01846553
Iteration 113, loss = 0.01853926
Iteration 114, loss = 0.01841252
Iteration 115, loss = 0.01844770
Iteration 116, loss = 0.01858704
Iteration 117, loss = 0.01836228
Iteration 118, loss = 0.01845071
Iteration 119, loss = 0.01845600
Iteration 120, loss = 0.01837182
Iteration 121, loss = 0.01826993
Iteration 122, loss = 0.01848302
Iteration 123, loss = 0.01818422
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67732206
Iteration 2, loss = 0.61805039
Iteration 3, loss = 0.53020940
Iteration 4, loss = 0.42887731
Iteration 5, loss = 0.33502815
Iteration 6, loss = 0.25887292
Iteration 7, loss = 0.20191118
Iteration 8, loss = 0.16077594
Iteration 9, loss = 0.13094437
Iteration 10, loss = 0.10962850
Iteration 11, loss = 0.09386211
Iteration 12, loss = 0.08181340
Iteration 13, loss = 0.07301747
Iteration 14, loss = 0.06565112
Iteration 15, loss = 0.05965206
Iteration 16, loss = 0.05511619
Iteration 17, loss = 0.05095950
Iteration 18, loss = 0.04798813
Iteration 19, loss = 0.04522239
Iteration 20, loss = 0.04312400
Iteration 21, loss = 0.04078058
Iteration 22, loss = 0.03921641
Iteration 23, loss = 0.03805512
Iteration 24, loss = 0.03706290
Iteration 25, loss = 0.03580176
Iteration 26, loss = 0.03502736
Iteration 27, loss = 0.03401958
Iteration 28, loss = 0.03295640
Iteration 29, loss = 0.03234171
Iteration 30, loss = 0.03150565
Iteration 31, loss = 0.03082254
Iteration 32, loss = 0.03032454
Iteration 33, loss = 0.02986937
Iteration 34, loss = 0.02962102
Iteration 35, loss = 0.02883339
Iteration 36, loss = 0.02895776
Iteration 37, loss = 0.02891541
Iteration 38, loss = 0.02835142
Iteration 39, loss = 0.02804028
Iteration 40, loss = 0.02772430
Iteration 41, loss = 0.02737937
Iteration 42, loss = 0.02693613
Iteration 43, loss = 0.02693872
Iteration 44, loss = 0.02665686
Iteration 45, loss = 0.02662049
Iteration 46, loss = 0.02636308
Iteration 47, loss = 0.02664513
Iteration 48, loss = 0.02703579
Iteration 49, loss = 0.02685243
Iteration 50, loss = 0.02652946
Iteration 51, loss = 0.02643558
Iteration 52, loss = 0.02615137
Iteration 53, loss = 0.02620522
Iteration 54, loss = 0.02601791
Iteration 55, loss = 0.02583133
Iteration 56, loss = 0.02547226
Iteration 57, loss = 0.02531394
Iteration 58, loss = 0.02512851
Iteration 59, loss = 0.02461372
Iteration 60, loss = 0.02496075
Iteration 61, loss = 0.02449730
Iteration 62, loss = 0.02451468
Iteration 63, loss = 0.02431855
Iteration 64, loss = 0.02450755
Iteration 65, loss = 0.02430034
Iteration 66, loss = 0.02462183
Iteration 67, loss = 0.02457314
Iteration 68, loss = 0.02445204
Iteration 69, loss = 0.02412324
Iteration 70, loss = 0.02426651
Iteration 71, loss = 0.02457519
Iteration 72, loss = 0.02434311
Iteration 73, loss = 0.02441068
Iteration 74, loss = 0.02393518
Iteration 75, loss = 0.02380269
Iteration 76, loss = 0.02464649
Iteration 77, loss = 0.02436623
Iteration 78, loss = 0.02402484
Iteration 79, loss = 0.02379943
Iteration 80, loss = 0.02375044
Iteration 81, loss = 0.02381802
Iteration 82, loss = 0.02388832
Iteration 83, loss = 0.02376479
Iteration 84, loss = 0.02350529
Iteration 85, loss = 0.02348603
Iteration 86, loss = 0.02329198
Iteration 87, loss = 0.02341482
Iteration 88, loss = 0.02360278
Iteration 89, loss = 0.02368432
Iteration 90, loss = 0.02351178
Iteration 91, loss = 0.02315599
Iteration 92, loss = 0.02348870
Iteration 93, loss = 0.02318549
Iteration 94, loss = 0.02333125
Iteration 95, loss = 0.02337456
Iteration 96, loss = 0.02324279
Iteration 97, loss = 0.02314392
Iteration 98, loss = 0.02343558
Iteration 99, loss = 0.02372266
Iteration 100, loss = 0.02359309
Iteration 101, loss = 0.02330246
Iteration 102, loss = 0.02308648
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67508927
Iteration 2, loss = 0.63469966
Iteration 3, loss = 0.57379459
Iteration 4, loss = 0.49610303
Iteration 5, loss = 0.41629820
Iteration 6, loss = 0.34220698
Iteration 7, loss = 0.27852650
Iteration 8, loss = 0.22640372
Iteration 9, loss = 0.18552985
Iteration 10, loss = 0.15383450
Iteration 11, loss = 0.12949012
Iteration 12, loss = 0.11066736
Iteration 13, loss = 0.09579918
Iteration 14, loss = 0.08434959
Iteration 15, loss = 0.07492173
Iteration 16, loss = 0.06743698
Iteration 17, loss = 0.06128082
Iteration 18, loss = 0.05593394
Iteration 19, loss = 0.05164427
Iteration 20, loss = 0.04802041
Iteration 21, loss = 0.04470259
Iteration 22, loss = 0.04208154
Iteration 23, loss = 0.03978264
Iteration 24, loss = 0.03767622
Iteration 25, loss = 0.03607371
Iteration 26, loss = 0.03422861
Iteration 27, loss = 0.03301666
Iteration 28, loss = 0.03169897
Iteration 29, loss = 0.03066736
Iteration 30, loss = 0.02960193
Iteration 31, loss = 0.02869884
Iteration 32, loss = 0.02778312
Iteration 33, loss = 0.02707877
Iteration 34, loss = 0.02635259
Iteration 35, loss = 0.02583496
Iteration 36, loss = 0.02523395
Iteration 37, loss = 0.02472076
Iteration 38, loss = 0.02423929
Iteration 39, loss = 0.02401801
Iteration 40, loss = 0.02344026
Iteration 41, loss = 0.02302378
Iteration 42, loss = 0.02292825
Iteration 43, loss = 0.02233524
Iteration 44, loss = 0.02223933
Iteration 45, loss = 0.02168262
Iteration 46, loss = 0.02173373
Iteration 47, loss = 0.02152216
Iteration 48, loss = 0.02151215
Iteration 49, loss = 0.02096263
Iteration 50, loss = 0.02072574
Iteration 51, loss = 0.02032801
Iteration 52, loss = 0.02056288
Iteration 53, loss = 0.02025295
Iteration 54, loss = 0.01983531
Iteration 55, loss = 0.01980495
Iteration 56, loss = 0.01966633
Iteration 57, loss = 0.01942298
Iteration 58, loss = 0.01929688
Iteration 59, loss = 0.01920969
Iteration 60, loss = 0.01903626
Iteration 61, loss = 0.01894615
Iteration 62, loss = 0.01892678
Iteration 63, loss = 0.01893929
Iteration 64, loss = 0.01880449
Iteration 65, loss = 0.01873030
Iteration 66, loss = 0.01856357
Iteration 67, loss = 0.01850081
Iteration 68, loss = 0.01829481
Iteration 69, loss = 0.01819910
Iteration 70, loss = 0.01840294
Iteration 71, loss = 0.01860680
Iteration 72, loss = 0.01827773
Iteration 73, loss = 0.01827182
Iteration 74, loss = 0.01814995
Iteration 75, loss = 0.01800123
Iteration 76, loss = 0.01797170
Iteration 77, loss = 0.01789560
Iteration 78, loss = 0.01784124
Iteration 79, loss = 0.01780296
Iteration 80, loss = 0.01779514
Iteration 81, loss = 0.01790585
Iteration 82, loss = 0.01764700
Iteration 83, loss = 0.01777147
Iteration 84, loss = 0.01760718
Iteration 85, loss = 0.01741207
Iteration 86, loss = 0.01741617
Iteration 87, loss = 0.01746495
Iteration 88, loss = 0.01738860
Iteration 89, loss = 0.01729987
Iteration 90, loss = 0.01733084
Iteration 91, loss = 0.01740147
Iteration 92, loss = 0.01723629
Iteration 93, loss = 0.01727616
Iteration 94, loss = 0.01704540
Iteration 95, loss = 0.01710671
Iteration 96, loss = 0.01712304
Iteration 97, loss = 0.01687785
Iteration 98, loss = 0.01724574
Iteration 99, loss = 0.01693045
Iteration 100, loss = 0.01680270
Iteration 101, loss = 0.01732790
Iteration 102, loss = 0.01726771
Iteration 103, loss = 0.01704177
Iteration 104, loss = 0.01688489
Iteration 105, loss = 0.01691916
Iteration 106, loss = 0.01685376
Iteration 107, loss = 0.01668988
Iteration 108, loss = 0.01673222
Iteration 109, loss = 0.01691196
Iteration 110, loss = 0.01665243
Iteration 111, loss = 0.01652769
Iteration 112, loss = 0.01679419
Iteration 113, loss = 0.01677835
Iteration 114, loss = 0.01690596
Iteration 115, loss = 0.01681688
Iteration 116, loss = 0.01680856
Iteration 117, loss = 0.01669242
Iteration 118, loss = 0.01671189
Iteration 119, loss = 0.01664746
Iteration 120, loss = 0.01665988
Iteration 121, loss = 0.01658311
Iteration 122, loss = 0.01646194
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67649403
Iteration 2, loss = 0.60638375
Iteration 3, loss = 0.49579000
Iteration 4, loss = 0.37918750
Iteration 5, loss = 0.28310871
Iteration 6, loss = 0.21256069
Iteration 7, loss = 0.16336938
Iteration 8, loss = 0.12938768
Iteration 9, loss = 0.10597056
Iteration 10, loss = 0.08952967
Iteration 11, loss = 0.07720230
Iteration 12, loss = 0.06802843
Iteration 13, loss = 0.06098315
Iteration 14, loss = 0.05547193
Iteration 15, loss = 0.05140103
Iteration 16, loss = 0.04752923
Iteration 17, loss = 0.04483270
Iteration 18, loss = 0.04231496
Iteration 19, loss = 0.04015428
Iteration 20, loss = 0.03858060
Iteration 21, loss = 0.03716439
Iteration 22, loss = 0.03595572
Iteration 23, loss = 0.03479964
Iteration 24, loss = 0.03423200
Iteration 25, loss = 0.03304104
Iteration 26, loss = 0.03219841
Iteration 27, loss = 0.03190147
Iteration 28, loss = 0.03115494
Iteration 29, loss = 0.03065052
Iteration 30, loss = 0.03009080
Iteration 31, loss = 0.02970060
Iteration 32, loss = 0.02949610
Iteration 33, loss = 0.02909731
Iteration 34, loss = 0.02865435
Iteration 35, loss = 0.02826563
Iteration 36, loss = 0.02840494
Iteration 37, loss = 0.02781709
Iteration 38, loss = 0.02768390
Iteration 39, loss = 0.02748284
Iteration 40, loss = 0.02754605
Iteration 41, loss = 0.02711226
Iteration 42, loss = 0.02711908
Iteration 43, loss = 0.02689613
Iteration 44, loss = 0.02654894
Iteration 45, loss = 0.02648326
Iteration 46, loss = 0.02632889
Iteration 47, loss = 0.02674300
Iteration 48, loss = 0.02619785
Iteration 49, loss = 0.02614466
Iteration 50, loss = 0.02602372
Iteration 51, loss = 0.02600088
Iteration 52, loss = 0.02602582
Iteration 53, loss = 0.02568432
Iteration 54, loss = 0.02574801
Iteration 55, loss = 0.02550370
Iteration 56, loss = 0.02547440
Iteration 57, loss = 0.02557375
Iteration 58, loss = 0.02541002
Iteration 59, loss = 0.02540253
Iteration 60, loss = 0.02551373
Iteration 61, loss = 0.02528112
Iteration 62, loss = 0.02519235
Iteration 63, loss = 0.02534048
Iteration 64, loss = 0.02507853
Iteration 65, loss = 0.02527149
Iteration 66, loss = 0.02522337
Iteration 67, loss = 0.02528636
Iteration 68, loss = 0.02503386
Iteration 69, loss = 0.02511384
Iteration 70, loss = 0.02491895
Iteration 71, loss = 0.02480431
Iteration 72, loss = 0.02487017
Iteration 73, loss = 0.02497760
Iteration 74, loss = 0.02495883
Iteration 75, loss = 0.02441552
Iteration 76, loss = 0.02489389
Iteration 77, loss = 0.02530833
Iteration 78, loss = 0.02470506
Iteration 79, loss = 0.02462885
Iteration 80, loss = 0.02488840
Iteration 81, loss = 0.02456429
Iteration 82, loss = 0.02467718
Iteration 83, loss = 0.02442178
Iteration 84, loss = 0.02467711
Iteration 85, loss = 0.02456757
Iteration 86, loss = 0.02445084
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71194494
Iteration 2, loss = 0.63902591
Iteration 3, loss = 0.53483972
Iteration 4, loss = 0.41434458
Iteration 5, loss = 0.31027718
Iteration 6, loss = 0.23124662
Iteration 7, loss = 0.17599634
Iteration 8, loss = 0.13824366
Iteration 9, loss = 0.11229118
Iteration 10, loss = 0.09393097
Iteration 11, loss = 0.08070765
Iteration 12, loss = 0.07049534
Iteration 13, loss = 0.06324414
Iteration 14, loss = 0.05743047
Iteration 15, loss = 0.05241036
Iteration 16, loss = 0.04858767
Iteration 17, loss = 0.04547988
Iteration 18, loss = 0.04291796
Iteration 19, loss = 0.04060969
Iteration 20, loss = 0.03869806
Iteration 21, loss = 0.03724984
Iteration 22, loss = 0.03560813
Iteration 23, loss = 0.03445235
Iteration 24, loss = 0.03338961
Iteration 25, loss = 0.03265393
Iteration 26, loss = 0.03156471
Iteration 27, loss = 0.03111817
Iteration 28, loss = 0.03053871
Iteration 29, loss = 0.02983588
Iteration 30, loss = 0.02954530
Iteration 31, loss = 0.02898053
Iteration 32, loss = 0.02859418
Iteration 33, loss = 0.02815124
Iteration 34, loss = 0.02781557
Iteration 35, loss = 0.02755912
Iteration 36, loss = 0.02714707
Iteration 37, loss = 0.02696033
Iteration 38, loss = 0.02663330
Iteration 39, loss = 0.02623897
Iteration 40, loss = 0.02614035
Iteration 41, loss = 0.02608353
Iteration 42, loss = 0.02574195
Iteration 43, loss = 0.02603555
Iteration 44, loss = 0.02538181
Iteration 45, loss = 0.02550012
Iteration 46, loss = 0.02522750
Iteration 47, loss = 0.02509911
Iteration 48, loss = 0.02531983
Iteration 49, loss = 0.02507082
Iteration 50, loss = 0.02490148
Iteration 51, loss = 0.02480220
Iteration 52, loss = 0.02481086
Iteration 53, loss = 0.02467328
Iteration 54, loss = 0.02435619
Iteration 55, loss = 0.02475128
Iteration 56, loss = 0.02440454
Iteration 57, loss = 0.02433656
Iteration 58, loss = 0.02453995
Iteration 59, loss = 0.02433533
Iteration 60, loss = 0.02421523
Iteration 61, loss = 0.02430745
Iteration 62, loss = 0.02413646
Iteration 63, loss = 0.02416284
Iteration 64, loss = 0.02394277
Iteration 65, loss = 0.02385362
Iteration 66, loss = 0.02395791
Iteration 67, loss = 0.02383168
Iteration 68, loss = 0.02379078
Iteration 69, loss = 0.02352122
Iteration 70, loss = 0.02378056
Iteration 71, loss = 0.02369058
Iteration 72, loss = 0.02377006
Iteration 73, loss = 0.02388359
Iteration 74, loss = 0.02374188
Iteration 75, loss = 0.02363055
Iteration 76, loss = 0.02397735
Iteration 77, loss = 0.02364423
Iteration 78, loss = 0.02371522
Iteration 79, loss = 0.02345714
Iteration 80, loss = 0.02322666
Iteration 81, loss = 0.02335757
Iteration 82, loss = 0.02327068
Iteration 83, loss = 0.02350278
Iteration 84, loss = 0.02359802
Iteration 85, loss = 0.02373784
Iteration 86, loss = 0.02318464
Iteration 87, loss = 0.02329183
Iteration 88, loss = 0.02322280
Iteration 89, loss = 0.02329027
Iteration 90, loss = 0.02309162
Iteration 91, loss = 0.02308025
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	103m55.627s
user	738m5.182s
sys	79m56.233s
Elapsed time: 6236 seconds
