Processing dataset: kept_v9_minimal_processing

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: kept_v9_minimal_processing (7438 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on kept_v9_minimal_processing...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on kept_v9_minimal_processing...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on kept_v9_minimal_processing...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on kept_v9_minimal_processing...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on kept_v9_minimal_processing...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.68246365
Iteration 2, loss = 0.60946758
Iteration 3, loss = 0.49768665
Iteration 4, loss = 0.37917446
Iteration 5, loss = 0.27954870
Iteration 6, loss = 0.20631599
Iteration 7, loss = 0.15562472
Iteration 8, loss = 0.12131405
Iteration 9, loss = 0.09771473
Iteration 10, loss = 0.08119452
Iteration 11, loss = 0.06914403
Iteration 12, loss = 0.06029790
Iteration 13, loss = 0.05336413
Iteration 14, loss = 0.04810580
Iteration 15, loss = 0.04384155
Iteration 16, loss = 0.04027687
Iteration 17, loss = 0.03759316
Iteration 18, loss = 0.03526276
Iteration 19, loss = 0.03324412
Iteration 20, loss = 0.03151730
Iteration 21, loss = 0.03020431
Iteration 22, loss = 0.02908287
Iteration 23, loss = 0.02785099
Iteration 24, loss = 0.02694514
Iteration 25, loss = 0.02602608
Iteration 26, loss = 0.02526414
Iteration 27, loss = 0.02461151
Iteration 28, loss = 0.02408344
Iteration 29, loss = 0.02377637
Iteration 30, loss = 0.02323337
Iteration 31, loss = 0.02280388
Iteration 32, loss = 0.02228674
Iteration 33, loss = 0.02186385
Iteration 34, loss = 0.02159825
Iteration 35, loss = 0.02136369
Iteration 36, loss = 0.02099890
Iteration 37, loss = 0.02084528
Iteration 38, loss = 0.02048418
Iteration 39, loss = 0.02041307
Iteration 40, loss = 0.02006391
Iteration 41, loss = 0.02009817
Iteration 42, loss = 0.01982673
Iteration 43, loss = 0.01977124
Iteration 44, loss = 0.01949526
Iteration 45, loss = 0.01941586
Iteration 46, loss = 0.01921972
Iteration 47, loss = 0.01910510
Iteration 48, loss = 0.01899651
Iteration 49, loss = 0.01889786
Iteration 50, loss = 0.01894694
Iteration 51, loss = 0.01891708
Iteration 52, loss = 0.01879842
Iteration 53, loss = 0.01841323
Iteration 54, loss = 0.01842636
Iteration 55, loss = 0.01847819
Iteration 56, loss = 0.01815047
Iteration 57, loss = 0.01835566
Iteration 58, loss = 0.01812811
Iteration 59, loss = 0.01832493
Iteration 60, loss = 0.01803506
Iteration 61, loss = 0.01797012
Iteration 62, loss = 0.01821939
Iteration 63, loss = 0.01787893
Iteration 64, loss = 0.01804663
Iteration 65, loss = 0.01775173
Iteration 66, loss = 0.01801663
Iteration 67, loss = 0.01792864
Iteration 68, loss = 0.01770251
Iteration 69, loss = 0.01774461
Iteration 70, loss = 0.01762280
Iteration 71, loss = 0.01753874
Iteration 72, loss = 0.01753891
Iteration 73, loss = 0.01756642
Iteration 74, loss = 0.01741981
Iteration 75, loss = 0.01755670
Iteration 76, loss = 0.01755153
Iteration 77, loss = 0.01757131
Iteration 78, loss = 0.01737333
Iteration 79, loss = 0.01762763
Iteration 80, loss = 0.01753479
Iteration 81, loss = 0.01708576
Iteration 82, loss = 0.01744026
Iteration 83, loss = 0.01746223
Iteration 84, loss = 0.01727588
Iteration 85, loss = 0.01739263
Iteration 86, loss = 0.01713007
Iteration 87, loss = 0.01714669
Iteration 88, loss = 0.01717416
Iteration 89, loss = 0.01711769
Iteration 90, loss = 0.01708038
Iteration 91, loss = 0.01705029
Iteration 92, loss = 0.01704195
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67563584
Iteration 2, loss = 0.60249492
Iteration 3, loss = 0.49194480
Iteration 4, loss = 0.37581459
Iteration 5, loss = 0.27651334
Iteration 6, loss = 0.20306921
Iteration 7, loss = 0.15249575
Iteration 8, loss = 0.11835911
Iteration 9, loss = 0.09516190
Iteration 10, loss = 0.07897409
Iteration 11, loss = 0.06693911
Iteration 12, loss = 0.05840636
Iteration 13, loss = 0.05182861
Iteration 14, loss = 0.04623437
Iteration 15, loss = 0.04227546
Iteration 16, loss = 0.03903404
Iteration 17, loss = 0.03618223
Iteration 18, loss = 0.03415764
Iteration 19, loss = 0.03220974
Iteration 20, loss = 0.03078992
Iteration 21, loss = 0.02960813
Iteration 22, loss = 0.02832929
Iteration 23, loss = 0.02725528
Iteration 24, loss = 0.02655259
Iteration 25, loss = 0.02564147
Iteration 26, loss = 0.02484178
Iteration 27, loss = 0.02436147
Iteration 28, loss = 0.02372437
Iteration 29, loss = 0.02324001
Iteration 30, loss = 0.02306021
Iteration 31, loss = 0.02250757
Iteration 32, loss = 0.02228227
Iteration 33, loss = 0.02184717
Iteration 34, loss = 0.02166385
Iteration 35, loss = 0.02132961
Iteration 36, loss = 0.02126437
Iteration 37, loss = 0.02082919
Iteration 38, loss = 0.02061296
Iteration 39, loss = 0.02042754
Iteration 40, loss = 0.02045764
Iteration 41, loss = 0.02012426
Iteration 42, loss = 0.01994782
Iteration 43, loss = 0.01988130
Iteration 44, loss = 0.01968470
Iteration 45, loss = 0.01960514
Iteration 46, loss = 0.01957780
Iteration 47, loss = 0.01950840
Iteration 48, loss = 0.01949666
Iteration 49, loss = 0.01945269
Iteration 50, loss = 0.01908893
Iteration 51, loss = 0.01889083
Iteration 52, loss = 0.01911401
Iteration 53, loss = 0.01909609
Iteration 54, loss = 0.01894392
Iteration 55, loss = 0.01884335
Iteration 56, loss = 0.01866289
Iteration 57, loss = 0.01855358
Iteration 58, loss = 0.01879418
Iteration 59, loss = 0.01859304
Iteration 60, loss = 0.01839246
Iteration 61, loss = 0.01869717
Iteration 62, loss = 0.01841784
Iteration 63, loss = 0.01838324
Iteration 64, loss = 0.01843286
Iteration 65, loss = 0.01813788
Iteration 66, loss = 0.01830391
Iteration 67, loss = 0.01840620
Iteration 68, loss = 0.01806410
Iteration 69, loss = 0.01826108
Iteration 70, loss = 0.01804980
Iteration 71, loss = 0.01821281
Iteration 72, loss = 0.01800516
Iteration 73, loss = 0.01802064
Iteration 74, loss = 0.01786396
Iteration 75, loss = 0.01783772
Iteration 76, loss = 0.01796564
Iteration 77, loss = 0.01795603
Iteration 78, loss = 0.01769763
Iteration 79, loss = 0.01786312
Iteration 80, loss = 0.01801626
Iteration 81, loss = 0.01787506
Iteration 82, loss = 0.01774838
Iteration 83, loss = 0.01783137
Iteration 84, loss = 0.01778604
Iteration 85, loss = 0.01781857
Iteration 86, loss = 0.01763311
Iteration 87, loss = 0.01777790
Iteration 88, loss = 0.01764581
Iteration 89, loss = 0.01759437
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70647779
Iteration 2, loss = 0.63637193
Iteration 3, loss = 0.52743514
Iteration 4, loss = 0.40213759
Iteration 5, loss = 0.29545827
Iteration 6, loss = 0.21697809
Iteration 7, loss = 0.16274628
Iteration 8, loss = 0.12640774
Iteration 9, loss = 0.10153933
Iteration 10, loss = 0.08393178
Iteration 11, loss = 0.07109848
Iteration 12, loss = 0.06177389
Iteration 13, loss = 0.05456631
Iteration 14, loss = 0.04906587
Iteration 15, loss = 0.04471583
Iteration 16, loss = 0.04097761
Iteration 17, loss = 0.03817698
Iteration 18, loss = 0.03540410
Iteration 19, loss = 0.03346476
Iteration 20, loss = 0.03179684
Iteration 21, loss = 0.03021033
Iteration 22, loss = 0.02910203
Iteration 23, loss = 0.02788961
Iteration 24, loss = 0.02732795
Iteration 25, loss = 0.02611062
Iteration 26, loss = 0.02544298
Iteration 27, loss = 0.02479442
Iteration 28, loss = 0.02417645
Iteration 29, loss = 0.02367587
Iteration 30, loss = 0.02330948
Iteration 31, loss = 0.02280217
Iteration 32, loss = 0.02233300
Iteration 33, loss = 0.02196103
Iteration 34, loss = 0.02168911
Iteration 35, loss = 0.02137552
Iteration 36, loss = 0.02104855
Iteration 37, loss = 0.02092295
Iteration 38, loss = 0.02054258
Iteration 39, loss = 0.02055994
Iteration 40, loss = 0.02032993
Iteration 41, loss = 0.02016261
Iteration 42, loss = 0.02027345
Iteration 43, loss = 0.01974572
Iteration 44, loss = 0.01946213
Iteration 45, loss = 0.01960230
Iteration 46, loss = 0.01941747
Iteration 47, loss = 0.01917517
Iteration 48, loss = 0.01917161
Iteration 49, loss = 0.01923710
Iteration 50, loss = 0.01883150
Iteration 51, loss = 0.01874252
Iteration 52, loss = 0.01868388
Iteration 53, loss = 0.01868787
Iteration 54, loss = 0.01868094
Iteration 55, loss = 0.01836931
Iteration 56, loss = 0.01830179
Iteration 57, loss = 0.01841156
Iteration 58, loss = 0.01836947
Iteration 59, loss = 0.01837311
Iteration 60, loss = 0.01831709
Iteration 61, loss = 0.01829553
Iteration 62, loss = 0.01800228
Iteration 63, loss = 0.01812131
Iteration 64, loss = 0.01779035
Iteration 65, loss = 0.01804888
Iteration 66, loss = 0.01803854
Iteration 67, loss = 0.01814149
Iteration 68, loss = 0.01779508
Iteration 69, loss = 0.01782534
Iteration 70, loss = 0.01779864
Iteration 71, loss = 0.01819914
Iteration 72, loss = 0.01802311
Iteration 73, loss = 0.01787036
Iteration 74, loss = 0.01758202
Iteration 75, loss = 0.01787349
Iteration 76, loss = 0.01760465
Iteration 77, loss = 0.01767726
Iteration 78, loss = 0.01755278
Iteration 79, loss = 0.01751303
Iteration 80, loss = 0.01723152
Iteration 81, loss = 0.01740460
Iteration 82, loss = 0.01748153
Iteration 83, loss = 0.01744603
Iteration 84, loss = 0.01738904
Iteration 85, loss = 0.01730927
Iteration 86, loss = 0.01740185
Iteration 87, loss = 0.01740385
Iteration 88, loss = 0.01735555
Iteration 89, loss = 0.01748506
Iteration 90, loss = 0.01724702
Iteration 91, loss = 0.01707729
Iteration 92, loss = 0.01734822
Iteration 93, loss = 0.01736757
Iteration 94, loss = 0.01730963
Iteration 95, loss = 0.01726698
Iteration 96, loss = 0.01735017
Iteration 97, loss = 0.01702444
Iteration 98, loss = 0.01707293
Iteration 99, loss = 0.01711914
Iteration 100, loss = 0.01700964
Iteration 101, loss = 0.01714489
Iteration 102, loss = 0.01741308
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67484458
Iteration 2, loss = 0.59943914
Iteration 3, loss = 0.48623126
Iteration 4, loss = 0.37221400
Iteration 5, loss = 0.27640818
Iteration 6, loss = 0.20498622
Iteration 7, loss = 0.15518526
Iteration 8, loss = 0.12086503
Iteration 9, loss = 0.09747160
Iteration 10, loss = 0.08084687
Iteration 11, loss = 0.06892419
Iteration 12, loss = 0.05989945
Iteration 13, loss = 0.05315600
Iteration 14, loss = 0.04792819
Iteration 15, loss = 0.04352109
Iteration 16, loss = 0.04006157
Iteration 17, loss = 0.03745177
Iteration 18, loss = 0.03501025
Iteration 19, loss = 0.03300077
Iteration 20, loss = 0.03147217
Iteration 21, loss = 0.02994191
Iteration 22, loss = 0.02877617
Iteration 23, loss = 0.02769240
Iteration 24, loss = 0.02687405
Iteration 25, loss = 0.02612389
Iteration 26, loss = 0.02554952
Iteration 27, loss = 0.02472827
Iteration 28, loss = 0.02414826
Iteration 29, loss = 0.02362725
Iteration 30, loss = 0.02338560
Iteration 31, loss = 0.02279426
Iteration 32, loss = 0.02255770
Iteration 33, loss = 0.02230716
Iteration 34, loss = 0.02176374
Iteration 35, loss = 0.02145117
Iteration 36, loss = 0.02130437
Iteration 37, loss = 0.02112398
Iteration 38, loss = 0.02120896
Iteration 39, loss = 0.02084337
Iteration 40, loss = 0.02057242
Iteration 41, loss = 0.02030255
Iteration 42, loss = 0.02024528
Iteration 43, loss = 0.02006230
Iteration 44, loss = 0.01970598
Iteration 45, loss = 0.01983273
Iteration 46, loss = 0.01962982
Iteration 47, loss = 0.01971920
Iteration 48, loss = 0.01926698
Iteration 49, loss = 0.01946102
Iteration 50, loss = 0.01938022
Iteration 51, loss = 0.01911109
Iteration 52, loss = 0.01917599
Iteration 53, loss = 0.01906358
Iteration 54, loss = 0.01897777
Iteration 55, loss = 0.01876598
Iteration 56, loss = 0.01898965
Iteration 57, loss = 0.01893342
Iteration 58, loss = 0.01861545
Iteration 59, loss = 0.01856576
Iteration 60, loss = 0.01836307
Iteration 61, loss = 0.01861334
Iteration 62, loss = 0.01837125
Iteration 63, loss = 0.01853158
Iteration 64, loss = 0.01853349
Iteration 65, loss = 0.01856338
Iteration 66, loss = 0.01818889
Iteration 67, loss = 0.01815745
Iteration 68, loss = 0.01836034
Iteration 69, loss = 0.01824065
Iteration 70, loss = 0.01825627
Iteration 71, loss = 0.01807595
Iteration 72, loss = 0.01809156
Iteration 73, loss = 0.01818349
Iteration 74, loss = 0.01817536
Iteration 75, loss = 0.01807094
Iteration 76, loss = 0.01786624
Iteration 77, loss = 0.01797215
Iteration 78, loss = 0.01779221
Iteration 79, loss = 0.01793010
Iteration 80, loss = 0.01798402
Iteration 81, loss = 0.01773542
Iteration 82, loss = 0.01765498
Iteration 83, loss = 0.01765082
Iteration 84, loss = 0.01776306
Iteration 85, loss = 0.01776865
Iteration 86, loss = 0.01779366
Iteration 87, loss = 0.01769929
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67392689
Iteration 2, loss = 0.59846677
Iteration 3, loss = 0.49135069
Iteration 4, loss = 0.38200426
Iteration 5, loss = 0.28722686
Iteration 6, loss = 0.21461563
Iteration 7, loss = 0.16283147
Iteration 8, loss = 0.12748404
Iteration 9, loss = 0.10264125
Iteration 10, loss = 0.08531316
Iteration 11, loss = 0.07271010
Iteration 12, loss = 0.06318023
Iteration 13, loss = 0.05617287
Iteration 14, loss = 0.05048216
Iteration 15, loss = 0.04600968
Iteration 16, loss = 0.04264709
Iteration 17, loss = 0.03965729
Iteration 18, loss = 0.03716467
Iteration 19, loss = 0.03496706
Iteration 20, loss = 0.03341143
Iteration 21, loss = 0.03180388
Iteration 22, loss = 0.03072207
Iteration 23, loss = 0.02963224
Iteration 24, loss = 0.02860113
Iteration 25, loss = 0.02786209
Iteration 26, loss = 0.02737533
Iteration 27, loss = 0.02625420
Iteration 28, loss = 0.02579795
Iteration 29, loss = 0.02524360
Iteration 30, loss = 0.02467882
Iteration 31, loss = 0.02440371
Iteration 32, loss = 0.02414085
Iteration 33, loss = 0.02390862
Iteration 34, loss = 0.02335000
Iteration 35, loss = 0.02300942
Iteration 36, loss = 0.02284521
Iteration 37, loss = 0.02247647
Iteration 38, loss = 0.02240036
Iteration 39, loss = 0.02214574
Iteration 40, loss = 0.02203367
Iteration 41, loss = 0.02182916
Iteration 42, loss = 0.02154576
Iteration 43, loss = 0.02145997
Iteration 44, loss = 0.02132119
Iteration 45, loss = 0.02131785
Iteration 46, loss = 0.02102410
Iteration 47, loss = 0.02113091
Iteration 48, loss = 0.02071127
Iteration 49, loss = 0.02084057
Iteration 50, loss = 0.02102618
Iteration 51, loss = 0.02046546
Iteration 52, loss = 0.02048032
Iteration 53, loss = 0.02036835
Iteration 54, loss = 0.02025459
Iteration 55, loss = 0.02020517
Iteration 56, loss = 0.02022767
Iteration 57, loss = 0.01996106
Iteration 58, loss = 0.02018033
Iteration 59, loss = 0.01978087
Iteration 60, loss = 0.01998141
Iteration 61, loss = 0.01979866
Iteration 62, loss = 0.01984978
Iteration 63, loss = 0.01966037
Iteration 64, loss = 0.01979912
Iteration 65, loss = 0.01969022
Iteration 66, loss = 0.01957066
Iteration 67, loss = 0.01961943
Iteration 68, loss = 0.01941489
Iteration 69, loss = 0.01962516
Iteration 70, loss = 0.01934948
Iteration 71, loss = 0.01970877
Iteration 72, loss = 0.01961244
Iteration 73, loss = 0.01926611
Iteration 74, loss = 0.01957601
Iteration 75, loss = 0.01928340
Iteration 76, loss = 0.01932912
Iteration 77, loss = 0.01927892
Iteration 78, loss = 0.01945404
Iteration 79, loss = 0.01931569
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68246365
Iteration 2, loss = 0.60946758
Iteration 3, loss = 0.49768665
Iteration 4, loss = 0.37917446
Iteration 5, loss = 0.27954870
Iteration 6, loss = 0.20631599
Iteration 7, loss = 0.15562472
Iteration 8, loss = 0.12131405
Iteration 9, loss = 0.09771473
Iteration 10, loss = 0.08119452
Iteration 11, loss = 0.06914403
Iteration 12, loss = 0.06029790
Iteration 13, loss = 0.05336413
Iteration 14, loss = 0.04810580
Iteration 15, loss = 0.04384155
Iteration 16, loss = 0.04027687
Iteration 17, loss = 0.03759316
Iteration 18, loss = 0.03526276
Iteration 19, loss = 0.03324412
Iteration 20, loss = 0.03151730
Iteration 21, loss = 0.03020431
Iteration 22, loss = 0.02908287
Iteration 23, loss = 0.02785099
Iteration 24, loss = 0.02694514
Iteration 25, loss = 0.02602608
Iteration 26, loss = 0.02526414
Iteration 27, loss = 0.02461151
Iteration 28, loss = 0.02408344
Iteration 29, loss = 0.02377637
Iteration 30, loss = 0.02323337
Iteration 31, loss = 0.02280388
Iteration 32, loss = 0.02228674
Iteration 33, loss = 0.02186385
Iteration 34, loss = 0.02159825
Iteration 35, loss = 0.02136369
Iteration 36, loss = 0.02099890
Iteration 37, loss = 0.02084528
Iteration 38, loss = 0.02048418
Iteration 39, loss = 0.02041307
Iteration 40, loss = 0.02006391
Iteration 41, loss = 0.02009817
Iteration 42, loss = 0.01982673
Iteration 43, loss = 0.01977124
Iteration 44, loss = 0.01949526
Iteration 45, loss = 0.01941586
Iteration 46, loss = 0.01921972
Iteration 47, loss = 0.01910510
Iteration 48, loss = 0.01899651
Iteration 49, loss = 0.01889786
Iteration 50, loss = 0.01894694
Iteration 51, loss = 0.01891708
Iteration 52, loss = 0.01879842
Iteration 53, loss = 0.01841323
Iteration 54, loss = 0.01842636
Iteration 55, loss = 0.01847819
Iteration 56, loss = 0.01815047
Iteration 57, loss = 0.01835566
Iteration 58, loss = 0.01812811
Iteration 59, loss = 0.01832493
Iteration 60, loss = 0.01803506
Iteration 61, loss = 0.01797012
Iteration 62, loss = 0.01821939
Iteration 63, loss = 0.01787893
Iteration 64, loss = 0.01804663
Iteration 65, loss = 0.01775173
Iteration 66, loss = 0.01801663
Iteration 67, loss = 0.01792864
Iteration 68, loss = 0.01770251
Iteration 69, loss = 0.01774461
Iteration 70, loss = 0.01762280
Iteration 71, loss = 0.01753874
Iteration 72, loss = 0.01753891
Iteration 73, loss = 0.01756642
Iteration 74, loss = 0.01741981
Iteration 75, loss = 0.01755670
Iteration 76, loss = 0.01755153
Iteration 77, loss = 0.01757131
Iteration 78, loss = 0.01737333
Iteration 79, loss = 0.01762763
Iteration 80, loss = 0.01753479
Iteration 81, loss = 0.01708576
Iteration 82, loss = 0.01744026
Iteration 83, loss = 0.01746223
Iteration 84, loss = 0.01727588
Iteration 85, loss = 0.01739263
Iteration 86, loss = 0.01713007
Iteration 87, loss = 0.01714669
Iteration 88, loss = 0.01717416
Iteration 89, loss = 0.01711769
Iteration 90, loss = 0.01708038
Iteration 91, loss = 0.01705029
Iteration 92, loss = 0.01704195
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67563584
Iteration 2, loss = 0.60249492
Iteration 3, loss = 0.49194480
Iteration 4, loss = 0.37581459
Iteration 5, loss = 0.27651334
Iteration 6, loss = 0.20306921
Iteration 7, loss = 0.15249575
Iteration 8, loss = 0.11835911
Iteration 9, loss = 0.09516190
Iteration 10, loss = 0.07897409
Iteration 11, loss = 0.06693911
Iteration 12, loss = 0.05840636
Iteration 13, loss = 0.05182861
Iteration 14, loss = 0.04623437
Iteration 15, loss = 0.04227546
Iteration 16, loss = 0.03903404
Iteration 17, loss = 0.03618223
Iteration 18, loss = 0.03415764
Iteration 19, loss = 0.03220974
Iteration 20, loss = 0.03078992
Iteration 21, loss = 0.02960813
Iteration 22, loss = 0.02832929
Iteration 23, loss = 0.02725528
Iteration 24, loss = 0.02655259
Iteration 25, loss = 0.02564147
Iteration 26, loss = 0.02484178
Iteration 27, loss = 0.02436147
Iteration 28, loss = 0.02372437
Iteration 29, loss = 0.02324001
Iteration 30, loss = 0.02306021
Iteration 31, loss = 0.02250757
Iteration 32, loss = 0.02228227
Iteration 33, loss = 0.02184717
Iteration 34, loss = 0.02166385
Iteration 35, loss = 0.02132961
Iteration 36, loss = 0.02126437
Iteration 37, loss = 0.02082919
Iteration 38, loss = 0.02061296
Iteration 39, loss = 0.02042754
Iteration 40, loss = 0.02045764
Iteration 41, loss = 0.02012426
Iteration 42, loss = 0.01994782
Iteration 43, loss = 0.01988130
Iteration 44, loss = 0.01968470
Iteration 45, loss = 0.01960514
Iteration 46, loss = 0.01957780
Iteration 47, loss = 0.01950840
Iteration 48, loss = 0.01949666
Iteration 49, loss = 0.01945269
Iteration 50, loss = 0.01908893
Iteration 51, loss = 0.01889083
Iteration 52, loss = 0.01911401
Iteration 53, loss = 0.01909609
Iteration 54, loss = 0.01894392
Iteration 55, loss = 0.01884335
Iteration 56, loss = 0.01866289
Iteration 57, loss = 0.01855358
Iteration 58, loss = 0.01879418
Iteration 59, loss = 0.01859304
Iteration 60, loss = 0.01839246
Iteration 61, loss = 0.01869717
Iteration 62, loss = 0.01841784
Iteration 63, loss = 0.01838324
Iteration 64, loss = 0.01843286
Iteration 65, loss = 0.01813788
Iteration 66, loss = 0.01830391
Iteration 67, loss = 0.01840620
Iteration 68, loss = 0.01806410
Iteration 69, loss = 0.01826108
Iteration 70, loss = 0.01804980
Iteration 71, loss = 0.01821281
Iteration 72, loss = 0.01800516
Iteration 73, loss = 0.01802064
Iteration 74, loss = 0.01786396
Iteration 75, loss = 0.01783772
Iteration 76, loss = 0.01796564
Iteration 77, loss = 0.01795603
Iteration 78, loss = 0.01769763
Iteration 79, loss = 0.01786312
Iteration 80, loss = 0.01801626
Iteration 81, loss = 0.01787506
Iteration 82, loss = 0.01774838
Iteration 83, loss = 0.01783137
Iteration 84, loss = 0.01778604
Iteration 85, loss = 0.01781857
Iteration 86, loss = 0.01763311
Iteration 87, loss = 0.01777790
Iteration 88, loss = 0.01764581
Iteration 89, loss = 0.01759437
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70647779
Iteration 2, loss = 0.63637193
Iteration 3, loss = 0.52743514
Iteration 4, loss = 0.40213759
Iteration 5, loss = 0.29545827
Iteration 6, loss = 0.21697809
Iteration 7, loss = 0.16274628
Iteration 8, loss = 0.12640774
Iteration 9, loss = 0.10153933
Iteration 10, loss = 0.08393178
Iteration 11, loss = 0.07109848
Iteration 12, loss = 0.06177389
Iteration 13, loss = 0.05456631
Iteration 14, loss = 0.04906587
Iteration 15, loss = 0.04471583
Iteration 16, loss = 0.04097761
Iteration 17, loss = 0.03817698
Iteration 18, loss = 0.03540410
Iteration 19, loss = 0.03346476
Iteration 20, loss = 0.03179684
Iteration 21, loss = 0.03021033
Iteration 22, loss = 0.02910203
Iteration 23, loss = 0.02788961
Iteration 24, loss = 0.02732795
Iteration 25, loss = 0.02611062
Iteration 26, loss = 0.02544298
Iteration 27, loss = 0.02479442
Iteration 28, loss = 0.02417645
Iteration 29, loss = 0.02367587
Iteration 30, loss = 0.02330948
Iteration 31, loss = 0.02280217
Iteration 32, loss = 0.02233300
Iteration 33, loss = 0.02196103
Iteration 34, loss = 0.02168911
Iteration 35, loss = 0.02137552
Iteration 36, loss = 0.02104855
Iteration 37, loss = 0.02092295
Iteration 38, loss = 0.02054258
Iteration 39, loss = 0.02055994
Iteration 40, loss = 0.02032993
Iteration 41, loss = 0.02016261
Iteration 42, loss = 0.02027345
Iteration 43, loss = 0.01974572
Iteration 44, loss = 0.01946213
Iteration 45, loss = 0.01960230
Iteration 46, loss = 0.01941747
Iteration 47, loss = 0.01917517
Iteration 48, loss = 0.01917161
Iteration 49, loss = 0.01923710
Iteration 50, loss = 0.01883150
Iteration 51, loss = 0.01874252
Iteration 52, loss = 0.01868388
Iteration 53, loss = 0.01868787
Iteration 54, loss = 0.01868094
Iteration 55, loss = 0.01836931
Iteration 56, loss = 0.01830179
Iteration 57, loss = 0.01841156
Iteration 58, loss = 0.01836947
Iteration 59, loss = 0.01837311
Iteration 60, loss = 0.01831709
Iteration 61, loss = 0.01829553
Iteration 62, loss = 0.01800228
Iteration 63, loss = 0.01812131
Iteration 64, loss = 0.01779035
Iteration 65, loss = 0.01804888
Iteration 66, loss = 0.01803854
Iteration 67, loss = 0.01814149
Iteration 68, loss = 0.01779508
Iteration 69, loss = 0.01782534
Iteration 70, loss = 0.01779864
Iteration 71, loss = 0.01819914
Iteration 72, loss = 0.01802311
Iteration 73, loss = 0.01787036
Iteration 74, loss = 0.01758202
Iteration 75, loss = 0.01787349
Iteration 76, loss = 0.01760465
Iteration 77, loss = 0.01767726
Iteration 78, loss = 0.01755278
Iteration 79, loss = 0.01751303
Iteration 80, loss = 0.01723152
Iteration 81, loss = 0.01740460
Iteration 82, loss = 0.01748153
Iteration 83, loss = 0.01744603
Iteration 84, loss = 0.01738904
Iteration 85, loss = 0.01730927
Iteration 86, loss = 0.01740185
Iteration 87, loss = 0.01740385
Iteration 88, loss = 0.01735555
Iteration 89, loss = 0.01748506
Iteration 90, loss = 0.01724702
Iteration 91, loss = 0.01707729
Iteration 92, loss = 0.01734822
Iteration 93, loss = 0.01736757
Iteration 94, loss = 0.01730963
Iteration 95, loss = 0.01726698
Iteration 96, loss = 0.01735017
Iteration 97, loss = 0.01702444
Iteration 98, loss = 0.01707293
Iteration 99, loss = 0.01711914
Iteration 100, loss = 0.01700964
Iteration 101, loss = 0.01714489
Iteration 102, loss = 0.01741308
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67484458
Iteration 2, loss = 0.59943914
Iteration 3, loss = 0.48623126
Iteration 4, loss = 0.37221400
Iteration 5, loss = 0.27640818
Iteration 6, loss = 0.20498622
Iteration 7, loss = 0.15518526
Iteration 8, loss = 0.12086503
Iteration 9, loss = 0.09747160
Iteration 10, loss = 0.08084687
Iteration 11, loss = 0.06892419
Iteration 12, loss = 0.05989945
Iteration 13, loss = 0.05315600
Iteration 14, loss = 0.04792819
Iteration 15, loss = 0.04352109
Iteration 16, loss = 0.04006157
Iteration 17, loss = 0.03745177
Iteration 18, loss = 0.03501025
Iteration 19, loss = 0.03300077
Iteration 20, loss = 0.03147217
Iteration 21, loss = 0.02994191
Iteration 22, loss = 0.02877617
Iteration 23, loss = 0.02769240
Iteration 24, loss = 0.02687405
Iteration 25, loss = 0.02612389
Iteration 26, loss = 0.02554952
Iteration 27, loss = 0.02472827
Iteration 28, loss = 0.02414826
Iteration 29, loss = 0.02362725
Iteration 30, loss = 0.02338560
Iteration 31, loss = 0.02279426
Iteration 32, loss = 0.02255770
Iteration 33, loss = 0.02230716
Iteration 34, loss = 0.02176374
Iteration 35, loss = 0.02145117
Iteration 36, loss = 0.02130437
Iteration 37, loss = 0.02112398
Iteration 38, loss = 0.02120896
Iteration 39, loss = 0.02084337
Iteration 40, loss = 0.02057242
Iteration 41, loss = 0.02030255
Iteration 42, loss = 0.02024528
Iteration 43, loss = 0.02006230
Iteration 44, loss = 0.01970598
Iteration 45, loss = 0.01983273
Iteration 46, loss = 0.01962982
Iteration 47, loss = 0.01971920
Iteration 48, loss = 0.01926698
Iteration 49, loss = 0.01946102
Iteration 50, loss = 0.01938022
Iteration 51, loss = 0.01911109
Iteration 52, loss = 0.01917599
Iteration 53, loss = 0.01906358
Iteration 54, loss = 0.01897777
Iteration 55, loss = 0.01876598
Iteration 56, loss = 0.01898965
Iteration 57, loss = 0.01893342
Iteration 58, loss = 0.01861545
Iteration 59, loss = 0.01856576
Iteration 60, loss = 0.01836307
Iteration 61, loss = 0.01861334
Iteration 62, loss = 0.01837125
Iteration 63, loss = 0.01853158
Iteration 64, loss = 0.01853349
Iteration 65, loss = 0.01856338
Iteration 66, loss = 0.01818889
Iteration 67, loss = 0.01815745
Iteration 68, loss = 0.01836034
Iteration 69, loss = 0.01824065
Iteration 70, loss = 0.01825627
Iteration 71, loss = 0.01807595
Iteration 72, loss = 0.01809156
Iteration 73, loss = 0.01818349
Iteration 74, loss = 0.01817536
Iteration 75, loss = 0.01807094
Iteration 76, loss = 0.01786624
Iteration 77, loss = 0.01797215
Iteration 78, loss = 0.01779221
Iteration 79, loss = 0.01793010
Iteration 80, loss = 0.01798402
Iteration 81, loss = 0.01773542
Iteration 82, loss = 0.01765498
Iteration 83, loss = 0.01765082
Iteration 84, loss = 0.01776306
Iteration 85, loss = 0.01776865
Iteration 86, loss = 0.01779366
Iteration 87, loss = 0.01769929
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67392689
Iteration 2, loss = 0.59846677
Iteration 3, loss = 0.49135069
Iteration 4, loss = 0.38200426
Iteration 5, loss = 0.28722686
Iteration 6, loss = 0.21461563
Iteration 7, loss = 0.16283147
Iteration 8, loss = 0.12748404
Iteration 9, loss = 0.10264125
Iteration 10, loss = 0.08531316
Iteration 11, loss = 0.07271010
Iteration 12, loss = 0.06318023
Iteration 13, loss = 0.05617287
Iteration 14, loss = 0.05048216
Iteration 15, loss = 0.04600968
Iteration 16, loss = 0.04264709
Iteration 17, loss = 0.03965729
Iteration 18, loss = 0.03716467
Iteration 19, loss = 0.03496706
Iteration 20, loss = 0.03341143
Iteration 21, loss = 0.03180388
Iteration 22, loss = 0.03072207
Iteration 23, loss = 0.02963224
Iteration 24, loss = 0.02860113
Iteration 25, loss = 0.02786209
Iteration 26, loss = 0.02737533
Iteration 27, loss = 0.02625420
Iteration 28, loss = 0.02579795
Iteration 29, loss = 0.02524360
Iteration 30, loss = 0.02467882
Iteration 31, loss = 0.02440371
Iteration 32, loss = 0.02414085
Iteration 33, loss = 0.02390862
Iteration 34, loss = 0.02335000
Iteration 35, loss = 0.02300942
Iteration 36, loss = 0.02284521
Iteration 37, loss = 0.02247647
Iteration 38, loss = 0.02240036
Iteration 39, loss = 0.02214574
Iteration 40, loss = 0.02203367
Iteration 41, loss = 0.02182916
Iteration 42, loss = 0.02154576
Iteration 43, loss = 0.02145997
Iteration 44, loss = 0.02132119
Iteration 45, loss = 0.02131785
Iteration 46, loss = 0.02102410
Iteration 47, loss = 0.02113091
Iteration 48, loss = 0.02071127
Iteration 49, loss = 0.02084057
Iteration 50, loss = 0.02102618
Iteration 51, loss = 0.02046546
Iteration 52, loss = 0.02048032
Iteration 53, loss = 0.02036835
Iteration 54, loss = 0.02025459
Iteration 55, loss = 0.02020517
Iteration 56, loss = 0.02022767
Iteration 57, loss = 0.01996106
Iteration 58, loss = 0.02018033
Iteration 59, loss = 0.01978087
Iteration 60, loss = 0.01998141
Iteration 61, loss = 0.01979866
Iteration 62, loss = 0.01984978
Iteration 63, loss = 0.01966037
Iteration 64, loss = 0.01979912
Iteration 65, loss = 0.01969022
Iteration 66, loss = 0.01957066
Iteration 67, loss = 0.01961943
Iteration 68, loss = 0.01941489
Iteration 69, loss = 0.01962516
Iteration 70, loss = 0.01934948
Iteration 71, loss = 0.01970877
Iteration 72, loss = 0.01961244
Iteration 73, loss = 0.01926611
Iteration 74, loss = 0.01957601
Iteration 75, loss = 0.01928340
Iteration 76, loss = 0.01932912
Iteration 77, loss = 0.01927892
Iteration 78, loss = 0.01945404
Iteration 79, loss = 0.01931569
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68246365
Iteration 2, loss = 0.60946758
Iteration 3, loss = 0.49768665
Iteration 4, loss = 0.37917446
Iteration 5, loss = 0.27954870
Iteration 6, loss = 0.20631599
Iteration 7, loss = 0.15562472
Iteration 8, loss = 0.12131405
Iteration 9, loss = 0.09771473
Iteration 10, loss = 0.08119452
Iteration 11, loss = 0.06914403
Iteration 12, loss = 0.06029790
Iteration 13, loss = 0.05336413
Iteration 14, loss = 0.04810580
Iteration 15, loss = 0.04384155
Iteration 16, loss = 0.04027687
Iteration 17, loss = 0.03759316
Iteration 18, loss = 0.03526276
Iteration 19, loss = 0.03324412
Iteration 20, loss = 0.03151730
Iteration 21, loss = 0.03020431
Iteration 22, loss = 0.02908287
Iteration 23, loss = 0.02785099
Iteration 24, loss = 0.02694514
Iteration 25, loss = 0.02602608
Iteration 26, loss = 0.02526414
Iteration 27, loss = 0.02461151
Iteration 28, loss = 0.02408344
Iteration 29, loss = 0.02377637
Iteration 30, loss = 0.02323337
Iteration 31, loss = 0.02280388
Iteration 32, loss = 0.02228674
Iteration 33, loss = 0.02186385
Iteration 34, loss = 0.02159825
Iteration 35, loss = 0.02136369
Iteration 36, loss = 0.02099890
Iteration 37, loss = 0.02084528
Iteration 38, loss = 0.02048418
Iteration 39, loss = 0.02041307
Iteration 40, loss = 0.02006391
Iteration 41, loss = 0.02009817
Iteration 42, loss = 0.01982673
Iteration 43, loss = 0.01977124
Iteration 44, loss = 0.01949526
Iteration 45, loss = 0.01941586
Iteration 46, loss = 0.01921972
Iteration 47, loss = 0.01910510
Iteration 48, loss = 0.01899651
Iteration 49, loss = 0.01889786
Iteration 50, loss = 0.01894694
Iteration 51, loss = 0.01891708
Iteration 52, loss = 0.01879842
Iteration 53, loss = 0.01841323
Iteration 54, loss = 0.01842636
Iteration 55, loss = 0.01847819
Iteration 56, loss = 0.01815047
Iteration 57, loss = 0.01835566
Iteration 58, loss = 0.01812811
Iteration 59, loss = 0.01832493
Iteration 60, loss = 0.01803506
Iteration 61, loss = 0.01797012
Iteration 62, loss = 0.01821939
Iteration 63, loss = 0.01787893
Iteration 64, loss = 0.01804663
Iteration 65, loss = 0.01775173
Iteration 66, loss = 0.01801663
Iteration 67, loss = 0.01792864
Iteration 68, loss = 0.01770251
Iteration 69, loss = 0.01774461
Iteration 70, loss = 0.01762280
Iteration 71, loss = 0.01753874
Iteration 72, loss = 0.01753891
Iteration 73, loss = 0.01756642
Iteration 74, loss = 0.01741981
Iteration 75, loss = 0.01755670
Iteration 76, loss = 0.01755153
Iteration 77, loss = 0.01757131
Iteration 78, loss = 0.01737333
Iteration 79, loss = 0.01762763
Iteration 80, loss = 0.01753479
Iteration 81, loss = 0.01708576
Iteration 82, loss = 0.01744026
Iteration 83, loss = 0.01746223
Iteration 84, loss = 0.01727588
Iteration 85, loss = 0.01739263
Iteration 86, loss = 0.01713007
Iteration 87, loss = 0.01714669
Iteration 88, loss = 0.01717416
Iteration 89, loss = 0.01711769
Iteration 90, loss = 0.01708038
Iteration 91, loss = 0.01705029
Iteration 92, loss = 0.01704195
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67563584
Iteration 2, loss = 0.60249492
Iteration 3, loss = 0.49194480
Iteration 4, loss = 0.37581459
Iteration 5, loss = 0.27651334
Iteration 6, loss = 0.20306921
Iteration 7, loss = 0.15249575
Iteration 8, loss = 0.11835911
Iteration 9, loss = 0.09516190
Iteration 10, loss = 0.07897409
Iteration 11, loss = 0.06693911
Iteration 12, loss = 0.05840636
Iteration 13, loss = 0.05182861
Iteration 14, loss = 0.04623437
Iteration 15, loss = 0.04227546
Iteration 16, loss = 0.03903404
Iteration 17, loss = 0.03618223
Iteration 18, loss = 0.03415764
Iteration 19, loss = 0.03220974
Iteration 20, loss = 0.03078992
Iteration 21, loss = 0.02960813
Iteration 22, loss = 0.02832929
Iteration 23, loss = 0.02725528
Iteration 24, loss = 0.02655259
Iteration 25, loss = 0.02564147
Iteration 26, loss = 0.02484178
Iteration 27, loss = 0.02436147
Iteration 28, loss = 0.02372437
Iteration 29, loss = 0.02324001
Iteration 30, loss = 0.02306021
Iteration 31, loss = 0.02250757
Iteration 32, loss = 0.02228227
Iteration 33, loss = 0.02184717
Iteration 34, loss = 0.02166385
Iteration 35, loss = 0.02132961
Iteration 36, loss = 0.02126437
Iteration 37, loss = 0.02082919
Iteration 38, loss = 0.02061296
Iteration 39, loss = 0.02042754
Iteration 40, loss = 0.02045764
Iteration 41, loss = 0.02012426
Iteration 42, loss = 0.01994782
Iteration 43, loss = 0.01988130
Iteration 44, loss = 0.01968470
Iteration 45, loss = 0.01960514
Iteration 46, loss = 0.01957780
Iteration 47, loss = 0.01950840
Iteration 48, loss = 0.01949666
Iteration 49, loss = 0.01945269
Iteration 50, loss = 0.01908893
Iteration 51, loss = 0.01889083
Iteration 52, loss = 0.01911401
Iteration 53, loss = 0.01909609
Iteration 54, loss = 0.01894392
Iteration 55, loss = 0.01884335
Iteration 56, loss = 0.01866289
Iteration 57, loss = 0.01855358
Iteration 58, loss = 0.01879418
Iteration 59, loss = 0.01859304
Iteration 60, loss = 0.01839246
Iteration 61, loss = 0.01869717
Iteration 62, loss = 0.01841784
Iteration 63, loss = 0.01838324
Iteration 64, loss = 0.01843286
Iteration 65, loss = 0.01813788
Iteration 66, loss = 0.01830391
Iteration 67, loss = 0.01840620
Iteration 68, loss = 0.01806410
Iteration 69, loss = 0.01826108
Iteration 70, loss = 0.01804980
Iteration 71, loss = 0.01821281
Iteration 72, loss = 0.01800516
Iteration 73, loss = 0.01802064
Iteration 74, loss = 0.01786396
Iteration 75, loss = 0.01783772
Iteration 76, loss = 0.01796564
Iteration 77, loss = 0.01795603
Iteration 78, loss = 0.01769763
Iteration 79, loss = 0.01786312
Iteration 80, loss = 0.01801626
Iteration 81, loss = 0.01787506
Iteration 82, loss = 0.01774838
Iteration 83, loss = 0.01783137
Iteration 84, loss = 0.01778604
Iteration 85, loss = 0.01781857
Iteration 86, loss = 0.01763311
Iteration 87, loss = 0.01777790
Iteration 88, loss = 0.01764581
Iteration 89, loss = 0.01759437
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70647779
Iteration 2, loss = 0.63637193
Iteration 3, loss = 0.52743514
Iteration 4, loss = 0.40213759
Iteration 5, loss = 0.29545827
Iteration 6, loss = 0.21697809
Iteration 7, loss = 0.16274628
Iteration 8, loss = 0.12640774
Iteration 9, loss = 0.10153933
Iteration 10, loss = 0.08393178
Iteration 11, loss = 0.07109848
Iteration 12, loss = 0.06177389
Iteration 13, loss = 0.05456631
Iteration 14, loss = 0.04906587
Iteration 15, loss = 0.04471583
Iteration 16, loss = 0.04097761
Iteration 17, loss = 0.03817698
Iteration 18, loss = 0.03540410
Iteration 19, loss = 0.03346476
Iteration 20, loss = 0.03179684
Iteration 21, loss = 0.03021033
Iteration 22, loss = 0.02910203
Iteration 23, loss = 0.02788961
Iteration 24, loss = 0.02732795
Iteration 25, loss = 0.02611062
Iteration 26, loss = 0.02544298
Iteration 27, loss = 0.02479442
Iteration 28, loss = 0.02417645
Iteration 29, loss = 0.02367587
Iteration 30, loss = 0.02330948
Iteration 31, loss = 0.02280217
Iteration 32, loss = 0.02233300
Iteration 33, loss = 0.02196103
Iteration 34, loss = 0.02168911
Iteration 35, loss = 0.02137552
Iteration 36, loss = 0.02104855
Iteration 37, loss = 0.02092295
Iteration 38, loss = 0.02054258
Iteration 39, loss = 0.02055994
Iteration 40, loss = 0.02032993
Iteration 41, loss = 0.02016261
Iteration 42, loss = 0.02027345
Iteration 43, loss = 0.01974572
Iteration 44, loss = 0.01946213
Iteration 45, loss = 0.01960230
Iteration 46, loss = 0.01941747
Iteration 47, loss = 0.01917517
Iteration 48, loss = 0.01917161
Iteration 49, loss = 0.01923710
Iteration 50, loss = 0.01883150
Iteration 51, loss = 0.01874252
Iteration 52, loss = 0.01868388
Iteration 53, loss = 0.01868787
Iteration 54, loss = 0.01868094
Iteration 55, loss = 0.01836931
Iteration 56, loss = 0.01830179
Iteration 57, loss = 0.01841156
Iteration 58, loss = 0.01836947
Iteration 59, loss = 0.01837311
Iteration 60, loss = 0.01831709
Iteration 61, loss = 0.01829553
Iteration 62, loss = 0.01800228
Iteration 63, loss = 0.01812131
Iteration 64, loss = 0.01779035
Iteration 65, loss = 0.01804888
Iteration 66, loss = 0.01803854
Iteration 67, loss = 0.01814149
Iteration 68, loss = 0.01779508
Iteration 69, loss = 0.01782534
Iteration 70, loss = 0.01779864
Iteration 71, loss = 0.01819914
Iteration 72, loss = 0.01802311
Iteration 73, loss = 0.01787036
Iteration 74, loss = 0.01758202
Iteration 75, loss = 0.01787349
Iteration 76, loss = 0.01760465
Iteration 77, loss = 0.01767726
Iteration 78, loss = 0.01755278
Iteration 79, loss = 0.01751303
Iteration 80, loss = 0.01723152
Iteration 81, loss = 0.01740460
Iteration 82, loss = 0.01748153
Iteration 83, loss = 0.01744603
Iteration 84, loss = 0.01738904
Iteration 85, loss = 0.01730927
Iteration 86, loss = 0.01740185
Iteration 87, loss = 0.01740385
Iteration 88, loss = 0.01735555
Iteration 89, loss = 0.01748506
Iteration 90, loss = 0.01724702
Iteration 91, loss = 0.01707729
Iteration 92, loss = 0.01734822
Iteration 93, loss = 0.01736757
Iteration 94, loss = 0.01730963
Iteration 95, loss = 0.01726698
Iteration 96, loss = 0.01735017
Iteration 97, loss = 0.01702444
Iteration 98, loss = 0.01707293
Iteration 99, loss = 0.01711914
Iteration 100, loss = 0.01700964
Iteration 101, loss = 0.01714489
Iteration 102, loss = 0.01741308
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67484458
Iteration 2, loss = 0.59943914
Iteration 3, loss = 0.48623126
Iteration 4, loss = 0.37221400
Iteration 5, loss = 0.27640818
Iteration 6, loss = 0.20498622
Iteration 7, loss = 0.15518526
Iteration 8, loss = 0.12086503
Iteration 9, loss = 0.09747160
Iteration 10, loss = 0.08084687
Iteration 11, loss = 0.06892419
Iteration 12, loss = 0.05989945
Iteration 13, loss = 0.05315600
Iteration 14, loss = 0.04792819
Iteration 15, loss = 0.04352109
Iteration 16, loss = 0.04006157
Iteration 17, loss = 0.03745177
Iteration 18, loss = 0.03501025
Iteration 19, loss = 0.03300077
Iteration 20, loss = 0.03147217
Iteration 21, loss = 0.02994191
Iteration 22, loss = 0.02877617
Iteration 23, loss = 0.02769240
Iteration 24, loss = 0.02687405
Iteration 25, loss = 0.02612389
Iteration 26, loss = 0.02554952
Iteration 27, loss = 0.02472827
Iteration 28, loss = 0.02414826
Iteration 29, loss = 0.02362725
Iteration 30, loss = 0.02338560
Iteration 31, loss = 0.02279426
Iteration 32, loss = 0.02255770
Iteration 33, loss = 0.02230716
Iteration 34, loss = 0.02176374
Iteration 35, loss = 0.02145117
Iteration 36, loss = 0.02130437
Iteration 37, loss = 0.02112398
Iteration 38, loss = 0.02120896
Iteration 39, loss = 0.02084337
Iteration 40, loss = 0.02057242
Iteration 41, loss = 0.02030255
Iteration 42, loss = 0.02024528
Iteration 43, loss = 0.02006230
Iteration 44, loss = 0.01970598
Iteration 45, loss = 0.01983273
Iteration 46, loss = 0.01962982
Iteration 47, loss = 0.01971920
Iteration 48, loss = 0.01926698
Iteration 49, loss = 0.01946102
Iteration 50, loss = 0.01938022
Iteration 51, loss = 0.01911109
Iteration 52, loss = 0.01917599
Iteration 53, loss = 0.01906358
Iteration 54, loss = 0.01897777
Iteration 55, loss = 0.01876598
Iteration 56, loss = 0.01898965
Iteration 57, loss = 0.01893342
Iteration 58, loss = 0.01861545
Iteration 59, loss = 0.01856576
Iteration 60, loss = 0.01836307
Iteration 61, loss = 0.01861334
Iteration 62, loss = 0.01837125
Iteration 63, loss = 0.01853158
Iteration 64, loss = 0.01853349
Iteration 65, loss = 0.01856338
Iteration 66, loss = 0.01818889
Iteration 67, loss = 0.01815745
Iteration 68, loss = 0.01836034
Iteration 69, loss = 0.01824065
Iteration 70, loss = 0.01825627
Iteration 71, loss = 0.01807595
Iteration 72, loss = 0.01809156
Iteration 73, loss = 0.01818349
Iteration 74, loss = 0.01817536
Iteration 75, loss = 0.01807094
Iteration 76, loss = 0.01786624
Iteration 77, loss = 0.01797215
Iteration 78, loss = 0.01779221
Iteration 79, loss = 0.01793010
Iteration 80, loss = 0.01798402
Iteration 81, loss = 0.01773542
Iteration 82, loss = 0.01765498
Iteration 83, loss = 0.01765082
Iteration 84, loss = 0.01776306
Iteration 85, loss = 0.01776865
Iteration 86, loss = 0.01779366
Iteration 87, loss = 0.01769929
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67392689
Iteration 2, loss = 0.59846677
Iteration 3, loss = 0.49135069
Iteration 4, loss = 0.38200426
Iteration 5, loss = 0.28722686
Iteration 6, loss = 0.21461563
Iteration 7, loss = 0.16283147
Iteration 8, loss = 0.12748404
Iteration 9, loss = 0.10264125
Iteration 10, loss = 0.08531316
Iteration 11, loss = 0.07271010
Iteration 12, loss = 0.06318023
Iteration 13, loss = 0.05617287
Iteration 14, loss = 0.05048216
Iteration 15, loss = 0.04600968
Iteration 16, loss = 0.04264709
Iteration 17, loss = 0.03965729
Iteration 18, loss = 0.03716467
Iteration 19, loss = 0.03496706
Iteration 20, loss = 0.03341143
Iteration 21, loss = 0.03180388
Iteration 22, loss = 0.03072207
Iteration 23, loss = 0.02963224
Iteration 24, loss = 0.02860113
Iteration 25, loss = 0.02786209
Iteration 26, loss = 0.02737533
Iteration 27, loss = 0.02625420
Iteration 28, loss = 0.02579795
Iteration 29, loss = 0.02524360
Iteration 30, loss = 0.02467882
Iteration 31, loss = 0.02440371
Iteration 32, loss = 0.02414085
Iteration 33, loss = 0.02390862
Iteration 34, loss = 0.02335000
Iteration 35, loss = 0.02300942
Iteration 36, loss = 0.02284521
Iteration 37, loss = 0.02247647
Iteration 38, loss = 0.02240036
Iteration 39, loss = 0.02214574
Iteration 40, loss = 0.02203367
Iteration 41, loss = 0.02182916
Iteration 42, loss = 0.02154576
Iteration 43, loss = 0.02145997
Iteration 44, loss = 0.02132119
Iteration 45, loss = 0.02131785
Iteration 46, loss = 0.02102410
Iteration 47, loss = 0.02113091
Iteration 48, loss = 0.02071127
Iteration 49, loss = 0.02084057
Iteration 50, loss = 0.02102618
Iteration 51, loss = 0.02046546
Iteration 52, loss = 0.02048032
Iteration 53, loss = 0.02036835
Iteration 54, loss = 0.02025459
Iteration 55, loss = 0.02020517
Iteration 56, loss = 0.02022767
Iteration 57, loss = 0.01996106
Iteration 58, loss = 0.02018033
Iteration 59, loss = 0.01978087
Iteration 60, loss = 0.01998141
Iteration 61, loss = 0.01979866
Iteration 62, loss = 0.01984978
Iteration 63, loss = 0.01966037
Iteration 64, loss = 0.01979912
Iteration 65, loss = 0.01969022
Iteration 66, loss = 0.01957066
Iteration 67, loss = 0.01961943
Iteration 68, loss = 0.01941489
Iteration 69, loss = 0.01962516
Iteration 70, loss = 0.01934948
Iteration 71, loss = 0.01970877
Iteration 72, loss = 0.01961244
Iteration 73, loss = 0.01926611
Iteration 74, loss = 0.01957601
Iteration 75, loss = 0.01928340
Iteration 76, loss = 0.01932912
Iteration 77, loss = 0.01927892
Iteration 78, loss = 0.01945404
Iteration 79, loss = 0.01931569
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68246365
Iteration 2, loss = 0.60946758
Iteration 3, loss = 0.49768665
Iteration 4, loss = 0.37917446
Iteration 5, loss = 0.27954870
Iteration 6, loss = 0.20631599
Iteration 7, loss = 0.15562472
Iteration 8, loss = 0.12131405
Iteration 9, loss = 0.09771473
Iteration 10, loss = 0.08119452
Iteration 11, loss = 0.06914403
Iteration 12, loss = 0.06029790
Iteration 13, loss = 0.05336413
Iteration 14, loss = 0.04810580
Iteration 15, loss = 0.04384155
Iteration 16, loss = 0.04027687
Iteration 17, loss = 0.03759316
Iteration 18, loss = 0.03526276
Iteration 19, loss = 0.03324412
Iteration 20, loss = 0.03151730
Iteration 21, loss = 0.03020431
Iteration 22, loss = 0.02908287
Iteration 23, loss = 0.02785099
Iteration 24, loss = 0.02694514
Iteration 25, loss = 0.02602608
Iteration 26, loss = 0.02526414
Iteration 27, loss = 0.02461151
Iteration 28, loss = 0.02408344
Iteration 29, loss = 0.02377637
Iteration 30, loss = 0.02323337
Iteration 31, loss = 0.02280388
Iteration 32, loss = 0.02228674
Iteration 33, loss = 0.02186385
Iteration 34, loss = 0.02159825
Iteration 35, loss = 0.02136369
Iteration 36, loss = 0.02099890
Iteration 37, loss = 0.02084528
Iteration 38, loss = 0.02048418
Iteration 39, loss = 0.02041307
Iteration 40, loss = 0.02006391
Iteration 41, loss = 0.02009817
Iteration 42, loss = 0.01982673
Iteration 43, loss = 0.01977124
Iteration 44, loss = 0.01949526
Iteration 45, loss = 0.01941586
Iteration 46, loss = 0.01921972
Iteration 47, loss = 0.01910510
Iteration 48, loss = 0.01899651
Iteration 49, loss = 0.01889786
Iteration 50, loss = 0.01894694
Iteration 51, loss = 0.01891708
Iteration 52, loss = 0.01879842
Iteration 53, loss = 0.01841323
Iteration 54, loss = 0.01842636
Iteration 55, loss = 0.01847819
Iteration 56, loss = 0.01815047
Iteration 57, loss = 0.01835566
Iteration 58, loss = 0.01812811
Iteration 59, loss = 0.01832493
Iteration 60, loss = 0.01803506
Iteration 61, loss = 0.01797012
Iteration 62, loss = 0.01821939
Iteration 63, loss = 0.01787893
Iteration 64, loss = 0.01804663
Iteration 65, loss = 0.01775173
Iteration 66, loss = 0.01801663
Iteration 67, loss = 0.01792864
Iteration 68, loss = 0.01770251
Iteration 69, loss = 0.01774461
Iteration 70, loss = 0.01762280
Iteration 71, loss = 0.01753874
Iteration 72, loss = 0.01753891
Iteration 73, loss = 0.01756642
Iteration 74, loss = 0.01741981
Iteration 75, loss = 0.01755670
Iteration 76, loss = 0.01755153
Iteration 77, loss = 0.01757131
Iteration 78, loss = 0.01737333
Iteration 79, loss = 0.01762763
Iteration 80, loss = 0.01753479
Iteration 81, loss = 0.01708576
Iteration 82, loss = 0.01744026
Iteration 83, loss = 0.01746223
Iteration 84, loss = 0.01727588
Iteration 85, loss = 0.01739263
Iteration 86, loss = 0.01713007
Iteration 87, loss = 0.01714669
Iteration 88, loss = 0.01717416
Iteration 89, loss = 0.01711769
Iteration 90, loss = 0.01708038
Iteration 91, loss = 0.01705029
Iteration 92, loss = 0.01704195
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67563584
Iteration 2, loss = 0.60249492
Iteration 3, loss = 0.49194480
Iteration 4, loss = 0.37581459
Iteration 5, loss = 0.27651334
Iteration 6, loss = 0.20306921
Iteration 7, loss = 0.15249575
Iteration 8, loss = 0.11835911
Iteration 9, loss = 0.09516190
Iteration 10, loss = 0.07897409
Iteration 11, loss = 0.06693911
Iteration 12, loss = 0.05840636
Iteration 13, loss = 0.05182861
Iteration 14, loss = 0.04623437
Iteration 15, loss = 0.04227546
Iteration 16, loss = 0.03903404
Iteration 17, loss = 0.03618223
Iteration 18, loss = 0.03415764
Iteration 19, loss = 0.03220974
Iteration 20, loss = 0.03078992
Iteration 21, loss = 0.02960813
Iteration 22, loss = 0.02832929
Iteration 23, loss = 0.02725528
Iteration 24, loss = 0.02655259
Iteration 25, loss = 0.02564147
Iteration 26, loss = 0.02484178
Iteration 27, loss = 0.02436147
Iteration 28, loss = 0.02372437
Iteration 29, loss = 0.02324001
Iteration 30, loss = 0.02306021
Iteration 31, loss = 0.02250757
Iteration 32, loss = 0.02228227
Iteration 33, loss = 0.02184717
Iteration 34, loss = 0.02166385
Iteration 35, loss = 0.02132961
Iteration 36, loss = 0.02126437
Iteration 37, loss = 0.02082919
Iteration 38, loss = 0.02061296
Iteration 39, loss = 0.02042754
Iteration 40, loss = 0.02045764
Iteration 41, loss = 0.02012426
Iteration 42, loss = 0.01994782
Iteration 43, loss = 0.01988130
Iteration 44, loss = 0.01968470
Iteration 45, loss = 0.01960514
Iteration 46, loss = 0.01957780
Iteration 47, loss = 0.01950840
Iteration 48, loss = 0.01949666
Iteration 49, loss = 0.01945269
Iteration 50, loss = 0.01908893
Iteration 51, loss = 0.01889083
Iteration 52, loss = 0.01911401
Iteration 53, loss = 0.01909609
Iteration 54, loss = 0.01894392
Iteration 55, loss = 0.01884335
Iteration 56, loss = 0.01866289
Iteration 57, loss = 0.01855358
Iteration 58, loss = 0.01879418
Iteration 59, loss = 0.01859304
Iteration 60, loss = 0.01839246
Iteration 61, loss = 0.01869717
Iteration 62, loss = 0.01841784
Iteration 63, loss = 0.01838324
Iteration 64, loss = 0.01843286
Iteration 65, loss = 0.01813788
Iteration 66, loss = 0.01830391
Iteration 67, loss = 0.01840620
Iteration 68, loss = 0.01806410
Iteration 69, loss = 0.01826108
Iteration 70, loss = 0.01804980
Iteration 71, loss = 0.01821281
Iteration 72, loss = 0.01800516
Iteration 73, loss = 0.01802064
Iteration 74, loss = 0.01786396
Iteration 75, loss = 0.01783772
Iteration 76, loss = 0.01796564
Iteration 77, loss = 0.01795603
Iteration 78, loss = 0.01769763
Iteration 79, loss = 0.01786312
Iteration 80, loss = 0.01801626
Iteration 81, loss = 0.01787506
Iteration 82, loss = 0.01774838
Iteration 83, loss = 0.01783137
Iteration 84, loss = 0.01778604
Iteration 85, loss = 0.01781857
Iteration 86, loss = 0.01763311
Iteration 87, loss = 0.01777790
Iteration 88, loss = 0.01764581
Iteration 89, loss = 0.01759437
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70647779
Iteration 2, loss = 0.63637193
Iteration 3, loss = 0.52743514
Iteration 4, loss = 0.40213759
Iteration 5, loss = 0.29545827
Iteration 6, loss = 0.21697809
Iteration 7, loss = 0.16274628
Iteration 8, loss = 0.12640774
Iteration 9, loss = 0.10153933
Iteration 10, loss = 0.08393178
Iteration 11, loss = 0.07109848
Iteration 12, loss = 0.06177389
Iteration 13, loss = 0.05456631
Iteration 14, loss = 0.04906587
Iteration 15, loss = 0.04471583
Iteration 16, loss = 0.04097761
Iteration 17, loss = 0.03817698
Iteration 18, loss = 0.03540410
Iteration 19, loss = 0.03346476
Iteration 20, loss = 0.03179684
Iteration 21, loss = 0.03021033
Iteration 22, loss = 0.02910203
Iteration 23, loss = 0.02788961
Iteration 24, loss = 0.02732795
Iteration 25, loss = 0.02611062
Iteration 26, loss = 0.02544298
Iteration 27, loss = 0.02479442
Iteration 28, loss = 0.02417645
Iteration 29, loss = 0.02367587
Iteration 30, loss = 0.02330948
Iteration 31, loss = 0.02280217
Iteration 32, loss = 0.02233300
Iteration 33, loss = 0.02196103
Iteration 34, loss = 0.02168911
Iteration 35, loss = 0.02137552
Iteration 36, loss = 0.02104855
Iteration 37, loss = 0.02092295
Iteration 38, loss = 0.02054258
Iteration 39, loss = 0.02055994
Iteration 40, loss = 0.02032993
Iteration 41, loss = 0.02016261
Iteration 42, loss = 0.02027345
Iteration 43, loss = 0.01974572
Iteration 44, loss = 0.01946213
Iteration 45, loss = 0.01960230
Iteration 46, loss = 0.01941747
Iteration 47, loss = 0.01917517
Iteration 48, loss = 0.01917161
Iteration 49, loss = 0.01923710
Iteration 50, loss = 0.01883150
Iteration 51, loss = 0.01874252
Iteration 52, loss = 0.01868388
Iteration 53, loss = 0.01868787
Iteration 54, loss = 0.01868094
Iteration 55, loss = 0.01836931
Iteration 56, loss = 0.01830179
Iteration 57, loss = 0.01841156
Iteration 58, loss = 0.01836947
Iteration 59, loss = 0.01837311
Iteration 60, loss = 0.01831709
Iteration 61, loss = 0.01829553
Iteration 62, loss = 0.01800228
Iteration 63, loss = 0.01812131
Iteration 64, loss = 0.01779035
Iteration 65, loss = 0.01804888
Iteration 66, loss = 0.01803854
Iteration 67, loss = 0.01814149
Iteration 68, loss = 0.01779508
Iteration 69, loss = 0.01782534
Iteration 70, loss = 0.01779864
Iteration 71, loss = 0.01819914
Iteration 72, loss = 0.01802311
Iteration 73, loss = 0.01787036
Iteration 74, loss = 0.01758202
Iteration 75, loss = 0.01787349
Iteration 76, loss = 0.01760465
Iteration 77, loss = 0.01767726
Iteration 78, loss = 0.01755278
Iteration 79, loss = 0.01751303
Iteration 80, loss = 0.01723152
Iteration 81, loss = 0.01740460
Iteration 82, loss = 0.01748153
Iteration 83, loss = 0.01744603
Iteration 84, loss = 0.01738904
Iteration 85, loss = 0.01730927
Iteration 86, loss = 0.01740185
Iteration 87, loss = 0.01740385
Iteration 88, loss = 0.01735555
Iteration 89, loss = 0.01748506
Iteration 90, loss = 0.01724702
Iteration 91, loss = 0.01707729
Iteration 92, loss = 0.01734822
Iteration 93, loss = 0.01736757
Iteration 94, loss = 0.01730963
Iteration 95, loss = 0.01726698
Iteration 96, loss = 0.01735017
Iteration 97, loss = 0.01702444
Iteration 98, loss = 0.01707293
Iteration 99, loss = 0.01711914
Iteration 100, loss = 0.01700964
Iteration 101, loss = 0.01714489
Iteration 102, loss = 0.01741308
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67484458
Iteration 2, loss = 0.59943914
Iteration 3, loss = 0.48623126
Iteration 4, loss = 0.37221400
Iteration 5, loss = 0.27640818
Iteration 6, loss = 0.20498622
Iteration 7, loss = 0.15518526
Iteration 8, loss = 0.12086503
Iteration 9, loss = 0.09747160
Iteration 10, loss = 0.08084687
Iteration 11, loss = 0.06892419
Iteration 12, loss = 0.05989945
Iteration 13, loss = 0.05315600
Iteration 14, loss = 0.04792819
Iteration 15, loss = 0.04352109
Iteration 16, loss = 0.04006157
Iteration 17, loss = 0.03745177
Iteration 18, loss = 0.03501025
Iteration 19, loss = 0.03300077
Iteration 20, loss = 0.03147217
Iteration 21, loss = 0.02994191
Iteration 22, loss = 0.02877617
Iteration 23, loss = 0.02769240
Iteration 24, loss = 0.02687405
Iteration 25, loss = 0.02612389
Iteration 26, loss = 0.02554952
Iteration 27, loss = 0.02472827
Iteration 28, loss = 0.02414826
Iteration 29, loss = 0.02362725
Iteration 30, loss = 0.02338560
Iteration 31, loss = 0.02279426
Iteration 32, loss = 0.02255770
Iteration 33, loss = 0.02230716
Iteration 34, loss = 0.02176374
Iteration 35, loss = 0.02145117
Iteration 36, loss = 0.02130437
Iteration 37, loss = 0.02112398
Iteration 38, loss = 0.02120896
Iteration 39, loss = 0.02084337
Iteration 40, loss = 0.02057242
Iteration 41, loss = 0.02030255
Iteration 42, loss = 0.02024528
Iteration 43, loss = 0.02006230
Iteration 44, loss = 0.01970598
Iteration 45, loss = 0.01983273
Iteration 46, loss = 0.01962982
Iteration 47, loss = 0.01971920
Iteration 48, loss = 0.01926698
Iteration 49, loss = 0.01946102
Iteration 50, loss = 0.01938022
Iteration 51, loss = 0.01911109
Iteration 52, loss = 0.01917599
Iteration 53, loss = 0.01906358
Iteration 54, loss = 0.01897777
Iteration 55, loss = 0.01876598
Iteration 56, loss = 0.01898965
Iteration 57, loss = 0.01893342
Iteration 58, loss = 0.01861545
Iteration 59, loss = 0.01856576
Iteration 60, loss = 0.01836307
Iteration 61, loss = 0.01861334
Iteration 62, loss = 0.01837125
Iteration 63, loss = 0.01853158
Iteration 64, loss = 0.01853349
Iteration 65, loss = 0.01856338
Iteration 66, loss = 0.01818889
Iteration 67, loss = 0.01815745
Iteration 68, loss = 0.01836034
Iteration 69, loss = 0.01824065
Iteration 70, loss = 0.01825627
Iteration 71, loss = 0.01807595
Iteration 72, loss = 0.01809156
Iteration 73, loss = 0.01818349
Iteration 74, loss = 0.01817536
Iteration 75, loss = 0.01807094
Iteration 76, loss = 0.01786624
Iteration 77, loss = 0.01797215
Iteration 78, loss = 0.01779221
Iteration 79, loss = 0.01793010
Iteration 80, loss = 0.01798402
Iteration 81, loss = 0.01773542
Iteration 82, loss = 0.01765498
Iteration 83, loss = 0.01765082
Iteration 84, loss = 0.01776306
Iteration 85, loss = 0.01776865
Iteration 86, loss = 0.01779366
Iteration 87, loss = 0.01769929
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67392689
Iteration 2, loss = 0.59846677
Iteration 3, loss = 0.49135069
Iteration 4, loss = 0.38200426
Iteration 5, loss = 0.28722686
Iteration 6, loss = 0.21461563
Iteration 7, loss = 0.16283147
Iteration 8, loss = 0.12748404
Iteration 9, loss = 0.10264125
Iteration 10, loss = 0.08531316
Iteration 11, loss = 0.07271010
Iteration 12, loss = 0.06318023
Iteration 13, loss = 0.05617287
Iteration 14, loss = 0.05048216
Iteration 15, loss = 0.04600968
Iteration 16, loss = 0.04264709
Iteration 17, loss = 0.03965729
Iteration 18, loss = 0.03716467
Iteration 19, loss = 0.03496706
Iteration 20, loss = 0.03341143
Iteration 21, loss = 0.03180388
Iteration 22, loss = 0.03072207
Iteration 23, loss = 0.02963224
Iteration 24, loss = 0.02860113
Iteration 25, loss = 0.02786209
Iteration 26, loss = 0.02737533
Iteration 27, loss = 0.02625420
Iteration 28, loss = 0.02579795
Iteration 29, loss = 0.02524360
Iteration 30, loss = 0.02467882
Iteration 31, loss = 0.02440371
Iteration 32, loss = 0.02414085
Iteration 33, loss = 0.02390862
Iteration 34, loss = 0.02335000
Iteration 35, loss = 0.02300942
Iteration 36, loss = 0.02284521
Iteration 37, loss = 0.02247647
Iteration 38, loss = 0.02240036
Iteration 39, loss = 0.02214574
Iteration 40, loss = 0.02203367
Iteration 41, loss = 0.02182916
Iteration 42, loss = 0.02154576
Iteration 43, loss = 0.02145997
Iteration 44, loss = 0.02132119
Iteration 45, loss = 0.02131785
Iteration 46, loss = 0.02102410
Iteration 47, loss = 0.02113091
Iteration 48, loss = 0.02071127
Iteration 49, loss = 0.02084057
Iteration 50, loss = 0.02102618
Iteration 51, loss = 0.02046546
Iteration 52, loss = 0.02048032
Iteration 53, loss = 0.02036835
Iteration 54, loss = 0.02025459
Iteration 55, loss = 0.02020517
Iteration 56, loss = 0.02022767
Iteration 57, loss = 0.01996106
Iteration 58, loss = 0.02018033
Iteration 59, loss = 0.01978087
Iteration 60, loss = 0.01998141
Iteration 61, loss = 0.01979866
Iteration 62, loss = 0.01984978
Iteration 63, loss = 0.01966037
Iteration 64, loss = 0.01979912
Iteration 65, loss = 0.01969022
Iteration 66, loss = 0.01957066
Iteration 67, loss = 0.01961943
Iteration 68, loss = 0.01941489
Iteration 69, loss = 0.01962516
Iteration 70, loss = 0.01934948
Iteration 71, loss = 0.01970877
Iteration 72, loss = 0.01961244
Iteration 73, loss = 0.01926611
Iteration 74, loss = 0.01957601
Iteration 75, loss = 0.01928340
Iteration 76, loss = 0.01932912
Iteration 77, loss = 0.01927892
Iteration 78, loss = 0.01945404
Iteration 79, loss = 0.01931569
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.66903117
Iteration 2, loss = 0.56891701
Iteration 3, loss = 0.43790277
Iteration 4, loss = 0.31418798
Iteration 5, loss = 0.22138323
Iteration 6, loss = 0.15995236
Iteration 7, loss = 0.12066446
Iteration 8, loss = 0.09495293
Iteration 9, loss = 0.07830883
Iteration 10, loss = 0.06630428
Iteration 11, loss = 0.05749324
Iteration 12, loss = 0.05105935
Iteration 13, loss = 0.04632034
Iteration 14, loss = 0.04226258
Iteration 15, loss = 0.03931793
Iteration 16, loss = 0.03685190
Iteration 17, loss = 0.03474164
Iteration 18, loss = 0.03303783
Iteration 19, loss = 0.03204047
Iteration 20, loss = 0.03062639
Iteration 21, loss = 0.02965848
Iteration 22, loss = 0.02893809
Iteration 23, loss = 0.02780946
Iteration 24, loss = 0.02714271
Iteration 25, loss = 0.02669436
Iteration 26, loss = 0.02621044
Iteration 27, loss = 0.02591190
Iteration 28, loss = 0.02519714
Iteration 29, loss = 0.02502804
Iteration 30, loss = 0.02451686
Iteration 31, loss = 0.02437494
Iteration 32, loss = 0.02424245
Iteration 33, loss = 0.02368898
Iteration 34, loss = 0.02334905
Iteration 35, loss = 0.02343338
Iteration 36, loss = 0.02312709
Iteration 37, loss = 0.02310263
Iteration 38, loss = 0.02269891
Iteration 39, loss = 0.02318649
Iteration 40, loss = 0.02245540
Iteration 41, loss = 0.02252233
Iteration 42, loss = 0.02244952
Iteration 43, loss = 0.02230674
Iteration 44, loss = 0.02202579
Iteration 45, loss = 0.02180746
Iteration 46, loss = 0.02162846
Iteration 47, loss = 0.02193021
Iteration 48, loss = 0.02162992
Iteration 49, loss = 0.02153017
Iteration 50, loss = 0.02145129
Iteration 51, loss = 0.02133351
Iteration 52, loss = 0.02130784
Iteration 53, loss = 0.02125697
Iteration 54, loss = 0.02120841
Iteration 55, loss = 0.02122785
Iteration 56, loss = 0.02126617
Iteration 57, loss = 0.02155981
Iteration 58, loss = 0.02084506
Iteration 59, loss = 0.02100970
Iteration 60, loss = 0.02092680
Iteration 61, loss = 0.02106987
Iteration 62, loss = 0.02136161
Iteration 63, loss = 0.02119960
Iteration 64, loss = 0.02110103
Iteration 65, loss = 0.02077130
Iteration 66, loss = 0.02088817
Iteration 67, loss = 0.02079125
Iteration 68, loss = 0.02058199
Iteration 69, loss = 0.02083543
Iteration 70, loss = 0.02059474
Iteration 71, loss = 0.02106648
Iteration 72, loss = 0.02087177
Iteration 73, loss = 0.02038005
Iteration 74, loss = 0.02049017
Iteration 75, loss = 0.02045936
Iteration 76, loss = 0.02026598
Iteration 77, loss = 0.02070386
Iteration 78, loss = 0.02078299
Iteration 79, loss = 0.02010190
Iteration 80, loss = 0.02045458
Iteration 81, loss = 0.02070410
Iteration 82, loss = 0.02028762
Iteration 83, loss = 0.02062140
Iteration 84, loss = 0.02038630
Iteration 85, loss = 0.02037983
Iteration 86, loss = 0.02029923
Iteration 87, loss = 0.02026605
Iteration 88, loss = 0.02039888
Iteration 89, loss = 0.02055046
Iteration 90, loss = 0.01998452
Iteration 91, loss = 0.02001227
Iteration 92, loss = 0.01986693
Iteration 93, loss = 0.02009153
Iteration 94, loss = 0.01998610
Iteration 95, loss = 0.01999438
Iteration 96, loss = 0.02017479
Iteration 97, loss = 0.01988461
Iteration 98, loss = 0.01995847
Iteration 99, loss = 0.01985474
Iteration 100, loss = 0.02037109
Iteration 101, loss = 0.02037219
Iteration 102, loss = 0.02022975
Iteration 103, loss = 0.01991199
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on kept_v9_minimal_processing...
Iteration 1, loss = 0.67941124
Iteration 2, loss = 0.64167981
Iteration 3, loss = 0.57787705
Iteration 4, loss = 0.49103871
Iteration 5, loss = 0.40500542
Iteration 6, loss = 0.32820903
Iteration 7, loss = 0.26403580
Iteration 8, loss = 0.21297579
Iteration 9, loss = 0.17303836
Iteration 10, loss = 0.14284109
Iteration 11, loss = 0.11987711
Iteration 12, loss = 0.10212510
Iteration 13, loss = 0.08850417
Iteration 14, loss = 0.07767140
Iteration 15, loss = 0.06898388
Iteration 16, loss = 0.06198506
Iteration 17, loss = 0.05629890
Iteration 18, loss = 0.05161503
Iteration 19, loss = 0.04760344
Iteration 20, loss = 0.04416705
Iteration 21, loss = 0.04142837
Iteration 22, loss = 0.03886302
Iteration 23, loss = 0.03675634
Iteration 24, loss = 0.03483820
Iteration 25, loss = 0.03320138
Iteration 26, loss = 0.03188673
Iteration 27, loss = 0.03053708
Iteration 28, loss = 0.02931648
Iteration 29, loss = 0.02837328
Iteration 30, loss = 0.02751220
Iteration 31, loss = 0.02666076
Iteration 32, loss = 0.02595619
Iteration 33, loss = 0.02510895
Iteration 34, loss = 0.02474427
Iteration 35, loss = 0.02380728
Iteration 36, loss = 0.02330297
Iteration 37, loss = 0.02303051
Iteration 38, loss = 0.02254081
Iteration 39, loss = 0.02242593
Iteration 40, loss = 0.02191872
Iteration 41, loss = 0.02119176
Iteration 42, loss = 0.02099940
Iteration 43, loss = 0.02078482
Iteration 44, loss = 0.02061004
Iteration 45, loss = 0.02024035
Iteration 46, loss = 0.01999581
Iteration 47, loss = 0.01960963
Iteration 48, loss = 0.01929787
Iteration 49, loss = 0.01927101
Iteration 50, loss = 0.01896615
Iteration 51, loss = 0.01889115
Iteration 52, loss = 0.01865472
Iteration 53, loss = 0.01886789
Iteration 54, loss = 0.01826841
Iteration 55, loss = 0.01832736
Iteration 56, loss = 0.01813449
Iteration 57, loss = 0.01827125
Iteration 58, loss = 0.01788322
Iteration 59, loss = 0.01802555
Iteration 60, loss = 0.01765052
Iteration 61, loss = 0.01741414
Iteration 62, loss = 0.01741414
Iteration 63, loss = 0.01736432
Iteration 64, loss = 0.01716707
Iteration 65, loss = 0.01708679
Iteration 66, loss = 0.01696066
Iteration 67, loss = 0.01719734
Iteration 68, loss = 0.01691703
Iteration 69, loss = 0.01659948
Iteration 70, loss = 0.01681643
Iteration 71, loss = 0.01659887
Iteration 72, loss = 0.01658756
Iteration 73, loss = 0.01634627
Iteration 74, loss = 0.01669904
Iteration 75, loss = 0.01645396
Iteration 76, loss = 0.01647453
Iteration 77, loss = 0.01651219
Iteration 78, loss = 0.01644275
Iteration 79, loss = 0.01631418
Iteration 80, loss = 0.01615149
Iteration 81, loss = 0.01604635
Iteration 82, loss = 0.01625881
Iteration 83, loss = 0.01631077
Iteration 84, loss = 0.01598128
Iteration 85, loss = 0.01617069
Iteration 86, loss = 0.01599205
Iteration 87, loss = 0.01577461
Iteration 88, loss = 0.01605592
Iteration 89, loss = 0.01582332
Iteration 90, loss = 0.01557673
Iteration 91, loss = 0.01584244
Iteration 92, loss = 0.01566946
Iteration 93, loss = 0.01559362
Iteration 94, loss = 0.01581513
Iteration 95, loss = 0.01557794
Iteration 96, loss = 0.01547554
Iteration 97, loss = 0.01580493
Iteration 98, loss = 0.01575124
Iteration 99, loss = 0.01537224
Iteration 100, loss = 0.01583540
Iteration 101, loss = 0.01531350
Iteration 102, loss = 0.01555127
Iteration 103, loss = 0.01540900
Iteration 104, loss = 0.01523414
Iteration 105, loss = 0.01548886
Iteration 106, loss = 0.01558436
Iteration 107, loss = 0.01543762
Iteration 108, loss = 0.01546300
Iteration 109, loss = 0.01541526
Iteration 110, loss = 0.01534372
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68036025
Iteration 2, loss = 0.62737317
Iteration 3, loss = 0.54687150
Iteration 4, loss = 0.45212496
Iteration 5, loss = 0.36289729
Iteration 6, loss = 0.28609766
Iteration 7, loss = 0.22434308
Iteration 8, loss = 0.17748391
Iteration 9, loss = 0.14270277
Iteration 10, loss = 0.11716266
Iteration 11, loss = 0.09823815
Iteration 12, loss = 0.08373653
Iteration 13, loss = 0.07278429
Iteration 14, loss = 0.06417213
Iteration 15, loss = 0.05745139
Iteration 16, loss = 0.05199112
Iteration 17, loss = 0.04756574
Iteration 18, loss = 0.04384270
Iteration 19, loss = 0.04064743
Iteration 20, loss = 0.03815563
Iteration 21, loss = 0.03580007
Iteration 22, loss = 0.03387412
Iteration 23, loss = 0.03227957
Iteration 24, loss = 0.03064095
Iteration 25, loss = 0.02950252
Iteration 26, loss = 0.02854589
Iteration 27, loss = 0.02737585
Iteration 28, loss = 0.02657749
Iteration 29, loss = 0.02575641
Iteration 30, loss = 0.02498357
Iteration 31, loss = 0.02425053
Iteration 32, loss = 0.02389584
Iteration 33, loss = 0.02330513
Iteration 34, loss = 0.02282160
Iteration 35, loss = 0.02221001
Iteration 36, loss = 0.02182917
Iteration 37, loss = 0.02147500
Iteration 38, loss = 0.02126943
Iteration 39, loss = 0.02080492
Iteration 40, loss = 0.02077085
Iteration 41, loss = 0.02019948
Iteration 42, loss = 0.02004177
Iteration 43, loss = 0.01987035
Iteration 44, loss = 0.01972596
Iteration 45, loss = 0.01931538
Iteration 46, loss = 0.01922404
Iteration 47, loss = 0.01890551
Iteration 48, loss = 0.01895912
Iteration 49, loss = 0.01880963
Iteration 50, loss = 0.01860551
Iteration 51, loss = 0.01831134
Iteration 52, loss = 0.01827101
Iteration 53, loss = 0.01810776
Iteration 54, loss = 0.01813288
Iteration 55, loss = 0.01797605
Iteration 56, loss = 0.01793553
Iteration 57, loss = 0.01806867
Iteration 58, loss = 0.01793506
Iteration 59, loss = 0.01736881
Iteration 60, loss = 0.01739089
Iteration 61, loss = 0.01745501
Iteration 62, loss = 0.01733801
Iteration 63, loss = 0.01730630
Iteration 64, loss = 0.01712056
Iteration 65, loss = 0.01722886
Iteration 66, loss = 0.01728312
Iteration 67, loss = 0.01698860
Iteration 68, loss = 0.01691521
Iteration 69, loss = 0.01696786
Iteration 70, loss = 0.01689918
Iteration 71, loss = 0.01702540
Iteration 72, loss = 0.01686460
Iteration 73, loss = 0.01676505
Iteration 74, loss = 0.01653191
Iteration 75, loss = 0.01655766
Iteration 76, loss = 0.01656540
Iteration 77, loss = 0.01670008
Iteration 78, loss = 0.01651997
Iteration 79, loss = 0.01639598
Iteration 80, loss = 0.01637337
Iteration 81, loss = 0.01659516
Iteration 82, loss = 0.01636055
Iteration 83, loss = 0.01648870
Iteration 84, loss = 0.01654540
Iteration 85, loss = 0.01636598
Iteration 86, loss = 0.01637013
Iteration 87, loss = 0.01626753
Iteration 88, loss = 0.01619310
Iteration 89, loss = 0.01610073
Iteration 90, loss = 0.01611342
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70793966
Iteration 2, loss = 0.64722960
Iteration 3, loss = 0.57426677
Iteration 4, loss = 0.49291894
Iteration 5, loss = 0.41391492
Iteration 6, loss = 0.34065691
Iteration 7, loss = 0.27753478
Iteration 8, loss = 0.22505658
Iteration 9, loss = 0.18319305
Iteration 10, loss = 0.15058167
Iteration 11, loss = 0.12549108
Iteration 12, loss = 0.10609162
Iteration 13, loss = 0.09102591
Iteration 14, loss = 0.07929275
Iteration 15, loss = 0.06998650
Iteration 16, loss = 0.06248820
Iteration 17, loss = 0.05643454
Iteration 18, loss = 0.05128738
Iteration 19, loss = 0.04714217
Iteration 20, loss = 0.04341114
Iteration 21, loss = 0.04032875
Iteration 22, loss = 0.03781865
Iteration 23, loss = 0.03561286
Iteration 24, loss = 0.03365470
Iteration 25, loss = 0.03209627
Iteration 26, loss = 0.03035559
Iteration 27, loss = 0.02915040
Iteration 28, loss = 0.02811507
Iteration 29, loss = 0.02711165
Iteration 30, loss = 0.02600424
Iteration 31, loss = 0.02526992
Iteration 32, loss = 0.02441212
Iteration 33, loss = 0.02363306
Iteration 34, loss = 0.02311151
Iteration 35, loss = 0.02255227
Iteration 36, loss = 0.02199789
Iteration 37, loss = 0.02139854
Iteration 38, loss = 0.02102654
Iteration 39, loss = 0.02065884
Iteration 40, loss = 0.02031799
Iteration 41, loss = 0.01985325
Iteration 42, loss = 0.01949762
Iteration 43, loss = 0.01920519
Iteration 44, loss = 0.01885561
Iteration 45, loss = 0.01851306
Iteration 46, loss = 0.01871816
Iteration 47, loss = 0.01824104
Iteration 48, loss = 0.01792655
Iteration 49, loss = 0.01772840
Iteration 50, loss = 0.01745614
Iteration 51, loss = 0.01734087
Iteration 52, loss = 0.01716174
Iteration 53, loss = 0.01710016
Iteration 54, loss = 0.01685980
Iteration 55, loss = 0.01687038
Iteration 56, loss = 0.01663277
Iteration 57, loss = 0.01659725
Iteration 58, loss = 0.01627923
Iteration 59, loss = 0.01635604
Iteration 60, loss = 0.01618295
Iteration 61, loss = 0.01644270
Iteration 62, loss = 0.01597646
Iteration 63, loss = 0.01583693
Iteration 64, loss = 0.01577090
Iteration 65, loss = 0.01559123
Iteration 66, loss = 0.01564514
Iteration 67, loss = 0.01549672
Iteration 68, loss = 0.01548478
Iteration 69, loss = 0.01542202
Iteration 70, loss = 0.01527174
Iteration 71, loss = 0.01528771
Iteration 72, loss = 0.01522643
Iteration 73, loss = 0.01513780
Iteration 74, loss = 0.01513165
Iteration 75, loss = 0.01500244
Iteration 76, loss = 0.01497079
Iteration 77, loss = 0.01495281
Iteration 78, loss = 0.01474740
Iteration 79, loss = 0.01505422
Iteration 80, loss = 0.01503702
Iteration 81, loss = 0.01491872
Iteration 82, loss = 0.01469789
Iteration 83, loss = 0.01482664
Iteration 84, loss = 0.01448166
Iteration 85, loss = 0.01453023
Iteration 86, loss = 0.01463559
Iteration 87, loss = 0.01458563
Iteration 88, loss = 0.01463039
Iteration 89, loss = 0.01421967
Iteration 90, loss = 0.01443260
Iteration 91, loss = 0.01425381
Iteration 92, loss = 0.01429743
Iteration 93, loss = 0.01445974
Iteration 94, loss = 0.01446492
Iteration 95, loss = 0.01426994
Iteration 96, loss = 0.01432798
Iteration 97, loss = 0.01424257
Iteration 98, loss = 0.01422908
Iteration 99, loss = 0.01425532
Iteration 100, loss = 0.01418670
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67533690
Iteration 2, loss = 0.60720728
Iteration 3, loss = 0.51133737
Iteration 4, loss = 0.41026778
Iteration 5, loss = 0.31912061
Iteration 6, loss = 0.24500336
Iteration 7, loss = 0.18988626
Iteration 8, loss = 0.14960610
Iteration 9, loss = 0.12078718
Iteration 10, loss = 0.09981369
Iteration 11, loss = 0.08455860
Iteration 12, loss = 0.07311258
Iteration 13, loss = 0.06413234
Iteration 14, loss = 0.05732344
Iteration 15, loss = 0.05188536
Iteration 16, loss = 0.04739054
Iteration 17, loss = 0.04357554
Iteration 18, loss = 0.04034838
Iteration 19, loss = 0.03854264
Iteration 20, loss = 0.03623334
Iteration 21, loss = 0.03425405
Iteration 22, loss = 0.03255604
Iteration 23, loss = 0.03092936
Iteration 24, loss = 0.02981997
Iteration 25, loss = 0.02863133
Iteration 26, loss = 0.02763879
Iteration 27, loss = 0.02672731
Iteration 28, loss = 0.02623499
Iteration 29, loss = 0.02583489
Iteration 30, loss = 0.02488744
Iteration 31, loss = 0.02422967
Iteration 32, loss = 0.02354854
Iteration 33, loss = 0.02307958
Iteration 34, loss = 0.02278963
Iteration 35, loss = 0.02233772
Iteration 36, loss = 0.02199989
Iteration 37, loss = 0.02165437
Iteration 38, loss = 0.02121822
Iteration 39, loss = 0.02093129
Iteration 40, loss = 0.02081566
Iteration 41, loss = 0.02062229
Iteration 42, loss = 0.02037779
Iteration 43, loss = 0.02058028
Iteration 44, loss = 0.02042335
Iteration 45, loss = 0.02030591
Iteration 46, loss = 0.01984656
Iteration 47, loss = 0.01963864
Iteration 48, loss = 0.01929908
Iteration 49, loss = 0.01932669
Iteration 50, loss = 0.01923536
Iteration 51, loss = 0.01888606
Iteration 52, loss = 0.01938223
Iteration 53, loss = 0.01937820
Iteration 54, loss = 0.01919034
Iteration 55, loss = 0.01908845
Iteration 56, loss = 0.01864752
Iteration 57, loss = 0.01831840
Iteration 58, loss = 0.01840089
Iteration 59, loss = 0.01828367
Iteration 60, loss = 0.01822867
Iteration 61, loss = 0.01896194
Iteration 62, loss = 0.01944488
Iteration 63, loss = 0.01870715
Iteration 64, loss = 0.01821275
Iteration 65, loss = 0.01797106
Iteration 66, loss = 0.01779439
Iteration 67, loss = 0.01776816
Iteration 68, loss = 0.01770427
Iteration 69, loss = 0.01751393
Iteration 70, loss = 0.01759043
Iteration 71, loss = 0.01740474
Iteration 72, loss = 0.01749396
Iteration 73, loss = 0.01729184
Iteration 74, loss = 0.01695397
Iteration 75, loss = 0.01812535
Iteration 76, loss = 0.01801787
Iteration 77, loss = 0.01779998
Iteration 78, loss = 0.01790706
Iteration 79, loss = 0.01740720
Iteration 80, loss = 0.01745826
Iteration 81, loss = 0.01751928
Iteration 82, loss = 0.01746001
Iteration 83, loss = 0.01729197
Iteration 84, loss = 0.01706021
Iteration 85, loss = 0.01705792
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67861863
Iteration 2, loss = 0.64094161
Iteration 3, loss = 0.58866384
Iteration 4, loss = 0.51879764
Iteration 5, loss = 0.44366822
Iteration 6, loss = 0.37241757
Iteration 7, loss = 0.30899529
Iteration 8, loss = 0.25538770
Iteration 9, loss = 0.21119570
Iteration 10, loss = 0.17573523
Iteration 11, loss = 0.14758903
Iteration 12, loss = 0.12538744
Iteration 13, loss = 0.10794206
Iteration 14, loss = 0.09386585
Iteration 15, loss = 0.08268821
Iteration 16, loss = 0.07352938
Iteration 17, loss = 0.06599050
Iteration 18, loss = 0.05979522
Iteration 19, loss = 0.05447218
Iteration 20, loss = 0.05005544
Iteration 21, loss = 0.04631969
Iteration 22, loss = 0.04318223
Iteration 23, loss = 0.04017041
Iteration 24, loss = 0.03779050
Iteration 25, loss = 0.03561258
Iteration 26, loss = 0.03370881
Iteration 27, loss = 0.03196153
Iteration 28, loss = 0.03057302
Iteration 29, loss = 0.02919236
Iteration 30, loss = 0.02797583
Iteration 31, loss = 0.02689351
Iteration 32, loss = 0.02594580
Iteration 33, loss = 0.02493938
Iteration 34, loss = 0.02422342
Iteration 35, loss = 0.02352187
Iteration 36, loss = 0.02282881
Iteration 37, loss = 0.02216712
Iteration 38, loss = 0.02160187
Iteration 39, loss = 0.02099257
Iteration 40, loss = 0.02057470
Iteration 41, loss = 0.02006112
Iteration 42, loss = 0.01968747
Iteration 43, loss = 0.01922708
Iteration 44, loss = 0.01889721
Iteration 45, loss = 0.01849588
Iteration 46, loss = 0.01815744
Iteration 47, loss = 0.01803594
Iteration 48, loss = 0.01770675
Iteration 49, loss = 0.01747251
Iteration 50, loss = 0.01721760
Iteration 51, loss = 0.01694494
Iteration 52, loss = 0.01666194
Iteration 53, loss = 0.01654645
Iteration 54, loss = 0.01639682
Iteration 55, loss = 0.01614181
Iteration 56, loss = 0.01617306
Iteration 57, loss = 0.01598748
Iteration 58, loss = 0.01554134
Iteration 59, loss = 0.01551049
Iteration 60, loss = 0.01539235
Iteration 61, loss = 0.01531530
Iteration 62, loss = 0.01506718
Iteration 63, loss = 0.01497390
Iteration 64, loss = 0.01484903
Iteration 65, loss = 0.01481109
Iteration 66, loss = 0.01464871
Iteration 67, loss = 0.01453023
Iteration 68, loss = 0.01449766
Iteration 69, loss = 0.01433147
Iteration 70, loss = 0.01430164
Iteration 71, loss = 0.01423839
Iteration 72, loss = 0.01421438
Iteration 73, loss = 0.01406069
Iteration 74, loss = 0.01410814
Iteration 75, loss = 0.01401325
Iteration 76, loss = 0.01387395
Iteration 77, loss = 0.01377727
Iteration 78, loss = 0.01384081
Iteration 79, loss = 0.01377354
Iteration 80, loss = 0.01360591
Iteration 81, loss = 0.01353881
Iteration 82, loss = 0.01349305
Iteration 83, loss = 0.01349643
Iteration 84, loss = 0.01344448
Iteration 85, loss = 0.01339611
Iteration 86, loss = 0.01338824
Iteration 87, loss = 0.01341375
Iteration 88, loss = 0.01324610
Iteration 89, loss = 0.01315917
Iteration 90, loss = 0.01311826
Iteration 91, loss = 0.01322595
Iteration 92, loss = 0.01318185
Iteration 93, loss = 0.01291611
Iteration 94, loss = 0.01302080
Iteration 95, loss = 0.01294529
Iteration 96, loss = 0.01294613
Iteration 97, loss = 0.01285697
Iteration 98, loss = 0.01286167
Iteration 99, loss = 0.01302026
Iteration 100, loss = 0.01274240
Iteration 101, loss = 0.01292546
Iteration 102, loss = 0.01266424
Iteration 103, loss = 0.01269659
Iteration 104, loss = 0.01282768
Iteration 105, loss = 0.01263716
Iteration 106, loss = 0.01270377
Iteration 107, loss = 0.01269050
Iteration 108, loss = 0.01260639
Iteration 109, loss = 0.01255580
Iteration 110, loss = 0.01254288
Iteration 111, loss = 0.01264670
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71103881
Iteration 2, loss = 0.67339714
Iteration 3, loss = 0.61780398
Iteration 4, loss = 0.53812045
Iteration 5, loss = 0.45167544
Iteration 6, loss = 0.36857742
Iteration 7, loss = 0.29770383
Iteration 8, loss = 0.23952469
Iteration 9, loss = 0.19432225
Iteration 10, loss = 0.15936728
Iteration 11, loss = 0.13269012
Iteration 12, loss = 0.11222958
Iteration 13, loss = 0.09665817
Iteration 14, loss = 0.08442084
Iteration 15, loss = 0.07479766
Iteration 16, loss = 0.06671907
Iteration 17, loss = 0.06043520
Iteration 18, loss = 0.05514378
Iteration 19, loss = 0.05099583
Iteration 20, loss = 0.04727843
Iteration 21, loss = 0.04409119
Iteration 22, loss = 0.04130122
Iteration 23, loss = 0.03912914
Iteration 24, loss = 0.03705473
Iteration 25, loss = 0.03530485
Iteration 26, loss = 0.03389105
Iteration 27, loss = 0.03267049
Iteration 28, loss = 0.03116754
Iteration 29, loss = 0.03031536
Iteration 30, loss = 0.02937623
Iteration 31, loss = 0.02832614
Iteration 32, loss = 0.02757028
Iteration 33, loss = 0.02701020
Iteration 34, loss = 0.02624324
Iteration 35, loss = 0.02567960
Iteration 36, loss = 0.02518885
Iteration 37, loss = 0.02461253
Iteration 38, loss = 0.02425881
Iteration 39, loss = 0.02377771
Iteration 40, loss = 0.02346828
Iteration 41, loss = 0.02313968
Iteration 42, loss = 0.02268069
Iteration 43, loss = 0.02250061
Iteration 44, loss = 0.02218439
Iteration 45, loss = 0.02184545
Iteration 46, loss = 0.02173926
Iteration 47, loss = 0.02150749
Iteration 48, loss = 0.02113674
Iteration 49, loss = 0.02090989
Iteration 50, loss = 0.02085865
Iteration 51, loss = 0.02072773
Iteration 52, loss = 0.02062972
Iteration 53, loss = 0.02056218
Iteration 54, loss = 0.02015142
Iteration 55, loss = 0.02004459
Iteration 56, loss = 0.02002310
Iteration 57, loss = 0.01998886
Iteration 58, loss = 0.01968984
Iteration 59, loss = 0.01976521
Iteration 60, loss = 0.01953114
Iteration 61, loss = 0.01941839
Iteration 62, loss = 0.01927728
Iteration 63, loss = 0.01932861
Iteration 64, loss = 0.01915988
Iteration 65, loss = 0.01921859
Iteration 66, loss = 0.01895668
Iteration 67, loss = 0.01877762
Iteration 68, loss = 0.01888937
Iteration 69, loss = 0.01867587
Iteration 70, loss = 0.01890462
Iteration 71, loss = 0.01869497
Iteration 72, loss = 0.01870397
Iteration 73, loss = 0.01849250
Iteration 74, loss = 0.01848769
Iteration 75, loss = 0.01855837
Iteration 76, loss = 0.01827417
Iteration 77, loss = 0.01829779
Iteration 78, loss = 0.01834105
Iteration 79, loss = 0.01815669
Iteration 80, loss = 0.01826443
Iteration 81, loss = 0.01819318
Iteration 82, loss = 0.01800858
Iteration 83, loss = 0.01816762
Iteration 84, loss = 0.01823950
Iteration 85, loss = 0.01788453
Iteration 86, loss = 0.01793468
Iteration 87, loss = 0.01800937
Iteration 88, loss = 0.01809097
Iteration 89, loss = 0.01786223
Iteration 90, loss = 0.01793710
Iteration 91, loss = 0.01783002
Iteration 92, loss = 0.01778334
Iteration 93, loss = 0.01769742
Iteration 94, loss = 0.01770760
Iteration 95, loss = 0.01770415
Iteration 96, loss = 0.01790631
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67919213
Iteration 2, loss = 0.62014568
Iteration 3, loss = 0.52509543
Iteration 4, loss = 0.41951357
Iteration 5, loss = 0.32416167
Iteration 6, loss = 0.24766298
Iteration 7, loss = 0.18991384
Iteration 8, loss = 0.14875012
Iteration 9, loss = 0.11969847
Iteration 10, loss = 0.09882006
Iteration 11, loss = 0.08331954
Iteration 12, loss = 0.07197454
Iteration 13, loss = 0.06315181
Iteration 14, loss = 0.05628740
Iteration 15, loss = 0.05088075
Iteration 16, loss = 0.04645338
Iteration 17, loss = 0.04291767
Iteration 18, loss = 0.04021415
Iteration 19, loss = 0.03754595
Iteration 20, loss = 0.03531539
Iteration 21, loss = 0.03346733
Iteration 22, loss = 0.03185652
Iteration 23, loss = 0.03121104
Iteration 24, loss = 0.02996597
Iteration 25, loss = 0.02868978
Iteration 26, loss = 0.02771280
Iteration 27, loss = 0.02664689
Iteration 28, loss = 0.02613783
Iteration 29, loss = 0.02557964
Iteration 30, loss = 0.02466489
Iteration 31, loss = 0.02413065
Iteration 32, loss = 0.02350594
Iteration 33, loss = 0.02304068
Iteration 34, loss = 0.02317873
Iteration 35, loss = 0.02303191
Iteration 36, loss = 0.02277300
Iteration 37, loss = 0.02266727
Iteration 38, loss = 0.02195878
Iteration 39, loss = 0.02150328
Iteration 40, loss = 0.02123002
Iteration 41, loss = 0.02087803
Iteration 42, loss = 0.02057610
Iteration 43, loss = 0.02031812
Iteration 44, loss = 0.01998765
Iteration 45, loss = 0.01996390
Iteration 46, loss = 0.01971805
Iteration 47, loss = 0.01981627
Iteration 48, loss = 0.01931671
Iteration 49, loss = 0.01965706
Iteration 50, loss = 0.01936584
Iteration 51, loss = 0.01905085
Iteration 52, loss = 0.01906650
Iteration 53, loss = 0.01908739
Iteration 54, loss = 0.01941601
Iteration 55, loss = 0.01912485
Iteration 56, loss = 0.01899501
Iteration 57, loss = 0.01878444
Iteration 58, loss = 0.01858749
Iteration 59, loss = 0.01843531
Iteration 60, loss = 0.01839096
Iteration 61, loss = 0.01844736
Iteration 62, loss = 0.01842716
Iteration 63, loss = 0.01853397
Iteration 64, loss = 0.01825757
Iteration 65, loss = 0.01809855
Iteration 66, loss = 0.01797415
Iteration 67, loss = 0.01805377
Iteration 68, loss = 0.01784034
Iteration 69, loss = 0.01788875
Iteration 70, loss = 0.01777167
Iteration 71, loss = 0.01757672
Iteration 72, loss = 0.01749298
Iteration 73, loss = 0.01751103
Iteration 74, loss = 0.01751007
Iteration 75, loss = 0.01734321
Iteration 76, loss = 0.01741564
Iteration 77, loss = 0.01728300
Iteration 78, loss = 0.01735311
Iteration 79, loss = 0.01733533
Iteration 80, loss = 0.01702068
Iteration 81, loss = 0.01705560
Iteration 82, loss = 0.01722940
Iteration 83, loss = 0.01724901
Iteration 84, loss = 0.01723618
Iteration 85, loss = 0.01722396
Iteration 86, loss = 0.01779495
Iteration 87, loss = 0.01746429
Iteration 88, loss = 0.01736199
Iteration 89, loss = 0.01753327
Iteration 90, loss = 0.01727476
Iteration 91, loss = 0.01726986
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68111490
Iteration 2, loss = 0.61831486
Iteration 3, loss = 0.52306533
Iteration 4, loss = 0.42017871
Iteration 5, loss = 0.32883959
Iteration 6, loss = 0.25437486
Iteration 7, loss = 0.19743325
Iteration 8, loss = 0.15604966
Iteration 9, loss = 0.12590479
Iteration 10, loss = 0.10405379
Iteration 11, loss = 0.08795994
Iteration 12, loss = 0.07582923
Iteration 13, loss = 0.06645120
Iteration 14, loss = 0.05908442
Iteration 15, loss = 0.05334950
Iteration 16, loss = 0.04878006
Iteration 17, loss = 0.04492271
Iteration 18, loss = 0.04152066
Iteration 19, loss = 0.03891151
Iteration 20, loss = 0.03672669
Iteration 21, loss = 0.03480512
Iteration 22, loss = 0.03311990
Iteration 23, loss = 0.03164877
Iteration 24, loss = 0.03018342
Iteration 25, loss = 0.02907951
Iteration 26, loss = 0.02796009
Iteration 27, loss = 0.02714922
Iteration 28, loss = 0.02636513
Iteration 29, loss = 0.02579600
Iteration 30, loss = 0.02566069
Iteration 31, loss = 0.02501211
Iteration 32, loss = 0.02444138
Iteration 33, loss = 0.02379629
Iteration 34, loss = 0.02322131
Iteration 35, loss = 0.02260755
Iteration 36, loss = 0.02231971
Iteration 37, loss = 0.02206459
Iteration 38, loss = 0.02287888
Iteration 39, loss = 0.02245298
Iteration 40, loss = 0.02156379
Iteration 41, loss = 0.02111699
Iteration 42, loss = 0.02064828
Iteration 43, loss = 0.02043745
Iteration 44, loss = 0.02030593
Iteration 45, loss = 0.02048045
Iteration 46, loss = 0.02005326
Iteration 47, loss = 0.01968830
Iteration 48, loss = 0.01937558
Iteration 49, loss = 0.01921976
Iteration 50, loss = 0.01898674
Iteration 51, loss = 0.01947285
Iteration 52, loss = 0.01946253
Iteration 53, loss = 0.01916489
Iteration 54, loss = 0.01868861
Iteration 55, loss = 0.01859094
Iteration 56, loss = 0.01832326
Iteration 57, loss = 0.01818231
Iteration 58, loss = 0.01812987
Iteration 59, loss = 0.01792691
Iteration 60, loss = 0.01819680
Iteration 61, loss = 0.01804016
Iteration 62, loss = 0.01808942
Iteration 63, loss = 0.01787862
Iteration 64, loss = 0.01782685
Iteration 65, loss = 0.01766006
Iteration 66, loss = 0.01865446
Iteration 67, loss = 0.01872105
Iteration 68, loss = 0.01825568
Iteration 69, loss = 0.01779865
Iteration 70, loss = 0.01772947
Iteration 71, loss = 0.01748174
Iteration 72, loss = 0.01724317
Iteration 73, loss = 0.01731235
Iteration 74, loss = 0.01704174
Iteration 75, loss = 0.01695967
Iteration 76, loss = 0.01704914
Iteration 77, loss = 0.01682362
Iteration 78, loss = 0.01687812
Iteration 79, loss = 0.01695227
Iteration 80, loss = 0.01751265
Iteration 81, loss = 0.01775895
Iteration 82, loss = 0.01769313
Iteration 83, loss = 0.01832017
Iteration 84, loss = 0.01857963
Iteration 85, loss = 0.01781027
Iteration 86, loss = 0.01726564
Iteration 87, loss = 0.01716157
Iteration 88, loss = 0.01697033
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70762021
Iteration 2, loss = 0.68846537
Iteration 3, loss = 0.66928560
Iteration 4, loss = 0.64740833
Iteration 5, loss = 0.62190089
Iteration 6, loss = 0.59185332
Iteration 7, loss = 0.55794153
Iteration 8, loss = 0.52055365
Iteration 9, loss = 0.48092829
Iteration 10, loss = 0.44094651
Iteration 11, loss = 0.40181884
Iteration 12, loss = 0.36453831
Iteration 13, loss = 0.32979572
Iteration 14, loss = 0.29768397
Iteration 15, loss = 0.26836356
Iteration 16, loss = 0.24174482
Iteration 17, loss = 0.21780063
Iteration 18, loss = 0.19643833
Iteration 19, loss = 0.17745134
Iteration 20, loss = 0.16060682
Iteration 21, loss = 0.14566767
Iteration 22, loss = 0.13250582
Iteration 23, loss = 0.12083280
Iteration 24, loss = 0.11057440
Iteration 25, loss = 0.10142262
Iteration 26, loss = 0.09338109
Iteration 27, loss = 0.08616535
Iteration 28, loss = 0.07981252
Iteration 29, loss = 0.07409220
Iteration 30, loss = 0.06899090
Iteration 31, loss = 0.06445216
Iteration 32, loss = 0.06029660
Iteration 33, loss = 0.05655486
Iteration 34, loss = 0.05318238
Iteration 35, loss = 0.05009438
Iteration 36, loss = 0.04735930
Iteration 37, loss = 0.04476137
Iteration 38, loss = 0.04247833
Iteration 39, loss = 0.04032124
Iteration 40, loss = 0.03836227
Iteration 41, loss = 0.03660282
Iteration 42, loss = 0.03491367
Iteration 43, loss = 0.03336278
Iteration 44, loss = 0.03197213
Iteration 45, loss = 0.03063354
Iteration 46, loss = 0.02944456
Iteration 47, loss = 0.02827369
Iteration 48, loss = 0.02721820
Iteration 49, loss = 0.02620333
Iteration 50, loss = 0.02526535
Iteration 51, loss = 0.02438961
Iteration 52, loss = 0.02359528
Iteration 53, loss = 0.02282683
Iteration 54, loss = 0.02210015
Iteration 55, loss = 0.02139911
Iteration 56, loss = 0.02074692
Iteration 57, loss = 0.02018932
Iteration 58, loss = 0.01959207
Iteration 59, loss = 0.01902797
Iteration 60, loss = 0.01852378
Iteration 61, loss = 0.01804932
Iteration 62, loss = 0.01757439
Iteration 63, loss = 0.01718027
Iteration 64, loss = 0.01678788
Iteration 65, loss = 0.01635767
Iteration 66, loss = 0.01599541
Iteration 67, loss = 0.01560859
Iteration 68, loss = 0.01525531
Iteration 69, loss = 0.01498958
Iteration 70, loss = 0.01464496
Iteration 71, loss = 0.01433881
Iteration 72, loss = 0.01408498
Iteration 73, loss = 0.01380845
Iteration 74, loss = 0.01354281
Iteration 75, loss = 0.01327141
Iteration 76, loss = 0.01303721
Iteration 77, loss = 0.01281745
Iteration 78, loss = 0.01260640
Iteration 79, loss = 0.01241246
Iteration 80, loss = 0.01219695
Iteration 81, loss = 0.01202158
Iteration 82, loss = 0.01179792
Iteration 83, loss = 0.01161357
Iteration 84, loss = 0.01148545
Iteration 85, loss = 0.01131152
Iteration 86, loss = 0.01112313
Iteration 87, loss = 0.01107089
Iteration 88, loss = 0.01087295
Iteration 89, loss = 0.01070657
Iteration 90, loss = 0.01056699
Iteration 91, loss = 0.01041611
Iteration 92, loss = 0.01030451
Iteration 93, loss = 0.01016383
Iteration 94, loss = 0.01005158
Iteration 95, loss = 0.00991479
Iteration 96, loss = 0.00979304
Iteration 97, loss = 0.00967451
Iteration 98, loss = 0.00961167
Iteration 99, loss = 0.00951387
Iteration 100, loss = 0.00944506
Iteration 101, loss = 0.00927720
Iteration 102, loss = 0.00925757
Iteration 103, loss = 0.00911404
Iteration 104, loss = 0.00902037
Iteration 105, loss = 0.00894445
Iteration 106, loss = 0.00883876
Iteration 107, loss = 0.00877679
Iteration 108, loss = 0.00872364
Iteration 109, loss = 0.00862193
Iteration 110, loss = 0.00856355
Iteration 111, loss = 0.00850187
Iteration 112, loss = 0.00841077
Iteration 113, loss = 0.00840116
Iteration 114, loss = 0.00826341
Iteration 115, loss = 0.00825543
Iteration 116, loss = 0.00814009
Iteration 117, loss = 0.00809016
Iteration 118, loss = 0.00805556
Iteration 119, loss = 0.00800205
Iteration 120, loss = 0.00793001
Iteration 121, loss = 0.00787405
Iteration 122, loss = 0.00783920
Iteration 123, loss = 0.00781311
Iteration 124, loss = 0.00773115
Iteration 125, loss = 0.00769918
Iteration 126, loss = 0.00763895
Iteration 127, loss = 0.00760368
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72475246
Iteration 2, loss = 0.69592000
Iteration 3, loss = 0.65760378
Iteration 4, loss = 0.61043672
Iteration 5, loss = 0.55424952
Iteration 6, loss = 0.49298484
Iteration 7, loss = 0.43108135
Iteration 8, loss = 0.37241833
Iteration 9, loss = 0.31891313
Iteration 10, loss = 0.27180740
Iteration 11, loss = 0.23146890
Iteration 12, loss = 0.19780425
Iteration 13, loss = 0.16966516
Iteration 14, loss = 0.14675609
Iteration 15, loss = 0.12795853
Iteration 16, loss = 0.11248748
Iteration 17, loss = 0.09969932
Iteration 18, loss = 0.08912305
Iteration 19, loss = 0.08021541
Iteration 20, loss = 0.07266741
Iteration 21, loss = 0.06636282
Iteration 22, loss = 0.06094260
Iteration 23, loss = 0.05627983
Iteration 24, loss = 0.05220292
Iteration 25, loss = 0.04870812
Iteration 26, loss = 0.04572159
Iteration 27, loss = 0.04310428
Iteration 28, loss = 0.04055768
Iteration 29, loss = 0.03844168
Iteration 30, loss = 0.03668963
Iteration 31, loss = 0.03475465
Iteration 32, loss = 0.03332308
Iteration 33, loss = 0.03179264
Iteration 34, loss = 0.03052107
Iteration 35, loss = 0.02937318
Iteration 36, loss = 0.02849257
Iteration 37, loss = 0.02744059
Iteration 38, loss = 0.02652800
Iteration 39, loss = 0.02583139
Iteration 40, loss = 0.02495856
Iteration 41, loss = 0.02434613
Iteration 42, loss = 0.02365524
Iteration 43, loss = 0.02300865
Iteration 44, loss = 0.02242727
Iteration 45, loss = 0.02194639
Iteration 46, loss = 0.02156974
Iteration 47, loss = 0.02108528
Iteration 48, loss = 0.02068360
Iteration 49, loss = 0.02029232
Iteration 50, loss = 0.01986563
Iteration 51, loss = 0.01960677
Iteration 52, loss = 0.01913666
Iteration 53, loss = 0.01898387
Iteration 54, loss = 0.01862746
Iteration 55, loss = 0.01848393
Iteration 56, loss = 0.01814931
Iteration 57, loss = 0.01786083
Iteration 58, loss = 0.01757947
Iteration 59, loss = 0.01742782
Iteration 60, loss = 0.01714001
Iteration 61, loss = 0.01699383
Iteration 62, loss = 0.01679926
Iteration 63, loss = 0.01667062
Iteration 64, loss = 0.01646263
Iteration 65, loss = 0.01626498
Iteration 66, loss = 0.01622641
Iteration 67, loss = 0.01602570
Iteration 68, loss = 0.01591395
Iteration 69, loss = 0.01569835
Iteration 70, loss = 0.01562799
Iteration 71, loss = 0.01555057
Iteration 72, loss = 0.01553727
Iteration 73, loss = 0.01518522
Iteration 74, loss = 0.01516058
Iteration 75, loss = 0.01505306
Iteration 76, loss = 0.01491497
Iteration 77, loss = 0.01481723
Iteration 78, loss = 0.01475322
Iteration 79, loss = 0.01452906
Iteration 80, loss = 0.01452637
Iteration 81, loss = 0.01447837
Iteration 82, loss = 0.01440472
Iteration 83, loss = 0.01435477
Iteration 84, loss = 0.01414246
Iteration 85, loss = 0.01423233
Iteration 86, loss = 0.01432517
Iteration 87, loss = 0.01414111
Iteration 88, loss = 0.01403336
Iteration 89, loss = 0.01397113
Iteration 90, loss = 0.01395144
Iteration 91, loss = 0.01394846
Iteration 92, loss = 0.01368458
Iteration 93, loss = 0.01367290
Iteration 94, loss = 0.01371778
Iteration 95, loss = 0.01358279
Iteration 96, loss = 0.01360732
Iteration 97, loss = 0.01357728
Iteration 98, loss = 0.01352961
Iteration 99, loss = 0.01340756
Iteration 100, loss = 0.01334366
Iteration 101, loss = 0.01344374
Iteration 102, loss = 0.01334426
Iteration 103, loss = 0.01330003
Iteration 104, loss = 0.01321863
Iteration 105, loss = 0.01310629
Iteration 106, loss = 0.01318623
Iteration 107, loss = 0.01305362
Iteration 108, loss = 0.01317109
Iteration 109, loss = 0.01297689
Iteration 110, loss = 0.01295952
Iteration 111, loss = 0.01301322
Iteration 112, loss = 0.01313831
Iteration 113, loss = 0.01300556
Iteration 114, loss = 0.01287462
Iteration 115, loss = 0.01289187
Iteration 116, loss = 0.01301320
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68184997
Iteration 2, loss = 0.65025861
Iteration 3, loss = 0.61019855
Iteration 4, loss = 0.55820868
Iteration 5, loss = 0.49949104
Iteration 6, loss = 0.43824188
Iteration 7, loss = 0.37810095
Iteration 8, loss = 0.32269129
Iteration 9, loss = 0.27431538
Iteration 10, loss = 0.23283057
Iteration 11, loss = 0.19797849
Iteration 12, loss = 0.16947545
Iteration 13, loss = 0.14600281
Iteration 14, loss = 0.12680383
Iteration 15, loss = 0.11101093
Iteration 16, loss = 0.09816153
Iteration 17, loss = 0.08758470
Iteration 18, loss = 0.07860415
Iteration 19, loss = 0.07107282
Iteration 20, loss = 0.06485338
Iteration 21, loss = 0.05945291
Iteration 22, loss = 0.05484510
Iteration 23, loss = 0.05088395
Iteration 24, loss = 0.04736329
Iteration 25, loss = 0.04436950
Iteration 26, loss = 0.04162604
Iteration 27, loss = 0.03930026
Iteration 28, loss = 0.03727953
Iteration 29, loss = 0.03533676
Iteration 30, loss = 0.03355945
Iteration 31, loss = 0.03217975
Iteration 32, loss = 0.03076963
Iteration 33, loss = 0.02947473
Iteration 34, loss = 0.02846373
Iteration 35, loss = 0.02737013
Iteration 36, loss = 0.02632483
Iteration 37, loss = 0.02560832
Iteration 38, loss = 0.02480791
Iteration 39, loss = 0.02407372
Iteration 40, loss = 0.02340562
Iteration 41, loss = 0.02276053
Iteration 42, loss = 0.02203905
Iteration 43, loss = 0.02168877
Iteration 44, loss = 0.02110912
Iteration 45, loss = 0.02062780
Iteration 46, loss = 0.02019301
Iteration 47, loss = 0.01983185
Iteration 48, loss = 0.01942538
Iteration 49, loss = 0.01889700
Iteration 50, loss = 0.01861287
Iteration 51, loss = 0.01827545
Iteration 52, loss = 0.01807552
Iteration 53, loss = 0.01771179
Iteration 54, loss = 0.01749453
Iteration 55, loss = 0.01730930
Iteration 56, loss = 0.01706024
Iteration 57, loss = 0.01684303
Iteration 58, loss = 0.01661032
Iteration 59, loss = 0.01645445
Iteration 60, loss = 0.01623506
Iteration 61, loss = 0.01605713
Iteration 62, loss = 0.01581108
Iteration 63, loss = 0.01576122
Iteration 64, loss = 0.01552254
Iteration 65, loss = 0.01539669
Iteration 66, loss = 0.01509031
Iteration 67, loss = 0.01501611
Iteration 68, loss = 0.01486091
Iteration 69, loss = 0.01488552
Iteration 70, loss = 0.01471014
Iteration 71, loss = 0.01457569
Iteration 72, loss = 0.01441388
Iteration 73, loss = 0.01440695
Iteration 74, loss = 0.01423062
Iteration 75, loss = 0.01411745
Iteration 76, loss = 0.01406576
Iteration 77, loss = 0.01390631
Iteration 78, loss = 0.01383716
Iteration 79, loss = 0.01388571
Iteration 80, loss = 0.01381796
Iteration 81, loss = 0.01362263
Iteration 82, loss = 0.01342933
Iteration 83, loss = 0.01343629
Iteration 84, loss = 0.01338222
Iteration 85, loss = 0.01353622
Iteration 86, loss = 0.01322697
Iteration 87, loss = 0.01315869
Iteration 88, loss = 0.01310188
Iteration 89, loss = 0.01310747
Iteration 90, loss = 0.01301021
Iteration 91, loss = 0.01297744
Iteration 92, loss = 0.01298657
Iteration 93, loss = 0.01289196
Iteration 94, loss = 0.01287764
Iteration 95, loss = 0.01269337
Iteration 96, loss = 0.01271267
Iteration 97, loss = 0.01265180
Iteration 98, loss = 0.01257612
Iteration 99, loss = 0.01253608
Iteration 100, loss = 0.01254869
Iteration 101, loss = 0.01244850
Iteration 102, loss = 0.01254035
Iteration 103, loss = 0.01242520
Iteration 104, loss = 0.01238041
Iteration 105, loss = 0.01241280
Iteration 106, loss = 0.01230659
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69690729
Iteration 2, loss = 0.64348244
Iteration 3, loss = 0.56929857
Iteration 4, loss = 0.47595910
Iteration 5, loss = 0.38309063
Iteration 6, loss = 0.30203876
Iteration 7, loss = 0.23719802
Iteration 8, loss = 0.18801009
Iteration 9, loss = 0.15128062
Iteration 10, loss = 0.12426573
Iteration 11, loss = 0.10418851
Iteration 12, loss = 0.08910976
Iteration 13, loss = 0.07740896
Iteration 14, loss = 0.06841358
Iteration 15, loss = 0.06104144
Iteration 16, loss = 0.05534005
Iteration 17, loss = 0.05055563
Iteration 18, loss = 0.04667839
Iteration 19, loss = 0.04326266
Iteration 20, loss = 0.04054321
Iteration 21, loss = 0.03821157
Iteration 22, loss = 0.03605895
Iteration 23, loss = 0.03434685
Iteration 24, loss = 0.03262904
Iteration 25, loss = 0.03160821
Iteration 26, loss = 0.03016026
Iteration 27, loss = 0.02916987
Iteration 28, loss = 0.02810970
Iteration 29, loss = 0.02744755
Iteration 30, loss = 0.02651012
Iteration 31, loss = 0.02588516
Iteration 32, loss = 0.02516283
Iteration 33, loss = 0.02457366
Iteration 34, loss = 0.02414266
Iteration 35, loss = 0.02368495
Iteration 36, loss = 0.02322533
Iteration 37, loss = 0.02268970
Iteration 38, loss = 0.02236525
Iteration 39, loss = 0.02208532
Iteration 40, loss = 0.02175051
Iteration 41, loss = 0.02176261
Iteration 42, loss = 0.02114037
Iteration 43, loss = 0.02086879
Iteration 44, loss = 0.02080043
Iteration 45, loss = 0.02060552
Iteration 46, loss = 0.02037566
Iteration 47, loss = 0.02013577
Iteration 48, loss = 0.01991909
Iteration 49, loss = 0.01982344
Iteration 50, loss = 0.01954902
Iteration 51, loss = 0.01955361
Iteration 52, loss = 0.01913654
Iteration 53, loss = 0.01906045
Iteration 54, loss = 0.01893490
Iteration 55, loss = 0.01918108
Iteration 56, loss = 0.01888956
Iteration 57, loss = 0.01855805
Iteration 58, loss = 0.01859117
Iteration 59, loss = 0.01838157
Iteration 60, loss = 0.01841076
Iteration 61, loss = 0.01835572
Iteration 62, loss = 0.01834007
Iteration 63, loss = 0.01814566
Iteration 64, loss = 0.01810191
Iteration 65, loss = 0.01814639
Iteration 66, loss = 0.01779997
Iteration 67, loss = 0.01785916
Iteration 68, loss = 0.01783957
Iteration 69, loss = 0.01778739
Iteration 70, loss = 0.01774843
Iteration 71, loss = 0.01758896
Iteration 72, loss = 0.01749439
Iteration 73, loss = 0.01762786
Iteration 74, loss = 0.01752010
Iteration 75, loss = 0.01742744
Iteration 76, loss = 0.01735818
Iteration 77, loss = 0.01751312
Iteration 78, loss = 0.01743739
Iteration 79, loss = 0.01716862
Iteration 80, loss = 0.01733724
Iteration 81, loss = 0.01734114
Iteration 82, loss = 0.01719212
Iteration 83, loss = 0.01716202
Iteration 84, loss = 0.01726584
Iteration 85, loss = 0.01694795
Iteration 86, loss = 0.01709571
Iteration 87, loss = 0.01694033
Iteration 88, loss = 0.01683683
Iteration 89, loss = 0.01689590
Iteration 90, loss = 0.01707061
Iteration 91, loss = 0.01701376
Iteration 92, loss = 0.01694245
Iteration 93, loss = 0.01680842
Iteration 94, loss = 0.01694783
Iteration 95, loss = 0.01667220
Iteration 96, loss = 0.01679609
Iteration 97, loss = 0.01655163
Iteration 98, loss = 0.01696769
Iteration 99, loss = 0.01655083
Iteration 100, loss = 0.01666439
Iteration 101, loss = 0.01659627
Iteration 102, loss = 0.01650975
Iteration 103, loss = 0.01669496
Iteration 104, loss = 0.01655907
Iteration 105, loss = 0.01682290
Iteration 106, loss = 0.01679772
Iteration 107, loss = 0.01671671
Iteration 108, loss = 0.01650892
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67824553
Iteration 2, loss = 0.61944367
Iteration 3, loss = 0.53660904
Iteration 4, loss = 0.44382742
Iteration 5, loss = 0.35493616
Iteration 6, loss = 0.27890827
Iteration 7, loss = 0.21881377
Iteration 8, loss = 0.17347672
Iteration 9, loss = 0.13990807
Iteration 10, loss = 0.11514668
Iteration 11, loss = 0.09691621
Iteration 12, loss = 0.08317368
Iteration 13, loss = 0.07248263
Iteration 14, loss = 0.06420068
Iteration 15, loss = 0.05768221
Iteration 16, loss = 0.05239731
Iteration 17, loss = 0.04809010
Iteration 18, loss = 0.04438843
Iteration 19, loss = 0.04152306
Iteration 20, loss = 0.03893431
Iteration 21, loss = 0.03673354
Iteration 22, loss = 0.03480588
Iteration 23, loss = 0.03338909
Iteration 24, loss = 0.03194054
Iteration 25, loss = 0.03070209
Iteration 26, loss = 0.02956744
Iteration 27, loss = 0.02856693
Iteration 28, loss = 0.02782496
Iteration 29, loss = 0.02715545
Iteration 30, loss = 0.02637063
Iteration 31, loss = 0.02586549
Iteration 32, loss = 0.02524298
Iteration 33, loss = 0.02466372
Iteration 34, loss = 0.02406559
Iteration 35, loss = 0.02400456
Iteration 36, loss = 0.02333096
Iteration 37, loss = 0.02305877
Iteration 38, loss = 0.02304470
Iteration 39, loss = 0.02241182
Iteration 40, loss = 0.02221640
Iteration 41, loss = 0.02182033
Iteration 42, loss = 0.02198451
Iteration 43, loss = 0.02128073
Iteration 44, loss = 0.02106371
Iteration 45, loss = 0.02093796
Iteration 46, loss = 0.02090284
Iteration 47, loss = 0.02075955
Iteration 48, loss = 0.02050093
Iteration 49, loss = 0.02042445
Iteration 50, loss = 0.02005138
Iteration 51, loss = 0.02007431
Iteration 52, loss = 0.01997136
Iteration 53, loss = 0.01995048
Iteration 54, loss = 0.01961356
Iteration 55, loss = 0.01958861
Iteration 56, loss = 0.01934569
Iteration 57, loss = 0.01960456
Iteration 58, loss = 0.01926225
Iteration 59, loss = 0.01926327
Iteration 60, loss = 0.01912523
Iteration 61, loss = 0.01904196
Iteration 62, loss = 0.01907317
Iteration 63, loss = 0.01904466
Iteration 64, loss = 0.01891442
Iteration 65, loss = 0.01883344
Iteration 66, loss = 0.01884292
Iteration 67, loss = 0.01860569
Iteration 68, loss = 0.01875096
Iteration 69, loss = 0.01840512
Iteration 70, loss = 0.01849595
Iteration 71, loss = 0.01855226
Iteration 72, loss = 0.01846356
Iteration 73, loss = 0.01839957
Iteration 74, loss = 0.01824489
Iteration 75, loss = 0.01846869
Iteration 76, loss = 0.01844927
Iteration 77, loss = 0.01820975
Iteration 78, loss = 0.01834631
Iteration 79, loss = 0.01817229
Iteration 80, loss = 0.01804563
Iteration 81, loss = 0.01819623
Iteration 82, loss = 0.01817564
Iteration 83, loss = 0.01802844
Iteration 84, loss = 0.01798917
Iteration 85, loss = 0.01813118
Iteration 86, loss = 0.01798253
Iteration 87, loss = 0.01800703
Iteration 88, loss = 0.01796057
Iteration 89, loss = 0.01811173
Iteration 90, loss = 0.01796482
Iteration 91, loss = 0.01785327
Iteration 92, loss = 0.01808111
Iteration 93, loss = 0.01788845
Iteration 94, loss = 0.01771209
Iteration 95, loss = 0.01792359
Iteration 96, loss = 0.01788284
Iteration 97, loss = 0.01760779
Iteration 98, loss = 0.01773005
Iteration 99, loss = 0.01783599
Iteration 100, loss = 0.01761807
Iteration 101, loss = 0.01762573
Iteration 102, loss = 0.01755838
Iteration 103, loss = 0.01783440
Iteration 104, loss = 0.01787299
Iteration 105, loss = 0.01745570
Iteration 106, loss = 0.01773868
Iteration 107, loss = 0.01790318
Iteration 108, loss = 0.01756744
Iteration 109, loss = 0.01747635
Iteration 110, loss = 0.01787987
Iteration 111, loss = 0.01782725
Iteration 112, loss = 0.01767609
Iteration 113, loss = 0.01742829
Iteration 114, loss = 0.01758121
Iteration 115, loss = 0.01750313
Iteration 116, loss = 0.01737136
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70351176
Iteration 2, loss = 0.68707273
Iteration 3, loss = 0.67115894
Iteration 4, loss = 0.65198468
Iteration 5, loss = 0.62812504
Iteration 6, loss = 0.59928040
Iteration 7, loss = 0.56628398
Iteration 8, loss = 0.53050469
Iteration 9, loss = 0.49268501
Iteration 10, loss = 0.45461230
Iteration 11, loss = 0.41695593
Iteration 12, loss = 0.38094458
Iteration 13, loss = 0.34676081
Iteration 14, loss = 0.31498579
Iteration 15, loss = 0.28554161
Iteration 16, loss = 0.25873243
Iteration 17, loss = 0.23432594
Iteration 18, loss = 0.21241068
Iteration 19, loss = 0.19267044
Iteration 20, loss = 0.17504817
Iteration 21, loss = 0.15942999
Iteration 22, loss = 0.14562804
Iteration 23, loss = 0.13324354
Iteration 24, loss = 0.12224952
Iteration 25, loss = 0.11258896
Iteration 26, loss = 0.10391486
Iteration 27, loss = 0.09619345
Iteration 28, loss = 0.08941161
Iteration 29, loss = 0.08325085
Iteration 30, loss = 0.07776296
Iteration 31, loss = 0.07282863
Iteration 32, loss = 0.06838385
Iteration 33, loss = 0.06435718
Iteration 34, loss = 0.06075429
Iteration 35, loss = 0.05739245
Iteration 36, loss = 0.05446830
Iteration 37, loss = 0.05162621
Iteration 38, loss = 0.04917251
Iteration 39, loss = 0.04679945
Iteration 40, loss = 0.04472261
Iteration 41, loss = 0.04270844
Iteration 42, loss = 0.04092498
Iteration 43, loss = 0.03929341
Iteration 44, loss = 0.03770106
Iteration 45, loss = 0.03625612
Iteration 46, loss = 0.03492510
Iteration 47, loss = 0.03375031
Iteration 48, loss = 0.03259187
Iteration 49, loss = 0.03145435
Iteration 50, loss = 0.03049819
Iteration 51, loss = 0.02952991
Iteration 52, loss = 0.02858930
Iteration 53, loss = 0.02778707
Iteration 54, loss = 0.02698452
Iteration 55, loss = 0.02621903
Iteration 56, loss = 0.02556664
Iteration 57, loss = 0.02489459
Iteration 58, loss = 0.02424161
Iteration 59, loss = 0.02368241
Iteration 60, loss = 0.02315430
Iteration 61, loss = 0.02256483
Iteration 62, loss = 0.02211320
Iteration 63, loss = 0.02164168
Iteration 64, loss = 0.02112860
Iteration 65, loss = 0.02079883
Iteration 66, loss = 0.02028380
Iteration 67, loss = 0.01992424
Iteration 68, loss = 0.01956751
Iteration 69, loss = 0.01916782
Iteration 70, loss = 0.01882703
Iteration 71, loss = 0.01856289
Iteration 72, loss = 0.01818847
Iteration 73, loss = 0.01788365
Iteration 74, loss = 0.01760291
Iteration 75, loss = 0.01733827
Iteration 76, loss = 0.01711032
Iteration 77, loss = 0.01686488
Iteration 78, loss = 0.01655708
Iteration 79, loss = 0.01635685
Iteration 80, loss = 0.01612439
Iteration 81, loss = 0.01590497
Iteration 82, loss = 0.01571748
Iteration 83, loss = 0.01551434
Iteration 84, loss = 0.01533552
Iteration 85, loss = 0.01515566
Iteration 86, loss = 0.01495983
Iteration 87, loss = 0.01485788
Iteration 88, loss = 0.01466667
Iteration 89, loss = 0.01444577
Iteration 90, loss = 0.01427854
Iteration 91, loss = 0.01420250
Iteration 92, loss = 0.01400552
Iteration 93, loss = 0.01392221
Iteration 94, loss = 0.01374332
Iteration 95, loss = 0.01363041
Iteration 96, loss = 0.01350309
Iteration 97, loss = 0.01336478
Iteration 98, loss = 0.01328155
Iteration 99, loss = 0.01315157
Iteration 100, loss = 0.01304097
Iteration 101, loss = 0.01291563
Iteration 102, loss = 0.01284492
Iteration 103, loss = 0.01269660
Iteration 104, loss = 0.01267175
Iteration 105, loss = 0.01250465
Iteration 106, loss = 0.01243628
Iteration 107, loss = 0.01231746
Iteration 108, loss = 0.01225899
Iteration 109, loss = 0.01214041
Iteration 110, loss = 0.01210146
Iteration 111, loss = 0.01202790
Iteration 112, loss = 0.01191827
Iteration 113, loss = 0.01190196
Iteration 114, loss = 0.01184636
Iteration 115, loss = 0.01169563
Iteration 116, loss = 0.01163350
Iteration 117, loss = 0.01163627
Iteration 118, loss = 0.01150601
Iteration 119, loss = 0.01138906
Iteration 120, loss = 0.01132726
Iteration 121, loss = 0.01131795
Iteration 122, loss = 0.01129804
Iteration 123, loss = 0.01119332
Iteration 124, loss = 0.01112323
Iteration 125, loss = 0.01109226
Iteration 126, loss = 0.01102474
Iteration 127, loss = 0.01101451
Iteration 128, loss = 0.01097587
Iteration 129, loss = 0.01085410
Iteration 130, loss = 0.01087553
Iteration 131, loss = 0.01080858
Iteration 132, loss = 0.01070514
Iteration 133, loss = 0.01069574
Iteration 134, loss = 0.01064514
Iteration 135, loss = 0.01060599
Iteration 136, loss = 0.01064530
Iteration 137, loss = 0.01052063
Iteration 138, loss = 0.01051970
Iteration 139, loss = 0.01047377
Iteration 140, loss = 0.01038807
Iteration 141, loss = 0.01034839
Iteration 142, loss = 0.01035076
Iteration 143, loss = 0.01033898
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70661021
Iteration 2, loss = 0.66581562
Iteration 3, loss = 0.61233798
Iteration 4, loss = 0.54694755
Iteration 5, loss = 0.47503712
Iteration 6, loss = 0.40374020
Iteration 7, loss = 0.33801257
Iteration 8, loss = 0.28105430
Iteration 9, loss = 0.23302190
Iteration 10, loss = 0.19406928
Iteration 11, loss = 0.16293490
Iteration 12, loss = 0.13814153
Iteration 13, loss = 0.11857014
Iteration 14, loss = 0.10311625
Iteration 15, loss = 0.09060171
Iteration 16, loss = 0.08056898
Iteration 17, loss = 0.07233833
Iteration 18, loss = 0.06559435
Iteration 19, loss = 0.05978680
Iteration 20, loss = 0.05503411
Iteration 21, loss = 0.05095520
Iteration 22, loss = 0.04738999
Iteration 23, loss = 0.04436167
Iteration 24, loss = 0.04182296
Iteration 25, loss = 0.03958444
Iteration 26, loss = 0.03749967
Iteration 27, loss = 0.03565288
Iteration 28, loss = 0.03419644
Iteration 29, loss = 0.03277945
Iteration 30, loss = 0.03157068
Iteration 31, loss = 0.03044636
Iteration 32, loss = 0.02936551
Iteration 33, loss = 0.02829622
Iteration 34, loss = 0.02737900
Iteration 35, loss = 0.02669360
Iteration 36, loss = 0.02600996
Iteration 37, loss = 0.02542722
Iteration 38, loss = 0.02469205
Iteration 39, loss = 0.02416467
Iteration 40, loss = 0.02367164
Iteration 41, loss = 0.02328559
Iteration 42, loss = 0.02270046
Iteration 43, loss = 0.02233950
Iteration 44, loss = 0.02206940
Iteration 45, loss = 0.02159695
Iteration 46, loss = 0.02128407
Iteration 47, loss = 0.02102860
Iteration 48, loss = 0.02078129
Iteration 49, loss = 0.02043645
Iteration 50, loss = 0.02033039
Iteration 51, loss = 0.01991821
Iteration 52, loss = 0.01971329
Iteration 53, loss = 0.01945365
Iteration 54, loss = 0.01926295
Iteration 55, loss = 0.01914042
Iteration 56, loss = 0.01890522
Iteration 57, loss = 0.01879183
Iteration 58, loss = 0.01857868
Iteration 59, loss = 0.01851300
Iteration 60, loss = 0.01823015
Iteration 61, loss = 0.01822016
Iteration 62, loss = 0.01827018
Iteration 63, loss = 0.01800571
Iteration 64, loss = 0.01775731
Iteration 65, loss = 0.01761793
Iteration 66, loss = 0.01748239
Iteration 67, loss = 0.01746781
Iteration 68, loss = 0.01742454
Iteration 69, loss = 0.01730562
Iteration 70, loss = 0.01727812
Iteration 71, loss = 0.01713969
Iteration 72, loss = 0.01715290
Iteration 73, loss = 0.01698958
Iteration 74, loss = 0.01684492
Iteration 75, loss = 0.01696524
Iteration 76, loss = 0.01679525
Iteration 77, loss = 0.01665213
Iteration 78, loss = 0.01671089
Iteration 79, loss = 0.01650255
Iteration 80, loss = 0.01666028
Iteration 81, loss = 0.01648150
Iteration 82, loss = 0.01633766
Iteration 83, loss = 0.01631187
Iteration 84, loss = 0.01625089
Iteration 85, loss = 0.01623465
Iteration 86, loss = 0.01618005
Iteration 87, loss = 0.01612035
Iteration 88, loss = 0.01604971
Iteration 89, loss = 0.01595616
Iteration 90, loss = 0.01604327
Iteration 91, loss = 0.01588855
Iteration 92, loss = 0.01604430
Iteration 93, loss = 0.01594841
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71485541
Iteration 2, loss = 0.66310398
Iteration 3, loss = 0.60014374
Iteration 4, loss = 0.53066650
Iteration 5, loss = 0.45665778
Iteration 6, loss = 0.38558596
Iteration 7, loss = 0.32252754
Iteration 8, loss = 0.26831121
Iteration 9, loss = 0.22367361
Iteration 10, loss = 0.18722784
Iteration 11, loss = 0.15797867
Iteration 12, loss = 0.13493151
Iteration 13, loss = 0.11648725
Iteration 14, loss = 0.10147618
Iteration 15, loss = 0.08947603
Iteration 16, loss = 0.07982716
Iteration 17, loss = 0.07181185
Iteration 18, loss = 0.06509413
Iteration 19, loss = 0.05966927
Iteration 20, loss = 0.05499617
Iteration 21, loss = 0.05100628
Iteration 22, loss = 0.04756059
Iteration 23, loss = 0.04456937
Iteration 24, loss = 0.04202834
Iteration 25, loss = 0.03992767
Iteration 26, loss = 0.03793516
Iteration 27, loss = 0.03600816
Iteration 28, loss = 0.03444900
Iteration 29, loss = 0.03304332
Iteration 30, loss = 0.03183653
Iteration 31, loss = 0.03068662
Iteration 32, loss = 0.02966900
Iteration 33, loss = 0.02894062
Iteration 34, loss = 0.02801171
Iteration 35, loss = 0.02731062
Iteration 36, loss = 0.02643397
Iteration 37, loss = 0.02591953
Iteration 38, loss = 0.02527437
Iteration 39, loss = 0.02465418
Iteration 40, loss = 0.02417410
Iteration 41, loss = 0.02375288
Iteration 42, loss = 0.02328453
Iteration 43, loss = 0.02286386
Iteration 44, loss = 0.02265762
Iteration 45, loss = 0.02212559
Iteration 46, loss = 0.02190230
Iteration 47, loss = 0.02142119
Iteration 48, loss = 0.02116726
Iteration 49, loss = 0.02093616
Iteration 50, loss = 0.02068524
Iteration 51, loss = 0.02050454
Iteration 52, loss = 0.02032819
Iteration 53, loss = 0.01999412
Iteration 54, loss = 0.01980624
Iteration 55, loss = 0.01964151
Iteration 56, loss = 0.01951750
Iteration 57, loss = 0.01934496
Iteration 58, loss = 0.01902641
Iteration 59, loss = 0.01890748
Iteration 60, loss = 0.01875606
Iteration 61, loss = 0.01869945
Iteration 62, loss = 0.01873372
Iteration 63, loss = 0.01843910
Iteration 64, loss = 0.01837647
Iteration 65, loss = 0.01809717
Iteration 66, loss = 0.01805951
Iteration 67, loss = 0.01797374
Iteration 68, loss = 0.01789852
Iteration 69, loss = 0.01797085
Iteration 70, loss = 0.01774056
Iteration 71, loss = 0.01774005
Iteration 72, loss = 0.01763644
Iteration 73, loss = 0.01747898
Iteration 74, loss = 0.01745977
Iteration 75, loss = 0.01734276
Iteration 76, loss = 0.01727354
Iteration 77, loss = 0.01723058
Iteration 78, loss = 0.01708748
Iteration 79, loss = 0.01702870
Iteration 80, loss = 0.01698152
Iteration 81, loss = 0.01692248
Iteration 82, loss = 0.01688770
Iteration 83, loss = 0.01692723
Iteration 84, loss = 0.01673194
Iteration 85, loss = 0.01670365
Iteration 86, loss = 0.01655225
Iteration 87, loss = 0.01664458
Iteration 88, loss = 0.01662596
Iteration 89, loss = 0.01643086
Iteration 90, loss = 0.01661468
Iteration 91, loss = 0.01654212
Iteration 92, loss = 0.01639283
Iteration 93, loss = 0.01638587
Iteration 94, loss = 0.01636873
Iteration 95, loss = 0.01628878
Iteration 96, loss = 0.01631682
Iteration 97, loss = 0.01622336
Iteration 98, loss = 0.01617250
Iteration 99, loss = 0.01628052
Iteration 100, loss = 0.01611447
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67937137
Iteration 2, loss = 0.65529602
Iteration 3, loss = 0.62209858
Iteration 4, loss = 0.57014278
Iteration 5, loss = 0.50471006
Iteration 6, loss = 0.43798878
Iteration 7, loss = 0.37505149
Iteration 8, loss = 0.31847614
Iteration 9, loss = 0.26892724
Iteration 10, loss = 0.22741349
Iteration 11, loss = 0.19279196
Iteration 12, loss = 0.16447444
Iteration 13, loss = 0.14163628
Iteration 14, loss = 0.12302007
Iteration 15, loss = 0.10796274
Iteration 16, loss = 0.09556113
Iteration 17, loss = 0.08530822
Iteration 18, loss = 0.07695684
Iteration 19, loss = 0.06974243
Iteration 20, loss = 0.06385279
Iteration 21, loss = 0.05864112
Iteration 22, loss = 0.05448884
Iteration 23, loss = 0.05059092
Iteration 24, loss = 0.04735145
Iteration 25, loss = 0.04437994
Iteration 26, loss = 0.04180766
Iteration 27, loss = 0.03961098
Iteration 28, loss = 0.03759236
Iteration 29, loss = 0.03580421
Iteration 30, loss = 0.03440922
Iteration 31, loss = 0.03274175
Iteration 32, loss = 0.03151107
Iteration 33, loss = 0.03037939
Iteration 34, loss = 0.02935707
Iteration 35, loss = 0.02834592
Iteration 36, loss = 0.02749158
Iteration 37, loss = 0.02669965
Iteration 38, loss = 0.02585436
Iteration 39, loss = 0.02512602
Iteration 40, loss = 0.02457131
Iteration 41, loss = 0.02387607
Iteration 42, loss = 0.02331960
Iteration 43, loss = 0.02277252
Iteration 44, loss = 0.02233895
Iteration 45, loss = 0.02192197
Iteration 46, loss = 0.02143833
Iteration 47, loss = 0.02116802
Iteration 48, loss = 0.02095797
Iteration 49, loss = 0.02044385
Iteration 50, loss = 0.02018213
Iteration 51, loss = 0.01989591
Iteration 52, loss = 0.01942610
Iteration 53, loss = 0.01926115
Iteration 54, loss = 0.01898953
Iteration 55, loss = 0.01879815
Iteration 56, loss = 0.01858859
Iteration 57, loss = 0.01850592
Iteration 58, loss = 0.01805244
Iteration 59, loss = 0.01784433
Iteration 60, loss = 0.01768081
Iteration 61, loss = 0.01754579
Iteration 62, loss = 0.01741867
Iteration 63, loss = 0.01727151
Iteration 64, loss = 0.01713908
Iteration 65, loss = 0.01693115
Iteration 66, loss = 0.01680387
Iteration 67, loss = 0.01684882
Iteration 68, loss = 0.01645795
Iteration 69, loss = 0.01643205
Iteration 70, loss = 0.01637846
Iteration 71, loss = 0.01628752
Iteration 72, loss = 0.01610455
Iteration 73, loss = 0.01600413
Iteration 74, loss = 0.01587832
Iteration 75, loss = 0.01578623
Iteration 76, loss = 0.01575687
Iteration 77, loss = 0.01564269
Iteration 78, loss = 0.01563675
Iteration 79, loss = 0.01562833
Iteration 80, loss = 0.01545476
Iteration 81, loss = 0.01527338
Iteration 82, loss = 0.01525916
Iteration 83, loss = 0.01520005
Iteration 84, loss = 0.01518347
Iteration 85, loss = 0.01509591
Iteration 86, loss = 0.01488824
Iteration 87, loss = 0.01487565
Iteration 88, loss = 0.01495968
Iteration 89, loss = 0.01487547
Iteration 90, loss = 0.01469715
Iteration 91, loss = 0.01475606
Iteration 92, loss = 0.01464437
Iteration 93, loss = 0.01465526
Iteration 94, loss = 0.01445317
Iteration 95, loss = 0.01458270
Iteration 96, loss = 0.01434725
Iteration 97, loss = 0.01437709
Iteration 98, loss = 0.01445768
Iteration 99, loss = 0.01432889
Iteration 100, loss = 0.01430021
Iteration 101, loss = 0.01434482
Iteration 102, loss = 0.01427174
Iteration 103, loss = 0.01425098
Iteration 104, loss = 0.01432192
Iteration 105, loss = 0.01420164
Iteration 106, loss = 0.01419794
Iteration 107, loss = 0.01412412
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68112219
Iteration 2, loss = 0.62687345
Iteration 3, loss = 0.54844434
Iteration 4, loss = 0.45494262
Iteration 5, loss = 0.36620545
Iteration 6, loss = 0.28905105
Iteration 7, loss = 0.22743868
Iteration 8, loss = 0.18060442
Iteration 9, loss = 0.14548992
Iteration 10, loss = 0.11946958
Iteration 11, loss = 0.10016348
Iteration 12, loss = 0.08560474
Iteration 13, loss = 0.07443827
Iteration 14, loss = 0.06558773
Iteration 15, loss = 0.05860605
Iteration 16, loss = 0.05312331
Iteration 17, loss = 0.04852772
Iteration 18, loss = 0.04474834
Iteration 19, loss = 0.04176092
Iteration 20, loss = 0.03921462
Iteration 21, loss = 0.03677571
Iteration 22, loss = 0.03486749
Iteration 23, loss = 0.03316164
Iteration 24, loss = 0.03172742
Iteration 25, loss = 0.03054583
Iteration 26, loss = 0.02931790
Iteration 27, loss = 0.02841718
Iteration 28, loss = 0.02741366
Iteration 29, loss = 0.02682265
Iteration 30, loss = 0.02600990
Iteration 31, loss = 0.02526806
Iteration 32, loss = 0.02471514
Iteration 33, loss = 0.02429330
Iteration 34, loss = 0.02384573
Iteration 35, loss = 0.02322010
Iteration 36, loss = 0.02289843
Iteration 37, loss = 0.02256929
Iteration 38, loss = 0.02229642
Iteration 39, loss = 0.02211181
Iteration 40, loss = 0.02151607
Iteration 41, loss = 0.02155999
Iteration 42, loss = 0.02118376
Iteration 43, loss = 0.02083265
Iteration 44, loss = 0.02077218
Iteration 45, loss = 0.02062345
Iteration 46, loss = 0.02029627
Iteration 47, loss = 0.02026908
Iteration 48, loss = 0.02008157
Iteration 49, loss = 0.02011916
Iteration 50, loss = 0.01956778
Iteration 51, loss = 0.01959550
Iteration 52, loss = 0.01955889
Iteration 53, loss = 0.01912244
Iteration 54, loss = 0.01938254
Iteration 55, loss = 0.01909635
Iteration 56, loss = 0.01914019
Iteration 57, loss = 0.01898393
Iteration 58, loss = 0.01877298
Iteration 59, loss = 0.01878335
Iteration 60, loss = 0.01844752
Iteration 61, loss = 0.01849533
Iteration 62, loss = 0.01844002
Iteration 63, loss = 0.01855500
Iteration 64, loss = 0.01836908
Iteration 65, loss = 0.01815766
Iteration 66, loss = 0.01817364
Iteration 67, loss = 0.01829251
Iteration 68, loss = 0.01855426
Iteration 69, loss = 0.01831005
Iteration 70, loss = 0.01789732
Iteration 71, loss = 0.01804290
Iteration 72, loss = 0.01798007
Iteration 73, loss = 0.01780437
Iteration 74, loss = 0.01776956
Iteration 75, loss = 0.01787666
Iteration 76, loss = 0.01760623
Iteration 77, loss = 0.01776673
Iteration 78, loss = 0.01794911
Iteration 79, loss = 0.01760515
Iteration 80, loss = 0.01756525
Iteration 81, loss = 0.01757754
Iteration 82, loss = 0.01752802
Iteration 83, loss = 0.01735236
Iteration 84, loss = 0.01743997
Iteration 85, loss = 0.01754463
Iteration 86, loss = 0.01745550
Iteration 87, loss = 0.01740170
Iteration 88, loss = 0.01741180
Iteration 89, loss = 0.01725011
Iteration 90, loss = 0.01731882
Iteration 91, loss = 0.01736578
Iteration 92, loss = 0.01729169
Iteration 93, loss = 0.01751949
Iteration 94, loss = 0.01740751
Iteration 95, loss = 0.01718261
Iteration 96, loss = 0.01716966
Iteration 97, loss = 0.01700379
Iteration 98, loss = 0.01716818
Iteration 99, loss = 0.01699721
Iteration 100, loss = 0.01710598
Iteration 101, loss = 0.01706729
Iteration 102, loss = 0.01715304
Iteration 103, loss = 0.01712609
Iteration 104, loss = 0.01698774
Iteration 105, loss = 0.01709279
Iteration 106, loss = 0.01719244
Iteration 107, loss = 0.01693640
Iteration 108, loss = 0.01691814
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69770718
Iteration 2, loss = 0.67325659
Iteration 3, loss = 0.64996200
Iteration 4, loss = 0.62444300
Iteration 5, loss = 0.59554934
Iteration 6, loss = 0.56342397
Iteration 7, loss = 0.52802254
Iteration 8, loss = 0.49080955
Iteration 9, loss = 0.45271325
Iteration 10, loss = 0.41514770
Iteration 11, loss = 0.37879121
Iteration 12, loss = 0.34433637
Iteration 13, loss = 0.31226615
Iteration 14, loss = 0.28258661
Iteration 15, loss = 0.25557796
Iteration 16, loss = 0.23114637
Iteration 17, loss = 0.20907157
Iteration 18, loss = 0.18943226
Iteration 19, loss = 0.17177509
Iteration 20, loss = 0.15617399
Iteration 21, loss = 0.14237582
Iteration 22, loss = 0.13011876
Iteration 23, loss = 0.11925361
Iteration 24, loss = 0.10961126
Iteration 25, loss = 0.10102097
Iteration 26, loss = 0.09345723
Iteration 27, loss = 0.08675265
Iteration 28, loss = 0.08067452
Iteration 29, loss = 0.07528168
Iteration 30, loss = 0.07040813
Iteration 31, loss = 0.06610002
Iteration 32, loss = 0.06214262
Iteration 33, loss = 0.05855437
Iteration 34, loss = 0.05536345
Iteration 35, loss = 0.05240506
Iteration 36, loss = 0.04974524
Iteration 37, loss = 0.04726907
Iteration 38, loss = 0.04500933
Iteration 39, loss = 0.04297365
Iteration 40, loss = 0.04104118
Iteration 41, loss = 0.03928843
Iteration 42, loss = 0.03765449
Iteration 43, loss = 0.03618268
Iteration 44, loss = 0.03477964
Iteration 45, loss = 0.03343060
Iteration 46, loss = 0.03228346
Iteration 47, loss = 0.03111418
Iteration 48, loss = 0.03012367
Iteration 49, loss = 0.02907628
Iteration 50, loss = 0.02815753
Iteration 51, loss = 0.02728908
Iteration 52, loss = 0.02646583
Iteration 53, loss = 0.02573290
Iteration 54, loss = 0.02492323
Iteration 55, loss = 0.02429238
Iteration 56, loss = 0.02371233
Iteration 57, loss = 0.02310181
Iteration 58, loss = 0.02245902
Iteration 59, loss = 0.02192700
Iteration 60, loss = 0.02139527
Iteration 61, loss = 0.02093727
Iteration 62, loss = 0.02040925
Iteration 63, loss = 0.01998364
Iteration 64, loss = 0.01957571
Iteration 65, loss = 0.01914461
Iteration 66, loss = 0.01879380
Iteration 67, loss = 0.01843020
Iteration 68, loss = 0.01804658
Iteration 69, loss = 0.01773133
Iteration 70, loss = 0.01737770
Iteration 71, loss = 0.01711710
Iteration 72, loss = 0.01680381
Iteration 73, loss = 0.01651310
Iteration 74, loss = 0.01619551
Iteration 75, loss = 0.01596266
Iteration 76, loss = 0.01576545
Iteration 77, loss = 0.01551174
Iteration 78, loss = 0.01523916
Iteration 79, loss = 0.01507880
Iteration 80, loss = 0.01480422
Iteration 81, loss = 0.01459646
Iteration 82, loss = 0.01439610
Iteration 83, loss = 0.01423375
Iteration 84, loss = 0.01406741
Iteration 85, loss = 0.01390187
Iteration 86, loss = 0.01373342
Iteration 87, loss = 0.01354304
Iteration 88, loss = 0.01338064
Iteration 89, loss = 0.01328608
Iteration 90, loss = 0.01307545
Iteration 91, loss = 0.01294542
Iteration 92, loss = 0.01283251
Iteration 93, loss = 0.01265922
Iteration 94, loss = 0.01250434
Iteration 95, loss = 0.01248128
Iteration 96, loss = 0.01231850
Iteration 97, loss = 0.01219149
Iteration 98, loss = 0.01207486
Iteration 99, loss = 0.01199291
Iteration 100, loss = 0.01185599
Iteration 101, loss = 0.01174848
Iteration 102, loss = 0.01170109
Iteration 103, loss = 0.01159544
Iteration 104, loss = 0.01148233
Iteration 105, loss = 0.01141521
Iteration 106, loss = 0.01130024
Iteration 107, loss = 0.01116488
Iteration 108, loss = 0.01109750
Iteration 109, loss = 0.01111482
Iteration 110, loss = 0.01095977
Iteration 111, loss = 0.01087828
Iteration 112, loss = 0.01080900
Iteration 113, loss = 0.01078606
Iteration 114, loss = 0.01065352
Iteration 115, loss = 0.01058776
Iteration 116, loss = 0.01051363
Iteration 117, loss = 0.01044921
Iteration 118, loss = 0.01040097
Iteration 119, loss = 0.01030214
Iteration 120, loss = 0.01030592
Iteration 121, loss = 0.01019283
Iteration 122, loss = 0.01020424
Iteration 123, loss = 0.01010872
Iteration 124, loss = 0.01004663
Iteration 125, loss = 0.00995555
Iteration 126, loss = 0.00997103
Iteration 127, loss = 0.00986213
Iteration 128, loss = 0.00982164
Iteration 129, loss = 0.00980569
Iteration 130, loss = 0.00971113
Iteration 131, loss = 0.00966696
Iteration 132, loss = 0.00963564
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68432822
Iteration 2, loss = 0.66348625
Iteration 3, loss = 0.64324132
Iteration 4, loss = 0.62014328
Iteration 5, loss = 0.59256792
Iteration 6, loss = 0.56066829
Iteration 7, loss = 0.52543278
Iteration 8, loss = 0.48816049
Iteration 9, loss = 0.45006423
Iteration 10, loss = 0.41309948
Iteration 11, loss = 0.37718391
Iteration 12, loss = 0.34348066
Iteration 13, loss = 0.31196688
Iteration 14, loss = 0.28269698
Iteration 15, loss = 0.25591268
Iteration 16, loss = 0.23160953
Iteration 17, loss = 0.20978626
Iteration 18, loss = 0.19017852
Iteration 19, loss = 0.17261901
Iteration 20, loss = 0.15703168
Iteration 21, loss = 0.14315140
Iteration 22, loss = 0.13090336
Iteration 23, loss = 0.11998520
Iteration 24, loss = 0.11036271
Iteration 25, loss = 0.10186781
Iteration 26, loss = 0.09418423
Iteration 27, loss = 0.08740138
Iteration 28, loss = 0.08126633
Iteration 29, loss = 0.07592269
Iteration 30, loss = 0.07096117
Iteration 31, loss = 0.06660803
Iteration 32, loss = 0.06262361
Iteration 33, loss = 0.05901813
Iteration 34, loss = 0.05577209
Iteration 35, loss = 0.05277080
Iteration 36, loss = 0.05005220
Iteration 37, loss = 0.04754943
Iteration 38, loss = 0.04538326
Iteration 39, loss = 0.04326705
Iteration 40, loss = 0.04129876
Iteration 41, loss = 0.03952412
Iteration 42, loss = 0.03790633
Iteration 43, loss = 0.03634054
Iteration 44, loss = 0.03495886
Iteration 45, loss = 0.03367892
Iteration 46, loss = 0.03247940
Iteration 47, loss = 0.03127335
Iteration 48, loss = 0.03020988
Iteration 49, loss = 0.02916226
Iteration 50, loss = 0.02830592
Iteration 51, loss = 0.02741608
Iteration 52, loss = 0.02654933
Iteration 53, loss = 0.02579724
Iteration 54, loss = 0.02509040
Iteration 55, loss = 0.02437726
Iteration 56, loss = 0.02372617
Iteration 57, loss = 0.02310090
Iteration 58, loss = 0.02254303
Iteration 59, loss = 0.02201082
Iteration 60, loss = 0.02144448
Iteration 61, loss = 0.02093171
Iteration 62, loss = 0.02046480
Iteration 63, loss = 0.02002229
Iteration 64, loss = 0.01959240
Iteration 65, loss = 0.01919007
Iteration 66, loss = 0.01882163
Iteration 67, loss = 0.01844871
Iteration 68, loss = 0.01810110
Iteration 69, loss = 0.01777358
Iteration 70, loss = 0.01748091
Iteration 71, loss = 0.01713740
Iteration 72, loss = 0.01681926
Iteration 73, loss = 0.01658849
Iteration 74, loss = 0.01627058
Iteration 75, loss = 0.01600710
Iteration 76, loss = 0.01577918
Iteration 77, loss = 0.01555034
Iteration 78, loss = 0.01531810
Iteration 79, loss = 0.01514099
Iteration 80, loss = 0.01492271
Iteration 81, loss = 0.01464886
Iteration 82, loss = 0.01442068
Iteration 83, loss = 0.01426283
Iteration 84, loss = 0.01407267
Iteration 85, loss = 0.01391092
Iteration 86, loss = 0.01374284
Iteration 87, loss = 0.01359592
Iteration 88, loss = 0.01345077
Iteration 89, loss = 0.01329872
Iteration 90, loss = 0.01313659
Iteration 91, loss = 0.01295710
Iteration 92, loss = 0.01287957
Iteration 93, loss = 0.01270845
Iteration 94, loss = 0.01260739
Iteration 95, loss = 0.01252734
Iteration 96, loss = 0.01239720
Iteration 97, loss = 0.01224611
Iteration 98, loss = 0.01213441
Iteration 99, loss = 0.01204353
Iteration 100, loss = 0.01187209
Iteration 101, loss = 0.01179640
Iteration 102, loss = 0.01175837
Iteration 103, loss = 0.01159580
Iteration 104, loss = 0.01154468
Iteration 105, loss = 0.01141106
Iteration 106, loss = 0.01136478
Iteration 107, loss = 0.01125318
Iteration 108, loss = 0.01116310
Iteration 109, loss = 0.01108760
Iteration 110, loss = 0.01099073
Iteration 111, loss = 0.01090659
Iteration 112, loss = 0.01082857
Iteration 113, loss = 0.01076929
Iteration 114, loss = 0.01070430
Iteration 115, loss = 0.01064767
Iteration 116, loss = 0.01061102
Iteration 117, loss = 0.01051901
Iteration 118, loss = 0.01042985
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69206687
Iteration 2, loss = 0.64531551
Iteration 3, loss = 0.58080824
Iteration 4, loss = 0.49989926
Iteration 5, loss = 0.41870304
Iteration 6, loss = 0.34271663
Iteration 7, loss = 0.27692099
Iteration 8, loss = 0.22217187
Iteration 9, loss = 0.17975202
Iteration 10, loss = 0.14723569
Iteration 11, loss = 0.12286137
Iteration 12, loss = 0.10415635
Iteration 13, loss = 0.08986971
Iteration 14, loss = 0.07873225
Iteration 15, loss = 0.06970908
Iteration 16, loss = 0.06271682
Iteration 17, loss = 0.05688481
Iteration 18, loss = 0.05198962
Iteration 19, loss = 0.04815889
Iteration 20, loss = 0.04470130
Iteration 21, loss = 0.04194909
Iteration 22, loss = 0.03947890
Iteration 23, loss = 0.03738338
Iteration 24, loss = 0.03541434
Iteration 25, loss = 0.03388134
Iteration 26, loss = 0.03240617
Iteration 27, loss = 0.03118637
Iteration 28, loss = 0.03016403
Iteration 29, loss = 0.02913248
Iteration 30, loss = 0.02817085
Iteration 31, loss = 0.02756556
Iteration 32, loss = 0.02674382
Iteration 33, loss = 0.02596463
Iteration 34, loss = 0.02538185
Iteration 35, loss = 0.02481353
Iteration 36, loss = 0.02437101
Iteration 37, loss = 0.02387731
Iteration 38, loss = 0.02350818
Iteration 39, loss = 0.02305304
Iteration 40, loss = 0.02265550
Iteration 41, loss = 0.02238988
Iteration 42, loss = 0.02205580
Iteration 43, loss = 0.02193495
Iteration 44, loss = 0.02150095
Iteration 45, loss = 0.02160736
Iteration 46, loss = 0.02141894
Iteration 47, loss = 0.02112525
Iteration 48, loss = 0.02088943
Iteration 49, loss = 0.02056099
Iteration 50, loss = 0.02025630
Iteration 51, loss = 0.02011213
Iteration 52, loss = 0.01998035
Iteration 53, loss = 0.01986879
Iteration 54, loss = 0.01976112
Iteration 55, loss = 0.01966200
Iteration 56, loss = 0.01944446
Iteration 57, loss = 0.01945734
Iteration 58, loss = 0.01917394
Iteration 59, loss = 0.01918203
Iteration 60, loss = 0.01913592
Iteration 61, loss = 0.01909898
Iteration 62, loss = 0.01886726
Iteration 63, loss = 0.01869072
Iteration 64, loss = 0.01852985
Iteration 65, loss = 0.01830617
Iteration 66, loss = 0.01856194
Iteration 67, loss = 0.01871826
Iteration 68, loss = 0.01851671
Iteration 69, loss = 0.01822807
Iteration 70, loss = 0.01816648
Iteration 71, loss = 0.01808870
Iteration 72, loss = 0.01808493
Iteration 73, loss = 0.01805584
Iteration 74, loss = 0.01821106
Iteration 75, loss = 0.01808029
Iteration 76, loss = 0.01791975
Iteration 77, loss = 0.01774995
Iteration 78, loss = 0.01777364
Iteration 79, loss = 0.01767963
Iteration 80, loss = 0.01773769
Iteration 81, loss = 0.01780619
Iteration 82, loss = 0.01756998
Iteration 83, loss = 0.01759240
Iteration 84, loss = 0.01746218
Iteration 85, loss = 0.01755275
Iteration 86, loss = 0.01736967
Iteration 87, loss = 0.01724937
Iteration 88, loss = 0.01740851
Iteration 89, loss = 0.01727069
Iteration 90, loss = 0.01727119
Iteration 91, loss = 0.01755871
Iteration 92, loss = 0.01723515
Iteration 93, loss = 0.01717607
Iteration 94, loss = 0.01732032
Iteration 95, loss = 0.01733041
Iteration 96, loss = 0.01724575
Iteration 97, loss = 0.01755369
Iteration 98, loss = 0.01747050
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67563584
Iteration 2, loss = 0.60249492
Iteration 3, loss = 0.49194480
Iteration 4, loss = 0.37581459
Iteration 5, loss = 0.27651334
Iteration 6, loss = 0.20306921
Iteration 7, loss = 0.15249575
Iteration 8, loss = 0.11835911
Iteration 9, loss = 0.09516190
Iteration 10, loss = 0.07897409
Iteration 11, loss = 0.06693911
Iteration 12, loss = 0.05840636
Iteration 13, loss = 0.05182861
Iteration 14, loss = 0.04623437
Iteration 15, loss = 0.04227546
Iteration 16, loss = 0.03903404
Iteration 17, loss = 0.03618223
Iteration 18, loss = 0.03415764
Iteration 19, loss = 0.03220974
Iteration 20, loss = 0.03078992
Iteration 21, loss = 0.02960813
Iteration 22, loss = 0.02832929
Iteration 23, loss = 0.02725528
Iteration 24, loss = 0.02655259
Iteration 25, loss = 0.02564147
Iteration 26, loss = 0.02484178
Iteration 27, loss = 0.02436147
Iteration 28, loss = 0.02372437
Iteration 29, loss = 0.02324001
Iteration 30, loss = 0.02306021
Iteration 31, loss = 0.02250757
Iteration 32, loss = 0.02228227
Iteration 33, loss = 0.02184717
Iteration 34, loss = 0.02166385
Iteration 35, loss = 0.02132961
Iteration 36, loss = 0.02126437
Iteration 37, loss = 0.02082919
Iteration 38, loss = 0.02061296
Iteration 39, loss = 0.02042754
Iteration 40, loss = 0.02045764
Iteration 41, loss = 0.02012426
Iteration 42, loss = 0.01994782
Iteration 43, loss = 0.01988130
Iteration 44, loss = 0.01968470
Iteration 45, loss = 0.01960514
Iteration 46, loss = 0.01957780
Iteration 47, loss = 0.01950840
Iteration 48, loss = 0.01949666
Iteration 49, loss = 0.01945269
Iteration 50, loss = 0.01908893
Iteration 51, loss = 0.01889083
Iteration 52, loss = 0.01911401
Iteration 53, loss = 0.01909609
Iteration 54, loss = 0.01894392
Iteration 55, loss = 0.01884335
Iteration 56, loss = 0.01866289
Iteration 57, loss = 0.01855358
Iteration 58, loss = 0.01879418
Iteration 59, loss = 0.01859304
Iteration 60, loss = 0.01839246
Iteration 61, loss = 0.01869717
Iteration 62, loss = 0.01841784
Iteration 63, loss = 0.01838324
Iteration 64, loss = 0.01843286
Iteration 65, loss = 0.01813788
Iteration 66, loss = 0.01830391
Iteration 67, loss = 0.01840620
Iteration 68, loss = 0.01806410
Iteration 69, loss = 0.01826108
Iteration 70, loss = 0.01804980
Iteration 71, loss = 0.01821281
Iteration 72, loss = 0.01800516
Iteration 73, loss = 0.01802064
Iteration 74, loss = 0.01786396
Iteration 75, loss = 0.01783772
Iteration 76, loss = 0.01796564
Iteration 77, loss = 0.01795603
Iteration 78, loss = 0.01769763
Iteration 79, loss = 0.01786312
Iteration 80, loss = 0.01801626
Iteration 81, loss = 0.01787506
Iteration 82, loss = 0.01774838
Iteration 83, loss = 0.01783137
Iteration 84, loss = 0.01778604
Iteration 85, loss = 0.01781857
Iteration 86, loss = 0.01763311
Iteration 87, loss = 0.01777790
Iteration 88, loss = 0.01764581
Iteration 89, loss = 0.01759437
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69180521
Iteration 2, loss = 0.62050510
Iteration 3, loss = 0.51471659
Iteration 4, loss = 0.39906607
Iteration 5, loss = 0.29922107
Iteration 6, loss = 0.22350050
Iteration 7, loss = 0.16965048
Iteration 8, loss = 0.13239774
Iteration 9, loss = 0.10640614
Iteration 10, loss = 0.08801936
Iteration 11, loss = 0.07475271
Iteration 12, loss = 0.06457490
Iteration 13, loss = 0.05665256
Iteration 14, loss = 0.05065157
Iteration 15, loss = 0.04609629
Iteration 16, loss = 0.04200615
Iteration 17, loss = 0.03895219
Iteration 18, loss = 0.03642845
Iteration 19, loss = 0.03409274
Iteration 20, loss = 0.03225466
Iteration 21, loss = 0.03065796
Iteration 22, loss = 0.02932376
Iteration 23, loss = 0.02794757
Iteration 24, loss = 0.02693046
Iteration 25, loss = 0.02612610
Iteration 26, loss = 0.02517647
Iteration 27, loss = 0.02456885
Iteration 28, loss = 0.02373300
Iteration 29, loss = 0.02336249
Iteration 30, loss = 0.02266495
Iteration 31, loss = 0.02211997
Iteration 32, loss = 0.02163945
Iteration 33, loss = 0.02132315
Iteration 34, loss = 0.02110122
Iteration 35, loss = 0.02068009
Iteration 36, loss = 0.02039545
Iteration 37, loss = 0.02023601
Iteration 38, loss = 0.01977545
Iteration 39, loss = 0.01949129
Iteration 40, loss = 0.01919572
Iteration 41, loss = 0.01919434
Iteration 42, loss = 0.01897066
Iteration 43, loss = 0.01885848
Iteration 44, loss = 0.01850500
Iteration 45, loss = 0.01846703
Iteration 46, loss = 0.01852517
Iteration 47, loss = 0.01831648
Iteration 48, loss = 0.01826382
Iteration 49, loss = 0.01799619
Iteration 50, loss = 0.01781447
Iteration 51, loss = 0.01779327
Iteration 52, loss = 0.01765259
Iteration 53, loss = 0.01766884
Iteration 54, loss = 0.01760548
Iteration 55, loss = 0.01749274
Iteration 56, loss = 0.01741531
Iteration 57, loss = 0.01726474
Iteration 58, loss = 0.01720359
Iteration 59, loss = 0.01707461
Iteration 60, loss = 0.01702477
Iteration 61, loss = 0.01705930
Iteration 62, loss = 0.01688616
Iteration 63, loss = 0.01705849
Iteration 64, loss = 0.01672707
Iteration 65, loss = 0.01692898
Iteration 66, loss = 0.01692397
Iteration 67, loss = 0.01693882
Iteration 68, loss = 0.01685594
Iteration 69, loss = 0.01671359
Iteration 70, loss = 0.01658177
Iteration 71, loss = 0.01656597
Iteration 72, loss = 0.01664628
Iteration 73, loss = 0.01646826
Iteration 74, loss = 0.01642361
Iteration 75, loss = 0.01654462
Iteration 76, loss = 0.01628996
Iteration 77, loss = 0.01615369
Iteration 78, loss = 0.01641285
Iteration 79, loss = 0.01638625
Iteration 80, loss = 0.01633436
Iteration 81, loss = 0.01624220
Iteration 82, loss = 0.01611764
Iteration 83, loss = 0.01619398
Iteration 84, loss = 0.01638438
Iteration 85, loss = 0.01619920
Iteration 86, loss = 0.01609449
Iteration 87, loss = 0.01621290
Iteration 88, loss = 0.01613233
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69622469
Iteration 2, loss = 0.66794302
Iteration 3, loss = 0.63396934
Iteration 4, loss = 0.59140734
Iteration 5, loss = 0.54262867
Iteration 6, loss = 0.49149940
Iteration 7, loss = 0.44097680
Iteration 8, loss = 0.39170605
Iteration 9, loss = 0.34503097
Iteration 10, loss = 0.30253618
Iteration 11, loss = 0.26441522
Iteration 12, loss = 0.23107213
Iteration 13, loss = 0.20219129
Iteration 14, loss = 0.17766684
Iteration 15, loss = 0.15674839
Iteration 16, loss = 0.13912779
Iteration 17, loss = 0.12403635
Iteration 18, loss = 0.11127039
Iteration 19, loss = 0.10037756
Iteration 20, loss = 0.09097920
Iteration 21, loss = 0.08312180
Iteration 22, loss = 0.07625035
Iteration 23, loss = 0.07020645
Iteration 24, loss = 0.06500405
Iteration 25, loss = 0.06043566
Iteration 26, loss = 0.05633012
Iteration 27, loss = 0.05279808
Iteration 28, loss = 0.04955721
Iteration 29, loss = 0.04678936
Iteration 30, loss = 0.04433986
Iteration 31, loss = 0.04200636
Iteration 32, loss = 0.03999263
Iteration 33, loss = 0.03808118
Iteration 34, loss = 0.03639790
Iteration 35, loss = 0.03477232
Iteration 36, loss = 0.03340710
Iteration 37, loss = 0.03209326
Iteration 38, loss = 0.03092216
Iteration 39, loss = 0.02985950
Iteration 40, loss = 0.02878978
Iteration 41, loss = 0.02785626
Iteration 42, loss = 0.02700171
Iteration 43, loss = 0.02624967
Iteration 44, loss = 0.02547550
Iteration 45, loss = 0.02483444
Iteration 46, loss = 0.02445873
Iteration 47, loss = 0.02379968
Iteration 48, loss = 0.02287727
Iteration 49, loss = 0.02239646
Iteration 50, loss = 0.02195928
Iteration 51, loss = 0.02151152
Iteration 52, loss = 0.02113341
Iteration 53, loss = 0.02066745
Iteration 54, loss = 0.02024431
Iteration 55, loss = 0.01992312
Iteration 56, loss = 0.01950993
Iteration 57, loss = 0.01911320
Iteration 58, loss = 0.01895950
Iteration 59, loss = 0.01864171
Iteration 60, loss = 0.01832785
Iteration 61, loss = 0.01824599
Iteration 62, loss = 0.01787970
Iteration 63, loss = 0.01759027
Iteration 64, loss = 0.01728809
Iteration 65, loss = 0.01705984
Iteration 66, loss = 0.01684227
Iteration 67, loss = 0.01656918
Iteration 68, loss = 0.01644529
Iteration 69, loss = 0.01619836
Iteration 70, loss = 0.01600618
Iteration 71, loss = 0.01586186
Iteration 72, loss = 0.01567894
Iteration 73, loss = 0.01544295
Iteration 74, loss = 0.01550289
Iteration 75, loss = 0.01527782
Iteration 76, loss = 0.01514179
Iteration 77, loss = 0.01503202
Iteration 78, loss = 0.01471830
Iteration 79, loss = 0.01474175
Iteration 80, loss = 0.01489562
Iteration 81, loss = 0.01463683
Iteration 82, loss = 0.01456012
Iteration 83, loss = 0.01444077
Iteration 84, loss = 0.01443026
Iteration 85, loss = 0.01426083
Iteration 86, loss = 0.01426749
Iteration 87, loss = 0.01415654
Iteration 88, loss = 0.01410156
Iteration 89, loss = 0.01395963
Iteration 90, loss = 0.01403863
Iteration 91, loss = 0.01383216
Iteration 92, loss = 0.01374332
Iteration 93, loss = 0.01385965
Iteration 94, loss = 0.01368760
Iteration 95, loss = 0.01339219
Iteration 96, loss = 0.01336753
Iteration 97, loss = 0.01321749
Iteration 98, loss = 0.01318219
Iteration 99, loss = 0.01313365
Iteration 100, loss = 0.01300805
Iteration 101, loss = 0.01287954
Iteration 102, loss = 0.01293908
Iteration 103, loss = 0.01278499
Iteration 104, loss = 0.01310129
Iteration 105, loss = 0.01282318
Iteration 106, loss = 0.01273937
Iteration 107, loss = 0.01258940
Iteration 108, loss = 0.01243956
Iteration 109, loss = 0.01243085
Iteration 110, loss = 0.01230580
Iteration 111, loss = 0.01234754
Iteration 112, loss = 0.01226550
Iteration 113, loss = 0.01248002
Iteration 114, loss = 0.01247205
Iteration 115, loss = 0.01252665
Iteration 116, loss = 0.01281763
Iteration 117, loss = 0.01283732
Iteration 118, loss = 0.01264231
Iteration 119, loss = 0.01242192
Iteration 120, loss = 0.01214455
Iteration 121, loss = 0.01204588
Iteration 122, loss = 0.01195647
Iteration 123, loss = 0.01196377
Iteration 124, loss = 0.01210020
Iteration 125, loss = 0.01193903
Iteration 126, loss = 0.01188167
Iteration 127, loss = 0.01180152
Iteration 128, loss = 0.01169786
Iteration 129, loss = 0.01165739
Iteration 130, loss = 0.01183112
Iteration 131, loss = 0.01186344
Iteration 132, loss = 0.01186174
Iteration 133, loss = 0.01168708
Iteration 134, loss = 0.01161665
Iteration 135, loss = 0.01165952
Iteration 136, loss = 0.01160326
Iteration 137, loss = 0.01158700
Iteration 138, loss = 0.01156842
Iteration 139, loss = 0.01145725
Iteration 140, loss = 0.01145095
Iteration 141, loss = 0.01145677
Iteration 142, loss = 0.01136748
Iteration 143, loss = 0.01136108
Iteration 144, loss = 0.01127935
Iteration 145, loss = 0.01133057
Iteration 146, loss = 0.01121260
Iteration 147, loss = 0.01120500
Iteration 148, loss = 0.01156205
Iteration 149, loss = 0.01164364
Iteration 150, loss = 0.01123408
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69982410
Iteration 2, loss = 0.63387341
Iteration 3, loss = 0.53941052
Iteration 4, loss = 0.42867189
Iteration 5, loss = 0.32855075
Iteration 6, loss = 0.24928802
Iteration 7, loss = 0.19151230
Iteration 8, loss = 0.14976051
Iteration 9, loss = 0.12055984
Iteration 10, loss = 0.09945402
Iteration 11, loss = 0.08409930
Iteration 12, loss = 0.07292229
Iteration 13, loss = 0.06411818
Iteration 14, loss = 0.05731276
Iteration 15, loss = 0.05177594
Iteration 16, loss = 0.04733307
Iteration 17, loss = 0.04381104
Iteration 18, loss = 0.04083740
Iteration 19, loss = 0.03854997
Iteration 20, loss = 0.03614062
Iteration 21, loss = 0.03432627
Iteration 22, loss = 0.03263881
Iteration 23, loss = 0.03138601
Iteration 24, loss = 0.03007043
Iteration 25, loss = 0.02881121
Iteration 26, loss = 0.02800402
Iteration 27, loss = 0.02718230
Iteration 28, loss = 0.02653556
Iteration 29, loss = 0.02600865
Iteration 30, loss = 0.02528170
Iteration 31, loss = 0.02478180
Iteration 32, loss = 0.02428657
Iteration 33, loss = 0.02399800
Iteration 34, loss = 0.02344497
Iteration 35, loss = 0.02292157
Iteration 36, loss = 0.02260510
Iteration 37, loss = 0.02221480
Iteration 38, loss = 0.02200589
Iteration 39, loss = 0.02161279
Iteration 40, loss = 0.02132767
Iteration 41, loss = 0.02119992
Iteration 42, loss = 0.02091183
Iteration 43, loss = 0.02097663
Iteration 44, loss = 0.02063971
Iteration 45, loss = 0.02032455
Iteration 46, loss = 0.02020633
Iteration 47, loss = 0.02009878
Iteration 48, loss = 0.02020056
Iteration 49, loss = 0.01997832
Iteration 50, loss = 0.01966924
Iteration 51, loss = 0.01957764
Iteration 52, loss = 0.01950434
Iteration 53, loss = 0.01947736
Iteration 54, loss = 0.01937519
Iteration 55, loss = 0.01906480
Iteration 56, loss = 0.01901982
Iteration 57, loss = 0.01892478
Iteration 58, loss = 0.01891758
Iteration 59, loss = 0.01923750
Iteration 60, loss = 0.01924834
Iteration 61, loss = 0.01980624
Iteration 62, loss = 0.01975421
Iteration 63, loss = 0.01942907
Iteration 64, loss = 0.01902969
Iteration 65, loss = 0.01897161
Iteration 66, loss = 0.01866287
Iteration 67, loss = 0.01854639
Iteration 68, loss = 0.01846160
Iteration 69, loss = 0.01826449
Iteration 70, loss = 0.01807343
Iteration 71, loss = 0.01805571
Iteration 72, loss = 0.01794861
Iteration 73, loss = 0.01798049
Iteration 74, loss = 0.01798200
Iteration 75, loss = 0.01777859
Iteration 76, loss = 0.01812836
Iteration 77, loss = 0.01885898
Iteration 78, loss = 0.01970969
Iteration 79, loss = 0.01875573
Iteration 80, loss = 0.01800843
Iteration 81, loss = 0.01779627
Iteration 82, loss = 0.01754179
Iteration 83, loss = 0.01740162
Iteration 84, loss = 0.01755669
Iteration 85, loss = 0.01758830
Iteration 86, loss = 0.01764029
Iteration 87, loss = 0.01743138
Iteration 88, loss = 0.01759833
Iteration 89, loss = 0.01738323
Iteration 90, loss = 0.01752349
Iteration 91, loss = 0.01773138
Iteration 92, loss = 0.01760908
Iteration 93, loss = 0.01751316
Iteration 94, loss = 0.01736406
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69040241
Iteration 2, loss = 0.65684488
Iteration 3, loss = 0.61206369
Iteration 4, loss = 0.55626006
Iteration 5, loss = 0.49538077
Iteration 6, loss = 0.43382517
Iteration 7, loss = 0.37620411
Iteration 8, loss = 0.32385285
Iteration 9, loss = 0.27716828
Iteration 10, loss = 0.23705614
Iteration 11, loss = 0.20295367
Iteration 12, loss = 0.17446892
Iteration 13, loss = 0.15078073
Iteration 14, loss = 0.13134243
Iteration 15, loss = 0.11520098
Iteration 16, loss = 0.10187626
Iteration 17, loss = 0.09079449
Iteration 18, loss = 0.08162287
Iteration 19, loss = 0.07388111
Iteration 20, loss = 0.06729117
Iteration 21, loss = 0.06166687
Iteration 22, loss = 0.05699236
Iteration 23, loss = 0.05267470
Iteration 24, loss = 0.04921724
Iteration 25, loss = 0.04599865
Iteration 26, loss = 0.04323089
Iteration 27, loss = 0.04081141
Iteration 28, loss = 0.03859886
Iteration 29, loss = 0.03661472
Iteration 30, loss = 0.03484341
Iteration 31, loss = 0.03325537
Iteration 32, loss = 0.03178124
Iteration 33, loss = 0.03048128
Iteration 34, loss = 0.02944223
Iteration 35, loss = 0.02824703
Iteration 36, loss = 0.02728443
Iteration 37, loss = 0.02637071
Iteration 38, loss = 0.02556682
Iteration 39, loss = 0.02474370
Iteration 40, loss = 0.02403832
Iteration 41, loss = 0.02338343
Iteration 42, loss = 0.02282182
Iteration 43, loss = 0.02228044
Iteration 44, loss = 0.02172084
Iteration 45, loss = 0.02117639
Iteration 46, loss = 0.02068658
Iteration 47, loss = 0.02038037
Iteration 48, loss = 0.02001326
Iteration 49, loss = 0.01944334
Iteration 50, loss = 0.01930054
Iteration 51, loss = 0.01886255
Iteration 52, loss = 0.01847803
Iteration 53, loss = 0.01818631
Iteration 54, loss = 0.01794267
Iteration 55, loss = 0.01761174
Iteration 56, loss = 0.01745728
Iteration 57, loss = 0.01709931
Iteration 58, loss = 0.01699620
Iteration 59, loss = 0.01672753
Iteration 60, loss = 0.01648856
Iteration 61, loss = 0.01634960
Iteration 62, loss = 0.01611825
Iteration 63, loss = 0.01601636
Iteration 64, loss = 0.01582148
Iteration 65, loss = 0.01563133
Iteration 66, loss = 0.01541898
Iteration 67, loss = 0.01549168
Iteration 68, loss = 0.01510814
Iteration 69, loss = 0.01512978
Iteration 70, loss = 0.01492351
Iteration 71, loss = 0.01473757
Iteration 72, loss = 0.01467109
Iteration 73, loss = 0.01461123
Iteration 74, loss = 0.01449858
Iteration 75, loss = 0.01438472
Iteration 76, loss = 0.01428744
Iteration 77, loss = 0.01413406
Iteration 78, loss = 0.01414694
Iteration 79, loss = 0.01396726
Iteration 80, loss = 0.01396595
Iteration 81, loss = 0.01388966
Iteration 82, loss = 0.01371817
Iteration 83, loss = 0.01373304
Iteration 84, loss = 0.01365698
Iteration 85, loss = 0.01350648
Iteration 86, loss = 0.01340806
Iteration 87, loss = 0.01338459
Iteration 88, loss = 0.01332295
Iteration 89, loss = 0.01328417
Iteration 90, loss = 0.01324893
Iteration 91, loss = 0.01314999
Iteration 92, loss = 0.01323507
Iteration 93, loss = 0.01300199
Iteration 94, loss = 0.01306844
Iteration 95, loss = 0.01295272
Iteration 96, loss = 0.01289036
Iteration 97, loss = 0.01283227
Iteration 98, loss = 0.01281421
Iteration 99, loss = 0.01271384
Iteration 100, loss = 0.01268502
Iteration 101, loss = 0.01262693
Iteration 102, loss = 0.01266965
Iteration 103, loss = 0.01265326
Iteration 104, loss = 0.01254493
Iteration 105, loss = 0.01249086
Iteration 106, loss = 0.01261727
Iteration 107, loss = 0.01255227
Iteration 108, loss = 0.01253281
Iteration 109, loss = 0.01245307
Iteration 110, loss = 0.01239772
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68277242
Iteration 2, loss = 0.63566938
Iteration 3, loss = 0.56998962
Iteration 4, loss = 0.49007346
Iteration 5, loss = 0.40855601
Iteration 6, loss = 0.33226472
Iteration 7, loss = 0.26736796
Iteration 8, loss = 0.21507433
Iteration 9, loss = 0.17433108
Iteration 10, loss = 0.14340403
Iteration 11, loss = 0.11991964
Iteration 12, loss = 0.10183129
Iteration 13, loss = 0.08828636
Iteration 14, loss = 0.07730973
Iteration 15, loss = 0.06880129
Iteration 16, loss = 0.06198603
Iteration 17, loss = 0.05621225
Iteration 18, loss = 0.05149651
Iteration 19, loss = 0.04767821
Iteration 20, loss = 0.04447932
Iteration 21, loss = 0.04150709
Iteration 22, loss = 0.03947464
Iteration 23, loss = 0.03728334
Iteration 24, loss = 0.03554198
Iteration 25, loss = 0.03379910
Iteration 26, loss = 0.03249870
Iteration 27, loss = 0.03116713
Iteration 28, loss = 0.03004051
Iteration 29, loss = 0.02910484
Iteration 30, loss = 0.02827299
Iteration 31, loss = 0.02748919
Iteration 32, loss = 0.02670823
Iteration 33, loss = 0.02611394
Iteration 34, loss = 0.02532241
Iteration 35, loss = 0.02487657
Iteration 36, loss = 0.02436798
Iteration 37, loss = 0.02409368
Iteration 38, loss = 0.02377735
Iteration 39, loss = 0.02337085
Iteration 40, loss = 0.02286390
Iteration 41, loss = 0.02253048
Iteration 42, loss = 0.02225919
Iteration 43, loss = 0.02183985
Iteration 44, loss = 0.02146050
Iteration 45, loss = 0.02131068
Iteration 46, loss = 0.02109512
Iteration 47, loss = 0.02090960
Iteration 48, loss = 0.02066694
Iteration 49, loss = 0.02054312
Iteration 50, loss = 0.02026471
Iteration 51, loss = 0.01988285
Iteration 52, loss = 0.01990752
Iteration 53, loss = 0.01981228
Iteration 54, loss = 0.01965591
Iteration 55, loss = 0.01951354
Iteration 56, loss = 0.01988534
Iteration 57, loss = 0.01923362
Iteration 58, loss = 0.01927682
Iteration 59, loss = 0.01908848
Iteration 60, loss = 0.01897333
Iteration 61, loss = 0.01895698
Iteration 62, loss = 0.01876109
Iteration 63, loss = 0.01882803
Iteration 64, loss = 0.01846447
Iteration 65, loss = 0.01840547
Iteration 66, loss = 0.01827012
Iteration 67, loss = 0.01830543
Iteration 68, loss = 0.01819482
Iteration 69, loss = 0.01813783
Iteration 70, loss = 0.01815596
Iteration 71, loss = 0.01810577
Iteration 72, loss = 0.01813893
Iteration 73, loss = 0.01823355
Iteration 74, loss = 0.01809304
Iteration 75, loss = 0.01772254
Iteration 76, loss = 0.01780471
Iteration 77, loss = 0.01796885
Iteration 78, loss = 0.01775477
Iteration 79, loss = 0.01760866
Iteration 80, loss = 0.01762796
Iteration 81, loss = 0.01768564
Iteration 82, loss = 0.01771529
Iteration 83, loss = 0.01743992
Iteration 84, loss = 0.01720935
Iteration 85, loss = 0.01726754
Iteration 86, loss = 0.01745367
Iteration 87, loss = 0.01753639
Iteration 88, loss = 0.01742359
Iteration 89, loss = 0.01716808
Iteration 90, loss = 0.01709257
Iteration 91, loss = 0.01741288
Iteration 92, loss = 0.01726568
Iteration 93, loss = 0.01707318
Iteration 94, loss = 0.01711742
Iteration 95, loss = 0.01719628
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68246365
Iteration 2, loss = 0.60946758
Iteration 3, loss = 0.49768665
Iteration 4, loss = 0.37917446
Iteration 5, loss = 0.27954870
Iteration 6, loss = 0.20631599
Iteration 7, loss = 0.15562472
Iteration 8, loss = 0.12131405
Iteration 9, loss = 0.09771473
Iteration 10, loss = 0.08119452
Iteration 11, loss = 0.06914403
Iteration 12, loss = 0.06029790
Iteration 13, loss = 0.05336413
Iteration 14, loss = 0.04810580
Iteration 15, loss = 0.04384155
Iteration 16, loss = 0.04027687
Iteration 17, loss = 0.03759316
Iteration 18, loss = 0.03526276
Iteration 19, loss = 0.03324412
Iteration 20, loss = 0.03151730
Iteration 21, loss = 0.03020431
Iteration 22, loss = 0.02908287
Iteration 23, loss = 0.02785099
Iteration 24, loss = 0.02694514
Iteration 25, loss = 0.02602608
Iteration 26, loss = 0.02526414
Iteration 27, loss = 0.02461151
Iteration 28, loss = 0.02408344
Iteration 29, loss = 0.02377637
Iteration 30, loss = 0.02323337
Iteration 31, loss = 0.02280388
Iteration 32, loss = 0.02228674
Iteration 33, loss = 0.02186385
Iteration 34, loss = 0.02159825
Iteration 35, loss = 0.02136369
Iteration 36, loss = 0.02099890
Iteration 37, loss = 0.02084528
Iteration 38, loss = 0.02048418
Iteration 39, loss = 0.02041307
Iteration 40, loss = 0.02006391
Iteration 41, loss = 0.02009817
Iteration 42, loss = 0.01982673
Iteration 43, loss = 0.01977124
Iteration 44, loss = 0.01949526
Iteration 45, loss = 0.01941586
Iteration 46, loss = 0.01921972
Iteration 47, loss = 0.01910510
Iteration 48, loss = 0.01899651
Iteration 49, loss = 0.01889786
Iteration 50, loss = 0.01894694
Iteration 51, loss = 0.01891708
Iteration 52, loss = 0.01879842
Iteration 53, loss = 0.01841323
Iteration 54, loss = 0.01842636
Iteration 55, loss = 0.01847819
Iteration 56, loss = 0.01815047
Iteration 57, loss = 0.01835566
Iteration 58, loss = 0.01812811
Iteration 59, loss = 0.01832493
Iteration 60, loss = 0.01803506
Iteration 61, loss = 0.01797012
Iteration 62, loss = 0.01821939
Iteration 63, loss = 0.01787893
Iteration 64, loss = 0.01804663
Iteration 65, loss = 0.01775173
Iteration 66, loss = 0.01801663
Iteration 67, loss = 0.01792864
Iteration 68, loss = 0.01770251
Iteration 69, loss = 0.01774461
Iteration 70, loss = 0.01762280
Iteration 71, loss = 0.01753874
Iteration 72, loss = 0.01753891
Iteration 73, loss = 0.01756642
Iteration 74, loss = 0.01741981
Iteration 75, loss = 0.01755670
Iteration 76, loss = 0.01755153
Iteration 77, loss = 0.01757131
Iteration 78, loss = 0.01737333
Iteration 79, loss = 0.01762763
Iteration 80, loss = 0.01753479
Iteration 81, loss = 0.01708576
Iteration 82, loss = 0.01744026
Iteration 83, loss = 0.01746223
Iteration 84, loss = 0.01727588
Iteration 85, loss = 0.01739263
Iteration 86, loss = 0.01713007
Iteration 87, loss = 0.01714669
Iteration 88, loss = 0.01717416
Iteration 89, loss = 0.01711769
Iteration 90, loss = 0.01708038
Iteration 91, loss = 0.01705029
Iteration 92, loss = 0.01704195
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70985422
Iteration 2, loss = 0.67823545
Iteration 3, loss = 0.64234267
Iteration 4, loss = 0.60148869
Iteration 5, loss = 0.55591746
Iteration 6, loss = 0.50717556
Iteration 7, loss = 0.45628778
Iteration 8, loss = 0.40516254
Iteration 9, loss = 0.35618478
Iteration 10, loss = 0.31146973
Iteration 11, loss = 0.27179159
Iteration 12, loss = 0.23704260
Iteration 13, loss = 0.20699118
Iteration 14, loss = 0.18104534
Iteration 15, loss = 0.15928308
Iteration 16, loss = 0.14074703
Iteration 17, loss = 0.12503577
Iteration 18, loss = 0.11166767
Iteration 19, loss = 0.10027063
Iteration 20, loss = 0.09062564
Iteration 21, loss = 0.08225343
Iteration 22, loss = 0.07509028
Iteration 23, loss = 0.06876930
Iteration 24, loss = 0.06342879
Iteration 25, loss = 0.05854344
Iteration 26, loss = 0.05419975
Iteration 27, loss = 0.05066622
Iteration 28, loss = 0.04747994
Iteration 29, loss = 0.04436971
Iteration 30, loss = 0.04174349
Iteration 31, loss = 0.03922367
Iteration 32, loss = 0.03707557
Iteration 33, loss = 0.03504344
Iteration 34, loss = 0.03318122
Iteration 35, loss = 0.03156628
Iteration 36, loss = 0.03008645
Iteration 37, loss = 0.02867868
Iteration 38, loss = 0.02745148
Iteration 39, loss = 0.02631802
Iteration 40, loss = 0.02526577
Iteration 41, loss = 0.02426171
Iteration 42, loss = 0.02341314
Iteration 43, loss = 0.02251940
Iteration 44, loss = 0.02166784
Iteration 45, loss = 0.02109801
Iteration 46, loss = 0.02034308
Iteration 47, loss = 0.01959827
Iteration 48, loss = 0.01917692
Iteration 49, loss = 0.01850454
Iteration 50, loss = 0.01799565
Iteration 51, loss = 0.01753204
Iteration 52, loss = 0.01709361
Iteration 53, loss = 0.01663741
Iteration 54, loss = 0.01618316
Iteration 55, loss = 0.01588165
Iteration 56, loss = 0.01546722
Iteration 57, loss = 0.01503298
Iteration 58, loss = 0.01479844
Iteration 59, loss = 0.01445105
Iteration 60, loss = 0.01413677
Iteration 61, loss = 0.01387558
Iteration 62, loss = 0.01352946
Iteration 63, loss = 0.01331636
Iteration 64, loss = 0.01306644
Iteration 65, loss = 0.01285102
Iteration 66, loss = 0.01260239
Iteration 67, loss = 0.01239467
Iteration 68, loss = 0.01221717
Iteration 69, loss = 0.01202768
Iteration 70, loss = 0.01185285
Iteration 71, loss = 0.01180760
Iteration 72, loss = 0.01160230
Iteration 73, loss = 0.01141499
Iteration 74, loss = 0.01123234
Iteration 75, loss = 0.01103681
Iteration 76, loss = 0.01087934
Iteration 77, loss = 0.01084488
Iteration 78, loss = 0.01065050
Iteration 79, loss = 0.01053290
Iteration 80, loss = 0.01036891
Iteration 81, loss = 0.01026134
Iteration 82, loss = 0.01010904
Iteration 83, loss = 0.01012051
Iteration 84, loss = 0.00998047
Iteration 85, loss = 0.01000820
Iteration 86, loss = 0.00995544
Iteration 87, loss = 0.00976016
Iteration 88, loss = 0.00966241
Iteration 89, loss = 0.00954987
Iteration 90, loss = 0.00944566
Iteration 91, loss = 0.00951425
Iteration 92, loss = 0.00948544
Iteration 93, loss = 0.00937242
Iteration 94, loss = 0.00925106
Iteration 95, loss = 0.00912126
Iteration 96, loss = 0.00898445
Iteration 97, loss = 0.00889387
Iteration 98, loss = 0.00884814
Iteration 99, loss = 0.00870276
Iteration 100, loss = 0.00869717
Iteration 101, loss = 0.00868417
Iteration 102, loss = 0.00854581
Iteration 103, loss = 0.00853249
Iteration 104, loss = 0.00844082
Iteration 105, loss = 0.00839055
Iteration 106, loss = 0.00834385
Iteration 107, loss = 0.00833449
Iteration 108, loss = 0.00824848
Iteration 109, loss = 0.00822906
Iteration 110, loss = 0.00817239
Iteration 111, loss = 0.00811783
Iteration 112, loss = 0.00812357
Iteration 113, loss = 0.00804500
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67947556
Iteration 2, loss = 0.61166307
Iteration 3, loss = 0.51076374
Iteration 4, loss = 0.40080492
Iteration 5, loss = 0.30261764
Iteration 6, loss = 0.22670161
Iteration 7, loss = 0.17245101
Iteration 8, loss = 0.13459807
Iteration 9, loss = 0.10819609
Iteration 10, loss = 0.08941303
Iteration 11, loss = 0.07571158
Iteration 12, loss = 0.06549746
Iteration 13, loss = 0.05745525
Iteration 14, loss = 0.05143527
Iteration 15, loss = 0.04674150
Iteration 16, loss = 0.04262752
Iteration 17, loss = 0.03956638
Iteration 18, loss = 0.03711425
Iteration 19, loss = 0.03472085
Iteration 20, loss = 0.03286861
Iteration 21, loss = 0.03102265
Iteration 22, loss = 0.02993081
Iteration 23, loss = 0.02866655
Iteration 24, loss = 0.02782375
Iteration 25, loss = 0.02664322
Iteration 26, loss = 0.02579941
Iteration 27, loss = 0.02533002
Iteration 28, loss = 0.02477935
Iteration 29, loss = 0.02394783
Iteration 30, loss = 0.02342808
Iteration 31, loss = 0.02300619
Iteration 32, loss = 0.02276475
Iteration 33, loss = 0.02219547
Iteration 34, loss = 0.02182150
Iteration 35, loss = 0.02184669
Iteration 36, loss = 0.02123961
Iteration 37, loss = 0.02090009
Iteration 38, loss = 0.02076563
Iteration 39, loss = 0.02053802
Iteration 40, loss = 0.02051648
Iteration 41, loss = 0.02044512
Iteration 42, loss = 0.02004378
Iteration 43, loss = 0.01968996
Iteration 44, loss = 0.01944003
Iteration 45, loss = 0.01951391
Iteration 46, loss = 0.01935942
Iteration 47, loss = 0.01913073
Iteration 48, loss = 0.01911910
Iteration 49, loss = 0.01898826
Iteration 50, loss = 0.01892425
Iteration 51, loss = 0.01892951
Iteration 52, loss = 0.01878230
Iteration 53, loss = 0.01848417
Iteration 54, loss = 0.01840960
Iteration 55, loss = 0.01846788
Iteration 56, loss = 0.01828447
Iteration 57, loss = 0.01811724
Iteration 58, loss = 0.01822416
Iteration 59, loss = 0.01811485
Iteration 60, loss = 0.01822411
Iteration 61, loss = 0.01779816
Iteration 62, loss = 0.01811132
Iteration 63, loss = 0.01793770
Iteration 64, loss = 0.01801325
Iteration 65, loss = 0.01791488
Iteration 66, loss = 0.01783452
Iteration 67, loss = 0.01777027
Iteration 68, loss = 0.01756842
Iteration 69, loss = 0.01758078
Iteration 70, loss = 0.01759345
Iteration 71, loss = 0.01761671
Iteration 72, loss = 0.01726205
Iteration 73, loss = 0.01757436
Iteration 74, loss = 0.01754195
Iteration 75, loss = 0.01750071
Iteration 76, loss = 0.01752061
Iteration 77, loss = 0.01717880
Iteration 78, loss = 0.01750874
Iteration 79, loss = 0.01732433
Iteration 80, loss = 0.01761986
Iteration 81, loss = 0.01734313
Iteration 82, loss = 0.01740345
Iteration 83, loss = 0.01706484
Iteration 84, loss = 0.01711955
Iteration 85, loss = 0.01728405
Iteration 86, loss = 0.01701800
Iteration 87, loss = 0.01714092
Iteration 88, loss = 0.01705855
Iteration 89, loss = 0.01712368
Iteration 90, loss = 0.01724393
Iteration 91, loss = 0.01702344
Iteration 92, loss = 0.01697509
Iteration 93, loss = 0.01698202
Iteration 94, loss = 0.01699860
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70133757
Iteration 2, loss = 0.68421616
Iteration 3, loss = 0.66941812
Iteration 4, loss = 0.65487292
Iteration 5, loss = 0.63945753
Iteration 6, loss = 0.62344791
Iteration 7, loss = 0.60643979
Iteration 8, loss = 0.58850847
Iteration 9, loss = 0.56950578
Iteration 10, loss = 0.54950244
Iteration 11, loss = 0.52870679
Iteration 12, loss = 0.50720086
Iteration 13, loss = 0.48497029
Iteration 14, loss = 0.46218417
Iteration 15, loss = 0.43927240
Iteration 16, loss = 0.41633116
Iteration 17, loss = 0.39372393
Iteration 18, loss = 0.37150051
Iteration 19, loss = 0.35008630
Iteration 20, loss = 0.32940983
Iteration 21, loss = 0.30965062
Iteration 22, loss = 0.29080244
Iteration 23, loss = 0.27292816
Iteration 24, loss = 0.25602958
Iteration 25, loss = 0.24002329
Iteration 26, loss = 0.22506315
Iteration 27, loss = 0.21104597
Iteration 28, loss = 0.19786326
Iteration 29, loss = 0.18559089
Iteration 30, loss = 0.17415313
Iteration 31, loss = 0.16351463
Iteration 32, loss = 0.15359610
Iteration 33, loss = 0.14442389
Iteration 34, loss = 0.13593253
Iteration 35, loss = 0.12805697
Iteration 36, loss = 0.12074602
Iteration 37, loss = 0.11394819
Iteration 38, loss = 0.10768652
Iteration 39, loss = 0.10188427
Iteration 40, loss = 0.09650034
Iteration 41, loss = 0.09149730
Iteration 42, loss = 0.08689246
Iteration 43, loss = 0.08259270
Iteration 44, loss = 0.07856058
Iteration 45, loss = 0.07478895
Iteration 46, loss = 0.07142561
Iteration 47, loss = 0.06815850
Iteration 48, loss = 0.06513797
Iteration 49, loss = 0.06233639
Iteration 50, loss = 0.05970492
Iteration 51, loss = 0.05726104
Iteration 52, loss = 0.05493574
Iteration 53, loss = 0.05280932
Iteration 54, loss = 0.05075905
Iteration 55, loss = 0.04881873
Iteration 56, loss = 0.04702940
Iteration 57, loss = 0.04531461
Iteration 58, loss = 0.04371051
Iteration 59, loss = 0.04229317
Iteration 60, loss = 0.04085656
Iteration 61, loss = 0.03954494
Iteration 62, loss = 0.03821055
Iteration 63, loss = 0.03693388
Iteration 64, loss = 0.03578792
Iteration 65, loss = 0.03468669
Iteration 66, loss = 0.03375135
Iteration 67, loss = 0.03281090
Iteration 68, loss = 0.03181670
Iteration 69, loss = 0.03092170
Iteration 70, loss = 0.03008473
Iteration 71, loss = 0.02929778
Iteration 72, loss = 0.02847605
Iteration 73, loss = 0.02773286
Iteration 74, loss = 0.02702310
Iteration 75, loss = 0.02639478
Iteration 76, loss = 0.02569549
Iteration 77, loss = 0.02508643
Iteration 78, loss = 0.02452469
Iteration 79, loss = 0.02395083
Iteration 80, loss = 0.02337520
Iteration 81, loss = 0.02288897
Iteration 82, loss = 0.02234399
Iteration 83, loss = 0.02186635
Iteration 84, loss = 0.02143938
Iteration 85, loss = 0.02096248
Iteration 86, loss = 0.02052724
Iteration 87, loss = 0.02012287
Iteration 88, loss = 0.01972801
Iteration 89, loss = 0.01934055
Iteration 90, loss = 0.01899105
Iteration 91, loss = 0.01861516
Iteration 92, loss = 0.01830260
Iteration 93, loss = 0.01794126
Iteration 94, loss = 0.01763951
Iteration 95, loss = 0.01731419
Iteration 96, loss = 0.01701277
Iteration 97, loss = 0.01673281
Iteration 98, loss = 0.01646494
Iteration 99, loss = 0.01617084
Iteration 100, loss = 0.01589196
Iteration 101, loss = 0.01564448
Iteration 102, loss = 0.01540094
Iteration 103, loss = 0.01528117
Iteration 104, loss = 0.01497819
Iteration 105, loss = 0.01475197
Iteration 106, loss = 0.01453346
Iteration 107, loss = 0.01427757
Iteration 108, loss = 0.01404902
Iteration 109, loss = 0.01383621
Iteration 110, loss = 0.01368699
Iteration 111, loss = 0.01351989
Iteration 112, loss = 0.01334385
Iteration 113, loss = 0.01318902
Iteration 114, loss = 0.01303623
Iteration 115, loss = 0.01285464
Iteration 116, loss = 0.01271136
Iteration 117, loss = 0.01257884
Iteration 118, loss = 0.01237729
Iteration 119, loss = 0.01221185
Iteration 120, loss = 0.01208411
Iteration 121, loss = 0.01189376
Iteration 122, loss = 0.01175060
Iteration 123, loss = 0.01163205
Iteration 124, loss = 0.01149134
Iteration 125, loss = 0.01136716
Iteration 126, loss = 0.01128494
Iteration 127, loss = 0.01111130
Iteration 128, loss = 0.01098600
Iteration 129, loss = 0.01087108
Iteration 130, loss = 0.01076439
Iteration 131, loss = 0.01065866
Iteration 132, loss = 0.01056320
Iteration 133, loss = 0.01045249
Iteration 134, loss = 0.01034828
Iteration 135, loss = 0.01032029
Iteration 136, loss = 0.01020473
Iteration 137, loss = 0.01011914
Iteration 138, loss = 0.01002515
Iteration 139, loss = 0.00994472
Iteration 140, loss = 0.00988710
Iteration 141, loss = 0.00975703
Iteration 142, loss = 0.00963413
Iteration 143, loss = 0.00951231
Iteration 144, loss = 0.00944073
Iteration 145, loss = 0.00939481
Iteration 146, loss = 0.00927365
Iteration 147, loss = 0.00920153
Iteration 148, loss = 0.00913128
Iteration 149, loss = 0.00905531
Iteration 150, loss = 0.00900545
Iteration 151, loss = 0.00893319
Iteration 152, loss = 0.00886270
Iteration 153, loss = 0.00881859
Iteration 154, loss = 0.00870702
Iteration 155, loss = 0.00864154
Iteration 156, loss = 0.00860347
Iteration 157, loss = 0.00853430
Iteration 158, loss = 0.00843890
Iteration 159, loss = 0.00839720
Iteration 160, loss = 0.00849794
Iteration 161, loss = 0.00836742
Iteration 162, loss = 0.00829702
Iteration 163, loss = 0.00825231
Iteration 164, loss = 0.00817581
Iteration 165, loss = 0.00811798
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67776711
Iteration 2, loss = 0.61561203
Iteration 3, loss = 0.51382537
Iteration 4, loss = 0.40170692
Iteration 5, loss = 0.30248632
Iteration 6, loss = 0.22575594
Iteration 7, loss = 0.17118384
Iteration 8, loss = 0.13333717
Iteration 9, loss = 0.10706957
Iteration 10, loss = 0.08819874
Iteration 11, loss = 0.07468318
Iteration 12, loss = 0.06442467
Iteration 13, loss = 0.05701754
Iteration 14, loss = 0.05066620
Iteration 15, loss = 0.04585754
Iteration 16, loss = 0.04226423
Iteration 17, loss = 0.03887082
Iteration 18, loss = 0.03638234
Iteration 19, loss = 0.03415454
Iteration 20, loss = 0.03238512
Iteration 21, loss = 0.03079169
Iteration 22, loss = 0.02924428
Iteration 23, loss = 0.02811294
Iteration 24, loss = 0.02695007
Iteration 25, loss = 0.02635420
Iteration 26, loss = 0.02553323
Iteration 27, loss = 0.02458759
Iteration 28, loss = 0.02406868
Iteration 29, loss = 0.02362376
Iteration 30, loss = 0.02289231
Iteration 31, loss = 0.02244692
Iteration 32, loss = 0.02219227
Iteration 33, loss = 0.02183442
Iteration 34, loss = 0.02127276
Iteration 35, loss = 0.02118942
Iteration 36, loss = 0.02070001
Iteration 37, loss = 0.02049167
Iteration 38, loss = 0.02024167
Iteration 39, loss = 0.01995168
Iteration 40, loss = 0.01980450
Iteration 41, loss = 0.01966531
Iteration 42, loss = 0.01962404
Iteration 43, loss = 0.01918638
Iteration 44, loss = 0.01914675
Iteration 45, loss = 0.01904889
Iteration 46, loss = 0.01904220
Iteration 47, loss = 0.01871972
Iteration 48, loss = 0.01856433
Iteration 49, loss = 0.01846862
Iteration 50, loss = 0.01850096
Iteration 51, loss = 0.01832106
Iteration 52, loss = 0.01827564
Iteration 53, loss = 0.01824698
Iteration 54, loss = 0.01800151
Iteration 55, loss = 0.01813688
Iteration 56, loss = 0.01776300
Iteration 57, loss = 0.01789461
Iteration 58, loss = 0.01774364
Iteration 59, loss = 0.01764591
Iteration 60, loss = 0.01755187
Iteration 61, loss = 0.01765476
Iteration 62, loss = 0.01778718
Iteration 63, loss = 0.01740195
Iteration 64, loss = 0.01744035
Iteration 65, loss = 0.01731678
Iteration 66, loss = 0.01728314
Iteration 67, loss = 0.01708110
Iteration 68, loss = 0.01711875
Iteration 69, loss = 0.01736506
Iteration 70, loss = 0.01708673
Iteration 71, loss = 0.01721390
Iteration 72, loss = 0.01701742
Iteration 73, loss = 0.01714889
Iteration 74, loss = 0.01708104
Iteration 75, loss = 0.01716587
Iteration 76, loss = 0.01732530
Iteration 77, loss = 0.01700678
Iteration 78, loss = 0.01665760
Iteration 79, loss = 0.01679343
Iteration 80, loss = 0.01702289
Iteration 81, loss = 0.01673677
Iteration 82, loss = 0.01696736
Iteration 83, loss = 0.01685625
Iteration 84, loss = 0.01687756
Iteration 85, loss = 0.01669546
Iteration 86, loss = 0.01644463
Iteration 87, loss = 0.01652668
Iteration 88, loss = 0.01680363
Iteration 89, loss = 0.01671865
Iteration 90, loss = 0.01682432
Iteration 91, loss = 0.01649129
Iteration 92, loss = 0.01656975
Iteration 93, loss = 0.01665880
Iteration 94, loss = 0.01675200
Iteration 95, loss = 0.01660491
Iteration 96, loss = 0.01651674
Iteration 97, loss = 0.01668425
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71039077
Iteration 2, loss = 0.67744758
Iteration 3, loss = 0.63110600
Iteration 4, loss = 0.56742579
Iteration 5, loss = 0.49292317
Iteration 6, loss = 0.41717964
Iteration 7, loss = 0.34849547
Iteration 8, loss = 0.28884431
Iteration 9, loss = 0.23914296
Iteration 10, loss = 0.19915333
Iteration 11, loss = 0.16696710
Iteration 12, loss = 0.14173330
Iteration 13, loss = 0.12154762
Iteration 14, loss = 0.10569788
Iteration 15, loss = 0.09281207
Iteration 16, loss = 0.08239081
Iteration 17, loss = 0.07395233
Iteration 18, loss = 0.06688802
Iteration 19, loss = 0.06104470
Iteration 20, loss = 0.05605682
Iteration 21, loss = 0.05184815
Iteration 22, loss = 0.04828693
Iteration 23, loss = 0.04498267
Iteration 24, loss = 0.04226800
Iteration 25, loss = 0.03992120
Iteration 26, loss = 0.03788891
Iteration 27, loss = 0.03600302
Iteration 28, loss = 0.03428071
Iteration 29, loss = 0.03285629
Iteration 30, loss = 0.03151778
Iteration 31, loss = 0.03029219
Iteration 32, loss = 0.02919773
Iteration 33, loss = 0.02825481
Iteration 34, loss = 0.02738280
Iteration 35, loss = 0.02655957
Iteration 36, loss = 0.02585444
Iteration 37, loss = 0.02517423
Iteration 38, loss = 0.02457746
Iteration 39, loss = 0.02401676
Iteration 40, loss = 0.02341133
Iteration 41, loss = 0.02291468
Iteration 42, loss = 0.02250949
Iteration 43, loss = 0.02205397
Iteration 44, loss = 0.02172011
Iteration 45, loss = 0.02132203
Iteration 46, loss = 0.02087942
Iteration 47, loss = 0.02065875
Iteration 48, loss = 0.02024473
Iteration 49, loss = 0.01992412
Iteration 50, loss = 0.01971307
Iteration 51, loss = 0.01949991
Iteration 52, loss = 0.01919760
Iteration 53, loss = 0.01891393
Iteration 54, loss = 0.01889264
Iteration 55, loss = 0.01859755
Iteration 56, loss = 0.01834827
Iteration 57, loss = 0.01818170
Iteration 58, loss = 0.01809427
Iteration 59, loss = 0.01795274
Iteration 60, loss = 0.01773302
Iteration 61, loss = 0.01759565
Iteration 62, loss = 0.01738763
Iteration 63, loss = 0.01731120
Iteration 64, loss = 0.01741413
Iteration 65, loss = 0.01718398
Iteration 66, loss = 0.01700748
Iteration 67, loss = 0.01681325
Iteration 68, loss = 0.01684362
Iteration 69, loss = 0.01665645
Iteration 70, loss = 0.01649428
Iteration 71, loss = 0.01649715
Iteration 72, loss = 0.01634016
Iteration 73, loss = 0.01632171
Iteration 74, loss = 0.01618208
Iteration 75, loss = 0.01616320
Iteration 76, loss = 0.01606931
Iteration 77, loss = 0.01595527
Iteration 78, loss = 0.01596852
Iteration 79, loss = 0.01581246
Iteration 80, loss = 0.01588004
Iteration 81, loss = 0.01573528
Iteration 82, loss = 0.01562555
Iteration 83, loss = 0.01569458
Iteration 84, loss = 0.01560800
Iteration 85, loss = 0.01544518
Iteration 86, loss = 0.01549974
Iteration 87, loss = 0.01539193
Iteration 88, loss = 0.01538154
Iteration 89, loss = 0.01529392
Iteration 90, loss = 0.01525085
Iteration 91, loss = 0.01539496
Iteration 92, loss = 0.01548183
Iteration 93, loss = 0.01530415
Iteration 94, loss = 0.01507781
Iteration 95, loss = 0.01525402
Iteration 96, loss = 0.01513876
Iteration 97, loss = 0.01493476
Iteration 98, loss = 0.01498808
Iteration 99, loss = 0.01486343
Iteration 100, loss = 0.01490361
Iteration 101, loss = 0.01495719
Iteration 102, loss = 0.01488308
Iteration 103, loss = 0.01479995
Iteration 104, loss = 0.01476596
Iteration 105, loss = 0.01481416
Iteration 106, loss = 0.01483000
Iteration 107, loss = 0.01486666
Iteration 108, loss = 0.01467864
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68340098
Iteration 2, loss = 0.62585338
Iteration 3, loss = 0.53328401
Iteration 4, loss = 0.42891707
Iteration 5, loss = 0.33480417
Iteration 6, loss = 0.25844625
Iteration 7, loss = 0.20096225
Iteration 8, loss = 0.15869379
Iteration 9, loss = 0.12815310
Iteration 10, loss = 0.10598994
Iteration 11, loss = 0.08951843
Iteration 12, loss = 0.07767216
Iteration 13, loss = 0.06810525
Iteration 14, loss = 0.06102630
Iteration 15, loss = 0.05518579
Iteration 16, loss = 0.05045459
Iteration 17, loss = 0.04675645
Iteration 18, loss = 0.04325639
Iteration 19, loss = 0.04063350
Iteration 20, loss = 0.03820727
Iteration 21, loss = 0.03621008
Iteration 22, loss = 0.03448166
Iteration 23, loss = 0.03281768
Iteration 24, loss = 0.03161486
Iteration 25, loss = 0.03034956
Iteration 26, loss = 0.02937487
Iteration 27, loss = 0.02839558
Iteration 28, loss = 0.02776314
Iteration 29, loss = 0.02695967
Iteration 30, loss = 0.02652427
Iteration 31, loss = 0.02547232
Iteration 32, loss = 0.02519199
Iteration 33, loss = 0.02476413
Iteration 34, loss = 0.02416431
Iteration 35, loss = 0.02392466
Iteration 36, loss = 0.02331719
Iteration 37, loss = 0.02313756
Iteration 38, loss = 0.02269191
Iteration 39, loss = 0.02246082
Iteration 40, loss = 0.02224227
Iteration 41, loss = 0.02202086
Iteration 42, loss = 0.02226074
Iteration 43, loss = 0.02259725
Iteration 44, loss = 0.02230531
Iteration 45, loss = 0.02152266
Iteration 46, loss = 0.02149219
Iteration 47, loss = 0.02097352
Iteration 48, loss = 0.02070350
Iteration 49, loss = 0.02089130
Iteration 50, loss = 0.02098073
Iteration 51, loss = 0.02081801
Iteration 52, loss = 0.02056589
Iteration 53, loss = 0.02021370
Iteration 54, loss = 0.02024036
Iteration 55, loss = 0.01992384
Iteration 56, loss = 0.01994412
Iteration 57, loss = 0.01989310
Iteration 58, loss = 0.01985546
Iteration 59, loss = 0.01963767
Iteration 60, loss = 0.01967683
Iteration 61, loss = 0.01975448
Iteration 62, loss = 0.01965964
Iteration 63, loss = 0.01954284
Iteration 64, loss = 0.01910369
Iteration 65, loss = 0.01901330
Iteration 66, loss = 0.01894467
Iteration 67, loss = 0.01911682
Iteration 68, loss = 0.01920710
Iteration 69, loss = 0.01880082
Iteration 70, loss = 0.01890701
Iteration 71, loss = 0.01891058
Iteration 72, loss = 0.02004557
Iteration 73, loss = 0.01994682
Iteration 74, loss = 0.01954871
Iteration 75, loss = 0.01961716
Iteration 76, loss = 0.01958605
Iteration 77, loss = 0.01936497
Iteration 78, loss = 0.01944712
Iteration 79, loss = 0.01934310
Iteration 80, loss = 0.01914576
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71017340
Iteration 2, loss = 0.67543587
Iteration 3, loss = 0.62982743
Iteration 4, loss = 0.57519978
Iteration 5, loss = 0.51314519
Iteration 6, loss = 0.45005563
Iteration 7, loss = 0.38969122
Iteration 8, loss = 0.33410232
Iteration 9, loss = 0.28491636
Iteration 10, loss = 0.24237370
Iteration 11, loss = 0.20666710
Iteration 12, loss = 0.17679606
Iteration 13, loss = 0.15215455
Iteration 14, loss = 0.13187829
Iteration 15, loss = 0.11531812
Iteration 16, loss = 0.10149691
Iteration 17, loss = 0.09013237
Iteration 18, loss = 0.08076116
Iteration 19, loss = 0.07277957
Iteration 20, loss = 0.06598519
Iteration 21, loss = 0.06032866
Iteration 22, loss = 0.05539718
Iteration 23, loss = 0.05119972
Iteration 24, loss = 0.04748371
Iteration 25, loss = 0.04428044
Iteration 26, loss = 0.04140072
Iteration 27, loss = 0.03895524
Iteration 28, loss = 0.03663920
Iteration 29, loss = 0.03478138
Iteration 30, loss = 0.03289784
Iteration 31, loss = 0.03127271
Iteration 32, loss = 0.02982233
Iteration 33, loss = 0.02851305
Iteration 34, loss = 0.02733244
Iteration 35, loss = 0.02627670
Iteration 36, loss = 0.02529471
Iteration 37, loss = 0.02433339
Iteration 38, loss = 0.02349513
Iteration 39, loss = 0.02276282
Iteration 40, loss = 0.02200312
Iteration 41, loss = 0.02146665
Iteration 42, loss = 0.02072129
Iteration 43, loss = 0.02027719
Iteration 44, loss = 0.01962373
Iteration 45, loss = 0.01929187
Iteration 46, loss = 0.01872771
Iteration 47, loss = 0.01825946
Iteration 48, loss = 0.01799223
Iteration 49, loss = 0.01744111
Iteration 50, loss = 0.01721927
Iteration 51, loss = 0.01681171
Iteration 52, loss = 0.01647896
Iteration 53, loss = 0.01619842
Iteration 54, loss = 0.01600266
Iteration 55, loss = 0.01563391
Iteration 56, loss = 0.01543029
Iteration 57, loss = 0.01516071
Iteration 58, loss = 0.01492434
Iteration 59, loss = 0.01473407
Iteration 60, loss = 0.01452160
Iteration 61, loss = 0.01439524
Iteration 62, loss = 0.01416433
Iteration 63, loss = 0.01395112
Iteration 64, loss = 0.01381487
Iteration 65, loss = 0.01366904
Iteration 66, loss = 0.01360869
Iteration 67, loss = 0.01336900
Iteration 68, loss = 0.01322499
Iteration 69, loss = 0.01326151
Iteration 70, loss = 0.01302427
Iteration 71, loss = 0.01283713
Iteration 72, loss = 0.01278492
Iteration 73, loss = 0.01273623
Iteration 74, loss = 0.01263337
Iteration 75, loss = 0.01252499
Iteration 76, loss = 0.01241827
Iteration 77, loss = 0.01230184
Iteration 78, loss = 0.01217149
Iteration 79, loss = 0.01208769
Iteration 80, loss = 0.01195458
Iteration 81, loss = 0.01195099
Iteration 82, loss = 0.01200930
Iteration 83, loss = 0.01173337
Iteration 84, loss = 0.01170592
Iteration 85, loss = 0.01168910
Iteration 86, loss = 0.01152917
Iteration 87, loss = 0.01154195
Iteration 88, loss = 0.01150934
Iteration 89, loss = 0.01136943
Iteration 90, loss = 0.01142603
Iteration 91, loss = 0.01142160
Iteration 92, loss = 0.01128404
Iteration 93, loss = 0.01118104
Iteration 94, loss = 0.01108275
Iteration 95, loss = 0.01111816
Iteration 96, loss = 0.01098173
Iteration 97, loss = 0.01108714
Iteration 98, loss = 0.01099491
Iteration 99, loss = 0.01100243
Iteration 100, loss = 0.01086840
Iteration 101, loss = 0.01089688
Iteration 102, loss = 0.01086924
Iteration 103, loss = 0.01083872
Iteration 104, loss = 0.01072254
Iteration 105, loss = 0.01068766
Iteration 106, loss = 0.01081074
Iteration 107, loss = 0.01068290
Iteration 108, loss = 0.01058180
Iteration 109, loss = 0.01067105
Iteration 110, loss = 0.01056811
Iteration 111, loss = 0.01046823
Iteration 112, loss = 0.01044795
Iteration 113, loss = 0.01046985
Iteration 114, loss = 0.01047065
Iteration 115, loss = 0.01042565
Iteration 116, loss = 0.01047820
Iteration 117, loss = 0.01030952
Iteration 118, loss = 0.01033539
Iteration 119, loss = 0.01038189
Iteration 120, loss = 0.01028844
Iteration 121, loss = 0.01030475
Iteration 122, loss = 0.01026650
Iteration 123, loss = 0.01042638
Iteration 124, loss = 0.01033874
Iteration 125, loss = 0.01030676
Iteration 126, loss = 0.01016855
Iteration 127, loss = 0.01012631
Iteration 128, loss = 0.01019006
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69075669
Iteration 2, loss = 0.64557904
Iteration 3, loss = 0.57760216
Iteration 4, loss = 0.48580742
Iteration 5, loss = 0.39252385
Iteration 6, loss = 0.30993160
Iteration 7, loss = 0.24342422
Iteration 8, loss = 0.19231997
Iteration 9, loss = 0.15426511
Iteration 10, loss = 0.12643774
Iteration 11, loss = 0.10558638
Iteration 12, loss = 0.09010641
Iteration 13, loss = 0.07807031
Iteration 14, loss = 0.06856420
Iteration 15, loss = 0.06122113
Iteration 16, loss = 0.05546418
Iteration 17, loss = 0.05071044
Iteration 18, loss = 0.04667535
Iteration 19, loss = 0.04330004
Iteration 20, loss = 0.04048335
Iteration 21, loss = 0.03799431
Iteration 22, loss = 0.03602703
Iteration 23, loss = 0.03412968
Iteration 24, loss = 0.03260036
Iteration 25, loss = 0.03141353
Iteration 26, loss = 0.02992987
Iteration 27, loss = 0.02903132
Iteration 28, loss = 0.02825558
Iteration 29, loss = 0.02728019
Iteration 30, loss = 0.02645600
Iteration 31, loss = 0.02597574
Iteration 32, loss = 0.02527443
Iteration 33, loss = 0.02451903
Iteration 34, loss = 0.02412401
Iteration 35, loss = 0.02354080
Iteration 36, loss = 0.02317765
Iteration 37, loss = 0.02267385
Iteration 38, loss = 0.02245405
Iteration 39, loss = 0.02233515
Iteration 40, loss = 0.02173446
Iteration 41, loss = 0.02144125
Iteration 42, loss = 0.02123533
Iteration 43, loss = 0.02112322
Iteration 44, loss = 0.02118964
Iteration 45, loss = 0.02046807
Iteration 46, loss = 0.02045698
Iteration 47, loss = 0.02014319
Iteration 48, loss = 0.02003150
Iteration 49, loss = 0.01982598
Iteration 50, loss = 0.01968572
Iteration 51, loss = 0.01946034
Iteration 52, loss = 0.01937739
Iteration 53, loss = 0.01916465
Iteration 54, loss = 0.01916983
Iteration 55, loss = 0.01892676
Iteration 56, loss = 0.01893693
Iteration 57, loss = 0.01895198
Iteration 58, loss = 0.01870535
Iteration 59, loss = 0.01852097
Iteration 60, loss = 0.01836733
Iteration 61, loss = 0.01836209
Iteration 62, loss = 0.01845062
Iteration 63, loss = 0.01818868
Iteration 64, loss = 0.01822108
Iteration 65, loss = 0.01802263
Iteration 66, loss = 0.01803327
Iteration 67, loss = 0.01812237
Iteration 68, loss = 0.01779407
Iteration 69, loss = 0.01795069
Iteration 70, loss = 0.01770368
Iteration 71, loss = 0.01782128
Iteration 72, loss = 0.01766160
Iteration 73, loss = 0.01762234
Iteration 74, loss = 0.01761772
Iteration 75, loss = 0.01736927
Iteration 76, loss = 0.01755246
Iteration 77, loss = 0.01750434
Iteration 78, loss = 0.01760304
Iteration 79, loss = 0.01765093
Iteration 80, loss = 0.01735505
Iteration 81, loss = 0.01739980
Iteration 82, loss = 0.01727376
Iteration 83, loss = 0.01735818
Iteration 84, loss = 0.01728945
Iteration 85, loss = 0.01726456
Iteration 86, loss = 0.01733537
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68959839
Iteration 2, loss = 0.66711352
Iteration 3, loss = 0.64828257
Iteration 4, loss = 0.63033533
Iteration 5, loss = 0.61174862
Iteration 6, loss = 0.59229295
Iteration 7, loss = 0.57174891
Iteration 8, loss = 0.55024642
Iteration 9, loss = 0.52754219
Iteration 10, loss = 0.50409577
Iteration 11, loss = 0.48001356
Iteration 12, loss = 0.45553655
Iteration 13, loss = 0.43099188
Iteration 14, loss = 0.40652984
Iteration 15, loss = 0.38277714
Iteration 16, loss = 0.35966085
Iteration 17, loss = 0.33756794
Iteration 18, loss = 0.31641807
Iteration 19, loss = 0.29627604
Iteration 20, loss = 0.27725418
Iteration 21, loss = 0.25944196
Iteration 22, loss = 0.24273578
Iteration 23, loss = 0.22708607
Iteration 24, loss = 0.21264562
Iteration 25, loss = 0.19910436
Iteration 26, loss = 0.18660430
Iteration 27, loss = 0.17496937
Iteration 28, loss = 0.16425823
Iteration 29, loss = 0.15440075
Iteration 30, loss = 0.14522775
Iteration 31, loss = 0.13684408
Iteration 32, loss = 0.12899501
Iteration 33, loss = 0.12181042
Iteration 34, loss = 0.11517956
Iteration 35, loss = 0.10905908
Iteration 36, loss = 0.10338016
Iteration 37, loss = 0.09816173
Iteration 38, loss = 0.09335587
Iteration 39, loss = 0.08877050
Iteration 40, loss = 0.08460465
Iteration 41, loss = 0.08074926
Iteration 42, loss = 0.07715008
Iteration 43, loss = 0.07385125
Iteration 44, loss = 0.07067476
Iteration 45, loss = 0.06774303
Iteration 46, loss = 0.06495441
Iteration 47, loss = 0.06241741
Iteration 48, loss = 0.06008519
Iteration 49, loss = 0.05783630
Iteration 50, loss = 0.05574636
Iteration 51, loss = 0.05377382
Iteration 52, loss = 0.05197089
Iteration 53, loss = 0.05020029
Iteration 54, loss = 0.04853740
Iteration 55, loss = 0.04701469
Iteration 56, loss = 0.04555189
Iteration 57, loss = 0.04415530
Iteration 58, loss = 0.04292960
Iteration 59, loss = 0.04172213
Iteration 60, loss = 0.04052009
Iteration 61, loss = 0.03942267
Iteration 62, loss = 0.03835179
Iteration 63, loss = 0.03733673
Iteration 64, loss = 0.03636106
Iteration 65, loss = 0.03548053
Iteration 66, loss = 0.03463971
Iteration 67, loss = 0.03376717
Iteration 68, loss = 0.03299438
Iteration 69, loss = 0.03220409
Iteration 70, loss = 0.03152911
Iteration 71, loss = 0.03080396
Iteration 72, loss = 0.03017702
Iteration 73, loss = 0.02956006
Iteration 74, loss = 0.02902542
Iteration 75, loss = 0.02840653
Iteration 76, loss = 0.02783250
Iteration 77, loss = 0.02728495
Iteration 78, loss = 0.02679979
Iteration 79, loss = 0.02631885
Iteration 80, loss = 0.02582145
Iteration 81, loss = 0.02532175
Iteration 82, loss = 0.02494260
Iteration 83, loss = 0.02446094
Iteration 84, loss = 0.02402100
Iteration 85, loss = 0.02357034
Iteration 86, loss = 0.02319985
Iteration 87, loss = 0.02303115
Iteration 88, loss = 0.02268756
Iteration 89, loss = 0.02236891
Iteration 90, loss = 0.02204517
Iteration 91, loss = 0.02172453
Iteration 92, loss = 0.02139469
Iteration 93, loss = 0.02111015
Iteration 94, loss = 0.02074101
Iteration 95, loss = 0.02048358
Iteration 96, loss = 0.02010961
Iteration 97, loss = 0.01986821
Iteration 98, loss = 0.01959101
Iteration 99, loss = 0.01935896
Iteration 100, loss = 0.01910288
Iteration 101, loss = 0.01893279
Iteration 102, loss = 0.01870017
Iteration 103, loss = 0.01849992
Iteration 104, loss = 0.01829274
Iteration 105, loss = 0.01806407
Iteration 106, loss = 0.01788362
Iteration 107, loss = 0.01768097
Iteration 108, loss = 0.01749201
Iteration 109, loss = 0.01731930
Iteration 110, loss = 0.01717842
Iteration 111, loss = 0.01694175
Iteration 112, loss = 0.01685736
Iteration 113, loss = 0.01664890
Iteration 114, loss = 0.01646863
Iteration 115, loss = 0.01635978
Iteration 116, loss = 0.01617720
Iteration 117, loss = 0.01603810
Iteration 118, loss = 0.01588670
Iteration 119, loss = 0.01570342
Iteration 120, loss = 0.01564026
Iteration 121, loss = 0.01549699
Iteration 122, loss = 0.01533891
Iteration 123, loss = 0.01526556
Iteration 124, loss = 0.01510805
Iteration 125, loss = 0.01501343
Iteration 126, loss = 0.01486223
Iteration 127, loss = 0.01475518
Iteration 128, loss = 0.01464212
Iteration 129, loss = 0.01453309
Iteration 130, loss = 0.01449476
Iteration 131, loss = 0.01432013
Iteration 132, loss = 0.01426229
Iteration 133, loss = 0.01419058
Iteration 134, loss = 0.01413834
Iteration 135, loss = 0.01411154
Iteration 136, loss = 0.01392346
Iteration 137, loss = 0.01386131
Iteration 138, loss = 0.01381983
Iteration 139, loss = 0.01362062
Iteration 140, loss = 0.01353182
Iteration 141, loss = 0.01343746
Iteration 142, loss = 0.01335487
Iteration 143, loss = 0.01332451
Iteration 144, loss = 0.01318878
Iteration 145, loss = 0.01314534
Iteration 146, loss = 0.01304358
Iteration 147, loss = 0.01297718
Iteration 148, loss = 0.01290029
Iteration 149, loss = 0.01284019
Iteration 150, loss = 0.01274993
Iteration 151, loss = 0.01269264
Iteration 152, loss = 0.01262763
Iteration 153, loss = 0.01273096
Iteration 154, loss = 0.01256669
Iteration 155, loss = 0.01255722
Iteration 156, loss = 0.01250177
Iteration 157, loss = 0.01245580
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69194624
Iteration 2, loss = 0.67937022
Iteration 3, loss = 0.66834079
Iteration 4, loss = 0.65741477
Iteration 5, loss = 0.64575602
Iteration 6, loss = 0.63321079
Iteration 7, loss = 0.61926175
Iteration 8, loss = 0.60410905
Iteration 9, loss = 0.58735846
Iteration 10, loss = 0.56900864
Iteration 11, loss = 0.54945780
Iteration 12, loss = 0.52846045
Iteration 13, loss = 0.50660907
Iteration 14, loss = 0.48388653
Iteration 15, loss = 0.46068441
Iteration 16, loss = 0.43732318
Iteration 17, loss = 0.41409578
Iteration 18, loss = 0.39118074
Iteration 19, loss = 0.36897650
Iteration 20, loss = 0.34736492
Iteration 21, loss = 0.32657093
Iteration 22, loss = 0.30659168
Iteration 23, loss = 0.28762511
Iteration 24, loss = 0.26956572
Iteration 25, loss = 0.25257964
Iteration 26, loss = 0.23652550
Iteration 27, loss = 0.22148633
Iteration 28, loss = 0.20742826
Iteration 29, loss = 0.19428187
Iteration 30, loss = 0.18207183
Iteration 31, loss = 0.17077235
Iteration 32, loss = 0.16024455
Iteration 33, loss = 0.15054186
Iteration 34, loss = 0.14151384
Iteration 35, loss = 0.13320234
Iteration 36, loss = 0.12564597
Iteration 37, loss = 0.11846174
Iteration 38, loss = 0.11193874
Iteration 39, loss = 0.10584340
Iteration 40, loss = 0.10022455
Iteration 41, loss = 0.09504579
Iteration 42, loss = 0.09023417
Iteration 43, loss = 0.08579619
Iteration 44, loss = 0.08164160
Iteration 45, loss = 0.07780006
Iteration 46, loss = 0.07426667
Iteration 47, loss = 0.07095199
Iteration 48, loss = 0.06786056
Iteration 49, loss = 0.06493755
Iteration 50, loss = 0.06228787
Iteration 51, loss = 0.05972247
Iteration 52, loss = 0.05735615
Iteration 53, loss = 0.05516082
Iteration 54, loss = 0.05307254
Iteration 55, loss = 0.05112967
Iteration 56, loss = 0.04929732
Iteration 57, loss = 0.04755019
Iteration 58, loss = 0.04592179
Iteration 59, loss = 0.04436437
Iteration 60, loss = 0.04296752
Iteration 61, loss = 0.04153568
Iteration 62, loss = 0.04026249
Iteration 63, loss = 0.03902794
Iteration 64, loss = 0.03787978
Iteration 65, loss = 0.03685867
Iteration 66, loss = 0.03571820
Iteration 67, loss = 0.03470851
Iteration 68, loss = 0.03373891
Iteration 69, loss = 0.03284678
Iteration 70, loss = 0.03197446
Iteration 71, loss = 0.03111292
Iteration 72, loss = 0.03036734
Iteration 73, loss = 0.02960932
Iteration 74, loss = 0.02892679
Iteration 75, loss = 0.02818858
Iteration 76, loss = 0.02753265
Iteration 77, loss = 0.02695397
Iteration 78, loss = 0.02630311
Iteration 79, loss = 0.02573069
Iteration 80, loss = 0.02518390
Iteration 81, loss = 0.02462484
Iteration 82, loss = 0.02414263
Iteration 83, loss = 0.02363350
Iteration 84, loss = 0.02315319
Iteration 85, loss = 0.02273830
Iteration 86, loss = 0.02226473
Iteration 87, loss = 0.02186795
Iteration 88, loss = 0.02146850
Iteration 89, loss = 0.02105216
Iteration 90, loss = 0.02068729
Iteration 91, loss = 0.02034360
Iteration 92, loss = 0.01998228
Iteration 93, loss = 0.01965755
Iteration 94, loss = 0.01928093
Iteration 95, loss = 0.01898607
Iteration 96, loss = 0.01866955
Iteration 97, loss = 0.01835331
Iteration 98, loss = 0.01812117
Iteration 99, loss = 0.01784148
Iteration 100, loss = 0.01753247
Iteration 101, loss = 0.01730515
Iteration 102, loss = 0.01706999
Iteration 103, loss = 0.01680906
Iteration 104, loss = 0.01654129
Iteration 105, loss = 0.01634149
Iteration 106, loss = 0.01610315
Iteration 107, loss = 0.01587356
Iteration 108, loss = 0.01565833
Iteration 109, loss = 0.01546218
Iteration 110, loss = 0.01524241
Iteration 111, loss = 0.01506483
Iteration 112, loss = 0.01486953
Iteration 113, loss = 0.01470300
Iteration 114, loss = 0.01449943
Iteration 115, loss = 0.01432501
Iteration 116, loss = 0.01420110
Iteration 117, loss = 0.01401303
Iteration 118, loss = 0.01386460
Iteration 119, loss = 0.01379484
Iteration 120, loss = 0.01363692
Iteration 121, loss = 0.01356644
Iteration 122, loss = 0.01339140
Iteration 123, loss = 0.01320460
Iteration 124, loss = 0.01302843
Iteration 125, loss = 0.01290204
Iteration 126, loss = 0.01276334
Iteration 127, loss = 0.01263578
Iteration 128, loss = 0.01252282
Iteration 129, loss = 0.01239438
Iteration 130, loss = 0.01228096
Iteration 131, loss = 0.01217485
Iteration 132, loss = 0.01209732
Iteration 133, loss = 0.01197577
Iteration 134, loss = 0.01187867
Iteration 135, loss = 0.01175582
Iteration 136, loss = 0.01165721
Iteration 137, loss = 0.01154389
Iteration 138, loss = 0.01147999
Iteration 139, loss = 0.01132989
Iteration 140, loss = 0.01125913
Iteration 141, loss = 0.01110887
Iteration 142, loss = 0.01099188
Iteration 143, loss = 0.01091711
Iteration 144, loss = 0.01091751
Iteration 145, loss = 0.01079306
Iteration 146, loss = 0.01079928
Iteration 147, loss = 0.01066186
Iteration 148, loss = 0.01057563
Iteration 149, loss = 0.01050215
Iteration 150, loss = 0.01040758
Iteration 151, loss = 0.01033894
Iteration 152, loss = 0.01024328
Iteration 153, loss = 0.01016188
Iteration 154, loss = 0.01015253
Iteration 155, loss = 0.01004594
Iteration 156, loss = 0.00997376
Iteration 157, loss = 0.00988024
Iteration 158, loss = 0.00988251
Iteration 159, loss = 0.00982187
Iteration 160, loss = 0.00977848
Iteration 161, loss = 0.00974965
Iteration 162, loss = 0.00972179
Iteration 163, loss = 0.00966414
Iteration 164, loss = 0.00958845
Iteration 165, loss = 0.00951447
Iteration 166, loss = 0.00941800
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69118696
Iteration 2, loss = 0.66852527
Iteration 3, loss = 0.64347513
Iteration 4, loss = 0.61044652
Iteration 5, loss = 0.56848305
Iteration 6, loss = 0.51971006
Iteration 7, loss = 0.46922189
Iteration 8, loss = 0.41931851
Iteration 9, loss = 0.37280710
Iteration 10, loss = 0.33023136
Iteration 11, loss = 0.29201095
Iteration 12, loss = 0.25783466
Iteration 13, loss = 0.22777410
Iteration 14, loss = 0.20167126
Iteration 15, loss = 0.17890349
Iteration 16, loss = 0.15946730
Iteration 17, loss = 0.14260840
Iteration 18, loss = 0.12815339
Iteration 19, loss = 0.11549586
Iteration 20, loss = 0.10476410
Iteration 21, loss = 0.09531821
Iteration 22, loss = 0.08717775
Iteration 23, loss = 0.08026698
Iteration 24, loss = 0.07408639
Iteration 25, loss = 0.06865116
Iteration 26, loss = 0.06373123
Iteration 27, loss = 0.05954254
Iteration 28, loss = 0.05578293
Iteration 29, loss = 0.05226497
Iteration 30, loss = 0.04915953
Iteration 31, loss = 0.04626702
Iteration 32, loss = 0.04384767
Iteration 33, loss = 0.04152738
Iteration 34, loss = 0.03957494
Iteration 35, loss = 0.03772277
Iteration 36, loss = 0.03613959
Iteration 37, loss = 0.03453791
Iteration 38, loss = 0.03306113
Iteration 39, loss = 0.03185956
Iteration 40, loss = 0.03077128
Iteration 41, loss = 0.02972820
Iteration 42, loss = 0.02849318
Iteration 43, loss = 0.02753438
Iteration 44, loss = 0.02664846
Iteration 45, loss = 0.02572547
Iteration 46, loss = 0.02489511
Iteration 47, loss = 0.02416396
Iteration 48, loss = 0.02334379
Iteration 49, loss = 0.02274674
Iteration 50, loss = 0.02216524
Iteration 51, loss = 0.02156290
Iteration 52, loss = 0.02099818
Iteration 53, loss = 0.02054217
Iteration 54, loss = 0.02004631
Iteration 55, loss = 0.01951900
Iteration 56, loss = 0.01914575
Iteration 57, loss = 0.01868453
Iteration 58, loss = 0.01826399
Iteration 59, loss = 0.01797510
Iteration 60, loss = 0.01749337
Iteration 61, loss = 0.01720747
Iteration 62, loss = 0.01692381
Iteration 63, loss = 0.01664447
Iteration 64, loss = 0.01631886
Iteration 65, loss = 0.01609799
Iteration 66, loss = 0.01592442
Iteration 67, loss = 0.01560766
Iteration 68, loss = 0.01528545
Iteration 69, loss = 0.01550889
Iteration 70, loss = 0.01526075
Iteration 71, loss = 0.01506457
Iteration 72, loss = 0.01477607
Iteration 73, loss = 0.01436900
Iteration 74, loss = 0.01465601
Iteration 75, loss = 0.01456163
Iteration 76, loss = 0.01432534
Iteration 77, loss = 0.01426056
Iteration 78, loss = 0.01395325
Iteration 79, loss = 0.01369403
Iteration 80, loss = 0.01340328
Iteration 81, loss = 0.01311713
Iteration 82, loss = 0.01289820
Iteration 83, loss = 0.01282211
Iteration 84, loss = 0.01271664
Iteration 85, loss = 0.01252275
Iteration 86, loss = 0.01241387
Iteration 87, loss = 0.01229518
Iteration 88, loss = 0.01216151
Iteration 89, loss = 0.01205621
Iteration 90, loss = 0.01203694
Iteration 91, loss = 0.01208152
Iteration 92, loss = 0.01193387
Iteration 93, loss = 0.01183379
Iteration 94, loss = 0.01169714
Iteration 95, loss = 0.01157182
Iteration 96, loss = 0.01148843
Iteration 97, loss = 0.01138048
Iteration 98, loss = 0.01129033
Iteration 99, loss = 0.01123555
Iteration 100, loss = 0.01108748
Iteration 101, loss = 0.01104202
Iteration 102, loss = 0.01095096
Iteration 103, loss = 0.01084163
Iteration 104, loss = 0.01099949
Iteration 105, loss = 0.01096339
Iteration 106, loss = 0.01119032
Iteration 107, loss = 0.01089256
Iteration 108, loss = 0.01078352
Iteration 109, loss = 0.01065519
Iteration 110, loss = 0.01058464
Iteration 111, loss = 0.01061262
Iteration 112, loss = 0.01046404
Iteration 113, loss = 0.01051141
Iteration 114, loss = 0.01047499
Iteration 115, loss = 0.01031074
Iteration 116, loss = 0.01035795
Iteration 117, loss = 0.01024808
Iteration 118, loss = 0.01018419
Iteration 119, loss = 0.01009063
Iteration 120, loss = 0.01000089
Iteration 121, loss = 0.00996356
Iteration 122, loss = 0.00990796
Iteration 123, loss = 0.00986333
Iteration 124, loss = 0.00991418
Iteration 125, loss = 0.00993313
Iteration 126, loss = 0.01022381
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67484458
Iteration 2, loss = 0.59943914
Iteration 3, loss = 0.48623126
Iteration 4, loss = 0.37221400
Iteration 5, loss = 0.27640818
Iteration 6, loss = 0.20498622
Iteration 7, loss = 0.15518526
Iteration 8, loss = 0.12086503
Iteration 9, loss = 0.09747160
Iteration 10, loss = 0.08084687
Iteration 11, loss = 0.06892419
Iteration 12, loss = 0.05989945
Iteration 13, loss = 0.05315600
Iteration 14, loss = 0.04792819
Iteration 15, loss = 0.04352109
Iteration 16, loss = 0.04006157
Iteration 17, loss = 0.03745177
Iteration 18, loss = 0.03501025
Iteration 19, loss = 0.03300077
Iteration 20, loss = 0.03147217
Iteration 21, loss = 0.02994191
Iteration 22, loss = 0.02877617
Iteration 23, loss = 0.02769240
Iteration 24, loss = 0.02687405
Iteration 25, loss = 0.02612389
Iteration 26, loss = 0.02554952
Iteration 27, loss = 0.02472827
Iteration 28, loss = 0.02414826
Iteration 29, loss = 0.02362725
Iteration 30, loss = 0.02338560
Iteration 31, loss = 0.02279426
Iteration 32, loss = 0.02255770
Iteration 33, loss = 0.02230716
Iteration 34, loss = 0.02176374
Iteration 35, loss = 0.02145117
Iteration 36, loss = 0.02130437
Iteration 37, loss = 0.02112398
Iteration 38, loss = 0.02120896
Iteration 39, loss = 0.02084337
Iteration 40, loss = 0.02057242
Iteration 41, loss = 0.02030255
Iteration 42, loss = 0.02024528
Iteration 43, loss = 0.02006230
Iteration 44, loss = 0.01970598
Iteration 45, loss = 0.01983273
Iteration 46, loss = 0.01962982
Iteration 47, loss = 0.01971920
Iteration 48, loss = 0.01926698
Iteration 49, loss = 0.01946102
Iteration 50, loss = 0.01938022
Iteration 51, loss = 0.01911109
Iteration 52, loss = 0.01917599
Iteration 53, loss = 0.01906358
Iteration 54, loss = 0.01897777
Iteration 55, loss = 0.01876598
Iteration 56, loss = 0.01898965
Iteration 57, loss = 0.01893342
Iteration 58, loss = 0.01861545
Iteration 59, loss = 0.01856576
Iteration 60, loss = 0.01836307
Iteration 61, loss = 0.01861334
Iteration 62, loss = 0.01837125
Iteration 63, loss = 0.01853158
Iteration 64, loss = 0.01853349
Iteration 65, loss = 0.01856338
Iteration 66, loss = 0.01818889
Iteration 67, loss = 0.01815745
Iteration 68, loss = 0.01836034
Iteration 69, loss = 0.01824065
Iteration 70, loss = 0.01825627
Iteration 71, loss = 0.01807595
Iteration 72, loss = 0.01809156
Iteration 73, loss = 0.01818349
Iteration 74, loss = 0.01817536
Iteration 75, loss = 0.01807094
Iteration 76, loss = 0.01786624
Iteration 77, loss = 0.01797215
Iteration 78, loss = 0.01779221
Iteration 79, loss = 0.01793010
Iteration 80, loss = 0.01798402
Iteration 81, loss = 0.01773542
Iteration 82, loss = 0.01765498
Iteration 83, loss = 0.01765082
Iteration 84, loss = 0.01776306
Iteration 85, loss = 0.01776865
Iteration 86, loss = 0.01779366
Iteration 87, loss = 0.01769929
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68578686
Iteration 2, loss = 0.65096173
Iteration 3, loss = 0.60419961
Iteration 4, loss = 0.54060140
Iteration 5, loss = 0.46866256
Iteration 6, loss = 0.39749295
Iteration 7, loss = 0.33233403
Iteration 8, loss = 0.27604589
Iteration 9, loss = 0.22922630
Iteration 10, loss = 0.19119183
Iteration 11, loss = 0.16095696
Iteration 12, loss = 0.13713051
Iteration 13, loss = 0.11791096
Iteration 14, loss = 0.10305601
Iteration 15, loss = 0.09090591
Iteration 16, loss = 0.08101790
Iteration 17, loss = 0.07309880
Iteration 18, loss = 0.06640510
Iteration 19, loss = 0.06080914
Iteration 20, loss = 0.05616652
Iteration 21, loss = 0.05209167
Iteration 22, loss = 0.04865915
Iteration 23, loss = 0.04586075
Iteration 24, loss = 0.04328211
Iteration 25, loss = 0.04105700
Iteration 26, loss = 0.03907518
Iteration 27, loss = 0.03724877
Iteration 28, loss = 0.03572825
Iteration 29, loss = 0.03448022
Iteration 30, loss = 0.03308367
Iteration 31, loss = 0.03194642
Iteration 32, loss = 0.03101095
Iteration 33, loss = 0.03020163
Iteration 34, loss = 0.02927660
Iteration 35, loss = 0.02850449
Iteration 36, loss = 0.02784480
Iteration 37, loss = 0.02722585
Iteration 38, loss = 0.02666475
Iteration 39, loss = 0.02605444
Iteration 40, loss = 0.02570991
Iteration 41, loss = 0.02507445
Iteration 42, loss = 0.02497152
Iteration 43, loss = 0.02444349
Iteration 44, loss = 0.02395637
Iteration 45, loss = 0.02371347
Iteration 46, loss = 0.02327884
Iteration 47, loss = 0.02304905
Iteration 48, loss = 0.02281656
Iteration 49, loss = 0.02256672
Iteration 50, loss = 0.02222743
Iteration 51, loss = 0.02208872
Iteration 52, loss = 0.02200405
Iteration 53, loss = 0.02179339
Iteration 54, loss = 0.02157165
Iteration 55, loss = 0.02136993
Iteration 56, loss = 0.02100146
Iteration 57, loss = 0.02094675
Iteration 58, loss = 0.02103743
Iteration 59, loss = 0.02062245
Iteration 60, loss = 0.02051485
Iteration 61, loss = 0.02046498
Iteration 62, loss = 0.02025096
Iteration 63, loss = 0.02017467
Iteration 64, loss = 0.02011356
Iteration 65, loss = 0.01991726
Iteration 66, loss = 0.01982143
Iteration 67, loss = 0.01969753
Iteration 68, loss = 0.01951599
Iteration 69, loss = 0.01980003
Iteration 70, loss = 0.01952858
Iteration 71, loss = 0.01937396
Iteration 72, loss = 0.01925976
Iteration 73, loss = 0.01938985
Iteration 74, loss = 0.01910098
Iteration 75, loss = 0.01918183
Iteration 76, loss = 0.01893202
Iteration 77, loss = 0.01885343
Iteration 78, loss = 0.01897947
Iteration 79, loss = 0.01891990
Iteration 80, loss = 0.01878825
Iteration 81, loss = 0.01869355
Iteration 82, loss = 0.01861182
Iteration 83, loss = 0.01857685
Iteration 84, loss = 0.01859119
Iteration 85, loss = 0.01855749
Iteration 86, loss = 0.01846695
Iteration 87, loss = 0.01843590
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69508055
Iteration 2, loss = 0.67307697
Iteration 3, loss = 0.65387907
Iteration 4, loss = 0.63535599
Iteration 5, loss = 0.61670688
Iteration 6, loss = 0.59731891
Iteration 7, loss = 0.57706804
Iteration 8, loss = 0.55588163
Iteration 9, loss = 0.53426381
Iteration 10, loss = 0.51208834
Iteration 11, loss = 0.48926968
Iteration 12, loss = 0.46605005
Iteration 13, loss = 0.44279898
Iteration 14, loss = 0.41969948
Iteration 15, loss = 0.39682478
Iteration 16, loss = 0.37440502
Iteration 17, loss = 0.35262351
Iteration 18, loss = 0.33159934
Iteration 19, loss = 0.31144297
Iteration 20, loss = 0.29235906
Iteration 21, loss = 0.27415690
Iteration 22, loss = 0.25709603
Iteration 23, loss = 0.24108956
Iteration 24, loss = 0.22612801
Iteration 25, loss = 0.21203104
Iteration 26, loss = 0.19898389
Iteration 27, loss = 0.18674507
Iteration 28, loss = 0.17536152
Iteration 29, loss = 0.16485801
Iteration 30, loss = 0.15513342
Iteration 31, loss = 0.14600213
Iteration 32, loss = 0.13758677
Iteration 33, loss = 0.12986291
Iteration 34, loss = 0.12267085
Iteration 35, loss = 0.11589937
Iteration 36, loss = 0.10971077
Iteration 37, loss = 0.10398842
Iteration 38, loss = 0.09864895
Iteration 39, loss = 0.09369346
Iteration 40, loss = 0.08906448
Iteration 41, loss = 0.08484106
Iteration 42, loss = 0.08084931
Iteration 43, loss = 0.07713836
Iteration 44, loss = 0.07366700
Iteration 45, loss = 0.07038336
Iteration 46, loss = 0.06740084
Iteration 47, loss = 0.06451241
Iteration 48, loss = 0.06184121
Iteration 49, loss = 0.05936367
Iteration 50, loss = 0.05701064
Iteration 51, loss = 0.05488471
Iteration 52, loss = 0.05281376
Iteration 53, loss = 0.05091065
Iteration 54, loss = 0.04911584
Iteration 55, loss = 0.04741309
Iteration 56, loss = 0.04581193
Iteration 57, loss = 0.04427221
Iteration 58, loss = 0.04284396
Iteration 59, loss = 0.04144873
Iteration 60, loss = 0.04019007
Iteration 61, loss = 0.03890847
Iteration 62, loss = 0.03778060
Iteration 63, loss = 0.03665104
Iteration 64, loss = 0.03557468
Iteration 65, loss = 0.03457946
Iteration 66, loss = 0.03363933
Iteration 67, loss = 0.03272134
Iteration 68, loss = 0.03184369
Iteration 69, loss = 0.03103711
Iteration 70, loss = 0.03022438
Iteration 71, loss = 0.02949789
Iteration 72, loss = 0.02876108
Iteration 73, loss = 0.02809924
Iteration 74, loss = 0.02741559
Iteration 75, loss = 0.02682447
Iteration 76, loss = 0.02620614
Iteration 77, loss = 0.02563728
Iteration 78, loss = 0.02512391
Iteration 79, loss = 0.02460489
Iteration 80, loss = 0.02405929
Iteration 81, loss = 0.02358829
Iteration 82, loss = 0.02312563
Iteration 83, loss = 0.02265870
Iteration 84, loss = 0.02221155
Iteration 85, loss = 0.02178239
Iteration 86, loss = 0.02135603
Iteration 87, loss = 0.02107099
Iteration 88, loss = 0.02062471
Iteration 89, loss = 0.02028622
Iteration 90, loss = 0.01994516
Iteration 91, loss = 0.01965420
Iteration 92, loss = 0.01931472
Iteration 93, loss = 0.01896497
Iteration 94, loss = 0.01867704
Iteration 95, loss = 0.01831083
Iteration 96, loss = 0.01801941
Iteration 97, loss = 0.01772109
Iteration 98, loss = 0.01741617
Iteration 99, loss = 0.01716254
Iteration 100, loss = 0.01693061
Iteration 101, loss = 0.01667307
Iteration 102, loss = 0.01647908
Iteration 103, loss = 0.01622234
Iteration 104, loss = 0.01599826
Iteration 105, loss = 0.01575736
Iteration 106, loss = 0.01555596
Iteration 107, loss = 0.01535648
Iteration 108, loss = 0.01517546
Iteration 109, loss = 0.01492742
Iteration 110, loss = 0.01487323
Iteration 111, loss = 0.01458247
Iteration 112, loss = 0.01441737
Iteration 113, loss = 0.01424359
Iteration 114, loss = 0.01409168
Iteration 115, loss = 0.01391506
Iteration 116, loss = 0.01375145
Iteration 117, loss = 0.01358615
Iteration 118, loss = 0.01343162
Iteration 119, loss = 0.01328712
Iteration 120, loss = 0.01313563
Iteration 121, loss = 0.01296788
Iteration 122, loss = 0.01293026
Iteration 123, loss = 0.01270798
Iteration 124, loss = 0.01257801
Iteration 125, loss = 0.01245011
Iteration 126, loss = 0.01233757
Iteration 127, loss = 0.01224012
Iteration 128, loss = 0.01211049
Iteration 129, loss = 0.01201351
Iteration 130, loss = 0.01187715
Iteration 131, loss = 0.01177237
Iteration 132, loss = 0.01167456
Iteration 133, loss = 0.01156996
Iteration 134, loss = 0.01149973
Iteration 135, loss = 0.01139229
Iteration 136, loss = 0.01127954
Iteration 137, loss = 0.01121449
Iteration 138, loss = 0.01107481
Iteration 139, loss = 0.01098208
Iteration 140, loss = 0.01086568
Iteration 141, loss = 0.01077861
Iteration 142, loss = 0.01077642
Iteration 143, loss = 0.01064772
Iteration 144, loss = 0.01061089
Iteration 145, loss = 0.01050297
Iteration 146, loss = 0.01045429
Iteration 147, loss = 0.01037242
Iteration 148, loss = 0.01024654
Iteration 149, loss = 0.01018671
Iteration 150, loss = 0.01009677
Iteration 151, loss = 0.01002395
Iteration 152, loss = 0.01004044
Iteration 153, loss = 0.00992024
Iteration 154, loss = 0.00986120
Iteration 155, loss = 0.00980361
Iteration 156, loss = 0.00970405
Iteration 157, loss = 0.00966226
Iteration 158, loss = 0.00954022
Iteration 159, loss = 0.00947302
Iteration 160, loss = 0.00950278
Iteration 161, loss = 0.00941741
Iteration 162, loss = 0.00934572
Iteration 163, loss = 0.00929761
Iteration 164, loss = 0.00921530
Iteration 165, loss = 0.00921461
Iteration 166, loss = 0.00911234
Iteration 167, loss = 0.00908356
Iteration 168, loss = 0.00903489
Iteration 169, loss = 0.00900857
Iteration 170, loss = 0.00891576
Iteration 171, loss = 0.00888953
Iteration 172, loss = 0.00882956
Iteration 173, loss = 0.00878134
Iteration 174, loss = 0.00874954
Iteration 175, loss = 0.00868219
Iteration 176, loss = 0.00866510
Iteration 177, loss = 0.00859662
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70902622
Iteration 2, loss = 0.68503764
Iteration 3, loss = 0.65805198
Iteration 4, loss = 0.62557727
Iteration 5, loss = 0.58739697
Iteration 6, loss = 0.54388674
Iteration 7, loss = 0.49710656
Iteration 8, loss = 0.44879674
Iteration 9, loss = 0.40046271
Iteration 10, loss = 0.35445031
Iteration 11, loss = 0.31201426
Iteration 12, loss = 0.27398720
Iteration 13, loss = 0.24024074
Iteration 14, loss = 0.21113723
Iteration 15, loss = 0.18601115
Iteration 16, loss = 0.16443992
Iteration 17, loss = 0.14610821
Iteration 18, loss = 0.13038521
Iteration 19, loss = 0.11692407
Iteration 20, loss = 0.10553958
Iteration 21, loss = 0.09578349
Iteration 22, loss = 0.08742385
Iteration 23, loss = 0.08007274
Iteration 24, loss = 0.07382795
Iteration 25, loss = 0.06821621
Iteration 26, loss = 0.06341554
Iteration 27, loss = 0.05908446
Iteration 28, loss = 0.05528299
Iteration 29, loss = 0.05184336
Iteration 30, loss = 0.04884903
Iteration 31, loss = 0.04613538
Iteration 32, loss = 0.04371370
Iteration 33, loss = 0.04147510
Iteration 34, loss = 0.03953289
Iteration 35, loss = 0.03768875
Iteration 36, loss = 0.03598605
Iteration 37, loss = 0.03446576
Iteration 38, loss = 0.03319461
Iteration 39, loss = 0.03191746
Iteration 40, loss = 0.03063722
Iteration 41, loss = 0.02954922
Iteration 42, loss = 0.02860695
Iteration 43, loss = 0.02769697
Iteration 44, loss = 0.02680571
Iteration 45, loss = 0.02606799
Iteration 46, loss = 0.02549672
Iteration 47, loss = 0.02476853
Iteration 48, loss = 0.02412302
Iteration 49, loss = 0.02339321
Iteration 50, loss = 0.02273268
Iteration 51, loss = 0.02224107
Iteration 52, loss = 0.02162673
Iteration 53, loss = 0.02116859
Iteration 54, loss = 0.02077579
Iteration 55, loss = 0.02039486
Iteration 56, loss = 0.01980365
Iteration 57, loss = 0.01971828
Iteration 58, loss = 0.01937976
Iteration 59, loss = 0.01903850
Iteration 60, loss = 0.01867560
Iteration 61, loss = 0.01831220
Iteration 62, loss = 0.01798275
Iteration 63, loss = 0.01763139
Iteration 64, loss = 0.01743351
Iteration 65, loss = 0.01721732
Iteration 66, loss = 0.01696940
Iteration 67, loss = 0.01672059
Iteration 68, loss = 0.01652760
Iteration 69, loss = 0.01612211
Iteration 70, loss = 0.01623959
Iteration 71, loss = 0.01605109
Iteration 72, loss = 0.01588225
Iteration 73, loss = 0.01560156
Iteration 74, loss = 0.01546865
Iteration 75, loss = 0.01523609
Iteration 76, loss = 0.01513461
Iteration 77, loss = 0.01489358
Iteration 78, loss = 0.01480822
Iteration 79, loss = 0.01467888
Iteration 80, loss = 0.01447590
Iteration 81, loss = 0.01446708
Iteration 82, loss = 0.01432076
Iteration 83, loss = 0.01421439
Iteration 84, loss = 0.01406546
Iteration 85, loss = 0.01396649
Iteration 86, loss = 0.01379522
Iteration 87, loss = 0.01381126
Iteration 88, loss = 0.01364760
Iteration 89, loss = 0.01355145
Iteration 90, loss = 0.01350898
Iteration 91, loss = 0.01338927
Iteration 92, loss = 0.01334121
Iteration 93, loss = 0.01318513
Iteration 94, loss = 0.01324943
Iteration 95, loss = 0.01321068
Iteration 96, loss = 0.01313012
Iteration 97, loss = 0.01306329
Iteration 98, loss = 0.01283578
Iteration 99, loss = 0.01274903
Iteration 100, loss = 0.01273516
Iteration 101, loss = 0.01258197
Iteration 102, loss = 0.01246698
Iteration 103, loss = 0.01286775
Iteration 104, loss = 0.01305148
Iteration 105, loss = 0.01326258
Iteration 106, loss = 0.01302498
Iteration 107, loss = 0.01266638
Iteration 108, loss = 0.01241671
Iteration 109, loss = 0.01217080
Iteration 110, loss = 0.01207991
Iteration 111, loss = 0.01206569
Iteration 112, loss = 0.01203552
Iteration 113, loss = 0.01237766
Iteration 114, loss = 0.01218409
Iteration 115, loss = 0.01205855
Iteration 116, loss = 0.01191306
Iteration 117, loss = 0.01190854
Iteration 118, loss = 0.01175615
Iteration 119, loss = 0.01171093
Iteration 120, loss = 0.01157741
Iteration 121, loss = 0.01159591
Iteration 122, loss = 0.01166589
Iteration 123, loss = 0.01158248
Iteration 124, loss = 0.01163216
Iteration 125, loss = 0.01153220
Iteration 126, loss = 0.01148713
Iteration 127, loss = 0.01146469
Iteration 128, loss = 0.01150078
Iteration 129, loss = 0.01158197
Iteration 130, loss = 0.01163506
Iteration 131, loss = 0.01153762
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67392689
Iteration 2, loss = 0.59846677
Iteration 3, loss = 0.49135069
Iteration 4, loss = 0.38200426
Iteration 5, loss = 0.28722686
Iteration 6, loss = 0.21461563
Iteration 7, loss = 0.16283147
Iteration 8, loss = 0.12748404
Iteration 9, loss = 0.10264125
Iteration 10, loss = 0.08531316
Iteration 11, loss = 0.07271010
Iteration 12, loss = 0.06318023
Iteration 13, loss = 0.05617287
Iteration 14, loss = 0.05048216
Iteration 15, loss = 0.04600968
Iteration 16, loss = 0.04264709
Iteration 17, loss = 0.03965729
Iteration 18, loss = 0.03716467
Iteration 19, loss = 0.03496706
Iteration 20, loss = 0.03341143
Iteration 21, loss = 0.03180388
Iteration 22, loss = 0.03072207
Iteration 23, loss = 0.02963224
Iteration 24, loss = 0.02860113
Iteration 25, loss = 0.02786209
Iteration 26, loss = 0.02737533
Iteration 27, loss = 0.02625420
Iteration 28, loss = 0.02579795
Iteration 29, loss = 0.02524360
Iteration 30, loss = 0.02467882
Iteration 31, loss = 0.02440371
Iteration 32, loss = 0.02414085
Iteration 33, loss = 0.02390862
Iteration 34, loss = 0.02335000
Iteration 35, loss = 0.02300942
Iteration 36, loss = 0.02284521
Iteration 37, loss = 0.02247647
Iteration 38, loss = 0.02240036
Iteration 39, loss = 0.02214574
Iteration 40, loss = 0.02203367
Iteration 41, loss = 0.02182916
Iteration 42, loss = 0.02154576
Iteration 43, loss = 0.02145997
Iteration 44, loss = 0.02132119
Iteration 45, loss = 0.02131785
Iteration 46, loss = 0.02102410
Iteration 47, loss = 0.02113091
Iteration 48, loss = 0.02071127
Iteration 49, loss = 0.02084057
Iteration 50, loss = 0.02102618
Iteration 51, loss = 0.02046546
Iteration 52, loss = 0.02048032
Iteration 53, loss = 0.02036835
Iteration 54, loss = 0.02025459
Iteration 55, loss = 0.02020517
Iteration 56, loss = 0.02022767
Iteration 57, loss = 0.01996106
Iteration 58, loss = 0.02018033
Iteration 59, loss = 0.01978087
Iteration 60, loss = 0.01998141
Iteration 61, loss = 0.01979866
Iteration 62, loss = 0.01984978
Iteration 63, loss = 0.01966037
Iteration 64, loss = 0.01979912
Iteration 65, loss = 0.01969022
Iteration 66, loss = 0.01957066
Iteration 67, loss = 0.01961943
Iteration 68, loss = 0.01941489
Iteration 69, loss = 0.01962516
Iteration 70, loss = 0.01934948
Iteration 71, loss = 0.01970877
Iteration 72, loss = 0.01961244
Iteration 73, loss = 0.01926611
Iteration 74, loss = 0.01957601
Iteration 75, loss = 0.01928340
Iteration 76, loss = 0.01932912
Iteration 77, loss = 0.01927892
Iteration 78, loss = 0.01945404
Iteration 79, loss = 0.01931569
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68063206
Iteration 2, loss = 0.66384987
Iteration 3, loss = 0.64913257
Iteration 4, loss = 0.63469496
Iteration 5, loss = 0.61966995
Iteration 6, loss = 0.60385856
Iteration 7, loss = 0.58693387
Iteration 8, loss = 0.56899973
Iteration 9, loss = 0.54984868
Iteration 10, loss = 0.52970201
Iteration 11, loss = 0.50866173
Iteration 12, loss = 0.48682148
Iteration 13, loss = 0.46428065
Iteration 14, loss = 0.44158954
Iteration 15, loss = 0.41882309
Iteration 16, loss = 0.39621705
Iteration 17, loss = 0.37417283
Iteration 18, loss = 0.35267459
Iteration 19, loss = 0.33207978
Iteration 20, loss = 0.31206766
Iteration 21, loss = 0.29314397
Iteration 22, loss = 0.27504732
Iteration 23, loss = 0.25790761
Iteration 24, loss = 0.24166601
Iteration 25, loss = 0.22649146
Iteration 26, loss = 0.21215078
Iteration 27, loss = 0.19863330
Iteration 28, loss = 0.18616194
Iteration 29, loss = 0.17443290
Iteration 30, loss = 0.16350920
Iteration 31, loss = 0.15332079
Iteration 32, loss = 0.14395517
Iteration 33, loss = 0.13517948
Iteration 34, loss = 0.12709834
Iteration 35, loss = 0.11957488
Iteration 36, loss = 0.11264855
Iteration 37, loss = 0.10625018
Iteration 38, loss = 0.10030659
Iteration 39, loss = 0.09474658
Iteration 40, loss = 0.08965065
Iteration 41, loss = 0.08492848
Iteration 42, loss = 0.08049863
Iteration 43, loss = 0.07643251
Iteration 44, loss = 0.07260831
Iteration 45, loss = 0.06903782
Iteration 46, loss = 0.06574562
Iteration 47, loss = 0.06264424
Iteration 48, loss = 0.05976857
Iteration 49, loss = 0.05709967
Iteration 50, loss = 0.05456207
Iteration 51, loss = 0.05221175
Iteration 52, loss = 0.05001491
Iteration 53, loss = 0.04791223
Iteration 54, loss = 0.04600334
Iteration 55, loss = 0.04415617
Iteration 56, loss = 0.04244197
Iteration 57, loss = 0.04083900
Iteration 58, loss = 0.03930321
Iteration 59, loss = 0.03785341
Iteration 60, loss = 0.03651295
Iteration 61, loss = 0.03521217
Iteration 62, loss = 0.03399193
Iteration 63, loss = 0.03282670
Iteration 64, loss = 0.03172333
Iteration 65, loss = 0.03067669
Iteration 66, loss = 0.02967771
Iteration 67, loss = 0.02874353
Iteration 68, loss = 0.02782974
Iteration 69, loss = 0.02696986
Iteration 70, loss = 0.02616110
Iteration 71, loss = 0.02538005
Iteration 72, loss = 0.02463211
Iteration 73, loss = 0.02392311
Iteration 74, loss = 0.02325436
Iteration 75, loss = 0.02260206
Iteration 76, loss = 0.02197841
Iteration 77, loss = 0.02139027
Iteration 78, loss = 0.02082498
Iteration 79, loss = 0.02027433
Iteration 80, loss = 0.01975562
Iteration 81, loss = 0.01925264
Iteration 82, loss = 0.01877367
Iteration 83, loss = 0.01831177
Iteration 84, loss = 0.01786198
Iteration 85, loss = 0.01744123
Iteration 86, loss = 0.01702967
Iteration 87, loss = 0.01663340
Iteration 88, loss = 0.01624491
Iteration 89, loss = 0.01587993
Iteration 90, loss = 0.01552743
Iteration 91, loss = 0.01518196
Iteration 92, loss = 0.01484957
Iteration 93, loss = 0.01453167
Iteration 94, loss = 0.01422143
Iteration 95, loss = 0.01392217
Iteration 96, loss = 0.01363616
Iteration 97, loss = 0.01335694
Iteration 98, loss = 0.01308837
Iteration 99, loss = 0.01282711
Iteration 100, loss = 0.01257443
Iteration 101, loss = 0.01233227
Iteration 102, loss = 0.01209199
Iteration 103, loss = 0.01186203
Iteration 104, loss = 0.01163722
Iteration 105, loss = 0.01142472
Iteration 106, loss = 0.01121156
Iteration 107, loss = 0.01101097
Iteration 108, loss = 0.01081880
Iteration 109, loss = 0.01062298
Iteration 110, loss = 0.01043875
Iteration 111, loss = 0.01025840
Iteration 112, loss = 0.01008500
Iteration 113, loss = 0.00991333
Iteration 114, loss = 0.00974849
Iteration 115, loss = 0.00958707
Iteration 116, loss = 0.00943079
Iteration 117, loss = 0.00927811
Iteration 118, loss = 0.00913025
Iteration 119, loss = 0.00898664
Iteration 120, loss = 0.00884512
Iteration 121, loss = 0.00870525
Iteration 122, loss = 0.00857357
Iteration 123, loss = 0.00844364
Iteration 124, loss = 0.00831620
Iteration 125, loss = 0.00819135
Iteration 126, loss = 0.00807206
Iteration 127, loss = 0.00795388
Iteration 128, loss = 0.00784174
Iteration 129, loss = 0.00772985
Iteration 130, loss = 0.00762065
Iteration 131, loss = 0.00751554
Iteration 132, loss = 0.00741181
Iteration 133, loss = 0.00731074
Iteration 134, loss = 0.00721024
Iteration 135, loss = 0.00711339
Iteration 136, loss = 0.00701956
Iteration 137, loss = 0.00692900
Iteration 138, loss = 0.00683661
Iteration 139, loss = 0.00674752
Iteration 140, loss = 0.00666457
Iteration 141, loss = 0.00657842
Iteration 142, loss = 0.00649499
Iteration 143, loss = 0.00641370
Iteration 144, loss = 0.00633461
Iteration 145, loss = 0.00625853
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70647779
Iteration 2, loss = 0.63637193
Iteration 3, loss = 0.52743514
Iteration 4, loss = 0.40213759
Iteration 5, loss = 0.29545827
Iteration 6, loss = 0.21697809
Iteration 7, loss = 0.16274628
Iteration 8, loss = 0.12640774
Iteration 9, loss = 0.10153933
Iteration 10, loss = 0.08393178
Iteration 11, loss = 0.07109848
Iteration 12, loss = 0.06177389
Iteration 13, loss = 0.05456631
Iteration 14, loss = 0.04906587
Iteration 15, loss = 0.04471583
Iteration 16, loss = 0.04097761
Iteration 17, loss = 0.03817698
Iteration 18, loss = 0.03540410
Iteration 19, loss = 0.03346476
Iteration 20, loss = 0.03179684
Iteration 21, loss = 0.03021033
Iteration 22, loss = 0.02910203
Iteration 23, loss = 0.02788961
Iteration 24, loss = 0.02732795
Iteration 25, loss = 0.02611062
Iteration 26, loss = 0.02544298
Iteration 27, loss = 0.02479442
Iteration 28, loss = 0.02417645
Iteration 29, loss = 0.02367587
Iteration 30, loss = 0.02330948
Iteration 31, loss = 0.02280217
Iteration 32, loss = 0.02233300
Iteration 33, loss = 0.02196103
Iteration 34, loss = 0.02168911
Iteration 35, loss = 0.02137552
Iteration 36, loss = 0.02104855
Iteration 37, loss = 0.02092295
Iteration 38, loss = 0.02054258
Iteration 39, loss = 0.02055994
Iteration 40, loss = 0.02032993
Iteration 41, loss = 0.02016261
Iteration 42, loss = 0.02027345
Iteration 43, loss = 0.01974572
Iteration 44, loss = 0.01946213
Iteration 45, loss = 0.01960230
Iteration 46, loss = 0.01941747
Iteration 47, loss = 0.01917517
Iteration 48, loss = 0.01917161
Iteration 49, loss = 0.01923710
Iteration 50, loss = 0.01883150
Iteration 51, loss = 0.01874252
Iteration 52, loss = 0.01868388
Iteration 53, loss = 0.01868787
Iteration 54, loss = 0.01868094
Iteration 55, loss = 0.01836931
Iteration 56, loss = 0.01830179
Iteration 57, loss = 0.01841156
Iteration 58, loss = 0.01836947
Iteration 59, loss = 0.01837311
Iteration 60, loss = 0.01831709
Iteration 61, loss = 0.01829553
Iteration 62, loss = 0.01800228
Iteration 63, loss = 0.01812131
Iteration 64, loss = 0.01779035
Iteration 65, loss = 0.01804888
Iteration 66, loss = 0.01803854
Iteration 67, loss = 0.01814149
Iteration 68, loss = 0.01779508
Iteration 69, loss = 0.01782534
Iteration 70, loss = 0.01779864
Iteration 71, loss = 0.01819914
Iteration 72, loss = 0.01802311
Iteration 73, loss = 0.01787036
Iteration 74, loss = 0.01758202
Iteration 75, loss = 0.01787349
Iteration 76, loss = 0.01760465
Iteration 77, loss = 0.01767726
Iteration 78, loss = 0.01755278
Iteration 79, loss = 0.01751303
Iteration 80, loss = 0.01723152
Iteration 81, loss = 0.01740460
Iteration 82, loss = 0.01748153
Iteration 83, loss = 0.01744603
Iteration 84, loss = 0.01738904
Iteration 85, loss = 0.01730927
Iteration 86, loss = 0.01740185
Iteration 87, loss = 0.01740385
Iteration 88, loss = 0.01735555
Iteration 89, loss = 0.01748506
Iteration 90, loss = 0.01724702
Iteration 91, loss = 0.01707729
Iteration 92, loss = 0.01734822
Iteration 93, loss = 0.01736757
Iteration 94, loss = 0.01730963
Iteration 95, loss = 0.01726698
Iteration 96, loss = 0.01735017
Iteration 97, loss = 0.01702444
Iteration 98, loss = 0.01707293
Iteration 99, loss = 0.01711914
Iteration 100, loss = 0.01700964
Iteration 101, loss = 0.01714489
Iteration 102, loss = 0.01741308
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69210333
Iteration 2, loss = 0.66587061
Iteration 3, loss = 0.63513450
Iteration 4, loss = 0.59502582
Iteration 5, loss = 0.54648933
Iteration 6, loss = 0.49473699
Iteration 7, loss = 0.44211075
Iteration 8, loss = 0.39177687
Iteration 9, loss = 0.34480501
Iteration 10, loss = 0.30247982
Iteration 11, loss = 0.26495057
Iteration 12, loss = 0.23181601
Iteration 13, loss = 0.20341489
Iteration 14, loss = 0.17907168
Iteration 15, loss = 0.15833545
Iteration 16, loss = 0.14069560
Iteration 17, loss = 0.12580107
Iteration 18, loss = 0.11298056
Iteration 19, loss = 0.10196024
Iteration 20, loss = 0.09273993
Iteration 21, loss = 0.08459997
Iteration 22, loss = 0.07779001
Iteration 23, loss = 0.07154251
Iteration 24, loss = 0.06629156
Iteration 25, loss = 0.06160886
Iteration 26, loss = 0.05742316
Iteration 27, loss = 0.05386097
Iteration 28, loss = 0.05048522
Iteration 29, loss = 0.04746400
Iteration 30, loss = 0.04484807
Iteration 31, loss = 0.04240460
Iteration 32, loss = 0.04027556
Iteration 33, loss = 0.03822092
Iteration 34, loss = 0.03661242
Iteration 35, loss = 0.03488756
Iteration 36, loss = 0.03335703
Iteration 37, loss = 0.03205468
Iteration 38, loss = 0.03082744
Iteration 39, loss = 0.02956216
Iteration 40, loss = 0.02861416
Iteration 41, loss = 0.02769845
Iteration 42, loss = 0.02682612
Iteration 43, loss = 0.02591691
Iteration 44, loss = 0.02505106
Iteration 45, loss = 0.02427916
Iteration 46, loss = 0.02355827
Iteration 47, loss = 0.02273218
Iteration 48, loss = 0.02231247
Iteration 49, loss = 0.02182207
Iteration 50, loss = 0.02124166
Iteration 51, loss = 0.02057953
Iteration 52, loss = 0.02007865
Iteration 53, loss = 0.01962914
Iteration 54, loss = 0.01922925
Iteration 55, loss = 0.01889583
Iteration 56, loss = 0.01843067
Iteration 57, loss = 0.01806364
Iteration 58, loss = 0.01777232
Iteration 59, loss = 0.01737542
Iteration 60, loss = 0.01704921
Iteration 61, loss = 0.01670501
Iteration 62, loss = 0.01639739
Iteration 63, loss = 0.01614314
Iteration 64, loss = 0.01595267
Iteration 65, loss = 0.01565803
Iteration 66, loss = 0.01544449
Iteration 67, loss = 0.01531252
Iteration 68, loss = 0.01501971
Iteration 69, loss = 0.01486611
Iteration 70, loss = 0.01460678
Iteration 71, loss = 0.01438657
Iteration 72, loss = 0.01424126
Iteration 73, loss = 0.01410791
Iteration 74, loss = 0.01396443
Iteration 75, loss = 0.01374340
Iteration 76, loss = 0.01358570
Iteration 77, loss = 0.01349222
Iteration 78, loss = 0.01331466
Iteration 79, loss = 0.01312034
Iteration 80, loss = 0.01300853
Iteration 81, loss = 0.01296327
Iteration 82, loss = 0.01294413
Iteration 83, loss = 0.01289237
Iteration 84, loss = 0.01288901
Iteration 85, loss = 0.01263267
Iteration 86, loss = 0.01248048
Iteration 87, loss = 0.01232296
Iteration 88, loss = 0.01224285
Iteration 89, loss = 0.01220049
Iteration 90, loss = 0.01215240
Iteration 91, loss = 0.01200373
Iteration 92, loss = 0.01188712
Iteration 93, loss = 0.01181419
Iteration 94, loss = 0.01169168
Iteration 95, loss = 0.01163784
Iteration 96, loss = 0.01157759
Iteration 97, loss = 0.01148126
Iteration 98, loss = 0.01152760
Iteration 99, loss = 0.01147324
Iteration 100, loss = 0.01138057
Iteration 101, loss = 0.01155645
Iteration 102, loss = 0.01133413
Iteration 103, loss = 0.01121614
Iteration 104, loss = 0.01100242
Iteration 105, loss = 0.01087119
Iteration 106, loss = 0.01063585
Iteration 107, loss = 0.01105263
Iteration 108, loss = 0.01109764
Iteration 109, loss = 0.01108897
Iteration 110, loss = 0.01057518
Iteration 111, loss = 0.01062247
Iteration 112, loss = 0.01059574
Iteration 113, loss = 0.01055930
Iteration 114, loss = 0.01043101
Iteration 115, loss = 0.01035655
Iteration 116, loss = 0.01047366
Iteration 117, loss = 0.01038639
Iteration 118, loss = 0.01038532
Iteration 119, loss = 0.01035709
Iteration 120, loss = 0.01023037
Iteration 121, loss = 0.01017514
Iteration 122, loss = 0.01014250
Iteration 123, loss = 0.01008151
Iteration 124, loss = 0.01008168
Iteration 125, loss = 0.01006925
Iteration 126, loss = 0.01004019
Iteration 127, loss = 0.01001408
Iteration 128, loss = 0.01004074
Iteration 129, loss = 0.00995804
Iteration 130, loss = 0.00993208
Iteration 131, loss = 0.00987338
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68121714
Iteration 2, loss = 0.65641479
Iteration 3, loss = 0.63236820
Iteration 4, loss = 0.60630700
Iteration 5, loss = 0.57748469
Iteration 6, loss = 0.54477165
Iteration 7, loss = 0.51005103
Iteration 8, loss = 0.47366876
Iteration 9, loss = 0.43724163
Iteration 10, loss = 0.40166965
Iteration 11, loss = 0.36772756
Iteration 12, loss = 0.33545421
Iteration 13, loss = 0.30540538
Iteration 14, loss = 0.27772443
Iteration 15, loss = 0.25220537
Iteration 16, loss = 0.22885586
Iteration 17, loss = 0.20778205
Iteration 18, loss = 0.18897471
Iteration 19, loss = 0.17182788
Iteration 20, loss = 0.15668450
Iteration 21, loss = 0.14315160
Iteration 22, loss = 0.13107234
Iteration 23, loss = 0.12035876
Iteration 24, loss = 0.11074252
Iteration 25, loss = 0.10227090
Iteration 26, loss = 0.09472200
Iteration 27, loss = 0.08787506
Iteration 28, loss = 0.08185451
Iteration 29, loss = 0.07644787
Iteration 30, loss = 0.07156960
Iteration 31, loss = 0.06712915
Iteration 32, loss = 0.06318476
Iteration 33, loss = 0.05953891
Iteration 34, loss = 0.05633398
Iteration 35, loss = 0.05336437
Iteration 36, loss = 0.05059325
Iteration 37, loss = 0.04817368
Iteration 38, loss = 0.04587782
Iteration 39, loss = 0.04382962
Iteration 40, loss = 0.04188436
Iteration 41, loss = 0.04015205
Iteration 42, loss = 0.03851413
Iteration 43, loss = 0.03703514
Iteration 44, loss = 0.03567366
Iteration 45, loss = 0.03429635
Iteration 46, loss = 0.03313833
Iteration 47, loss = 0.03198770
Iteration 48, loss = 0.03090843
Iteration 49, loss = 0.02997324
Iteration 50, loss = 0.02906759
Iteration 51, loss = 0.02816980
Iteration 52, loss = 0.02734825
Iteration 53, loss = 0.02660126
Iteration 54, loss = 0.02591848
Iteration 55, loss = 0.02522611
Iteration 56, loss = 0.02456271
Iteration 57, loss = 0.02395395
Iteration 58, loss = 0.02339358
Iteration 59, loss = 0.02284368
Iteration 60, loss = 0.02233027
Iteration 61, loss = 0.02186334
Iteration 62, loss = 0.02144854
Iteration 63, loss = 0.02099110
Iteration 64, loss = 0.02054695
Iteration 65, loss = 0.02017342
Iteration 66, loss = 0.01979210
Iteration 67, loss = 0.01946859
Iteration 68, loss = 0.01910968
Iteration 69, loss = 0.01875745
Iteration 70, loss = 0.01842885
Iteration 71, loss = 0.01813631
Iteration 72, loss = 0.01782614
Iteration 73, loss = 0.01758483
Iteration 74, loss = 0.01729667
Iteration 75, loss = 0.01712045
Iteration 76, loss = 0.01685194
Iteration 77, loss = 0.01660147
Iteration 78, loss = 0.01637366
Iteration 79, loss = 0.01618470
Iteration 80, loss = 0.01593530
Iteration 81, loss = 0.01576034
Iteration 82, loss = 0.01556171
Iteration 83, loss = 0.01539415
Iteration 84, loss = 0.01522961
Iteration 85, loss = 0.01504835
Iteration 86, loss = 0.01493495
Iteration 87, loss = 0.01470689
Iteration 88, loss = 0.01454158
Iteration 89, loss = 0.01437831
Iteration 90, loss = 0.01424331
Iteration 91, loss = 0.01413352
Iteration 92, loss = 0.01396952
Iteration 93, loss = 0.01385211
Iteration 94, loss = 0.01373542
Iteration 95, loss = 0.01362185
Iteration 96, loss = 0.01353581
Iteration 97, loss = 0.01335198
Iteration 98, loss = 0.01325448
Iteration 99, loss = 0.01320680
Iteration 100, loss = 0.01309901
Iteration 101, loss = 0.01302194
Iteration 102, loss = 0.01286567
Iteration 103, loss = 0.01280351
Iteration 104, loss = 0.01268158
Iteration 105, loss = 0.01265224
Iteration 106, loss = 0.01253853
Iteration 107, loss = 0.01247246
Iteration 108, loss = 0.01235165
Iteration 109, loss = 0.01225954
Iteration 110, loss = 0.01221115
Iteration 111, loss = 0.01214041
Iteration 112, loss = 0.01211772
Iteration 113, loss = 0.01194906
Iteration 114, loss = 0.01189663
Iteration 115, loss = 0.01186257
Iteration 116, loss = 0.01179023
Iteration 117, loss = 0.01173830
Iteration 118, loss = 0.01167742
Iteration 119, loss = 0.01161987
Iteration 120, loss = 0.01154923
Iteration 121, loss = 0.01150263
Iteration 122, loss = 0.01151494
Iteration 123, loss = 0.01139420
Iteration 124, loss = 0.01132890
Iteration 125, loss = 0.01131214
Iteration 126, loss = 0.01126082
Iteration 127, loss = 0.01116437
Iteration 128, loss = 0.01114011
Iteration 129, loss = 0.01113550
Iteration 130, loss = 0.01104744
Iteration 131, loss = 0.01107504
Iteration 132, loss = 0.01093409
Iteration 133, loss = 0.01089007
Iteration 134, loss = 0.01087891
Iteration 135, loss = 0.01080731
Iteration 136, loss = 0.01085672
Iteration 137, loss = 0.01074038
Iteration 138, loss = 0.01074292
Iteration 139, loss = 0.01067660
Iteration 140, loss = 0.01071677
Iteration 141, loss = 0.01062092
Iteration 142, loss = 0.01066185
Iteration 143, loss = 0.01055733
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71618376
Iteration 2, loss = 0.65084821
Iteration 3, loss = 0.55324912
Iteration 4, loss = 0.43620749
Iteration 5, loss = 0.32799933
Iteration 6, loss = 0.24461309
Iteration 7, loss = 0.18452977
Iteration 8, loss = 0.14346908
Iteration 9, loss = 0.11476242
Iteration 10, loss = 0.09481137
Iteration 11, loss = 0.08027613
Iteration 12, loss = 0.06950288
Iteration 13, loss = 0.06127587
Iteration 14, loss = 0.05474629
Iteration 15, loss = 0.04950125
Iteration 16, loss = 0.04543985
Iteration 17, loss = 0.04198727
Iteration 18, loss = 0.03939004
Iteration 19, loss = 0.03702503
Iteration 20, loss = 0.03503961
Iteration 21, loss = 0.03300789
Iteration 22, loss = 0.03174187
Iteration 23, loss = 0.03038926
Iteration 24, loss = 0.02945545
Iteration 25, loss = 0.02833275
Iteration 26, loss = 0.02740782
Iteration 27, loss = 0.02696959
Iteration 28, loss = 0.02589346
Iteration 29, loss = 0.02552235
Iteration 30, loss = 0.02504111
Iteration 31, loss = 0.02444770
Iteration 32, loss = 0.02401309
Iteration 33, loss = 0.02351753
Iteration 34, loss = 0.02298758
Iteration 35, loss = 0.02292301
Iteration 36, loss = 0.02244488
Iteration 37, loss = 0.02245164
Iteration 38, loss = 0.02197992
Iteration 39, loss = 0.02175001
Iteration 40, loss = 0.02180871
Iteration 41, loss = 0.02143985
Iteration 42, loss = 0.02104490
Iteration 43, loss = 0.02098982
Iteration 44, loss = 0.02089684
Iteration 45, loss = 0.02068464
Iteration 46, loss = 0.02045967
Iteration 47, loss = 0.02032339
Iteration 48, loss = 0.02016579
Iteration 49, loss = 0.02010053
Iteration 50, loss = 0.01988290
Iteration 51, loss = 0.01968658
Iteration 52, loss = 0.01969675
Iteration 53, loss = 0.01978582
Iteration 54, loss = 0.01962525
Iteration 55, loss = 0.01962593
Iteration 56, loss = 0.01957634
Iteration 57, loss = 0.01923312
Iteration 58, loss = 0.01942971
Iteration 59, loss = 0.01940054
Iteration 60, loss = 0.01916985
Iteration 61, loss = 0.01917943
Iteration 62, loss = 0.01920099
Iteration 63, loss = 0.01899650
Iteration 64, loss = 0.01889650
Iteration 65, loss = 0.01886153
Iteration 66, loss = 0.01894790
Iteration 67, loss = 0.01872240
Iteration 68, loss = 0.01924216
Iteration 69, loss = 0.01887941
Iteration 70, loss = 0.01874859
Iteration 71, loss = 0.01869508
Iteration 72, loss = 0.01856551
Iteration 73, loss = 0.01839762
Iteration 74, loss = 0.01858885
Iteration 75, loss = 0.01865528
Iteration 76, loss = 0.01876746
Iteration 77, loss = 0.01842201
Iteration 78, loss = 0.01848116
Iteration 79, loss = 0.01829643
Iteration 80, loss = 0.01838676
Iteration 81, loss = 0.01830085
Iteration 82, loss = 0.01841917
Iteration 83, loss = 0.01816668
Iteration 84, loss = 0.01830847
Iteration 85, loss = 0.01827129
Iteration 86, loss = 0.01816949
Iteration 87, loss = 0.01849206
Iteration 88, loss = 0.01821456
Iteration 89, loss = 0.01811242
Iteration 90, loss = 0.01796840
Iteration 91, loss = 0.01814376
Iteration 92, loss = 0.01836734
Iteration 93, loss = 0.01840908
Iteration 94, loss = 0.01809368
Iteration 95, loss = 0.01804961
Iteration 96, loss = 0.01795368
Iteration 97, loss = 0.01816006
Iteration 98, loss = 0.01779074
Iteration 99, loss = 0.01797396
Iteration 100, loss = 0.01782456
Iteration 101, loss = 0.01818689
Iteration 102, loss = 0.01792308
Iteration 103, loss = 0.01775662
Iteration 104, loss = 0.01777259
Iteration 105, loss = 0.01805085
Iteration 106, loss = 0.01791617
Iteration 107, loss = 0.01780914
Iteration 108, loss = 0.01775925
Iteration 109, loss = 0.01756637
Iteration 110, loss = 0.01795587
Iteration 111, loss = 0.01784726
Iteration 112, loss = 0.01767148
Iteration 113, loss = 0.01782127
Iteration 114, loss = 0.01782953
Iteration 115, loss = 0.01764952
Iteration 116, loss = 0.01771722
Iteration 117, loss = 0.01770521
Iteration 118, loss = 0.01760547
Iteration 119, loss = 0.01764611
Iteration 120, loss = 0.01772218
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69360218
Iteration 2, loss = 0.62843489
Iteration 3, loss = 0.53134271
Iteration 4, loss = 0.41720684
Iteration 5, loss = 0.31548012
Iteration 6, loss = 0.23568042
Iteration 7, loss = 0.17791641
Iteration 8, loss = 0.13799275
Iteration 9, loss = 0.11017817
Iteration 10, loss = 0.09062195
Iteration 11, loss = 0.07618564
Iteration 12, loss = 0.06556430
Iteration 13, loss = 0.05762168
Iteration 14, loss = 0.05145061
Iteration 15, loss = 0.04640022
Iteration 16, loss = 0.04266658
Iteration 17, loss = 0.03927499
Iteration 18, loss = 0.03674341
Iteration 19, loss = 0.03451081
Iteration 20, loss = 0.03266936
Iteration 21, loss = 0.03120030
Iteration 22, loss = 0.02958647
Iteration 23, loss = 0.02854486
Iteration 24, loss = 0.02760072
Iteration 25, loss = 0.02653060
Iteration 26, loss = 0.02614158
Iteration 27, loss = 0.02517038
Iteration 28, loss = 0.02460467
Iteration 29, loss = 0.02406081
Iteration 30, loss = 0.02350005
Iteration 31, loss = 0.02321830
Iteration 32, loss = 0.02258622
Iteration 33, loss = 0.02232739
Iteration 34, loss = 0.02181191
Iteration 35, loss = 0.02170794
Iteration 36, loss = 0.02119923
Iteration 37, loss = 0.02104963
Iteration 38, loss = 0.02085594
Iteration 39, loss = 0.02077942
Iteration 40, loss = 0.02029697
Iteration 41, loss = 0.02013437
Iteration 42, loss = 0.02001735
Iteration 43, loss = 0.01998420
Iteration 44, loss = 0.02000200
Iteration 45, loss = 0.01971392
Iteration 46, loss = 0.01953552
Iteration 47, loss = 0.01942923
Iteration 48, loss = 0.01924479
Iteration 49, loss = 0.01918327
Iteration 50, loss = 0.01917469
Iteration 51, loss = 0.01902895
Iteration 52, loss = 0.01903387
Iteration 53, loss = 0.01893073
Iteration 54, loss = 0.01874587
Iteration 55, loss = 0.01871141
Iteration 56, loss = 0.01856626
Iteration 57, loss = 0.01858783
Iteration 58, loss = 0.01848738
Iteration 59, loss = 0.01822531
Iteration 60, loss = 0.01839303
Iteration 61, loss = 0.01836041
Iteration 62, loss = 0.01840706
Iteration 63, loss = 0.01821215
Iteration 64, loss = 0.01823607
Iteration 65, loss = 0.01814941
Iteration 66, loss = 0.01802952
Iteration 67, loss = 0.01804074
Iteration 68, loss = 0.01794167
Iteration 69, loss = 0.01808137
Iteration 70, loss = 0.01772721
Iteration 71, loss = 0.01782404
Iteration 72, loss = 0.01799297
Iteration 73, loss = 0.01801936
Iteration 74, loss = 0.01782037
Iteration 75, loss = 0.01781995
Iteration 76, loss = 0.01788651
Iteration 77, loss = 0.01771513
Iteration 78, loss = 0.01781790
Iteration 79, loss = 0.01773885
Iteration 80, loss = 0.01760837
Iteration 81, loss = 0.01768958
Iteration 82, loss = 0.01763825
Iteration 83, loss = 0.01774235
Iteration 84, loss = 0.01785678
Iteration 85, loss = 0.01772493
Iteration 86, loss = 0.01758489
Iteration 87, loss = 0.01775199
Iteration 88, loss = 0.01739094
Iteration 89, loss = 0.01769626
Iteration 90, loss = 0.01743314
Iteration 91, loss = 0.01751953
Iteration 92, loss = 0.01741080
Iteration 93, loss = 0.01730546
Iteration 94, loss = 0.01733519
Iteration 95, loss = 0.01740657
Iteration 96, loss = 0.01738540
Iteration 97, loss = 0.01749122
Iteration 98, loss = 0.01724152
Iteration 99, loss = 0.01745888
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	115m3.342s
user	809m29.902s
sys	92m55.483s
Elapsed time: 6903 seconds
