Processing dataset: prepended_v6_custom_stopwords

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: prepended_v6_custom_stopwords (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on prepended_v6_custom_stopwords...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on prepended_v6_custom_stopwords...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on prepended_v6_custom_stopwords...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on prepended_v6_custom_stopwords...
/opt/ohpc/pub/spack/v0.21.1/opt/spack/linux-rocky8-x86_64_v3/gcc-8.5.0/py-joblib-1.2.0-g6vh2usiiivwucfr57aoxwqvljmoxokc/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on prepended_v6_custom_stopwords...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.70435501
Iteration 2, loss = 0.62728183
Iteration 3, loss = 0.51437906
Iteration 4, loss = 0.39247408
Iteration 5, loss = 0.29145794
Iteration 6, loss = 0.21695364
Iteration 7, loss = 0.16554978
Iteration 8, loss = 0.13067595
Iteration 9, loss = 0.10626822
Iteration 10, loss = 0.08914057
Iteration 11, loss = 0.07685323
Iteration 12, loss = 0.06756219
Iteration 13, loss = 0.06042668
Iteration 14, loss = 0.05483094
Iteration 15, loss = 0.05021856
Iteration 16, loss = 0.04677476
Iteration 17, loss = 0.04376692
Iteration 18, loss = 0.04148947
Iteration 19, loss = 0.03935313
Iteration 20, loss = 0.03754967
Iteration 21, loss = 0.03601185
Iteration 22, loss = 0.03476223
Iteration 23, loss = 0.03362978
Iteration 24, loss = 0.03281631
Iteration 25, loss = 0.03197180
Iteration 26, loss = 0.03120172
Iteration 27, loss = 0.03034537
Iteration 28, loss = 0.02993750
Iteration 29, loss = 0.02951960
Iteration 30, loss = 0.02895282
Iteration 31, loss = 0.02871308
Iteration 32, loss = 0.02782143
Iteration 33, loss = 0.02762572
Iteration 34, loss = 0.02769134
Iteration 35, loss = 0.02717225
Iteration 36, loss = 0.02686616
Iteration 37, loss = 0.02641456
Iteration 38, loss = 0.02621393
Iteration 39, loss = 0.02628956
Iteration 40, loss = 0.02631816
Iteration 41, loss = 0.02599454
Iteration 42, loss = 0.02577782
Iteration 43, loss = 0.02556370
Iteration 44, loss = 0.02537917
Iteration 45, loss = 0.02538006
Iteration 46, loss = 0.02502907
Iteration 47, loss = 0.02509117
Iteration 48, loss = 0.02510517
Iteration 49, loss = 0.02466405
Iteration 50, loss = 0.02475366
Iteration 51, loss = 0.02464314
Iteration 52, loss = 0.02467268
Iteration 53, loss = 0.02446797
Iteration 54, loss = 0.02427843
Iteration 55, loss = 0.02437354
Iteration 56, loss = 0.02432166
Iteration 57, loss = 0.02432493
Iteration 58, loss = 0.02397981
Iteration 59, loss = 0.02426657
Iteration 60, loss = 0.02412190
Iteration 61, loss = 0.02415037
Iteration 62, loss = 0.02397253
Iteration 63, loss = 0.02427533
Iteration 64, loss = 0.02373623
Iteration 65, loss = 0.02392321
Iteration 66, loss = 0.02353754
Iteration 67, loss = 0.02400496
Iteration 68, loss = 0.02393141
Iteration 69, loss = 0.02366927
Iteration 70, loss = 0.02349025
Iteration 71, loss = 0.02368599
Iteration 72, loss = 0.02367690
Iteration 73, loss = 0.02356853
Iteration 74, loss = 0.02348152
Iteration 75, loss = 0.02340327
Iteration 76, loss = 0.02347401
Iteration 77, loss = 0.02331509
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67445161
Iteration 2, loss = 0.60493125
Iteration 3, loss = 0.49922063
Iteration 4, loss = 0.38940362
Iteration 5, loss = 0.29149273
Iteration 6, loss = 0.21865304
Iteration 7, loss = 0.16681018
Iteration 8, loss = 0.13188259
Iteration 9, loss = 0.10748897
Iteration 10, loss = 0.09032374
Iteration 11, loss = 0.07746035
Iteration 12, loss = 0.06833552
Iteration 13, loss = 0.06114615
Iteration 14, loss = 0.05560371
Iteration 15, loss = 0.05110309
Iteration 16, loss = 0.04767235
Iteration 17, loss = 0.04455093
Iteration 18, loss = 0.04204715
Iteration 19, loss = 0.03997549
Iteration 20, loss = 0.03845714
Iteration 21, loss = 0.03700746
Iteration 22, loss = 0.03560248
Iteration 23, loss = 0.03479581
Iteration 24, loss = 0.03340833
Iteration 25, loss = 0.03253842
Iteration 26, loss = 0.03182669
Iteration 27, loss = 0.03121694
Iteration 28, loss = 0.03079994
Iteration 29, loss = 0.03015695
Iteration 30, loss = 0.02976948
Iteration 31, loss = 0.02931994
Iteration 32, loss = 0.02898095
Iteration 33, loss = 0.02873362
Iteration 34, loss = 0.02823461
Iteration 35, loss = 0.02780820
Iteration 36, loss = 0.02751219
Iteration 37, loss = 0.02775765
Iteration 38, loss = 0.02735130
Iteration 39, loss = 0.02708643
Iteration 40, loss = 0.02699037
Iteration 41, loss = 0.02667709
Iteration 42, loss = 0.02648857
Iteration 43, loss = 0.02637070
Iteration 44, loss = 0.02610956
Iteration 45, loss = 0.02606354
Iteration 46, loss = 0.02575951
Iteration 47, loss = 0.02568791
Iteration 48, loss = 0.02570235
Iteration 49, loss = 0.02585590
Iteration 50, loss = 0.02607123
Iteration 51, loss = 0.02539777
Iteration 52, loss = 0.02546790
Iteration 53, loss = 0.02532471
Iteration 54, loss = 0.02546113
Iteration 55, loss = 0.02530626
Iteration 56, loss = 0.02506329
Iteration 57, loss = 0.02499907
Iteration 58, loss = 0.02509928
Iteration 59, loss = 0.02505561
Iteration 60, loss = 0.02494620
Iteration 61, loss = 0.02506289
Iteration 62, loss = 0.02480351
Iteration 63, loss = 0.02483093
Iteration 64, loss = 0.02465390
Iteration 65, loss = 0.02457489
Iteration 66, loss = 0.02482621
Iteration 67, loss = 0.02462514
Iteration 68, loss = 0.02469843
Iteration 69, loss = 0.02455381
Iteration 70, loss = 0.02441403
Iteration 71, loss = 0.02447787
Iteration 72, loss = 0.02458195
Iteration 73, loss = 0.02447503
Iteration 74, loss = 0.02460127
Iteration 75, loss = 0.02476505
Iteration 76, loss = 0.02470579
Iteration 77, loss = 0.02450978
Iteration 78, loss = 0.02462227
Iteration 79, loss = 0.02414786
Iteration 80, loss = 0.02407267
Iteration 81, loss = 0.02419734
Iteration 82, loss = 0.02444721
Iteration 83, loss = 0.02416833
Iteration 84, loss = 0.02422419
Iteration 85, loss = 0.02439097
Iteration 86, loss = 0.02399159
Iteration 87, loss = 0.02445058
Iteration 88, loss = 0.02444588
Iteration 89, loss = 0.02389238
Iteration 90, loss = 0.02411081
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67433577
Iteration 2, loss = 0.60298029
Iteration 3, loss = 0.49355616
Iteration 4, loss = 0.38064102
Iteration 5, loss = 0.28460211
Iteration 6, loss = 0.21253050
Iteration 7, loss = 0.16214350
Iteration 8, loss = 0.12748033
Iteration 9, loss = 0.10369341
Iteration 10, loss = 0.08663775
Iteration 11, loss = 0.07443956
Iteration 12, loss = 0.06494460
Iteration 13, loss = 0.05819374
Iteration 14, loss = 0.05238761
Iteration 15, loss = 0.04805312
Iteration 16, loss = 0.04448159
Iteration 17, loss = 0.04139905
Iteration 18, loss = 0.03927127
Iteration 19, loss = 0.03724346
Iteration 20, loss = 0.03544835
Iteration 21, loss = 0.03397083
Iteration 22, loss = 0.03269237
Iteration 23, loss = 0.03176336
Iteration 24, loss = 0.03064322
Iteration 25, loss = 0.03012935
Iteration 26, loss = 0.02899292
Iteration 27, loss = 0.02834070
Iteration 28, loss = 0.02799639
Iteration 29, loss = 0.02743428
Iteration 30, loss = 0.02689381
Iteration 31, loss = 0.02648250
Iteration 32, loss = 0.02609928
Iteration 33, loss = 0.02583997
Iteration 34, loss = 0.02556306
Iteration 35, loss = 0.02530974
Iteration 36, loss = 0.02520846
Iteration 37, loss = 0.02478691
Iteration 38, loss = 0.02424268
Iteration 39, loss = 0.02412967
Iteration 40, loss = 0.02404329
Iteration 41, loss = 0.02395094
Iteration 42, loss = 0.02374902
Iteration 43, loss = 0.02370853
Iteration 44, loss = 0.02341138
Iteration 45, loss = 0.02340009
Iteration 46, loss = 0.02339301
Iteration 47, loss = 0.02299180
Iteration 48, loss = 0.02307684
Iteration 49, loss = 0.02333569
Iteration 50, loss = 0.02297506
Iteration 51, loss = 0.02279112
Iteration 52, loss = 0.02286067
Iteration 53, loss = 0.02284448
Iteration 54, loss = 0.02239483
Iteration 55, loss = 0.02241702
Iteration 56, loss = 0.02229799
Iteration 57, loss = 0.02216536
Iteration 58, loss = 0.02223384
Iteration 59, loss = 0.02231482
Iteration 60, loss = 0.02204534
Iteration 61, loss = 0.02210796
Iteration 62, loss = 0.02202939
Iteration 63, loss = 0.02212335
Iteration 64, loss = 0.02210884
Iteration 65, loss = 0.02213807
Iteration 66, loss = 0.02198367
Iteration 67, loss = 0.02216888
Iteration 68, loss = 0.02160017
Iteration 69, loss = 0.02186999
Iteration 70, loss = 0.02184731
Iteration 71, loss = 0.02189620
Iteration 72, loss = 0.02179960
Iteration 73, loss = 0.02170375
Iteration 74, loss = 0.02203734
Iteration 75, loss = 0.02174289
Iteration 76, loss = 0.02157361
Iteration 77, loss = 0.02149782
Iteration 78, loss = 0.02156520
Iteration 79, loss = 0.02164980
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70207344
Iteration 2, loss = 0.63254840
Iteration 3, loss = 0.52612288
Iteration 4, loss = 0.40731278
Iteration 5, loss = 0.30501050
Iteration 6, loss = 0.22711655
Iteration 7, loss = 0.17166749
Iteration 8, loss = 0.13342349
Iteration 9, loss = 0.10757845
Iteration 10, loss = 0.08914369
Iteration 11, loss = 0.07609954
Iteration 12, loss = 0.06630869
Iteration 13, loss = 0.05919679
Iteration 14, loss = 0.05332136
Iteration 15, loss = 0.04861940
Iteration 16, loss = 0.04494310
Iteration 17, loss = 0.04178658
Iteration 18, loss = 0.03935998
Iteration 19, loss = 0.03739660
Iteration 20, loss = 0.03585110
Iteration 21, loss = 0.03434176
Iteration 22, loss = 0.03284213
Iteration 23, loss = 0.03183563
Iteration 24, loss = 0.03090489
Iteration 25, loss = 0.02995685
Iteration 26, loss = 0.02913438
Iteration 27, loss = 0.02859461
Iteration 28, loss = 0.02827344
Iteration 29, loss = 0.02745906
Iteration 30, loss = 0.02687906
Iteration 31, loss = 0.02646542
Iteration 32, loss = 0.02621806
Iteration 33, loss = 0.02572736
Iteration 34, loss = 0.02543333
Iteration 35, loss = 0.02511730
Iteration 36, loss = 0.02460330
Iteration 37, loss = 0.02448594
Iteration 38, loss = 0.02453022
Iteration 39, loss = 0.02412078
Iteration 40, loss = 0.02396960
Iteration 41, loss = 0.02381928
Iteration 42, loss = 0.02371262
Iteration 43, loss = 0.02362756
Iteration 44, loss = 0.02334587
Iteration 45, loss = 0.02334379
Iteration 46, loss = 0.02308312
Iteration 47, loss = 0.02292125
Iteration 48, loss = 0.02283724
Iteration 49, loss = 0.02280600
Iteration 50, loss = 0.02280553
Iteration 51, loss = 0.02264347
Iteration 52, loss = 0.02253529
Iteration 53, loss = 0.02227900
Iteration 54, loss = 0.02240815
Iteration 55, loss = 0.02233172
Iteration 56, loss = 0.02212443
Iteration 57, loss = 0.02234806
Iteration 58, loss = 0.02218888
Iteration 59, loss = 0.02227635
Iteration 60, loss = 0.02218103
Iteration 61, loss = 0.02212008
Iteration 62, loss = 0.02204220
Iteration 63, loss = 0.02184004
Iteration 64, loss = 0.02189733
Iteration 65, loss = 0.02185477
Iteration 66, loss = 0.02177294
Iteration 67, loss = 0.02194686
Iteration 68, loss = 0.02170407
Iteration 69, loss = 0.02141785
Iteration 70, loss = 0.02153847
Iteration 71, loss = 0.02174795
Iteration 72, loss = 0.02135847
Iteration 73, loss = 0.02159786
Iteration 74, loss = 0.02173922
Iteration 75, loss = 0.02140959
Iteration 76, loss = 0.02127176
Iteration 77, loss = 0.02153963
Iteration 78, loss = 0.02133559
Iteration 79, loss = 0.02156920
Iteration 80, loss = 0.02148804
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69653199
Iteration 2, loss = 0.61678980
Iteration 3, loss = 0.50821113
Iteration 4, loss = 0.39061074
Iteration 5, loss = 0.29001911
Iteration 6, loss = 0.21545608
Iteration 7, loss = 0.16374371
Iteration 8, loss = 0.12881356
Iteration 9, loss = 0.10440275
Iteration 10, loss = 0.08740504
Iteration 11, loss = 0.07497453
Iteration 12, loss = 0.06551845
Iteration 13, loss = 0.05821645
Iteration 14, loss = 0.05296140
Iteration 15, loss = 0.04833238
Iteration 16, loss = 0.04468970
Iteration 17, loss = 0.04176596
Iteration 18, loss = 0.03938627
Iteration 19, loss = 0.03733579
Iteration 20, loss = 0.03555370
Iteration 21, loss = 0.03424463
Iteration 22, loss = 0.03283239
Iteration 23, loss = 0.03175788
Iteration 24, loss = 0.03072850
Iteration 25, loss = 0.03011316
Iteration 26, loss = 0.02909235
Iteration 27, loss = 0.02889603
Iteration 28, loss = 0.02809036
Iteration 29, loss = 0.02761769
Iteration 30, loss = 0.02706980
Iteration 31, loss = 0.02677431
Iteration 32, loss = 0.02624118
Iteration 33, loss = 0.02567348
Iteration 34, loss = 0.02565439
Iteration 35, loss = 0.02532806
Iteration 36, loss = 0.02510531
Iteration 37, loss = 0.02475113
Iteration 38, loss = 0.02466768
Iteration 39, loss = 0.02423849
Iteration 40, loss = 0.02450801
Iteration 41, loss = 0.02415839
Iteration 42, loss = 0.02387914
Iteration 43, loss = 0.02379758
Iteration 44, loss = 0.02346351
Iteration 45, loss = 0.02341052
Iteration 46, loss = 0.02321928
Iteration 47, loss = 0.02332452
Iteration 48, loss = 0.02326560
Iteration 49, loss = 0.02301975
Iteration 50, loss = 0.02314209
Iteration 51, loss = 0.02277468
Iteration 52, loss = 0.02308939
Iteration 53, loss = 0.02270607
Iteration 54, loss = 0.02267760
Iteration 55, loss = 0.02249498
Iteration 56, loss = 0.02239424
Iteration 57, loss = 0.02261781
Iteration 58, loss = 0.02293814
Iteration 59, loss = 0.02251796
Iteration 60, loss = 0.02240895
Iteration 61, loss = 0.02223653
Iteration 62, loss = 0.02240685
Iteration 63, loss = 0.02215924
Iteration 64, loss = 0.02184773
Iteration 65, loss = 0.02214073
Iteration 66, loss = 0.02203720
Iteration 67, loss = 0.02211500
Iteration 68, loss = 0.02202613
Iteration 69, loss = 0.02226904
Iteration 70, loss = 0.02200427
Iteration 71, loss = 0.02194662
Iteration 72, loss = 0.02157353
Iteration 73, loss = 0.02202780
Iteration 74, loss = 0.02176222
Iteration 75, loss = 0.02179921
Iteration 76, loss = 0.02175396
Iteration 77, loss = 0.02169542
Iteration 78, loss = 0.02197595
Iteration 79, loss = 0.02152209
Iteration 80, loss = 0.02155500
Iteration 81, loss = 0.02168712
Iteration 82, loss = 0.02166692
Iteration 83, loss = 0.02155510
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70435501
Iteration 2, loss = 0.62728183
Iteration 3, loss = 0.51437906
Iteration 4, loss = 0.39247408
Iteration 5, loss = 0.29145794
Iteration 6, loss = 0.21695364
Iteration 7, loss = 0.16554978
Iteration 8, loss = 0.13067595
Iteration 9, loss = 0.10626822
Iteration 10, loss = 0.08914057
Iteration 11, loss = 0.07685323
Iteration 12, loss = 0.06756219
Iteration 13, loss = 0.06042668
Iteration 14, loss = 0.05483094
Iteration 15, loss = 0.05021856
Iteration 16, loss = 0.04677476
Iteration 17, loss = 0.04376692
Iteration 18, loss = 0.04148947
Iteration 19, loss = 0.03935313
Iteration 20, loss = 0.03754967
Iteration 21, loss = 0.03601185
Iteration 22, loss = 0.03476223
Iteration 23, loss = 0.03362978
Iteration 24, loss = 0.03281631
Iteration 25, loss = 0.03197180
Iteration 26, loss = 0.03120172
Iteration 27, loss = 0.03034537
Iteration 28, loss = 0.02993750
Iteration 29, loss = 0.02951960
Iteration 30, loss = 0.02895282
Iteration 31, loss = 0.02871308
Iteration 32, loss = 0.02782143
Iteration 33, loss = 0.02762572
Iteration 34, loss = 0.02769134
Iteration 35, loss = 0.02717225
Iteration 36, loss = 0.02686616
Iteration 37, loss = 0.02641456
Iteration 38, loss = 0.02621393
Iteration 39, loss = 0.02628956
Iteration 40, loss = 0.02631816
Iteration 41, loss = 0.02599454
Iteration 42, loss = 0.02577782
Iteration 43, loss = 0.02556370
Iteration 44, loss = 0.02537917
Iteration 45, loss = 0.02538006
Iteration 46, loss = 0.02502907
Iteration 47, loss = 0.02509117
Iteration 48, loss = 0.02510517
Iteration 49, loss = 0.02466405
Iteration 50, loss = 0.02475366
Iteration 51, loss = 0.02464314
Iteration 52, loss = 0.02467268
Iteration 53, loss = 0.02446797
Iteration 54, loss = 0.02427843
Iteration 55, loss = 0.02437354
Iteration 56, loss = 0.02432166
Iteration 57, loss = 0.02432493
Iteration 58, loss = 0.02397981
Iteration 59, loss = 0.02426657
Iteration 60, loss = 0.02412190
Iteration 61, loss = 0.02415037
Iteration 62, loss = 0.02397253
Iteration 63, loss = 0.02427533
Iteration 64, loss = 0.02373623
Iteration 65, loss = 0.02392321
Iteration 66, loss = 0.02353754
Iteration 67, loss = 0.02400496
Iteration 68, loss = 0.02393141
Iteration 69, loss = 0.02366927
Iteration 70, loss = 0.02349025
Iteration 71, loss = 0.02368599
Iteration 72, loss = 0.02367690
Iteration 73, loss = 0.02356853
Iteration 74, loss = 0.02348152
Iteration 75, loss = 0.02340327
Iteration 76, loss = 0.02347401
Iteration 77, loss = 0.02331509
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67445161
Iteration 2, loss = 0.60493125
Iteration 3, loss = 0.49922063
Iteration 4, loss = 0.38940362
Iteration 5, loss = 0.29149273
Iteration 6, loss = 0.21865304
Iteration 7, loss = 0.16681018
Iteration 8, loss = 0.13188259
Iteration 9, loss = 0.10748897
Iteration 10, loss = 0.09032374
Iteration 11, loss = 0.07746035
Iteration 12, loss = 0.06833552
Iteration 13, loss = 0.06114615
Iteration 14, loss = 0.05560371
Iteration 15, loss = 0.05110309
Iteration 16, loss = 0.04767235
Iteration 17, loss = 0.04455093
Iteration 18, loss = 0.04204715
Iteration 19, loss = 0.03997549
Iteration 20, loss = 0.03845714
Iteration 21, loss = 0.03700746
Iteration 22, loss = 0.03560248
Iteration 23, loss = 0.03479581
Iteration 24, loss = 0.03340833
Iteration 25, loss = 0.03253842
Iteration 26, loss = 0.03182669
Iteration 27, loss = 0.03121694
Iteration 28, loss = 0.03079994
Iteration 29, loss = 0.03015695
Iteration 30, loss = 0.02976948
Iteration 31, loss = 0.02931994
Iteration 32, loss = 0.02898095
Iteration 33, loss = 0.02873362
Iteration 34, loss = 0.02823461
Iteration 35, loss = 0.02780820
Iteration 36, loss = 0.02751219
Iteration 37, loss = 0.02775765
Iteration 38, loss = 0.02735130
Iteration 39, loss = 0.02708643
Iteration 40, loss = 0.02699037
Iteration 41, loss = 0.02667709
Iteration 42, loss = 0.02648857
Iteration 43, loss = 0.02637070
Iteration 44, loss = 0.02610956
Iteration 45, loss = 0.02606354
Iteration 46, loss = 0.02575951
Iteration 47, loss = 0.02568791
Iteration 48, loss = 0.02570235
Iteration 49, loss = 0.02585590
Iteration 50, loss = 0.02607123
Iteration 51, loss = 0.02539777
Iteration 52, loss = 0.02546790
Iteration 53, loss = 0.02532471
Iteration 54, loss = 0.02546113
Iteration 55, loss = 0.02530626
Iteration 56, loss = 0.02506329
Iteration 57, loss = 0.02499907
Iteration 58, loss = 0.02509928
Iteration 59, loss = 0.02505561
Iteration 60, loss = 0.02494620
Iteration 61, loss = 0.02506289
Iteration 62, loss = 0.02480351
Iteration 63, loss = 0.02483093
Iteration 64, loss = 0.02465390
Iteration 65, loss = 0.02457489
Iteration 66, loss = 0.02482621
Iteration 67, loss = 0.02462514
Iteration 68, loss = 0.02469843
Iteration 69, loss = 0.02455381
Iteration 70, loss = 0.02441403
Iteration 71, loss = 0.02447787
Iteration 72, loss = 0.02458195
Iteration 73, loss = 0.02447503
Iteration 74, loss = 0.02460127
Iteration 75, loss = 0.02476505
Iteration 76, loss = 0.02470579
Iteration 77, loss = 0.02450978
Iteration 78, loss = 0.02462227
Iteration 79, loss = 0.02414786
Iteration 80, loss = 0.02407267
Iteration 81, loss = 0.02419734
Iteration 82, loss = 0.02444721
Iteration 83, loss = 0.02416833
Iteration 84, loss = 0.02422419
Iteration 85, loss = 0.02439097
Iteration 86, loss = 0.02399159
Iteration 87, loss = 0.02445058
Iteration 88, loss = 0.02444588
Iteration 89, loss = 0.02389238
Iteration 90, loss = 0.02411081
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67433577
Iteration 2, loss = 0.60298029
Iteration 3, loss = 0.49355616
Iteration 4, loss = 0.38064102
Iteration 5, loss = 0.28460211
Iteration 6, loss = 0.21253050
Iteration 7, loss = 0.16214350
Iteration 8, loss = 0.12748033
Iteration 9, loss = 0.10369341
Iteration 10, loss = 0.08663775
Iteration 11, loss = 0.07443956
Iteration 12, loss = 0.06494460
Iteration 13, loss = 0.05819374
Iteration 14, loss = 0.05238761
Iteration 15, loss = 0.04805312
Iteration 16, loss = 0.04448159
Iteration 17, loss = 0.04139905
Iteration 18, loss = 0.03927127
Iteration 19, loss = 0.03724346
Iteration 20, loss = 0.03544835
Iteration 21, loss = 0.03397083
Iteration 22, loss = 0.03269237
Iteration 23, loss = 0.03176336
Iteration 24, loss = 0.03064322
Iteration 25, loss = 0.03012935
Iteration 26, loss = 0.02899292
Iteration 27, loss = 0.02834070
Iteration 28, loss = 0.02799639
Iteration 29, loss = 0.02743428
Iteration 30, loss = 0.02689381
Iteration 31, loss = 0.02648250
Iteration 32, loss = 0.02609928
Iteration 33, loss = 0.02583997
Iteration 34, loss = 0.02556306
Iteration 35, loss = 0.02530974
Iteration 36, loss = 0.02520846
Iteration 37, loss = 0.02478691
Iteration 38, loss = 0.02424268
Iteration 39, loss = 0.02412967
Iteration 40, loss = 0.02404329
Iteration 41, loss = 0.02395094
Iteration 42, loss = 0.02374902
Iteration 43, loss = 0.02370853
Iteration 44, loss = 0.02341138
Iteration 45, loss = 0.02340009
Iteration 46, loss = 0.02339301
Iteration 47, loss = 0.02299180
Iteration 48, loss = 0.02307684
Iteration 49, loss = 0.02333569
Iteration 50, loss = 0.02297506
Iteration 51, loss = 0.02279112
Iteration 52, loss = 0.02286067
Iteration 53, loss = 0.02284448
Iteration 54, loss = 0.02239483
Iteration 55, loss = 0.02241702
Iteration 56, loss = 0.02229799
Iteration 57, loss = 0.02216536
Iteration 58, loss = 0.02223384
Iteration 59, loss = 0.02231482
Iteration 60, loss = 0.02204534
Iteration 61, loss = 0.02210796
Iteration 62, loss = 0.02202939
Iteration 63, loss = 0.02212335
Iteration 64, loss = 0.02210884
Iteration 65, loss = 0.02213807
Iteration 66, loss = 0.02198367
Iteration 67, loss = 0.02216888
Iteration 68, loss = 0.02160017
Iteration 69, loss = 0.02186999
Iteration 70, loss = 0.02184731
Iteration 71, loss = 0.02189620
Iteration 72, loss = 0.02179960
Iteration 73, loss = 0.02170375
Iteration 74, loss = 0.02203734
Iteration 75, loss = 0.02174289
Iteration 76, loss = 0.02157361
Iteration 77, loss = 0.02149782
Iteration 78, loss = 0.02156520
Iteration 79, loss = 0.02164980
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70207344
Iteration 2, loss = 0.63254840
Iteration 3, loss = 0.52612288
Iteration 4, loss = 0.40731278
Iteration 5, loss = 0.30501050
Iteration 6, loss = 0.22711655
Iteration 7, loss = 0.17166749
Iteration 8, loss = 0.13342349
Iteration 9, loss = 0.10757845
Iteration 10, loss = 0.08914369
Iteration 11, loss = 0.07609954
Iteration 12, loss = 0.06630869
Iteration 13, loss = 0.05919679
Iteration 14, loss = 0.05332136
Iteration 15, loss = 0.04861940
Iteration 16, loss = 0.04494310
Iteration 17, loss = 0.04178658
Iteration 18, loss = 0.03935998
Iteration 19, loss = 0.03739660
Iteration 20, loss = 0.03585110
Iteration 21, loss = 0.03434176
Iteration 22, loss = 0.03284213
Iteration 23, loss = 0.03183563
Iteration 24, loss = 0.03090489
Iteration 25, loss = 0.02995685
Iteration 26, loss = 0.02913438
Iteration 27, loss = 0.02859461
Iteration 28, loss = 0.02827344
Iteration 29, loss = 0.02745906
Iteration 30, loss = 0.02687906
Iteration 31, loss = 0.02646542
Iteration 32, loss = 0.02621806
Iteration 33, loss = 0.02572736
Iteration 34, loss = 0.02543333
Iteration 35, loss = 0.02511730
Iteration 36, loss = 0.02460330
Iteration 37, loss = 0.02448594
Iteration 38, loss = 0.02453022
Iteration 39, loss = 0.02412078
Iteration 40, loss = 0.02396960
Iteration 41, loss = 0.02381928
Iteration 42, loss = 0.02371262
Iteration 43, loss = 0.02362756
Iteration 44, loss = 0.02334587
Iteration 45, loss = 0.02334379
Iteration 46, loss = 0.02308312
Iteration 47, loss = 0.02292125
Iteration 48, loss = 0.02283724
Iteration 49, loss = 0.02280600
Iteration 50, loss = 0.02280553
Iteration 51, loss = 0.02264347
Iteration 52, loss = 0.02253529
Iteration 53, loss = 0.02227900
Iteration 54, loss = 0.02240815
Iteration 55, loss = 0.02233172
Iteration 56, loss = 0.02212443
Iteration 57, loss = 0.02234806
Iteration 58, loss = 0.02218888
Iteration 59, loss = 0.02227635
Iteration 60, loss = 0.02218103
Iteration 61, loss = 0.02212008
Iteration 62, loss = 0.02204220
Iteration 63, loss = 0.02184004
Iteration 64, loss = 0.02189733
Iteration 65, loss = 0.02185477
Iteration 66, loss = 0.02177294
Iteration 67, loss = 0.02194686
Iteration 68, loss = 0.02170407
Iteration 69, loss = 0.02141785
Iteration 70, loss = 0.02153847
Iteration 71, loss = 0.02174795
Iteration 72, loss = 0.02135847
Iteration 73, loss = 0.02159786
Iteration 74, loss = 0.02173922
Iteration 75, loss = 0.02140959
Iteration 76, loss = 0.02127176
Iteration 77, loss = 0.02153963
Iteration 78, loss = 0.02133559
Iteration 79, loss = 0.02156920
Iteration 80, loss = 0.02148804
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69653199
Iteration 2, loss = 0.61678980
Iteration 3, loss = 0.50821113
Iteration 4, loss = 0.39061074
Iteration 5, loss = 0.29001911
Iteration 6, loss = 0.21545608
Iteration 7, loss = 0.16374371
Iteration 8, loss = 0.12881356
Iteration 9, loss = 0.10440275
Iteration 10, loss = 0.08740504
Iteration 11, loss = 0.07497453
Iteration 12, loss = 0.06551845
Iteration 13, loss = 0.05821645
Iteration 14, loss = 0.05296140
Iteration 15, loss = 0.04833238
Iteration 16, loss = 0.04468970
Iteration 17, loss = 0.04176596
Iteration 18, loss = 0.03938627
Iteration 19, loss = 0.03733579
Iteration 20, loss = 0.03555370
Iteration 21, loss = 0.03424463
Iteration 22, loss = 0.03283239
Iteration 23, loss = 0.03175788
Iteration 24, loss = 0.03072850
Iteration 25, loss = 0.03011316
Iteration 26, loss = 0.02909235
Iteration 27, loss = 0.02889603
Iteration 28, loss = 0.02809036
Iteration 29, loss = 0.02761769
Iteration 30, loss = 0.02706980
Iteration 31, loss = 0.02677431
Iteration 32, loss = 0.02624118
Iteration 33, loss = 0.02567348
Iteration 34, loss = 0.02565439
Iteration 35, loss = 0.02532806
Iteration 36, loss = 0.02510531
Iteration 37, loss = 0.02475113
Iteration 38, loss = 0.02466768
Iteration 39, loss = 0.02423849
Iteration 40, loss = 0.02450801
Iteration 41, loss = 0.02415839
Iteration 42, loss = 0.02387914
Iteration 43, loss = 0.02379758
Iteration 44, loss = 0.02346351
Iteration 45, loss = 0.02341052
Iteration 46, loss = 0.02321928
Iteration 47, loss = 0.02332452
Iteration 48, loss = 0.02326560
Iteration 49, loss = 0.02301975
Iteration 50, loss = 0.02314209
Iteration 51, loss = 0.02277468
Iteration 52, loss = 0.02308939
Iteration 53, loss = 0.02270607
Iteration 54, loss = 0.02267760
Iteration 55, loss = 0.02249498
Iteration 56, loss = 0.02239424
Iteration 57, loss = 0.02261781
Iteration 58, loss = 0.02293814
Iteration 59, loss = 0.02251796
Iteration 60, loss = 0.02240895
Iteration 61, loss = 0.02223653
Iteration 62, loss = 0.02240685
Iteration 63, loss = 0.02215924
Iteration 64, loss = 0.02184773
Iteration 65, loss = 0.02214073
Iteration 66, loss = 0.02203720
Iteration 67, loss = 0.02211500
Iteration 68, loss = 0.02202613
Iteration 69, loss = 0.02226904
Iteration 70, loss = 0.02200427
Iteration 71, loss = 0.02194662
Iteration 72, loss = 0.02157353
Iteration 73, loss = 0.02202780
Iteration 74, loss = 0.02176222
Iteration 75, loss = 0.02179921
Iteration 76, loss = 0.02175396
Iteration 77, loss = 0.02169542
Iteration 78, loss = 0.02197595
Iteration 79, loss = 0.02152209
Iteration 80, loss = 0.02155500
Iteration 81, loss = 0.02168712
Iteration 82, loss = 0.02166692
Iteration 83, loss = 0.02155510
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70435501
Iteration 2, loss = 0.62728183
Iteration 3, loss = 0.51437906
Iteration 4, loss = 0.39247408
Iteration 5, loss = 0.29145794
Iteration 6, loss = 0.21695364
Iteration 7, loss = 0.16554978
Iteration 8, loss = 0.13067595
Iteration 9, loss = 0.10626822
Iteration 10, loss = 0.08914057
Iteration 11, loss = 0.07685323
Iteration 12, loss = 0.06756219
Iteration 13, loss = 0.06042668
Iteration 14, loss = 0.05483094
Iteration 15, loss = 0.05021856
Iteration 16, loss = 0.04677476
Iteration 17, loss = 0.04376692
Iteration 18, loss = 0.04148947
Iteration 19, loss = 0.03935313
Iteration 20, loss = 0.03754967
Iteration 21, loss = 0.03601185
Iteration 22, loss = 0.03476223
Iteration 23, loss = 0.03362978
Iteration 24, loss = 0.03281631
Iteration 25, loss = 0.03197180
Iteration 26, loss = 0.03120172
Iteration 27, loss = 0.03034537
Iteration 28, loss = 0.02993750
Iteration 29, loss = 0.02951960
Iteration 30, loss = 0.02895282
Iteration 31, loss = 0.02871308
Iteration 32, loss = 0.02782143
Iteration 33, loss = 0.02762572
Iteration 34, loss = 0.02769134
Iteration 35, loss = 0.02717225
Iteration 36, loss = 0.02686616
Iteration 37, loss = 0.02641456
Iteration 38, loss = 0.02621393
Iteration 39, loss = 0.02628956
Iteration 40, loss = 0.02631816
Iteration 41, loss = 0.02599454
Iteration 42, loss = 0.02577782
Iteration 43, loss = 0.02556370
Iteration 44, loss = 0.02537917
Iteration 45, loss = 0.02538006
Iteration 46, loss = 0.02502907
Iteration 47, loss = 0.02509117
Iteration 48, loss = 0.02510517
Iteration 49, loss = 0.02466405
Iteration 50, loss = 0.02475366
Iteration 51, loss = 0.02464314
Iteration 52, loss = 0.02467268
Iteration 53, loss = 0.02446797
Iteration 54, loss = 0.02427843
Iteration 55, loss = 0.02437354
Iteration 56, loss = 0.02432166
Iteration 57, loss = 0.02432493
Iteration 58, loss = 0.02397981
Iteration 59, loss = 0.02426657
Iteration 60, loss = 0.02412190
Iteration 61, loss = 0.02415037
Iteration 62, loss = 0.02397253
Iteration 63, loss = 0.02427533
Iteration 64, loss = 0.02373623
Iteration 65, loss = 0.02392321
Iteration 66, loss = 0.02353754
Iteration 67, loss = 0.02400496
Iteration 68, loss = 0.02393141
Iteration 69, loss = 0.02366927
Iteration 70, loss = 0.02349025
Iteration 71, loss = 0.02368599
Iteration 72, loss = 0.02367690
Iteration 73, loss = 0.02356853
Iteration 74, loss = 0.02348152
Iteration 75, loss = 0.02340327
Iteration 76, loss = 0.02347401
Iteration 77, loss = 0.02331509
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67445161
Iteration 2, loss = 0.60493125
Iteration 3, loss = 0.49922063
Iteration 4, loss = 0.38940362
Iteration 5, loss = 0.29149273
Iteration 6, loss = 0.21865304
Iteration 7, loss = 0.16681018
Iteration 8, loss = 0.13188259
Iteration 9, loss = 0.10748897
Iteration 10, loss = 0.09032374
Iteration 11, loss = 0.07746035
Iteration 12, loss = 0.06833552
Iteration 13, loss = 0.06114615
Iteration 14, loss = 0.05560371
Iteration 15, loss = 0.05110309
Iteration 16, loss = 0.04767235
Iteration 17, loss = 0.04455093
Iteration 18, loss = 0.04204715
Iteration 19, loss = 0.03997549
Iteration 20, loss = 0.03845714
Iteration 21, loss = 0.03700746
Iteration 22, loss = 0.03560248
Iteration 23, loss = 0.03479581
Iteration 24, loss = 0.03340833
Iteration 25, loss = 0.03253842
Iteration 26, loss = 0.03182669
Iteration 27, loss = 0.03121694
Iteration 28, loss = 0.03079994
Iteration 29, loss = 0.03015695
Iteration 30, loss = 0.02976948
Iteration 31, loss = 0.02931994
Iteration 32, loss = 0.02898095
Iteration 33, loss = 0.02873362
Iteration 34, loss = 0.02823461
Iteration 35, loss = 0.02780820
Iteration 36, loss = 0.02751219
Iteration 37, loss = 0.02775765
Iteration 38, loss = 0.02735130
Iteration 39, loss = 0.02708643
Iteration 40, loss = 0.02699037
Iteration 41, loss = 0.02667709
Iteration 42, loss = 0.02648857
Iteration 43, loss = 0.02637070
Iteration 44, loss = 0.02610956
Iteration 45, loss = 0.02606354
Iteration 46, loss = 0.02575951
Iteration 47, loss = 0.02568791
Iteration 48, loss = 0.02570235
Iteration 49, loss = 0.02585590
Iteration 50, loss = 0.02607123
Iteration 51, loss = 0.02539777
Iteration 52, loss = 0.02546790
Iteration 53, loss = 0.02532471
Iteration 54, loss = 0.02546113
Iteration 55, loss = 0.02530626
Iteration 56, loss = 0.02506329
Iteration 57, loss = 0.02499907
Iteration 58, loss = 0.02509928
Iteration 59, loss = 0.02505561
Iteration 60, loss = 0.02494620
Iteration 61, loss = 0.02506289
Iteration 62, loss = 0.02480351
Iteration 63, loss = 0.02483093
Iteration 64, loss = 0.02465390
Iteration 65, loss = 0.02457489
Iteration 66, loss = 0.02482621
Iteration 67, loss = 0.02462514
Iteration 68, loss = 0.02469843
Iteration 69, loss = 0.02455381
Iteration 70, loss = 0.02441403
Iteration 71, loss = 0.02447787
Iteration 72, loss = 0.02458195
Iteration 73, loss = 0.02447503
Iteration 74, loss = 0.02460127
Iteration 75, loss = 0.02476505
Iteration 76, loss = 0.02470579
Iteration 77, loss = 0.02450978
Iteration 78, loss = 0.02462227
Iteration 79, loss = 0.02414786
Iteration 80, loss = 0.02407267
Iteration 81, loss = 0.02419734
Iteration 82, loss = 0.02444721
Iteration 83, loss = 0.02416833
Iteration 84, loss = 0.02422419
Iteration 85, loss = 0.02439097
Iteration 86, loss = 0.02399159
Iteration 87, loss = 0.02445058
Iteration 88, loss = 0.02444588
Iteration 89, loss = 0.02389238
Iteration 90, loss = 0.02411081
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67433577
Iteration 2, loss = 0.60298029
Iteration 3, loss = 0.49355616
Iteration 4, loss = 0.38064102
Iteration 5, loss = 0.28460211
Iteration 6, loss = 0.21253050
Iteration 7, loss = 0.16214350
Iteration 8, loss = 0.12748033
Iteration 9, loss = 0.10369341
Iteration 10, loss = 0.08663775
Iteration 11, loss = 0.07443956
Iteration 12, loss = 0.06494460
Iteration 13, loss = 0.05819374
Iteration 14, loss = 0.05238761
Iteration 15, loss = 0.04805312
Iteration 16, loss = 0.04448159
Iteration 17, loss = 0.04139905
Iteration 18, loss = 0.03927127
Iteration 19, loss = 0.03724346
Iteration 20, loss = 0.03544835
Iteration 21, loss = 0.03397083
Iteration 22, loss = 0.03269237
Iteration 23, loss = 0.03176336
Iteration 24, loss = 0.03064322
Iteration 25, loss = 0.03012935
Iteration 26, loss = 0.02899292
Iteration 27, loss = 0.02834070
Iteration 28, loss = 0.02799639
Iteration 29, loss = 0.02743428
Iteration 30, loss = 0.02689381
Iteration 31, loss = 0.02648250
Iteration 32, loss = 0.02609928
Iteration 33, loss = 0.02583997
Iteration 34, loss = 0.02556306
Iteration 35, loss = 0.02530974
Iteration 36, loss = 0.02520846
Iteration 37, loss = 0.02478691
Iteration 38, loss = 0.02424268
Iteration 39, loss = 0.02412967
Iteration 40, loss = 0.02404329
Iteration 41, loss = 0.02395094
Iteration 42, loss = 0.02374902
Iteration 43, loss = 0.02370853
Iteration 44, loss = 0.02341138
Iteration 45, loss = 0.02340009
Iteration 46, loss = 0.02339301
Iteration 47, loss = 0.02299180
Iteration 48, loss = 0.02307684
Iteration 49, loss = 0.02333569
Iteration 50, loss = 0.02297506
Iteration 51, loss = 0.02279112
Iteration 52, loss = 0.02286067
Iteration 53, loss = 0.02284448
Iteration 54, loss = 0.02239483
Iteration 55, loss = 0.02241702
Iteration 56, loss = 0.02229799
Iteration 57, loss = 0.02216536
Iteration 58, loss = 0.02223384
Iteration 59, loss = 0.02231482
Iteration 60, loss = 0.02204534
Iteration 61, loss = 0.02210796
Iteration 62, loss = 0.02202939
Iteration 63, loss = 0.02212335
Iteration 64, loss = 0.02210884
Iteration 65, loss = 0.02213807
Iteration 66, loss = 0.02198367
Iteration 67, loss = 0.02216888
Iteration 68, loss = 0.02160017
Iteration 69, loss = 0.02186999
Iteration 70, loss = 0.02184731
Iteration 71, loss = 0.02189620
Iteration 72, loss = 0.02179960
Iteration 73, loss = 0.02170375
Iteration 74, loss = 0.02203734
Iteration 75, loss = 0.02174289
Iteration 76, loss = 0.02157361
Iteration 77, loss = 0.02149782
Iteration 78, loss = 0.02156520
Iteration 79, loss = 0.02164980
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70207344
Iteration 2, loss = 0.63254840
Iteration 3, loss = 0.52612288
Iteration 4, loss = 0.40731278
Iteration 5, loss = 0.30501050
Iteration 6, loss = 0.22711655
Iteration 7, loss = 0.17166749
Iteration 8, loss = 0.13342349
Iteration 9, loss = 0.10757845
Iteration 10, loss = 0.08914369
Iteration 11, loss = 0.07609954
Iteration 12, loss = 0.06630869
Iteration 13, loss = 0.05919679
Iteration 14, loss = 0.05332136
Iteration 15, loss = 0.04861940
Iteration 16, loss = 0.04494310
Iteration 17, loss = 0.04178658
Iteration 18, loss = 0.03935998
Iteration 19, loss = 0.03739660
Iteration 20, loss = 0.03585110
Iteration 21, loss = 0.03434176
Iteration 22, loss = 0.03284213
Iteration 23, loss = 0.03183563
Iteration 24, loss = 0.03090489
Iteration 25, loss = 0.02995685
Iteration 26, loss = 0.02913438
Iteration 27, loss = 0.02859461
Iteration 28, loss = 0.02827344
Iteration 29, loss = 0.02745906
Iteration 30, loss = 0.02687906
Iteration 31, loss = 0.02646542
Iteration 32, loss = 0.02621806
Iteration 33, loss = 0.02572736
Iteration 34, loss = 0.02543333
Iteration 35, loss = 0.02511730
Iteration 36, loss = 0.02460330
Iteration 37, loss = 0.02448594
Iteration 38, loss = 0.02453022
Iteration 39, loss = 0.02412078
Iteration 40, loss = 0.02396960
Iteration 41, loss = 0.02381928
Iteration 42, loss = 0.02371262
Iteration 43, loss = 0.02362756
Iteration 44, loss = 0.02334587
Iteration 45, loss = 0.02334379
Iteration 46, loss = 0.02308312
Iteration 47, loss = 0.02292125
Iteration 48, loss = 0.02283724
Iteration 49, loss = 0.02280600
Iteration 50, loss = 0.02280553
Iteration 51, loss = 0.02264347
Iteration 52, loss = 0.02253529
Iteration 53, loss = 0.02227900
Iteration 54, loss = 0.02240815
Iteration 55, loss = 0.02233172
Iteration 56, loss = 0.02212443
Iteration 57, loss = 0.02234806
Iteration 58, loss = 0.02218888
Iteration 59, loss = 0.02227635
Iteration 60, loss = 0.02218103
Iteration 61, loss = 0.02212008
Iteration 62, loss = 0.02204220
Iteration 63, loss = 0.02184004
Iteration 64, loss = 0.02189733
Iteration 65, loss = 0.02185477
Iteration 66, loss = 0.02177294
Iteration 67, loss = 0.02194686
Iteration 68, loss = 0.02170407
Iteration 69, loss = 0.02141785
Iteration 70, loss = 0.02153847
Iteration 71, loss = 0.02174795
Iteration 72, loss = 0.02135847
Iteration 73, loss = 0.02159786
Iteration 74, loss = 0.02173922
Iteration 75, loss = 0.02140959
Iteration 76, loss = 0.02127176
Iteration 77, loss = 0.02153963
Iteration 78, loss = 0.02133559
Iteration 79, loss = 0.02156920
Iteration 80, loss = 0.02148804
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69653199
Iteration 2, loss = 0.61678980
Iteration 3, loss = 0.50821113
Iteration 4, loss = 0.39061074
Iteration 5, loss = 0.29001911
Iteration 6, loss = 0.21545608
Iteration 7, loss = 0.16374371
Iteration 8, loss = 0.12881356
Iteration 9, loss = 0.10440275
Iteration 10, loss = 0.08740504
Iteration 11, loss = 0.07497453
Iteration 12, loss = 0.06551845
Iteration 13, loss = 0.05821645
Iteration 14, loss = 0.05296140
Iteration 15, loss = 0.04833238
Iteration 16, loss = 0.04468970
Iteration 17, loss = 0.04176596
Iteration 18, loss = 0.03938627
Iteration 19, loss = 0.03733579
Iteration 20, loss = 0.03555370
Iteration 21, loss = 0.03424463
Iteration 22, loss = 0.03283239
Iteration 23, loss = 0.03175788
Iteration 24, loss = 0.03072850
Iteration 25, loss = 0.03011316
Iteration 26, loss = 0.02909235
Iteration 27, loss = 0.02889603
Iteration 28, loss = 0.02809036
Iteration 29, loss = 0.02761769
Iteration 30, loss = 0.02706980
Iteration 31, loss = 0.02677431
Iteration 32, loss = 0.02624118
Iteration 33, loss = 0.02567348
Iteration 34, loss = 0.02565439
Iteration 35, loss = 0.02532806
Iteration 36, loss = 0.02510531
Iteration 37, loss = 0.02475113
Iteration 38, loss = 0.02466768
Iteration 39, loss = 0.02423849
Iteration 40, loss = 0.02450801
Iteration 41, loss = 0.02415839
Iteration 42, loss = 0.02387914
Iteration 43, loss = 0.02379758
Iteration 44, loss = 0.02346351
Iteration 45, loss = 0.02341052
Iteration 46, loss = 0.02321928
Iteration 47, loss = 0.02332452
Iteration 48, loss = 0.02326560
Iteration 49, loss = 0.02301975
Iteration 50, loss = 0.02314209
Iteration 51, loss = 0.02277468
Iteration 52, loss = 0.02308939
Iteration 53, loss = 0.02270607
Iteration 54, loss = 0.02267760
Iteration 55, loss = 0.02249498
Iteration 56, loss = 0.02239424
Iteration 57, loss = 0.02261781
Iteration 58, loss = 0.02293814
Iteration 59, loss = 0.02251796
Iteration 60, loss = 0.02240895
Iteration 61, loss = 0.02223653
Iteration 62, loss = 0.02240685
Iteration 63, loss = 0.02215924
Iteration 64, loss = 0.02184773
Iteration 65, loss = 0.02214073
Iteration 66, loss = 0.02203720
Iteration 67, loss = 0.02211500
Iteration 68, loss = 0.02202613
Iteration 69, loss = 0.02226904
Iteration 70, loss = 0.02200427
Iteration 71, loss = 0.02194662
Iteration 72, loss = 0.02157353
Iteration 73, loss = 0.02202780
Iteration 74, loss = 0.02176222
Iteration 75, loss = 0.02179921
Iteration 76, loss = 0.02175396
Iteration 77, loss = 0.02169542
Iteration 78, loss = 0.02197595
Iteration 79, loss = 0.02152209
Iteration 80, loss = 0.02155500
Iteration 81, loss = 0.02168712
Iteration 82, loss = 0.02166692
Iteration 83, loss = 0.02155510
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70435501
Iteration 2, loss = 0.62728183
Iteration 3, loss = 0.51437906
Iteration 4, loss = 0.39247408
Iteration 5, loss = 0.29145794
Iteration 6, loss = 0.21695364
Iteration 7, loss = 0.16554978
Iteration 8, loss = 0.13067595
Iteration 9, loss = 0.10626822
Iteration 10, loss = 0.08914057
Iteration 11, loss = 0.07685323
Iteration 12, loss = 0.06756219
Iteration 13, loss = 0.06042668
Iteration 14, loss = 0.05483094
Iteration 15, loss = 0.05021856
Iteration 16, loss = 0.04677476
Iteration 17, loss = 0.04376692
Iteration 18, loss = 0.04148947
Iteration 19, loss = 0.03935313
Iteration 20, loss = 0.03754967
Iteration 21, loss = 0.03601185
Iteration 22, loss = 0.03476223
Iteration 23, loss = 0.03362978
Iteration 24, loss = 0.03281631
Iteration 25, loss = 0.03197180
Iteration 26, loss = 0.03120172
Iteration 27, loss = 0.03034537
Iteration 28, loss = 0.02993750
Iteration 29, loss = 0.02951960
Iteration 30, loss = 0.02895282
Iteration 31, loss = 0.02871308
Iteration 32, loss = 0.02782143
Iteration 33, loss = 0.02762572
Iteration 34, loss = 0.02769134
Iteration 35, loss = 0.02717225
Iteration 36, loss = 0.02686616
Iteration 37, loss = 0.02641456
Iteration 38, loss = 0.02621393
Iteration 39, loss = 0.02628956
Iteration 40, loss = 0.02631816
Iteration 41, loss = 0.02599454
Iteration 42, loss = 0.02577782
Iteration 43, loss = 0.02556370
Iteration 44, loss = 0.02537917
Iteration 45, loss = 0.02538006
Iteration 46, loss = 0.02502907
Iteration 47, loss = 0.02509117
Iteration 48, loss = 0.02510517
Iteration 49, loss = 0.02466405
Iteration 50, loss = 0.02475366
Iteration 51, loss = 0.02464314
Iteration 52, loss = 0.02467268
Iteration 53, loss = 0.02446797
Iteration 54, loss = 0.02427843
Iteration 55, loss = 0.02437354
Iteration 56, loss = 0.02432166
Iteration 57, loss = 0.02432493
Iteration 58, loss = 0.02397981
Iteration 59, loss = 0.02426657
Iteration 60, loss = 0.02412190
Iteration 61, loss = 0.02415037
Iteration 62, loss = 0.02397253
Iteration 63, loss = 0.02427533
Iteration 64, loss = 0.02373623
Iteration 65, loss = 0.02392321
Iteration 66, loss = 0.02353754
Iteration 67, loss = 0.02400496
Iteration 68, loss = 0.02393141
Iteration 69, loss = 0.02366927
Iteration 70, loss = 0.02349025
Iteration 71, loss = 0.02368599
Iteration 72, loss = 0.02367690
Iteration 73, loss = 0.02356853
Iteration 74, loss = 0.02348152
Iteration 75, loss = 0.02340327
Iteration 76, loss = 0.02347401
Iteration 77, loss = 0.02331509
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67445161
Iteration 2, loss = 0.60493125
Iteration 3, loss = 0.49922063
Iteration 4, loss = 0.38940362
Iteration 5, loss = 0.29149273
Iteration 6, loss = 0.21865304
Iteration 7, loss = 0.16681018
Iteration 8, loss = 0.13188259
Iteration 9, loss = 0.10748897
Iteration 10, loss = 0.09032374
Iteration 11, loss = 0.07746035
Iteration 12, loss = 0.06833552
Iteration 13, loss = 0.06114615
Iteration 14, loss = 0.05560371
Iteration 15, loss = 0.05110309
Iteration 16, loss = 0.04767235
Iteration 17, loss = 0.04455093
Iteration 18, loss = 0.04204715
Iteration 19, loss = 0.03997549
Iteration 20, loss = 0.03845714
Iteration 21, loss = 0.03700746
Iteration 22, loss = 0.03560248
Iteration 23, loss = 0.03479581
Iteration 24, loss = 0.03340833
Iteration 25, loss = 0.03253842
Iteration 26, loss = 0.03182669
Iteration 27, loss = 0.03121694
Iteration 28, loss = 0.03079994
Iteration 29, loss = 0.03015695
Iteration 30, loss = 0.02976948
Iteration 31, loss = 0.02931994
Iteration 32, loss = 0.02898095
Iteration 33, loss = 0.02873362
Iteration 34, loss = 0.02823461
Iteration 35, loss = 0.02780820
Iteration 36, loss = 0.02751219
Iteration 37, loss = 0.02775765
Iteration 38, loss = 0.02735130
Iteration 39, loss = 0.02708643
Iteration 40, loss = 0.02699037
Iteration 41, loss = 0.02667709
Iteration 42, loss = 0.02648857
Iteration 43, loss = 0.02637070
Iteration 44, loss = 0.02610956
Iteration 45, loss = 0.02606354
Iteration 46, loss = 0.02575951
Iteration 47, loss = 0.02568791
Iteration 48, loss = 0.02570235
Iteration 49, loss = 0.02585590
Iteration 50, loss = 0.02607123
Iteration 51, loss = 0.02539777
Iteration 52, loss = 0.02546790
Iteration 53, loss = 0.02532471
Iteration 54, loss = 0.02546113
Iteration 55, loss = 0.02530626
Iteration 56, loss = 0.02506329
Iteration 57, loss = 0.02499907
Iteration 58, loss = 0.02509928
Iteration 59, loss = 0.02505561
Iteration 60, loss = 0.02494620
Iteration 61, loss = 0.02506289
Iteration 62, loss = 0.02480351
Iteration 63, loss = 0.02483093
Iteration 64, loss = 0.02465390
Iteration 65, loss = 0.02457489
Iteration 66, loss = 0.02482621
Iteration 67, loss = 0.02462514
Iteration 68, loss = 0.02469843
Iteration 69, loss = 0.02455381
Iteration 70, loss = 0.02441403
Iteration 71, loss = 0.02447787
Iteration 72, loss = 0.02458195
Iteration 73, loss = 0.02447503
Iteration 74, loss = 0.02460127
Iteration 75, loss = 0.02476505
Iteration 76, loss = 0.02470579
Iteration 77, loss = 0.02450978
Iteration 78, loss = 0.02462227
Iteration 79, loss = 0.02414786
Iteration 80, loss = 0.02407267
Iteration 81, loss = 0.02419734
Iteration 82, loss = 0.02444721
Iteration 83, loss = 0.02416833
Iteration 84, loss = 0.02422419
Iteration 85, loss = 0.02439097
Iteration 86, loss = 0.02399159
Iteration 87, loss = 0.02445058
Iteration 88, loss = 0.02444588
Iteration 89, loss = 0.02389238
Iteration 90, loss = 0.02411081
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67433577
Iteration 2, loss = 0.60298029
Iteration 3, loss = 0.49355616
Iteration 4, loss = 0.38064102
Iteration 5, loss = 0.28460211
Iteration 6, loss = 0.21253050
Iteration 7, loss = 0.16214350
Iteration 8, loss = 0.12748033
Iteration 9, loss = 0.10369341
Iteration 10, loss = 0.08663775
Iteration 11, loss = 0.07443956
Iteration 12, loss = 0.06494460
Iteration 13, loss = 0.05819374
Iteration 14, loss = 0.05238761
Iteration 15, loss = 0.04805312
Iteration 16, loss = 0.04448159
Iteration 17, loss = 0.04139905
Iteration 18, loss = 0.03927127
Iteration 19, loss = 0.03724346
Iteration 20, loss = 0.03544835
Iteration 21, loss = 0.03397083
Iteration 22, loss = 0.03269237
Iteration 23, loss = 0.03176336
Iteration 24, loss = 0.03064322
Iteration 25, loss = 0.03012935
Iteration 26, loss = 0.02899292
Iteration 27, loss = 0.02834070
Iteration 28, loss = 0.02799639
Iteration 29, loss = 0.02743428
Iteration 30, loss = 0.02689381
Iteration 31, loss = 0.02648250
Iteration 32, loss = 0.02609928
Iteration 33, loss = 0.02583997
Iteration 34, loss = 0.02556306
Iteration 35, loss = 0.02530974
Iteration 36, loss = 0.02520846
Iteration 37, loss = 0.02478691
Iteration 38, loss = 0.02424268
Iteration 39, loss = 0.02412967
Iteration 40, loss = 0.02404329
Iteration 41, loss = 0.02395094
Iteration 42, loss = 0.02374902
Iteration 43, loss = 0.02370853
Iteration 44, loss = 0.02341138
Iteration 45, loss = 0.02340009
Iteration 46, loss = 0.02339301
Iteration 47, loss = 0.02299180
Iteration 48, loss = 0.02307684
Iteration 49, loss = 0.02333569
Iteration 50, loss = 0.02297506
Iteration 51, loss = 0.02279112
Iteration 52, loss = 0.02286067
Iteration 53, loss = 0.02284448
Iteration 54, loss = 0.02239483
Iteration 55, loss = 0.02241702
Iteration 56, loss = 0.02229799
Iteration 57, loss = 0.02216536
Iteration 58, loss = 0.02223384
Iteration 59, loss = 0.02231482
Iteration 60, loss = 0.02204534
Iteration 61, loss = 0.02210796
Iteration 62, loss = 0.02202939
Iteration 63, loss = 0.02212335
Iteration 64, loss = 0.02210884
Iteration 65, loss = 0.02213807
Iteration 66, loss = 0.02198367
Iteration 67, loss = 0.02216888
Iteration 68, loss = 0.02160017
Iteration 69, loss = 0.02186999
Iteration 70, loss = 0.02184731
Iteration 71, loss = 0.02189620
Iteration 72, loss = 0.02179960
Iteration 73, loss = 0.02170375
Iteration 74, loss = 0.02203734
Iteration 75, loss = 0.02174289
Iteration 76, loss = 0.02157361
Iteration 77, loss = 0.02149782
Iteration 78, loss = 0.02156520
Iteration 79, loss = 0.02164980
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70207344
Iteration 2, loss = 0.63254840
Iteration 3, loss = 0.52612288
Iteration 4, loss = 0.40731278
Iteration 5, loss = 0.30501050
Iteration 6, loss = 0.22711655
Iteration 7, loss = 0.17166749
Iteration 8, loss = 0.13342349
Iteration 9, loss = 0.10757845
Iteration 10, loss = 0.08914369
Iteration 11, loss = 0.07609954
Iteration 12, loss = 0.06630869
Iteration 13, loss = 0.05919679
Iteration 14, loss = 0.05332136
Iteration 15, loss = 0.04861940
Iteration 16, loss = 0.04494310
Iteration 17, loss = 0.04178658
Iteration 18, loss = 0.03935998
Iteration 19, loss = 0.03739660
Iteration 20, loss = 0.03585110
Iteration 21, loss = 0.03434176
Iteration 22, loss = 0.03284213
Iteration 23, loss = 0.03183563
Iteration 24, loss = 0.03090489
Iteration 25, loss = 0.02995685
Iteration 26, loss = 0.02913438
Iteration 27, loss = 0.02859461
Iteration 28, loss = 0.02827344
Iteration 29, loss = 0.02745906
Iteration 30, loss = 0.02687906
Iteration 31, loss = 0.02646542
Iteration 32, loss = 0.02621806
Iteration 33, loss = 0.02572736
Iteration 34, loss = 0.02543333
Iteration 35, loss = 0.02511730
Iteration 36, loss = 0.02460330
Iteration 37, loss = 0.02448594
Iteration 38, loss = 0.02453022
Iteration 39, loss = 0.02412078
Iteration 40, loss = 0.02396960
Iteration 41, loss = 0.02381928
Iteration 42, loss = 0.02371262
Iteration 43, loss = 0.02362756
Iteration 44, loss = 0.02334587
Iteration 45, loss = 0.02334379
Iteration 46, loss = 0.02308312
Iteration 47, loss = 0.02292125
Iteration 48, loss = 0.02283724
Iteration 49, loss = 0.02280600
Iteration 50, loss = 0.02280553
Iteration 51, loss = 0.02264347
Iteration 52, loss = 0.02253529
Iteration 53, loss = 0.02227900
Iteration 54, loss = 0.02240815
Iteration 55, loss = 0.02233172
Iteration 56, loss = 0.02212443
Iteration 57, loss = 0.02234806
Iteration 58, loss = 0.02218888
Iteration 59, loss = 0.02227635
Iteration 60, loss = 0.02218103
Iteration 61, loss = 0.02212008
Iteration 62, loss = 0.02204220
Iteration 63, loss = 0.02184004
Iteration 64, loss = 0.02189733
Iteration 65, loss = 0.02185477
Iteration 66, loss = 0.02177294
Iteration 67, loss = 0.02194686
Iteration 68, loss = 0.02170407
Iteration 69, loss = 0.02141785
Iteration 70, loss = 0.02153847
Iteration 71, loss = 0.02174795
Iteration 72, loss = 0.02135847
Iteration 73, loss = 0.02159786
Iteration 74, loss = 0.02173922
Iteration 75, loss = 0.02140959
Iteration 76, loss = 0.02127176
Iteration 77, loss = 0.02153963
Iteration 78, loss = 0.02133559
Iteration 79, loss = 0.02156920
Iteration 80, loss = 0.02148804
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69653199
Iteration 2, loss = 0.61678980
Iteration 3, loss = 0.50821113
Iteration 4, loss = 0.39061074
Iteration 5, loss = 0.29001911
Iteration 6, loss = 0.21545608
Iteration 7, loss = 0.16374371
Iteration 8, loss = 0.12881356
Iteration 9, loss = 0.10440275
Iteration 10, loss = 0.08740504
Iteration 11, loss = 0.07497453
Iteration 12, loss = 0.06551845
Iteration 13, loss = 0.05821645
Iteration 14, loss = 0.05296140
Iteration 15, loss = 0.04833238
Iteration 16, loss = 0.04468970
Iteration 17, loss = 0.04176596
Iteration 18, loss = 0.03938627
Iteration 19, loss = 0.03733579
Iteration 20, loss = 0.03555370
Iteration 21, loss = 0.03424463
Iteration 22, loss = 0.03283239
Iteration 23, loss = 0.03175788
Iteration 24, loss = 0.03072850
Iteration 25, loss = 0.03011316
Iteration 26, loss = 0.02909235
Iteration 27, loss = 0.02889603
Iteration 28, loss = 0.02809036
Iteration 29, loss = 0.02761769
Iteration 30, loss = 0.02706980
Iteration 31, loss = 0.02677431
Iteration 32, loss = 0.02624118
Iteration 33, loss = 0.02567348
Iteration 34, loss = 0.02565439
Iteration 35, loss = 0.02532806
Iteration 36, loss = 0.02510531
Iteration 37, loss = 0.02475113
Iteration 38, loss = 0.02466768
Iteration 39, loss = 0.02423849
Iteration 40, loss = 0.02450801
Iteration 41, loss = 0.02415839
Iteration 42, loss = 0.02387914
Iteration 43, loss = 0.02379758
Iteration 44, loss = 0.02346351
Iteration 45, loss = 0.02341052
Iteration 46, loss = 0.02321928
Iteration 47, loss = 0.02332452
Iteration 48, loss = 0.02326560
Iteration 49, loss = 0.02301975
Iteration 50, loss = 0.02314209
Iteration 51, loss = 0.02277468
Iteration 52, loss = 0.02308939
Iteration 53, loss = 0.02270607
Iteration 54, loss = 0.02267760
Iteration 55, loss = 0.02249498
Iteration 56, loss = 0.02239424
Iteration 57, loss = 0.02261781
Iteration 58, loss = 0.02293814
Iteration 59, loss = 0.02251796
Iteration 60, loss = 0.02240895
Iteration 61, loss = 0.02223653
Iteration 62, loss = 0.02240685
Iteration 63, loss = 0.02215924
Iteration 64, loss = 0.02184773
Iteration 65, loss = 0.02214073
Iteration 66, loss = 0.02203720
Iteration 67, loss = 0.02211500
Iteration 68, loss = 0.02202613
Iteration 69, loss = 0.02226904
Iteration 70, loss = 0.02200427
Iteration 71, loss = 0.02194662
Iteration 72, loss = 0.02157353
Iteration 73, loss = 0.02202780
Iteration 74, loss = 0.02176222
Iteration 75, loss = 0.02179921
Iteration 76, loss = 0.02175396
Iteration 77, loss = 0.02169542
Iteration 78, loss = 0.02197595
Iteration 79, loss = 0.02152209
Iteration 80, loss = 0.02155500
Iteration 81, loss = 0.02168712
Iteration 82, loss = 0.02166692
Iteration 83, loss = 0.02155510
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68129161
Iteration 2, loss = 0.59497559
Iteration 3, loss = 0.46163984
Iteration 4, loss = 0.33429198
Iteration 5, loss = 0.23831622
Iteration 6, loss = 0.17388591
Iteration 7, loss = 0.13255878
Iteration 8, loss = 0.10519771
Iteration 9, loss = 0.08673134
Iteration 10, loss = 0.07402539
Iteration 11, loss = 0.06482386
Iteration 12, loss = 0.05801984
Iteration 13, loss = 0.05262534
Iteration 14, loss = 0.04852159
Iteration 15, loss = 0.04529799
Iteration 16, loss = 0.04265413
Iteration 17, loss = 0.04044436
Iteration 18, loss = 0.03866021
Iteration 19, loss = 0.03688340
Iteration 20, loss = 0.03560177
Iteration 21, loss = 0.03462963
Iteration 22, loss = 0.03364977
Iteration 23, loss = 0.03282606
Iteration 24, loss = 0.03212310
Iteration 25, loss = 0.03135768
Iteration 26, loss = 0.03088464
Iteration 27, loss = 0.03037899
Iteration 28, loss = 0.03059859
Iteration 29, loss = 0.02978666
Iteration 30, loss = 0.02936986
Iteration 31, loss = 0.02892476
Iteration 32, loss = 0.02857154
Iteration 33, loss = 0.02830382
Iteration 34, loss = 0.02816355
Iteration 35, loss = 0.02803528
Iteration 36, loss = 0.02791802
Iteration 37, loss = 0.02751797
Iteration 38, loss = 0.02772962
Iteration 39, loss = 0.02752521
Iteration 40, loss = 0.02724983
Iteration 41, loss = 0.02724990
Iteration 42, loss = 0.02685676
Iteration 43, loss = 0.02706544
Iteration 44, loss = 0.02666907
Iteration 45, loss = 0.02672538
Iteration 46, loss = 0.02662341
Iteration 47, loss = 0.02653374
Iteration 48, loss = 0.02632564
Iteration 49, loss = 0.02619078
Iteration 50, loss = 0.02632421
Iteration 51, loss = 0.02594213
Iteration 52, loss = 0.02626737
Iteration 53, loss = 0.02631235
Iteration 54, loss = 0.02617337
Iteration 55, loss = 0.02586228
Iteration 56, loss = 0.02578077
Iteration 57, loss = 0.02582609
Iteration 58, loss = 0.02582077
Iteration 59, loss = 0.02565536
Iteration 60, loss = 0.02587778
Iteration 61, loss = 0.02567682
Iteration 62, loss = 0.02592120
Iteration 63, loss = 0.02553627
Iteration 64, loss = 0.02584753
Iteration 65, loss = 0.02553779
Iteration 66, loss = 0.02538572
Iteration 67, loss = 0.02549664
Iteration 68, loss = 0.02551987
Iteration 69, loss = 0.02538778
Iteration 70, loss = 0.02563826
Iteration 71, loss = 0.02519935
Iteration 72, loss = 0.02506876
Iteration 73, loss = 0.02531955
Iteration 74, loss = 0.02546384
Iteration 75, loss = 0.02567662
Iteration 76, loss = 0.02551548
Iteration 77, loss = 0.02527331
Iteration 78, loss = 0.02521041
Iteration 79, loss = 0.02546804
Iteration 80, loss = 0.02499608
Iteration 81, loss = 0.02505877
Iteration 82, loss = 0.02503551
Iteration 83, loss = 0.02489449
Iteration 84, loss = 0.02539135
Iteration 85, loss = 0.02506167
Iteration 86, loss = 0.02531775
Iteration 87, loss = 0.02520392
Iteration 88, loss = 0.02497448
Iteration 89, loss = 0.02471339
Iteration 90, loss = 0.02549972
Iteration 91, loss = 0.02472127
Iteration 92, loss = 0.02480065
Iteration 93, loss = 0.02529563
Iteration 94, loss = 0.02539539
Iteration 95, loss = 0.02489031
Iteration 96, loss = 0.02508566
Iteration 97, loss = 0.02508196
Iteration 98, loss = 0.02527138
Iteration 99, loss = 0.02493422
Iteration 100, loss = 0.02455598
Iteration 101, loss = 0.02514099
Iteration 102, loss = 0.02467526
Iteration 103, loss = 0.02496077
Iteration 104, loss = 0.02479933
Iteration 105, loss = 0.02480336
Iteration 106, loss = 0.02491539
Iteration 107, loss = 0.02475613
Iteration 108, loss = 0.02503946
Iteration 109, loss = 0.02515057
Iteration 110, loss = 0.02465812
Iteration 111, loss = 0.02495953
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on prepended_v6_custom_stopwords...
Iteration 1, loss = 0.70198966
Iteration 2, loss = 0.65703584
Iteration 3, loss = 0.59358155
Iteration 4, loss = 0.51203698
Iteration 5, loss = 0.42689291
Iteration 6, loss = 0.34763544
Iteration 7, loss = 0.27977458
Iteration 8, loss = 0.22558244
Iteration 9, loss = 0.18350646
Iteration 10, loss = 0.15145431
Iteration 11, loss = 0.12688097
Iteration 12, loss = 0.10834957
Iteration 13, loss = 0.09380106
Iteration 14, loss = 0.08239805
Iteration 15, loss = 0.07333186
Iteration 16, loss = 0.06607210
Iteration 17, loss = 0.06009178
Iteration 18, loss = 0.05497916
Iteration 19, loss = 0.05078485
Iteration 20, loss = 0.04712132
Iteration 21, loss = 0.04432269
Iteration 22, loss = 0.04153559
Iteration 23, loss = 0.03933400
Iteration 24, loss = 0.03729589
Iteration 25, loss = 0.03550392
Iteration 26, loss = 0.03404554
Iteration 27, loss = 0.03274432
Iteration 28, loss = 0.03145682
Iteration 29, loss = 0.03052862
Iteration 30, loss = 0.02928431
Iteration 31, loss = 0.02867282
Iteration 32, loss = 0.02768506
Iteration 33, loss = 0.02711749
Iteration 34, loss = 0.02666304
Iteration 35, loss = 0.02606533
Iteration 36, loss = 0.02563155
Iteration 37, loss = 0.02480697
Iteration 38, loss = 0.02443237
Iteration 39, loss = 0.02409301
Iteration 40, loss = 0.02368495
Iteration 41, loss = 0.02347889
Iteration 42, loss = 0.02285148
Iteration 43, loss = 0.02254091
Iteration 44, loss = 0.02219714
Iteration 45, loss = 0.02197463
Iteration 46, loss = 0.02173917
Iteration 47, loss = 0.02140422
Iteration 48, loss = 0.02112112
Iteration 49, loss = 0.02113283
Iteration 50, loss = 0.02082722
Iteration 51, loss = 0.02056845
Iteration 52, loss = 0.02057389
Iteration 53, loss = 0.02051585
Iteration 54, loss = 0.02043950
Iteration 55, loss = 0.02021206
Iteration 56, loss = 0.02021700
Iteration 57, loss = 0.02019787
Iteration 58, loss = 0.01984838
Iteration 59, loss = 0.01964036
Iteration 60, loss = 0.01960386
Iteration 61, loss = 0.01960399
Iteration 62, loss = 0.01933710
Iteration 63, loss = 0.01941265
Iteration 64, loss = 0.01914785
Iteration 65, loss = 0.01896709
Iteration 66, loss = 0.01902403
Iteration 67, loss = 0.01931580
Iteration 68, loss = 0.01903380
Iteration 69, loss = 0.01873770
Iteration 70, loss = 0.01856696
Iteration 71, loss = 0.01860334
Iteration 72, loss = 0.01870688
Iteration 73, loss = 0.01850922
Iteration 74, loss = 0.01851896
Iteration 75, loss = 0.01848204
Iteration 76, loss = 0.01846218
Iteration 77, loss = 0.01820360
Iteration 78, loss = 0.01801920
Iteration 79, loss = 0.01810177
Iteration 80, loss = 0.01814856
Iteration 81, loss = 0.01830249
Iteration 82, loss = 0.01836960
Iteration 83, loss = 0.01809497
Iteration 84, loss = 0.01808815
Iteration 85, loss = 0.01766955
Iteration 86, loss = 0.01799131
Iteration 87, loss = 0.01785472
Iteration 88, loss = 0.01800206
Iteration 89, loss = 0.01805794
Iteration 90, loss = 0.01757636
Iteration 91, loss = 0.01772081
Iteration 92, loss = 0.01770842
Iteration 93, loss = 0.01736705
Iteration 94, loss = 0.01766314
Iteration 95, loss = 0.01759376
Iteration 96, loss = 0.01751878
Iteration 97, loss = 0.01754190
Iteration 98, loss = 0.01746478
Iteration 99, loss = 0.01735308
Iteration 100, loss = 0.01745555
Iteration 101, loss = 0.01776242
Iteration 102, loss = 0.01748530
Iteration 103, loss = 0.01739220
Iteration 104, loss = 0.01738100
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69189730
Iteration 2, loss = 0.65042964
Iteration 3, loss = 0.59818554
Iteration 4, loss = 0.53393403
Iteration 5, loss = 0.46376783
Iteration 6, loss = 0.39454805
Iteration 7, loss = 0.33187413
Iteration 8, loss = 0.27705528
Iteration 9, loss = 0.23116613
Iteration 10, loss = 0.19350448
Iteration 11, loss = 0.16337751
Iteration 12, loss = 0.13928268
Iteration 13, loss = 0.12018631
Iteration 14, loss = 0.10464024
Iteration 15, loss = 0.09225686
Iteration 16, loss = 0.08202719
Iteration 17, loss = 0.07386361
Iteration 18, loss = 0.06694724
Iteration 19, loss = 0.06117591
Iteration 20, loss = 0.05645788
Iteration 21, loss = 0.05243591
Iteration 22, loss = 0.04880669
Iteration 23, loss = 0.04579964
Iteration 24, loss = 0.04312341
Iteration 25, loss = 0.04093869
Iteration 26, loss = 0.03891971
Iteration 27, loss = 0.03725608
Iteration 28, loss = 0.03576210
Iteration 29, loss = 0.03415031
Iteration 30, loss = 0.03300287
Iteration 31, loss = 0.03188425
Iteration 32, loss = 0.03080010
Iteration 33, loss = 0.02991915
Iteration 34, loss = 0.02911829
Iteration 35, loss = 0.02834965
Iteration 36, loss = 0.02759695
Iteration 37, loss = 0.02716269
Iteration 38, loss = 0.02640742
Iteration 39, loss = 0.02592473
Iteration 40, loss = 0.02542160
Iteration 41, loss = 0.02506278
Iteration 42, loss = 0.02467917
Iteration 43, loss = 0.02423893
Iteration 44, loss = 0.02381135
Iteration 45, loss = 0.02359058
Iteration 46, loss = 0.02328191
Iteration 47, loss = 0.02302625
Iteration 48, loss = 0.02266490
Iteration 49, loss = 0.02258913
Iteration 50, loss = 0.02229902
Iteration 51, loss = 0.02195724
Iteration 52, loss = 0.02173093
Iteration 53, loss = 0.02159156
Iteration 54, loss = 0.02145092
Iteration 55, loss = 0.02125450
Iteration 56, loss = 0.02102846
Iteration 57, loss = 0.02088778
Iteration 58, loss = 0.02070266
Iteration 59, loss = 0.02055002
Iteration 60, loss = 0.02046704
Iteration 61, loss = 0.02044781
Iteration 62, loss = 0.02030543
Iteration 63, loss = 0.02024335
Iteration 64, loss = 0.02016785
Iteration 65, loss = 0.02002397
Iteration 66, loss = 0.01987801
Iteration 67, loss = 0.01967900
Iteration 68, loss = 0.01971895
Iteration 69, loss = 0.01944101
Iteration 70, loss = 0.01937944
Iteration 71, loss = 0.01941680
Iteration 72, loss = 0.01937836
Iteration 73, loss = 0.01917760
Iteration 74, loss = 0.01917469
Iteration 75, loss = 0.01919781
Iteration 76, loss = 0.01917701
Iteration 77, loss = 0.01917556
Iteration 78, loss = 0.01901279
Iteration 79, loss = 0.01893193
Iteration 80, loss = 0.01899167
Iteration 81, loss = 0.01888492
Iteration 82, loss = 0.01872239
Iteration 83, loss = 0.01871320
Iteration 84, loss = 0.01884998
Iteration 85, loss = 0.01864823
Iteration 86, loss = 0.01863283
Iteration 87, loss = 0.01849495
Iteration 88, loss = 0.01852681
Iteration 89, loss = 0.01847956
Iteration 90, loss = 0.01847777
Iteration 91, loss = 0.01841398
Iteration 92, loss = 0.01835584
Iteration 93, loss = 0.01818165
Iteration 94, loss = 0.01834637
Iteration 95, loss = 0.01831557
Iteration 96, loss = 0.01816821
Iteration 97, loss = 0.01830808
Iteration 98, loss = 0.01833313
Iteration 99, loss = 0.01833501
Iteration 100, loss = 0.01810130
Iteration 101, loss = 0.01810718
Iteration 102, loss = 0.01809496
Iteration 103, loss = 0.01809924
Iteration 104, loss = 0.01812857
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69365253
Iteration 2, loss = 0.67240250
Iteration 3, loss = 0.65432106
Iteration 4, loss = 0.63691869
Iteration 5, loss = 0.61888812
Iteration 6, loss = 0.60059568
Iteration 7, loss = 0.58097410
Iteration 8, loss = 0.56050382
Iteration 9, loss = 0.53921527
Iteration 10, loss = 0.51742804
Iteration 11, loss = 0.49495855
Iteration 12, loss = 0.47223314
Iteration 13, loss = 0.44944045
Iteration 14, loss = 0.42671089
Iteration 15, loss = 0.40417906
Iteration 16, loss = 0.38216113
Iteration 17, loss = 0.36070668
Iteration 18, loss = 0.33991755
Iteration 19, loss = 0.31987267
Iteration 20, loss = 0.30079930
Iteration 21, loss = 0.28260652
Iteration 22, loss = 0.26541554
Iteration 23, loss = 0.24911826
Iteration 24, loss = 0.23374900
Iteration 25, loss = 0.21933664
Iteration 26, loss = 0.20582404
Iteration 27, loss = 0.19331891
Iteration 28, loss = 0.18148250
Iteration 29, loss = 0.17056096
Iteration 30, loss = 0.16040961
Iteration 31, loss = 0.15094485
Iteration 32, loss = 0.14218773
Iteration 33, loss = 0.13405913
Iteration 34, loss = 0.12655057
Iteration 35, loss = 0.11950046
Iteration 36, loss = 0.11304784
Iteration 37, loss = 0.10706635
Iteration 38, loss = 0.10150608
Iteration 39, loss = 0.09627982
Iteration 40, loss = 0.09165284
Iteration 41, loss = 0.08708646
Iteration 42, loss = 0.08295572
Iteration 43, loss = 0.07908966
Iteration 44, loss = 0.07553426
Iteration 45, loss = 0.07219962
Iteration 46, loss = 0.06903746
Iteration 47, loss = 0.06617218
Iteration 48, loss = 0.06343565
Iteration 49, loss = 0.06088533
Iteration 50, loss = 0.05852035
Iteration 51, loss = 0.05626612
Iteration 52, loss = 0.05419009
Iteration 53, loss = 0.05220113
Iteration 54, loss = 0.05037394
Iteration 55, loss = 0.04862166
Iteration 56, loss = 0.04697375
Iteration 57, loss = 0.04542240
Iteration 58, loss = 0.04398832
Iteration 59, loss = 0.04254229
Iteration 60, loss = 0.04123355
Iteration 61, loss = 0.04006110
Iteration 62, loss = 0.03884462
Iteration 63, loss = 0.03773977
Iteration 64, loss = 0.03666769
Iteration 65, loss = 0.03566504
Iteration 66, loss = 0.03472009
Iteration 67, loss = 0.03379253
Iteration 68, loss = 0.03297022
Iteration 69, loss = 0.03210207
Iteration 70, loss = 0.03130741
Iteration 71, loss = 0.03055883
Iteration 72, loss = 0.02982571
Iteration 73, loss = 0.02914990
Iteration 74, loss = 0.02848798
Iteration 75, loss = 0.02787465
Iteration 76, loss = 0.02723691
Iteration 77, loss = 0.02666226
Iteration 78, loss = 0.02611168
Iteration 79, loss = 0.02557071
Iteration 80, loss = 0.02509152
Iteration 81, loss = 0.02463585
Iteration 82, loss = 0.02416258
Iteration 83, loss = 0.02371610
Iteration 84, loss = 0.02329456
Iteration 85, loss = 0.02289795
Iteration 86, loss = 0.02251779
Iteration 87, loss = 0.02213570
Iteration 88, loss = 0.02177350
Iteration 89, loss = 0.02136377
Iteration 90, loss = 0.02101555
Iteration 91, loss = 0.02070935
Iteration 92, loss = 0.02036947
Iteration 93, loss = 0.02005688
Iteration 94, loss = 0.01973470
Iteration 95, loss = 0.01946788
Iteration 96, loss = 0.01917956
Iteration 97, loss = 0.01889726
Iteration 98, loss = 0.01866372
Iteration 99, loss = 0.01841621
Iteration 100, loss = 0.01813327
Iteration 101, loss = 0.01792530
Iteration 102, loss = 0.01767359
Iteration 103, loss = 0.01743762
Iteration 104, loss = 0.01722406
Iteration 105, loss = 0.01703865
Iteration 106, loss = 0.01680931
Iteration 107, loss = 0.01662016
Iteration 108, loss = 0.01643849
Iteration 109, loss = 0.01624961
Iteration 110, loss = 0.01605366
Iteration 111, loss = 0.01589446
Iteration 112, loss = 0.01570175
Iteration 113, loss = 0.01556484
Iteration 114, loss = 0.01538211
Iteration 115, loss = 0.01522123
Iteration 116, loss = 0.01510159
Iteration 117, loss = 0.01492941
Iteration 118, loss = 0.01476510
Iteration 119, loss = 0.01464148
Iteration 120, loss = 0.01452101
Iteration 121, loss = 0.01437530
Iteration 122, loss = 0.01425198
Iteration 123, loss = 0.01413571
Iteration 124, loss = 0.01403917
Iteration 125, loss = 0.01393026
Iteration 126, loss = 0.01381652
Iteration 127, loss = 0.01369905
Iteration 128, loss = 0.01360337
Iteration 129, loss = 0.01352071
Iteration 130, loss = 0.01335738
Iteration 131, loss = 0.01324799
Iteration 132, loss = 0.01315707
Iteration 133, loss = 0.01303355
Iteration 134, loss = 0.01294167
Iteration 135, loss = 0.01282923
Iteration 136, loss = 0.01274483
Iteration 137, loss = 0.01265081
Iteration 138, loss = 0.01259074
Iteration 139, loss = 0.01248953
Iteration 140, loss = 0.01239943
Iteration 141, loss = 0.01230694
Iteration 142, loss = 0.01223625
Iteration 143, loss = 0.01213990
Iteration 144, loss = 0.01207514
Iteration 145, loss = 0.01199907
Iteration 146, loss = 0.01190777
Iteration 147, loss = 0.01187210
Iteration 148, loss = 0.01178420
Iteration 149, loss = 0.01174284
Iteration 150, loss = 0.01164964
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68415100
Iteration 2, loss = 0.65418221
Iteration 3, loss = 0.61430764
Iteration 4, loss = 0.56044529
Iteration 5, loss = 0.49611802
Iteration 6, loss = 0.43125547
Iteration 7, loss = 0.37013766
Iteration 8, loss = 0.31490782
Iteration 9, loss = 0.26698260
Iteration 10, loss = 0.22645002
Iteration 11, loss = 0.19241322
Iteration 12, loss = 0.16454861
Iteration 13, loss = 0.14166762
Iteration 14, loss = 0.12289565
Iteration 15, loss = 0.10760168
Iteration 16, loss = 0.09507177
Iteration 17, loss = 0.08455628
Iteration 18, loss = 0.07595849
Iteration 19, loss = 0.06856972
Iteration 20, loss = 0.06236787
Iteration 21, loss = 0.05715175
Iteration 22, loss = 0.05265402
Iteration 23, loss = 0.04877144
Iteration 24, loss = 0.04528887
Iteration 25, loss = 0.04231259
Iteration 26, loss = 0.03976392
Iteration 27, loss = 0.03761438
Iteration 28, loss = 0.03545687
Iteration 29, loss = 0.03354289
Iteration 30, loss = 0.03192619
Iteration 31, loss = 0.03048038
Iteration 32, loss = 0.02921684
Iteration 33, loss = 0.02799474
Iteration 34, loss = 0.02686152
Iteration 35, loss = 0.02593108
Iteration 36, loss = 0.02505885
Iteration 37, loss = 0.02427798
Iteration 38, loss = 0.02355303
Iteration 39, loss = 0.02281089
Iteration 40, loss = 0.02212171
Iteration 41, loss = 0.02156088
Iteration 42, loss = 0.02116176
Iteration 43, loss = 0.02049727
Iteration 44, loss = 0.02004999
Iteration 45, loss = 0.01958544
Iteration 46, loss = 0.01918300
Iteration 47, loss = 0.01878551
Iteration 48, loss = 0.01843028
Iteration 49, loss = 0.01818661
Iteration 50, loss = 0.01777089
Iteration 51, loss = 0.01762522
Iteration 52, loss = 0.01723833
Iteration 53, loss = 0.01707037
Iteration 54, loss = 0.01684094
Iteration 55, loss = 0.01663691
Iteration 56, loss = 0.01633193
Iteration 57, loss = 0.01608967
Iteration 58, loss = 0.01586056
Iteration 59, loss = 0.01568329
Iteration 60, loss = 0.01560149
Iteration 61, loss = 0.01540151
Iteration 62, loss = 0.01514371
Iteration 63, loss = 0.01512604
Iteration 64, loss = 0.01493222
Iteration 65, loss = 0.01476952
Iteration 66, loss = 0.01471271
Iteration 67, loss = 0.01458079
Iteration 68, loss = 0.01441287
Iteration 69, loss = 0.01441126
Iteration 70, loss = 0.01420609
Iteration 71, loss = 0.01413191
Iteration 72, loss = 0.01405043
Iteration 73, loss = 0.01385337
Iteration 74, loss = 0.01380521
Iteration 75, loss = 0.01375904
Iteration 76, loss = 0.01377333
Iteration 77, loss = 0.01359777
Iteration 78, loss = 0.01349150
Iteration 79, loss = 0.01344700
Iteration 80, loss = 0.01334606
Iteration 81, loss = 0.01327024
Iteration 82, loss = 0.01322962
Iteration 83, loss = 0.01316354
Iteration 84, loss = 0.01311928
Iteration 85, loss = 0.01308395
Iteration 86, loss = 0.01301616
Iteration 87, loss = 0.01296085
Iteration 88, loss = 0.01292422
Iteration 89, loss = 0.01288315
Iteration 90, loss = 0.01290116
Iteration 91, loss = 0.01269245
Iteration 92, loss = 0.01275584
Iteration 93, loss = 0.01267724
Iteration 94, loss = 0.01260782
Iteration 95, loss = 0.01260809
Iteration 96, loss = 0.01253957
Iteration 97, loss = 0.01250564
Iteration 98, loss = 0.01243282
Iteration 99, loss = 0.01247415
Iteration 100, loss = 0.01243471
Iteration 101, loss = 0.01250167
Iteration 102, loss = 0.01230692
Iteration 103, loss = 0.01228522
Iteration 104, loss = 0.01230758
Iteration 105, loss = 0.01223604
Iteration 106, loss = 0.01225280
Iteration 107, loss = 0.01219830
Iteration 108, loss = 0.01217723
Iteration 109, loss = 0.01215448
Iteration 110, loss = 0.01212142
Iteration 111, loss = 0.01216976
Iteration 112, loss = 0.01214227
Iteration 113, loss = 0.01211301
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68489106
Iteration 2, loss = 0.65912432
Iteration 3, loss = 0.62409267
Iteration 4, loss = 0.57216575
Iteration 5, loss = 0.50990524
Iteration 6, loss = 0.44677402
Iteration 7, loss = 0.38696604
Iteration 8, loss = 0.33285127
Iteration 9, loss = 0.28541552
Iteration 10, loss = 0.24461327
Iteration 11, loss = 0.21013407
Iteration 12, loss = 0.18143374
Iteration 13, loss = 0.15731300
Iteration 14, loss = 0.13753860
Iteration 15, loss = 0.12103061
Iteration 16, loss = 0.10735236
Iteration 17, loss = 0.09601941
Iteration 18, loss = 0.08631699
Iteration 19, loss = 0.07830138
Iteration 20, loss = 0.07137389
Iteration 21, loss = 0.06554961
Iteration 22, loss = 0.06049987
Iteration 23, loss = 0.05612161
Iteration 24, loss = 0.05224291
Iteration 25, loss = 0.04899067
Iteration 26, loss = 0.04602719
Iteration 27, loss = 0.04348655
Iteration 28, loss = 0.04131947
Iteration 29, loss = 0.03920032
Iteration 30, loss = 0.03736552
Iteration 31, loss = 0.03578243
Iteration 32, loss = 0.03434147
Iteration 33, loss = 0.03310919
Iteration 34, loss = 0.03181936
Iteration 35, loss = 0.03088213
Iteration 36, loss = 0.02977423
Iteration 37, loss = 0.02892517
Iteration 38, loss = 0.02807172
Iteration 39, loss = 0.02736955
Iteration 40, loss = 0.02650455
Iteration 41, loss = 0.02594768
Iteration 42, loss = 0.02540515
Iteration 43, loss = 0.02483650
Iteration 44, loss = 0.02431833
Iteration 45, loss = 0.02377082
Iteration 46, loss = 0.02325230
Iteration 47, loss = 0.02296444
Iteration 48, loss = 0.02248750
Iteration 49, loss = 0.02217198
Iteration 50, loss = 0.02181641
Iteration 51, loss = 0.02148794
Iteration 52, loss = 0.02126448
Iteration 53, loss = 0.02097893
Iteration 54, loss = 0.02074773
Iteration 55, loss = 0.02048976
Iteration 56, loss = 0.02026870
Iteration 57, loss = 0.02004628
Iteration 58, loss = 0.01973741
Iteration 59, loss = 0.01961337
Iteration 60, loss = 0.01928182
Iteration 61, loss = 0.01921576
Iteration 62, loss = 0.01896251
Iteration 63, loss = 0.01885134
Iteration 64, loss = 0.01865979
Iteration 65, loss = 0.01871232
Iteration 66, loss = 0.01845069
Iteration 67, loss = 0.01820583
Iteration 68, loss = 0.01808666
Iteration 69, loss = 0.01805505
Iteration 70, loss = 0.01805804
Iteration 71, loss = 0.01781487
Iteration 72, loss = 0.01768429
Iteration 73, loss = 0.01759679
Iteration 74, loss = 0.01753008
Iteration 75, loss = 0.01745327
Iteration 76, loss = 0.01734902
Iteration 77, loss = 0.01732821
Iteration 78, loss = 0.01719679
Iteration 79, loss = 0.01715520
Iteration 80, loss = 0.01725357
Iteration 81, loss = 0.01702734
Iteration 82, loss = 0.01707287
Iteration 83, loss = 0.01685327
Iteration 84, loss = 0.01686869
Iteration 85, loss = 0.01683507
Iteration 86, loss = 0.01664820
Iteration 87, loss = 0.01666941
Iteration 88, loss = 0.01657773
Iteration 89, loss = 0.01651584
Iteration 90, loss = 0.01656757
Iteration 91, loss = 0.01643589
Iteration 92, loss = 0.01628967
Iteration 93, loss = 0.01643229
Iteration 94, loss = 0.01636010
Iteration 95, loss = 0.01627111
Iteration 96, loss = 0.01625060
Iteration 97, loss = 0.01612209
Iteration 98, loss = 0.01605472
Iteration 99, loss = 0.01615749
Iteration 100, loss = 0.01627690
Iteration 101, loss = 0.01637384
Iteration 102, loss = 0.01609857
Iteration 103, loss = 0.01619536
Iteration 104, loss = 0.01598327
Iteration 105, loss = 0.01582358
Iteration 106, loss = 0.01586984
Iteration 107, loss = 0.01584612
Iteration 108, loss = 0.01577817
Iteration 109, loss = 0.01575455
Iteration 110, loss = 0.01573287
Iteration 111, loss = 0.01578757
Iteration 112, loss = 0.01575258
Iteration 113, loss = 0.01561147
Iteration 114, loss = 0.01594825
Iteration 115, loss = 0.01553279
Iteration 116, loss = 0.01553803
Iteration 117, loss = 0.01575186
Iteration 118, loss = 0.01561499
Iteration 119, loss = 0.01556232
Iteration 120, loss = 0.01551991
Iteration 121, loss = 0.01552856
Iteration 122, loss = 0.01566246
Iteration 123, loss = 0.01554543
Iteration 124, loss = 0.01543705
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69478557
Iteration 2, loss = 0.64284317
Iteration 3, loss = 0.56886306
Iteration 4, loss = 0.48007989
Iteration 5, loss = 0.39161945
Iteration 6, loss = 0.31265046
Iteration 7, loss = 0.24802367
Iteration 8, loss = 0.19751705
Iteration 9, loss = 0.15983164
Iteration 10, loss = 0.13181347
Iteration 11, loss = 0.11094902
Iteration 12, loss = 0.09491313
Iteration 13, loss = 0.08281055
Iteration 14, loss = 0.07324897
Iteration 15, loss = 0.06577363
Iteration 16, loss = 0.05970052
Iteration 17, loss = 0.05481272
Iteration 18, loss = 0.05061112
Iteration 19, loss = 0.04713486
Iteration 20, loss = 0.04463859
Iteration 21, loss = 0.04211898
Iteration 22, loss = 0.03991044
Iteration 23, loss = 0.03800143
Iteration 24, loss = 0.03642219
Iteration 25, loss = 0.03497259
Iteration 26, loss = 0.03391610
Iteration 27, loss = 0.03270775
Iteration 28, loss = 0.03194607
Iteration 29, loss = 0.03084303
Iteration 30, loss = 0.03018473
Iteration 31, loss = 0.02934188
Iteration 32, loss = 0.02906407
Iteration 33, loss = 0.02842800
Iteration 34, loss = 0.02775632
Iteration 35, loss = 0.02724796
Iteration 36, loss = 0.02688862
Iteration 37, loss = 0.02660666
Iteration 38, loss = 0.02620879
Iteration 39, loss = 0.02576886
Iteration 40, loss = 0.02567818
Iteration 41, loss = 0.02528778
Iteration 42, loss = 0.02502393
Iteration 43, loss = 0.02489288
Iteration 44, loss = 0.02462258
Iteration 45, loss = 0.02432103
Iteration 46, loss = 0.02421039
Iteration 47, loss = 0.02398626
Iteration 48, loss = 0.02371403
Iteration 49, loss = 0.02357250
Iteration 50, loss = 0.02334250
Iteration 51, loss = 0.02334069
Iteration 52, loss = 0.02339007
Iteration 53, loss = 0.02316128
Iteration 54, loss = 0.02299914
Iteration 55, loss = 0.02287032
Iteration 56, loss = 0.02276404
Iteration 57, loss = 0.02250749
Iteration 58, loss = 0.02256146
Iteration 59, loss = 0.02228694
Iteration 60, loss = 0.02239888
Iteration 61, loss = 0.02234730
Iteration 62, loss = 0.02234301
Iteration 63, loss = 0.02187769
Iteration 64, loss = 0.02205012
Iteration 65, loss = 0.02211281
Iteration 66, loss = 0.02184010
Iteration 67, loss = 0.02196768
Iteration 68, loss = 0.02173124
Iteration 69, loss = 0.02179087
Iteration 70, loss = 0.02205598
Iteration 71, loss = 0.02172436
Iteration 72, loss = 0.02170757
Iteration 73, loss = 0.02158438
Iteration 74, loss = 0.02156934
Iteration 75, loss = 0.02126625
Iteration 76, loss = 0.02139137
Iteration 77, loss = 0.02159953
Iteration 78, loss = 0.02114981
Iteration 79, loss = 0.02141740
Iteration 80, loss = 0.02131614
Iteration 81, loss = 0.02170311
Iteration 82, loss = 0.02103249
Iteration 83, loss = 0.02106833
Iteration 84, loss = 0.02108175
Iteration 85, loss = 0.02110060
Iteration 86, loss = 0.02112108
Iteration 87, loss = 0.02101858
Iteration 88, loss = 0.02094807
Iteration 89, loss = 0.02088910
Iteration 90, loss = 0.02115185
Iteration 91, loss = 0.02096503
Iteration 92, loss = 0.02088266
Iteration 93, loss = 0.02115497
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71055806
Iteration 2, loss = 0.68132095
Iteration 3, loss = 0.64683680
Iteration 4, loss = 0.60420953
Iteration 5, loss = 0.55636590
Iteration 6, loss = 0.50606450
Iteration 7, loss = 0.45505934
Iteration 8, loss = 0.40542778
Iteration 9, loss = 0.35811091
Iteration 10, loss = 0.31457474
Iteration 11, loss = 0.27576779
Iteration 12, loss = 0.24142988
Iteration 13, loss = 0.21172465
Iteration 14, loss = 0.18634879
Iteration 15, loss = 0.16460731
Iteration 16, loss = 0.14635280
Iteration 17, loss = 0.13061134
Iteration 18, loss = 0.11737079
Iteration 19, loss = 0.10604239
Iteration 20, loss = 0.09649583
Iteration 21, loss = 0.08821635
Iteration 22, loss = 0.08083672
Iteration 23, loss = 0.07466012
Iteration 24, loss = 0.06908836
Iteration 25, loss = 0.06449719
Iteration 26, loss = 0.06040274
Iteration 27, loss = 0.05664572
Iteration 28, loss = 0.05328719
Iteration 29, loss = 0.05043810
Iteration 30, loss = 0.04773694
Iteration 31, loss = 0.04539031
Iteration 32, loss = 0.04329985
Iteration 33, loss = 0.04143134
Iteration 34, loss = 0.03964816
Iteration 35, loss = 0.03811069
Iteration 36, loss = 0.03663738
Iteration 37, loss = 0.03528087
Iteration 38, loss = 0.03413081
Iteration 39, loss = 0.03311726
Iteration 40, loss = 0.03205648
Iteration 41, loss = 0.03106665
Iteration 42, loss = 0.03023826
Iteration 43, loss = 0.02952738
Iteration 44, loss = 0.02877864
Iteration 45, loss = 0.02812915
Iteration 46, loss = 0.02746487
Iteration 47, loss = 0.02687635
Iteration 48, loss = 0.02615819
Iteration 49, loss = 0.02584805
Iteration 50, loss = 0.02499179
Iteration 51, loss = 0.02469359
Iteration 52, loss = 0.02425634
Iteration 53, loss = 0.02381536
Iteration 54, loss = 0.02315498
Iteration 55, loss = 0.02336576
Iteration 56, loss = 0.02301710
Iteration 57, loss = 0.02255921
Iteration 58, loss = 0.02229108
Iteration 59, loss = 0.02193639
Iteration 60, loss = 0.02165240
Iteration 61, loss = 0.02121036
Iteration 62, loss = 0.02118292
Iteration 63, loss = 0.02093600
Iteration 64, loss = 0.02069861
Iteration 65, loss = 0.02040174
Iteration 66, loss = 0.02038519
Iteration 67, loss = 0.02011746
Iteration 68, loss = 0.01996256
Iteration 69, loss = 0.01970574
Iteration 70, loss = 0.01939869
Iteration 71, loss = 0.01936065
Iteration 72, loss = 0.01910116
Iteration 73, loss = 0.01931441
Iteration 74, loss = 0.01923380
Iteration 75, loss = 0.01912841
Iteration 76, loss = 0.01892639
Iteration 77, loss = 0.01863313
Iteration 78, loss = 0.01847403
Iteration 79, loss = 0.01833030
Iteration 80, loss = 0.01812899
Iteration 81, loss = 0.01794196
Iteration 82, loss = 0.01785353
Iteration 83, loss = 0.01764890
Iteration 84, loss = 0.01762123
Iteration 85, loss = 0.01766493
Iteration 86, loss = 0.01755487
Iteration 87, loss = 0.01740687
Iteration 88, loss = 0.01726523
Iteration 89, loss = 0.01721608
Iteration 90, loss = 0.01722613
Iteration 91, loss = 0.01711493
Iteration 92, loss = 0.01712735
Iteration 93, loss = 0.01690296
Iteration 94, loss = 0.01731164
Iteration 95, loss = 0.01714979
Iteration 96, loss = 0.01702676
Iteration 97, loss = 0.01689950
Iteration 98, loss = 0.01694034
Iteration 99, loss = 0.01701246
Iteration 100, loss = 0.01690594
Iteration 101, loss = 0.01682880
Iteration 102, loss = 0.01661527
Iteration 103, loss = 0.01691794
Iteration 104, loss = 0.01642918
Iteration 105, loss = 0.01620181
Iteration 106, loss = 0.01655283
Iteration 107, loss = 0.01647341
Iteration 108, loss = 0.01630938
Iteration 109, loss = 0.01617232
Iteration 110, loss = 0.01608676
Iteration 111, loss = 0.01599764
Iteration 112, loss = 0.01593753
Iteration 113, loss = 0.01586233
Iteration 114, loss = 0.01581718
Iteration 115, loss = 0.01594986
Iteration 116, loss = 0.01594699
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69702010
Iteration 2, loss = 0.65636974
Iteration 3, loss = 0.60879845
Iteration 4, loss = 0.55316675
Iteration 5, loss = 0.49092757
Iteration 6, loss = 0.42768813
Iteration 7, loss = 0.36787346
Iteration 8, loss = 0.31312681
Iteration 9, loss = 0.26539999
Iteration 10, loss = 0.22486706
Iteration 11, loss = 0.19135279
Iteration 12, loss = 0.16380194
Iteration 13, loss = 0.14117584
Iteration 14, loss = 0.12279025
Iteration 15, loss = 0.10758808
Iteration 16, loss = 0.09525335
Iteration 17, loss = 0.08490834
Iteration 18, loss = 0.07647146
Iteration 19, loss = 0.06917715
Iteration 20, loss = 0.06315786
Iteration 21, loss = 0.05784250
Iteration 22, loss = 0.05353498
Iteration 23, loss = 0.04975351
Iteration 24, loss = 0.04625580
Iteration 25, loss = 0.04348541
Iteration 26, loss = 0.04089668
Iteration 27, loss = 0.03858092
Iteration 28, loss = 0.03663520
Iteration 29, loss = 0.03493613
Iteration 30, loss = 0.03331177
Iteration 31, loss = 0.03178885
Iteration 32, loss = 0.03061254
Iteration 33, loss = 0.02939010
Iteration 34, loss = 0.02825635
Iteration 35, loss = 0.02741738
Iteration 36, loss = 0.02643762
Iteration 37, loss = 0.02565725
Iteration 38, loss = 0.02489985
Iteration 39, loss = 0.02446720
Iteration 40, loss = 0.02368830
Iteration 41, loss = 0.02313214
Iteration 42, loss = 0.02249862
Iteration 43, loss = 0.02204656
Iteration 44, loss = 0.02156919
Iteration 45, loss = 0.02127119
Iteration 46, loss = 0.02085201
Iteration 47, loss = 0.02037869
Iteration 48, loss = 0.02016089
Iteration 49, loss = 0.01980306
Iteration 50, loss = 0.01948833
Iteration 51, loss = 0.01929690
Iteration 52, loss = 0.01887301
Iteration 53, loss = 0.01858348
Iteration 54, loss = 0.01850049
Iteration 55, loss = 0.01811730
Iteration 56, loss = 0.01792634
Iteration 57, loss = 0.01790905
Iteration 58, loss = 0.01770746
Iteration 59, loss = 0.01739651
Iteration 60, loss = 0.01716861
Iteration 61, loss = 0.01719091
Iteration 62, loss = 0.01703463
Iteration 63, loss = 0.01679173
Iteration 64, loss = 0.01661608
Iteration 65, loss = 0.01661498
Iteration 66, loss = 0.01642082
Iteration 67, loss = 0.01629471
Iteration 68, loss = 0.01616265
Iteration 69, loss = 0.01607064
Iteration 70, loss = 0.01604401
Iteration 71, loss = 0.01587524
Iteration 72, loss = 0.01592430
Iteration 73, loss = 0.01559205
Iteration 74, loss = 0.01570900
Iteration 75, loss = 0.01556091
Iteration 76, loss = 0.01543552
Iteration 77, loss = 0.01538999
Iteration 78, loss = 0.01530902
Iteration 79, loss = 0.01528840
Iteration 80, loss = 0.01511645
Iteration 81, loss = 0.01516356
Iteration 82, loss = 0.01505262
Iteration 83, loss = 0.01496339
Iteration 84, loss = 0.01488210
Iteration 85, loss = 0.01484739
Iteration 86, loss = 0.01481087
Iteration 87, loss = 0.01476331
Iteration 88, loss = 0.01470604
Iteration 89, loss = 0.01454859
Iteration 90, loss = 0.01460226
Iteration 91, loss = 0.01456362
Iteration 92, loss = 0.01464259
Iteration 93, loss = 0.01457877
Iteration 94, loss = 0.01453683
Iteration 95, loss = 0.01450726
Iteration 96, loss = 0.01436242
Iteration 97, loss = 0.01436162
Iteration 98, loss = 0.01432803
Iteration 99, loss = 0.01421489
Iteration 100, loss = 0.01428884
Iteration 101, loss = 0.01428847
Iteration 102, loss = 0.01425122
Iteration 103, loss = 0.01418537
Iteration 104, loss = 0.01414145
Iteration 105, loss = 0.01421947
Iteration 106, loss = 0.01397194
Iteration 107, loss = 0.01399417
Iteration 108, loss = 0.01402444
Iteration 109, loss = 0.01399329
Iteration 110, loss = 0.01400937
Iteration 111, loss = 0.01395087
Iteration 112, loss = 0.01404126
Iteration 113, loss = 0.01388331
Iteration 114, loss = 0.01384861
Iteration 115, loss = 0.01388167
Iteration 116, loss = 0.01388228
Iteration 117, loss = 0.01392895
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71638188
Iteration 2, loss = 0.69399096
Iteration 3, loss = 0.67365426
Iteration 4, loss = 0.65136733
Iteration 5, loss = 0.62578420
Iteration 6, loss = 0.59676304
Iteration 7, loss = 0.56413840
Iteration 8, loss = 0.52910923
Iteration 9, loss = 0.49193030
Iteration 10, loss = 0.45441592
Iteration 11, loss = 0.41664382
Iteration 12, loss = 0.38028505
Iteration 13, loss = 0.34562512
Iteration 14, loss = 0.31340305
Iteration 15, loss = 0.28410747
Iteration 16, loss = 0.25754499
Iteration 17, loss = 0.23345187
Iteration 18, loss = 0.21202304
Iteration 19, loss = 0.19288412
Iteration 20, loss = 0.17593549
Iteration 21, loss = 0.16077388
Iteration 22, loss = 0.14737462
Iteration 23, loss = 0.13549653
Iteration 24, loss = 0.12490620
Iteration 25, loss = 0.11556748
Iteration 26, loss = 0.10715676
Iteration 27, loss = 0.09972161
Iteration 28, loss = 0.09306781
Iteration 29, loss = 0.08705815
Iteration 30, loss = 0.08168388
Iteration 31, loss = 0.07691006
Iteration 32, loss = 0.07248721
Iteration 33, loss = 0.06855290
Iteration 34, loss = 0.06494419
Iteration 35, loss = 0.06160067
Iteration 36, loss = 0.05874273
Iteration 37, loss = 0.05591300
Iteration 38, loss = 0.05345424
Iteration 39, loss = 0.05112583
Iteration 40, loss = 0.04906914
Iteration 41, loss = 0.04703910
Iteration 42, loss = 0.04526404
Iteration 43, loss = 0.04359455
Iteration 44, loss = 0.04204684
Iteration 45, loss = 0.04056575
Iteration 46, loss = 0.03923357
Iteration 47, loss = 0.03801289
Iteration 48, loss = 0.03689431
Iteration 49, loss = 0.03576263
Iteration 50, loss = 0.03473485
Iteration 51, loss = 0.03377746
Iteration 52, loss = 0.03287797
Iteration 53, loss = 0.03203721
Iteration 54, loss = 0.03123041
Iteration 55, loss = 0.03051953
Iteration 56, loss = 0.02977983
Iteration 57, loss = 0.02909526
Iteration 58, loss = 0.02847798
Iteration 59, loss = 0.02789394
Iteration 60, loss = 0.02732455
Iteration 61, loss = 0.02681018
Iteration 62, loss = 0.02628836
Iteration 63, loss = 0.02581371
Iteration 64, loss = 0.02538951
Iteration 65, loss = 0.02494257
Iteration 66, loss = 0.02448905
Iteration 67, loss = 0.02410969
Iteration 68, loss = 0.02370062
Iteration 69, loss = 0.02337418
Iteration 70, loss = 0.02301339
Iteration 71, loss = 0.02270861
Iteration 72, loss = 0.02238054
Iteration 73, loss = 0.02206648
Iteration 74, loss = 0.02181244
Iteration 75, loss = 0.02150513
Iteration 76, loss = 0.02129681
Iteration 77, loss = 0.02102710
Iteration 78, loss = 0.02078264
Iteration 79, loss = 0.02049802
Iteration 80, loss = 0.02030580
Iteration 81, loss = 0.02010967
Iteration 82, loss = 0.01987047
Iteration 83, loss = 0.01971550
Iteration 84, loss = 0.01948250
Iteration 85, loss = 0.01929542
Iteration 86, loss = 0.01908656
Iteration 87, loss = 0.01897525
Iteration 88, loss = 0.01880673
Iteration 89, loss = 0.01867429
Iteration 90, loss = 0.01846010
Iteration 91, loss = 0.01830726
Iteration 92, loss = 0.01817848
Iteration 93, loss = 0.01810343
Iteration 94, loss = 0.01788720
Iteration 95, loss = 0.01778499
Iteration 96, loss = 0.01763903
Iteration 97, loss = 0.01753951
Iteration 98, loss = 0.01738198
Iteration 99, loss = 0.01732595
Iteration 100, loss = 0.01716976
Iteration 101, loss = 0.01704633
Iteration 102, loss = 0.01702602
Iteration 103, loss = 0.01690970
Iteration 104, loss = 0.01675677
Iteration 105, loss = 0.01664152
Iteration 106, loss = 0.01667462
Iteration 107, loss = 0.01651857
Iteration 108, loss = 0.01637816
Iteration 109, loss = 0.01639105
Iteration 110, loss = 0.01627018
Iteration 111, loss = 0.01618546
Iteration 112, loss = 0.01606504
Iteration 113, loss = 0.01603005
Iteration 114, loss = 0.01600594
Iteration 115, loss = 0.01587411
Iteration 116, loss = 0.01583334
Iteration 117, loss = 0.01577088
Iteration 118, loss = 0.01566322
Iteration 119, loss = 0.01562766
Iteration 120, loss = 0.01553881
Iteration 121, loss = 0.01551846
Iteration 122, loss = 0.01541639
Iteration 123, loss = 0.01542990
Iteration 124, loss = 0.01528552
Iteration 125, loss = 0.01524676
Iteration 126, loss = 0.01517022
Iteration 127, loss = 0.01516937
Iteration 128, loss = 0.01506935
Iteration 129, loss = 0.01505525
Iteration 130, loss = 0.01498237
Iteration 131, loss = 0.01494036
Iteration 132, loss = 0.01492508
Iteration 133, loss = 0.01485034
Iteration 134, loss = 0.01485561
Iteration 135, loss = 0.01477089
Iteration 136, loss = 0.01474304
Iteration 137, loss = 0.01470994
Iteration 138, loss = 0.01467367
Iteration 139, loss = 0.01461380
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67718094
Iteration 2, loss = 0.63446907
Iteration 3, loss = 0.57967039
Iteration 4, loss = 0.51230870
Iteration 5, loss = 0.44244578
Iteration 6, loss = 0.37571480
Iteration 7, loss = 0.31511720
Iteration 8, loss = 0.26257367
Iteration 9, loss = 0.21867126
Iteration 10, loss = 0.18282290
Iteration 11, loss = 0.15401453
Iteration 12, loss = 0.13139279
Iteration 13, loss = 0.11300244
Iteration 14, loss = 0.09847050
Iteration 15, loss = 0.08666756
Iteration 16, loss = 0.07710675
Iteration 17, loss = 0.06913955
Iteration 18, loss = 0.06266313
Iteration 19, loss = 0.05719692
Iteration 20, loss = 0.05258962
Iteration 21, loss = 0.04886220
Iteration 22, loss = 0.04525904
Iteration 23, loss = 0.04262553
Iteration 24, loss = 0.03999429
Iteration 25, loss = 0.03779813
Iteration 26, loss = 0.03572126
Iteration 27, loss = 0.03419327
Iteration 28, loss = 0.03253347
Iteration 29, loss = 0.03118142
Iteration 30, loss = 0.02996955
Iteration 31, loss = 0.02880545
Iteration 32, loss = 0.02788147
Iteration 33, loss = 0.02697908
Iteration 34, loss = 0.02619924
Iteration 35, loss = 0.02546845
Iteration 36, loss = 0.02471087
Iteration 37, loss = 0.02426590
Iteration 38, loss = 0.02373023
Iteration 39, loss = 0.02299123
Iteration 40, loss = 0.02260400
Iteration 41, loss = 0.02212883
Iteration 42, loss = 0.02175356
Iteration 43, loss = 0.02137413
Iteration 44, loss = 0.02118982
Iteration 45, loss = 0.02075067
Iteration 46, loss = 0.02034393
Iteration 47, loss = 0.02005632
Iteration 48, loss = 0.01993636
Iteration 49, loss = 0.01964728
Iteration 50, loss = 0.01931099
Iteration 51, loss = 0.01915134
Iteration 52, loss = 0.01893402
Iteration 53, loss = 0.01867813
Iteration 54, loss = 0.01857516
Iteration 55, loss = 0.01836696
Iteration 56, loss = 0.01834511
Iteration 57, loss = 0.01800705
Iteration 58, loss = 0.01813307
Iteration 59, loss = 0.01782392
Iteration 60, loss = 0.01767300
Iteration 61, loss = 0.01761265
Iteration 62, loss = 0.01741511
Iteration 63, loss = 0.01732572
Iteration 64, loss = 0.01719276
Iteration 65, loss = 0.01721707
Iteration 66, loss = 0.01706997
Iteration 67, loss = 0.01690593
Iteration 68, loss = 0.01686829
Iteration 69, loss = 0.01679926
Iteration 70, loss = 0.01679041
Iteration 71, loss = 0.01666098
Iteration 72, loss = 0.01668586
Iteration 73, loss = 0.01650315
Iteration 74, loss = 0.01647764
Iteration 75, loss = 0.01627270
Iteration 76, loss = 0.01625079
Iteration 77, loss = 0.01613288
Iteration 78, loss = 0.01617079
Iteration 79, loss = 0.01605771
Iteration 80, loss = 0.01606027
Iteration 81, loss = 0.01626684
Iteration 82, loss = 0.01596034
Iteration 83, loss = 0.01610362
Iteration 84, loss = 0.01585416
Iteration 85, loss = 0.01598635
Iteration 86, loss = 0.01572009
Iteration 87, loss = 0.01578466
Iteration 88, loss = 0.01574063
Iteration 89, loss = 0.01568195
Iteration 90, loss = 0.01559577
Iteration 91, loss = 0.01559742
Iteration 92, loss = 0.01561998
Iteration 93, loss = 0.01564721
Iteration 94, loss = 0.01556258
Iteration 95, loss = 0.01549233
Iteration 96, loss = 0.01559823
Iteration 97, loss = 0.01559495
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69323871
Iteration 2, loss = 0.62808807
Iteration 3, loss = 0.53856347
Iteration 4, loss = 0.43712438
Iteration 5, loss = 0.34447075
Iteration 6, loss = 0.26714763
Iteration 7, loss = 0.20819838
Iteration 8, loss = 0.16551711
Iteration 9, loss = 0.13469510
Iteration 10, loss = 0.11217646
Iteration 11, loss = 0.09621137
Iteration 12, loss = 0.08376977
Iteration 13, loss = 0.07410430
Iteration 14, loss = 0.06650666
Iteration 15, loss = 0.06076195
Iteration 16, loss = 0.05573024
Iteration 17, loss = 0.05174861
Iteration 18, loss = 0.04836524
Iteration 19, loss = 0.04550218
Iteration 20, loss = 0.04308899
Iteration 21, loss = 0.04113244
Iteration 22, loss = 0.03919604
Iteration 23, loss = 0.03788864
Iteration 24, loss = 0.03618463
Iteration 25, loss = 0.03499971
Iteration 26, loss = 0.03405806
Iteration 27, loss = 0.03325114
Iteration 28, loss = 0.03246510
Iteration 29, loss = 0.03162756
Iteration 30, loss = 0.03092791
Iteration 31, loss = 0.03029875
Iteration 32, loss = 0.02995430
Iteration 33, loss = 0.02930549
Iteration 34, loss = 0.02881940
Iteration 35, loss = 0.02837599
Iteration 36, loss = 0.02813834
Iteration 37, loss = 0.02773145
Iteration 38, loss = 0.02731896
Iteration 39, loss = 0.02705005
Iteration 40, loss = 0.02680437
Iteration 41, loss = 0.02659420
Iteration 42, loss = 0.02617252
Iteration 43, loss = 0.02582974
Iteration 44, loss = 0.02582396
Iteration 45, loss = 0.02554963
Iteration 46, loss = 0.02621901
Iteration 47, loss = 0.02561903
Iteration 48, loss = 0.02557458
Iteration 49, loss = 0.02510508
Iteration 50, loss = 0.02484084
Iteration 51, loss = 0.02478223
Iteration 52, loss = 0.02481734
Iteration 53, loss = 0.02453138
Iteration 54, loss = 0.02423491
Iteration 55, loss = 0.02415135
Iteration 56, loss = 0.02416245
Iteration 57, loss = 0.02425978
Iteration 58, loss = 0.02408608
Iteration 59, loss = 0.02388537
Iteration 60, loss = 0.02400553
Iteration 61, loss = 0.02367628
Iteration 62, loss = 0.02369044
Iteration 63, loss = 0.02360448
Iteration 64, loss = 0.02359715
Iteration 65, loss = 0.02353280
Iteration 66, loss = 0.02347051
Iteration 67, loss = 0.02349247
Iteration 68, loss = 0.02325364
Iteration 69, loss = 0.02314696
Iteration 70, loss = 0.02313849
Iteration 71, loss = 0.02338090
Iteration 72, loss = 0.02317892
Iteration 73, loss = 0.02296857
Iteration 74, loss = 0.02333488
Iteration 75, loss = 0.02351468
Iteration 76, loss = 0.02316156
Iteration 77, loss = 0.02284376
Iteration 78, loss = 0.02307728
Iteration 79, loss = 0.02281281
Iteration 80, loss = 0.02250093
Iteration 81, loss = 0.02291206
Iteration 82, loss = 0.02270931
Iteration 83, loss = 0.02242202
Iteration 84, loss = 0.02255939
Iteration 85, loss = 0.02250941
Iteration 86, loss = 0.02270133
Iteration 87, loss = 0.02257980
Iteration 88, loss = 0.02249708
Iteration 89, loss = 0.02260125
Iteration 90, loss = 0.02225029
Iteration 91, loss = 0.02225570
Iteration 92, loss = 0.02272075
Iteration 93, loss = 0.02248124
Iteration 94, loss = 0.02217788
Iteration 95, loss = 0.02242511
Iteration 96, loss = 0.02208594
Iteration 97, loss = 0.02200422
Iteration 98, loss = 0.02220995
Iteration 99, loss = 0.02248716
Iteration 100, loss = 0.02225857
Iteration 101, loss = 0.02235698
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67587335
Iteration 2, loss = 0.62012484
Iteration 3, loss = 0.53570036
Iteration 4, loss = 0.44061016
Iteration 5, loss = 0.35321503
Iteration 6, loss = 0.27974191
Iteration 7, loss = 0.22109647
Iteration 8, loss = 0.17708380
Iteration 9, loss = 0.14428391
Iteration 10, loss = 0.11985816
Iteration 11, loss = 0.10157354
Iteration 12, loss = 0.08769895
Iteration 13, loss = 0.07715155
Iteration 14, loss = 0.06870247
Iteration 15, loss = 0.06192387
Iteration 16, loss = 0.05669548
Iteration 17, loss = 0.05218272
Iteration 18, loss = 0.04850979
Iteration 19, loss = 0.04548692
Iteration 20, loss = 0.04272571
Iteration 21, loss = 0.04057981
Iteration 22, loss = 0.03876953
Iteration 23, loss = 0.03698930
Iteration 24, loss = 0.03566490
Iteration 25, loss = 0.03436828
Iteration 26, loss = 0.03356096
Iteration 27, loss = 0.03246797
Iteration 28, loss = 0.03140816
Iteration 29, loss = 0.03093688
Iteration 30, loss = 0.03009539
Iteration 31, loss = 0.02954848
Iteration 32, loss = 0.02884078
Iteration 33, loss = 0.02832467
Iteration 34, loss = 0.02801633
Iteration 35, loss = 0.02758535
Iteration 36, loss = 0.02720944
Iteration 37, loss = 0.02678665
Iteration 38, loss = 0.02669393
Iteration 39, loss = 0.02598712
Iteration 40, loss = 0.02623970
Iteration 41, loss = 0.02567743
Iteration 42, loss = 0.02554235
Iteration 43, loss = 0.02524303
Iteration 44, loss = 0.02507068
Iteration 45, loss = 0.02476770
Iteration 46, loss = 0.02465881
Iteration 47, loss = 0.02480197
Iteration 48, loss = 0.02433001
Iteration 49, loss = 0.02415788
Iteration 50, loss = 0.02418284
Iteration 51, loss = 0.02414227
Iteration 52, loss = 0.02390955
Iteration 53, loss = 0.02392449
Iteration 54, loss = 0.02351175
Iteration 55, loss = 0.02365069
Iteration 56, loss = 0.02355077
Iteration 57, loss = 0.02335178
Iteration 58, loss = 0.02325592
Iteration 59, loss = 0.02321159
Iteration 60, loss = 0.02315242
Iteration 61, loss = 0.02301015
Iteration 62, loss = 0.02303723
Iteration 63, loss = 0.02301974
Iteration 64, loss = 0.02279019
Iteration 65, loss = 0.02284971
Iteration 66, loss = 0.02272559
Iteration 67, loss = 0.02275039
Iteration 68, loss = 0.02253714
Iteration 69, loss = 0.02270317
Iteration 70, loss = 0.02258317
Iteration 71, loss = 0.02233099
Iteration 72, loss = 0.02253293
Iteration 73, loss = 0.02227949
Iteration 74, loss = 0.02235283
Iteration 75, loss = 0.02216949
Iteration 76, loss = 0.02243413
Iteration 77, loss = 0.02268606
Iteration 78, loss = 0.02244619
Iteration 79, loss = 0.02231881
Iteration 80, loss = 0.02221602
Iteration 81, loss = 0.02209007
Iteration 82, loss = 0.02216733
Iteration 83, loss = 0.02211833
Iteration 84, loss = 0.02213102
Iteration 85, loss = 0.02214957
Iteration 86, loss = 0.02207326
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71362945
Iteration 2, loss = 0.66788717
Iteration 3, loss = 0.60535559
Iteration 4, loss = 0.52609572
Iteration 5, loss = 0.44048361
Iteration 6, loss = 0.36007061
Iteration 7, loss = 0.29075729
Iteration 8, loss = 0.23534464
Iteration 9, loss = 0.19139010
Iteration 10, loss = 0.15828889
Iteration 11, loss = 0.13262334
Iteration 12, loss = 0.11316948
Iteration 13, loss = 0.09793086
Iteration 14, loss = 0.08598871
Iteration 15, loss = 0.07683246
Iteration 16, loss = 0.06890016
Iteration 17, loss = 0.06275670
Iteration 18, loss = 0.05762837
Iteration 19, loss = 0.05328947
Iteration 20, loss = 0.04963217
Iteration 21, loss = 0.04648911
Iteration 22, loss = 0.04393543
Iteration 23, loss = 0.04160994
Iteration 24, loss = 0.03983809
Iteration 25, loss = 0.03796724
Iteration 26, loss = 0.03639477
Iteration 27, loss = 0.03490356
Iteration 28, loss = 0.03380351
Iteration 29, loss = 0.03279322
Iteration 30, loss = 0.03188307
Iteration 31, loss = 0.03109897
Iteration 32, loss = 0.03038078
Iteration 33, loss = 0.02986741
Iteration 34, loss = 0.02894003
Iteration 35, loss = 0.02834296
Iteration 36, loss = 0.02768494
Iteration 37, loss = 0.02727971
Iteration 38, loss = 0.02687115
Iteration 39, loss = 0.02632449
Iteration 40, loss = 0.02609100
Iteration 41, loss = 0.02564947
Iteration 42, loss = 0.02521305
Iteration 43, loss = 0.02485821
Iteration 44, loss = 0.02472838
Iteration 45, loss = 0.02431973
Iteration 46, loss = 0.02412691
Iteration 47, loss = 0.02376168
Iteration 48, loss = 0.02372252
Iteration 49, loss = 0.02351990
Iteration 50, loss = 0.02331311
Iteration 51, loss = 0.02307476
Iteration 52, loss = 0.02318441
Iteration 53, loss = 0.02303157
Iteration 54, loss = 0.02264285
Iteration 55, loss = 0.02252012
Iteration 56, loss = 0.02246166
Iteration 57, loss = 0.02219502
Iteration 58, loss = 0.02231460
Iteration 59, loss = 0.02204209
Iteration 60, loss = 0.02217118
Iteration 61, loss = 0.02191161
Iteration 62, loss = 0.02174277
Iteration 63, loss = 0.02155372
Iteration 64, loss = 0.02155929
Iteration 65, loss = 0.02141130
Iteration 66, loss = 0.02140174
Iteration 67, loss = 0.02143786
Iteration 68, loss = 0.02149785
Iteration 69, loss = 0.02130218
Iteration 70, loss = 0.02121231
Iteration 71, loss = 0.02097797
Iteration 72, loss = 0.02101513
Iteration 73, loss = 0.02088483
Iteration 74, loss = 0.02110099
Iteration 75, loss = 0.02080135
Iteration 76, loss = 0.02089971
Iteration 77, loss = 0.02068627
Iteration 78, loss = 0.02064804
Iteration 79, loss = 0.02070588
Iteration 80, loss = 0.02065063
Iteration 81, loss = 0.02059081
Iteration 82, loss = 0.02053702
Iteration 83, loss = 0.02040811
Iteration 84, loss = 0.02040364
Iteration 85, loss = 0.02037929
Iteration 86, loss = 0.02084579
Iteration 87, loss = 0.02049002
Iteration 88, loss = 0.02027073
Iteration 89, loss = 0.02017873
Iteration 90, loss = 0.02024993
Iteration 91, loss = 0.02046771
Iteration 92, loss = 0.02014037
Iteration 93, loss = 0.01999767
Iteration 94, loss = 0.02019095
Iteration 95, loss = 0.01986683
Iteration 96, loss = 0.02020382
Iteration 97, loss = 0.02018927
Iteration 98, loss = 0.02023407
Iteration 99, loss = 0.02014893
Iteration 100, loss = 0.02033398
Iteration 101, loss = 0.02005089
Iteration 102, loss = 0.01991602
Iteration 103, loss = 0.01974620
Iteration 104, loss = 0.01987562
Iteration 105, loss = 0.01995187
Iteration 106, loss = 0.01964067
Iteration 107, loss = 0.01985358
Iteration 108, loss = 0.01958691
Iteration 109, loss = 0.01965342
Iteration 110, loss = 0.02019525
Iteration 111, loss = 0.02000536
Iteration 112, loss = 0.01963866
Iteration 113, loss = 0.02003216
Iteration 114, loss = 0.01972503
Iteration 115, loss = 0.02005326
Iteration 116, loss = 0.01989616
Iteration 117, loss = 0.01970290
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67382278
Iteration 2, loss = 0.66163330
Iteration 3, loss = 0.64912589
Iteration 4, loss = 0.63311577
Iteration 5, loss = 0.61250875
Iteration 6, loss = 0.58634304
Iteration 7, loss = 0.55562205
Iteration 8, loss = 0.52112104
Iteration 9, loss = 0.48430517
Iteration 10, loss = 0.44702848
Iteration 11, loss = 0.41022409
Iteration 12, loss = 0.37447377
Iteration 13, loss = 0.34077814
Iteration 14, loss = 0.30957900
Iteration 15, loss = 0.28080550
Iteration 16, loss = 0.25438889
Iteration 17, loss = 0.23039219
Iteration 18, loss = 0.20887823
Iteration 19, loss = 0.18977637
Iteration 20, loss = 0.17269182
Iteration 21, loss = 0.15730622
Iteration 22, loss = 0.14386954
Iteration 23, loss = 0.13189626
Iteration 24, loss = 0.12136910
Iteration 25, loss = 0.11190966
Iteration 26, loss = 0.10362571
Iteration 27, loss = 0.09619206
Iteration 28, loss = 0.08955070
Iteration 29, loss = 0.08369883
Iteration 30, loss = 0.07848132
Iteration 31, loss = 0.07366455
Iteration 32, loss = 0.06942200
Iteration 33, loss = 0.06559668
Iteration 34, loss = 0.06208138
Iteration 35, loss = 0.05896757
Iteration 36, loss = 0.05603531
Iteration 37, loss = 0.05345955
Iteration 38, loss = 0.05105583
Iteration 39, loss = 0.04883880
Iteration 40, loss = 0.04684156
Iteration 41, loss = 0.04502074
Iteration 42, loss = 0.04328011
Iteration 43, loss = 0.04163207
Iteration 44, loss = 0.04015595
Iteration 45, loss = 0.03888484
Iteration 46, loss = 0.03763201
Iteration 47, loss = 0.03635629
Iteration 48, loss = 0.03527013
Iteration 49, loss = 0.03415710
Iteration 50, loss = 0.03323719
Iteration 51, loss = 0.03234731
Iteration 52, loss = 0.03152607
Iteration 53, loss = 0.03070671
Iteration 54, loss = 0.02995722
Iteration 55, loss = 0.02923696
Iteration 56, loss = 0.02857941
Iteration 57, loss = 0.02795458
Iteration 58, loss = 0.02736916
Iteration 59, loss = 0.02699187
Iteration 60, loss = 0.02629319
Iteration 61, loss = 0.02574432
Iteration 62, loss = 0.02527402
Iteration 63, loss = 0.02483840
Iteration 64, loss = 0.02436207
Iteration 65, loss = 0.02400672
Iteration 66, loss = 0.02361591
Iteration 67, loss = 0.02320700
Iteration 68, loss = 0.02283362
Iteration 69, loss = 0.02250393
Iteration 70, loss = 0.02218868
Iteration 71, loss = 0.02187435
Iteration 72, loss = 0.02160816
Iteration 73, loss = 0.02129730
Iteration 74, loss = 0.02106989
Iteration 75, loss = 0.02083233
Iteration 76, loss = 0.02054482
Iteration 77, loss = 0.02027001
Iteration 78, loss = 0.02006657
Iteration 79, loss = 0.01993572
Iteration 80, loss = 0.01965458
Iteration 81, loss = 0.01949162
Iteration 82, loss = 0.01922768
Iteration 83, loss = 0.01908153
Iteration 84, loss = 0.01889504
Iteration 85, loss = 0.01880653
Iteration 86, loss = 0.01853566
Iteration 87, loss = 0.01836880
Iteration 88, loss = 0.01823321
Iteration 89, loss = 0.01812189
Iteration 90, loss = 0.01792716
Iteration 91, loss = 0.01779737
Iteration 92, loss = 0.01765225
Iteration 93, loss = 0.01749877
Iteration 94, loss = 0.01738475
Iteration 95, loss = 0.01724449
Iteration 96, loss = 0.01715542
Iteration 97, loss = 0.01701747
Iteration 98, loss = 0.01691206
Iteration 99, loss = 0.01678236
Iteration 100, loss = 0.01668590
Iteration 101, loss = 0.01659989
Iteration 102, loss = 0.01654344
Iteration 103, loss = 0.01640735
Iteration 104, loss = 0.01631566
Iteration 105, loss = 0.01619192
Iteration 106, loss = 0.01614398
Iteration 107, loss = 0.01604281
Iteration 108, loss = 0.01598472
Iteration 109, loss = 0.01590949
Iteration 110, loss = 0.01581236
Iteration 111, loss = 0.01574418
Iteration 112, loss = 0.01566934
Iteration 113, loss = 0.01557602
Iteration 114, loss = 0.01554699
Iteration 115, loss = 0.01546994
Iteration 116, loss = 0.01542885
Iteration 117, loss = 0.01534605
Iteration 118, loss = 0.01524711
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70398322
Iteration 2, loss = 0.64719149
Iteration 3, loss = 0.57138274
Iteration 4, loss = 0.48436674
Iteration 5, loss = 0.39956579
Iteration 6, loss = 0.32502698
Iteration 7, loss = 0.26242204
Iteration 8, loss = 0.21271666
Iteration 9, loss = 0.17400471
Iteration 10, loss = 0.14405811
Iteration 11, loss = 0.12102624
Iteration 12, loss = 0.10324743
Iteration 13, loss = 0.08956410
Iteration 14, loss = 0.07869965
Iteration 15, loss = 0.07004673
Iteration 16, loss = 0.06295962
Iteration 17, loss = 0.05727083
Iteration 18, loss = 0.05221447
Iteration 19, loss = 0.04827936
Iteration 20, loss = 0.04475620
Iteration 21, loss = 0.04194435
Iteration 22, loss = 0.03967715
Iteration 23, loss = 0.03756081
Iteration 24, loss = 0.03537468
Iteration 25, loss = 0.03378948
Iteration 26, loss = 0.03248718
Iteration 27, loss = 0.03117315
Iteration 28, loss = 0.02995862
Iteration 29, loss = 0.02898639
Iteration 30, loss = 0.02803632
Iteration 31, loss = 0.02727328
Iteration 32, loss = 0.02651938
Iteration 33, loss = 0.02591676
Iteration 34, loss = 0.02519570
Iteration 35, loss = 0.02469483
Iteration 36, loss = 0.02419940
Iteration 37, loss = 0.02366297
Iteration 38, loss = 0.02318209
Iteration 39, loss = 0.02298818
Iteration 40, loss = 0.02256717
Iteration 41, loss = 0.02221697
Iteration 42, loss = 0.02184089
Iteration 43, loss = 0.02156586
Iteration 44, loss = 0.02132970
Iteration 45, loss = 0.02116926
Iteration 46, loss = 0.02112580
Iteration 47, loss = 0.02068744
Iteration 48, loss = 0.02057831
Iteration 49, loss = 0.02027932
Iteration 50, loss = 0.02021336
Iteration 51, loss = 0.02024980
Iteration 52, loss = 0.01991923
Iteration 53, loss = 0.01963276
Iteration 54, loss = 0.01968310
Iteration 55, loss = 0.01937389
Iteration 56, loss = 0.01915833
Iteration 57, loss = 0.01900846
Iteration 58, loss = 0.01894134
Iteration 59, loss = 0.01908716
Iteration 60, loss = 0.01914894
Iteration 61, loss = 0.01863322
Iteration 62, loss = 0.01868107
Iteration 63, loss = 0.01852620
Iteration 64, loss = 0.01839568
Iteration 65, loss = 0.01828367
Iteration 66, loss = 0.01828090
Iteration 67, loss = 0.01831796
Iteration 68, loss = 0.01833839
Iteration 69, loss = 0.01825420
Iteration 70, loss = 0.01814413
Iteration 71, loss = 0.01794008
Iteration 72, loss = 0.01782562
Iteration 73, loss = 0.01804746
Iteration 74, loss = 0.01789280
Iteration 75, loss = 0.01782769
Iteration 76, loss = 0.01789071
Iteration 77, loss = 0.01802435
Iteration 78, loss = 0.01780558
Iteration 79, loss = 0.01765961
Iteration 80, loss = 0.01776597
Iteration 81, loss = 0.01731817
Iteration 82, loss = 0.01751608
Iteration 83, loss = 0.01758041
Iteration 84, loss = 0.01759383
Iteration 85, loss = 0.01762584
Iteration 86, loss = 0.01756238
Iteration 87, loss = 0.01757330
Iteration 88, loss = 0.01729199
Iteration 89, loss = 0.01745750
Iteration 90, loss = 0.01722719
Iteration 91, loss = 0.01750887
Iteration 92, loss = 0.01739435
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68057879
Iteration 2, loss = 0.63452354
Iteration 3, loss = 0.57182117
Iteration 4, loss = 0.49364625
Iteration 5, loss = 0.41297291
Iteration 6, loss = 0.33850737
Iteration 7, loss = 0.27476396
Iteration 8, loss = 0.22276402
Iteration 9, loss = 0.18225972
Iteration 10, loss = 0.15119064
Iteration 11, loss = 0.12736896
Iteration 12, loss = 0.10888841
Iteration 13, loss = 0.09465647
Iteration 14, loss = 0.08329304
Iteration 15, loss = 0.07447562
Iteration 16, loss = 0.06728731
Iteration 17, loss = 0.06119612
Iteration 18, loss = 0.05652695
Iteration 19, loss = 0.05235174
Iteration 20, loss = 0.04879643
Iteration 21, loss = 0.04613282
Iteration 22, loss = 0.04339345
Iteration 23, loss = 0.04121902
Iteration 24, loss = 0.03939458
Iteration 25, loss = 0.03802655
Iteration 26, loss = 0.03643884
Iteration 27, loss = 0.03524127
Iteration 28, loss = 0.03405355
Iteration 29, loss = 0.03343187
Iteration 30, loss = 0.03204328
Iteration 31, loss = 0.03138974
Iteration 32, loss = 0.03087752
Iteration 33, loss = 0.02980528
Iteration 34, loss = 0.02935070
Iteration 35, loss = 0.02870813
Iteration 36, loss = 0.02806685
Iteration 37, loss = 0.02763470
Iteration 38, loss = 0.02716481
Iteration 39, loss = 0.02696825
Iteration 40, loss = 0.02674163
Iteration 41, loss = 0.02625744
Iteration 42, loss = 0.02575750
Iteration 43, loss = 0.02562952
Iteration 44, loss = 0.02550078
Iteration 45, loss = 0.02495007
Iteration 46, loss = 0.02473509
Iteration 47, loss = 0.02459512
Iteration 48, loss = 0.02435830
Iteration 49, loss = 0.02419120
Iteration 50, loss = 0.02409497
Iteration 51, loss = 0.02410418
Iteration 52, loss = 0.02381648
Iteration 53, loss = 0.02359116
Iteration 54, loss = 0.02340590
Iteration 55, loss = 0.02335002
Iteration 56, loss = 0.02342740
Iteration 57, loss = 0.02296043
Iteration 58, loss = 0.02299955
Iteration 59, loss = 0.02292860
Iteration 60, loss = 0.02297655
Iteration 61, loss = 0.02263722
Iteration 62, loss = 0.02289129
Iteration 63, loss = 0.02258349
Iteration 64, loss = 0.02256332
Iteration 65, loss = 0.02269327
Iteration 66, loss = 0.02246851
Iteration 67, loss = 0.02227532
Iteration 68, loss = 0.02232152
Iteration 69, loss = 0.02219865
Iteration 70, loss = 0.02203349
Iteration 71, loss = 0.02208020
Iteration 72, loss = 0.02183106
Iteration 73, loss = 0.02170046
Iteration 74, loss = 0.02198371
Iteration 75, loss = 0.02172784
Iteration 76, loss = 0.02172064
Iteration 77, loss = 0.02160273
Iteration 78, loss = 0.02170039
Iteration 79, loss = 0.02185039
Iteration 80, loss = 0.02170412
Iteration 81, loss = 0.02158123
Iteration 82, loss = 0.02139245
Iteration 83, loss = 0.02132709
Iteration 84, loss = 0.02137957
Iteration 85, loss = 0.02152091
Iteration 86, loss = 0.02149087
Iteration 87, loss = 0.02127232
Iteration 88, loss = 0.02135563
Iteration 89, loss = 0.02133304
Iteration 90, loss = 0.02135350
Iteration 91, loss = 0.02121757
Iteration 92, loss = 0.02132271
Iteration 93, loss = 0.02161882
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67606312
Iteration 2, loss = 0.60436237
Iteration 3, loss = 0.49629376
Iteration 4, loss = 0.38628795
Iteration 5, loss = 0.29320411
Iteration 6, loss = 0.22205701
Iteration 7, loss = 0.17190014
Iteration 8, loss = 0.13641707
Iteration 9, loss = 0.11128495
Iteration 10, loss = 0.09330278
Iteration 11, loss = 0.08035641
Iteration 12, loss = 0.07062165
Iteration 13, loss = 0.06290668
Iteration 14, loss = 0.05716010
Iteration 15, loss = 0.05233661
Iteration 16, loss = 0.04860603
Iteration 17, loss = 0.04557496
Iteration 18, loss = 0.04277991
Iteration 19, loss = 0.04063806
Iteration 20, loss = 0.03854866
Iteration 21, loss = 0.03690161
Iteration 22, loss = 0.03576916
Iteration 23, loss = 0.03429817
Iteration 24, loss = 0.03366039
Iteration 25, loss = 0.03247092
Iteration 26, loss = 0.03163913
Iteration 27, loss = 0.03101159
Iteration 28, loss = 0.03043370
Iteration 29, loss = 0.02964545
Iteration 30, loss = 0.02940880
Iteration 31, loss = 0.02897287
Iteration 32, loss = 0.02839774
Iteration 33, loss = 0.02795974
Iteration 34, loss = 0.02768370
Iteration 35, loss = 0.02772770
Iteration 36, loss = 0.02696513
Iteration 37, loss = 0.02711056
Iteration 38, loss = 0.02670150
Iteration 39, loss = 0.02642023
Iteration 40, loss = 0.02612063
Iteration 41, loss = 0.02619880
Iteration 42, loss = 0.02602488
Iteration 43, loss = 0.02564135
Iteration 44, loss = 0.02534203
Iteration 45, loss = 0.02545843
Iteration 46, loss = 0.02516054
Iteration 47, loss = 0.02522555
Iteration 48, loss = 0.02557466
Iteration 49, loss = 0.02490317
Iteration 50, loss = 0.02465932
Iteration 51, loss = 0.02482436
Iteration 52, loss = 0.02483600
Iteration 53, loss = 0.02457815
Iteration 54, loss = 0.02445407
Iteration 55, loss = 0.02448663
Iteration 56, loss = 0.02462885
Iteration 57, loss = 0.02418320
Iteration 58, loss = 0.02418100
Iteration 59, loss = 0.02404159
Iteration 60, loss = 0.02405511
Iteration 61, loss = 0.02400771
Iteration 62, loss = 0.02399201
Iteration 63, loss = 0.02402481
Iteration 64, loss = 0.02386151
Iteration 65, loss = 0.02400253
Iteration 66, loss = 0.02391739
Iteration 67, loss = 0.02378865
Iteration 68, loss = 0.02401631
Iteration 69, loss = 0.02331372
Iteration 70, loss = 0.02369981
Iteration 71, loss = 0.02411299
Iteration 72, loss = 0.02355253
Iteration 73, loss = 0.02360632
Iteration 74, loss = 0.02360881
Iteration 75, loss = 0.02329528
Iteration 76, loss = 0.02347698
Iteration 77, loss = 0.02336975
Iteration 78, loss = 0.02340179
Iteration 79, loss = 0.02344548
Iteration 80, loss = 0.02340559
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67990039
Iteration 2, loss = 0.65361924
Iteration 3, loss = 0.62365451
Iteration 4, loss = 0.58545039
Iteration 5, loss = 0.53990959
Iteration 6, loss = 0.49074819
Iteration 7, loss = 0.44082188
Iteration 8, loss = 0.39270729
Iteration 9, loss = 0.34774729
Iteration 10, loss = 0.30667382
Iteration 11, loss = 0.27025372
Iteration 12, loss = 0.23812374
Iteration 13, loss = 0.21006842
Iteration 14, loss = 0.18595915
Iteration 15, loss = 0.16537006
Iteration 16, loss = 0.14759433
Iteration 17, loss = 0.13249381
Iteration 18, loss = 0.11952830
Iteration 19, loss = 0.10832211
Iteration 20, loss = 0.09886658
Iteration 21, loss = 0.09046845
Iteration 22, loss = 0.08329740
Iteration 23, loss = 0.07700235
Iteration 24, loss = 0.07148397
Iteration 25, loss = 0.06669698
Iteration 26, loss = 0.06226399
Iteration 27, loss = 0.05862883
Iteration 28, loss = 0.05537123
Iteration 29, loss = 0.05240676
Iteration 30, loss = 0.04976981
Iteration 31, loss = 0.04734972
Iteration 32, loss = 0.04518944
Iteration 33, loss = 0.04315576
Iteration 34, loss = 0.04131118
Iteration 35, loss = 0.03967684
Iteration 36, loss = 0.03823129
Iteration 37, loss = 0.03686130
Iteration 38, loss = 0.03564229
Iteration 39, loss = 0.03440052
Iteration 40, loss = 0.03326926
Iteration 41, loss = 0.03250397
Iteration 42, loss = 0.03148424
Iteration 43, loss = 0.03066374
Iteration 44, loss = 0.03003778
Iteration 45, loss = 0.02939599
Iteration 46, loss = 0.02866766
Iteration 47, loss = 0.02794455
Iteration 48, loss = 0.02731262
Iteration 49, loss = 0.02677192
Iteration 50, loss = 0.02633214
Iteration 51, loss = 0.02588932
Iteration 52, loss = 0.02564538
Iteration 53, loss = 0.02507015
Iteration 54, loss = 0.02473774
Iteration 55, loss = 0.02443551
Iteration 56, loss = 0.02401780
Iteration 57, loss = 0.02375765
Iteration 58, loss = 0.02325533
Iteration 59, loss = 0.02293760
Iteration 60, loss = 0.02262782
Iteration 61, loss = 0.02234236
Iteration 62, loss = 0.02209120
Iteration 63, loss = 0.02178474
Iteration 64, loss = 0.02172718
Iteration 65, loss = 0.02155948
Iteration 66, loss = 0.02117630
Iteration 67, loss = 0.02100593
Iteration 68, loss = 0.02087730
Iteration 69, loss = 0.02090362
Iteration 70, loss = 0.02078690
Iteration 71, loss = 0.02063535
Iteration 72, loss = 0.02036699
Iteration 73, loss = 0.02004989
Iteration 74, loss = 0.01995724
Iteration 75, loss = 0.01966788
Iteration 76, loss = 0.01981178
Iteration 77, loss = 0.01962025
Iteration 78, loss = 0.01965888
Iteration 79, loss = 0.01948400
Iteration 80, loss = 0.01928989
Iteration 81, loss = 0.01918359
Iteration 82, loss = 0.01904320
Iteration 83, loss = 0.01889476
Iteration 84, loss = 0.01877744
Iteration 85, loss = 0.01845405
Iteration 86, loss = 0.01832002
Iteration 87, loss = 0.01824732
Iteration 88, loss = 0.01816680
Iteration 89, loss = 0.01798398
Iteration 90, loss = 0.01823457
Iteration 91, loss = 0.01804954
Iteration 92, loss = 0.01804021
Iteration 93, loss = 0.01794099
Iteration 94, loss = 0.01783865
Iteration 95, loss = 0.01768181
Iteration 96, loss = 0.01787343
Iteration 97, loss = 0.01785523
Iteration 98, loss = 0.01777227
Iteration 99, loss = 0.01759967
Iteration 100, loss = 0.01753773
Iteration 101, loss = 0.01729851
Iteration 102, loss = 0.01733451
Iteration 103, loss = 0.01742578
Iteration 104, loss = 0.01732956
Iteration 105, loss = 0.01763449
Iteration 106, loss = 0.01814740
Iteration 107, loss = 0.01741658
Iteration 108, loss = 0.01738720
Iteration 109, loss = 0.01709860
Iteration 110, loss = 0.01705997
Iteration 111, loss = 0.01682646
Iteration 112, loss = 0.01689668
Iteration 113, loss = 0.01694075
Iteration 114, loss = 0.01711872
Iteration 115, loss = 0.01713028
Iteration 116, loss = 0.01691867
Iteration 117, loss = 0.01686902
Iteration 118, loss = 0.01672464
Iteration 119, loss = 0.01655251
Iteration 120, loss = 0.01664543
Iteration 121, loss = 0.01669833
Iteration 122, loss = 0.01696358
Iteration 123, loss = 0.01692900
Iteration 124, loss = 0.01647393
Iteration 125, loss = 0.01674939
Iteration 126, loss = 0.01672052
Iteration 127, loss = 0.01663343
Iteration 128, loss = 0.01651341
Iteration 129, loss = 0.01643697
Iteration 130, loss = 0.01636853
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71468586
Iteration 2, loss = 0.64143485
Iteration 3, loss = 0.55295170
Iteration 4, loss = 0.45315471
Iteration 5, loss = 0.35594906
Iteration 6, loss = 0.27640607
Iteration 7, loss = 0.21481415
Iteration 8, loss = 0.16965023
Iteration 9, loss = 0.13727076
Iteration 10, loss = 0.11453672
Iteration 11, loss = 0.09703888
Iteration 12, loss = 0.08419216
Iteration 13, loss = 0.07391849
Iteration 14, loss = 0.06612143
Iteration 15, loss = 0.05998514
Iteration 16, loss = 0.05486748
Iteration 17, loss = 0.05078218
Iteration 18, loss = 0.04735628
Iteration 19, loss = 0.04461171
Iteration 20, loss = 0.04204924
Iteration 21, loss = 0.04030771
Iteration 22, loss = 0.03864627
Iteration 23, loss = 0.03692796
Iteration 24, loss = 0.03553152
Iteration 25, loss = 0.03440055
Iteration 26, loss = 0.03315244
Iteration 27, loss = 0.03248649
Iteration 28, loss = 0.03190887
Iteration 29, loss = 0.03096071
Iteration 30, loss = 0.03043097
Iteration 31, loss = 0.02970040
Iteration 32, loss = 0.02913493
Iteration 33, loss = 0.02887574
Iteration 34, loss = 0.02799919
Iteration 35, loss = 0.02849654
Iteration 36, loss = 0.02818464
Iteration 37, loss = 0.02758529
Iteration 38, loss = 0.02719398
Iteration 39, loss = 0.02706352
Iteration 40, loss = 0.02661878
Iteration 41, loss = 0.02624460
Iteration 42, loss = 0.02604151
Iteration 43, loss = 0.02587003
Iteration 44, loss = 0.02575067
Iteration 45, loss = 0.02540771
Iteration 46, loss = 0.02507133
Iteration 47, loss = 0.02478196
Iteration 48, loss = 0.02474897
Iteration 49, loss = 0.02441617
Iteration 50, loss = 0.02432927
Iteration 51, loss = 0.02422906
Iteration 52, loss = 0.02411189
Iteration 53, loss = 0.02395399
Iteration 54, loss = 0.02374916
Iteration 55, loss = 0.02355267
Iteration 56, loss = 0.02347768
Iteration 57, loss = 0.02338502
Iteration 58, loss = 0.02334388
Iteration 59, loss = 0.02304556
Iteration 60, loss = 0.02338375
Iteration 61, loss = 0.02315320
Iteration 62, loss = 0.02286324
Iteration 63, loss = 0.02305488
Iteration 64, loss = 0.02281072
Iteration 65, loss = 0.02276888
Iteration 66, loss = 0.02278208
Iteration 67, loss = 0.02243800
Iteration 68, loss = 0.02248417
Iteration 69, loss = 0.02228781
Iteration 70, loss = 0.02231385
Iteration 71, loss = 0.02236396
Iteration 72, loss = 0.02220879
Iteration 73, loss = 0.02310787
Iteration 74, loss = 0.02463328
Iteration 75, loss = 0.02373530
Iteration 76, loss = 0.02317538
Iteration 77, loss = 0.02315854
Iteration 78, loss = 0.02298534
Iteration 79, loss = 0.02306598
Iteration 80, loss = 0.02260548
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67384701
Iteration 2, loss = 0.60384732
Iteration 3, loss = 0.50078085
Iteration 4, loss = 0.39115725
Iteration 5, loss = 0.29728060
Iteration 6, loss = 0.22539818
Iteration 7, loss = 0.17369829
Iteration 8, loss = 0.13742129
Iteration 9, loss = 0.11215396
Iteration 10, loss = 0.09403709
Iteration 11, loss = 0.08042931
Iteration 12, loss = 0.07083861
Iteration 13, loss = 0.06295929
Iteration 14, loss = 0.05699668
Iteration 15, loss = 0.05201909
Iteration 16, loss = 0.04829063
Iteration 17, loss = 0.04500200
Iteration 18, loss = 0.04271256
Iteration 19, loss = 0.04019994
Iteration 20, loss = 0.03822678
Iteration 21, loss = 0.03672718
Iteration 22, loss = 0.03510264
Iteration 23, loss = 0.03397730
Iteration 24, loss = 0.03281193
Iteration 25, loss = 0.03200763
Iteration 26, loss = 0.03125272
Iteration 27, loss = 0.03039916
Iteration 28, loss = 0.02984434
Iteration 29, loss = 0.02918469
Iteration 30, loss = 0.02875037
Iteration 31, loss = 0.02831331
Iteration 32, loss = 0.02787420
Iteration 33, loss = 0.02760528
Iteration 34, loss = 0.02685514
Iteration 35, loss = 0.02671968
Iteration 36, loss = 0.02651061
Iteration 37, loss = 0.02598248
Iteration 38, loss = 0.02589379
Iteration 39, loss = 0.02574039
Iteration 40, loss = 0.02566695
Iteration 41, loss = 0.02522810
Iteration 42, loss = 0.02498189
Iteration 43, loss = 0.02495329
Iteration 44, loss = 0.02469374
Iteration 45, loss = 0.02467948
Iteration 46, loss = 0.02456438
Iteration 47, loss = 0.02443523
Iteration 48, loss = 0.02445614
Iteration 49, loss = 0.02412907
Iteration 50, loss = 0.02393046
Iteration 51, loss = 0.02407639
Iteration 52, loss = 0.02387398
Iteration 53, loss = 0.02378103
Iteration 54, loss = 0.02397362
Iteration 55, loss = 0.02342065
Iteration 56, loss = 0.02367526
Iteration 57, loss = 0.02336412
Iteration 58, loss = 0.02367026
Iteration 59, loss = 0.02322309
Iteration 60, loss = 0.02341944
Iteration 61, loss = 0.02318021
Iteration 62, loss = 0.02309675
Iteration 63, loss = 0.02308680
Iteration 64, loss = 0.02306591
Iteration 65, loss = 0.02303199
Iteration 66, loss = 0.02289535
Iteration 67, loss = 0.02283709
Iteration 68, loss = 0.02268738
Iteration 69, loss = 0.02264485
Iteration 70, loss = 0.02276380
Iteration 71, loss = 0.02289318
Iteration 72, loss = 0.02275055
Iteration 73, loss = 0.02270938
Iteration 74, loss = 0.02237435
Iteration 75, loss = 0.02243622
Iteration 76, loss = 0.02240583
Iteration 77, loss = 0.02248518
Iteration 78, loss = 0.02266461
Iteration 79, loss = 0.02244565
Iteration 80, loss = 0.02250742
Iteration 81, loss = 0.02265361
Iteration 82, loss = 0.02319226
Iteration 83, loss = 0.02255257
Iteration 84, loss = 0.02246530
Iteration 85, loss = 0.02261861
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73203276
Iteration 2, loss = 0.70939003
Iteration 3, loss = 0.68839165
Iteration 4, loss = 0.66419272
Iteration 5, loss = 0.63623143
Iteration 6, loss = 0.60464362
Iteration 7, loss = 0.57032608
Iteration 8, loss = 0.53409402
Iteration 9, loss = 0.49695902
Iteration 10, loss = 0.45916493
Iteration 11, loss = 0.42219317
Iteration 12, loss = 0.38633787
Iteration 13, loss = 0.35233319
Iteration 14, loss = 0.32075949
Iteration 15, loss = 0.29162865
Iteration 16, loss = 0.26517917
Iteration 17, loss = 0.24113458
Iteration 18, loss = 0.21958878
Iteration 19, loss = 0.20025091
Iteration 20, loss = 0.18295611
Iteration 21, loss = 0.16750449
Iteration 22, loss = 0.15371551
Iteration 23, loss = 0.14144062
Iteration 24, loss = 0.13052140
Iteration 25, loss = 0.12073413
Iteration 26, loss = 0.11198982
Iteration 27, loss = 0.10417170
Iteration 28, loss = 0.09713044
Iteration 29, loss = 0.09083716
Iteration 30, loss = 0.08526321
Iteration 31, loss = 0.08003267
Iteration 32, loss = 0.07536097
Iteration 33, loss = 0.07116836
Iteration 34, loss = 0.06738754
Iteration 35, loss = 0.06382241
Iteration 36, loss = 0.06060570
Iteration 37, loss = 0.05776442
Iteration 38, loss = 0.05500679
Iteration 39, loss = 0.05253690
Iteration 40, loss = 0.05028816
Iteration 41, loss = 0.04820137
Iteration 42, loss = 0.04624215
Iteration 43, loss = 0.04443242
Iteration 44, loss = 0.04277999
Iteration 45, loss = 0.04123129
Iteration 46, loss = 0.03978952
Iteration 47, loss = 0.03841374
Iteration 48, loss = 0.03715735
Iteration 49, loss = 0.03599270
Iteration 50, loss = 0.03493469
Iteration 51, loss = 0.03390354
Iteration 52, loss = 0.03289911
Iteration 53, loss = 0.03192537
Iteration 54, loss = 0.03111094
Iteration 55, loss = 0.03029649
Iteration 56, loss = 0.02948852
Iteration 57, loss = 0.02883970
Iteration 58, loss = 0.02808412
Iteration 59, loss = 0.02741088
Iteration 60, loss = 0.02681388
Iteration 61, loss = 0.02622433
Iteration 62, loss = 0.02567577
Iteration 63, loss = 0.02516558
Iteration 64, loss = 0.02465687
Iteration 65, loss = 0.02417852
Iteration 66, loss = 0.02371892
Iteration 67, loss = 0.02329886
Iteration 68, loss = 0.02293290
Iteration 69, loss = 0.02248984
Iteration 70, loss = 0.02209866
Iteration 71, loss = 0.02177607
Iteration 72, loss = 0.02140492
Iteration 73, loss = 0.02111369
Iteration 74, loss = 0.02076320
Iteration 75, loss = 0.02044879
Iteration 76, loss = 0.02020547
Iteration 77, loss = 0.01992866
Iteration 78, loss = 0.01968096
Iteration 79, loss = 0.01938677
Iteration 80, loss = 0.01911394
Iteration 81, loss = 0.01890094
Iteration 82, loss = 0.01869272
Iteration 83, loss = 0.01845697
Iteration 84, loss = 0.01822021
Iteration 85, loss = 0.01803384
Iteration 86, loss = 0.01783631
Iteration 87, loss = 0.01764604
Iteration 88, loss = 0.01746515
Iteration 89, loss = 0.01728283
Iteration 90, loss = 0.01726685
Iteration 91, loss = 0.01698762
Iteration 92, loss = 0.01688165
Iteration 93, loss = 0.01664472
Iteration 94, loss = 0.01652460
Iteration 95, loss = 0.01635150
Iteration 96, loss = 0.01620044
Iteration 97, loss = 0.01613976
Iteration 98, loss = 0.01595474
Iteration 99, loss = 0.01584352
Iteration 100, loss = 0.01574073
Iteration 101, loss = 0.01561035
Iteration 102, loss = 0.01549519
Iteration 103, loss = 0.01546417
Iteration 104, loss = 0.01527876
Iteration 105, loss = 0.01515412
Iteration 106, loss = 0.01510428
Iteration 107, loss = 0.01503381
Iteration 108, loss = 0.01502456
Iteration 109, loss = 0.01479987
Iteration 110, loss = 0.01469514
Iteration 111, loss = 0.01467943
Iteration 112, loss = 0.01452322
Iteration 113, loss = 0.01441352
Iteration 114, loss = 0.01434467
Iteration 115, loss = 0.01424679
Iteration 116, loss = 0.01420592
Iteration 117, loss = 0.01414157
Iteration 118, loss = 0.01401891
Iteration 119, loss = 0.01395371
Iteration 120, loss = 0.01387508
Iteration 121, loss = 0.01381252
Iteration 122, loss = 0.01377692
Iteration 123, loss = 0.01372479
Iteration 124, loss = 0.01361144
Iteration 125, loss = 0.01359715
Iteration 126, loss = 0.01351786
Iteration 127, loss = 0.01346279
Iteration 128, loss = 0.01338985
Iteration 129, loss = 0.01337345
Iteration 130, loss = 0.01329701
Iteration 131, loss = 0.01327500
Iteration 132, loss = 0.01325358
Iteration 133, loss = 0.01315238
Iteration 134, loss = 0.01309189
Iteration 135, loss = 0.01309786
Iteration 136, loss = 0.01302976
Iteration 137, loss = 0.01292865
Iteration 138, loss = 0.01291915
Iteration 139, loss = 0.01292751
Iteration 140, loss = 0.01282479
Iteration 141, loss = 0.01283572
Iteration 142, loss = 0.01276554
Iteration 143, loss = 0.01270775
Iteration 144, loss = 0.01275604
Iteration 145, loss = 0.01266795
Iteration 146, loss = 0.01256275
Iteration 147, loss = 0.01266608
Iteration 148, loss = 0.01252503
Iteration 149, loss = 0.01250260
Iteration 150, loss = 0.01248282
Iteration 151, loss = 0.01239916
Iteration 152, loss = 0.01239328
Iteration 153, loss = 0.01234399
Iteration 154, loss = 0.01234970
Iteration 155, loss = 0.01230013
Iteration 156, loss = 0.01225935
Iteration 157, loss = 0.01222557
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71744440
Iteration 2, loss = 0.66384440
Iteration 3, loss = 0.58431512
Iteration 4, loss = 0.48218613
Iteration 5, loss = 0.37914504
Iteration 6, loss = 0.29133006
Iteration 7, loss = 0.22495712
Iteration 8, loss = 0.17579026
Iteration 9, loss = 0.14080888
Iteration 10, loss = 0.11587255
Iteration 11, loss = 0.09771260
Iteration 12, loss = 0.08378092
Iteration 13, loss = 0.07356368
Iteration 14, loss = 0.06549425
Iteration 15, loss = 0.05931629
Iteration 16, loss = 0.05406027
Iteration 17, loss = 0.04984136
Iteration 18, loss = 0.04643971
Iteration 19, loss = 0.04356988
Iteration 20, loss = 0.04122187
Iteration 21, loss = 0.03901732
Iteration 22, loss = 0.03723688
Iteration 23, loss = 0.03589871
Iteration 24, loss = 0.03436232
Iteration 25, loss = 0.03309712
Iteration 26, loss = 0.03227198
Iteration 27, loss = 0.03112018
Iteration 28, loss = 0.03030416
Iteration 29, loss = 0.02957476
Iteration 30, loss = 0.02905519
Iteration 31, loss = 0.02838306
Iteration 32, loss = 0.02772493
Iteration 33, loss = 0.02744380
Iteration 34, loss = 0.02687059
Iteration 35, loss = 0.02651897
Iteration 36, loss = 0.02614643
Iteration 37, loss = 0.02600336
Iteration 38, loss = 0.02559647
Iteration 39, loss = 0.02505100
Iteration 40, loss = 0.02481206
Iteration 41, loss = 0.02465558
Iteration 42, loss = 0.02500617
Iteration 43, loss = 0.02449225
Iteration 44, loss = 0.02414624
Iteration 45, loss = 0.02367476
Iteration 46, loss = 0.02357044
Iteration 47, loss = 0.02343246
Iteration 48, loss = 0.02360845
Iteration 49, loss = 0.02622696
Iteration 50, loss = 0.02613814
Iteration 51, loss = 0.02507363
Iteration 52, loss = 0.02423856
Iteration 53, loss = 0.02380529
Iteration 54, loss = 0.02352731
Iteration 55, loss = 0.02361297
Iteration 56, loss = 0.02323948
Iteration 57, loss = 0.02288837
Iteration 58, loss = 0.02290842
Iteration 59, loss = 0.02286792
Iteration 60, loss = 0.02273941
Iteration 61, loss = 0.02254963
Iteration 62, loss = 0.02227121
Iteration 63, loss = 0.02240655
Iteration 64, loss = 0.02207114
Iteration 65, loss = 0.02218519
Iteration 66, loss = 0.02179661
Iteration 67, loss = 0.02174641
Iteration 68, loss = 0.02175159
Iteration 69, loss = 0.02147500
Iteration 70, loss = 0.02146186
Iteration 71, loss = 0.02129825
Iteration 72, loss = 0.02125277
Iteration 73, loss = 0.02154512
Iteration 74, loss = 0.02151781
Iteration 75, loss = 0.02104387
Iteration 76, loss = 0.02096482
Iteration 77, loss = 0.02086025
Iteration 78, loss = 0.02102126
Iteration 79, loss = 0.02123072
Iteration 80, loss = 0.02080515
Iteration 81, loss = 0.02084499
Iteration 82, loss = 0.02084731
Iteration 83, loss = 0.02053374
Iteration 84, loss = 0.02083314
Iteration 85, loss = 0.02047799
Iteration 86, loss = 0.02065297
Iteration 87, loss = 0.02050102
Iteration 88, loss = 0.02042117
Iteration 89, loss = 0.02043389
Iteration 90, loss = 0.02049939
Iteration 91, loss = 0.02064124
Iteration 92, loss = 0.02076507
Iteration 93, loss = 0.02075923
Iteration 94, loss = 0.02024739
Iteration 95, loss = 0.02037552
Iteration 96, loss = 0.02010635
Iteration 97, loss = 0.02037437
Iteration 98, loss = 0.02159677
Iteration 99, loss = 0.02170541
Iteration 100, loss = 0.02123682
Iteration 101, loss = 0.02105081
Iteration 102, loss = 0.02124764
Iteration 103, loss = 0.02108812
Iteration 104, loss = 0.02101592
Iteration 105, loss = 0.02082578
Iteration 106, loss = 0.02114933
Iteration 107, loss = 0.02065483
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67720857
Iteration 2, loss = 0.62444008
Iteration 3, loss = 0.54164986
Iteration 4, loss = 0.44742679
Iteration 5, loss = 0.35658137
Iteration 6, loss = 0.27868305
Iteration 7, loss = 0.21785796
Iteration 8, loss = 0.17265407
Iteration 9, loss = 0.13981206
Iteration 10, loss = 0.11596107
Iteration 11, loss = 0.09850413
Iteration 12, loss = 0.08544281
Iteration 13, loss = 0.07562064
Iteration 14, loss = 0.06786335
Iteration 15, loss = 0.06169085
Iteration 16, loss = 0.05683633
Iteration 17, loss = 0.05285199
Iteration 18, loss = 0.05023331
Iteration 19, loss = 0.04751104
Iteration 20, loss = 0.04490118
Iteration 21, loss = 0.04311373
Iteration 22, loss = 0.04137080
Iteration 23, loss = 0.03953996
Iteration 24, loss = 0.03816229
Iteration 25, loss = 0.03729643
Iteration 26, loss = 0.03593636
Iteration 27, loss = 0.03515697
Iteration 28, loss = 0.03426094
Iteration 29, loss = 0.03356749
Iteration 30, loss = 0.03257097
Iteration 31, loss = 0.03205396
Iteration 32, loss = 0.03182723
Iteration 33, loss = 0.03125214
Iteration 34, loss = 0.03095612
Iteration 35, loss = 0.03033688
Iteration 36, loss = 0.02994979
Iteration 37, loss = 0.02945523
Iteration 38, loss = 0.02906487
Iteration 39, loss = 0.02888709
Iteration 40, loss = 0.02866612
Iteration 41, loss = 0.02809681
Iteration 42, loss = 0.02799138
Iteration 43, loss = 0.02901865
Iteration 44, loss = 0.02971151
Iteration 45, loss = 0.02876829
Iteration 46, loss = 0.02842384
Iteration 47, loss = 0.02795903
Iteration 48, loss = 0.02794113
Iteration 49, loss = 0.02763847
Iteration 50, loss = 0.02774278
Iteration 51, loss = 0.02743223
Iteration 52, loss = 0.02726756
Iteration 53, loss = 0.02693274
Iteration 54, loss = 0.02677806
Iteration 55, loss = 0.02661958
Iteration 56, loss = 0.02653311
Iteration 57, loss = 0.02656201
Iteration 58, loss = 0.02638603
Iteration 59, loss = 0.02600774
Iteration 60, loss = 0.02608911
Iteration 61, loss = 0.02589906
Iteration 62, loss = 0.02566636
Iteration 63, loss = 0.02578657
Iteration 64, loss = 0.02565239
Iteration 65, loss = 0.02559823
Iteration 66, loss = 0.02532848
Iteration 67, loss = 0.02548159
Iteration 68, loss = 0.02540126
Iteration 69, loss = 0.02525923
Iteration 70, loss = 0.02571975
Iteration 71, loss = 0.02585531
Iteration 72, loss = 0.02556702
Iteration 73, loss = 0.02555211
Iteration 74, loss = 0.02543289
Iteration 75, loss = 0.02525446
Iteration 76, loss = 0.02517832
Iteration 77, loss = 0.02503623
Iteration 78, loss = 0.02493125
Iteration 79, loss = 0.02467506
Iteration 80, loss = 0.02506046
Iteration 81, loss = 0.02435119
Iteration 82, loss = 0.02451207
Iteration 83, loss = 0.02426474
Iteration 84, loss = 0.02592339
Iteration 85, loss = 0.02573300
Iteration 86, loss = 0.02524298
Iteration 87, loss = 0.02551697
Iteration 88, loss = 0.02503417
Iteration 89, loss = 0.02499428
Iteration 90, loss = 0.02494059
Iteration 91, loss = 0.02486987
Iteration 92, loss = 0.02452972
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72072036
Iteration 2, loss = 0.68838955
Iteration 3, loss = 0.64147127
Iteration 4, loss = 0.57954619
Iteration 5, loss = 0.50707930
Iteration 6, loss = 0.43208089
Iteration 7, loss = 0.36058347
Iteration 8, loss = 0.29803206
Iteration 9, loss = 0.24550461
Iteration 10, loss = 0.20313282
Iteration 11, loss = 0.16929789
Iteration 12, loss = 0.14270442
Iteration 13, loss = 0.12173879
Iteration 14, loss = 0.10520733
Iteration 15, loss = 0.09202222
Iteration 16, loss = 0.08139669
Iteration 17, loss = 0.07281066
Iteration 18, loss = 0.06564943
Iteration 19, loss = 0.05977153
Iteration 20, loss = 0.05469386
Iteration 21, loss = 0.05061919
Iteration 22, loss = 0.04689899
Iteration 23, loss = 0.04380789
Iteration 24, loss = 0.04104164
Iteration 25, loss = 0.03876780
Iteration 26, loss = 0.03669967
Iteration 27, loss = 0.03507170
Iteration 28, loss = 0.03343880
Iteration 29, loss = 0.03206654
Iteration 30, loss = 0.03063420
Iteration 31, loss = 0.02940299
Iteration 32, loss = 0.02833709
Iteration 33, loss = 0.02747885
Iteration 34, loss = 0.02671072
Iteration 35, loss = 0.02593199
Iteration 36, loss = 0.02530632
Iteration 37, loss = 0.02461092
Iteration 38, loss = 0.02399499
Iteration 39, loss = 0.02337984
Iteration 40, loss = 0.02309936
Iteration 41, loss = 0.02245120
Iteration 42, loss = 0.02203990
Iteration 43, loss = 0.02169411
Iteration 44, loss = 0.02129371
Iteration 45, loss = 0.02111699
Iteration 46, loss = 0.02077250
Iteration 47, loss = 0.02037341
Iteration 48, loss = 0.02023176
Iteration 49, loss = 0.01983948
Iteration 50, loss = 0.01956381
Iteration 51, loss = 0.01940571
Iteration 52, loss = 0.01931695
Iteration 53, loss = 0.01888942
Iteration 54, loss = 0.01880155
Iteration 55, loss = 0.01858715
Iteration 56, loss = 0.01859956
Iteration 57, loss = 0.01826456
Iteration 58, loss = 0.01829975
Iteration 59, loss = 0.01794402
Iteration 60, loss = 0.01782252
Iteration 61, loss = 0.01790324
Iteration 62, loss = 0.01748833
Iteration 63, loss = 0.01758564
Iteration 64, loss = 0.01733878
Iteration 65, loss = 0.01711734
Iteration 66, loss = 0.01708062
Iteration 67, loss = 0.01729108
Iteration 68, loss = 0.01691439
Iteration 69, loss = 0.01676292
Iteration 70, loss = 0.01674669
Iteration 71, loss = 0.01670256
Iteration 72, loss = 0.01665410
Iteration 73, loss = 0.01650830
Iteration 74, loss = 0.01655089
Iteration 75, loss = 0.01649480
Iteration 76, loss = 0.01632696
Iteration 77, loss = 0.01628027
Iteration 78, loss = 0.01614595
Iteration 79, loss = 0.01616717
Iteration 80, loss = 0.01610585
Iteration 81, loss = 0.01603377
Iteration 82, loss = 0.01596464
Iteration 83, loss = 0.01585811
Iteration 84, loss = 0.01598501
Iteration 85, loss = 0.01589011
Iteration 86, loss = 0.01581790
Iteration 87, loss = 0.01574183
Iteration 88, loss = 0.01572678
Iteration 89, loss = 0.01560617
Iteration 90, loss = 0.01560695
Iteration 91, loss = 0.01558013
Iteration 92, loss = 0.01560102
Iteration 93, loss = 0.01551717
Iteration 94, loss = 0.01533045
Iteration 95, loss = 0.01544586
Iteration 96, loss = 0.01539417
Iteration 97, loss = 0.01539784
Iteration 98, loss = 0.01540800
Iteration 99, loss = 0.01549019
Iteration 100, loss = 0.01548933
Iteration 101, loss = 0.01523264
Iteration 102, loss = 0.01533700
Iteration 103, loss = 0.01534337
Iteration 104, loss = 0.01511063
Iteration 105, loss = 0.01519347
Iteration 106, loss = 0.01517022
Iteration 107, loss = 0.01516662
Iteration 108, loss = 0.01503656
Iteration 109, loss = 0.01507721
Iteration 110, loss = 0.01497433
Iteration 111, loss = 0.01514551
Iteration 112, loss = 0.01524922
Iteration 113, loss = 0.01486216
Iteration 114, loss = 0.01487209
Iteration 115, loss = 0.01484180
Iteration 116, loss = 0.01499546
Iteration 117, loss = 0.01486442
Iteration 118, loss = 0.01495380
Iteration 119, loss = 0.01497335
Iteration 120, loss = 0.01488512
Iteration 121, loss = 0.01491708
Iteration 122, loss = 0.01502975
Iteration 123, loss = 0.01482532
Iteration 124, loss = 0.01489769
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68641170
Iteration 2, loss = 0.64570941
Iteration 3, loss = 0.58594562
Iteration 4, loss = 0.50830418
Iteration 5, loss = 0.42690641
Iteration 6, loss = 0.35057918
Iteration 7, loss = 0.28411020
Iteration 8, loss = 0.22999489
Iteration 9, loss = 0.18694217
Iteration 10, loss = 0.15416228
Iteration 11, loss = 0.12894784
Iteration 12, loss = 0.10960539
Iteration 13, loss = 0.09462247
Iteration 14, loss = 0.08281685
Iteration 15, loss = 0.07339153
Iteration 16, loss = 0.06569635
Iteration 17, loss = 0.05953512
Iteration 18, loss = 0.05439792
Iteration 19, loss = 0.05024868
Iteration 20, loss = 0.04653942
Iteration 21, loss = 0.04342963
Iteration 22, loss = 0.04082467
Iteration 23, loss = 0.03875468
Iteration 24, loss = 0.03662714
Iteration 25, loss = 0.03477698
Iteration 26, loss = 0.03334119
Iteration 27, loss = 0.03180736
Iteration 28, loss = 0.03064340
Iteration 29, loss = 0.02960697
Iteration 30, loss = 0.02867326
Iteration 31, loss = 0.02782227
Iteration 32, loss = 0.02691027
Iteration 33, loss = 0.02629787
Iteration 34, loss = 0.02574100
Iteration 35, loss = 0.02524774
Iteration 36, loss = 0.02487660
Iteration 37, loss = 0.02415706
Iteration 38, loss = 0.02368593
Iteration 39, loss = 0.02331152
Iteration 40, loss = 0.02286176
Iteration 41, loss = 0.02252796
Iteration 42, loss = 0.02213151
Iteration 43, loss = 0.02199900
Iteration 44, loss = 0.02174633
Iteration 45, loss = 0.02138292
Iteration 46, loss = 0.02118101
Iteration 47, loss = 0.02094710
Iteration 48, loss = 0.02068747
Iteration 49, loss = 0.02034562
Iteration 50, loss = 0.02036082
Iteration 51, loss = 0.02008779
Iteration 52, loss = 0.01980153
Iteration 53, loss = 0.01971783
Iteration 54, loss = 0.01958300
Iteration 55, loss = 0.01949942
Iteration 56, loss = 0.01931604
Iteration 57, loss = 0.01919035
Iteration 58, loss = 0.01901499
Iteration 59, loss = 0.01888624
Iteration 60, loss = 0.01886311
Iteration 61, loss = 0.01875266
Iteration 62, loss = 0.01877511
Iteration 63, loss = 0.01867232
Iteration 64, loss = 0.01847016
Iteration 65, loss = 0.01841880
Iteration 66, loss = 0.01848157
Iteration 67, loss = 0.01825400
Iteration 68, loss = 0.01828272
Iteration 69, loss = 0.01809536
Iteration 70, loss = 0.01814896
Iteration 71, loss = 0.01799202
Iteration 72, loss = 0.01799830
Iteration 73, loss = 0.01795366
Iteration 74, loss = 0.01798019
Iteration 75, loss = 0.01778706
Iteration 76, loss = 0.01773178
Iteration 77, loss = 0.01787766
Iteration 78, loss = 0.01745641
Iteration 79, loss = 0.01778104
Iteration 80, loss = 0.01761545
Iteration 81, loss = 0.01750299
Iteration 82, loss = 0.01742021
Iteration 83, loss = 0.01742881
Iteration 84, loss = 0.01743669
Iteration 85, loss = 0.01705101
Iteration 86, loss = 0.01732008
Iteration 87, loss = 0.01753927
Iteration 88, loss = 0.01728470
Iteration 89, loss = 0.01747129
Iteration 90, loss = 0.01769498
Iteration 91, loss = 0.01715238
Iteration 92, loss = 0.01704964
Iteration 93, loss = 0.01711669
Iteration 94, loss = 0.01704338
Iteration 95, loss = 0.01739357
Iteration 96, loss = 0.01706623
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67630423
Iteration 2, loss = 0.62368095
Iteration 3, loss = 0.54773151
Iteration 4, loss = 0.46115159
Iteration 5, loss = 0.37586500
Iteration 6, loss = 0.29995024
Iteration 7, loss = 0.23802338
Iteration 8, loss = 0.19004329
Iteration 9, loss = 0.15370361
Iteration 10, loss = 0.12681760
Iteration 11, loss = 0.10649755
Iteration 12, loss = 0.09146772
Iteration 13, loss = 0.07968460
Iteration 14, loss = 0.07038722
Iteration 15, loss = 0.06307687
Iteration 16, loss = 0.05707240
Iteration 17, loss = 0.05230517
Iteration 18, loss = 0.04857147
Iteration 19, loss = 0.04500547
Iteration 20, loss = 0.04199615
Iteration 21, loss = 0.03956017
Iteration 22, loss = 0.03778667
Iteration 23, loss = 0.03586125
Iteration 24, loss = 0.03425612
Iteration 25, loss = 0.03284460
Iteration 26, loss = 0.03178182
Iteration 27, loss = 0.03067897
Iteration 28, loss = 0.02974109
Iteration 29, loss = 0.02892505
Iteration 30, loss = 0.02826905
Iteration 31, loss = 0.02745207
Iteration 32, loss = 0.02698350
Iteration 33, loss = 0.02650588
Iteration 34, loss = 0.02589309
Iteration 35, loss = 0.02542563
Iteration 36, loss = 0.02508788
Iteration 37, loss = 0.02443449
Iteration 38, loss = 0.02427606
Iteration 39, loss = 0.02422519
Iteration 40, loss = 0.02372015
Iteration 41, loss = 0.02331251
Iteration 42, loss = 0.02320359
Iteration 43, loss = 0.02273707
Iteration 44, loss = 0.02281553
Iteration 45, loss = 0.02250563
Iteration 46, loss = 0.02219424
Iteration 47, loss = 0.02232274
Iteration 48, loss = 0.02195139
Iteration 49, loss = 0.02178522
Iteration 50, loss = 0.02165334
Iteration 51, loss = 0.02141589
Iteration 52, loss = 0.02156044
Iteration 53, loss = 0.02122614
Iteration 54, loss = 0.02113783
Iteration 55, loss = 0.02108580
Iteration 56, loss = 0.02115444
Iteration 57, loss = 0.02085925
Iteration 58, loss = 0.02046539
Iteration 59, loss = 0.02060899
Iteration 60, loss = 0.02051703
Iteration 61, loss = 0.02066993
Iteration 62, loss = 0.02028399
Iteration 63, loss = 0.02042414
Iteration 64, loss = 0.02022539
Iteration 65, loss = 0.02012852
Iteration 66, loss = 0.02009278
Iteration 67, loss = 0.01992565
Iteration 68, loss = 0.01990441
Iteration 69, loss = 0.01992626
Iteration 70, loss = 0.01972752
Iteration 71, loss = 0.01991828
Iteration 72, loss = 0.01992076
Iteration 73, loss = 0.01982936
Iteration 74, loss = 0.01976827
Iteration 75, loss = 0.01981890
Iteration 76, loss = 0.01951013
Iteration 77, loss = 0.01972625
Iteration 78, loss = 0.01964437
Iteration 79, loss = 0.01950384
Iteration 80, loss = 0.01940834
Iteration 81, loss = 0.01955451
Iteration 82, loss = 0.01964894
Iteration 83, loss = 0.01951268
Iteration 84, loss = 0.01936843
Iteration 85, loss = 0.01925806
Iteration 86, loss = 0.01941799
Iteration 87, loss = 0.01927448
Iteration 88, loss = 0.01935604
Iteration 89, loss = 0.01926958
Iteration 90, loss = 0.01952980
Iteration 91, loss = 0.01938957
Iteration 92, loss = 0.01923130
Iteration 93, loss = 0.01919715
Iteration 94, loss = 0.01921395
Iteration 95, loss = 0.01925845
Iteration 96, loss = 0.01925222
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67445161
Iteration 2, loss = 0.60493125
Iteration 3, loss = 0.49922063
Iteration 4, loss = 0.38940362
Iteration 5, loss = 0.29149273
Iteration 6, loss = 0.21865304
Iteration 7, loss = 0.16681018
Iteration 8, loss = 0.13188259
Iteration 9, loss = 0.10748897
Iteration 10, loss = 0.09032374
Iteration 11, loss = 0.07746035
Iteration 12, loss = 0.06833552
Iteration 13, loss = 0.06114615
Iteration 14, loss = 0.05560371
Iteration 15, loss = 0.05110309
Iteration 16, loss = 0.04767235
Iteration 17, loss = 0.04455093
Iteration 18, loss = 0.04204715
Iteration 19, loss = 0.03997549
Iteration 20, loss = 0.03845714
Iteration 21, loss = 0.03700746
Iteration 22, loss = 0.03560248
Iteration 23, loss = 0.03479581
Iteration 24, loss = 0.03340833
Iteration 25, loss = 0.03253842
Iteration 26, loss = 0.03182669
Iteration 27, loss = 0.03121694
Iteration 28, loss = 0.03079994
Iteration 29, loss = 0.03015695
Iteration 30, loss = 0.02976948
Iteration 31, loss = 0.02931994
Iteration 32, loss = 0.02898095
Iteration 33, loss = 0.02873362
Iteration 34, loss = 0.02823461
Iteration 35, loss = 0.02780820
Iteration 36, loss = 0.02751219
Iteration 37, loss = 0.02775765
Iteration 38, loss = 0.02735130
Iteration 39, loss = 0.02708643
Iteration 40, loss = 0.02699037
Iteration 41, loss = 0.02667709
Iteration 42, loss = 0.02648857
Iteration 43, loss = 0.02637070
Iteration 44, loss = 0.02610956
Iteration 45, loss = 0.02606354
Iteration 46, loss = 0.02575951
Iteration 47, loss = 0.02568791
Iteration 48, loss = 0.02570235
Iteration 49, loss = 0.02585590
Iteration 50, loss = 0.02607123
Iteration 51, loss = 0.02539777
Iteration 52, loss = 0.02546790
Iteration 53, loss = 0.02532471
Iteration 54, loss = 0.02546113
Iteration 55, loss = 0.02530626
Iteration 56, loss = 0.02506329
Iteration 57, loss = 0.02499907
Iteration 58, loss = 0.02509928
Iteration 59, loss = 0.02505561
Iteration 60, loss = 0.02494620
Iteration 61, loss = 0.02506289
Iteration 62, loss = 0.02480351
Iteration 63, loss = 0.02483093
Iteration 64, loss = 0.02465390
Iteration 65, loss = 0.02457489
Iteration 66, loss = 0.02482621
Iteration 67, loss = 0.02462514
Iteration 68, loss = 0.02469843
Iteration 69, loss = 0.02455381
Iteration 70, loss = 0.02441403
Iteration 71, loss = 0.02447787
Iteration 72, loss = 0.02458195
Iteration 73, loss = 0.02447503
Iteration 74, loss = 0.02460127
Iteration 75, loss = 0.02476505
Iteration 76, loss = 0.02470579
Iteration 77, loss = 0.02450978
Iteration 78, loss = 0.02462227
Iteration 79, loss = 0.02414786
Iteration 80, loss = 0.02407267
Iteration 81, loss = 0.02419734
Iteration 82, loss = 0.02444721
Iteration 83, loss = 0.02416833
Iteration 84, loss = 0.02422419
Iteration 85, loss = 0.02439097
Iteration 86, loss = 0.02399159
Iteration 87, loss = 0.02445058
Iteration 88, loss = 0.02444588
Iteration 89, loss = 0.02389238
Iteration 90, loss = 0.02411081
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70435501
Iteration 2, loss = 0.62728183
Iteration 3, loss = 0.51437906
Iteration 4, loss = 0.39247408
Iteration 5, loss = 0.29145794
Iteration 6, loss = 0.21695364
Iteration 7, loss = 0.16554978
Iteration 8, loss = 0.13067595
Iteration 9, loss = 0.10626822
Iteration 10, loss = 0.08914057
Iteration 11, loss = 0.07685323
Iteration 12, loss = 0.06756219
Iteration 13, loss = 0.06042668
Iteration 14, loss = 0.05483094
Iteration 15, loss = 0.05021856
Iteration 16, loss = 0.04677476
Iteration 17, loss = 0.04376692
Iteration 18, loss = 0.04148947
Iteration 19, loss = 0.03935313
Iteration 20, loss = 0.03754967
Iteration 21, loss = 0.03601185
Iteration 22, loss = 0.03476223
Iteration 23, loss = 0.03362978
Iteration 24, loss = 0.03281631
Iteration 25, loss = 0.03197180
Iteration 26, loss = 0.03120172
Iteration 27, loss = 0.03034537
Iteration 28, loss = 0.02993750
Iteration 29, loss = 0.02951960
Iteration 30, loss = 0.02895282
Iteration 31, loss = 0.02871308
Iteration 32, loss = 0.02782143
Iteration 33, loss = 0.02762572
Iteration 34, loss = 0.02769134
Iteration 35, loss = 0.02717225
Iteration 36, loss = 0.02686616
Iteration 37, loss = 0.02641456
Iteration 38, loss = 0.02621393
Iteration 39, loss = 0.02628956
Iteration 40, loss = 0.02631816
Iteration 41, loss = 0.02599454
Iteration 42, loss = 0.02577782
Iteration 43, loss = 0.02556370
Iteration 44, loss = 0.02537917
Iteration 45, loss = 0.02538006
Iteration 46, loss = 0.02502907
Iteration 47, loss = 0.02509117
Iteration 48, loss = 0.02510517
Iteration 49, loss = 0.02466405
Iteration 50, loss = 0.02475366
Iteration 51, loss = 0.02464314
Iteration 52, loss = 0.02467268
Iteration 53, loss = 0.02446797
Iteration 54, loss = 0.02427843
Iteration 55, loss = 0.02437354
Iteration 56, loss = 0.02432166
Iteration 57, loss = 0.02432493
Iteration 58, loss = 0.02397981
Iteration 59, loss = 0.02426657
Iteration 60, loss = 0.02412190
Iteration 61, loss = 0.02415037
Iteration 62, loss = 0.02397253
Iteration 63, loss = 0.02427533
Iteration 64, loss = 0.02373623
Iteration 65, loss = 0.02392321
Iteration 66, loss = 0.02353754
Iteration 67, loss = 0.02400496
Iteration 68, loss = 0.02393141
Iteration 69, loss = 0.02366927
Iteration 70, loss = 0.02349025
Iteration 71, loss = 0.02368599
Iteration 72, loss = 0.02367690
Iteration 73, loss = 0.02356853
Iteration 74, loss = 0.02348152
Iteration 75, loss = 0.02340327
Iteration 76, loss = 0.02347401
Iteration 77, loss = 0.02331509
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67854111
Iteration 2, loss = 0.65914523
Iteration 3, loss = 0.63978600
Iteration 4, loss = 0.61710265
Iteration 5, loss = 0.59036625
Iteration 6, loss = 0.55980554
Iteration 7, loss = 0.52593436
Iteration 8, loss = 0.48981501
Iteration 9, loss = 0.45326151
Iteration 10, loss = 0.41691652
Iteration 11, loss = 0.38145364
Iteration 12, loss = 0.34805469
Iteration 13, loss = 0.31647610
Iteration 14, loss = 0.28716513
Iteration 15, loss = 0.26025366
Iteration 16, loss = 0.23569981
Iteration 17, loss = 0.21357928
Iteration 18, loss = 0.19359439
Iteration 19, loss = 0.17581788
Iteration 20, loss = 0.15998327
Iteration 21, loss = 0.14583744
Iteration 22, loss = 0.13339205
Iteration 23, loss = 0.12226608
Iteration 24, loss = 0.11243162
Iteration 25, loss = 0.10366479
Iteration 26, loss = 0.09592364
Iteration 27, loss = 0.08901720
Iteration 28, loss = 0.08283589
Iteration 29, loss = 0.07731798
Iteration 30, loss = 0.07239336
Iteration 31, loss = 0.06799480
Iteration 32, loss = 0.06395041
Iteration 33, loss = 0.06040893
Iteration 34, loss = 0.05708010
Iteration 35, loss = 0.05414174
Iteration 36, loss = 0.05141845
Iteration 37, loss = 0.04892492
Iteration 38, loss = 0.04670017
Iteration 39, loss = 0.04460865
Iteration 40, loss = 0.04270260
Iteration 41, loss = 0.04092874
Iteration 42, loss = 0.03933514
Iteration 43, loss = 0.03783093
Iteration 44, loss = 0.03648304
Iteration 45, loss = 0.03516342
Iteration 46, loss = 0.03398503
Iteration 47, loss = 0.03287300
Iteration 48, loss = 0.03183302
Iteration 49, loss = 0.03084197
Iteration 50, loss = 0.03000970
Iteration 51, loss = 0.02913025
Iteration 52, loss = 0.02831229
Iteration 53, loss = 0.02763177
Iteration 54, loss = 0.02690547
Iteration 55, loss = 0.02620337
Iteration 56, loss = 0.02563695
Iteration 57, loss = 0.02506833
Iteration 58, loss = 0.02444482
Iteration 59, loss = 0.02398316
Iteration 60, loss = 0.02339931
Iteration 61, loss = 0.02292680
Iteration 62, loss = 0.02251841
Iteration 63, loss = 0.02206174
Iteration 64, loss = 0.02165507
Iteration 65, loss = 0.02129154
Iteration 66, loss = 0.02094432
Iteration 67, loss = 0.02060251
Iteration 68, loss = 0.02023086
Iteration 69, loss = 0.01994036
Iteration 70, loss = 0.01965830
Iteration 71, loss = 0.01933943
Iteration 72, loss = 0.01908221
Iteration 73, loss = 0.01877989
Iteration 74, loss = 0.01857723
Iteration 75, loss = 0.01835837
Iteration 76, loss = 0.01813924
Iteration 77, loss = 0.01785006
Iteration 78, loss = 0.01767852
Iteration 79, loss = 0.01748339
Iteration 80, loss = 0.01726611
Iteration 81, loss = 0.01705605
Iteration 82, loss = 0.01687107
Iteration 83, loss = 0.01672431
Iteration 84, loss = 0.01659029
Iteration 85, loss = 0.01638550
Iteration 86, loss = 0.01622097
Iteration 87, loss = 0.01606654
Iteration 88, loss = 0.01593840
Iteration 89, loss = 0.01577938
Iteration 90, loss = 0.01568629
Iteration 91, loss = 0.01559831
Iteration 92, loss = 0.01540070
Iteration 93, loss = 0.01533188
Iteration 94, loss = 0.01517620
Iteration 95, loss = 0.01505721
Iteration 96, loss = 0.01493559
Iteration 97, loss = 0.01487299
Iteration 98, loss = 0.01473487
Iteration 99, loss = 0.01471322
Iteration 100, loss = 0.01453253
Iteration 101, loss = 0.01444256
Iteration 102, loss = 0.01438422
Iteration 103, loss = 0.01436315
Iteration 104, loss = 0.01422009
Iteration 105, loss = 0.01412699
Iteration 106, loss = 0.01405270
Iteration 107, loss = 0.01400070
Iteration 108, loss = 0.01396999
Iteration 109, loss = 0.01385766
Iteration 110, loss = 0.01378394
Iteration 111, loss = 0.01366511
Iteration 112, loss = 0.01363147
Iteration 113, loss = 0.01355103
Iteration 114, loss = 0.01345953
Iteration 115, loss = 0.01342507
Iteration 116, loss = 0.01334548
Iteration 117, loss = 0.01330665
Iteration 118, loss = 0.01324679
Iteration 119, loss = 0.01319412
Iteration 120, loss = 0.01314459
Iteration 121, loss = 0.01308172
Iteration 122, loss = 0.01302580
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67763225
Iteration 2, loss = 0.63380573
Iteration 3, loss = 0.58049699
Iteration 4, loss = 0.51469871
Iteration 5, loss = 0.44392835
Iteration 6, loss = 0.37655038
Iteration 7, loss = 0.31573172
Iteration 8, loss = 0.26302774
Iteration 9, loss = 0.21881475
Iteration 10, loss = 0.18282056
Iteration 11, loss = 0.15404366
Iteration 12, loss = 0.13128993
Iteration 13, loss = 0.11286454
Iteration 14, loss = 0.09822494
Iteration 15, loss = 0.08644287
Iteration 16, loss = 0.07699175
Iteration 17, loss = 0.06896002
Iteration 18, loss = 0.06255001
Iteration 19, loss = 0.05708445
Iteration 20, loss = 0.05259443
Iteration 21, loss = 0.04857280
Iteration 22, loss = 0.04517657
Iteration 23, loss = 0.04241197
Iteration 24, loss = 0.03986222
Iteration 25, loss = 0.03774997
Iteration 26, loss = 0.03568429
Iteration 27, loss = 0.03430784
Iteration 28, loss = 0.03260760
Iteration 29, loss = 0.03118056
Iteration 30, loss = 0.03003160
Iteration 31, loss = 0.02898396
Iteration 32, loss = 0.02810826
Iteration 33, loss = 0.02727335
Iteration 34, loss = 0.02651518
Iteration 35, loss = 0.02553946
Iteration 36, loss = 0.02505968
Iteration 37, loss = 0.02436189
Iteration 38, loss = 0.02394064
Iteration 39, loss = 0.02324560
Iteration 40, loss = 0.02285926
Iteration 41, loss = 0.02245535
Iteration 42, loss = 0.02211478
Iteration 43, loss = 0.02163711
Iteration 44, loss = 0.02119731
Iteration 45, loss = 0.02099893
Iteration 46, loss = 0.02072043
Iteration 47, loss = 0.02047637
Iteration 48, loss = 0.02014795
Iteration 49, loss = 0.01992754
Iteration 50, loss = 0.01963255
Iteration 51, loss = 0.01952276
Iteration 52, loss = 0.01931090
Iteration 53, loss = 0.01915854
Iteration 54, loss = 0.01897172
Iteration 55, loss = 0.01873110
Iteration 56, loss = 0.01859852
Iteration 57, loss = 0.01846846
Iteration 58, loss = 0.01835907
Iteration 59, loss = 0.01822852
Iteration 60, loss = 0.01803458
Iteration 61, loss = 0.01799703
Iteration 62, loss = 0.01781027
Iteration 63, loss = 0.01769988
Iteration 64, loss = 0.01755328
Iteration 65, loss = 0.01761504
Iteration 66, loss = 0.01735676
Iteration 67, loss = 0.01727279
Iteration 68, loss = 0.01733610
Iteration 69, loss = 0.01718006
Iteration 70, loss = 0.01715735
Iteration 71, loss = 0.01705958
Iteration 72, loss = 0.01695711
Iteration 73, loss = 0.01693123
Iteration 74, loss = 0.01688283
Iteration 75, loss = 0.01692946
Iteration 76, loss = 0.01672985
Iteration 77, loss = 0.01664828
Iteration 78, loss = 0.01667470
Iteration 79, loss = 0.01643575
Iteration 80, loss = 0.01636448
Iteration 81, loss = 0.01656063
Iteration 82, loss = 0.01641634
Iteration 83, loss = 0.01642785
Iteration 84, loss = 0.01628922
Iteration 85, loss = 0.01630085
Iteration 86, loss = 0.01620690
Iteration 87, loss = 0.01614528
Iteration 88, loss = 0.01620849
Iteration 89, loss = 0.01620220
Iteration 90, loss = 0.01601077
Iteration 91, loss = 0.01608791
Iteration 92, loss = 0.01627700
Iteration 93, loss = 0.01608701
Iteration 94, loss = 0.01589039
Iteration 95, loss = 0.01595605
Iteration 96, loss = 0.01602504
Iteration 97, loss = 0.01592226
Iteration 98, loss = 0.01584068
Iteration 99, loss = 0.01576105
Iteration 100, loss = 0.01594263
Iteration 101, loss = 0.01573926
Iteration 102, loss = 0.01602708
Iteration 103, loss = 0.01573382
Iteration 104, loss = 0.01567479
Iteration 105, loss = 0.01572209
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67714787
Iteration 2, loss = 0.62339326
Iteration 3, loss = 0.54020636
Iteration 4, loss = 0.44618842
Iteration 5, loss = 0.35873419
Iteration 6, loss = 0.28433501
Iteration 7, loss = 0.22483456
Iteration 8, loss = 0.17978363
Iteration 9, loss = 0.14616672
Iteration 10, loss = 0.12120444
Iteration 11, loss = 0.10235331
Iteration 12, loss = 0.08796528
Iteration 13, loss = 0.07689154
Iteration 14, loss = 0.06818061
Iteration 15, loss = 0.06118581
Iteration 16, loss = 0.05564700
Iteration 17, loss = 0.05093525
Iteration 18, loss = 0.04729961
Iteration 19, loss = 0.04397828
Iteration 20, loss = 0.04119540
Iteration 21, loss = 0.03891403
Iteration 22, loss = 0.03681209
Iteration 23, loss = 0.03505461
Iteration 24, loss = 0.03372816
Iteration 25, loss = 0.03217243
Iteration 26, loss = 0.03107649
Iteration 27, loss = 0.03009122
Iteration 28, loss = 0.02932672
Iteration 29, loss = 0.02844426
Iteration 30, loss = 0.02766006
Iteration 31, loss = 0.02695293
Iteration 32, loss = 0.02637604
Iteration 33, loss = 0.02572700
Iteration 34, loss = 0.02541704
Iteration 35, loss = 0.02495380
Iteration 36, loss = 0.02452657
Iteration 37, loss = 0.02410388
Iteration 38, loss = 0.02376684
Iteration 39, loss = 0.02342793
Iteration 40, loss = 0.02340092
Iteration 41, loss = 0.02300827
Iteration 42, loss = 0.02291748
Iteration 43, loss = 0.02241117
Iteration 44, loss = 0.02243389
Iteration 45, loss = 0.02213580
Iteration 46, loss = 0.02187428
Iteration 47, loss = 0.02174047
Iteration 48, loss = 0.02148262
Iteration 49, loss = 0.02134950
Iteration 50, loss = 0.02125609
Iteration 51, loss = 0.02102843
Iteration 52, loss = 0.02106800
Iteration 53, loss = 0.02096322
Iteration 54, loss = 0.02082401
Iteration 55, loss = 0.02078566
Iteration 56, loss = 0.02045392
Iteration 57, loss = 0.02050472
Iteration 58, loss = 0.02035642
Iteration 59, loss = 0.02025859
Iteration 60, loss = 0.02027457
Iteration 61, loss = 0.01995653
Iteration 62, loss = 0.02006435
Iteration 63, loss = 0.02007642
Iteration 64, loss = 0.01982312
Iteration 65, loss = 0.01993455
Iteration 66, loss = 0.01991009
Iteration 67, loss = 0.01994464
Iteration 68, loss = 0.01976946
Iteration 69, loss = 0.02008088
Iteration 70, loss = 0.01964344
Iteration 71, loss = 0.01948397
Iteration 72, loss = 0.01943763
Iteration 73, loss = 0.01941535
Iteration 74, loss = 0.01954143
Iteration 75, loss = 0.01932015
Iteration 76, loss = 0.01933361
Iteration 77, loss = 0.01933231
Iteration 78, loss = 0.01938401
Iteration 79, loss = 0.01925879
Iteration 80, loss = 0.01958921
Iteration 81, loss = 0.01901140
Iteration 82, loss = 0.01912565
Iteration 83, loss = 0.01901650
Iteration 84, loss = 0.01919601
Iteration 85, loss = 0.01922030
Iteration 86, loss = 0.01903265
Iteration 87, loss = 0.01920727
Iteration 88, loss = 0.01911068
Iteration 89, loss = 0.01905935
Iteration 90, loss = 0.01891730
Iteration 91, loss = 0.01903982
Iteration 92, loss = 0.01885722
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70320162
Iteration 2, loss = 0.68262012
Iteration 3, loss = 0.66620309
Iteration 4, loss = 0.65083967
Iteration 5, loss = 0.63535857
Iteration 6, loss = 0.61918961
Iteration 7, loss = 0.60205626
Iteration 8, loss = 0.58369718
Iteration 9, loss = 0.56428360
Iteration 10, loss = 0.54389548
Iteration 11, loss = 0.52251028
Iteration 12, loss = 0.50047964
Iteration 13, loss = 0.47792648
Iteration 14, loss = 0.45491138
Iteration 15, loss = 0.43187782
Iteration 16, loss = 0.40887937
Iteration 17, loss = 0.38646336
Iteration 18, loss = 0.36429760
Iteration 19, loss = 0.34299013
Iteration 20, loss = 0.32251353
Iteration 21, loss = 0.30297625
Iteration 22, loss = 0.28433329
Iteration 23, loss = 0.26674306
Iteration 24, loss = 0.25014096
Iteration 25, loss = 0.23443555
Iteration 26, loss = 0.21973150
Iteration 27, loss = 0.20599477
Iteration 28, loss = 0.19323073
Iteration 29, loss = 0.18136559
Iteration 30, loss = 0.17038994
Iteration 31, loss = 0.16019773
Iteration 32, loss = 0.15075498
Iteration 33, loss = 0.14211218
Iteration 34, loss = 0.13405042
Iteration 35, loss = 0.12662249
Iteration 36, loss = 0.11976555
Iteration 37, loss = 0.11344684
Iteration 38, loss = 0.10758387
Iteration 39, loss = 0.10204135
Iteration 40, loss = 0.09697748
Iteration 41, loss = 0.09231186
Iteration 42, loss = 0.08794171
Iteration 43, loss = 0.08389922
Iteration 44, loss = 0.08012460
Iteration 45, loss = 0.07660805
Iteration 46, loss = 0.07337771
Iteration 47, loss = 0.07030454
Iteration 48, loss = 0.06744690
Iteration 49, loss = 0.06475521
Iteration 50, loss = 0.06225291
Iteration 51, loss = 0.05992006
Iteration 52, loss = 0.05772080
Iteration 53, loss = 0.05562117
Iteration 54, loss = 0.05372358
Iteration 55, loss = 0.05186536
Iteration 56, loss = 0.05014840
Iteration 57, loss = 0.04851964
Iteration 58, loss = 0.04698722
Iteration 59, loss = 0.04554657
Iteration 60, loss = 0.04418767
Iteration 61, loss = 0.04288796
Iteration 62, loss = 0.04163913
Iteration 63, loss = 0.04046865
Iteration 64, loss = 0.03934782
Iteration 65, loss = 0.03830050
Iteration 66, loss = 0.03725856
Iteration 67, loss = 0.03631917
Iteration 68, loss = 0.03541950
Iteration 69, loss = 0.03452816
Iteration 70, loss = 0.03365281
Iteration 71, loss = 0.03285578
Iteration 72, loss = 0.03210174
Iteration 73, loss = 0.03141093
Iteration 74, loss = 0.03069925
Iteration 75, loss = 0.03002894
Iteration 76, loss = 0.02941407
Iteration 77, loss = 0.02880550
Iteration 78, loss = 0.02819342
Iteration 79, loss = 0.02766884
Iteration 80, loss = 0.02711325
Iteration 81, loss = 0.02660509
Iteration 82, loss = 0.02611170
Iteration 83, loss = 0.02565213
Iteration 84, loss = 0.02519777
Iteration 85, loss = 0.02473901
Iteration 86, loss = 0.02431741
Iteration 87, loss = 0.02387194
Iteration 88, loss = 0.02343969
Iteration 89, loss = 0.02304438
Iteration 90, loss = 0.02263338
Iteration 91, loss = 0.02233515
Iteration 92, loss = 0.02192299
Iteration 93, loss = 0.02160560
Iteration 94, loss = 0.02127781
Iteration 95, loss = 0.02095126
Iteration 96, loss = 0.02064449
Iteration 97, loss = 0.02036282
Iteration 98, loss = 0.02007616
Iteration 99, loss = 0.01980089
Iteration 100, loss = 0.01954093
Iteration 101, loss = 0.01933690
Iteration 102, loss = 0.01904109
Iteration 103, loss = 0.01879006
Iteration 104, loss = 0.01856231
Iteration 105, loss = 0.01832757
Iteration 106, loss = 0.01809394
Iteration 107, loss = 0.01785271
Iteration 108, loss = 0.01769747
Iteration 109, loss = 0.01747535
Iteration 110, loss = 0.01729561
Iteration 111, loss = 0.01715833
Iteration 112, loss = 0.01695454
Iteration 113, loss = 0.01673863
Iteration 114, loss = 0.01659083
Iteration 115, loss = 0.01636813
Iteration 116, loss = 0.01619012
Iteration 117, loss = 0.01602487
Iteration 118, loss = 0.01585138
Iteration 119, loss = 0.01569689
Iteration 120, loss = 0.01553604
Iteration 121, loss = 0.01538636
Iteration 122, loss = 0.01523823
Iteration 123, loss = 0.01508416
Iteration 124, loss = 0.01493706
Iteration 125, loss = 0.01486333
Iteration 126, loss = 0.01470525
Iteration 127, loss = 0.01456296
Iteration 128, loss = 0.01443424
Iteration 129, loss = 0.01431319
Iteration 130, loss = 0.01425129
Iteration 131, loss = 0.01408688
Iteration 132, loss = 0.01398447
Iteration 133, loss = 0.01386362
Iteration 134, loss = 0.01374895
Iteration 135, loss = 0.01366251
Iteration 136, loss = 0.01353129
Iteration 137, loss = 0.01342823
Iteration 138, loss = 0.01332448
Iteration 139, loss = 0.01323078
Iteration 140, loss = 0.01312211
Iteration 141, loss = 0.01305023
Iteration 142, loss = 0.01298209
Iteration 143, loss = 0.01285489
Iteration 144, loss = 0.01283504
Iteration 145, loss = 0.01269406
Iteration 146, loss = 0.01259065
Iteration 147, loss = 0.01254804
Iteration 148, loss = 0.01246092
Iteration 149, loss = 0.01245482
Iteration 150, loss = 0.01235739
Iteration 151, loss = 0.01227350
Iteration 152, loss = 0.01220343
Iteration 153, loss = 0.01213965
Iteration 154, loss = 0.01206185
Iteration 155, loss = 0.01199352
Iteration 156, loss = 0.01192902
Iteration 157, loss = 0.01184211
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71377182
Iteration 2, loss = 0.69245517
Iteration 3, loss = 0.67470267
Iteration 4, loss = 0.65767111
Iteration 5, loss = 0.64011436
Iteration 6, loss = 0.62167174
Iteration 7, loss = 0.60225516
Iteration 8, loss = 0.58155566
Iteration 9, loss = 0.55987608
Iteration 10, loss = 0.53716411
Iteration 11, loss = 0.51381560
Iteration 12, loss = 0.49028898
Iteration 13, loss = 0.46632833
Iteration 14, loss = 0.44265322
Iteration 15, loss = 0.41932805
Iteration 16, loss = 0.39645833
Iteration 17, loss = 0.37440693
Iteration 18, loss = 0.35297569
Iteration 19, loss = 0.33245559
Iteration 20, loss = 0.31266533
Iteration 21, loss = 0.29387208
Iteration 22, loss = 0.27586724
Iteration 23, loss = 0.25866059
Iteration 24, loss = 0.24235671
Iteration 25, loss = 0.22704908
Iteration 26, loss = 0.21254286
Iteration 27, loss = 0.19915316
Iteration 28, loss = 0.18657772
Iteration 29, loss = 0.17501737
Iteration 30, loss = 0.16418079
Iteration 31, loss = 0.15417162
Iteration 32, loss = 0.14494709
Iteration 33, loss = 0.13639490
Iteration 34, loss = 0.12848759
Iteration 35, loss = 0.12119736
Iteration 36, loss = 0.11445001
Iteration 37, loss = 0.10825619
Iteration 38, loss = 0.10247502
Iteration 39, loss = 0.09713187
Iteration 40, loss = 0.09215390
Iteration 41, loss = 0.08756271
Iteration 42, loss = 0.08331439
Iteration 43, loss = 0.07937973
Iteration 44, loss = 0.07572502
Iteration 45, loss = 0.07230955
Iteration 46, loss = 0.06910511
Iteration 47, loss = 0.06613217
Iteration 48, loss = 0.06337601
Iteration 49, loss = 0.06073336
Iteration 50, loss = 0.05836397
Iteration 51, loss = 0.05604035
Iteration 52, loss = 0.05386200
Iteration 53, loss = 0.05180288
Iteration 54, loss = 0.04993932
Iteration 55, loss = 0.04812658
Iteration 56, loss = 0.04640034
Iteration 57, loss = 0.04483779
Iteration 58, loss = 0.04333258
Iteration 59, loss = 0.04189304
Iteration 60, loss = 0.04055200
Iteration 61, loss = 0.03927980
Iteration 62, loss = 0.03804056
Iteration 63, loss = 0.03692226
Iteration 64, loss = 0.03581175
Iteration 65, loss = 0.03478247
Iteration 66, loss = 0.03380279
Iteration 67, loss = 0.03286220
Iteration 68, loss = 0.03197717
Iteration 69, loss = 0.03110733
Iteration 70, loss = 0.03033871
Iteration 71, loss = 0.02954162
Iteration 72, loss = 0.02879669
Iteration 73, loss = 0.02809855
Iteration 74, loss = 0.02739884
Iteration 75, loss = 0.02676636
Iteration 76, loss = 0.02612908
Iteration 77, loss = 0.02552632
Iteration 78, loss = 0.02496325
Iteration 79, loss = 0.02440765
Iteration 80, loss = 0.02387265
Iteration 81, loss = 0.02333907
Iteration 82, loss = 0.02284950
Iteration 83, loss = 0.02234980
Iteration 84, loss = 0.02189264
Iteration 85, loss = 0.02144875
Iteration 86, loss = 0.02104143
Iteration 87, loss = 0.02061919
Iteration 88, loss = 0.02023446
Iteration 89, loss = 0.01987876
Iteration 90, loss = 0.01952056
Iteration 91, loss = 0.01916680
Iteration 92, loss = 0.01881440
Iteration 93, loss = 0.01849194
Iteration 94, loss = 0.01817894
Iteration 95, loss = 0.01787796
Iteration 96, loss = 0.01755981
Iteration 97, loss = 0.01725579
Iteration 98, loss = 0.01697306
Iteration 99, loss = 0.01670312
Iteration 100, loss = 0.01642403
Iteration 101, loss = 0.01617810
Iteration 102, loss = 0.01592828
Iteration 103, loss = 0.01569419
Iteration 104, loss = 0.01546602
Iteration 105, loss = 0.01523108
Iteration 106, loss = 0.01502747
Iteration 107, loss = 0.01483390
Iteration 108, loss = 0.01467044
Iteration 109, loss = 0.01443039
Iteration 110, loss = 0.01422741
Iteration 111, loss = 0.01405170
Iteration 112, loss = 0.01383913
Iteration 113, loss = 0.01367532
Iteration 114, loss = 0.01348371
Iteration 115, loss = 0.01332577
Iteration 116, loss = 0.01316074
Iteration 117, loss = 0.01300454
Iteration 118, loss = 0.01282901
Iteration 119, loss = 0.01269603
Iteration 120, loss = 0.01252710
Iteration 121, loss = 0.01239159
Iteration 122, loss = 0.01224514
Iteration 123, loss = 0.01209917
Iteration 124, loss = 0.01195478
Iteration 125, loss = 0.01182523
Iteration 126, loss = 0.01169012
Iteration 127, loss = 0.01157202
Iteration 128, loss = 0.01144193
Iteration 129, loss = 0.01131339
Iteration 130, loss = 0.01119457
Iteration 131, loss = 0.01108260
Iteration 132, loss = 0.01097505
Iteration 133, loss = 0.01085360
Iteration 134, loss = 0.01073830
Iteration 135, loss = 0.01063932
Iteration 136, loss = 0.01057451
Iteration 137, loss = 0.01045863
Iteration 138, loss = 0.01036057
Iteration 139, loss = 0.01025528
Iteration 140, loss = 0.01016251
Iteration 141, loss = 0.01007395
Iteration 142, loss = 0.00997878
Iteration 143, loss = 0.00989062
Iteration 144, loss = 0.00979756
Iteration 145, loss = 0.00972256
Iteration 146, loss = 0.00962876
Iteration 147, loss = 0.00957182
Iteration 148, loss = 0.00947088
Iteration 149, loss = 0.00939698
Iteration 150, loss = 0.00931817
Iteration 151, loss = 0.00924874
Iteration 152, loss = 0.00917023
Iteration 153, loss = 0.00912025
Iteration 154, loss = 0.00903409
Iteration 155, loss = 0.00895469
Iteration 156, loss = 0.00888813
Iteration 157, loss = 0.00883811
Iteration 158, loss = 0.00876096
Iteration 159, loss = 0.00869652
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68397751
Iteration 2, loss = 0.65018268
Iteration 3, loss = 0.61379430
Iteration 4, loss = 0.57129096
Iteration 5, loss = 0.52350514
Iteration 6, loss = 0.47413317
Iteration 7, loss = 0.42572037
Iteration 8, loss = 0.37993682
Iteration 9, loss = 0.33769775
Iteration 10, loss = 0.29918801
Iteration 11, loss = 0.26495206
Iteration 12, loss = 0.23464921
Iteration 13, loss = 0.20821857
Iteration 14, loss = 0.18501829
Iteration 15, loss = 0.16477151
Iteration 16, loss = 0.14753424
Iteration 17, loss = 0.13250641
Iteration 18, loss = 0.11954554
Iteration 19, loss = 0.10846358
Iteration 20, loss = 0.09887822
Iteration 21, loss = 0.09043048
Iteration 22, loss = 0.08315104
Iteration 23, loss = 0.07699000
Iteration 24, loss = 0.07119597
Iteration 25, loss = 0.06635209
Iteration 26, loss = 0.06204859
Iteration 27, loss = 0.05818884
Iteration 28, loss = 0.05489076
Iteration 29, loss = 0.05188632
Iteration 30, loss = 0.04920436
Iteration 31, loss = 0.04689532
Iteration 32, loss = 0.04469459
Iteration 33, loss = 0.04266866
Iteration 34, loss = 0.04092782
Iteration 35, loss = 0.03926730
Iteration 36, loss = 0.03765698
Iteration 37, loss = 0.03628610
Iteration 38, loss = 0.03491593
Iteration 39, loss = 0.03386901
Iteration 40, loss = 0.03276054
Iteration 41, loss = 0.03176066
Iteration 42, loss = 0.03099803
Iteration 43, loss = 0.03023468
Iteration 44, loss = 0.02942174
Iteration 45, loss = 0.02859816
Iteration 46, loss = 0.02785811
Iteration 47, loss = 0.02724991
Iteration 48, loss = 0.02653530
Iteration 49, loss = 0.02596179
Iteration 50, loss = 0.02541991
Iteration 51, loss = 0.02494053
Iteration 52, loss = 0.02449593
Iteration 53, loss = 0.02408417
Iteration 54, loss = 0.02370743
Iteration 55, loss = 0.02334924
Iteration 56, loss = 0.02308960
Iteration 57, loss = 0.02276455
Iteration 58, loss = 0.02245668
Iteration 59, loss = 0.02205502
Iteration 60, loss = 0.02191376
Iteration 61, loss = 0.02164792
Iteration 62, loss = 0.02146230
Iteration 63, loss = 0.02107001
Iteration 64, loss = 0.02081866
Iteration 65, loss = 0.02064973
Iteration 66, loss = 0.02035931
Iteration 67, loss = 0.02015171
Iteration 68, loss = 0.01992704
Iteration 69, loss = 0.01962847
Iteration 70, loss = 0.01954923
Iteration 71, loss = 0.01928508
Iteration 72, loss = 0.01916941
Iteration 73, loss = 0.01894006
Iteration 74, loss = 0.01889914
Iteration 75, loss = 0.01868666
Iteration 76, loss = 0.01858529
Iteration 77, loss = 0.01847594
Iteration 78, loss = 0.01841592
Iteration 79, loss = 0.01826157
Iteration 80, loss = 0.01817211
Iteration 81, loss = 0.01809917
Iteration 82, loss = 0.01798115
Iteration 83, loss = 0.01795891
Iteration 84, loss = 0.01789221
Iteration 85, loss = 0.01768795
Iteration 86, loss = 0.01774677
Iteration 87, loss = 0.01753192
Iteration 88, loss = 0.01746990
Iteration 89, loss = 0.01728795
Iteration 90, loss = 0.01727124
Iteration 91, loss = 0.01717127
Iteration 92, loss = 0.01714214
Iteration 93, loss = 0.01700576
Iteration 94, loss = 0.01697364
Iteration 95, loss = 0.01692211
Iteration 96, loss = 0.01680791
Iteration 97, loss = 0.01699720
Iteration 98, loss = 0.01718606
Iteration 99, loss = 0.01703494
Iteration 100, loss = 0.01733579
Iteration 101, loss = 0.01724377
Iteration 102, loss = 0.01705013
Iteration 103, loss = 0.01681769
Iteration 104, loss = 0.01661728
Iteration 105, loss = 0.01649339
Iteration 106, loss = 0.01647466
Iteration 107, loss = 0.01649094
Iteration 108, loss = 0.01637601
Iteration 109, loss = 0.01624506
Iteration 110, loss = 0.01610304
Iteration 111, loss = 0.01600286
Iteration 112, loss = 0.01583443
Iteration 113, loss = 0.01599307
Iteration 114, loss = 0.01626349
Iteration 115, loss = 0.01605734
Iteration 116, loss = 0.01642789
Iteration 117, loss = 0.01646287
Iteration 118, loss = 0.01623097
Iteration 119, loss = 0.01604402
Iteration 120, loss = 0.01586315
Iteration 121, loss = 0.01578701
Iteration 122, loss = 0.01560038
Iteration 123, loss = 0.01592215
Iteration 124, loss = 0.01592246
Iteration 125, loss = 0.01587161
Iteration 126, loss = 0.01575622
Iteration 127, loss = 0.01559333
Iteration 128, loss = 0.01565810
Iteration 129, loss = 0.01566020
Iteration 130, loss = 0.01557232
Iteration 131, loss = 0.01545879
Iteration 132, loss = 0.01539261
Iteration 133, loss = 0.01539336
Iteration 134, loss = 0.01549563
Iteration 135, loss = 0.01530728
Iteration 136, loss = 0.01535974
Iteration 137, loss = 0.01531576
Iteration 138, loss = 0.01544776
Iteration 139, loss = 0.01532120
Iteration 140, loss = 0.01525625
Iteration 141, loss = 0.01515267
Iteration 142, loss = 0.01513707
Iteration 143, loss = 0.01521854
Iteration 144, loss = 0.01512373
Iteration 145, loss = 0.01510614
Iteration 146, loss = 0.01505030
Iteration 147, loss = 0.01520175
Iteration 148, loss = 0.01512665
Iteration 149, loss = 0.01496100
Iteration 150, loss = 0.01511285
Iteration 151, loss = 0.01513325
Iteration 152, loss = 0.01493960
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67479977
Iteration 2, loss = 0.60556332
Iteration 3, loss = 0.50705376
Iteration 4, loss = 0.40049843
Iteration 5, loss = 0.30580620
Iteration 6, loss = 0.23076217
Iteration 7, loss = 0.17673499
Iteration 8, loss = 0.13880269
Iteration 9, loss = 0.11226232
Iteration 10, loss = 0.09354087
Iteration 11, loss = 0.07977477
Iteration 12, loss = 0.06933550
Iteration 13, loss = 0.06156391
Iteration 14, loss = 0.05531837
Iteration 15, loss = 0.05055750
Iteration 16, loss = 0.04644692
Iteration 17, loss = 0.04304408
Iteration 18, loss = 0.04036833
Iteration 19, loss = 0.03836895
Iteration 20, loss = 0.03629338
Iteration 21, loss = 0.03470679
Iteration 22, loss = 0.03319178
Iteration 23, loss = 0.03195968
Iteration 24, loss = 0.03067210
Iteration 25, loss = 0.03011798
Iteration 26, loss = 0.02936059
Iteration 27, loss = 0.02839668
Iteration 28, loss = 0.02794877
Iteration 29, loss = 0.02727608
Iteration 30, loss = 0.02671276
Iteration 31, loss = 0.02619956
Iteration 32, loss = 0.02582169
Iteration 33, loss = 0.02568013
Iteration 34, loss = 0.02536401
Iteration 35, loss = 0.02486080
Iteration 36, loss = 0.02441599
Iteration 37, loss = 0.02411420
Iteration 38, loss = 0.02394058
Iteration 39, loss = 0.02371282
Iteration 40, loss = 0.02339443
Iteration 41, loss = 0.02328479
Iteration 42, loss = 0.02301503
Iteration 43, loss = 0.02321445
Iteration 44, loss = 0.02288374
Iteration 45, loss = 0.02281702
Iteration 46, loss = 0.02259216
Iteration 47, loss = 0.02220097
Iteration 48, loss = 0.02233753
Iteration 49, loss = 0.02212236
Iteration 50, loss = 0.02190584
Iteration 51, loss = 0.02192857
Iteration 52, loss = 0.02180736
Iteration 53, loss = 0.02206224
Iteration 54, loss = 0.02163478
Iteration 55, loss = 0.02178695
Iteration 56, loss = 0.02141493
Iteration 57, loss = 0.02198423
Iteration 58, loss = 0.02131294
Iteration 59, loss = 0.02138656
Iteration 60, loss = 0.02141454
Iteration 61, loss = 0.02111028
Iteration 62, loss = 0.02104509
Iteration 63, loss = 0.02134378
Iteration 64, loss = 0.02110621
Iteration 65, loss = 0.02098343
Iteration 66, loss = 0.02138077
Iteration 67, loss = 0.02081569
Iteration 68, loss = 0.02083810
Iteration 69, loss = 0.02062338
Iteration 70, loss = 0.02088720
Iteration 71, loss = 0.02073093
Iteration 72, loss = 0.02082312
Iteration 73, loss = 0.02066833
Iteration 74, loss = 0.02085262
Iteration 75, loss = 0.02069322
Iteration 76, loss = 0.02043845
Iteration 77, loss = 0.02042965
Iteration 78, loss = 0.02047886
Iteration 79, loss = 0.02061227
Iteration 80, loss = 0.02055979
Iteration 81, loss = 0.02042595
Iteration 82, loss = 0.02041792
Iteration 83, loss = 0.02053366
Iteration 84, loss = 0.02048516
Iteration 85, loss = 0.02065173
Iteration 86, loss = 0.02041369
Iteration 87, loss = 0.02030521
Iteration 88, loss = 0.02062311
Iteration 89, loss = 0.02013077
Iteration 90, loss = 0.02027789
Iteration 91, loss = 0.02019278
Iteration 92, loss = 0.02016164
Iteration 93, loss = 0.02061034
Iteration 94, loss = 0.02037917
Iteration 95, loss = 0.02025119
Iteration 96, loss = 0.02015372
Iteration 97, loss = 0.02023646
Iteration 98, loss = 0.02011955
Iteration 99, loss = 0.02007179
Iteration 100, loss = 0.02025346
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69508727
Iteration 2, loss = 0.66737507
Iteration 3, loss = 0.63682866
Iteration 4, loss = 0.59860652
Iteration 5, loss = 0.55189158
Iteration 6, loss = 0.50020576
Iteration 7, loss = 0.44702469
Iteration 8, loss = 0.39552069
Iteration 9, loss = 0.34750049
Iteration 10, loss = 0.30405938
Iteration 11, loss = 0.26580465
Iteration 12, loss = 0.23212656
Iteration 13, loss = 0.20325540
Iteration 14, loss = 0.17851897
Iteration 15, loss = 0.15748836
Iteration 16, loss = 0.13970442
Iteration 17, loss = 0.12477258
Iteration 18, loss = 0.11188940
Iteration 19, loss = 0.10105207
Iteration 20, loss = 0.09166819
Iteration 21, loss = 0.08371665
Iteration 22, loss = 0.07688771
Iteration 23, loss = 0.07093758
Iteration 24, loss = 0.06557238
Iteration 25, loss = 0.06111238
Iteration 26, loss = 0.05716419
Iteration 27, loss = 0.05328121
Iteration 28, loss = 0.05026253
Iteration 29, loss = 0.04771560
Iteration 30, loss = 0.04524973
Iteration 31, loss = 0.04302159
Iteration 32, loss = 0.04093029
Iteration 33, loss = 0.03898590
Iteration 34, loss = 0.03730784
Iteration 35, loss = 0.03576574
Iteration 36, loss = 0.03438614
Iteration 37, loss = 0.03302900
Iteration 38, loss = 0.03189235
Iteration 39, loss = 0.03074005
Iteration 40, loss = 0.02979426
Iteration 41, loss = 0.02888523
Iteration 42, loss = 0.02811008
Iteration 43, loss = 0.02726647
Iteration 44, loss = 0.02664214
Iteration 45, loss = 0.02594460
Iteration 46, loss = 0.02527565
Iteration 47, loss = 0.02469481
Iteration 48, loss = 0.02415816
Iteration 49, loss = 0.02359781
Iteration 50, loss = 0.02315621
Iteration 51, loss = 0.02283509
Iteration 52, loss = 0.02237903
Iteration 53, loss = 0.02194016
Iteration 54, loss = 0.02153021
Iteration 55, loss = 0.02121555
Iteration 56, loss = 0.02089536
Iteration 57, loss = 0.02050343
Iteration 58, loss = 0.02023121
Iteration 59, loss = 0.02009824
Iteration 60, loss = 0.01993101
Iteration 61, loss = 0.01990802
Iteration 62, loss = 0.01957601
Iteration 63, loss = 0.01927774
Iteration 64, loss = 0.01887954
Iteration 65, loss = 0.01853771
Iteration 66, loss = 0.01842093
Iteration 67, loss = 0.01827717
Iteration 68, loss = 0.01822985
Iteration 69, loss = 0.01809413
Iteration 70, loss = 0.01765053
Iteration 71, loss = 0.01753249
Iteration 72, loss = 0.01734581
Iteration 73, loss = 0.01727738
Iteration 74, loss = 0.01704379
Iteration 75, loss = 0.01698838
Iteration 76, loss = 0.01679206
Iteration 77, loss = 0.01661601
Iteration 78, loss = 0.01655387
Iteration 79, loss = 0.01639490
Iteration 80, loss = 0.01626040
Iteration 81, loss = 0.01617178
Iteration 82, loss = 0.01605843
Iteration 83, loss = 0.01599914
Iteration 84, loss = 0.01595810
Iteration 85, loss = 0.01585894
Iteration 86, loss = 0.01572785
Iteration 87, loss = 0.01558868
Iteration 88, loss = 0.01548755
Iteration 89, loss = 0.01562574
Iteration 90, loss = 0.01560271
Iteration 91, loss = 0.01552066
Iteration 92, loss = 0.01562020
Iteration 93, loss = 0.01527463
Iteration 94, loss = 0.01522654
Iteration 95, loss = 0.01509496
Iteration 96, loss = 0.01499033
Iteration 97, loss = 0.01491364
Iteration 98, loss = 0.01494945
Iteration 99, loss = 0.01485947
Iteration 100, loss = 0.01487117
Iteration 101, loss = 0.01494706
Iteration 102, loss = 0.01488103
Iteration 103, loss = 0.01566257
Iteration 104, loss = 0.01501157
Iteration 105, loss = 0.01525870
Iteration 106, loss = 0.01523036
Iteration 107, loss = 0.01501324
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67707847
Iteration 2, loss = 0.65136794
Iteration 3, loss = 0.62192144
Iteration 4, loss = 0.58401144
Iteration 5, loss = 0.53802989
Iteration 6, loss = 0.48884800
Iteration 7, loss = 0.43902972
Iteration 8, loss = 0.39090665
Iteration 9, loss = 0.34610831
Iteration 10, loss = 0.30466407
Iteration 11, loss = 0.26781786
Iteration 12, loss = 0.23548905
Iteration 13, loss = 0.20697796
Iteration 14, loss = 0.18274746
Iteration 15, loss = 0.16169704
Iteration 16, loss = 0.14377729
Iteration 17, loss = 0.12852909
Iteration 18, loss = 0.11542521
Iteration 19, loss = 0.10428144
Iteration 20, loss = 0.09475081
Iteration 21, loss = 0.08667246
Iteration 22, loss = 0.07941224
Iteration 23, loss = 0.07328453
Iteration 24, loss = 0.06791062
Iteration 25, loss = 0.06313676
Iteration 26, loss = 0.05894310
Iteration 27, loss = 0.05521453
Iteration 28, loss = 0.05194958
Iteration 29, loss = 0.04887957
Iteration 30, loss = 0.04629691
Iteration 31, loss = 0.04388543
Iteration 32, loss = 0.04168294
Iteration 33, loss = 0.03970696
Iteration 34, loss = 0.03795790
Iteration 35, loss = 0.03638884
Iteration 36, loss = 0.03508539
Iteration 37, loss = 0.03368242
Iteration 38, loss = 0.03243725
Iteration 39, loss = 0.03126550
Iteration 40, loss = 0.03025015
Iteration 41, loss = 0.02927694
Iteration 42, loss = 0.02834418
Iteration 43, loss = 0.02754059
Iteration 44, loss = 0.02675353
Iteration 45, loss = 0.02611596
Iteration 46, loss = 0.02544715
Iteration 47, loss = 0.02483997
Iteration 48, loss = 0.02420232
Iteration 49, loss = 0.02364650
Iteration 50, loss = 0.02313951
Iteration 51, loss = 0.02261850
Iteration 52, loss = 0.02222204
Iteration 53, loss = 0.02178279
Iteration 54, loss = 0.02130138
Iteration 55, loss = 0.02098389
Iteration 56, loss = 0.02062365
Iteration 57, loss = 0.02038607
Iteration 58, loss = 0.02004161
Iteration 59, loss = 0.01976759
Iteration 60, loss = 0.01943683
Iteration 61, loss = 0.01953086
Iteration 62, loss = 0.01933117
Iteration 63, loss = 0.01896537
Iteration 64, loss = 0.01863013
Iteration 65, loss = 0.01831716
Iteration 66, loss = 0.01802008
Iteration 67, loss = 0.01784382
Iteration 68, loss = 0.01766370
Iteration 69, loss = 0.01749264
Iteration 70, loss = 0.01734636
Iteration 71, loss = 0.01717636
Iteration 72, loss = 0.01699473
Iteration 73, loss = 0.01688196
Iteration 74, loss = 0.01673396
Iteration 75, loss = 0.01661563
Iteration 76, loss = 0.01647001
Iteration 77, loss = 0.01633656
Iteration 78, loss = 0.01618713
Iteration 79, loss = 0.01606578
Iteration 80, loss = 0.01597848
Iteration 81, loss = 0.01584347
Iteration 82, loss = 0.01579200
Iteration 83, loss = 0.01580066
Iteration 84, loss = 0.01569393
Iteration 85, loss = 0.01589841
Iteration 86, loss = 0.01579371
Iteration 87, loss = 0.01580899
Iteration 88, loss = 0.01624659
Iteration 89, loss = 0.01611688
Iteration 90, loss = 0.01585869
Iteration 91, loss = 0.01556097
Iteration 92, loss = 0.01527250
Iteration 93, loss = 0.01497338
Iteration 94, loss = 0.01491456
Iteration 95, loss = 0.01478335
Iteration 96, loss = 0.01472670
Iteration 97, loss = 0.01466584
Iteration 98, loss = 0.01477036
Iteration 99, loss = 0.01465169
Iteration 100, loss = 0.01449111
Iteration 101, loss = 0.01447819
Iteration 102, loss = 0.01439370
Iteration 103, loss = 0.01429230
Iteration 104, loss = 0.01429271
Iteration 105, loss = 0.01423976
Iteration 106, loss = 0.01421332
Iteration 107, loss = 0.01414390
Iteration 108, loss = 0.01413357
Iteration 109, loss = 0.01409403
Iteration 110, loss = 0.01399991
Iteration 111, loss = 0.01404564
Iteration 112, loss = 0.01399867
Iteration 113, loss = 0.01400993
Iteration 114, loss = 0.01378632
Iteration 115, loss = 0.01400355
Iteration 116, loss = 0.01389442
Iteration 117, loss = 0.01389861
Iteration 118, loss = 0.01384757
Iteration 119, loss = 0.01384927
Iteration 120, loss = 0.01374251
Iteration 121, loss = 0.01377870
Iteration 122, loss = 0.01369457
Iteration 123, loss = 0.01358751
Iteration 124, loss = 0.01380785
Iteration 125, loss = 0.01377185
Iteration 126, loss = 0.01374234
Iteration 127, loss = 0.01366794
Iteration 128, loss = 0.01365145
Iteration 129, loss = 0.01349902
Iteration 130, loss = 0.01353206
Iteration 131, loss = 0.01344106
Iteration 132, loss = 0.01347380
Iteration 133, loss = 0.01344075
Iteration 134, loss = 0.01338870
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70895789
Iteration 2, loss = 0.63621792
Iteration 3, loss = 0.53699764
Iteration 4, loss = 0.42696425
Iteration 5, loss = 0.32848072
Iteration 6, loss = 0.24950683
Iteration 7, loss = 0.19138511
Iteration 8, loss = 0.14987244
Iteration 9, loss = 0.12072316
Iteration 10, loss = 0.09963287
Iteration 11, loss = 0.08444596
Iteration 12, loss = 0.07310513
Iteration 13, loss = 0.06429238
Iteration 14, loss = 0.05756798
Iteration 15, loss = 0.05231316
Iteration 16, loss = 0.04771582
Iteration 17, loss = 0.04435491
Iteration 18, loss = 0.04131047
Iteration 19, loss = 0.03897336
Iteration 20, loss = 0.03679398
Iteration 21, loss = 0.03517706
Iteration 22, loss = 0.03348871
Iteration 23, loss = 0.03231148
Iteration 24, loss = 0.03106011
Iteration 25, loss = 0.03001004
Iteration 26, loss = 0.02927679
Iteration 27, loss = 0.02828892
Iteration 28, loss = 0.02797854
Iteration 29, loss = 0.02722491
Iteration 30, loss = 0.02667498
Iteration 31, loss = 0.02617593
Iteration 32, loss = 0.02564120
Iteration 33, loss = 0.02550255
Iteration 34, loss = 0.02499821
Iteration 35, loss = 0.02464772
Iteration 36, loss = 0.02441194
Iteration 37, loss = 0.02412580
Iteration 38, loss = 0.02392473
Iteration 39, loss = 0.02365007
Iteration 40, loss = 0.02343922
Iteration 41, loss = 0.02342223
Iteration 42, loss = 0.02301212
Iteration 43, loss = 0.02268228
Iteration 44, loss = 0.02275939
Iteration 45, loss = 0.02243044
Iteration 46, loss = 0.02238486
Iteration 47, loss = 0.02199790
Iteration 48, loss = 0.02207035
Iteration 49, loss = 0.02188123
Iteration 50, loss = 0.02182872
Iteration 51, loss = 0.02190522
Iteration 52, loss = 0.02169124
Iteration 53, loss = 0.02149090
Iteration 54, loss = 0.02135253
Iteration 55, loss = 0.02160765
Iteration 56, loss = 0.02139187
Iteration 57, loss = 0.02121796
Iteration 58, loss = 0.02132372
Iteration 59, loss = 0.02126130
Iteration 60, loss = 0.02143447
Iteration 61, loss = 0.02135080
Iteration 62, loss = 0.02117628
Iteration 63, loss = 0.02112586
Iteration 64, loss = 0.02133183
Iteration 65, loss = 0.02078902
Iteration 66, loss = 0.02087978
Iteration 67, loss = 0.02082378
Iteration 68, loss = 0.02104510
Iteration 69, loss = 0.02077311
Iteration 70, loss = 0.02060952
Iteration 71, loss = 0.02081327
Iteration 72, loss = 0.02086069
Iteration 73, loss = 0.02039704
Iteration 74, loss = 0.02061405
Iteration 75, loss = 0.02062636
Iteration 76, loss = 0.02047057
Iteration 77, loss = 0.02052461
Iteration 78, loss = 0.02067966
Iteration 79, loss = 0.02050179
Iteration 80, loss = 0.02035436
Iteration 81, loss = 0.02044869
Iteration 82, loss = 0.02031288
Iteration 83, loss = 0.02038183
Iteration 84, loss = 0.02052932
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70465615
Iteration 2, loss = 0.64303546
Iteration 3, loss = 0.54702563
Iteration 4, loss = 0.43222556
Iteration 5, loss = 0.32766747
Iteration 6, loss = 0.24671468
Iteration 7, loss = 0.18724539
Iteration 8, loss = 0.14627193
Iteration 9, loss = 0.11724448
Iteration 10, loss = 0.09701939
Iteration 11, loss = 0.08210763
Iteration 12, loss = 0.07107016
Iteration 13, loss = 0.06253377
Iteration 14, loss = 0.05635357
Iteration 15, loss = 0.05117977
Iteration 16, loss = 0.04662740
Iteration 17, loss = 0.04330369
Iteration 18, loss = 0.04057071
Iteration 19, loss = 0.03819847
Iteration 20, loss = 0.03607586
Iteration 21, loss = 0.03447761
Iteration 22, loss = 0.03302479
Iteration 23, loss = 0.03168955
Iteration 24, loss = 0.03050705
Iteration 25, loss = 0.02965013
Iteration 26, loss = 0.02886485
Iteration 27, loss = 0.02810291
Iteration 28, loss = 0.02714442
Iteration 29, loss = 0.02685409
Iteration 30, loss = 0.02603130
Iteration 31, loss = 0.02577852
Iteration 32, loss = 0.02541297
Iteration 33, loss = 0.02488146
Iteration 34, loss = 0.02459010
Iteration 35, loss = 0.02423170
Iteration 36, loss = 0.02419387
Iteration 37, loss = 0.02346117
Iteration 38, loss = 0.02314221
Iteration 39, loss = 0.02333586
Iteration 40, loss = 0.02297994
Iteration 41, loss = 0.02267633
Iteration 42, loss = 0.02255911
Iteration 43, loss = 0.02232740
Iteration 44, loss = 0.02229320
Iteration 45, loss = 0.02184466
Iteration 46, loss = 0.02166819
Iteration 47, loss = 0.02165887
Iteration 48, loss = 0.02141017
Iteration 49, loss = 0.02154666
Iteration 50, loss = 0.02124205
Iteration 51, loss = 0.02126277
Iteration 52, loss = 0.02098853
Iteration 53, loss = 0.02090531
Iteration 54, loss = 0.02090638
Iteration 55, loss = 0.02071626
Iteration 56, loss = 0.02088128
Iteration 57, loss = 0.02057179
Iteration 58, loss = 0.02055367
Iteration 59, loss = 0.02047465
Iteration 60, loss = 0.02049018
Iteration 61, loss = 0.02037219
Iteration 62, loss = 0.02035257
Iteration 63, loss = 0.02026078
Iteration 64, loss = 0.02031069
Iteration 65, loss = 0.01997320
Iteration 66, loss = 0.01996381
Iteration 67, loss = 0.02012182
Iteration 68, loss = 0.02014626
Iteration 69, loss = 0.02022266
Iteration 70, loss = 0.02003799
Iteration 71, loss = 0.02001694
Iteration 72, loss = 0.01981717
Iteration 73, loss = 0.01952291
Iteration 74, loss = 0.01988590
Iteration 75, loss = 0.01979311
Iteration 76, loss = 0.01967883
Iteration 77, loss = 0.01954170
Iteration 78, loss = 0.01955906
Iteration 79, loss = 0.01985430
Iteration 80, loss = 0.01999405
Iteration 81, loss = 0.01968606
Iteration 82, loss = 0.01964029
Iteration 83, loss = 0.01988096
Iteration 84, loss = 0.02001865
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67404905
Iteration 2, loss = 0.60185163
Iteration 3, loss = 0.49430424
Iteration 4, loss = 0.38244734
Iteration 5, loss = 0.28605991
Iteration 6, loss = 0.21330312
Iteration 7, loss = 0.16263251
Iteration 8, loss = 0.12781918
Iteration 9, loss = 0.10348789
Iteration 10, loss = 0.08648436
Iteration 11, loss = 0.07432535
Iteration 12, loss = 0.06492608
Iteration 13, loss = 0.05798959
Iteration 14, loss = 0.05230204
Iteration 15, loss = 0.04794542
Iteration 16, loss = 0.04435496
Iteration 17, loss = 0.04152075
Iteration 18, loss = 0.03885874
Iteration 19, loss = 0.03706504
Iteration 20, loss = 0.03537602
Iteration 21, loss = 0.03393248
Iteration 22, loss = 0.03247676
Iteration 23, loss = 0.03133522
Iteration 24, loss = 0.03035728
Iteration 25, loss = 0.02993656
Iteration 26, loss = 0.02890823
Iteration 27, loss = 0.02844042
Iteration 28, loss = 0.02773782
Iteration 29, loss = 0.02716378
Iteration 30, loss = 0.02681943
Iteration 31, loss = 0.02645748
Iteration 32, loss = 0.02608637
Iteration 33, loss = 0.02571176
Iteration 34, loss = 0.02538154
Iteration 35, loss = 0.02506326
Iteration 36, loss = 0.02498985
Iteration 37, loss = 0.02455039
Iteration 38, loss = 0.02448664
Iteration 39, loss = 0.02419569
Iteration 40, loss = 0.02400870
Iteration 41, loss = 0.02382247
Iteration 42, loss = 0.02377782
Iteration 43, loss = 0.02355593
Iteration 44, loss = 0.02313091
Iteration 45, loss = 0.02301586
Iteration 46, loss = 0.02299231
Iteration 47, loss = 0.02294053
Iteration 48, loss = 0.02309584
Iteration 49, loss = 0.02274763
Iteration 50, loss = 0.02297588
Iteration 51, loss = 0.02269669
Iteration 52, loss = 0.02224810
Iteration 53, loss = 0.02267214
Iteration 54, loss = 0.02246920
Iteration 55, loss = 0.02239588
Iteration 56, loss = 0.02226213
Iteration 57, loss = 0.02227446
Iteration 58, loss = 0.02190132
Iteration 59, loss = 0.02204581
Iteration 60, loss = 0.02233035
Iteration 61, loss = 0.02221264
Iteration 62, loss = 0.02205298
Iteration 63, loss = 0.02219880
Iteration 64, loss = 0.02185728
Iteration 65, loss = 0.02174364
Iteration 66, loss = 0.02177509
Iteration 67, loss = 0.02193929
Iteration 68, loss = 0.02165543
Iteration 69, loss = 0.02176855
Iteration 70, loss = 0.02169905
Iteration 71, loss = 0.02161726
Iteration 72, loss = 0.02160062
Iteration 73, loss = 0.02170764
Iteration 74, loss = 0.02139231
Iteration 75, loss = 0.02151892
Iteration 76, loss = 0.02159406
Iteration 77, loss = 0.02138719
Iteration 78, loss = 0.02132895
Iteration 79, loss = 0.02161843
Iteration 80, loss = 0.02123493
Iteration 81, loss = 0.02157237
Iteration 82, loss = 0.02131818
Iteration 83, loss = 0.02145538
Iteration 84, loss = 0.02151176
Iteration 85, loss = 0.02161894
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69982262
Iteration 2, loss = 0.66179373
Iteration 3, loss = 0.60880201
Iteration 4, loss = 0.54366565
Iteration 5, loss = 0.47151194
Iteration 6, loss = 0.39953920
Iteration 7, loss = 0.33265997
Iteration 8, loss = 0.27474366
Iteration 9, loss = 0.22700056
Iteration 10, loss = 0.18892507
Iteration 11, loss = 0.15863497
Iteration 12, loss = 0.13502779
Iteration 13, loss = 0.11603363
Iteration 14, loss = 0.10118580
Iteration 15, loss = 0.08914053
Iteration 16, loss = 0.07952124
Iteration 17, loss = 0.07145814
Iteration 18, loss = 0.06491175
Iteration 19, loss = 0.05940027
Iteration 20, loss = 0.05494137
Iteration 21, loss = 0.05085687
Iteration 22, loss = 0.04760023
Iteration 23, loss = 0.04457905
Iteration 24, loss = 0.04222054
Iteration 25, loss = 0.03996381
Iteration 26, loss = 0.03801828
Iteration 27, loss = 0.03628500
Iteration 28, loss = 0.03480752
Iteration 29, loss = 0.03342728
Iteration 30, loss = 0.03228708
Iteration 31, loss = 0.03125635
Iteration 32, loss = 0.03014704
Iteration 33, loss = 0.02954877
Iteration 34, loss = 0.02862179
Iteration 35, loss = 0.02784922
Iteration 36, loss = 0.02720502
Iteration 37, loss = 0.02653936
Iteration 38, loss = 0.02616347
Iteration 39, loss = 0.02565417
Iteration 40, loss = 0.02521662
Iteration 41, loss = 0.02451126
Iteration 42, loss = 0.02425594
Iteration 43, loss = 0.02404231
Iteration 44, loss = 0.02363266
Iteration 45, loss = 0.02322391
Iteration 46, loss = 0.02294862
Iteration 47, loss = 0.02267600
Iteration 48, loss = 0.02237120
Iteration 49, loss = 0.02211364
Iteration 50, loss = 0.02182574
Iteration 51, loss = 0.02183777
Iteration 52, loss = 0.02155906
Iteration 53, loss = 0.02127995
Iteration 54, loss = 0.02108046
Iteration 55, loss = 0.02088698
Iteration 56, loss = 0.02071546
Iteration 57, loss = 0.02067604
Iteration 58, loss = 0.02041211
Iteration 59, loss = 0.02042017
Iteration 60, loss = 0.02022510
Iteration 61, loss = 0.02005176
Iteration 62, loss = 0.02004266
Iteration 63, loss = 0.01989218
Iteration 64, loss = 0.01980013
Iteration 65, loss = 0.01975357
Iteration 66, loss = 0.01960951
Iteration 67, loss = 0.01957158
Iteration 68, loss = 0.01957525
Iteration 69, loss = 0.01937695
Iteration 70, loss = 0.01945696
Iteration 71, loss = 0.01904120
Iteration 72, loss = 0.01905053
Iteration 73, loss = 0.01916959
Iteration 74, loss = 0.01898932
Iteration 75, loss = 0.01882466
Iteration 76, loss = 0.01892446
Iteration 77, loss = 0.01883807
Iteration 78, loss = 0.01884393
Iteration 79, loss = 0.01881403
Iteration 80, loss = 0.01869329
Iteration 81, loss = 0.01869527
Iteration 82, loss = 0.01856180
Iteration 83, loss = 0.01861471
Iteration 84, loss = 0.01850383
Iteration 85, loss = 0.01847663
Iteration 86, loss = 0.01853208
Iteration 87, loss = 0.01840532
Iteration 88, loss = 0.01828595
Iteration 89, loss = 0.01814731
Iteration 90, loss = 0.01825922
Iteration 91, loss = 0.01831269
Iteration 92, loss = 0.01812958
Iteration 93, loss = 0.01819927
Iteration 94, loss = 0.01812096
Iteration 95, loss = 0.01795687
Iteration 96, loss = 0.01830228
Iteration 97, loss = 0.01795932
Iteration 98, loss = 0.01802166
Iteration 99, loss = 0.01802372
Iteration 100, loss = 0.01801464
Iteration 101, loss = 0.01790347
Iteration 102, loss = 0.01787757
Iteration 103, loss = 0.01784871
Iteration 104, loss = 0.01770271
Iteration 105, loss = 0.01788848
Iteration 106, loss = 0.01793258
Iteration 107, loss = 0.01773017
Iteration 108, loss = 0.01775866
Iteration 109, loss = 0.01769537
Iteration 110, loss = 0.01770750
Iteration 111, loss = 0.01775414
Iteration 112, loss = 0.01756635
Iteration 113, loss = 0.01746018
Iteration 114, loss = 0.01760147
Iteration 115, loss = 0.01786579
Iteration 116, loss = 0.01762132
Iteration 117, loss = 0.01754524
Iteration 118, loss = 0.01767660
Iteration 119, loss = 0.01757658
Iteration 120, loss = 0.01746367
Iteration 121, loss = 0.01737968
Iteration 122, loss = 0.01743438
Iteration 123, loss = 0.01743918
Iteration 124, loss = 0.01752032
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69680060
Iteration 2, loss = 0.61792216
Iteration 3, loss = 0.50854574
Iteration 4, loss = 0.39052057
Iteration 5, loss = 0.28970470
Iteration 6, loss = 0.21503513
Iteration 7, loss = 0.16360769
Iteration 8, loss = 0.12837163
Iteration 9, loss = 0.10425559
Iteration 10, loss = 0.08715229
Iteration 11, loss = 0.07464415
Iteration 12, loss = 0.06540995
Iteration 13, loss = 0.05862946
Iteration 14, loss = 0.05240673
Iteration 15, loss = 0.04803109
Iteration 16, loss = 0.04468115
Iteration 17, loss = 0.04181369
Iteration 18, loss = 0.03903505
Iteration 19, loss = 0.03704016
Iteration 20, loss = 0.03542423
Iteration 21, loss = 0.03392445
Iteration 22, loss = 0.03266478
Iteration 23, loss = 0.03163205
Iteration 24, loss = 0.03062648
Iteration 25, loss = 0.02974748
Iteration 26, loss = 0.02884745
Iteration 27, loss = 0.02845547
Iteration 28, loss = 0.02784933
Iteration 29, loss = 0.02721167
Iteration 30, loss = 0.02686353
Iteration 31, loss = 0.02644971
Iteration 32, loss = 0.02590635
Iteration 33, loss = 0.02568846
Iteration 34, loss = 0.02521390
Iteration 35, loss = 0.02496225
Iteration 36, loss = 0.02483220
Iteration 37, loss = 0.02479006
Iteration 38, loss = 0.02427063
Iteration 39, loss = 0.02392613
Iteration 40, loss = 0.02397593
Iteration 41, loss = 0.02377950
Iteration 42, loss = 0.02337720
Iteration 43, loss = 0.02362826
Iteration 44, loss = 0.02330132
Iteration 45, loss = 0.02329759
Iteration 46, loss = 0.02314098
Iteration 47, loss = 0.02294763
Iteration 48, loss = 0.02263341
Iteration 49, loss = 0.02286333
Iteration 50, loss = 0.02266443
Iteration 51, loss = 0.02257341
Iteration 52, loss = 0.02255329
Iteration 53, loss = 0.02237247
Iteration 54, loss = 0.02267411
Iteration 55, loss = 0.02237761
Iteration 56, loss = 0.02202837
Iteration 57, loss = 0.02232496
Iteration 58, loss = 0.02218413
Iteration 59, loss = 0.02195402
Iteration 60, loss = 0.02196197
Iteration 61, loss = 0.02205668
Iteration 62, loss = 0.02197084
Iteration 63, loss = 0.02186126
Iteration 64, loss = 0.02182398
Iteration 65, loss = 0.02181825
Iteration 66, loss = 0.02149064
Iteration 67, loss = 0.02191308
Iteration 68, loss = 0.02187987
Iteration 69, loss = 0.02164144
Iteration 70, loss = 0.02152230
Iteration 71, loss = 0.02131257
Iteration 72, loss = 0.02172997
Iteration 73, loss = 0.02142088
Iteration 74, loss = 0.02160070
Iteration 75, loss = 0.02155402
Iteration 76, loss = 0.02132718
Iteration 77, loss = 0.02136642
Iteration 78, loss = 0.02173563
Iteration 79, loss = 0.02160756
Iteration 80, loss = 0.02150343
Iteration 81, loss = 0.02145038
Iteration 82, loss = 0.02174087
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68377378
Iteration 2, loss = 0.65739968
Iteration 3, loss = 0.61984770
Iteration 4, loss = 0.56890856
Iteration 5, loss = 0.50746939
Iteration 6, loss = 0.44262361
Iteration 7, loss = 0.38093851
Iteration 8, loss = 0.32438575
Iteration 9, loss = 0.27465416
Iteration 10, loss = 0.23226593
Iteration 11, loss = 0.19646244
Iteration 12, loss = 0.16722614
Iteration 13, loss = 0.14298890
Iteration 14, loss = 0.12353232
Iteration 15, loss = 0.10744046
Iteration 16, loss = 0.09425407
Iteration 17, loss = 0.08365676
Iteration 18, loss = 0.07465783
Iteration 19, loss = 0.06714112
Iteration 20, loss = 0.06081473
Iteration 21, loss = 0.05549940
Iteration 22, loss = 0.05091202
Iteration 23, loss = 0.04705008
Iteration 24, loss = 0.04357285
Iteration 25, loss = 0.04056936
Iteration 26, loss = 0.03799687
Iteration 27, loss = 0.03581048
Iteration 28, loss = 0.03378502
Iteration 29, loss = 0.03198591
Iteration 30, loss = 0.03038806
Iteration 31, loss = 0.02901091
Iteration 32, loss = 0.02757915
Iteration 33, loss = 0.02647936
Iteration 34, loss = 0.02538731
Iteration 35, loss = 0.02447605
Iteration 36, loss = 0.02358494
Iteration 37, loss = 0.02284782
Iteration 38, loss = 0.02207522
Iteration 39, loss = 0.02138853
Iteration 40, loss = 0.02084716
Iteration 41, loss = 0.02023959
Iteration 42, loss = 0.01980985
Iteration 43, loss = 0.01919303
Iteration 44, loss = 0.01886928
Iteration 45, loss = 0.01853618
Iteration 46, loss = 0.01794360
Iteration 47, loss = 0.01763519
Iteration 48, loss = 0.01728303
Iteration 49, loss = 0.01697535
Iteration 50, loss = 0.01676518
Iteration 51, loss = 0.01646790
Iteration 52, loss = 0.01615491
Iteration 53, loss = 0.01596108
Iteration 54, loss = 0.01568962
Iteration 55, loss = 0.01553188
Iteration 56, loss = 0.01525814
Iteration 57, loss = 0.01503092
Iteration 58, loss = 0.01487404
Iteration 59, loss = 0.01473598
Iteration 60, loss = 0.01456274
Iteration 61, loss = 0.01438964
Iteration 62, loss = 0.01432465
Iteration 63, loss = 0.01424557
Iteration 64, loss = 0.01414114
Iteration 65, loss = 0.01394359
Iteration 66, loss = 0.01370126
Iteration 67, loss = 0.01358687
Iteration 68, loss = 0.01356033
Iteration 69, loss = 0.01343915
Iteration 70, loss = 0.01332234
Iteration 71, loss = 0.01316185
Iteration 72, loss = 0.01328122
Iteration 73, loss = 0.01296872
Iteration 74, loss = 0.01291043
Iteration 75, loss = 0.01286209
Iteration 76, loss = 0.01279220
Iteration 77, loss = 0.01275629
Iteration 78, loss = 0.01274981
Iteration 79, loss = 0.01265643
Iteration 80, loss = 0.01255273
Iteration 81, loss = 0.01245123
Iteration 82, loss = 0.01238427
Iteration 83, loss = 0.01236304
Iteration 84, loss = 0.01237179
Iteration 85, loss = 0.01218482
Iteration 86, loss = 0.01217779
Iteration 87, loss = 0.01213795
Iteration 88, loss = 0.01210819
Iteration 89, loss = 0.01209496
Iteration 90, loss = 0.01197982
Iteration 91, loss = 0.01199229
Iteration 92, loss = 0.01192906
Iteration 93, loss = 0.01185601
Iteration 94, loss = 0.01180684
Iteration 95, loss = 0.01181402
Iteration 96, loss = 0.01175880
Iteration 97, loss = 0.01183849
Iteration 98, loss = 0.01169125
Iteration 99, loss = 0.01165633
Iteration 100, loss = 0.01169229
Iteration 101, loss = 0.01157326
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68057258
Iteration 2, loss = 0.62175103
Iteration 3, loss = 0.53135291
Iteration 4, loss = 0.42927497
Iteration 5, loss = 0.33714988
Iteration 6, loss = 0.26153414
Iteration 7, loss = 0.20496223
Iteration 8, loss = 0.16286327
Iteration 9, loss = 0.13225461
Iteration 10, loss = 0.11000223
Iteration 11, loss = 0.09528935
Iteration 12, loss = 0.08354262
Iteration 13, loss = 0.07480679
Iteration 14, loss = 0.06656066
Iteration 15, loss = 0.06036815
Iteration 16, loss = 0.05532803
Iteration 17, loss = 0.05106314
Iteration 18, loss = 0.04748882
Iteration 19, loss = 0.04451916
Iteration 20, loss = 0.04206016
Iteration 21, loss = 0.03984149
Iteration 22, loss = 0.03848187
Iteration 23, loss = 0.03781741
Iteration 24, loss = 0.03623567
Iteration 25, loss = 0.03476845
Iteration 26, loss = 0.03367353
Iteration 27, loss = 0.03286959
Iteration 28, loss = 0.03204778
Iteration 29, loss = 0.03120188
Iteration 30, loss = 0.03038336
Iteration 31, loss = 0.02966468
Iteration 32, loss = 0.02905192
Iteration 33, loss = 0.02868113
Iteration 34, loss = 0.02866737
Iteration 35, loss = 0.02791048
Iteration 36, loss = 0.02759448
Iteration 37, loss = 0.02702128
Iteration 38, loss = 0.02665438
Iteration 39, loss = 0.02627161
Iteration 40, loss = 0.02603880
Iteration 41, loss = 0.02555207
Iteration 42, loss = 0.02554594
Iteration 43, loss = 0.02512944
Iteration 44, loss = 0.02536436
Iteration 45, loss = 0.02516385
Iteration 46, loss = 0.02482073
Iteration 47, loss = 0.02459922
Iteration 48, loss = 0.02433246
Iteration 49, loss = 0.02421687
Iteration 50, loss = 0.02389698
Iteration 51, loss = 0.02387003
Iteration 52, loss = 0.02457519
Iteration 53, loss = 0.02429324
Iteration 54, loss = 0.02377748
Iteration 55, loss = 0.02342092
Iteration 56, loss = 0.02347401
Iteration 57, loss = 0.02338642
Iteration 58, loss = 0.02314908
Iteration 59, loss = 0.02287556
Iteration 60, loss = 0.02265025
Iteration 61, loss = 0.02265536
Iteration 62, loss = 0.02258451
Iteration 63, loss = 0.02251394
Iteration 64, loss = 0.02225604
Iteration 65, loss = 0.02212240
Iteration 66, loss = 0.02199206
Iteration 67, loss = 0.02191428
Iteration 68, loss = 0.02457950
Iteration 69, loss = 0.02288789
Iteration 70, loss = 0.02204472
Iteration 71, loss = 0.02177141
Iteration 72, loss = 0.02183122
Iteration 73, loss = 0.02178089
Iteration 74, loss = 0.02162788
Iteration 75, loss = 0.02157971
Iteration 76, loss = 0.02142773
Iteration 77, loss = 0.02127938
Iteration 78, loss = 0.02128637
Iteration 79, loss = 0.02116722
Iteration 80, loss = 0.02113828
Iteration 81, loss = 0.02114032
Iteration 82, loss = 0.02102038
Iteration 83, loss = 0.02097996
Iteration 84, loss = 0.02099663
Iteration 85, loss = 0.02096974
Iteration 86, loss = 0.02121728
Iteration 87, loss = 0.02150433
Iteration 88, loss = 0.02135704
Iteration 89, loss = 0.02117154
Iteration 90, loss = 0.02125485
Iteration 91, loss = 0.02107423
Iteration 92, loss = 0.02091853
Iteration 93, loss = 0.02095422
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70660515
Iteration 2, loss = 0.69180700
Iteration 3, loss = 0.67884052
Iteration 4, loss = 0.66592517
Iteration 5, loss = 0.65209186
Iteration 6, loss = 0.63762560
Iteration 7, loss = 0.62195424
Iteration 8, loss = 0.60492532
Iteration 9, loss = 0.58640675
Iteration 10, loss = 0.56678117
Iteration 11, loss = 0.54580529
Iteration 12, loss = 0.52378607
Iteration 13, loss = 0.50075785
Iteration 14, loss = 0.47709559
Iteration 15, loss = 0.45305687
Iteration 16, loss = 0.42904698
Iteration 17, loss = 0.40531763
Iteration 18, loss = 0.38200676
Iteration 19, loss = 0.35941375
Iteration 20, loss = 0.33767360
Iteration 21, loss = 0.31683231
Iteration 22, loss = 0.29696241
Iteration 23, loss = 0.27813816
Iteration 24, loss = 0.26033796
Iteration 25, loss = 0.24355609
Iteration 26, loss = 0.22789053
Iteration 27, loss = 0.21320583
Iteration 28, loss = 0.19952586
Iteration 29, loss = 0.18675569
Iteration 30, loss = 0.17496115
Iteration 31, loss = 0.16392792
Iteration 32, loss = 0.15374784
Iteration 33, loss = 0.14438493
Iteration 34, loss = 0.13562270
Iteration 35, loss = 0.12758443
Iteration 36, loss = 0.12012764
Iteration 37, loss = 0.11322012
Iteration 38, loss = 0.10687432
Iteration 39, loss = 0.10096613
Iteration 40, loss = 0.09549944
Iteration 41, loss = 0.09040418
Iteration 42, loss = 0.08572968
Iteration 43, loss = 0.08136599
Iteration 44, loss = 0.07729079
Iteration 45, loss = 0.07355172
Iteration 46, loss = 0.07003209
Iteration 47, loss = 0.06676591
Iteration 48, loss = 0.06370451
Iteration 49, loss = 0.06089600
Iteration 50, loss = 0.05823866
Iteration 51, loss = 0.05575978
Iteration 52, loss = 0.05344598
Iteration 53, loss = 0.05125583
Iteration 54, loss = 0.04923022
Iteration 55, loss = 0.04732884
Iteration 56, loss = 0.04550402
Iteration 57, loss = 0.04381081
Iteration 58, loss = 0.04220145
Iteration 59, loss = 0.04068986
Iteration 60, loss = 0.03923999
Iteration 61, loss = 0.03788712
Iteration 62, loss = 0.03658710
Iteration 63, loss = 0.03538227
Iteration 64, loss = 0.03421718
Iteration 65, loss = 0.03311738
Iteration 66, loss = 0.03206690
Iteration 67, loss = 0.03109102
Iteration 68, loss = 0.03012980
Iteration 69, loss = 0.02923503
Iteration 70, loss = 0.02835569
Iteration 71, loss = 0.02755043
Iteration 72, loss = 0.02676183
Iteration 73, loss = 0.02599826
Iteration 74, loss = 0.02529134
Iteration 75, loss = 0.02458804
Iteration 76, loss = 0.02391814
Iteration 77, loss = 0.02328534
Iteration 78, loss = 0.02268715
Iteration 79, loss = 0.02209447
Iteration 80, loss = 0.02152715
Iteration 81, loss = 0.02099886
Iteration 82, loss = 0.02047582
Iteration 83, loss = 0.01997980
Iteration 84, loss = 0.01950151
Iteration 85, loss = 0.01904909
Iteration 86, loss = 0.01859409
Iteration 87, loss = 0.01815393
Iteration 88, loss = 0.01773853
Iteration 89, loss = 0.01734482
Iteration 90, loss = 0.01696171
Iteration 91, loss = 0.01658997
Iteration 92, loss = 0.01623730
Iteration 93, loss = 0.01588537
Iteration 94, loss = 0.01556450
Iteration 95, loss = 0.01523127
Iteration 96, loss = 0.01492655
Iteration 97, loss = 0.01462107
Iteration 98, loss = 0.01432895
Iteration 99, loss = 0.01404061
Iteration 100, loss = 0.01376774
Iteration 101, loss = 0.01350604
Iteration 102, loss = 0.01325082
Iteration 103, loss = 0.01300393
Iteration 104, loss = 0.01275431
Iteration 105, loss = 0.01252394
Iteration 106, loss = 0.01230243
Iteration 107, loss = 0.01207150
Iteration 108, loss = 0.01186239
Iteration 109, loss = 0.01166056
Iteration 110, loss = 0.01145665
Iteration 111, loss = 0.01126414
Iteration 112, loss = 0.01107316
Iteration 113, loss = 0.01089083
Iteration 114, loss = 0.01071526
Iteration 115, loss = 0.01053743
Iteration 116, loss = 0.01037008
Iteration 117, loss = 0.01020587
Iteration 118, loss = 0.01004623
Iteration 119, loss = 0.00988939
Iteration 120, loss = 0.00974076
Iteration 121, loss = 0.00959470
Iteration 122, loss = 0.00944394
Iteration 123, loss = 0.00929684
Iteration 124, loss = 0.00916290
Iteration 125, loss = 0.00901995
Iteration 126, loss = 0.00889092
Iteration 127, loss = 0.00875648
Iteration 128, loss = 0.00863502
Iteration 129, loss = 0.00851121
Iteration 130, loss = 0.00838705
Iteration 131, loss = 0.00827355
Iteration 132, loss = 0.00816383
Iteration 133, loss = 0.00804909
Iteration 134, loss = 0.00794061
Iteration 135, loss = 0.00783517
Iteration 136, loss = 0.00772986
Iteration 137, loss = 0.00762623
Iteration 138, loss = 0.00753013
Iteration 139, loss = 0.00743432
Iteration 140, loss = 0.00733901
Iteration 141, loss = 0.00724885
Iteration 142, loss = 0.00715756
Iteration 143, loss = 0.00706921
Iteration 144, loss = 0.00697971
Iteration 145, loss = 0.00689381
Iteration 146, loss = 0.00681462
Iteration 147, loss = 0.00673619
Iteration 148, loss = 0.00665239
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68722650
Iteration 2, loss = 0.66121514
Iteration 3, loss = 0.62457646
Iteration 4, loss = 0.57221626
Iteration 5, loss = 0.51028676
Iteration 6, loss = 0.44699330
Iteration 7, loss = 0.38683722
Iteration 8, loss = 0.33144990
Iteration 9, loss = 0.28244561
Iteration 10, loss = 0.24041569
Iteration 11, loss = 0.20478431
Iteration 12, loss = 0.17521884
Iteration 13, loss = 0.15102906
Iteration 14, loss = 0.13118157
Iteration 15, loss = 0.11494310
Iteration 16, loss = 0.10154004
Iteration 17, loss = 0.09037967
Iteration 18, loss = 0.08125699
Iteration 19, loss = 0.07347271
Iteration 20, loss = 0.06698993
Iteration 21, loss = 0.06137066
Iteration 22, loss = 0.05674244
Iteration 23, loss = 0.05256309
Iteration 24, loss = 0.04907007
Iteration 25, loss = 0.04605858
Iteration 26, loss = 0.04318101
Iteration 27, loss = 0.04081707
Iteration 28, loss = 0.03876197
Iteration 29, loss = 0.03685606
Iteration 30, loss = 0.03529647
Iteration 31, loss = 0.03375087
Iteration 32, loss = 0.03248964
Iteration 33, loss = 0.03102789
Iteration 34, loss = 0.02998658
Iteration 35, loss = 0.02896716
Iteration 36, loss = 0.02807161
Iteration 37, loss = 0.02726678
Iteration 38, loss = 0.02654815
Iteration 39, loss = 0.02578375
Iteration 40, loss = 0.02523112
Iteration 41, loss = 0.02462359
Iteration 42, loss = 0.02402976
Iteration 43, loss = 0.02363721
Iteration 44, loss = 0.02290314
Iteration 45, loss = 0.02247925
Iteration 46, loss = 0.02223270
Iteration 47, loss = 0.02170799
Iteration 48, loss = 0.02135160
Iteration 49, loss = 0.02103781
Iteration 50, loss = 0.02081490
Iteration 51, loss = 0.02049551
Iteration 52, loss = 0.02037982
Iteration 53, loss = 0.01998298
Iteration 54, loss = 0.01977626
Iteration 55, loss = 0.01947262
Iteration 56, loss = 0.01924119
Iteration 57, loss = 0.01898442
Iteration 58, loss = 0.01899433
Iteration 59, loss = 0.01866082
Iteration 60, loss = 0.01846294
Iteration 61, loss = 0.01835944
Iteration 62, loss = 0.01836259
Iteration 63, loss = 0.01801897
Iteration 64, loss = 0.01801607
Iteration 65, loss = 0.01786958
Iteration 66, loss = 0.01757896
Iteration 67, loss = 0.01752031
Iteration 68, loss = 0.01747239
Iteration 69, loss = 0.01739547
Iteration 70, loss = 0.01712827
Iteration 71, loss = 0.01708986
Iteration 72, loss = 0.01695654
Iteration 73, loss = 0.01683420
Iteration 74, loss = 0.01681812
Iteration 75, loss = 0.01671131
Iteration 76, loss = 0.01673091
Iteration 77, loss = 0.01669340
Iteration 78, loss = 0.01652850
Iteration 79, loss = 0.01649135
Iteration 80, loss = 0.01639288
Iteration 81, loss = 0.01629757
Iteration 82, loss = 0.01621165
Iteration 83, loss = 0.01609810
Iteration 84, loss = 0.01616586
Iteration 85, loss = 0.01615035
Iteration 86, loss = 0.01598498
Iteration 87, loss = 0.01594640
Iteration 88, loss = 0.01582792
Iteration 89, loss = 0.01588274
Iteration 90, loss = 0.01595544
Iteration 91, loss = 0.01578442
Iteration 92, loss = 0.01569452
Iteration 93, loss = 0.01571844
Iteration 94, loss = 0.01564666
Iteration 95, loss = 0.01564084
Iteration 96, loss = 0.01548324
Iteration 97, loss = 0.01544863
Iteration 98, loss = 0.01551035
Iteration 99, loss = 0.01537809
Iteration 100, loss = 0.01545483
Iteration 101, loss = 0.01530586
Iteration 102, loss = 0.01523995
Iteration 103, loss = 0.01528677
Iteration 104, loss = 0.01526365
Iteration 105, loss = 0.01518780
Iteration 106, loss = 0.01519978
Iteration 107, loss = 0.01514414
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70238754
Iteration 2, loss = 0.63363880
Iteration 3, loss = 0.52703321
Iteration 4, loss = 0.40798148
Iteration 5, loss = 0.30480836
Iteration 6, loss = 0.22632362
Iteration 7, loss = 0.17083067
Iteration 8, loss = 0.13294682
Iteration 9, loss = 0.10713500
Iteration 10, loss = 0.08917343
Iteration 11, loss = 0.07593059
Iteration 12, loss = 0.06643163
Iteration 13, loss = 0.05909584
Iteration 14, loss = 0.05317896
Iteration 15, loss = 0.04852757
Iteration 16, loss = 0.04503680
Iteration 17, loss = 0.04191022
Iteration 18, loss = 0.03951522
Iteration 19, loss = 0.03743109
Iteration 20, loss = 0.03589529
Iteration 21, loss = 0.03413873
Iteration 22, loss = 0.03288591
Iteration 23, loss = 0.03151214
Iteration 24, loss = 0.03064196
Iteration 25, loss = 0.02975162
Iteration 26, loss = 0.02904849
Iteration 27, loss = 0.02826403
Iteration 28, loss = 0.02779195
Iteration 29, loss = 0.02750552
Iteration 30, loss = 0.02696217
Iteration 31, loss = 0.02647767
Iteration 32, loss = 0.02604422
Iteration 33, loss = 0.02573178
Iteration 34, loss = 0.02529826
Iteration 35, loss = 0.02509393
Iteration 36, loss = 0.02476787
Iteration 37, loss = 0.02446344
Iteration 38, loss = 0.02442731
Iteration 39, loss = 0.02417866
Iteration 40, loss = 0.02397274
Iteration 41, loss = 0.02358760
Iteration 42, loss = 0.02363647
Iteration 43, loss = 0.02332163
Iteration 44, loss = 0.02319712
Iteration 45, loss = 0.02327862
Iteration 46, loss = 0.02298211
Iteration 47, loss = 0.02286917
Iteration 48, loss = 0.02272430
Iteration 49, loss = 0.02288619
Iteration 50, loss = 0.02255945
Iteration 51, loss = 0.02247637
Iteration 52, loss = 0.02236646
Iteration 53, loss = 0.02226052
Iteration 54, loss = 0.02244331
Iteration 55, loss = 0.02235030
Iteration 56, loss = 0.02216235
Iteration 57, loss = 0.02198910
Iteration 58, loss = 0.02198033
Iteration 59, loss = 0.02227692
Iteration 60, loss = 0.02219884
Iteration 61, loss = 0.02183671
Iteration 62, loss = 0.02163308
Iteration 63, loss = 0.02204478
Iteration 64, loss = 0.02172574
Iteration 65, loss = 0.02168168
Iteration 66, loss = 0.02166168
Iteration 67, loss = 0.02166023
Iteration 68, loss = 0.02142424
Iteration 69, loss = 0.02159879
Iteration 70, loss = 0.02164096
Iteration 71, loss = 0.02139627
Iteration 72, loss = 0.02161970
Iteration 73, loss = 0.02161463
Iteration 74, loss = 0.02151579
Iteration 75, loss = 0.02147581
Iteration 76, loss = 0.02158748
Iteration 77, loss = 0.02158535
Iteration 78, loss = 0.02150576
Iteration 79, loss = 0.02124759
Iteration 80, loss = 0.02149572
Iteration 81, loss = 0.02094451
Iteration 82, loss = 0.02136984
Iteration 83, loss = 0.02109172
Iteration 84, loss = 0.02143550
Iteration 85, loss = 0.02140963
Iteration 86, loss = 0.02123122
Iteration 87, loss = 0.02138412
Iteration 88, loss = 0.02099469
Iteration 89, loss = 0.02133162
Iteration 90, loss = 0.02113426
Iteration 91, loss = 0.02110959
Iteration 92, loss = 0.02095440
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72896602
Iteration 2, loss = 0.71012581
Iteration 3, loss = 0.69439441
Iteration 4, loss = 0.67930903
Iteration 5, loss = 0.66414152
Iteration 6, loss = 0.64808957
Iteration 7, loss = 0.63099631
Iteration 8, loss = 0.61293380
Iteration 9, loss = 0.59372839
Iteration 10, loss = 0.57339366
Iteration 11, loss = 0.55195979
Iteration 12, loss = 0.52985297
Iteration 13, loss = 0.50687300
Iteration 14, loss = 0.48340478
Iteration 15, loss = 0.45962122
Iteration 16, loss = 0.43573434
Iteration 17, loss = 0.41210095
Iteration 18, loss = 0.38871910
Iteration 19, loss = 0.36600821
Iteration 20, loss = 0.34414961
Iteration 21, loss = 0.32315947
Iteration 22, loss = 0.30313206
Iteration 23, loss = 0.28411800
Iteration 24, loss = 0.26612147
Iteration 25, loss = 0.24915201
Iteration 26, loss = 0.23322515
Iteration 27, loss = 0.21839822
Iteration 28, loss = 0.20449476
Iteration 29, loss = 0.19159934
Iteration 30, loss = 0.17965534
Iteration 31, loss = 0.16857884
Iteration 32, loss = 0.15829563
Iteration 33, loss = 0.14876693
Iteration 34, loss = 0.13995766
Iteration 35, loss = 0.13181988
Iteration 36, loss = 0.12427105
Iteration 37, loss = 0.11730709
Iteration 38, loss = 0.11089230
Iteration 39, loss = 0.10487155
Iteration 40, loss = 0.09934153
Iteration 41, loss = 0.09422622
Iteration 42, loss = 0.08948974
Iteration 43, loss = 0.08511872
Iteration 44, loss = 0.08101862
Iteration 45, loss = 0.07722520
Iteration 46, loss = 0.07369558
Iteration 47, loss = 0.07038429
Iteration 48, loss = 0.06731582
Iteration 49, loss = 0.06445508
Iteration 50, loss = 0.06172859
Iteration 51, loss = 0.05920027
Iteration 52, loss = 0.05680367
Iteration 53, loss = 0.05455870
Iteration 54, loss = 0.05248521
Iteration 55, loss = 0.05050304
Iteration 56, loss = 0.04865367
Iteration 57, loss = 0.04689047
Iteration 58, loss = 0.04521865
Iteration 59, loss = 0.04365687
Iteration 60, loss = 0.04217169
Iteration 61, loss = 0.04076923
Iteration 62, loss = 0.03944551
Iteration 63, loss = 0.03818690
Iteration 64, loss = 0.03705253
Iteration 65, loss = 0.03590192
Iteration 66, loss = 0.03484031
Iteration 67, loss = 0.03378892
Iteration 68, loss = 0.03282391
Iteration 69, loss = 0.03191001
Iteration 70, loss = 0.03107505
Iteration 71, loss = 0.03020731
Iteration 72, loss = 0.02940713
Iteration 73, loss = 0.02866020
Iteration 74, loss = 0.02791371
Iteration 75, loss = 0.02722593
Iteration 76, loss = 0.02655486
Iteration 77, loss = 0.02592541
Iteration 78, loss = 0.02529732
Iteration 79, loss = 0.02471323
Iteration 80, loss = 0.02416126
Iteration 81, loss = 0.02362613
Iteration 82, loss = 0.02309667
Iteration 83, loss = 0.02259206
Iteration 84, loss = 0.02211406
Iteration 85, loss = 0.02165396
Iteration 86, loss = 0.02120426
Iteration 87, loss = 0.02076879
Iteration 88, loss = 0.02035188
Iteration 89, loss = 0.01996606
Iteration 90, loss = 0.01959004
Iteration 91, loss = 0.01921340
Iteration 92, loss = 0.01883777
Iteration 93, loss = 0.01852144
Iteration 94, loss = 0.01815993
Iteration 95, loss = 0.01783461
Iteration 96, loss = 0.01752704
Iteration 97, loss = 0.01723538
Iteration 98, loss = 0.01693363
Iteration 99, loss = 0.01664979
Iteration 100, loss = 0.01637972
Iteration 101, loss = 0.01612259
Iteration 102, loss = 0.01585971
Iteration 103, loss = 0.01561531
Iteration 104, loss = 0.01538177
Iteration 105, loss = 0.01517381
Iteration 106, loss = 0.01492420
Iteration 107, loss = 0.01470440
Iteration 108, loss = 0.01451566
Iteration 109, loss = 0.01434814
Iteration 110, loss = 0.01411607
Iteration 111, loss = 0.01391918
Iteration 112, loss = 0.01372290
Iteration 113, loss = 0.01351799
Iteration 114, loss = 0.01335022
Iteration 115, loss = 0.01315673
Iteration 116, loss = 0.01299631
Iteration 117, loss = 0.01286505
Iteration 118, loss = 0.01268857
Iteration 119, loss = 0.01256613
Iteration 120, loss = 0.01239681
Iteration 121, loss = 0.01225372
Iteration 122, loss = 0.01211347
Iteration 123, loss = 0.01199842
Iteration 124, loss = 0.01188288
Iteration 125, loss = 0.01175629
Iteration 126, loss = 0.01162932
Iteration 127, loss = 0.01150467
Iteration 128, loss = 0.01138341
Iteration 129, loss = 0.01126978
Iteration 130, loss = 0.01113028
Iteration 131, loss = 0.01101223
Iteration 132, loss = 0.01087625
Iteration 133, loss = 0.01076517
Iteration 134, loss = 0.01064977
Iteration 135, loss = 0.01055915
Iteration 136, loss = 0.01043260
Iteration 137, loss = 0.01033413
Iteration 138, loss = 0.01023402
Iteration 139, loss = 0.01013671
Iteration 140, loss = 0.01004917
Iteration 141, loss = 0.00995724
Iteration 142, loss = 0.00987516
Iteration 143, loss = 0.00979708
Iteration 144, loss = 0.00970967
Iteration 145, loss = 0.00961538
Iteration 146, loss = 0.00952951
Iteration 147, loss = 0.00945670
Iteration 148, loss = 0.00937082
Iteration 149, loss = 0.00930417
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71835808
Iteration 2, loss = 0.69168692
Iteration 3, loss = 0.66697309
Iteration 4, loss = 0.63916775
Iteration 5, loss = 0.60809088
Iteration 6, loss = 0.57405234
Iteration 7, loss = 0.53722590
Iteration 8, loss = 0.49921054
Iteration 9, loss = 0.46114454
Iteration 10, loss = 0.42326578
Iteration 11, loss = 0.38700684
Iteration 12, loss = 0.35289209
Iteration 13, loss = 0.32093495
Iteration 14, loss = 0.29158894
Iteration 15, loss = 0.26474541
Iteration 16, loss = 0.24051892
Iteration 17, loss = 0.21863181
Iteration 18, loss = 0.19901814
Iteration 19, loss = 0.18142634
Iteration 20, loss = 0.16583469
Iteration 21, loss = 0.15187033
Iteration 22, loss = 0.13949474
Iteration 23, loss = 0.12857255
Iteration 24, loss = 0.11880431
Iteration 25, loss = 0.11016952
Iteration 26, loss = 0.10240851
Iteration 27, loss = 0.09544311
Iteration 28, loss = 0.08926317
Iteration 29, loss = 0.08368188
Iteration 30, loss = 0.07870135
Iteration 31, loss = 0.07415111
Iteration 32, loss = 0.07004955
Iteration 33, loss = 0.06637093
Iteration 34, loss = 0.06296426
Iteration 35, loss = 0.05994928
Iteration 36, loss = 0.05714900
Iteration 37, loss = 0.05453067
Iteration 38, loss = 0.05217260
Iteration 39, loss = 0.05002765
Iteration 40, loss = 0.04804196
Iteration 41, loss = 0.04615101
Iteration 42, loss = 0.04450940
Iteration 43, loss = 0.04289782
Iteration 44, loss = 0.04144250
Iteration 45, loss = 0.04001235
Iteration 46, loss = 0.03877551
Iteration 47, loss = 0.03758422
Iteration 48, loss = 0.03645579
Iteration 49, loss = 0.03549492
Iteration 50, loss = 0.03445607
Iteration 51, loss = 0.03351313
Iteration 52, loss = 0.03263753
Iteration 53, loss = 0.03186167
Iteration 54, loss = 0.03112427
Iteration 55, loss = 0.03042152
Iteration 56, loss = 0.02970624
Iteration 57, loss = 0.02905008
Iteration 58, loss = 0.02844381
Iteration 59, loss = 0.02787849
Iteration 60, loss = 0.02733675
Iteration 61, loss = 0.02676995
Iteration 62, loss = 0.02637634
Iteration 63, loss = 0.02585727
Iteration 64, loss = 0.02538245
Iteration 65, loss = 0.02497262
Iteration 66, loss = 0.02458613
Iteration 67, loss = 0.02417862
Iteration 68, loss = 0.02379752
Iteration 69, loss = 0.02347712
Iteration 70, loss = 0.02313986
Iteration 71, loss = 0.02285594
Iteration 72, loss = 0.02247759
Iteration 73, loss = 0.02221586
Iteration 74, loss = 0.02202709
Iteration 75, loss = 0.02165286
Iteration 76, loss = 0.02138488
Iteration 77, loss = 0.02119040
Iteration 78, loss = 0.02096159
Iteration 79, loss = 0.02071578
Iteration 80, loss = 0.02045424
Iteration 81, loss = 0.02031125
Iteration 82, loss = 0.02007309
Iteration 83, loss = 0.01987636
Iteration 84, loss = 0.01972778
Iteration 85, loss = 0.01954401
Iteration 86, loss = 0.01933776
Iteration 87, loss = 0.01913105
Iteration 88, loss = 0.01898403
Iteration 89, loss = 0.01886656
Iteration 90, loss = 0.01870504
Iteration 91, loss = 0.01855130
Iteration 92, loss = 0.01836714
Iteration 93, loss = 0.01825130
Iteration 94, loss = 0.01813639
Iteration 95, loss = 0.01797474
Iteration 96, loss = 0.01786887
Iteration 97, loss = 0.01776517
Iteration 98, loss = 0.01768143
Iteration 99, loss = 0.01753045
Iteration 100, loss = 0.01738885
Iteration 101, loss = 0.01732242
Iteration 102, loss = 0.01725433
Iteration 103, loss = 0.01711424
Iteration 104, loss = 0.01701836
Iteration 105, loss = 0.01692648
Iteration 106, loss = 0.01684321
Iteration 107, loss = 0.01684586
Iteration 108, loss = 0.01671267
Iteration 109, loss = 0.01656753
Iteration 110, loss = 0.01652400
Iteration 111, loss = 0.01646159
Iteration 112, loss = 0.01636638
Iteration 113, loss = 0.01627282
Iteration 114, loss = 0.01619805
Iteration 115, loss = 0.01617740
Iteration 116, loss = 0.01610729
Iteration 117, loss = 0.01601330
Iteration 118, loss = 0.01605010
Iteration 119, loss = 0.01588190
Iteration 120, loss = 0.01581860
Iteration 121, loss = 0.01570393
Iteration 122, loss = 0.01567484
Iteration 123, loss = 0.01563661
Iteration 124, loss = 0.01555710
Iteration 125, loss = 0.01553395
Iteration 126, loss = 0.01548579
Iteration 127, loss = 0.01541312
Iteration 128, loss = 0.01537848
Iteration 129, loss = 0.01534872
Iteration 130, loss = 0.01529752
Iteration 131, loss = 0.01524662
Iteration 132, loss = 0.01516810
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68930577
Iteration 2, loss = 0.63283624
Iteration 3, loss = 0.55627983
Iteration 4, loss = 0.46417065
Iteration 5, loss = 0.37600023
Iteration 6, loss = 0.29900948
Iteration 7, loss = 0.23681964
Iteration 8, loss = 0.18917045
Iteration 9, loss = 0.15294460
Iteration 10, loss = 0.12613853
Iteration 11, loss = 0.10597738
Iteration 12, loss = 0.09093082
Iteration 13, loss = 0.07923424
Iteration 14, loss = 0.07002136
Iteration 15, loss = 0.06274977
Iteration 16, loss = 0.05674640
Iteration 17, loss = 0.05227287
Iteration 18, loss = 0.04796215
Iteration 19, loss = 0.04478165
Iteration 20, loss = 0.04205733
Iteration 21, loss = 0.03960840
Iteration 22, loss = 0.03756288
Iteration 23, loss = 0.03582299
Iteration 24, loss = 0.03420631
Iteration 25, loss = 0.03306836
Iteration 26, loss = 0.03184201
Iteration 27, loss = 0.03075856
Iteration 28, loss = 0.02994699
Iteration 29, loss = 0.02914135
Iteration 30, loss = 0.02843140
Iteration 31, loss = 0.02751426
Iteration 32, loss = 0.02687928
Iteration 33, loss = 0.02633521
Iteration 34, loss = 0.02605672
Iteration 35, loss = 0.02565831
Iteration 36, loss = 0.02521159
Iteration 37, loss = 0.02467319
Iteration 38, loss = 0.02425927
Iteration 39, loss = 0.02399791
Iteration 40, loss = 0.02363252
Iteration 41, loss = 0.02348727
Iteration 42, loss = 0.02332583
Iteration 43, loss = 0.02308132
Iteration 44, loss = 0.02272058
Iteration 45, loss = 0.02248802
Iteration 46, loss = 0.02247601
Iteration 47, loss = 0.02218935
Iteration 48, loss = 0.02211111
Iteration 49, loss = 0.02188174
Iteration 50, loss = 0.02179459
Iteration 51, loss = 0.02158137
Iteration 52, loss = 0.02153890
Iteration 53, loss = 0.02143291
Iteration 54, loss = 0.02118830
Iteration 55, loss = 0.02115295
Iteration 56, loss = 0.02097676
Iteration 57, loss = 0.02096326
Iteration 58, loss = 0.02088719
Iteration 59, loss = 0.02079352
Iteration 60, loss = 0.02072490
Iteration 61, loss = 0.02053694
Iteration 62, loss = 0.02039762
Iteration 63, loss = 0.02054715
Iteration 64, loss = 0.02052034
Iteration 65, loss = 0.02024698
Iteration 66, loss = 0.02052827
Iteration 67, loss = 0.02028867
Iteration 68, loss = 0.02031808
Iteration 69, loss = 0.02015943
Iteration 70, loss = 0.02019189
Iteration 71, loss = 0.01996583
Iteration 72, loss = 0.02012082
Iteration 73, loss = 0.02008482
Iteration 74, loss = 0.01989994
Iteration 75, loss = 0.01982876
Iteration 76, loss = 0.01981384
Iteration 77, loss = 0.01977093
Iteration 78, loss = 0.01977701
Iteration 79, loss = 0.01976429
Iteration 80, loss = 0.01961171
Iteration 81, loss = 0.01976035
Iteration 82, loss = 0.01972860
Iteration 83, loss = 0.01952114
Iteration 84, loss = 0.01966770
Iteration 85, loss = 0.01948565
Iteration 86, loss = 0.01953904
Iteration 87, loss = 0.01959156
Iteration 88, loss = 0.01944704
Iteration 89, loss = 0.01970777
Iteration 90, loss = 0.01953152
Iteration 91, loss = 0.01943510
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	88m45.714s
user	410m53.299s
sys	51m17.333s
Elapsed time: 5326 seconds
