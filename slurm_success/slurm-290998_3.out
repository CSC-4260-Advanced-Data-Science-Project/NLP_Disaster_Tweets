Processing dataset: dropped_v4_stemmed

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: dropped_v4_stemmed (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on dropped_v4_stemmed...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on dropped_v4_stemmed...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on dropped_v4_stemmed...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on dropped_v4_stemmed...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on dropped_v4_stemmed...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.68275623
Iteration 2, loss = 0.61097720
Iteration 3, loss = 0.50494559
Iteration 4, loss = 0.39428406
Iteration 5, loss = 0.29838958
Iteration 6, loss = 0.22497913
Iteration 7, loss = 0.17310672
Iteration 8, loss = 0.13727081
Iteration 9, loss = 0.11194658
Iteration 10, loss = 0.09398745
Iteration 11, loss = 0.08095498
Iteration 12, loss = 0.07102413
Iteration 13, loss = 0.06346551
Iteration 14, loss = 0.05747512
Iteration 15, loss = 0.05282660
Iteration 16, loss = 0.04910778
Iteration 17, loss = 0.04577706
Iteration 18, loss = 0.04303775
Iteration 19, loss = 0.04085492
Iteration 20, loss = 0.03929699
Iteration 21, loss = 0.03762388
Iteration 22, loss = 0.03635293
Iteration 23, loss = 0.03493530
Iteration 24, loss = 0.03392822
Iteration 25, loss = 0.03323480
Iteration 26, loss = 0.03213560
Iteration 27, loss = 0.03135056
Iteration 28, loss = 0.03063096
Iteration 29, loss = 0.03006584
Iteration 30, loss = 0.02977786
Iteration 31, loss = 0.02919369
Iteration 32, loss = 0.02868431
Iteration 33, loss = 0.02857861
Iteration 34, loss = 0.02824286
Iteration 35, loss = 0.02776994
Iteration 36, loss = 0.02776298
Iteration 37, loss = 0.02708895
Iteration 38, loss = 0.02745081
Iteration 39, loss = 0.02699855
Iteration 40, loss = 0.02690271
Iteration 41, loss = 0.02646495
Iteration 42, loss = 0.02627437
Iteration 43, loss = 0.02623093
Iteration 44, loss = 0.02594396
Iteration 45, loss = 0.02591020
Iteration 46, loss = 0.02568673
Iteration 47, loss = 0.02554805
Iteration 48, loss = 0.02571301
Iteration 49, loss = 0.02545379
Iteration 50, loss = 0.02531957
Iteration 51, loss = 0.02520277
Iteration 52, loss = 0.02520890
Iteration 53, loss = 0.02532253
Iteration 54, loss = 0.02507677
Iteration 55, loss = 0.02487957
Iteration 56, loss = 0.02500244
Iteration 57, loss = 0.02458165
Iteration 58, loss = 0.02488057
Iteration 59, loss = 0.02498274
Iteration 60, loss = 0.02469917
Iteration 61, loss = 0.02448112
Iteration 62, loss = 0.02480192
Iteration 63, loss = 0.02463825
Iteration 64, loss = 0.02432839
Iteration 65, loss = 0.02439904
Iteration 66, loss = 0.02436317
Iteration 67, loss = 0.02437301
Iteration 68, loss = 0.02427648
Iteration 69, loss = 0.02427169
Iteration 70, loss = 0.02431122
Iteration 71, loss = 0.02389527
Iteration 72, loss = 0.02429513
Iteration 73, loss = 0.02406085
Iteration 74, loss = 0.02409098
Iteration 75, loss = 0.02446806
Iteration 76, loss = 0.02407482
Iteration 77, loss = 0.02397006
Iteration 78, loss = 0.02388362
Iteration 79, loss = 0.02363812
Iteration 80, loss = 0.02398221
Iteration 81, loss = 0.02390209
Iteration 82, loss = 0.02381481
Iteration 83, loss = 0.02398812
Iteration 84, loss = 0.02391397
Iteration 85, loss = 0.02373156
Iteration 86, loss = 0.02392831
Iteration 87, loss = 0.02360973
Iteration 88, loss = 0.02389342
Iteration 89, loss = 0.02350453
Iteration 90, loss = 0.02386613
Iteration 91, loss = 0.02389765
Iteration 92, loss = 0.02381644
Iteration 93, loss = 0.02383082
Iteration 94, loss = 0.02353176
Iteration 95, loss = 0.02376491
Iteration 96, loss = 0.02382945
Iteration 97, loss = 0.02365434
Iteration 98, loss = 0.02358502
Iteration 99, loss = 0.02382176
Iteration 100, loss = 0.02345039
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67168460
Iteration 2, loss = 0.58998671
Iteration 3, loss = 0.47771364
Iteration 4, loss = 0.36795110
Iteration 5, loss = 0.27754717
Iteration 6, loss = 0.21037015
Iteration 7, loss = 0.16309806
Iteration 8, loss = 0.13059039
Iteration 9, loss = 0.10756984
Iteration 10, loss = 0.09116040
Iteration 11, loss = 0.07884627
Iteration 12, loss = 0.06969955
Iteration 13, loss = 0.06252215
Iteration 14, loss = 0.05717135
Iteration 15, loss = 0.05244387
Iteration 16, loss = 0.04872509
Iteration 17, loss = 0.04583969
Iteration 18, loss = 0.04322622
Iteration 19, loss = 0.04114289
Iteration 20, loss = 0.03930689
Iteration 21, loss = 0.03799553
Iteration 22, loss = 0.03658282
Iteration 23, loss = 0.03556817
Iteration 24, loss = 0.03437195
Iteration 25, loss = 0.03323720
Iteration 26, loss = 0.03302574
Iteration 27, loss = 0.03224795
Iteration 28, loss = 0.03143967
Iteration 29, loss = 0.03082989
Iteration 30, loss = 0.03017492
Iteration 31, loss = 0.02983862
Iteration 32, loss = 0.02937484
Iteration 33, loss = 0.02881416
Iteration 34, loss = 0.02871091
Iteration 35, loss = 0.02860801
Iteration 36, loss = 0.02852565
Iteration 37, loss = 0.02797483
Iteration 38, loss = 0.02803295
Iteration 39, loss = 0.02760679
Iteration 40, loss = 0.02734028
Iteration 41, loss = 0.02727772
Iteration 42, loss = 0.02700486
Iteration 43, loss = 0.02659487
Iteration 44, loss = 0.02667643
Iteration 45, loss = 0.02656607
Iteration 46, loss = 0.02647840
Iteration 47, loss = 0.02629363
Iteration 48, loss = 0.02617160
Iteration 49, loss = 0.02606430
Iteration 50, loss = 0.02576541
Iteration 51, loss = 0.02585909
Iteration 52, loss = 0.02608201
Iteration 53, loss = 0.02565410
Iteration 54, loss = 0.02575267
Iteration 55, loss = 0.02561481
Iteration 56, loss = 0.02590566
Iteration 57, loss = 0.02553603
Iteration 58, loss = 0.02555760
Iteration 59, loss = 0.02526397
Iteration 60, loss = 0.02506970
Iteration 61, loss = 0.02514058
Iteration 62, loss = 0.02510162
Iteration 63, loss = 0.02529390
Iteration 64, loss = 0.02510942
Iteration 65, loss = 0.02502706
Iteration 66, loss = 0.02494881
Iteration 67, loss = 0.02483676
Iteration 68, loss = 0.02482013
Iteration 69, loss = 0.02473304
Iteration 70, loss = 0.02456214
Iteration 71, loss = 0.02452532
Iteration 72, loss = 0.02474315
Iteration 73, loss = 0.02465573
Iteration 74, loss = 0.02448805
Iteration 75, loss = 0.02489814
Iteration 76, loss = 0.02462768
Iteration 77, loss = 0.02438849
Iteration 78, loss = 0.02481653
Iteration 79, loss = 0.02460696
Iteration 80, loss = 0.02460938
Iteration 81, loss = 0.02447887
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67473308
Iteration 2, loss = 0.61030984
Iteration 3, loss = 0.49940301
Iteration 4, loss = 0.38524137
Iteration 5, loss = 0.28861812
Iteration 6, loss = 0.21636025
Iteration 7, loss = 0.16590161
Iteration 8, loss = 0.13120220
Iteration 9, loss = 0.10692209
Iteration 10, loss = 0.08966515
Iteration 11, loss = 0.07710183
Iteration 12, loss = 0.06744514
Iteration 13, loss = 0.06023316
Iteration 14, loss = 0.05423386
Iteration 15, loss = 0.04983723
Iteration 16, loss = 0.04604535
Iteration 17, loss = 0.04314862
Iteration 18, loss = 0.04056127
Iteration 19, loss = 0.03830051
Iteration 20, loss = 0.03635439
Iteration 21, loss = 0.03490517
Iteration 22, loss = 0.03386261
Iteration 23, loss = 0.03254655
Iteration 24, loss = 0.03138304
Iteration 25, loss = 0.03045716
Iteration 26, loss = 0.03002941
Iteration 27, loss = 0.02883012
Iteration 28, loss = 0.02848767
Iteration 29, loss = 0.02805541
Iteration 30, loss = 0.02737420
Iteration 31, loss = 0.02709297
Iteration 32, loss = 0.02662497
Iteration 33, loss = 0.02626906
Iteration 34, loss = 0.02593437
Iteration 35, loss = 0.02555295
Iteration 36, loss = 0.02522020
Iteration 37, loss = 0.02515719
Iteration 38, loss = 0.02479197
Iteration 39, loss = 0.02476350
Iteration 40, loss = 0.02415287
Iteration 41, loss = 0.02421522
Iteration 42, loss = 0.02433233
Iteration 43, loss = 0.02394751
Iteration 44, loss = 0.02387634
Iteration 45, loss = 0.02343215
Iteration 46, loss = 0.02367494
Iteration 47, loss = 0.02356790
Iteration 48, loss = 0.02311694
Iteration 49, loss = 0.02307509
Iteration 50, loss = 0.02293132
Iteration 51, loss = 0.02302599
Iteration 52, loss = 0.02300852
Iteration 53, loss = 0.02263666
Iteration 54, loss = 0.02274640
Iteration 55, loss = 0.02269658
Iteration 56, loss = 0.02270989
Iteration 57, loss = 0.02240852
Iteration 58, loss = 0.02241082
Iteration 59, loss = 0.02226385
Iteration 60, loss = 0.02225120
Iteration 61, loss = 0.02220053
Iteration 62, loss = 0.02266801
Iteration 63, loss = 0.02235354
Iteration 64, loss = 0.02223941
Iteration 65, loss = 0.02200070
Iteration 66, loss = 0.02177930
Iteration 67, loss = 0.02208353
Iteration 68, loss = 0.02211040
Iteration 69, loss = 0.02203244
Iteration 70, loss = 0.02188145
Iteration 71, loss = 0.02154717
Iteration 72, loss = 0.02174839
Iteration 73, loss = 0.02206264
Iteration 74, loss = 0.02229168
Iteration 75, loss = 0.02207569
Iteration 76, loss = 0.02185891
Iteration 77, loss = 0.02179031
Iteration 78, loss = 0.02165088
Iteration 79, loss = 0.02169638
Iteration 80, loss = 0.02155912
Iteration 81, loss = 0.02148134
Iteration 82, loss = 0.02134635
Iteration 83, loss = 0.02159662
Iteration 84, loss = 0.02163584
Iteration 85, loss = 0.02154005
Iteration 86, loss = 0.02143464
Iteration 87, loss = 0.02155383
Iteration 88, loss = 0.02122646
Iteration 89, loss = 0.02128509
Iteration 90, loss = 0.02141837
Iteration 91, loss = 0.02136581
Iteration 92, loss = 0.02143329
Iteration 93, loss = 0.02145463
Iteration 94, loss = 0.02144484
Iteration 95, loss = 0.02143561
Iteration 96, loss = 0.02127501
Iteration 97, loss = 0.02117987
Iteration 98, loss = 0.02128008
Iteration 99, loss = 0.02113895
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67717925
Iteration 2, loss = 0.59951618
Iteration 3, loss = 0.48625888
Iteration 4, loss = 0.37157302
Iteration 5, loss = 0.27814028
Iteration 6, loss = 0.20971876
Iteration 7, loss = 0.16131206
Iteration 8, loss = 0.12818689
Iteration 9, loss = 0.10469874
Iteration 10, loss = 0.08807818
Iteration 11, loss = 0.07558642
Iteration 12, loss = 0.06632283
Iteration 13, loss = 0.05918496
Iteration 14, loss = 0.05347795
Iteration 15, loss = 0.04899718
Iteration 16, loss = 0.04546980
Iteration 17, loss = 0.04248274
Iteration 18, loss = 0.03987007
Iteration 19, loss = 0.03771957
Iteration 20, loss = 0.03609139
Iteration 21, loss = 0.03448051
Iteration 22, loss = 0.03303209
Iteration 23, loss = 0.03200021
Iteration 24, loss = 0.03077909
Iteration 25, loss = 0.02999685
Iteration 26, loss = 0.02948185
Iteration 27, loss = 0.02848985
Iteration 28, loss = 0.02776926
Iteration 29, loss = 0.02747347
Iteration 30, loss = 0.02707857
Iteration 31, loss = 0.02635081
Iteration 32, loss = 0.02618354
Iteration 33, loss = 0.02587239
Iteration 34, loss = 0.02560002
Iteration 35, loss = 0.02505102
Iteration 36, loss = 0.02503678
Iteration 37, loss = 0.02473709
Iteration 38, loss = 0.02440183
Iteration 39, loss = 0.02431758
Iteration 40, loss = 0.02408592
Iteration 41, loss = 0.02374832
Iteration 42, loss = 0.02347172
Iteration 43, loss = 0.02345353
Iteration 44, loss = 0.02337945
Iteration 45, loss = 0.02351829
Iteration 46, loss = 0.02330809
Iteration 47, loss = 0.02316609
Iteration 48, loss = 0.02282901
Iteration 49, loss = 0.02286750
Iteration 50, loss = 0.02269613
Iteration 51, loss = 0.02266405
Iteration 52, loss = 0.02244926
Iteration 53, loss = 0.02248425
Iteration 54, loss = 0.02267032
Iteration 55, loss = 0.02232411
Iteration 56, loss = 0.02235975
Iteration 57, loss = 0.02198650
Iteration 58, loss = 0.02218967
Iteration 59, loss = 0.02240262
Iteration 60, loss = 0.02236670
Iteration 61, loss = 0.02214261
Iteration 62, loss = 0.02207033
Iteration 63, loss = 0.02188552
Iteration 64, loss = 0.02187608
Iteration 65, loss = 0.02161899
Iteration 66, loss = 0.02169062
Iteration 67, loss = 0.02156679
Iteration 68, loss = 0.02174016
Iteration 69, loss = 0.02165991
Iteration 70, loss = 0.02151541
Iteration 71, loss = 0.02146790
Iteration 72, loss = 0.02145489
Iteration 73, loss = 0.02181701
Iteration 74, loss = 0.02164225
Iteration 75, loss = 0.02133129
Iteration 76, loss = 0.02128680
Iteration 77, loss = 0.02146205
Iteration 78, loss = 0.02135114
Iteration 79, loss = 0.02140987
Iteration 80, loss = 0.02168351
Iteration 81, loss = 0.02137830
Iteration 82, loss = 0.02137918
Iteration 83, loss = 0.02124765
Iteration 84, loss = 0.02137376
Iteration 85, loss = 0.02124803
Iteration 86, loss = 0.02143133
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69599962
Iteration 2, loss = 0.61932894
Iteration 3, loss = 0.50866756
Iteration 4, loss = 0.39121339
Iteration 5, loss = 0.29134350
Iteration 6, loss = 0.21805645
Iteration 7, loss = 0.16701171
Iteration 8, loss = 0.13194978
Iteration 9, loss = 0.10772261
Iteration 10, loss = 0.09028980
Iteration 11, loss = 0.07786404
Iteration 12, loss = 0.06841326
Iteration 13, loss = 0.06082020
Iteration 14, loss = 0.05489021
Iteration 15, loss = 0.05045808
Iteration 16, loss = 0.04672734
Iteration 17, loss = 0.04333755
Iteration 18, loss = 0.04098204
Iteration 19, loss = 0.03897269
Iteration 20, loss = 0.03683822
Iteration 21, loss = 0.03525117
Iteration 22, loss = 0.03384063
Iteration 23, loss = 0.03269648
Iteration 24, loss = 0.03203809
Iteration 25, loss = 0.03079331
Iteration 26, loss = 0.03007160
Iteration 27, loss = 0.02925316
Iteration 28, loss = 0.02881597
Iteration 29, loss = 0.02831358
Iteration 30, loss = 0.02763260
Iteration 31, loss = 0.02722275
Iteration 32, loss = 0.02680105
Iteration 33, loss = 0.02626085
Iteration 34, loss = 0.02596216
Iteration 35, loss = 0.02565945
Iteration 36, loss = 0.02596583
Iteration 37, loss = 0.02516723
Iteration 38, loss = 0.02494957
Iteration 39, loss = 0.02473931
Iteration 40, loss = 0.02455921
Iteration 41, loss = 0.02416975
Iteration 42, loss = 0.02439807
Iteration 43, loss = 0.02421444
Iteration 44, loss = 0.02381981
Iteration 45, loss = 0.02367311
Iteration 46, loss = 0.02363085
Iteration 47, loss = 0.02362212
Iteration 48, loss = 0.02328973
Iteration 49, loss = 0.02324793
Iteration 50, loss = 0.02325008
Iteration 51, loss = 0.02286165
Iteration 52, loss = 0.02301603
Iteration 53, loss = 0.02300190
Iteration 54, loss = 0.02271896
Iteration 55, loss = 0.02293411
Iteration 56, loss = 0.02250172
Iteration 57, loss = 0.02256488
Iteration 58, loss = 0.02239355
Iteration 59, loss = 0.02245230
Iteration 60, loss = 0.02220222
Iteration 61, loss = 0.02206800
Iteration 62, loss = 0.02244530
Iteration 63, loss = 0.02233851
Iteration 64, loss = 0.02203804
Iteration 65, loss = 0.02207829
Iteration 66, loss = 0.02210517
Iteration 67, loss = 0.02237737
Iteration 68, loss = 0.02203767
Iteration 69, loss = 0.02216067
Iteration 70, loss = 0.02198605
Iteration 71, loss = 0.02182604
Iteration 72, loss = 0.02220641
Iteration 73, loss = 0.02174626
Iteration 74, loss = 0.02225161
Iteration 75, loss = 0.02186452
Iteration 76, loss = 0.02173460
Iteration 77, loss = 0.02167871
Iteration 78, loss = 0.02169295
Iteration 79, loss = 0.02162065
Iteration 80, loss = 0.02178099
Iteration 81, loss = 0.02170332
Iteration 82, loss = 0.02151866
Iteration 83, loss = 0.02142105
Iteration 84, loss = 0.02171717
Iteration 85, loss = 0.02173604
Iteration 86, loss = 0.02150683
Iteration 87, loss = 0.02156845
Iteration 88, loss = 0.02169035
Iteration 89, loss = 0.02149565
Iteration 90, loss = 0.02146846
Iteration 91, loss = 0.02158737
Iteration 92, loss = 0.02145543
Iteration 93, loss = 0.02163851
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68275623
Iteration 2, loss = 0.61097720
Iteration 3, loss = 0.50494559
Iteration 4, loss = 0.39428406
Iteration 5, loss = 0.29838958
Iteration 6, loss = 0.22497913
Iteration 7, loss = 0.17310672
Iteration 8, loss = 0.13727081
Iteration 9, loss = 0.11194658
Iteration 10, loss = 0.09398745
Iteration 11, loss = 0.08095498
Iteration 12, loss = 0.07102413
Iteration 13, loss = 0.06346551
Iteration 14, loss = 0.05747512
Iteration 15, loss = 0.05282660
Iteration 16, loss = 0.04910778
Iteration 17, loss = 0.04577706
Iteration 18, loss = 0.04303775
Iteration 19, loss = 0.04085492
Iteration 20, loss = 0.03929699
Iteration 21, loss = 0.03762388
Iteration 22, loss = 0.03635293
Iteration 23, loss = 0.03493530
Iteration 24, loss = 0.03392822
Iteration 25, loss = 0.03323480
Iteration 26, loss = 0.03213560
Iteration 27, loss = 0.03135056
Iteration 28, loss = 0.03063096
Iteration 29, loss = 0.03006584
Iteration 30, loss = 0.02977786
Iteration 31, loss = 0.02919369
Iteration 32, loss = 0.02868431
Iteration 33, loss = 0.02857861
Iteration 34, loss = 0.02824286
Iteration 35, loss = 0.02776994
Iteration 36, loss = 0.02776298
Iteration 37, loss = 0.02708895
Iteration 38, loss = 0.02745081
Iteration 39, loss = 0.02699855
Iteration 40, loss = 0.02690271
Iteration 41, loss = 0.02646495
Iteration 42, loss = 0.02627437
Iteration 43, loss = 0.02623093
Iteration 44, loss = 0.02594396
Iteration 45, loss = 0.02591020
Iteration 46, loss = 0.02568673
Iteration 47, loss = 0.02554805
Iteration 48, loss = 0.02571301
Iteration 49, loss = 0.02545379
Iteration 50, loss = 0.02531957
Iteration 51, loss = 0.02520277
Iteration 52, loss = 0.02520890
Iteration 53, loss = 0.02532253
Iteration 54, loss = 0.02507677
Iteration 55, loss = 0.02487957
Iteration 56, loss = 0.02500244
Iteration 57, loss = 0.02458165
Iteration 58, loss = 0.02488057
Iteration 59, loss = 0.02498274
Iteration 60, loss = 0.02469917
Iteration 61, loss = 0.02448112
Iteration 62, loss = 0.02480192
Iteration 63, loss = 0.02463825
Iteration 64, loss = 0.02432839
Iteration 65, loss = 0.02439904
Iteration 66, loss = 0.02436317
Iteration 67, loss = 0.02437301
Iteration 68, loss = 0.02427648
Iteration 69, loss = 0.02427169
Iteration 70, loss = 0.02431122
Iteration 71, loss = 0.02389527
Iteration 72, loss = 0.02429513
Iteration 73, loss = 0.02406085
Iteration 74, loss = 0.02409098
Iteration 75, loss = 0.02446806
Iteration 76, loss = 0.02407482
Iteration 77, loss = 0.02397006
Iteration 78, loss = 0.02388362
Iteration 79, loss = 0.02363812
Iteration 80, loss = 0.02398221
Iteration 81, loss = 0.02390209
Iteration 82, loss = 0.02381481
Iteration 83, loss = 0.02398812
Iteration 84, loss = 0.02391397
Iteration 85, loss = 0.02373156
Iteration 86, loss = 0.02392831
Iteration 87, loss = 0.02360973
Iteration 88, loss = 0.02389342
Iteration 89, loss = 0.02350453
Iteration 90, loss = 0.02386613
Iteration 91, loss = 0.02389765
Iteration 92, loss = 0.02381644
Iteration 93, loss = 0.02383082
Iteration 94, loss = 0.02353176
Iteration 95, loss = 0.02376491
Iteration 96, loss = 0.02382945
Iteration 97, loss = 0.02365434
Iteration 98, loss = 0.02358502
Iteration 99, loss = 0.02382176
Iteration 100, loss = 0.02345039
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67168460
Iteration 2, loss = 0.58998671
Iteration 3, loss = 0.47771364
Iteration 4, loss = 0.36795110
Iteration 5, loss = 0.27754717
Iteration 6, loss = 0.21037015
Iteration 7, loss = 0.16309806
Iteration 8, loss = 0.13059039
Iteration 9, loss = 0.10756984
Iteration 10, loss = 0.09116040
Iteration 11, loss = 0.07884627
Iteration 12, loss = 0.06969955
Iteration 13, loss = 0.06252215
Iteration 14, loss = 0.05717135
Iteration 15, loss = 0.05244387
Iteration 16, loss = 0.04872509
Iteration 17, loss = 0.04583969
Iteration 18, loss = 0.04322622
Iteration 19, loss = 0.04114289
Iteration 20, loss = 0.03930689
Iteration 21, loss = 0.03799553
Iteration 22, loss = 0.03658282
Iteration 23, loss = 0.03556817
Iteration 24, loss = 0.03437195
Iteration 25, loss = 0.03323720
Iteration 26, loss = 0.03302574
Iteration 27, loss = 0.03224795
Iteration 28, loss = 0.03143967
Iteration 29, loss = 0.03082989
Iteration 30, loss = 0.03017492
Iteration 31, loss = 0.02983862
Iteration 32, loss = 0.02937484
Iteration 33, loss = 0.02881416
Iteration 34, loss = 0.02871091
Iteration 35, loss = 0.02860801
Iteration 36, loss = 0.02852565
Iteration 37, loss = 0.02797483
Iteration 38, loss = 0.02803295
Iteration 39, loss = 0.02760679
Iteration 40, loss = 0.02734028
Iteration 41, loss = 0.02727772
Iteration 42, loss = 0.02700486
Iteration 43, loss = 0.02659487
Iteration 44, loss = 0.02667643
Iteration 45, loss = 0.02656607
Iteration 46, loss = 0.02647840
Iteration 47, loss = 0.02629363
Iteration 48, loss = 0.02617160
Iteration 49, loss = 0.02606430
Iteration 50, loss = 0.02576541
Iteration 51, loss = 0.02585909
Iteration 52, loss = 0.02608201
Iteration 53, loss = 0.02565410
Iteration 54, loss = 0.02575267
Iteration 55, loss = 0.02561481
Iteration 56, loss = 0.02590566
Iteration 57, loss = 0.02553603
Iteration 58, loss = 0.02555760
Iteration 59, loss = 0.02526397
Iteration 60, loss = 0.02506970
Iteration 61, loss = 0.02514058
Iteration 62, loss = 0.02510162
Iteration 63, loss = 0.02529390
Iteration 64, loss = 0.02510942
Iteration 65, loss = 0.02502706
Iteration 66, loss = 0.02494881
Iteration 67, loss = 0.02483676
Iteration 68, loss = 0.02482013
Iteration 69, loss = 0.02473304
Iteration 70, loss = 0.02456214
Iteration 71, loss = 0.02452532
Iteration 72, loss = 0.02474315
Iteration 73, loss = 0.02465573
Iteration 74, loss = 0.02448805
Iteration 75, loss = 0.02489814
Iteration 76, loss = 0.02462768
Iteration 77, loss = 0.02438849
Iteration 78, loss = 0.02481653
Iteration 79, loss = 0.02460696
Iteration 80, loss = 0.02460938
Iteration 81, loss = 0.02447887
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67473308
Iteration 2, loss = 0.61030984
Iteration 3, loss = 0.49940301
Iteration 4, loss = 0.38524137
Iteration 5, loss = 0.28861812
Iteration 6, loss = 0.21636025
Iteration 7, loss = 0.16590161
Iteration 8, loss = 0.13120220
Iteration 9, loss = 0.10692209
Iteration 10, loss = 0.08966515
Iteration 11, loss = 0.07710183
Iteration 12, loss = 0.06744514
Iteration 13, loss = 0.06023316
Iteration 14, loss = 0.05423386
Iteration 15, loss = 0.04983723
Iteration 16, loss = 0.04604535
Iteration 17, loss = 0.04314862
Iteration 18, loss = 0.04056127
Iteration 19, loss = 0.03830051
Iteration 20, loss = 0.03635439
Iteration 21, loss = 0.03490517
Iteration 22, loss = 0.03386261
Iteration 23, loss = 0.03254655
Iteration 24, loss = 0.03138304
Iteration 25, loss = 0.03045716
Iteration 26, loss = 0.03002941
Iteration 27, loss = 0.02883012
Iteration 28, loss = 0.02848767
Iteration 29, loss = 0.02805541
Iteration 30, loss = 0.02737420
Iteration 31, loss = 0.02709297
Iteration 32, loss = 0.02662497
Iteration 33, loss = 0.02626906
Iteration 34, loss = 0.02593437
Iteration 35, loss = 0.02555295
Iteration 36, loss = 0.02522020
Iteration 37, loss = 0.02515719
Iteration 38, loss = 0.02479197
Iteration 39, loss = 0.02476350
Iteration 40, loss = 0.02415287
Iteration 41, loss = 0.02421522
Iteration 42, loss = 0.02433233
Iteration 43, loss = 0.02394751
Iteration 44, loss = 0.02387634
Iteration 45, loss = 0.02343215
Iteration 46, loss = 0.02367494
Iteration 47, loss = 0.02356790
Iteration 48, loss = 0.02311694
Iteration 49, loss = 0.02307509
Iteration 50, loss = 0.02293132
Iteration 51, loss = 0.02302599
Iteration 52, loss = 0.02300852
Iteration 53, loss = 0.02263666
Iteration 54, loss = 0.02274640
Iteration 55, loss = 0.02269658
Iteration 56, loss = 0.02270989
Iteration 57, loss = 0.02240852
Iteration 58, loss = 0.02241082
Iteration 59, loss = 0.02226385
Iteration 60, loss = 0.02225120
Iteration 61, loss = 0.02220053
Iteration 62, loss = 0.02266801
Iteration 63, loss = 0.02235354
Iteration 64, loss = 0.02223941
Iteration 65, loss = 0.02200070
Iteration 66, loss = 0.02177930
Iteration 67, loss = 0.02208353
Iteration 68, loss = 0.02211040
Iteration 69, loss = 0.02203244
Iteration 70, loss = 0.02188145
Iteration 71, loss = 0.02154717
Iteration 72, loss = 0.02174839
Iteration 73, loss = 0.02206264
Iteration 74, loss = 0.02229168
Iteration 75, loss = 0.02207569
Iteration 76, loss = 0.02185891
Iteration 77, loss = 0.02179031
Iteration 78, loss = 0.02165088
Iteration 79, loss = 0.02169638
Iteration 80, loss = 0.02155912
Iteration 81, loss = 0.02148134
Iteration 82, loss = 0.02134635
Iteration 83, loss = 0.02159662
Iteration 84, loss = 0.02163584
Iteration 85, loss = 0.02154005
Iteration 86, loss = 0.02143464
Iteration 87, loss = 0.02155383
Iteration 88, loss = 0.02122646
Iteration 89, loss = 0.02128509
Iteration 90, loss = 0.02141837
Iteration 91, loss = 0.02136581
Iteration 92, loss = 0.02143329
Iteration 93, loss = 0.02145463
Iteration 94, loss = 0.02144484
Iteration 95, loss = 0.02143561
Iteration 96, loss = 0.02127501
Iteration 97, loss = 0.02117987
Iteration 98, loss = 0.02128008
Iteration 99, loss = 0.02113895
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67717925
Iteration 2, loss = 0.59951618
Iteration 3, loss = 0.48625888
Iteration 4, loss = 0.37157302
Iteration 5, loss = 0.27814028
Iteration 6, loss = 0.20971876
Iteration 7, loss = 0.16131206
Iteration 8, loss = 0.12818689
Iteration 9, loss = 0.10469874
Iteration 10, loss = 0.08807818
Iteration 11, loss = 0.07558642
Iteration 12, loss = 0.06632283
Iteration 13, loss = 0.05918496
Iteration 14, loss = 0.05347795
Iteration 15, loss = 0.04899718
Iteration 16, loss = 0.04546980
Iteration 17, loss = 0.04248274
Iteration 18, loss = 0.03987007
Iteration 19, loss = 0.03771957
Iteration 20, loss = 0.03609139
Iteration 21, loss = 0.03448051
Iteration 22, loss = 0.03303209
Iteration 23, loss = 0.03200021
Iteration 24, loss = 0.03077909
Iteration 25, loss = 0.02999685
Iteration 26, loss = 0.02948185
Iteration 27, loss = 0.02848985
Iteration 28, loss = 0.02776926
Iteration 29, loss = 0.02747347
Iteration 30, loss = 0.02707857
Iteration 31, loss = 0.02635081
Iteration 32, loss = 0.02618354
Iteration 33, loss = 0.02587239
Iteration 34, loss = 0.02560002
Iteration 35, loss = 0.02505102
Iteration 36, loss = 0.02503678
Iteration 37, loss = 0.02473709
Iteration 38, loss = 0.02440183
Iteration 39, loss = 0.02431758
Iteration 40, loss = 0.02408592
Iteration 41, loss = 0.02374832
Iteration 42, loss = 0.02347172
Iteration 43, loss = 0.02345353
Iteration 44, loss = 0.02337945
Iteration 45, loss = 0.02351829
Iteration 46, loss = 0.02330809
Iteration 47, loss = 0.02316609
Iteration 48, loss = 0.02282901
Iteration 49, loss = 0.02286750
Iteration 50, loss = 0.02269613
Iteration 51, loss = 0.02266405
Iteration 52, loss = 0.02244926
Iteration 53, loss = 0.02248425
Iteration 54, loss = 0.02267032
Iteration 55, loss = 0.02232411
Iteration 56, loss = 0.02235975
Iteration 57, loss = 0.02198650
Iteration 58, loss = 0.02218967
Iteration 59, loss = 0.02240262
Iteration 60, loss = 0.02236670
Iteration 61, loss = 0.02214261
Iteration 62, loss = 0.02207033
Iteration 63, loss = 0.02188552
Iteration 64, loss = 0.02187608
Iteration 65, loss = 0.02161899
Iteration 66, loss = 0.02169062
Iteration 67, loss = 0.02156679
Iteration 68, loss = 0.02174016
Iteration 69, loss = 0.02165991
Iteration 70, loss = 0.02151541
Iteration 71, loss = 0.02146790
Iteration 72, loss = 0.02145489
Iteration 73, loss = 0.02181701
Iteration 74, loss = 0.02164225
Iteration 75, loss = 0.02133129
Iteration 76, loss = 0.02128680
Iteration 77, loss = 0.02146205
Iteration 78, loss = 0.02135114
Iteration 79, loss = 0.02140987
Iteration 80, loss = 0.02168351
Iteration 81, loss = 0.02137830
Iteration 82, loss = 0.02137918
Iteration 83, loss = 0.02124765
Iteration 84, loss = 0.02137376
Iteration 85, loss = 0.02124803
Iteration 86, loss = 0.02143133
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69599962
Iteration 2, loss = 0.61932894
Iteration 3, loss = 0.50866756
Iteration 4, loss = 0.39121339
Iteration 5, loss = 0.29134350
Iteration 6, loss = 0.21805645
Iteration 7, loss = 0.16701171
Iteration 8, loss = 0.13194978
Iteration 9, loss = 0.10772261
Iteration 10, loss = 0.09028980
Iteration 11, loss = 0.07786404
Iteration 12, loss = 0.06841326
Iteration 13, loss = 0.06082020
Iteration 14, loss = 0.05489021
Iteration 15, loss = 0.05045808
Iteration 16, loss = 0.04672734
Iteration 17, loss = 0.04333755
Iteration 18, loss = 0.04098204
Iteration 19, loss = 0.03897269
Iteration 20, loss = 0.03683822
Iteration 21, loss = 0.03525117
Iteration 22, loss = 0.03384063
Iteration 23, loss = 0.03269648
Iteration 24, loss = 0.03203809
Iteration 25, loss = 0.03079331
Iteration 26, loss = 0.03007160
Iteration 27, loss = 0.02925316
Iteration 28, loss = 0.02881597
Iteration 29, loss = 0.02831358
Iteration 30, loss = 0.02763260
Iteration 31, loss = 0.02722275
Iteration 32, loss = 0.02680105
Iteration 33, loss = 0.02626085
Iteration 34, loss = 0.02596216
Iteration 35, loss = 0.02565945
Iteration 36, loss = 0.02596583
Iteration 37, loss = 0.02516723
Iteration 38, loss = 0.02494957
Iteration 39, loss = 0.02473931
Iteration 40, loss = 0.02455921
Iteration 41, loss = 0.02416975
Iteration 42, loss = 0.02439807
Iteration 43, loss = 0.02421444
Iteration 44, loss = 0.02381981
Iteration 45, loss = 0.02367311
Iteration 46, loss = 0.02363085
Iteration 47, loss = 0.02362212
Iteration 48, loss = 0.02328973
Iteration 49, loss = 0.02324793
Iteration 50, loss = 0.02325008
Iteration 51, loss = 0.02286165
Iteration 52, loss = 0.02301603
Iteration 53, loss = 0.02300190
Iteration 54, loss = 0.02271896
Iteration 55, loss = 0.02293411
Iteration 56, loss = 0.02250172
Iteration 57, loss = 0.02256488
Iteration 58, loss = 0.02239355
Iteration 59, loss = 0.02245230
Iteration 60, loss = 0.02220222
Iteration 61, loss = 0.02206800
Iteration 62, loss = 0.02244530
Iteration 63, loss = 0.02233851
Iteration 64, loss = 0.02203804
Iteration 65, loss = 0.02207829
Iteration 66, loss = 0.02210517
Iteration 67, loss = 0.02237737
Iteration 68, loss = 0.02203767
Iteration 69, loss = 0.02216067
Iteration 70, loss = 0.02198605
Iteration 71, loss = 0.02182604
Iteration 72, loss = 0.02220641
Iteration 73, loss = 0.02174626
Iteration 74, loss = 0.02225161
Iteration 75, loss = 0.02186452
Iteration 76, loss = 0.02173460
Iteration 77, loss = 0.02167871
Iteration 78, loss = 0.02169295
Iteration 79, loss = 0.02162065
Iteration 80, loss = 0.02178099
Iteration 81, loss = 0.02170332
Iteration 82, loss = 0.02151866
Iteration 83, loss = 0.02142105
Iteration 84, loss = 0.02171717
Iteration 85, loss = 0.02173604
Iteration 86, loss = 0.02150683
Iteration 87, loss = 0.02156845
Iteration 88, loss = 0.02169035
Iteration 89, loss = 0.02149565
Iteration 90, loss = 0.02146846
Iteration 91, loss = 0.02158737
Iteration 92, loss = 0.02145543
Iteration 93, loss = 0.02163851
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68275623
Iteration 2, loss = 0.61097720
Iteration 3, loss = 0.50494559
Iteration 4, loss = 0.39428406
Iteration 5, loss = 0.29838958
Iteration 6, loss = 0.22497913
Iteration 7, loss = 0.17310672
Iteration 8, loss = 0.13727081
Iteration 9, loss = 0.11194658
Iteration 10, loss = 0.09398745
Iteration 11, loss = 0.08095498
Iteration 12, loss = 0.07102413
Iteration 13, loss = 0.06346551
Iteration 14, loss = 0.05747512
Iteration 15, loss = 0.05282660
Iteration 16, loss = 0.04910778
Iteration 17, loss = 0.04577706
Iteration 18, loss = 0.04303775
Iteration 19, loss = 0.04085492
Iteration 20, loss = 0.03929699
Iteration 21, loss = 0.03762388
Iteration 22, loss = 0.03635293
Iteration 23, loss = 0.03493530
Iteration 24, loss = 0.03392822
Iteration 25, loss = 0.03323480
Iteration 26, loss = 0.03213560
Iteration 27, loss = 0.03135056
Iteration 28, loss = 0.03063096
Iteration 29, loss = 0.03006584
Iteration 30, loss = 0.02977786
Iteration 31, loss = 0.02919369
Iteration 32, loss = 0.02868431
Iteration 33, loss = 0.02857861
Iteration 34, loss = 0.02824286
Iteration 35, loss = 0.02776994
Iteration 36, loss = 0.02776298
Iteration 37, loss = 0.02708895
Iteration 38, loss = 0.02745081
Iteration 39, loss = 0.02699855
Iteration 40, loss = 0.02690271
Iteration 41, loss = 0.02646495
Iteration 42, loss = 0.02627437
Iteration 43, loss = 0.02623093
Iteration 44, loss = 0.02594396
Iteration 45, loss = 0.02591020
Iteration 46, loss = 0.02568673
Iteration 47, loss = 0.02554805
Iteration 48, loss = 0.02571301
Iteration 49, loss = 0.02545379
Iteration 50, loss = 0.02531957
Iteration 51, loss = 0.02520277
Iteration 52, loss = 0.02520890
Iteration 53, loss = 0.02532253
Iteration 54, loss = 0.02507677
Iteration 55, loss = 0.02487957
Iteration 56, loss = 0.02500244
Iteration 57, loss = 0.02458165
Iteration 58, loss = 0.02488057
Iteration 59, loss = 0.02498274
Iteration 60, loss = 0.02469917
Iteration 61, loss = 0.02448112
Iteration 62, loss = 0.02480192
Iteration 63, loss = 0.02463825
Iteration 64, loss = 0.02432839
Iteration 65, loss = 0.02439904
Iteration 66, loss = 0.02436317
Iteration 67, loss = 0.02437301
Iteration 68, loss = 0.02427648
Iteration 69, loss = 0.02427169
Iteration 70, loss = 0.02431122
Iteration 71, loss = 0.02389527
Iteration 72, loss = 0.02429513
Iteration 73, loss = 0.02406085
Iteration 74, loss = 0.02409098
Iteration 75, loss = 0.02446806
Iteration 76, loss = 0.02407482
Iteration 77, loss = 0.02397006
Iteration 78, loss = 0.02388362
Iteration 79, loss = 0.02363812
Iteration 80, loss = 0.02398221
Iteration 81, loss = 0.02390209
Iteration 82, loss = 0.02381481
Iteration 83, loss = 0.02398812
Iteration 84, loss = 0.02391397
Iteration 85, loss = 0.02373156
Iteration 86, loss = 0.02392831
Iteration 87, loss = 0.02360973
Iteration 88, loss = 0.02389342
Iteration 89, loss = 0.02350453
Iteration 90, loss = 0.02386613
Iteration 91, loss = 0.02389765
Iteration 92, loss = 0.02381644
Iteration 93, loss = 0.02383082
Iteration 94, loss = 0.02353176
Iteration 95, loss = 0.02376491
Iteration 96, loss = 0.02382945
Iteration 97, loss = 0.02365434
Iteration 98, loss = 0.02358502
Iteration 99, loss = 0.02382176
Iteration 100, loss = 0.02345039
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67168460
Iteration 2, loss = 0.58998671
Iteration 3, loss = 0.47771364
Iteration 4, loss = 0.36795110
Iteration 5, loss = 0.27754717
Iteration 6, loss = 0.21037015
Iteration 7, loss = 0.16309806
Iteration 8, loss = 0.13059039
Iteration 9, loss = 0.10756984
Iteration 10, loss = 0.09116040
Iteration 11, loss = 0.07884627
Iteration 12, loss = 0.06969955
Iteration 13, loss = 0.06252215
Iteration 14, loss = 0.05717135
Iteration 15, loss = 0.05244387
Iteration 16, loss = 0.04872509
Iteration 17, loss = 0.04583969
Iteration 18, loss = 0.04322622
Iteration 19, loss = 0.04114289
Iteration 20, loss = 0.03930689
Iteration 21, loss = 0.03799553
Iteration 22, loss = 0.03658282
Iteration 23, loss = 0.03556817
Iteration 24, loss = 0.03437195
Iteration 25, loss = 0.03323720
Iteration 26, loss = 0.03302574
Iteration 27, loss = 0.03224795
Iteration 28, loss = 0.03143967
Iteration 29, loss = 0.03082989
Iteration 30, loss = 0.03017492
Iteration 31, loss = 0.02983862
Iteration 32, loss = 0.02937484
Iteration 33, loss = 0.02881416
Iteration 34, loss = 0.02871091
Iteration 35, loss = 0.02860801
Iteration 36, loss = 0.02852565
Iteration 37, loss = 0.02797483
Iteration 38, loss = 0.02803295
Iteration 39, loss = 0.02760679
Iteration 40, loss = 0.02734028
Iteration 41, loss = 0.02727772
Iteration 42, loss = 0.02700486
Iteration 43, loss = 0.02659487
Iteration 44, loss = 0.02667643
Iteration 45, loss = 0.02656607
Iteration 46, loss = 0.02647840
Iteration 47, loss = 0.02629363
Iteration 48, loss = 0.02617160
Iteration 49, loss = 0.02606430
Iteration 50, loss = 0.02576541
Iteration 51, loss = 0.02585909
Iteration 52, loss = 0.02608201
Iteration 53, loss = 0.02565410
Iteration 54, loss = 0.02575267
Iteration 55, loss = 0.02561481
Iteration 56, loss = 0.02590566
Iteration 57, loss = 0.02553603
Iteration 58, loss = 0.02555760
Iteration 59, loss = 0.02526397
Iteration 60, loss = 0.02506970
Iteration 61, loss = 0.02514058
Iteration 62, loss = 0.02510162
Iteration 63, loss = 0.02529390
Iteration 64, loss = 0.02510942
Iteration 65, loss = 0.02502706
Iteration 66, loss = 0.02494881
Iteration 67, loss = 0.02483676
Iteration 68, loss = 0.02482013
Iteration 69, loss = 0.02473304
Iteration 70, loss = 0.02456214
Iteration 71, loss = 0.02452532
Iteration 72, loss = 0.02474315
Iteration 73, loss = 0.02465573
Iteration 74, loss = 0.02448805
Iteration 75, loss = 0.02489814
Iteration 76, loss = 0.02462768
Iteration 77, loss = 0.02438849
Iteration 78, loss = 0.02481653
Iteration 79, loss = 0.02460696
Iteration 80, loss = 0.02460938
Iteration 81, loss = 0.02447887
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67473308
Iteration 2, loss = 0.61030984
Iteration 3, loss = 0.49940301
Iteration 4, loss = 0.38524137
Iteration 5, loss = 0.28861812
Iteration 6, loss = 0.21636025
Iteration 7, loss = 0.16590161
Iteration 8, loss = 0.13120220
Iteration 9, loss = 0.10692209
Iteration 10, loss = 0.08966515
Iteration 11, loss = 0.07710183
Iteration 12, loss = 0.06744514
Iteration 13, loss = 0.06023316
Iteration 14, loss = 0.05423386
Iteration 15, loss = 0.04983723
Iteration 16, loss = 0.04604535
Iteration 17, loss = 0.04314862
Iteration 18, loss = 0.04056127
Iteration 19, loss = 0.03830051
Iteration 20, loss = 0.03635439
Iteration 21, loss = 0.03490517
Iteration 22, loss = 0.03386261
Iteration 23, loss = 0.03254655
Iteration 24, loss = 0.03138304
Iteration 25, loss = 0.03045716
Iteration 26, loss = 0.03002941
Iteration 27, loss = 0.02883012
Iteration 28, loss = 0.02848767
Iteration 29, loss = 0.02805541
Iteration 30, loss = 0.02737420
Iteration 31, loss = 0.02709297
Iteration 32, loss = 0.02662497
Iteration 33, loss = 0.02626906
Iteration 34, loss = 0.02593437
Iteration 35, loss = 0.02555295
Iteration 36, loss = 0.02522020
Iteration 37, loss = 0.02515719
Iteration 38, loss = 0.02479197
Iteration 39, loss = 0.02476350
Iteration 40, loss = 0.02415287
Iteration 41, loss = 0.02421522
Iteration 42, loss = 0.02433233
Iteration 43, loss = 0.02394751
Iteration 44, loss = 0.02387634
Iteration 45, loss = 0.02343215
Iteration 46, loss = 0.02367494
Iteration 47, loss = 0.02356790
Iteration 48, loss = 0.02311694
Iteration 49, loss = 0.02307509
Iteration 50, loss = 0.02293132
Iteration 51, loss = 0.02302599
Iteration 52, loss = 0.02300852
Iteration 53, loss = 0.02263666
Iteration 54, loss = 0.02274640
Iteration 55, loss = 0.02269658
Iteration 56, loss = 0.02270989
Iteration 57, loss = 0.02240852
Iteration 58, loss = 0.02241082
Iteration 59, loss = 0.02226385
Iteration 60, loss = 0.02225120
Iteration 61, loss = 0.02220053
Iteration 62, loss = 0.02266801
Iteration 63, loss = 0.02235354
Iteration 64, loss = 0.02223941
Iteration 65, loss = 0.02200070
Iteration 66, loss = 0.02177930
Iteration 67, loss = 0.02208353
Iteration 68, loss = 0.02211040
Iteration 69, loss = 0.02203244
Iteration 70, loss = 0.02188145
Iteration 71, loss = 0.02154717
Iteration 72, loss = 0.02174839
Iteration 73, loss = 0.02206264
Iteration 74, loss = 0.02229168
Iteration 75, loss = 0.02207569
Iteration 76, loss = 0.02185891
Iteration 77, loss = 0.02179031
Iteration 78, loss = 0.02165088
Iteration 79, loss = 0.02169638
Iteration 80, loss = 0.02155912
Iteration 81, loss = 0.02148134
Iteration 82, loss = 0.02134635
Iteration 83, loss = 0.02159662
Iteration 84, loss = 0.02163584
Iteration 85, loss = 0.02154005
Iteration 86, loss = 0.02143464
Iteration 87, loss = 0.02155383
Iteration 88, loss = 0.02122646
Iteration 89, loss = 0.02128509
Iteration 90, loss = 0.02141837
Iteration 91, loss = 0.02136581
Iteration 92, loss = 0.02143329
Iteration 93, loss = 0.02145463
Iteration 94, loss = 0.02144484
Iteration 95, loss = 0.02143561
Iteration 96, loss = 0.02127501
Iteration 97, loss = 0.02117987
Iteration 98, loss = 0.02128008
Iteration 99, loss = 0.02113895
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67717925
Iteration 2, loss = 0.59951618
Iteration 3, loss = 0.48625888
Iteration 4, loss = 0.37157302
Iteration 5, loss = 0.27814028
Iteration 6, loss = 0.20971876
Iteration 7, loss = 0.16131206
Iteration 8, loss = 0.12818689
Iteration 9, loss = 0.10469874
Iteration 10, loss = 0.08807818
Iteration 11, loss = 0.07558642
Iteration 12, loss = 0.06632283
Iteration 13, loss = 0.05918496
Iteration 14, loss = 0.05347795
Iteration 15, loss = 0.04899718
Iteration 16, loss = 0.04546980
Iteration 17, loss = 0.04248274
Iteration 18, loss = 0.03987007
Iteration 19, loss = 0.03771957
Iteration 20, loss = 0.03609139
Iteration 21, loss = 0.03448051
Iteration 22, loss = 0.03303209
Iteration 23, loss = 0.03200021
Iteration 24, loss = 0.03077909
Iteration 25, loss = 0.02999685
Iteration 26, loss = 0.02948185
Iteration 27, loss = 0.02848985
Iteration 28, loss = 0.02776926
Iteration 29, loss = 0.02747347
Iteration 30, loss = 0.02707857
Iteration 31, loss = 0.02635081
Iteration 32, loss = 0.02618354
Iteration 33, loss = 0.02587239
Iteration 34, loss = 0.02560002
Iteration 35, loss = 0.02505102
Iteration 36, loss = 0.02503678
Iteration 37, loss = 0.02473709
Iteration 38, loss = 0.02440183
Iteration 39, loss = 0.02431758
Iteration 40, loss = 0.02408592
Iteration 41, loss = 0.02374832
Iteration 42, loss = 0.02347172
Iteration 43, loss = 0.02345353
Iteration 44, loss = 0.02337945
Iteration 45, loss = 0.02351829
Iteration 46, loss = 0.02330809
Iteration 47, loss = 0.02316609
Iteration 48, loss = 0.02282901
Iteration 49, loss = 0.02286750
Iteration 50, loss = 0.02269613
Iteration 51, loss = 0.02266405
Iteration 52, loss = 0.02244926
Iteration 53, loss = 0.02248425
Iteration 54, loss = 0.02267032
Iteration 55, loss = 0.02232411
Iteration 56, loss = 0.02235975
Iteration 57, loss = 0.02198650
Iteration 58, loss = 0.02218967
Iteration 59, loss = 0.02240262
Iteration 60, loss = 0.02236670
Iteration 61, loss = 0.02214261
Iteration 62, loss = 0.02207033
Iteration 63, loss = 0.02188552
Iteration 64, loss = 0.02187608
Iteration 65, loss = 0.02161899
Iteration 66, loss = 0.02169062
Iteration 67, loss = 0.02156679
Iteration 68, loss = 0.02174016
Iteration 69, loss = 0.02165991
Iteration 70, loss = 0.02151541
Iteration 71, loss = 0.02146790
Iteration 72, loss = 0.02145489
Iteration 73, loss = 0.02181701
Iteration 74, loss = 0.02164225
Iteration 75, loss = 0.02133129
Iteration 76, loss = 0.02128680
Iteration 77, loss = 0.02146205
Iteration 78, loss = 0.02135114
Iteration 79, loss = 0.02140987
Iteration 80, loss = 0.02168351
Iteration 81, loss = 0.02137830
Iteration 82, loss = 0.02137918
Iteration 83, loss = 0.02124765
Iteration 84, loss = 0.02137376
Iteration 85, loss = 0.02124803
Iteration 86, loss = 0.02143133
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69599962
Iteration 2, loss = 0.61932894
Iteration 3, loss = 0.50866756
Iteration 4, loss = 0.39121339
Iteration 5, loss = 0.29134350
Iteration 6, loss = 0.21805645
Iteration 7, loss = 0.16701171
Iteration 8, loss = 0.13194978
Iteration 9, loss = 0.10772261
Iteration 10, loss = 0.09028980
Iteration 11, loss = 0.07786404
Iteration 12, loss = 0.06841326
Iteration 13, loss = 0.06082020
Iteration 14, loss = 0.05489021
Iteration 15, loss = 0.05045808
Iteration 16, loss = 0.04672734
Iteration 17, loss = 0.04333755
Iteration 18, loss = 0.04098204
Iteration 19, loss = 0.03897269
Iteration 20, loss = 0.03683822
Iteration 21, loss = 0.03525117
Iteration 22, loss = 0.03384063
Iteration 23, loss = 0.03269648
Iteration 24, loss = 0.03203809
Iteration 25, loss = 0.03079331
Iteration 26, loss = 0.03007160
Iteration 27, loss = 0.02925316
Iteration 28, loss = 0.02881597
Iteration 29, loss = 0.02831358
Iteration 30, loss = 0.02763260
Iteration 31, loss = 0.02722275
Iteration 32, loss = 0.02680105
Iteration 33, loss = 0.02626085
Iteration 34, loss = 0.02596216
Iteration 35, loss = 0.02565945
Iteration 36, loss = 0.02596583
Iteration 37, loss = 0.02516723
Iteration 38, loss = 0.02494957
Iteration 39, loss = 0.02473931
Iteration 40, loss = 0.02455921
Iteration 41, loss = 0.02416975
Iteration 42, loss = 0.02439807
Iteration 43, loss = 0.02421444
Iteration 44, loss = 0.02381981
Iteration 45, loss = 0.02367311
Iteration 46, loss = 0.02363085
Iteration 47, loss = 0.02362212
Iteration 48, loss = 0.02328973
Iteration 49, loss = 0.02324793
Iteration 50, loss = 0.02325008
Iteration 51, loss = 0.02286165
Iteration 52, loss = 0.02301603
Iteration 53, loss = 0.02300190
Iteration 54, loss = 0.02271896
Iteration 55, loss = 0.02293411
Iteration 56, loss = 0.02250172
Iteration 57, loss = 0.02256488
Iteration 58, loss = 0.02239355
Iteration 59, loss = 0.02245230
Iteration 60, loss = 0.02220222
Iteration 61, loss = 0.02206800
Iteration 62, loss = 0.02244530
Iteration 63, loss = 0.02233851
Iteration 64, loss = 0.02203804
Iteration 65, loss = 0.02207829
Iteration 66, loss = 0.02210517
Iteration 67, loss = 0.02237737
Iteration 68, loss = 0.02203767
Iteration 69, loss = 0.02216067
Iteration 70, loss = 0.02198605
Iteration 71, loss = 0.02182604
Iteration 72, loss = 0.02220641
Iteration 73, loss = 0.02174626
Iteration 74, loss = 0.02225161
Iteration 75, loss = 0.02186452
Iteration 76, loss = 0.02173460
Iteration 77, loss = 0.02167871
Iteration 78, loss = 0.02169295
Iteration 79, loss = 0.02162065
Iteration 80, loss = 0.02178099
Iteration 81, loss = 0.02170332
Iteration 82, loss = 0.02151866
Iteration 83, loss = 0.02142105
Iteration 84, loss = 0.02171717
Iteration 85, loss = 0.02173604
Iteration 86, loss = 0.02150683
Iteration 87, loss = 0.02156845
Iteration 88, loss = 0.02169035
Iteration 89, loss = 0.02149565
Iteration 90, loss = 0.02146846
Iteration 91, loss = 0.02158737
Iteration 92, loss = 0.02145543
Iteration 93, loss = 0.02163851
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68275623
Iteration 2, loss = 0.61097720
Iteration 3, loss = 0.50494559
Iteration 4, loss = 0.39428406
Iteration 5, loss = 0.29838958
Iteration 6, loss = 0.22497913
Iteration 7, loss = 0.17310672
Iteration 8, loss = 0.13727081
Iteration 9, loss = 0.11194658
Iteration 10, loss = 0.09398745
Iteration 11, loss = 0.08095498
Iteration 12, loss = 0.07102413
Iteration 13, loss = 0.06346551
Iteration 14, loss = 0.05747512
Iteration 15, loss = 0.05282660
Iteration 16, loss = 0.04910778
Iteration 17, loss = 0.04577706
Iteration 18, loss = 0.04303775
Iteration 19, loss = 0.04085492
Iteration 20, loss = 0.03929699
Iteration 21, loss = 0.03762388
Iteration 22, loss = 0.03635293
Iteration 23, loss = 0.03493530
Iteration 24, loss = 0.03392822
Iteration 25, loss = 0.03323480
Iteration 26, loss = 0.03213560
Iteration 27, loss = 0.03135056
Iteration 28, loss = 0.03063096
Iteration 29, loss = 0.03006584
Iteration 30, loss = 0.02977786
Iteration 31, loss = 0.02919369
Iteration 32, loss = 0.02868431
Iteration 33, loss = 0.02857861
Iteration 34, loss = 0.02824286
Iteration 35, loss = 0.02776994
Iteration 36, loss = 0.02776298
Iteration 37, loss = 0.02708895
Iteration 38, loss = 0.02745081
Iteration 39, loss = 0.02699855
Iteration 40, loss = 0.02690271
Iteration 41, loss = 0.02646495
Iteration 42, loss = 0.02627437
Iteration 43, loss = 0.02623093
Iteration 44, loss = 0.02594396
Iteration 45, loss = 0.02591020
Iteration 46, loss = 0.02568673
Iteration 47, loss = 0.02554805
Iteration 48, loss = 0.02571301
Iteration 49, loss = 0.02545379
Iteration 50, loss = 0.02531957
Iteration 51, loss = 0.02520277
Iteration 52, loss = 0.02520890
Iteration 53, loss = 0.02532253
Iteration 54, loss = 0.02507677
Iteration 55, loss = 0.02487957
Iteration 56, loss = 0.02500244
Iteration 57, loss = 0.02458165
Iteration 58, loss = 0.02488057
Iteration 59, loss = 0.02498274
Iteration 60, loss = 0.02469917
Iteration 61, loss = 0.02448112
Iteration 62, loss = 0.02480192
Iteration 63, loss = 0.02463825
Iteration 64, loss = 0.02432839
Iteration 65, loss = 0.02439904
Iteration 66, loss = 0.02436317
Iteration 67, loss = 0.02437301
Iteration 68, loss = 0.02427648
Iteration 69, loss = 0.02427169
Iteration 70, loss = 0.02431122
Iteration 71, loss = 0.02389527
Iteration 72, loss = 0.02429513
Iteration 73, loss = 0.02406085
Iteration 74, loss = 0.02409098
Iteration 75, loss = 0.02446806
Iteration 76, loss = 0.02407482
Iteration 77, loss = 0.02397006
Iteration 78, loss = 0.02388362
Iteration 79, loss = 0.02363812
Iteration 80, loss = 0.02398221
Iteration 81, loss = 0.02390209
Iteration 82, loss = 0.02381481
Iteration 83, loss = 0.02398812
Iteration 84, loss = 0.02391397
Iteration 85, loss = 0.02373156
Iteration 86, loss = 0.02392831
Iteration 87, loss = 0.02360973
Iteration 88, loss = 0.02389342
Iteration 89, loss = 0.02350453
Iteration 90, loss = 0.02386613
Iteration 91, loss = 0.02389765
Iteration 92, loss = 0.02381644
Iteration 93, loss = 0.02383082
Iteration 94, loss = 0.02353176
Iteration 95, loss = 0.02376491
Iteration 96, loss = 0.02382945
Iteration 97, loss = 0.02365434
Iteration 98, loss = 0.02358502
Iteration 99, loss = 0.02382176
Iteration 100, loss = 0.02345039
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67168460
Iteration 2, loss = 0.58998671
Iteration 3, loss = 0.47771364
Iteration 4, loss = 0.36795110
Iteration 5, loss = 0.27754717
Iteration 6, loss = 0.21037015
Iteration 7, loss = 0.16309806
Iteration 8, loss = 0.13059039
Iteration 9, loss = 0.10756984
Iteration 10, loss = 0.09116040
Iteration 11, loss = 0.07884627
Iteration 12, loss = 0.06969955
Iteration 13, loss = 0.06252215
Iteration 14, loss = 0.05717135
Iteration 15, loss = 0.05244387
Iteration 16, loss = 0.04872509
Iteration 17, loss = 0.04583969
Iteration 18, loss = 0.04322622
Iteration 19, loss = 0.04114289
Iteration 20, loss = 0.03930689
Iteration 21, loss = 0.03799553
Iteration 22, loss = 0.03658282
Iteration 23, loss = 0.03556817
Iteration 24, loss = 0.03437195
Iteration 25, loss = 0.03323720
Iteration 26, loss = 0.03302574
Iteration 27, loss = 0.03224795
Iteration 28, loss = 0.03143967
Iteration 29, loss = 0.03082989
Iteration 30, loss = 0.03017492
Iteration 31, loss = 0.02983862
Iteration 32, loss = 0.02937484
Iteration 33, loss = 0.02881416
Iteration 34, loss = 0.02871091
Iteration 35, loss = 0.02860801
Iteration 36, loss = 0.02852565
Iteration 37, loss = 0.02797483
Iteration 38, loss = 0.02803295
Iteration 39, loss = 0.02760679
Iteration 40, loss = 0.02734028
Iteration 41, loss = 0.02727772
Iteration 42, loss = 0.02700486
Iteration 43, loss = 0.02659487
Iteration 44, loss = 0.02667643
Iteration 45, loss = 0.02656607
Iteration 46, loss = 0.02647840
Iteration 47, loss = 0.02629363
Iteration 48, loss = 0.02617160
Iteration 49, loss = 0.02606430
Iteration 50, loss = 0.02576541
Iteration 51, loss = 0.02585909
Iteration 52, loss = 0.02608201
Iteration 53, loss = 0.02565410
Iteration 54, loss = 0.02575267
Iteration 55, loss = 0.02561481
Iteration 56, loss = 0.02590566
Iteration 57, loss = 0.02553603
Iteration 58, loss = 0.02555760
Iteration 59, loss = 0.02526397
Iteration 60, loss = 0.02506970
Iteration 61, loss = 0.02514058
Iteration 62, loss = 0.02510162
Iteration 63, loss = 0.02529390
Iteration 64, loss = 0.02510942
Iteration 65, loss = 0.02502706
Iteration 66, loss = 0.02494881
Iteration 67, loss = 0.02483676
Iteration 68, loss = 0.02482013
Iteration 69, loss = 0.02473304
Iteration 70, loss = 0.02456214
Iteration 71, loss = 0.02452532
Iteration 72, loss = 0.02474315
Iteration 73, loss = 0.02465573
Iteration 74, loss = 0.02448805
Iteration 75, loss = 0.02489814
Iteration 76, loss = 0.02462768
Iteration 77, loss = 0.02438849
Iteration 78, loss = 0.02481653
Iteration 79, loss = 0.02460696
Iteration 80, loss = 0.02460938
Iteration 81, loss = 0.02447887
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67473308
Iteration 2, loss = 0.61030984
Iteration 3, loss = 0.49940301
Iteration 4, loss = 0.38524137
Iteration 5, loss = 0.28861812
Iteration 6, loss = 0.21636025
Iteration 7, loss = 0.16590161
Iteration 8, loss = 0.13120220
Iteration 9, loss = 0.10692209
Iteration 10, loss = 0.08966515
Iteration 11, loss = 0.07710183
Iteration 12, loss = 0.06744514
Iteration 13, loss = 0.06023316
Iteration 14, loss = 0.05423386
Iteration 15, loss = 0.04983723
Iteration 16, loss = 0.04604535
Iteration 17, loss = 0.04314862
Iteration 18, loss = 0.04056127
Iteration 19, loss = 0.03830051
Iteration 20, loss = 0.03635439
Iteration 21, loss = 0.03490517
Iteration 22, loss = 0.03386261
Iteration 23, loss = 0.03254655
Iteration 24, loss = 0.03138304
Iteration 25, loss = 0.03045716
Iteration 26, loss = 0.03002941
Iteration 27, loss = 0.02883012
Iteration 28, loss = 0.02848767
Iteration 29, loss = 0.02805541
Iteration 30, loss = 0.02737420
Iteration 31, loss = 0.02709297
Iteration 32, loss = 0.02662497
Iteration 33, loss = 0.02626906
Iteration 34, loss = 0.02593437
Iteration 35, loss = 0.02555295
Iteration 36, loss = 0.02522020
Iteration 37, loss = 0.02515719
Iteration 38, loss = 0.02479197
Iteration 39, loss = 0.02476350
Iteration 40, loss = 0.02415287
Iteration 41, loss = 0.02421522
Iteration 42, loss = 0.02433233
Iteration 43, loss = 0.02394751
Iteration 44, loss = 0.02387634
Iteration 45, loss = 0.02343215
Iteration 46, loss = 0.02367494
Iteration 47, loss = 0.02356790
Iteration 48, loss = 0.02311694
Iteration 49, loss = 0.02307509
Iteration 50, loss = 0.02293132
Iteration 51, loss = 0.02302599
Iteration 52, loss = 0.02300852
Iteration 53, loss = 0.02263666
Iteration 54, loss = 0.02274640
Iteration 55, loss = 0.02269658
Iteration 56, loss = 0.02270989
Iteration 57, loss = 0.02240852
Iteration 58, loss = 0.02241082
Iteration 59, loss = 0.02226385
Iteration 60, loss = 0.02225120
Iteration 61, loss = 0.02220053
Iteration 62, loss = 0.02266801
Iteration 63, loss = 0.02235354
Iteration 64, loss = 0.02223941
Iteration 65, loss = 0.02200070
Iteration 66, loss = 0.02177930
Iteration 67, loss = 0.02208353
Iteration 68, loss = 0.02211040
Iteration 69, loss = 0.02203244
Iteration 70, loss = 0.02188145
Iteration 71, loss = 0.02154717
Iteration 72, loss = 0.02174839
Iteration 73, loss = 0.02206264
Iteration 74, loss = 0.02229168
Iteration 75, loss = 0.02207569
Iteration 76, loss = 0.02185891
Iteration 77, loss = 0.02179031
Iteration 78, loss = 0.02165088
Iteration 79, loss = 0.02169638
Iteration 80, loss = 0.02155912
Iteration 81, loss = 0.02148134
Iteration 82, loss = 0.02134635
Iteration 83, loss = 0.02159662
Iteration 84, loss = 0.02163584
Iteration 85, loss = 0.02154005
Iteration 86, loss = 0.02143464
Iteration 87, loss = 0.02155383
Iteration 88, loss = 0.02122646
Iteration 89, loss = 0.02128509
Iteration 90, loss = 0.02141837
Iteration 91, loss = 0.02136581
Iteration 92, loss = 0.02143329
Iteration 93, loss = 0.02145463
Iteration 94, loss = 0.02144484
Iteration 95, loss = 0.02143561
Iteration 96, loss = 0.02127501
Iteration 97, loss = 0.02117987
Iteration 98, loss = 0.02128008
Iteration 99, loss = 0.02113895
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67717925
Iteration 2, loss = 0.59951618
Iteration 3, loss = 0.48625888
Iteration 4, loss = 0.37157302
Iteration 5, loss = 0.27814028
Iteration 6, loss = 0.20971876
Iteration 7, loss = 0.16131206
Iteration 8, loss = 0.12818689
Iteration 9, loss = 0.10469874
Iteration 10, loss = 0.08807818
Iteration 11, loss = 0.07558642
Iteration 12, loss = 0.06632283
Iteration 13, loss = 0.05918496
Iteration 14, loss = 0.05347795
Iteration 15, loss = 0.04899718
Iteration 16, loss = 0.04546980
Iteration 17, loss = 0.04248274
Iteration 18, loss = 0.03987007
Iteration 19, loss = 0.03771957
Iteration 20, loss = 0.03609139
Iteration 21, loss = 0.03448051
Iteration 22, loss = 0.03303209
Iteration 23, loss = 0.03200021
Iteration 24, loss = 0.03077909
Iteration 25, loss = 0.02999685
Iteration 26, loss = 0.02948185
Iteration 27, loss = 0.02848985
Iteration 28, loss = 0.02776926
Iteration 29, loss = 0.02747347
Iteration 30, loss = 0.02707857
Iteration 31, loss = 0.02635081
Iteration 32, loss = 0.02618354
Iteration 33, loss = 0.02587239
Iteration 34, loss = 0.02560002
Iteration 35, loss = 0.02505102
Iteration 36, loss = 0.02503678
Iteration 37, loss = 0.02473709
Iteration 38, loss = 0.02440183
Iteration 39, loss = 0.02431758
Iteration 40, loss = 0.02408592
Iteration 41, loss = 0.02374832
Iteration 42, loss = 0.02347172
Iteration 43, loss = 0.02345353
Iteration 44, loss = 0.02337945
Iteration 45, loss = 0.02351829
Iteration 46, loss = 0.02330809
Iteration 47, loss = 0.02316609
Iteration 48, loss = 0.02282901
Iteration 49, loss = 0.02286750
Iteration 50, loss = 0.02269613
Iteration 51, loss = 0.02266405
Iteration 52, loss = 0.02244926
Iteration 53, loss = 0.02248425
Iteration 54, loss = 0.02267032
Iteration 55, loss = 0.02232411
Iteration 56, loss = 0.02235975
Iteration 57, loss = 0.02198650
Iteration 58, loss = 0.02218967
Iteration 59, loss = 0.02240262
Iteration 60, loss = 0.02236670
Iteration 61, loss = 0.02214261
Iteration 62, loss = 0.02207033
Iteration 63, loss = 0.02188552
Iteration 64, loss = 0.02187608
Iteration 65, loss = 0.02161899
Iteration 66, loss = 0.02169062
Iteration 67, loss = 0.02156679
Iteration 68, loss = 0.02174016
Iteration 69, loss = 0.02165991
Iteration 70, loss = 0.02151541
Iteration 71, loss = 0.02146790
Iteration 72, loss = 0.02145489
Iteration 73, loss = 0.02181701
Iteration 74, loss = 0.02164225
Iteration 75, loss = 0.02133129
Iteration 76, loss = 0.02128680
Iteration 77, loss = 0.02146205
Iteration 78, loss = 0.02135114
Iteration 79, loss = 0.02140987
Iteration 80, loss = 0.02168351
Iteration 81, loss = 0.02137830
Iteration 82, loss = 0.02137918
Iteration 83, loss = 0.02124765
Iteration 84, loss = 0.02137376
Iteration 85, loss = 0.02124803
Iteration 86, loss = 0.02143133
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69599962
Iteration 2, loss = 0.61932894
Iteration 3, loss = 0.50866756
Iteration 4, loss = 0.39121339
Iteration 5, loss = 0.29134350
Iteration 6, loss = 0.21805645
Iteration 7, loss = 0.16701171
Iteration 8, loss = 0.13194978
Iteration 9, loss = 0.10772261
Iteration 10, loss = 0.09028980
Iteration 11, loss = 0.07786404
Iteration 12, loss = 0.06841326
Iteration 13, loss = 0.06082020
Iteration 14, loss = 0.05489021
Iteration 15, loss = 0.05045808
Iteration 16, loss = 0.04672734
Iteration 17, loss = 0.04333755
Iteration 18, loss = 0.04098204
Iteration 19, loss = 0.03897269
Iteration 20, loss = 0.03683822
Iteration 21, loss = 0.03525117
Iteration 22, loss = 0.03384063
Iteration 23, loss = 0.03269648
Iteration 24, loss = 0.03203809
Iteration 25, loss = 0.03079331
Iteration 26, loss = 0.03007160
Iteration 27, loss = 0.02925316
Iteration 28, loss = 0.02881597
Iteration 29, loss = 0.02831358
Iteration 30, loss = 0.02763260
Iteration 31, loss = 0.02722275
Iteration 32, loss = 0.02680105
Iteration 33, loss = 0.02626085
Iteration 34, loss = 0.02596216
Iteration 35, loss = 0.02565945
Iteration 36, loss = 0.02596583
Iteration 37, loss = 0.02516723
Iteration 38, loss = 0.02494957
Iteration 39, loss = 0.02473931
Iteration 40, loss = 0.02455921
Iteration 41, loss = 0.02416975
Iteration 42, loss = 0.02439807
Iteration 43, loss = 0.02421444
Iteration 44, loss = 0.02381981
Iteration 45, loss = 0.02367311
Iteration 46, loss = 0.02363085
Iteration 47, loss = 0.02362212
Iteration 48, loss = 0.02328973
Iteration 49, loss = 0.02324793
Iteration 50, loss = 0.02325008
Iteration 51, loss = 0.02286165
Iteration 52, loss = 0.02301603
Iteration 53, loss = 0.02300190
Iteration 54, loss = 0.02271896
Iteration 55, loss = 0.02293411
Iteration 56, loss = 0.02250172
Iteration 57, loss = 0.02256488
Iteration 58, loss = 0.02239355
Iteration 59, loss = 0.02245230
Iteration 60, loss = 0.02220222
Iteration 61, loss = 0.02206800
Iteration 62, loss = 0.02244530
Iteration 63, loss = 0.02233851
Iteration 64, loss = 0.02203804
Iteration 65, loss = 0.02207829
Iteration 66, loss = 0.02210517
Iteration 67, loss = 0.02237737
Iteration 68, loss = 0.02203767
Iteration 69, loss = 0.02216067
Iteration 70, loss = 0.02198605
Iteration 71, loss = 0.02182604
Iteration 72, loss = 0.02220641
Iteration 73, loss = 0.02174626
Iteration 74, loss = 0.02225161
Iteration 75, loss = 0.02186452
Iteration 76, loss = 0.02173460
Iteration 77, loss = 0.02167871
Iteration 78, loss = 0.02169295
Iteration 79, loss = 0.02162065
Iteration 80, loss = 0.02178099
Iteration 81, loss = 0.02170332
Iteration 82, loss = 0.02151866
Iteration 83, loss = 0.02142105
Iteration 84, loss = 0.02171717
Iteration 85, loss = 0.02173604
Iteration 86, loss = 0.02150683
Iteration 87, loss = 0.02156845
Iteration 88, loss = 0.02169035
Iteration 89, loss = 0.02149565
Iteration 90, loss = 0.02146846
Iteration 91, loss = 0.02158737
Iteration 92, loss = 0.02145543
Iteration 93, loss = 0.02163851
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70526870
Iteration 2, loss = 0.61917442
Iteration 3, loss = 0.48963910
Iteration 4, loss = 0.35923460
Iteration 5, loss = 0.25716946
Iteration 6, loss = 0.18752521
Iteration 7, loss = 0.14243057
Iteration 8, loss = 0.11265371
Iteration 9, loss = 0.09286793
Iteration 10, loss = 0.07914103
Iteration 11, loss = 0.06903033
Iteration 12, loss = 0.06123344
Iteration 13, loss = 0.05582699
Iteration 14, loss = 0.05105281
Iteration 15, loss = 0.04735472
Iteration 16, loss = 0.04451677
Iteration 17, loss = 0.04235273
Iteration 18, loss = 0.04031466
Iteration 19, loss = 0.03865146
Iteration 20, loss = 0.03674979
Iteration 21, loss = 0.03587305
Iteration 22, loss = 0.03459962
Iteration 23, loss = 0.03367854
Iteration 24, loss = 0.03286407
Iteration 25, loss = 0.03186391
Iteration 26, loss = 0.03151882
Iteration 27, loss = 0.03123644
Iteration 28, loss = 0.03077364
Iteration 29, loss = 0.03010419
Iteration 30, loss = 0.03001365
Iteration 31, loss = 0.02973483
Iteration 32, loss = 0.02917814
Iteration 33, loss = 0.02871988
Iteration 34, loss = 0.02841900
Iteration 35, loss = 0.02820842
Iteration 36, loss = 0.02808766
Iteration 37, loss = 0.02781212
Iteration 38, loss = 0.02790235
Iteration 39, loss = 0.02757456
Iteration 40, loss = 0.02747063
Iteration 41, loss = 0.02718496
Iteration 42, loss = 0.02709144
Iteration 43, loss = 0.02703786
Iteration 44, loss = 0.02680542
Iteration 45, loss = 0.02700559
Iteration 46, loss = 0.02665656
Iteration 47, loss = 0.02651682
Iteration 48, loss = 0.02659530
Iteration 49, loss = 0.02645549
Iteration 50, loss = 0.02630117
Iteration 51, loss = 0.02609436
Iteration 52, loss = 0.02627347
Iteration 53, loss = 0.02618701
Iteration 54, loss = 0.02608285
Iteration 55, loss = 0.02598651
Iteration 56, loss = 0.02597987
Iteration 57, loss = 0.02635929
Iteration 58, loss = 0.02600792
Iteration 59, loss = 0.02622608
Iteration 60, loss = 0.02567393
Iteration 61, loss = 0.02562297
Iteration 62, loss = 0.02596634
Iteration 63, loss = 0.02567565
Iteration 64, loss = 0.02612257
Iteration 65, loss = 0.02550032
Iteration 66, loss = 0.02560199
Iteration 67, loss = 0.02560326
Iteration 68, loss = 0.02570533
Iteration 69, loss = 0.02551223
Iteration 70, loss = 0.02555447
Iteration 71, loss = 0.02509846
Iteration 72, loss = 0.02585425
Iteration 73, loss = 0.02531881
Iteration 74, loss = 0.02540253
Iteration 75, loss = 0.02546724
Iteration 76, loss = 0.02510317
Iteration 77, loss = 0.02529160
Iteration 78, loss = 0.02512493
Iteration 79, loss = 0.02518599
Iteration 80, loss = 0.02514468
Iteration 81, loss = 0.02529366
Iteration 82, loss = 0.02552402
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on dropped_v4_stemmed...
Iteration 1, loss = 0.68525044
Iteration 2, loss = 0.66285161
Iteration 3, loss = 0.64188656
Iteration 4, loss = 0.61851770
Iteration 5, loss = 0.59244798
Iteration 6, loss = 0.56391081
Iteration 7, loss = 0.53326144
Iteration 8, loss = 0.50111365
Iteration 9, loss = 0.46769650
Iteration 10, loss = 0.43357160
Iteration 11, loss = 0.39979084
Iteration 12, loss = 0.36683139
Iteration 13, loss = 0.33556049
Iteration 14, loss = 0.30615760
Iteration 15, loss = 0.27887081
Iteration 16, loss = 0.25371291
Iteration 17, loss = 0.23076809
Iteration 18, loss = 0.21019935
Iteration 19, loss = 0.19153211
Iteration 20, loss = 0.17491483
Iteration 21, loss = 0.15994514
Iteration 22, loss = 0.14673724
Iteration 23, loss = 0.13490652
Iteration 24, loss = 0.12435031
Iteration 25, loss = 0.11501425
Iteration 26, loss = 0.10661435
Iteration 27, loss = 0.09916930
Iteration 28, loss = 0.09247173
Iteration 29, loss = 0.08649150
Iteration 30, loss = 0.08109345
Iteration 31, loss = 0.07623208
Iteration 32, loss = 0.07185006
Iteration 33, loss = 0.06783849
Iteration 34, loss = 0.06423370
Iteration 35, loss = 0.06102868
Iteration 36, loss = 0.05796451
Iteration 37, loss = 0.05530953
Iteration 38, loss = 0.05274506
Iteration 39, loss = 0.05042652
Iteration 40, loss = 0.04831586
Iteration 41, loss = 0.04633177
Iteration 42, loss = 0.04461477
Iteration 43, loss = 0.04293479
Iteration 44, loss = 0.04131209
Iteration 45, loss = 0.03987593
Iteration 46, loss = 0.03857689
Iteration 47, loss = 0.03730320
Iteration 48, loss = 0.03620124
Iteration 49, loss = 0.03506689
Iteration 50, loss = 0.03407022
Iteration 51, loss = 0.03306427
Iteration 52, loss = 0.03215552
Iteration 53, loss = 0.03133502
Iteration 54, loss = 0.03057961
Iteration 55, loss = 0.02979090
Iteration 56, loss = 0.02917358
Iteration 57, loss = 0.02845236
Iteration 58, loss = 0.02780867
Iteration 59, loss = 0.02724983
Iteration 60, loss = 0.02665790
Iteration 61, loss = 0.02611774
Iteration 62, loss = 0.02564719
Iteration 63, loss = 0.02515720
Iteration 64, loss = 0.02471884
Iteration 65, loss = 0.02425937
Iteration 66, loss = 0.02384279
Iteration 67, loss = 0.02346470
Iteration 68, loss = 0.02309085
Iteration 69, loss = 0.02269673
Iteration 70, loss = 0.02240262
Iteration 71, loss = 0.02205408
Iteration 72, loss = 0.02173421
Iteration 73, loss = 0.02144289
Iteration 74, loss = 0.02116315
Iteration 75, loss = 0.02089825
Iteration 76, loss = 0.02063507
Iteration 77, loss = 0.02041049
Iteration 78, loss = 0.02015938
Iteration 79, loss = 0.01991144
Iteration 80, loss = 0.01972180
Iteration 81, loss = 0.01947540
Iteration 82, loss = 0.01926565
Iteration 83, loss = 0.01905294
Iteration 84, loss = 0.01883729
Iteration 85, loss = 0.01870599
Iteration 86, loss = 0.01851344
Iteration 87, loss = 0.01832549
Iteration 88, loss = 0.01820108
Iteration 89, loss = 0.01806010
Iteration 90, loss = 0.01788769
Iteration 91, loss = 0.01774283
Iteration 92, loss = 0.01758835
Iteration 93, loss = 0.01749474
Iteration 94, loss = 0.01727533
Iteration 95, loss = 0.01718765
Iteration 96, loss = 0.01707529
Iteration 97, loss = 0.01692457
Iteration 98, loss = 0.01679161
Iteration 99, loss = 0.01673160
Iteration 100, loss = 0.01662760
Iteration 101, loss = 0.01650598
Iteration 102, loss = 0.01642068
Iteration 103, loss = 0.01634602
Iteration 104, loss = 0.01620851
Iteration 105, loss = 0.01610376
Iteration 106, loss = 0.01599447
Iteration 107, loss = 0.01598273
Iteration 108, loss = 0.01592320
Iteration 109, loss = 0.01579344
Iteration 110, loss = 0.01570492
Iteration 111, loss = 0.01562493
Iteration 112, loss = 0.01557204
Iteration 113, loss = 0.01545075
Iteration 114, loss = 0.01538417
Iteration 115, loss = 0.01530089
Iteration 116, loss = 0.01526760
Iteration 117, loss = 0.01517932
Iteration 118, loss = 0.01513910
Iteration 119, loss = 0.01507467
Iteration 120, loss = 0.01498493
Iteration 121, loss = 0.01491997
Iteration 122, loss = 0.01487353
Iteration 123, loss = 0.01485253
Iteration 124, loss = 0.01477086
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67877368
Iteration 2, loss = 0.64503404
Iteration 3, loss = 0.59665369
Iteration 4, loss = 0.53302996
Iteration 5, loss = 0.46346975
Iteration 6, loss = 0.39463575
Iteration 7, loss = 0.33158073
Iteration 8, loss = 0.27672089
Iteration 9, loss = 0.23080225
Iteration 10, loss = 0.19322545
Iteration 11, loss = 0.16320978
Iteration 12, loss = 0.13921336
Iteration 13, loss = 0.12002486
Iteration 14, loss = 0.10470463
Iteration 15, loss = 0.09229150
Iteration 16, loss = 0.08218495
Iteration 17, loss = 0.07381102
Iteration 18, loss = 0.06691442
Iteration 19, loss = 0.06107658
Iteration 20, loss = 0.05601738
Iteration 21, loss = 0.05194675
Iteration 22, loss = 0.04842823
Iteration 23, loss = 0.04517542
Iteration 24, loss = 0.04243939
Iteration 25, loss = 0.04015217
Iteration 26, loss = 0.03812405
Iteration 27, loss = 0.03607757
Iteration 28, loss = 0.03441792
Iteration 29, loss = 0.03309897
Iteration 30, loss = 0.03165058
Iteration 31, loss = 0.03045436
Iteration 32, loss = 0.02939798
Iteration 33, loss = 0.02845806
Iteration 34, loss = 0.02747739
Iteration 35, loss = 0.02681198
Iteration 36, loss = 0.02602314
Iteration 37, loss = 0.02531970
Iteration 38, loss = 0.02466875
Iteration 39, loss = 0.02406587
Iteration 40, loss = 0.02362429
Iteration 41, loss = 0.02318499
Iteration 42, loss = 0.02263804
Iteration 43, loss = 0.02230560
Iteration 44, loss = 0.02187405
Iteration 45, loss = 0.02147221
Iteration 46, loss = 0.02108978
Iteration 47, loss = 0.02085793
Iteration 48, loss = 0.02057989
Iteration 49, loss = 0.02022314
Iteration 50, loss = 0.01997499
Iteration 51, loss = 0.01974672
Iteration 52, loss = 0.01961451
Iteration 53, loss = 0.01929841
Iteration 54, loss = 0.01919988
Iteration 55, loss = 0.01895822
Iteration 56, loss = 0.01882904
Iteration 57, loss = 0.01859064
Iteration 58, loss = 0.01846816
Iteration 59, loss = 0.01843565
Iteration 60, loss = 0.01818960
Iteration 61, loss = 0.01802076
Iteration 62, loss = 0.01787700
Iteration 63, loss = 0.01781708
Iteration 64, loss = 0.01763179
Iteration 65, loss = 0.01762569
Iteration 66, loss = 0.01766201
Iteration 67, loss = 0.01731342
Iteration 68, loss = 0.01740992
Iteration 69, loss = 0.01709511
Iteration 70, loss = 0.01708016
Iteration 71, loss = 0.01696193
Iteration 72, loss = 0.01696906
Iteration 73, loss = 0.01679072
Iteration 74, loss = 0.01680448
Iteration 75, loss = 0.01661326
Iteration 76, loss = 0.01661919
Iteration 77, loss = 0.01648597
Iteration 78, loss = 0.01645259
Iteration 79, loss = 0.01638529
Iteration 80, loss = 0.01635168
Iteration 81, loss = 0.01630053
Iteration 82, loss = 0.01637391
Iteration 83, loss = 0.01618815
Iteration 84, loss = 0.01617895
Iteration 85, loss = 0.01623912
Iteration 86, loss = 0.01609632
Iteration 87, loss = 0.01600925
Iteration 88, loss = 0.01590851
Iteration 89, loss = 0.01597458
Iteration 90, loss = 0.01597597
Iteration 91, loss = 0.01587541
Iteration 92, loss = 0.01577419
Iteration 93, loss = 0.01572068
Iteration 94, loss = 0.01586912
Iteration 95, loss = 0.01569678
Iteration 96, loss = 0.01557935
Iteration 97, loss = 0.01570491
Iteration 98, loss = 0.01573621
Iteration 99, loss = 0.01554681
Iteration 100, loss = 0.01575195
Iteration 101, loss = 0.01547610
Iteration 102, loss = 0.01546149
Iteration 103, loss = 0.01566098
Iteration 104, loss = 0.01549927
Iteration 105, loss = 0.01552391
Iteration 106, loss = 0.01547198
Iteration 107, loss = 0.01537557
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67760075
Iteration 2, loss = 0.63602676
Iteration 3, loss = 0.57409808
Iteration 4, loss = 0.49549104
Iteration 5, loss = 0.41420966
Iteration 6, loss = 0.33920761
Iteration 7, loss = 0.27642218
Iteration 8, loss = 0.22580819
Iteration 9, loss = 0.18600114
Iteration 10, loss = 0.15530213
Iteration 11, loss = 0.13187185
Iteration 12, loss = 0.11322215
Iteration 13, loss = 0.09877145
Iteration 14, loss = 0.08719251
Iteration 15, loss = 0.07807804
Iteration 16, loss = 0.07055933
Iteration 17, loss = 0.06416044
Iteration 18, loss = 0.05908052
Iteration 19, loss = 0.05473333
Iteration 20, loss = 0.05122264
Iteration 21, loss = 0.04848426
Iteration 22, loss = 0.04552844
Iteration 23, loss = 0.04327211
Iteration 24, loss = 0.04080570
Iteration 25, loss = 0.03916423
Iteration 26, loss = 0.03757036
Iteration 27, loss = 0.03614412
Iteration 28, loss = 0.03483853
Iteration 29, loss = 0.03395686
Iteration 30, loss = 0.03292586
Iteration 31, loss = 0.03229364
Iteration 32, loss = 0.03129814
Iteration 33, loss = 0.03042032
Iteration 34, loss = 0.02976437
Iteration 35, loss = 0.02930401
Iteration 36, loss = 0.02876162
Iteration 37, loss = 0.02808694
Iteration 38, loss = 0.02764732
Iteration 39, loss = 0.02709504
Iteration 40, loss = 0.02679939
Iteration 41, loss = 0.02646985
Iteration 42, loss = 0.02611186
Iteration 43, loss = 0.02574451
Iteration 44, loss = 0.02534229
Iteration 45, loss = 0.02523562
Iteration 46, loss = 0.02508071
Iteration 47, loss = 0.02505209
Iteration 48, loss = 0.02457871
Iteration 49, loss = 0.02406411
Iteration 50, loss = 0.02402208
Iteration 51, loss = 0.02377395
Iteration 52, loss = 0.02371615
Iteration 53, loss = 0.02361686
Iteration 54, loss = 0.02316477
Iteration 55, loss = 0.02344824
Iteration 56, loss = 0.02303354
Iteration 57, loss = 0.02288670
Iteration 58, loss = 0.02304413
Iteration 59, loss = 0.02286979
Iteration 60, loss = 0.02269982
Iteration 61, loss = 0.02235345
Iteration 62, loss = 0.02241043
Iteration 63, loss = 0.02238418
Iteration 64, loss = 0.02229225
Iteration 65, loss = 0.02196518
Iteration 66, loss = 0.02189215
Iteration 67, loss = 0.02182385
Iteration 68, loss = 0.02199542
Iteration 69, loss = 0.02185936
Iteration 70, loss = 0.02152309
Iteration 71, loss = 0.02164304
Iteration 72, loss = 0.02141729
Iteration 73, loss = 0.02161461
Iteration 74, loss = 0.02129357
Iteration 75, loss = 0.02124032
Iteration 76, loss = 0.02135324
Iteration 77, loss = 0.02124122
Iteration 78, loss = 0.02125929
Iteration 79, loss = 0.02110458
Iteration 80, loss = 0.02095440
Iteration 81, loss = 0.02087535
Iteration 82, loss = 0.02086399
Iteration 83, loss = 0.02083657
Iteration 84, loss = 0.02094256
Iteration 85, loss = 0.02089558
Iteration 86, loss = 0.02098735
Iteration 87, loss = 0.02071989
Iteration 88, loss = 0.02067886
Iteration 89, loss = 0.02058835
Iteration 90, loss = 0.02063378
Iteration 91, loss = 0.02046322
Iteration 92, loss = 0.02080839
Iteration 93, loss = 0.02062292
Iteration 94, loss = 0.02058287
Iteration 95, loss = 0.02061045
Iteration 96, loss = 0.02060358
Iteration 97, loss = 0.02054199
Iteration 98, loss = 0.02042037
Iteration 99, loss = 0.02042191
Iteration 100, loss = 0.02034732
Iteration 101, loss = 0.02055347
Iteration 102, loss = 0.02065985
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69985841
Iteration 2, loss = 0.64894974
Iteration 3, loss = 0.58298844
Iteration 4, loss = 0.49972863
Iteration 5, loss = 0.41434685
Iteration 6, loss = 0.33755013
Iteration 7, loss = 0.27327666
Iteration 8, loss = 0.22218133
Iteration 9, loss = 0.18242972
Iteration 10, loss = 0.15185227
Iteration 11, loss = 0.12856054
Iteration 12, loss = 0.11050378
Iteration 13, loss = 0.09632276
Iteration 14, loss = 0.08506164
Iteration 15, loss = 0.07607986
Iteration 16, loss = 0.06905057
Iteration 17, loss = 0.06316437
Iteration 18, loss = 0.05804422
Iteration 19, loss = 0.05397188
Iteration 20, loss = 0.05056547
Iteration 21, loss = 0.04763697
Iteration 22, loss = 0.04470470
Iteration 23, loss = 0.04261266
Iteration 24, loss = 0.04041341
Iteration 25, loss = 0.03872715
Iteration 26, loss = 0.03729866
Iteration 27, loss = 0.03574938
Iteration 28, loss = 0.03466218
Iteration 29, loss = 0.03359751
Iteration 30, loss = 0.03261507
Iteration 31, loss = 0.03172490
Iteration 32, loss = 0.03098146
Iteration 33, loss = 0.03027313
Iteration 34, loss = 0.02984710
Iteration 35, loss = 0.02938638
Iteration 36, loss = 0.02866537
Iteration 37, loss = 0.02799016
Iteration 38, loss = 0.02765087
Iteration 39, loss = 0.02737440
Iteration 40, loss = 0.02681080
Iteration 41, loss = 0.02633814
Iteration 42, loss = 0.02609665
Iteration 43, loss = 0.02554791
Iteration 44, loss = 0.02553892
Iteration 45, loss = 0.02535107
Iteration 46, loss = 0.02488179
Iteration 47, loss = 0.02502511
Iteration 48, loss = 0.02438992
Iteration 49, loss = 0.02411015
Iteration 50, loss = 0.02415310
Iteration 51, loss = 0.02405007
Iteration 52, loss = 0.02362284
Iteration 53, loss = 0.02383230
Iteration 54, loss = 0.02363566
Iteration 55, loss = 0.02351064
Iteration 56, loss = 0.02340028
Iteration 57, loss = 0.02299322
Iteration 58, loss = 0.02290513
Iteration 59, loss = 0.02283582
Iteration 60, loss = 0.02274857
Iteration 61, loss = 0.02270760
Iteration 62, loss = 0.02217925
Iteration 63, loss = 0.02239247
Iteration 64, loss = 0.02227373
Iteration 65, loss = 0.02238104
Iteration 66, loss = 0.02221745
Iteration 67, loss = 0.02211077
Iteration 68, loss = 0.02182521
Iteration 69, loss = 0.02203179
Iteration 70, loss = 0.02206929
Iteration 71, loss = 0.02204719
Iteration 72, loss = 0.02180303
Iteration 73, loss = 0.02171954
Iteration 74, loss = 0.02157625
Iteration 75, loss = 0.02187100
Iteration 76, loss = 0.02147190
Iteration 77, loss = 0.02173276
Iteration 78, loss = 0.02173300
Iteration 79, loss = 0.02148779
Iteration 80, loss = 0.02140727
Iteration 81, loss = 0.02118665
Iteration 82, loss = 0.02144579
Iteration 83, loss = 0.02123220
Iteration 84, loss = 0.02154219
Iteration 85, loss = 0.02134811
Iteration 86, loss = 0.02111337
Iteration 87, loss = 0.02101475
Iteration 88, loss = 0.02142311
Iteration 89, loss = 0.02091231
Iteration 90, loss = 0.02103292
Iteration 91, loss = 0.02097129
Iteration 92, loss = 0.02118574
Iteration 93, loss = 0.02121389
Iteration 94, loss = 0.02078049
Iteration 95, loss = 0.02063813
Iteration 96, loss = 0.02079622
Iteration 97, loss = 0.02059961
Iteration 98, loss = 0.02079550
Iteration 99, loss = 0.02078560
Iteration 100, loss = 0.02054701
Iteration 101, loss = 0.02110877
Iteration 102, loss = 0.02088603
Iteration 103, loss = 0.02101462
Iteration 104, loss = 0.02052993
Iteration 105, loss = 0.02049432
Iteration 106, loss = 0.02062843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67265238
Iteration 2, loss = 0.65360971
Iteration 3, loss = 0.63811638
Iteration 4, loss = 0.62360255
Iteration 5, loss = 0.60852963
Iteration 6, loss = 0.59283403
Iteration 7, loss = 0.57612413
Iteration 8, loss = 0.55823210
Iteration 9, loss = 0.53927308
Iteration 10, loss = 0.51931207
Iteration 11, loss = 0.49833567
Iteration 12, loss = 0.47656369
Iteration 13, loss = 0.45431815
Iteration 14, loss = 0.43185178
Iteration 15, loss = 0.40922492
Iteration 16, loss = 0.38691022
Iteration 17, loss = 0.36517573
Iteration 18, loss = 0.34395177
Iteration 19, loss = 0.32347574
Iteration 20, loss = 0.30407804
Iteration 21, loss = 0.28530362
Iteration 22, loss = 0.26763521
Iteration 23, loss = 0.25107022
Iteration 24, loss = 0.23528961
Iteration 25, loss = 0.22051833
Iteration 26, loss = 0.20692563
Iteration 27, loss = 0.19412445
Iteration 28, loss = 0.18238694
Iteration 29, loss = 0.17142329
Iteration 30, loss = 0.16132308
Iteration 31, loss = 0.15197435
Iteration 32, loss = 0.14333543
Iteration 33, loss = 0.13528548
Iteration 34, loss = 0.12793649
Iteration 35, loss = 0.12104036
Iteration 36, loss = 0.11472186
Iteration 37, loss = 0.10881638
Iteration 38, loss = 0.10333227
Iteration 39, loss = 0.09825046
Iteration 40, loss = 0.09358505
Iteration 41, loss = 0.08916751
Iteration 42, loss = 0.08515908
Iteration 43, loss = 0.08133383
Iteration 44, loss = 0.07779576
Iteration 45, loss = 0.07456537
Iteration 46, loss = 0.07148550
Iteration 47, loss = 0.06863651
Iteration 48, loss = 0.06598091
Iteration 49, loss = 0.06342958
Iteration 50, loss = 0.06109391
Iteration 51, loss = 0.05885192
Iteration 52, loss = 0.05679968
Iteration 53, loss = 0.05480188
Iteration 54, loss = 0.05297924
Iteration 55, loss = 0.05119668
Iteration 56, loss = 0.04957963
Iteration 57, loss = 0.04801844
Iteration 58, loss = 0.04649898
Iteration 59, loss = 0.04513143
Iteration 60, loss = 0.04379745
Iteration 61, loss = 0.04253075
Iteration 62, loss = 0.04135787
Iteration 63, loss = 0.04021454
Iteration 64, loss = 0.03910081
Iteration 65, loss = 0.03813189
Iteration 66, loss = 0.03712215
Iteration 67, loss = 0.03623841
Iteration 68, loss = 0.03534250
Iteration 69, loss = 0.03445851
Iteration 70, loss = 0.03363346
Iteration 71, loss = 0.03287091
Iteration 72, loss = 0.03211854
Iteration 73, loss = 0.03139231
Iteration 74, loss = 0.03069079
Iteration 75, loss = 0.03002930
Iteration 76, loss = 0.02939873
Iteration 77, loss = 0.02881923
Iteration 78, loss = 0.02820419
Iteration 79, loss = 0.02765922
Iteration 80, loss = 0.02711891
Iteration 81, loss = 0.02658824
Iteration 82, loss = 0.02608468
Iteration 83, loss = 0.02560437
Iteration 84, loss = 0.02513180
Iteration 85, loss = 0.02473218
Iteration 86, loss = 0.02427286
Iteration 87, loss = 0.02387100
Iteration 88, loss = 0.02345548
Iteration 89, loss = 0.02307194
Iteration 90, loss = 0.02273229
Iteration 91, loss = 0.02236356
Iteration 92, loss = 0.02202871
Iteration 93, loss = 0.02166811
Iteration 94, loss = 0.02134954
Iteration 95, loss = 0.02104390
Iteration 96, loss = 0.02073994
Iteration 97, loss = 0.02043941
Iteration 98, loss = 0.02015306
Iteration 99, loss = 0.01988319
Iteration 100, loss = 0.01960914
Iteration 101, loss = 0.01933299
Iteration 102, loss = 0.01909297
Iteration 103, loss = 0.01882297
Iteration 104, loss = 0.01860383
Iteration 105, loss = 0.01834863
Iteration 106, loss = 0.01812576
Iteration 107, loss = 0.01789147
Iteration 108, loss = 0.01770904
Iteration 109, loss = 0.01748728
Iteration 110, loss = 0.01730377
Iteration 111, loss = 0.01715802
Iteration 112, loss = 0.01693573
Iteration 113, loss = 0.01677925
Iteration 114, loss = 0.01659204
Iteration 115, loss = 0.01641686
Iteration 116, loss = 0.01626413
Iteration 117, loss = 0.01614616
Iteration 118, loss = 0.01594874
Iteration 119, loss = 0.01581923
Iteration 120, loss = 0.01567346
Iteration 121, loss = 0.01548027
Iteration 122, loss = 0.01533163
Iteration 123, loss = 0.01515359
Iteration 124, loss = 0.01497201
Iteration 125, loss = 0.01483495
Iteration 126, loss = 0.01470613
Iteration 127, loss = 0.01455244
Iteration 128, loss = 0.01448665
Iteration 129, loss = 0.01433927
Iteration 130, loss = 0.01421728
Iteration 131, loss = 0.01410477
Iteration 132, loss = 0.01398279
Iteration 133, loss = 0.01388439
Iteration 134, loss = 0.01379734
Iteration 135, loss = 0.01365281
Iteration 136, loss = 0.01352589
Iteration 137, loss = 0.01342371
Iteration 138, loss = 0.01333870
Iteration 139, loss = 0.01322578
Iteration 140, loss = 0.01314492
Iteration 141, loss = 0.01300864
Iteration 142, loss = 0.01292187
Iteration 143, loss = 0.01283326
Iteration 144, loss = 0.01275736
Iteration 145, loss = 0.01264999
Iteration 146, loss = 0.01259287
Iteration 147, loss = 0.01252525
Iteration 148, loss = 0.01243735
Iteration 149, loss = 0.01237246
Iteration 150, loss = 0.01226510
Iteration 151, loss = 0.01223214
Iteration 152, loss = 0.01212244
Iteration 153, loss = 0.01204360
Iteration 154, loss = 0.01197833
Iteration 155, loss = 0.01190377
Iteration 156, loss = 0.01186321
Iteration 157, loss = 0.01179958
Iteration 158, loss = 0.01172468
Iteration 159, loss = 0.01167337
Iteration 160, loss = 0.01159340
Iteration 161, loss = 0.01152616
Iteration 162, loss = 0.01146752
Iteration 163, loss = 0.01140489
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69021230
Iteration 2, loss = 0.67422881
Iteration 3, loss = 0.66101734
Iteration 4, loss = 0.64852303
Iteration 5, loss = 0.63581947
Iteration 6, loss = 0.62252712
Iteration 7, loss = 0.60818884
Iteration 8, loss = 0.59296168
Iteration 9, loss = 0.57684046
Iteration 10, loss = 0.55979766
Iteration 11, loss = 0.54173291
Iteration 12, loss = 0.52299879
Iteration 13, loss = 0.50343184
Iteration 14, loss = 0.48334784
Iteration 15, loss = 0.46291099
Iteration 16, loss = 0.44231357
Iteration 17, loss = 0.42155229
Iteration 18, loss = 0.40109508
Iteration 19, loss = 0.38068510
Iteration 20, loss = 0.36080169
Iteration 21, loss = 0.34154202
Iteration 22, loss = 0.32292915
Iteration 23, loss = 0.30500828
Iteration 24, loss = 0.28798230
Iteration 25, loss = 0.27164058
Iteration 26, loss = 0.25622810
Iteration 27, loss = 0.24173613
Iteration 28, loss = 0.22797793
Iteration 29, loss = 0.21514618
Iteration 30, loss = 0.20303927
Iteration 31, loss = 0.19174059
Iteration 32, loss = 0.18117812
Iteration 33, loss = 0.17130342
Iteration 34, loss = 0.16205082
Iteration 35, loss = 0.15340422
Iteration 36, loss = 0.14533557
Iteration 37, loss = 0.13786427
Iteration 38, loss = 0.13078077
Iteration 39, loss = 0.12427734
Iteration 40, loss = 0.11818448
Iteration 41, loss = 0.11254432
Iteration 42, loss = 0.10717259
Iteration 43, loss = 0.10219669
Iteration 44, loss = 0.09757639
Iteration 45, loss = 0.09322680
Iteration 46, loss = 0.08911465
Iteration 47, loss = 0.08527996
Iteration 48, loss = 0.08171152
Iteration 49, loss = 0.07834047
Iteration 50, loss = 0.07518637
Iteration 51, loss = 0.07224466
Iteration 52, loss = 0.06941411
Iteration 53, loss = 0.06679511
Iteration 54, loss = 0.06431321
Iteration 55, loss = 0.06202479
Iteration 56, loss = 0.05974817
Iteration 57, loss = 0.05766480
Iteration 58, loss = 0.05566981
Iteration 59, loss = 0.05380813
Iteration 60, loss = 0.05204548
Iteration 61, loss = 0.05035409
Iteration 62, loss = 0.04874061
Iteration 63, loss = 0.04723637
Iteration 64, loss = 0.04578947
Iteration 65, loss = 0.04440385
Iteration 66, loss = 0.04309818
Iteration 67, loss = 0.04186599
Iteration 68, loss = 0.04066518
Iteration 69, loss = 0.03953908
Iteration 70, loss = 0.03845996
Iteration 71, loss = 0.03740002
Iteration 72, loss = 0.03642995
Iteration 73, loss = 0.03547223
Iteration 74, loss = 0.03454706
Iteration 75, loss = 0.03366714
Iteration 76, loss = 0.03283601
Iteration 77, loss = 0.03204253
Iteration 78, loss = 0.03123354
Iteration 79, loss = 0.03050705
Iteration 80, loss = 0.02978995
Iteration 81, loss = 0.02910546
Iteration 82, loss = 0.02843973
Iteration 83, loss = 0.02783338
Iteration 84, loss = 0.02720317
Iteration 85, loss = 0.02662578
Iteration 86, loss = 0.02606957
Iteration 87, loss = 0.02554360
Iteration 88, loss = 0.02499432
Iteration 89, loss = 0.02447680
Iteration 90, loss = 0.02399492
Iteration 91, loss = 0.02352837
Iteration 92, loss = 0.02308547
Iteration 93, loss = 0.02263834
Iteration 94, loss = 0.02219920
Iteration 95, loss = 0.02181168
Iteration 96, loss = 0.02142438
Iteration 97, loss = 0.02106451
Iteration 98, loss = 0.02068663
Iteration 99, loss = 0.02031735
Iteration 100, loss = 0.01996858
Iteration 101, loss = 0.01961352
Iteration 102, loss = 0.01928808
Iteration 103, loss = 0.01901612
Iteration 104, loss = 0.01866460
Iteration 105, loss = 0.01836622
Iteration 106, loss = 0.01807173
Iteration 107, loss = 0.01779878
Iteration 108, loss = 0.01751135
Iteration 109, loss = 0.01724795
Iteration 110, loss = 0.01701787
Iteration 111, loss = 0.01674467
Iteration 112, loss = 0.01648924
Iteration 113, loss = 0.01624251
Iteration 114, loss = 0.01602214
Iteration 115, loss = 0.01577675
Iteration 116, loss = 0.01557932
Iteration 117, loss = 0.01546406
Iteration 118, loss = 0.01520876
Iteration 119, loss = 0.01500410
Iteration 120, loss = 0.01480516
Iteration 121, loss = 0.01461614
Iteration 122, loss = 0.01441269
Iteration 123, loss = 0.01422899
Iteration 124, loss = 0.01405129
Iteration 125, loss = 0.01387595
Iteration 126, loss = 0.01370922
Iteration 127, loss = 0.01359391
Iteration 128, loss = 0.01339242
Iteration 129, loss = 0.01323855
Iteration 130, loss = 0.01309155
Iteration 131, loss = 0.01294669
Iteration 132, loss = 0.01279593
Iteration 133, loss = 0.01268870
Iteration 134, loss = 0.01252798
Iteration 135, loss = 0.01239934
Iteration 136, loss = 0.01227257
Iteration 137, loss = 0.01214444
Iteration 138, loss = 0.01201732
Iteration 139, loss = 0.01191352
Iteration 140, loss = 0.01178895
Iteration 141, loss = 0.01167674
Iteration 142, loss = 0.01155311
Iteration 143, loss = 0.01144640
Iteration 144, loss = 0.01135383
Iteration 145, loss = 0.01124607
Iteration 146, loss = 0.01113809
Iteration 147, loss = 0.01102957
Iteration 148, loss = 0.01093880
Iteration 149, loss = 0.01084901
Iteration 150, loss = 0.01075480
Iteration 151, loss = 0.01067076
Iteration 152, loss = 0.01055637
Iteration 153, loss = 0.01048227
Iteration 154, loss = 0.01037860
Iteration 155, loss = 0.01029317
Iteration 156, loss = 0.01020120
Iteration 157, loss = 0.01012307
Iteration 158, loss = 0.01004251
Iteration 159, loss = 0.00998199
Iteration 160, loss = 0.00988067
Iteration 161, loss = 0.00980376
Iteration 162, loss = 0.00971987
Iteration 163, loss = 0.00964316
Iteration 164, loss = 0.00956963
Iteration 165, loss = 0.00947276
Iteration 166, loss = 0.00940337
Iteration 167, loss = 0.00938461
Iteration 168, loss = 0.00928964
Iteration 169, loss = 0.00922822
Iteration 170, loss = 0.00917511
Iteration 171, loss = 0.00908988
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68353807
Iteration 2, loss = 0.65586356
Iteration 3, loss = 0.62441537
Iteration 4, loss = 0.58570829
Iteration 5, loss = 0.54004080
Iteration 6, loss = 0.49115135
Iteration 7, loss = 0.44286915
Iteration 8, loss = 0.39628740
Iteration 9, loss = 0.35254978
Iteration 10, loss = 0.31272891
Iteration 11, loss = 0.27672556
Iteration 12, loss = 0.24495575
Iteration 13, loss = 0.21702868
Iteration 14, loss = 0.19256452
Iteration 15, loss = 0.17159941
Iteration 16, loss = 0.15364869
Iteration 17, loss = 0.13803465
Iteration 18, loss = 0.12474960
Iteration 19, loss = 0.11322285
Iteration 20, loss = 0.10336386
Iteration 21, loss = 0.09474056
Iteration 22, loss = 0.08718699
Iteration 23, loss = 0.08066067
Iteration 24, loss = 0.07478471
Iteration 25, loss = 0.06984531
Iteration 26, loss = 0.06525710
Iteration 27, loss = 0.06130129
Iteration 28, loss = 0.05771006
Iteration 29, loss = 0.05450244
Iteration 30, loss = 0.05171299
Iteration 31, loss = 0.04914498
Iteration 32, loss = 0.04680031
Iteration 33, loss = 0.04478454
Iteration 34, loss = 0.04280742
Iteration 35, loss = 0.04114940
Iteration 36, loss = 0.03956904
Iteration 37, loss = 0.03809570
Iteration 38, loss = 0.03668903
Iteration 39, loss = 0.03547197
Iteration 40, loss = 0.03437261
Iteration 41, loss = 0.03327331
Iteration 42, loss = 0.03228882
Iteration 43, loss = 0.03138141
Iteration 44, loss = 0.03057907
Iteration 45, loss = 0.02974945
Iteration 46, loss = 0.02935004
Iteration 47, loss = 0.02863363
Iteration 48, loss = 0.02807848
Iteration 49, loss = 0.02740180
Iteration 50, loss = 0.02686216
Iteration 51, loss = 0.02629409
Iteration 52, loss = 0.02575667
Iteration 53, loss = 0.02500227
Iteration 54, loss = 0.02473326
Iteration 55, loss = 0.02440913
Iteration 56, loss = 0.02401521
Iteration 57, loss = 0.02346313
Iteration 58, loss = 0.02305448
Iteration 59, loss = 0.02258157
Iteration 60, loss = 0.02231236
Iteration 61, loss = 0.02196705
Iteration 62, loss = 0.02176515
Iteration 63, loss = 0.02134841
Iteration 64, loss = 0.02128633
Iteration 65, loss = 0.02108601
Iteration 66, loss = 0.02074992
Iteration 67, loss = 0.02061229
Iteration 68, loss = 0.02030723
Iteration 69, loss = 0.02018527
Iteration 70, loss = 0.01999609
Iteration 71, loss = 0.01977947
Iteration 72, loss = 0.01952425
Iteration 73, loss = 0.01935260
Iteration 74, loss = 0.01917519
Iteration 75, loss = 0.01903174
Iteration 76, loss = 0.01879520
Iteration 77, loss = 0.01871954
Iteration 78, loss = 0.01860416
Iteration 79, loss = 0.01843725
Iteration 80, loss = 0.01840736
Iteration 81, loss = 0.01815825
Iteration 82, loss = 0.01808344
Iteration 83, loss = 0.01793283
Iteration 84, loss = 0.01781338
Iteration 85, loss = 0.01783640
Iteration 86, loss = 0.01777120
Iteration 87, loss = 0.01774887
Iteration 88, loss = 0.01756839
Iteration 89, loss = 0.01746690
Iteration 90, loss = 0.01731042
Iteration 91, loss = 0.01721468
Iteration 92, loss = 0.01712222
Iteration 93, loss = 0.01704084
Iteration 94, loss = 0.01703183
Iteration 95, loss = 0.01703547
Iteration 96, loss = 0.01700532
Iteration 97, loss = 0.01685340
Iteration 98, loss = 0.01675671
Iteration 99, loss = 0.01682646
Iteration 100, loss = 0.01670136
Iteration 101, loss = 0.01666383
Iteration 102, loss = 0.01668486
Iteration 103, loss = 0.01662819
Iteration 104, loss = 0.01655966
Iteration 105, loss = 0.01641262
Iteration 106, loss = 0.01638140
Iteration 107, loss = 0.01616958
Iteration 108, loss = 0.01623959
Iteration 109, loss = 0.01648376
Iteration 110, loss = 0.01643028
Iteration 111, loss = 0.01631167
Iteration 112, loss = 0.01606652
Iteration 113, loss = 0.01599341
Iteration 114, loss = 0.01575998
Iteration 115, loss = 0.01600530
Iteration 116, loss = 0.01590648
Iteration 117, loss = 0.01601528
Iteration 118, loss = 0.01596085
Iteration 119, loss = 0.01583007
Iteration 120, loss = 0.01567588
Iteration 121, loss = 0.01561970
Iteration 122, loss = 0.01557580
Iteration 123, loss = 0.01559661
Iteration 124, loss = 0.01557515
Iteration 125, loss = 0.01560248
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70581680
Iteration 2, loss = 0.69219378
Iteration 3, loss = 0.68022257
Iteration 4, loss = 0.66812375
Iteration 5, loss = 0.65524011
Iteration 6, loss = 0.64126440
Iteration 7, loss = 0.62596471
Iteration 8, loss = 0.60947178
Iteration 9, loss = 0.59141194
Iteration 10, loss = 0.57227190
Iteration 11, loss = 0.55180222
Iteration 12, loss = 0.53055568
Iteration 13, loss = 0.50864411
Iteration 14, loss = 0.48622917
Iteration 15, loss = 0.46349221
Iteration 16, loss = 0.44076644
Iteration 17, loss = 0.41822555
Iteration 18, loss = 0.39604105
Iteration 19, loss = 0.37429640
Iteration 20, loss = 0.35309938
Iteration 21, loss = 0.33249513
Iteration 22, loss = 0.31281832
Iteration 23, loss = 0.29390849
Iteration 24, loss = 0.27598678
Iteration 25, loss = 0.25899168
Iteration 26, loss = 0.24300320
Iteration 27, loss = 0.22794747
Iteration 28, loss = 0.21380720
Iteration 29, loss = 0.20068714
Iteration 30, loss = 0.18828262
Iteration 31, loss = 0.17683197
Iteration 32, loss = 0.16611608
Iteration 33, loss = 0.15621090
Iteration 34, loss = 0.14703044
Iteration 35, loss = 0.13854816
Iteration 36, loss = 0.13076439
Iteration 37, loss = 0.12350075
Iteration 38, loss = 0.11677925
Iteration 39, loss = 0.11058016
Iteration 40, loss = 0.10480831
Iteration 41, loss = 0.09952145
Iteration 42, loss = 0.09453792
Iteration 43, loss = 0.08994828
Iteration 44, loss = 0.08566571
Iteration 45, loss = 0.08170742
Iteration 46, loss = 0.07802922
Iteration 47, loss = 0.07455839
Iteration 48, loss = 0.07135006
Iteration 49, loss = 0.06837376
Iteration 50, loss = 0.06557696
Iteration 51, loss = 0.06294958
Iteration 52, loss = 0.06052115
Iteration 53, loss = 0.05826160
Iteration 54, loss = 0.05609937
Iteration 55, loss = 0.05407192
Iteration 56, loss = 0.05219532
Iteration 57, loss = 0.05039315
Iteration 58, loss = 0.04871306
Iteration 59, loss = 0.04713877
Iteration 60, loss = 0.04560435
Iteration 61, loss = 0.04418481
Iteration 62, loss = 0.04283824
Iteration 63, loss = 0.04155781
Iteration 64, loss = 0.04035538
Iteration 65, loss = 0.03920466
Iteration 66, loss = 0.03811184
Iteration 67, loss = 0.03708764
Iteration 68, loss = 0.03610643
Iteration 69, loss = 0.03520691
Iteration 70, loss = 0.03429233
Iteration 71, loss = 0.03345263
Iteration 72, loss = 0.03261377
Iteration 73, loss = 0.03182694
Iteration 74, loss = 0.03111298
Iteration 75, loss = 0.03036269
Iteration 76, loss = 0.02968910
Iteration 77, loss = 0.02903969
Iteration 78, loss = 0.02840678
Iteration 79, loss = 0.02781948
Iteration 80, loss = 0.02723674
Iteration 81, loss = 0.02669025
Iteration 82, loss = 0.02616624
Iteration 83, loss = 0.02564247
Iteration 84, loss = 0.02516942
Iteration 85, loss = 0.02469550
Iteration 86, loss = 0.02426532
Iteration 87, loss = 0.02382109
Iteration 88, loss = 0.02341524
Iteration 89, loss = 0.02299967
Iteration 90, loss = 0.02260984
Iteration 91, loss = 0.02224033
Iteration 92, loss = 0.02189046
Iteration 93, loss = 0.02150543
Iteration 94, loss = 0.02114834
Iteration 95, loss = 0.02087632
Iteration 96, loss = 0.02052842
Iteration 97, loss = 0.02023770
Iteration 98, loss = 0.01994013
Iteration 99, loss = 0.01966998
Iteration 100, loss = 0.01938073
Iteration 101, loss = 0.01913474
Iteration 102, loss = 0.01883677
Iteration 103, loss = 0.01861457
Iteration 104, loss = 0.01837742
Iteration 105, loss = 0.01812319
Iteration 106, loss = 0.01791457
Iteration 107, loss = 0.01767202
Iteration 108, loss = 0.01748524
Iteration 109, loss = 0.01725016
Iteration 110, loss = 0.01704713
Iteration 111, loss = 0.01684581
Iteration 112, loss = 0.01667370
Iteration 113, loss = 0.01648562
Iteration 114, loss = 0.01631688
Iteration 115, loss = 0.01615461
Iteration 116, loss = 0.01598103
Iteration 117, loss = 0.01580508
Iteration 118, loss = 0.01567117
Iteration 119, loss = 0.01552784
Iteration 120, loss = 0.01533486
Iteration 121, loss = 0.01519725
Iteration 122, loss = 0.01502581
Iteration 123, loss = 0.01488771
Iteration 124, loss = 0.01475721
Iteration 125, loss = 0.01461595
Iteration 126, loss = 0.01448889
Iteration 127, loss = 0.01438628
Iteration 128, loss = 0.01426737
Iteration 129, loss = 0.01411038
Iteration 130, loss = 0.01399003
Iteration 131, loss = 0.01390221
Iteration 132, loss = 0.01378785
Iteration 133, loss = 0.01368578
Iteration 134, loss = 0.01355708
Iteration 135, loss = 0.01353190
Iteration 136, loss = 0.01338514
Iteration 137, loss = 0.01324541
Iteration 138, loss = 0.01318925
Iteration 139, loss = 0.01307055
Iteration 140, loss = 0.01298388
Iteration 141, loss = 0.01287438
Iteration 142, loss = 0.01280109
Iteration 143, loss = 0.01269174
Iteration 144, loss = 0.01263966
Iteration 145, loss = 0.01252848
Iteration 146, loss = 0.01244194
Iteration 147, loss = 0.01238934
Iteration 148, loss = 0.01232430
Iteration 149, loss = 0.01225826
Iteration 150, loss = 0.01218265
Iteration 151, loss = 0.01214920
Iteration 152, loss = 0.01206078
Iteration 153, loss = 0.01201826
Iteration 154, loss = 0.01192043
Iteration 155, loss = 0.01186996
Iteration 156, loss = 0.01177803
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68946063
Iteration 2, loss = 0.66602948
Iteration 3, loss = 0.63556328
Iteration 4, loss = 0.59408466
Iteration 5, loss = 0.54118366
Iteration 6, loss = 0.48170014
Iteration 7, loss = 0.42114389
Iteration 8, loss = 0.36422846
Iteration 9, loss = 0.31254766
Iteration 10, loss = 0.26674340
Iteration 11, loss = 0.22785035
Iteration 12, loss = 0.19503595
Iteration 13, loss = 0.16775042
Iteration 14, loss = 0.14534659
Iteration 15, loss = 0.12682839
Iteration 16, loss = 0.11166286
Iteration 17, loss = 0.09905492
Iteration 18, loss = 0.08849701
Iteration 19, loss = 0.07964315
Iteration 20, loss = 0.07219561
Iteration 21, loss = 0.06582753
Iteration 22, loss = 0.06045236
Iteration 23, loss = 0.05567763
Iteration 24, loss = 0.05164683
Iteration 25, loss = 0.04806432
Iteration 26, loss = 0.04494238
Iteration 27, loss = 0.04214098
Iteration 28, loss = 0.03971535
Iteration 29, loss = 0.03758195
Iteration 30, loss = 0.03557187
Iteration 31, loss = 0.03386684
Iteration 32, loss = 0.03228732
Iteration 33, loss = 0.03089815
Iteration 34, loss = 0.02957165
Iteration 35, loss = 0.02840890
Iteration 36, loss = 0.02734895
Iteration 37, loss = 0.02637870
Iteration 38, loss = 0.02555615
Iteration 39, loss = 0.02468737
Iteration 40, loss = 0.02389257
Iteration 41, loss = 0.02322582
Iteration 42, loss = 0.02249819
Iteration 43, loss = 0.02196626
Iteration 44, loss = 0.02138433
Iteration 45, loss = 0.02098695
Iteration 46, loss = 0.02039968
Iteration 47, loss = 0.01994682
Iteration 48, loss = 0.01949308
Iteration 49, loss = 0.01911465
Iteration 50, loss = 0.01878100
Iteration 51, loss = 0.01839468
Iteration 52, loss = 0.01807246
Iteration 53, loss = 0.01786096
Iteration 54, loss = 0.01751203
Iteration 55, loss = 0.01720298
Iteration 56, loss = 0.01695441
Iteration 57, loss = 0.01673990
Iteration 58, loss = 0.01651757
Iteration 59, loss = 0.01626739
Iteration 60, loss = 0.01604485
Iteration 61, loss = 0.01583907
Iteration 62, loss = 0.01573323
Iteration 63, loss = 0.01558787
Iteration 64, loss = 0.01540244
Iteration 65, loss = 0.01522618
Iteration 66, loss = 0.01506756
Iteration 67, loss = 0.01497860
Iteration 68, loss = 0.01480934
Iteration 69, loss = 0.01467713
Iteration 70, loss = 0.01454040
Iteration 71, loss = 0.01444614
Iteration 72, loss = 0.01429543
Iteration 73, loss = 0.01423425
Iteration 74, loss = 0.01418964
Iteration 75, loss = 0.01404435
Iteration 76, loss = 0.01397181
Iteration 77, loss = 0.01384026
Iteration 78, loss = 0.01371292
Iteration 79, loss = 0.01362351
Iteration 80, loss = 0.01350963
Iteration 81, loss = 0.01346003
Iteration 82, loss = 0.01339386
Iteration 83, loss = 0.01330269
Iteration 84, loss = 0.01326628
Iteration 85, loss = 0.01322712
Iteration 86, loss = 0.01312494
Iteration 87, loss = 0.01306205
Iteration 88, loss = 0.01297728
Iteration 89, loss = 0.01295882
Iteration 90, loss = 0.01290072
Iteration 91, loss = 0.01293142
Iteration 92, loss = 0.01275343
Iteration 93, loss = 0.01277850
Iteration 94, loss = 0.01274609
Iteration 95, loss = 0.01274515
Iteration 96, loss = 0.01265545
Iteration 97, loss = 0.01255291
Iteration 98, loss = 0.01254996
Iteration 99, loss = 0.01251561
Iteration 100, loss = 0.01239452
Iteration 101, loss = 0.01239430
Iteration 102, loss = 0.01237274
Iteration 103, loss = 0.01232064
Iteration 104, loss = 0.01230977
Iteration 105, loss = 0.01221040
Iteration 106, loss = 0.01222372
Iteration 107, loss = 0.01213032
Iteration 108, loss = 0.01213004
Iteration 109, loss = 0.01210060
Iteration 110, loss = 0.01212563
Iteration 111, loss = 0.01202700
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69040299
Iteration 2, loss = 0.63643409
Iteration 3, loss = 0.55292611
Iteration 4, loss = 0.45120161
Iteration 5, loss = 0.35618514
Iteration 6, loss = 0.27811620
Iteration 7, loss = 0.21774528
Iteration 8, loss = 0.17379631
Iteration 9, loss = 0.14177620
Iteration 10, loss = 0.11833172
Iteration 11, loss = 0.10111659
Iteration 12, loss = 0.08790261
Iteration 13, loss = 0.07760017
Iteration 14, loss = 0.06991459
Iteration 15, loss = 0.06348027
Iteration 16, loss = 0.05837245
Iteration 17, loss = 0.05415042
Iteration 18, loss = 0.05090852
Iteration 19, loss = 0.04775598
Iteration 20, loss = 0.04529201
Iteration 21, loss = 0.04342062
Iteration 22, loss = 0.04152052
Iteration 23, loss = 0.03979662
Iteration 24, loss = 0.03882534
Iteration 25, loss = 0.03683861
Iteration 26, loss = 0.03610420
Iteration 27, loss = 0.03516323
Iteration 28, loss = 0.03449598
Iteration 29, loss = 0.03346259
Iteration 30, loss = 0.03277814
Iteration 31, loss = 0.03228974
Iteration 32, loss = 0.03173400
Iteration 33, loss = 0.03116663
Iteration 34, loss = 0.03055747
Iteration 35, loss = 0.03039502
Iteration 36, loss = 0.02988718
Iteration 37, loss = 0.02921511
Iteration 38, loss = 0.02916453
Iteration 39, loss = 0.02894168
Iteration 40, loss = 0.02850247
Iteration 41, loss = 0.02836182
Iteration 42, loss = 0.02800727
Iteration 43, loss = 0.02801235
Iteration 44, loss = 0.02786316
Iteration 45, loss = 0.02726286
Iteration 46, loss = 0.02717481
Iteration 47, loss = 0.02688069
Iteration 48, loss = 0.02683640
Iteration 49, loss = 0.02665422
Iteration 50, loss = 0.02678714
Iteration 51, loss = 0.02639453
Iteration 52, loss = 0.02643979
Iteration 53, loss = 0.02613389
Iteration 54, loss = 0.02613544
Iteration 55, loss = 0.02570132
Iteration 56, loss = 0.02737974
Iteration 57, loss = 0.02701439
Iteration 58, loss = 0.02630115
Iteration 59, loss = 0.02624423
Iteration 60, loss = 0.02575450
Iteration 61, loss = 0.02588991
Iteration 62, loss = 0.02578730
Iteration 63, loss = 0.02601788
Iteration 64, loss = 0.02628352
Iteration 65, loss = 0.02591990
Iteration 66, loss = 0.02575996
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71265009
Iteration 2, loss = 0.64366279
Iteration 3, loss = 0.55284473
Iteration 4, loss = 0.44760129
Iteration 5, loss = 0.35219612
Iteration 6, loss = 0.27399985
Iteration 7, loss = 0.21593444
Iteration 8, loss = 0.17274600
Iteration 9, loss = 0.14077732
Iteration 10, loss = 0.11745665
Iteration 11, loss = 0.10027271
Iteration 12, loss = 0.08755143
Iteration 13, loss = 0.07770350
Iteration 14, loss = 0.06957037
Iteration 15, loss = 0.06308132
Iteration 16, loss = 0.05800649
Iteration 17, loss = 0.05397423
Iteration 18, loss = 0.05022844
Iteration 19, loss = 0.04714690
Iteration 20, loss = 0.04468166
Iteration 21, loss = 0.04274217
Iteration 22, loss = 0.04051764
Iteration 23, loss = 0.03902539
Iteration 24, loss = 0.03753539
Iteration 25, loss = 0.03627019
Iteration 26, loss = 0.03518588
Iteration 27, loss = 0.03408075
Iteration 28, loss = 0.03341538
Iteration 29, loss = 0.03263108
Iteration 30, loss = 0.03177163
Iteration 31, loss = 0.03154172
Iteration 32, loss = 0.03053516
Iteration 33, loss = 0.03022771
Iteration 34, loss = 0.02949832
Iteration 35, loss = 0.02920884
Iteration 36, loss = 0.02939603
Iteration 37, loss = 0.02909949
Iteration 38, loss = 0.02864500
Iteration 39, loss = 0.02834546
Iteration 40, loss = 0.02803857
Iteration 41, loss = 0.02744784
Iteration 42, loss = 0.02752129
Iteration 43, loss = 0.02710373
Iteration 44, loss = 0.02690863
Iteration 45, loss = 0.02731047
Iteration 46, loss = 0.02828225
Iteration 47, loss = 0.02762351
Iteration 48, loss = 0.02727038
Iteration 49, loss = 0.02708921
Iteration 50, loss = 0.02659920
Iteration 51, loss = 0.02637537
Iteration 52, loss = 0.02602665
Iteration 53, loss = 0.02602564
Iteration 54, loss = 0.02552377
Iteration 55, loss = 0.02519900
Iteration 56, loss = 0.02503232
Iteration 57, loss = 0.02500945
Iteration 58, loss = 0.02482330
Iteration 59, loss = 0.02455316
Iteration 60, loss = 0.02547761
Iteration 61, loss = 0.02579412
Iteration 62, loss = 0.02534453
Iteration 63, loss = 0.02523334
Iteration 64, loss = 0.02515166
Iteration 65, loss = 0.02537703
Iteration 66, loss = 0.02482592
Iteration 67, loss = 0.02660425
Iteration 68, loss = 0.02656462
Iteration 69, loss = 0.02629022
Iteration 70, loss = 0.02562674
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71128853
Iteration 2, loss = 0.66084385
Iteration 3, loss = 0.59513496
Iteration 4, loss = 0.51768745
Iteration 5, loss = 0.43687713
Iteration 6, loss = 0.36005493
Iteration 7, loss = 0.29335248
Iteration 8, loss = 0.23878830
Iteration 9, loss = 0.19535918
Iteration 10, loss = 0.16162426
Iteration 11, loss = 0.13572086
Iteration 12, loss = 0.11579492
Iteration 13, loss = 0.10020084
Iteration 14, loss = 0.08790822
Iteration 15, loss = 0.07835577
Iteration 16, loss = 0.07037858
Iteration 17, loss = 0.06386804
Iteration 18, loss = 0.05847305
Iteration 19, loss = 0.05408911
Iteration 20, loss = 0.05022767
Iteration 21, loss = 0.04698730
Iteration 22, loss = 0.04424637
Iteration 23, loss = 0.04169601
Iteration 24, loss = 0.03981071
Iteration 25, loss = 0.03795512
Iteration 26, loss = 0.03623929
Iteration 27, loss = 0.03474791
Iteration 28, loss = 0.03338908
Iteration 29, loss = 0.03224124
Iteration 30, loss = 0.03120197
Iteration 31, loss = 0.03028109
Iteration 32, loss = 0.02946579
Iteration 33, loss = 0.02872831
Iteration 34, loss = 0.02792127
Iteration 35, loss = 0.02731815
Iteration 36, loss = 0.02658778
Iteration 37, loss = 0.02627692
Iteration 38, loss = 0.02588262
Iteration 39, loss = 0.02517375
Iteration 40, loss = 0.02477741
Iteration 41, loss = 0.02444701
Iteration 42, loss = 0.02435396
Iteration 43, loss = 0.02396935
Iteration 44, loss = 0.02369560
Iteration 45, loss = 0.02334435
Iteration 46, loss = 0.02301486
Iteration 47, loss = 0.02277173
Iteration 48, loss = 0.02288923
Iteration 49, loss = 0.02245724
Iteration 50, loss = 0.02218727
Iteration 51, loss = 0.02187074
Iteration 52, loss = 0.02162111
Iteration 53, loss = 0.02166182
Iteration 54, loss = 0.02149906
Iteration 55, loss = 0.02121112
Iteration 56, loss = 0.02112388
Iteration 57, loss = 0.02106853
Iteration 58, loss = 0.02105072
Iteration 59, loss = 0.02096638
Iteration 60, loss = 0.02091440
Iteration 61, loss = 0.02086284
Iteration 62, loss = 0.02072578
Iteration 63, loss = 0.02048807
Iteration 64, loss = 0.02019432
Iteration 65, loss = 0.02009850
Iteration 66, loss = 0.02013969
Iteration 67, loss = 0.01984224
Iteration 68, loss = 0.01991127
Iteration 69, loss = 0.02000166
Iteration 70, loss = 0.01996818
Iteration 71, loss = 0.01972560
Iteration 72, loss = 0.01942315
Iteration 73, loss = 0.01960009
Iteration 74, loss = 0.01963289
Iteration 75, loss = 0.01968672
Iteration 76, loss = 0.01944617
Iteration 77, loss = 0.01936869
Iteration 78, loss = 0.01948930
Iteration 79, loss = 0.01924818
Iteration 80, loss = 0.01919371
Iteration 81, loss = 0.01924796
Iteration 82, loss = 0.01922142
Iteration 83, loss = 0.01915509
Iteration 84, loss = 0.01944811
Iteration 85, loss = 0.01933698
Iteration 86, loss = 0.01923763
Iteration 87, loss = 0.01899075
Iteration 88, loss = 0.01910229
Iteration 89, loss = 0.01918712
Iteration 90, loss = 0.01922552
Iteration 91, loss = 0.01909278
Iteration 92, loss = 0.01880595
Iteration 93, loss = 0.01892584
Iteration 94, loss = 0.01875839
Iteration 95, loss = 0.01871783
Iteration 96, loss = 0.01867250
Iteration 97, loss = 0.01895159
Iteration 98, loss = 0.01909476
Iteration 99, loss = 0.01867109
Iteration 100, loss = 0.01860983
Iteration 101, loss = 0.01836049
Iteration 102, loss = 0.01839612
Iteration 103, loss = 0.01823570
Iteration 104, loss = 0.01837495
Iteration 105, loss = 0.01853095
Iteration 106, loss = 0.01856938
Iteration 107, loss = 0.01844627
Iteration 108, loss = 0.01823800
Iteration 109, loss = 0.01846810
Iteration 110, loss = 0.01851038
Iteration 111, loss = 0.01826300
Iteration 112, loss = 0.01831789
Iteration 113, loss = 0.01841844
Iteration 114, loss = 0.01824713
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70683009
Iteration 2, loss = 0.68332510
Iteration 3, loss = 0.65404913
Iteration 4, loss = 0.61746065
Iteration 5, loss = 0.57494191
Iteration 6, loss = 0.52770564
Iteration 7, loss = 0.47808363
Iteration 8, loss = 0.42831900
Iteration 9, loss = 0.38042361
Iteration 10, loss = 0.33572949
Iteration 11, loss = 0.29467782
Iteration 12, loss = 0.25826889
Iteration 13, loss = 0.22671290
Iteration 14, loss = 0.19947370
Iteration 15, loss = 0.17623717
Iteration 16, loss = 0.15658762
Iteration 17, loss = 0.13974895
Iteration 18, loss = 0.12548203
Iteration 19, loss = 0.11334758
Iteration 20, loss = 0.10289794
Iteration 21, loss = 0.09402533
Iteration 22, loss = 0.08633162
Iteration 23, loss = 0.07967309
Iteration 24, loss = 0.07400429
Iteration 25, loss = 0.06892356
Iteration 26, loss = 0.06438472
Iteration 27, loss = 0.06039500
Iteration 28, loss = 0.05678673
Iteration 29, loss = 0.05352523
Iteration 30, loss = 0.05078358
Iteration 31, loss = 0.04819444
Iteration 32, loss = 0.04594000
Iteration 33, loss = 0.04379069
Iteration 34, loss = 0.04200208
Iteration 35, loss = 0.04035592
Iteration 36, loss = 0.03878165
Iteration 37, loss = 0.03726928
Iteration 38, loss = 0.03598986
Iteration 39, loss = 0.03482639
Iteration 40, loss = 0.03367095
Iteration 41, loss = 0.03285533
Iteration 42, loss = 0.03176111
Iteration 43, loss = 0.03108075
Iteration 44, loss = 0.03014872
Iteration 45, loss = 0.02946729
Iteration 46, loss = 0.02860170
Iteration 47, loss = 0.02794721
Iteration 48, loss = 0.02730608
Iteration 49, loss = 0.02654024
Iteration 50, loss = 0.02605759
Iteration 51, loss = 0.02567256
Iteration 52, loss = 0.02523993
Iteration 53, loss = 0.02469678
Iteration 54, loss = 0.02422532
Iteration 55, loss = 0.02373093
Iteration 56, loss = 0.02340572
Iteration 57, loss = 0.02295720
Iteration 58, loss = 0.02289370
Iteration 59, loss = 0.02241601
Iteration 60, loss = 0.02212493
Iteration 61, loss = 0.02184486
Iteration 62, loss = 0.02155923
Iteration 63, loss = 0.02127268
Iteration 64, loss = 0.02088319
Iteration 65, loss = 0.02078417
Iteration 66, loss = 0.02068160
Iteration 67, loss = 0.02055226
Iteration 68, loss = 0.02027414
Iteration 69, loss = 0.02001575
Iteration 70, loss = 0.01976467
Iteration 71, loss = 0.01957895
Iteration 72, loss = 0.01931954
Iteration 73, loss = 0.01916817
Iteration 74, loss = 0.01896616
Iteration 75, loss = 0.01891882
Iteration 76, loss = 0.01869858
Iteration 77, loss = 0.01856894
Iteration 78, loss = 0.01832051
Iteration 79, loss = 0.01818992
Iteration 80, loss = 0.01810207
Iteration 81, loss = 0.01795454
Iteration 82, loss = 0.01782785
Iteration 83, loss = 0.01769224
Iteration 84, loss = 0.01775489
Iteration 85, loss = 0.01767685
Iteration 86, loss = 0.01762576
Iteration 87, loss = 0.01755807
Iteration 88, loss = 0.01737230
Iteration 89, loss = 0.01718781
Iteration 90, loss = 0.01703996
Iteration 91, loss = 0.01697401
Iteration 92, loss = 0.01694549
Iteration 93, loss = 0.01689567
Iteration 94, loss = 0.01676159
Iteration 95, loss = 0.01671061
Iteration 96, loss = 0.01666837
Iteration 97, loss = 0.01665868
Iteration 98, loss = 0.01659765
Iteration 99, loss = 0.01660686
Iteration 100, loss = 0.01706780
Iteration 101, loss = 0.01690324
Iteration 102, loss = 0.01660982
Iteration 103, loss = 0.01636166
Iteration 104, loss = 0.01632501
Iteration 105, loss = 0.01635606
Iteration 106, loss = 0.01632411
Iteration 107, loss = 0.01642946
Iteration 108, loss = 0.01632400
Iteration 109, loss = 0.01615943
Iteration 110, loss = 0.01598594
Iteration 111, loss = 0.01592703
Iteration 112, loss = 0.01585877
Iteration 113, loss = 0.01582286
Iteration 114, loss = 0.01564808
Iteration 115, loss = 0.01563096
Iteration 116, loss = 0.01559964
Iteration 117, loss = 0.01550043
Iteration 118, loss = 0.01550934
Iteration 119, loss = 0.01550239
Iteration 120, loss = 0.01551280
Iteration 121, loss = 0.01532906
Iteration 122, loss = 0.01534995
Iteration 123, loss = 0.01522894
Iteration 124, loss = 0.01535107
Iteration 125, loss = 0.01533566
Iteration 126, loss = 0.01531051
Iteration 127, loss = 0.01523678
Iteration 128, loss = 0.01533424
Iteration 129, loss = 0.01529395
Iteration 130, loss = 0.01522184
Iteration 131, loss = 0.01517197
Iteration 132, loss = 0.01513437
Iteration 133, loss = 0.01514433
Iteration 134, loss = 0.01548020
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68277486
Iteration 2, loss = 0.64504697
Iteration 3, loss = 0.58737285
Iteration 4, loss = 0.51178455
Iteration 5, loss = 0.43244215
Iteration 6, loss = 0.35761851
Iteration 7, loss = 0.29195750
Iteration 8, loss = 0.23755369
Iteration 9, loss = 0.19434179
Iteration 10, loss = 0.16103927
Iteration 11, loss = 0.13514122
Iteration 12, loss = 0.11502013
Iteration 13, loss = 0.09941732
Iteration 14, loss = 0.08712025
Iteration 15, loss = 0.07740439
Iteration 16, loss = 0.06941134
Iteration 17, loss = 0.06289928
Iteration 18, loss = 0.05748003
Iteration 19, loss = 0.05275948
Iteration 20, loss = 0.04906740
Iteration 21, loss = 0.04575914
Iteration 22, loss = 0.04294461
Iteration 23, loss = 0.04061039
Iteration 24, loss = 0.03842612
Iteration 25, loss = 0.03652059
Iteration 26, loss = 0.03494672
Iteration 27, loss = 0.03360384
Iteration 28, loss = 0.03223341
Iteration 29, loss = 0.03107615
Iteration 30, loss = 0.03008426
Iteration 31, loss = 0.02916212
Iteration 32, loss = 0.02820493
Iteration 33, loss = 0.02732465
Iteration 34, loss = 0.02676157
Iteration 35, loss = 0.02618575
Iteration 36, loss = 0.02551237
Iteration 37, loss = 0.02499260
Iteration 38, loss = 0.02451936
Iteration 39, loss = 0.02419903
Iteration 40, loss = 0.02396928
Iteration 41, loss = 0.02318723
Iteration 42, loss = 0.02332447
Iteration 43, loss = 0.02279961
Iteration 44, loss = 0.02244149
Iteration 45, loss = 0.02231597
Iteration 46, loss = 0.02187770
Iteration 47, loss = 0.02159403
Iteration 48, loss = 0.02130931
Iteration 49, loss = 0.02115667
Iteration 50, loss = 0.02107581
Iteration 51, loss = 0.02063709
Iteration 52, loss = 0.02056098
Iteration 53, loss = 0.02041269
Iteration 54, loss = 0.02009990
Iteration 55, loss = 0.02011242
Iteration 56, loss = 0.01996183
Iteration 57, loss = 0.01979612
Iteration 58, loss = 0.01958146
Iteration 59, loss = 0.01959949
Iteration 60, loss = 0.01951887
Iteration 61, loss = 0.01937927
Iteration 62, loss = 0.01923306
Iteration 63, loss = 0.01920732
Iteration 64, loss = 0.01909368
Iteration 65, loss = 0.01881670
Iteration 66, loss = 0.01893421
Iteration 67, loss = 0.01915669
Iteration 68, loss = 0.01874578
Iteration 69, loss = 0.01880616
Iteration 70, loss = 0.01877265
Iteration 71, loss = 0.01880021
Iteration 72, loss = 0.01868922
Iteration 73, loss = 0.01853305
Iteration 74, loss = 0.01826753
Iteration 75, loss = 0.01838511
Iteration 76, loss = 0.01822925
Iteration 77, loss = 0.01818541
Iteration 78, loss = 0.01823301
Iteration 79, loss = 0.01819009
Iteration 80, loss = 0.01813561
Iteration 81, loss = 0.01821551
Iteration 82, loss = 0.01806699
Iteration 83, loss = 0.01793182
Iteration 84, loss = 0.01793487
Iteration 85, loss = 0.01797107
Iteration 86, loss = 0.01809572
Iteration 87, loss = 0.01799709
Iteration 88, loss = 0.01775032
Iteration 89, loss = 0.01778664
Iteration 90, loss = 0.01781851
Iteration 91, loss = 0.01757967
Iteration 92, loss = 0.01787261
Iteration 93, loss = 0.01786808
Iteration 94, loss = 0.01762052
Iteration 95, loss = 0.01765212
Iteration 96, loss = 0.01755875
Iteration 97, loss = 0.01763541
Iteration 98, loss = 0.01746594
Iteration 99, loss = 0.01752293
Iteration 100, loss = 0.01745102
Iteration 101, loss = 0.01763304
Iteration 102, loss = 0.01761620
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68707602
Iteration 2, loss = 0.62959810
Iteration 3, loss = 0.54723900
Iteration 4, loss = 0.45060662
Iteration 5, loss = 0.35829458
Iteration 6, loss = 0.28026893
Iteration 7, loss = 0.21958414
Iteration 8, loss = 0.17464982
Iteration 9, loss = 0.14248243
Iteration 10, loss = 0.11842793
Iteration 11, loss = 0.10045201
Iteration 12, loss = 0.08695760
Iteration 13, loss = 0.07647905
Iteration 14, loss = 0.06818848
Iteration 15, loss = 0.06161915
Iteration 16, loss = 0.05649487
Iteration 17, loss = 0.05192469
Iteration 18, loss = 0.04835086
Iteration 19, loss = 0.04526454
Iteration 20, loss = 0.04288207
Iteration 21, loss = 0.04051022
Iteration 22, loss = 0.03842975
Iteration 23, loss = 0.03698951
Iteration 24, loss = 0.03575874
Iteration 25, loss = 0.03422305
Iteration 26, loss = 0.03322605
Iteration 27, loss = 0.03222452
Iteration 28, loss = 0.03112728
Iteration 29, loss = 0.03050610
Iteration 30, loss = 0.02967011
Iteration 31, loss = 0.02888918
Iteration 32, loss = 0.02842666
Iteration 33, loss = 0.02784231
Iteration 34, loss = 0.02751043
Iteration 35, loss = 0.02682346
Iteration 36, loss = 0.02649064
Iteration 37, loss = 0.02628589
Iteration 38, loss = 0.02596180
Iteration 39, loss = 0.02542121
Iteration 40, loss = 0.02517572
Iteration 41, loss = 0.02491944
Iteration 42, loss = 0.02449400
Iteration 43, loss = 0.02454625
Iteration 44, loss = 0.02424149
Iteration 45, loss = 0.02425390
Iteration 46, loss = 0.02374821
Iteration 47, loss = 0.02364338
Iteration 48, loss = 0.02339316
Iteration 49, loss = 0.02347937
Iteration 50, loss = 0.02304391
Iteration 51, loss = 0.02316790
Iteration 52, loss = 0.02283930
Iteration 53, loss = 0.02275069
Iteration 54, loss = 0.02264205
Iteration 55, loss = 0.02262744
Iteration 56, loss = 0.02262011
Iteration 57, loss = 0.02248580
Iteration 58, loss = 0.02225793
Iteration 59, loss = 0.02229900
Iteration 60, loss = 0.02208513
Iteration 61, loss = 0.02208043
Iteration 62, loss = 0.02185270
Iteration 63, loss = 0.02365149
Iteration 64, loss = 0.02345038
Iteration 65, loss = 0.02280964
Iteration 66, loss = 0.02269065
Iteration 67, loss = 0.02236830
Iteration 68, loss = 0.02240671
Iteration 69, loss = 0.02204162
Iteration 70, loss = 0.02195747
Iteration 71, loss = 0.02213453
Iteration 72, loss = 0.02181822
Iteration 73, loss = 0.02200499
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67723672
Iteration 2, loss = 0.64904557
Iteration 3, loss = 0.61138319
Iteration 4, loss = 0.56234118
Iteration 5, loss = 0.50540104
Iteration 6, loss = 0.44584493
Iteration 7, loss = 0.38855116
Iteration 8, loss = 0.33502444
Iteration 9, loss = 0.28656207
Iteration 10, loss = 0.24484925
Iteration 11, loss = 0.20910236
Iteration 12, loss = 0.17916753
Iteration 13, loss = 0.15423609
Iteration 14, loss = 0.13392432
Iteration 15, loss = 0.11702817
Iteration 16, loss = 0.10310489
Iteration 17, loss = 0.09163186
Iteration 18, loss = 0.08203159
Iteration 19, loss = 0.07378016
Iteration 20, loss = 0.06704271
Iteration 21, loss = 0.06121782
Iteration 22, loss = 0.05616963
Iteration 23, loss = 0.05189025
Iteration 24, loss = 0.04814809
Iteration 25, loss = 0.04486541
Iteration 26, loss = 0.04203752
Iteration 27, loss = 0.03956573
Iteration 28, loss = 0.03733484
Iteration 29, loss = 0.03526403
Iteration 30, loss = 0.03347137
Iteration 31, loss = 0.03190479
Iteration 32, loss = 0.03043687
Iteration 33, loss = 0.02915039
Iteration 34, loss = 0.02798227
Iteration 35, loss = 0.02687303
Iteration 36, loss = 0.02589439
Iteration 37, loss = 0.02502802
Iteration 38, loss = 0.02435256
Iteration 39, loss = 0.02350086
Iteration 40, loss = 0.02278644
Iteration 41, loss = 0.02217853
Iteration 42, loss = 0.02174330
Iteration 43, loss = 0.02102353
Iteration 44, loss = 0.02040241
Iteration 45, loss = 0.02007995
Iteration 46, loss = 0.01973797
Iteration 47, loss = 0.01930229
Iteration 48, loss = 0.01884828
Iteration 49, loss = 0.01850505
Iteration 50, loss = 0.01807305
Iteration 51, loss = 0.01782831
Iteration 52, loss = 0.01757830
Iteration 53, loss = 0.01722248
Iteration 54, loss = 0.01694741
Iteration 55, loss = 0.01674182
Iteration 56, loss = 0.01648351
Iteration 57, loss = 0.01640867
Iteration 58, loss = 0.01608007
Iteration 59, loss = 0.01602303
Iteration 60, loss = 0.01576382
Iteration 61, loss = 0.01555951
Iteration 62, loss = 0.01547601
Iteration 63, loss = 0.01522555
Iteration 64, loss = 0.01504451
Iteration 65, loss = 0.01492504
Iteration 66, loss = 0.01479799
Iteration 67, loss = 0.01474796
Iteration 68, loss = 0.01458084
Iteration 69, loss = 0.01444062
Iteration 70, loss = 0.01435572
Iteration 71, loss = 0.01422059
Iteration 72, loss = 0.01414183
Iteration 73, loss = 0.01403560
Iteration 74, loss = 0.01391034
Iteration 75, loss = 0.01387867
Iteration 76, loss = 0.01375688
Iteration 77, loss = 0.01359571
Iteration 78, loss = 0.01355986
Iteration 79, loss = 0.01346374
Iteration 80, loss = 0.01344328
Iteration 81, loss = 0.01330213
Iteration 82, loss = 0.01335688
Iteration 83, loss = 0.01319057
Iteration 84, loss = 0.01316738
Iteration 85, loss = 0.01310394
Iteration 86, loss = 0.01307337
Iteration 87, loss = 0.01293957
Iteration 88, loss = 0.01301119
Iteration 89, loss = 0.01293448
Iteration 90, loss = 0.01283273
Iteration 91, loss = 0.01275899
Iteration 92, loss = 0.01274807
Iteration 93, loss = 0.01282882
Iteration 94, loss = 0.01255757
Iteration 95, loss = 0.01261220
Iteration 96, loss = 0.01248352
Iteration 97, loss = 0.01246648
Iteration 98, loss = 0.01249135
Iteration 99, loss = 0.01250664
Iteration 100, loss = 0.01249237
Iteration 101, loss = 0.01254529
Iteration 102, loss = 0.01237177
Iteration 103, loss = 0.01227863
Iteration 104, loss = 0.01226568
Iteration 105, loss = 0.01234766
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67442093
Iteration 2, loss = 0.65826635
Iteration 3, loss = 0.64184028
Iteration 4, loss = 0.62249235
Iteration 5, loss = 0.59905298
Iteration 6, loss = 0.57120596
Iteration 7, loss = 0.53969592
Iteration 8, loss = 0.50557835
Iteration 9, loss = 0.47004308
Iteration 10, loss = 0.43425559
Iteration 11, loss = 0.39907957
Iteration 12, loss = 0.36570120
Iteration 13, loss = 0.33394882
Iteration 14, loss = 0.30434212
Iteration 15, loss = 0.27697230
Iteration 16, loss = 0.25213963
Iteration 17, loss = 0.22939486
Iteration 18, loss = 0.20889069
Iteration 19, loss = 0.19060472
Iteration 20, loss = 0.17409615
Iteration 21, loss = 0.15950464
Iteration 22, loss = 0.14635454
Iteration 23, loss = 0.13478690
Iteration 24, loss = 0.12434414
Iteration 25, loss = 0.11520076
Iteration 26, loss = 0.10694106
Iteration 27, loss = 0.09957313
Iteration 28, loss = 0.09303648
Iteration 29, loss = 0.08708986
Iteration 30, loss = 0.08173097
Iteration 31, loss = 0.07694350
Iteration 32, loss = 0.07261569
Iteration 33, loss = 0.06866383
Iteration 34, loss = 0.06509063
Iteration 35, loss = 0.06187034
Iteration 36, loss = 0.05882783
Iteration 37, loss = 0.05616861
Iteration 38, loss = 0.05360893
Iteration 39, loss = 0.05130890
Iteration 40, loss = 0.04919420
Iteration 41, loss = 0.04724613
Iteration 42, loss = 0.04546717
Iteration 43, loss = 0.04377733
Iteration 44, loss = 0.04221576
Iteration 45, loss = 0.04078400
Iteration 46, loss = 0.03942184
Iteration 47, loss = 0.03817488
Iteration 48, loss = 0.03706906
Iteration 49, loss = 0.03593917
Iteration 50, loss = 0.03491564
Iteration 51, loss = 0.03394708
Iteration 52, loss = 0.03306813
Iteration 53, loss = 0.03226125
Iteration 54, loss = 0.03139796
Iteration 55, loss = 0.03063297
Iteration 56, loss = 0.02996122
Iteration 57, loss = 0.02928249
Iteration 58, loss = 0.02862053
Iteration 59, loss = 0.02804093
Iteration 60, loss = 0.02743322
Iteration 61, loss = 0.02691643
Iteration 62, loss = 0.02642182
Iteration 63, loss = 0.02590187
Iteration 64, loss = 0.02545307
Iteration 65, loss = 0.02502061
Iteration 66, loss = 0.02458244
Iteration 67, loss = 0.02422091
Iteration 68, loss = 0.02379049
Iteration 69, loss = 0.02345080
Iteration 70, loss = 0.02309960
Iteration 71, loss = 0.02275343
Iteration 72, loss = 0.02242961
Iteration 73, loss = 0.02213618
Iteration 74, loss = 0.02186547
Iteration 75, loss = 0.02156434
Iteration 76, loss = 0.02136774
Iteration 77, loss = 0.02105988
Iteration 78, loss = 0.02081370
Iteration 79, loss = 0.02057847
Iteration 80, loss = 0.02036849
Iteration 81, loss = 0.02013800
Iteration 82, loss = 0.01993774
Iteration 83, loss = 0.01974943
Iteration 84, loss = 0.01952727
Iteration 85, loss = 0.01935682
Iteration 86, loss = 0.01920208
Iteration 87, loss = 0.01906700
Iteration 88, loss = 0.01885450
Iteration 89, loss = 0.01865302
Iteration 90, loss = 0.01849745
Iteration 91, loss = 0.01835576
Iteration 92, loss = 0.01824315
Iteration 93, loss = 0.01807325
Iteration 94, loss = 0.01791295
Iteration 95, loss = 0.01783132
Iteration 96, loss = 0.01770703
Iteration 97, loss = 0.01756175
Iteration 98, loss = 0.01743701
Iteration 99, loss = 0.01733832
Iteration 100, loss = 0.01720941
Iteration 101, loss = 0.01708603
Iteration 102, loss = 0.01699491
Iteration 103, loss = 0.01687121
Iteration 104, loss = 0.01676630
Iteration 105, loss = 0.01670703
Iteration 106, loss = 0.01661940
Iteration 107, loss = 0.01650773
Iteration 108, loss = 0.01640926
Iteration 109, loss = 0.01633844
Iteration 110, loss = 0.01625448
Iteration 111, loss = 0.01617346
Iteration 112, loss = 0.01610320
Iteration 113, loss = 0.01607079
Iteration 114, loss = 0.01593875
Iteration 115, loss = 0.01586821
Iteration 116, loss = 0.01583012
Iteration 117, loss = 0.01576951
Iteration 118, loss = 0.01568804
Iteration 119, loss = 0.01558034
Iteration 120, loss = 0.01560587
Iteration 121, loss = 0.01550382
Iteration 122, loss = 0.01541887
Iteration 123, loss = 0.01536971
Iteration 124, loss = 0.01530380
Iteration 125, loss = 0.01525901
Iteration 126, loss = 0.01520528
Iteration 127, loss = 0.01515281
Iteration 128, loss = 0.01510889
Iteration 129, loss = 0.01503220
Iteration 130, loss = 0.01502934
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70682151
Iteration 2, loss = 0.67783989
Iteration 3, loss = 0.63493358
Iteration 4, loss = 0.57610005
Iteration 5, loss = 0.50692624
Iteration 6, loss = 0.43405313
Iteration 7, loss = 0.36476565
Iteration 8, loss = 0.30412675
Iteration 9, loss = 0.25270613
Iteration 10, loss = 0.21092996
Iteration 11, loss = 0.17727563
Iteration 12, loss = 0.15044231
Iteration 13, loss = 0.12925186
Iteration 14, loss = 0.11217842
Iteration 15, loss = 0.09848644
Iteration 16, loss = 0.08743902
Iteration 17, loss = 0.07832187
Iteration 18, loss = 0.07078217
Iteration 19, loss = 0.06443009
Iteration 20, loss = 0.05893212
Iteration 21, loss = 0.05441982
Iteration 22, loss = 0.05054587
Iteration 23, loss = 0.04717236
Iteration 24, loss = 0.04418011
Iteration 25, loss = 0.04166420
Iteration 26, loss = 0.03939108
Iteration 27, loss = 0.03729847
Iteration 28, loss = 0.03563087
Iteration 29, loss = 0.03393902
Iteration 30, loss = 0.03257952
Iteration 31, loss = 0.03135533
Iteration 32, loss = 0.03012702
Iteration 33, loss = 0.02910733
Iteration 34, loss = 0.02815716
Iteration 35, loss = 0.02726856
Iteration 36, loss = 0.02659617
Iteration 37, loss = 0.02582248
Iteration 38, loss = 0.02493625
Iteration 39, loss = 0.02436636
Iteration 40, loss = 0.02394048
Iteration 41, loss = 0.02333311
Iteration 42, loss = 0.02300816
Iteration 43, loss = 0.02245537
Iteration 44, loss = 0.02197123
Iteration 45, loss = 0.02167819
Iteration 46, loss = 0.02129617
Iteration 47, loss = 0.02081984
Iteration 48, loss = 0.02054446
Iteration 49, loss = 0.02034820
Iteration 50, loss = 0.02005433
Iteration 51, loss = 0.01977508
Iteration 52, loss = 0.01950504
Iteration 53, loss = 0.01931495
Iteration 54, loss = 0.01912177
Iteration 55, loss = 0.01902549
Iteration 56, loss = 0.01867931
Iteration 57, loss = 0.01856294
Iteration 58, loss = 0.01838422
Iteration 59, loss = 0.01805590
Iteration 60, loss = 0.01792212
Iteration 61, loss = 0.01781292
Iteration 62, loss = 0.01778138
Iteration 63, loss = 0.01756228
Iteration 64, loss = 0.01738514
Iteration 65, loss = 0.01722922
Iteration 66, loss = 0.01720372
Iteration 67, loss = 0.01717826
Iteration 68, loss = 0.01702863
Iteration 69, loss = 0.01689577
Iteration 70, loss = 0.01692655
Iteration 71, loss = 0.01681361
Iteration 72, loss = 0.01662900
Iteration 73, loss = 0.01650466
Iteration 74, loss = 0.01648092
Iteration 75, loss = 0.01626685
Iteration 76, loss = 0.01626045
Iteration 77, loss = 0.01618502
Iteration 78, loss = 0.01628595
Iteration 79, loss = 0.01600152
Iteration 80, loss = 0.01606097
Iteration 81, loss = 0.01591706
Iteration 82, loss = 0.01581642
Iteration 83, loss = 0.01587760
Iteration 84, loss = 0.01577364
Iteration 85, loss = 0.01566864
Iteration 86, loss = 0.01565203
Iteration 87, loss = 0.01562469
Iteration 88, loss = 0.01573521
Iteration 89, loss = 0.01548674
Iteration 90, loss = 0.01553029
Iteration 91, loss = 0.01545563
Iteration 92, loss = 0.01547844
Iteration 93, loss = 0.01535436
Iteration 94, loss = 0.01534501
Iteration 95, loss = 0.01533053
Iteration 96, loss = 0.01532573
Iteration 97, loss = 0.01533624
Iteration 98, loss = 0.01514539
Iteration 99, loss = 0.01517664
Iteration 100, loss = 0.01508566
Iteration 101, loss = 0.01506035
Iteration 102, loss = 0.01507628
Iteration 103, loss = 0.01510234
Iteration 104, loss = 0.01504099
Iteration 105, loss = 0.01505356
Iteration 106, loss = 0.01491005
Iteration 107, loss = 0.01488517
Iteration 108, loss = 0.01490875
Iteration 109, loss = 0.01490604
Iteration 110, loss = 0.01482646
Iteration 111, loss = 0.01489025
Iteration 112, loss = 0.01484822
Iteration 113, loss = 0.01481893
Iteration 114, loss = 0.01478542
Iteration 115, loss = 0.01494855
Iteration 116, loss = 0.01474134
Iteration 117, loss = 0.01479698
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67967804
Iteration 2, loss = 0.63702015
Iteration 3, loss = 0.56496022
Iteration 4, loss = 0.47178538
Iteration 5, loss = 0.38209757
Iteration 6, loss = 0.30438134
Iteration 7, loss = 0.24183790
Iteration 8, loss = 0.19391551
Iteration 9, loss = 0.15809339
Iteration 10, loss = 0.13106550
Iteration 11, loss = 0.11089334
Iteration 12, loss = 0.09564018
Iteration 13, loss = 0.08390033
Iteration 14, loss = 0.07457563
Iteration 15, loss = 0.06709716
Iteration 16, loss = 0.06103503
Iteration 17, loss = 0.05615229
Iteration 18, loss = 0.05215245
Iteration 19, loss = 0.04854318
Iteration 20, loss = 0.04580162
Iteration 21, loss = 0.04300506
Iteration 22, loss = 0.04099453
Iteration 23, loss = 0.03902993
Iteration 24, loss = 0.03760036
Iteration 25, loss = 0.03639859
Iteration 26, loss = 0.03475686
Iteration 27, loss = 0.03380520
Iteration 28, loss = 0.03289424
Iteration 29, loss = 0.03204100
Iteration 30, loss = 0.03119320
Iteration 31, loss = 0.03033564
Iteration 32, loss = 0.02978965
Iteration 33, loss = 0.02924973
Iteration 34, loss = 0.02854115
Iteration 35, loss = 0.02805738
Iteration 36, loss = 0.02786498
Iteration 37, loss = 0.02757708
Iteration 38, loss = 0.02696664
Iteration 39, loss = 0.02649468
Iteration 40, loss = 0.02632462
Iteration 41, loss = 0.02608749
Iteration 42, loss = 0.02571237
Iteration 43, loss = 0.02524587
Iteration 44, loss = 0.02531342
Iteration 45, loss = 0.02508954
Iteration 46, loss = 0.02473101
Iteration 47, loss = 0.02448813
Iteration 48, loss = 0.02444294
Iteration 49, loss = 0.02425825
Iteration 50, loss = 0.02417838
Iteration 51, loss = 0.02398213
Iteration 52, loss = 0.02397664
Iteration 53, loss = 0.02360202
Iteration 54, loss = 0.02359149
Iteration 55, loss = 0.02356282
Iteration 56, loss = 0.02338877
Iteration 57, loss = 0.02325866
Iteration 58, loss = 0.02323131
Iteration 59, loss = 0.02329260
Iteration 60, loss = 0.02315415
Iteration 61, loss = 0.02297024
Iteration 62, loss = 0.02297609
Iteration 63, loss = 0.02287772
Iteration 64, loss = 0.02270068
Iteration 65, loss = 0.02283183
Iteration 66, loss = 0.02274944
Iteration 67, loss = 0.02266990
Iteration 68, loss = 0.02250979
Iteration 69, loss = 0.02234500
Iteration 70, loss = 0.02228311
Iteration 71, loss = 0.02220967
Iteration 72, loss = 0.02276792
Iteration 73, loss = 0.02194787
Iteration 74, loss = 0.02217633
Iteration 75, loss = 0.02218601
Iteration 76, loss = 0.02205234
Iteration 77, loss = 0.02209875
Iteration 78, loss = 0.02207180
Iteration 79, loss = 0.02199895
Iteration 80, loss = 0.02214843
Iteration 81, loss = 0.02203989
Iteration 82, loss = 0.02198254
Iteration 83, loss = 0.02174047
Iteration 84, loss = 0.02191965
Iteration 85, loss = 0.02162633
Iteration 86, loss = 0.02179338
Iteration 87, loss = 0.02193128
Iteration 88, loss = 0.02144267
Iteration 89, loss = 0.02170900
Iteration 90, loss = 0.02176062
Iteration 91, loss = 0.02173117
Iteration 92, loss = 0.02159351
Iteration 93, loss = 0.02175543
Iteration 94, loss = 0.02164488
Iteration 95, loss = 0.02152466
Iteration 96, loss = 0.02133031
Iteration 97, loss = 0.02145302
Iteration 98, loss = 0.02156856
Iteration 99, loss = 0.02144974
Iteration 100, loss = 0.02138493
Iteration 101, loss = 0.02134485
Iteration 102, loss = 0.02140747
Iteration 103, loss = 0.02157948
Iteration 104, loss = 0.02115727
Iteration 105, loss = 0.02151855
Iteration 106, loss = 0.02133089
Iteration 107, loss = 0.02140794
Iteration 108, loss = 0.02144894
Iteration 109, loss = 0.02123258
Iteration 110, loss = 0.02137359
Iteration 111, loss = 0.02130756
Iteration 112, loss = 0.02146096
Iteration 113, loss = 0.02146546
Iteration 114, loss = 0.02144476
Iteration 115, loss = 0.02106856
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68825522
Iteration 2, loss = 0.63038654
Iteration 3, loss = 0.55168462
Iteration 4, loss = 0.45895080
Iteration 5, loss = 0.37201688
Iteration 6, loss = 0.29698452
Iteration 7, loss = 0.23657816
Iteration 8, loss = 0.19024187
Iteration 9, loss = 0.15526642
Iteration 10, loss = 0.12906273
Iteration 11, loss = 0.10907266
Iteration 12, loss = 0.09389218
Iteration 13, loss = 0.08220262
Iteration 14, loss = 0.07291397
Iteration 15, loss = 0.06530763
Iteration 16, loss = 0.05931967
Iteration 17, loss = 0.05447950
Iteration 18, loss = 0.05046177
Iteration 19, loss = 0.04690919
Iteration 20, loss = 0.04391678
Iteration 21, loss = 0.04144718
Iteration 22, loss = 0.03916187
Iteration 23, loss = 0.03726620
Iteration 24, loss = 0.03585993
Iteration 25, loss = 0.03418619
Iteration 26, loss = 0.03313351
Iteration 27, loss = 0.03166285
Iteration 28, loss = 0.03098723
Iteration 29, loss = 0.03012987
Iteration 30, loss = 0.02921553
Iteration 31, loss = 0.02854080
Iteration 32, loss = 0.02786078
Iteration 33, loss = 0.02736234
Iteration 34, loss = 0.02661604
Iteration 35, loss = 0.02621188
Iteration 36, loss = 0.02592795
Iteration 37, loss = 0.02536433
Iteration 38, loss = 0.02513914
Iteration 39, loss = 0.02473462
Iteration 40, loss = 0.02421246
Iteration 41, loss = 0.02383679
Iteration 42, loss = 0.02372494
Iteration 43, loss = 0.02361677
Iteration 44, loss = 0.02294392
Iteration 45, loss = 0.02313234
Iteration 46, loss = 0.02310047
Iteration 47, loss = 0.02277865
Iteration 48, loss = 0.02256340
Iteration 49, loss = 0.02226269
Iteration 50, loss = 0.02243049
Iteration 51, loss = 0.02197397
Iteration 52, loss = 0.02164328
Iteration 53, loss = 0.02186051
Iteration 54, loss = 0.02202083
Iteration 55, loss = 0.02154505
Iteration 56, loss = 0.02145322
Iteration 57, loss = 0.02111554
Iteration 58, loss = 0.02111808
Iteration 59, loss = 0.02103316
Iteration 60, loss = 0.02097907
Iteration 61, loss = 0.02088152
Iteration 62, loss = 0.02076869
Iteration 63, loss = 0.02078315
Iteration 64, loss = 0.02064938
Iteration 65, loss = 0.02069308
Iteration 66, loss = 0.02045610
Iteration 67, loss = 0.02077965
Iteration 68, loss = 0.02040350
Iteration 69, loss = 0.02034271
Iteration 70, loss = 0.02011423
Iteration 71, loss = 0.02028445
Iteration 72, loss = 0.02008352
Iteration 73, loss = 0.02009257
Iteration 74, loss = 0.02023638
Iteration 75, loss = 0.01983976
Iteration 76, loss = 0.01989879
Iteration 77, loss = 0.01986407
Iteration 78, loss = 0.01993724
Iteration 79, loss = 0.01984979
Iteration 80, loss = 0.01972889
Iteration 81, loss = 0.01973272
Iteration 82, loss = 0.01977040
Iteration 83, loss = 0.01970947
Iteration 84, loss = 0.01966856
Iteration 85, loss = 0.01960745
Iteration 86, loss = 0.01976244
Iteration 87, loss = 0.01968785
Iteration 88, loss = 0.01970852
Iteration 89, loss = 0.01960673
Iteration 90, loss = 0.01948196
Iteration 91, loss = 0.01953765
Iteration 92, loss = 0.01949321
Iteration 93, loss = 0.01953563
Iteration 94, loss = 0.01931864
Iteration 95, loss = 0.01947610
Iteration 96, loss = 0.01960133
Iteration 97, loss = 0.01941516
Iteration 98, loss = 0.01955037
Iteration 99, loss = 0.01956213
Iteration 100, loss = 0.01921780
Iteration 101, loss = 0.01927801
Iteration 102, loss = 0.01940801
Iteration 103, loss = 0.01936045
Iteration 104, loss = 0.01943899
Iteration 105, loss = 0.01925999
Iteration 106, loss = 0.01909292
Iteration 107, loss = 0.01916326
Iteration 108, loss = 0.01948124
Iteration 109, loss = 0.01925782
Iteration 110, loss = 0.01943480
Iteration 111, loss = 0.01920941
Iteration 112, loss = 0.01925244
Iteration 113, loss = 0.01941334
Iteration 114, loss = 0.01906145
Iteration 115, loss = 0.01926155
Iteration 116, loss = 0.01927405
Iteration 117, loss = 0.01905828
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70696373
Iteration 2, loss = 0.63550580
Iteration 3, loss = 0.53643479
Iteration 4, loss = 0.42314266
Iteration 5, loss = 0.32441214
Iteration 6, loss = 0.24706954
Iteration 7, loss = 0.18986137
Iteration 8, loss = 0.14973409
Iteration 9, loss = 0.12132033
Iteration 10, loss = 0.10114551
Iteration 11, loss = 0.08707256
Iteration 12, loss = 0.07539678
Iteration 13, loss = 0.06696783
Iteration 14, loss = 0.06053978
Iteration 15, loss = 0.05500586
Iteration 16, loss = 0.05078195
Iteration 17, loss = 0.04713524
Iteration 18, loss = 0.04415197
Iteration 19, loss = 0.04198251
Iteration 20, loss = 0.03963340
Iteration 21, loss = 0.03824400
Iteration 22, loss = 0.03661629
Iteration 23, loss = 0.03510650
Iteration 24, loss = 0.03400712
Iteration 25, loss = 0.03305221
Iteration 26, loss = 0.03197197
Iteration 27, loss = 0.03160016
Iteration 28, loss = 0.03076825
Iteration 29, loss = 0.03006184
Iteration 30, loss = 0.02977294
Iteration 31, loss = 0.02925889
Iteration 32, loss = 0.02889748
Iteration 33, loss = 0.02803431
Iteration 34, loss = 0.02782898
Iteration 35, loss = 0.02761506
Iteration 36, loss = 0.02727309
Iteration 37, loss = 0.02686245
Iteration 38, loss = 0.02663923
Iteration 39, loss = 0.02617650
Iteration 40, loss = 0.02627715
Iteration 41, loss = 0.02616226
Iteration 42, loss = 0.02580305
Iteration 43, loss = 0.02554586
Iteration 44, loss = 0.02530456
Iteration 45, loss = 0.02535674
Iteration 46, loss = 0.02547690
Iteration 47, loss = 0.02495881
Iteration 48, loss = 0.02502417
Iteration 49, loss = 0.02476748
Iteration 50, loss = 0.02490675
Iteration 51, loss = 0.02466371
Iteration 52, loss = 0.02430890
Iteration 53, loss = 0.02425646
Iteration 54, loss = 0.02426417
Iteration 55, loss = 0.02429153
Iteration 56, loss = 0.02414821
Iteration 57, loss = 0.02429137
Iteration 58, loss = 0.02380596
Iteration 59, loss = 0.02434895
Iteration 60, loss = 0.02380900
Iteration 61, loss = 0.02394849
Iteration 62, loss = 0.02393325
Iteration 63, loss = 0.02375794
Iteration 64, loss = 0.02368419
Iteration 65, loss = 0.02370575
Iteration 66, loss = 0.02338647
Iteration 67, loss = 0.02334898
Iteration 68, loss = 0.02333247
Iteration 69, loss = 0.02332868
Iteration 70, loss = 0.02320155
Iteration 71, loss = 0.02341654
Iteration 72, loss = 0.02310483
Iteration 73, loss = 0.02327381
Iteration 74, loss = 0.02338828
Iteration 75, loss = 0.02315487
Iteration 76, loss = 0.02342349
Iteration 77, loss = 0.02325367
Iteration 78, loss = 0.02311228
Iteration 79, loss = 0.02314273
Iteration 80, loss = 0.02318633
Iteration 81, loss = 0.02309960
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67982930
Iteration 2, loss = 0.65729266
Iteration 3, loss = 0.63838226
Iteration 4, loss = 0.62006238
Iteration 5, loss = 0.60186591
Iteration 6, loss = 0.58326435
Iteration 7, loss = 0.56373333
Iteration 8, loss = 0.54351563
Iteration 9, loss = 0.52239211
Iteration 10, loss = 0.50067603
Iteration 11, loss = 0.47822029
Iteration 12, loss = 0.45535141
Iteration 13, loss = 0.43208555
Iteration 14, loss = 0.40868006
Iteration 15, loss = 0.38553670
Iteration 16, loss = 0.36277343
Iteration 17, loss = 0.34091398
Iteration 18, loss = 0.31970405
Iteration 19, loss = 0.29946258
Iteration 20, loss = 0.28035361
Iteration 21, loss = 0.26223363
Iteration 22, loss = 0.24530979
Iteration 23, loss = 0.22943109
Iteration 24, loss = 0.21447612
Iteration 25, loss = 0.20063095
Iteration 26, loss = 0.18784570
Iteration 27, loss = 0.17592273
Iteration 28, loss = 0.16486535
Iteration 29, loss = 0.15467994
Iteration 30, loss = 0.14525491
Iteration 31, loss = 0.13651117
Iteration 32, loss = 0.12845458
Iteration 33, loss = 0.12111635
Iteration 34, loss = 0.11418312
Iteration 35, loss = 0.10784375
Iteration 36, loss = 0.10202831
Iteration 37, loss = 0.09662212
Iteration 38, loss = 0.09156647
Iteration 39, loss = 0.08698093
Iteration 40, loss = 0.08265375
Iteration 41, loss = 0.07867217
Iteration 42, loss = 0.07493216
Iteration 43, loss = 0.07149269
Iteration 44, loss = 0.06826920
Iteration 45, loss = 0.06530081
Iteration 46, loss = 0.06247896
Iteration 47, loss = 0.05985713
Iteration 48, loss = 0.05741792
Iteration 49, loss = 0.05508475
Iteration 50, loss = 0.05295470
Iteration 51, loss = 0.05092081
Iteration 52, loss = 0.04899496
Iteration 53, loss = 0.04720009
Iteration 54, loss = 0.04551458
Iteration 55, loss = 0.04392791
Iteration 56, loss = 0.04238481
Iteration 57, loss = 0.04095561
Iteration 58, loss = 0.03964845
Iteration 59, loss = 0.03831506
Iteration 60, loss = 0.03709552
Iteration 61, loss = 0.03597764
Iteration 62, loss = 0.03486091
Iteration 63, loss = 0.03383106
Iteration 64, loss = 0.03285581
Iteration 65, loss = 0.03190618
Iteration 66, loss = 0.03101366
Iteration 67, loss = 0.03015824
Iteration 68, loss = 0.02934654
Iteration 69, loss = 0.02855989
Iteration 70, loss = 0.02781807
Iteration 71, loss = 0.02712110
Iteration 72, loss = 0.02642955
Iteration 73, loss = 0.02577837
Iteration 74, loss = 0.02516437
Iteration 75, loss = 0.02454146
Iteration 76, loss = 0.02397592
Iteration 77, loss = 0.02343802
Iteration 78, loss = 0.02291776
Iteration 79, loss = 0.02238643
Iteration 80, loss = 0.02191418
Iteration 81, loss = 0.02145775
Iteration 82, loss = 0.02097879
Iteration 83, loss = 0.02056820
Iteration 84, loss = 0.02016223
Iteration 85, loss = 0.01973927
Iteration 86, loss = 0.01937922
Iteration 87, loss = 0.01898178
Iteration 88, loss = 0.01861235
Iteration 89, loss = 0.01827008
Iteration 90, loss = 0.01793718
Iteration 91, loss = 0.01760408
Iteration 92, loss = 0.01728299
Iteration 93, loss = 0.01701205
Iteration 94, loss = 0.01672467
Iteration 95, loss = 0.01645968
Iteration 96, loss = 0.01619452
Iteration 97, loss = 0.01594580
Iteration 98, loss = 0.01569630
Iteration 99, loss = 0.01541112
Iteration 100, loss = 0.01524246
Iteration 101, loss = 0.01495041
Iteration 102, loss = 0.01469829
Iteration 103, loss = 0.01448185
Iteration 104, loss = 0.01429507
Iteration 105, loss = 0.01407360
Iteration 106, loss = 0.01386844
Iteration 107, loss = 0.01366686
Iteration 108, loss = 0.01351987
Iteration 109, loss = 0.01332021
Iteration 110, loss = 0.01312349
Iteration 111, loss = 0.01295086
Iteration 112, loss = 0.01277302
Iteration 113, loss = 0.01260878
Iteration 114, loss = 0.01244701
Iteration 115, loss = 0.01234292
Iteration 116, loss = 0.01215702
Iteration 117, loss = 0.01198736
Iteration 118, loss = 0.01188372
Iteration 119, loss = 0.01172260
Iteration 120, loss = 0.01159329
Iteration 121, loss = 0.01147004
Iteration 122, loss = 0.01134364
Iteration 123, loss = 0.01121709
Iteration 124, loss = 0.01110344
Iteration 125, loss = 0.01099528
Iteration 126, loss = 0.01091740
Iteration 127, loss = 0.01078441
Iteration 128, loss = 0.01066341
Iteration 129, loss = 0.01053386
Iteration 130, loss = 0.01040197
Iteration 131, loss = 0.01030401
Iteration 132, loss = 0.01018525
Iteration 133, loss = 0.01010395
Iteration 134, loss = 0.00999085
Iteration 135, loss = 0.00989183
Iteration 136, loss = 0.00980406
Iteration 137, loss = 0.00971239
Iteration 138, loss = 0.00962396
Iteration 139, loss = 0.00953133
Iteration 140, loss = 0.00946437
Iteration 141, loss = 0.00936191
Iteration 142, loss = 0.00926335
Iteration 143, loss = 0.00919347
Iteration 144, loss = 0.00916884
Iteration 145, loss = 0.00911472
Iteration 146, loss = 0.00902010
Iteration 147, loss = 0.00892757
Iteration 148, loss = 0.00885253
Iteration 149, loss = 0.00877681
Iteration 150, loss = 0.00870836
Iteration 151, loss = 0.00863534
Iteration 152, loss = 0.00858096
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67622048
Iteration 2, loss = 0.65935750
Iteration 3, loss = 0.64212948
Iteration 4, loss = 0.62175784
Iteration 5, loss = 0.59720926
Iteration 6, loss = 0.56910064
Iteration 7, loss = 0.53799687
Iteration 8, loss = 0.50466634
Iteration 9, loss = 0.47072741
Iteration 10, loss = 0.43681282
Iteration 11, loss = 0.40362503
Iteration 12, loss = 0.37156003
Iteration 13, loss = 0.34097566
Iteration 14, loss = 0.31226409
Iteration 15, loss = 0.28548902
Iteration 16, loss = 0.26069429
Iteration 17, loss = 0.23777194
Iteration 18, loss = 0.21716096
Iteration 19, loss = 0.19842064
Iteration 20, loss = 0.18158368
Iteration 21, loss = 0.16634629
Iteration 22, loss = 0.15303246
Iteration 23, loss = 0.14082971
Iteration 24, loss = 0.13000862
Iteration 25, loss = 0.12046347
Iteration 26, loss = 0.11178693
Iteration 27, loss = 0.10411219
Iteration 28, loss = 0.09722389
Iteration 29, loss = 0.09093703
Iteration 30, loss = 0.08541327
Iteration 31, loss = 0.08036179
Iteration 32, loss = 0.07581671
Iteration 33, loss = 0.07167489
Iteration 34, loss = 0.06800379
Iteration 35, loss = 0.06448425
Iteration 36, loss = 0.06134422
Iteration 37, loss = 0.05852202
Iteration 38, loss = 0.05588877
Iteration 39, loss = 0.05347605
Iteration 40, loss = 0.05122961
Iteration 41, loss = 0.04926870
Iteration 42, loss = 0.04731238
Iteration 43, loss = 0.04555676
Iteration 44, loss = 0.04388874
Iteration 45, loss = 0.04243793
Iteration 46, loss = 0.04097996
Iteration 47, loss = 0.03971019
Iteration 48, loss = 0.03842127
Iteration 49, loss = 0.03728070
Iteration 50, loss = 0.03621332
Iteration 51, loss = 0.03523225
Iteration 52, loss = 0.03427203
Iteration 53, loss = 0.03337063
Iteration 54, loss = 0.03253873
Iteration 55, loss = 0.03172893
Iteration 56, loss = 0.03094466
Iteration 57, loss = 0.03029332
Iteration 58, loss = 0.02961499
Iteration 59, loss = 0.02901070
Iteration 60, loss = 0.02838060
Iteration 61, loss = 0.02779937
Iteration 62, loss = 0.02733237
Iteration 63, loss = 0.02679943
Iteration 64, loss = 0.02626054
Iteration 65, loss = 0.02579691
Iteration 66, loss = 0.02533921
Iteration 67, loss = 0.02493377
Iteration 68, loss = 0.02456022
Iteration 69, loss = 0.02415144
Iteration 70, loss = 0.02377177
Iteration 71, loss = 0.02346145
Iteration 72, loss = 0.02313823
Iteration 73, loss = 0.02278474
Iteration 74, loss = 0.02249079
Iteration 75, loss = 0.02222826
Iteration 76, loss = 0.02190507
Iteration 77, loss = 0.02165029
Iteration 78, loss = 0.02140045
Iteration 79, loss = 0.02112910
Iteration 80, loss = 0.02091938
Iteration 81, loss = 0.02065821
Iteration 82, loss = 0.02045559
Iteration 83, loss = 0.02027102
Iteration 84, loss = 0.02004064
Iteration 85, loss = 0.01988648
Iteration 86, loss = 0.01963108
Iteration 87, loss = 0.01951071
Iteration 88, loss = 0.01926980
Iteration 89, loss = 0.01911328
Iteration 90, loss = 0.01891343
Iteration 91, loss = 0.01878171
Iteration 92, loss = 0.01864637
Iteration 93, loss = 0.01848171
Iteration 94, loss = 0.01830793
Iteration 95, loss = 0.01818841
Iteration 96, loss = 0.01807812
Iteration 97, loss = 0.01795717
Iteration 98, loss = 0.01781874
Iteration 99, loss = 0.01771531
Iteration 100, loss = 0.01760458
Iteration 101, loss = 0.01749777
Iteration 102, loss = 0.01735407
Iteration 103, loss = 0.01729845
Iteration 104, loss = 0.01712025
Iteration 105, loss = 0.01706140
Iteration 106, loss = 0.01692044
Iteration 107, loss = 0.01685366
Iteration 108, loss = 0.01671512
Iteration 109, loss = 0.01665337
Iteration 110, loss = 0.01656069
Iteration 111, loss = 0.01648581
Iteration 112, loss = 0.01637853
Iteration 113, loss = 0.01632149
Iteration 114, loss = 0.01629243
Iteration 115, loss = 0.01617619
Iteration 116, loss = 0.01611329
Iteration 117, loss = 0.01601263
Iteration 118, loss = 0.01597505
Iteration 119, loss = 0.01590908
Iteration 120, loss = 0.01589466
Iteration 121, loss = 0.01577069
Iteration 122, loss = 0.01568131
Iteration 123, loss = 0.01568024
Iteration 124, loss = 0.01560558
Iteration 125, loss = 0.01553780
Iteration 126, loss = 0.01546051
Iteration 127, loss = 0.01535948
Iteration 128, loss = 0.01535775
Iteration 129, loss = 0.01532341
Iteration 130, loss = 0.01523961
Iteration 131, loss = 0.01521343
Iteration 132, loss = 0.01513821
Iteration 133, loss = 0.01515173
Iteration 134, loss = 0.01503665
Iteration 135, loss = 0.01498018
Iteration 136, loss = 0.01504995
Iteration 137, loss = 0.01491485
Iteration 138, loss = 0.01491915
Iteration 139, loss = 0.01491777
Iteration 140, loss = 0.01478450
Iteration 141, loss = 0.01477251
Iteration 142, loss = 0.01472042
Iteration 143, loss = 0.01469243
Iteration 144, loss = 0.01462264
Iteration 145, loss = 0.01462130
Iteration 146, loss = 0.01454002
Iteration 147, loss = 0.01452447
Iteration 148, loss = 0.01449808
Iteration 149, loss = 0.01445226
Iteration 150, loss = 0.01443354
Iteration 151, loss = 0.01442807
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68428949
Iteration 2, loss = 0.63566566
Iteration 3, loss = 0.56415052
Iteration 4, loss = 0.47833273
Iteration 5, loss = 0.39404236
Iteration 6, loss = 0.31754059
Iteration 7, loss = 0.25364312
Iteration 8, loss = 0.20350516
Iteration 9, loss = 0.16533919
Iteration 10, loss = 0.13638277
Iteration 11, loss = 0.11481945
Iteration 12, loss = 0.09834443
Iteration 13, loss = 0.08548241
Iteration 14, loss = 0.07560103
Iteration 15, loss = 0.06757416
Iteration 16, loss = 0.06112489
Iteration 17, loss = 0.05620687
Iteration 18, loss = 0.05172347
Iteration 19, loss = 0.04789432
Iteration 20, loss = 0.04481842
Iteration 21, loss = 0.04194672
Iteration 22, loss = 0.04000130
Iteration 23, loss = 0.03799675
Iteration 24, loss = 0.03626043
Iteration 25, loss = 0.03481692
Iteration 26, loss = 0.03351769
Iteration 27, loss = 0.03228627
Iteration 28, loss = 0.03105524
Iteration 29, loss = 0.03031977
Iteration 30, loss = 0.02957156
Iteration 31, loss = 0.02876318
Iteration 32, loss = 0.02815010
Iteration 33, loss = 0.02740125
Iteration 34, loss = 0.02687373
Iteration 35, loss = 0.02623907
Iteration 36, loss = 0.02596074
Iteration 37, loss = 0.02547044
Iteration 38, loss = 0.02517541
Iteration 39, loss = 0.02476495
Iteration 40, loss = 0.02447309
Iteration 41, loss = 0.02434813
Iteration 42, loss = 0.02400091
Iteration 43, loss = 0.02358810
Iteration 44, loss = 0.02326675
Iteration 45, loss = 0.02307191
Iteration 46, loss = 0.02290569
Iteration 47, loss = 0.02272206
Iteration 48, loss = 0.02272463
Iteration 49, loss = 0.02221611
Iteration 50, loss = 0.02229561
Iteration 51, loss = 0.02215288
Iteration 52, loss = 0.02176274
Iteration 53, loss = 0.02189791
Iteration 54, loss = 0.02152338
Iteration 55, loss = 0.02160734
Iteration 56, loss = 0.02136749
Iteration 57, loss = 0.02156569
Iteration 58, loss = 0.02128877
Iteration 59, loss = 0.02122169
Iteration 60, loss = 0.02115420
Iteration 61, loss = 0.02110469
Iteration 62, loss = 0.02086227
Iteration 63, loss = 0.02087755
Iteration 64, loss = 0.02074396
Iteration 65, loss = 0.02084384
Iteration 66, loss = 0.02069391
Iteration 67, loss = 0.02062170
Iteration 68, loss = 0.02059441
Iteration 69, loss = 0.02052494
Iteration 70, loss = 0.02052761
Iteration 71, loss = 0.02027803
Iteration 72, loss = 0.02016822
Iteration 73, loss = 0.02045377
Iteration 74, loss = 0.02041059
Iteration 75, loss = 0.01999935
Iteration 76, loss = 0.02016531
Iteration 77, loss = 0.02019483
Iteration 78, loss = 0.02012814
Iteration 79, loss = 0.02011370
Iteration 80, loss = 0.01994068
Iteration 81, loss = 0.02003784
Iteration 82, loss = 0.02005986
Iteration 83, loss = 0.01992107
Iteration 84, loss = 0.01983653
Iteration 85, loss = 0.01972082
Iteration 86, loss = 0.01975667
Iteration 87, loss = 0.01977360
Iteration 88, loss = 0.01981225
Iteration 89, loss = 0.01967531
Iteration 90, loss = 0.02009300
Iteration 91, loss = 0.01956247
Iteration 92, loss = 0.01984386
Iteration 93, loss = 0.01942583
Iteration 94, loss = 0.01952091
Iteration 95, loss = 0.01962932
Iteration 96, loss = 0.01947758
Iteration 97, loss = 0.01948102
Iteration 98, loss = 0.01960222
Iteration 99, loss = 0.01953916
Iteration 100, loss = 0.01960317
Iteration 101, loss = 0.01948267
Iteration 102, loss = 0.01938799
Iteration 103, loss = 0.01957074
Iteration 104, loss = 0.01946582
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71227199
Iteration 2, loss = 0.68780457
Iteration 3, loss = 0.66521509
Iteration 4, loss = 0.64004662
Iteration 5, loss = 0.61168782
Iteration 6, loss = 0.58062029
Iteration 7, loss = 0.54755500
Iteration 8, loss = 0.51313359
Iteration 9, loss = 0.47797693
Iteration 10, loss = 0.44280752
Iteration 11, loss = 0.40779257
Iteration 12, loss = 0.37442213
Iteration 13, loss = 0.34240110
Iteration 14, loss = 0.31256971
Iteration 15, loss = 0.28491501
Iteration 16, loss = 0.25952149
Iteration 17, loss = 0.23646762
Iteration 18, loss = 0.21569013
Iteration 19, loss = 0.19697465
Iteration 20, loss = 0.18017491
Iteration 21, loss = 0.16517881
Iteration 22, loss = 0.15184509
Iteration 23, loss = 0.13991608
Iteration 24, loss = 0.12926235
Iteration 25, loss = 0.11971471
Iteration 26, loss = 0.11124068
Iteration 27, loss = 0.10364295
Iteration 28, loss = 0.09676655
Iteration 29, loss = 0.09060171
Iteration 30, loss = 0.08509636
Iteration 31, loss = 0.08003748
Iteration 32, loss = 0.07550900
Iteration 33, loss = 0.07137113
Iteration 34, loss = 0.06760686
Iteration 35, loss = 0.06421982
Iteration 36, loss = 0.06103694
Iteration 37, loss = 0.05818706
Iteration 38, loss = 0.05552526
Iteration 39, loss = 0.05305765
Iteration 40, loss = 0.05086465
Iteration 41, loss = 0.04878001
Iteration 42, loss = 0.04692891
Iteration 43, loss = 0.04509889
Iteration 44, loss = 0.04347135
Iteration 45, loss = 0.04185734
Iteration 46, loss = 0.04041187
Iteration 47, loss = 0.03914854
Iteration 48, loss = 0.03790656
Iteration 49, loss = 0.03670207
Iteration 50, loss = 0.03550872
Iteration 51, loss = 0.03452126
Iteration 52, loss = 0.03351078
Iteration 53, loss = 0.03266895
Iteration 54, loss = 0.03172010
Iteration 55, loss = 0.03092139
Iteration 56, loss = 0.03012599
Iteration 57, loss = 0.02940434
Iteration 58, loss = 0.02882278
Iteration 59, loss = 0.02809123
Iteration 60, loss = 0.02744039
Iteration 61, loss = 0.02690440
Iteration 62, loss = 0.02625602
Iteration 63, loss = 0.02575731
Iteration 64, loss = 0.02525575
Iteration 65, loss = 0.02477120
Iteration 66, loss = 0.02431657
Iteration 67, loss = 0.02387933
Iteration 68, loss = 0.02345756
Iteration 69, loss = 0.02308873
Iteration 70, loss = 0.02264463
Iteration 71, loss = 0.02229698
Iteration 72, loss = 0.02191537
Iteration 73, loss = 0.02164084
Iteration 74, loss = 0.02128312
Iteration 75, loss = 0.02099089
Iteration 76, loss = 0.02066822
Iteration 77, loss = 0.02036632
Iteration 78, loss = 0.02014781
Iteration 79, loss = 0.01991237
Iteration 80, loss = 0.01961735
Iteration 81, loss = 0.01936332
Iteration 82, loss = 0.01913302
Iteration 83, loss = 0.01892515
Iteration 84, loss = 0.01870089
Iteration 85, loss = 0.01848220
Iteration 86, loss = 0.01828045
Iteration 87, loss = 0.01808418
Iteration 88, loss = 0.01788296
Iteration 89, loss = 0.01772336
Iteration 90, loss = 0.01764660
Iteration 91, loss = 0.01738972
Iteration 92, loss = 0.01724211
Iteration 93, loss = 0.01702946
Iteration 94, loss = 0.01690149
Iteration 95, loss = 0.01676182
Iteration 96, loss = 0.01664150
Iteration 97, loss = 0.01646008
Iteration 98, loss = 0.01637512
Iteration 99, loss = 0.01622978
Iteration 100, loss = 0.01608174
Iteration 101, loss = 0.01598290
Iteration 102, loss = 0.01588307
Iteration 103, loss = 0.01576134
Iteration 104, loss = 0.01562126
Iteration 105, loss = 0.01557787
Iteration 106, loss = 0.01548762
Iteration 107, loss = 0.01531526
Iteration 108, loss = 0.01526835
Iteration 109, loss = 0.01510707
Iteration 110, loss = 0.01507345
Iteration 111, loss = 0.01492310
Iteration 112, loss = 0.01482494
Iteration 113, loss = 0.01474801
Iteration 114, loss = 0.01475957
Iteration 115, loss = 0.01469155
Iteration 116, loss = 0.01460646
Iteration 117, loss = 0.01446138
Iteration 118, loss = 0.01437881
Iteration 119, loss = 0.01429861
Iteration 120, loss = 0.01423911
Iteration 121, loss = 0.01420606
Iteration 122, loss = 0.01411116
Iteration 123, loss = 0.01405480
Iteration 124, loss = 0.01408743
Iteration 125, loss = 0.01393573
Iteration 126, loss = 0.01386676
Iteration 127, loss = 0.01384173
Iteration 128, loss = 0.01375091
Iteration 129, loss = 0.01366872
Iteration 130, loss = 0.01362731
Iteration 131, loss = 0.01353332
Iteration 132, loss = 0.01354099
Iteration 133, loss = 0.01344982
Iteration 134, loss = 0.01343376
Iteration 135, loss = 0.01341426
Iteration 136, loss = 0.01334125
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71146370
Iteration 2, loss = 0.68655287
Iteration 3, loss = 0.65037603
Iteration 4, loss = 0.60276281
Iteration 5, loss = 0.54858165
Iteration 6, loss = 0.48923544
Iteration 7, loss = 0.42771832
Iteration 8, loss = 0.36878642
Iteration 9, loss = 0.31533542
Iteration 10, loss = 0.26861005
Iteration 11, loss = 0.22855846
Iteration 12, loss = 0.19537192
Iteration 13, loss = 0.16760018
Iteration 14, loss = 0.14493494
Iteration 15, loss = 0.12641853
Iteration 16, loss = 0.11123505
Iteration 17, loss = 0.09844979
Iteration 18, loss = 0.08803203
Iteration 19, loss = 0.07929387
Iteration 20, loss = 0.07192364
Iteration 21, loss = 0.06559457
Iteration 22, loss = 0.06030322
Iteration 23, loss = 0.05575219
Iteration 24, loss = 0.05185212
Iteration 25, loss = 0.04815960
Iteration 26, loss = 0.04521840
Iteration 27, loss = 0.04254362
Iteration 28, loss = 0.04024156
Iteration 29, loss = 0.03812067
Iteration 30, loss = 0.03631495
Iteration 31, loss = 0.03461029
Iteration 32, loss = 0.03298925
Iteration 33, loss = 0.03171715
Iteration 34, loss = 0.03047440
Iteration 35, loss = 0.02939287
Iteration 36, loss = 0.02828026
Iteration 37, loss = 0.02740443
Iteration 38, loss = 0.02663841
Iteration 39, loss = 0.02595235
Iteration 40, loss = 0.02513991
Iteration 41, loss = 0.02449395
Iteration 42, loss = 0.02371723
Iteration 43, loss = 0.02324915
Iteration 44, loss = 0.02275536
Iteration 45, loss = 0.02223855
Iteration 46, loss = 0.02169714
Iteration 47, loss = 0.02134447
Iteration 48, loss = 0.02093276
Iteration 49, loss = 0.02067107
Iteration 50, loss = 0.02018032
Iteration 51, loss = 0.01998989
Iteration 52, loss = 0.01952213
Iteration 53, loss = 0.01925787
Iteration 54, loss = 0.01902119
Iteration 55, loss = 0.01882060
Iteration 56, loss = 0.01852445
Iteration 57, loss = 0.01827525
Iteration 58, loss = 0.01807947
Iteration 59, loss = 0.01782636
Iteration 60, loss = 0.01762289
Iteration 61, loss = 0.01762557
Iteration 62, loss = 0.01728912
Iteration 63, loss = 0.01713024
Iteration 64, loss = 0.01698720
Iteration 65, loss = 0.01678935
Iteration 66, loss = 0.01666572
Iteration 67, loss = 0.01655354
Iteration 68, loss = 0.01642598
Iteration 69, loss = 0.01634100
Iteration 70, loss = 0.01617839
Iteration 71, loss = 0.01606807
Iteration 72, loss = 0.01604909
Iteration 73, loss = 0.01588685
Iteration 74, loss = 0.01580321
Iteration 75, loss = 0.01566407
Iteration 76, loss = 0.01559816
Iteration 77, loss = 0.01549500
Iteration 78, loss = 0.01538013
Iteration 79, loss = 0.01535061
Iteration 80, loss = 0.01524364
Iteration 81, loss = 0.01525806
Iteration 82, loss = 0.01513249
Iteration 83, loss = 0.01515411
Iteration 84, loss = 0.01506590
Iteration 85, loss = 0.01501148
Iteration 86, loss = 0.01487067
Iteration 87, loss = 0.01482426
Iteration 88, loss = 0.01473994
Iteration 89, loss = 0.01467059
Iteration 90, loss = 0.01475098
Iteration 91, loss = 0.01466312
Iteration 92, loss = 0.01466280
Iteration 93, loss = 0.01445449
Iteration 94, loss = 0.01449159
Iteration 95, loss = 0.01440346
Iteration 96, loss = 0.01440764
Iteration 97, loss = 0.01435707
Iteration 98, loss = 0.01428232
Iteration 99, loss = 0.01433137
Iteration 100, loss = 0.01422541
Iteration 101, loss = 0.01419911
Iteration 102, loss = 0.01425111
Iteration 103, loss = 0.01404838
Iteration 104, loss = 0.01402810
Iteration 105, loss = 0.01407036
Iteration 106, loss = 0.01406821
Iteration 107, loss = 0.01411630
Iteration 108, loss = 0.01394940
Iteration 109, loss = 0.01393918
Iteration 110, loss = 0.01398346
Iteration 111, loss = 0.01388266
Iteration 112, loss = 0.01394567
Iteration 113, loss = 0.01388510
Iteration 114, loss = 0.01381723
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68049524
Iteration 2, loss = 0.62745478
Iteration 3, loss = 0.55963481
Iteration 4, loss = 0.48207063
Iteration 5, loss = 0.40536783
Iteration 6, loss = 0.33541456
Iteration 7, loss = 0.27518632
Iteration 8, loss = 0.22558517
Iteration 9, loss = 0.18598317
Iteration 10, loss = 0.15514868
Iteration 11, loss = 0.13084505
Iteration 12, loss = 0.11234625
Iteration 13, loss = 0.09744489
Iteration 14, loss = 0.08576258
Iteration 15, loss = 0.07624166
Iteration 16, loss = 0.06840285
Iteration 17, loss = 0.06225467
Iteration 18, loss = 0.05704722
Iteration 19, loss = 0.05246234
Iteration 20, loss = 0.04888987
Iteration 21, loss = 0.04562535
Iteration 22, loss = 0.04300823
Iteration 23, loss = 0.04054986
Iteration 24, loss = 0.03842653
Iteration 25, loss = 0.03677411
Iteration 26, loss = 0.03509338
Iteration 27, loss = 0.03374965
Iteration 28, loss = 0.03249443
Iteration 29, loss = 0.03132796
Iteration 30, loss = 0.03020784
Iteration 31, loss = 0.02931719
Iteration 32, loss = 0.02838498
Iteration 33, loss = 0.02799508
Iteration 34, loss = 0.02726136
Iteration 35, loss = 0.02638007
Iteration 36, loss = 0.02585327
Iteration 37, loss = 0.02534576
Iteration 38, loss = 0.02480068
Iteration 39, loss = 0.02453299
Iteration 40, loss = 0.02405126
Iteration 41, loss = 0.02409056
Iteration 42, loss = 0.02352690
Iteration 43, loss = 0.02295402
Iteration 44, loss = 0.02263969
Iteration 45, loss = 0.02245106
Iteration 46, loss = 0.02202381
Iteration 47, loss = 0.02175224
Iteration 48, loss = 0.02168569
Iteration 49, loss = 0.02131747
Iteration 50, loss = 0.02136419
Iteration 51, loss = 0.02112734
Iteration 52, loss = 0.02090814
Iteration 53, loss = 0.02075966
Iteration 54, loss = 0.02055547
Iteration 55, loss = 0.02030452
Iteration 56, loss = 0.02013713
Iteration 57, loss = 0.02010329
Iteration 58, loss = 0.02002748
Iteration 59, loss = 0.01986547
Iteration 60, loss = 0.01972434
Iteration 61, loss = 0.01947252
Iteration 62, loss = 0.01940858
Iteration 63, loss = 0.01948920
Iteration 64, loss = 0.01936157
Iteration 65, loss = 0.01913816
Iteration 66, loss = 0.01914006
Iteration 67, loss = 0.01899853
Iteration 68, loss = 0.01903325
Iteration 69, loss = 0.01895456
Iteration 70, loss = 0.01879843
Iteration 71, loss = 0.01867419
Iteration 72, loss = 0.01864268
Iteration 73, loss = 0.01855923
Iteration 74, loss = 0.01857927
Iteration 75, loss = 0.01858573
Iteration 76, loss = 0.01849637
Iteration 77, loss = 0.01836011
Iteration 78, loss = 0.01890677
Iteration 79, loss = 0.01854439
Iteration 80, loss = 0.01819403
Iteration 81, loss = 0.01815184
Iteration 82, loss = 0.01813409
Iteration 83, loss = 0.01828784
Iteration 84, loss = 0.01808867
Iteration 85, loss = 0.01810803
Iteration 86, loss = 0.01803633
Iteration 87, loss = 0.01821537
Iteration 88, loss = 0.01797755
Iteration 89, loss = 0.01814741
Iteration 90, loss = 0.01793528
Iteration 91, loss = 0.01782099
Iteration 92, loss = 0.01784864
Iteration 93, loss = 0.01829112
Iteration 94, loss = 0.01795527
Iteration 95, loss = 0.01782916
Iteration 96, loss = 0.01791229
Iteration 97, loss = 0.01794785
Iteration 98, loss = 0.01758530
Iteration 99, loss = 0.01796450
Iteration 100, loss = 0.01768561
Iteration 101, loss = 0.01754525
Iteration 102, loss = 0.01746207
Iteration 103, loss = 0.01761336
Iteration 104, loss = 0.01743049
Iteration 105, loss = 0.01769341
Iteration 106, loss = 0.01768911
Iteration 107, loss = 0.01745036
Iteration 108, loss = 0.01754442
Iteration 109, loss = 0.01755178
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69079168
Iteration 2, loss = 0.65077203
Iteration 3, loss = 0.59789238
Iteration 4, loss = 0.53460249
Iteration 5, loss = 0.46588174
Iteration 6, loss = 0.39879560
Iteration 7, loss = 0.33642917
Iteration 8, loss = 0.28193384
Iteration 9, loss = 0.23583945
Iteration 10, loss = 0.19788259
Iteration 11, loss = 0.16723559
Iteration 12, loss = 0.14266319
Iteration 13, loss = 0.12310406
Iteration 14, loss = 0.10744291
Iteration 15, loss = 0.09465061
Iteration 16, loss = 0.08437255
Iteration 17, loss = 0.07582511
Iteration 18, loss = 0.06881100
Iteration 19, loss = 0.06295795
Iteration 20, loss = 0.05790226
Iteration 21, loss = 0.05358622
Iteration 22, loss = 0.04992636
Iteration 23, loss = 0.04678877
Iteration 24, loss = 0.04412668
Iteration 25, loss = 0.04164172
Iteration 26, loss = 0.03969837
Iteration 27, loss = 0.03775568
Iteration 28, loss = 0.03603391
Iteration 29, loss = 0.03473334
Iteration 30, loss = 0.03335392
Iteration 31, loss = 0.03197450
Iteration 32, loss = 0.03109576
Iteration 33, loss = 0.03004607
Iteration 34, loss = 0.02915033
Iteration 35, loss = 0.02849650
Iteration 36, loss = 0.02777359
Iteration 37, loss = 0.02696431
Iteration 38, loss = 0.02644166
Iteration 39, loss = 0.02581963
Iteration 40, loss = 0.02526101
Iteration 41, loss = 0.02482343
Iteration 42, loss = 0.02441724
Iteration 43, loss = 0.02402635
Iteration 44, loss = 0.02371632
Iteration 45, loss = 0.02327408
Iteration 46, loss = 0.02284644
Iteration 47, loss = 0.02264698
Iteration 48, loss = 0.02254088
Iteration 49, loss = 0.02203039
Iteration 50, loss = 0.02174456
Iteration 51, loss = 0.02149726
Iteration 52, loss = 0.02134160
Iteration 53, loss = 0.02108764
Iteration 54, loss = 0.02087919
Iteration 55, loss = 0.02080033
Iteration 56, loss = 0.02057518
Iteration 57, loss = 0.02034141
Iteration 58, loss = 0.02013081
Iteration 59, loss = 0.02015379
Iteration 60, loss = 0.02016954
Iteration 61, loss = 0.01990768
Iteration 62, loss = 0.01968890
Iteration 63, loss = 0.01954746
Iteration 64, loss = 0.01953030
Iteration 65, loss = 0.01937087
Iteration 66, loss = 0.01924835
Iteration 67, loss = 0.01914203
Iteration 68, loss = 0.01919234
Iteration 69, loss = 0.01904579
Iteration 70, loss = 0.01880778
Iteration 71, loss = 0.01877266
Iteration 72, loss = 0.01883217
Iteration 73, loss = 0.01874802
Iteration 74, loss = 0.01856723
Iteration 75, loss = 0.01852009
Iteration 76, loss = 0.01841756
Iteration 77, loss = 0.01836074
Iteration 78, loss = 0.01831607
Iteration 79, loss = 0.01847752
Iteration 80, loss = 0.01814461
Iteration 81, loss = 0.01820295
Iteration 82, loss = 0.01812012
Iteration 83, loss = 0.01796518
Iteration 84, loss = 0.01804830
Iteration 85, loss = 0.01802795
Iteration 86, loss = 0.01785996
Iteration 87, loss = 0.01793817
Iteration 88, loss = 0.01788247
Iteration 89, loss = 0.01783288
Iteration 90, loss = 0.01781984
Iteration 91, loss = 0.01767959
Iteration 92, loss = 0.01779554
Iteration 93, loss = 0.01768322
Iteration 94, loss = 0.01771598
Iteration 95, loss = 0.01747241
Iteration 96, loss = 0.01757935
Iteration 97, loss = 0.01755392
Iteration 98, loss = 0.01738997
Iteration 99, loss = 0.01768154
Iteration 100, loss = 0.01750240
Iteration 101, loss = 0.01742403
Iteration 102, loss = 0.01755953
Iteration 103, loss = 0.01747225
Iteration 104, loss = 0.01766430
Iteration 105, loss = 0.01733616
Iteration 106, loss = 0.01736638
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68274292
Iteration 2, loss = 0.63711396
Iteration 3, loss = 0.56260626
Iteration 4, loss = 0.46930290
Iteration 5, loss = 0.37939998
Iteration 6, loss = 0.30212651
Iteration 7, loss = 0.23969531
Iteration 8, loss = 0.19196956
Iteration 9, loss = 0.15595979
Iteration 10, loss = 0.12953110
Iteration 11, loss = 0.10921996
Iteration 12, loss = 0.09406389
Iteration 13, loss = 0.08225133
Iteration 14, loss = 0.07291935
Iteration 15, loss = 0.06563332
Iteration 16, loss = 0.05942463
Iteration 17, loss = 0.05456801
Iteration 18, loss = 0.05019652
Iteration 19, loss = 0.04696293
Iteration 20, loss = 0.04404557
Iteration 21, loss = 0.04151597
Iteration 22, loss = 0.03939484
Iteration 23, loss = 0.03737338
Iteration 24, loss = 0.03570556
Iteration 25, loss = 0.03429473
Iteration 26, loss = 0.03306682
Iteration 27, loss = 0.03190783
Iteration 28, loss = 0.03101260
Iteration 29, loss = 0.03039457
Iteration 30, loss = 0.02937665
Iteration 31, loss = 0.02854503
Iteration 32, loss = 0.02791155
Iteration 33, loss = 0.02742847
Iteration 34, loss = 0.02687737
Iteration 35, loss = 0.02617140
Iteration 36, loss = 0.02596827
Iteration 37, loss = 0.02530448
Iteration 38, loss = 0.02503033
Iteration 39, loss = 0.02473108
Iteration 40, loss = 0.02428135
Iteration 41, loss = 0.02399227
Iteration 42, loss = 0.02408957
Iteration 43, loss = 0.02354454
Iteration 44, loss = 0.02329824
Iteration 45, loss = 0.02307370
Iteration 46, loss = 0.02300443
Iteration 47, loss = 0.02277932
Iteration 48, loss = 0.02262868
Iteration 49, loss = 0.02226770
Iteration 50, loss = 0.02230704
Iteration 51, loss = 0.02194547
Iteration 52, loss = 0.02190260
Iteration 53, loss = 0.02175084
Iteration 54, loss = 0.02164443
Iteration 55, loss = 0.02147602
Iteration 56, loss = 0.02123833
Iteration 57, loss = 0.02132170
Iteration 58, loss = 0.02133811
Iteration 59, loss = 0.02122590
Iteration 60, loss = 0.02104328
Iteration 61, loss = 0.02096233
Iteration 62, loss = 0.02067071
Iteration 63, loss = 0.02093217
Iteration 64, loss = 0.02069481
Iteration 65, loss = 0.02062991
Iteration 66, loss = 0.02076517
Iteration 67, loss = 0.02052490
Iteration 68, loss = 0.02042020
Iteration 69, loss = 0.02060888
Iteration 70, loss = 0.02056854
Iteration 71, loss = 0.02031566
Iteration 72, loss = 0.02038518
Iteration 73, loss = 0.02020241
Iteration 74, loss = 0.02031385
Iteration 75, loss = 0.02018775
Iteration 76, loss = 0.02029493
Iteration 77, loss = 0.02039337
Iteration 78, loss = 0.02001610
Iteration 79, loss = 0.02038133
Iteration 80, loss = 0.01976249
Iteration 81, loss = 0.01994993
Iteration 82, loss = 0.01998507
Iteration 83, loss = 0.01967419
Iteration 84, loss = 0.01983964
Iteration 85, loss = 0.01982614
Iteration 86, loss = 0.01968872
Iteration 87, loss = 0.01981575
Iteration 88, loss = 0.01966572
Iteration 89, loss = 0.01958806
Iteration 90, loss = 0.01972948
Iteration 91, loss = 0.01951418
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69319802
Iteration 2, loss = 0.63117850
Iteration 3, loss = 0.53856129
Iteration 4, loss = 0.43037447
Iteration 5, loss = 0.33173300
Iteration 6, loss = 0.25278325
Iteration 7, loss = 0.19419439
Iteration 8, loss = 0.15250682
Iteration 9, loss = 0.12323468
Iteration 10, loss = 0.10208450
Iteration 11, loss = 0.08663497
Iteration 12, loss = 0.07511500
Iteration 13, loss = 0.06619477
Iteration 14, loss = 0.05916325
Iteration 15, loss = 0.05368220
Iteration 16, loss = 0.04921535
Iteration 17, loss = 0.04555932
Iteration 18, loss = 0.04247188
Iteration 19, loss = 0.04004613
Iteration 20, loss = 0.03770270
Iteration 21, loss = 0.03574540
Iteration 22, loss = 0.03423337
Iteration 23, loss = 0.03298163
Iteration 24, loss = 0.03155037
Iteration 25, loss = 0.03062637
Iteration 26, loss = 0.02982570
Iteration 27, loss = 0.02897273
Iteration 28, loss = 0.02820360
Iteration 29, loss = 0.02759934
Iteration 30, loss = 0.02673466
Iteration 31, loss = 0.02626559
Iteration 32, loss = 0.02581165
Iteration 33, loss = 0.02548356
Iteration 34, loss = 0.02504950
Iteration 35, loss = 0.02470875
Iteration 36, loss = 0.02421170
Iteration 37, loss = 0.02393094
Iteration 38, loss = 0.02346603
Iteration 39, loss = 0.02357966
Iteration 40, loss = 0.02318712
Iteration 41, loss = 0.02304338
Iteration 42, loss = 0.02286181
Iteration 43, loss = 0.02271658
Iteration 44, loss = 0.02263941
Iteration 45, loss = 0.02251574
Iteration 46, loss = 0.02207618
Iteration 47, loss = 0.02210097
Iteration 48, loss = 0.02175144
Iteration 49, loss = 0.02171333
Iteration 50, loss = 0.02170369
Iteration 51, loss = 0.02139822
Iteration 52, loss = 0.02129043
Iteration 53, loss = 0.02136154
Iteration 54, loss = 0.02116239
Iteration 55, loss = 0.02120405
Iteration 56, loss = 0.02125050
Iteration 57, loss = 0.02091372
Iteration 58, loss = 0.02081567
Iteration 59, loss = 0.02086828
Iteration 60, loss = 0.02083224
Iteration 61, loss = 0.02074462
Iteration 62, loss = 0.02030448
Iteration 63, loss = 0.02070704
Iteration 64, loss = 0.02042775
Iteration 65, loss = 0.02047259
Iteration 66, loss = 0.02028847
Iteration 67, loss = 0.02032651
Iteration 68, loss = 0.02037068
Iteration 69, loss = 0.02037115
Iteration 70, loss = 0.02016035
Iteration 71, loss = 0.02030508
Iteration 72, loss = 0.02027449
Iteration 73, loss = 0.01996998
Iteration 74, loss = 0.02002826
Iteration 75, loss = 0.02006718
Iteration 76, loss = 0.02004182
Iteration 77, loss = 0.02024390
Iteration 78, loss = 0.01982866
Iteration 79, loss = 0.02011556
Iteration 80, loss = 0.01983082
Iteration 81, loss = 0.01977955
Iteration 82, loss = 0.01968176
Iteration 83, loss = 0.01980101
Iteration 84, loss = 0.01964161
Iteration 85, loss = 0.01983812
Iteration 86, loss = 0.01998298
Iteration 87, loss = 0.01998492
Iteration 88, loss = 0.02012547
Iteration 89, loss = 0.01966098
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68027527
Iteration 2, loss = 0.65277550
Iteration 3, loss = 0.62159272
Iteration 4, loss = 0.58264081
Iteration 5, loss = 0.53673507
Iteration 6, loss = 0.48745486
Iteration 7, loss = 0.43823334
Iteration 8, loss = 0.39107825
Iteration 9, loss = 0.34705679
Iteration 10, loss = 0.30665920
Iteration 11, loss = 0.27080756
Iteration 12, loss = 0.23893508
Iteration 13, loss = 0.21121279
Iteration 14, loss = 0.18734668
Iteration 15, loss = 0.16668216
Iteration 16, loss = 0.14942342
Iteration 17, loss = 0.13430730
Iteration 18, loss = 0.12146129
Iteration 19, loss = 0.11034998
Iteration 20, loss = 0.10079380
Iteration 21, loss = 0.09251873
Iteration 22, loss = 0.08523459
Iteration 23, loss = 0.07901097
Iteration 24, loss = 0.07361308
Iteration 25, loss = 0.06874825
Iteration 26, loss = 0.06435573
Iteration 27, loss = 0.06050516
Iteration 28, loss = 0.05700398
Iteration 29, loss = 0.05376046
Iteration 30, loss = 0.05116001
Iteration 31, loss = 0.04885856
Iteration 32, loss = 0.04657227
Iteration 33, loss = 0.04448393
Iteration 34, loss = 0.04254797
Iteration 35, loss = 0.04081568
Iteration 36, loss = 0.03921698
Iteration 37, loss = 0.03783300
Iteration 38, loss = 0.03642945
Iteration 39, loss = 0.03536312
Iteration 40, loss = 0.03422552
Iteration 41, loss = 0.03320922
Iteration 42, loss = 0.03226943
Iteration 43, loss = 0.03131494
Iteration 44, loss = 0.03050414
Iteration 45, loss = 0.02951654
Iteration 46, loss = 0.02904106
Iteration 47, loss = 0.02838299
Iteration 48, loss = 0.02769047
Iteration 49, loss = 0.02715900
Iteration 50, loss = 0.02640582
Iteration 51, loss = 0.02609783
Iteration 52, loss = 0.02584229
Iteration 53, loss = 0.02519844
Iteration 54, loss = 0.02481683
Iteration 55, loss = 0.02435211
Iteration 56, loss = 0.02385057
Iteration 57, loss = 0.02356123
Iteration 58, loss = 0.02307516
Iteration 59, loss = 0.02281399
Iteration 60, loss = 0.02252128
Iteration 61, loss = 0.02233159
Iteration 62, loss = 0.02204183
Iteration 63, loss = 0.02168846
Iteration 64, loss = 0.02120538
Iteration 65, loss = 0.02102124
Iteration 66, loss = 0.02072685
Iteration 67, loss = 0.02070900
Iteration 68, loss = 0.02051648
Iteration 69, loss = 0.02024216
Iteration 70, loss = 0.02005068
Iteration 71, loss = 0.01970542
Iteration 72, loss = 0.01976619
Iteration 73, loss = 0.01945612
Iteration 74, loss = 0.01919560
Iteration 75, loss = 0.01907098
Iteration 76, loss = 0.01894366
Iteration 77, loss = 0.01887173
Iteration 78, loss = 0.01881631
Iteration 79, loss = 0.01870150
Iteration 80, loss = 0.01858203
Iteration 81, loss = 0.01853024
Iteration 82, loss = 0.01840736
Iteration 83, loss = 0.01811431
Iteration 84, loss = 0.01793735
Iteration 85, loss = 0.01826424
Iteration 86, loss = 0.01847206
Iteration 87, loss = 0.01836091
Iteration 88, loss = 0.01797631
Iteration 89, loss = 0.01770400
Iteration 90, loss = 0.01758909
Iteration 91, loss = 0.01774891
Iteration 92, loss = 0.01735791
Iteration 93, loss = 0.01716737
Iteration 94, loss = 0.01710262
Iteration 95, loss = 0.01723567
Iteration 96, loss = 0.01731332
Iteration 97, loss = 0.01725619
Iteration 98, loss = 0.01732311
Iteration 99, loss = 0.01728250
Iteration 100, loss = 0.01717845
Iteration 101, loss = 0.01707232
Iteration 102, loss = 0.01671507
Iteration 103, loss = 0.01648489
Iteration 104, loss = 0.01655430
Iteration 105, loss = 0.01647203
Iteration 106, loss = 0.01639972
Iteration 107, loss = 0.01633895
Iteration 108, loss = 0.01670442
Iteration 109, loss = 0.01653189
Iteration 110, loss = 0.01627618
Iteration 111, loss = 0.01625329
Iteration 112, loss = 0.01611951
Iteration 113, loss = 0.01611429
Iteration 114, loss = 0.01610908
Iteration 115, loss = 0.01598436
Iteration 116, loss = 0.01586073
Iteration 117, loss = 0.01596167
Iteration 118, loss = 0.01602294
Iteration 119, loss = 0.01589641
Iteration 120, loss = 0.01575327
Iteration 121, loss = 0.01566140
Iteration 122, loss = 0.01571412
Iteration 123, loss = 0.01558486
Iteration 124, loss = 0.01548496
Iteration 125, loss = 0.01544732
Iteration 126, loss = 0.01568166
Iteration 127, loss = 0.01571945
Iteration 128, loss = 0.01554738
Iteration 129, loss = 0.01544883
Iteration 130, loss = 0.01536848
Iteration 131, loss = 0.01556334
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70727617
Iteration 2, loss = 0.64372140
Iteration 3, loss = 0.55925511
Iteration 4, loss = 0.45742856
Iteration 5, loss = 0.36388104
Iteration 6, loss = 0.28501292
Iteration 7, loss = 0.22450471
Iteration 8, loss = 0.17893603
Iteration 9, loss = 0.14559100
Iteration 10, loss = 0.12121129
Iteration 11, loss = 0.10303988
Iteration 12, loss = 0.08926345
Iteration 13, loss = 0.07848830
Iteration 14, loss = 0.07005715
Iteration 15, loss = 0.06380133
Iteration 16, loss = 0.05827192
Iteration 17, loss = 0.05366799
Iteration 18, loss = 0.05011891
Iteration 19, loss = 0.04666111
Iteration 20, loss = 0.04407315
Iteration 21, loss = 0.04170411
Iteration 22, loss = 0.03982850
Iteration 23, loss = 0.03822466
Iteration 24, loss = 0.03670282
Iteration 25, loss = 0.03521401
Iteration 26, loss = 0.03393249
Iteration 27, loss = 0.03295137
Iteration 28, loss = 0.03203046
Iteration 29, loss = 0.03114284
Iteration 30, loss = 0.03042077
Iteration 31, loss = 0.03024648
Iteration 32, loss = 0.02945390
Iteration 33, loss = 0.02872591
Iteration 34, loss = 0.02828503
Iteration 35, loss = 0.03002808
Iteration 36, loss = 0.02954927
Iteration 37, loss = 0.02862150
Iteration 38, loss = 0.02798819
Iteration 39, loss = 0.02744004
Iteration 40, loss = 0.02692001
Iteration 41, loss = 0.02670034
Iteration 42, loss = 0.02631161
Iteration 43, loss = 0.02579346
Iteration 44, loss = 0.02581090
Iteration 45, loss = 0.02591606
Iteration 46, loss = 0.02541774
Iteration 47, loss = 0.02522509
Iteration 48, loss = 0.02476408
Iteration 49, loss = 0.02437719
Iteration 50, loss = 0.02439748
Iteration 51, loss = 0.02389845
Iteration 52, loss = 0.02363470
Iteration 53, loss = 0.02355335
Iteration 54, loss = 0.02335367
Iteration 55, loss = 0.02317488
Iteration 56, loss = 0.02318076
Iteration 57, loss = 0.02314124
Iteration 58, loss = 0.02284768
Iteration 59, loss = 0.02285257
Iteration 60, loss = 0.02295197
Iteration 61, loss = 0.02289343
Iteration 62, loss = 0.02275797
Iteration 63, loss = 0.02270095
Iteration 64, loss = 0.02233466
Iteration 65, loss = 0.02234481
Iteration 66, loss = 0.02241048
Iteration 67, loss = 0.02215371
Iteration 68, loss = 0.02235842
Iteration 69, loss = 0.02209662
Iteration 70, loss = 0.02205032
Iteration 71, loss = 0.02196464
Iteration 72, loss = 0.02191938
Iteration 73, loss = 0.02219204
Iteration 74, loss = 0.02207153
Iteration 75, loss = 0.02187027
Iteration 76, loss = 0.02180760
Iteration 77, loss = 0.02172623
Iteration 78, loss = 0.02157438
Iteration 79, loss = 0.02172519
Iteration 80, loss = 0.02138035
Iteration 81, loss = 0.02139647
Iteration 82, loss = 0.02144239
Iteration 83, loss = 0.02143822
Iteration 84, loss = 0.02135468
Iteration 85, loss = 0.02126756
Iteration 86, loss = 0.02120387
Iteration 87, loss = 0.02127699
Iteration 88, loss = 0.02121232
Iteration 89, loss = 0.02134214
Iteration 90, loss = 0.02150216
Iteration 91, loss = 0.02116655
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67168460
Iteration 2, loss = 0.58998671
Iteration 3, loss = 0.47771364
Iteration 4, loss = 0.36795110
Iteration 5, loss = 0.27754717
Iteration 6, loss = 0.21037015
Iteration 7, loss = 0.16309806
Iteration 8, loss = 0.13059039
Iteration 9, loss = 0.10756984
Iteration 10, loss = 0.09116040
Iteration 11, loss = 0.07884627
Iteration 12, loss = 0.06969955
Iteration 13, loss = 0.06252215
Iteration 14, loss = 0.05717135
Iteration 15, loss = 0.05244387
Iteration 16, loss = 0.04872509
Iteration 17, loss = 0.04583969
Iteration 18, loss = 0.04322622
Iteration 19, loss = 0.04114289
Iteration 20, loss = 0.03930689
Iteration 21, loss = 0.03799553
Iteration 22, loss = 0.03658282
Iteration 23, loss = 0.03556817
Iteration 24, loss = 0.03437195
Iteration 25, loss = 0.03323720
Iteration 26, loss = 0.03302574
Iteration 27, loss = 0.03224795
Iteration 28, loss = 0.03143967
Iteration 29, loss = 0.03082989
Iteration 30, loss = 0.03017492
Iteration 31, loss = 0.02983862
Iteration 32, loss = 0.02937484
Iteration 33, loss = 0.02881416
Iteration 34, loss = 0.02871091
Iteration 35, loss = 0.02860801
Iteration 36, loss = 0.02852565
Iteration 37, loss = 0.02797483
Iteration 38, loss = 0.02803295
Iteration 39, loss = 0.02760679
Iteration 40, loss = 0.02734028
Iteration 41, loss = 0.02727772
Iteration 42, loss = 0.02700486
Iteration 43, loss = 0.02659487
Iteration 44, loss = 0.02667643
Iteration 45, loss = 0.02656607
Iteration 46, loss = 0.02647840
Iteration 47, loss = 0.02629363
Iteration 48, loss = 0.02617160
Iteration 49, loss = 0.02606430
Iteration 50, loss = 0.02576541
Iteration 51, loss = 0.02585909
Iteration 52, loss = 0.02608201
Iteration 53, loss = 0.02565410
Iteration 54, loss = 0.02575267
Iteration 55, loss = 0.02561481
Iteration 56, loss = 0.02590566
Iteration 57, loss = 0.02553603
Iteration 58, loss = 0.02555760
Iteration 59, loss = 0.02526397
Iteration 60, loss = 0.02506970
Iteration 61, loss = 0.02514058
Iteration 62, loss = 0.02510162
Iteration 63, loss = 0.02529390
Iteration 64, loss = 0.02510942
Iteration 65, loss = 0.02502706
Iteration 66, loss = 0.02494881
Iteration 67, loss = 0.02483676
Iteration 68, loss = 0.02482013
Iteration 69, loss = 0.02473304
Iteration 70, loss = 0.02456214
Iteration 71, loss = 0.02452532
Iteration 72, loss = 0.02474315
Iteration 73, loss = 0.02465573
Iteration 74, loss = 0.02448805
Iteration 75, loss = 0.02489814
Iteration 76, loss = 0.02462768
Iteration 77, loss = 0.02438849
Iteration 78, loss = 0.02481653
Iteration 79, loss = 0.02460696
Iteration 80, loss = 0.02460938
Iteration 81, loss = 0.02447887
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68283967
Iteration 2, loss = 0.61716203
Iteration 3, loss = 0.51238506
Iteration 4, loss = 0.40231587
Iteration 5, loss = 0.30803141
Iteration 6, loss = 0.23551492
Iteration 7, loss = 0.18264898
Iteration 8, loss = 0.14537898
Iteration 9, loss = 0.11895917
Iteration 10, loss = 0.09976248
Iteration 11, loss = 0.08563508
Iteration 12, loss = 0.07526382
Iteration 13, loss = 0.06693265
Iteration 14, loss = 0.06062827
Iteration 15, loss = 0.05547696
Iteration 16, loss = 0.05102855
Iteration 17, loss = 0.04777911
Iteration 18, loss = 0.04492376
Iteration 19, loss = 0.04263418
Iteration 20, loss = 0.04071939
Iteration 21, loss = 0.03880573
Iteration 22, loss = 0.03733257
Iteration 23, loss = 0.03606384
Iteration 24, loss = 0.03482727
Iteration 25, loss = 0.03394115
Iteration 26, loss = 0.03314410
Iteration 27, loss = 0.03225023
Iteration 28, loss = 0.03143767
Iteration 29, loss = 0.03094563
Iteration 30, loss = 0.03034101
Iteration 31, loss = 0.02981063
Iteration 32, loss = 0.02906866
Iteration 33, loss = 0.02887313
Iteration 34, loss = 0.02873132
Iteration 35, loss = 0.02817239
Iteration 36, loss = 0.02786539
Iteration 37, loss = 0.02804685
Iteration 38, loss = 0.02738984
Iteration 39, loss = 0.02704117
Iteration 40, loss = 0.02665606
Iteration 41, loss = 0.02684156
Iteration 42, loss = 0.02650553
Iteration 43, loss = 0.02643367
Iteration 44, loss = 0.02610182
Iteration 45, loss = 0.02617819
Iteration 46, loss = 0.02587476
Iteration 47, loss = 0.02544439
Iteration 48, loss = 0.02536776
Iteration 49, loss = 0.02550160
Iteration 50, loss = 0.02525837
Iteration 51, loss = 0.02497168
Iteration 52, loss = 0.02499881
Iteration 53, loss = 0.02496206
Iteration 54, loss = 0.02491393
Iteration 55, loss = 0.02505963
Iteration 56, loss = 0.02486746
Iteration 57, loss = 0.02490824
Iteration 58, loss = 0.02464024
Iteration 59, loss = 0.02433403
Iteration 60, loss = 0.02514724
Iteration 61, loss = 0.02439461
Iteration 62, loss = 0.02429750
Iteration 63, loss = 0.02442024
Iteration 64, loss = 0.02419942
Iteration 65, loss = 0.02407321
Iteration 66, loss = 0.02425911
Iteration 67, loss = 0.02419525
Iteration 68, loss = 0.02401523
Iteration 69, loss = 0.02414001
Iteration 70, loss = 0.02407327
Iteration 71, loss = 0.02424163
Iteration 72, loss = 0.02405838
Iteration 73, loss = 0.02392497
Iteration 74, loss = 0.02390784
Iteration 75, loss = 0.02364425
Iteration 76, loss = 0.02361545
Iteration 77, loss = 0.02376339
Iteration 78, loss = 0.02366964
Iteration 79, loss = 0.02390066
Iteration 80, loss = 0.02360821
Iteration 81, loss = 0.02368399
Iteration 82, loss = 0.02356781
Iteration 83, loss = 0.02386511
Iteration 84, loss = 0.02366130
Iteration 85, loss = 0.02353768
Iteration 86, loss = 0.02329171
Iteration 87, loss = 0.02340002
Iteration 88, loss = 0.02347929
Iteration 89, loss = 0.02351112
Iteration 90, loss = 0.02359906
Iteration 91, loss = 0.02333376
Iteration 92, loss = 0.02345884
Iteration 93, loss = 0.02347426
Iteration 94, loss = 0.02319193
Iteration 95, loss = 0.02336679
Iteration 96, loss = 0.02334818
Iteration 97, loss = 0.02314700
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67501906
Iteration 2, loss = 0.61130481
Iteration 3, loss = 0.50212905
Iteration 4, loss = 0.38711858
Iteration 5, loss = 0.28989716
Iteration 6, loss = 0.21696122
Iteration 7, loss = 0.16591510
Iteration 8, loss = 0.13092061
Iteration 9, loss = 0.10677035
Iteration 10, loss = 0.08958748
Iteration 11, loss = 0.07676033
Iteration 12, loss = 0.06732103
Iteration 13, loss = 0.06013167
Iteration 14, loss = 0.05424302
Iteration 15, loss = 0.04966434
Iteration 16, loss = 0.04603911
Iteration 17, loss = 0.04316927
Iteration 18, loss = 0.04062149
Iteration 19, loss = 0.03825123
Iteration 20, loss = 0.03652441
Iteration 21, loss = 0.03498291
Iteration 22, loss = 0.03355895
Iteration 23, loss = 0.03241119
Iteration 24, loss = 0.03158377
Iteration 25, loss = 0.03060620
Iteration 26, loss = 0.02987162
Iteration 27, loss = 0.02885471
Iteration 28, loss = 0.02841768
Iteration 29, loss = 0.02771546
Iteration 30, loss = 0.02736701
Iteration 31, loss = 0.02693664
Iteration 32, loss = 0.02643394
Iteration 33, loss = 0.02595134
Iteration 34, loss = 0.02592854
Iteration 35, loss = 0.02546290
Iteration 36, loss = 0.02525498
Iteration 37, loss = 0.02517106
Iteration 38, loss = 0.02522050
Iteration 39, loss = 0.02437823
Iteration 40, loss = 0.02432161
Iteration 41, loss = 0.02416831
Iteration 42, loss = 0.02403741
Iteration 43, loss = 0.02392381
Iteration 44, loss = 0.02388640
Iteration 45, loss = 0.02355658
Iteration 46, loss = 0.02351138
Iteration 47, loss = 0.02347422
Iteration 48, loss = 0.02305192
Iteration 49, loss = 0.02317364
Iteration 50, loss = 0.02285398
Iteration 51, loss = 0.02274886
Iteration 52, loss = 0.02265462
Iteration 53, loss = 0.02264266
Iteration 54, loss = 0.02250401
Iteration 55, loss = 0.02260323
Iteration 56, loss = 0.02216739
Iteration 57, loss = 0.02234032
Iteration 58, loss = 0.02233564
Iteration 59, loss = 0.02235302
Iteration 60, loss = 0.02242404
Iteration 61, loss = 0.02219875
Iteration 62, loss = 0.02210230
Iteration 63, loss = 0.02193619
Iteration 64, loss = 0.02211350
Iteration 65, loss = 0.02237032
Iteration 66, loss = 0.02224086
Iteration 67, loss = 0.02205910
Iteration 68, loss = 0.02180258
Iteration 69, loss = 0.02179739
Iteration 70, loss = 0.02197193
Iteration 71, loss = 0.02171825
Iteration 72, loss = 0.02172496
Iteration 73, loss = 0.02159917
Iteration 74, loss = 0.02161991
Iteration 75, loss = 0.02134384
Iteration 76, loss = 0.02151510
Iteration 77, loss = 0.02151816
Iteration 78, loss = 0.02155020
Iteration 79, loss = 0.02154656
Iteration 80, loss = 0.02183314
Iteration 81, loss = 0.02138990
Iteration 82, loss = 0.02139372
Iteration 83, loss = 0.02145844
Iteration 84, loss = 0.02133164
Iteration 85, loss = 0.02141014
Iteration 86, loss = 0.02137174
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70610295
Iteration 2, loss = 0.67685968
Iteration 3, loss = 0.64373659
Iteration 4, loss = 0.60479476
Iteration 5, loss = 0.56011526
Iteration 6, loss = 0.51154637
Iteration 7, loss = 0.46087492
Iteration 8, loss = 0.41037623
Iteration 9, loss = 0.36223571
Iteration 10, loss = 0.31799620
Iteration 11, loss = 0.27849898
Iteration 12, loss = 0.24381307
Iteration 13, loss = 0.21386940
Iteration 14, loss = 0.18822660
Iteration 15, loss = 0.16642387
Iteration 16, loss = 0.14793599
Iteration 17, loss = 0.13222795
Iteration 18, loss = 0.11888948
Iteration 19, loss = 0.10757117
Iteration 20, loss = 0.09771081
Iteration 21, loss = 0.08942394
Iteration 22, loss = 0.08212235
Iteration 23, loss = 0.07575855
Iteration 24, loss = 0.07023728
Iteration 25, loss = 0.06536811
Iteration 26, loss = 0.06113837
Iteration 27, loss = 0.05730994
Iteration 28, loss = 0.05392067
Iteration 29, loss = 0.05100568
Iteration 30, loss = 0.04824898
Iteration 31, loss = 0.04591002
Iteration 32, loss = 0.04364772
Iteration 33, loss = 0.04174692
Iteration 34, loss = 0.03973202
Iteration 35, loss = 0.03829384
Iteration 36, loss = 0.03671609
Iteration 37, loss = 0.03541231
Iteration 38, loss = 0.03416817
Iteration 39, loss = 0.03296972
Iteration 40, loss = 0.03194585
Iteration 41, loss = 0.03090991
Iteration 42, loss = 0.02993814
Iteration 43, loss = 0.02915897
Iteration 44, loss = 0.02844861
Iteration 45, loss = 0.02777246
Iteration 46, loss = 0.02697920
Iteration 47, loss = 0.02645483
Iteration 48, loss = 0.02572250
Iteration 49, loss = 0.02524904
Iteration 50, loss = 0.02483676
Iteration 51, loss = 0.02434867
Iteration 52, loss = 0.02397890
Iteration 53, loss = 0.02356514
Iteration 54, loss = 0.02331248
Iteration 55, loss = 0.02271151
Iteration 56, loss = 0.02249651
Iteration 57, loss = 0.02207978
Iteration 58, loss = 0.02167099
Iteration 59, loss = 0.02124120
Iteration 60, loss = 0.02108667
Iteration 61, loss = 0.02070773
Iteration 62, loss = 0.02040461
Iteration 63, loss = 0.02020746
Iteration 64, loss = 0.01992973
Iteration 65, loss = 0.01971084
Iteration 66, loss = 0.01942982
Iteration 67, loss = 0.01913023
Iteration 68, loss = 0.01911366
Iteration 69, loss = 0.01891954
Iteration 70, loss = 0.01874499
Iteration 71, loss = 0.01887478
Iteration 72, loss = 0.01894852
Iteration 73, loss = 0.01847518
Iteration 74, loss = 0.01812493
Iteration 75, loss = 0.01810529
Iteration 76, loss = 0.01791634
Iteration 77, loss = 0.01775045
Iteration 78, loss = 0.01749378
Iteration 79, loss = 0.01742647
Iteration 80, loss = 0.01773444
Iteration 81, loss = 0.01777272
Iteration 82, loss = 0.01749528
Iteration 83, loss = 0.01732474
Iteration 84, loss = 0.01704344
Iteration 85, loss = 0.01714939
Iteration 86, loss = 0.01707893
Iteration 87, loss = 0.01682871
Iteration 88, loss = 0.01663911
Iteration 89, loss = 0.01661412
Iteration 90, loss = 0.01641145
Iteration 91, loss = 0.01665943
Iteration 92, loss = 0.01668784
Iteration 93, loss = 0.01646571
Iteration 94, loss = 0.01643780
Iteration 95, loss = 0.01634760
Iteration 96, loss = 0.01603691
Iteration 97, loss = 0.01601293
Iteration 98, loss = 0.01587654
Iteration 99, loss = 0.01575562
Iteration 100, loss = 0.01580678
Iteration 101, loss = 0.01560929
Iteration 102, loss = 0.01557904
Iteration 103, loss = 0.01555707
Iteration 104, loss = 0.01551106
Iteration 105, loss = 0.01524965
Iteration 106, loss = 0.01553838
Iteration 107, loss = 0.01558689
Iteration 108, loss = 0.01542806
Iteration 109, loss = 0.01540587
Iteration 110, loss = 0.01526623
Iteration 111, loss = 0.01520212
Iteration 112, loss = 0.01509632
Iteration 113, loss = 0.01529724
Iteration 114, loss = 0.01531530
Iteration 115, loss = 0.01516885
Iteration 116, loss = 0.01509297
Iteration 117, loss = 0.01500623
Iteration 118, loss = 0.01493068
Iteration 119, loss = 0.01492677
Iteration 120, loss = 0.01496828
Iteration 121, loss = 0.01481340
Iteration 122, loss = 0.01502143
Iteration 123, loss = 0.01497743
Iteration 124, loss = 0.01500417
Iteration 125, loss = 0.01494951
Iteration 126, loss = 0.01510427
Iteration 127, loss = 0.01475196
Iteration 128, loss = 0.01495606
Iteration 129, loss = 0.01513790
Iteration 130, loss = 0.01471371
Iteration 131, loss = 0.01464046
Iteration 132, loss = 0.01464054
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69849333
Iteration 2, loss = 0.63596396
Iteration 3, loss = 0.54335715
Iteration 4, loss = 0.43446564
Iteration 5, loss = 0.33603918
Iteration 6, loss = 0.25653093
Iteration 7, loss = 0.19737135
Iteration 8, loss = 0.15528317
Iteration 9, loss = 0.12542442
Iteration 10, loss = 0.10398615
Iteration 11, loss = 0.08835003
Iteration 12, loss = 0.07652194
Iteration 13, loss = 0.06744399
Iteration 14, loss = 0.06033608
Iteration 15, loss = 0.05482659
Iteration 16, loss = 0.05037670
Iteration 17, loss = 0.04655147
Iteration 18, loss = 0.04344120
Iteration 19, loss = 0.04083693
Iteration 20, loss = 0.03862140
Iteration 21, loss = 0.03685846
Iteration 22, loss = 0.03505623
Iteration 23, loss = 0.03381173
Iteration 24, loss = 0.03251883
Iteration 25, loss = 0.03172297
Iteration 26, loss = 0.03049390
Iteration 27, loss = 0.02991255
Iteration 28, loss = 0.02877317
Iteration 29, loss = 0.02841713
Iteration 30, loss = 0.02759891
Iteration 31, loss = 0.02733964
Iteration 32, loss = 0.02693527
Iteration 33, loss = 0.02598803
Iteration 34, loss = 0.02569214
Iteration 35, loss = 0.02530764
Iteration 36, loss = 0.02494479
Iteration 37, loss = 0.02472367
Iteration 38, loss = 0.02443446
Iteration 39, loss = 0.02400810
Iteration 40, loss = 0.02392603
Iteration 41, loss = 0.02368545
Iteration 42, loss = 0.02354571
Iteration 43, loss = 0.02346646
Iteration 44, loss = 0.02335190
Iteration 45, loss = 0.02303681
Iteration 46, loss = 0.02289852
Iteration 47, loss = 0.02281895
Iteration 48, loss = 0.02244270
Iteration 49, loss = 0.02281750
Iteration 50, loss = 0.02232327
Iteration 51, loss = 0.02224872
Iteration 52, loss = 0.02205730
Iteration 53, loss = 0.02211637
Iteration 54, loss = 0.02222554
Iteration 55, loss = 0.02185020
Iteration 56, loss = 0.02177833
Iteration 57, loss = 0.02170313
Iteration 58, loss = 0.02158847
Iteration 59, loss = 0.02167255
Iteration 60, loss = 0.02173270
Iteration 61, loss = 0.02138369
Iteration 62, loss = 0.02122220
Iteration 63, loss = 0.02146078
Iteration 64, loss = 0.02129528
Iteration 65, loss = 0.02131760
Iteration 66, loss = 0.02123239
Iteration 67, loss = 0.02099931
Iteration 68, loss = 0.02101685
Iteration 69, loss = 0.02118613
Iteration 70, loss = 0.02083182
Iteration 71, loss = 0.02101718
Iteration 72, loss = 0.02087489
Iteration 73, loss = 0.02098828
Iteration 74, loss = 0.02099142
Iteration 75, loss = 0.02076865
Iteration 76, loss = 0.02059483
Iteration 77, loss = 0.02088846
Iteration 78, loss = 0.02064111
Iteration 79, loss = 0.02046916
Iteration 80, loss = 0.02041974
Iteration 81, loss = 0.02043007
Iteration 82, loss = 0.02060616
Iteration 83, loss = 0.02047977
Iteration 84, loss = 0.02044182
Iteration 85, loss = 0.02024625
Iteration 86, loss = 0.02041474
Iteration 87, loss = 0.02038920
Iteration 88, loss = 0.02032370
Iteration 89, loss = 0.02036584
Iteration 90, loss = 0.02051653
Iteration 91, loss = 0.02064994
Iteration 92, loss = 0.02040593
Iteration 93, loss = 0.02046066
Iteration 94, loss = 0.02045796
Iteration 95, loss = 0.02033402
Iteration 96, loss = 0.02025651
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68275623
Iteration 2, loss = 0.61097720
Iteration 3, loss = 0.50494559
Iteration 4, loss = 0.39428406
Iteration 5, loss = 0.29838958
Iteration 6, loss = 0.22497913
Iteration 7, loss = 0.17310672
Iteration 8, loss = 0.13727081
Iteration 9, loss = 0.11194658
Iteration 10, loss = 0.09398745
Iteration 11, loss = 0.08095498
Iteration 12, loss = 0.07102413
Iteration 13, loss = 0.06346551
Iteration 14, loss = 0.05747512
Iteration 15, loss = 0.05282660
Iteration 16, loss = 0.04910778
Iteration 17, loss = 0.04577706
Iteration 18, loss = 0.04303775
Iteration 19, loss = 0.04085492
Iteration 20, loss = 0.03929699
Iteration 21, loss = 0.03762388
Iteration 22, loss = 0.03635293
Iteration 23, loss = 0.03493530
Iteration 24, loss = 0.03392822
Iteration 25, loss = 0.03323480
Iteration 26, loss = 0.03213560
Iteration 27, loss = 0.03135056
Iteration 28, loss = 0.03063096
Iteration 29, loss = 0.03006584
Iteration 30, loss = 0.02977786
Iteration 31, loss = 0.02919369
Iteration 32, loss = 0.02868431
Iteration 33, loss = 0.02857861
Iteration 34, loss = 0.02824286
Iteration 35, loss = 0.02776994
Iteration 36, loss = 0.02776298
Iteration 37, loss = 0.02708895
Iteration 38, loss = 0.02745081
Iteration 39, loss = 0.02699855
Iteration 40, loss = 0.02690271
Iteration 41, loss = 0.02646495
Iteration 42, loss = 0.02627437
Iteration 43, loss = 0.02623093
Iteration 44, loss = 0.02594396
Iteration 45, loss = 0.02591020
Iteration 46, loss = 0.02568673
Iteration 47, loss = 0.02554805
Iteration 48, loss = 0.02571301
Iteration 49, loss = 0.02545379
Iteration 50, loss = 0.02531957
Iteration 51, loss = 0.02520277
Iteration 52, loss = 0.02520890
Iteration 53, loss = 0.02532253
Iteration 54, loss = 0.02507677
Iteration 55, loss = 0.02487957
Iteration 56, loss = 0.02500244
Iteration 57, loss = 0.02458165
Iteration 58, loss = 0.02488057
Iteration 59, loss = 0.02498274
Iteration 60, loss = 0.02469917
Iteration 61, loss = 0.02448112
Iteration 62, loss = 0.02480192
Iteration 63, loss = 0.02463825
Iteration 64, loss = 0.02432839
Iteration 65, loss = 0.02439904
Iteration 66, loss = 0.02436317
Iteration 67, loss = 0.02437301
Iteration 68, loss = 0.02427648
Iteration 69, loss = 0.02427169
Iteration 70, loss = 0.02431122
Iteration 71, loss = 0.02389527
Iteration 72, loss = 0.02429513
Iteration 73, loss = 0.02406085
Iteration 74, loss = 0.02409098
Iteration 75, loss = 0.02446806
Iteration 76, loss = 0.02407482
Iteration 77, loss = 0.02397006
Iteration 78, loss = 0.02388362
Iteration 79, loss = 0.02363812
Iteration 80, loss = 0.02398221
Iteration 81, loss = 0.02390209
Iteration 82, loss = 0.02381481
Iteration 83, loss = 0.02398812
Iteration 84, loss = 0.02391397
Iteration 85, loss = 0.02373156
Iteration 86, loss = 0.02392831
Iteration 87, loss = 0.02360973
Iteration 88, loss = 0.02389342
Iteration 89, loss = 0.02350453
Iteration 90, loss = 0.02386613
Iteration 91, loss = 0.02389765
Iteration 92, loss = 0.02381644
Iteration 93, loss = 0.02383082
Iteration 94, loss = 0.02353176
Iteration 95, loss = 0.02376491
Iteration 96, loss = 0.02382945
Iteration 97, loss = 0.02365434
Iteration 98, loss = 0.02358502
Iteration 99, loss = 0.02382176
Iteration 100, loss = 0.02345039
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67752096
Iteration 2, loss = 0.64884963
Iteration 3, loss = 0.61011785
Iteration 4, loss = 0.55786982
Iteration 5, loss = 0.49809966
Iteration 6, loss = 0.43872374
Iteration 7, loss = 0.38207298
Iteration 8, loss = 0.33023858
Iteration 9, loss = 0.28387145
Iteration 10, loss = 0.24381020
Iteration 11, loss = 0.20968496
Iteration 12, loss = 0.18076152
Iteration 13, loss = 0.15708229
Iteration 14, loss = 0.13705368
Iteration 15, loss = 0.12065967
Iteration 16, loss = 0.10714062
Iteration 17, loss = 0.09567268
Iteration 18, loss = 0.08616262
Iteration 19, loss = 0.07804393
Iteration 20, loss = 0.07130794
Iteration 21, loss = 0.06543968
Iteration 22, loss = 0.06040659
Iteration 23, loss = 0.05609670
Iteration 24, loss = 0.05229528
Iteration 25, loss = 0.04900303
Iteration 26, loss = 0.04607942
Iteration 27, loss = 0.04347952
Iteration 28, loss = 0.04123761
Iteration 29, loss = 0.03914371
Iteration 30, loss = 0.03737600
Iteration 31, loss = 0.03571514
Iteration 32, loss = 0.03416038
Iteration 33, loss = 0.03281286
Iteration 34, loss = 0.03158911
Iteration 35, loss = 0.03048778
Iteration 36, loss = 0.02950183
Iteration 37, loss = 0.02872520
Iteration 38, loss = 0.02764776
Iteration 39, loss = 0.02702791
Iteration 40, loss = 0.02626681
Iteration 41, loss = 0.02561580
Iteration 42, loss = 0.02504928
Iteration 43, loss = 0.02438416
Iteration 44, loss = 0.02388043
Iteration 45, loss = 0.02340795
Iteration 46, loss = 0.02289850
Iteration 47, loss = 0.02256917
Iteration 48, loss = 0.02206836
Iteration 49, loss = 0.02175918
Iteration 50, loss = 0.02140930
Iteration 51, loss = 0.02112073
Iteration 52, loss = 0.02073748
Iteration 53, loss = 0.02036781
Iteration 54, loss = 0.02020898
Iteration 55, loss = 0.02006718
Iteration 56, loss = 0.01975126
Iteration 57, loss = 0.01952967
Iteration 58, loss = 0.01921053
Iteration 59, loss = 0.01893433
Iteration 60, loss = 0.01871988
Iteration 61, loss = 0.01870370
Iteration 62, loss = 0.01857910
Iteration 63, loss = 0.01830154
Iteration 64, loss = 0.01814464
Iteration 65, loss = 0.01796484
Iteration 66, loss = 0.01777747
Iteration 67, loss = 0.01768332
Iteration 68, loss = 0.01754719
Iteration 69, loss = 0.01773507
Iteration 70, loss = 0.01729642
Iteration 71, loss = 0.01728155
Iteration 72, loss = 0.01739872
Iteration 73, loss = 0.01697537
Iteration 74, loss = 0.01696550
Iteration 75, loss = 0.01672750
Iteration 76, loss = 0.01688952
Iteration 77, loss = 0.01677727
Iteration 78, loss = 0.01657687
Iteration 79, loss = 0.01656728
Iteration 80, loss = 0.01643501
Iteration 81, loss = 0.01636463
Iteration 82, loss = 0.01624799
Iteration 83, loss = 0.01613318
Iteration 84, loss = 0.01612906
Iteration 85, loss = 0.01600117
Iteration 86, loss = 0.01605933
Iteration 87, loss = 0.01600341
Iteration 88, loss = 0.01582470
Iteration 89, loss = 0.01589434
Iteration 90, loss = 0.01576907
Iteration 91, loss = 0.01565236
Iteration 92, loss = 0.01565078
Iteration 93, loss = 0.01566173
Iteration 94, loss = 0.01556577
Iteration 95, loss = 0.01550065
Iteration 96, loss = 0.01550185
Iteration 97, loss = 0.01544120
Iteration 98, loss = 0.01545929
Iteration 99, loss = 0.01554506
Iteration 100, loss = 0.01550868
Iteration 101, loss = 0.01528036
Iteration 102, loss = 0.01529546
Iteration 103, loss = 0.01511005
Iteration 104, loss = 0.01511528
Iteration 105, loss = 0.01526638
Iteration 106, loss = 0.01503040
Iteration 107, loss = 0.01500038
Iteration 108, loss = 0.01505871
Iteration 109, loss = 0.01509070
Iteration 110, loss = 0.01501284
Iteration 111, loss = 0.01504317
Iteration 112, loss = 0.01503202
Iteration 113, loss = 0.01500944
Iteration 114, loss = 0.01505921
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67588132
Iteration 2, loss = 0.64795070
Iteration 3, loss = 0.61766255
Iteration 4, loss = 0.58068959
Iteration 5, loss = 0.53667160
Iteration 6, loss = 0.48906704
Iteration 7, loss = 0.43996707
Iteration 8, loss = 0.39237843
Iteration 9, loss = 0.34738363
Iteration 10, loss = 0.30612308
Iteration 11, loss = 0.26932606
Iteration 12, loss = 0.23684601
Iteration 13, loss = 0.20875198
Iteration 14, loss = 0.18443320
Iteration 15, loss = 0.16378970
Iteration 16, loss = 0.14615791
Iteration 17, loss = 0.13087899
Iteration 18, loss = 0.11794385
Iteration 19, loss = 0.10692873
Iteration 20, loss = 0.09737339
Iteration 21, loss = 0.08923296
Iteration 22, loss = 0.08210824
Iteration 23, loss = 0.07585000
Iteration 24, loss = 0.07034473
Iteration 25, loss = 0.06546205
Iteration 26, loss = 0.06117755
Iteration 27, loss = 0.05742827
Iteration 28, loss = 0.05415989
Iteration 29, loss = 0.05112507
Iteration 30, loss = 0.04836418
Iteration 31, loss = 0.04584529
Iteration 32, loss = 0.04362609
Iteration 33, loss = 0.04144901
Iteration 34, loss = 0.03989499
Iteration 35, loss = 0.03821554
Iteration 36, loss = 0.03657965
Iteration 37, loss = 0.03527720
Iteration 38, loss = 0.03401837
Iteration 39, loss = 0.03274504
Iteration 40, loss = 0.03159542
Iteration 41, loss = 0.03055942
Iteration 42, loss = 0.02961080
Iteration 43, loss = 0.02871603
Iteration 44, loss = 0.02792483
Iteration 45, loss = 0.02715313
Iteration 46, loss = 0.02645823
Iteration 47, loss = 0.02583095
Iteration 48, loss = 0.02525237
Iteration 49, loss = 0.02468302
Iteration 50, loss = 0.02412873
Iteration 51, loss = 0.02351588
Iteration 52, loss = 0.02318277
Iteration 53, loss = 0.02263427
Iteration 54, loss = 0.02223196
Iteration 55, loss = 0.02174899
Iteration 56, loss = 0.02123546
Iteration 57, loss = 0.02088386
Iteration 58, loss = 0.02058760
Iteration 59, loss = 0.02025200
Iteration 60, loss = 0.01994165
Iteration 61, loss = 0.01964104
Iteration 62, loss = 0.01935710
Iteration 63, loss = 0.01915836
Iteration 64, loss = 0.01888838
Iteration 65, loss = 0.01867787
Iteration 66, loss = 0.01845229
Iteration 67, loss = 0.01820819
Iteration 68, loss = 0.01798187
Iteration 69, loss = 0.01781127
Iteration 70, loss = 0.01771980
Iteration 71, loss = 0.01812716
Iteration 72, loss = 0.01774355
Iteration 73, loss = 0.01749123
Iteration 74, loss = 0.01727333
Iteration 75, loss = 0.01703226
Iteration 76, loss = 0.01679737
Iteration 77, loss = 0.01657351
Iteration 78, loss = 0.01645381
Iteration 79, loss = 0.01630220
Iteration 80, loss = 0.01615065
Iteration 81, loss = 0.01617160
Iteration 82, loss = 0.01619165
Iteration 83, loss = 0.01624433
Iteration 84, loss = 0.01610015
Iteration 85, loss = 0.01595057
Iteration 86, loss = 0.01570743
Iteration 87, loss = 0.01552020
Iteration 88, loss = 0.01536225
Iteration 89, loss = 0.01529641
Iteration 90, loss = 0.01518039
Iteration 91, loss = 0.01509278
Iteration 92, loss = 0.01505653
Iteration 93, loss = 0.01495679
Iteration 94, loss = 0.01487099
Iteration 95, loss = 0.01482711
Iteration 96, loss = 0.01476441
Iteration 97, loss = 0.01467692
Iteration 98, loss = 0.01475648
Iteration 99, loss = 0.01467690
Iteration 100, loss = 0.01453596
Iteration 101, loss = 0.01448565
Iteration 102, loss = 0.01433286
Iteration 103, loss = 0.01427634
Iteration 104, loss = 0.01422489
Iteration 105, loss = 0.01420307
Iteration 106, loss = 0.01412234
Iteration 107, loss = 0.01396510
Iteration 108, loss = 0.01399384
Iteration 109, loss = 0.01419465
Iteration 110, loss = 0.01410050
Iteration 111, loss = 0.01406644
Iteration 112, loss = 0.01398093
Iteration 113, loss = 0.01396054
Iteration 114, loss = 0.01404060
Iteration 115, loss = 0.01397692
Iteration 116, loss = 0.01391978
Iteration 117, loss = 0.01400294
Iteration 118, loss = 0.01393280
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70137271
Iteration 2, loss = 0.62788687
Iteration 3, loss = 0.52495416
Iteration 4, loss = 0.41050307
Iteration 5, loss = 0.31109651
Iteration 6, loss = 0.23547969
Iteration 7, loss = 0.18109440
Iteration 8, loss = 0.14323501
Iteration 9, loss = 0.11671314
Iteration 10, loss = 0.09736274
Iteration 11, loss = 0.08322941
Iteration 12, loss = 0.07290711
Iteration 13, loss = 0.06465786
Iteration 14, loss = 0.05806316
Iteration 15, loss = 0.05260827
Iteration 16, loss = 0.04853755
Iteration 17, loss = 0.04505792
Iteration 18, loss = 0.04207606
Iteration 19, loss = 0.03980770
Iteration 20, loss = 0.03758087
Iteration 21, loss = 0.03596092
Iteration 22, loss = 0.03431700
Iteration 23, loss = 0.03309096
Iteration 24, loss = 0.03237240
Iteration 25, loss = 0.03074628
Iteration 26, loss = 0.03010207
Iteration 27, loss = 0.02924337
Iteration 28, loss = 0.02882237
Iteration 29, loss = 0.02826097
Iteration 30, loss = 0.02726473
Iteration 31, loss = 0.02712522
Iteration 32, loss = 0.02627003
Iteration 33, loss = 0.02599438
Iteration 34, loss = 0.02550197
Iteration 35, loss = 0.02523446
Iteration 36, loss = 0.02488341
Iteration 37, loss = 0.02434166
Iteration 38, loss = 0.02416897
Iteration 39, loss = 0.02398149
Iteration 40, loss = 0.02373211
Iteration 41, loss = 0.02355204
Iteration 42, loss = 0.02317359
Iteration 43, loss = 0.02315797
Iteration 44, loss = 0.02302155
Iteration 45, loss = 0.02289871
Iteration 46, loss = 0.02267786
Iteration 47, loss = 0.02264640
Iteration 48, loss = 0.02227678
Iteration 49, loss = 0.02230401
Iteration 50, loss = 0.02225574
Iteration 51, loss = 0.02211526
Iteration 52, loss = 0.02189655
Iteration 53, loss = 0.02192853
Iteration 54, loss = 0.02214332
Iteration 55, loss = 0.02195937
Iteration 56, loss = 0.02185745
Iteration 57, loss = 0.02164428
Iteration 58, loss = 0.02143227
Iteration 59, loss = 0.02133808
Iteration 60, loss = 0.02149007
Iteration 61, loss = 0.02146004
Iteration 62, loss = 0.02116989
Iteration 63, loss = 0.02121872
Iteration 64, loss = 0.02096607
Iteration 65, loss = 0.02114200
Iteration 66, loss = 0.02122525
Iteration 67, loss = 0.02103107
Iteration 68, loss = 0.02093252
Iteration 69, loss = 0.02097697
Iteration 70, loss = 0.02072653
Iteration 71, loss = 0.02077589
Iteration 72, loss = 0.02080662
Iteration 73, loss = 0.02073418
Iteration 74, loss = 0.02098114
Iteration 75, loss = 0.02066129
Iteration 76, loss = 0.02059531
Iteration 77, loss = 0.02052032
Iteration 78, loss = 0.02047616
Iteration 79, loss = 0.02042311
Iteration 80, loss = 0.02082433
Iteration 81, loss = 0.02040771
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70203751
Iteration 2, loss = 0.66630676
Iteration 3, loss = 0.62308415
Iteration 4, loss = 0.57171012
Iteration 5, loss = 0.51240420
Iteration 6, loss = 0.45070340
Iteration 7, loss = 0.39135290
Iteration 8, loss = 0.33659056
Iteration 9, loss = 0.28780636
Iteration 10, loss = 0.24580933
Iteration 11, loss = 0.21009656
Iteration 12, loss = 0.18075711
Iteration 13, loss = 0.15613053
Iteration 14, loss = 0.13610476
Iteration 15, loss = 0.11963338
Iteration 16, loss = 0.10590269
Iteration 17, loss = 0.09452138
Iteration 18, loss = 0.08507568
Iteration 19, loss = 0.07706037
Iteration 20, loss = 0.07026019
Iteration 21, loss = 0.06446280
Iteration 22, loss = 0.05964067
Iteration 23, loss = 0.05532758
Iteration 24, loss = 0.05150448
Iteration 25, loss = 0.04835043
Iteration 26, loss = 0.04543825
Iteration 27, loss = 0.04302995
Iteration 28, loss = 0.04088231
Iteration 29, loss = 0.03859619
Iteration 30, loss = 0.03682623
Iteration 31, loss = 0.03524802
Iteration 32, loss = 0.03370950
Iteration 33, loss = 0.03241488
Iteration 34, loss = 0.03117594
Iteration 35, loss = 0.03004787
Iteration 36, loss = 0.02908845
Iteration 37, loss = 0.02824161
Iteration 38, loss = 0.02743893
Iteration 39, loss = 0.02665999
Iteration 40, loss = 0.02589863
Iteration 41, loss = 0.02533700
Iteration 42, loss = 0.02459340
Iteration 43, loss = 0.02411161
Iteration 44, loss = 0.02352022
Iteration 45, loss = 0.02305014
Iteration 46, loss = 0.02266140
Iteration 47, loss = 0.02216643
Iteration 48, loss = 0.02189979
Iteration 49, loss = 0.02142078
Iteration 50, loss = 0.02117768
Iteration 51, loss = 0.02093883
Iteration 52, loss = 0.02074546
Iteration 53, loss = 0.02021719
Iteration 54, loss = 0.01997304
Iteration 55, loss = 0.01969172
Iteration 56, loss = 0.01940192
Iteration 57, loss = 0.01923477
Iteration 58, loss = 0.01900511
Iteration 59, loss = 0.01876177
Iteration 60, loss = 0.01859625
Iteration 61, loss = 0.01840231
Iteration 62, loss = 0.01823443
Iteration 63, loss = 0.01813780
Iteration 64, loss = 0.01813540
Iteration 65, loss = 0.01782337
Iteration 66, loss = 0.01754678
Iteration 67, loss = 0.01746595
Iteration 68, loss = 0.01733196
Iteration 69, loss = 0.01732592
Iteration 70, loss = 0.01735360
Iteration 71, loss = 0.01710633
Iteration 72, loss = 0.01688045
Iteration 73, loss = 0.01680855
Iteration 74, loss = 0.01666952
Iteration 75, loss = 0.01673184
Iteration 76, loss = 0.01652151
Iteration 77, loss = 0.01638708
Iteration 78, loss = 0.01636095
Iteration 79, loss = 0.01614750
Iteration 80, loss = 0.01622658
Iteration 81, loss = 0.01602187
Iteration 82, loss = 0.01603097
Iteration 83, loss = 0.01614445
Iteration 84, loss = 0.01600555
Iteration 85, loss = 0.01588203
Iteration 86, loss = 0.01584828
Iteration 87, loss = 0.01572232
Iteration 88, loss = 0.01564585
Iteration 89, loss = 0.01559999
Iteration 90, loss = 0.01554129
Iteration 91, loss = 0.01544086
Iteration 92, loss = 0.01555752
Iteration 93, loss = 0.01556109
Iteration 94, loss = 0.01539282
Iteration 95, loss = 0.01523542
Iteration 96, loss = 0.01530689
Iteration 97, loss = 0.01527983
Iteration 98, loss = 0.01529642
Iteration 99, loss = 0.01530398
Iteration 100, loss = 0.01523062
Iteration 101, loss = 0.01510515
Iteration 102, loss = 0.01499804
Iteration 103, loss = 0.01513579
Iteration 104, loss = 0.01499147
Iteration 105, loss = 0.01496653
Iteration 106, loss = 0.01496482
Iteration 107, loss = 0.01496410
Iteration 108, loss = 0.01496317
Iteration 109, loss = 0.01488870
Iteration 110, loss = 0.01491073
Iteration 111, loss = 0.01475602
Iteration 112, loss = 0.01482933
Iteration 113, loss = 0.01466169
Iteration 114, loss = 0.01474064
Iteration 115, loss = 0.01468343
Iteration 116, loss = 0.01483240
Iteration 117, loss = 0.01454989
Iteration 118, loss = 0.01459896
Iteration 119, loss = 0.01460251
Iteration 120, loss = 0.01464148
Iteration 121, loss = 0.01468800
Iteration 122, loss = 0.01455054
Iteration 123, loss = 0.01445476
Iteration 124, loss = 0.01447476
Iteration 125, loss = 0.01454671
Iteration 126, loss = 0.01440125
Iteration 127, loss = 0.01447324
Iteration 128, loss = 0.01451519
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67669605
Iteration 2, loss = 0.59783750
Iteration 3, loss = 0.48817462
Iteration 4, loss = 0.37517841
Iteration 5, loss = 0.28011443
Iteration 6, loss = 0.21023780
Iteration 7, loss = 0.16158617
Iteration 8, loss = 0.12797961
Iteration 9, loss = 0.10448600
Iteration 10, loss = 0.08770815
Iteration 11, loss = 0.07543456
Iteration 12, loss = 0.06624083
Iteration 13, loss = 0.05912772
Iteration 14, loss = 0.05326418
Iteration 15, loss = 0.04905322
Iteration 16, loss = 0.04549287
Iteration 17, loss = 0.04208589
Iteration 18, loss = 0.03975651
Iteration 19, loss = 0.03773935
Iteration 20, loss = 0.03583690
Iteration 21, loss = 0.03452040
Iteration 22, loss = 0.03285878
Iteration 23, loss = 0.03185097
Iteration 24, loss = 0.03068288
Iteration 25, loss = 0.03004149
Iteration 26, loss = 0.02936687
Iteration 27, loss = 0.02853192
Iteration 28, loss = 0.02781756
Iteration 29, loss = 0.02736902
Iteration 30, loss = 0.02711912
Iteration 31, loss = 0.02620065
Iteration 32, loss = 0.02594975
Iteration 33, loss = 0.02562707
Iteration 34, loss = 0.02527576
Iteration 35, loss = 0.02493133
Iteration 36, loss = 0.02491390
Iteration 37, loss = 0.02436345
Iteration 38, loss = 0.02439450
Iteration 39, loss = 0.02427659
Iteration 40, loss = 0.02402775
Iteration 41, loss = 0.02360599
Iteration 42, loss = 0.02365796
Iteration 43, loss = 0.02362404
Iteration 44, loss = 0.02346588
Iteration 45, loss = 0.02323504
Iteration 46, loss = 0.02312946
Iteration 47, loss = 0.02279429
Iteration 48, loss = 0.02267432
Iteration 49, loss = 0.02260601
Iteration 50, loss = 0.02261971
Iteration 51, loss = 0.02277308
Iteration 52, loss = 0.02243347
Iteration 53, loss = 0.02213769
Iteration 54, loss = 0.02222002
Iteration 55, loss = 0.02179916
Iteration 56, loss = 0.02205498
Iteration 57, loss = 0.02205185
Iteration 58, loss = 0.02220950
Iteration 59, loss = 0.02200276
Iteration 60, loss = 0.02199655
Iteration 61, loss = 0.02146061
Iteration 62, loss = 0.02178055
Iteration 63, loss = 0.02173761
Iteration 64, loss = 0.02180847
Iteration 65, loss = 0.02173720
Iteration 66, loss = 0.02176042
Iteration 67, loss = 0.02169629
Iteration 68, loss = 0.02141434
Iteration 69, loss = 0.02177627
Iteration 70, loss = 0.02155242
Iteration 71, loss = 0.02120584
Iteration 72, loss = 0.02146397
Iteration 73, loss = 0.02159171
Iteration 74, loss = 0.02154060
Iteration 75, loss = 0.02161611
Iteration 76, loss = 0.02136322
Iteration 77, loss = 0.02105727
Iteration 78, loss = 0.02134514
Iteration 79, loss = 0.02114370
Iteration 80, loss = 0.02131897
Iteration 81, loss = 0.02120189
Iteration 82, loss = 0.02107401
Iteration 83, loss = 0.02101359
Iteration 84, loss = 0.02082548
Iteration 85, loss = 0.02114866
Iteration 86, loss = 0.02127820
Iteration 87, loss = 0.02124893
Iteration 88, loss = 0.02109037
Iteration 89, loss = 0.02103350
Iteration 90, loss = 0.02097947
Iteration 91, loss = 0.02126399
Iteration 92, loss = 0.02093863
Iteration 93, loss = 0.02104867
Iteration 94, loss = 0.02086759
Iteration 95, loss = 0.02100804
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69416107
Iteration 2, loss = 0.65114995
Iteration 3, loss = 0.59473643
Iteration 4, loss = 0.52683265
Iteration 5, loss = 0.45581899
Iteration 6, loss = 0.38923301
Iteration 7, loss = 0.32851798
Iteration 8, loss = 0.27609452
Iteration 9, loss = 0.23211930
Iteration 10, loss = 0.19561691
Iteration 11, loss = 0.16611667
Iteration 12, loss = 0.14254405
Iteration 13, loss = 0.12335242
Iteration 14, loss = 0.10801094
Iteration 15, loss = 0.09559607
Iteration 16, loss = 0.08524286
Iteration 17, loss = 0.07669333
Iteration 18, loss = 0.06968207
Iteration 19, loss = 0.06365300
Iteration 20, loss = 0.05876802
Iteration 21, loss = 0.05442577
Iteration 22, loss = 0.05087322
Iteration 23, loss = 0.04755890
Iteration 24, loss = 0.04481063
Iteration 25, loss = 0.04250728
Iteration 26, loss = 0.04023061
Iteration 27, loss = 0.03836727
Iteration 28, loss = 0.03665279
Iteration 29, loss = 0.03516658
Iteration 30, loss = 0.03379287
Iteration 31, loss = 0.03266321
Iteration 32, loss = 0.03152118
Iteration 33, loss = 0.03056359
Iteration 34, loss = 0.02986660
Iteration 35, loss = 0.02883870
Iteration 36, loss = 0.02812203
Iteration 37, loss = 0.02746144
Iteration 38, loss = 0.02686047
Iteration 39, loss = 0.02631862
Iteration 40, loss = 0.02555224
Iteration 41, loss = 0.02527195
Iteration 42, loss = 0.02472429
Iteration 43, loss = 0.02453524
Iteration 44, loss = 0.02409420
Iteration 45, loss = 0.02357050
Iteration 46, loss = 0.02331294
Iteration 47, loss = 0.02299696
Iteration 48, loss = 0.02260187
Iteration 49, loss = 0.02236878
Iteration 50, loss = 0.02235375
Iteration 51, loss = 0.02179908
Iteration 52, loss = 0.02166720
Iteration 53, loss = 0.02144054
Iteration 54, loss = 0.02107550
Iteration 55, loss = 0.02114584
Iteration 56, loss = 0.02085878
Iteration 57, loss = 0.02083821
Iteration 58, loss = 0.02058345
Iteration 59, loss = 0.02034613
Iteration 60, loss = 0.02037325
Iteration 61, loss = 0.02006413
Iteration 62, loss = 0.02012250
Iteration 63, loss = 0.01972257
Iteration 64, loss = 0.01976346
Iteration 65, loss = 0.01985262
Iteration 66, loss = 0.01961798
Iteration 67, loss = 0.01943774
Iteration 68, loss = 0.01919710
Iteration 69, loss = 0.01925909
Iteration 70, loss = 0.01911151
Iteration 71, loss = 0.01904985
Iteration 72, loss = 0.01893438
Iteration 73, loss = 0.01895621
Iteration 74, loss = 0.01884966
Iteration 75, loss = 0.01874851
Iteration 76, loss = 0.01869605
Iteration 77, loss = 0.01870328
Iteration 78, loss = 0.01853875
Iteration 79, loss = 0.01855028
Iteration 80, loss = 0.01855296
Iteration 81, loss = 0.01833681
Iteration 82, loss = 0.01847974
Iteration 83, loss = 0.01825870
Iteration 84, loss = 0.01821853
Iteration 85, loss = 0.01821040
Iteration 86, loss = 0.01807149
Iteration 87, loss = 0.01809938
Iteration 88, loss = 0.01819460
Iteration 89, loss = 0.01795066
Iteration 90, loss = 0.01792440
Iteration 91, loss = 0.01805508
Iteration 92, loss = 0.01805447
Iteration 93, loss = 0.01788721
Iteration 94, loss = 0.01781769
Iteration 95, loss = 0.01793745
Iteration 96, loss = 0.01797451
Iteration 97, loss = 0.01768089
Iteration 98, loss = 0.01772681
Iteration 99, loss = 0.01785279
Iteration 100, loss = 0.01776490
Iteration 101, loss = 0.01764580
Iteration 102, loss = 0.01750315
Iteration 103, loss = 0.01756005
Iteration 104, loss = 0.01756340
Iteration 105, loss = 0.01737757
Iteration 106, loss = 0.01754371
Iteration 107, loss = 0.01752081
Iteration 108, loss = 0.01735763
Iteration 109, loss = 0.01750586
Iteration 110, loss = 0.01746983
Iteration 111, loss = 0.01752766
Iteration 112, loss = 0.01735278
Iteration 113, loss = 0.01743122
Iteration 114, loss = 0.01721066
Iteration 115, loss = 0.01733105
Iteration 116, loss = 0.01719719
Iteration 117, loss = 0.01737120
Iteration 118, loss = 0.01722749
Iteration 119, loss = 0.01722472
Iteration 120, loss = 0.01743522
Iteration 121, loss = 0.01729280
Iteration 122, loss = 0.01710633
Iteration 123, loss = 0.01713682
Iteration 124, loss = 0.01737739
Iteration 125, loss = 0.01710795
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68216769
Iteration 2, loss = 0.62213075
Iteration 3, loss = 0.53757149
Iteration 4, loss = 0.44118759
Iteration 5, loss = 0.35172976
Iteration 6, loss = 0.27614125
Iteration 7, loss = 0.21714172
Iteration 8, loss = 0.17291410
Iteration 9, loss = 0.14128606
Iteration 10, loss = 0.11830757
Iteration 11, loss = 0.10086236
Iteration 12, loss = 0.08769418
Iteration 13, loss = 0.07724407
Iteration 14, loss = 0.06900528
Iteration 15, loss = 0.06257724
Iteration 16, loss = 0.05716993
Iteration 17, loss = 0.05289213
Iteration 18, loss = 0.04938449
Iteration 19, loss = 0.04627783
Iteration 20, loss = 0.04360138
Iteration 21, loss = 0.04145114
Iteration 22, loss = 0.03958881
Iteration 23, loss = 0.03768279
Iteration 24, loss = 0.03628052
Iteration 25, loss = 0.03528124
Iteration 26, loss = 0.03409820
Iteration 27, loss = 0.03293473
Iteration 28, loss = 0.03203656
Iteration 29, loss = 0.03123319
Iteration 30, loss = 0.03042039
Iteration 31, loss = 0.02983413
Iteration 32, loss = 0.02930796
Iteration 33, loss = 0.02856483
Iteration 34, loss = 0.02810722
Iteration 35, loss = 0.02769499
Iteration 36, loss = 0.02740995
Iteration 37, loss = 0.02686712
Iteration 38, loss = 0.02641498
Iteration 39, loss = 0.02603614
Iteration 40, loss = 0.02594434
Iteration 41, loss = 0.02559782
Iteration 42, loss = 0.02546358
Iteration 43, loss = 0.02628085
Iteration 44, loss = 0.02693222
Iteration 45, loss = 0.02601372
Iteration 46, loss = 0.02552346
Iteration 47, loss = 0.02584595
Iteration 48, loss = 0.02586960
Iteration 49, loss = 0.02569791
Iteration 50, loss = 0.02501912
Iteration 51, loss = 0.02541302
Iteration 52, loss = 0.02517093
Iteration 53, loss = 0.02514725
Iteration 54, loss = 0.02475007
Iteration 55, loss = 0.02448000
Iteration 56, loss = 0.02408299
Iteration 57, loss = 0.02669919
Iteration 58, loss = 0.02782008
Iteration 59, loss = 0.02707392
Iteration 60, loss = 0.02617277
Iteration 61, loss = 0.02570636
Iteration 62, loss = 0.02494476
Iteration 63, loss = 0.02434290
Iteration 64, loss = 0.02415510
Iteration 65, loss = 0.02399751
Iteration 66, loss = 0.02381410
Iteration 67, loss = 0.02332806
Iteration 68, loss = 0.02303008
Iteration 69, loss = 0.02287865
Iteration 70, loss = 0.02276152
Iteration 71, loss = 0.02267445
Iteration 72, loss = 0.02337020
Iteration 73, loss = 0.02368791
Iteration 74, loss = 0.02315145
Iteration 75, loss = 0.02275564
Iteration 76, loss = 0.02266993
Iteration 77, loss = 0.02262797
Iteration 78, loss = 0.02246033
Iteration 79, loss = 0.02236528
Iteration 80, loss = 0.02236709
Iteration 81, loss = 0.02232421
Iteration 82, loss = 0.02203981
Iteration 83, loss = 0.02204570
Iteration 84, loss = 0.02190505
Iteration 85, loss = 0.02213002
Iteration 86, loss = 0.02201475
Iteration 87, loss = 0.02179964
Iteration 88, loss = 0.02167849
Iteration 89, loss = 0.02164803
Iteration 90, loss = 0.02168220
Iteration 91, loss = 0.02155489
Iteration 92, loss = 0.02138831
Iteration 93, loss = 0.02141933
Iteration 94, loss = 0.02141203
Iteration 95, loss = 0.02133445
Iteration 96, loss = 0.02150965
Iteration 97, loss = 0.02136916
Iteration 98, loss = 0.02120786
Iteration 99, loss = 0.02112752
Iteration 100, loss = 0.02116780
Iteration 101, loss = 0.02117544
Iteration 102, loss = 0.02122470
Iteration 103, loss = 0.02096833
Iteration 104, loss = 0.02085512
Iteration 105, loss = 0.02104739
Iteration 106, loss = 0.02097581
Iteration 107, loss = 0.02097047
Iteration 108, loss = 0.02082242
Iteration 109, loss = 0.02063159
Iteration 110, loss = 0.02081273
Iteration 111, loss = 0.02075701
Iteration 112, loss = 0.02072803
Iteration 113, loss = 0.02060619
Iteration 114, loss = 0.02088238
Iteration 115, loss = 0.02081361
Iteration 116, loss = 0.02047390
Iteration 117, loss = 0.02058303
Iteration 118, loss = 0.02036025
Iteration 119, loss = 0.02060018
Iteration 120, loss = 0.02057359
Iteration 121, loss = 0.02054711
Iteration 122, loss = 0.02032105
Iteration 123, loss = 0.02073121
Iteration 124, loss = 0.02034340
Iteration 125, loss = 0.02030489
Iteration 126, loss = 0.02277909
Iteration 127, loss = 0.02293811
Iteration 128, loss = 0.02240977
Iteration 129, loss = 0.02218929
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67836641
Iteration 2, loss = 0.64388866
Iteration 3, loss = 0.59660562
Iteration 4, loss = 0.53292126
Iteration 5, loss = 0.46229749
Iteration 6, loss = 0.39365842
Iteration 7, loss = 0.33077269
Iteration 8, loss = 0.27639620
Iteration 9, loss = 0.23070809
Iteration 10, loss = 0.19331698
Iteration 11, loss = 0.16333488
Iteration 12, loss = 0.13940957
Iteration 13, loss = 0.12036707
Iteration 14, loss = 0.10495940
Iteration 15, loss = 0.09262108
Iteration 16, loss = 0.08242923
Iteration 17, loss = 0.07405908
Iteration 18, loss = 0.06706361
Iteration 19, loss = 0.06137117
Iteration 20, loss = 0.05646782
Iteration 21, loss = 0.05226744
Iteration 22, loss = 0.04859131
Iteration 23, loss = 0.04551347
Iteration 24, loss = 0.04272489
Iteration 25, loss = 0.04030817
Iteration 26, loss = 0.03812285
Iteration 27, loss = 0.03644192
Iteration 28, loss = 0.03473629
Iteration 29, loss = 0.03325608
Iteration 30, loss = 0.03200473
Iteration 31, loss = 0.03074729
Iteration 32, loss = 0.02962090
Iteration 33, loss = 0.02882602
Iteration 34, loss = 0.02780527
Iteration 35, loss = 0.02710749
Iteration 36, loss = 0.02619008
Iteration 37, loss = 0.02554963
Iteration 38, loss = 0.02502492
Iteration 39, loss = 0.02438454
Iteration 40, loss = 0.02391737
Iteration 41, loss = 0.02349998
Iteration 42, loss = 0.02289196
Iteration 43, loss = 0.02248316
Iteration 44, loss = 0.02227996
Iteration 45, loss = 0.02186847
Iteration 46, loss = 0.02152898
Iteration 47, loss = 0.02119581
Iteration 48, loss = 0.02084243
Iteration 49, loss = 0.02053111
Iteration 50, loss = 0.02051696
Iteration 51, loss = 0.01997715
Iteration 52, loss = 0.01979325
Iteration 53, loss = 0.01956815
Iteration 54, loss = 0.01943494
Iteration 55, loss = 0.01939797
Iteration 56, loss = 0.01906989
Iteration 57, loss = 0.01886207
Iteration 58, loss = 0.01862726
Iteration 59, loss = 0.01853492
Iteration 60, loss = 0.01826090
Iteration 61, loss = 0.01811356
Iteration 62, loss = 0.01807120
Iteration 63, loss = 0.01796333
Iteration 64, loss = 0.01785067
Iteration 65, loss = 0.01777969
Iteration 66, loss = 0.01768200
Iteration 67, loss = 0.01747512
Iteration 68, loss = 0.01756726
Iteration 69, loss = 0.01725975
Iteration 70, loss = 0.01718625
Iteration 71, loss = 0.01713618
Iteration 72, loss = 0.01699411
Iteration 73, loss = 0.01706127
Iteration 74, loss = 0.01688852
Iteration 75, loss = 0.01688703
Iteration 76, loss = 0.01672986
Iteration 77, loss = 0.01667823
Iteration 78, loss = 0.01661970
Iteration 79, loss = 0.01662441
Iteration 80, loss = 0.01647437
Iteration 81, loss = 0.01662407
Iteration 82, loss = 0.01639138
Iteration 83, loss = 0.01634242
Iteration 84, loss = 0.01631729
Iteration 85, loss = 0.01631443
Iteration 86, loss = 0.01615915
Iteration 87, loss = 0.01611468
Iteration 88, loss = 0.01610379
Iteration 89, loss = 0.01607569
Iteration 90, loss = 0.01607545
Iteration 91, loss = 0.01581055
Iteration 92, loss = 0.01590908
Iteration 93, loss = 0.01588188
Iteration 94, loss = 0.01589483
Iteration 95, loss = 0.01587289
Iteration 96, loss = 0.01584849
Iteration 97, loss = 0.01592919
Iteration 98, loss = 0.01565555
Iteration 99, loss = 0.01573784
Iteration 100, loss = 0.01569131
Iteration 101, loss = 0.01566390
Iteration 102, loss = 0.01572840
Iteration 103, loss = 0.01550602
Iteration 104, loss = 0.01557805
Iteration 105, loss = 0.01560591
Iteration 106, loss = 0.01548553
Iteration 107, loss = 0.01544216
Iteration 108, loss = 0.01551609
Iteration 109, loss = 0.01560970
Iteration 110, loss = 0.01529969
Iteration 111, loss = 0.01542217
Iteration 112, loss = 0.01538992
Iteration 113, loss = 0.01542589
Iteration 114, loss = 0.01526337
Iteration 115, loss = 0.01531095
Iteration 116, loss = 0.01527493
Iteration 117, loss = 0.01528168
Iteration 118, loss = 0.01526335
Iteration 119, loss = 0.01513716
Iteration 120, loss = 0.01525357
Iteration 121, loss = 0.01524419
Iteration 122, loss = 0.01514274
Iteration 123, loss = 0.01507782
Iteration 124, loss = 0.01516302
Iteration 125, loss = 0.01516770
Iteration 126, loss = 0.01508590
Iteration 127, loss = 0.01517850
Iteration 128, loss = 0.01506579
Iteration 129, loss = 0.01521501
Iteration 130, loss = 0.01497606
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69597017
Iteration 2, loss = 0.61982262
Iteration 3, loss = 0.50869409
Iteration 4, loss = 0.39031005
Iteration 5, loss = 0.29118126
Iteration 6, loss = 0.21807021
Iteration 7, loss = 0.16717455
Iteration 8, loss = 0.13227427
Iteration 9, loss = 0.10793385
Iteration 10, loss = 0.09048464
Iteration 11, loss = 0.07786252
Iteration 12, loss = 0.06814690
Iteration 13, loss = 0.06095866
Iteration 14, loss = 0.05500274
Iteration 15, loss = 0.05047490
Iteration 16, loss = 0.04664655
Iteration 17, loss = 0.04370051
Iteration 18, loss = 0.04093488
Iteration 19, loss = 0.03862449
Iteration 20, loss = 0.03682744
Iteration 21, loss = 0.03538675
Iteration 22, loss = 0.03386383
Iteration 23, loss = 0.03282072
Iteration 24, loss = 0.03202886
Iteration 25, loss = 0.03083555
Iteration 26, loss = 0.02996155
Iteration 27, loss = 0.02928379
Iteration 28, loss = 0.02863334
Iteration 29, loss = 0.02807632
Iteration 30, loss = 0.02741202
Iteration 31, loss = 0.02702831
Iteration 32, loss = 0.02682187
Iteration 33, loss = 0.02652035
Iteration 34, loss = 0.02634020
Iteration 35, loss = 0.02572416
Iteration 36, loss = 0.02514738
Iteration 37, loss = 0.02517647
Iteration 38, loss = 0.02505569
Iteration 39, loss = 0.02466689
Iteration 40, loss = 0.02422375
Iteration 41, loss = 0.02416645
Iteration 42, loss = 0.02407918
Iteration 43, loss = 0.02401764
Iteration 44, loss = 0.02391795
Iteration 45, loss = 0.02378652
Iteration 46, loss = 0.02329737
Iteration 47, loss = 0.02346236
Iteration 48, loss = 0.02340365
Iteration 49, loss = 0.02323665
Iteration 50, loss = 0.02316750
Iteration 51, loss = 0.02299773
Iteration 52, loss = 0.02274388
Iteration 53, loss = 0.02290193
Iteration 54, loss = 0.02260310
Iteration 55, loss = 0.02265602
Iteration 56, loss = 0.02270862
Iteration 57, loss = 0.02251001
Iteration 58, loss = 0.02246201
Iteration 59, loss = 0.02226563
Iteration 60, loss = 0.02246453
Iteration 61, loss = 0.02227724
Iteration 62, loss = 0.02230755
Iteration 63, loss = 0.02237744
Iteration 64, loss = 0.02205786
Iteration 65, loss = 0.02232360
Iteration 66, loss = 0.02248871
Iteration 67, loss = 0.02220908
Iteration 68, loss = 0.02218337
Iteration 69, loss = 0.02197853
Iteration 70, loss = 0.02185889
Iteration 71, loss = 0.02200545
Iteration 72, loss = 0.02203364
Iteration 73, loss = 0.02186053
Iteration 74, loss = 0.02178836
Iteration 75, loss = 0.02198754
Iteration 76, loss = 0.02180593
Iteration 77, loss = 0.02174343
Iteration 78, loss = 0.02205949
Iteration 79, loss = 0.02152845
Iteration 80, loss = 0.02177302
Iteration 81, loss = 0.02165754
Iteration 82, loss = 0.02181594
Iteration 83, loss = 0.02177169
Iteration 84, loss = 0.02189179
Iteration 85, loss = 0.02163120
Iteration 86, loss = 0.02144226
Iteration 87, loss = 0.02154589
Iteration 88, loss = 0.02151479
Iteration 89, loss = 0.02165976
Iteration 90, loss = 0.02149675
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68589365
Iteration 2, loss = 0.67036617
Iteration 3, loss = 0.65707332
Iteration 4, loss = 0.64347766
Iteration 5, loss = 0.62929452
Iteration 6, loss = 0.61424211
Iteration 7, loss = 0.59766600
Iteration 8, loss = 0.57982302
Iteration 9, loss = 0.56095465
Iteration 10, loss = 0.54097923
Iteration 11, loss = 0.51990709
Iteration 12, loss = 0.49814087
Iteration 13, loss = 0.47587151
Iteration 14, loss = 0.45331512
Iteration 15, loss = 0.43084859
Iteration 16, loss = 0.40850804
Iteration 17, loss = 0.38648663
Iteration 18, loss = 0.36505983
Iteration 19, loss = 0.34435867
Iteration 20, loss = 0.32446822
Iteration 21, loss = 0.30525930
Iteration 22, loss = 0.28705242
Iteration 23, loss = 0.26971252
Iteration 24, loss = 0.25331694
Iteration 25, loss = 0.23791063
Iteration 26, loss = 0.22336763
Iteration 27, loss = 0.20976211
Iteration 28, loss = 0.19705235
Iteration 29, loss = 0.18521978
Iteration 30, loss = 0.17414812
Iteration 31, loss = 0.16379068
Iteration 32, loss = 0.15417217
Iteration 33, loss = 0.14526049
Iteration 34, loss = 0.13687044
Iteration 35, loss = 0.12915029
Iteration 36, loss = 0.12198053
Iteration 37, loss = 0.11530103
Iteration 38, loss = 0.10911614
Iteration 39, loss = 0.10330112
Iteration 40, loss = 0.09798496
Iteration 41, loss = 0.09297969
Iteration 42, loss = 0.08832028
Iteration 43, loss = 0.08401484
Iteration 44, loss = 0.08000088
Iteration 45, loss = 0.07621926
Iteration 46, loss = 0.07265353
Iteration 47, loss = 0.06941564
Iteration 48, loss = 0.06633957
Iteration 49, loss = 0.06342695
Iteration 50, loss = 0.06076542
Iteration 51, loss = 0.05826763
Iteration 52, loss = 0.05589044
Iteration 53, loss = 0.05365165
Iteration 54, loss = 0.05155541
Iteration 55, loss = 0.04958332
Iteration 56, loss = 0.04772427
Iteration 57, loss = 0.04596170
Iteration 58, loss = 0.04431150
Iteration 59, loss = 0.04275885
Iteration 60, loss = 0.04125888
Iteration 61, loss = 0.03985683
Iteration 62, loss = 0.03853621
Iteration 63, loss = 0.03728361
Iteration 64, loss = 0.03604864
Iteration 65, loss = 0.03487684
Iteration 66, loss = 0.03379721
Iteration 67, loss = 0.03273053
Iteration 68, loss = 0.03173675
Iteration 69, loss = 0.03075059
Iteration 70, loss = 0.02985563
Iteration 71, loss = 0.02898782
Iteration 72, loss = 0.02816065
Iteration 73, loss = 0.02734253
Iteration 74, loss = 0.02658622
Iteration 75, loss = 0.02586666
Iteration 76, loss = 0.02514530
Iteration 77, loss = 0.02449966
Iteration 78, loss = 0.02386086
Iteration 79, loss = 0.02323335
Iteration 80, loss = 0.02264678
Iteration 81, loss = 0.02209539
Iteration 82, loss = 0.02153433
Iteration 83, loss = 0.02100672
Iteration 84, loss = 0.02051250
Iteration 85, loss = 0.02000949
Iteration 86, loss = 0.01954419
Iteration 87, loss = 0.01908641
Iteration 88, loss = 0.01865778
Iteration 89, loss = 0.01823622
Iteration 90, loss = 0.01782248
Iteration 91, loss = 0.01742624
Iteration 92, loss = 0.01705944
Iteration 93, loss = 0.01669236
Iteration 94, loss = 0.01633876
Iteration 95, loss = 0.01599552
Iteration 96, loss = 0.01566643
Iteration 97, loss = 0.01536294
Iteration 98, loss = 0.01504949
Iteration 99, loss = 0.01475218
Iteration 100, loss = 0.01445903
Iteration 101, loss = 0.01418361
Iteration 102, loss = 0.01391568
Iteration 103, loss = 0.01364691
Iteration 104, loss = 0.01338981
Iteration 105, loss = 0.01314255
Iteration 106, loss = 0.01290879
Iteration 107, loss = 0.01267188
Iteration 108, loss = 0.01244628
Iteration 109, loss = 0.01222659
Iteration 110, loss = 0.01200872
Iteration 111, loss = 0.01180322
Iteration 112, loss = 0.01159873
Iteration 113, loss = 0.01139595
Iteration 114, loss = 0.01121008
Iteration 115, loss = 0.01102812
Iteration 116, loss = 0.01084392
Iteration 117, loss = 0.01066220
Iteration 118, loss = 0.01048667
Iteration 119, loss = 0.01032596
Iteration 120, loss = 0.01016142
Iteration 121, loss = 0.00999731
Iteration 122, loss = 0.00984841
Iteration 123, loss = 0.00968726
Iteration 124, loss = 0.00954056
Iteration 125, loss = 0.00940069
Iteration 126, loss = 0.00925707
Iteration 127, loss = 0.00912197
Iteration 128, loss = 0.00898845
Iteration 129, loss = 0.00886069
Iteration 130, loss = 0.00873859
Iteration 131, loss = 0.00861303
Iteration 132, loss = 0.00849721
Iteration 133, loss = 0.00837437
Iteration 134, loss = 0.00826170
Iteration 135, loss = 0.00814828
Iteration 136, loss = 0.00803691
Iteration 137, loss = 0.00793069
Iteration 138, loss = 0.00782573
Iteration 139, loss = 0.00772004
Iteration 140, loss = 0.00762163
Iteration 141, loss = 0.00752525
Iteration 142, loss = 0.00742896
Iteration 143, loss = 0.00733465
Iteration 144, loss = 0.00724429
Iteration 145, loss = 0.00715462
Iteration 146, loss = 0.00706723
Iteration 147, loss = 0.00698019
Iteration 148, loss = 0.00689686
Iteration 149, loss = 0.00681632
Iteration 150, loss = 0.00673568
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68104396
Iteration 2, loss = 0.66261970
Iteration 3, loss = 0.64438707
Iteration 4, loss = 0.62332457
Iteration 5, loss = 0.59878555
Iteration 6, loss = 0.57100544
Iteration 7, loss = 0.54026667
Iteration 8, loss = 0.50774984
Iteration 9, loss = 0.47389814
Iteration 10, loss = 0.44000964
Iteration 11, loss = 0.40685752
Iteration 12, loss = 0.37444564
Iteration 13, loss = 0.34364704
Iteration 14, loss = 0.31447492
Iteration 15, loss = 0.28720503
Iteration 16, loss = 0.26224021
Iteration 17, loss = 0.23891086
Iteration 18, loss = 0.21792787
Iteration 19, loss = 0.19883347
Iteration 20, loss = 0.18174542
Iteration 21, loss = 0.16642826
Iteration 22, loss = 0.15252990
Iteration 23, loss = 0.14038081
Iteration 24, loss = 0.12944963
Iteration 25, loss = 0.11966357
Iteration 26, loss = 0.11097504
Iteration 27, loss = 0.10312563
Iteration 28, loss = 0.09610318
Iteration 29, loss = 0.08983080
Iteration 30, loss = 0.08421994
Iteration 31, loss = 0.07920993
Iteration 32, loss = 0.07446750
Iteration 33, loss = 0.07033162
Iteration 34, loss = 0.06652488
Iteration 35, loss = 0.06306546
Iteration 36, loss = 0.05991712
Iteration 37, loss = 0.05705293
Iteration 38, loss = 0.05438715
Iteration 39, loss = 0.05195585
Iteration 40, loss = 0.04970828
Iteration 41, loss = 0.04764429
Iteration 42, loss = 0.04567610
Iteration 43, loss = 0.04391521
Iteration 44, loss = 0.04225498
Iteration 45, loss = 0.04072512
Iteration 46, loss = 0.03933112
Iteration 47, loss = 0.03793723
Iteration 48, loss = 0.03667040
Iteration 49, loss = 0.03557558
Iteration 50, loss = 0.03448221
Iteration 51, loss = 0.03345168
Iteration 52, loss = 0.03242207
Iteration 53, loss = 0.03153616
Iteration 54, loss = 0.03067477
Iteration 55, loss = 0.02988328
Iteration 56, loss = 0.02909226
Iteration 57, loss = 0.02840863
Iteration 58, loss = 0.02770381
Iteration 59, loss = 0.02705834
Iteration 60, loss = 0.02642928
Iteration 61, loss = 0.02584896
Iteration 62, loss = 0.02533967
Iteration 63, loss = 0.02475277
Iteration 64, loss = 0.02424973
Iteration 65, loss = 0.02381713
Iteration 66, loss = 0.02337836
Iteration 67, loss = 0.02295048
Iteration 68, loss = 0.02254246
Iteration 69, loss = 0.02212715
Iteration 70, loss = 0.02174984
Iteration 71, loss = 0.02140332
Iteration 72, loss = 0.02103065
Iteration 73, loss = 0.02072816
Iteration 74, loss = 0.02041860
Iteration 75, loss = 0.02009202
Iteration 76, loss = 0.01984066
Iteration 77, loss = 0.01951697
Iteration 78, loss = 0.01926543
Iteration 79, loss = 0.01898801
Iteration 80, loss = 0.01880843
Iteration 81, loss = 0.01852983
Iteration 82, loss = 0.01832025
Iteration 83, loss = 0.01807977
Iteration 84, loss = 0.01798335
Iteration 85, loss = 0.01768322
Iteration 86, loss = 0.01747224
Iteration 87, loss = 0.01731508
Iteration 88, loss = 0.01710945
Iteration 89, loss = 0.01697425
Iteration 90, loss = 0.01679026
Iteration 91, loss = 0.01658993
Iteration 92, loss = 0.01645404
Iteration 93, loss = 0.01627843
Iteration 94, loss = 0.01614160
Iteration 95, loss = 0.01603528
Iteration 96, loss = 0.01588047
Iteration 97, loss = 0.01571719
Iteration 98, loss = 0.01560563
Iteration 99, loss = 0.01545157
Iteration 100, loss = 0.01541631
Iteration 101, loss = 0.01523233
Iteration 102, loss = 0.01517495
Iteration 103, loss = 0.01504205
Iteration 104, loss = 0.01491998
Iteration 105, loss = 0.01480956
Iteration 106, loss = 0.01470732
Iteration 107, loss = 0.01463828
Iteration 108, loss = 0.01450345
Iteration 109, loss = 0.01444610
Iteration 110, loss = 0.01437656
Iteration 111, loss = 0.01430899
Iteration 112, loss = 0.01416332
Iteration 113, loss = 0.01406453
Iteration 114, loss = 0.01403440
Iteration 115, loss = 0.01392839
Iteration 116, loss = 0.01384492
Iteration 117, loss = 0.01377115
Iteration 118, loss = 0.01370319
Iteration 119, loss = 0.01360692
Iteration 120, loss = 0.01362707
Iteration 121, loss = 0.01350074
Iteration 122, loss = 0.01341423
Iteration 123, loss = 0.01335773
Iteration 124, loss = 0.01329804
Iteration 125, loss = 0.01324069
Iteration 126, loss = 0.01318889
Iteration 127, loss = 0.01313811
Iteration 128, loss = 0.01308301
Iteration 129, loss = 0.01303982
Iteration 130, loss = 0.01297179
Iteration 131, loss = 0.01298270
Iteration 132, loss = 0.01285597
Iteration 133, loss = 0.01279842
Iteration 134, loss = 0.01274720
Iteration 135, loss = 0.01275098
Iteration 136, loss = 0.01268744
Iteration 137, loss = 0.01262354
Iteration 138, loss = 0.01263779
Iteration 139, loss = 0.01267356
Iteration 140, loss = 0.01255694
Iteration 141, loss = 0.01244847
Iteration 142, loss = 0.01240442
Iteration 143, loss = 0.01236628
Iteration 144, loss = 0.01235689
Iteration 145, loss = 0.01232798
Iteration 146, loss = 0.01234308
Iteration 147, loss = 0.01227282
Iteration 148, loss = 0.01219704
Iteration 149, loss = 0.01218359
Iteration 150, loss = 0.01213385
Iteration 151, loss = 0.01211227
Iteration 152, loss = 0.01207732
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68077207
Iteration 2, loss = 0.63579086
Iteration 3, loss = 0.56283032
Iteration 4, loss = 0.47342206
Iteration 5, loss = 0.38688489
Iteration 6, loss = 0.30986859
Iteration 7, loss = 0.24704828
Iteration 8, loss = 0.19833034
Iteration 9, loss = 0.16143163
Iteration 10, loss = 0.13386519
Iteration 11, loss = 0.11323235
Iteration 12, loss = 0.09763545
Iteration 13, loss = 0.08536415
Iteration 14, loss = 0.07583315
Iteration 15, loss = 0.06817729
Iteration 16, loss = 0.06203594
Iteration 17, loss = 0.05702778
Iteration 18, loss = 0.05267741
Iteration 19, loss = 0.04933942
Iteration 20, loss = 0.04644267
Iteration 21, loss = 0.04385235
Iteration 22, loss = 0.04165986
Iteration 23, loss = 0.03984403
Iteration 24, loss = 0.03808141
Iteration 25, loss = 0.03666013
Iteration 26, loss = 0.03525004
Iteration 27, loss = 0.03436497
Iteration 28, loss = 0.03314936
Iteration 29, loss = 0.03243090
Iteration 30, loss = 0.03170313
Iteration 31, loss = 0.03085689
Iteration 32, loss = 0.03009094
Iteration 33, loss = 0.02965039
Iteration 34, loss = 0.02904835
Iteration 35, loss = 0.02851439
Iteration 36, loss = 0.02835434
Iteration 37, loss = 0.02757752
Iteration 38, loss = 0.02721413
Iteration 39, loss = 0.02698207
Iteration 40, loss = 0.02662695
Iteration 41, loss = 0.02661964
Iteration 42, loss = 0.02596568
Iteration 43, loss = 0.02581096
Iteration 44, loss = 0.02559816
Iteration 45, loss = 0.02527493
Iteration 46, loss = 0.02503604
Iteration 47, loss = 0.02494592
Iteration 48, loss = 0.02482731
Iteration 49, loss = 0.02491605
Iteration 50, loss = 0.02433791
Iteration 51, loss = 0.02448793
Iteration 52, loss = 0.02427875
Iteration 53, loss = 0.02406112
Iteration 54, loss = 0.02394888
Iteration 55, loss = 0.02375324
Iteration 56, loss = 0.02372568
Iteration 57, loss = 0.02356089
Iteration 58, loss = 0.02348085
Iteration 59, loss = 0.02376212
Iteration 60, loss = 0.02341875
Iteration 61, loss = 0.02325223
Iteration 62, loss = 0.02323525
Iteration 63, loss = 0.02305411
Iteration 64, loss = 0.02297780
Iteration 65, loss = 0.02304943
Iteration 66, loss = 0.02283528
Iteration 67, loss = 0.02311235
Iteration 68, loss = 0.02281774
Iteration 69, loss = 0.02275636
Iteration 70, loss = 0.02254477
Iteration 71, loss = 0.02242012
Iteration 72, loss = 0.02226268
Iteration 73, loss = 0.02266853
Iteration 74, loss = 0.02237041
Iteration 75, loss = 0.02254698
Iteration 76, loss = 0.02245596
Iteration 77, loss = 0.02225816
Iteration 78, loss = 0.02239239
Iteration 79, loss = 0.02228862
Iteration 80, loss = 0.02236619
Iteration 81, loss = 0.02239631
Iteration 82, loss = 0.02213555
Iteration 83, loss = 0.02206556
Iteration 84, loss = 0.02206739
Iteration 85, loss = 0.02197314
Iteration 86, loss = 0.02217151
Iteration 87, loss = 0.02207534
Iteration 88, loss = 0.02192722
Iteration 89, loss = 0.02195267
Iteration 90, loss = 0.02176473
Iteration 91, loss = 0.02182738
Iteration 92, loss = 0.02205971
Iteration 93, loss = 0.02198763
Iteration 94, loss = 0.02182356
Iteration 95, loss = 0.02206218
Iteration 96, loss = 0.02195768
Iteration 97, loss = 0.02185643
Iteration 98, loss = 0.02183610
Iteration 99, loss = 0.02178623
Iteration 100, loss = 0.02177122
Iteration 101, loss = 0.02184019
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	82m6.589s
user	339m23.362s
sys	34m35.512s
Elapsed time: 4927 seconds
