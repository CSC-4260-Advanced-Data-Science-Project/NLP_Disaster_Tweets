Processing dataset: kept_v2_no_emojis_mentions

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: kept_v2_no_emojis_mentions (7443 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on kept_v2_no_emojis_mentions...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on kept_v2_no_emojis_mentions...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on kept_v2_no_emojis_mentions...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on kept_v2_no_emojis_mentions...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on kept_v2_no_emojis_mentions...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67357894
Iteration 2, loss = 0.59567092
Iteration 3, loss = 0.48638507
Iteration 4, loss = 0.37556227
Iteration 5, loss = 0.28192356
Iteration 6, loss = 0.21177155
Iteration 7, loss = 0.16282510
Iteration 8, loss = 0.12911732
Iteration 9, loss = 0.10599070
Iteration 10, loss = 0.08970477
Iteration 11, loss = 0.07767355
Iteration 12, loss = 0.06863711
Iteration 13, loss = 0.06155037
Iteration 14, loss = 0.05630676
Iteration 15, loss = 0.05195748
Iteration 16, loss = 0.04869525
Iteration 17, loss = 0.04595342
Iteration 18, loss = 0.04342333
Iteration 19, loss = 0.04188495
Iteration 20, loss = 0.04011013
Iteration 21, loss = 0.03879237
Iteration 22, loss = 0.03724736
Iteration 23, loss = 0.03630495
Iteration 24, loss = 0.03535958
Iteration 25, loss = 0.03456081
Iteration 26, loss = 0.03431580
Iteration 27, loss = 0.03354838
Iteration 28, loss = 0.03291508
Iteration 29, loss = 0.03210765
Iteration 30, loss = 0.03198039
Iteration 31, loss = 0.03141971
Iteration 32, loss = 0.03088510
Iteration 33, loss = 0.03056253
Iteration 34, loss = 0.03032292
Iteration 35, loss = 0.02999026
Iteration 36, loss = 0.02980295
Iteration 37, loss = 0.02978964
Iteration 38, loss = 0.02959659
Iteration 39, loss = 0.02925493
Iteration 40, loss = 0.02876988
Iteration 41, loss = 0.02906910
Iteration 42, loss = 0.02887446
Iteration 43, loss = 0.02877878
Iteration 44, loss = 0.02834873
Iteration 45, loss = 0.02824688
Iteration 46, loss = 0.02803783
Iteration 47, loss = 0.02837163
Iteration 48, loss = 0.02815531
Iteration 49, loss = 0.02780884
Iteration 50, loss = 0.02801989
Iteration 51, loss = 0.02776989
Iteration 52, loss = 0.02761230
Iteration 53, loss = 0.02785919
Iteration 54, loss = 0.02732945
Iteration 55, loss = 0.02735518
Iteration 56, loss = 0.02756288
Iteration 57, loss = 0.02748601
Iteration 58, loss = 0.02733139
Iteration 59, loss = 0.02740945
Iteration 60, loss = 0.02722472
Iteration 61, loss = 0.02744070
Iteration 62, loss = 0.02759578
Iteration 63, loss = 0.02704149
Iteration 64, loss = 0.02693119
Iteration 65, loss = 0.02687273
Iteration 66, loss = 0.02686433
Iteration 67, loss = 0.02651787
Iteration 68, loss = 0.02711962
Iteration 69, loss = 0.02695497
Iteration 70, loss = 0.02711520
Iteration 71, loss = 0.02677303
Iteration 72, loss = 0.02692713
Iteration 73, loss = 0.02675237
Iteration 74, loss = 0.02687503
Iteration 75, loss = 0.02692427
Iteration 76, loss = 0.02662596
Iteration 77, loss = 0.02668532
Iteration 78, loss = 0.02676109
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67495143
Iteration 2, loss = 0.59535102
Iteration 3, loss = 0.48422289
Iteration 4, loss = 0.37243195
Iteration 5, loss = 0.28028335
Iteration 6, loss = 0.21140555
Iteration 7, loss = 0.16317942
Iteration 8, loss = 0.12983132
Iteration 9, loss = 0.10674169
Iteration 10, loss = 0.09005491
Iteration 11, loss = 0.07761679
Iteration 12, loss = 0.06838416
Iteration 13, loss = 0.06131320
Iteration 14, loss = 0.05596959
Iteration 15, loss = 0.05147643
Iteration 16, loss = 0.04778245
Iteration 17, loss = 0.04457613
Iteration 18, loss = 0.04228027
Iteration 19, loss = 0.03987467
Iteration 20, loss = 0.03804730
Iteration 21, loss = 0.03662360
Iteration 22, loss = 0.03542834
Iteration 23, loss = 0.03405074
Iteration 24, loss = 0.03332218
Iteration 25, loss = 0.03231565
Iteration 26, loss = 0.03159938
Iteration 27, loss = 0.03073354
Iteration 28, loss = 0.03011569
Iteration 29, loss = 0.02975943
Iteration 30, loss = 0.02896793
Iteration 31, loss = 0.02890512
Iteration 32, loss = 0.02839364
Iteration 33, loss = 0.02804501
Iteration 34, loss = 0.02761731
Iteration 35, loss = 0.02716495
Iteration 36, loss = 0.02693055
Iteration 37, loss = 0.02674411
Iteration 38, loss = 0.02655186
Iteration 39, loss = 0.02621933
Iteration 40, loss = 0.02626269
Iteration 41, loss = 0.02574367
Iteration 42, loss = 0.02562492
Iteration 43, loss = 0.02565354
Iteration 44, loss = 0.02536442
Iteration 45, loss = 0.02534364
Iteration 46, loss = 0.02541077
Iteration 47, loss = 0.02480189
Iteration 48, loss = 0.02480024
Iteration 49, loss = 0.02480755
Iteration 50, loss = 0.02485377
Iteration 51, loss = 0.02470237
Iteration 52, loss = 0.02446464
Iteration 53, loss = 0.02468003
Iteration 54, loss = 0.02443635
Iteration 55, loss = 0.02412294
Iteration 56, loss = 0.02429331
Iteration 57, loss = 0.02462227
Iteration 58, loss = 0.02411977
Iteration 59, loss = 0.02399623
Iteration 60, loss = 0.02403763
Iteration 61, loss = 0.02397538
Iteration 62, loss = 0.02387098
Iteration 63, loss = 0.02378943
Iteration 64, loss = 0.02373367
Iteration 65, loss = 0.02388870
Iteration 66, loss = 0.02397299
Iteration 67, loss = 0.02372506
Iteration 68, loss = 0.02341853
Iteration 69, loss = 0.02378351
Iteration 70, loss = 0.02365277
Iteration 71, loss = 0.02384210
Iteration 72, loss = 0.02325374
Iteration 73, loss = 0.02344954
Iteration 74, loss = 0.02348184
Iteration 75, loss = 0.02320154
Iteration 76, loss = 0.02353956
Iteration 77, loss = 0.02325249
Iteration 78, loss = 0.02300293
Iteration 79, loss = 0.02346245
Iteration 80, loss = 0.02393153
Iteration 81, loss = 0.02356798
Iteration 82, loss = 0.02334165
Iteration 83, loss = 0.02334506
Iteration 84, loss = 0.02316220
Iteration 85, loss = 0.02318885
Iteration 86, loss = 0.02314855
Iteration 87, loss = 0.02323605
Iteration 88, loss = 0.02313095
Iteration 89, loss = 0.02313660
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70845789
Iteration 2, loss = 0.62314742
Iteration 3, loss = 0.51191625
Iteration 4, loss = 0.39325294
Iteration 5, loss = 0.29422034
Iteration 6, loss = 0.21895490
Iteration 7, loss = 0.16652661
Iteration 8, loss = 0.13103941
Iteration 9, loss = 0.10630880
Iteration 10, loss = 0.08883991
Iteration 11, loss = 0.07624631
Iteration 12, loss = 0.06663846
Iteration 13, loss = 0.05937903
Iteration 14, loss = 0.05376056
Iteration 15, loss = 0.04931387
Iteration 16, loss = 0.04534511
Iteration 17, loss = 0.04247010
Iteration 18, loss = 0.03995595
Iteration 19, loss = 0.03780159
Iteration 20, loss = 0.03596800
Iteration 21, loss = 0.03438244
Iteration 22, loss = 0.03300400
Iteration 23, loss = 0.03184997
Iteration 24, loss = 0.03111630
Iteration 25, loss = 0.03024289
Iteration 26, loss = 0.02917888
Iteration 27, loss = 0.02840487
Iteration 28, loss = 0.02777821
Iteration 29, loss = 0.02723550
Iteration 30, loss = 0.02687825
Iteration 31, loss = 0.02631389
Iteration 32, loss = 0.02604536
Iteration 33, loss = 0.02560523
Iteration 34, loss = 0.02541228
Iteration 35, loss = 0.02487914
Iteration 36, loss = 0.02485184
Iteration 37, loss = 0.02452936
Iteration 38, loss = 0.02428618
Iteration 39, loss = 0.02403326
Iteration 40, loss = 0.02388658
Iteration 41, loss = 0.02362371
Iteration 42, loss = 0.02376580
Iteration 43, loss = 0.02332747
Iteration 44, loss = 0.02342225
Iteration 45, loss = 0.02301247
Iteration 46, loss = 0.02294267
Iteration 47, loss = 0.02270401
Iteration 48, loss = 0.02265905
Iteration 49, loss = 0.02263118
Iteration 50, loss = 0.02223060
Iteration 51, loss = 0.02266223
Iteration 52, loss = 0.02232853
Iteration 53, loss = 0.02207727
Iteration 54, loss = 0.02256231
Iteration 55, loss = 0.02186539
Iteration 56, loss = 0.02199488
Iteration 57, loss = 0.02178004
Iteration 58, loss = 0.02172361
Iteration 59, loss = 0.02192583
Iteration 60, loss = 0.02200576
Iteration 61, loss = 0.02200279
Iteration 62, loss = 0.02147073
Iteration 63, loss = 0.02178888
Iteration 64, loss = 0.02163846
Iteration 65, loss = 0.02170517
Iteration 66, loss = 0.02144520
Iteration 67, loss = 0.02162907
Iteration 68, loss = 0.02149568
Iteration 69, loss = 0.02131563
Iteration 70, loss = 0.02142592
Iteration 71, loss = 0.02120282
Iteration 72, loss = 0.02117341
Iteration 73, loss = 0.02124595
Iteration 74, loss = 0.02127862
Iteration 75, loss = 0.02113085
Iteration 76, loss = 0.02148707
Iteration 77, loss = 0.02133623
Iteration 78, loss = 0.02129967
Iteration 79, loss = 0.02096586
Iteration 80, loss = 0.02117086
Iteration 81, loss = 0.02080003
Iteration 82, loss = 0.02098062
Iteration 83, loss = 0.02109211
Iteration 84, loss = 0.02106506
Iteration 85, loss = 0.02119213
Iteration 86, loss = 0.02097070
Iteration 87, loss = 0.02070253
Iteration 88, loss = 0.02079564
Iteration 89, loss = 0.02125728
Iteration 90, loss = 0.02098426
Iteration 91, loss = 0.02116057
Iteration 92, loss = 0.02090810
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67637146
Iteration 2, loss = 0.59805321
Iteration 3, loss = 0.48406295
Iteration 4, loss = 0.37077968
Iteration 5, loss = 0.27825732
Iteration 6, loss = 0.20981626
Iteration 7, loss = 0.16213887
Iteration 8, loss = 0.12955415
Iteration 9, loss = 0.10674435
Iteration 10, loss = 0.09059714
Iteration 11, loss = 0.07876512
Iteration 12, loss = 0.06995711
Iteration 13, loss = 0.06304073
Iteration 14, loss = 0.05772161
Iteration 15, loss = 0.05327498
Iteration 16, loss = 0.04977620
Iteration 17, loss = 0.04681542
Iteration 18, loss = 0.04427226
Iteration 19, loss = 0.04225916
Iteration 20, loss = 0.04050860
Iteration 21, loss = 0.03901566
Iteration 22, loss = 0.03798845
Iteration 23, loss = 0.03664751
Iteration 24, loss = 0.03583129
Iteration 25, loss = 0.03505329
Iteration 26, loss = 0.03421169
Iteration 27, loss = 0.03370497
Iteration 28, loss = 0.03293763
Iteration 29, loss = 0.03268844
Iteration 30, loss = 0.03216863
Iteration 31, loss = 0.03163717
Iteration 32, loss = 0.03139890
Iteration 33, loss = 0.03085434
Iteration 34, loss = 0.03077615
Iteration 35, loss = 0.03014935
Iteration 36, loss = 0.02985440
Iteration 37, loss = 0.02985620
Iteration 38, loss = 0.02953266
Iteration 39, loss = 0.02922401
Iteration 40, loss = 0.02918591
Iteration 41, loss = 0.02902672
Iteration 42, loss = 0.02885290
Iteration 43, loss = 0.02868887
Iteration 44, loss = 0.02847932
Iteration 45, loss = 0.02848994
Iteration 46, loss = 0.02845846
Iteration 47, loss = 0.02860677
Iteration 48, loss = 0.02789333
Iteration 49, loss = 0.02826578
Iteration 50, loss = 0.02777349
Iteration 51, loss = 0.02779171
Iteration 52, loss = 0.02769129
Iteration 53, loss = 0.02763249
Iteration 54, loss = 0.02767248
Iteration 55, loss = 0.02749225
Iteration 56, loss = 0.02736658
Iteration 57, loss = 0.02755343
Iteration 58, loss = 0.02727402
Iteration 59, loss = 0.02732878
Iteration 60, loss = 0.02741780
Iteration 61, loss = 0.02706961
Iteration 62, loss = 0.02705319
Iteration 63, loss = 0.02707112
Iteration 64, loss = 0.02686935
Iteration 65, loss = 0.02709448
Iteration 66, loss = 0.02684947
Iteration 67, loss = 0.02681817
Iteration 68, loss = 0.02684700
Iteration 69, loss = 0.02692082
Iteration 70, loss = 0.02690226
Iteration 71, loss = 0.02683142
Iteration 72, loss = 0.02682574
Iteration 73, loss = 0.02664585
Iteration 74, loss = 0.02651974
Iteration 75, loss = 0.02656947
Iteration 76, loss = 0.02650132
Iteration 77, loss = 0.02664877
Iteration 78, loss = 0.02667060
Iteration 79, loss = 0.02667079
Iteration 80, loss = 0.02643458
Iteration 81, loss = 0.02659312
Iteration 82, loss = 0.02661261
Iteration 83, loss = 0.02655758
Iteration 84, loss = 0.02621299
Iteration 85, loss = 0.02652322
Iteration 86, loss = 0.02654294
Iteration 87, loss = 0.02645772
Iteration 88, loss = 0.02643246
Iteration 89, loss = 0.02631221
Iteration 90, loss = 0.02655840
Iteration 91, loss = 0.02632901
Iteration 92, loss = 0.02646106
Iteration 93, loss = 0.02610453
Iteration 94, loss = 0.02622777
Iteration 95, loss = 0.02616485
Iteration 96, loss = 0.02603792
Iteration 97, loss = 0.02617692
Iteration 98, loss = 0.02616166
Iteration 99, loss = 0.02621865
Iteration 100, loss = 0.02661547
Iteration 101, loss = 0.02648051
Iteration 102, loss = 0.02587121
Iteration 103, loss = 0.02616008
Iteration 104, loss = 0.02627808
Iteration 105, loss = 0.02602170
Iteration 106, loss = 0.02603333
Iteration 107, loss = 0.02620999
Iteration 108, loss = 0.02589541
Iteration 109, loss = 0.02596563
Iteration 110, loss = 0.02603529
Iteration 111, loss = 0.02590887
Iteration 112, loss = 0.02589941
Iteration 113, loss = 0.02624306
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70816341
Iteration 2, loss = 0.63007283
Iteration 3, loss = 0.52279169
Iteration 4, loss = 0.40151831
Iteration 5, loss = 0.29959569
Iteration 6, loss = 0.22311922
Iteration 7, loss = 0.16992649
Iteration 8, loss = 0.13389029
Iteration 9, loss = 0.10901512
Iteration 10, loss = 0.09154913
Iteration 11, loss = 0.07875982
Iteration 12, loss = 0.06914053
Iteration 13, loss = 0.06195832
Iteration 14, loss = 0.05631171
Iteration 15, loss = 0.05169020
Iteration 16, loss = 0.04818807
Iteration 17, loss = 0.04525781
Iteration 18, loss = 0.04276255
Iteration 19, loss = 0.04045152
Iteration 20, loss = 0.03899906
Iteration 21, loss = 0.03730964
Iteration 22, loss = 0.03610245
Iteration 23, loss = 0.03497826
Iteration 24, loss = 0.03414290
Iteration 25, loss = 0.03310378
Iteration 26, loss = 0.03250489
Iteration 27, loss = 0.03174134
Iteration 28, loss = 0.03122053
Iteration 29, loss = 0.03078367
Iteration 30, loss = 0.03023896
Iteration 31, loss = 0.02981513
Iteration 32, loss = 0.02930775
Iteration 33, loss = 0.02904125
Iteration 34, loss = 0.02885150
Iteration 35, loss = 0.02845746
Iteration 36, loss = 0.02823680
Iteration 37, loss = 0.02793141
Iteration 38, loss = 0.02800728
Iteration 39, loss = 0.02739930
Iteration 40, loss = 0.02743775
Iteration 41, loss = 0.02726345
Iteration 42, loss = 0.02698949
Iteration 43, loss = 0.02680706
Iteration 44, loss = 0.02669276
Iteration 45, loss = 0.02669378
Iteration 46, loss = 0.02659841
Iteration 47, loss = 0.02643516
Iteration 48, loss = 0.02640411
Iteration 49, loss = 0.02639046
Iteration 50, loss = 0.02612787
Iteration 51, loss = 0.02587320
Iteration 52, loss = 0.02589305
Iteration 53, loss = 0.02569418
Iteration 54, loss = 0.02591077
Iteration 55, loss = 0.02590182
Iteration 56, loss = 0.02552897
Iteration 57, loss = 0.02553361
Iteration 58, loss = 0.02555042
Iteration 59, loss = 0.02564105
Iteration 60, loss = 0.02524942
Iteration 61, loss = 0.02540136
Iteration 62, loss = 0.02545680
Iteration 63, loss = 0.02519711
Iteration 64, loss = 0.02534409
Iteration 65, loss = 0.02515358
Iteration 66, loss = 0.02541309
Iteration 67, loss = 0.02529970
Iteration 68, loss = 0.02521235
Iteration 69, loss = 0.02514159
Iteration 70, loss = 0.02498719
Iteration 71, loss = 0.02506427
Iteration 72, loss = 0.02503243
Iteration 73, loss = 0.02499330
Iteration 74, loss = 0.02522265
Iteration 75, loss = 0.02492300
Iteration 76, loss = 0.02473544
Iteration 77, loss = 0.02473381
Iteration 78, loss = 0.02505443
Iteration 79, loss = 0.02463450
Iteration 80, loss = 0.02470289
Iteration 81, loss = 0.02491694
Iteration 82, loss = 0.02452840
Iteration 83, loss = 0.02464212
Iteration 84, loss = 0.02461144
Iteration 85, loss = 0.02469169
Iteration 86, loss = 0.02463607
Iteration 87, loss = 0.02485895
Iteration 88, loss = 0.02433327
Iteration 89, loss = 0.02489027
Iteration 90, loss = 0.02439055
Iteration 91, loss = 0.02467697
Iteration 92, loss = 0.02457110
Iteration 93, loss = 0.02427149
Iteration 94, loss = 0.02439785
Iteration 95, loss = 0.02454921
Iteration 96, loss = 0.02446172
Iteration 97, loss = 0.02444378
Iteration 98, loss = 0.02465886
Iteration 99, loss = 0.02463213
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67357894
Iteration 2, loss = 0.59567092
Iteration 3, loss = 0.48638507
Iteration 4, loss = 0.37556227
Iteration 5, loss = 0.28192356
Iteration 6, loss = 0.21177155
Iteration 7, loss = 0.16282510
Iteration 8, loss = 0.12911732
Iteration 9, loss = 0.10599070
Iteration 10, loss = 0.08970477
Iteration 11, loss = 0.07767355
Iteration 12, loss = 0.06863711
Iteration 13, loss = 0.06155037
Iteration 14, loss = 0.05630676
Iteration 15, loss = 0.05195748
Iteration 16, loss = 0.04869525
Iteration 17, loss = 0.04595342
Iteration 18, loss = 0.04342333
Iteration 19, loss = 0.04188495
Iteration 20, loss = 0.04011013
Iteration 21, loss = 0.03879237
Iteration 22, loss = 0.03724736
Iteration 23, loss = 0.03630495
Iteration 24, loss = 0.03535958
Iteration 25, loss = 0.03456081
Iteration 26, loss = 0.03431580
Iteration 27, loss = 0.03354838
Iteration 28, loss = 0.03291508
Iteration 29, loss = 0.03210765
Iteration 30, loss = 0.03198039
Iteration 31, loss = 0.03141971
Iteration 32, loss = 0.03088510
Iteration 33, loss = 0.03056253
Iteration 34, loss = 0.03032292
Iteration 35, loss = 0.02999026
Iteration 36, loss = 0.02980295
Iteration 37, loss = 0.02978964
Iteration 38, loss = 0.02959659
Iteration 39, loss = 0.02925493
Iteration 40, loss = 0.02876988
Iteration 41, loss = 0.02906910
Iteration 42, loss = 0.02887446
Iteration 43, loss = 0.02877878
Iteration 44, loss = 0.02834873
Iteration 45, loss = 0.02824688
Iteration 46, loss = 0.02803783
Iteration 47, loss = 0.02837163
Iteration 48, loss = 0.02815531
Iteration 49, loss = 0.02780884
Iteration 50, loss = 0.02801989
Iteration 51, loss = 0.02776989
Iteration 52, loss = 0.02761230
Iteration 53, loss = 0.02785919
Iteration 54, loss = 0.02732945
Iteration 55, loss = 0.02735518
Iteration 56, loss = 0.02756288
Iteration 57, loss = 0.02748601
Iteration 58, loss = 0.02733139
Iteration 59, loss = 0.02740945
Iteration 60, loss = 0.02722472
Iteration 61, loss = 0.02744070
Iteration 62, loss = 0.02759578
Iteration 63, loss = 0.02704149
Iteration 64, loss = 0.02693119
Iteration 65, loss = 0.02687273
Iteration 66, loss = 0.02686433
Iteration 67, loss = 0.02651787
Iteration 68, loss = 0.02711962
Iteration 69, loss = 0.02695497
Iteration 70, loss = 0.02711520
Iteration 71, loss = 0.02677303
Iteration 72, loss = 0.02692713
Iteration 73, loss = 0.02675237
Iteration 74, loss = 0.02687503
Iteration 75, loss = 0.02692427
Iteration 76, loss = 0.02662596
Iteration 77, loss = 0.02668532
Iteration 78, loss = 0.02676109
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67495143
Iteration 2, loss = 0.59535102
Iteration 3, loss = 0.48422289
Iteration 4, loss = 0.37243195
Iteration 5, loss = 0.28028335
Iteration 6, loss = 0.21140555
Iteration 7, loss = 0.16317942
Iteration 8, loss = 0.12983132
Iteration 9, loss = 0.10674169
Iteration 10, loss = 0.09005491
Iteration 11, loss = 0.07761679
Iteration 12, loss = 0.06838416
Iteration 13, loss = 0.06131320
Iteration 14, loss = 0.05596959
Iteration 15, loss = 0.05147643
Iteration 16, loss = 0.04778245
Iteration 17, loss = 0.04457613
Iteration 18, loss = 0.04228027
Iteration 19, loss = 0.03987467
Iteration 20, loss = 0.03804730
Iteration 21, loss = 0.03662360
Iteration 22, loss = 0.03542834
Iteration 23, loss = 0.03405074
Iteration 24, loss = 0.03332218
Iteration 25, loss = 0.03231565
Iteration 26, loss = 0.03159938
Iteration 27, loss = 0.03073354
Iteration 28, loss = 0.03011569
Iteration 29, loss = 0.02975943
Iteration 30, loss = 0.02896793
Iteration 31, loss = 0.02890512
Iteration 32, loss = 0.02839364
Iteration 33, loss = 0.02804501
Iteration 34, loss = 0.02761731
Iteration 35, loss = 0.02716495
Iteration 36, loss = 0.02693055
Iteration 37, loss = 0.02674411
Iteration 38, loss = 0.02655186
Iteration 39, loss = 0.02621933
Iteration 40, loss = 0.02626269
Iteration 41, loss = 0.02574367
Iteration 42, loss = 0.02562492
Iteration 43, loss = 0.02565354
Iteration 44, loss = 0.02536442
Iteration 45, loss = 0.02534364
Iteration 46, loss = 0.02541077
Iteration 47, loss = 0.02480189
Iteration 48, loss = 0.02480024
Iteration 49, loss = 0.02480755
Iteration 50, loss = 0.02485377
Iteration 51, loss = 0.02470237
Iteration 52, loss = 0.02446464
Iteration 53, loss = 0.02468003
Iteration 54, loss = 0.02443635
Iteration 55, loss = 0.02412294
Iteration 56, loss = 0.02429331
Iteration 57, loss = 0.02462227
Iteration 58, loss = 0.02411977
Iteration 59, loss = 0.02399623
Iteration 60, loss = 0.02403763
Iteration 61, loss = 0.02397538
Iteration 62, loss = 0.02387098
Iteration 63, loss = 0.02378943
Iteration 64, loss = 0.02373367
Iteration 65, loss = 0.02388870
Iteration 66, loss = 0.02397299
Iteration 67, loss = 0.02372506
Iteration 68, loss = 0.02341853
Iteration 69, loss = 0.02378351
Iteration 70, loss = 0.02365277
Iteration 71, loss = 0.02384210
Iteration 72, loss = 0.02325374
Iteration 73, loss = 0.02344954
Iteration 74, loss = 0.02348184
Iteration 75, loss = 0.02320154
Iteration 76, loss = 0.02353956
Iteration 77, loss = 0.02325249
Iteration 78, loss = 0.02300293
Iteration 79, loss = 0.02346245
Iteration 80, loss = 0.02393153
Iteration 81, loss = 0.02356798
Iteration 82, loss = 0.02334165
Iteration 83, loss = 0.02334506
Iteration 84, loss = 0.02316220
Iteration 85, loss = 0.02318885
Iteration 86, loss = 0.02314855
Iteration 87, loss = 0.02323605
Iteration 88, loss = 0.02313095
Iteration 89, loss = 0.02313660
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70845789
Iteration 2, loss = 0.62314742
Iteration 3, loss = 0.51191625
Iteration 4, loss = 0.39325294
Iteration 5, loss = 0.29422034
Iteration 6, loss = 0.21895490
Iteration 7, loss = 0.16652661
Iteration 8, loss = 0.13103941
Iteration 9, loss = 0.10630880
Iteration 10, loss = 0.08883991
Iteration 11, loss = 0.07624631
Iteration 12, loss = 0.06663846
Iteration 13, loss = 0.05937903
Iteration 14, loss = 0.05376056
Iteration 15, loss = 0.04931387
Iteration 16, loss = 0.04534511
Iteration 17, loss = 0.04247010
Iteration 18, loss = 0.03995595
Iteration 19, loss = 0.03780159
Iteration 20, loss = 0.03596800
Iteration 21, loss = 0.03438244
Iteration 22, loss = 0.03300400
Iteration 23, loss = 0.03184997
Iteration 24, loss = 0.03111630
Iteration 25, loss = 0.03024289
Iteration 26, loss = 0.02917888
Iteration 27, loss = 0.02840487
Iteration 28, loss = 0.02777821
Iteration 29, loss = 0.02723550
Iteration 30, loss = 0.02687825
Iteration 31, loss = 0.02631389
Iteration 32, loss = 0.02604536
Iteration 33, loss = 0.02560523
Iteration 34, loss = 0.02541228
Iteration 35, loss = 0.02487914
Iteration 36, loss = 0.02485184
Iteration 37, loss = 0.02452936
Iteration 38, loss = 0.02428618
Iteration 39, loss = 0.02403326
Iteration 40, loss = 0.02388658
Iteration 41, loss = 0.02362371
Iteration 42, loss = 0.02376580
Iteration 43, loss = 0.02332747
Iteration 44, loss = 0.02342225
Iteration 45, loss = 0.02301247
Iteration 46, loss = 0.02294267
Iteration 47, loss = 0.02270401
Iteration 48, loss = 0.02265905
Iteration 49, loss = 0.02263118
Iteration 50, loss = 0.02223060
Iteration 51, loss = 0.02266223
Iteration 52, loss = 0.02232853
Iteration 53, loss = 0.02207727
Iteration 54, loss = 0.02256231
Iteration 55, loss = 0.02186539
Iteration 56, loss = 0.02199488
Iteration 57, loss = 0.02178004
Iteration 58, loss = 0.02172361
Iteration 59, loss = 0.02192583
Iteration 60, loss = 0.02200576
Iteration 61, loss = 0.02200279
Iteration 62, loss = 0.02147073
Iteration 63, loss = 0.02178888
Iteration 64, loss = 0.02163846
Iteration 65, loss = 0.02170517
Iteration 66, loss = 0.02144520
Iteration 67, loss = 0.02162907
Iteration 68, loss = 0.02149568
Iteration 69, loss = 0.02131563
Iteration 70, loss = 0.02142592
Iteration 71, loss = 0.02120282
Iteration 72, loss = 0.02117341
Iteration 73, loss = 0.02124595
Iteration 74, loss = 0.02127862
Iteration 75, loss = 0.02113085
Iteration 76, loss = 0.02148707
Iteration 77, loss = 0.02133623
Iteration 78, loss = 0.02129967
Iteration 79, loss = 0.02096586
Iteration 80, loss = 0.02117086
Iteration 81, loss = 0.02080003
Iteration 82, loss = 0.02098062
Iteration 83, loss = 0.02109211
Iteration 84, loss = 0.02106506
Iteration 85, loss = 0.02119213
Iteration 86, loss = 0.02097070
Iteration 87, loss = 0.02070253
Iteration 88, loss = 0.02079564
Iteration 89, loss = 0.02125728
Iteration 90, loss = 0.02098426
Iteration 91, loss = 0.02116057
Iteration 92, loss = 0.02090810
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67637146
Iteration 2, loss = 0.59805321
Iteration 3, loss = 0.48406295
Iteration 4, loss = 0.37077968
Iteration 5, loss = 0.27825732
Iteration 6, loss = 0.20981626
Iteration 7, loss = 0.16213887
Iteration 8, loss = 0.12955415
Iteration 9, loss = 0.10674435
Iteration 10, loss = 0.09059714
Iteration 11, loss = 0.07876512
Iteration 12, loss = 0.06995711
Iteration 13, loss = 0.06304073
Iteration 14, loss = 0.05772161
Iteration 15, loss = 0.05327498
Iteration 16, loss = 0.04977620
Iteration 17, loss = 0.04681542
Iteration 18, loss = 0.04427226
Iteration 19, loss = 0.04225916
Iteration 20, loss = 0.04050860
Iteration 21, loss = 0.03901566
Iteration 22, loss = 0.03798845
Iteration 23, loss = 0.03664751
Iteration 24, loss = 0.03583129
Iteration 25, loss = 0.03505329
Iteration 26, loss = 0.03421169
Iteration 27, loss = 0.03370497
Iteration 28, loss = 0.03293763
Iteration 29, loss = 0.03268844
Iteration 30, loss = 0.03216863
Iteration 31, loss = 0.03163717
Iteration 32, loss = 0.03139890
Iteration 33, loss = 0.03085434
Iteration 34, loss = 0.03077615
Iteration 35, loss = 0.03014935
Iteration 36, loss = 0.02985440
Iteration 37, loss = 0.02985620
Iteration 38, loss = 0.02953266
Iteration 39, loss = 0.02922401
Iteration 40, loss = 0.02918591
Iteration 41, loss = 0.02902672
Iteration 42, loss = 0.02885290
Iteration 43, loss = 0.02868887
Iteration 44, loss = 0.02847932
Iteration 45, loss = 0.02848994
Iteration 46, loss = 0.02845846
Iteration 47, loss = 0.02860677
Iteration 48, loss = 0.02789333
Iteration 49, loss = 0.02826578
Iteration 50, loss = 0.02777349
Iteration 51, loss = 0.02779171
Iteration 52, loss = 0.02769129
Iteration 53, loss = 0.02763249
Iteration 54, loss = 0.02767248
Iteration 55, loss = 0.02749225
Iteration 56, loss = 0.02736658
Iteration 57, loss = 0.02755343
Iteration 58, loss = 0.02727402
Iteration 59, loss = 0.02732878
Iteration 60, loss = 0.02741780
Iteration 61, loss = 0.02706961
Iteration 62, loss = 0.02705319
Iteration 63, loss = 0.02707112
Iteration 64, loss = 0.02686935
Iteration 65, loss = 0.02709448
Iteration 66, loss = 0.02684947
Iteration 67, loss = 0.02681817
Iteration 68, loss = 0.02684700
Iteration 69, loss = 0.02692082
Iteration 70, loss = 0.02690226
Iteration 71, loss = 0.02683142
Iteration 72, loss = 0.02682574
Iteration 73, loss = 0.02664585
Iteration 74, loss = 0.02651974
Iteration 75, loss = 0.02656947
Iteration 76, loss = 0.02650132
Iteration 77, loss = 0.02664877
Iteration 78, loss = 0.02667060
Iteration 79, loss = 0.02667079
Iteration 80, loss = 0.02643458
Iteration 81, loss = 0.02659312
Iteration 82, loss = 0.02661261
Iteration 83, loss = 0.02655758
Iteration 84, loss = 0.02621299
Iteration 85, loss = 0.02652322
Iteration 86, loss = 0.02654294
Iteration 87, loss = 0.02645772
Iteration 88, loss = 0.02643246
Iteration 89, loss = 0.02631221
Iteration 90, loss = 0.02655840
Iteration 91, loss = 0.02632901
Iteration 92, loss = 0.02646106
Iteration 93, loss = 0.02610453
Iteration 94, loss = 0.02622777
Iteration 95, loss = 0.02616485
Iteration 96, loss = 0.02603792
Iteration 97, loss = 0.02617692
Iteration 98, loss = 0.02616166
Iteration 99, loss = 0.02621865
Iteration 100, loss = 0.02661547
Iteration 101, loss = 0.02648051
Iteration 102, loss = 0.02587121
Iteration 103, loss = 0.02616008
Iteration 104, loss = 0.02627808
Iteration 105, loss = 0.02602170
Iteration 106, loss = 0.02603333
Iteration 107, loss = 0.02620999
Iteration 108, loss = 0.02589541
Iteration 109, loss = 0.02596563
Iteration 110, loss = 0.02603529
Iteration 111, loss = 0.02590887
Iteration 112, loss = 0.02589941
Iteration 113, loss = 0.02624306
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70816341
Iteration 2, loss = 0.63007283
Iteration 3, loss = 0.52279169
Iteration 4, loss = 0.40151831
Iteration 5, loss = 0.29959569
Iteration 6, loss = 0.22311922
Iteration 7, loss = 0.16992649
Iteration 8, loss = 0.13389029
Iteration 9, loss = 0.10901512
Iteration 10, loss = 0.09154913
Iteration 11, loss = 0.07875982
Iteration 12, loss = 0.06914053
Iteration 13, loss = 0.06195832
Iteration 14, loss = 0.05631171
Iteration 15, loss = 0.05169020
Iteration 16, loss = 0.04818807
Iteration 17, loss = 0.04525781
Iteration 18, loss = 0.04276255
Iteration 19, loss = 0.04045152
Iteration 20, loss = 0.03899906
Iteration 21, loss = 0.03730964
Iteration 22, loss = 0.03610245
Iteration 23, loss = 0.03497826
Iteration 24, loss = 0.03414290
Iteration 25, loss = 0.03310378
Iteration 26, loss = 0.03250489
Iteration 27, loss = 0.03174134
Iteration 28, loss = 0.03122053
Iteration 29, loss = 0.03078367
Iteration 30, loss = 0.03023896
Iteration 31, loss = 0.02981513
Iteration 32, loss = 0.02930775
Iteration 33, loss = 0.02904125
Iteration 34, loss = 0.02885150
Iteration 35, loss = 0.02845746
Iteration 36, loss = 0.02823680
Iteration 37, loss = 0.02793141
Iteration 38, loss = 0.02800728
Iteration 39, loss = 0.02739930
Iteration 40, loss = 0.02743775
Iteration 41, loss = 0.02726345
Iteration 42, loss = 0.02698949
Iteration 43, loss = 0.02680706
Iteration 44, loss = 0.02669276
Iteration 45, loss = 0.02669378
Iteration 46, loss = 0.02659841
Iteration 47, loss = 0.02643516
Iteration 48, loss = 0.02640411
Iteration 49, loss = 0.02639046
Iteration 50, loss = 0.02612787
Iteration 51, loss = 0.02587320
Iteration 52, loss = 0.02589305
Iteration 53, loss = 0.02569418
Iteration 54, loss = 0.02591077
Iteration 55, loss = 0.02590182
Iteration 56, loss = 0.02552897
Iteration 57, loss = 0.02553361
Iteration 58, loss = 0.02555042
Iteration 59, loss = 0.02564105
Iteration 60, loss = 0.02524942
Iteration 61, loss = 0.02540136
Iteration 62, loss = 0.02545680
Iteration 63, loss = 0.02519711
Iteration 64, loss = 0.02534409
Iteration 65, loss = 0.02515358
Iteration 66, loss = 0.02541309
Iteration 67, loss = 0.02529970
Iteration 68, loss = 0.02521235
Iteration 69, loss = 0.02514159
Iteration 70, loss = 0.02498719
Iteration 71, loss = 0.02506427
Iteration 72, loss = 0.02503243
Iteration 73, loss = 0.02499330
Iteration 74, loss = 0.02522265
Iteration 75, loss = 0.02492300
Iteration 76, loss = 0.02473544
Iteration 77, loss = 0.02473381
Iteration 78, loss = 0.02505443
Iteration 79, loss = 0.02463450
Iteration 80, loss = 0.02470289
Iteration 81, loss = 0.02491694
Iteration 82, loss = 0.02452840
Iteration 83, loss = 0.02464212
Iteration 84, loss = 0.02461144
Iteration 85, loss = 0.02469169
Iteration 86, loss = 0.02463607
Iteration 87, loss = 0.02485895
Iteration 88, loss = 0.02433327
Iteration 89, loss = 0.02489027
Iteration 90, loss = 0.02439055
Iteration 91, loss = 0.02467697
Iteration 92, loss = 0.02457110
Iteration 93, loss = 0.02427149
Iteration 94, loss = 0.02439785
Iteration 95, loss = 0.02454921
Iteration 96, loss = 0.02446172
Iteration 97, loss = 0.02444378
Iteration 98, loss = 0.02465886
Iteration 99, loss = 0.02463213
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67357894
Iteration 2, loss = 0.59567092
Iteration 3, loss = 0.48638507
Iteration 4, loss = 0.37556227
Iteration 5, loss = 0.28192356
Iteration 6, loss = 0.21177155
Iteration 7, loss = 0.16282510
Iteration 8, loss = 0.12911732
Iteration 9, loss = 0.10599070
Iteration 10, loss = 0.08970477
Iteration 11, loss = 0.07767355
Iteration 12, loss = 0.06863711
Iteration 13, loss = 0.06155037
Iteration 14, loss = 0.05630676
Iteration 15, loss = 0.05195748
Iteration 16, loss = 0.04869525
Iteration 17, loss = 0.04595342
Iteration 18, loss = 0.04342333
Iteration 19, loss = 0.04188495
Iteration 20, loss = 0.04011013
Iteration 21, loss = 0.03879237
Iteration 22, loss = 0.03724736
Iteration 23, loss = 0.03630495
Iteration 24, loss = 0.03535958
Iteration 25, loss = 0.03456081
Iteration 26, loss = 0.03431580
Iteration 27, loss = 0.03354838
Iteration 28, loss = 0.03291508
Iteration 29, loss = 0.03210765
Iteration 30, loss = 0.03198039
Iteration 31, loss = 0.03141971
Iteration 32, loss = 0.03088510
Iteration 33, loss = 0.03056253
Iteration 34, loss = 0.03032292
Iteration 35, loss = 0.02999026
Iteration 36, loss = 0.02980295
Iteration 37, loss = 0.02978964
Iteration 38, loss = 0.02959659
Iteration 39, loss = 0.02925493
Iteration 40, loss = 0.02876988
Iteration 41, loss = 0.02906910
Iteration 42, loss = 0.02887446
Iteration 43, loss = 0.02877878
Iteration 44, loss = 0.02834873
Iteration 45, loss = 0.02824688
Iteration 46, loss = 0.02803783
Iteration 47, loss = 0.02837163
Iteration 48, loss = 0.02815531
Iteration 49, loss = 0.02780884
Iteration 50, loss = 0.02801989
Iteration 51, loss = 0.02776989
Iteration 52, loss = 0.02761230
Iteration 53, loss = 0.02785919
Iteration 54, loss = 0.02732945
Iteration 55, loss = 0.02735518
Iteration 56, loss = 0.02756288
Iteration 57, loss = 0.02748601
Iteration 58, loss = 0.02733139
Iteration 59, loss = 0.02740945
Iteration 60, loss = 0.02722472
Iteration 61, loss = 0.02744070
Iteration 62, loss = 0.02759578
Iteration 63, loss = 0.02704149
Iteration 64, loss = 0.02693119
Iteration 65, loss = 0.02687273
Iteration 66, loss = 0.02686433
Iteration 67, loss = 0.02651787
Iteration 68, loss = 0.02711962
Iteration 69, loss = 0.02695497
Iteration 70, loss = 0.02711520
Iteration 71, loss = 0.02677303
Iteration 72, loss = 0.02692713
Iteration 73, loss = 0.02675237
Iteration 74, loss = 0.02687503
Iteration 75, loss = 0.02692427
Iteration 76, loss = 0.02662596
Iteration 77, loss = 0.02668532
Iteration 78, loss = 0.02676109
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67495143
Iteration 2, loss = 0.59535102
Iteration 3, loss = 0.48422289
Iteration 4, loss = 0.37243195
Iteration 5, loss = 0.28028335
Iteration 6, loss = 0.21140555
Iteration 7, loss = 0.16317942
Iteration 8, loss = 0.12983132
Iteration 9, loss = 0.10674169
Iteration 10, loss = 0.09005491
Iteration 11, loss = 0.07761679
Iteration 12, loss = 0.06838416
Iteration 13, loss = 0.06131320
Iteration 14, loss = 0.05596959
Iteration 15, loss = 0.05147643
Iteration 16, loss = 0.04778245
Iteration 17, loss = 0.04457613
Iteration 18, loss = 0.04228027
Iteration 19, loss = 0.03987467
Iteration 20, loss = 0.03804730
Iteration 21, loss = 0.03662360
Iteration 22, loss = 0.03542834
Iteration 23, loss = 0.03405074
Iteration 24, loss = 0.03332218
Iteration 25, loss = 0.03231565
Iteration 26, loss = 0.03159938
Iteration 27, loss = 0.03073354
Iteration 28, loss = 0.03011569
Iteration 29, loss = 0.02975943
Iteration 30, loss = 0.02896793
Iteration 31, loss = 0.02890512
Iteration 32, loss = 0.02839364
Iteration 33, loss = 0.02804501
Iteration 34, loss = 0.02761731
Iteration 35, loss = 0.02716495
Iteration 36, loss = 0.02693055
Iteration 37, loss = 0.02674411
Iteration 38, loss = 0.02655186
Iteration 39, loss = 0.02621933
Iteration 40, loss = 0.02626269
Iteration 41, loss = 0.02574367
Iteration 42, loss = 0.02562492
Iteration 43, loss = 0.02565354
Iteration 44, loss = 0.02536442
Iteration 45, loss = 0.02534364
Iteration 46, loss = 0.02541077
Iteration 47, loss = 0.02480189
Iteration 48, loss = 0.02480024
Iteration 49, loss = 0.02480755
Iteration 50, loss = 0.02485377
Iteration 51, loss = 0.02470237
Iteration 52, loss = 0.02446464
Iteration 53, loss = 0.02468003
Iteration 54, loss = 0.02443635
Iteration 55, loss = 0.02412294
Iteration 56, loss = 0.02429331
Iteration 57, loss = 0.02462227
Iteration 58, loss = 0.02411977
Iteration 59, loss = 0.02399623
Iteration 60, loss = 0.02403763
Iteration 61, loss = 0.02397538
Iteration 62, loss = 0.02387098
Iteration 63, loss = 0.02378943
Iteration 64, loss = 0.02373367
Iteration 65, loss = 0.02388870
Iteration 66, loss = 0.02397299
Iteration 67, loss = 0.02372506
Iteration 68, loss = 0.02341853
Iteration 69, loss = 0.02378351
Iteration 70, loss = 0.02365277
Iteration 71, loss = 0.02384210
Iteration 72, loss = 0.02325374
Iteration 73, loss = 0.02344954
Iteration 74, loss = 0.02348184
Iteration 75, loss = 0.02320154
Iteration 76, loss = 0.02353956
Iteration 77, loss = 0.02325249
Iteration 78, loss = 0.02300293
Iteration 79, loss = 0.02346245
Iteration 80, loss = 0.02393153
Iteration 81, loss = 0.02356798
Iteration 82, loss = 0.02334165
Iteration 83, loss = 0.02334506
Iteration 84, loss = 0.02316220
Iteration 85, loss = 0.02318885
Iteration 86, loss = 0.02314855
Iteration 87, loss = 0.02323605
Iteration 88, loss = 0.02313095
Iteration 89, loss = 0.02313660
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70845789
Iteration 2, loss = 0.62314742
Iteration 3, loss = 0.51191625
Iteration 4, loss = 0.39325294
Iteration 5, loss = 0.29422034
Iteration 6, loss = 0.21895490
Iteration 7, loss = 0.16652661
Iteration 8, loss = 0.13103941
Iteration 9, loss = 0.10630880
Iteration 10, loss = 0.08883991
Iteration 11, loss = 0.07624631
Iteration 12, loss = 0.06663846
Iteration 13, loss = 0.05937903
Iteration 14, loss = 0.05376056
Iteration 15, loss = 0.04931387
Iteration 16, loss = 0.04534511
Iteration 17, loss = 0.04247010
Iteration 18, loss = 0.03995595
Iteration 19, loss = 0.03780159
Iteration 20, loss = 0.03596800
Iteration 21, loss = 0.03438244
Iteration 22, loss = 0.03300400
Iteration 23, loss = 0.03184997
Iteration 24, loss = 0.03111630
Iteration 25, loss = 0.03024289
Iteration 26, loss = 0.02917888
Iteration 27, loss = 0.02840487
Iteration 28, loss = 0.02777821
Iteration 29, loss = 0.02723550
Iteration 30, loss = 0.02687825
Iteration 31, loss = 0.02631389
Iteration 32, loss = 0.02604536
Iteration 33, loss = 0.02560523
Iteration 34, loss = 0.02541228
Iteration 35, loss = 0.02487914
Iteration 36, loss = 0.02485184
Iteration 37, loss = 0.02452936
Iteration 38, loss = 0.02428618
Iteration 39, loss = 0.02403326
Iteration 40, loss = 0.02388658
Iteration 41, loss = 0.02362371
Iteration 42, loss = 0.02376580
Iteration 43, loss = 0.02332747
Iteration 44, loss = 0.02342225
Iteration 45, loss = 0.02301247
Iteration 46, loss = 0.02294267
Iteration 47, loss = 0.02270401
Iteration 48, loss = 0.02265905
Iteration 49, loss = 0.02263118
Iteration 50, loss = 0.02223060
Iteration 51, loss = 0.02266223
Iteration 52, loss = 0.02232853
Iteration 53, loss = 0.02207727
Iteration 54, loss = 0.02256231
Iteration 55, loss = 0.02186539
Iteration 56, loss = 0.02199488
Iteration 57, loss = 0.02178004
Iteration 58, loss = 0.02172361
Iteration 59, loss = 0.02192583
Iteration 60, loss = 0.02200576
Iteration 61, loss = 0.02200279
Iteration 62, loss = 0.02147073
Iteration 63, loss = 0.02178888
Iteration 64, loss = 0.02163846
Iteration 65, loss = 0.02170517
Iteration 66, loss = 0.02144520
Iteration 67, loss = 0.02162907
Iteration 68, loss = 0.02149568
Iteration 69, loss = 0.02131563
Iteration 70, loss = 0.02142592
Iteration 71, loss = 0.02120282
Iteration 72, loss = 0.02117341
Iteration 73, loss = 0.02124595
Iteration 74, loss = 0.02127862
Iteration 75, loss = 0.02113085
Iteration 76, loss = 0.02148707
Iteration 77, loss = 0.02133623
Iteration 78, loss = 0.02129967
Iteration 79, loss = 0.02096586
Iteration 80, loss = 0.02117086
Iteration 81, loss = 0.02080003
Iteration 82, loss = 0.02098062
Iteration 83, loss = 0.02109211
Iteration 84, loss = 0.02106506
Iteration 85, loss = 0.02119213
Iteration 86, loss = 0.02097070
Iteration 87, loss = 0.02070253
Iteration 88, loss = 0.02079564
Iteration 89, loss = 0.02125728
Iteration 90, loss = 0.02098426
Iteration 91, loss = 0.02116057
Iteration 92, loss = 0.02090810
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67637146
Iteration 2, loss = 0.59805321
Iteration 3, loss = 0.48406295
Iteration 4, loss = 0.37077968
Iteration 5, loss = 0.27825732
Iteration 6, loss = 0.20981626
Iteration 7, loss = 0.16213887
Iteration 8, loss = 0.12955415
Iteration 9, loss = 0.10674435
Iteration 10, loss = 0.09059714
Iteration 11, loss = 0.07876512
Iteration 12, loss = 0.06995711
Iteration 13, loss = 0.06304073
Iteration 14, loss = 0.05772161
Iteration 15, loss = 0.05327498
Iteration 16, loss = 0.04977620
Iteration 17, loss = 0.04681542
Iteration 18, loss = 0.04427226
Iteration 19, loss = 0.04225916
Iteration 20, loss = 0.04050860
Iteration 21, loss = 0.03901566
Iteration 22, loss = 0.03798845
Iteration 23, loss = 0.03664751
Iteration 24, loss = 0.03583129
Iteration 25, loss = 0.03505329
Iteration 26, loss = 0.03421169
Iteration 27, loss = 0.03370497
Iteration 28, loss = 0.03293763
Iteration 29, loss = 0.03268844
Iteration 30, loss = 0.03216863
Iteration 31, loss = 0.03163717
Iteration 32, loss = 0.03139890
Iteration 33, loss = 0.03085434
Iteration 34, loss = 0.03077615
Iteration 35, loss = 0.03014935
Iteration 36, loss = 0.02985440
Iteration 37, loss = 0.02985620
Iteration 38, loss = 0.02953266
Iteration 39, loss = 0.02922401
Iteration 40, loss = 0.02918591
Iteration 41, loss = 0.02902672
Iteration 42, loss = 0.02885290
Iteration 43, loss = 0.02868887
Iteration 44, loss = 0.02847932
Iteration 45, loss = 0.02848994
Iteration 46, loss = 0.02845846
Iteration 47, loss = 0.02860677
Iteration 48, loss = 0.02789333
Iteration 49, loss = 0.02826578
Iteration 50, loss = 0.02777349
Iteration 51, loss = 0.02779171
Iteration 52, loss = 0.02769129
Iteration 53, loss = 0.02763249
Iteration 54, loss = 0.02767248
Iteration 55, loss = 0.02749225
Iteration 56, loss = 0.02736658
Iteration 57, loss = 0.02755343
Iteration 58, loss = 0.02727402
Iteration 59, loss = 0.02732878
Iteration 60, loss = 0.02741780
Iteration 61, loss = 0.02706961
Iteration 62, loss = 0.02705319
Iteration 63, loss = 0.02707112
Iteration 64, loss = 0.02686935
Iteration 65, loss = 0.02709448
Iteration 66, loss = 0.02684947
Iteration 67, loss = 0.02681817
Iteration 68, loss = 0.02684700
Iteration 69, loss = 0.02692082
Iteration 70, loss = 0.02690226
Iteration 71, loss = 0.02683142
Iteration 72, loss = 0.02682574
Iteration 73, loss = 0.02664585
Iteration 74, loss = 0.02651974
Iteration 75, loss = 0.02656947
Iteration 76, loss = 0.02650132
Iteration 77, loss = 0.02664877
Iteration 78, loss = 0.02667060
Iteration 79, loss = 0.02667079
Iteration 80, loss = 0.02643458
Iteration 81, loss = 0.02659312
Iteration 82, loss = 0.02661261
Iteration 83, loss = 0.02655758
Iteration 84, loss = 0.02621299
Iteration 85, loss = 0.02652322
Iteration 86, loss = 0.02654294
Iteration 87, loss = 0.02645772
Iteration 88, loss = 0.02643246
Iteration 89, loss = 0.02631221
Iteration 90, loss = 0.02655840
Iteration 91, loss = 0.02632901
Iteration 92, loss = 0.02646106
Iteration 93, loss = 0.02610453
Iteration 94, loss = 0.02622777
Iteration 95, loss = 0.02616485
Iteration 96, loss = 0.02603792
Iteration 97, loss = 0.02617692
Iteration 98, loss = 0.02616166
Iteration 99, loss = 0.02621865
Iteration 100, loss = 0.02661547
Iteration 101, loss = 0.02648051
Iteration 102, loss = 0.02587121
Iteration 103, loss = 0.02616008
Iteration 104, loss = 0.02627808
Iteration 105, loss = 0.02602170
Iteration 106, loss = 0.02603333
Iteration 107, loss = 0.02620999
Iteration 108, loss = 0.02589541
Iteration 109, loss = 0.02596563
Iteration 110, loss = 0.02603529
Iteration 111, loss = 0.02590887
Iteration 112, loss = 0.02589941
Iteration 113, loss = 0.02624306
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70816341
Iteration 2, loss = 0.63007283
Iteration 3, loss = 0.52279169
Iteration 4, loss = 0.40151831
Iteration 5, loss = 0.29959569
Iteration 6, loss = 0.22311922
Iteration 7, loss = 0.16992649
Iteration 8, loss = 0.13389029
Iteration 9, loss = 0.10901512
Iteration 10, loss = 0.09154913
Iteration 11, loss = 0.07875982
Iteration 12, loss = 0.06914053
Iteration 13, loss = 0.06195832
Iteration 14, loss = 0.05631171
Iteration 15, loss = 0.05169020
Iteration 16, loss = 0.04818807
Iteration 17, loss = 0.04525781
Iteration 18, loss = 0.04276255
Iteration 19, loss = 0.04045152
Iteration 20, loss = 0.03899906
Iteration 21, loss = 0.03730964
Iteration 22, loss = 0.03610245
Iteration 23, loss = 0.03497826
Iteration 24, loss = 0.03414290
Iteration 25, loss = 0.03310378
Iteration 26, loss = 0.03250489
Iteration 27, loss = 0.03174134
Iteration 28, loss = 0.03122053
Iteration 29, loss = 0.03078367
Iteration 30, loss = 0.03023896
Iteration 31, loss = 0.02981513
Iteration 32, loss = 0.02930775
Iteration 33, loss = 0.02904125
Iteration 34, loss = 0.02885150
Iteration 35, loss = 0.02845746
Iteration 36, loss = 0.02823680
Iteration 37, loss = 0.02793141
Iteration 38, loss = 0.02800728
Iteration 39, loss = 0.02739930
Iteration 40, loss = 0.02743775
Iteration 41, loss = 0.02726345
Iteration 42, loss = 0.02698949
Iteration 43, loss = 0.02680706
Iteration 44, loss = 0.02669276
Iteration 45, loss = 0.02669378
Iteration 46, loss = 0.02659841
Iteration 47, loss = 0.02643516
Iteration 48, loss = 0.02640411
Iteration 49, loss = 0.02639046
Iteration 50, loss = 0.02612787
Iteration 51, loss = 0.02587320
Iteration 52, loss = 0.02589305
Iteration 53, loss = 0.02569418
Iteration 54, loss = 0.02591077
Iteration 55, loss = 0.02590182
Iteration 56, loss = 0.02552897
Iteration 57, loss = 0.02553361
Iteration 58, loss = 0.02555042
Iteration 59, loss = 0.02564105
Iteration 60, loss = 0.02524942
Iteration 61, loss = 0.02540136
Iteration 62, loss = 0.02545680
Iteration 63, loss = 0.02519711
Iteration 64, loss = 0.02534409
Iteration 65, loss = 0.02515358
Iteration 66, loss = 0.02541309
Iteration 67, loss = 0.02529970
Iteration 68, loss = 0.02521235
Iteration 69, loss = 0.02514159
Iteration 70, loss = 0.02498719
Iteration 71, loss = 0.02506427
Iteration 72, loss = 0.02503243
Iteration 73, loss = 0.02499330
Iteration 74, loss = 0.02522265
Iteration 75, loss = 0.02492300
Iteration 76, loss = 0.02473544
Iteration 77, loss = 0.02473381
Iteration 78, loss = 0.02505443
Iteration 79, loss = 0.02463450
Iteration 80, loss = 0.02470289
Iteration 81, loss = 0.02491694
Iteration 82, loss = 0.02452840
Iteration 83, loss = 0.02464212
Iteration 84, loss = 0.02461144
Iteration 85, loss = 0.02469169
Iteration 86, loss = 0.02463607
Iteration 87, loss = 0.02485895
Iteration 88, loss = 0.02433327
Iteration 89, loss = 0.02489027
Iteration 90, loss = 0.02439055
Iteration 91, loss = 0.02467697
Iteration 92, loss = 0.02457110
Iteration 93, loss = 0.02427149
Iteration 94, loss = 0.02439785
Iteration 95, loss = 0.02454921
Iteration 96, loss = 0.02446172
Iteration 97, loss = 0.02444378
Iteration 98, loss = 0.02465886
Iteration 99, loss = 0.02463213
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67357894
Iteration 2, loss = 0.59567092
Iteration 3, loss = 0.48638507
Iteration 4, loss = 0.37556227
Iteration 5, loss = 0.28192356
Iteration 6, loss = 0.21177155
Iteration 7, loss = 0.16282510
Iteration 8, loss = 0.12911732
Iteration 9, loss = 0.10599070
Iteration 10, loss = 0.08970477
Iteration 11, loss = 0.07767355
Iteration 12, loss = 0.06863711
Iteration 13, loss = 0.06155037
Iteration 14, loss = 0.05630676
Iteration 15, loss = 0.05195748
Iteration 16, loss = 0.04869525
Iteration 17, loss = 0.04595342
Iteration 18, loss = 0.04342333
Iteration 19, loss = 0.04188495
Iteration 20, loss = 0.04011013
Iteration 21, loss = 0.03879237
Iteration 22, loss = 0.03724736
Iteration 23, loss = 0.03630495
Iteration 24, loss = 0.03535958
Iteration 25, loss = 0.03456081
Iteration 26, loss = 0.03431580
Iteration 27, loss = 0.03354838
Iteration 28, loss = 0.03291508
Iteration 29, loss = 0.03210765
Iteration 30, loss = 0.03198039
Iteration 31, loss = 0.03141971
Iteration 32, loss = 0.03088510
Iteration 33, loss = 0.03056253
Iteration 34, loss = 0.03032292
Iteration 35, loss = 0.02999026
Iteration 36, loss = 0.02980295
Iteration 37, loss = 0.02978964
Iteration 38, loss = 0.02959659
Iteration 39, loss = 0.02925493
Iteration 40, loss = 0.02876988
Iteration 41, loss = 0.02906910
Iteration 42, loss = 0.02887446
Iteration 43, loss = 0.02877878
Iteration 44, loss = 0.02834873
Iteration 45, loss = 0.02824688
Iteration 46, loss = 0.02803783
Iteration 47, loss = 0.02837163
Iteration 48, loss = 0.02815531
Iteration 49, loss = 0.02780884
Iteration 50, loss = 0.02801989
Iteration 51, loss = 0.02776989
Iteration 52, loss = 0.02761230
Iteration 53, loss = 0.02785919
Iteration 54, loss = 0.02732945
Iteration 55, loss = 0.02735518
Iteration 56, loss = 0.02756288
Iteration 57, loss = 0.02748601
Iteration 58, loss = 0.02733139
Iteration 59, loss = 0.02740945
Iteration 60, loss = 0.02722472
Iteration 61, loss = 0.02744070
Iteration 62, loss = 0.02759578
Iteration 63, loss = 0.02704149
Iteration 64, loss = 0.02693119
Iteration 65, loss = 0.02687273
Iteration 66, loss = 0.02686433
Iteration 67, loss = 0.02651787
Iteration 68, loss = 0.02711962
Iteration 69, loss = 0.02695497
Iteration 70, loss = 0.02711520
Iteration 71, loss = 0.02677303
Iteration 72, loss = 0.02692713
Iteration 73, loss = 0.02675237
Iteration 74, loss = 0.02687503
Iteration 75, loss = 0.02692427
Iteration 76, loss = 0.02662596
Iteration 77, loss = 0.02668532
Iteration 78, loss = 0.02676109
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67495143
Iteration 2, loss = 0.59535102
Iteration 3, loss = 0.48422289
Iteration 4, loss = 0.37243195
Iteration 5, loss = 0.28028335
Iteration 6, loss = 0.21140555
Iteration 7, loss = 0.16317942
Iteration 8, loss = 0.12983132
Iteration 9, loss = 0.10674169
Iteration 10, loss = 0.09005491
Iteration 11, loss = 0.07761679
Iteration 12, loss = 0.06838416
Iteration 13, loss = 0.06131320
Iteration 14, loss = 0.05596959
Iteration 15, loss = 0.05147643
Iteration 16, loss = 0.04778245
Iteration 17, loss = 0.04457613
Iteration 18, loss = 0.04228027
Iteration 19, loss = 0.03987467
Iteration 20, loss = 0.03804730
Iteration 21, loss = 0.03662360
Iteration 22, loss = 0.03542834
Iteration 23, loss = 0.03405074
Iteration 24, loss = 0.03332218
Iteration 25, loss = 0.03231565
Iteration 26, loss = 0.03159938
Iteration 27, loss = 0.03073354
Iteration 28, loss = 0.03011569
Iteration 29, loss = 0.02975943
Iteration 30, loss = 0.02896793
Iteration 31, loss = 0.02890512
Iteration 32, loss = 0.02839364
Iteration 33, loss = 0.02804501
Iteration 34, loss = 0.02761731
Iteration 35, loss = 0.02716495
Iteration 36, loss = 0.02693055
Iteration 37, loss = 0.02674411
Iteration 38, loss = 0.02655186
Iteration 39, loss = 0.02621933
Iteration 40, loss = 0.02626269
Iteration 41, loss = 0.02574367
Iteration 42, loss = 0.02562492
Iteration 43, loss = 0.02565354
Iteration 44, loss = 0.02536442
Iteration 45, loss = 0.02534364
Iteration 46, loss = 0.02541077
Iteration 47, loss = 0.02480189
Iteration 48, loss = 0.02480024
Iteration 49, loss = 0.02480755
Iteration 50, loss = 0.02485377
Iteration 51, loss = 0.02470237
Iteration 52, loss = 0.02446464
Iteration 53, loss = 0.02468003
Iteration 54, loss = 0.02443635
Iteration 55, loss = 0.02412294
Iteration 56, loss = 0.02429331
Iteration 57, loss = 0.02462227
Iteration 58, loss = 0.02411977
Iteration 59, loss = 0.02399623
Iteration 60, loss = 0.02403763
Iteration 61, loss = 0.02397538
Iteration 62, loss = 0.02387098
Iteration 63, loss = 0.02378943
Iteration 64, loss = 0.02373367
Iteration 65, loss = 0.02388870
Iteration 66, loss = 0.02397299
Iteration 67, loss = 0.02372506
Iteration 68, loss = 0.02341853
Iteration 69, loss = 0.02378351
Iteration 70, loss = 0.02365277
Iteration 71, loss = 0.02384210
Iteration 72, loss = 0.02325374
Iteration 73, loss = 0.02344954
Iteration 74, loss = 0.02348184
Iteration 75, loss = 0.02320154
Iteration 76, loss = 0.02353956
Iteration 77, loss = 0.02325249
Iteration 78, loss = 0.02300293
Iteration 79, loss = 0.02346245
Iteration 80, loss = 0.02393153
Iteration 81, loss = 0.02356798
Iteration 82, loss = 0.02334165
Iteration 83, loss = 0.02334506
Iteration 84, loss = 0.02316220
Iteration 85, loss = 0.02318885
Iteration 86, loss = 0.02314855
Iteration 87, loss = 0.02323605
Iteration 88, loss = 0.02313095
Iteration 89, loss = 0.02313660
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70845789
Iteration 2, loss = 0.62314742
Iteration 3, loss = 0.51191625
Iteration 4, loss = 0.39325294
Iteration 5, loss = 0.29422034
Iteration 6, loss = 0.21895490
Iteration 7, loss = 0.16652661
Iteration 8, loss = 0.13103941
Iteration 9, loss = 0.10630880
Iteration 10, loss = 0.08883991
Iteration 11, loss = 0.07624631
Iteration 12, loss = 0.06663846
Iteration 13, loss = 0.05937903
Iteration 14, loss = 0.05376056
Iteration 15, loss = 0.04931387
Iteration 16, loss = 0.04534511
Iteration 17, loss = 0.04247010
Iteration 18, loss = 0.03995595
Iteration 19, loss = 0.03780159
Iteration 20, loss = 0.03596800
Iteration 21, loss = 0.03438244
Iteration 22, loss = 0.03300400
Iteration 23, loss = 0.03184997
Iteration 24, loss = 0.03111630
Iteration 25, loss = 0.03024289
Iteration 26, loss = 0.02917888
Iteration 27, loss = 0.02840487
Iteration 28, loss = 0.02777821
Iteration 29, loss = 0.02723550
Iteration 30, loss = 0.02687825
Iteration 31, loss = 0.02631389
Iteration 32, loss = 0.02604536
Iteration 33, loss = 0.02560523
Iteration 34, loss = 0.02541228
Iteration 35, loss = 0.02487914
Iteration 36, loss = 0.02485184
Iteration 37, loss = 0.02452936
Iteration 38, loss = 0.02428618
Iteration 39, loss = 0.02403326
Iteration 40, loss = 0.02388658
Iteration 41, loss = 0.02362371
Iteration 42, loss = 0.02376580
Iteration 43, loss = 0.02332747
Iteration 44, loss = 0.02342225
Iteration 45, loss = 0.02301247
Iteration 46, loss = 0.02294267
Iteration 47, loss = 0.02270401
Iteration 48, loss = 0.02265905
Iteration 49, loss = 0.02263118
Iteration 50, loss = 0.02223060
Iteration 51, loss = 0.02266223
Iteration 52, loss = 0.02232853
Iteration 53, loss = 0.02207727
Iteration 54, loss = 0.02256231
Iteration 55, loss = 0.02186539
Iteration 56, loss = 0.02199488
Iteration 57, loss = 0.02178004
Iteration 58, loss = 0.02172361
Iteration 59, loss = 0.02192583
Iteration 60, loss = 0.02200576
Iteration 61, loss = 0.02200279
Iteration 62, loss = 0.02147073
Iteration 63, loss = 0.02178888
Iteration 64, loss = 0.02163846
Iteration 65, loss = 0.02170517
Iteration 66, loss = 0.02144520
Iteration 67, loss = 0.02162907
Iteration 68, loss = 0.02149568
Iteration 69, loss = 0.02131563
Iteration 70, loss = 0.02142592
Iteration 71, loss = 0.02120282
Iteration 72, loss = 0.02117341
Iteration 73, loss = 0.02124595
Iteration 74, loss = 0.02127862
Iteration 75, loss = 0.02113085
Iteration 76, loss = 0.02148707
Iteration 77, loss = 0.02133623
Iteration 78, loss = 0.02129967
Iteration 79, loss = 0.02096586
Iteration 80, loss = 0.02117086
Iteration 81, loss = 0.02080003
Iteration 82, loss = 0.02098062
Iteration 83, loss = 0.02109211
Iteration 84, loss = 0.02106506
Iteration 85, loss = 0.02119213
Iteration 86, loss = 0.02097070
Iteration 87, loss = 0.02070253
Iteration 88, loss = 0.02079564
Iteration 89, loss = 0.02125728
Iteration 90, loss = 0.02098426
Iteration 91, loss = 0.02116057
Iteration 92, loss = 0.02090810
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67637146
Iteration 2, loss = 0.59805321
Iteration 3, loss = 0.48406295
Iteration 4, loss = 0.37077968
Iteration 5, loss = 0.27825732
Iteration 6, loss = 0.20981626
Iteration 7, loss = 0.16213887
Iteration 8, loss = 0.12955415
Iteration 9, loss = 0.10674435
Iteration 10, loss = 0.09059714
Iteration 11, loss = 0.07876512
Iteration 12, loss = 0.06995711
Iteration 13, loss = 0.06304073
Iteration 14, loss = 0.05772161
Iteration 15, loss = 0.05327498
Iteration 16, loss = 0.04977620
Iteration 17, loss = 0.04681542
Iteration 18, loss = 0.04427226
Iteration 19, loss = 0.04225916
Iteration 20, loss = 0.04050860
Iteration 21, loss = 0.03901566
Iteration 22, loss = 0.03798845
Iteration 23, loss = 0.03664751
Iteration 24, loss = 0.03583129
Iteration 25, loss = 0.03505329
Iteration 26, loss = 0.03421169
Iteration 27, loss = 0.03370497
Iteration 28, loss = 0.03293763
Iteration 29, loss = 0.03268844
Iteration 30, loss = 0.03216863
Iteration 31, loss = 0.03163717
Iteration 32, loss = 0.03139890
Iteration 33, loss = 0.03085434
Iteration 34, loss = 0.03077615
Iteration 35, loss = 0.03014935
Iteration 36, loss = 0.02985440
Iteration 37, loss = 0.02985620
Iteration 38, loss = 0.02953266
Iteration 39, loss = 0.02922401
Iteration 40, loss = 0.02918591
Iteration 41, loss = 0.02902672
Iteration 42, loss = 0.02885290
Iteration 43, loss = 0.02868887
Iteration 44, loss = 0.02847932
Iteration 45, loss = 0.02848994
Iteration 46, loss = 0.02845846
Iteration 47, loss = 0.02860677
Iteration 48, loss = 0.02789333
Iteration 49, loss = 0.02826578
Iteration 50, loss = 0.02777349
Iteration 51, loss = 0.02779171
Iteration 52, loss = 0.02769129
Iteration 53, loss = 0.02763249
Iteration 54, loss = 0.02767248
Iteration 55, loss = 0.02749225
Iteration 56, loss = 0.02736658
Iteration 57, loss = 0.02755343
Iteration 58, loss = 0.02727402
Iteration 59, loss = 0.02732878
Iteration 60, loss = 0.02741780
Iteration 61, loss = 0.02706961
Iteration 62, loss = 0.02705319
Iteration 63, loss = 0.02707112
Iteration 64, loss = 0.02686935
Iteration 65, loss = 0.02709448
Iteration 66, loss = 0.02684947
Iteration 67, loss = 0.02681817
Iteration 68, loss = 0.02684700
Iteration 69, loss = 0.02692082
Iteration 70, loss = 0.02690226
Iteration 71, loss = 0.02683142
Iteration 72, loss = 0.02682574
Iteration 73, loss = 0.02664585
Iteration 74, loss = 0.02651974
Iteration 75, loss = 0.02656947
Iteration 76, loss = 0.02650132
Iteration 77, loss = 0.02664877
Iteration 78, loss = 0.02667060
Iteration 79, loss = 0.02667079
Iteration 80, loss = 0.02643458
Iteration 81, loss = 0.02659312
Iteration 82, loss = 0.02661261
Iteration 83, loss = 0.02655758
Iteration 84, loss = 0.02621299
Iteration 85, loss = 0.02652322
Iteration 86, loss = 0.02654294
Iteration 87, loss = 0.02645772
Iteration 88, loss = 0.02643246
Iteration 89, loss = 0.02631221
Iteration 90, loss = 0.02655840
Iteration 91, loss = 0.02632901
Iteration 92, loss = 0.02646106
Iteration 93, loss = 0.02610453
Iteration 94, loss = 0.02622777
Iteration 95, loss = 0.02616485
Iteration 96, loss = 0.02603792
Iteration 97, loss = 0.02617692
Iteration 98, loss = 0.02616166
Iteration 99, loss = 0.02621865
Iteration 100, loss = 0.02661547
Iteration 101, loss = 0.02648051
Iteration 102, loss = 0.02587121
Iteration 103, loss = 0.02616008
Iteration 104, loss = 0.02627808
Iteration 105, loss = 0.02602170
Iteration 106, loss = 0.02603333
Iteration 107, loss = 0.02620999
Iteration 108, loss = 0.02589541
Iteration 109, loss = 0.02596563
Iteration 110, loss = 0.02603529
Iteration 111, loss = 0.02590887
Iteration 112, loss = 0.02589941
Iteration 113, loss = 0.02624306
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70816341
Iteration 2, loss = 0.63007283
Iteration 3, loss = 0.52279169
Iteration 4, loss = 0.40151831
Iteration 5, loss = 0.29959569
Iteration 6, loss = 0.22311922
Iteration 7, loss = 0.16992649
Iteration 8, loss = 0.13389029
Iteration 9, loss = 0.10901512
Iteration 10, loss = 0.09154913
Iteration 11, loss = 0.07875982
Iteration 12, loss = 0.06914053
Iteration 13, loss = 0.06195832
Iteration 14, loss = 0.05631171
Iteration 15, loss = 0.05169020
Iteration 16, loss = 0.04818807
Iteration 17, loss = 0.04525781
Iteration 18, loss = 0.04276255
Iteration 19, loss = 0.04045152
Iteration 20, loss = 0.03899906
Iteration 21, loss = 0.03730964
Iteration 22, loss = 0.03610245
Iteration 23, loss = 0.03497826
Iteration 24, loss = 0.03414290
Iteration 25, loss = 0.03310378
Iteration 26, loss = 0.03250489
Iteration 27, loss = 0.03174134
Iteration 28, loss = 0.03122053
Iteration 29, loss = 0.03078367
Iteration 30, loss = 0.03023896
Iteration 31, loss = 0.02981513
Iteration 32, loss = 0.02930775
Iteration 33, loss = 0.02904125
Iteration 34, loss = 0.02885150
Iteration 35, loss = 0.02845746
Iteration 36, loss = 0.02823680
Iteration 37, loss = 0.02793141
Iteration 38, loss = 0.02800728
Iteration 39, loss = 0.02739930
Iteration 40, loss = 0.02743775
Iteration 41, loss = 0.02726345
Iteration 42, loss = 0.02698949
Iteration 43, loss = 0.02680706
Iteration 44, loss = 0.02669276
Iteration 45, loss = 0.02669378
Iteration 46, loss = 0.02659841
Iteration 47, loss = 0.02643516
Iteration 48, loss = 0.02640411
Iteration 49, loss = 0.02639046
Iteration 50, loss = 0.02612787
Iteration 51, loss = 0.02587320
Iteration 52, loss = 0.02589305
Iteration 53, loss = 0.02569418
Iteration 54, loss = 0.02591077
Iteration 55, loss = 0.02590182
Iteration 56, loss = 0.02552897
Iteration 57, loss = 0.02553361
Iteration 58, loss = 0.02555042
Iteration 59, loss = 0.02564105
Iteration 60, loss = 0.02524942
Iteration 61, loss = 0.02540136
Iteration 62, loss = 0.02545680
Iteration 63, loss = 0.02519711
Iteration 64, loss = 0.02534409
Iteration 65, loss = 0.02515358
Iteration 66, loss = 0.02541309
Iteration 67, loss = 0.02529970
Iteration 68, loss = 0.02521235
Iteration 69, loss = 0.02514159
Iteration 70, loss = 0.02498719
Iteration 71, loss = 0.02506427
Iteration 72, loss = 0.02503243
Iteration 73, loss = 0.02499330
Iteration 74, loss = 0.02522265
Iteration 75, loss = 0.02492300
Iteration 76, loss = 0.02473544
Iteration 77, loss = 0.02473381
Iteration 78, loss = 0.02505443
Iteration 79, loss = 0.02463450
Iteration 80, loss = 0.02470289
Iteration 81, loss = 0.02491694
Iteration 82, loss = 0.02452840
Iteration 83, loss = 0.02464212
Iteration 84, loss = 0.02461144
Iteration 85, loss = 0.02469169
Iteration 86, loss = 0.02463607
Iteration 87, loss = 0.02485895
Iteration 88, loss = 0.02433327
Iteration 89, loss = 0.02489027
Iteration 90, loss = 0.02439055
Iteration 91, loss = 0.02467697
Iteration 92, loss = 0.02457110
Iteration 93, loss = 0.02427149
Iteration 94, loss = 0.02439785
Iteration 95, loss = 0.02454921
Iteration 96, loss = 0.02446172
Iteration 97, loss = 0.02444378
Iteration 98, loss = 0.02465886
Iteration 99, loss = 0.02463213
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67231016
Iteration 2, loss = 0.58249944
Iteration 3, loss = 0.44887693
Iteration 4, loss = 0.32598138
Iteration 5, loss = 0.23385462
Iteration 6, loss = 0.17222639
Iteration 7, loss = 0.13220797
Iteration 8, loss = 0.10604575
Iteration 9, loss = 0.08813774
Iteration 10, loss = 0.07581111
Iteration 11, loss = 0.06652633
Iteration 12, loss = 0.05986196
Iteration 13, loss = 0.05461711
Iteration 14, loss = 0.05049575
Iteration 15, loss = 0.04751628
Iteration 16, loss = 0.04485723
Iteration 17, loss = 0.04283834
Iteration 18, loss = 0.04075839
Iteration 19, loss = 0.03935308
Iteration 20, loss = 0.03778917
Iteration 21, loss = 0.03688739
Iteration 22, loss = 0.03601369
Iteration 23, loss = 0.03518393
Iteration 24, loss = 0.03430820
Iteration 25, loss = 0.03373686
Iteration 26, loss = 0.03307086
Iteration 27, loss = 0.03257093
Iteration 28, loss = 0.03196868
Iteration 29, loss = 0.03181759
Iteration 30, loss = 0.03146986
Iteration 31, loss = 0.03099056
Iteration 32, loss = 0.03087793
Iteration 33, loss = 0.03061387
Iteration 34, loss = 0.03032393
Iteration 35, loss = 0.03009780
Iteration 36, loss = 0.02995282
Iteration 37, loss = 0.02987234
Iteration 38, loss = 0.02980198
Iteration 39, loss = 0.02957072
Iteration 40, loss = 0.02945207
Iteration 41, loss = 0.02917251
Iteration 42, loss = 0.02896427
Iteration 43, loss = 0.02906984
Iteration 44, loss = 0.02897130
Iteration 45, loss = 0.02889870
Iteration 46, loss = 0.02867596
Iteration 47, loss = 0.02897487
Iteration 48, loss = 0.02834746
Iteration 49, loss = 0.02835790
Iteration 50, loss = 0.02832300
Iteration 51, loss = 0.02842752
Iteration 52, loss = 0.02838078
Iteration 53, loss = 0.02838836
Iteration 54, loss = 0.02830127
Iteration 55, loss = 0.02789894
Iteration 56, loss = 0.02790897
Iteration 57, loss = 0.02805880
Iteration 58, loss = 0.02833903
Iteration 59, loss = 0.02811964
Iteration 60, loss = 0.02789767
Iteration 61, loss = 0.02795405
Iteration 62, loss = 0.02770887
Iteration 63, loss = 0.02777365
Iteration 64, loss = 0.02783745
Iteration 65, loss = 0.02797642
Iteration 66, loss = 0.02764363
Iteration 67, loss = 0.02788226
Iteration 68, loss = 0.02785186
Iteration 69, loss = 0.02765711
Iteration 70, loss = 0.02770161
Iteration 71, loss = 0.02754298
Iteration 72, loss = 0.02760671
Iteration 73, loss = 0.02788450
Iteration 74, loss = 0.02747033
Iteration 75, loss = 0.02761864
Iteration 76, loss = 0.02747547
Iteration 77, loss = 0.02754754
Iteration 78, loss = 0.02776428
Iteration 79, loss = 0.02752176
Iteration 80, loss = 0.02765741
Iteration 81, loss = 0.02743700
Iteration 82, loss = 0.02738308
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on kept_v2_no_emojis_mentions...
Iteration 1, loss = 0.70902860
Iteration 2, loss = 0.66057819
Iteration 3, loss = 0.58819378
Iteration 4, loss = 0.49644155
Iteration 5, loss = 0.40375183
Iteration 6, loss = 0.32104433
Iteration 7, loss = 0.25457391
Iteration 8, loss = 0.20298074
Iteration 9, loss = 0.16443531
Iteration 10, loss = 0.13616309
Iteration 11, loss = 0.11463523
Iteration 12, loss = 0.09868226
Iteration 13, loss = 0.08628344
Iteration 14, loss = 0.07648630
Iteration 15, loss = 0.06896511
Iteration 16, loss = 0.06269377
Iteration 17, loss = 0.05768591
Iteration 18, loss = 0.05353239
Iteration 19, loss = 0.05020224
Iteration 20, loss = 0.04715794
Iteration 21, loss = 0.04452008
Iteration 22, loss = 0.04250373
Iteration 23, loss = 0.04053937
Iteration 24, loss = 0.03889537
Iteration 25, loss = 0.03748309
Iteration 26, loss = 0.03612013
Iteration 27, loss = 0.03514836
Iteration 28, loss = 0.03405646
Iteration 29, loss = 0.03331873
Iteration 30, loss = 0.03237951
Iteration 31, loss = 0.03166410
Iteration 32, loss = 0.03101630
Iteration 33, loss = 0.03047631
Iteration 34, loss = 0.02984749
Iteration 35, loss = 0.02927553
Iteration 36, loss = 0.02933203
Iteration 37, loss = 0.02868816
Iteration 38, loss = 0.02808651
Iteration 39, loss = 0.02790238
Iteration 40, loss = 0.02743406
Iteration 41, loss = 0.02733641
Iteration 42, loss = 0.02690708
Iteration 43, loss = 0.02655736
Iteration 44, loss = 0.02655237
Iteration 45, loss = 0.02621554
Iteration 46, loss = 0.02627805
Iteration 47, loss = 0.02576949
Iteration 48, loss = 0.02560872
Iteration 49, loss = 0.02567181
Iteration 50, loss = 0.02550440
Iteration 51, loss = 0.02537923
Iteration 52, loss = 0.02487947
Iteration 53, loss = 0.02499411
Iteration 54, loss = 0.02495941
Iteration 55, loss = 0.02453940
Iteration 56, loss = 0.02457824
Iteration 57, loss = 0.02439317
Iteration 58, loss = 0.02444045
Iteration 59, loss = 0.02432698
Iteration 60, loss = 0.02414806
Iteration 61, loss = 0.02421906
Iteration 62, loss = 0.02402669
Iteration 63, loss = 0.02396429
Iteration 64, loss = 0.02374881
Iteration 65, loss = 0.02375694
Iteration 66, loss = 0.02366571
Iteration 67, loss = 0.02361033
Iteration 68, loss = 0.02357974
Iteration 69, loss = 0.02360650
Iteration 70, loss = 0.02362225
Iteration 71, loss = 0.02330392
Iteration 72, loss = 0.02357498
Iteration 73, loss = 0.02327518
Iteration 74, loss = 0.02310971
Iteration 75, loss = 0.02321154
Iteration 76, loss = 0.02313652
Iteration 77, loss = 0.02313793
Iteration 78, loss = 0.02303320
Iteration 79, loss = 0.02345380
Iteration 80, loss = 0.02319538
Iteration 81, loss = 0.02294454
Iteration 82, loss = 0.02315242
Iteration 83, loss = 0.02299465
Iteration 84, loss = 0.02318098
Iteration 85, loss = 0.02309732
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68210057
Iteration 2, loss = 0.64499679
Iteration 3, loss = 0.59068042
Iteration 4, loss = 0.51884794
Iteration 5, loss = 0.44337496
Iteration 6, loss = 0.37305741
Iteration 7, loss = 0.31167859
Iteration 8, loss = 0.26000543
Iteration 9, loss = 0.21743322
Iteration 10, loss = 0.18340372
Iteration 11, loss = 0.15604780
Iteration 12, loss = 0.13441994
Iteration 13, loss = 0.11708581
Iteration 14, loss = 0.10324428
Iteration 15, loss = 0.09198628
Iteration 16, loss = 0.08260489
Iteration 17, loss = 0.07483705
Iteration 18, loss = 0.06852977
Iteration 19, loss = 0.06302787
Iteration 20, loss = 0.05842750
Iteration 21, loss = 0.05454549
Iteration 22, loss = 0.05110478
Iteration 23, loss = 0.04819753
Iteration 24, loss = 0.04567218
Iteration 25, loss = 0.04357179
Iteration 26, loss = 0.04132185
Iteration 27, loss = 0.03973712
Iteration 28, loss = 0.03806175
Iteration 29, loss = 0.03651453
Iteration 30, loss = 0.03531641
Iteration 31, loss = 0.03420388
Iteration 32, loss = 0.03313151
Iteration 33, loss = 0.03224026
Iteration 34, loss = 0.03115864
Iteration 35, loss = 0.03047037
Iteration 36, loss = 0.02968285
Iteration 37, loss = 0.02907964
Iteration 38, loss = 0.02857397
Iteration 39, loss = 0.02787216
Iteration 40, loss = 0.02739394
Iteration 41, loss = 0.02672771
Iteration 42, loss = 0.02659060
Iteration 43, loss = 0.02603150
Iteration 44, loss = 0.02566357
Iteration 45, loss = 0.02516065
Iteration 46, loss = 0.02488322
Iteration 47, loss = 0.02461103
Iteration 48, loss = 0.02416449
Iteration 49, loss = 0.02396122
Iteration 50, loss = 0.02373816
Iteration 51, loss = 0.02340298
Iteration 52, loss = 0.02320961
Iteration 53, loss = 0.02287357
Iteration 54, loss = 0.02279387
Iteration 55, loss = 0.02276103
Iteration 56, loss = 0.02246608
Iteration 57, loss = 0.02217281
Iteration 58, loss = 0.02215626
Iteration 59, loss = 0.02186745
Iteration 60, loss = 0.02171625
Iteration 61, loss = 0.02165528
Iteration 62, loss = 0.02158239
Iteration 63, loss = 0.02140276
Iteration 64, loss = 0.02120687
Iteration 65, loss = 0.02100660
Iteration 66, loss = 0.02095733
Iteration 67, loss = 0.02089317
Iteration 68, loss = 0.02078098
Iteration 69, loss = 0.02070303
Iteration 70, loss = 0.02057679
Iteration 71, loss = 0.02040514
Iteration 72, loss = 0.02042218
Iteration 73, loss = 0.02037497
Iteration 74, loss = 0.02011916
Iteration 75, loss = 0.02002789
Iteration 76, loss = 0.01997455
Iteration 77, loss = 0.01997975
Iteration 78, loss = 0.01996414
Iteration 79, loss = 0.01987752
Iteration 80, loss = 0.01984429
Iteration 81, loss = 0.01975299
Iteration 82, loss = 0.01962466
Iteration 83, loss = 0.01965453
Iteration 84, loss = 0.01948098
Iteration 85, loss = 0.01950590
Iteration 86, loss = 0.01946115
Iteration 87, loss = 0.01960849
Iteration 88, loss = 0.01947262
Iteration 89, loss = 0.01928282
Iteration 90, loss = 0.01933930
Iteration 91, loss = 0.01912364
Iteration 92, loss = 0.01934564
Iteration 93, loss = 0.01922904
Iteration 94, loss = 0.01900869
Iteration 95, loss = 0.01907998
Iteration 96, loss = 0.01907932
Iteration 97, loss = 0.01885942
Iteration 98, loss = 0.01890995
Iteration 99, loss = 0.01897035
Iteration 100, loss = 0.01900066
Iteration 101, loss = 0.01872247
Iteration 102, loss = 0.01899776
Iteration 103, loss = 0.01872479
Iteration 104, loss = 0.01871115
Iteration 105, loss = 0.01878901
Iteration 106, loss = 0.01876867
Iteration 107, loss = 0.01876824
Iteration 108, loss = 0.01862739
Iteration 109, loss = 0.01849643
Iteration 110, loss = 0.01855847
Iteration 111, loss = 0.01850133
Iteration 112, loss = 0.01849865
Iteration 113, loss = 0.01854421
Iteration 114, loss = 0.01856589
Iteration 115, loss = 0.01850242
Iteration 116, loss = 0.01836332
Iteration 117, loss = 0.01858393
Iteration 118, loss = 0.01866660
Iteration 119, loss = 0.01833849
Iteration 120, loss = 0.01838812
Iteration 121, loss = 0.01844260
Iteration 122, loss = 0.01858871
Iteration 123, loss = 0.01850133
Iteration 124, loss = 0.01836181
Iteration 125, loss = 0.01840561
Iteration 126, loss = 0.01822283
Iteration 127, loss = 0.01832912
Iteration 128, loss = 0.01853803
Iteration 129, loss = 0.01813466
Iteration 130, loss = 0.01814546
Iteration 131, loss = 0.01844446
Iteration 132, loss = 0.01817608
Iteration 133, loss = 0.01819177
Iteration 134, loss = 0.01810313
Iteration 135, loss = 0.01828096
Iteration 136, loss = 0.01821516
Iteration 137, loss = 0.01819159
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72006082
Iteration 2, loss = 0.68581948
Iteration 3, loss = 0.64727276
Iteration 4, loss = 0.60040644
Iteration 5, loss = 0.54914236
Iteration 6, loss = 0.49652685
Iteration 7, loss = 0.44540908
Iteration 8, loss = 0.39717241
Iteration 9, loss = 0.35245860
Iteration 10, loss = 0.31226597
Iteration 11, loss = 0.27630950
Iteration 12, loss = 0.24457604
Iteration 13, loss = 0.21648377
Iteration 14, loss = 0.19217851
Iteration 15, loss = 0.17100488
Iteration 16, loss = 0.15265735
Iteration 17, loss = 0.13693795
Iteration 18, loss = 0.12337971
Iteration 19, loss = 0.11186860
Iteration 20, loss = 0.10194636
Iteration 21, loss = 0.09324264
Iteration 22, loss = 0.08561601
Iteration 23, loss = 0.07897948
Iteration 24, loss = 0.07314077
Iteration 25, loss = 0.06799519
Iteration 26, loss = 0.06339766
Iteration 27, loss = 0.05946574
Iteration 28, loss = 0.05576719
Iteration 29, loss = 0.05258597
Iteration 30, loss = 0.04970449
Iteration 31, loss = 0.04722195
Iteration 32, loss = 0.04484895
Iteration 33, loss = 0.04276016
Iteration 34, loss = 0.04077865
Iteration 35, loss = 0.03924025
Iteration 36, loss = 0.03761113
Iteration 37, loss = 0.03630621
Iteration 38, loss = 0.03492260
Iteration 39, loss = 0.03383409
Iteration 40, loss = 0.03277397
Iteration 41, loss = 0.03159925
Iteration 42, loss = 0.03050678
Iteration 43, loss = 0.02956586
Iteration 44, loss = 0.02865454
Iteration 45, loss = 0.02778104
Iteration 46, loss = 0.02708670
Iteration 47, loss = 0.02637799
Iteration 48, loss = 0.02574415
Iteration 49, loss = 0.02513046
Iteration 50, loss = 0.02447906
Iteration 51, loss = 0.02385729
Iteration 52, loss = 0.02334481
Iteration 53, loss = 0.02286339
Iteration 54, loss = 0.02246949
Iteration 55, loss = 0.02205991
Iteration 56, loss = 0.02166524
Iteration 57, loss = 0.02127152
Iteration 58, loss = 0.02077860
Iteration 59, loss = 0.02040008
Iteration 60, loss = 0.01995921
Iteration 61, loss = 0.01971537
Iteration 62, loss = 0.01950890
Iteration 63, loss = 0.01924872
Iteration 64, loss = 0.01897333
Iteration 65, loss = 0.01869758
Iteration 66, loss = 0.01838490
Iteration 67, loss = 0.01814639
Iteration 68, loss = 0.01786350
Iteration 69, loss = 0.01766750
Iteration 70, loss = 0.01750257
Iteration 71, loss = 0.01723989
Iteration 72, loss = 0.01705410
Iteration 73, loss = 0.01688300
Iteration 74, loss = 0.01669299
Iteration 75, loss = 0.01655665
Iteration 76, loss = 0.01634714
Iteration 77, loss = 0.01617489
Iteration 78, loss = 0.01604928
Iteration 79, loss = 0.01584866
Iteration 80, loss = 0.01588228
Iteration 81, loss = 0.01567749
Iteration 82, loss = 0.01554056
Iteration 83, loss = 0.01547415
Iteration 84, loss = 0.01532236
Iteration 85, loss = 0.01521414
Iteration 86, loss = 0.01507990
Iteration 87, loss = 0.01495751
Iteration 88, loss = 0.01480661
Iteration 89, loss = 0.01489612
Iteration 90, loss = 0.01471694
Iteration 91, loss = 0.01471850
Iteration 92, loss = 0.01456617
Iteration 93, loss = 0.01456623
Iteration 94, loss = 0.01466565
Iteration 95, loss = 0.01458039
Iteration 96, loss = 0.01438788
Iteration 97, loss = 0.01424551
Iteration 98, loss = 0.01418190
Iteration 99, loss = 0.01401220
Iteration 100, loss = 0.01389073
Iteration 101, loss = 0.01390988
Iteration 102, loss = 0.01377327
Iteration 103, loss = 0.01374129
Iteration 104, loss = 0.01357339
Iteration 105, loss = 0.01355587
Iteration 106, loss = 0.01350923
Iteration 107, loss = 0.01348095
Iteration 108, loss = 0.01337141
Iteration 109, loss = 0.01333517
Iteration 110, loss = 0.01334453
Iteration 111, loss = 0.01327401
Iteration 112, loss = 0.01316519
Iteration 113, loss = 0.01313841
Iteration 114, loss = 0.01310134
Iteration 115, loss = 0.01296837
Iteration 116, loss = 0.01299872
Iteration 117, loss = 0.01289578
Iteration 118, loss = 0.01289677
Iteration 119, loss = 0.01291578
Iteration 120, loss = 0.01276743
Iteration 121, loss = 0.01281115
Iteration 122, loss = 0.01288088
Iteration 123, loss = 0.01297741
Iteration 124, loss = 0.01287664
Iteration 125, loss = 0.01283826
Iteration 126, loss = 0.01283795
Iteration 127, loss = 0.01290101
Iteration 128, loss = 0.01255286
Iteration 129, loss = 0.01238958
Iteration 130, loss = 0.01262310
Iteration 131, loss = 0.01283547
Iteration 132, loss = 0.01294974
Iteration 133, loss = 0.01296462
Iteration 134, loss = 0.01285436
Iteration 135, loss = 0.01256987
Iteration 136, loss = 0.01246401
Iteration 137, loss = 0.01236181
Iteration 138, loss = 0.01236400
Iteration 139, loss = 0.01231128
Iteration 140, loss = 0.01224481
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68321238
Iteration 2, loss = 0.65604719
Iteration 3, loss = 0.62635802
Iteration 4, loss = 0.58931506
Iteration 5, loss = 0.54572565
Iteration 6, loss = 0.49844972
Iteration 7, loss = 0.45032836
Iteration 8, loss = 0.40316834
Iteration 9, loss = 0.35823113
Iteration 10, loss = 0.31667170
Iteration 11, loss = 0.27907584
Iteration 12, loss = 0.24536382
Iteration 13, loss = 0.21597253
Iteration 14, loss = 0.19067269
Iteration 15, loss = 0.16914795
Iteration 16, loss = 0.15036827
Iteration 17, loss = 0.13455675
Iteration 18, loss = 0.12113788
Iteration 19, loss = 0.10929890
Iteration 20, loss = 0.09950182
Iteration 21, loss = 0.09093275
Iteration 22, loss = 0.08337671
Iteration 23, loss = 0.07696556
Iteration 24, loss = 0.07132314
Iteration 25, loss = 0.06633348
Iteration 26, loss = 0.06187801
Iteration 27, loss = 0.05800242
Iteration 28, loss = 0.05464334
Iteration 29, loss = 0.05146589
Iteration 30, loss = 0.04866481
Iteration 31, loss = 0.04612210
Iteration 32, loss = 0.04384970
Iteration 33, loss = 0.04172889
Iteration 34, loss = 0.03986408
Iteration 35, loss = 0.03812244
Iteration 36, loss = 0.03672837
Iteration 37, loss = 0.03513106
Iteration 38, loss = 0.03405647
Iteration 39, loss = 0.03285212
Iteration 40, loss = 0.03171351
Iteration 41, loss = 0.03063910
Iteration 42, loss = 0.02972931
Iteration 43, loss = 0.02874345
Iteration 44, loss = 0.02780488
Iteration 45, loss = 0.02700578
Iteration 46, loss = 0.02608483
Iteration 47, loss = 0.02560340
Iteration 48, loss = 0.02513295
Iteration 49, loss = 0.02466730
Iteration 50, loss = 0.02398040
Iteration 51, loss = 0.02340574
Iteration 52, loss = 0.02285004
Iteration 53, loss = 0.02223856
Iteration 54, loss = 0.02176754
Iteration 55, loss = 0.02137751
Iteration 56, loss = 0.02079457
Iteration 57, loss = 0.02062555
Iteration 58, loss = 0.02034547
Iteration 59, loss = 0.01994539
Iteration 60, loss = 0.01962841
Iteration 61, loss = 0.01921563
Iteration 62, loss = 0.01903228
Iteration 63, loss = 0.01854998
Iteration 64, loss = 0.01834330
Iteration 65, loss = 0.01813605
Iteration 66, loss = 0.01784985
Iteration 67, loss = 0.01770278
Iteration 68, loss = 0.01727959
Iteration 69, loss = 0.01735779
Iteration 70, loss = 0.01714411
Iteration 71, loss = 0.01687668
Iteration 72, loss = 0.01664371
Iteration 73, loss = 0.01640559
Iteration 74, loss = 0.01636718
Iteration 75, loss = 0.01612403
Iteration 76, loss = 0.01612074
Iteration 77, loss = 0.01593750
Iteration 78, loss = 0.01585739
Iteration 79, loss = 0.01563943
Iteration 80, loss = 0.01546991
Iteration 81, loss = 0.01552419
Iteration 82, loss = 0.01532295
Iteration 83, loss = 0.01512491
Iteration 84, loss = 0.01493124
Iteration 85, loss = 0.01485386
Iteration 86, loss = 0.01481694
Iteration 87, loss = 0.01472044
Iteration 88, loss = 0.01463043
Iteration 89, loss = 0.01439672
Iteration 90, loss = 0.01432854
Iteration 91, loss = 0.01406433
Iteration 92, loss = 0.01410466
Iteration 93, loss = 0.01408731
Iteration 94, loss = 0.01399588
Iteration 95, loss = 0.01390908
Iteration 96, loss = 0.01400666
Iteration 97, loss = 0.01396180
Iteration 98, loss = 0.01382499
Iteration 99, loss = 0.01358466
Iteration 100, loss = 0.01352645
Iteration 101, loss = 0.01339412
Iteration 102, loss = 0.01343050
Iteration 103, loss = 0.01329636
Iteration 104, loss = 0.01320518
Iteration 105, loss = 0.01315598
Iteration 106, loss = 0.01304513
Iteration 107, loss = 0.01303073
Iteration 108, loss = 0.01296906
Iteration 109, loss = 0.01295812
Iteration 110, loss = 0.01271384
Iteration 111, loss = 0.01282668
Iteration 112, loss = 0.01293037
Iteration 113, loss = 0.01285790
Iteration 114, loss = 0.01279569
Iteration 115, loss = 0.01272863
Iteration 116, loss = 0.01262235
Iteration 117, loss = 0.01249983
Iteration 118, loss = 0.01246586
Iteration 119, loss = 0.01245802
Iteration 120, loss = 0.01236594
Iteration 121, loss = 0.01232127
Iteration 122, loss = 0.01235447
Iteration 123, loss = 0.01246024
Iteration 124, loss = 0.01288960
Iteration 125, loss = 0.01281173
Iteration 126, loss = 0.01268836
Iteration 127, loss = 0.01261088
Iteration 128, loss = 0.01251666
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68045755
Iteration 2, loss = 0.62936231
Iteration 3, loss = 0.55072639
Iteration 4, loss = 0.46025560
Iteration 5, loss = 0.37377186
Iteration 6, loss = 0.29866535
Iteration 7, loss = 0.23762643
Iteration 8, loss = 0.19068966
Iteration 9, loss = 0.15570620
Iteration 10, loss = 0.12952216
Iteration 11, loss = 0.11001308
Iteration 12, loss = 0.09506574
Iteration 13, loss = 0.08340955
Iteration 14, loss = 0.07451888
Iteration 15, loss = 0.06729727
Iteration 16, loss = 0.06103424
Iteration 17, loss = 0.05666815
Iteration 18, loss = 0.05244491
Iteration 19, loss = 0.04928692
Iteration 20, loss = 0.04620814
Iteration 21, loss = 0.04397900
Iteration 22, loss = 0.04184384
Iteration 23, loss = 0.03973234
Iteration 24, loss = 0.03824337
Iteration 25, loss = 0.03678760
Iteration 26, loss = 0.03561256
Iteration 27, loss = 0.03459829
Iteration 28, loss = 0.03343902
Iteration 29, loss = 0.03266187
Iteration 30, loss = 0.03184665
Iteration 31, loss = 0.03106035
Iteration 32, loss = 0.03046153
Iteration 33, loss = 0.02983703
Iteration 34, loss = 0.02934524
Iteration 35, loss = 0.02873327
Iteration 36, loss = 0.02825167
Iteration 37, loss = 0.02804385
Iteration 38, loss = 0.02759691
Iteration 39, loss = 0.02707593
Iteration 40, loss = 0.02696388
Iteration 41, loss = 0.02661315
Iteration 42, loss = 0.02636388
Iteration 43, loss = 0.02593789
Iteration 44, loss = 0.02592241
Iteration 45, loss = 0.02568340
Iteration 46, loss = 0.02526983
Iteration 47, loss = 0.02515158
Iteration 48, loss = 0.02472789
Iteration 49, loss = 0.02470710
Iteration 50, loss = 0.02457835
Iteration 51, loss = 0.02459768
Iteration 52, loss = 0.02429421
Iteration 53, loss = 0.02431195
Iteration 54, loss = 0.02413419
Iteration 55, loss = 0.02389065
Iteration 56, loss = 0.02391241
Iteration 57, loss = 0.02390606
Iteration 58, loss = 0.02351149
Iteration 59, loss = 0.02339624
Iteration 60, loss = 0.02353243
Iteration 61, loss = 0.02321487
Iteration 62, loss = 0.02336391
Iteration 63, loss = 0.02354276
Iteration 64, loss = 0.02319547
Iteration 65, loss = 0.02307627
Iteration 66, loss = 0.02287467
Iteration 67, loss = 0.02282590
Iteration 68, loss = 0.02307901
Iteration 69, loss = 0.02279333
Iteration 70, loss = 0.02283092
Iteration 71, loss = 0.02273628
Iteration 72, loss = 0.02265854
Iteration 73, loss = 0.02249840
Iteration 74, loss = 0.02244296
Iteration 75, loss = 0.02262398
Iteration 76, loss = 0.02234731
Iteration 77, loss = 0.02231678
Iteration 78, loss = 0.02218952
Iteration 79, loss = 0.02250241
Iteration 80, loss = 0.02245610
Iteration 81, loss = 0.02214577
Iteration 82, loss = 0.02226068
Iteration 83, loss = 0.02218476
Iteration 84, loss = 0.02224402
Iteration 85, loss = 0.02218865
Iteration 86, loss = 0.02203091
Iteration 87, loss = 0.02202703
Iteration 88, loss = 0.02200385
Iteration 89, loss = 0.02214563
Iteration 90, loss = 0.02199622
Iteration 91, loss = 0.02192994
Iteration 92, loss = 0.02208337
Iteration 93, loss = 0.02177152
Iteration 94, loss = 0.02226847
Iteration 95, loss = 0.02192673
Iteration 96, loss = 0.02187044
Iteration 97, loss = 0.02215256
Iteration 98, loss = 0.02171826
Iteration 99, loss = 0.02185720
Iteration 100, loss = 0.02207728
Iteration 101, loss = 0.02204517
Iteration 102, loss = 0.02172563
Iteration 103, loss = 0.02162333
Iteration 104, loss = 0.02183348
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68116097
Iteration 2, loss = 0.65243218
Iteration 3, loss = 0.62056151
Iteration 4, loss = 0.58179445
Iteration 5, loss = 0.53687735
Iteration 6, loss = 0.48799671
Iteration 7, loss = 0.43856452
Iteration 8, loss = 0.39112894
Iteration 9, loss = 0.34713520
Iteration 10, loss = 0.30702181
Iteration 11, loss = 0.27095663
Iteration 12, loss = 0.23939914
Iteration 13, loss = 0.21157452
Iteration 14, loss = 0.18769774
Iteration 15, loss = 0.16724118
Iteration 16, loss = 0.14974801
Iteration 17, loss = 0.13449539
Iteration 18, loss = 0.12135121
Iteration 19, loss = 0.11029677
Iteration 20, loss = 0.10043413
Iteration 21, loss = 0.09210659
Iteration 22, loss = 0.08489992
Iteration 23, loss = 0.07850378
Iteration 24, loss = 0.07296103
Iteration 25, loss = 0.06826953
Iteration 26, loss = 0.06400864
Iteration 27, loss = 0.06037546
Iteration 28, loss = 0.05672507
Iteration 29, loss = 0.05361365
Iteration 30, loss = 0.05089842
Iteration 31, loss = 0.04829801
Iteration 32, loss = 0.04637286
Iteration 33, loss = 0.04405384
Iteration 34, loss = 0.04231564
Iteration 35, loss = 0.04069199
Iteration 36, loss = 0.03912250
Iteration 37, loss = 0.03739731
Iteration 38, loss = 0.03599244
Iteration 39, loss = 0.03497668
Iteration 40, loss = 0.03386481
Iteration 41, loss = 0.03279228
Iteration 42, loss = 0.03181213
Iteration 43, loss = 0.03092003
Iteration 44, loss = 0.03001363
Iteration 45, loss = 0.02918453
Iteration 46, loss = 0.02863362
Iteration 47, loss = 0.02789134
Iteration 48, loss = 0.02717775
Iteration 49, loss = 0.02644371
Iteration 50, loss = 0.02600149
Iteration 51, loss = 0.02552391
Iteration 52, loss = 0.02514204
Iteration 53, loss = 0.02451433
Iteration 54, loss = 0.02407119
Iteration 55, loss = 0.02356694
Iteration 56, loss = 0.02309249
Iteration 57, loss = 0.02270761
Iteration 58, loss = 0.02227034
Iteration 59, loss = 0.02192994
Iteration 60, loss = 0.02143739
Iteration 61, loss = 0.02124259
Iteration 62, loss = 0.02114367
Iteration 63, loss = 0.02094233
Iteration 64, loss = 0.02047063
Iteration 65, loss = 0.02055813
Iteration 66, loss = 0.02036290
Iteration 67, loss = 0.02014316
Iteration 68, loss = 0.01975517
Iteration 69, loss = 0.01951539
Iteration 70, loss = 0.01917664
Iteration 71, loss = 0.01889045
Iteration 72, loss = 0.01876036
Iteration 73, loss = 0.01858440
Iteration 74, loss = 0.01852196
Iteration 75, loss = 0.01838261
Iteration 76, loss = 0.01821075
Iteration 77, loss = 0.01787890
Iteration 78, loss = 0.01789747
Iteration 79, loss = 0.01776829
Iteration 80, loss = 0.01773737
Iteration 81, loss = 0.01754372
Iteration 82, loss = 0.01733343
Iteration 83, loss = 0.01714473
Iteration 84, loss = 0.01718623
Iteration 85, loss = 0.01741718
Iteration 86, loss = 0.01724925
Iteration 87, loss = 0.01716196
Iteration 88, loss = 0.01688222
Iteration 89, loss = 0.01699452
Iteration 90, loss = 0.01684396
Iteration 91, loss = 0.01661952
Iteration 92, loss = 0.01641019
Iteration 93, loss = 0.01627663
Iteration 94, loss = 0.01604376
Iteration 95, loss = 0.01602479
Iteration 96, loss = 0.01606355
Iteration 97, loss = 0.01620952
Iteration 98, loss = 0.01608471
Iteration 99, loss = 0.01619077
Iteration 100, loss = 0.01596866
Iteration 101, loss = 0.01590594
Iteration 102, loss = 0.01572228
Iteration 103, loss = 0.01563977
Iteration 104, loss = 0.01550352
Iteration 105, loss = 0.01545894
Iteration 106, loss = 0.01534329
Iteration 107, loss = 0.01536762
Iteration 108, loss = 0.01526097
Iteration 109, loss = 0.01515319
Iteration 110, loss = 0.01490749
Iteration 111, loss = 0.01542093
Iteration 112, loss = 0.01549218
Iteration 113, loss = 0.01515285
Iteration 114, loss = 0.01495968
Iteration 115, loss = 0.01483869
Iteration 116, loss = 0.01474062
Iteration 117, loss = 0.01468849
Iteration 118, loss = 0.01459427
Iteration 119, loss = 0.01459585
Iteration 120, loss = 0.01443048
Iteration 121, loss = 0.01475389
Iteration 122, loss = 0.01474829
Iteration 123, loss = 0.01475326
Iteration 124, loss = 0.01471271
Iteration 125, loss = 0.01458379
Iteration 126, loss = 0.01482189
Iteration 127, loss = 0.01483773
Iteration 128, loss = 0.01475249
Iteration 129, loss = 0.01485555
Iteration 130, loss = 0.01455396
Iteration 131, loss = 0.01487138
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72875876
Iteration 2, loss = 0.69692863
Iteration 3, loss = 0.65781888
Iteration 4, loss = 0.60983537
Iteration 5, loss = 0.55340574
Iteration 6, loss = 0.49256859
Iteration 7, loss = 0.42956103
Iteration 8, loss = 0.37007037
Iteration 9, loss = 0.31591514
Iteration 10, loss = 0.26880100
Iteration 11, loss = 0.22879567
Iteration 12, loss = 0.19555796
Iteration 13, loss = 0.16817549
Iteration 14, loss = 0.14568740
Iteration 15, loss = 0.12723831
Iteration 16, loss = 0.11230812
Iteration 17, loss = 0.09995564
Iteration 18, loss = 0.08947981
Iteration 19, loss = 0.08085863
Iteration 20, loss = 0.07365905
Iteration 21, loss = 0.06751684
Iteration 22, loss = 0.06214515
Iteration 23, loss = 0.05762585
Iteration 24, loss = 0.05371226
Iteration 25, loss = 0.05022088
Iteration 26, loss = 0.04724020
Iteration 27, loss = 0.04452449
Iteration 28, loss = 0.04217175
Iteration 29, loss = 0.04007878
Iteration 30, loss = 0.03829985
Iteration 31, loss = 0.03650914
Iteration 32, loss = 0.03496197
Iteration 33, loss = 0.03356642
Iteration 34, loss = 0.03224180
Iteration 35, loss = 0.03116996
Iteration 36, loss = 0.03014645
Iteration 37, loss = 0.02931919
Iteration 38, loss = 0.02852220
Iteration 39, loss = 0.02747319
Iteration 40, loss = 0.02691822
Iteration 41, loss = 0.02616540
Iteration 42, loss = 0.02560737
Iteration 43, loss = 0.02501235
Iteration 44, loss = 0.02445137
Iteration 45, loss = 0.02383476
Iteration 46, loss = 0.02339902
Iteration 47, loss = 0.02300266
Iteration 48, loss = 0.02256218
Iteration 49, loss = 0.02216388
Iteration 50, loss = 0.02180272
Iteration 51, loss = 0.02144004
Iteration 52, loss = 0.02112001
Iteration 53, loss = 0.02080600
Iteration 54, loss = 0.02048519
Iteration 55, loss = 0.02024761
Iteration 56, loss = 0.01999313
Iteration 57, loss = 0.01992754
Iteration 58, loss = 0.01967293
Iteration 59, loss = 0.01933990
Iteration 60, loss = 0.01916446
Iteration 61, loss = 0.01897296
Iteration 62, loss = 0.01877182
Iteration 63, loss = 0.01864997
Iteration 64, loss = 0.01849914
Iteration 65, loss = 0.01831384
Iteration 66, loss = 0.01811371
Iteration 67, loss = 0.01799321
Iteration 68, loss = 0.01780055
Iteration 69, loss = 0.01775723
Iteration 70, loss = 0.01759052
Iteration 71, loss = 0.01754958
Iteration 72, loss = 0.01735748
Iteration 73, loss = 0.01740217
Iteration 74, loss = 0.01731835
Iteration 75, loss = 0.01706117
Iteration 76, loss = 0.01688969
Iteration 77, loss = 0.01680247
Iteration 78, loss = 0.01685675
Iteration 79, loss = 0.01664329
Iteration 80, loss = 0.01666788
Iteration 81, loss = 0.01662252
Iteration 82, loss = 0.01648310
Iteration 83, loss = 0.01643201
Iteration 84, loss = 0.01630321
Iteration 85, loss = 0.01611173
Iteration 86, loss = 0.01631340
Iteration 87, loss = 0.01615695
Iteration 88, loss = 0.01611886
Iteration 89, loss = 0.01602828
Iteration 90, loss = 0.01597510
Iteration 91, loss = 0.01593897
Iteration 92, loss = 0.01594453
Iteration 93, loss = 0.01590383
Iteration 94, loss = 0.01566805
Iteration 95, loss = 0.01577331
Iteration 96, loss = 0.01581422
Iteration 97, loss = 0.01573948
Iteration 98, loss = 0.01571736
Iteration 99, loss = 0.01565877
Iteration 100, loss = 0.01542200
Iteration 101, loss = 0.01565340
Iteration 102, loss = 0.01539115
Iteration 103, loss = 0.01533399
Iteration 104, loss = 0.01536068
Iteration 105, loss = 0.01520582
Iteration 106, loss = 0.01517883
Iteration 107, loss = 0.01523064
Iteration 108, loss = 0.01518054
Iteration 109, loss = 0.01511804
Iteration 110, loss = 0.01522035
Iteration 111, loss = 0.01518796
Iteration 112, loss = 0.01514304
Iteration 113, loss = 0.01510463
Iteration 114, loss = 0.01525190
Iteration 115, loss = 0.01522384
Iteration 116, loss = 0.01505262
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72772619
Iteration 2, loss = 0.69591465
Iteration 3, loss = 0.65070994
Iteration 4, loss = 0.59365124
Iteration 5, loss = 0.52854302
Iteration 6, loss = 0.46063722
Iteration 7, loss = 0.39546706
Iteration 8, loss = 0.33655464
Iteration 9, loss = 0.28540664
Iteration 10, loss = 0.24214043
Iteration 11, loss = 0.20626529
Iteration 12, loss = 0.17675057
Iteration 13, loss = 0.15275341
Iteration 14, loss = 0.13318322
Iteration 15, loss = 0.11714428
Iteration 16, loss = 0.10412133
Iteration 17, loss = 0.09313098
Iteration 18, loss = 0.08398406
Iteration 19, loss = 0.07632007
Iteration 20, loss = 0.06970903
Iteration 21, loss = 0.06424749
Iteration 22, loss = 0.05945723
Iteration 23, loss = 0.05545002
Iteration 24, loss = 0.05171395
Iteration 25, loss = 0.04859951
Iteration 26, loss = 0.04574211
Iteration 27, loss = 0.04323384
Iteration 28, loss = 0.04097118
Iteration 29, loss = 0.03915409
Iteration 30, loss = 0.03744807
Iteration 31, loss = 0.03576276
Iteration 32, loss = 0.03432827
Iteration 33, loss = 0.03312205
Iteration 34, loss = 0.03196043
Iteration 35, loss = 0.03083179
Iteration 36, loss = 0.02986629
Iteration 37, loss = 0.02903000
Iteration 38, loss = 0.02816320
Iteration 39, loss = 0.02741043
Iteration 40, loss = 0.02662579
Iteration 41, loss = 0.02603969
Iteration 42, loss = 0.02537919
Iteration 43, loss = 0.02490445
Iteration 44, loss = 0.02443159
Iteration 45, loss = 0.02389928
Iteration 46, loss = 0.02335848
Iteration 47, loss = 0.02302179
Iteration 48, loss = 0.02249035
Iteration 49, loss = 0.02211409
Iteration 50, loss = 0.02176192
Iteration 51, loss = 0.02146994
Iteration 52, loss = 0.02107767
Iteration 53, loss = 0.02081854
Iteration 54, loss = 0.02061167
Iteration 55, loss = 0.02044650
Iteration 56, loss = 0.02009029
Iteration 57, loss = 0.01983730
Iteration 58, loss = 0.01972735
Iteration 59, loss = 0.01940914
Iteration 60, loss = 0.01933502
Iteration 61, loss = 0.01909865
Iteration 62, loss = 0.01898908
Iteration 63, loss = 0.01869553
Iteration 64, loss = 0.01861611
Iteration 65, loss = 0.01836916
Iteration 66, loss = 0.01814168
Iteration 67, loss = 0.01803326
Iteration 68, loss = 0.01791261
Iteration 69, loss = 0.01788313
Iteration 70, loss = 0.01771126
Iteration 71, loss = 0.01758934
Iteration 72, loss = 0.01760718
Iteration 73, loss = 0.01736467
Iteration 74, loss = 0.01729968
Iteration 75, loss = 0.01725104
Iteration 76, loss = 0.01706256
Iteration 77, loss = 0.01700120
Iteration 78, loss = 0.01695710
Iteration 79, loss = 0.01673096
Iteration 80, loss = 0.01676827
Iteration 81, loss = 0.01669473
Iteration 82, loss = 0.01660253
Iteration 83, loss = 0.01660063
Iteration 84, loss = 0.01645865
Iteration 85, loss = 0.01641741
Iteration 86, loss = 0.01633258
Iteration 87, loss = 0.01632258
Iteration 88, loss = 0.01629229
Iteration 89, loss = 0.01629782
Iteration 90, loss = 0.01614917
Iteration 91, loss = 0.01603777
Iteration 92, loss = 0.01592571
Iteration 93, loss = 0.01588668
Iteration 94, loss = 0.01597408
Iteration 95, loss = 0.01593891
Iteration 96, loss = 0.01596169
Iteration 97, loss = 0.01575923
Iteration 98, loss = 0.01567244
Iteration 99, loss = 0.01561199
Iteration 100, loss = 0.01559803
Iteration 101, loss = 0.01556756
Iteration 102, loss = 0.01559847
Iteration 103, loss = 0.01543368
Iteration 104, loss = 0.01561569
Iteration 105, loss = 0.01567989
Iteration 106, loss = 0.01566203
Iteration 107, loss = 0.01531235
Iteration 108, loss = 0.01534254
Iteration 109, loss = 0.01540086
Iteration 110, loss = 0.01552227
Iteration 111, loss = 0.01518153
Iteration 112, loss = 0.01525907
Iteration 113, loss = 0.01525909
Iteration 114, loss = 0.01523634
Iteration 115, loss = 0.01511088
Iteration 116, loss = 0.01521913
Iteration 117, loss = 0.01517121
Iteration 118, loss = 0.01523001
Iteration 119, loss = 0.01500792
Iteration 120, loss = 0.01506508
Iteration 121, loss = 0.01497058
Iteration 122, loss = 0.01503193
Iteration 123, loss = 0.01513273
Iteration 124, loss = 0.01505860
Iteration 125, loss = 0.01496587
Iteration 126, loss = 0.01486613
Iteration 127, loss = 0.01495326
Iteration 128, loss = 0.01493598
Iteration 129, loss = 0.01492460
Iteration 130, loss = 0.01488519
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71457863
Iteration 2, loss = 0.66879409
Iteration 3, loss = 0.60818188
Iteration 4, loss = 0.53301004
Iteration 5, loss = 0.44899893
Iteration 6, loss = 0.36773280
Iteration 7, loss = 0.29749698
Iteration 8, loss = 0.24055414
Iteration 9, loss = 0.19616028
Iteration 10, loss = 0.16220244
Iteration 11, loss = 0.13633434
Iteration 12, loss = 0.11673639
Iteration 13, loss = 0.10136345
Iteration 14, loss = 0.08951464
Iteration 15, loss = 0.07987842
Iteration 16, loss = 0.07250322
Iteration 17, loss = 0.06609708
Iteration 18, loss = 0.06104049
Iteration 19, loss = 0.05671069
Iteration 20, loss = 0.05276763
Iteration 21, loss = 0.04978633
Iteration 22, loss = 0.04710981
Iteration 23, loss = 0.04474000
Iteration 24, loss = 0.04271741
Iteration 25, loss = 0.04107010
Iteration 26, loss = 0.03971028
Iteration 27, loss = 0.03812326
Iteration 28, loss = 0.03691395
Iteration 29, loss = 0.03564700
Iteration 30, loss = 0.03455915
Iteration 31, loss = 0.03391159
Iteration 32, loss = 0.03342524
Iteration 33, loss = 0.03217325
Iteration 34, loss = 0.03198596
Iteration 35, loss = 0.03113166
Iteration 36, loss = 0.03035207
Iteration 37, loss = 0.02994115
Iteration 38, loss = 0.02944308
Iteration 39, loss = 0.02898776
Iteration 40, loss = 0.02853474
Iteration 41, loss = 0.02832285
Iteration 42, loss = 0.02793346
Iteration 43, loss = 0.02735180
Iteration 44, loss = 0.02718151
Iteration 45, loss = 0.02683059
Iteration 46, loss = 0.02675961
Iteration 47, loss = 0.02636684
Iteration 48, loss = 0.02628920
Iteration 49, loss = 0.02611527
Iteration 50, loss = 0.02597930
Iteration 51, loss = 0.02576890
Iteration 52, loss = 0.02519096
Iteration 53, loss = 0.02499137
Iteration 54, loss = 0.02493449
Iteration 55, loss = 0.02476021
Iteration 56, loss = 0.02475219
Iteration 57, loss = 0.02452276
Iteration 58, loss = 0.02425181
Iteration 59, loss = 0.02435983
Iteration 60, loss = 0.02450120
Iteration 61, loss = 0.02425966
Iteration 62, loss = 0.02384896
Iteration 63, loss = 0.02398823
Iteration 64, loss = 0.02377274
Iteration 65, loss = 0.02352810
Iteration 66, loss = 0.02338674
Iteration 67, loss = 0.02358436
Iteration 68, loss = 0.02332577
Iteration 69, loss = 0.02360474
Iteration 70, loss = 0.02328098
Iteration 71, loss = 0.02317862
Iteration 72, loss = 0.02306709
Iteration 73, loss = 0.02311803
Iteration 74, loss = 0.02319942
Iteration 75, loss = 0.02289609
Iteration 76, loss = 0.02293950
Iteration 77, loss = 0.02284831
Iteration 78, loss = 0.02274412
Iteration 79, loss = 0.02257361
Iteration 80, loss = 0.02285441
Iteration 81, loss = 0.02252937
Iteration 82, loss = 0.02272651
Iteration 83, loss = 0.02260935
Iteration 84, loss = 0.02256444
Iteration 85, loss = 0.02249349
Iteration 86, loss = 0.02246414
Iteration 87, loss = 0.02233945
Iteration 88, loss = 0.02255294
Iteration 89, loss = 0.02251610
Iteration 90, loss = 0.02252914
Iteration 91, loss = 0.02266683
Iteration 92, loss = 0.02235578
Iteration 93, loss = 0.02221724
Iteration 94, loss = 0.02224242
Iteration 95, loss = 0.02206656
Iteration 96, loss = 0.02221474
Iteration 97, loss = 0.02190344
Iteration 98, loss = 0.02231085
Iteration 99, loss = 0.02221490
Iteration 100, loss = 0.02193738
Iteration 101, loss = 0.02200564
Iteration 102, loss = 0.02190695
Iteration 103, loss = 0.02204684
Iteration 104, loss = 0.02183088
Iteration 105, loss = 0.02182052
Iteration 106, loss = 0.02200421
Iteration 107, loss = 0.02195619
Iteration 108, loss = 0.02196612
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72286064
Iteration 2, loss = 0.68049814
Iteration 3, loss = 0.63010795
Iteration 4, loss = 0.57078237
Iteration 5, loss = 0.50223507
Iteration 6, loss = 0.42962051
Iteration 7, loss = 0.36131660
Iteration 8, loss = 0.30087010
Iteration 9, loss = 0.25010429
Iteration 10, loss = 0.20869723
Iteration 11, loss = 0.17539699
Iteration 12, loss = 0.14935126
Iteration 13, loss = 0.12828939
Iteration 14, loss = 0.11180475
Iteration 15, loss = 0.09844137
Iteration 16, loss = 0.08764758
Iteration 17, loss = 0.07885309
Iteration 18, loss = 0.07170010
Iteration 19, loss = 0.06556606
Iteration 20, loss = 0.06040405
Iteration 21, loss = 0.05618154
Iteration 22, loss = 0.05249869
Iteration 23, loss = 0.04903352
Iteration 24, loss = 0.04646688
Iteration 25, loss = 0.04387149
Iteration 26, loss = 0.04183713
Iteration 27, loss = 0.03982296
Iteration 28, loss = 0.03825097
Iteration 29, loss = 0.03674327
Iteration 30, loss = 0.03534674
Iteration 31, loss = 0.03400443
Iteration 32, loss = 0.03302816
Iteration 33, loss = 0.03207428
Iteration 34, loss = 0.03103240
Iteration 35, loss = 0.03027259
Iteration 36, loss = 0.02960428
Iteration 37, loss = 0.02889553
Iteration 38, loss = 0.02818765
Iteration 39, loss = 0.02759684
Iteration 40, loss = 0.02717027
Iteration 41, loss = 0.02672392
Iteration 42, loss = 0.02624784
Iteration 43, loss = 0.02584992
Iteration 44, loss = 0.02532917
Iteration 45, loss = 0.02497495
Iteration 46, loss = 0.02466426
Iteration 47, loss = 0.02437775
Iteration 48, loss = 0.02396295
Iteration 49, loss = 0.02384650
Iteration 50, loss = 0.02340012
Iteration 51, loss = 0.02313083
Iteration 52, loss = 0.02296480
Iteration 53, loss = 0.02284626
Iteration 54, loss = 0.02242843
Iteration 55, loss = 0.02237390
Iteration 56, loss = 0.02213987
Iteration 57, loss = 0.02202385
Iteration 58, loss = 0.02194323
Iteration 59, loss = 0.02160864
Iteration 60, loss = 0.02156122
Iteration 61, loss = 0.02135568
Iteration 62, loss = 0.02112896
Iteration 63, loss = 0.02106793
Iteration 64, loss = 0.02102366
Iteration 65, loss = 0.02070187
Iteration 66, loss = 0.02072149
Iteration 67, loss = 0.02048459
Iteration 68, loss = 0.02067572
Iteration 69, loss = 0.02039790
Iteration 70, loss = 0.02028716
Iteration 71, loss = 0.02029855
Iteration 72, loss = 0.02023851
Iteration 73, loss = 0.02011452
Iteration 74, loss = 0.02008106
Iteration 75, loss = 0.01987340
Iteration 76, loss = 0.01999605
Iteration 77, loss = 0.01970968
Iteration 78, loss = 0.01965613
Iteration 79, loss = 0.01965820
Iteration 80, loss = 0.01957715
Iteration 81, loss = 0.01954927
Iteration 82, loss = 0.01942069
Iteration 83, loss = 0.01946574
Iteration 84, loss = 0.01946267
Iteration 85, loss = 0.01929150
Iteration 86, loss = 0.01921335
Iteration 87, loss = 0.01908726
Iteration 88, loss = 0.01909337
Iteration 89, loss = 0.01924879
Iteration 90, loss = 0.01919723
Iteration 91, loss = 0.01900311
Iteration 92, loss = 0.01885752
Iteration 93, loss = 0.01901499
Iteration 94, loss = 0.01899676
Iteration 95, loss = 0.01896991
Iteration 96, loss = 0.01884556
Iteration 97, loss = 0.01877528
Iteration 98, loss = 0.01879238
Iteration 99, loss = 0.01883027
Iteration 100, loss = 0.01874612
Iteration 101, loss = 0.01875238
Iteration 102, loss = 0.01859487
Iteration 103, loss = 0.01868904
Iteration 104, loss = 0.01859758
Iteration 105, loss = 0.01856468
Iteration 106, loss = 0.01851942
Iteration 107, loss = 0.01849187
Iteration 108, loss = 0.01849119
Iteration 109, loss = 0.01856386
Iteration 110, loss = 0.01839614
Iteration 111, loss = 0.01830668
Iteration 112, loss = 0.01849937
Iteration 113, loss = 0.01841320
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68541978
Iteration 2, loss = 0.66545746
Iteration 3, loss = 0.64619621
Iteration 4, loss = 0.62466792
Iteration 5, loss = 0.59920365
Iteration 6, loss = 0.56979412
Iteration 7, loss = 0.53659067
Iteration 8, loss = 0.50084444
Iteration 9, loss = 0.46368843
Iteration 10, loss = 0.42709348
Iteration 11, loss = 0.39149740
Iteration 12, loss = 0.35766885
Iteration 13, loss = 0.32591077
Iteration 14, loss = 0.29643561
Iteration 15, loss = 0.26938705
Iteration 16, loss = 0.24469434
Iteration 17, loss = 0.22241575
Iteration 18, loss = 0.20237605
Iteration 19, loss = 0.18448861
Iteration 20, loss = 0.16840284
Iteration 21, loss = 0.15407142
Iteration 22, loss = 0.14128440
Iteration 23, loss = 0.12989834
Iteration 24, loss = 0.11976914
Iteration 25, loss = 0.11076749
Iteration 26, loss = 0.10268812
Iteration 27, loss = 0.09556918
Iteration 28, loss = 0.08905668
Iteration 29, loss = 0.08322455
Iteration 30, loss = 0.07805830
Iteration 31, loss = 0.07334335
Iteration 32, loss = 0.06909942
Iteration 33, loss = 0.06525135
Iteration 34, loss = 0.06167517
Iteration 35, loss = 0.05842615
Iteration 36, loss = 0.05552920
Iteration 37, loss = 0.05290312
Iteration 38, loss = 0.05041490
Iteration 39, loss = 0.04811562
Iteration 40, loss = 0.04602862
Iteration 41, loss = 0.04411372
Iteration 42, loss = 0.04232571
Iteration 43, loss = 0.04063813
Iteration 44, loss = 0.03913871
Iteration 45, loss = 0.03766223
Iteration 46, loss = 0.03641088
Iteration 47, loss = 0.03509039
Iteration 48, loss = 0.03397392
Iteration 49, loss = 0.03285397
Iteration 50, loss = 0.03184448
Iteration 51, loss = 0.03090025
Iteration 52, loss = 0.02997494
Iteration 53, loss = 0.02908773
Iteration 54, loss = 0.02834358
Iteration 55, loss = 0.02758506
Iteration 56, loss = 0.02689910
Iteration 57, loss = 0.02619327
Iteration 58, loss = 0.02551802
Iteration 59, loss = 0.02495569
Iteration 60, loss = 0.02440107
Iteration 61, loss = 0.02384433
Iteration 62, loss = 0.02330071
Iteration 63, loss = 0.02282030
Iteration 64, loss = 0.02237876
Iteration 65, loss = 0.02191964
Iteration 66, loss = 0.02149098
Iteration 67, loss = 0.02109090
Iteration 68, loss = 0.02066010
Iteration 69, loss = 0.02032613
Iteration 70, loss = 0.01995713
Iteration 71, loss = 0.01965885
Iteration 72, loss = 0.01930550
Iteration 73, loss = 0.01895871
Iteration 74, loss = 0.01867556
Iteration 75, loss = 0.01839102
Iteration 76, loss = 0.01813129
Iteration 77, loss = 0.01791991
Iteration 78, loss = 0.01765626
Iteration 79, loss = 0.01744885
Iteration 80, loss = 0.01713861
Iteration 81, loss = 0.01692658
Iteration 82, loss = 0.01667957
Iteration 83, loss = 0.01649198
Iteration 84, loss = 0.01630544
Iteration 85, loss = 0.01619620
Iteration 86, loss = 0.01595627
Iteration 87, loss = 0.01573041
Iteration 88, loss = 0.01559896
Iteration 89, loss = 0.01539735
Iteration 90, loss = 0.01524831
Iteration 91, loss = 0.01512134
Iteration 92, loss = 0.01490268
Iteration 93, loss = 0.01480172
Iteration 94, loss = 0.01463150
Iteration 95, loss = 0.01445904
Iteration 96, loss = 0.01434432
Iteration 97, loss = 0.01421081
Iteration 98, loss = 0.01406611
Iteration 99, loss = 0.01401040
Iteration 100, loss = 0.01386627
Iteration 101, loss = 0.01385636
Iteration 102, loss = 0.01367414
Iteration 103, loss = 0.01355793
Iteration 104, loss = 0.01346238
Iteration 105, loss = 0.01336091
Iteration 106, loss = 0.01322944
Iteration 107, loss = 0.01314528
Iteration 108, loss = 0.01303685
Iteration 109, loss = 0.01298807
Iteration 110, loss = 0.01288775
Iteration 111, loss = 0.01277289
Iteration 112, loss = 0.01270561
Iteration 113, loss = 0.01259446
Iteration 114, loss = 0.01257992
Iteration 115, loss = 0.01246161
Iteration 116, loss = 0.01238668
Iteration 117, loss = 0.01231453
Iteration 118, loss = 0.01226335
Iteration 119, loss = 0.01217060
Iteration 120, loss = 0.01218348
Iteration 121, loss = 0.01202917
Iteration 122, loss = 0.01202334
Iteration 123, loss = 0.01193224
Iteration 124, loss = 0.01184103
Iteration 125, loss = 0.01177295
Iteration 126, loss = 0.01176858
Iteration 127, loss = 0.01171482
Iteration 128, loss = 0.01168283
Iteration 129, loss = 0.01163275
Iteration 130, loss = 0.01153210
Iteration 131, loss = 0.01146099
Iteration 132, loss = 0.01144992
Iteration 133, loss = 0.01144078
Iteration 134, loss = 0.01130734
Iteration 135, loss = 0.01130136
Iteration 136, loss = 0.01129836
Iteration 137, loss = 0.01115593
Iteration 138, loss = 0.01116213
Iteration 139, loss = 0.01115889
Iteration 140, loss = 0.01109987
Iteration 141, loss = 0.01103219
Iteration 142, loss = 0.01098342
Iteration 143, loss = 0.01098104
Iteration 144, loss = 0.01097739
Iteration 145, loss = 0.01092322
Iteration 146, loss = 0.01087101
Iteration 147, loss = 0.01088846
Iteration 148, loss = 0.01074730
Iteration 149, loss = 0.01072443
Iteration 150, loss = 0.01071076
Iteration 151, loss = 0.01066913
Iteration 152, loss = 0.01068937
Iteration 153, loss = 0.01061403
Iteration 154, loss = 0.01058152
Iteration 155, loss = 0.01060151
Iteration 156, loss = 0.01055912
Iteration 157, loss = 0.01049332
Iteration 158, loss = 0.01048227
Iteration 159, loss = 0.01043997
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67640334
Iteration 2, loss = 0.62555471
Iteration 3, loss = 0.54700964
Iteration 4, loss = 0.45468764
Iteration 5, loss = 0.36788652
Iteration 6, loss = 0.29361218
Iteration 7, loss = 0.23380358
Iteration 8, loss = 0.18813167
Iteration 9, loss = 0.15387398
Iteration 10, loss = 0.12811004
Iteration 11, loss = 0.10872920
Iteration 12, loss = 0.09430583
Iteration 13, loss = 0.08280840
Iteration 14, loss = 0.07383086
Iteration 15, loss = 0.06659693
Iteration 16, loss = 0.06092531
Iteration 17, loss = 0.05630090
Iteration 18, loss = 0.05225415
Iteration 19, loss = 0.04899342
Iteration 20, loss = 0.04612389
Iteration 21, loss = 0.04369911
Iteration 22, loss = 0.04170104
Iteration 23, loss = 0.03984276
Iteration 24, loss = 0.03823319
Iteration 25, loss = 0.03687786
Iteration 26, loss = 0.03578987
Iteration 27, loss = 0.03463650
Iteration 28, loss = 0.03380021
Iteration 29, loss = 0.03257977
Iteration 30, loss = 0.03210791
Iteration 31, loss = 0.03140546
Iteration 32, loss = 0.03057739
Iteration 33, loss = 0.03000830
Iteration 34, loss = 0.02941423
Iteration 35, loss = 0.02902466
Iteration 36, loss = 0.02866314
Iteration 37, loss = 0.02836392
Iteration 38, loss = 0.02769783
Iteration 39, loss = 0.02759592
Iteration 40, loss = 0.02718162
Iteration 41, loss = 0.02696380
Iteration 42, loss = 0.02657212
Iteration 43, loss = 0.02628616
Iteration 44, loss = 0.02613741
Iteration 45, loss = 0.02608177
Iteration 46, loss = 0.02581062
Iteration 47, loss = 0.02562594
Iteration 48, loss = 0.02573682
Iteration 49, loss = 0.02540401
Iteration 50, loss = 0.02519382
Iteration 51, loss = 0.02500846
Iteration 52, loss = 0.02471293
Iteration 53, loss = 0.02459342
Iteration 54, loss = 0.02447394
Iteration 55, loss = 0.02460234
Iteration 56, loss = 0.02418381
Iteration 57, loss = 0.02428309
Iteration 58, loss = 0.02425635
Iteration 59, loss = 0.02394717
Iteration 60, loss = 0.02395096
Iteration 61, loss = 0.02404266
Iteration 62, loss = 0.02365332
Iteration 63, loss = 0.02362161
Iteration 64, loss = 0.02367698
Iteration 65, loss = 0.02358404
Iteration 66, loss = 0.02361679
Iteration 67, loss = 0.02316713
Iteration 68, loss = 0.02345965
Iteration 69, loss = 0.02316423
Iteration 70, loss = 0.02335375
Iteration 71, loss = 0.02329990
Iteration 72, loss = 0.02327501
Iteration 73, loss = 0.02314760
Iteration 74, loss = 0.02306486
Iteration 75, loss = 0.02310135
Iteration 76, loss = 0.02314420
Iteration 77, loss = 0.02288210
Iteration 78, loss = 0.02285402
Iteration 79, loss = 0.02294685
Iteration 80, loss = 0.02283806
Iteration 81, loss = 0.02268782
Iteration 82, loss = 0.02286274
Iteration 83, loss = 0.02285944
Iteration 84, loss = 0.02273375
Iteration 85, loss = 0.02284159
Iteration 86, loss = 0.02265327
Iteration 87, loss = 0.02280777
Iteration 88, loss = 0.02265672
Iteration 89, loss = 0.02270738
Iteration 90, loss = 0.02236666
Iteration 91, loss = 0.02267428
Iteration 92, loss = 0.02251234
Iteration 93, loss = 0.02235164
Iteration 94, loss = 0.02230129
Iteration 95, loss = 0.02248608
Iteration 96, loss = 0.02225715
Iteration 97, loss = 0.02238716
Iteration 98, loss = 0.02235325
Iteration 99, loss = 0.02226572
Iteration 100, loss = 0.02239630
Iteration 101, loss = 0.02260535
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68393917
Iteration 2, loss = 0.64508294
Iteration 3, loss = 0.58505668
Iteration 4, loss = 0.50814031
Iteration 5, loss = 0.42848412
Iteration 6, loss = 0.35501272
Iteration 7, loss = 0.29052848
Iteration 8, loss = 0.23779991
Iteration 9, loss = 0.19580846
Iteration 10, loss = 0.16309457
Iteration 11, loss = 0.13773105
Iteration 12, loss = 0.11830722
Iteration 13, loss = 0.10294548
Iteration 14, loss = 0.09083952
Iteration 15, loss = 0.08151497
Iteration 16, loss = 0.07389727
Iteration 17, loss = 0.06769227
Iteration 18, loss = 0.06244597
Iteration 19, loss = 0.05836014
Iteration 20, loss = 0.05458628
Iteration 21, loss = 0.05143377
Iteration 22, loss = 0.04877457
Iteration 23, loss = 0.04660837
Iteration 24, loss = 0.04452498
Iteration 25, loss = 0.04261104
Iteration 26, loss = 0.04109697
Iteration 27, loss = 0.03997838
Iteration 28, loss = 0.03877667
Iteration 29, loss = 0.03763768
Iteration 30, loss = 0.03656643
Iteration 31, loss = 0.03572517
Iteration 32, loss = 0.03497560
Iteration 33, loss = 0.03396282
Iteration 34, loss = 0.03333152
Iteration 35, loss = 0.03276156
Iteration 36, loss = 0.03261211
Iteration 37, loss = 0.03199359
Iteration 38, loss = 0.03156129
Iteration 39, loss = 0.03096885
Iteration 40, loss = 0.03056625
Iteration 41, loss = 0.03014600
Iteration 42, loss = 0.02994539
Iteration 43, loss = 0.02968339
Iteration 44, loss = 0.02895633
Iteration 45, loss = 0.02902836
Iteration 46, loss = 0.02862497
Iteration 47, loss = 0.02834012
Iteration 48, loss = 0.02812173
Iteration 49, loss = 0.02786519
Iteration 50, loss = 0.02762637
Iteration 51, loss = 0.02761016
Iteration 52, loss = 0.02750986
Iteration 53, loss = 0.02746562
Iteration 54, loss = 0.02715409
Iteration 55, loss = 0.02689727
Iteration 56, loss = 0.02694556
Iteration 57, loss = 0.02707846
Iteration 58, loss = 0.02675929
Iteration 59, loss = 0.02670022
Iteration 60, loss = 0.02639832
Iteration 61, loss = 0.02649935
Iteration 62, loss = 0.02612770
Iteration 63, loss = 0.02608809
Iteration 64, loss = 0.02599350
Iteration 65, loss = 0.02598666
Iteration 66, loss = 0.02604958
Iteration 67, loss = 0.02590109
Iteration 68, loss = 0.02551486
Iteration 69, loss = 0.02581383
Iteration 70, loss = 0.02558544
Iteration 71, loss = 0.02538706
Iteration 72, loss = 0.02562843
Iteration 73, loss = 0.02543845
Iteration 74, loss = 0.02541545
Iteration 75, loss = 0.02536380
Iteration 76, loss = 0.02513291
Iteration 77, loss = 0.02527327
Iteration 78, loss = 0.02509805
Iteration 79, loss = 0.02512007
Iteration 80, loss = 0.02486520
Iteration 81, loss = 0.02480972
Iteration 82, loss = 0.02487271
Iteration 83, loss = 0.02494160
Iteration 84, loss = 0.02480041
Iteration 85, loss = 0.02473541
Iteration 86, loss = 0.02496160
Iteration 87, loss = 0.02487104
Iteration 88, loss = 0.02459988
Iteration 89, loss = 0.02470587
Iteration 90, loss = 0.02476782
Iteration 91, loss = 0.02491995
Iteration 92, loss = 0.02465528
Iteration 93, loss = 0.02446121
Iteration 94, loss = 0.02466640
Iteration 95, loss = 0.02444745
Iteration 96, loss = 0.02468737
Iteration 97, loss = 0.02446018
Iteration 98, loss = 0.02445094
Iteration 99, loss = 0.02429050
Iteration 100, loss = 0.02425536
Iteration 101, loss = 0.02417441
Iteration 102, loss = 0.02408855
Iteration 103, loss = 0.02410649
Iteration 104, loss = 0.02460577
Iteration 105, loss = 0.02457820
Iteration 106, loss = 0.02435700
Iteration 107, loss = 0.02399559
Iteration 108, loss = 0.02427038
Iteration 109, loss = 0.02417249
Iteration 110, loss = 0.02408081
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68519774
Iteration 2, loss = 0.61196190
Iteration 3, loss = 0.51010039
Iteration 4, loss = 0.40080357
Iteration 5, loss = 0.30610394
Iteration 6, loss = 0.23250207
Iteration 7, loss = 0.17915228
Iteration 8, loss = 0.14143155
Iteration 9, loss = 0.11519193
Iteration 10, loss = 0.09656363
Iteration 11, loss = 0.08265299
Iteration 12, loss = 0.07203558
Iteration 13, loss = 0.06396499
Iteration 14, loss = 0.05749200
Iteration 15, loss = 0.05287444
Iteration 16, loss = 0.04849835
Iteration 17, loss = 0.04543408
Iteration 18, loss = 0.04236326
Iteration 19, loss = 0.04002577
Iteration 20, loss = 0.03813901
Iteration 21, loss = 0.03631800
Iteration 22, loss = 0.03485752
Iteration 23, loss = 0.03364472
Iteration 24, loss = 0.03241962
Iteration 25, loss = 0.03174322
Iteration 26, loss = 0.03066086
Iteration 27, loss = 0.03017574
Iteration 28, loss = 0.02940595
Iteration 29, loss = 0.02880024
Iteration 30, loss = 0.02793809
Iteration 31, loss = 0.02761867
Iteration 32, loss = 0.02714187
Iteration 33, loss = 0.02667216
Iteration 34, loss = 0.02641194
Iteration 35, loss = 0.02615339
Iteration 36, loss = 0.02588296
Iteration 37, loss = 0.02556521
Iteration 38, loss = 0.02523283
Iteration 39, loss = 0.02489421
Iteration 40, loss = 0.02497730
Iteration 41, loss = 0.02469105
Iteration 42, loss = 0.02434150
Iteration 43, loss = 0.02412470
Iteration 44, loss = 0.02404842
Iteration 45, loss = 0.02434804
Iteration 46, loss = 0.02395177
Iteration 47, loss = 0.02372364
Iteration 48, loss = 0.02346041
Iteration 49, loss = 0.02345500
Iteration 50, loss = 0.02336797
Iteration 51, loss = 0.02297771
Iteration 52, loss = 0.02305492
Iteration 53, loss = 0.02305204
Iteration 54, loss = 0.02321877
Iteration 55, loss = 0.02294030
Iteration 56, loss = 0.02273519
Iteration 57, loss = 0.02272595
Iteration 58, loss = 0.02259415
Iteration 59, loss = 0.02248288
Iteration 60, loss = 0.02237728
Iteration 61, loss = 0.02249432
Iteration 62, loss = 0.02223522
Iteration 63, loss = 0.02256935
Iteration 64, loss = 0.02236140
Iteration 65, loss = 0.02259215
Iteration 66, loss = 0.02215562
Iteration 67, loss = 0.02229268
Iteration 68, loss = 0.02245600
Iteration 69, loss = 0.02199682
Iteration 70, loss = 0.02182689
Iteration 71, loss = 0.02197052
Iteration 72, loss = 0.02176961
Iteration 73, loss = 0.02184481
Iteration 74, loss = 0.02162555
Iteration 75, loss = 0.02179941
Iteration 76, loss = 0.02162057
Iteration 77, loss = 0.02164219
Iteration 78, loss = 0.02189864
Iteration 79, loss = 0.02167711
Iteration 80, loss = 0.02156363
Iteration 81, loss = 0.02185389
Iteration 82, loss = 0.02180268
Iteration 83, loss = 0.02155682
Iteration 84, loss = 0.02156076
Iteration 85, loss = 0.02193180
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68667240
Iteration 2, loss = 0.64185072
Iteration 3, loss = 0.57774431
Iteration 4, loss = 0.49515437
Iteration 5, loss = 0.41021220
Iteration 6, loss = 0.33350337
Iteration 7, loss = 0.26947938
Iteration 8, loss = 0.21840557
Iteration 9, loss = 0.17843792
Iteration 10, loss = 0.14806881
Iteration 11, loss = 0.12469506
Iteration 12, loss = 0.10671508
Iteration 13, loss = 0.09239835
Iteration 14, loss = 0.08117967
Iteration 15, loss = 0.07236896
Iteration 16, loss = 0.06526793
Iteration 17, loss = 0.05925231
Iteration 18, loss = 0.05437362
Iteration 19, loss = 0.05039895
Iteration 20, loss = 0.04660997
Iteration 21, loss = 0.04367639
Iteration 22, loss = 0.04110635
Iteration 23, loss = 0.03881576
Iteration 24, loss = 0.03689578
Iteration 25, loss = 0.03506418
Iteration 26, loss = 0.03355949
Iteration 27, loss = 0.03220505
Iteration 28, loss = 0.03118326
Iteration 29, loss = 0.03008930
Iteration 30, loss = 0.02904995
Iteration 31, loss = 0.02800780
Iteration 32, loss = 0.02733446
Iteration 33, loss = 0.02683768
Iteration 34, loss = 0.02611888
Iteration 35, loss = 0.02540785
Iteration 36, loss = 0.02494715
Iteration 37, loss = 0.02440223
Iteration 38, loss = 0.02388763
Iteration 39, loss = 0.02359455
Iteration 40, loss = 0.02327147
Iteration 41, loss = 0.02290897
Iteration 42, loss = 0.02245065
Iteration 43, loss = 0.02214157
Iteration 44, loss = 0.02202681
Iteration 45, loss = 0.02177450
Iteration 46, loss = 0.02128838
Iteration 47, loss = 0.02109673
Iteration 48, loss = 0.02082908
Iteration 49, loss = 0.02056742
Iteration 50, loss = 0.02039659
Iteration 51, loss = 0.02029972
Iteration 52, loss = 0.02016990
Iteration 53, loss = 0.01990545
Iteration 54, loss = 0.01965783
Iteration 55, loss = 0.01944854
Iteration 56, loss = 0.01952077
Iteration 57, loss = 0.01941704
Iteration 58, loss = 0.01923696
Iteration 59, loss = 0.01896105
Iteration 60, loss = 0.01915193
Iteration 61, loss = 0.01882829
Iteration 62, loss = 0.01877624
Iteration 63, loss = 0.01870468
Iteration 64, loss = 0.01870432
Iteration 65, loss = 0.01851584
Iteration 66, loss = 0.01856930
Iteration 67, loss = 0.01820012
Iteration 68, loss = 0.01833178
Iteration 69, loss = 0.01842657
Iteration 70, loss = 0.01819920
Iteration 71, loss = 0.01847387
Iteration 72, loss = 0.01802497
Iteration 73, loss = 0.01799147
Iteration 74, loss = 0.01801602
Iteration 75, loss = 0.01782476
Iteration 76, loss = 0.01783309
Iteration 77, loss = 0.01763163
Iteration 78, loss = 0.01780788
Iteration 79, loss = 0.01769639
Iteration 80, loss = 0.01754009
Iteration 81, loss = 0.01751606
Iteration 82, loss = 0.01758373
Iteration 83, loss = 0.01748588
Iteration 84, loss = 0.01724020
Iteration 85, loss = 0.01720611
Iteration 86, loss = 0.01734818
Iteration 87, loss = 0.01755055
Iteration 88, loss = 0.01728034
Iteration 89, loss = 0.01707209
Iteration 90, loss = 0.01712268
Iteration 91, loss = 0.01721818
Iteration 92, loss = 0.01718234
Iteration 93, loss = 0.01717695
Iteration 94, loss = 0.01712612
Iteration 95, loss = 0.01707215
Iteration 96, loss = 0.01706119
Iteration 97, loss = 0.01715694
Iteration 98, loss = 0.01688106
Iteration 99, loss = 0.01712582
Iteration 100, loss = 0.01706353
Iteration 101, loss = 0.01685826
Iteration 102, loss = 0.01684900
Iteration 103, loss = 0.01676934
Iteration 104, loss = 0.01683192
Iteration 105, loss = 0.01672903
Iteration 106, loss = 0.01687439
Iteration 107, loss = 0.01652369
Iteration 108, loss = 0.01683729
Iteration 109, loss = 0.01684053
Iteration 110, loss = 0.01688084
Iteration 111, loss = 0.01666383
Iteration 112, loss = 0.01671447
Iteration 113, loss = 0.01675649
Iteration 114, loss = 0.01661243
Iteration 115, loss = 0.01666236
Iteration 116, loss = 0.01663832
Iteration 117, loss = 0.01661865
Iteration 118, loss = 0.01654399
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71169709
Iteration 2, loss = 0.69602143
Iteration 3, loss = 0.68232210
Iteration 4, loss = 0.66865836
Iteration 5, loss = 0.65451370
Iteration 6, loss = 0.63949874
Iteration 7, loss = 0.62296941
Iteration 8, loss = 0.60525451
Iteration 9, loss = 0.58604099
Iteration 10, loss = 0.56559747
Iteration 11, loss = 0.54429911
Iteration 12, loss = 0.52166352
Iteration 13, loss = 0.49841846
Iteration 14, loss = 0.47484864
Iteration 15, loss = 0.45111364
Iteration 16, loss = 0.42771270
Iteration 17, loss = 0.40456291
Iteration 18, loss = 0.38200707
Iteration 19, loss = 0.36005532
Iteration 20, loss = 0.33904867
Iteration 21, loss = 0.31895166
Iteration 22, loss = 0.29967043
Iteration 23, loss = 0.28144658
Iteration 24, loss = 0.26416479
Iteration 25, loss = 0.24781987
Iteration 26, loss = 0.23251546
Iteration 27, loss = 0.21818046
Iteration 28, loss = 0.20473290
Iteration 29, loss = 0.19223697
Iteration 30, loss = 0.18056519
Iteration 31, loss = 0.16978137
Iteration 32, loss = 0.15970730
Iteration 33, loss = 0.15036634
Iteration 34, loss = 0.14169638
Iteration 35, loss = 0.13366094
Iteration 36, loss = 0.12624521
Iteration 37, loss = 0.11937294
Iteration 38, loss = 0.11295384
Iteration 39, loss = 0.10703104
Iteration 40, loss = 0.10158731
Iteration 41, loss = 0.09646565
Iteration 42, loss = 0.09178913
Iteration 43, loss = 0.08737331
Iteration 44, loss = 0.08327169
Iteration 45, loss = 0.07951329
Iteration 46, loss = 0.07596390
Iteration 47, loss = 0.07265380
Iteration 48, loss = 0.06956326
Iteration 49, loss = 0.06673694
Iteration 50, loss = 0.06400151
Iteration 51, loss = 0.06147726
Iteration 52, loss = 0.05908148
Iteration 53, loss = 0.05687675
Iteration 54, loss = 0.05476746
Iteration 55, loss = 0.05282692
Iteration 56, loss = 0.05096328
Iteration 57, loss = 0.04925145
Iteration 58, loss = 0.04757393
Iteration 59, loss = 0.04605302
Iteration 60, loss = 0.04456859
Iteration 61, loss = 0.04319553
Iteration 62, loss = 0.04190007
Iteration 63, loss = 0.04064821
Iteration 64, loss = 0.03946851
Iteration 65, loss = 0.03836221
Iteration 66, loss = 0.03730472
Iteration 67, loss = 0.03629797
Iteration 68, loss = 0.03534889
Iteration 69, loss = 0.03445509
Iteration 70, loss = 0.03355876
Iteration 71, loss = 0.03272879
Iteration 72, loss = 0.03193292
Iteration 73, loss = 0.03120136
Iteration 74, loss = 0.03047252
Iteration 75, loss = 0.02981122
Iteration 76, loss = 0.02911884
Iteration 77, loss = 0.02849292
Iteration 78, loss = 0.02788841
Iteration 79, loss = 0.02732695
Iteration 80, loss = 0.02675131
Iteration 81, loss = 0.02622669
Iteration 82, loss = 0.02570973
Iteration 83, loss = 0.02524472
Iteration 84, loss = 0.02483162
Iteration 85, loss = 0.02441563
Iteration 86, loss = 0.02397535
Iteration 87, loss = 0.02361127
Iteration 88, loss = 0.02319296
Iteration 89, loss = 0.02281291
Iteration 90, loss = 0.02238397
Iteration 91, loss = 0.02198951
Iteration 92, loss = 0.02162309
Iteration 93, loss = 0.02130198
Iteration 94, loss = 0.02091513
Iteration 95, loss = 0.02059558
Iteration 96, loss = 0.02021816
Iteration 97, loss = 0.01995392
Iteration 98, loss = 0.01972068
Iteration 99, loss = 0.01944422
Iteration 100, loss = 0.01914956
Iteration 101, loss = 0.01895208
Iteration 102, loss = 0.01862643
Iteration 103, loss = 0.01837960
Iteration 104, loss = 0.01815075
Iteration 105, loss = 0.01789629
Iteration 106, loss = 0.01773757
Iteration 107, loss = 0.01745264
Iteration 108, loss = 0.01733746
Iteration 109, loss = 0.01710204
Iteration 110, loss = 0.01697851
Iteration 111, loss = 0.01673645
Iteration 112, loss = 0.01652652
Iteration 113, loss = 0.01633345
Iteration 114, loss = 0.01614764
Iteration 115, loss = 0.01598831
Iteration 116, loss = 0.01580151
Iteration 117, loss = 0.01565213
Iteration 118, loss = 0.01550214
Iteration 119, loss = 0.01535201
Iteration 120, loss = 0.01517926
Iteration 121, loss = 0.01509074
Iteration 122, loss = 0.01488788
Iteration 123, loss = 0.01475791
Iteration 124, loss = 0.01460355
Iteration 125, loss = 0.01457766
Iteration 126, loss = 0.01437988
Iteration 127, loss = 0.01427260
Iteration 128, loss = 0.01412123
Iteration 129, loss = 0.01400481
Iteration 130, loss = 0.01389615
Iteration 131, loss = 0.01379618
Iteration 132, loss = 0.01368726
Iteration 133, loss = 0.01356636
Iteration 134, loss = 0.01345668
Iteration 135, loss = 0.01335673
Iteration 136, loss = 0.01324692
Iteration 137, loss = 0.01314882
Iteration 138, loss = 0.01306885
Iteration 139, loss = 0.01294237
Iteration 140, loss = 0.01285373
Iteration 141, loss = 0.01277226
Iteration 142, loss = 0.01266714
Iteration 143, loss = 0.01258552
Iteration 144, loss = 0.01250108
Iteration 145, loss = 0.01243033
Iteration 146, loss = 0.01236798
Iteration 147, loss = 0.01232547
Iteration 148, loss = 0.01225308
Iteration 149, loss = 0.01215913
Iteration 150, loss = 0.01204648
Iteration 151, loss = 0.01199451
Iteration 152, loss = 0.01193756
Iteration 153, loss = 0.01182648
Iteration 154, loss = 0.01176192
Iteration 155, loss = 0.01174283
Iteration 156, loss = 0.01165371
Iteration 157, loss = 0.01158221
Iteration 158, loss = 0.01153312
Iteration 159, loss = 0.01146292
Iteration 160, loss = 0.01138521
Iteration 161, loss = 0.01133003
Iteration 162, loss = 0.01128786
Iteration 163, loss = 0.01124210
Iteration 164, loss = 0.01120453
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67672454
Iteration 2, loss = 0.61200464
Iteration 3, loss = 0.52346350
Iteration 4, loss = 0.42382897
Iteration 5, loss = 0.32949255
Iteration 6, loss = 0.25327564
Iteration 7, loss = 0.19663873
Iteration 8, loss = 0.15544380
Iteration 9, loss = 0.12639931
Iteration 10, loss = 0.10533493
Iteration 11, loss = 0.08944015
Iteration 12, loss = 0.07773578
Iteration 13, loss = 0.06860987
Iteration 14, loss = 0.06155022
Iteration 15, loss = 0.05600954
Iteration 16, loss = 0.05118139
Iteration 17, loss = 0.04777519
Iteration 18, loss = 0.04449612
Iteration 19, loss = 0.04207109
Iteration 20, loss = 0.03975118
Iteration 21, loss = 0.03765833
Iteration 22, loss = 0.03604651
Iteration 23, loss = 0.03446873
Iteration 24, loss = 0.03300907
Iteration 25, loss = 0.03189540
Iteration 26, loss = 0.03080224
Iteration 27, loss = 0.03010639
Iteration 28, loss = 0.02923447
Iteration 29, loss = 0.02879071
Iteration 30, loss = 0.02797285
Iteration 31, loss = 0.02743528
Iteration 32, loss = 0.02693059
Iteration 33, loss = 0.02657812
Iteration 34, loss = 0.02603171
Iteration 35, loss = 0.02561981
Iteration 36, loss = 0.02526824
Iteration 37, loss = 0.02477372
Iteration 38, loss = 0.02448689
Iteration 39, loss = 0.02424194
Iteration 40, loss = 0.02386098
Iteration 41, loss = 0.02340862
Iteration 42, loss = 0.02366109
Iteration 43, loss = 0.02374072
Iteration 44, loss = 0.02337754
Iteration 45, loss = 0.02280437
Iteration 46, loss = 0.02253654
Iteration 47, loss = 0.02243772
Iteration 48, loss = 0.02212019
Iteration 49, loss = 0.02195941
Iteration 50, loss = 0.02195576
Iteration 51, loss = 0.02191229
Iteration 52, loss = 0.02164673
Iteration 53, loss = 0.02148137
Iteration 54, loss = 0.02134929
Iteration 55, loss = 0.02136582
Iteration 56, loss = 0.02138115
Iteration 57, loss = 0.02116572
Iteration 58, loss = 0.02096092
Iteration 59, loss = 0.02088244
Iteration 60, loss = 0.02083158
Iteration 61, loss = 0.02087387
Iteration 62, loss = 0.02071596
Iteration 63, loss = 0.02132624
Iteration 64, loss = 0.02212467
Iteration 65, loss = 0.02159001
Iteration 66, loss = 0.02114733
Iteration 67, loss = 0.02086298
Iteration 68, loss = 0.02072211
Iteration 69, loss = 0.02053446
Iteration 70, loss = 0.02029033
Iteration 71, loss = 0.02029247
Iteration 72, loss = 0.02001230
Iteration 73, loss = 0.02013512
Iteration 74, loss = 0.02002554
Iteration 75, loss = 0.02008444
Iteration 76, loss = 0.02045837
Iteration 77, loss = 0.02021229
Iteration 78, loss = 0.02008655
Iteration 79, loss = 0.01987786
Iteration 80, loss = 0.02003608
Iteration 81, loss = 0.01980954
Iteration 82, loss = 0.01979605
Iteration 83, loss = 0.02021439
Iteration 84, loss = 0.02027665
Iteration 85, loss = 0.02050367
Iteration 86, loss = 0.02013928
Iteration 87, loss = 0.01988929
Iteration 88, loss = 0.01990114
Iteration 89, loss = 0.01985680
Iteration 90, loss = 0.01995489
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68586174
Iteration 2, loss = 0.66720307
Iteration 3, loss = 0.64857065
Iteration 4, loss = 0.62685143
Iteration 5, loss = 0.60099529
Iteration 6, loss = 0.57143073
Iteration 7, loss = 0.53854191
Iteration 8, loss = 0.50345712
Iteration 9, loss = 0.46771775
Iteration 10, loss = 0.43202756
Iteration 11, loss = 0.39747171
Iteration 12, loss = 0.36414911
Iteration 13, loss = 0.33266162
Iteration 14, loss = 0.30336902
Iteration 15, loss = 0.27608557
Iteration 16, loss = 0.25114040
Iteration 17, loss = 0.22831349
Iteration 18, loss = 0.20779578
Iteration 19, loss = 0.18926046
Iteration 20, loss = 0.17270302
Iteration 21, loss = 0.15799004
Iteration 22, loss = 0.14478598
Iteration 23, loss = 0.13308318
Iteration 24, loss = 0.12267891
Iteration 25, loss = 0.11339145
Iteration 26, loss = 0.10511464
Iteration 27, loss = 0.09779669
Iteration 28, loss = 0.09117518
Iteration 29, loss = 0.08526427
Iteration 30, loss = 0.07994672
Iteration 31, loss = 0.07515316
Iteration 32, loss = 0.07081310
Iteration 33, loss = 0.06699714
Iteration 34, loss = 0.06337202
Iteration 35, loss = 0.06020644
Iteration 36, loss = 0.05723587
Iteration 37, loss = 0.05450050
Iteration 38, loss = 0.05203865
Iteration 39, loss = 0.04977245
Iteration 40, loss = 0.04771726
Iteration 41, loss = 0.04578553
Iteration 42, loss = 0.04398318
Iteration 43, loss = 0.04236954
Iteration 44, loss = 0.04079318
Iteration 45, loss = 0.03946855
Iteration 46, loss = 0.03809808
Iteration 47, loss = 0.03687817
Iteration 48, loss = 0.03567801
Iteration 49, loss = 0.03458772
Iteration 50, loss = 0.03355071
Iteration 51, loss = 0.03260879
Iteration 52, loss = 0.03178280
Iteration 53, loss = 0.03090309
Iteration 54, loss = 0.03011903
Iteration 55, loss = 0.02931458
Iteration 56, loss = 0.02872397
Iteration 57, loss = 0.02796811
Iteration 58, loss = 0.02733677
Iteration 59, loss = 0.02672805
Iteration 60, loss = 0.02619895
Iteration 61, loss = 0.02564859
Iteration 62, loss = 0.02516178
Iteration 63, loss = 0.02463820
Iteration 64, loss = 0.02418609
Iteration 65, loss = 0.02371212
Iteration 66, loss = 0.02338453
Iteration 67, loss = 0.02291366
Iteration 68, loss = 0.02250635
Iteration 69, loss = 0.02214373
Iteration 70, loss = 0.02181686
Iteration 71, loss = 0.02154793
Iteration 72, loss = 0.02115644
Iteration 73, loss = 0.02079690
Iteration 74, loss = 0.02059902
Iteration 75, loss = 0.02035357
Iteration 76, loss = 0.01998859
Iteration 77, loss = 0.01977375
Iteration 78, loss = 0.01949992
Iteration 79, loss = 0.01928801
Iteration 80, loss = 0.01901898
Iteration 81, loss = 0.01879440
Iteration 82, loss = 0.01854405
Iteration 83, loss = 0.01836703
Iteration 84, loss = 0.01820520
Iteration 85, loss = 0.01794112
Iteration 86, loss = 0.01784578
Iteration 87, loss = 0.01764961
Iteration 88, loss = 0.01747035
Iteration 89, loss = 0.01727650
Iteration 90, loss = 0.01712971
Iteration 91, loss = 0.01698270
Iteration 92, loss = 0.01689889
Iteration 93, loss = 0.01666374
Iteration 94, loss = 0.01650648
Iteration 95, loss = 0.01642462
Iteration 96, loss = 0.01625737
Iteration 97, loss = 0.01616052
Iteration 98, loss = 0.01596028
Iteration 99, loss = 0.01590596
Iteration 100, loss = 0.01578263
Iteration 101, loss = 0.01565932
Iteration 102, loss = 0.01557132
Iteration 103, loss = 0.01545449
Iteration 104, loss = 0.01535558
Iteration 105, loss = 0.01532382
Iteration 106, loss = 0.01509583
Iteration 107, loss = 0.01508606
Iteration 108, loss = 0.01502315
Iteration 109, loss = 0.01493792
Iteration 110, loss = 0.01474962
Iteration 111, loss = 0.01464938
Iteration 112, loss = 0.01460252
Iteration 113, loss = 0.01452875
Iteration 114, loss = 0.01452940
Iteration 115, loss = 0.01438660
Iteration 116, loss = 0.01436651
Iteration 117, loss = 0.01419106
Iteration 118, loss = 0.01417092
Iteration 119, loss = 0.01410005
Iteration 120, loss = 0.01400691
Iteration 121, loss = 0.01396907
Iteration 122, loss = 0.01394136
Iteration 123, loss = 0.01381016
Iteration 124, loss = 0.01376150
Iteration 125, loss = 0.01372031
Iteration 126, loss = 0.01367447
Iteration 127, loss = 0.01356192
Iteration 128, loss = 0.01359602
Iteration 129, loss = 0.01348682
Iteration 130, loss = 0.01345858
Iteration 131, loss = 0.01346615
Iteration 132, loss = 0.01339550
Iteration 133, loss = 0.01328841
Iteration 134, loss = 0.01327652
Iteration 135, loss = 0.01322430
Iteration 136, loss = 0.01310884
Iteration 137, loss = 0.01310511
Iteration 138, loss = 0.01303251
Iteration 139, loss = 0.01297065
Iteration 140, loss = 0.01296974
Iteration 141, loss = 0.01291200
Iteration 142, loss = 0.01286974
Iteration 143, loss = 0.01284376
Iteration 144, loss = 0.01279232
Iteration 145, loss = 0.01280774
Iteration 146, loss = 0.01282533
Iteration 147, loss = 0.01282999
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67723548
Iteration 2, loss = 0.64090401
Iteration 3, loss = 0.58189727
Iteration 4, loss = 0.50511325
Iteration 5, loss = 0.42532467
Iteration 6, loss = 0.34966829
Iteration 7, loss = 0.28447530
Iteration 8, loss = 0.23099124
Iteration 9, loss = 0.18935679
Iteration 10, loss = 0.15728211
Iteration 11, loss = 0.13287718
Iteration 12, loss = 0.11370958
Iteration 13, loss = 0.09934875
Iteration 14, loss = 0.08770956
Iteration 15, loss = 0.07875866
Iteration 16, loss = 0.07116581
Iteration 17, loss = 0.06502577
Iteration 18, loss = 0.06005069
Iteration 19, loss = 0.05577706
Iteration 20, loss = 0.05244130
Iteration 21, loss = 0.04938362
Iteration 22, loss = 0.04678749
Iteration 23, loss = 0.04462140
Iteration 24, loss = 0.04258088
Iteration 25, loss = 0.04083273
Iteration 26, loss = 0.03947631
Iteration 27, loss = 0.03804518
Iteration 28, loss = 0.03685649
Iteration 29, loss = 0.03568444
Iteration 30, loss = 0.03488702
Iteration 31, loss = 0.03385809
Iteration 32, loss = 0.03307986
Iteration 33, loss = 0.03235921
Iteration 34, loss = 0.03202345
Iteration 35, loss = 0.03140602
Iteration 36, loss = 0.03094513
Iteration 37, loss = 0.03033169
Iteration 38, loss = 0.02974666
Iteration 39, loss = 0.02960329
Iteration 40, loss = 0.02888249
Iteration 41, loss = 0.02877702
Iteration 42, loss = 0.02835251
Iteration 43, loss = 0.02801176
Iteration 44, loss = 0.02788581
Iteration 45, loss = 0.02756335
Iteration 46, loss = 0.02721495
Iteration 47, loss = 0.02715308
Iteration 48, loss = 0.02678690
Iteration 49, loss = 0.02690506
Iteration 50, loss = 0.02657278
Iteration 51, loss = 0.02615757
Iteration 52, loss = 0.02610191
Iteration 53, loss = 0.02614100
Iteration 54, loss = 0.02599033
Iteration 55, loss = 0.02579019
Iteration 56, loss = 0.02542517
Iteration 57, loss = 0.02556934
Iteration 58, loss = 0.02519334
Iteration 59, loss = 0.02515538
Iteration 60, loss = 0.02493981
Iteration 61, loss = 0.02494023
Iteration 62, loss = 0.02468641
Iteration 63, loss = 0.02474539
Iteration 64, loss = 0.02467974
Iteration 65, loss = 0.02462032
Iteration 66, loss = 0.02456171
Iteration 67, loss = 0.02454670
Iteration 68, loss = 0.02450252
Iteration 69, loss = 0.02443045
Iteration 70, loss = 0.02445136
Iteration 71, loss = 0.02428454
Iteration 72, loss = 0.02414913
Iteration 73, loss = 0.02410290
Iteration 74, loss = 0.02411278
Iteration 75, loss = 0.02399929
Iteration 76, loss = 0.02366870
Iteration 77, loss = 0.02441739
Iteration 78, loss = 0.02405518
Iteration 79, loss = 0.02389659
Iteration 80, loss = 0.02380846
Iteration 81, loss = 0.02391609
Iteration 82, loss = 0.02383678
Iteration 83, loss = 0.02370197
Iteration 84, loss = 0.02348340
Iteration 85, loss = 0.02355396
Iteration 86, loss = 0.02373730
Iteration 87, loss = 0.02328050
Iteration 88, loss = 0.02358031
Iteration 89, loss = 0.02331713
Iteration 90, loss = 0.02392751
Iteration 91, loss = 0.02344748
Iteration 92, loss = 0.02348839
Iteration 93, loss = 0.02345374
Iteration 94, loss = 0.02346776
Iteration 95, loss = 0.02312039
Iteration 96, loss = 0.02322980
Iteration 97, loss = 0.02329842
Iteration 98, loss = 0.02332380
Iteration 99, loss = 0.02313498
Iteration 100, loss = 0.02307046
Iteration 101, loss = 0.02301943
Iteration 102, loss = 0.02351677
Iteration 103, loss = 0.02310580
Iteration 104, loss = 0.02307382
Iteration 105, loss = 0.02304414
Iteration 106, loss = 0.02308990
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67837925
Iteration 2, loss = 0.64678013
Iteration 3, loss = 0.60823517
Iteration 4, loss = 0.56051240
Iteration 5, loss = 0.50579467
Iteration 6, loss = 0.44980683
Iteration 7, loss = 0.39475897
Iteration 8, loss = 0.34297009
Iteration 9, loss = 0.29600192
Iteration 10, loss = 0.25416385
Iteration 11, loss = 0.21850689
Iteration 12, loss = 0.18794002
Iteration 13, loss = 0.16251290
Iteration 14, loss = 0.14159174
Iteration 15, loss = 0.12413483
Iteration 16, loss = 0.10990209
Iteration 17, loss = 0.09787265
Iteration 18, loss = 0.08779644
Iteration 19, loss = 0.07952931
Iteration 20, loss = 0.07243277
Iteration 21, loss = 0.06648950
Iteration 22, loss = 0.06137307
Iteration 23, loss = 0.05684837
Iteration 24, loss = 0.05301251
Iteration 25, loss = 0.04962398
Iteration 26, loss = 0.04663109
Iteration 27, loss = 0.04407936
Iteration 28, loss = 0.04164944
Iteration 29, loss = 0.03960516
Iteration 30, loss = 0.03774871
Iteration 31, loss = 0.03612196
Iteration 32, loss = 0.03471641
Iteration 33, loss = 0.03320827
Iteration 34, loss = 0.03200892
Iteration 35, loss = 0.03086789
Iteration 36, loss = 0.02989882
Iteration 37, loss = 0.02887228
Iteration 38, loss = 0.02798488
Iteration 39, loss = 0.02722996
Iteration 40, loss = 0.02649486
Iteration 41, loss = 0.02587599
Iteration 42, loss = 0.02517346
Iteration 43, loss = 0.02471604
Iteration 44, loss = 0.02395911
Iteration 45, loss = 0.02353337
Iteration 46, loss = 0.02306372
Iteration 47, loss = 0.02264266
Iteration 48, loss = 0.02221047
Iteration 49, loss = 0.02194418
Iteration 50, loss = 0.02158858
Iteration 51, loss = 0.02123582
Iteration 52, loss = 0.02085753
Iteration 53, loss = 0.02053050
Iteration 54, loss = 0.02026346
Iteration 55, loss = 0.01996543
Iteration 56, loss = 0.01968736
Iteration 57, loss = 0.01956564
Iteration 58, loss = 0.01939524
Iteration 59, loss = 0.01898000
Iteration 60, loss = 0.01883635
Iteration 61, loss = 0.01865239
Iteration 62, loss = 0.01846428
Iteration 63, loss = 0.01828714
Iteration 64, loss = 0.01814569
Iteration 65, loss = 0.01798406
Iteration 66, loss = 0.01782331
Iteration 67, loss = 0.01770713
Iteration 68, loss = 0.01753634
Iteration 69, loss = 0.01757279
Iteration 70, loss = 0.01748886
Iteration 71, loss = 0.01719452
Iteration 72, loss = 0.01704243
Iteration 73, loss = 0.01708243
Iteration 74, loss = 0.01692676
Iteration 75, loss = 0.01676852
Iteration 76, loss = 0.01657120
Iteration 77, loss = 0.01662969
Iteration 78, loss = 0.01642900
Iteration 79, loss = 0.01636644
Iteration 80, loss = 0.01625449
Iteration 81, loss = 0.01613353
Iteration 82, loss = 0.01626682
Iteration 83, loss = 0.01609624
Iteration 84, loss = 0.01600564
Iteration 85, loss = 0.01613696
Iteration 86, loss = 0.01575567
Iteration 87, loss = 0.01577243
Iteration 88, loss = 0.01573307
Iteration 89, loss = 0.01577184
Iteration 90, loss = 0.01578078
Iteration 91, loss = 0.01572919
Iteration 92, loss = 0.01561028
Iteration 93, loss = 0.01551501
Iteration 94, loss = 0.01535785
Iteration 95, loss = 0.01534241
Iteration 96, loss = 0.01525917
Iteration 97, loss = 0.01521530
Iteration 98, loss = 0.01515769
Iteration 99, loss = 0.01514090
Iteration 100, loss = 0.01527102
Iteration 101, loss = 0.01509164
Iteration 102, loss = 0.01506268
Iteration 103, loss = 0.01505769
Iteration 104, loss = 0.01479080
Iteration 105, loss = 0.01497705
Iteration 106, loss = 0.01489577
Iteration 107, loss = 0.01484111
Iteration 108, loss = 0.01485987
Iteration 109, loss = 0.01486665
Iteration 110, loss = 0.01480010
Iteration 111, loss = 0.01485255
Iteration 112, loss = 0.01472687
Iteration 113, loss = 0.01475940
Iteration 114, loss = 0.01463874
Iteration 115, loss = 0.01457740
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69271059
Iteration 2, loss = 0.64731571
Iteration 3, loss = 0.58704902
Iteration 4, loss = 0.50861502
Iteration 5, loss = 0.42526494
Iteration 6, loss = 0.34806746
Iteration 7, loss = 0.28173285
Iteration 8, loss = 0.22888046
Iteration 9, loss = 0.18714335
Iteration 10, loss = 0.15549212
Iteration 11, loss = 0.13110043
Iteration 12, loss = 0.11272520
Iteration 13, loss = 0.09834999
Iteration 14, loss = 0.08681218
Iteration 15, loss = 0.07785416
Iteration 16, loss = 0.07039635
Iteration 17, loss = 0.06435916
Iteration 18, loss = 0.05938864
Iteration 19, loss = 0.05539907
Iteration 20, loss = 0.05174304
Iteration 21, loss = 0.04899502
Iteration 22, loss = 0.04629298
Iteration 23, loss = 0.04396303
Iteration 24, loss = 0.04195036
Iteration 25, loss = 0.04041135
Iteration 26, loss = 0.03879766
Iteration 27, loss = 0.03740931
Iteration 28, loss = 0.03635489
Iteration 29, loss = 0.03523037
Iteration 30, loss = 0.03417854
Iteration 31, loss = 0.03313349
Iteration 32, loss = 0.03235910
Iteration 33, loss = 0.03172590
Iteration 34, loss = 0.03114107
Iteration 35, loss = 0.03064755
Iteration 36, loss = 0.03003558
Iteration 37, loss = 0.02944701
Iteration 38, loss = 0.02906081
Iteration 39, loss = 0.02838919
Iteration 40, loss = 0.02819543
Iteration 41, loss = 0.02797340
Iteration 42, loss = 0.02788581
Iteration 43, loss = 0.02718929
Iteration 44, loss = 0.02686198
Iteration 45, loss = 0.02652507
Iteration 46, loss = 0.02614576
Iteration 47, loss = 0.02599988
Iteration 48, loss = 0.02584504
Iteration 49, loss = 0.02570564
Iteration 50, loss = 0.02545299
Iteration 51, loss = 0.02536221
Iteration 52, loss = 0.02491486
Iteration 53, loss = 0.02489113
Iteration 54, loss = 0.02482872
Iteration 55, loss = 0.02455493
Iteration 56, loss = 0.02458806
Iteration 57, loss = 0.02433086
Iteration 58, loss = 0.02417477
Iteration 59, loss = 0.02406681
Iteration 60, loss = 0.02378639
Iteration 61, loss = 0.02381657
Iteration 62, loss = 0.02362194
Iteration 63, loss = 0.02354568
Iteration 64, loss = 0.02356654
Iteration 65, loss = 0.02343465
Iteration 66, loss = 0.02349580
Iteration 67, loss = 0.02340923
Iteration 68, loss = 0.02320694
Iteration 69, loss = 0.02318426
Iteration 70, loss = 0.02317787
Iteration 71, loss = 0.02299458
Iteration 72, loss = 0.02295922
Iteration 73, loss = 0.02272947
Iteration 74, loss = 0.02310315
Iteration 75, loss = 0.02308345
Iteration 76, loss = 0.02289589
Iteration 77, loss = 0.02268334
Iteration 78, loss = 0.02285861
Iteration 79, loss = 0.02258770
Iteration 80, loss = 0.02257127
Iteration 81, loss = 0.02247325
Iteration 82, loss = 0.02237378
Iteration 83, loss = 0.02246331
Iteration 84, loss = 0.02244036
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67590856
Iteration 2, loss = 0.61004689
Iteration 3, loss = 0.51391863
Iteration 4, loss = 0.41042555
Iteration 5, loss = 0.31954150
Iteration 6, loss = 0.24703126
Iteration 7, loss = 0.19301030
Iteration 8, loss = 0.15436833
Iteration 9, loss = 0.12617185
Iteration 10, loss = 0.10628227
Iteration 11, loss = 0.09132171
Iteration 12, loss = 0.07998405
Iteration 13, loss = 0.07102210
Iteration 14, loss = 0.06419756
Iteration 15, loss = 0.05854468
Iteration 16, loss = 0.05390738
Iteration 17, loss = 0.05016287
Iteration 18, loss = 0.04713132
Iteration 19, loss = 0.04426283
Iteration 20, loss = 0.04228282
Iteration 21, loss = 0.03998591
Iteration 22, loss = 0.03836577
Iteration 23, loss = 0.03649764
Iteration 24, loss = 0.03616501
Iteration 25, loss = 0.03487612
Iteration 26, loss = 0.03409089
Iteration 27, loss = 0.03297872
Iteration 28, loss = 0.03240683
Iteration 29, loss = 0.03192347
Iteration 30, loss = 0.03141344
Iteration 31, loss = 0.03068532
Iteration 32, loss = 0.02981750
Iteration 33, loss = 0.02934651
Iteration 34, loss = 0.02880720
Iteration 35, loss = 0.02823550
Iteration 36, loss = 0.02815699
Iteration 37, loss = 0.02767171
Iteration 38, loss = 0.02767828
Iteration 39, loss = 0.02686968
Iteration 40, loss = 0.02661019
Iteration 41, loss = 0.02639844
Iteration 42, loss = 0.02616730
Iteration 43, loss = 0.02583626
Iteration 44, loss = 0.02550268
Iteration 45, loss = 0.02537667
Iteration 46, loss = 0.02543265
Iteration 47, loss = 0.02502827
Iteration 48, loss = 0.02538142
Iteration 49, loss = 0.02538442
Iteration 50, loss = 0.02520460
Iteration 51, loss = 0.02485008
Iteration 52, loss = 0.02453544
Iteration 53, loss = 0.02458643
Iteration 54, loss = 0.02446315
Iteration 55, loss = 0.02412804
Iteration 56, loss = 0.02410608
Iteration 57, loss = 0.02375126
Iteration 58, loss = 0.02376751
Iteration 59, loss = 0.02366508
Iteration 60, loss = 0.02419387
Iteration 61, loss = 0.02445380
Iteration 62, loss = 0.02432198
Iteration 63, loss = 0.02412752
Iteration 64, loss = 0.02380938
Iteration 65, loss = 0.02355717
Iteration 66, loss = 0.02398712
Iteration 67, loss = 0.02403491
Iteration 68, loss = 0.02402915
Iteration 69, loss = 0.02372915
Iteration 70, loss = 0.02401630
Iteration 71, loss = 0.02378546
Iteration 72, loss = 0.02355802
Iteration 73, loss = 0.02324725
Iteration 74, loss = 0.02330350
Iteration 75, loss = 0.02354960
Iteration 76, loss = 0.02361424
Iteration 77, loss = 0.02313222
Iteration 78, loss = 0.02334012
Iteration 79, loss = 0.02315580
Iteration 80, loss = 0.02314740
Iteration 81, loss = 0.02315596
Iteration 82, loss = 0.02265609
Iteration 83, loss = 0.02296865
Iteration 84, loss = 0.02257961
Iteration 85, loss = 0.02284748
Iteration 86, loss = 0.02239656
Iteration 87, loss = 0.02261344
Iteration 88, loss = 0.02237267
Iteration 89, loss = 0.02245771
Iteration 90, loss = 0.02256774
Iteration 91, loss = 0.02227892
Iteration 92, loss = 0.02265587
Iteration 93, loss = 0.02272418
Iteration 94, loss = 0.02250895
Iteration 95, loss = 0.02290056
Iteration 96, loss = 0.02282830
Iteration 97, loss = 0.02285864
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71768793
Iteration 2, loss = 0.69872972
Iteration 3, loss = 0.68288083
Iteration 4, loss = 0.66791344
Iteration 5, loss = 0.65228795
Iteration 6, loss = 0.63592361
Iteration 7, loss = 0.61844715
Iteration 8, loss = 0.59984422
Iteration 9, loss = 0.58014914
Iteration 10, loss = 0.55947365
Iteration 11, loss = 0.53825642
Iteration 12, loss = 0.51636981
Iteration 13, loss = 0.49421193
Iteration 14, loss = 0.47182380
Iteration 15, loss = 0.44949041
Iteration 16, loss = 0.42735093
Iteration 17, loss = 0.40557427
Iteration 18, loss = 0.38407889
Iteration 19, loss = 0.36318319
Iteration 20, loss = 0.34293543
Iteration 21, loss = 0.32343158
Iteration 22, loss = 0.30454189
Iteration 23, loss = 0.28659489
Iteration 24, loss = 0.26940577
Iteration 25, loss = 0.25321790
Iteration 26, loss = 0.23781534
Iteration 27, loss = 0.22335343
Iteration 28, loss = 0.20980775
Iteration 29, loss = 0.19715158
Iteration 30, loss = 0.18532622
Iteration 31, loss = 0.17429675
Iteration 32, loss = 0.16404170
Iteration 33, loss = 0.15454813
Iteration 34, loss = 0.14573166
Iteration 35, loss = 0.13749037
Iteration 36, loss = 0.12984902
Iteration 37, loss = 0.12277095
Iteration 38, loss = 0.11629557
Iteration 39, loss = 0.11016986
Iteration 40, loss = 0.10450639
Iteration 41, loss = 0.09928105
Iteration 42, loss = 0.09440171
Iteration 43, loss = 0.08987476
Iteration 44, loss = 0.08566937
Iteration 45, loss = 0.08177927
Iteration 46, loss = 0.07805677
Iteration 47, loss = 0.07461889
Iteration 48, loss = 0.07146709
Iteration 49, loss = 0.06844686
Iteration 50, loss = 0.06568368
Iteration 51, loss = 0.06309912
Iteration 52, loss = 0.06065026
Iteration 53, loss = 0.05830234
Iteration 54, loss = 0.05614818
Iteration 55, loss = 0.05411455
Iteration 56, loss = 0.05218573
Iteration 57, loss = 0.05039067
Iteration 58, loss = 0.04869077
Iteration 59, loss = 0.04709182
Iteration 60, loss = 0.04556585
Iteration 61, loss = 0.04411044
Iteration 62, loss = 0.04272245
Iteration 63, loss = 0.04143706
Iteration 64, loss = 0.04024006
Iteration 65, loss = 0.03905601
Iteration 66, loss = 0.03792028
Iteration 67, loss = 0.03692362
Iteration 68, loss = 0.03592478
Iteration 69, loss = 0.03494553
Iteration 70, loss = 0.03408513
Iteration 71, loss = 0.03321064
Iteration 72, loss = 0.03237871
Iteration 73, loss = 0.03158204
Iteration 74, loss = 0.03081100
Iteration 75, loss = 0.03003235
Iteration 76, loss = 0.02933413
Iteration 77, loss = 0.02863812
Iteration 78, loss = 0.02802988
Iteration 79, loss = 0.02738293
Iteration 80, loss = 0.02679060
Iteration 81, loss = 0.02624453
Iteration 82, loss = 0.02567644
Iteration 83, loss = 0.02517098
Iteration 84, loss = 0.02464200
Iteration 85, loss = 0.02421604
Iteration 86, loss = 0.02376270
Iteration 87, loss = 0.02334751
Iteration 88, loss = 0.02286503
Iteration 89, loss = 0.02249136
Iteration 90, loss = 0.02203568
Iteration 91, loss = 0.02162300
Iteration 92, loss = 0.02123576
Iteration 93, loss = 0.02085229
Iteration 94, loss = 0.02050598
Iteration 95, loss = 0.02014409
Iteration 96, loss = 0.01981881
Iteration 97, loss = 0.01951006
Iteration 98, loss = 0.01919720
Iteration 99, loss = 0.01890060
Iteration 100, loss = 0.01859658
Iteration 101, loss = 0.01831719
Iteration 102, loss = 0.01807121
Iteration 103, loss = 0.01777310
Iteration 104, loss = 0.01758092
Iteration 105, loss = 0.01727471
Iteration 106, loss = 0.01717251
Iteration 107, loss = 0.01688290
Iteration 108, loss = 0.01660251
Iteration 109, loss = 0.01635760
Iteration 110, loss = 0.01612417
Iteration 111, loss = 0.01593696
Iteration 112, loss = 0.01571002
Iteration 113, loss = 0.01554263
Iteration 114, loss = 0.01540389
Iteration 115, loss = 0.01519213
Iteration 116, loss = 0.01506870
Iteration 117, loss = 0.01487693
Iteration 118, loss = 0.01464986
Iteration 119, loss = 0.01446711
Iteration 120, loss = 0.01432451
Iteration 121, loss = 0.01414633
Iteration 122, loss = 0.01402706
Iteration 123, loss = 0.01385781
Iteration 124, loss = 0.01368958
Iteration 125, loss = 0.01352180
Iteration 126, loss = 0.01342150
Iteration 127, loss = 0.01324732
Iteration 128, loss = 0.01316247
Iteration 129, loss = 0.01298352
Iteration 130, loss = 0.01285427
Iteration 131, loss = 0.01272892
Iteration 132, loss = 0.01261034
Iteration 133, loss = 0.01248246
Iteration 134, loss = 0.01237640
Iteration 135, loss = 0.01224245
Iteration 136, loss = 0.01214555
Iteration 137, loss = 0.01205197
Iteration 138, loss = 0.01194815
Iteration 139, loss = 0.01186150
Iteration 140, loss = 0.01180984
Iteration 141, loss = 0.01168800
Iteration 142, loss = 0.01156642
Iteration 143, loss = 0.01145925
Iteration 144, loss = 0.01135990
Iteration 145, loss = 0.01128121
Iteration 146, loss = 0.01114452
Iteration 147, loss = 0.01107296
Iteration 148, loss = 0.01098544
Iteration 149, loss = 0.01088306
Iteration 150, loss = 0.01082947
Iteration 151, loss = 0.01072159
Iteration 152, loss = 0.01067577
Iteration 153, loss = 0.01057649
Iteration 154, loss = 0.01051990
Iteration 155, loss = 0.01043122
Iteration 156, loss = 0.01035611
Iteration 157, loss = 0.01029272
Iteration 158, loss = 0.01025259
Iteration 159, loss = 0.01013265
Iteration 160, loss = 0.01006504
Iteration 161, loss = 0.00999983
Iteration 162, loss = 0.00993444
Iteration 163, loss = 0.00986999
Iteration 164, loss = 0.00980963
Iteration 165, loss = 0.00974722
Iteration 166, loss = 0.00969863
Iteration 167, loss = 0.00964229
Iteration 168, loss = 0.00959419
Iteration 169, loss = 0.00956999
Iteration 170, loss = 0.00945095
Iteration 171, loss = 0.00939595
Iteration 172, loss = 0.00931202
Iteration 173, loss = 0.00927561
Iteration 174, loss = 0.00929067
Iteration 175, loss = 0.00917898
Iteration 176, loss = 0.00914760
Iteration 177, loss = 0.00907207
Iteration 178, loss = 0.00902450
Iteration 179, loss = 0.00902035
Iteration 180, loss = 0.00896888
Iteration 181, loss = 0.00891232
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70435961
Iteration 2, loss = 0.68795065
Iteration 3, loss = 0.67455177
Iteration 4, loss = 0.66108418
Iteration 5, loss = 0.64679787
Iteration 6, loss = 0.63106287
Iteration 7, loss = 0.61386164
Iteration 8, loss = 0.59488950
Iteration 9, loss = 0.57424308
Iteration 10, loss = 0.55213143
Iteration 11, loss = 0.52885356
Iteration 12, loss = 0.50459997
Iteration 13, loss = 0.47994819
Iteration 14, loss = 0.45508029
Iteration 15, loss = 0.43035654
Iteration 16, loss = 0.40607082
Iteration 17, loss = 0.38255080
Iteration 18, loss = 0.35972613
Iteration 19, loss = 0.33792151
Iteration 20, loss = 0.31706227
Iteration 21, loss = 0.29731757
Iteration 22, loss = 0.27855507
Iteration 23, loss = 0.26090770
Iteration 24, loss = 0.24432674
Iteration 25, loss = 0.22876801
Iteration 26, loss = 0.21435005
Iteration 27, loss = 0.20083962
Iteration 28, loss = 0.18825747
Iteration 29, loss = 0.17661540
Iteration 30, loss = 0.16584114
Iteration 31, loss = 0.15582890
Iteration 32, loss = 0.14657439
Iteration 33, loss = 0.13806067
Iteration 34, loss = 0.13012348
Iteration 35, loss = 0.12283578
Iteration 36, loss = 0.11610353
Iteration 37, loss = 0.10988551
Iteration 38, loss = 0.10409212
Iteration 39, loss = 0.09876313
Iteration 40, loss = 0.09374533
Iteration 41, loss = 0.08919194
Iteration 42, loss = 0.08487897
Iteration 43, loss = 0.08090798
Iteration 44, loss = 0.07718973
Iteration 45, loss = 0.07372966
Iteration 46, loss = 0.07054825
Iteration 47, loss = 0.06753485
Iteration 48, loss = 0.06474662
Iteration 49, loss = 0.06212218
Iteration 50, loss = 0.05966208
Iteration 51, loss = 0.05737304
Iteration 52, loss = 0.05521893
Iteration 53, loss = 0.05318262
Iteration 54, loss = 0.05128188
Iteration 55, loss = 0.04947752
Iteration 56, loss = 0.04778437
Iteration 57, loss = 0.04619773
Iteration 58, loss = 0.04468408
Iteration 59, loss = 0.04327790
Iteration 60, loss = 0.04190750
Iteration 61, loss = 0.04063289
Iteration 62, loss = 0.03940897
Iteration 63, loss = 0.03825388
Iteration 64, loss = 0.03713141
Iteration 65, loss = 0.03610107
Iteration 66, loss = 0.03508974
Iteration 67, loss = 0.03414963
Iteration 68, loss = 0.03322951
Iteration 69, loss = 0.03236088
Iteration 70, loss = 0.03154845
Iteration 71, loss = 0.03075217
Iteration 72, loss = 0.02999883
Iteration 73, loss = 0.02928609
Iteration 74, loss = 0.02858877
Iteration 75, loss = 0.02794546
Iteration 76, loss = 0.02730331
Iteration 77, loss = 0.02669603
Iteration 78, loss = 0.02612464
Iteration 79, loss = 0.02555592
Iteration 80, loss = 0.02502200
Iteration 81, loss = 0.02451483
Iteration 82, loss = 0.02401702
Iteration 83, loss = 0.02355434
Iteration 84, loss = 0.02308614
Iteration 85, loss = 0.02265191
Iteration 86, loss = 0.02222965
Iteration 87, loss = 0.02181246
Iteration 88, loss = 0.02143544
Iteration 89, loss = 0.02104385
Iteration 90, loss = 0.02067035
Iteration 91, loss = 0.02031235
Iteration 92, loss = 0.01996815
Iteration 93, loss = 0.01964080
Iteration 94, loss = 0.01931309
Iteration 95, loss = 0.01901489
Iteration 96, loss = 0.01871110
Iteration 97, loss = 0.01842775
Iteration 98, loss = 0.01815341
Iteration 99, loss = 0.01787496
Iteration 100, loss = 0.01760586
Iteration 101, loss = 0.01735412
Iteration 102, loss = 0.01710204
Iteration 103, loss = 0.01686284
Iteration 104, loss = 0.01663115
Iteration 105, loss = 0.01641210
Iteration 106, loss = 0.01617962
Iteration 107, loss = 0.01596515
Iteration 108, loss = 0.01575842
Iteration 109, loss = 0.01555300
Iteration 110, loss = 0.01536022
Iteration 111, loss = 0.01516885
Iteration 112, loss = 0.01498835
Iteration 113, loss = 0.01480496
Iteration 114, loss = 0.01462322
Iteration 115, loss = 0.01444976
Iteration 116, loss = 0.01428423
Iteration 117, loss = 0.01412617
Iteration 118, loss = 0.01397157
Iteration 119, loss = 0.01381327
Iteration 120, loss = 0.01366086
Iteration 121, loss = 0.01351259
Iteration 122, loss = 0.01336071
Iteration 123, loss = 0.01321872
Iteration 124, loss = 0.01308252
Iteration 125, loss = 0.01297763
Iteration 126, loss = 0.01282989
Iteration 127, loss = 0.01270304
Iteration 128, loss = 0.01257582
Iteration 129, loss = 0.01244369
Iteration 130, loss = 0.01232724
Iteration 131, loss = 0.01221822
Iteration 132, loss = 0.01209522
Iteration 133, loss = 0.01199757
Iteration 134, loss = 0.01187970
Iteration 135, loss = 0.01178260
Iteration 136, loss = 0.01167211
Iteration 137, loss = 0.01156491
Iteration 138, loss = 0.01148831
Iteration 139, loss = 0.01138137
Iteration 140, loss = 0.01128665
Iteration 141, loss = 0.01119286
Iteration 142, loss = 0.01112137
Iteration 143, loss = 0.01101959
Iteration 144, loss = 0.01094394
Iteration 145, loss = 0.01085704
Iteration 146, loss = 0.01076904
Iteration 147, loss = 0.01068844
Iteration 148, loss = 0.01060717
Iteration 149, loss = 0.01052911
Iteration 150, loss = 0.01045185
Iteration 151, loss = 0.01037614
Iteration 152, loss = 0.01029833
Iteration 153, loss = 0.01022510
Iteration 154, loss = 0.01015264
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72934290
Iteration 2, loss = 0.69814017
Iteration 3, loss = 0.66466428
Iteration 4, loss = 0.62385961
Iteration 5, loss = 0.57774400
Iteration 6, loss = 0.52774137
Iteration 7, loss = 0.47685375
Iteration 8, loss = 0.42652472
Iteration 9, loss = 0.37830430
Iteration 10, loss = 0.33355282
Iteration 11, loss = 0.29297569
Iteration 12, loss = 0.25700750
Iteration 13, loss = 0.22561726
Iteration 14, loss = 0.19849592
Iteration 15, loss = 0.17544189
Iteration 16, loss = 0.15574561
Iteration 17, loss = 0.13900821
Iteration 18, loss = 0.12469801
Iteration 19, loss = 0.11249766
Iteration 20, loss = 0.10199971
Iteration 21, loss = 0.09286974
Iteration 22, loss = 0.08509073
Iteration 23, loss = 0.07838382
Iteration 24, loss = 0.07230789
Iteration 25, loss = 0.06705061
Iteration 26, loss = 0.06237973
Iteration 27, loss = 0.05824505
Iteration 28, loss = 0.05454016
Iteration 29, loss = 0.05127620
Iteration 30, loss = 0.04840300
Iteration 31, loss = 0.04579921
Iteration 32, loss = 0.04344179
Iteration 33, loss = 0.04122225
Iteration 34, loss = 0.03932104
Iteration 35, loss = 0.03760362
Iteration 36, loss = 0.03598496
Iteration 37, loss = 0.03443621
Iteration 38, loss = 0.03305892
Iteration 39, loss = 0.03184714
Iteration 40, loss = 0.03061465
Iteration 41, loss = 0.02966768
Iteration 42, loss = 0.02860228
Iteration 43, loss = 0.02782184
Iteration 44, loss = 0.02686687
Iteration 45, loss = 0.02600500
Iteration 46, loss = 0.02516686
Iteration 47, loss = 0.02442418
Iteration 48, loss = 0.02384625
Iteration 49, loss = 0.02328427
Iteration 50, loss = 0.02264174
Iteration 51, loss = 0.02210859
Iteration 52, loss = 0.02152277
Iteration 53, loss = 0.02104106
Iteration 54, loss = 0.02059480
Iteration 55, loss = 0.02015332
Iteration 56, loss = 0.01979864
Iteration 57, loss = 0.01939438
Iteration 58, loss = 0.01901710
Iteration 59, loss = 0.01872567
Iteration 60, loss = 0.01832358
Iteration 61, loss = 0.01798434
Iteration 62, loss = 0.01771954
Iteration 63, loss = 0.01742356
Iteration 64, loss = 0.01722044
Iteration 65, loss = 0.01686701
Iteration 66, loss = 0.01671383
Iteration 67, loss = 0.01639270
Iteration 68, loss = 0.01618386
Iteration 69, loss = 0.01597993
Iteration 70, loss = 0.01577518
Iteration 71, loss = 0.01560420
Iteration 72, loss = 0.01541157
Iteration 73, loss = 0.01526789
Iteration 74, loss = 0.01499581
Iteration 75, loss = 0.01483704
Iteration 76, loss = 0.01468918
Iteration 77, loss = 0.01452313
Iteration 78, loss = 0.01454565
Iteration 79, loss = 0.01448285
Iteration 80, loss = 0.01435845
Iteration 81, loss = 0.01426033
Iteration 82, loss = 0.01400831
Iteration 83, loss = 0.01394562
Iteration 84, loss = 0.01384124
Iteration 85, loss = 0.01372291
Iteration 86, loss = 0.01362219
Iteration 87, loss = 0.01350676
Iteration 88, loss = 0.01335129
Iteration 89, loss = 0.01316221
Iteration 90, loss = 0.01307454
Iteration 91, loss = 0.01297194
Iteration 92, loss = 0.01291301
Iteration 93, loss = 0.01279929
Iteration 94, loss = 0.01273306
Iteration 95, loss = 0.01261304
Iteration 96, loss = 0.01255010
Iteration 97, loss = 0.01245462
Iteration 98, loss = 0.01232667
Iteration 99, loss = 0.01237693
Iteration 100, loss = 0.01224440
Iteration 101, loss = 0.01217253
Iteration 102, loss = 0.01212267
Iteration 103, loss = 0.01207945
Iteration 104, loss = 0.01197885
Iteration 105, loss = 0.01197980
Iteration 106, loss = 0.01181924
Iteration 107, loss = 0.01202917
Iteration 108, loss = 0.01211627
Iteration 109, loss = 0.01207073
Iteration 110, loss = 0.01193116
Iteration 111, loss = 0.01179557
Iteration 112, loss = 0.01163759
Iteration 113, loss = 0.01173013
Iteration 114, loss = 0.01180635
Iteration 115, loss = 0.01170590
Iteration 116, loss = 0.01156356
Iteration 117, loss = 0.01140848
Iteration 118, loss = 0.01129734
Iteration 119, loss = 0.01127787
Iteration 120, loss = 0.01138396
Iteration 121, loss = 0.01137042
Iteration 122, loss = 0.01129453
Iteration 123, loss = 0.01123605
Iteration 124, loss = 0.01110236
Iteration 125, loss = 0.01113358
Iteration 126, loss = 0.01102940
Iteration 127, loss = 0.01096940
Iteration 128, loss = 0.01101103
Iteration 129, loss = 0.01096887
Iteration 130, loss = 0.01092943
Iteration 131, loss = 0.01086486
Iteration 132, loss = 0.01078884
Iteration 133, loss = 0.01075982
Iteration 134, loss = 0.01073844
Iteration 135, loss = 0.01073822
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67730828
Iteration 2, loss = 0.62217987
Iteration 3, loss = 0.53858145
Iteration 4, loss = 0.44198265
Iteration 5, loss = 0.35089346
Iteration 6, loss = 0.27459969
Iteration 7, loss = 0.21546740
Iteration 8, loss = 0.17168145
Iteration 9, loss = 0.13986075
Iteration 10, loss = 0.11701877
Iteration 11, loss = 0.10024890
Iteration 12, loss = 0.08767135
Iteration 13, loss = 0.07764924
Iteration 14, loss = 0.06989895
Iteration 15, loss = 0.06352329
Iteration 16, loss = 0.05854572
Iteration 17, loss = 0.05450105
Iteration 18, loss = 0.05090399
Iteration 19, loss = 0.04823103
Iteration 20, loss = 0.04588120
Iteration 21, loss = 0.04372135
Iteration 22, loss = 0.04198384
Iteration 23, loss = 0.04068334
Iteration 24, loss = 0.03945216
Iteration 25, loss = 0.03820243
Iteration 26, loss = 0.03698778
Iteration 27, loss = 0.03607998
Iteration 28, loss = 0.03545849
Iteration 29, loss = 0.03461250
Iteration 30, loss = 0.03396297
Iteration 31, loss = 0.03352132
Iteration 32, loss = 0.03298194
Iteration 33, loss = 0.03272358
Iteration 34, loss = 0.03207003
Iteration 35, loss = 0.03183895
Iteration 36, loss = 0.03145278
Iteration 37, loss = 0.03075330
Iteration 38, loss = 0.03024079
Iteration 39, loss = 0.03007610
Iteration 40, loss = 0.02979391
Iteration 41, loss = 0.02934416
Iteration 42, loss = 0.02939954
Iteration 43, loss = 0.02902818
Iteration 44, loss = 0.02851149
Iteration 45, loss = 0.02845157
Iteration 46, loss = 0.02818554
Iteration 47, loss = 0.02805524
Iteration 48, loss = 0.02784762
Iteration 49, loss = 0.02773999
Iteration 50, loss = 0.02768021
Iteration 51, loss = 0.02756978
Iteration 52, loss = 0.02750629
Iteration 53, loss = 0.02717171
Iteration 54, loss = 0.02719659
Iteration 55, loss = 0.02704473
Iteration 56, loss = 0.02718420
Iteration 57, loss = 0.02684179
Iteration 58, loss = 0.02676838
Iteration 59, loss = 0.02647396
Iteration 60, loss = 0.02631721
Iteration 61, loss = 0.02664910
Iteration 62, loss = 0.02741427
Iteration 63, loss = 0.02763185
Iteration 64, loss = 0.02733725
Iteration 65, loss = 0.02658869
Iteration 66, loss = 0.02634081
Iteration 67, loss = 0.02618886
Iteration 68, loss = 0.02598335
Iteration 69, loss = 0.02692347
Iteration 70, loss = 0.02681808
Iteration 71, loss = 0.02634373
Iteration 72, loss = 0.02620619
Iteration 73, loss = 0.02593214
Iteration 74, loss = 0.02623873
Iteration 75, loss = 0.02590008
Iteration 76, loss = 0.02601355
Iteration 77, loss = 0.02598671
Iteration 78, loss = 0.02566534
Iteration 79, loss = 0.02596145
Iteration 80, loss = 0.02636533
Iteration 81, loss = 0.02631320
Iteration 82, loss = 0.02608426
Iteration 83, loss = 0.02625193
Iteration 84, loss = 0.02589810
Iteration 85, loss = 0.02571023
Iteration 86, loss = 0.02533454
Iteration 87, loss = 0.02516703
Iteration 88, loss = 0.02507007
Iteration 89, loss = 0.02509116
Iteration 90, loss = 0.02501273
Iteration 91, loss = 0.02494316
Iteration 92, loss = 0.02550421
Iteration 93, loss = 0.02628539
Iteration 94, loss = 0.02560702
Iteration 95, loss = 0.02513456
Iteration 96, loss = 0.02499781
Iteration 97, loss = 0.02487439
Iteration 98, loss = 0.02500043
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68835639
Iteration 2, loss = 0.65801560
Iteration 3, loss = 0.61797048
Iteration 4, loss = 0.56548976
Iteration 5, loss = 0.50561626
Iteration 6, loss = 0.44468265
Iteration 7, loss = 0.38603365
Iteration 8, loss = 0.33198507
Iteration 9, loss = 0.28418520
Iteration 10, loss = 0.24265158
Iteration 11, loss = 0.20784993
Iteration 12, loss = 0.17884439
Iteration 13, loss = 0.15473651
Iteration 14, loss = 0.13515338
Iteration 15, loss = 0.11886931
Iteration 16, loss = 0.10548562
Iteration 17, loss = 0.09448479
Iteration 18, loss = 0.08510836
Iteration 19, loss = 0.07742576
Iteration 20, loss = 0.07089931
Iteration 21, loss = 0.06517933
Iteration 22, loss = 0.06026084
Iteration 23, loss = 0.05608743
Iteration 24, loss = 0.05247899
Iteration 25, loss = 0.04929276
Iteration 26, loss = 0.04649821
Iteration 27, loss = 0.04394783
Iteration 28, loss = 0.04188359
Iteration 29, loss = 0.03984036
Iteration 30, loss = 0.03794602
Iteration 31, loss = 0.03647228
Iteration 32, loss = 0.03503760
Iteration 33, loss = 0.03365428
Iteration 34, loss = 0.03257953
Iteration 35, loss = 0.03152924
Iteration 36, loss = 0.03055892
Iteration 37, loss = 0.02962293
Iteration 38, loss = 0.02880565
Iteration 39, loss = 0.02813983
Iteration 40, loss = 0.02726343
Iteration 41, loss = 0.02656279
Iteration 42, loss = 0.02602361
Iteration 43, loss = 0.02543348
Iteration 44, loss = 0.02491699
Iteration 45, loss = 0.02441987
Iteration 46, loss = 0.02400401
Iteration 47, loss = 0.02353064
Iteration 48, loss = 0.02306651
Iteration 49, loss = 0.02282570
Iteration 50, loss = 0.02237830
Iteration 51, loss = 0.02237109
Iteration 52, loss = 0.02193519
Iteration 53, loss = 0.02144803
Iteration 54, loss = 0.02125637
Iteration 55, loss = 0.02102971
Iteration 56, loss = 0.02079605
Iteration 57, loss = 0.02053395
Iteration 58, loss = 0.02029753
Iteration 59, loss = 0.02026814
Iteration 60, loss = 0.01985771
Iteration 61, loss = 0.01967993
Iteration 62, loss = 0.01943418
Iteration 63, loss = 0.01941949
Iteration 64, loss = 0.01905884
Iteration 65, loss = 0.01910551
Iteration 66, loss = 0.01887772
Iteration 67, loss = 0.01865918
Iteration 68, loss = 0.01865792
Iteration 69, loss = 0.01841435
Iteration 70, loss = 0.01831285
Iteration 71, loss = 0.01827495
Iteration 72, loss = 0.01826314
Iteration 73, loss = 0.01803127
Iteration 74, loss = 0.01792182
Iteration 75, loss = 0.01783581
Iteration 76, loss = 0.01769011
Iteration 77, loss = 0.01763583
Iteration 78, loss = 0.01758043
Iteration 79, loss = 0.01749662
Iteration 80, loss = 0.01735768
Iteration 81, loss = 0.01733201
Iteration 82, loss = 0.01740356
Iteration 83, loss = 0.01720173
Iteration 84, loss = 0.01707388
Iteration 85, loss = 0.01702630
Iteration 86, loss = 0.01689781
Iteration 87, loss = 0.01678616
Iteration 88, loss = 0.01678541
Iteration 89, loss = 0.01672467
Iteration 90, loss = 0.01663032
Iteration 91, loss = 0.01649353
Iteration 92, loss = 0.01661949
Iteration 93, loss = 0.01651987
Iteration 94, loss = 0.01643276
Iteration 95, loss = 0.01647521
Iteration 96, loss = 0.01643549
Iteration 97, loss = 0.01634097
Iteration 98, loss = 0.01641808
Iteration 99, loss = 0.01629877
Iteration 100, loss = 0.01629276
Iteration 101, loss = 0.01619367
Iteration 102, loss = 0.01621299
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68082105
Iteration 2, loss = 0.64565217
Iteration 3, loss = 0.58135945
Iteration 4, loss = 0.49486194
Iteration 5, loss = 0.40582762
Iteration 6, loss = 0.32543595
Iteration 7, loss = 0.25824517
Iteration 8, loss = 0.20638023
Iteration 9, loss = 0.16695276
Iteration 10, loss = 0.13756897
Iteration 11, loss = 0.11568307
Iteration 12, loss = 0.09933920
Iteration 13, loss = 0.08668844
Iteration 14, loss = 0.07681151
Iteration 15, loss = 0.06902297
Iteration 16, loss = 0.06268443
Iteration 17, loss = 0.05768762
Iteration 18, loss = 0.05329448
Iteration 19, loss = 0.04969512
Iteration 20, loss = 0.04666049
Iteration 21, loss = 0.04405286
Iteration 22, loss = 0.04199124
Iteration 23, loss = 0.04005400
Iteration 24, loss = 0.03839470
Iteration 25, loss = 0.03710062
Iteration 26, loss = 0.03561384
Iteration 27, loss = 0.03432418
Iteration 28, loss = 0.03353321
Iteration 29, loss = 0.03260544
Iteration 30, loss = 0.03172972
Iteration 31, loss = 0.03107389
Iteration 32, loss = 0.03035218
Iteration 33, loss = 0.02966341
Iteration 34, loss = 0.02919480
Iteration 35, loss = 0.02853999
Iteration 36, loss = 0.02820438
Iteration 37, loss = 0.02792618
Iteration 38, loss = 0.02735224
Iteration 39, loss = 0.02715967
Iteration 40, loss = 0.02684614
Iteration 41, loss = 0.02647986
Iteration 42, loss = 0.02619895
Iteration 43, loss = 0.02606218
Iteration 44, loss = 0.02586417
Iteration 45, loss = 0.02562121
Iteration 46, loss = 0.02518913
Iteration 47, loss = 0.02521592
Iteration 48, loss = 0.02497586
Iteration 49, loss = 0.02483120
Iteration 50, loss = 0.02456637
Iteration 51, loss = 0.02452314
Iteration 52, loss = 0.02420447
Iteration 53, loss = 0.02412192
Iteration 54, loss = 0.02411651
Iteration 55, loss = 0.02382714
Iteration 56, loss = 0.02377324
Iteration 57, loss = 0.02367910
Iteration 58, loss = 0.02357237
Iteration 59, loss = 0.02338443
Iteration 60, loss = 0.02351851
Iteration 61, loss = 0.02335254
Iteration 62, loss = 0.02322889
Iteration 63, loss = 0.02308777
Iteration 64, loss = 0.02305757
Iteration 65, loss = 0.02320578
Iteration 66, loss = 0.02295098
Iteration 67, loss = 0.02313959
Iteration 68, loss = 0.02283182
Iteration 69, loss = 0.02302953
Iteration 70, loss = 0.02277250
Iteration 71, loss = 0.02270767
Iteration 72, loss = 0.02257279
Iteration 73, loss = 0.02251820
Iteration 74, loss = 0.02247783
Iteration 75, loss = 0.02258797
Iteration 76, loss = 0.02266611
Iteration 77, loss = 0.02248494
Iteration 78, loss = 0.02216535
Iteration 79, loss = 0.02239546
Iteration 80, loss = 0.02251023
Iteration 81, loss = 0.02219983
Iteration 82, loss = 0.02212598
Iteration 83, loss = 0.02220148
Iteration 84, loss = 0.02215423
Iteration 85, loss = 0.02222933
Iteration 86, loss = 0.02191593
Iteration 87, loss = 0.02207668
Iteration 88, loss = 0.02203133
Iteration 89, loss = 0.02194791
Iteration 90, loss = 0.02186644
Iteration 91, loss = 0.02188214
Iteration 92, loss = 0.02194367
Iteration 93, loss = 0.02185532
Iteration 94, loss = 0.02181702
Iteration 95, loss = 0.02178052
Iteration 96, loss = 0.02196427
Iteration 97, loss = 0.02172052
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67605461
Iteration 2, loss = 0.62001442
Iteration 3, loss = 0.53907039
Iteration 4, loss = 0.44559453
Iteration 5, loss = 0.35791335
Iteration 6, loss = 0.28283262
Iteration 7, loss = 0.22352648
Iteration 8, loss = 0.17832105
Iteration 9, loss = 0.14468784
Iteration 10, loss = 0.11972427
Iteration 11, loss = 0.10102662
Iteration 12, loss = 0.08679004
Iteration 13, loss = 0.07577510
Iteration 14, loss = 0.06731098
Iteration 15, loss = 0.06030719
Iteration 16, loss = 0.05471490
Iteration 17, loss = 0.05023926
Iteration 18, loss = 0.04630592
Iteration 19, loss = 0.04298146
Iteration 20, loss = 0.04027747
Iteration 21, loss = 0.03798424
Iteration 22, loss = 0.03591752
Iteration 23, loss = 0.03410817
Iteration 24, loss = 0.03278934
Iteration 25, loss = 0.03127659
Iteration 26, loss = 0.03012962
Iteration 27, loss = 0.02904335
Iteration 28, loss = 0.02796764
Iteration 29, loss = 0.02721638
Iteration 30, loss = 0.02650597
Iteration 31, loss = 0.02584834
Iteration 32, loss = 0.02534277
Iteration 33, loss = 0.02453933
Iteration 34, loss = 0.02420755
Iteration 35, loss = 0.02358252
Iteration 36, loss = 0.02330178
Iteration 37, loss = 0.02296353
Iteration 38, loss = 0.02285289
Iteration 39, loss = 0.02210284
Iteration 40, loss = 0.02184580
Iteration 41, loss = 0.02158699
Iteration 42, loss = 0.02140682
Iteration 43, loss = 0.02098074
Iteration 44, loss = 0.02092709
Iteration 45, loss = 0.02045768
Iteration 46, loss = 0.02044294
Iteration 47, loss = 0.02040413
Iteration 48, loss = 0.02003489
Iteration 49, loss = 0.01969767
Iteration 50, loss = 0.01978964
Iteration 51, loss = 0.01964778
Iteration 52, loss = 0.01942343
Iteration 53, loss = 0.01926515
Iteration 54, loss = 0.01928169
Iteration 55, loss = 0.01895336
Iteration 56, loss = 0.01898522
Iteration 57, loss = 0.01890027
Iteration 58, loss = 0.01898176
Iteration 59, loss = 0.01870664
Iteration 60, loss = 0.01866003
Iteration 61, loss = 0.01860704
Iteration 62, loss = 0.01844702
Iteration 63, loss = 0.01834925
Iteration 64, loss = 0.01813022
Iteration 65, loss = 0.01809377
Iteration 66, loss = 0.01804015
Iteration 67, loss = 0.01815344
Iteration 68, loss = 0.01804047
Iteration 69, loss = 0.01796450
Iteration 70, loss = 0.01796950
Iteration 71, loss = 0.01786662
Iteration 72, loss = 0.01769956
Iteration 73, loss = 0.01796454
Iteration 74, loss = 0.01786780
Iteration 75, loss = 0.01771571
Iteration 76, loss = 0.01765908
Iteration 77, loss = 0.01773403
Iteration 78, loss = 0.01770611
Iteration 79, loss = 0.01745848
Iteration 80, loss = 0.01749604
Iteration 81, loss = 0.01736428
Iteration 82, loss = 0.01763340
Iteration 83, loss = 0.01748688
Iteration 84, loss = 0.01762158
Iteration 85, loss = 0.01752300
Iteration 86, loss = 0.01742514
Iteration 87, loss = 0.01714827
Iteration 88, loss = 0.01720462
Iteration 89, loss = 0.01718845
Iteration 90, loss = 0.01708345
Iteration 91, loss = 0.01705909
Iteration 92, loss = 0.01706604
Iteration 93, loss = 0.01701225
Iteration 94, loss = 0.01728231
Iteration 95, loss = 0.01696884
Iteration 96, loss = 0.01711573
Iteration 97, loss = 0.01707904
Iteration 98, loss = 0.01707154
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67357894
Iteration 2, loss = 0.59567092
Iteration 3, loss = 0.48638507
Iteration 4, loss = 0.37556227
Iteration 5, loss = 0.28192356
Iteration 6, loss = 0.21177155
Iteration 7, loss = 0.16282510
Iteration 8, loss = 0.12911732
Iteration 9, loss = 0.10599070
Iteration 10, loss = 0.08970477
Iteration 11, loss = 0.07767355
Iteration 12, loss = 0.06863711
Iteration 13, loss = 0.06155037
Iteration 14, loss = 0.05630676
Iteration 15, loss = 0.05195748
Iteration 16, loss = 0.04869525
Iteration 17, loss = 0.04595342
Iteration 18, loss = 0.04342333
Iteration 19, loss = 0.04188495
Iteration 20, loss = 0.04011013
Iteration 21, loss = 0.03879237
Iteration 22, loss = 0.03724736
Iteration 23, loss = 0.03630495
Iteration 24, loss = 0.03535958
Iteration 25, loss = 0.03456081
Iteration 26, loss = 0.03431580
Iteration 27, loss = 0.03354838
Iteration 28, loss = 0.03291508
Iteration 29, loss = 0.03210765
Iteration 30, loss = 0.03198039
Iteration 31, loss = 0.03141971
Iteration 32, loss = 0.03088510
Iteration 33, loss = 0.03056253
Iteration 34, loss = 0.03032292
Iteration 35, loss = 0.02999026
Iteration 36, loss = 0.02980295
Iteration 37, loss = 0.02978964
Iteration 38, loss = 0.02959659
Iteration 39, loss = 0.02925493
Iteration 40, loss = 0.02876988
Iteration 41, loss = 0.02906910
Iteration 42, loss = 0.02887446
Iteration 43, loss = 0.02877878
Iteration 44, loss = 0.02834873
Iteration 45, loss = 0.02824688
Iteration 46, loss = 0.02803783
Iteration 47, loss = 0.02837163
Iteration 48, loss = 0.02815531
Iteration 49, loss = 0.02780884
Iteration 50, loss = 0.02801989
Iteration 51, loss = 0.02776989
Iteration 52, loss = 0.02761230
Iteration 53, loss = 0.02785919
Iteration 54, loss = 0.02732945
Iteration 55, loss = 0.02735518
Iteration 56, loss = 0.02756288
Iteration 57, loss = 0.02748601
Iteration 58, loss = 0.02733139
Iteration 59, loss = 0.02740945
Iteration 60, loss = 0.02722472
Iteration 61, loss = 0.02744070
Iteration 62, loss = 0.02759578
Iteration 63, loss = 0.02704149
Iteration 64, loss = 0.02693119
Iteration 65, loss = 0.02687273
Iteration 66, loss = 0.02686433
Iteration 67, loss = 0.02651787
Iteration 68, loss = 0.02711962
Iteration 69, loss = 0.02695497
Iteration 70, loss = 0.02711520
Iteration 71, loss = 0.02677303
Iteration 72, loss = 0.02692713
Iteration 73, loss = 0.02675237
Iteration 74, loss = 0.02687503
Iteration 75, loss = 0.02692427
Iteration 76, loss = 0.02662596
Iteration 77, loss = 0.02668532
Iteration 78, loss = 0.02676109
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67846055
Iteration 2, loss = 0.61643952
Iteration 3, loss = 0.52005616
Iteration 4, loss = 0.41241814
Iteration 5, loss = 0.31672187
Iteration 6, loss = 0.24107433
Iteration 7, loss = 0.18640094
Iteration 8, loss = 0.14770873
Iteration 9, loss = 0.12082793
Iteration 10, loss = 0.10135953
Iteration 11, loss = 0.08727199
Iteration 12, loss = 0.07667822
Iteration 13, loss = 0.06847782
Iteration 14, loss = 0.06224043
Iteration 15, loss = 0.05718257
Iteration 16, loss = 0.05296049
Iteration 17, loss = 0.04952323
Iteration 18, loss = 0.04701671
Iteration 19, loss = 0.04480594
Iteration 20, loss = 0.04255233
Iteration 21, loss = 0.04082931
Iteration 22, loss = 0.03974152
Iteration 23, loss = 0.03822831
Iteration 24, loss = 0.03694256
Iteration 25, loss = 0.03630598
Iteration 26, loss = 0.03512904
Iteration 27, loss = 0.03448739
Iteration 28, loss = 0.03381408
Iteration 29, loss = 0.03332053
Iteration 30, loss = 0.03263855
Iteration 31, loss = 0.03206841
Iteration 32, loss = 0.03188499
Iteration 33, loss = 0.03165556
Iteration 34, loss = 0.03123109
Iteration 35, loss = 0.03111064
Iteration 36, loss = 0.03050443
Iteration 37, loss = 0.03015401
Iteration 38, loss = 0.02979061
Iteration 39, loss = 0.02991903
Iteration 40, loss = 0.02929954
Iteration 41, loss = 0.02910948
Iteration 42, loss = 0.02889946
Iteration 43, loss = 0.02900285
Iteration 44, loss = 0.02879682
Iteration 45, loss = 0.02862568
Iteration 46, loss = 0.02864155
Iteration 47, loss = 0.02854326
Iteration 48, loss = 0.02826711
Iteration 49, loss = 0.02812655
Iteration 50, loss = 0.02805749
Iteration 51, loss = 0.02781535
Iteration 52, loss = 0.02778835
Iteration 53, loss = 0.02772343
Iteration 54, loss = 0.02786110
Iteration 55, loss = 0.02759486
Iteration 56, loss = 0.02739248
Iteration 57, loss = 0.02754365
Iteration 58, loss = 0.02716357
Iteration 59, loss = 0.02724499
Iteration 60, loss = 0.02714687
Iteration 61, loss = 0.02704913
Iteration 62, loss = 0.02681558
Iteration 63, loss = 0.02699249
Iteration 64, loss = 0.02680707
Iteration 65, loss = 0.02685768
Iteration 66, loss = 0.02687042
Iteration 67, loss = 0.02673884
Iteration 68, loss = 0.02671487
Iteration 69, loss = 0.02679579
Iteration 70, loss = 0.02679982
Iteration 71, loss = 0.02693962
Iteration 72, loss = 0.02667547
Iteration 73, loss = 0.02670348
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67881753
Iteration 2, loss = 0.64210916
Iteration 3, loss = 0.58887021
Iteration 4, loss = 0.51982732
Iteration 5, loss = 0.44655012
Iteration 6, loss = 0.37754249
Iteration 7, loss = 0.31559639
Iteration 8, loss = 0.26291012
Iteration 9, loss = 0.21973086
Iteration 10, loss = 0.18491912
Iteration 11, loss = 0.15701272
Iteration 12, loss = 0.13492375
Iteration 13, loss = 0.11734573
Iteration 14, loss = 0.10315321
Iteration 15, loss = 0.09181680
Iteration 16, loss = 0.08242812
Iteration 17, loss = 0.07467407
Iteration 18, loss = 0.06828530
Iteration 19, loss = 0.06280799
Iteration 20, loss = 0.05810182
Iteration 21, loss = 0.05411534
Iteration 22, loss = 0.05079526
Iteration 23, loss = 0.04779531
Iteration 24, loss = 0.04517517
Iteration 25, loss = 0.04286235
Iteration 26, loss = 0.04100633
Iteration 27, loss = 0.03922063
Iteration 28, loss = 0.03757289
Iteration 29, loss = 0.03619950
Iteration 30, loss = 0.03481090
Iteration 31, loss = 0.03356144
Iteration 32, loss = 0.03276605
Iteration 33, loss = 0.03158733
Iteration 34, loss = 0.03080233
Iteration 35, loss = 0.03001346
Iteration 36, loss = 0.02926957
Iteration 37, loss = 0.02862396
Iteration 38, loss = 0.02798335
Iteration 39, loss = 0.02749747
Iteration 40, loss = 0.02695089
Iteration 41, loss = 0.02648950
Iteration 42, loss = 0.02586059
Iteration 43, loss = 0.02546875
Iteration 44, loss = 0.02519165
Iteration 45, loss = 0.02480996
Iteration 46, loss = 0.02441330
Iteration 47, loss = 0.02418296
Iteration 48, loss = 0.02383512
Iteration 49, loss = 0.02354118
Iteration 50, loss = 0.02327385
Iteration 51, loss = 0.02316168
Iteration 52, loss = 0.02277162
Iteration 53, loss = 0.02255401
Iteration 54, loss = 0.02243746
Iteration 55, loss = 0.02206121
Iteration 56, loss = 0.02209241
Iteration 57, loss = 0.02172609
Iteration 58, loss = 0.02166364
Iteration 59, loss = 0.02161978
Iteration 60, loss = 0.02131408
Iteration 61, loss = 0.02120487
Iteration 62, loss = 0.02100404
Iteration 63, loss = 0.02088096
Iteration 64, loss = 0.02087771
Iteration 65, loss = 0.02054348
Iteration 66, loss = 0.02053564
Iteration 67, loss = 0.02055167
Iteration 68, loss = 0.02039011
Iteration 69, loss = 0.02016611
Iteration 70, loss = 0.02016277
Iteration 71, loss = 0.02007790
Iteration 72, loss = 0.02007271
Iteration 73, loss = 0.02000956
Iteration 74, loss = 0.01972225
Iteration 75, loss = 0.01977456
Iteration 76, loss = 0.01973997
Iteration 77, loss = 0.01958056
Iteration 78, loss = 0.01971008
Iteration 79, loss = 0.01976417
Iteration 80, loss = 0.01940438
Iteration 81, loss = 0.01933429
Iteration 82, loss = 0.01948062
Iteration 83, loss = 0.01923438
Iteration 84, loss = 0.01920049
Iteration 85, loss = 0.01911520
Iteration 86, loss = 0.01931886
Iteration 87, loss = 0.01931496
Iteration 88, loss = 0.01897849
Iteration 89, loss = 0.01907763
Iteration 90, loss = 0.01890292
Iteration 91, loss = 0.01892659
Iteration 92, loss = 0.01886363
Iteration 93, loss = 0.01877776
Iteration 94, loss = 0.01876121
Iteration 95, loss = 0.01887292
Iteration 96, loss = 0.01879832
Iteration 97, loss = 0.01866653
Iteration 98, loss = 0.01881100
Iteration 99, loss = 0.01858777
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68173350
Iteration 2, loss = 0.61187315
Iteration 3, loss = 0.51384140
Iteration 4, loss = 0.40641187
Iteration 5, loss = 0.31205960
Iteration 6, loss = 0.23813835
Iteration 7, loss = 0.18410461
Iteration 8, loss = 0.14635906
Iteration 9, loss = 0.11974357
Iteration 10, loss = 0.10081317
Iteration 11, loss = 0.08725042
Iteration 12, loss = 0.07668952
Iteration 13, loss = 0.06856441
Iteration 14, loss = 0.06246487
Iteration 15, loss = 0.05750762
Iteration 16, loss = 0.05324138
Iteration 17, loss = 0.05011972
Iteration 18, loss = 0.04761569
Iteration 19, loss = 0.04505091
Iteration 20, loss = 0.04299036
Iteration 21, loss = 0.04126232
Iteration 22, loss = 0.03989343
Iteration 23, loss = 0.03878758
Iteration 24, loss = 0.03779405
Iteration 25, loss = 0.03677903
Iteration 26, loss = 0.03557768
Iteration 27, loss = 0.03505848
Iteration 28, loss = 0.03421414
Iteration 29, loss = 0.03385932
Iteration 30, loss = 0.03324336
Iteration 31, loss = 0.03278240
Iteration 32, loss = 0.03233897
Iteration 33, loss = 0.03192717
Iteration 34, loss = 0.03132546
Iteration 35, loss = 0.03099007
Iteration 36, loss = 0.03082635
Iteration 37, loss = 0.03061546
Iteration 38, loss = 0.03038134
Iteration 39, loss = 0.03007908
Iteration 40, loss = 0.02970748
Iteration 41, loss = 0.02960801
Iteration 42, loss = 0.02945082
Iteration 43, loss = 0.02950371
Iteration 44, loss = 0.02933444
Iteration 45, loss = 0.02918912
Iteration 46, loss = 0.02867078
Iteration 47, loss = 0.02888552
Iteration 48, loss = 0.02878580
Iteration 49, loss = 0.02854945
Iteration 50, loss = 0.02859093
Iteration 51, loss = 0.02818900
Iteration 52, loss = 0.02831817
Iteration 53, loss = 0.02828262
Iteration 54, loss = 0.02807910
Iteration 55, loss = 0.02799086
Iteration 56, loss = 0.02773243
Iteration 57, loss = 0.02785627
Iteration 58, loss = 0.02756794
Iteration 59, loss = 0.02760195
Iteration 60, loss = 0.02770204
Iteration 61, loss = 0.02758753
Iteration 62, loss = 0.02762021
Iteration 63, loss = 0.02753201
Iteration 64, loss = 0.02741800
Iteration 65, loss = 0.02752616
Iteration 66, loss = 0.02741351
Iteration 67, loss = 0.02711440
Iteration 68, loss = 0.02708900
Iteration 69, loss = 0.02730818
Iteration 70, loss = 0.02698897
Iteration 71, loss = 0.02727791
Iteration 72, loss = 0.02699815
Iteration 73, loss = 0.02738323
Iteration 74, loss = 0.02747409
Iteration 75, loss = 0.02685677
Iteration 76, loss = 0.02730246
Iteration 77, loss = 0.02672872
Iteration 78, loss = 0.02697290
Iteration 79, loss = 0.02707080
Iteration 80, loss = 0.02675892
Iteration 81, loss = 0.02705633
Iteration 82, loss = 0.02679585
Iteration 83, loss = 0.02686049
Iteration 84, loss = 0.02681849
Iteration 85, loss = 0.02705231
Iteration 86, loss = 0.02653112
Iteration 87, loss = 0.02650909
Iteration 88, loss = 0.02682710
Iteration 89, loss = 0.02696051
Iteration 90, loss = 0.02652539
Iteration 91, loss = 0.02657457
Iteration 92, loss = 0.02672832
Iteration 93, loss = 0.02647635
Iteration 94, loss = 0.02657409
Iteration 95, loss = 0.02639249
Iteration 96, loss = 0.02674925
Iteration 97, loss = 0.02658757
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69616595
Iteration 2, loss = 0.68049786
Iteration 3, loss = 0.66687809
Iteration 4, loss = 0.65303892
Iteration 5, loss = 0.63907825
Iteration 6, loss = 0.62388504
Iteration 7, loss = 0.60765610
Iteration 8, loss = 0.59003215
Iteration 9, loss = 0.57125162
Iteration 10, loss = 0.55112601
Iteration 11, loss = 0.52962047
Iteration 12, loss = 0.50698266
Iteration 13, loss = 0.48342890
Iteration 14, loss = 0.45970794
Iteration 15, loss = 0.43566192
Iteration 16, loss = 0.41164915
Iteration 17, loss = 0.38815003
Iteration 18, loss = 0.36515099
Iteration 19, loss = 0.34307294
Iteration 20, loss = 0.32191518
Iteration 21, loss = 0.30186765
Iteration 22, loss = 0.28279489
Iteration 23, loss = 0.26484839
Iteration 24, loss = 0.24804959
Iteration 25, loss = 0.23227836
Iteration 26, loss = 0.21754813
Iteration 27, loss = 0.20381174
Iteration 28, loss = 0.19103082
Iteration 29, loss = 0.17916475
Iteration 30, loss = 0.16819903
Iteration 31, loss = 0.15807899
Iteration 32, loss = 0.14862743
Iteration 33, loss = 0.13987550
Iteration 34, loss = 0.13191932
Iteration 35, loss = 0.12450028
Iteration 36, loss = 0.11769735
Iteration 37, loss = 0.11135320
Iteration 38, loss = 0.10550190
Iteration 39, loss = 0.10014327
Iteration 40, loss = 0.09518456
Iteration 41, loss = 0.09052170
Iteration 42, loss = 0.08622072
Iteration 43, loss = 0.08222548
Iteration 44, loss = 0.07855563
Iteration 45, loss = 0.07512951
Iteration 46, loss = 0.07189834
Iteration 47, loss = 0.06893368
Iteration 48, loss = 0.06614904
Iteration 49, loss = 0.06353553
Iteration 50, loss = 0.06103228
Iteration 51, loss = 0.05873002
Iteration 52, loss = 0.05661104
Iteration 53, loss = 0.05456955
Iteration 54, loss = 0.05262228
Iteration 55, loss = 0.05084163
Iteration 56, loss = 0.04915299
Iteration 57, loss = 0.04756841
Iteration 58, loss = 0.04605065
Iteration 59, loss = 0.04463803
Iteration 60, loss = 0.04328809
Iteration 61, loss = 0.04201134
Iteration 62, loss = 0.04082351
Iteration 63, loss = 0.03968227
Iteration 64, loss = 0.03861145
Iteration 65, loss = 0.03756062
Iteration 66, loss = 0.03657466
Iteration 67, loss = 0.03566443
Iteration 68, loss = 0.03482154
Iteration 69, loss = 0.03396395
Iteration 70, loss = 0.03314215
Iteration 71, loss = 0.03239500
Iteration 72, loss = 0.03166515
Iteration 73, loss = 0.03092692
Iteration 74, loss = 0.03025361
Iteration 75, loss = 0.02961411
Iteration 76, loss = 0.02899563
Iteration 77, loss = 0.02840839
Iteration 78, loss = 0.02786743
Iteration 79, loss = 0.02729567
Iteration 80, loss = 0.02676820
Iteration 81, loss = 0.02626821
Iteration 82, loss = 0.02579363
Iteration 83, loss = 0.02533147
Iteration 84, loss = 0.02490529
Iteration 85, loss = 0.02444969
Iteration 86, loss = 0.02405140
Iteration 87, loss = 0.02365464
Iteration 88, loss = 0.02327132
Iteration 89, loss = 0.02289540
Iteration 90, loss = 0.02253664
Iteration 91, loss = 0.02219334
Iteration 92, loss = 0.02187306
Iteration 93, loss = 0.02154327
Iteration 94, loss = 0.02121947
Iteration 95, loss = 0.02091923
Iteration 96, loss = 0.02064018
Iteration 97, loss = 0.02035734
Iteration 98, loss = 0.02007805
Iteration 99, loss = 0.01983247
Iteration 100, loss = 0.01956364
Iteration 101, loss = 0.01932187
Iteration 102, loss = 0.01909678
Iteration 103, loss = 0.01889353
Iteration 104, loss = 0.01865707
Iteration 105, loss = 0.01844844
Iteration 106, loss = 0.01824149
Iteration 107, loss = 0.01802856
Iteration 108, loss = 0.01786165
Iteration 109, loss = 0.01761640
Iteration 110, loss = 0.01746209
Iteration 111, loss = 0.01725972
Iteration 112, loss = 0.01708495
Iteration 113, loss = 0.01693566
Iteration 114, loss = 0.01678759
Iteration 115, loss = 0.01662879
Iteration 116, loss = 0.01645674
Iteration 117, loss = 0.01629844
Iteration 118, loss = 0.01618278
Iteration 119, loss = 0.01599731
Iteration 120, loss = 0.01587298
Iteration 121, loss = 0.01572068
Iteration 122, loss = 0.01560482
Iteration 123, loss = 0.01551262
Iteration 124, loss = 0.01536372
Iteration 125, loss = 0.01525249
Iteration 126, loss = 0.01513997
Iteration 127, loss = 0.01500651
Iteration 128, loss = 0.01487825
Iteration 129, loss = 0.01477486
Iteration 130, loss = 0.01467650
Iteration 131, loss = 0.01452634
Iteration 132, loss = 0.01443040
Iteration 133, loss = 0.01432326
Iteration 134, loss = 0.01422328
Iteration 135, loss = 0.01411813
Iteration 136, loss = 0.01404451
Iteration 137, loss = 0.01391470
Iteration 138, loss = 0.01381475
Iteration 139, loss = 0.01370809
Iteration 140, loss = 0.01363961
Iteration 141, loss = 0.01355867
Iteration 142, loss = 0.01347451
Iteration 143, loss = 0.01335963
Iteration 144, loss = 0.01327728
Iteration 145, loss = 0.01322949
Iteration 146, loss = 0.01311436
Iteration 147, loss = 0.01304973
Iteration 148, loss = 0.01296868
Iteration 149, loss = 0.01289351
Iteration 150, loss = 0.01285386
Iteration 151, loss = 0.01275286
Iteration 152, loss = 0.01269213
Iteration 153, loss = 0.01269234
Iteration 154, loss = 0.01258914
Iteration 155, loss = 0.01254543
Iteration 156, loss = 0.01249371
Iteration 157, loss = 0.01243337
Iteration 158, loss = 0.01236757
Iteration 159, loss = 0.01230320
Iteration 160, loss = 0.01224051
Iteration 161, loss = 0.01216456
Iteration 162, loss = 0.01210330
Iteration 163, loss = 0.01209358
Iteration 164, loss = 0.01206503
Iteration 165, loss = 0.01191315
Iteration 166, loss = 0.01187649
Iteration 167, loss = 0.01180420
Iteration 168, loss = 0.01179066
Iteration 169, loss = 0.01175216
Iteration 170, loss = 0.01168356
Iteration 171, loss = 0.01161742
Iteration 172, loss = 0.01158839
Iteration 173, loss = 0.01152855
Iteration 174, loss = 0.01148990
Iteration 175, loss = 0.01142889
Iteration 176, loss = 0.01139204
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70845789
Iteration 2, loss = 0.62314742
Iteration 3, loss = 0.51191625
Iteration 4, loss = 0.39325294
Iteration 5, loss = 0.29422034
Iteration 6, loss = 0.21895490
Iteration 7, loss = 0.16652661
Iteration 8, loss = 0.13103941
Iteration 9, loss = 0.10630880
Iteration 10, loss = 0.08883991
Iteration 11, loss = 0.07624631
Iteration 12, loss = 0.06663846
Iteration 13, loss = 0.05937903
Iteration 14, loss = 0.05376056
Iteration 15, loss = 0.04931387
Iteration 16, loss = 0.04534511
Iteration 17, loss = 0.04247010
Iteration 18, loss = 0.03995595
Iteration 19, loss = 0.03780159
Iteration 20, loss = 0.03596800
Iteration 21, loss = 0.03438244
Iteration 22, loss = 0.03300400
Iteration 23, loss = 0.03184997
Iteration 24, loss = 0.03111630
Iteration 25, loss = 0.03024289
Iteration 26, loss = 0.02917888
Iteration 27, loss = 0.02840487
Iteration 28, loss = 0.02777821
Iteration 29, loss = 0.02723550
Iteration 30, loss = 0.02687825
Iteration 31, loss = 0.02631389
Iteration 32, loss = 0.02604536
Iteration 33, loss = 0.02560523
Iteration 34, loss = 0.02541228
Iteration 35, loss = 0.02487914
Iteration 36, loss = 0.02485184
Iteration 37, loss = 0.02452936
Iteration 38, loss = 0.02428618
Iteration 39, loss = 0.02403326
Iteration 40, loss = 0.02388658
Iteration 41, loss = 0.02362371
Iteration 42, loss = 0.02376580
Iteration 43, loss = 0.02332747
Iteration 44, loss = 0.02342225
Iteration 45, loss = 0.02301247
Iteration 46, loss = 0.02294267
Iteration 47, loss = 0.02270401
Iteration 48, loss = 0.02265905
Iteration 49, loss = 0.02263118
Iteration 50, loss = 0.02223060
Iteration 51, loss = 0.02266223
Iteration 52, loss = 0.02232853
Iteration 53, loss = 0.02207727
Iteration 54, loss = 0.02256231
Iteration 55, loss = 0.02186539
Iteration 56, loss = 0.02199488
Iteration 57, loss = 0.02178004
Iteration 58, loss = 0.02172361
Iteration 59, loss = 0.02192583
Iteration 60, loss = 0.02200576
Iteration 61, loss = 0.02200279
Iteration 62, loss = 0.02147073
Iteration 63, loss = 0.02178888
Iteration 64, loss = 0.02163846
Iteration 65, loss = 0.02170517
Iteration 66, loss = 0.02144520
Iteration 67, loss = 0.02162907
Iteration 68, loss = 0.02149568
Iteration 69, loss = 0.02131563
Iteration 70, loss = 0.02142592
Iteration 71, loss = 0.02120282
Iteration 72, loss = 0.02117341
Iteration 73, loss = 0.02124595
Iteration 74, loss = 0.02127862
Iteration 75, loss = 0.02113085
Iteration 76, loss = 0.02148707
Iteration 77, loss = 0.02133623
Iteration 78, loss = 0.02129967
Iteration 79, loss = 0.02096586
Iteration 80, loss = 0.02117086
Iteration 81, loss = 0.02080003
Iteration 82, loss = 0.02098062
Iteration 83, loss = 0.02109211
Iteration 84, loss = 0.02106506
Iteration 85, loss = 0.02119213
Iteration 86, loss = 0.02097070
Iteration 87, loss = 0.02070253
Iteration 88, loss = 0.02079564
Iteration 89, loss = 0.02125728
Iteration 90, loss = 0.02098426
Iteration 91, loss = 0.02116057
Iteration 92, loss = 0.02090810
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68769247
Iteration 2, loss = 0.66916824
Iteration 3, loss = 0.65066931
Iteration 4, loss = 0.62916811
Iteration 5, loss = 0.60364899
Iteration 6, loss = 0.57378853
Iteration 7, loss = 0.54063799
Iteration 8, loss = 0.50507288
Iteration 9, loss = 0.46845624
Iteration 10, loss = 0.43164702
Iteration 11, loss = 0.39600272
Iteration 12, loss = 0.36175090
Iteration 13, loss = 0.32969369
Iteration 14, loss = 0.29974011
Iteration 15, loss = 0.27198382
Iteration 16, loss = 0.24657440
Iteration 17, loss = 0.22373735
Iteration 18, loss = 0.20299876
Iteration 19, loss = 0.18439544
Iteration 20, loss = 0.16782650
Iteration 21, loss = 0.15309342
Iteration 22, loss = 0.13999204
Iteration 23, loss = 0.12823193
Iteration 24, loss = 0.11782711
Iteration 25, loss = 0.10861128
Iteration 26, loss = 0.10038950
Iteration 27, loss = 0.09306340
Iteration 28, loss = 0.08645823
Iteration 29, loss = 0.08064386
Iteration 30, loss = 0.07530454
Iteration 31, loss = 0.07054668
Iteration 32, loss = 0.06623218
Iteration 33, loss = 0.06238054
Iteration 34, loss = 0.05887666
Iteration 35, loss = 0.05561705
Iteration 36, loss = 0.05266332
Iteration 37, loss = 0.04995402
Iteration 38, loss = 0.04754042
Iteration 39, loss = 0.04532336
Iteration 40, loss = 0.04320332
Iteration 41, loss = 0.04125693
Iteration 42, loss = 0.03949555
Iteration 43, loss = 0.03781748
Iteration 44, loss = 0.03632163
Iteration 45, loss = 0.03491386
Iteration 46, loss = 0.03357984
Iteration 47, loss = 0.03230256
Iteration 48, loss = 0.03120709
Iteration 49, loss = 0.03008508
Iteration 50, loss = 0.02910013
Iteration 51, loss = 0.02809871
Iteration 52, loss = 0.02722044
Iteration 53, loss = 0.02643094
Iteration 54, loss = 0.02562535
Iteration 55, loss = 0.02487783
Iteration 56, loss = 0.02415407
Iteration 57, loss = 0.02350581
Iteration 58, loss = 0.02286746
Iteration 59, loss = 0.02226734
Iteration 60, loss = 0.02169341
Iteration 61, loss = 0.02118002
Iteration 62, loss = 0.02062622
Iteration 63, loss = 0.02017469
Iteration 64, loss = 0.01969299
Iteration 65, loss = 0.01930127
Iteration 66, loss = 0.01886197
Iteration 67, loss = 0.01843945
Iteration 68, loss = 0.01810138
Iteration 69, loss = 0.01773067
Iteration 70, loss = 0.01739139
Iteration 71, loss = 0.01706897
Iteration 72, loss = 0.01672930
Iteration 73, loss = 0.01639285
Iteration 74, loss = 0.01615315
Iteration 75, loss = 0.01582795
Iteration 76, loss = 0.01558471
Iteration 77, loss = 0.01532680
Iteration 78, loss = 0.01508659
Iteration 79, loss = 0.01480540
Iteration 80, loss = 0.01460970
Iteration 81, loss = 0.01436373
Iteration 82, loss = 0.01417471
Iteration 83, loss = 0.01397291
Iteration 84, loss = 0.01376723
Iteration 85, loss = 0.01355913
Iteration 86, loss = 0.01340495
Iteration 87, loss = 0.01324976
Iteration 88, loss = 0.01309191
Iteration 89, loss = 0.01286507
Iteration 90, loss = 0.01271341
Iteration 91, loss = 0.01261680
Iteration 92, loss = 0.01245745
Iteration 93, loss = 0.01228277
Iteration 94, loss = 0.01216494
Iteration 95, loss = 0.01203851
Iteration 96, loss = 0.01188195
Iteration 97, loss = 0.01179219
Iteration 98, loss = 0.01165992
Iteration 99, loss = 0.01148997
Iteration 100, loss = 0.01138833
Iteration 101, loss = 0.01129435
Iteration 102, loss = 0.01118738
Iteration 103, loss = 0.01107540
Iteration 104, loss = 0.01098495
Iteration 105, loss = 0.01089338
Iteration 106, loss = 0.01078166
Iteration 107, loss = 0.01073695
Iteration 108, loss = 0.01067782
Iteration 109, loss = 0.01054370
Iteration 110, loss = 0.01045030
Iteration 111, loss = 0.01040097
Iteration 112, loss = 0.01029749
Iteration 113, loss = 0.01020916
Iteration 114, loss = 0.01012262
Iteration 115, loss = 0.01006809
Iteration 116, loss = 0.00996604
Iteration 117, loss = 0.00995665
Iteration 118, loss = 0.00987148
Iteration 119, loss = 0.00977786
Iteration 120, loss = 0.00972307
Iteration 121, loss = 0.00963876
Iteration 122, loss = 0.00957215
Iteration 123, loss = 0.00951851
Iteration 124, loss = 0.00947433
Iteration 125, loss = 0.00941174
Iteration 126, loss = 0.00935599
Iteration 127, loss = 0.00927447
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71713662
Iteration 2, loss = 0.69551517
Iteration 3, loss = 0.67327144
Iteration 4, loss = 0.64852414
Iteration 5, loss = 0.62042517
Iteration 6, loss = 0.58956093
Iteration 7, loss = 0.55596713
Iteration 8, loss = 0.52065315
Iteration 9, loss = 0.48468456
Iteration 10, loss = 0.44830121
Iteration 11, loss = 0.41210571
Iteration 12, loss = 0.37791281
Iteration 13, loss = 0.34532181
Iteration 14, loss = 0.31503962
Iteration 15, loss = 0.28710286
Iteration 16, loss = 0.26131567
Iteration 17, loss = 0.23781149
Iteration 18, loss = 0.21660658
Iteration 19, loss = 0.19738735
Iteration 20, loss = 0.18002962
Iteration 21, loss = 0.16466821
Iteration 22, loss = 0.15071882
Iteration 23, loss = 0.13839396
Iteration 24, loss = 0.12736050
Iteration 25, loss = 0.11750741
Iteration 26, loss = 0.10868936
Iteration 27, loss = 0.10079198
Iteration 28, loss = 0.09380025
Iteration 29, loss = 0.08740920
Iteration 30, loss = 0.08171144
Iteration 31, loss = 0.07663941
Iteration 32, loss = 0.07204958
Iteration 33, loss = 0.06781127
Iteration 34, loss = 0.06397238
Iteration 35, loss = 0.06053274
Iteration 36, loss = 0.05737797
Iteration 37, loss = 0.05450449
Iteration 38, loss = 0.05187655
Iteration 39, loss = 0.04946973
Iteration 40, loss = 0.04720274
Iteration 41, loss = 0.04515280
Iteration 42, loss = 0.04332754
Iteration 43, loss = 0.04154197
Iteration 44, loss = 0.03994878
Iteration 45, loss = 0.03838743
Iteration 46, loss = 0.03698326
Iteration 47, loss = 0.03569648
Iteration 48, loss = 0.03447511
Iteration 49, loss = 0.03333501
Iteration 50, loss = 0.03227250
Iteration 51, loss = 0.03124045
Iteration 52, loss = 0.03027984
Iteration 53, loss = 0.02945714
Iteration 54, loss = 0.02859968
Iteration 55, loss = 0.02782167
Iteration 56, loss = 0.02709698
Iteration 57, loss = 0.02639299
Iteration 58, loss = 0.02571349
Iteration 59, loss = 0.02508179
Iteration 60, loss = 0.02449002
Iteration 61, loss = 0.02392890
Iteration 62, loss = 0.02337938
Iteration 63, loss = 0.02289873
Iteration 64, loss = 0.02237471
Iteration 65, loss = 0.02194330
Iteration 66, loss = 0.02152523
Iteration 67, loss = 0.02109113
Iteration 68, loss = 0.02071175
Iteration 69, loss = 0.02031087
Iteration 70, loss = 0.01991838
Iteration 71, loss = 0.01960152
Iteration 72, loss = 0.01925589
Iteration 73, loss = 0.01899473
Iteration 74, loss = 0.01863728
Iteration 75, loss = 0.01837502
Iteration 76, loss = 0.01810490
Iteration 77, loss = 0.01779906
Iteration 78, loss = 0.01755699
Iteration 79, loss = 0.01728527
Iteration 80, loss = 0.01708839
Iteration 81, loss = 0.01683604
Iteration 82, loss = 0.01663012
Iteration 83, loss = 0.01640106
Iteration 84, loss = 0.01619175
Iteration 85, loss = 0.01598054
Iteration 86, loss = 0.01578930
Iteration 87, loss = 0.01562621
Iteration 88, loss = 0.01549614
Iteration 89, loss = 0.01525401
Iteration 90, loss = 0.01510325
Iteration 91, loss = 0.01495626
Iteration 92, loss = 0.01480275
Iteration 93, loss = 0.01465089
Iteration 94, loss = 0.01450068
Iteration 95, loss = 0.01437963
Iteration 96, loss = 0.01422800
Iteration 97, loss = 0.01407560
Iteration 98, loss = 0.01396309
Iteration 99, loss = 0.01385586
Iteration 100, loss = 0.01373974
Iteration 101, loss = 0.01363577
Iteration 102, loss = 0.01350278
Iteration 103, loss = 0.01340529
Iteration 104, loss = 0.01328799
Iteration 105, loss = 0.01317219
Iteration 106, loss = 0.01317430
Iteration 107, loss = 0.01302227
Iteration 108, loss = 0.01296457
Iteration 109, loss = 0.01278417
Iteration 110, loss = 0.01273403
Iteration 111, loss = 0.01264594
Iteration 112, loss = 0.01254973
Iteration 113, loss = 0.01245738
Iteration 114, loss = 0.01240331
Iteration 115, loss = 0.01233363
Iteration 116, loss = 0.01225158
Iteration 117, loss = 0.01217800
Iteration 118, loss = 0.01209757
Iteration 119, loss = 0.01204717
Iteration 120, loss = 0.01195208
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68046213
Iteration 2, loss = 0.64177254
Iteration 3, loss = 0.59020105
Iteration 4, loss = 0.52308027
Iteration 5, loss = 0.45214487
Iteration 6, loss = 0.38394109
Iteration 7, loss = 0.32227537
Iteration 8, loss = 0.26856788
Iteration 9, loss = 0.22415734
Iteration 10, loss = 0.18781268
Iteration 11, loss = 0.15860965
Iteration 12, loss = 0.13590954
Iteration 13, loss = 0.11747243
Iteration 14, loss = 0.10289242
Iteration 15, loss = 0.09110610
Iteration 16, loss = 0.08160874
Iteration 17, loss = 0.07344956
Iteration 18, loss = 0.06707810
Iteration 19, loss = 0.06152809
Iteration 20, loss = 0.05689483
Iteration 21, loss = 0.05273662
Iteration 22, loss = 0.04946409
Iteration 23, loss = 0.04641841
Iteration 24, loss = 0.04384459
Iteration 25, loss = 0.04153365
Iteration 26, loss = 0.03963169
Iteration 27, loss = 0.03789356
Iteration 28, loss = 0.03622558
Iteration 29, loss = 0.03488729
Iteration 30, loss = 0.03355945
Iteration 31, loss = 0.03254715
Iteration 32, loss = 0.03145521
Iteration 33, loss = 0.03055698
Iteration 34, loss = 0.02966785
Iteration 35, loss = 0.02903665
Iteration 36, loss = 0.02820260
Iteration 37, loss = 0.02765792
Iteration 38, loss = 0.02690478
Iteration 39, loss = 0.02633518
Iteration 40, loss = 0.02583391
Iteration 41, loss = 0.02549372
Iteration 42, loss = 0.02511157
Iteration 43, loss = 0.02472045
Iteration 44, loss = 0.02419435
Iteration 45, loss = 0.02396863
Iteration 46, loss = 0.02359736
Iteration 47, loss = 0.02325337
Iteration 48, loss = 0.02305611
Iteration 49, loss = 0.02268634
Iteration 50, loss = 0.02257395
Iteration 51, loss = 0.02219715
Iteration 52, loss = 0.02205391
Iteration 53, loss = 0.02189134
Iteration 54, loss = 0.02152682
Iteration 55, loss = 0.02148797
Iteration 56, loss = 0.02122166
Iteration 57, loss = 0.02101970
Iteration 58, loss = 0.02093741
Iteration 59, loss = 0.02080533
Iteration 60, loss = 0.02065592
Iteration 61, loss = 0.02064205
Iteration 62, loss = 0.02044701
Iteration 63, loss = 0.02031112
Iteration 64, loss = 0.02005664
Iteration 65, loss = 0.02010279
Iteration 66, loss = 0.02004760
Iteration 67, loss = 0.01993849
Iteration 68, loss = 0.01978395
Iteration 69, loss = 0.01974472
Iteration 70, loss = 0.01957059
Iteration 71, loss = 0.01962212
Iteration 72, loss = 0.01951217
Iteration 73, loss = 0.01932702
Iteration 74, loss = 0.01929121
Iteration 75, loss = 0.01925186
Iteration 76, loss = 0.01925083
Iteration 77, loss = 0.01915472
Iteration 78, loss = 0.01911804
Iteration 79, loss = 0.01888864
Iteration 80, loss = 0.01896110
Iteration 81, loss = 0.01893722
Iteration 82, loss = 0.01867218
Iteration 83, loss = 0.01871407
Iteration 84, loss = 0.01871173
Iteration 85, loss = 0.01866624
Iteration 86, loss = 0.01851728
Iteration 87, loss = 0.01850709
Iteration 88, loss = 0.01848177
Iteration 89, loss = 0.01856592
Iteration 90, loss = 0.01838944
Iteration 91, loss = 0.01834725
Iteration 92, loss = 0.01833616
Iteration 93, loss = 0.01831287
Iteration 94, loss = 0.01832801
Iteration 95, loss = 0.01834149
Iteration 96, loss = 0.01820819
Iteration 97, loss = 0.01816722
Iteration 98, loss = 0.01816106
Iteration 99, loss = 0.01819012
Iteration 100, loss = 0.01816380
Iteration 101, loss = 0.01804774
Iteration 102, loss = 0.01812208
Iteration 103, loss = 0.01806397
Iteration 104, loss = 0.01807029
Iteration 105, loss = 0.01805999
Iteration 106, loss = 0.01796511
Iteration 107, loss = 0.01801984
Iteration 108, loss = 0.01781047
Iteration 109, loss = 0.01776723
Iteration 110, loss = 0.01790096
Iteration 111, loss = 0.01768831
Iteration 112, loss = 0.01790959
Iteration 113, loss = 0.01804366
Iteration 114, loss = 0.01777618
Iteration 115, loss = 0.01779219
Iteration 116, loss = 0.01775782
Iteration 117, loss = 0.01760022
Iteration 118, loss = 0.01767666
Iteration 119, loss = 0.01761116
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67621868
Iteration 2, loss = 0.61268525
Iteration 3, loss = 0.51682064
Iteration 4, loss = 0.41325067
Iteration 5, loss = 0.31893510
Iteration 6, loss = 0.24401302
Iteration 7, loss = 0.18865998
Iteration 8, loss = 0.14927355
Iteration 9, loss = 0.12131589
Iteration 10, loss = 0.10150527
Iteration 11, loss = 0.08683101
Iteration 12, loss = 0.07609789
Iteration 13, loss = 0.06787655
Iteration 14, loss = 0.06137512
Iteration 15, loss = 0.05621577
Iteration 16, loss = 0.05202081
Iteration 17, loss = 0.04838084
Iteration 18, loss = 0.04562966
Iteration 19, loss = 0.04345051
Iteration 20, loss = 0.04122824
Iteration 21, loss = 0.03948760
Iteration 22, loss = 0.03815919
Iteration 23, loss = 0.03686124
Iteration 24, loss = 0.03570416
Iteration 25, loss = 0.03467552
Iteration 26, loss = 0.03371212
Iteration 27, loss = 0.03311475
Iteration 28, loss = 0.03244675
Iteration 29, loss = 0.03158435
Iteration 30, loss = 0.03120910
Iteration 31, loss = 0.03082128
Iteration 32, loss = 0.03041281
Iteration 33, loss = 0.02994743
Iteration 34, loss = 0.02948044
Iteration 35, loss = 0.02914737
Iteration 36, loss = 0.02894378
Iteration 37, loss = 0.02870789
Iteration 38, loss = 0.02827067
Iteration 39, loss = 0.02798952
Iteration 40, loss = 0.02789050
Iteration 41, loss = 0.02781202
Iteration 42, loss = 0.02784305
Iteration 43, loss = 0.02722525
Iteration 44, loss = 0.02711400
Iteration 45, loss = 0.02702987
Iteration 46, loss = 0.02661562
Iteration 47, loss = 0.02683445
Iteration 48, loss = 0.02677907
Iteration 49, loss = 0.02638384
Iteration 50, loss = 0.02635293
Iteration 51, loss = 0.02625041
Iteration 52, loss = 0.02615891
Iteration 53, loss = 0.02622339
Iteration 54, loss = 0.02571026
Iteration 55, loss = 0.02576882
Iteration 56, loss = 0.02590729
Iteration 57, loss = 0.02582406
Iteration 58, loss = 0.02565564
Iteration 59, loss = 0.02560840
Iteration 60, loss = 0.02555025
Iteration 61, loss = 0.02553024
Iteration 62, loss = 0.02553960
Iteration 63, loss = 0.02558412
Iteration 64, loss = 0.02540895
Iteration 65, loss = 0.02539183
Iteration 66, loss = 0.02513067
Iteration 67, loss = 0.02514770
Iteration 68, loss = 0.02510504
Iteration 69, loss = 0.02529557
Iteration 70, loss = 0.02518379
Iteration 71, loss = 0.02514184
Iteration 72, loss = 0.02518592
Iteration 73, loss = 0.02503688
Iteration 74, loss = 0.02506135
Iteration 75, loss = 0.02466384
Iteration 76, loss = 0.02493764
Iteration 77, loss = 0.02507120
Iteration 78, loss = 0.02470400
Iteration 79, loss = 0.02431907
Iteration 80, loss = 0.02487322
Iteration 81, loss = 0.02469836
Iteration 82, loss = 0.02489608
Iteration 83, loss = 0.02464782
Iteration 84, loss = 0.02429180
Iteration 85, loss = 0.02466160
Iteration 86, loss = 0.02494213
Iteration 87, loss = 0.02463110
Iteration 88, loss = 0.02453137
Iteration 89, loss = 0.02452292
Iteration 90, loss = 0.02451538
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67589865
Iteration 2, loss = 0.60624823
Iteration 3, loss = 0.50158277
Iteration 4, loss = 0.39443025
Iteration 5, loss = 0.30185283
Iteration 6, loss = 0.23065184
Iteration 7, loss = 0.17869458
Iteration 8, loss = 0.14233752
Iteration 9, loss = 0.11650296
Iteration 10, loss = 0.09795333
Iteration 11, loss = 0.08425184
Iteration 12, loss = 0.07400977
Iteration 13, loss = 0.06606395
Iteration 14, loss = 0.05985134
Iteration 15, loss = 0.05499797
Iteration 16, loss = 0.05087121
Iteration 17, loss = 0.04738067
Iteration 18, loss = 0.04465368
Iteration 19, loss = 0.04238733
Iteration 20, loss = 0.04032367
Iteration 21, loss = 0.03854740
Iteration 22, loss = 0.03724070
Iteration 23, loss = 0.03585370
Iteration 24, loss = 0.03476448
Iteration 25, loss = 0.03385348
Iteration 26, loss = 0.03294983
Iteration 27, loss = 0.03230736
Iteration 28, loss = 0.03143198
Iteration 29, loss = 0.03112319
Iteration 30, loss = 0.03016927
Iteration 31, loss = 0.02978288
Iteration 32, loss = 0.02937605
Iteration 33, loss = 0.02892284
Iteration 34, loss = 0.02855146
Iteration 35, loss = 0.02814170
Iteration 36, loss = 0.02781388
Iteration 37, loss = 0.02731613
Iteration 38, loss = 0.02753522
Iteration 39, loss = 0.02689472
Iteration 40, loss = 0.02672074
Iteration 41, loss = 0.02659583
Iteration 42, loss = 0.02650299
Iteration 43, loss = 0.02630121
Iteration 44, loss = 0.02592183
Iteration 45, loss = 0.02592332
Iteration 46, loss = 0.02567347
Iteration 47, loss = 0.02560037
Iteration 48, loss = 0.02540553
Iteration 49, loss = 0.02549097
Iteration 50, loss = 0.02559212
Iteration 51, loss = 0.02538073
Iteration 52, loss = 0.02497325
Iteration 53, loss = 0.02485366
Iteration 54, loss = 0.02509643
Iteration 55, loss = 0.02468587
Iteration 56, loss = 0.02490060
Iteration 57, loss = 0.02470242
Iteration 58, loss = 0.02463581
Iteration 59, loss = 0.02464575
Iteration 60, loss = 0.02434944
Iteration 61, loss = 0.02428101
Iteration 62, loss = 0.02431521
Iteration 63, loss = 0.02453490
Iteration 64, loss = 0.02408255
Iteration 65, loss = 0.02416640
Iteration 66, loss = 0.02421805
Iteration 67, loss = 0.02406263
Iteration 68, loss = 0.02404137
Iteration 69, loss = 0.02391180
Iteration 70, loss = 0.02393865
Iteration 71, loss = 0.02395797
Iteration 72, loss = 0.02396130
Iteration 73, loss = 0.02399760
Iteration 74, loss = 0.02398872
Iteration 75, loss = 0.02398003
Iteration 76, loss = 0.02383756
Iteration 77, loss = 0.02386854
Iteration 78, loss = 0.02366967
Iteration 79, loss = 0.02372038
Iteration 80, loss = 0.02367563
Iteration 81, loss = 0.02382661
Iteration 82, loss = 0.02378655
Iteration 83, loss = 0.02374526
Iteration 84, loss = 0.02373311
Iteration 85, loss = 0.02405162
Iteration 86, loss = 0.02343698
Iteration 87, loss = 0.02341429
Iteration 88, loss = 0.02353154
Iteration 89, loss = 0.02356149
Iteration 90, loss = 0.02354077
Iteration 91, loss = 0.02342187
Iteration 92, loss = 0.02336002
Iteration 93, loss = 0.02313525
Iteration 94, loss = 0.02348958
Iteration 95, loss = 0.02339950
Iteration 96, loss = 0.02373228
Iteration 97, loss = 0.02329575
Iteration 98, loss = 0.02375278
Iteration 99, loss = 0.02339548
Iteration 100, loss = 0.02329180
Iteration 101, loss = 0.02345591
Iteration 102, loss = 0.02323843
Iteration 103, loss = 0.02330491
Iteration 104, loss = 0.02318111
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69970144
Iteration 2, loss = 0.63954813
Iteration 3, loss = 0.55700645
Iteration 4, loss = 0.45339414
Iteration 5, loss = 0.35629501
Iteration 6, loss = 0.27522967
Iteration 7, loss = 0.21463706
Iteration 8, loss = 0.17033472
Iteration 9, loss = 0.13847443
Iteration 10, loss = 0.11550084
Iteration 11, loss = 0.09864530
Iteration 12, loss = 0.08572337
Iteration 13, loss = 0.07599193
Iteration 14, loss = 0.06834857
Iteration 15, loss = 0.06227781
Iteration 16, loss = 0.05741004
Iteration 17, loss = 0.05357931
Iteration 18, loss = 0.05029295
Iteration 19, loss = 0.04761815
Iteration 20, loss = 0.04540336
Iteration 21, loss = 0.04356808
Iteration 22, loss = 0.04164882
Iteration 23, loss = 0.04005532
Iteration 24, loss = 0.03865528
Iteration 25, loss = 0.03768006
Iteration 26, loss = 0.03676115
Iteration 27, loss = 0.03608883
Iteration 28, loss = 0.03501785
Iteration 29, loss = 0.03399502
Iteration 30, loss = 0.03339070
Iteration 31, loss = 0.03275614
Iteration 32, loss = 0.03185523
Iteration 33, loss = 0.03160493
Iteration 34, loss = 0.03130575
Iteration 35, loss = 0.03073451
Iteration 36, loss = 0.03039054
Iteration 37, loss = 0.03005007
Iteration 38, loss = 0.02974798
Iteration 39, loss = 0.02935836
Iteration 40, loss = 0.02898043
Iteration 41, loss = 0.02910862
Iteration 42, loss = 0.02891319
Iteration 43, loss = 0.02861225
Iteration 44, loss = 0.02823830
Iteration 45, loss = 0.02825377
Iteration 46, loss = 0.02831913
Iteration 47, loss = 0.02785078
Iteration 48, loss = 0.02787042
Iteration 49, loss = 0.02784368
Iteration 50, loss = 0.02732836
Iteration 51, loss = 0.02705784
Iteration 52, loss = 0.02703151
Iteration 53, loss = 0.02682261
Iteration 54, loss = 0.02733936
Iteration 55, loss = 0.02705252
Iteration 56, loss = 0.02734733
Iteration 57, loss = 0.02693370
Iteration 58, loss = 0.02679504
Iteration 59, loss = 0.02715413
Iteration 60, loss = 0.02661180
Iteration 61, loss = 0.02749850
Iteration 62, loss = 0.02724630
Iteration 63, loss = 0.02672179
Iteration 64, loss = 0.02669780
Iteration 65, loss = 0.02630287
Iteration 66, loss = 0.02596871
Iteration 67, loss = 0.02628889
Iteration 68, loss = 0.02642908
Iteration 69, loss = 0.02645232
Iteration 70, loss = 0.02612313
Iteration 71, loss = 0.02667194
Iteration 72, loss = 0.02659137
Iteration 73, loss = 0.02617278
Iteration 74, loss = 0.02576430
Iteration 75, loss = 0.02583056
Iteration 76, loss = 0.02626356
Iteration 77, loss = 0.02591292
Iteration 78, loss = 0.02585988
Iteration 79, loss = 0.02563415
Iteration 80, loss = 0.02568821
Iteration 81, loss = 0.02537586
Iteration 82, loss = 0.02537517
Iteration 83, loss = 0.02520185
Iteration 84, loss = 0.02520577
Iteration 85, loss = 0.02495566
Iteration 86, loss = 0.02509760
Iteration 87, loss = 0.02555219
Iteration 88, loss = 0.02543065
Iteration 89, loss = 0.02519900
Iteration 90, loss = 0.02561952
Iteration 91, loss = 0.02567028
Iteration 92, loss = 0.02576080
Iteration 93, loss = 0.02612797
Iteration 94, loss = 0.02599844
Iteration 95, loss = 0.02550770
Iteration 96, loss = 0.02546604
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67495143
Iteration 2, loss = 0.59535102
Iteration 3, loss = 0.48422289
Iteration 4, loss = 0.37243195
Iteration 5, loss = 0.28028335
Iteration 6, loss = 0.21140555
Iteration 7, loss = 0.16317942
Iteration 8, loss = 0.12983132
Iteration 9, loss = 0.10674169
Iteration 10, loss = 0.09005491
Iteration 11, loss = 0.07761679
Iteration 12, loss = 0.06838416
Iteration 13, loss = 0.06131320
Iteration 14, loss = 0.05596959
Iteration 15, loss = 0.05147643
Iteration 16, loss = 0.04778245
Iteration 17, loss = 0.04457613
Iteration 18, loss = 0.04228027
Iteration 19, loss = 0.03987467
Iteration 20, loss = 0.03804730
Iteration 21, loss = 0.03662360
Iteration 22, loss = 0.03542834
Iteration 23, loss = 0.03405074
Iteration 24, loss = 0.03332218
Iteration 25, loss = 0.03231565
Iteration 26, loss = 0.03159938
Iteration 27, loss = 0.03073354
Iteration 28, loss = 0.03011569
Iteration 29, loss = 0.02975943
Iteration 30, loss = 0.02896793
Iteration 31, loss = 0.02890512
Iteration 32, loss = 0.02839364
Iteration 33, loss = 0.02804501
Iteration 34, loss = 0.02761731
Iteration 35, loss = 0.02716495
Iteration 36, loss = 0.02693055
Iteration 37, loss = 0.02674411
Iteration 38, loss = 0.02655186
Iteration 39, loss = 0.02621933
Iteration 40, loss = 0.02626269
Iteration 41, loss = 0.02574367
Iteration 42, loss = 0.02562492
Iteration 43, loss = 0.02565354
Iteration 44, loss = 0.02536442
Iteration 45, loss = 0.02534364
Iteration 46, loss = 0.02541077
Iteration 47, loss = 0.02480189
Iteration 48, loss = 0.02480024
Iteration 49, loss = 0.02480755
Iteration 50, loss = 0.02485377
Iteration 51, loss = 0.02470237
Iteration 52, loss = 0.02446464
Iteration 53, loss = 0.02468003
Iteration 54, loss = 0.02443635
Iteration 55, loss = 0.02412294
Iteration 56, loss = 0.02429331
Iteration 57, loss = 0.02462227
Iteration 58, loss = 0.02411977
Iteration 59, loss = 0.02399623
Iteration 60, loss = 0.02403763
Iteration 61, loss = 0.02397538
Iteration 62, loss = 0.02387098
Iteration 63, loss = 0.02378943
Iteration 64, loss = 0.02373367
Iteration 65, loss = 0.02388870
Iteration 66, loss = 0.02397299
Iteration 67, loss = 0.02372506
Iteration 68, loss = 0.02341853
Iteration 69, loss = 0.02378351
Iteration 70, loss = 0.02365277
Iteration 71, loss = 0.02384210
Iteration 72, loss = 0.02325374
Iteration 73, loss = 0.02344954
Iteration 74, loss = 0.02348184
Iteration 75, loss = 0.02320154
Iteration 76, loss = 0.02353956
Iteration 77, loss = 0.02325249
Iteration 78, loss = 0.02300293
Iteration 79, loss = 0.02346245
Iteration 80, loss = 0.02393153
Iteration 81, loss = 0.02356798
Iteration 82, loss = 0.02334165
Iteration 83, loss = 0.02334506
Iteration 84, loss = 0.02316220
Iteration 85, loss = 0.02318885
Iteration 86, loss = 0.02314855
Iteration 87, loss = 0.02323605
Iteration 88, loss = 0.02313095
Iteration 89, loss = 0.02313660
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72564058
Iteration 2, loss = 0.69742835
Iteration 3, loss = 0.66433838
Iteration 4, loss = 0.62456083
Iteration 5, loss = 0.57967725
Iteration 6, loss = 0.53023408
Iteration 7, loss = 0.47811166
Iteration 8, loss = 0.42657172
Iteration 9, loss = 0.37723247
Iteration 10, loss = 0.33167480
Iteration 11, loss = 0.29078862
Iteration 12, loss = 0.25445837
Iteration 13, loss = 0.22322193
Iteration 14, loss = 0.19619916
Iteration 15, loss = 0.17318534
Iteration 16, loss = 0.15352091
Iteration 17, loss = 0.13682165
Iteration 18, loss = 0.12260424
Iteration 19, loss = 0.11038861
Iteration 20, loss = 0.10004702
Iteration 21, loss = 0.09122908
Iteration 22, loss = 0.08367179
Iteration 23, loss = 0.07681519
Iteration 24, loss = 0.07113773
Iteration 25, loss = 0.06614796
Iteration 26, loss = 0.06158392
Iteration 27, loss = 0.05758876
Iteration 28, loss = 0.05423524
Iteration 29, loss = 0.05098265
Iteration 30, loss = 0.04813107
Iteration 31, loss = 0.04558170
Iteration 32, loss = 0.04325800
Iteration 33, loss = 0.04133170
Iteration 34, loss = 0.03931859
Iteration 35, loss = 0.03775471
Iteration 36, loss = 0.03604414
Iteration 37, loss = 0.03459159
Iteration 38, loss = 0.03325423
Iteration 39, loss = 0.03201411
Iteration 40, loss = 0.03082815
Iteration 41, loss = 0.02972097
Iteration 42, loss = 0.02871737
Iteration 43, loss = 0.02784040
Iteration 44, loss = 0.02723077
Iteration 45, loss = 0.02637315
Iteration 46, loss = 0.02572721
Iteration 47, loss = 0.02497598
Iteration 48, loss = 0.02431602
Iteration 49, loss = 0.02372687
Iteration 50, loss = 0.02312939
Iteration 51, loss = 0.02254993
Iteration 52, loss = 0.02197313
Iteration 53, loss = 0.02160021
Iteration 54, loss = 0.02096043
Iteration 55, loss = 0.02074379
Iteration 56, loss = 0.02035208
Iteration 57, loss = 0.01999194
Iteration 58, loss = 0.01963168
Iteration 59, loss = 0.01932838
Iteration 60, loss = 0.01903580
Iteration 61, loss = 0.01877038
Iteration 62, loss = 0.01849173
Iteration 63, loss = 0.01816213
Iteration 64, loss = 0.01779089
Iteration 65, loss = 0.01747455
Iteration 66, loss = 0.01714354
Iteration 67, loss = 0.01694004
Iteration 68, loss = 0.01673371
Iteration 69, loss = 0.01651391
Iteration 70, loss = 0.01610404
Iteration 71, loss = 0.01610223
Iteration 72, loss = 0.01598863
Iteration 73, loss = 0.01598127
Iteration 74, loss = 0.01569683
Iteration 75, loss = 0.01550255
Iteration 76, loss = 0.01537258
Iteration 77, loss = 0.01516463
Iteration 78, loss = 0.01529973
Iteration 79, loss = 0.01510041
Iteration 80, loss = 0.01513361
Iteration 81, loss = 0.01496403
Iteration 82, loss = 0.01484178
Iteration 83, loss = 0.01448843
Iteration 84, loss = 0.01430745
Iteration 85, loss = 0.01424133
Iteration 86, loss = 0.01400460
Iteration 87, loss = 0.01398104
Iteration 88, loss = 0.01398995
Iteration 89, loss = 0.01371392
Iteration 90, loss = 0.01357760
Iteration 91, loss = 0.01348561
Iteration 92, loss = 0.01333494
Iteration 93, loss = 0.01327578
Iteration 94, loss = 0.01315184
Iteration 95, loss = 0.01308906
Iteration 96, loss = 0.01290722
Iteration 97, loss = 0.01293818
Iteration 98, loss = 0.01286319
Iteration 99, loss = 0.01283156
Iteration 100, loss = 0.01276215
Iteration 101, loss = 0.01263749
Iteration 102, loss = 0.01270636
Iteration 103, loss = 0.01270990
Iteration 104, loss = 0.01271183
Iteration 105, loss = 0.01237029
Iteration 106, loss = 0.01243989
Iteration 107, loss = 0.01235315
Iteration 108, loss = 0.01215693
Iteration 109, loss = 0.01223034
Iteration 110, loss = 0.01224117
Iteration 111, loss = 0.01217841
Iteration 112, loss = 0.01205638
Iteration 113, loss = 0.01205663
Iteration 114, loss = 0.01198541
Iteration 115, loss = 0.01192840
Iteration 116, loss = 0.01183952
Iteration 117, loss = 0.01184558
Iteration 118, loss = 0.01172942
Iteration 119, loss = 0.01176403
Iteration 120, loss = 0.01171097
Iteration 121, loss = 0.01154692
Iteration 122, loss = 0.01161904
Iteration 123, loss = 0.01157817
Iteration 124, loss = 0.01156437
Iteration 125, loss = 0.01150781
Iteration 126, loss = 0.01151262
Iteration 127, loss = 0.01160354
Iteration 128, loss = 0.01146861
Iteration 129, loss = 0.01151823
Iteration 130, loss = 0.01150945
Iteration 131, loss = 0.01146344
Iteration 132, loss = 0.01178014
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67637146
Iteration 2, loss = 0.59805321
Iteration 3, loss = 0.48406295
Iteration 4, loss = 0.37077968
Iteration 5, loss = 0.27825732
Iteration 6, loss = 0.20981626
Iteration 7, loss = 0.16213887
Iteration 8, loss = 0.12955415
Iteration 9, loss = 0.10674435
Iteration 10, loss = 0.09059714
Iteration 11, loss = 0.07876512
Iteration 12, loss = 0.06995711
Iteration 13, loss = 0.06304073
Iteration 14, loss = 0.05772161
Iteration 15, loss = 0.05327498
Iteration 16, loss = 0.04977620
Iteration 17, loss = 0.04681542
Iteration 18, loss = 0.04427226
Iteration 19, loss = 0.04225916
Iteration 20, loss = 0.04050860
Iteration 21, loss = 0.03901566
Iteration 22, loss = 0.03798845
Iteration 23, loss = 0.03664751
Iteration 24, loss = 0.03583129
Iteration 25, loss = 0.03505329
Iteration 26, loss = 0.03421169
Iteration 27, loss = 0.03370497
Iteration 28, loss = 0.03293763
Iteration 29, loss = 0.03268844
Iteration 30, loss = 0.03216863
Iteration 31, loss = 0.03163717
Iteration 32, loss = 0.03139890
Iteration 33, loss = 0.03085434
Iteration 34, loss = 0.03077615
Iteration 35, loss = 0.03014935
Iteration 36, loss = 0.02985440
Iteration 37, loss = 0.02985620
Iteration 38, loss = 0.02953266
Iteration 39, loss = 0.02922401
Iteration 40, loss = 0.02918591
Iteration 41, loss = 0.02902672
Iteration 42, loss = 0.02885290
Iteration 43, loss = 0.02868887
Iteration 44, loss = 0.02847932
Iteration 45, loss = 0.02848994
Iteration 46, loss = 0.02845846
Iteration 47, loss = 0.02860677
Iteration 48, loss = 0.02789333
Iteration 49, loss = 0.02826578
Iteration 50, loss = 0.02777349
Iteration 51, loss = 0.02779171
Iteration 52, loss = 0.02769129
Iteration 53, loss = 0.02763249
Iteration 54, loss = 0.02767248
Iteration 55, loss = 0.02749225
Iteration 56, loss = 0.02736658
Iteration 57, loss = 0.02755343
Iteration 58, loss = 0.02727402
Iteration 59, loss = 0.02732878
Iteration 60, loss = 0.02741780
Iteration 61, loss = 0.02706961
Iteration 62, loss = 0.02705319
Iteration 63, loss = 0.02707112
Iteration 64, loss = 0.02686935
Iteration 65, loss = 0.02709448
Iteration 66, loss = 0.02684947
Iteration 67, loss = 0.02681817
Iteration 68, loss = 0.02684700
Iteration 69, loss = 0.02692082
Iteration 70, loss = 0.02690226
Iteration 71, loss = 0.02683142
Iteration 72, loss = 0.02682574
Iteration 73, loss = 0.02664585
Iteration 74, loss = 0.02651974
Iteration 75, loss = 0.02656947
Iteration 76, loss = 0.02650132
Iteration 77, loss = 0.02664877
Iteration 78, loss = 0.02667060
Iteration 79, loss = 0.02667079
Iteration 80, loss = 0.02643458
Iteration 81, loss = 0.02659312
Iteration 82, loss = 0.02661261
Iteration 83, loss = 0.02655758
Iteration 84, loss = 0.02621299
Iteration 85, loss = 0.02652322
Iteration 86, loss = 0.02654294
Iteration 87, loss = 0.02645772
Iteration 88, loss = 0.02643246
Iteration 89, loss = 0.02631221
Iteration 90, loss = 0.02655840
Iteration 91, loss = 0.02632901
Iteration 92, loss = 0.02646106
Iteration 93, loss = 0.02610453
Iteration 94, loss = 0.02622777
Iteration 95, loss = 0.02616485
Iteration 96, loss = 0.02603792
Iteration 97, loss = 0.02617692
Iteration 98, loss = 0.02616166
Iteration 99, loss = 0.02621865
Iteration 100, loss = 0.02661547
Iteration 101, loss = 0.02648051
Iteration 102, loss = 0.02587121
Iteration 103, loss = 0.02616008
Iteration 104, loss = 0.02627808
Iteration 105, loss = 0.02602170
Iteration 106, loss = 0.02603333
Iteration 107, loss = 0.02620999
Iteration 108, loss = 0.02589541
Iteration 109, loss = 0.02596563
Iteration 110, loss = 0.02603529
Iteration 111, loss = 0.02590887
Iteration 112, loss = 0.02589941
Iteration 113, loss = 0.02624306
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70218295
Iteration 2, loss = 0.66199874
Iteration 3, loss = 0.60679297
Iteration 4, loss = 0.53883900
Iteration 5, loss = 0.46365047
Iteration 6, loss = 0.39130859
Iteration 7, loss = 0.32660120
Iteration 8, loss = 0.27080887
Iteration 9, loss = 0.22496450
Iteration 10, loss = 0.18796260
Iteration 11, loss = 0.15834980
Iteration 12, loss = 0.13481159
Iteration 13, loss = 0.11614429
Iteration 14, loss = 0.10134281
Iteration 15, loss = 0.08934952
Iteration 16, loss = 0.07948610
Iteration 17, loss = 0.07155994
Iteration 18, loss = 0.06481436
Iteration 19, loss = 0.05927528
Iteration 20, loss = 0.05453867
Iteration 21, loss = 0.05052055
Iteration 22, loss = 0.04689429
Iteration 23, loss = 0.04402870
Iteration 24, loss = 0.04138819
Iteration 25, loss = 0.03905896
Iteration 26, loss = 0.03698350
Iteration 27, loss = 0.03534947
Iteration 28, loss = 0.03368204
Iteration 29, loss = 0.03220468
Iteration 30, loss = 0.03100812
Iteration 31, loss = 0.02978650
Iteration 32, loss = 0.02870569
Iteration 33, loss = 0.02784982
Iteration 34, loss = 0.02706341
Iteration 35, loss = 0.02614637
Iteration 36, loss = 0.02536181
Iteration 37, loss = 0.02478629
Iteration 38, loss = 0.02412694
Iteration 39, loss = 0.02361781
Iteration 40, loss = 0.02309392
Iteration 41, loss = 0.02246089
Iteration 42, loss = 0.02211688
Iteration 43, loss = 0.02172847
Iteration 44, loss = 0.02150344
Iteration 45, loss = 0.02092721
Iteration 46, loss = 0.02061577
Iteration 47, loss = 0.02039112
Iteration 48, loss = 0.01997639
Iteration 49, loss = 0.01991445
Iteration 50, loss = 0.01961487
Iteration 51, loss = 0.01911920
Iteration 52, loss = 0.01898239
Iteration 53, loss = 0.01874010
Iteration 54, loss = 0.01852960
Iteration 55, loss = 0.01835656
Iteration 56, loss = 0.01813777
Iteration 57, loss = 0.01812974
Iteration 58, loss = 0.01787811
Iteration 59, loss = 0.01769417
Iteration 60, loss = 0.01762584
Iteration 61, loss = 0.01743833
Iteration 62, loss = 0.01723108
Iteration 63, loss = 0.01715167
Iteration 64, loss = 0.01703073
Iteration 65, loss = 0.01703838
Iteration 66, loss = 0.01682489
Iteration 67, loss = 0.01660597
Iteration 68, loss = 0.01657583
Iteration 69, loss = 0.01670709
Iteration 70, loss = 0.01637420
Iteration 71, loss = 0.01632060
Iteration 72, loss = 0.01624188
Iteration 73, loss = 0.01631874
Iteration 74, loss = 0.01617556
Iteration 75, loss = 0.01595700
Iteration 76, loss = 0.01597750
Iteration 77, loss = 0.01583026
Iteration 78, loss = 0.01588839
Iteration 79, loss = 0.01575823
Iteration 80, loss = 0.01564420
Iteration 81, loss = 0.01562049
Iteration 82, loss = 0.01554348
Iteration 83, loss = 0.01548282
Iteration 84, loss = 0.01553652
Iteration 85, loss = 0.01541207
Iteration 86, loss = 0.01551173
Iteration 87, loss = 0.01531084
Iteration 88, loss = 0.01532690
Iteration 89, loss = 0.01528214
Iteration 90, loss = 0.01521563
Iteration 91, loss = 0.01511353
Iteration 92, loss = 0.01513078
Iteration 93, loss = 0.01516682
Iteration 94, loss = 0.01495528
Iteration 95, loss = 0.01505053
Iteration 96, loss = 0.01494256
Iteration 97, loss = 0.01492668
Iteration 98, loss = 0.01487336
Iteration 99, loss = 0.01487540
Iteration 100, loss = 0.01479018
Iteration 101, loss = 0.01472854
Iteration 102, loss = 0.01478941
Iteration 103, loss = 0.01472899
Iteration 104, loss = 0.01458811
Iteration 105, loss = 0.01480375
Iteration 106, loss = 0.01451958
Iteration 107, loss = 0.01456288
Iteration 108, loss = 0.01466151
Iteration 109, loss = 0.01467587
Iteration 110, loss = 0.01448159
Iteration 111, loss = 0.01448598
Iteration 112, loss = 0.01465508
Iteration 113, loss = 0.01442394
Iteration 114, loss = 0.01459219
Iteration 115, loss = 0.01445044
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67809110
Iteration 2, loss = 0.62093884
Iteration 3, loss = 0.53273482
Iteration 4, loss = 0.43183866
Iteration 5, loss = 0.33847422
Iteration 6, loss = 0.26212782
Iteration 7, loss = 0.20399797
Iteration 8, loss = 0.16196037
Iteration 9, loss = 0.13176202
Iteration 10, loss = 0.11023530
Iteration 11, loss = 0.09433714
Iteration 12, loss = 0.08198741
Iteration 13, loss = 0.07267191
Iteration 14, loss = 0.06561393
Iteration 15, loss = 0.05984038
Iteration 16, loss = 0.05532479
Iteration 17, loss = 0.05157969
Iteration 18, loss = 0.04828117
Iteration 19, loss = 0.04553601
Iteration 20, loss = 0.04334486
Iteration 21, loss = 0.04108376
Iteration 22, loss = 0.03945601
Iteration 23, loss = 0.03797892
Iteration 24, loss = 0.03718492
Iteration 25, loss = 0.03630480
Iteration 26, loss = 0.03515576
Iteration 27, loss = 0.03426866
Iteration 28, loss = 0.03339006
Iteration 29, loss = 0.03250906
Iteration 30, loss = 0.03161142
Iteration 31, loss = 0.03114010
Iteration 32, loss = 0.03061455
Iteration 33, loss = 0.03047649
Iteration 34, loss = 0.03005960
Iteration 35, loss = 0.02928524
Iteration 36, loss = 0.02897200
Iteration 37, loss = 0.02861976
Iteration 38, loss = 0.02831946
Iteration 39, loss = 0.02806303
Iteration 40, loss = 0.02781535
Iteration 41, loss = 0.02788676
Iteration 42, loss = 0.02769614
Iteration 43, loss = 0.02721840
Iteration 44, loss = 0.02700952
Iteration 45, loss = 0.02688757
Iteration 46, loss = 0.02642429
Iteration 47, loss = 0.02634713
Iteration 48, loss = 0.02646648
Iteration 49, loss = 0.02618232
Iteration 50, loss = 0.02587552
Iteration 51, loss = 0.02598440
Iteration 52, loss = 0.02543790
Iteration 53, loss = 0.02526083
Iteration 54, loss = 0.02530694
Iteration 55, loss = 0.02525312
Iteration 56, loss = 0.02480687
Iteration 57, loss = 0.02491819
Iteration 58, loss = 0.02512442
Iteration 59, loss = 0.02513895
Iteration 60, loss = 0.02481410
Iteration 61, loss = 0.02519405
Iteration 62, loss = 0.02553958
Iteration 63, loss = 0.02530396
Iteration 64, loss = 0.02504609
Iteration 65, loss = 0.02494911
Iteration 66, loss = 0.02466387
Iteration 67, loss = 0.02476319
Iteration 68, loss = 0.02482627
Iteration 69, loss = 0.02460441
Iteration 70, loss = 0.02522791
Iteration 71, loss = 0.02610687
Iteration 72, loss = 0.02449079
Iteration 73, loss = 0.02413863
Iteration 74, loss = 0.02389459
Iteration 75, loss = 0.02394075
Iteration 76, loss = 0.02414587
Iteration 77, loss = 0.02409317
Iteration 78, loss = 0.02400705
Iteration 79, loss = 0.02431667
Iteration 80, loss = 0.02404884
Iteration 81, loss = 0.02392058
Iteration 82, loss = 0.02377571
Iteration 83, loss = 0.02354009
Iteration 84, loss = 0.02359431
Iteration 85, loss = 0.02355164
Iteration 86, loss = 0.02347960
Iteration 87, loss = 0.02362446
Iteration 88, loss = 0.02404889
Iteration 89, loss = 0.02358258
Iteration 90, loss = 0.02400147
Iteration 91, loss = 0.02395146
Iteration 92, loss = 0.02387981
Iteration 93, loss = 0.02355868
Iteration 94, loss = 0.02362142
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68851447
Iteration 2, loss = 0.65126762
Iteration 3, loss = 0.60414235
Iteration 4, loss = 0.54548602
Iteration 5, loss = 0.48175231
Iteration 6, loss = 0.41981872
Iteration 7, loss = 0.36270623
Iteration 8, loss = 0.31110422
Iteration 9, loss = 0.26633754
Iteration 10, loss = 0.22815400
Iteration 11, loss = 0.19578915
Iteration 12, loss = 0.16906912
Iteration 13, loss = 0.14695190
Iteration 14, loss = 0.12860621
Iteration 15, loss = 0.11369672
Iteration 16, loss = 0.10112502
Iteration 17, loss = 0.09073097
Iteration 18, loss = 0.08204227
Iteration 19, loss = 0.07469125
Iteration 20, loss = 0.06851720
Iteration 21, loss = 0.06317889
Iteration 22, loss = 0.05867998
Iteration 23, loss = 0.05454406
Iteration 24, loss = 0.05095450
Iteration 25, loss = 0.04798648
Iteration 26, loss = 0.04527673
Iteration 27, loss = 0.04292375
Iteration 28, loss = 0.04064831
Iteration 29, loss = 0.03870076
Iteration 30, loss = 0.03723497
Iteration 31, loss = 0.03565090
Iteration 32, loss = 0.03409420
Iteration 33, loss = 0.03284407
Iteration 34, loss = 0.03159392
Iteration 35, loss = 0.03063183
Iteration 36, loss = 0.02957913
Iteration 37, loss = 0.02872717
Iteration 38, loss = 0.02805586
Iteration 39, loss = 0.02719781
Iteration 40, loss = 0.02646136
Iteration 41, loss = 0.02589502
Iteration 42, loss = 0.02525311
Iteration 43, loss = 0.02469029
Iteration 44, loss = 0.02415668
Iteration 45, loss = 0.02354041
Iteration 46, loss = 0.02318766
Iteration 47, loss = 0.02268931
Iteration 48, loss = 0.02242756
Iteration 49, loss = 0.02217083
Iteration 50, loss = 0.02171976
Iteration 51, loss = 0.02146468
Iteration 52, loss = 0.02112138
Iteration 53, loss = 0.02077882
Iteration 54, loss = 0.02060477
Iteration 55, loss = 0.02036923
Iteration 56, loss = 0.01997894
Iteration 57, loss = 0.01980119
Iteration 58, loss = 0.01961961
Iteration 59, loss = 0.01945011
Iteration 60, loss = 0.01906541
Iteration 61, loss = 0.01901032
Iteration 62, loss = 0.01882791
Iteration 63, loss = 0.01873145
Iteration 64, loss = 0.01850611
Iteration 65, loss = 0.01834928
Iteration 66, loss = 0.01819857
Iteration 67, loss = 0.01811369
Iteration 68, loss = 0.01794878
Iteration 69, loss = 0.01780039
Iteration 70, loss = 0.01771270
Iteration 71, loss = 0.01758853
Iteration 72, loss = 0.01744924
Iteration 73, loss = 0.01743826
Iteration 74, loss = 0.01722783
Iteration 75, loss = 0.01720685
Iteration 76, loss = 0.01725758
Iteration 77, loss = 0.01698470
Iteration 78, loss = 0.01701814
Iteration 79, loss = 0.01689433
Iteration 80, loss = 0.01681353
Iteration 81, loss = 0.01669088
Iteration 82, loss = 0.01660383
Iteration 83, loss = 0.01653533
Iteration 84, loss = 0.01650441
Iteration 85, loss = 0.01633624
Iteration 86, loss = 0.01647343
Iteration 87, loss = 0.01633085
Iteration 88, loss = 0.01632460
Iteration 89, loss = 0.01611782
Iteration 90, loss = 0.01605934
Iteration 91, loss = 0.01595632
Iteration 92, loss = 0.01595195
Iteration 93, loss = 0.01595237
Iteration 94, loss = 0.01590368
Iteration 95, loss = 0.01579357
Iteration 96, loss = 0.01583414
Iteration 97, loss = 0.01571167
Iteration 98, loss = 0.01575518
Iteration 99, loss = 0.01557790
Iteration 100, loss = 0.01559986
Iteration 101, loss = 0.01562906
Iteration 102, loss = 0.01562056
Iteration 103, loss = 0.01561453
Iteration 104, loss = 0.01549488
Iteration 105, loss = 0.01542883
Iteration 106, loss = 0.01536570
Iteration 107, loss = 0.01548395
Iteration 108, loss = 0.01532524
Iteration 109, loss = 0.01536463
Iteration 110, loss = 0.01531695
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68782512
Iteration 2, loss = 0.67090486
Iteration 3, loss = 0.65622323
Iteration 4, loss = 0.64174442
Iteration 5, loss = 0.62696962
Iteration 6, loss = 0.61104327
Iteration 7, loss = 0.59414186
Iteration 8, loss = 0.57579709
Iteration 9, loss = 0.55620665
Iteration 10, loss = 0.53528604
Iteration 11, loss = 0.51319028
Iteration 12, loss = 0.49008581
Iteration 13, loss = 0.46643715
Iteration 14, loss = 0.44254847
Iteration 15, loss = 0.41855953
Iteration 16, loss = 0.39483566
Iteration 17, loss = 0.37176290
Iteration 18, loss = 0.34957703
Iteration 19, loss = 0.32824621
Iteration 20, loss = 0.30770586
Iteration 21, loss = 0.28822243
Iteration 22, loss = 0.26997144
Iteration 23, loss = 0.25265160
Iteration 24, loss = 0.23644081
Iteration 25, loss = 0.22120586
Iteration 26, loss = 0.20704072
Iteration 27, loss = 0.19382680
Iteration 28, loss = 0.18159312
Iteration 29, loss = 0.17023233
Iteration 30, loss = 0.15959586
Iteration 31, loss = 0.14985299
Iteration 32, loss = 0.14080025
Iteration 33, loss = 0.13244731
Iteration 34, loss = 0.12467669
Iteration 35, loss = 0.11755098
Iteration 36, loss = 0.11094376
Iteration 37, loss = 0.10481742
Iteration 38, loss = 0.09918822
Iteration 39, loss = 0.09391551
Iteration 40, loss = 0.08905709
Iteration 41, loss = 0.08456746
Iteration 42, loss = 0.08036510
Iteration 43, loss = 0.07646994
Iteration 44, loss = 0.07286215
Iteration 45, loss = 0.06946977
Iteration 46, loss = 0.06633355
Iteration 47, loss = 0.06341702
Iteration 48, loss = 0.06066685
Iteration 49, loss = 0.05811430
Iteration 50, loss = 0.05568279
Iteration 51, loss = 0.05344100
Iteration 52, loss = 0.05130675
Iteration 53, loss = 0.04932314
Iteration 54, loss = 0.04744035
Iteration 55, loss = 0.04565761
Iteration 56, loss = 0.04399844
Iteration 57, loss = 0.04241781
Iteration 58, loss = 0.04092993
Iteration 59, loss = 0.03953044
Iteration 60, loss = 0.03818648
Iteration 61, loss = 0.03691351
Iteration 62, loss = 0.03572070
Iteration 63, loss = 0.03459716
Iteration 64, loss = 0.03350480
Iteration 65, loss = 0.03249527
Iteration 66, loss = 0.03150570
Iteration 67, loss = 0.03057181
Iteration 68, loss = 0.02968891
Iteration 69, loss = 0.02885140
Iteration 70, loss = 0.02803649
Iteration 71, loss = 0.02726606
Iteration 72, loss = 0.02654642
Iteration 73, loss = 0.02583442
Iteration 74, loss = 0.02517183
Iteration 75, loss = 0.02452616
Iteration 76, loss = 0.02391640
Iteration 77, loss = 0.02332107
Iteration 78, loss = 0.02274859
Iteration 79, loss = 0.02219763
Iteration 80, loss = 0.02167364
Iteration 81, loss = 0.02117061
Iteration 82, loss = 0.02067086
Iteration 83, loss = 0.02021041
Iteration 84, loss = 0.01975655
Iteration 85, loss = 0.01932292
Iteration 86, loss = 0.01890260
Iteration 87, loss = 0.01850509
Iteration 88, loss = 0.01811490
Iteration 89, loss = 0.01773618
Iteration 90, loss = 0.01738652
Iteration 91, loss = 0.01702699
Iteration 92, loss = 0.01670032
Iteration 93, loss = 0.01636513
Iteration 94, loss = 0.01604178
Iteration 95, loss = 0.01574828
Iteration 96, loss = 0.01544667
Iteration 97, loss = 0.01516338
Iteration 98, loss = 0.01487905
Iteration 99, loss = 0.01460791
Iteration 100, loss = 0.01435020
Iteration 101, loss = 0.01409429
Iteration 102, loss = 0.01385099
Iteration 103, loss = 0.01361093
Iteration 104, loss = 0.01338246
Iteration 105, loss = 0.01316068
Iteration 106, loss = 0.01294634
Iteration 107, loss = 0.01273937
Iteration 108, loss = 0.01252791
Iteration 109, loss = 0.01232921
Iteration 110, loss = 0.01213748
Iteration 111, loss = 0.01194956
Iteration 112, loss = 0.01176640
Iteration 113, loss = 0.01158851
Iteration 114, loss = 0.01141539
Iteration 115, loss = 0.01124910
Iteration 116, loss = 0.01108325
Iteration 117, loss = 0.01092463
Iteration 118, loss = 0.01076099
Iteration 119, loss = 0.01060805
Iteration 120, loss = 0.01046105
Iteration 121, loss = 0.01031464
Iteration 122, loss = 0.01016750
Iteration 123, loss = 0.01003191
Iteration 124, loss = 0.00989678
Iteration 125, loss = 0.00976797
Iteration 126, loss = 0.00964665
Iteration 127, loss = 0.00952084
Iteration 128, loss = 0.00939616
Iteration 129, loss = 0.00927841
Iteration 130, loss = 0.00916351
Iteration 131, loss = 0.00905089
Iteration 132, loss = 0.00894058
Iteration 133, loss = 0.00883751
Iteration 134, loss = 0.00872919
Iteration 135, loss = 0.00862438
Iteration 136, loss = 0.00852724
Iteration 137, loss = 0.00842664
Iteration 138, loss = 0.00833287
Iteration 139, loss = 0.00823800
Iteration 140, loss = 0.00814248
Iteration 141, loss = 0.00804976
Iteration 142, loss = 0.00796356
Iteration 143, loss = 0.00787545
Iteration 144, loss = 0.00779032
Iteration 145, loss = 0.00771005
Iteration 146, loss = 0.00762527
Iteration 147, loss = 0.00754680
Iteration 148, loss = 0.00746871
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68392265
Iteration 2, loss = 0.66602449
Iteration 3, loss = 0.64803406
Iteration 4, loss = 0.62656988
Iteration 5, loss = 0.60014747
Iteration 6, loss = 0.56887524
Iteration 7, loss = 0.53319547
Iteration 8, loss = 0.49448902
Iteration 9, loss = 0.45529069
Iteration 10, loss = 0.41641328
Iteration 11, loss = 0.37921653
Iteration 12, loss = 0.34438689
Iteration 13, loss = 0.31197443
Iteration 14, loss = 0.28219375
Iteration 15, loss = 0.25520356
Iteration 16, loss = 0.23080072
Iteration 17, loss = 0.20894449
Iteration 18, loss = 0.18950579
Iteration 19, loss = 0.17228993
Iteration 20, loss = 0.15688418
Iteration 21, loss = 0.14342086
Iteration 22, loss = 0.13139624
Iteration 23, loss = 0.12076350
Iteration 24, loss = 0.11141115
Iteration 25, loss = 0.10302781
Iteration 26, loss = 0.09557402
Iteration 27, loss = 0.08896051
Iteration 28, loss = 0.08308142
Iteration 29, loss = 0.07773031
Iteration 30, loss = 0.07292860
Iteration 31, loss = 0.06854271
Iteration 32, loss = 0.06467711
Iteration 33, loss = 0.06110088
Iteration 34, loss = 0.05796325
Iteration 35, loss = 0.05495632
Iteration 36, loss = 0.05226585
Iteration 37, loss = 0.04981262
Iteration 38, loss = 0.04751739
Iteration 39, loss = 0.04544944
Iteration 40, loss = 0.04350187
Iteration 41, loss = 0.04169587
Iteration 42, loss = 0.04009409
Iteration 43, loss = 0.03859038
Iteration 44, loss = 0.03711203
Iteration 45, loss = 0.03578503
Iteration 46, loss = 0.03457603
Iteration 47, loss = 0.03336483
Iteration 48, loss = 0.03235109
Iteration 49, loss = 0.03131513
Iteration 50, loss = 0.03035304
Iteration 51, loss = 0.02948122
Iteration 52, loss = 0.02864811
Iteration 53, loss = 0.02781346
Iteration 54, loss = 0.02709242
Iteration 55, loss = 0.02634492
Iteration 56, loss = 0.02568903
Iteration 57, loss = 0.02511189
Iteration 58, loss = 0.02439780
Iteration 59, loss = 0.02386481
Iteration 60, loss = 0.02332671
Iteration 61, loss = 0.02282911
Iteration 62, loss = 0.02231759
Iteration 63, loss = 0.02184527
Iteration 64, loss = 0.02141425
Iteration 65, loss = 0.02099283
Iteration 66, loss = 0.02056157
Iteration 67, loss = 0.02020763
Iteration 68, loss = 0.01988268
Iteration 69, loss = 0.01948334
Iteration 70, loss = 0.01912774
Iteration 71, loss = 0.01882298
Iteration 72, loss = 0.01852503
Iteration 73, loss = 0.01817760
Iteration 74, loss = 0.01788846
Iteration 75, loss = 0.01761651
Iteration 76, loss = 0.01736018
Iteration 77, loss = 0.01708394
Iteration 78, loss = 0.01683376
Iteration 79, loss = 0.01661679
Iteration 80, loss = 0.01639269
Iteration 81, loss = 0.01620536
Iteration 82, loss = 0.01601068
Iteration 83, loss = 0.01582675
Iteration 84, loss = 0.01562399
Iteration 85, loss = 0.01537032
Iteration 86, loss = 0.01520684
Iteration 87, loss = 0.01504855
Iteration 88, loss = 0.01488386
Iteration 89, loss = 0.01466398
Iteration 90, loss = 0.01455726
Iteration 91, loss = 0.01440223
Iteration 92, loss = 0.01420298
Iteration 93, loss = 0.01409319
Iteration 94, loss = 0.01391935
Iteration 95, loss = 0.01381224
Iteration 96, loss = 0.01368471
Iteration 97, loss = 0.01359870
Iteration 98, loss = 0.01339663
Iteration 99, loss = 0.01334613
Iteration 100, loss = 0.01317392
Iteration 101, loss = 0.01311834
Iteration 102, loss = 0.01297798
Iteration 103, loss = 0.01285561
Iteration 104, loss = 0.01276108
Iteration 105, loss = 0.01264088
Iteration 106, loss = 0.01257801
Iteration 107, loss = 0.01246579
Iteration 108, loss = 0.01233770
Iteration 109, loss = 0.01229501
Iteration 110, loss = 0.01225877
Iteration 111, loss = 0.01213415
Iteration 112, loss = 0.01206569
Iteration 113, loss = 0.01201990
Iteration 114, loss = 0.01187778
Iteration 115, loss = 0.01179386
Iteration 116, loss = 0.01171035
Iteration 117, loss = 0.01163736
Iteration 118, loss = 0.01159551
Iteration 119, loss = 0.01152213
Iteration 120, loss = 0.01142386
Iteration 121, loss = 0.01141484
Iteration 122, loss = 0.01131184
Iteration 123, loss = 0.01124274
Iteration 124, loss = 0.01120811
Iteration 125, loss = 0.01115637
Iteration 126, loss = 0.01106830
Iteration 127, loss = 0.01100177
Iteration 128, loss = 0.01100086
Iteration 129, loss = 0.01088023
Iteration 130, loss = 0.01084299
Iteration 131, loss = 0.01079602
Iteration 132, loss = 0.01078787
Iteration 133, loss = 0.01073059
Iteration 134, loss = 0.01070954
Iteration 135, loss = 0.01060929
Iteration 136, loss = 0.01053857
Iteration 137, loss = 0.01054030
Iteration 138, loss = 0.01047119
Iteration 139, loss = 0.01040522
Iteration 140, loss = 0.01038479
Iteration 141, loss = 0.01034090
Iteration 142, loss = 0.01033986
Iteration 143, loss = 0.01027899
Iteration 144, loss = 0.01022499
Iteration 145, loss = 0.01019303
Iteration 146, loss = 0.01015669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71126342
Iteration 2, loss = 0.63467491
Iteration 3, loss = 0.52706260
Iteration 4, loss = 0.40634331
Iteration 5, loss = 0.30297605
Iteration 6, loss = 0.22572477
Iteration 7, loss = 0.17174905
Iteration 8, loss = 0.13498439
Iteration 9, loss = 0.10959622
Iteration 10, loss = 0.09182811
Iteration 11, loss = 0.07891140
Iteration 12, loss = 0.06919365
Iteration 13, loss = 0.06210796
Iteration 14, loss = 0.05645386
Iteration 15, loss = 0.05200207
Iteration 16, loss = 0.04828071
Iteration 17, loss = 0.04534989
Iteration 18, loss = 0.04275678
Iteration 19, loss = 0.04079902
Iteration 20, loss = 0.03892968
Iteration 21, loss = 0.03768944
Iteration 22, loss = 0.03619243
Iteration 23, loss = 0.03497111
Iteration 24, loss = 0.03413300
Iteration 25, loss = 0.03329295
Iteration 26, loss = 0.03266478
Iteration 27, loss = 0.03187087
Iteration 28, loss = 0.03111749
Iteration 29, loss = 0.03081593
Iteration 30, loss = 0.03021312
Iteration 31, loss = 0.03008013
Iteration 32, loss = 0.02955973
Iteration 33, loss = 0.02914942
Iteration 34, loss = 0.02873664
Iteration 35, loss = 0.02861381
Iteration 36, loss = 0.02865014
Iteration 37, loss = 0.02799337
Iteration 38, loss = 0.02780946
Iteration 39, loss = 0.02769315
Iteration 40, loss = 0.02752313
Iteration 41, loss = 0.02731516
Iteration 42, loss = 0.02705581
Iteration 43, loss = 0.02700595
Iteration 44, loss = 0.02666453
Iteration 45, loss = 0.02659061
Iteration 46, loss = 0.02656434
Iteration 47, loss = 0.02642910
Iteration 48, loss = 0.02626436
Iteration 49, loss = 0.02635456
Iteration 50, loss = 0.02604259
Iteration 51, loss = 0.02604828
Iteration 52, loss = 0.02570018
Iteration 53, loss = 0.02564461
Iteration 54, loss = 0.02586624
Iteration 55, loss = 0.02546064
Iteration 56, loss = 0.02608253
Iteration 57, loss = 0.02569512
Iteration 58, loss = 0.02551847
Iteration 59, loss = 0.02551517
Iteration 60, loss = 0.02559890
Iteration 61, loss = 0.02540783
Iteration 62, loss = 0.02552889
Iteration 63, loss = 0.02535218
Iteration 64, loss = 0.02507086
Iteration 65, loss = 0.02540607
Iteration 66, loss = 0.02516480
Iteration 67, loss = 0.02505184
Iteration 68, loss = 0.02527623
Iteration 69, loss = 0.02519809
Iteration 70, loss = 0.02540995
Iteration 71, loss = 0.02485052
Iteration 72, loss = 0.02476117
Iteration 73, loss = 0.02498104
Iteration 74, loss = 0.02493227
Iteration 75, loss = 0.02502368
Iteration 76, loss = 0.02484526
Iteration 77, loss = 0.02511482
Iteration 78, loss = 0.02478051
Iteration 79, loss = 0.02464335
Iteration 80, loss = 0.02469904
Iteration 81, loss = 0.02455922
Iteration 82, loss = 0.02491087
Iteration 83, loss = 0.02507332
Iteration 84, loss = 0.02477934
Iteration 85, loss = 0.02441452
Iteration 86, loss = 0.02459293
Iteration 87, loss = 0.02472765
Iteration 88, loss = 0.02488220
Iteration 89, loss = 0.02430050
Iteration 90, loss = 0.02480446
Iteration 91, loss = 0.02466254
Iteration 92, loss = 0.02458326
Iteration 93, loss = 0.02447422
Iteration 94, loss = 0.02431005
Iteration 95, loss = 0.02439189
Iteration 96, loss = 0.02486260
Iteration 97, loss = 0.02436227
Iteration 98, loss = 0.02462108
Iteration 99, loss = 0.02454561
Iteration 100, loss = 0.02454075
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	106m22.309s
user	764m14.499s
sys	86m58.542s
Elapsed time: 6383 seconds
