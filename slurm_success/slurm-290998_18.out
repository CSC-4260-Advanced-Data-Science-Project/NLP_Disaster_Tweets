Processing dataset: prepended_v1_basic_clean

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: prepended_v1_basic_clean (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on prepended_v1_basic_clean...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on prepended_v1_basic_clean...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on prepended_v1_basic_clean...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on prepended_v1_basic_clean...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on prepended_v1_basic_clean...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67540705
Iteration 2, loss = 0.60397350
Iteration 3, loss = 0.49837562
Iteration 4, loss = 0.38393836
Iteration 5, loss = 0.28460819
Iteration 6, loss = 0.20942708
Iteration 7, loss = 0.15744825
Iteration 8, loss = 0.12227189
Iteration 9, loss = 0.09862313
Iteration 10, loss = 0.08173279
Iteration 11, loss = 0.06975665
Iteration 12, loss = 0.06100124
Iteration 13, loss = 0.05440982
Iteration 14, loss = 0.04905865
Iteration 15, loss = 0.04513880
Iteration 16, loss = 0.04210834
Iteration 17, loss = 0.03916725
Iteration 18, loss = 0.03699268
Iteration 19, loss = 0.03519505
Iteration 20, loss = 0.03379424
Iteration 21, loss = 0.03217712
Iteration 22, loss = 0.03121081
Iteration 23, loss = 0.03018662
Iteration 24, loss = 0.02939793
Iteration 25, loss = 0.02885869
Iteration 26, loss = 0.02814988
Iteration 27, loss = 0.02745706
Iteration 28, loss = 0.02677052
Iteration 29, loss = 0.02630295
Iteration 30, loss = 0.02594278
Iteration 31, loss = 0.02572679
Iteration 32, loss = 0.02546122
Iteration 33, loss = 0.02501874
Iteration 34, loss = 0.02497856
Iteration 35, loss = 0.02469906
Iteration 36, loss = 0.02433131
Iteration 37, loss = 0.02408326
Iteration 38, loss = 0.02383298
Iteration 39, loss = 0.02367340
Iteration 40, loss = 0.02369683
Iteration 41, loss = 0.02352199
Iteration 42, loss = 0.02314441
Iteration 43, loss = 0.02327190
Iteration 44, loss = 0.02294948
Iteration 45, loss = 0.02315134
Iteration 46, loss = 0.02303778
Iteration 47, loss = 0.02281314
Iteration 48, loss = 0.02273637
Iteration 49, loss = 0.02276807
Iteration 50, loss = 0.02275280
Iteration 51, loss = 0.02255962
Iteration 52, loss = 0.02269195
Iteration 53, loss = 0.02243172
Iteration 54, loss = 0.02242290
Iteration 55, loss = 0.02233635
Iteration 56, loss = 0.02219052
Iteration 57, loss = 0.02209635
Iteration 58, loss = 0.02205721
Iteration 59, loss = 0.02237974
Iteration 60, loss = 0.02187438
Iteration 61, loss = 0.02207571
Iteration 62, loss = 0.02232483
Iteration 63, loss = 0.02176384
Iteration 64, loss = 0.02172827
Iteration 65, loss = 0.02172104
Iteration 66, loss = 0.02194838
Iteration 67, loss = 0.02178182
Iteration 68, loss = 0.02214132
Iteration 69, loss = 0.02168402
Iteration 70, loss = 0.02157922
Iteration 71, loss = 0.02153945
Iteration 72, loss = 0.02160320
Iteration 73, loss = 0.02181148
Iteration 74, loss = 0.02126889
Iteration 75, loss = 0.02139328
Iteration 76, loss = 0.02146620
Iteration 77, loss = 0.02184006
Iteration 78, loss = 0.02121192
Iteration 79, loss = 0.02135932
Iteration 80, loss = 0.02151899
Iteration 81, loss = 0.02133616
Iteration 82, loss = 0.02132068
Iteration 83, loss = 0.02128781
Iteration 84, loss = 0.02152527
Iteration 85, loss = 0.02160018
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70261432
Iteration 2, loss = 0.63395319
Iteration 3, loss = 0.52782527
Iteration 4, loss = 0.40533498
Iteration 5, loss = 0.29800188
Iteration 6, loss = 0.21842587
Iteration 7, loss = 0.16389433
Iteration 8, loss = 0.12664667
Iteration 9, loss = 0.10225601
Iteration 10, loss = 0.08471769
Iteration 11, loss = 0.07259631
Iteration 12, loss = 0.06318709
Iteration 13, loss = 0.05664903
Iteration 14, loss = 0.05115793
Iteration 15, loss = 0.04710349
Iteration 16, loss = 0.04376690
Iteration 17, loss = 0.04088590
Iteration 18, loss = 0.03888690
Iteration 19, loss = 0.03711722
Iteration 20, loss = 0.03543170
Iteration 21, loss = 0.03410876
Iteration 22, loss = 0.03298757
Iteration 23, loss = 0.03204101
Iteration 24, loss = 0.03117661
Iteration 25, loss = 0.03026795
Iteration 26, loss = 0.02989494
Iteration 27, loss = 0.02900927
Iteration 28, loss = 0.02857241
Iteration 29, loss = 0.02821653
Iteration 30, loss = 0.02803244
Iteration 31, loss = 0.02770934
Iteration 32, loss = 0.02705837
Iteration 33, loss = 0.02692899
Iteration 34, loss = 0.02649781
Iteration 35, loss = 0.02609252
Iteration 36, loss = 0.02605700
Iteration 37, loss = 0.02579184
Iteration 38, loss = 0.02545987
Iteration 39, loss = 0.02532570
Iteration 40, loss = 0.02532630
Iteration 41, loss = 0.02502069
Iteration 42, loss = 0.02499386
Iteration 43, loss = 0.02491914
Iteration 44, loss = 0.02462658
Iteration 45, loss = 0.02484467
Iteration 46, loss = 0.02464916
Iteration 47, loss = 0.02457313
Iteration 48, loss = 0.02445370
Iteration 49, loss = 0.02459232
Iteration 50, loss = 0.02423669
Iteration 51, loss = 0.02431195
Iteration 52, loss = 0.02434659
Iteration 53, loss = 0.02414338
Iteration 54, loss = 0.02386866
Iteration 55, loss = 0.02375443
Iteration 56, loss = 0.02361195
Iteration 57, loss = 0.02350248
Iteration 58, loss = 0.02358147
Iteration 59, loss = 0.02358810
Iteration 60, loss = 0.02341428
Iteration 61, loss = 0.02318042
Iteration 62, loss = 0.02352089
Iteration 63, loss = 0.02328087
Iteration 64, loss = 0.02371909
Iteration 65, loss = 0.02349498
Iteration 66, loss = 0.02330249
Iteration 67, loss = 0.02328691
Iteration 68, loss = 0.02331555
Iteration 69, loss = 0.02333079
Iteration 70, loss = 0.02301818
Iteration 71, loss = 0.02306397
Iteration 72, loss = 0.02349125
Iteration 73, loss = 0.02321121
Iteration 74, loss = 0.02343910
Iteration 75, loss = 0.02304336
Iteration 76, loss = 0.02319108
Iteration 77, loss = 0.02309650
Iteration 78, loss = 0.02298624
Iteration 79, loss = 0.02317644
Iteration 80, loss = 0.02298495
Iteration 81, loss = 0.02319000
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70357417
Iteration 2, loss = 0.60687572
Iteration 3, loss = 0.48252595
Iteration 4, loss = 0.35979592
Iteration 5, loss = 0.26235219
Iteration 6, loss = 0.19285684
Iteration 7, loss = 0.14540726
Iteration 8, loss = 0.11315632
Iteration 9, loss = 0.09155219
Iteration 10, loss = 0.07634304
Iteration 11, loss = 0.06515841
Iteration 12, loss = 0.05707389
Iteration 13, loss = 0.05077862
Iteration 14, loss = 0.04616330
Iteration 15, loss = 0.04199208
Iteration 16, loss = 0.03910347
Iteration 17, loss = 0.03650341
Iteration 18, loss = 0.03414416
Iteration 19, loss = 0.03256408
Iteration 20, loss = 0.03073583
Iteration 21, loss = 0.02993810
Iteration 22, loss = 0.02869811
Iteration 23, loss = 0.02765009
Iteration 24, loss = 0.02690681
Iteration 25, loss = 0.02625840
Iteration 26, loss = 0.02576794
Iteration 27, loss = 0.02480345
Iteration 28, loss = 0.02447489
Iteration 29, loss = 0.02392672
Iteration 30, loss = 0.02370552
Iteration 31, loss = 0.02330818
Iteration 32, loss = 0.02283731
Iteration 33, loss = 0.02294479
Iteration 34, loss = 0.02237922
Iteration 35, loss = 0.02222166
Iteration 36, loss = 0.02199888
Iteration 37, loss = 0.02179487
Iteration 38, loss = 0.02169023
Iteration 39, loss = 0.02141947
Iteration 40, loss = 0.02129920
Iteration 41, loss = 0.02123120
Iteration 42, loss = 0.02113391
Iteration 43, loss = 0.02064702
Iteration 44, loss = 0.02070563
Iteration 45, loss = 0.02051424
Iteration 46, loss = 0.02065548
Iteration 47, loss = 0.02057076
Iteration 48, loss = 0.02070415
Iteration 49, loss = 0.02030455
Iteration 50, loss = 0.01988287
Iteration 51, loss = 0.02004898
Iteration 52, loss = 0.01994437
Iteration 53, loss = 0.01980838
Iteration 54, loss = 0.01983560
Iteration 55, loss = 0.01980076
Iteration 56, loss = 0.01997152
Iteration 57, loss = 0.01960722
Iteration 58, loss = 0.01974029
Iteration 59, loss = 0.01943131
Iteration 60, loss = 0.01946577
Iteration 61, loss = 0.01939631
Iteration 62, loss = 0.01970012
Iteration 63, loss = 0.01921180
Iteration 64, loss = 0.01917371
Iteration 65, loss = 0.01947600
Iteration 66, loss = 0.01944706
Iteration 67, loss = 0.01956558
Iteration 68, loss = 0.01969304
Iteration 69, loss = 0.01928246
Iteration 70, loss = 0.01940586
Iteration 71, loss = 0.01914331
Iteration 72, loss = 0.01919148
Iteration 73, loss = 0.01931630
Iteration 74, loss = 0.01908843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67255382
Iteration 2, loss = 0.59396885
Iteration 3, loss = 0.48492950
Iteration 4, loss = 0.37174055
Iteration 5, loss = 0.27442226
Iteration 6, loss = 0.20001404
Iteration 7, loss = 0.15002226
Iteration 8, loss = 0.11604354
Iteration 9, loss = 0.09318681
Iteration 10, loss = 0.07731486
Iteration 11, loss = 0.06572350
Iteration 12, loss = 0.05741628
Iteration 13, loss = 0.05087969
Iteration 14, loss = 0.04609662
Iteration 15, loss = 0.04229312
Iteration 16, loss = 0.03909403
Iteration 17, loss = 0.03648133
Iteration 18, loss = 0.03428299
Iteration 19, loss = 0.03257325
Iteration 20, loss = 0.03116287
Iteration 21, loss = 0.02976276
Iteration 22, loss = 0.02893946
Iteration 23, loss = 0.02790123
Iteration 24, loss = 0.02708185
Iteration 25, loss = 0.02640693
Iteration 26, loss = 0.02602623
Iteration 27, loss = 0.02528681
Iteration 28, loss = 0.02461079
Iteration 29, loss = 0.02430545
Iteration 30, loss = 0.02382530
Iteration 31, loss = 0.02347238
Iteration 32, loss = 0.02333434
Iteration 33, loss = 0.02292113
Iteration 34, loss = 0.02264280
Iteration 35, loss = 0.02260949
Iteration 36, loss = 0.02213977
Iteration 37, loss = 0.02191965
Iteration 38, loss = 0.02184127
Iteration 39, loss = 0.02168270
Iteration 40, loss = 0.02167544
Iteration 41, loss = 0.02127203
Iteration 42, loss = 0.02125007
Iteration 43, loss = 0.02106468
Iteration 44, loss = 0.02092598
Iteration 45, loss = 0.02087826
Iteration 46, loss = 0.02080594
Iteration 47, loss = 0.02072385
Iteration 48, loss = 0.02075633
Iteration 49, loss = 0.02043913
Iteration 50, loss = 0.02049685
Iteration 51, loss = 0.02044420
Iteration 52, loss = 0.02036229
Iteration 53, loss = 0.02011137
Iteration 54, loss = 0.02023243
Iteration 55, loss = 0.02017131
Iteration 56, loss = 0.02015803
Iteration 57, loss = 0.02003055
Iteration 58, loss = 0.02013231
Iteration 59, loss = 0.01976684
Iteration 60, loss = 0.01996309
Iteration 61, loss = 0.01984527
Iteration 62, loss = 0.01981756
Iteration 63, loss = 0.01979002
Iteration 64, loss = 0.01971791
Iteration 65, loss = 0.01975117
Iteration 66, loss = 0.01964331
Iteration 67, loss = 0.01963170
Iteration 68, loss = 0.01932971
Iteration 69, loss = 0.01959404
Iteration 70, loss = 0.01959852
Iteration 71, loss = 0.01967908
Iteration 72, loss = 0.01953696
Iteration 73, loss = 0.01937466
Iteration 74, loss = 0.01953844
Iteration 75, loss = 0.01934858
Iteration 76, loss = 0.01929362
Iteration 77, loss = 0.01928688
Iteration 78, loss = 0.01936423
Iteration 79, loss = 0.01923701
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67251691
Iteration 2, loss = 0.59168608
Iteration 3, loss = 0.47473378
Iteration 4, loss = 0.35771885
Iteration 5, loss = 0.26239713
Iteration 6, loss = 0.19327421
Iteration 7, loss = 0.14616824
Iteration 8, loss = 0.11439610
Iteration 9, loss = 0.09254662
Iteration 10, loss = 0.07718516
Iteration 11, loss = 0.06605291
Iteration 12, loss = 0.05786672
Iteration 13, loss = 0.05135540
Iteration 14, loss = 0.04690683
Iteration 15, loss = 0.04280125
Iteration 16, loss = 0.03948696
Iteration 17, loss = 0.03678409
Iteration 18, loss = 0.03489118
Iteration 19, loss = 0.03316252
Iteration 20, loss = 0.03162130
Iteration 21, loss = 0.03019906
Iteration 22, loss = 0.02929125
Iteration 23, loss = 0.02810474
Iteration 24, loss = 0.02726958
Iteration 25, loss = 0.02673354
Iteration 26, loss = 0.02623652
Iteration 27, loss = 0.02538149
Iteration 28, loss = 0.02514829
Iteration 29, loss = 0.02433693
Iteration 30, loss = 0.02408561
Iteration 31, loss = 0.02373105
Iteration 32, loss = 0.02352286
Iteration 33, loss = 0.02324443
Iteration 34, loss = 0.02287229
Iteration 35, loss = 0.02253854
Iteration 36, loss = 0.02246123
Iteration 37, loss = 0.02212557
Iteration 38, loss = 0.02200256
Iteration 39, loss = 0.02173094
Iteration 40, loss = 0.02190754
Iteration 41, loss = 0.02143015
Iteration 42, loss = 0.02157942
Iteration 43, loss = 0.02122516
Iteration 44, loss = 0.02140328
Iteration 45, loss = 0.02115360
Iteration 46, loss = 0.02098686
Iteration 47, loss = 0.02082561
Iteration 48, loss = 0.02059332
Iteration 49, loss = 0.02054442
Iteration 50, loss = 0.02078470
Iteration 51, loss = 0.02085912
Iteration 52, loss = 0.02047748
Iteration 53, loss = 0.02021323
Iteration 54, loss = 0.02021561
Iteration 55, loss = 0.02036424
Iteration 56, loss = 0.02014974
Iteration 57, loss = 0.02029736
Iteration 58, loss = 0.02022973
Iteration 59, loss = 0.02022261
Iteration 60, loss = 0.02011436
Iteration 61, loss = 0.02019674
Iteration 62, loss = 0.01963631
Iteration 63, loss = 0.01980837
Iteration 64, loss = 0.01975901
Iteration 65, loss = 0.01992847
Iteration 66, loss = 0.02001099
Iteration 67, loss = 0.01982470
Iteration 68, loss = 0.01958582
Iteration 69, loss = 0.01966906
Iteration 70, loss = 0.01985603
Iteration 71, loss = 0.01960349
Iteration 72, loss = 0.01958071
Iteration 73, loss = 0.01949045
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67540705
Iteration 2, loss = 0.60397350
Iteration 3, loss = 0.49837562
Iteration 4, loss = 0.38393836
Iteration 5, loss = 0.28460819
Iteration 6, loss = 0.20942708
Iteration 7, loss = 0.15744825
Iteration 8, loss = 0.12227189
Iteration 9, loss = 0.09862313
Iteration 10, loss = 0.08173279
Iteration 11, loss = 0.06975665
Iteration 12, loss = 0.06100124
Iteration 13, loss = 0.05440982
Iteration 14, loss = 0.04905865
Iteration 15, loss = 0.04513880
Iteration 16, loss = 0.04210834
Iteration 17, loss = 0.03916725
Iteration 18, loss = 0.03699268
Iteration 19, loss = 0.03519505
Iteration 20, loss = 0.03379424
Iteration 21, loss = 0.03217712
Iteration 22, loss = 0.03121081
Iteration 23, loss = 0.03018662
Iteration 24, loss = 0.02939793
Iteration 25, loss = 0.02885869
Iteration 26, loss = 0.02814988
Iteration 27, loss = 0.02745706
Iteration 28, loss = 0.02677052
Iteration 29, loss = 0.02630295
Iteration 30, loss = 0.02594278
Iteration 31, loss = 0.02572679
Iteration 32, loss = 0.02546122
Iteration 33, loss = 0.02501874
Iteration 34, loss = 0.02497856
Iteration 35, loss = 0.02469906
Iteration 36, loss = 0.02433131
Iteration 37, loss = 0.02408326
Iteration 38, loss = 0.02383298
Iteration 39, loss = 0.02367340
Iteration 40, loss = 0.02369683
Iteration 41, loss = 0.02352199
Iteration 42, loss = 0.02314441
Iteration 43, loss = 0.02327190
Iteration 44, loss = 0.02294948
Iteration 45, loss = 0.02315134
Iteration 46, loss = 0.02303778
Iteration 47, loss = 0.02281314
Iteration 48, loss = 0.02273637
Iteration 49, loss = 0.02276807
Iteration 50, loss = 0.02275280
Iteration 51, loss = 0.02255962
Iteration 52, loss = 0.02269195
Iteration 53, loss = 0.02243172
Iteration 54, loss = 0.02242290
Iteration 55, loss = 0.02233635
Iteration 56, loss = 0.02219052
Iteration 57, loss = 0.02209635
Iteration 58, loss = 0.02205721
Iteration 59, loss = 0.02237974
Iteration 60, loss = 0.02187438
Iteration 61, loss = 0.02207571
Iteration 62, loss = 0.02232483
Iteration 63, loss = 0.02176384
Iteration 64, loss = 0.02172827
Iteration 65, loss = 0.02172104
Iteration 66, loss = 0.02194838
Iteration 67, loss = 0.02178182
Iteration 68, loss = 0.02214132
Iteration 69, loss = 0.02168402
Iteration 70, loss = 0.02157922
Iteration 71, loss = 0.02153945
Iteration 72, loss = 0.02160320
Iteration 73, loss = 0.02181148
Iteration 74, loss = 0.02126889
Iteration 75, loss = 0.02139328
Iteration 76, loss = 0.02146620
Iteration 77, loss = 0.02184006
Iteration 78, loss = 0.02121192
Iteration 79, loss = 0.02135932
Iteration 80, loss = 0.02151899
Iteration 81, loss = 0.02133616
Iteration 82, loss = 0.02132068
Iteration 83, loss = 0.02128781
Iteration 84, loss = 0.02152527
Iteration 85, loss = 0.02160018
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70261432
Iteration 2, loss = 0.63395319
Iteration 3, loss = 0.52782527
Iteration 4, loss = 0.40533498
Iteration 5, loss = 0.29800188
Iteration 6, loss = 0.21842587
Iteration 7, loss = 0.16389433
Iteration 8, loss = 0.12664667
Iteration 9, loss = 0.10225601
Iteration 10, loss = 0.08471769
Iteration 11, loss = 0.07259631
Iteration 12, loss = 0.06318709
Iteration 13, loss = 0.05664903
Iteration 14, loss = 0.05115793
Iteration 15, loss = 0.04710349
Iteration 16, loss = 0.04376690
Iteration 17, loss = 0.04088590
Iteration 18, loss = 0.03888690
Iteration 19, loss = 0.03711722
Iteration 20, loss = 0.03543170
Iteration 21, loss = 0.03410876
Iteration 22, loss = 0.03298757
Iteration 23, loss = 0.03204101
Iteration 24, loss = 0.03117661
Iteration 25, loss = 0.03026795
Iteration 26, loss = 0.02989494
Iteration 27, loss = 0.02900927
Iteration 28, loss = 0.02857241
Iteration 29, loss = 0.02821653
Iteration 30, loss = 0.02803244
Iteration 31, loss = 0.02770934
Iteration 32, loss = 0.02705837
Iteration 33, loss = 0.02692899
Iteration 34, loss = 0.02649781
Iteration 35, loss = 0.02609252
Iteration 36, loss = 0.02605700
Iteration 37, loss = 0.02579184
Iteration 38, loss = 0.02545987
Iteration 39, loss = 0.02532570
Iteration 40, loss = 0.02532630
Iteration 41, loss = 0.02502069
Iteration 42, loss = 0.02499386
Iteration 43, loss = 0.02491914
Iteration 44, loss = 0.02462658
Iteration 45, loss = 0.02484467
Iteration 46, loss = 0.02464916
Iteration 47, loss = 0.02457313
Iteration 48, loss = 0.02445370
Iteration 49, loss = 0.02459232
Iteration 50, loss = 0.02423669
Iteration 51, loss = 0.02431195
Iteration 52, loss = 0.02434659
Iteration 53, loss = 0.02414338
Iteration 54, loss = 0.02386866
Iteration 55, loss = 0.02375443
Iteration 56, loss = 0.02361195
Iteration 57, loss = 0.02350248
Iteration 58, loss = 0.02358147
Iteration 59, loss = 0.02358810
Iteration 60, loss = 0.02341428
Iteration 61, loss = 0.02318042
Iteration 62, loss = 0.02352089
Iteration 63, loss = 0.02328087
Iteration 64, loss = 0.02371909
Iteration 65, loss = 0.02349498
Iteration 66, loss = 0.02330249
Iteration 67, loss = 0.02328691
Iteration 68, loss = 0.02331555
Iteration 69, loss = 0.02333079
Iteration 70, loss = 0.02301818
Iteration 71, loss = 0.02306397
Iteration 72, loss = 0.02349125
Iteration 73, loss = 0.02321121
Iteration 74, loss = 0.02343910
Iteration 75, loss = 0.02304336
Iteration 76, loss = 0.02319108
Iteration 77, loss = 0.02309650
Iteration 78, loss = 0.02298624
Iteration 79, loss = 0.02317644
Iteration 80, loss = 0.02298495
Iteration 81, loss = 0.02319000
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70357417
Iteration 2, loss = 0.60687572
Iteration 3, loss = 0.48252595
Iteration 4, loss = 0.35979592
Iteration 5, loss = 0.26235219
Iteration 6, loss = 0.19285684
Iteration 7, loss = 0.14540726
Iteration 8, loss = 0.11315632
Iteration 9, loss = 0.09155219
Iteration 10, loss = 0.07634304
Iteration 11, loss = 0.06515841
Iteration 12, loss = 0.05707389
Iteration 13, loss = 0.05077862
Iteration 14, loss = 0.04616330
Iteration 15, loss = 0.04199208
Iteration 16, loss = 0.03910347
Iteration 17, loss = 0.03650341
Iteration 18, loss = 0.03414416
Iteration 19, loss = 0.03256408
Iteration 20, loss = 0.03073583
Iteration 21, loss = 0.02993810
Iteration 22, loss = 0.02869811
Iteration 23, loss = 0.02765009
Iteration 24, loss = 0.02690681
Iteration 25, loss = 0.02625840
Iteration 26, loss = 0.02576794
Iteration 27, loss = 0.02480345
Iteration 28, loss = 0.02447489
Iteration 29, loss = 0.02392672
Iteration 30, loss = 0.02370552
Iteration 31, loss = 0.02330818
Iteration 32, loss = 0.02283731
Iteration 33, loss = 0.02294479
Iteration 34, loss = 0.02237922
Iteration 35, loss = 0.02222166
Iteration 36, loss = 0.02199888
Iteration 37, loss = 0.02179487
Iteration 38, loss = 0.02169023
Iteration 39, loss = 0.02141947
Iteration 40, loss = 0.02129920
Iteration 41, loss = 0.02123120
Iteration 42, loss = 0.02113391
Iteration 43, loss = 0.02064702
Iteration 44, loss = 0.02070563
Iteration 45, loss = 0.02051424
Iteration 46, loss = 0.02065548
Iteration 47, loss = 0.02057076
Iteration 48, loss = 0.02070415
Iteration 49, loss = 0.02030455
Iteration 50, loss = 0.01988287
Iteration 51, loss = 0.02004898
Iteration 52, loss = 0.01994437
Iteration 53, loss = 0.01980838
Iteration 54, loss = 0.01983560
Iteration 55, loss = 0.01980076
Iteration 56, loss = 0.01997152
Iteration 57, loss = 0.01960722
Iteration 58, loss = 0.01974029
Iteration 59, loss = 0.01943131
Iteration 60, loss = 0.01946577
Iteration 61, loss = 0.01939631
Iteration 62, loss = 0.01970012
Iteration 63, loss = 0.01921180
Iteration 64, loss = 0.01917371
Iteration 65, loss = 0.01947600
Iteration 66, loss = 0.01944706
Iteration 67, loss = 0.01956558
Iteration 68, loss = 0.01969304
Iteration 69, loss = 0.01928246
Iteration 70, loss = 0.01940586
Iteration 71, loss = 0.01914331
Iteration 72, loss = 0.01919148
Iteration 73, loss = 0.01931630
Iteration 74, loss = 0.01908843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67255382
Iteration 2, loss = 0.59396885
Iteration 3, loss = 0.48492950
Iteration 4, loss = 0.37174055
Iteration 5, loss = 0.27442226
Iteration 6, loss = 0.20001404
Iteration 7, loss = 0.15002226
Iteration 8, loss = 0.11604354
Iteration 9, loss = 0.09318681
Iteration 10, loss = 0.07731486
Iteration 11, loss = 0.06572350
Iteration 12, loss = 0.05741628
Iteration 13, loss = 0.05087969
Iteration 14, loss = 0.04609662
Iteration 15, loss = 0.04229312
Iteration 16, loss = 0.03909403
Iteration 17, loss = 0.03648133
Iteration 18, loss = 0.03428299
Iteration 19, loss = 0.03257325
Iteration 20, loss = 0.03116287
Iteration 21, loss = 0.02976276
Iteration 22, loss = 0.02893946
Iteration 23, loss = 0.02790123
Iteration 24, loss = 0.02708185
Iteration 25, loss = 0.02640693
Iteration 26, loss = 0.02602623
Iteration 27, loss = 0.02528681
Iteration 28, loss = 0.02461079
Iteration 29, loss = 0.02430545
Iteration 30, loss = 0.02382530
Iteration 31, loss = 0.02347238
Iteration 32, loss = 0.02333434
Iteration 33, loss = 0.02292113
Iteration 34, loss = 0.02264280
Iteration 35, loss = 0.02260949
Iteration 36, loss = 0.02213977
Iteration 37, loss = 0.02191965
Iteration 38, loss = 0.02184127
Iteration 39, loss = 0.02168270
Iteration 40, loss = 0.02167544
Iteration 41, loss = 0.02127203
Iteration 42, loss = 0.02125007
Iteration 43, loss = 0.02106468
Iteration 44, loss = 0.02092598
Iteration 45, loss = 0.02087826
Iteration 46, loss = 0.02080594
Iteration 47, loss = 0.02072385
Iteration 48, loss = 0.02075633
Iteration 49, loss = 0.02043913
Iteration 50, loss = 0.02049685
Iteration 51, loss = 0.02044420
Iteration 52, loss = 0.02036229
Iteration 53, loss = 0.02011137
Iteration 54, loss = 0.02023243
Iteration 55, loss = 0.02017131
Iteration 56, loss = 0.02015803
Iteration 57, loss = 0.02003055
Iteration 58, loss = 0.02013231
Iteration 59, loss = 0.01976684
Iteration 60, loss = 0.01996309
Iteration 61, loss = 0.01984527
Iteration 62, loss = 0.01981756
Iteration 63, loss = 0.01979002
Iteration 64, loss = 0.01971791
Iteration 65, loss = 0.01975117
Iteration 66, loss = 0.01964331
Iteration 67, loss = 0.01963170
Iteration 68, loss = 0.01932971
Iteration 69, loss = 0.01959404
Iteration 70, loss = 0.01959852
Iteration 71, loss = 0.01967908
Iteration 72, loss = 0.01953696
Iteration 73, loss = 0.01937466
Iteration 74, loss = 0.01953844
Iteration 75, loss = 0.01934858
Iteration 76, loss = 0.01929362
Iteration 77, loss = 0.01928688
Iteration 78, loss = 0.01936423
Iteration 79, loss = 0.01923701
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67251691
Iteration 2, loss = 0.59168608
Iteration 3, loss = 0.47473378
Iteration 4, loss = 0.35771885
Iteration 5, loss = 0.26239713
Iteration 6, loss = 0.19327421
Iteration 7, loss = 0.14616824
Iteration 8, loss = 0.11439610
Iteration 9, loss = 0.09254662
Iteration 10, loss = 0.07718516
Iteration 11, loss = 0.06605291
Iteration 12, loss = 0.05786672
Iteration 13, loss = 0.05135540
Iteration 14, loss = 0.04690683
Iteration 15, loss = 0.04280125
Iteration 16, loss = 0.03948696
Iteration 17, loss = 0.03678409
Iteration 18, loss = 0.03489118
Iteration 19, loss = 0.03316252
Iteration 20, loss = 0.03162130
Iteration 21, loss = 0.03019906
Iteration 22, loss = 0.02929125
Iteration 23, loss = 0.02810474
Iteration 24, loss = 0.02726958
Iteration 25, loss = 0.02673354
Iteration 26, loss = 0.02623652
Iteration 27, loss = 0.02538149
Iteration 28, loss = 0.02514829
Iteration 29, loss = 0.02433693
Iteration 30, loss = 0.02408561
Iteration 31, loss = 0.02373105
Iteration 32, loss = 0.02352286
Iteration 33, loss = 0.02324443
Iteration 34, loss = 0.02287229
Iteration 35, loss = 0.02253854
Iteration 36, loss = 0.02246123
Iteration 37, loss = 0.02212557
Iteration 38, loss = 0.02200256
Iteration 39, loss = 0.02173094
Iteration 40, loss = 0.02190754
Iteration 41, loss = 0.02143015
Iteration 42, loss = 0.02157942
Iteration 43, loss = 0.02122516
Iteration 44, loss = 0.02140328
Iteration 45, loss = 0.02115360
Iteration 46, loss = 0.02098686
Iteration 47, loss = 0.02082561
Iteration 48, loss = 0.02059332
Iteration 49, loss = 0.02054442
Iteration 50, loss = 0.02078470
Iteration 51, loss = 0.02085912
Iteration 52, loss = 0.02047748
Iteration 53, loss = 0.02021323
Iteration 54, loss = 0.02021561
Iteration 55, loss = 0.02036424
Iteration 56, loss = 0.02014974
Iteration 57, loss = 0.02029736
Iteration 58, loss = 0.02022973
Iteration 59, loss = 0.02022261
Iteration 60, loss = 0.02011436
Iteration 61, loss = 0.02019674
Iteration 62, loss = 0.01963631
Iteration 63, loss = 0.01980837
Iteration 64, loss = 0.01975901
Iteration 65, loss = 0.01992847
Iteration 66, loss = 0.02001099
Iteration 67, loss = 0.01982470
Iteration 68, loss = 0.01958582
Iteration 69, loss = 0.01966906
Iteration 70, loss = 0.01985603
Iteration 71, loss = 0.01960349
Iteration 72, loss = 0.01958071
Iteration 73, loss = 0.01949045
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67540705
Iteration 2, loss = 0.60397350
Iteration 3, loss = 0.49837562
Iteration 4, loss = 0.38393836
Iteration 5, loss = 0.28460819
Iteration 6, loss = 0.20942708
Iteration 7, loss = 0.15744825
Iteration 8, loss = 0.12227189
Iteration 9, loss = 0.09862313
Iteration 10, loss = 0.08173279
Iteration 11, loss = 0.06975665
Iteration 12, loss = 0.06100124
Iteration 13, loss = 0.05440982
Iteration 14, loss = 0.04905865
Iteration 15, loss = 0.04513880
Iteration 16, loss = 0.04210834
Iteration 17, loss = 0.03916725
Iteration 18, loss = 0.03699268
Iteration 19, loss = 0.03519505
Iteration 20, loss = 0.03379424
Iteration 21, loss = 0.03217712
Iteration 22, loss = 0.03121081
Iteration 23, loss = 0.03018662
Iteration 24, loss = 0.02939793
Iteration 25, loss = 0.02885869
Iteration 26, loss = 0.02814988
Iteration 27, loss = 0.02745706
Iteration 28, loss = 0.02677052
Iteration 29, loss = 0.02630295
Iteration 30, loss = 0.02594278
Iteration 31, loss = 0.02572679
Iteration 32, loss = 0.02546122
Iteration 33, loss = 0.02501874
Iteration 34, loss = 0.02497856
Iteration 35, loss = 0.02469906
Iteration 36, loss = 0.02433131
Iteration 37, loss = 0.02408326
Iteration 38, loss = 0.02383298
Iteration 39, loss = 0.02367340
Iteration 40, loss = 0.02369683
Iteration 41, loss = 0.02352199
Iteration 42, loss = 0.02314441
Iteration 43, loss = 0.02327190
Iteration 44, loss = 0.02294948
Iteration 45, loss = 0.02315134
Iteration 46, loss = 0.02303778
Iteration 47, loss = 0.02281314
Iteration 48, loss = 0.02273637
Iteration 49, loss = 0.02276807
Iteration 50, loss = 0.02275280
Iteration 51, loss = 0.02255962
Iteration 52, loss = 0.02269195
Iteration 53, loss = 0.02243172
Iteration 54, loss = 0.02242290
Iteration 55, loss = 0.02233635
Iteration 56, loss = 0.02219052
Iteration 57, loss = 0.02209635
Iteration 58, loss = 0.02205721
Iteration 59, loss = 0.02237974
Iteration 60, loss = 0.02187438
Iteration 61, loss = 0.02207571
Iteration 62, loss = 0.02232483
Iteration 63, loss = 0.02176384
Iteration 64, loss = 0.02172827
Iteration 65, loss = 0.02172104
Iteration 66, loss = 0.02194838
Iteration 67, loss = 0.02178182
Iteration 68, loss = 0.02214132
Iteration 69, loss = 0.02168402
Iteration 70, loss = 0.02157922
Iteration 71, loss = 0.02153945
Iteration 72, loss = 0.02160320
Iteration 73, loss = 0.02181148
Iteration 74, loss = 0.02126889
Iteration 75, loss = 0.02139328
Iteration 76, loss = 0.02146620
Iteration 77, loss = 0.02184006
Iteration 78, loss = 0.02121192
Iteration 79, loss = 0.02135932
Iteration 80, loss = 0.02151899
Iteration 81, loss = 0.02133616
Iteration 82, loss = 0.02132068
Iteration 83, loss = 0.02128781
Iteration 84, loss = 0.02152527
Iteration 85, loss = 0.02160018
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70261432
Iteration 2, loss = 0.63395319
Iteration 3, loss = 0.52782527
Iteration 4, loss = 0.40533498
Iteration 5, loss = 0.29800188
Iteration 6, loss = 0.21842587
Iteration 7, loss = 0.16389433
Iteration 8, loss = 0.12664667
Iteration 9, loss = 0.10225601
Iteration 10, loss = 0.08471769
Iteration 11, loss = 0.07259631
Iteration 12, loss = 0.06318709
Iteration 13, loss = 0.05664903
Iteration 14, loss = 0.05115793
Iteration 15, loss = 0.04710349
Iteration 16, loss = 0.04376690
Iteration 17, loss = 0.04088590
Iteration 18, loss = 0.03888690
Iteration 19, loss = 0.03711722
Iteration 20, loss = 0.03543170
Iteration 21, loss = 0.03410876
Iteration 22, loss = 0.03298757
Iteration 23, loss = 0.03204101
Iteration 24, loss = 0.03117661
Iteration 25, loss = 0.03026795
Iteration 26, loss = 0.02989494
Iteration 27, loss = 0.02900927
Iteration 28, loss = 0.02857241
Iteration 29, loss = 0.02821653
Iteration 30, loss = 0.02803244
Iteration 31, loss = 0.02770934
Iteration 32, loss = 0.02705837
Iteration 33, loss = 0.02692899
Iteration 34, loss = 0.02649781
Iteration 35, loss = 0.02609252
Iteration 36, loss = 0.02605700
Iteration 37, loss = 0.02579184
Iteration 38, loss = 0.02545987
Iteration 39, loss = 0.02532570
Iteration 40, loss = 0.02532630
Iteration 41, loss = 0.02502069
Iteration 42, loss = 0.02499386
Iteration 43, loss = 0.02491914
Iteration 44, loss = 0.02462658
Iteration 45, loss = 0.02484467
Iteration 46, loss = 0.02464916
Iteration 47, loss = 0.02457313
Iteration 48, loss = 0.02445370
Iteration 49, loss = 0.02459232
Iteration 50, loss = 0.02423669
Iteration 51, loss = 0.02431195
Iteration 52, loss = 0.02434659
Iteration 53, loss = 0.02414338
Iteration 54, loss = 0.02386866
Iteration 55, loss = 0.02375443
Iteration 56, loss = 0.02361195
Iteration 57, loss = 0.02350248
Iteration 58, loss = 0.02358147
Iteration 59, loss = 0.02358810
Iteration 60, loss = 0.02341428
Iteration 61, loss = 0.02318042
Iteration 62, loss = 0.02352089
Iteration 63, loss = 0.02328087
Iteration 64, loss = 0.02371909
Iteration 65, loss = 0.02349498
Iteration 66, loss = 0.02330249
Iteration 67, loss = 0.02328691
Iteration 68, loss = 0.02331555
Iteration 69, loss = 0.02333079
Iteration 70, loss = 0.02301818
Iteration 71, loss = 0.02306397
Iteration 72, loss = 0.02349125
Iteration 73, loss = 0.02321121
Iteration 74, loss = 0.02343910
Iteration 75, loss = 0.02304336
Iteration 76, loss = 0.02319108
Iteration 77, loss = 0.02309650
Iteration 78, loss = 0.02298624
Iteration 79, loss = 0.02317644
Iteration 80, loss = 0.02298495
Iteration 81, loss = 0.02319000
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70357417
Iteration 2, loss = 0.60687572
Iteration 3, loss = 0.48252595
Iteration 4, loss = 0.35979592
Iteration 5, loss = 0.26235219
Iteration 6, loss = 0.19285684
Iteration 7, loss = 0.14540726
Iteration 8, loss = 0.11315632
Iteration 9, loss = 0.09155219
Iteration 10, loss = 0.07634304
Iteration 11, loss = 0.06515841
Iteration 12, loss = 0.05707389
Iteration 13, loss = 0.05077862
Iteration 14, loss = 0.04616330
Iteration 15, loss = 0.04199208
Iteration 16, loss = 0.03910347
Iteration 17, loss = 0.03650341
Iteration 18, loss = 0.03414416
Iteration 19, loss = 0.03256408
Iteration 20, loss = 0.03073583
Iteration 21, loss = 0.02993810
Iteration 22, loss = 0.02869811
Iteration 23, loss = 0.02765009
Iteration 24, loss = 0.02690681
Iteration 25, loss = 0.02625840
Iteration 26, loss = 0.02576794
Iteration 27, loss = 0.02480345
Iteration 28, loss = 0.02447489
Iteration 29, loss = 0.02392672
Iteration 30, loss = 0.02370552
Iteration 31, loss = 0.02330818
Iteration 32, loss = 0.02283731
Iteration 33, loss = 0.02294479
Iteration 34, loss = 0.02237922
Iteration 35, loss = 0.02222166
Iteration 36, loss = 0.02199888
Iteration 37, loss = 0.02179487
Iteration 38, loss = 0.02169023
Iteration 39, loss = 0.02141947
Iteration 40, loss = 0.02129920
Iteration 41, loss = 0.02123120
Iteration 42, loss = 0.02113391
Iteration 43, loss = 0.02064702
Iteration 44, loss = 0.02070563
Iteration 45, loss = 0.02051424
Iteration 46, loss = 0.02065548
Iteration 47, loss = 0.02057076
Iteration 48, loss = 0.02070415
Iteration 49, loss = 0.02030455
Iteration 50, loss = 0.01988287
Iteration 51, loss = 0.02004898
Iteration 52, loss = 0.01994437
Iteration 53, loss = 0.01980838
Iteration 54, loss = 0.01983560
Iteration 55, loss = 0.01980076
Iteration 56, loss = 0.01997152
Iteration 57, loss = 0.01960722
Iteration 58, loss = 0.01974029
Iteration 59, loss = 0.01943131
Iteration 60, loss = 0.01946577
Iteration 61, loss = 0.01939631
Iteration 62, loss = 0.01970012
Iteration 63, loss = 0.01921180
Iteration 64, loss = 0.01917371
Iteration 65, loss = 0.01947600
Iteration 66, loss = 0.01944706
Iteration 67, loss = 0.01956558
Iteration 68, loss = 0.01969304
Iteration 69, loss = 0.01928246
Iteration 70, loss = 0.01940586
Iteration 71, loss = 0.01914331
Iteration 72, loss = 0.01919148
Iteration 73, loss = 0.01931630
Iteration 74, loss = 0.01908843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67255382
Iteration 2, loss = 0.59396885
Iteration 3, loss = 0.48492950
Iteration 4, loss = 0.37174055
Iteration 5, loss = 0.27442226
Iteration 6, loss = 0.20001404
Iteration 7, loss = 0.15002226
Iteration 8, loss = 0.11604354
Iteration 9, loss = 0.09318681
Iteration 10, loss = 0.07731486
Iteration 11, loss = 0.06572350
Iteration 12, loss = 0.05741628
Iteration 13, loss = 0.05087969
Iteration 14, loss = 0.04609662
Iteration 15, loss = 0.04229312
Iteration 16, loss = 0.03909403
Iteration 17, loss = 0.03648133
Iteration 18, loss = 0.03428299
Iteration 19, loss = 0.03257325
Iteration 20, loss = 0.03116287
Iteration 21, loss = 0.02976276
Iteration 22, loss = 0.02893946
Iteration 23, loss = 0.02790123
Iteration 24, loss = 0.02708185
Iteration 25, loss = 0.02640693
Iteration 26, loss = 0.02602623
Iteration 27, loss = 0.02528681
Iteration 28, loss = 0.02461079
Iteration 29, loss = 0.02430545
Iteration 30, loss = 0.02382530
Iteration 31, loss = 0.02347238
Iteration 32, loss = 0.02333434
Iteration 33, loss = 0.02292113
Iteration 34, loss = 0.02264280
Iteration 35, loss = 0.02260949
Iteration 36, loss = 0.02213977
Iteration 37, loss = 0.02191965
Iteration 38, loss = 0.02184127
Iteration 39, loss = 0.02168270
Iteration 40, loss = 0.02167544
Iteration 41, loss = 0.02127203
Iteration 42, loss = 0.02125007
Iteration 43, loss = 0.02106468
Iteration 44, loss = 0.02092598
Iteration 45, loss = 0.02087826
Iteration 46, loss = 0.02080594
Iteration 47, loss = 0.02072385
Iteration 48, loss = 0.02075633
Iteration 49, loss = 0.02043913
Iteration 50, loss = 0.02049685
Iteration 51, loss = 0.02044420
Iteration 52, loss = 0.02036229
Iteration 53, loss = 0.02011137
Iteration 54, loss = 0.02023243
Iteration 55, loss = 0.02017131
Iteration 56, loss = 0.02015803
Iteration 57, loss = 0.02003055
Iteration 58, loss = 0.02013231
Iteration 59, loss = 0.01976684
Iteration 60, loss = 0.01996309
Iteration 61, loss = 0.01984527
Iteration 62, loss = 0.01981756
Iteration 63, loss = 0.01979002
Iteration 64, loss = 0.01971791
Iteration 65, loss = 0.01975117
Iteration 66, loss = 0.01964331
Iteration 67, loss = 0.01963170
Iteration 68, loss = 0.01932971
Iteration 69, loss = 0.01959404
Iteration 70, loss = 0.01959852
Iteration 71, loss = 0.01967908
Iteration 72, loss = 0.01953696
Iteration 73, loss = 0.01937466
Iteration 74, loss = 0.01953844
Iteration 75, loss = 0.01934858
Iteration 76, loss = 0.01929362
Iteration 77, loss = 0.01928688
Iteration 78, loss = 0.01936423
Iteration 79, loss = 0.01923701
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67251691
Iteration 2, loss = 0.59168608
Iteration 3, loss = 0.47473378
Iteration 4, loss = 0.35771885
Iteration 5, loss = 0.26239713
Iteration 6, loss = 0.19327421
Iteration 7, loss = 0.14616824
Iteration 8, loss = 0.11439610
Iteration 9, loss = 0.09254662
Iteration 10, loss = 0.07718516
Iteration 11, loss = 0.06605291
Iteration 12, loss = 0.05786672
Iteration 13, loss = 0.05135540
Iteration 14, loss = 0.04690683
Iteration 15, loss = 0.04280125
Iteration 16, loss = 0.03948696
Iteration 17, loss = 0.03678409
Iteration 18, loss = 0.03489118
Iteration 19, loss = 0.03316252
Iteration 20, loss = 0.03162130
Iteration 21, loss = 0.03019906
Iteration 22, loss = 0.02929125
Iteration 23, loss = 0.02810474
Iteration 24, loss = 0.02726958
Iteration 25, loss = 0.02673354
Iteration 26, loss = 0.02623652
Iteration 27, loss = 0.02538149
Iteration 28, loss = 0.02514829
Iteration 29, loss = 0.02433693
Iteration 30, loss = 0.02408561
Iteration 31, loss = 0.02373105
Iteration 32, loss = 0.02352286
Iteration 33, loss = 0.02324443
Iteration 34, loss = 0.02287229
Iteration 35, loss = 0.02253854
Iteration 36, loss = 0.02246123
Iteration 37, loss = 0.02212557
Iteration 38, loss = 0.02200256
Iteration 39, loss = 0.02173094
Iteration 40, loss = 0.02190754
Iteration 41, loss = 0.02143015
Iteration 42, loss = 0.02157942
Iteration 43, loss = 0.02122516
Iteration 44, loss = 0.02140328
Iteration 45, loss = 0.02115360
Iteration 46, loss = 0.02098686
Iteration 47, loss = 0.02082561
Iteration 48, loss = 0.02059332
Iteration 49, loss = 0.02054442
Iteration 50, loss = 0.02078470
Iteration 51, loss = 0.02085912
Iteration 52, loss = 0.02047748
Iteration 53, loss = 0.02021323
Iteration 54, loss = 0.02021561
Iteration 55, loss = 0.02036424
Iteration 56, loss = 0.02014974
Iteration 57, loss = 0.02029736
Iteration 58, loss = 0.02022973
Iteration 59, loss = 0.02022261
Iteration 60, loss = 0.02011436
Iteration 61, loss = 0.02019674
Iteration 62, loss = 0.01963631
Iteration 63, loss = 0.01980837
Iteration 64, loss = 0.01975901
Iteration 65, loss = 0.01992847
Iteration 66, loss = 0.02001099
Iteration 67, loss = 0.01982470
Iteration 68, loss = 0.01958582
Iteration 69, loss = 0.01966906
Iteration 70, loss = 0.01985603
Iteration 71, loss = 0.01960349
Iteration 72, loss = 0.01958071
Iteration 73, loss = 0.01949045
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67540705
Iteration 2, loss = 0.60397350
Iteration 3, loss = 0.49837562
Iteration 4, loss = 0.38393836
Iteration 5, loss = 0.28460819
Iteration 6, loss = 0.20942708
Iteration 7, loss = 0.15744825
Iteration 8, loss = 0.12227189
Iteration 9, loss = 0.09862313
Iteration 10, loss = 0.08173279
Iteration 11, loss = 0.06975665
Iteration 12, loss = 0.06100124
Iteration 13, loss = 0.05440982
Iteration 14, loss = 0.04905865
Iteration 15, loss = 0.04513880
Iteration 16, loss = 0.04210834
Iteration 17, loss = 0.03916725
Iteration 18, loss = 0.03699268
Iteration 19, loss = 0.03519505
Iteration 20, loss = 0.03379424
Iteration 21, loss = 0.03217712
Iteration 22, loss = 0.03121081
Iteration 23, loss = 0.03018662
Iteration 24, loss = 0.02939793
Iteration 25, loss = 0.02885869
Iteration 26, loss = 0.02814988
Iteration 27, loss = 0.02745706
Iteration 28, loss = 0.02677052
Iteration 29, loss = 0.02630295
Iteration 30, loss = 0.02594278
Iteration 31, loss = 0.02572679
Iteration 32, loss = 0.02546122
Iteration 33, loss = 0.02501874
Iteration 34, loss = 0.02497856
Iteration 35, loss = 0.02469906
Iteration 36, loss = 0.02433131
Iteration 37, loss = 0.02408326
Iteration 38, loss = 0.02383298
Iteration 39, loss = 0.02367340
Iteration 40, loss = 0.02369683
Iteration 41, loss = 0.02352199
Iteration 42, loss = 0.02314441
Iteration 43, loss = 0.02327190
Iteration 44, loss = 0.02294948
Iteration 45, loss = 0.02315134
Iteration 46, loss = 0.02303778
Iteration 47, loss = 0.02281314
Iteration 48, loss = 0.02273637
Iteration 49, loss = 0.02276807
Iteration 50, loss = 0.02275280
Iteration 51, loss = 0.02255962
Iteration 52, loss = 0.02269195
Iteration 53, loss = 0.02243172
Iteration 54, loss = 0.02242290
Iteration 55, loss = 0.02233635
Iteration 56, loss = 0.02219052
Iteration 57, loss = 0.02209635
Iteration 58, loss = 0.02205721
Iteration 59, loss = 0.02237974
Iteration 60, loss = 0.02187438
Iteration 61, loss = 0.02207571
Iteration 62, loss = 0.02232483
Iteration 63, loss = 0.02176384
Iteration 64, loss = 0.02172827
Iteration 65, loss = 0.02172104
Iteration 66, loss = 0.02194838
Iteration 67, loss = 0.02178182
Iteration 68, loss = 0.02214132
Iteration 69, loss = 0.02168402
Iteration 70, loss = 0.02157922
Iteration 71, loss = 0.02153945
Iteration 72, loss = 0.02160320
Iteration 73, loss = 0.02181148
Iteration 74, loss = 0.02126889
Iteration 75, loss = 0.02139328
Iteration 76, loss = 0.02146620
Iteration 77, loss = 0.02184006
Iteration 78, loss = 0.02121192
Iteration 79, loss = 0.02135932
Iteration 80, loss = 0.02151899
Iteration 81, loss = 0.02133616
Iteration 82, loss = 0.02132068
Iteration 83, loss = 0.02128781
Iteration 84, loss = 0.02152527
Iteration 85, loss = 0.02160018
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70261432
Iteration 2, loss = 0.63395319
Iteration 3, loss = 0.52782527
Iteration 4, loss = 0.40533498
Iteration 5, loss = 0.29800188
Iteration 6, loss = 0.21842587
Iteration 7, loss = 0.16389433
Iteration 8, loss = 0.12664667
Iteration 9, loss = 0.10225601
Iteration 10, loss = 0.08471769
Iteration 11, loss = 0.07259631
Iteration 12, loss = 0.06318709
Iteration 13, loss = 0.05664903
Iteration 14, loss = 0.05115793
Iteration 15, loss = 0.04710349
Iteration 16, loss = 0.04376690
Iteration 17, loss = 0.04088590
Iteration 18, loss = 0.03888690
Iteration 19, loss = 0.03711722
Iteration 20, loss = 0.03543170
Iteration 21, loss = 0.03410876
Iteration 22, loss = 0.03298757
Iteration 23, loss = 0.03204101
Iteration 24, loss = 0.03117661
Iteration 25, loss = 0.03026795
Iteration 26, loss = 0.02989494
Iteration 27, loss = 0.02900927
Iteration 28, loss = 0.02857241
Iteration 29, loss = 0.02821653
Iteration 30, loss = 0.02803244
Iteration 31, loss = 0.02770934
Iteration 32, loss = 0.02705837
Iteration 33, loss = 0.02692899
Iteration 34, loss = 0.02649781
Iteration 35, loss = 0.02609252
Iteration 36, loss = 0.02605700
Iteration 37, loss = 0.02579184
Iteration 38, loss = 0.02545987
Iteration 39, loss = 0.02532570
Iteration 40, loss = 0.02532630
Iteration 41, loss = 0.02502069
Iteration 42, loss = 0.02499386
Iteration 43, loss = 0.02491914
Iteration 44, loss = 0.02462658
Iteration 45, loss = 0.02484467
Iteration 46, loss = 0.02464916
Iteration 47, loss = 0.02457313
Iteration 48, loss = 0.02445370
Iteration 49, loss = 0.02459232
Iteration 50, loss = 0.02423669
Iteration 51, loss = 0.02431195
Iteration 52, loss = 0.02434659
Iteration 53, loss = 0.02414338
Iteration 54, loss = 0.02386866
Iteration 55, loss = 0.02375443
Iteration 56, loss = 0.02361195
Iteration 57, loss = 0.02350248
Iteration 58, loss = 0.02358147
Iteration 59, loss = 0.02358810
Iteration 60, loss = 0.02341428
Iteration 61, loss = 0.02318042
Iteration 62, loss = 0.02352089
Iteration 63, loss = 0.02328087
Iteration 64, loss = 0.02371909
Iteration 65, loss = 0.02349498
Iteration 66, loss = 0.02330249
Iteration 67, loss = 0.02328691
Iteration 68, loss = 0.02331555
Iteration 69, loss = 0.02333079
Iteration 70, loss = 0.02301818
Iteration 71, loss = 0.02306397
Iteration 72, loss = 0.02349125
Iteration 73, loss = 0.02321121
Iteration 74, loss = 0.02343910
Iteration 75, loss = 0.02304336
Iteration 76, loss = 0.02319108
Iteration 77, loss = 0.02309650
Iteration 78, loss = 0.02298624
Iteration 79, loss = 0.02317644
Iteration 80, loss = 0.02298495
Iteration 81, loss = 0.02319000
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70357417
Iteration 2, loss = 0.60687572
Iteration 3, loss = 0.48252595
Iteration 4, loss = 0.35979592
Iteration 5, loss = 0.26235219
Iteration 6, loss = 0.19285684
Iteration 7, loss = 0.14540726
Iteration 8, loss = 0.11315632
Iteration 9, loss = 0.09155219
Iteration 10, loss = 0.07634304
Iteration 11, loss = 0.06515841
Iteration 12, loss = 0.05707389
Iteration 13, loss = 0.05077862
Iteration 14, loss = 0.04616330
Iteration 15, loss = 0.04199208
Iteration 16, loss = 0.03910347
Iteration 17, loss = 0.03650341
Iteration 18, loss = 0.03414416
Iteration 19, loss = 0.03256408
Iteration 20, loss = 0.03073583
Iteration 21, loss = 0.02993810
Iteration 22, loss = 0.02869811
Iteration 23, loss = 0.02765009
Iteration 24, loss = 0.02690681
Iteration 25, loss = 0.02625840
Iteration 26, loss = 0.02576794
Iteration 27, loss = 0.02480345
Iteration 28, loss = 0.02447489
Iteration 29, loss = 0.02392672
Iteration 30, loss = 0.02370552
Iteration 31, loss = 0.02330818
Iteration 32, loss = 0.02283731
Iteration 33, loss = 0.02294479
Iteration 34, loss = 0.02237922
Iteration 35, loss = 0.02222166
Iteration 36, loss = 0.02199888
Iteration 37, loss = 0.02179487
Iteration 38, loss = 0.02169023
Iteration 39, loss = 0.02141947
Iteration 40, loss = 0.02129920
Iteration 41, loss = 0.02123120
Iteration 42, loss = 0.02113391
Iteration 43, loss = 0.02064702
Iteration 44, loss = 0.02070563
Iteration 45, loss = 0.02051424
Iteration 46, loss = 0.02065548
Iteration 47, loss = 0.02057076
Iteration 48, loss = 0.02070415
Iteration 49, loss = 0.02030455
Iteration 50, loss = 0.01988287
Iteration 51, loss = 0.02004898
Iteration 52, loss = 0.01994437
Iteration 53, loss = 0.01980838
Iteration 54, loss = 0.01983560
Iteration 55, loss = 0.01980076
Iteration 56, loss = 0.01997152
Iteration 57, loss = 0.01960722
Iteration 58, loss = 0.01974029
Iteration 59, loss = 0.01943131
Iteration 60, loss = 0.01946577
Iteration 61, loss = 0.01939631
Iteration 62, loss = 0.01970012
Iteration 63, loss = 0.01921180
Iteration 64, loss = 0.01917371
Iteration 65, loss = 0.01947600
Iteration 66, loss = 0.01944706
Iteration 67, loss = 0.01956558
Iteration 68, loss = 0.01969304
Iteration 69, loss = 0.01928246
Iteration 70, loss = 0.01940586
Iteration 71, loss = 0.01914331
Iteration 72, loss = 0.01919148
Iteration 73, loss = 0.01931630
Iteration 74, loss = 0.01908843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67255382
Iteration 2, loss = 0.59396885
Iteration 3, loss = 0.48492950
Iteration 4, loss = 0.37174055
Iteration 5, loss = 0.27442226
Iteration 6, loss = 0.20001404
Iteration 7, loss = 0.15002226
Iteration 8, loss = 0.11604354
Iteration 9, loss = 0.09318681
Iteration 10, loss = 0.07731486
Iteration 11, loss = 0.06572350
Iteration 12, loss = 0.05741628
Iteration 13, loss = 0.05087969
Iteration 14, loss = 0.04609662
Iteration 15, loss = 0.04229312
Iteration 16, loss = 0.03909403
Iteration 17, loss = 0.03648133
Iteration 18, loss = 0.03428299
Iteration 19, loss = 0.03257325
Iteration 20, loss = 0.03116287
Iteration 21, loss = 0.02976276
Iteration 22, loss = 0.02893946
Iteration 23, loss = 0.02790123
Iteration 24, loss = 0.02708185
Iteration 25, loss = 0.02640693
Iteration 26, loss = 0.02602623
Iteration 27, loss = 0.02528681
Iteration 28, loss = 0.02461079
Iteration 29, loss = 0.02430545
Iteration 30, loss = 0.02382530
Iteration 31, loss = 0.02347238
Iteration 32, loss = 0.02333434
Iteration 33, loss = 0.02292113
Iteration 34, loss = 0.02264280
Iteration 35, loss = 0.02260949
Iteration 36, loss = 0.02213977
Iteration 37, loss = 0.02191965
Iteration 38, loss = 0.02184127
Iteration 39, loss = 0.02168270
Iteration 40, loss = 0.02167544
Iteration 41, loss = 0.02127203
Iteration 42, loss = 0.02125007
Iteration 43, loss = 0.02106468
Iteration 44, loss = 0.02092598
Iteration 45, loss = 0.02087826
Iteration 46, loss = 0.02080594
Iteration 47, loss = 0.02072385
Iteration 48, loss = 0.02075633
Iteration 49, loss = 0.02043913
Iteration 50, loss = 0.02049685
Iteration 51, loss = 0.02044420
Iteration 52, loss = 0.02036229
Iteration 53, loss = 0.02011137
Iteration 54, loss = 0.02023243
Iteration 55, loss = 0.02017131
Iteration 56, loss = 0.02015803
Iteration 57, loss = 0.02003055
Iteration 58, loss = 0.02013231
Iteration 59, loss = 0.01976684
Iteration 60, loss = 0.01996309
Iteration 61, loss = 0.01984527
Iteration 62, loss = 0.01981756
Iteration 63, loss = 0.01979002
Iteration 64, loss = 0.01971791
Iteration 65, loss = 0.01975117
Iteration 66, loss = 0.01964331
Iteration 67, loss = 0.01963170
Iteration 68, loss = 0.01932971
Iteration 69, loss = 0.01959404
Iteration 70, loss = 0.01959852
Iteration 71, loss = 0.01967908
Iteration 72, loss = 0.01953696
Iteration 73, loss = 0.01937466
Iteration 74, loss = 0.01953844
Iteration 75, loss = 0.01934858
Iteration 76, loss = 0.01929362
Iteration 77, loss = 0.01928688
Iteration 78, loss = 0.01936423
Iteration 79, loss = 0.01923701
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67251691
Iteration 2, loss = 0.59168608
Iteration 3, loss = 0.47473378
Iteration 4, loss = 0.35771885
Iteration 5, loss = 0.26239713
Iteration 6, loss = 0.19327421
Iteration 7, loss = 0.14616824
Iteration 8, loss = 0.11439610
Iteration 9, loss = 0.09254662
Iteration 10, loss = 0.07718516
Iteration 11, loss = 0.06605291
Iteration 12, loss = 0.05786672
Iteration 13, loss = 0.05135540
Iteration 14, loss = 0.04690683
Iteration 15, loss = 0.04280125
Iteration 16, loss = 0.03948696
Iteration 17, loss = 0.03678409
Iteration 18, loss = 0.03489118
Iteration 19, loss = 0.03316252
Iteration 20, loss = 0.03162130
Iteration 21, loss = 0.03019906
Iteration 22, loss = 0.02929125
Iteration 23, loss = 0.02810474
Iteration 24, loss = 0.02726958
Iteration 25, loss = 0.02673354
Iteration 26, loss = 0.02623652
Iteration 27, loss = 0.02538149
Iteration 28, loss = 0.02514829
Iteration 29, loss = 0.02433693
Iteration 30, loss = 0.02408561
Iteration 31, loss = 0.02373105
Iteration 32, loss = 0.02352286
Iteration 33, loss = 0.02324443
Iteration 34, loss = 0.02287229
Iteration 35, loss = 0.02253854
Iteration 36, loss = 0.02246123
Iteration 37, loss = 0.02212557
Iteration 38, loss = 0.02200256
Iteration 39, loss = 0.02173094
Iteration 40, loss = 0.02190754
Iteration 41, loss = 0.02143015
Iteration 42, loss = 0.02157942
Iteration 43, loss = 0.02122516
Iteration 44, loss = 0.02140328
Iteration 45, loss = 0.02115360
Iteration 46, loss = 0.02098686
Iteration 47, loss = 0.02082561
Iteration 48, loss = 0.02059332
Iteration 49, loss = 0.02054442
Iteration 50, loss = 0.02078470
Iteration 51, loss = 0.02085912
Iteration 52, loss = 0.02047748
Iteration 53, loss = 0.02021323
Iteration 54, loss = 0.02021561
Iteration 55, loss = 0.02036424
Iteration 56, loss = 0.02014974
Iteration 57, loss = 0.02029736
Iteration 58, loss = 0.02022973
Iteration 59, loss = 0.02022261
Iteration 60, loss = 0.02011436
Iteration 61, loss = 0.02019674
Iteration 62, loss = 0.01963631
Iteration 63, loss = 0.01980837
Iteration 64, loss = 0.01975901
Iteration 65, loss = 0.01992847
Iteration 66, loss = 0.02001099
Iteration 67, loss = 0.01982470
Iteration 68, loss = 0.01958582
Iteration 69, loss = 0.01966906
Iteration 70, loss = 0.01985603
Iteration 71, loss = 0.01960349
Iteration 72, loss = 0.01958071
Iteration 73, loss = 0.01949045
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70362859
Iteration 2, loss = 0.59413944
Iteration 3, loss = 0.44986164
Iteration 4, loss = 0.31586315
Iteration 5, loss = 0.22000740
Iteration 6, loss = 0.15779965
Iteration 7, loss = 0.11892563
Iteration 8, loss = 0.09435040
Iteration 9, loss = 0.07735558
Iteration 10, loss = 0.06572755
Iteration 11, loss = 0.05757119
Iteration 12, loss = 0.05133200
Iteration 13, loss = 0.04654212
Iteration 14, loss = 0.04272490
Iteration 15, loss = 0.03998104
Iteration 16, loss = 0.03769673
Iteration 17, loss = 0.03551883
Iteration 18, loss = 0.03439948
Iteration 19, loss = 0.03255443
Iteration 20, loss = 0.03179353
Iteration 21, loss = 0.03057416
Iteration 22, loss = 0.02980448
Iteration 23, loss = 0.02912062
Iteration 24, loss = 0.02858390
Iteration 25, loss = 0.02777544
Iteration 26, loss = 0.02737200
Iteration 27, loss = 0.02740623
Iteration 28, loss = 0.02678080
Iteration 29, loss = 0.02652029
Iteration 30, loss = 0.02617848
Iteration 31, loss = 0.02586895
Iteration 32, loss = 0.02547164
Iteration 33, loss = 0.02547469
Iteration 34, loss = 0.02505089
Iteration 35, loss = 0.02487092
Iteration 36, loss = 0.02468294
Iteration 37, loss = 0.02468004
Iteration 38, loss = 0.02457186
Iteration 39, loss = 0.02441931
Iteration 40, loss = 0.02437461
Iteration 41, loss = 0.02429965
Iteration 42, loss = 0.02398949
Iteration 43, loss = 0.02409644
Iteration 44, loss = 0.02394143
Iteration 45, loss = 0.02355229
Iteration 46, loss = 0.02400547
Iteration 47, loss = 0.02354494
Iteration 48, loss = 0.02342775
Iteration 49, loss = 0.02383659
Iteration 50, loss = 0.02380739
Iteration 51, loss = 0.02379166
Iteration 52, loss = 0.02353876
Iteration 53, loss = 0.02377079
Iteration 54, loss = 0.02305216
Iteration 55, loss = 0.02329796
Iteration 56, loss = 0.02320886
Iteration 57, loss = 0.02333669
Iteration 58, loss = 0.02303962
Iteration 59, loss = 0.02272515
Iteration 60, loss = 0.02302421
Iteration 61, loss = 0.02300165
Iteration 62, loss = 0.02303649
Iteration 63, loss = 0.02292009
Iteration 64, loss = 0.02292467
Iteration 65, loss = 0.02292290
Iteration 66, loss = 0.02288080
Iteration 67, loss = 0.02293474
Iteration 68, loss = 0.02303733
Iteration 69, loss = 0.02309408
Iteration 70, loss = 0.02235389
Iteration 71, loss = 0.02262102
Iteration 72, loss = 0.02304980
Iteration 73, loss = 0.02261566
Iteration 74, loss = 0.02279602
Iteration 75, loss = 0.02278697
Iteration 76, loss = 0.02281440
Iteration 77, loss = 0.02291010
Iteration 78, loss = 0.02268666
Iteration 79, loss = 0.02283877
Iteration 80, loss = 0.02261412
Iteration 81, loss = 0.02276685
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on prepended_v1_basic_clean...
Iteration 1, loss = 0.69278239
Iteration 2, loss = 0.64759312
Iteration 3, loss = 0.58157102
Iteration 4, loss = 0.49595249
Iteration 5, loss = 0.40944605
Iteration 6, loss = 0.33083630
Iteration 7, loss = 0.26478945
Iteration 8, loss = 0.21205333
Iteration 9, loss = 0.17149880
Iteration 10, loss = 0.14054372
Iteration 11, loss = 0.11733804
Iteration 12, loss = 0.09951569
Iteration 13, loss = 0.08602288
Iteration 14, loss = 0.07523067
Iteration 15, loss = 0.06686583
Iteration 16, loss = 0.06020678
Iteration 17, loss = 0.05462751
Iteration 18, loss = 0.05013511
Iteration 19, loss = 0.04629081
Iteration 20, loss = 0.04322242
Iteration 21, loss = 0.04053217
Iteration 22, loss = 0.03861277
Iteration 23, loss = 0.03637893
Iteration 24, loss = 0.03459653
Iteration 25, loss = 0.03326870
Iteration 26, loss = 0.03186415
Iteration 27, loss = 0.03087758
Iteration 28, loss = 0.02978872
Iteration 29, loss = 0.02880939
Iteration 30, loss = 0.02794042
Iteration 31, loss = 0.02711599
Iteration 32, loss = 0.02644218
Iteration 33, loss = 0.02595677
Iteration 34, loss = 0.02542417
Iteration 35, loss = 0.02482896
Iteration 36, loss = 0.02462266
Iteration 37, loss = 0.02412655
Iteration 38, loss = 0.02366013
Iteration 39, loss = 0.02331659
Iteration 40, loss = 0.02312314
Iteration 41, loss = 0.02291238
Iteration 42, loss = 0.02247610
Iteration 43, loss = 0.02212695
Iteration 44, loss = 0.02191232
Iteration 45, loss = 0.02177029
Iteration 46, loss = 0.02165907
Iteration 47, loss = 0.02137193
Iteration 48, loss = 0.02116768
Iteration 49, loss = 0.02097567
Iteration 50, loss = 0.02085760
Iteration 51, loss = 0.02067707
Iteration 52, loss = 0.02059909
Iteration 53, loss = 0.02033511
Iteration 54, loss = 0.02055086
Iteration 55, loss = 0.02041572
Iteration 56, loss = 0.02010194
Iteration 57, loss = 0.02004162
Iteration 58, loss = 0.01987619
Iteration 59, loss = 0.01963436
Iteration 60, loss = 0.01963710
Iteration 61, loss = 0.01970264
Iteration 62, loss = 0.01971846
Iteration 63, loss = 0.01930025
Iteration 64, loss = 0.01957937
Iteration 65, loss = 0.01981464
Iteration 66, loss = 0.01939828
Iteration 67, loss = 0.01920398
Iteration 68, loss = 0.01917859
Iteration 69, loss = 0.01915566
Iteration 70, loss = 0.01897359
Iteration 71, loss = 0.01884448
Iteration 72, loss = 0.01882835
Iteration 73, loss = 0.01877606
Iteration 74, loss = 0.01877391
Iteration 75, loss = 0.01865575
Iteration 76, loss = 0.01879002
Iteration 77, loss = 0.01873769
Iteration 78, loss = 0.01874093
Iteration 79, loss = 0.01848409
Iteration 80, loss = 0.01861026
Iteration 81, loss = 0.01859216
Iteration 82, loss = 0.01859675
Iteration 83, loss = 0.01848667
Iteration 84, loss = 0.01836812
Iteration 85, loss = 0.01831101
Iteration 86, loss = 0.01834792
Iteration 87, loss = 0.01831729
Iteration 88, loss = 0.01817643
Iteration 89, loss = 0.01846776
Iteration 90, loss = 0.01843107
Iteration 91, loss = 0.01830665
Iteration 92, loss = 0.01828450
Iteration 93, loss = 0.01828182
Iteration 94, loss = 0.01824840
Iteration 95, loss = 0.01823708
Iteration 96, loss = 0.01807602
Iteration 97, loss = 0.01790871
Iteration 98, loss = 0.01800896
Iteration 99, loss = 0.01828333
Iteration 100, loss = 0.01804503
Iteration 101, loss = 0.01815352
Iteration 102, loss = 0.01803460
Iteration 103, loss = 0.01797271
Iteration 104, loss = 0.01807324
Iteration 105, loss = 0.01810392
Iteration 106, loss = 0.01797098
Iteration 107, loss = 0.01796686
Iteration 108, loss = 0.01780967
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69083186
Iteration 2, loss = 0.67149007
Iteration 3, loss = 0.65152210
Iteration 4, loss = 0.62695458
Iteration 5, loss = 0.59746827
Iteration 6, loss = 0.56323099
Iteration 7, loss = 0.52603644
Iteration 8, loss = 0.48773508
Iteration 9, loss = 0.44909561
Iteration 10, loss = 0.41129717
Iteration 11, loss = 0.37543866
Iteration 12, loss = 0.34167153
Iteration 13, loss = 0.31025064
Iteration 14, loss = 0.28130559
Iteration 15, loss = 0.25506215
Iteration 16, loss = 0.23098820
Iteration 17, loss = 0.20954033
Iteration 18, loss = 0.19009017
Iteration 19, loss = 0.17281184
Iteration 20, loss = 0.15725908
Iteration 21, loss = 0.14360081
Iteration 22, loss = 0.13142502
Iteration 23, loss = 0.12046137
Iteration 24, loss = 0.11089955
Iteration 25, loss = 0.10233337
Iteration 26, loss = 0.09474152
Iteration 27, loss = 0.08788612
Iteration 28, loss = 0.08180144
Iteration 29, loss = 0.07637402
Iteration 30, loss = 0.07150157
Iteration 31, loss = 0.06707422
Iteration 32, loss = 0.06312645
Iteration 33, loss = 0.05957188
Iteration 34, loss = 0.05624908
Iteration 35, loss = 0.05328064
Iteration 36, loss = 0.05062591
Iteration 37, loss = 0.04821602
Iteration 38, loss = 0.04590326
Iteration 39, loss = 0.04382910
Iteration 40, loss = 0.04187687
Iteration 41, loss = 0.04014450
Iteration 42, loss = 0.03850018
Iteration 43, loss = 0.03701897
Iteration 44, loss = 0.03563017
Iteration 45, loss = 0.03436079
Iteration 46, loss = 0.03310032
Iteration 47, loss = 0.03197727
Iteration 48, loss = 0.03091905
Iteration 49, loss = 0.02996787
Iteration 50, loss = 0.02900254
Iteration 51, loss = 0.02812466
Iteration 52, loss = 0.02734737
Iteration 53, loss = 0.02658284
Iteration 54, loss = 0.02586966
Iteration 55, loss = 0.02517296
Iteration 56, loss = 0.02456478
Iteration 57, loss = 0.02393451
Iteration 58, loss = 0.02335843
Iteration 59, loss = 0.02284444
Iteration 60, loss = 0.02226912
Iteration 61, loss = 0.02197045
Iteration 62, loss = 0.02138899
Iteration 63, loss = 0.02097962
Iteration 64, loss = 0.02049116
Iteration 65, loss = 0.02012403
Iteration 66, loss = 0.01974271
Iteration 67, loss = 0.01937568
Iteration 68, loss = 0.01907431
Iteration 69, loss = 0.01873308
Iteration 70, loss = 0.01846040
Iteration 71, loss = 0.01810839
Iteration 72, loss = 0.01780954
Iteration 73, loss = 0.01753787
Iteration 74, loss = 0.01728067
Iteration 75, loss = 0.01701612
Iteration 76, loss = 0.01679770
Iteration 77, loss = 0.01653830
Iteration 78, loss = 0.01633612
Iteration 79, loss = 0.01613782
Iteration 80, loss = 0.01593153
Iteration 81, loss = 0.01574306
Iteration 82, loss = 0.01548645
Iteration 83, loss = 0.01541599
Iteration 84, loss = 0.01516445
Iteration 85, loss = 0.01497943
Iteration 86, loss = 0.01479380
Iteration 87, loss = 0.01469336
Iteration 88, loss = 0.01454898
Iteration 89, loss = 0.01439228
Iteration 90, loss = 0.01422357
Iteration 91, loss = 0.01409459
Iteration 92, loss = 0.01397114
Iteration 93, loss = 0.01389814
Iteration 94, loss = 0.01370912
Iteration 95, loss = 0.01359741
Iteration 96, loss = 0.01346239
Iteration 97, loss = 0.01337147
Iteration 98, loss = 0.01326745
Iteration 99, loss = 0.01320203
Iteration 100, loss = 0.01307822
Iteration 101, loss = 0.01291850
Iteration 102, loss = 0.01285786
Iteration 103, loss = 0.01274009
Iteration 104, loss = 0.01264606
Iteration 105, loss = 0.01257455
Iteration 106, loss = 0.01254026
Iteration 107, loss = 0.01239682
Iteration 108, loss = 0.01236642
Iteration 109, loss = 0.01224212
Iteration 110, loss = 0.01221111
Iteration 111, loss = 0.01208316
Iteration 112, loss = 0.01204397
Iteration 113, loss = 0.01196001
Iteration 114, loss = 0.01188538
Iteration 115, loss = 0.01185198
Iteration 116, loss = 0.01180196
Iteration 117, loss = 0.01174674
Iteration 118, loss = 0.01164298
Iteration 119, loss = 0.01160900
Iteration 120, loss = 0.01155884
Iteration 121, loss = 0.01145769
Iteration 122, loss = 0.01141760
Iteration 123, loss = 0.01139333
Iteration 124, loss = 0.01129966
Iteration 125, loss = 0.01129554
Iteration 126, loss = 0.01120167
Iteration 127, loss = 0.01116235
Iteration 128, loss = 0.01109991
Iteration 129, loss = 0.01109511
Iteration 130, loss = 0.01105713
Iteration 131, loss = 0.01097477
Iteration 132, loss = 0.01096112
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67784008
Iteration 2, loss = 0.64050869
Iteration 3, loss = 0.58822464
Iteration 4, loss = 0.51795458
Iteration 5, loss = 0.44354851
Iteration 6, loss = 0.37353190
Iteration 7, loss = 0.31002349
Iteration 8, loss = 0.25559303
Iteration 9, loss = 0.20979782
Iteration 10, loss = 0.17348772
Iteration 11, loss = 0.14486101
Iteration 12, loss = 0.12213515
Iteration 13, loss = 0.10436200
Iteration 14, loss = 0.09050250
Iteration 15, loss = 0.07929741
Iteration 16, loss = 0.07030692
Iteration 17, loss = 0.06280476
Iteration 18, loss = 0.05679370
Iteration 19, loss = 0.05170057
Iteration 20, loss = 0.04735085
Iteration 21, loss = 0.04387813
Iteration 22, loss = 0.04066411
Iteration 23, loss = 0.03810960
Iteration 24, loss = 0.03581359
Iteration 25, loss = 0.03400392
Iteration 26, loss = 0.03204694
Iteration 27, loss = 0.03057571
Iteration 28, loss = 0.02901864
Iteration 29, loss = 0.02790599
Iteration 30, loss = 0.02684747
Iteration 31, loss = 0.02588512
Iteration 32, loss = 0.02493352
Iteration 33, loss = 0.02413723
Iteration 34, loss = 0.02352142
Iteration 35, loss = 0.02275279
Iteration 36, loss = 0.02215745
Iteration 37, loss = 0.02167584
Iteration 38, loss = 0.02113403
Iteration 39, loss = 0.02061494
Iteration 40, loss = 0.02028231
Iteration 41, loss = 0.01998222
Iteration 42, loss = 0.01943648
Iteration 43, loss = 0.01919471
Iteration 44, loss = 0.01889115
Iteration 45, loss = 0.01864740
Iteration 46, loss = 0.01833626
Iteration 47, loss = 0.01799009
Iteration 48, loss = 0.01778455
Iteration 49, loss = 0.01748039
Iteration 50, loss = 0.01732094
Iteration 51, loss = 0.01717195
Iteration 52, loss = 0.01697588
Iteration 53, loss = 0.01693365
Iteration 54, loss = 0.01670145
Iteration 55, loss = 0.01667653
Iteration 56, loss = 0.01639724
Iteration 57, loss = 0.01629553
Iteration 58, loss = 0.01624112
Iteration 59, loss = 0.01604678
Iteration 60, loss = 0.01585481
Iteration 61, loss = 0.01586632
Iteration 62, loss = 0.01561040
Iteration 63, loss = 0.01561559
Iteration 64, loss = 0.01551846
Iteration 65, loss = 0.01541583
Iteration 66, loss = 0.01549414
Iteration 67, loss = 0.01528406
Iteration 68, loss = 0.01514450
Iteration 69, loss = 0.01503829
Iteration 70, loss = 0.01494864
Iteration 71, loss = 0.01499105
Iteration 72, loss = 0.01502608
Iteration 73, loss = 0.01474895
Iteration 74, loss = 0.01470137
Iteration 75, loss = 0.01481033
Iteration 76, loss = 0.01461636
Iteration 77, loss = 0.01459830
Iteration 78, loss = 0.01456000
Iteration 79, loss = 0.01445800
Iteration 80, loss = 0.01453188
Iteration 81, loss = 0.01440699
Iteration 82, loss = 0.01424766
Iteration 83, loss = 0.01444312
Iteration 84, loss = 0.01434010
Iteration 85, loss = 0.01430070
Iteration 86, loss = 0.01422200
Iteration 87, loss = 0.01409434
Iteration 88, loss = 0.01420448
Iteration 89, loss = 0.01408577
Iteration 90, loss = 0.01407595
Iteration 91, loss = 0.01412525
Iteration 92, loss = 0.01401873
Iteration 93, loss = 0.01398778
Iteration 94, loss = 0.01393437
Iteration 95, loss = 0.01397756
Iteration 96, loss = 0.01382874
Iteration 97, loss = 0.01394305
Iteration 98, loss = 0.01381316
Iteration 99, loss = 0.01380414
Iteration 100, loss = 0.01381855
Iteration 101, loss = 0.01381853
Iteration 102, loss = 0.01376562
Iteration 103, loss = 0.01381828
Iteration 104, loss = 0.01381240
Iteration 105, loss = 0.01370810
Iteration 106, loss = 0.01361839
Iteration 107, loss = 0.01365983
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69643622
Iteration 2, loss = 0.66044151
Iteration 3, loss = 0.61603000
Iteration 4, loss = 0.56103027
Iteration 5, loss = 0.49646073
Iteration 6, loss = 0.43085706
Iteration 7, loss = 0.36909156
Iteration 8, loss = 0.31302873
Iteration 9, loss = 0.26433937
Iteration 10, loss = 0.22259599
Iteration 11, loss = 0.18812218
Iteration 12, loss = 0.15945002
Iteration 13, loss = 0.13630507
Iteration 14, loss = 0.11734788
Iteration 15, loss = 0.10195131
Iteration 16, loss = 0.08940404
Iteration 17, loss = 0.07912740
Iteration 18, loss = 0.07046459
Iteration 19, loss = 0.06333515
Iteration 20, loss = 0.05740503
Iteration 21, loss = 0.05226332
Iteration 22, loss = 0.04798592
Iteration 23, loss = 0.04421976
Iteration 24, loss = 0.04091757
Iteration 25, loss = 0.03822157
Iteration 26, loss = 0.03568886
Iteration 27, loss = 0.03355564
Iteration 28, loss = 0.03170044
Iteration 29, loss = 0.03001327
Iteration 30, loss = 0.02848659
Iteration 31, loss = 0.02711524
Iteration 32, loss = 0.02585769
Iteration 33, loss = 0.02479509
Iteration 34, loss = 0.02383627
Iteration 35, loss = 0.02302191
Iteration 36, loss = 0.02206496
Iteration 37, loss = 0.02132602
Iteration 38, loss = 0.02073302
Iteration 39, loss = 0.02007710
Iteration 40, loss = 0.01947315
Iteration 41, loss = 0.01900277
Iteration 42, loss = 0.01842313
Iteration 43, loss = 0.01791487
Iteration 44, loss = 0.01752182
Iteration 45, loss = 0.01712522
Iteration 46, loss = 0.01687372
Iteration 47, loss = 0.01657472
Iteration 48, loss = 0.01619968
Iteration 49, loss = 0.01589647
Iteration 50, loss = 0.01567638
Iteration 51, loss = 0.01537729
Iteration 52, loss = 0.01499958
Iteration 53, loss = 0.01489767
Iteration 54, loss = 0.01462621
Iteration 55, loss = 0.01444499
Iteration 56, loss = 0.01421659
Iteration 57, loss = 0.01414235
Iteration 58, loss = 0.01393744
Iteration 59, loss = 0.01380886
Iteration 60, loss = 0.01366134
Iteration 61, loss = 0.01358079
Iteration 62, loss = 0.01332979
Iteration 63, loss = 0.01314759
Iteration 64, loss = 0.01307273
Iteration 65, loss = 0.01295002
Iteration 66, loss = 0.01276688
Iteration 67, loss = 0.01267869
Iteration 68, loss = 0.01263958
Iteration 69, loss = 0.01255856
Iteration 70, loss = 0.01239315
Iteration 71, loss = 0.01243027
Iteration 72, loss = 0.01233082
Iteration 73, loss = 0.01227789
Iteration 74, loss = 0.01208607
Iteration 75, loss = 0.01203164
Iteration 76, loss = 0.01196206
Iteration 77, loss = 0.01188263
Iteration 78, loss = 0.01182148
Iteration 79, loss = 0.01181742
Iteration 80, loss = 0.01170702
Iteration 81, loss = 0.01156597
Iteration 82, loss = 0.01160406
Iteration 83, loss = 0.01151173
Iteration 84, loss = 0.01156442
Iteration 85, loss = 0.01142640
Iteration 86, loss = 0.01146636
Iteration 87, loss = 0.01142831
Iteration 88, loss = 0.01124919
Iteration 89, loss = 0.01130576
Iteration 90, loss = 0.01116330
Iteration 91, loss = 0.01119456
Iteration 92, loss = 0.01107179
Iteration 93, loss = 0.01115427
Iteration 94, loss = 0.01102020
Iteration 95, loss = 0.01108360
Iteration 96, loss = 0.01102353
Iteration 97, loss = 0.01089236
Iteration 98, loss = 0.01093026
Iteration 99, loss = 0.01106487
Iteration 100, loss = 0.01101111
Iteration 101, loss = 0.01089162
Iteration 102, loss = 0.01084126
Iteration 103, loss = 0.01086547
Iteration 104, loss = 0.01085474
Iteration 105, loss = 0.01075367
Iteration 106, loss = 0.01066529
Iteration 107, loss = 0.01069574
Iteration 108, loss = 0.01061501
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70213544
Iteration 2, loss = 0.67470744
Iteration 3, loss = 0.63689203
Iteration 4, loss = 0.58852687
Iteration 5, loss = 0.53022981
Iteration 6, loss = 0.46710358
Iteration 7, loss = 0.40404155
Iteration 8, loss = 0.34490724
Iteration 9, loss = 0.29189595
Iteration 10, loss = 0.24634529
Iteration 11, loss = 0.20813959
Iteration 12, loss = 0.17636190
Iteration 13, loss = 0.15056054
Iteration 14, loss = 0.12939284
Iteration 15, loss = 0.11223308
Iteration 16, loss = 0.09826110
Iteration 17, loss = 0.08676187
Iteration 18, loss = 0.07715153
Iteration 19, loss = 0.06919409
Iteration 20, loss = 0.06250606
Iteration 21, loss = 0.05690418
Iteration 22, loss = 0.05206304
Iteration 23, loss = 0.04789657
Iteration 24, loss = 0.04427684
Iteration 25, loss = 0.04118179
Iteration 26, loss = 0.03836583
Iteration 27, loss = 0.03599866
Iteration 28, loss = 0.03374652
Iteration 29, loss = 0.03194177
Iteration 30, loss = 0.03033750
Iteration 31, loss = 0.02874163
Iteration 32, loss = 0.02735953
Iteration 33, loss = 0.02616201
Iteration 34, loss = 0.02505087
Iteration 35, loss = 0.02409674
Iteration 36, loss = 0.02304873
Iteration 37, loss = 0.02225321
Iteration 38, loss = 0.02143339
Iteration 39, loss = 0.02078907
Iteration 40, loss = 0.02018731
Iteration 41, loss = 0.01955539
Iteration 42, loss = 0.01899974
Iteration 43, loss = 0.01845973
Iteration 44, loss = 0.01795557
Iteration 45, loss = 0.01754587
Iteration 46, loss = 0.01713134
Iteration 47, loss = 0.01675281
Iteration 48, loss = 0.01640120
Iteration 49, loss = 0.01603324
Iteration 50, loss = 0.01573305
Iteration 51, loss = 0.01546609
Iteration 52, loss = 0.01514054
Iteration 53, loss = 0.01492067
Iteration 54, loss = 0.01466519
Iteration 55, loss = 0.01445421
Iteration 56, loss = 0.01422544
Iteration 57, loss = 0.01403952
Iteration 58, loss = 0.01379232
Iteration 59, loss = 0.01359525
Iteration 60, loss = 0.01345112
Iteration 61, loss = 0.01324167
Iteration 62, loss = 0.01312304
Iteration 63, loss = 0.01299144
Iteration 64, loss = 0.01284680
Iteration 65, loss = 0.01273084
Iteration 66, loss = 0.01253315
Iteration 67, loss = 0.01245218
Iteration 68, loss = 0.01244281
Iteration 69, loss = 0.01219814
Iteration 70, loss = 0.01208294
Iteration 71, loss = 0.01198899
Iteration 72, loss = 0.01196545
Iteration 73, loss = 0.01176225
Iteration 74, loss = 0.01171387
Iteration 75, loss = 0.01168470
Iteration 76, loss = 0.01155927
Iteration 77, loss = 0.01148605
Iteration 78, loss = 0.01139004
Iteration 79, loss = 0.01135174
Iteration 80, loss = 0.01125726
Iteration 81, loss = 0.01118909
Iteration 82, loss = 0.01115711
Iteration 83, loss = 0.01113444
Iteration 84, loss = 0.01105972
Iteration 85, loss = 0.01098082
Iteration 86, loss = 0.01098101
Iteration 87, loss = 0.01084415
Iteration 88, loss = 0.01082077
Iteration 89, loss = 0.01076020
Iteration 90, loss = 0.01075265
Iteration 91, loss = 0.01072019
Iteration 92, loss = 0.01067079
Iteration 93, loss = 0.01064751
Iteration 94, loss = 0.01062105
Iteration 95, loss = 0.01062401
Iteration 96, loss = 0.01044409
Iteration 97, loss = 0.01044220
Iteration 98, loss = 0.01045383
Iteration 99, loss = 0.01035673
Iteration 100, loss = 0.01030035
Iteration 101, loss = 0.01028004
Iteration 102, loss = 0.01024726
Iteration 103, loss = 0.01019417
Iteration 104, loss = 0.01015860
Iteration 105, loss = 0.01019251
Iteration 106, loss = 0.01016576
Iteration 107, loss = 0.01012672
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69664730
Iteration 2, loss = 0.64899563
Iteration 3, loss = 0.58645812
Iteration 4, loss = 0.50376387
Iteration 5, loss = 0.41466557
Iteration 6, loss = 0.33332677
Iteration 7, loss = 0.26600026
Iteration 8, loss = 0.21232299
Iteration 9, loss = 0.17128362
Iteration 10, loss = 0.14066103
Iteration 11, loss = 0.11747600
Iteration 12, loss = 0.09977965
Iteration 13, loss = 0.08635042
Iteration 14, loss = 0.07591460
Iteration 15, loss = 0.06755199
Iteration 16, loss = 0.06086878
Iteration 17, loss = 0.05551170
Iteration 18, loss = 0.05128990
Iteration 19, loss = 0.04744534
Iteration 20, loss = 0.04423836
Iteration 21, loss = 0.04179807
Iteration 22, loss = 0.03972791
Iteration 23, loss = 0.03748579
Iteration 24, loss = 0.03578767
Iteration 25, loss = 0.03431960
Iteration 26, loss = 0.03303843
Iteration 27, loss = 0.03189086
Iteration 28, loss = 0.03089365
Iteration 29, loss = 0.03000849
Iteration 30, loss = 0.02953110
Iteration 31, loss = 0.02857632
Iteration 32, loss = 0.02789571
Iteration 33, loss = 0.02719677
Iteration 34, loss = 0.02670404
Iteration 35, loss = 0.02631541
Iteration 36, loss = 0.02582515
Iteration 37, loss = 0.02547730
Iteration 38, loss = 0.02517301
Iteration 39, loss = 0.02464783
Iteration 40, loss = 0.02444909
Iteration 41, loss = 0.02448573
Iteration 42, loss = 0.02389892
Iteration 43, loss = 0.02376238
Iteration 44, loss = 0.02340693
Iteration 45, loss = 0.02319288
Iteration 46, loss = 0.02306544
Iteration 47, loss = 0.02304596
Iteration 48, loss = 0.02274818
Iteration 49, loss = 0.02258203
Iteration 50, loss = 0.02237049
Iteration 51, loss = 0.02222949
Iteration 52, loss = 0.02234243
Iteration 53, loss = 0.02201601
Iteration 54, loss = 0.02181972
Iteration 55, loss = 0.02158323
Iteration 56, loss = 0.02173241
Iteration 57, loss = 0.02138122
Iteration 58, loss = 0.02142771
Iteration 59, loss = 0.02129952
Iteration 60, loss = 0.02131919
Iteration 61, loss = 0.02131197
Iteration 62, loss = 0.02115859
Iteration 63, loss = 0.02109769
Iteration 64, loss = 0.02100932
Iteration 65, loss = 0.02091432
Iteration 66, loss = 0.02091556
Iteration 67, loss = 0.02075133
Iteration 68, loss = 0.02079398
Iteration 69, loss = 0.02073925
Iteration 70, loss = 0.02049953
Iteration 71, loss = 0.02063586
Iteration 72, loss = 0.02041031
Iteration 73, loss = 0.02034024
Iteration 74, loss = 0.02043544
Iteration 75, loss = 0.02027380
Iteration 76, loss = 0.02033288
Iteration 77, loss = 0.02037641
Iteration 78, loss = 0.02043741
Iteration 79, loss = 0.02026849
Iteration 80, loss = 0.02016385
Iteration 81, loss = 0.02004116
Iteration 82, loss = 0.02007843
Iteration 83, loss = 0.02027324
Iteration 84, loss = 0.02018320
Iteration 85, loss = 0.01996465
Iteration 86, loss = 0.02012862
Iteration 87, loss = 0.02028675
Iteration 88, loss = 0.01984302
Iteration 89, loss = 0.02013673
Iteration 90, loss = 0.01987096
Iteration 91, loss = 0.01997234
Iteration 92, loss = 0.02029887
Iteration 93, loss = 0.01981903
Iteration 94, loss = 0.02017306
Iteration 95, loss = 0.01978969
Iteration 96, loss = 0.01978585
Iteration 97, loss = 0.01986518
Iteration 98, loss = 0.01994739
Iteration 99, loss = 0.02000301
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68165722
Iteration 2, loss = 0.62731297
Iteration 3, loss = 0.55485016
Iteration 4, loss = 0.47016750
Iteration 5, loss = 0.38759660
Iteration 6, loss = 0.31454582
Iteration 7, loss = 0.25323590
Iteration 8, loss = 0.20389719
Iteration 9, loss = 0.16572772
Iteration 10, loss = 0.13648111
Iteration 11, loss = 0.11417228
Iteration 12, loss = 0.09694098
Iteration 13, loss = 0.08366533
Iteration 14, loss = 0.07364773
Iteration 15, loss = 0.06526514
Iteration 16, loss = 0.05844315
Iteration 17, loss = 0.05323224
Iteration 18, loss = 0.04867316
Iteration 19, loss = 0.04490168
Iteration 20, loss = 0.04173855
Iteration 21, loss = 0.03908262
Iteration 22, loss = 0.03676330
Iteration 23, loss = 0.03493604
Iteration 24, loss = 0.03325131
Iteration 25, loss = 0.03158578
Iteration 26, loss = 0.03039701
Iteration 27, loss = 0.02900906
Iteration 28, loss = 0.02794252
Iteration 29, loss = 0.02718272
Iteration 30, loss = 0.02632209
Iteration 31, loss = 0.02548318
Iteration 32, loss = 0.02504934
Iteration 33, loss = 0.02429459
Iteration 34, loss = 0.02387802
Iteration 35, loss = 0.02327910
Iteration 36, loss = 0.02287718
Iteration 37, loss = 0.02278239
Iteration 38, loss = 0.02199911
Iteration 39, loss = 0.02158482
Iteration 40, loss = 0.02123727
Iteration 41, loss = 0.02102528
Iteration 42, loss = 0.02090390
Iteration 43, loss = 0.02044737
Iteration 44, loss = 0.02034711
Iteration 45, loss = 0.02036164
Iteration 46, loss = 0.01989418
Iteration 47, loss = 0.01956202
Iteration 48, loss = 0.01935586
Iteration 49, loss = 0.01909339
Iteration 50, loss = 0.01911315
Iteration 51, loss = 0.01875971
Iteration 52, loss = 0.01879391
Iteration 53, loss = 0.01852798
Iteration 54, loss = 0.01864488
Iteration 55, loss = 0.01856248
Iteration 56, loss = 0.01812493
Iteration 57, loss = 0.01811640
Iteration 58, loss = 0.01784543
Iteration 59, loss = 0.01800816
Iteration 60, loss = 0.01793053
Iteration 61, loss = 0.01783474
Iteration 62, loss = 0.01779269
Iteration 63, loss = 0.01772820
Iteration 64, loss = 0.01759324
Iteration 65, loss = 0.01746727
Iteration 66, loss = 0.01733963
Iteration 67, loss = 0.01732481
Iteration 68, loss = 0.01721634
Iteration 69, loss = 0.01721327
Iteration 70, loss = 0.01727546
Iteration 71, loss = 0.01732582
Iteration 72, loss = 0.01709835
Iteration 73, loss = 0.01701651
Iteration 74, loss = 0.01702333
Iteration 75, loss = 0.01688605
Iteration 76, loss = 0.01709720
Iteration 77, loss = 0.01686642
Iteration 78, loss = 0.01688500
Iteration 79, loss = 0.01673504
Iteration 80, loss = 0.01654615
Iteration 81, loss = 0.01658742
Iteration 82, loss = 0.01647538
Iteration 83, loss = 0.01660604
Iteration 84, loss = 0.01632112
Iteration 85, loss = 0.01670699
Iteration 86, loss = 0.01663809
Iteration 87, loss = 0.01658351
Iteration 88, loss = 0.01640558
Iteration 89, loss = 0.01643804
Iteration 90, loss = 0.01640173
Iteration 91, loss = 0.01674392
Iteration 92, loss = 0.01669204
Iteration 93, loss = 0.01610702
Iteration 94, loss = 0.01637361
Iteration 95, loss = 0.01630302
Iteration 96, loss = 0.01619803
Iteration 97, loss = 0.01615001
Iteration 98, loss = 0.01614110
Iteration 99, loss = 0.01620342
Iteration 100, loss = 0.01604057
Iteration 101, loss = 0.01620189
Iteration 102, loss = 0.01620315
Iteration 103, loss = 0.01613474
Iteration 104, loss = 0.01624612
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67740514
Iteration 2, loss = 0.62678745
Iteration 3, loss = 0.54448469
Iteration 4, loss = 0.44990176
Iteration 5, loss = 0.36026104
Iteration 6, loss = 0.28323930
Iteration 7, loss = 0.22157799
Iteration 8, loss = 0.17489215
Iteration 9, loss = 0.14020452
Iteration 10, loss = 0.11489921
Iteration 11, loss = 0.09600733
Iteration 12, loss = 0.08176979
Iteration 13, loss = 0.07098440
Iteration 14, loss = 0.06257693
Iteration 15, loss = 0.05592397
Iteration 16, loss = 0.05054297
Iteration 17, loss = 0.04619285
Iteration 18, loss = 0.04269659
Iteration 19, loss = 0.03960562
Iteration 20, loss = 0.03687888
Iteration 21, loss = 0.03478831
Iteration 22, loss = 0.03295184
Iteration 23, loss = 0.03132671
Iteration 24, loss = 0.03001815
Iteration 25, loss = 0.02870964
Iteration 26, loss = 0.02783084
Iteration 27, loss = 0.02680391
Iteration 28, loss = 0.02627823
Iteration 29, loss = 0.02544728
Iteration 30, loss = 0.02463036
Iteration 31, loss = 0.02400548
Iteration 32, loss = 0.02352465
Iteration 33, loss = 0.02321159
Iteration 34, loss = 0.02237624
Iteration 35, loss = 0.02225018
Iteration 36, loss = 0.02181368
Iteration 37, loss = 0.02134709
Iteration 38, loss = 0.02119874
Iteration 39, loss = 0.02084347
Iteration 40, loss = 0.02062025
Iteration 41, loss = 0.02026252
Iteration 42, loss = 0.02008421
Iteration 43, loss = 0.01996976
Iteration 44, loss = 0.01969260
Iteration 45, loss = 0.01956714
Iteration 46, loss = 0.01943750
Iteration 47, loss = 0.01914601
Iteration 48, loss = 0.01901343
Iteration 49, loss = 0.01898292
Iteration 50, loss = 0.01883928
Iteration 51, loss = 0.01876803
Iteration 52, loss = 0.01857788
Iteration 53, loss = 0.01847383
Iteration 54, loss = 0.01845392
Iteration 55, loss = 0.01833795
Iteration 56, loss = 0.01814641
Iteration 57, loss = 0.01821534
Iteration 58, loss = 0.01805328
Iteration 59, loss = 0.01809436
Iteration 60, loss = 0.01796009
Iteration 61, loss = 0.01785130
Iteration 62, loss = 0.01768581
Iteration 63, loss = 0.01774858
Iteration 64, loss = 0.01761463
Iteration 65, loss = 0.01780794
Iteration 66, loss = 0.01748935
Iteration 67, loss = 0.01737111
Iteration 68, loss = 0.01753520
Iteration 69, loss = 0.01737388
Iteration 70, loss = 0.01737068
Iteration 71, loss = 0.01722387
Iteration 72, loss = 0.01726340
Iteration 73, loss = 0.01731374
Iteration 74, loss = 0.01710130
Iteration 75, loss = 0.01725320
Iteration 76, loss = 0.01740068
Iteration 77, loss = 0.01704612
Iteration 78, loss = 0.01699918
Iteration 79, loss = 0.01705154
Iteration 80, loss = 0.01710601
Iteration 81, loss = 0.01694509
Iteration 82, loss = 0.01699689
Iteration 83, loss = 0.01689376
Iteration 84, loss = 0.01680214
Iteration 85, loss = 0.01694740
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72134972
Iteration 2, loss = 0.67375260
Iteration 3, loss = 0.60497891
Iteration 4, loss = 0.51437045
Iteration 5, loss = 0.41842449
Iteration 6, loss = 0.33030370
Iteration 7, loss = 0.25855657
Iteration 8, loss = 0.20311092
Iteration 9, loss = 0.16228744
Iteration 10, loss = 0.13184785
Iteration 11, loss = 0.10958132
Iteration 12, loss = 0.09296417
Iteration 13, loss = 0.08045668
Iteration 14, loss = 0.07091011
Iteration 15, loss = 0.06336701
Iteration 16, loss = 0.05720027
Iteration 17, loss = 0.05233026
Iteration 18, loss = 0.04850796
Iteration 19, loss = 0.04494915
Iteration 20, loss = 0.04218442
Iteration 21, loss = 0.03991015
Iteration 22, loss = 0.03793431
Iteration 23, loss = 0.03611667
Iteration 24, loss = 0.03474930
Iteration 25, loss = 0.03362336
Iteration 26, loss = 0.03218836
Iteration 27, loss = 0.03137728
Iteration 28, loss = 0.03031600
Iteration 29, loss = 0.02948143
Iteration 30, loss = 0.02886810
Iteration 31, loss = 0.02839987
Iteration 32, loss = 0.02781191
Iteration 33, loss = 0.02721571
Iteration 34, loss = 0.02693543
Iteration 35, loss = 0.02633589
Iteration 36, loss = 0.02617964
Iteration 37, loss = 0.02572383
Iteration 38, loss = 0.02530742
Iteration 39, loss = 0.02503627
Iteration 40, loss = 0.02482604
Iteration 41, loss = 0.02452044
Iteration 42, loss = 0.02422668
Iteration 43, loss = 0.02402877
Iteration 44, loss = 0.02385567
Iteration 45, loss = 0.02365696
Iteration 46, loss = 0.02333142
Iteration 47, loss = 0.02342346
Iteration 48, loss = 0.02309831
Iteration 49, loss = 0.02296708
Iteration 50, loss = 0.02301588
Iteration 51, loss = 0.02279317
Iteration 52, loss = 0.02273877
Iteration 53, loss = 0.02248821
Iteration 54, loss = 0.02265031
Iteration 55, loss = 0.02253584
Iteration 56, loss = 0.02224691
Iteration 57, loss = 0.02224391
Iteration 58, loss = 0.02217753
Iteration 59, loss = 0.02198217
Iteration 60, loss = 0.02200273
Iteration 61, loss = 0.02184489
Iteration 62, loss = 0.02197063
Iteration 63, loss = 0.02172975
Iteration 64, loss = 0.02173906
Iteration 65, loss = 0.02191052
Iteration 66, loss = 0.02162438
Iteration 67, loss = 0.02162806
Iteration 68, loss = 0.02164515
Iteration 69, loss = 0.02149282
Iteration 70, loss = 0.02137935
Iteration 71, loss = 0.02124205
Iteration 72, loss = 0.02144426
Iteration 73, loss = 0.02136038
Iteration 74, loss = 0.02124416
Iteration 75, loss = 0.02141270
Iteration 76, loss = 0.02131482
Iteration 77, loss = 0.02122837
Iteration 78, loss = 0.02145518
Iteration 79, loss = 0.02109225
Iteration 80, loss = 0.02115219
Iteration 81, loss = 0.02109344
Iteration 82, loss = 0.02120435
Iteration 83, loss = 0.02104523
Iteration 84, loss = 0.02084201
Iteration 85, loss = 0.02095495
Iteration 86, loss = 0.02092104
Iteration 87, loss = 0.02097998
Iteration 88, loss = 0.02092555
Iteration 89, loss = 0.02081111
Iteration 90, loss = 0.02106132
Iteration 91, loss = 0.02073465
Iteration 92, loss = 0.02071165
Iteration 93, loss = 0.02078574
Iteration 94, loss = 0.02108589
Iteration 95, loss = 0.02081847
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69264195
Iteration 2, loss = 0.64179188
Iteration 3, loss = 0.56025831
Iteration 4, loss = 0.45663335
Iteration 5, loss = 0.35930581
Iteration 6, loss = 0.27663034
Iteration 7, loss = 0.21366952
Iteration 8, loss = 0.16720449
Iteration 9, loss = 0.13424564
Iteration 10, loss = 0.11057985
Iteration 11, loss = 0.09309041
Iteration 12, loss = 0.08012276
Iteration 13, loss = 0.07049680
Iteration 14, loss = 0.06291029
Iteration 15, loss = 0.05690666
Iteration 16, loss = 0.05254884
Iteration 17, loss = 0.04860915
Iteration 18, loss = 0.04537376
Iteration 19, loss = 0.04292692
Iteration 20, loss = 0.04063246
Iteration 21, loss = 0.03879217
Iteration 22, loss = 0.03757590
Iteration 23, loss = 0.03613698
Iteration 24, loss = 0.03493060
Iteration 25, loss = 0.03386018
Iteration 26, loss = 0.03284309
Iteration 27, loss = 0.03195212
Iteration 28, loss = 0.03130175
Iteration 29, loss = 0.03072046
Iteration 30, loss = 0.03008721
Iteration 31, loss = 0.02965578
Iteration 32, loss = 0.02925281
Iteration 33, loss = 0.03016940
Iteration 34, loss = 0.03032772
Iteration 35, loss = 0.02924261
Iteration 36, loss = 0.02866830
Iteration 37, loss = 0.02817512
Iteration 38, loss = 0.02769624
Iteration 39, loss = 0.02751253
Iteration 40, loss = 0.02706877
Iteration 41, loss = 0.02679735
Iteration 42, loss = 0.02642249
Iteration 43, loss = 0.02641326
Iteration 44, loss = 0.02602906
Iteration 45, loss = 0.02572022
Iteration 46, loss = 0.02570145
Iteration 47, loss = 0.02555407
Iteration 48, loss = 0.02537876
Iteration 49, loss = 0.02534496
Iteration 50, loss = 0.02485588
Iteration 51, loss = 0.02489632
Iteration 52, loss = 0.02473639
Iteration 53, loss = 0.02463305
Iteration 54, loss = 0.02453608
Iteration 55, loss = 0.02429489
Iteration 56, loss = 0.02432408
Iteration 57, loss = 0.02434688
Iteration 58, loss = 0.02435562
Iteration 59, loss = 0.02418010
Iteration 60, loss = 0.02396566
Iteration 61, loss = 0.02432359
Iteration 62, loss = 0.02389835
Iteration 63, loss = 0.02385197
Iteration 64, loss = 0.02546861
Iteration 65, loss = 0.02577606
Iteration 66, loss = 0.02512377
Iteration 67, loss = 0.02471349
Iteration 68, loss = 0.02459446
Iteration 69, loss = 0.02438191
Iteration 70, loss = 0.02406551
Iteration 71, loss = 0.02436027
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70216280
Iteration 2, loss = 0.64602656
Iteration 3, loss = 0.56710835
Iteration 4, loss = 0.47272405
Iteration 5, loss = 0.38049301
Iteration 6, loss = 0.29930309
Iteration 7, loss = 0.23427805
Iteration 8, loss = 0.18452088
Iteration 9, loss = 0.14776310
Iteration 10, loss = 0.12067645
Iteration 11, loss = 0.10074446
Iteration 12, loss = 0.08569501
Iteration 13, loss = 0.07423932
Iteration 14, loss = 0.06546423
Iteration 15, loss = 0.05847121
Iteration 16, loss = 0.05281752
Iteration 17, loss = 0.04829906
Iteration 18, loss = 0.04477843
Iteration 19, loss = 0.04152296
Iteration 20, loss = 0.03875955
Iteration 21, loss = 0.03666429
Iteration 22, loss = 0.03462847
Iteration 23, loss = 0.03301623
Iteration 24, loss = 0.03166307
Iteration 25, loss = 0.03030155
Iteration 26, loss = 0.02949164
Iteration 27, loss = 0.02835947
Iteration 28, loss = 0.02758658
Iteration 29, loss = 0.02670661
Iteration 30, loss = 0.02610197
Iteration 31, loss = 0.02549961
Iteration 32, loss = 0.02482830
Iteration 33, loss = 0.02463152
Iteration 34, loss = 0.02400939
Iteration 35, loss = 0.02355077
Iteration 36, loss = 0.02320888
Iteration 37, loss = 0.02312533
Iteration 38, loss = 0.02254750
Iteration 39, loss = 0.02216728
Iteration 40, loss = 0.02179537
Iteration 41, loss = 0.02167487
Iteration 42, loss = 0.02140258
Iteration 43, loss = 0.02136183
Iteration 44, loss = 0.02133992
Iteration 45, loss = 0.02082161
Iteration 46, loss = 0.02068555
Iteration 47, loss = 0.02067342
Iteration 48, loss = 0.02038247
Iteration 49, loss = 0.02021444
Iteration 50, loss = 0.02017872
Iteration 51, loss = 0.01999100
Iteration 52, loss = 0.01985966
Iteration 53, loss = 0.02005736
Iteration 54, loss = 0.01964435
Iteration 55, loss = 0.01946963
Iteration 56, loss = 0.01959531
Iteration 57, loss = 0.01946401
Iteration 58, loss = 0.01937510
Iteration 59, loss = 0.01954795
Iteration 60, loss = 0.01916121
Iteration 61, loss = 0.01896389
Iteration 62, loss = 0.01920351
Iteration 63, loss = 0.01914739
Iteration 64, loss = 0.01885201
Iteration 65, loss = 0.01892527
Iteration 66, loss = 0.01883817
Iteration 67, loss = 0.01878364
Iteration 68, loss = 0.01886066
Iteration 69, loss = 0.01878682
Iteration 70, loss = 0.01893910
Iteration 71, loss = 0.01872883
Iteration 72, loss = 0.01859734
Iteration 73, loss = 0.01851903
Iteration 74, loss = 0.01849703
Iteration 75, loss = 0.01853409
Iteration 76, loss = 0.01851478
Iteration 77, loss = 0.01857500
Iteration 78, loss = 0.01831910
Iteration 79, loss = 0.01849258
Iteration 80, loss = 0.01826508
Iteration 81, loss = 0.01834994
Iteration 82, loss = 0.01809598
Iteration 83, loss = 0.01828948
Iteration 84, loss = 0.01818105
Iteration 85, loss = 0.01803213
Iteration 86, loss = 0.01813343
Iteration 87, loss = 0.01832317
Iteration 88, loss = 0.01808668
Iteration 89, loss = 0.01826483
Iteration 90, loss = 0.01802769
Iteration 91, loss = 0.01803818
Iteration 92, loss = 0.01794250
Iteration 93, loss = 0.01815918
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67804775
Iteration 2, loss = 0.65216110
Iteration 3, loss = 0.61688357
Iteration 4, loss = 0.56683217
Iteration 5, loss = 0.50722804
Iteration 6, loss = 0.44507416
Iteration 7, loss = 0.38476903
Iteration 8, loss = 0.32893491
Iteration 9, loss = 0.27936019
Iteration 10, loss = 0.23602056
Iteration 11, loss = 0.19978612
Iteration 12, loss = 0.16984057
Iteration 13, loss = 0.14543116
Iteration 14, loss = 0.12550757
Iteration 15, loss = 0.10935445
Iteration 16, loss = 0.09608943
Iteration 17, loss = 0.08527597
Iteration 18, loss = 0.07641128
Iteration 19, loss = 0.06904594
Iteration 20, loss = 0.06284021
Iteration 21, loss = 0.05755990
Iteration 22, loss = 0.05302259
Iteration 23, loss = 0.04917681
Iteration 24, loss = 0.04585925
Iteration 25, loss = 0.04299676
Iteration 26, loss = 0.04048992
Iteration 27, loss = 0.03828791
Iteration 28, loss = 0.03632025
Iteration 29, loss = 0.03455786
Iteration 30, loss = 0.03303880
Iteration 31, loss = 0.03180739
Iteration 32, loss = 0.03043287
Iteration 33, loss = 0.02929544
Iteration 34, loss = 0.02832985
Iteration 35, loss = 0.02738563
Iteration 36, loss = 0.02664678
Iteration 37, loss = 0.02580982
Iteration 38, loss = 0.02521842
Iteration 39, loss = 0.02457495
Iteration 40, loss = 0.02394312
Iteration 41, loss = 0.02347796
Iteration 42, loss = 0.02292975
Iteration 43, loss = 0.02253849
Iteration 44, loss = 0.02211705
Iteration 45, loss = 0.02160753
Iteration 46, loss = 0.02126799
Iteration 47, loss = 0.02083579
Iteration 48, loss = 0.02053826
Iteration 49, loss = 0.02028618
Iteration 50, loss = 0.01996431
Iteration 51, loss = 0.01974408
Iteration 52, loss = 0.01961373
Iteration 53, loss = 0.01934064
Iteration 54, loss = 0.01910587
Iteration 55, loss = 0.01884984
Iteration 56, loss = 0.01864008
Iteration 57, loss = 0.01854934
Iteration 58, loss = 0.01833975
Iteration 59, loss = 0.01810916
Iteration 60, loss = 0.01796090
Iteration 61, loss = 0.01777240
Iteration 62, loss = 0.01766881
Iteration 63, loss = 0.01749659
Iteration 64, loss = 0.01750669
Iteration 65, loss = 0.01722638
Iteration 66, loss = 0.01728914
Iteration 67, loss = 0.01711529
Iteration 68, loss = 0.01702814
Iteration 69, loss = 0.01683419
Iteration 70, loss = 0.01689331
Iteration 71, loss = 0.01669854
Iteration 72, loss = 0.01657353
Iteration 73, loss = 0.01653346
Iteration 74, loss = 0.01646076
Iteration 75, loss = 0.01634954
Iteration 76, loss = 0.01636758
Iteration 77, loss = 0.01630795
Iteration 78, loss = 0.01619609
Iteration 79, loss = 0.01620529
Iteration 80, loss = 0.01615686
Iteration 81, loss = 0.01590608
Iteration 82, loss = 0.01593972
Iteration 83, loss = 0.01586214
Iteration 84, loss = 0.01579882
Iteration 85, loss = 0.01573428
Iteration 86, loss = 0.01579353
Iteration 87, loss = 0.01585703
Iteration 88, loss = 0.01580760
Iteration 89, loss = 0.01559269
Iteration 90, loss = 0.01565811
Iteration 91, loss = 0.01549938
Iteration 92, loss = 0.01561861
Iteration 93, loss = 0.01540612
Iteration 94, loss = 0.01540853
Iteration 95, loss = 0.01543408
Iteration 96, loss = 0.01531458
Iteration 97, loss = 0.01534852
Iteration 98, loss = 0.01536262
Iteration 99, loss = 0.01534385
Iteration 100, loss = 0.01526927
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72750629
Iteration 2, loss = 0.70694871
Iteration 3, loss = 0.69009437
Iteration 4, loss = 0.67392182
Iteration 5, loss = 0.65746347
Iteration 6, loss = 0.64010029
Iteration 7, loss = 0.62166252
Iteration 8, loss = 0.60204595
Iteration 9, loss = 0.58173311
Iteration 10, loss = 0.56008613
Iteration 11, loss = 0.53788529
Iteration 12, loss = 0.51502034
Iteration 13, loss = 0.49188183
Iteration 14, loss = 0.46849105
Iteration 15, loss = 0.44516247
Iteration 16, loss = 0.42189964
Iteration 17, loss = 0.39909446
Iteration 18, loss = 0.37666021
Iteration 19, loss = 0.35475057
Iteration 20, loss = 0.33370795
Iteration 21, loss = 0.31337063
Iteration 22, loss = 0.29404133
Iteration 23, loss = 0.27566002
Iteration 24, loss = 0.25824636
Iteration 25, loss = 0.24181920
Iteration 26, loss = 0.22641826
Iteration 27, loss = 0.21201864
Iteration 28, loss = 0.19858018
Iteration 29, loss = 0.18608615
Iteration 30, loss = 0.17439209
Iteration 31, loss = 0.16360288
Iteration 32, loss = 0.15363590
Iteration 33, loss = 0.14437456
Iteration 34, loss = 0.13584351
Iteration 35, loss = 0.12794723
Iteration 36, loss = 0.12064451
Iteration 37, loss = 0.11385964
Iteration 38, loss = 0.10765575
Iteration 39, loss = 0.10189524
Iteration 40, loss = 0.09655007
Iteration 41, loss = 0.09165662
Iteration 42, loss = 0.08708304
Iteration 43, loss = 0.08283060
Iteration 44, loss = 0.07891582
Iteration 45, loss = 0.07528718
Iteration 46, loss = 0.07187053
Iteration 47, loss = 0.06876884
Iteration 48, loss = 0.06582879
Iteration 49, loss = 0.06303188
Iteration 50, loss = 0.06050144
Iteration 51, loss = 0.05806684
Iteration 52, loss = 0.05584359
Iteration 53, loss = 0.05375108
Iteration 54, loss = 0.05178262
Iteration 55, loss = 0.04992381
Iteration 56, loss = 0.04818698
Iteration 57, loss = 0.04652637
Iteration 58, loss = 0.04493908
Iteration 59, loss = 0.04350317
Iteration 60, loss = 0.04215277
Iteration 61, loss = 0.04082273
Iteration 62, loss = 0.03961373
Iteration 63, loss = 0.03842529
Iteration 64, loss = 0.03731026
Iteration 65, loss = 0.03626976
Iteration 66, loss = 0.03529940
Iteration 67, loss = 0.03435026
Iteration 68, loss = 0.03342320
Iteration 69, loss = 0.03259511
Iteration 70, loss = 0.03177240
Iteration 71, loss = 0.03098074
Iteration 72, loss = 0.03026081
Iteration 73, loss = 0.02953477
Iteration 74, loss = 0.02887528
Iteration 75, loss = 0.02822308
Iteration 76, loss = 0.02764715
Iteration 77, loss = 0.02705584
Iteration 78, loss = 0.02650400
Iteration 79, loss = 0.02593159
Iteration 80, loss = 0.02543993
Iteration 81, loss = 0.02492957
Iteration 82, loss = 0.02446131
Iteration 83, loss = 0.02403019
Iteration 84, loss = 0.02355510
Iteration 85, loss = 0.02317592
Iteration 86, loss = 0.02271656
Iteration 87, loss = 0.02230129
Iteration 88, loss = 0.02190430
Iteration 89, loss = 0.02152715
Iteration 90, loss = 0.02116944
Iteration 91, loss = 0.02083861
Iteration 92, loss = 0.02051731
Iteration 93, loss = 0.02021178
Iteration 94, loss = 0.01989862
Iteration 95, loss = 0.01960555
Iteration 96, loss = 0.01932577
Iteration 97, loss = 0.01904201
Iteration 98, loss = 0.01878867
Iteration 99, loss = 0.01853603
Iteration 100, loss = 0.01829855
Iteration 101, loss = 0.01804013
Iteration 102, loss = 0.01778406
Iteration 103, loss = 0.01755910
Iteration 104, loss = 0.01736450
Iteration 105, loss = 0.01713341
Iteration 106, loss = 0.01693692
Iteration 107, loss = 0.01673462
Iteration 108, loss = 0.01654899
Iteration 109, loss = 0.01637840
Iteration 110, loss = 0.01616071
Iteration 111, loss = 0.01599414
Iteration 112, loss = 0.01579335
Iteration 113, loss = 0.01563681
Iteration 114, loss = 0.01550994
Iteration 115, loss = 0.01535471
Iteration 116, loss = 0.01517678
Iteration 117, loss = 0.01504282
Iteration 118, loss = 0.01487334
Iteration 119, loss = 0.01474250
Iteration 120, loss = 0.01456454
Iteration 121, loss = 0.01449928
Iteration 122, loss = 0.01431180
Iteration 123, loss = 0.01418125
Iteration 124, loss = 0.01409581
Iteration 125, loss = 0.01399106
Iteration 126, loss = 0.01388653
Iteration 127, loss = 0.01377474
Iteration 128, loss = 0.01366904
Iteration 129, loss = 0.01355932
Iteration 130, loss = 0.01344124
Iteration 131, loss = 0.01335652
Iteration 132, loss = 0.01323792
Iteration 133, loss = 0.01312224
Iteration 134, loss = 0.01302209
Iteration 135, loss = 0.01293487
Iteration 136, loss = 0.01283099
Iteration 137, loss = 0.01275812
Iteration 138, loss = 0.01265436
Iteration 139, loss = 0.01256610
Iteration 140, loss = 0.01249771
Iteration 141, loss = 0.01238063
Iteration 142, loss = 0.01230123
Iteration 143, loss = 0.01221742
Iteration 144, loss = 0.01214204
Iteration 145, loss = 0.01204371
Iteration 146, loss = 0.01199126
Iteration 147, loss = 0.01193293
Iteration 148, loss = 0.01189557
Iteration 149, loss = 0.01180274
Iteration 150, loss = 0.01176414
Iteration 151, loss = 0.01169138
Iteration 152, loss = 0.01161821
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72925701
Iteration 2, loss = 0.70006051
Iteration 3, loss = 0.67292587
Iteration 4, loss = 0.64400172
Iteration 5, loss = 0.61247623
Iteration 6, loss = 0.57987631
Iteration 7, loss = 0.54601878
Iteration 8, loss = 0.51116506
Iteration 9, loss = 0.47609217
Iteration 10, loss = 0.44093263
Iteration 11, loss = 0.40623666
Iteration 12, loss = 0.37242238
Iteration 13, loss = 0.34002591
Iteration 14, loss = 0.30956470
Iteration 15, loss = 0.28114677
Iteration 16, loss = 0.25507962
Iteration 17, loss = 0.23123254
Iteration 18, loss = 0.20969623
Iteration 19, loss = 0.19035004
Iteration 20, loss = 0.17290868
Iteration 21, loss = 0.15736751
Iteration 22, loss = 0.14358571
Iteration 23, loss = 0.13135189
Iteration 24, loss = 0.12046961
Iteration 25, loss = 0.11085547
Iteration 26, loss = 0.10231839
Iteration 27, loss = 0.09477015
Iteration 28, loss = 0.08795963
Iteration 29, loss = 0.08192012
Iteration 30, loss = 0.07651311
Iteration 31, loss = 0.07167487
Iteration 32, loss = 0.06734475
Iteration 33, loss = 0.06339006
Iteration 34, loss = 0.05984877
Iteration 35, loss = 0.05663136
Iteration 36, loss = 0.05372228
Iteration 37, loss = 0.05104256
Iteration 38, loss = 0.04857916
Iteration 39, loss = 0.04639451
Iteration 40, loss = 0.04435216
Iteration 41, loss = 0.04248608
Iteration 42, loss = 0.04072346
Iteration 43, loss = 0.03914390
Iteration 44, loss = 0.03766113
Iteration 45, loss = 0.03634266
Iteration 46, loss = 0.03502515
Iteration 47, loss = 0.03387281
Iteration 48, loss = 0.03276497
Iteration 49, loss = 0.03177508
Iteration 50, loss = 0.03077384
Iteration 51, loss = 0.02988227
Iteration 52, loss = 0.02903183
Iteration 53, loss = 0.02828846
Iteration 54, loss = 0.02752927
Iteration 55, loss = 0.02687042
Iteration 56, loss = 0.02617514
Iteration 57, loss = 0.02556479
Iteration 58, loss = 0.02494443
Iteration 59, loss = 0.02441997
Iteration 60, loss = 0.02388999
Iteration 61, loss = 0.02336215
Iteration 62, loss = 0.02289524
Iteration 63, loss = 0.02246680
Iteration 64, loss = 0.02204040
Iteration 65, loss = 0.02163140
Iteration 66, loss = 0.02124887
Iteration 67, loss = 0.02089774
Iteration 68, loss = 0.02056061
Iteration 69, loss = 0.02018766
Iteration 70, loss = 0.01993336
Iteration 71, loss = 0.01960084
Iteration 72, loss = 0.01929821
Iteration 73, loss = 0.01901993
Iteration 74, loss = 0.01878712
Iteration 75, loss = 0.01855103
Iteration 76, loss = 0.01825939
Iteration 77, loss = 0.01804083
Iteration 78, loss = 0.01780820
Iteration 79, loss = 0.01762385
Iteration 80, loss = 0.01742408
Iteration 81, loss = 0.01719446
Iteration 82, loss = 0.01703117
Iteration 83, loss = 0.01680606
Iteration 84, loss = 0.01667952
Iteration 85, loss = 0.01647745
Iteration 86, loss = 0.01632248
Iteration 87, loss = 0.01622430
Iteration 88, loss = 0.01599576
Iteration 89, loss = 0.01585105
Iteration 90, loss = 0.01572933
Iteration 91, loss = 0.01557933
Iteration 92, loss = 0.01553572
Iteration 93, loss = 0.01536055
Iteration 94, loss = 0.01525733
Iteration 95, loss = 0.01510671
Iteration 96, loss = 0.01497950
Iteration 97, loss = 0.01489166
Iteration 98, loss = 0.01477057
Iteration 99, loss = 0.01465634
Iteration 100, loss = 0.01459612
Iteration 101, loss = 0.01447165
Iteration 102, loss = 0.01436705
Iteration 103, loss = 0.01429861
Iteration 104, loss = 0.01417931
Iteration 105, loss = 0.01416334
Iteration 106, loss = 0.01403583
Iteration 107, loss = 0.01391539
Iteration 108, loss = 0.01386049
Iteration 109, loss = 0.01379963
Iteration 110, loss = 0.01374239
Iteration 111, loss = 0.01365889
Iteration 112, loss = 0.01357207
Iteration 113, loss = 0.01351312
Iteration 114, loss = 0.01344731
Iteration 115, loss = 0.01337071
Iteration 116, loss = 0.01331265
Iteration 117, loss = 0.01327060
Iteration 118, loss = 0.01318163
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69231239
Iteration 2, loss = 0.65408516
Iteration 3, loss = 0.60343937
Iteration 4, loss = 0.53650702
Iteration 5, loss = 0.46173534
Iteration 6, loss = 0.38900073
Iteration 7, loss = 0.32274443
Iteration 8, loss = 0.26577634
Iteration 9, loss = 0.21861542
Iteration 10, loss = 0.18075042
Iteration 11, loss = 0.15070250
Iteration 12, loss = 0.12715349
Iteration 13, loss = 0.10841063
Iteration 14, loss = 0.09384034
Iteration 15, loss = 0.08205867
Iteration 16, loss = 0.07258928
Iteration 17, loss = 0.06499364
Iteration 18, loss = 0.05856841
Iteration 19, loss = 0.05317357
Iteration 20, loss = 0.04884002
Iteration 21, loss = 0.04495255
Iteration 22, loss = 0.04173328
Iteration 23, loss = 0.03899456
Iteration 24, loss = 0.03658300
Iteration 25, loss = 0.03444722
Iteration 26, loss = 0.03268340
Iteration 27, loss = 0.03096162
Iteration 28, loss = 0.02953794
Iteration 29, loss = 0.02822840
Iteration 30, loss = 0.02703499
Iteration 31, loss = 0.02608582
Iteration 32, loss = 0.02525436
Iteration 33, loss = 0.02434343
Iteration 34, loss = 0.02362841
Iteration 35, loss = 0.02295929
Iteration 36, loss = 0.02222796
Iteration 37, loss = 0.02173859
Iteration 38, loss = 0.02120755
Iteration 39, loss = 0.02076904
Iteration 40, loss = 0.02024288
Iteration 41, loss = 0.01984074
Iteration 42, loss = 0.01941807
Iteration 43, loss = 0.01900202
Iteration 44, loss = 0.01876797
Iteration 45, loss = 0.01854454
Iteration 46, loss = 0.01830284
Iteration 47, loss = 0.01794071
Iteration 48, loss = 0.01763907
Iteration 49, loss = 0.01740959
Iteration 50, loss = 0.01727480
Iteration 51, loss = 0.01713028
Iteration 52, loss = 0.01680698
Iteration 53, loss = 0.01661209
Iteration 54, loss = 0.01662317
Iteration 55, loss = 0.01631351
Iteration 56, loss = 0.01605943
Iteration 57, loss = 0.01599137
Iteration 58, loss = 0.01597025
Iteration 59, loss = 0.01585533
Iteration 60, loss = 0.01563921
Iteration 61, loss = 0.01545876
Iteration 62, loss = 0.01539070
Iteration 63, loss = 0.01522346
Iteration 64, loss = 0.01537024
Iteration 65, loss = 0.01521343
Iteration 66, loss = 0.01503779
Iteration 67, loss = 0.01506800
Iteration 68, loss = 0.01485417
Iteration 69, loss = 0.01482079
Iteration 70, loss = 0.01471409
Iteration 71, loss = 0.01480993
Iteration 72, loss = 0.01458405
Iteration 73, loss = 0.01448235
Iteration 74, loss = 0.01443350
Iteration 75, loss = 0.01438199
Iteration 76, loss = 0.01428141
Iteration 77, loss = 0.01425868
Iteration 78, loss = 0.01440040
Iteration 79, loss = 0.01421559
Iteration 80, loss = 0.01417604
Iteration 81, loss = 0.01400194
Iteration 82, loss = 0.01396206
Iteration 83, loss = 0.01407000
Iteration 84, loss = 0.01406160
Iteration 85, loss = 0.01396312
Iteration 86, loss = 0.01409858
Iteration 87, loss = 0.01372881
Iteration 88, loss = 0.01385404
Iteration 89, loss = 0.01369780
Iteration 90, loss = 0.01371687
Iteration 91, loss = 0.01377935
Iteration 92, loss = 0.01366178
Iteration 93, loss = 0.01366422
Iteration 94, loss = 0.01354957
Iteration 95, loss = 0.01356354
Iteration 96, loss = 0.01349257
Iteration 97, loss = 0.01353834
Iteration 98, loss = 0.01345339
Iteration 99, loss = 0.01352393
Iteration 100, loss = 0.01365565
Iteration 101, loss = 0.01349793
Iteration 102, loss = 0.01360385
Iteration 103, loss = 0.01341056
Iteration 104, loss = 0.01353054
Iteration 105, loss = 0.01349452
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70681265
Iteration 2, loss = 0.64024003
Iteration 3, loss = 0.54762736
Iteration 4, loss = 0.44299401
Iteration 5, loss = 0.34585280
Iteration 6, loss = 0.26577458
Iteration 7, loss = 0.20505580
Iteration 8, loss = 0.16203024
Iteration 9, loss = 0.13097457
Iteration 10, loss = 0.10774833
Iteration 11, loss = 0.09092984
Iteration 12, loss = 0.07850311
Iteration 13, loss = 0.06896619
Iteration 14, loss = 0.06123805
Iteration 15, loss = 0.05536473
Iteration 16, loss = 0.05071689
Iteration 17, loss = 0.04683075
Iteration 18, loss = 0.04361276
Iteration 19, loss = 0.04084267
Iteration 20, loss = 0.03867799
Iteration 21, loss = 0.03688155
Iteration 22, loss = 0.03521083
Iteration 23, loss = 0.03369992
Iteration 24, loss = 0.03241392
Iteration 25, loss = 0.03136283
Iteration 26, loss = 0.03034621
Iteration 27, loss = 0.02995664
Iteration 28, loss = 0.02919782
Iteration 29, loss = 0.02847220
Iteration 30, loss = 0.02812170
Iteration 31, loss = 0.02734428
Iteration 32, loss = 0.02696366
Iteration 33, loss = 0.02636275
Iteration 34, loss = 0.02604345
Iteration 35, loss = 0.02555755
Iteration 36, loss = 0.02499400
Iteration 37, loss = 0.02475912
Iteration 38, loss = 0.02455992
Iteration 39, loss = 0.02428161
Iteration 40, loss = 0.02391436
Iteration 41, loss = 0.02374806
Iteration 42, loss = 0.02358431
Iteration 43, loss = 0.02324368
Iteration 44, loss = 0.02298516
Iteration 45, loss = 0.02271365
Iteration 46, loss = 0.02269138
Iteration 47, loss = 0.02258810
Iteration 48, loss = 0.02242838
Iteration 49, loss = 0.02212268
Iteration 50, loss = 0.02231297
Iteration 51, loss = 0.02192034
Iteration 52, loss = 0.02203001
Iteration 53, loss = 0.02171244
Iteration 54, loss = 0.02159499
Iteration 55, loss = 0.02160270
Iteration 56, loss = 0.02143103
Iteration 57, loss = 0.02163853
Iteration 58, loss = 0.02123593
Iteration 59, loss = 0.02116178
Iteration 60, loss = 0.02105757
Iteration 61, loss = 0.02114512
Iteration 62, loss = 0.02101715
Iteration 63, loss = 0.02087543
Iteration 64, loss = 0.02094453
Iteration 65, loss = 0.02093273
Iteration 66, loss = 0.02073177
Iteration 67, loss = 0.02073166
Iteration 68, loss = 0.02100043
Iteration 69, loss = 0.02077315
Iteration 70, loss = 0.02060481
Iteration 71, loss = 0.02072293
Iteration 72, loss = 0.02260125
Iteration 73, loss = 0.02302298
Iteration 74, loss = 0.02257891
Iteration 75, loss = 0.02220855
Iteration 76, loss = 0.02208220
Iteration 77, loss = 0.02191521
Iteration 78, loss = 0.02151905
Iteration 79, loss = 0.02117317
Iteration 80, loss = 0.02112744
Iteration 81, loss = 0.02081199
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71407498
Iteration 2, loss = 0.68148152
Iteration 3, loss = 0.64312060
Iteration 4, loss = 0.59632003
Iteration 5, loss = 0.54473002
Iteration 6, loss = 0.49049076
Iteration 7, loss = 0.43659419
Iteration 8, loss = 0.38454377
Iteration 9, loss = 0.33583285
Iteration 10, loss = 0.29176136
Iteration 11, loss = 0.25318548
Iteration 12, loss = 0.21958705
Iteration 13, loss = 0.19102295
Iteration 14, loss = 0.16669236
Iteration 15, loss = 0.14645340
Iteration 16, loss = 0.12925411
Iteration 17, loss = 0.11489248
Iteration 18, loss = 0.10281884
Iteration 19, loss = 0.09244870
Iteration 20, loss = 0.08375452
Iteration 21, loss = 0.07622038
Iteration 22, loss = 0.07005969
Iteration 23, loss = 0.06449799
Iteration 24, loss = 0.05975427
Iteration 25, loss = 0.05565823
Iteration 26, loss = 0.05189884
Iteration 27, loss = 0.04863417
Iteration 28, loss = 0.04566574
Iteration 29, loss = 0.04310829
Iteration 30, loss = 0.04079143
Iteration 31, loss = 0.03879153
Iteration 32, loss = 0.03695923
Iteration 33, loss = 0.03532499
Iteration 34, loss = 0.03376357
Iteration 35, loss = 0.03234104
Iteration 36, loss = 0.03102774
Iteration 37, loss = 0.02985313
Iteration 38, loss = 0.02887105
Iteration 39, loss = 0.02786126
Iteration 40, loss = 0.02722072
Iteration 41, loss = 0.02633206
Iteration 42, loss = 0.02572398
Iteration 43, loss = 0.02503157
Iteration 44, loss = 0.02420589
Iteration 45, loss = 0.02355913
Iteration 46, loss = 0.02303133
Iteration 47, loss = 0.02253490
Iteration 48, loss = 0.02204065
Iteration 49, loss = 0.02153172
Iteration 50, loss = 0.02109379
Iteration 51, loss = 0.02072795
Iteration 52, loss = 0.02033466
Iteration 53, loss = 0.02011396
Iteration 54, loss = 0.01992796
Iteration 55, loss = 0.01950557
Iteration 56, loss = 0.01911136
Iteration 57, loss = 0.01877361
Iteration 58, loss = 0.01857605
Iteration 59, loss = 0.01825479
Iteration 60, loss = 0.01793323
Iteration 61, loss = 0.01777466
Iteration 62, loss = 0.01749151
Iteration 63, loss = 0.01731184
Iteration 64, loss = 0.01705026
Iteration 65, loss = 0.01683403
Iteration 66, loss = 0.01663974
Iteration 67, loss = 0.01648650
Iteration 68, loss = 0.01633544
Iteration 69, loss = 0.01609650
Iteration 70, loss = 0.01609418
Iteration 71, loss = 0.01594885
Iteration 72, loss = 0.01585552
Iteration 73, loss = 0.01562170
Iteration 74, loss = 0.01549702
Iteration 75, loss = 0.01541474
Iteration 76, loss = 0.01551077
Iteration 77, loss = 0.01550797
Iteration 78, loss = 0.01548612
Iteration 79, loss = 0.01524220
Iteration 80, loss = 0.01511191
Iteration 81, loss = 0.01488760
Iteration 82, loss = 0.01476952
Iteration 83, loss = 0.01453607
Iteration 84, loss = 0.01457170
Iteration 85, loss = 0.01440826
Iteration 86, loss = 0.01459894
Iteration 87, loss = 0.01463098
Iteration 88, loss = 0.01474767
Iteration 89, loss = 0.01453948
Iteration 90, loss = 0.01445273
Iteration 91, loss = 0.01434383
Iteration 92, loss = 0.01438728
Iteration 93, loss = 0.01407147
Iteration 94, loss = 0.01426051
Iteration 95, loss = 0.01408225
Iteration 96, loss = 0.01394038
Iteration 97, loss = 0.01381918
Iteration 98, loss = 0.01368381
Iteration 99, loss = 0.01356461
Iteration 100, loss = 0.01357155
Iteration 101, loss = 0.01397058
Iteration 102, loss = 0.01423554
Iteration 103, loss = 0.01371655
Iteration 104, loss = 0.01413177
Iteration 105, loss = 0.01443535
Iteration 106, loss = 0.01403782
Iteration 107, loss = 0.01366833
Iteration 108, loss = 0.01329170
Iteration 109, loss = 0.01318689
Iteration 110, loss = 0.01314003
Iteration 111, loss = 0.01306947
Iteration 112, loss = 0.01298917
Iteration 113, loss = 0.01296895
Iteration 114, loss = 0.01304143
Iteration 115, loss = 0.01306020
Iteration 116, loss = 0.01302215
Iteration 117, loss = 0.01313763
Iteration 118, loss = 0.01312724
Iteration 119, loss = 0.01310007
Iteration 120, loss = 0.01322156
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67686632
Iteration 2, loss = 0.61466736
Iteration 3, loss = 0.52497928
Iteration 4, loss = 0.42734162
Iteration 5, loss = 0.33521964
Iteration 6, loss = 0.25789948
Iteration 7, loss = 0.19930574
Iteration 8, loss = 0.15602332
Iteration 9, loss = 0.12520016
Iteration 10, loss = 0.10290171
Iteration 11, loss = 0.08665025
Iteration 12, loss = 0.07452778
Iteration 13, loss = 0.06497513
Iteration 14, loss = 0.05786613
Iteration 15, loss = 0.05221153
Iteration 16, loss = 0.04765857
Iteration 17, loss = 0.04396793
Iteration 18, loss = 0.04089503
Iteration 19, loss = 0.03843228
Iteration 20, loss = 0.03624200
Iteration 21, loss = 0.03439786
Iteration 22, loss = 0.03278144
Iteration 23, loss = 0.03135699
Iteration 24, loss = 0.03081110
Iteration 25, loss = 0.02956177
Iteration 26, loss = 0.02880969
Iteration 27, loss = 0.02776481
Iteration 28, loss = 0.02715045
Iteration 29, loss = 0.02648783
Iteration 30, loss = 0.02604348
Iteration 31, loss = 0.02582202
Iteration 32, loss = 0.02599861
Iteration 33, loss = 0.02525969
Iteration 34, loss = 0.02467974
Iteration 35, loss = 0.02556650
Iteration 36, loss = 0.02639617
Iteration 37, loss = 0.02558894
Iteration 38, loss = 0.02483884
Iteration 39, loss = 0.02453608
Iteration 40, loss = 0.02405611
Iteration 41, loss = 0.02344323
Iteration 42, loss = 0.02311647
Iteration 43, loss = 0.02286319
Iteration 44, loss = 0.02227775
Iteration 45, loss = 0.02205528
Iteration 46, loss = 0.02207554
Iteration 47, loss = 0.02166560
Iteration 48, loss = 0.02128045
Iteration 49, loss = 0.02120014
Iteration 50, loss = 0.02100308
Iteration 51, loss = 0.02107798
Iteration 52, loss = 0.02071431
Iteration 53, loss = 0.02061921
Iteration 54, loss = 0.02060341
Iteration 55, loss = 0.02032279
Iteration 56, loss = 0.02040871
Iteration 57, loss = 0.02039509
Iteration 58, loss = 0.02007063
Iteration 59, loss = 0.02020373
Iteration 60, loss = 0.02126553
Iteration 61, loss = 0.02307830
Iteration 62, loss = 0.02238888
Iteration 63, loss = 0.02196362
Iteration 64, loss = 0.02155927
Iteration 65, loss = 0.02123128
Iteration 66, loss = 0.02080316
Iteration 67, loss = 0.02065234
Iteration 68, loss = 0.02055590
Iteration 69, loss = 0.02200459
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69118831
Iteration 2, loss = 0.67469518
Iteration 3, loss = 0.66042353
Iteration 4, loss = 0.64626273
Iteration 5, loss = 0.63117677
Iteration 6, loss = 0.61476211
Iteration 7, loss = 0.59689340
Iteration 8, loss = 0.57782614
Iteration 9, loss = 0.55699739
Iteration 10, loss = 0.53495493
Iteration 11, loss = 0.51187544
Iteration 12, loss = 0.48803137
Iteration 13, loss = 0.46388955
Iteration 14, loss = 0.43959649
Iteration 15, loss = 0.41557344
Iteration 16, loss = 0.39196340
Iteration 17, loss = 0.36884743
Iteration 18, loss = 0.34661577
Iteration 19, loss = 0.32528932
Iteration 20, loss = 0.30484648
Iteration 21, loss = 0.28558591
Iteration 22, loss = 0.26710288
Iteration 23, loss = 0.24984886
Iteration 24, loss = 0.23351277
Iteration 25, loss = 0.21831793
Iteration 26, loss = 0.20405881
Iteration 27, loss = 0.19076607
Iteration 28, loss = 0.17845266
Iteration 29, loss = 0.16706303
Iteration 30, loss = 0.15649291
Iteration 31, loss = 0.14670544
Iteration 32, loss = 0.13764543
Iteration 33, loss = 0.12932661
Iteration 34, loss = 0.12160053
Iteration 35, loss = 0.11447063
Iteration 36, loss = 0.10794421
Iteration 37, loss = 0.10181675
Iteration 38, loss = 0.09616087
Iteration 39, loss = 0.09101611
Iteration 40, loss = 0.08618292
Iteration 41, loss = 0.08175425
Iteration 42, loss = 0.07760275
Iteration 43, loss = 0.07373905
Iteration 44, loss = 0.07013786
Iteration 45, loss = 0.06680255
Iteration 46, loss = 0.06370694
Iteration 47, loss = 0.06076333
Iteration 48, loss = 0.05805692
Iteration 49, loss = 0.05555686
Iteration 50, loss = 0.05319020
Iteration 51, loss = 0.05100215
Iteration 52, loss = 0.04893629
Iteration 53, loss = 0.04697288
Iteration 54, loss = 0.04513884
Iteration 55, loss = 0.04342544
Iteration 56, loss = 0.04179228
Iteration 57, loss = 0.04025291
Iteration 58, loss = 0.03878670
Iteration 59, loss = 0.03741138
Iteration 60, loss = 0.03612573
Iteration 61, loss = 0.03488150
Iteration 62, loss = 0.03372113
Iteration 63, loss = 0.03260198
Iteration 64, loss = 0.03155545
Iteration 65, loss = 0.03056213
Iteration 66, loss = 0.02963044
Iteration 67, loss = 0.02871101
Iteration 68, loss = 0.02784984
Iteration 69, loss = 0.02704042
Iteration 70, loss = 0.02625142
Iteration 71, loss = 0.02550479
Iteration 72, loss = 0.02480351
Iteration 73, loss = 0.02412325
Iteration 74, loss = 0.02347109
Iteration 75, loss = 0.02283980
Iteration 76, loss = 0.02224169
Iteration 77, loss = 0.02166935
Iteration 78, loss = 0.02110484
Iteration 79, loss = 0.02058038
Iteration 80, loss = 0.02005087
Iteration 81, loss = 0.01956153
Iteration 82, loss = 0.01909721
Iteration 83, loss = 0.01863604
Iteration 84, loss = 0.01820335
Iteration 85, loss = 0.01779845
Iteration 86, loss = 0.01737419
Iteration 87, loss = 0.01698087
Iteration 88, loss = 0.01659664
Iteration 89, loss = 0.01623681
Iteration 90, loss = 0.01589827
Iteration 91, loss = 0.01554309
Iteration 92, loss = 0.01521558
Iteration 93, loss = 0.01489139
Iteration 94, loss = 0.01457993
Iteration 95, loss = 0.01428350
Iteration 96, loss = 0.01399881
Iteration 97, loss = 0.01372324
Iteration 98, loss = 0.01346035
Iteration 99, loss = 0.01319643
Iteration 100, loss = 0.01295024
Iteration 101, loss = 0.01270094
Iteration 102, loss = 0.01246526
Iteration 103, loss = 0.01223994
Iteration 104, loss = 0.01201749
Iteration 105, loss = 0.01179278
Iteration 106, loss = 0.01158895
Iteration 107, loss = 0.01138579
Iteration 108, loss = 0.01118352
Iteration 109, loss = 0.01099160
Iteration 110, loss = 0.01080404
Iteration 111, loss = 0.01062286
Iteration 112, loss = 0.01044676
Iteration 113, loss = 0.01027318
Iteration 114, loss = 0.01010991
Iteration 115, loss = 0.00994479
Iteration 116, loss = 0.00978664
Iteration 117, loss = 0.00963182
Iteration 118, loss = 0.00948874
Iteration 119, loss = 0.00934096
Iteration 120, loss = 0.00919418
Iteration 121, loss = 0.00905274
Iteration 122, loss = 0.00891683
Iteration 123, loss = 0.00878647
Iteration 124, loss = 0.00866407
Iteration 125, loss = 0.00853193
Iteration 126, loss = 0.00841851
Iteration 127, loss = 0.00829777
Iteration 128, loss = 0.00817592
Iteration 129, loss = 0.00806359
Iteration 130, loss = 0.00795445
Iteration 131, loss = 0.00784441
Iteration 132, loss = 0.00773617
Iteration 133, loss = 0.00763683
Iteration 134, loss = 0.00753413
Iteration 135, loss = 0.00743474
Iteration 136, loss = 0.00733759
Iteration 137, loss = 0.00724244
Iteration 138, loss = 0.00714963
Iteration 139, loss = 0.00705868
Iteration 140, loss = 0.00696994
Iteration 141, loss = 0.00688329
Iteration 142, loss = 0.00679940
Iteration 143, loss = 0.00671685
Iteration 144, loss = 0.00663969
Iteration 145, loss = 0.00655673
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67395448
Iteration 2, loss = 0.60612676
Iteration 3, loss = 0.50324819
Iteration 4, loss = 0.39164210
Iteration 5, loss = 0.29216922
Iteration 6, loss = 0.21646606
Iteration 7, loss = 0.16272073
Iteration 8, loss = 0.12636385
Iteration 9, loss = 0.10105400
Iteration 10, loss = 0.08354224
Iteration 11, loss = 0.07074639
Iteration 12, loss = 0.06112282
Iteration 13, loss = 0.05379002
Iteration 14, loss = 0.04820746
Iteration 15, loss = 0.04370237
Iteration 16, loss = 0.03996270
Iteration 17, loss = 0.03719417
Iteration 18, loss = 0.03471085
Iteration 19, loss = 0.03272647
Iteration 20, loss = 0.03120200
Iteration 21, loss = 0.02974852
Iteration 22, loss = 0.02810345
Iteration 23, loss = 0.02713656
Iteration 24, loss = 0.02645594
Iteration 25, loss = 0.02570487
Iteration 26, loss = 0.02460884
Iteration 27, loss = 0.02415301
Iteration 28, loss = 0.02336099
Iteration 29, loss = 0.02309813
Iteration 30, loss = 0.02260377
Iteration 31, loss = 0.02202992
Iteration 32, loss = 0.02167151
Iteration 33, loss = 0.02143171
Iteration 34, loss = 0.02093438
Iteration 35, loss = 0.02097879
Iteration 36, loss = 0.02051128
Iteration 37, loss = 0.02029744
Iteration 38, loss = 0.02031286
Iteration 39, loss = 0.02007427
Iteration 40, loss = 0.01972841
Iteration 41, loss = 0.01956050
Iteration 42, loss = 0.01934399
Iteration 43, loss = 0.01921677
Iteration 44, loss = 0.01904752
Iteration 45, loss = 0.01879446
Iteration 46, loss = 0.01891755
Iteration 47, loss = 0.01882729
Iteration 48, loss = 0.01858410
Iteration 49, loss = 0.01850686
Iteration 50, loss = 0.01882276
Iteration 51, loss = 0.01866071
Iteration 52, loss = 0.01841575
Iteration 53, loss = 0.01815546
Iteration 54, loss = 0.01842617
Iteration 55, loss = 0.01799093
Iteration 56, loss = 0.01805651
Iteration 57, loss = 0.01793526
Iteration 58, loss = 0.01803715
Iteration 59, loss = 0.01805021
Iteration 60, loss = 0.01789858
Iteration 61, loss = 0.01768711
Iteration 62, loss = 0.01778515
Iteration 63, loss = 0.01751244
Iteration 64, loss = 0.01758807
Iteration 65, loss = 0.01772236
Iteration 66, loss = 0.01760665
Iteration 67, loss = 0.01722125
Iteration 68, loss = 0.01763566
Iteration 69, loss = 0.01738119
Iteration 70, loss = 0.01767355
Iteration 71, loss = 0.01729744
Iteration 72, loss = 0.01731441
Iteration 73, loss = 0.01737672
Iteration 74, loss = 0.01733267
Iteration 75, loss = 0.01722483
Iteration 76, loss = 0.01720930
Iteration 77, loss = 0.01713848
Iteration 78, loss = 0.01734947
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70988107
Iteration 2, loss = 0.62519921
Iteration 3, loss = 0.51380297
Iteration 4, loss = 0.39333465
Iteration 5, loss = 0.29364058
Iteration 6, loss = 0.21846615
Iteration 7, loss = 0.16583603
Iteration 8, loss = 0.12982338
Iteration 9, loss = 0.10495187
Iteration 10, loss = 0.08734541
Iteration 11, loss = 0.07455376
Iteration 12, loss = 0.06512240
Iteration 13, loss = 0.05786565
Iteration 14, loss = 0.05250168
Iteration 15, loss = 0.04789309
Iteration 16, loss = 0.04452458
Iteration 17, loss = 0.04150721
Iteration 18, loss = 0.03920140
Iteration 19, loss = 0.03702248
Iteration 20, loss = 0.03539768
Iteration 21, loss = 0.03396092
Iteration 22, loss = 0.03264640
Iteration 23, loss = 0.03155200
Iteration 24, loss = 0.03090125
Iteration 25, loss = 0.02986888
Iteration 26, loss = 0.02946777
Iteration 27, loss = 0.02892741
Iteration 28, loss = 0.02804122
Iteration 29, loss = 0.02781795
Iteration 30, loss = 0.02747643
Iteration 31, loss = 0.02688493
Iteration 32, loss = 0.02650817
Iteration 33, loss = 0.02607883
Iteration 34, loss = 0.02594354
Iteration 35, loss = 0.02556708
Iteration 36, loss = 0.02524946
Iteration 37, loss = 0.02525565
Iteration 38, loss = 0.02500933
Iteration 39, loss = 0.02474407
Iteration 40, loss = 0.02429800
Iteration 41, loss = 0.02408888
Iteration 42, loss = 0.02442533
Iteration 43, loss = 0.02414091
Iteration 44, loss = 0.02387848
Iteration 45, loss = 0.02445788
Iteration 46, loss = 0.02384262
Iteration 47, loss = 0.02336417
Iteration 48, loss = 0.02336916
Iteration 49, loss = 0.02330679
Iteration 50, loss = 0.02348551
Iteration 51, loss = 0.02334470
Iteration 52, loss = 0.02312791
Iteration 53, loss = 0.02309932
Iteration 54, loss = 0.02309005
Iteration 55, loss = 0.02321329
Iteration 56, loss = 0.02294804
Iteration 57, loss = 0.02299646
Iteration 58, loss = 0.02276000
Iteration 59, loss = 0.02289933
Iteration 60, loss = 0.02251316
Iteration 61, loss = 0.02292538
Iteration 62, loss = 0.02276450
Iteration 63, loss = 0.02246278
Iteration 64, loss = 0.02280520
Iteration 65, loss = 0.02255859
Iteration 66, loss = 0.02235872
Iteration 67, loss = 0.02238791
Iteration 68, loss = 0.02232895
Iteration 69, loss = 0.02234270
Iteration 70, loss = 0.02208356
Iteration 71, loss = 0.02239880
Iteration 72, loss = 0.02215752
Iteration 73, loss = 0.02217674
Iteration 74, loss = 0.02219414
Iteration 75, loss = 0.02218524
Iteration 76, loss = 0.02203612
Iteration 77, loss = 0.02227684
Iteration 78, loss = 0.02230726
Iteration 79, loss = 0.02231664
Iteration 80, loss = 0.02214225
Iteration 81, loss = 0.02227097
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68147153
Iteration 2, loss = 0.64297829
Iteration 3, loss = 0.59205632
Iteration 4, loss = 0.52698255
Iteration 5, loss = 0.45702120
Iteration 6, loss = 0.38881441
Iteration 7, loss = 0.32540860
Iteration 8, loss = 0.27021689
Iteration 9, loss = 0.22376190
Iteration 10, loss = 0.18582656
Iteration 11, loss = 0.15580413
Iteration 12, loss = 0.13197725
Iteration 13, loss = 0.11306792
Iteration 14, loss = 0.09807210
Iteration 15, loss = 0.08613154
Iteration 16, loss = 0.07647606
Iteration 17, loss = 0.06871823
Iteration 18, loss = 0.06230231
Iteration 19, loss = 0.05681744
Iteration 20, loss = 0.05236423
Iteration 21, loss = 0.04874069
Iteration 22, loss = 0.04534649
Iteration 23, loss = 0.04254522
Iteration 24, loss = 0.04016372
Iteration 25, loss = 0.03810981
Iteration 26, loss = 0.03628252
Iteration 27, loss = 0.03468839
Iteration 28, loss = 0.03325672
Iteration 29, loss = 0.03193693
Iteration 30, loss = 0.03087133
Iteration 31, loss = 0.02991485
Iteration 32, loss = 0.02895415
Iteration 33, loss = 0.02811081
Iteration 34, loss = 0.02733790
Iteration 35, loss = 0.02675862
Iteration 36, loss = 0.02603395
Iteration 37, loss = 0.02558254
Iteration 38, loss = 0.02503458
Iteration 39, loss = 0.02448076
Iteration 40, loss = 0.02401133
Iteration 41, loss = 0.02384220
Iteration 42, loss = 0.02326237
Iteration 43, loss = 0.02291334
Iteration 44, loss = 0.02277817
Iteration 45, loss = 0.02253527
Iteration 46, loss = 0.02212316
Iteration 47, loss = 0.02183350
Iteration 48, loss = 0.02156402
Iteration 49, loss = 0.02132874
Iteration 50, loss = 0.02114909
Iteration 51, loss = 0.02088205
Iteration 52, loss = 0.02074467
Iteration 53, loss = 0.02058532
Iteration 54, loss = 0.02048831
Iteration 55, loss = 0.02025596
Iteration 56, loss = 0.02030310
Iteration 57, loss = 0.02000909
Iteration 58, loss = 0.01984613
Iteration 59, loss = 0.01974744
Iteration 60, loss = 0.01969215
Iteration 61, loss = 0.01955670
Iteration 62, loss = 0.01943664
Iteration 63, loss = 0.01938348
Iteration 64, loss = 0.01917778
Iteration 65, loss = 0.01923041
Iteration 66, loss = 0.01903693
Iteration 67, loss = 0.01895712
Iteration 68, loss = 0.01887413
Iteration 69, loss = 0.01872347
Iteration 70, loss = 0.01882993
Iteration 71, loss = 0.01864036
Iteration 72, loss = 0.01860869
Iteration 73, loss = 0.01860333
Iteration 74, loss = 0.01854830
Iteration 75, loss = 0.01856342
Iteration 76, loss = 0.01825521
Iteration 77, loss = 0.01841157
Iteration 78, loss = 0.01830206
Iteration 79, loss = 0.01822104
Iteration 80, loss = 0.01832112
Iteration 81, loss = 0.01820414
Iteration 82, loss = 0.01799508
Iteration 83, loss = 0.01798654
Iteration 84, loss = 0.01811197
Iteration 85, loss = 0.01791933
Iteration 86, loss = 0.01781901
Iteration 87, loss = 0.01791964
Iteration 88, loss = 0.01775495
Iteration 89, loss = 0.01783965
Iteration 90, loss = 0.01785714
Iteration 91, loss = 0.01764689
Iteration 92, loss = 0.01763092
Iteration 93, loss = 0.01766565
Iteration 94, loss = 0.01764529
Iteration 95, loss = 0.01757898
Iteration 96, loss = 0.01745078
Iteration 97, loss = 0.01748114
Iteration 98, loss = 0.01763395
Iteration 99, loss = 0.01754796
Iteration 100, loss = 0.01748324
Iteration 101, loss = 0.01743427
Iteration 102, loss = 0.01748644
Iteration 103, loss = 0.01741835
Iteration 104, loss = 0.01732880
Iteration 105, loss = 0.01739557
Iteration 106, loss = 0.01742963
Iteration 107, loss = 0.01740599
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68260238
Iteration 2, loss = 0.61862023
Iteration 3, loss = 0.53069403
Iteration 4, loss = 0.43471971
Iteration 5, loss = 0.34731186
Iteration 6, loss = 0.27336854
Iteration 7, loss = 0.21462469
Iteration 8, loss = 0.17021785
Iteration 9, loss = 0.13699255
Iteration 10, loss = 0.11270858
Iteration 11, loss = 0.09435957
Iteration 12, loss = 0.08062455
Iteration 13, loss = 0.06993139
Iteration 14, loss = 0.06158133
Iteration 15, loss = 0.05515306
Iteration 16, loss = 0.04980778
Iteration 17, loss = 0.04567903
Iteration 18, loss = 0.04198509
Iteration 19, loss = 0.03889161
Iteration 20, loss = 0.03642066
Iteration 21, loss = 0.03430155
Iteration 22, loss = 0.03244285
Iteration 23, loss = 0.03109737
Iteration 24, loss = 0.02946338
Iteration 25, loss = 0.02832275
Iteration 26, loss = 0.02721860
Iteration 27, loss = 0.02627715
Iteration 28, loss = 0.02561431
Iteration 29, loss = 0.02454653
Iteration 30, loss = 0.02443958
Iteration 31, loss = 0.02338442
Iteration 32, loss = 0.02277981
Iteration 33, loss = 0.02261116
Iteration 34, loss = 0.02195705
Iteration 35, loss = 0.02154369
Iteration 36, loss = 0.02122397
Iteration 37, loss = 0.02095847
Iteration 38, loss = 0.02062836
Iteration 39, loss = 0.02044693
Iteration 40, loss = 0.01995633
Iteration 41, loss = 0.01993980
Iteration 42, loss = 0.01971407
Iteration 43, loss = 0.01957717
Iteration 44, loss = 0.01932572
Iteration 45, loss = 0.01910737
Iteration 46, loss = 0.01886368
Iteration 47, loss = 0.01867551
Iteration 48, loss = 0.01864480
Iteration 49, loss = 0.01850125
Iteration 50, loss = 0.01828487
Iteration 51, loss = 0.01829020
Iteration 52, loss = 0.01808042
Iteration 53, loss = 0.01795250
Iteration 54, loss = 0.01793561
Iteration 55, loss = 0.01791729
Iteration 56, loss = 0.01788642
Iteration 57, loss = 0.01751915
Iteration 58, loss = 0.01751596
Iteration 59, loss = 0.01739993
Iteration 60, loss = 0.01731039
Iteration 61, loss = 0.01729386
Iteration 62, loss = 0.01745777
Iteration 63, loss = 0.01727421
Iteration 64, loss = 0.01718472
Iteration 65, loss = 0.01707729
Iteration 66, loss = 0.01692187
Iteration 67, loss = 0.01696480
Iteration 68, loss = 0.01704388
Iteration 69, loss = 0.01690036
Iteration 70, loss = 0.01685372
Iteration 71, loss = 0.01682310
Iteration 72, loss = 0.01678273
Iteration 73, loss = 0.01693597
Iteration 74, loss = 0.01692178
Iteration 75, loss = 0.01678705
Iteration 76, loss = 0.01671983
Iteration 77, loss = 0.01671277
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.75029256
Iteration 2, loss = 0.73070084
Iteration 3, loss = 0.71472299
Iteration 4, loss = 0.69960315
Iteration 5, loss = 0.68355211
Iteration 6, loss = 0.66601430
Iteration 7, loss = 0.64653549
Iteration 8, loss = 0.62533003
Iteration 9, loss = 0.60247489
Iteration 10, loss = 0.57822278
Iteration 11, loss = 0.55284384
Iteration 12, loss = 0.52693451
Iteration 13, loss = 0.50035377
Iteration 14, loss = 0.47408422
Iteration 15, loss = 0.44814614
Iteration 16, loss = 0.42230129
Iteration 17, loss = 0.39752319
Iteration 18, loss = 0.37332462
Iteration 19, loss = 0.35003301
Iteration 20, loss = 0.32752027
Iteration 21, loss = 0.30627698
Iteration 22, loss = 0.28603669
Iteration 23, loss = 0.26709459
Iteration 24, loss = 0.24934112
Iteration 25, loss = 0.23285303
Iteration 26, loss = 0.21748364
Iteration 27, loss = 0.20328681
Iteration 28, loss = 0.19013185
Iteration 29, loss = 0.17799028
Iteration 30, loss = 0.16683748
Iteration 31, loss = 0.15651772
Iteration 32, loss = 0.14707432
Iteration 33, loss = 0.13832970
Iteration 34, loss = 0.13026856
Iteration 35, loss = 0.12291791
Iteration 36, loss = 0.11611765
Iteration 37, loss = 0.10978902
Iteration 38, loss = 0.10399964
Iteration 39, loss = 0.09864551
Iteration 40, loss = 0.09369824
Iteration 41, loss = 0.08914127
Iteration 42, loss = 0.08486150
Iteration 43, loss = 0.08101345
Iteration 44, loss = 0.07731755
Iteration 45, loss = 0.07391931
Iteration 46, loss = 0.07074914
Iteration 47, loss = 0.06780132
Iteration 48, loss = 0.06509507
Iteration 49, loss = 0.06255579
Iteration 50, loss = 0.06014197
Iteration 51, loss = 0.05791402
Iteration 52, loss = 0.05582042
Iteration 53, loss = 0.05378103
Iteration 54, loss = 0.05192762
Iteration 55, loss = 0.05018655
Iteration 56, loss = 0.04855447
Iteration 57, loss = 0.04692341
Iteration 58, loss = 0.04538728
Iteration 59, loss = 0.04395185
Iteration 60, loss = 0.04256124
Iteration 61, loss = 0.04125477
Iteration 62, loss = 0.03998735
Iteration 63, loss = 0.03883702
Iteration 64, loss = 0.03771039
Iteration 65, loss = 0.03664600
Iteration 66, loss = 0.03562309
Iteration 67, loss = 0.03465818
Iteration 68, loss = 0.03372067
Iteration 69, loss = 0.03285966
Iteration 70, loss = 0.03203532
Iteration 71, loss = 0.03124329
Iteration 72, loss = 0.03054736
Iteration 73, loss = 0.02977391
Iteration 74, loss = 0.02908731
Iteration 75, loss = 0.02840638
Iteration 76, loss = 0.02779739
Iteration 77, loss = 0.02720305
Iteration 78, loss = 0.02661472
Iteration 79, loss = 0.02611487
Iteration 80, loss = 0.02559934
Iteration 81, loss = 0.02507683
Iteration 82, loss = 0.02459049
Iteration 83, loss = 0.02412515
Iteration 84, loss = 0.02368325
Iteration 85, loss = 0.02326550
Iteration 86, loss = 0.02283577
Iteration 87, loss = 0.02245996
Iteration 88, loss = 0.02206315
Iteration 89, loss = 0.02166912
Iteration 90, loss = 0.02130461
Iteration 91, loss = 0.02098072
Iteration 92, loss = 0.02061867
Iteration 93, loss = 0.02031419
Iteration 94, loss = 0.01994580
Iteration 95, loss = 0.01966690
Iteration 96, loss = 0.01941349
Iteration 97, loss = 0.01909549
Iteration 98, loss = 0.01884130
Iteration 99, loss = 0.01857171
Iteration 100, loss = 0.01837442
Iteration 101, loss = 0.01808458
Iteration 102, loss = 0.01784874
Iteration 103, loss = 0.01762177
Iteration 104, loss = 0.01738885
Iteration 105, loss = 0.01717113
Iteration 106, loss = 0.01698043
Iteration 107, loss = 0.01676578
Iteration 108, loss = 0.01656421
Iteration 109, loss = 0.01637628
Iteration 110, loss = 0.01623082
Iteration 111, loss = 0.01603041
Iteration 112, loss = 0.01586421
Iteration 113, loss = 0.01567919
Iteration 114, loss = 0.01552702
Iteration 115, loss = 0.01535625
Iteration 116, loss = 0.01521607
Iteration 117, loss = 0.01505261
Iteration 118, loss = 0.01490553
Iteration 119, loss = 0.01476013
Iteration 120, loss = 0.01459561
Iteration 121, loss = 0.01445428
Iteration 122, loss = 0.01432094
Iteration 123, loss = 0.01416518
Iteration 124, loss = 0.01406844
Iteration 125, loss = 0.01393621
Iteration 126, loss = 0.01386496
Iteration 127, loss = 0.01373819
Iteration 128, loss = 0.01365368
Iteration 129, loss = 0.01351026
Iteration 130, loss = 0.01338213
Iteration 131, loss = 0.01327071
Iteration 132, loss = 0.01317655
Iteration 133, loss = 0.01309032
Iteration 134, loss = 0.01295350
Iteration 135, loss = 0.01285659
Iteration 136, loss = 0.01276440
Iteration 137, loss = 0.01268658
Iteration 138, loss = 0.01259890
Iteration 139, loss = 0.01252406
Iteration 140, loss = 0.01245310
Iteration 141, loss = 0.01236209
Iteration 142, loss = 0.01226272
Iteration 143, loss = 0.01221778
Iteration 144, loss = 0.01212743
Iteration 145, loss = 0.01202841
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71961140
Iteration 2, loss = 0.69325576
Iteration 3, loss = 0.66948288
Iteration 4, loss = 0.64323276
Iteration 5, loss = 0.61386234
Iteration 6, loss = 0.58163710
Iteration 7, loss = 0.54740067
Iteration 8, loss = 0.51149260
Iteration 9, loss = 0.47555560
Iteration 10, loss = 0.43912540
Iteration 11, loss = 0.40388664
Iteration 12, loss = 0.36997509
Iteration 13, loss = 0.33779423
Iteration 14, loss = 0.30765711
Iteration 15, loss = 0.27997075
Iteration 16, loss = 0.25429788
Iteration 17, loss = 0.23102634
Iteration 18, loss = 0.20999743
Iteration 19, loss = 0.19108541
Iteration 20, loss = 0.17410277
Iteration 21, loss = 0.15906410
Iteration 22, loss = 0.14559963
Iteration 23, loss = 0.13364574
Iteration 24, loss = 0.12295655
Iteration 25, loss = 0.11355393
Iteration 26, loss = 0.10510500
Iteration 27, loss = 0.09750843
Iteration 28, loss = 0.09081902
Iteration 29, loss = 0.08475623
Iteration 30, loss = 0.07944580
Iteration 31, loss = 0.07446732
Iteration 32, loss = 0.07011521
Iteration 33, loss = 0.06617873
Iteration 34, loss = 0.06252959
Iteration 35, loss = 0.05926031
Iteration 36, loss = 0.05628829
Iteration 37, loss = 0.05352864
Iteration 38, loss = 0.05108483
Iteration 39, loss = 0.04877883
Iteration 40, loss = 0.04665299
Iteration 41, loss = 0.04468371
Iteration 42, loss = 0.04287969
Iteration 43, loss = 0.04125012
Iteration 44, loss = 0.03973483
Iteration 45, loss = 0.03823955
Iteration 46, loss = 0.03693125
Iteration 47, loss = 0.03572191
Iteration 48, loss = 0.03451470
Iteration 49, loss = 0.03344999
Iteration 50, loss = 0.03243169
Iteration 51, loss = 0.03147422
Iteration 52, loss = 0.03058493
Iteration 53, loss = 0.02976640
Iteration 54, loss = 0.02894820
Iteration 55, loss = 0.02819779
Iteration 56, loss = 0.02746661
Iteration 57, loss = 0.02686107
Iteration 58, loss = 0.02622843
Iteration 59, loss = 0.02560371
Iteration 60, loss = 0.02507146
Iteration 61, loss = 0.02455987
Iteration 62, loss = 0.02397728
Iteration 63, loss = 0.02355995
Iteration 64, loss = 0.02307933
Iteration 65, loss = 0.02264747
Iteration 66, loss = 0.02220599
Iteration 67, loss = 0.02184884
Iteration 68, loss = 0.02145026
Iteration 69, loss = 0.02111311
Iteration 70, loss = 0.02079655
Iteration 71, loss = 0.02048298
Iteration 72, loss = 0.02012352
Iteration 73, loss = 0.01988716
Iteration 74, loss = 0.01954614
Iteration 75, loss = 0.01925109
Iteration 76, loss = 0.01898627
Iteration 77, loss = 0.01878630
Iteration 78, loss = 0.01850337
Iteration 79, loss = 0.01824545
Iteration 80, loss = 0.01803684
Iteration 81, loss = 0.01781140
Iteration 82, loss = 0.01758752
Iteration 83, loss = 0.01739669
Iteration 84, loss = 0.01718858
Iteration 85, loss = 0.01703030
Iteration 86, loss = 0.01685570
Iteration 87, loss = 0.01666991
Iteration 88, loss = 0.01651082
Iteration 89, loss = 0.01637021
Iteration 90, loss = 0.01626925
Iteration 91, loss = 0.01604158
Iteration 92, loss = 0.01591582
Iteration 93, loss = 0.01578973
Iteration 94, loss = 0.01561137
Iteration 95, loss = 0.01549287
Iteration 96, loss = 0.01536095
Iteration 97, loss = 0.01521409
Iteration 98, loss = 0.01511294
Iteration 99, loss = 0.01498262
Iteration 100, loss = 0.01484912
Iteration 101, loss = 0.01481665
Iteration 102, loss = 0.01470132
Iteration 103, loss = 0.01457552
Iteration 104, loss = 0.01444662
Iteration 105, loss = 0.01436786
Iteration 106, loss = 0.01429719
Iteration 107, loss = 0.01421748
Iteration 108, loss = 0.01411403
Iteration 109, loss = 0.01400919
Iteration 110, loss = 0.01392615
Iteration 111, loss = 0.01394098
Iteration 112, loss = 0.01376779
Iteration 113, loss = 0.01368820
Iteration 114, loss = 0.01362527
Iteration 115, loss = 0.01354731
Iteration 116, loss = 0.01352832
Iteration 117, loss = 0.01338972
Iteration 118, loss = 0.01336692
Iteration 119, loss = 0.01325995
Iteration 120, loss = 0.01323382
Iteration 121, loss = 0.01315440
Iteration 122, loss = 0.01312831
Iteration 123, loss = 0.01306021
Iteration 124, loss = 0.01297165
Iteration 125, loss = 0.01295408
Iteration 126, loss = 0.01290960
Iteration 127, loss = 0.01283015
Iteration 128, loss = 0.01276319
Iteration 129, loss = 0.01272119
Iteration 130, loss = 0.01268441
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67752679
Iteration 2, loss = 0.63154846
Iteration 3, loss = 0.56269507
Iteration 4, loss = 0.48153747
Iteration 5, loss = 0.40022827
Iteration 6, loss = 0.32469565
Iteration 7, loss = 0.26033596
Iteration 8, loss = 0.20842091
Iteration 9, loss = 0.16810332
Iteration 10, loss = 0.13727368
Iteration 11, loss = 0.11402242
Iteration 12, loss = 0.09618304
Iteration 13, loss = 0.08263249
Iteration 14, loss = 0.07197190
Iteration 15, loss = 0.06358964
Iteration 16, loss = 0.05682073
Iteration 17, loss = 0.05138089
Iteration 18, loss = 0.04669193
Iteration 19, loss = 0.04314984
Iteration 20, loss = 0.03983197
Iteration 21, loss = 0.03731881
Iteration 22, loss = 0.03487592
Iteration 23, loss = 0.03285456
Iteration 24, loss = 0.03135915
Iteration 25, loss = 0.02995753
Iteration 26, loss = 0.02853098
Iteration 27, loss = 0.02744187
Iteration 28, loss = 0.02631632
Iteration 29, loss = 0.02539475
Iteration 30, loss = 0.02471186
Iteration 31, loss = 0.02399736
Iteration 32, loss = 0.02322691
Iteration 33, loss = 0.02267963
Iteration 34, loss = 0.02236210
Iteration 35, loss = 0.02172913
Iteration 36, loss = 0.02124929
Iteration 37, loss = 0.02107547
Iteration 38, loss = 0.02032278
Iteration 39, loss = 0.02008053
Iteration 40, loss = 0.02013664
Iteration 41, loss = 0.01981512
Iteration 42, loss = 0.01931823
Iteration 43, loss = 0.01903403
Iteration 44, loss = 0.01891677
Iteration 45, loss = 0.01860303
Iteration 46, loss = 0.01824259
Iteration 47, loss = 0.01820693
Iteration 48, loss = 0.01817373
Iteration 49, loss = 0.01817172
Iteration 50, loss = 0.01777854
Iteration 51, loss = 0.01769279
Iteration 52, loss = 0.01745759
Iteration 53, loss = 0.01725144
Iteration 54, loss = 0.01718911
Iteration 55, loss = 0.01746424
Iteration 56, loss = 0.01696896
Iteration 57, loss = 0.01721632
Iteration 58, loss = 0.01691397
Iteration 59, loss = 0.01696207
Iteration 60, loss = 0.01681771
Iteration 61, loss = 0.01647867
Iteration 62, loss = 0.01641792
Iteration 63, loss = 0.01641784
Iteration 64, loss = 0.01631527
Iteration 65, loss = 0.01646061
Iteration 66, loss = 0.01629696
Iteration 67, loss = 0.01615696
Iteration 68, loss = 0.01619148
Iteration 69, loss = 0.01616341
Iteration 70, loss = 0.01627465
Iteration 71, loss = 0.01628553
Iteration 72, loss = 0.01597726
Iteration 73, loss = 0.01589025
Iteration 74, loss = 0.01582553
Iteration 75, loss = 0.01587541
Iteration 76, loss = 0.01568196
Iteration 77, loss = 0.01558104
Iteration 78, loss = 0.01557749
Iteration 79, loss = 0.01547326
Iteration 80, loss = 0.01557106
Iteration 81, loss = 0.01532999
Iteration 82, loss = 0.01543594
Iteration 83, loss = 0.01542465
Iteration 84, loss = 0.01539016
Iteration 85, loss = 0.01541533
Iteration 86, loss = 0.01558170
Iteration 87, loss = 0.01536723
Iteration 88, loss = 0.01519843
Iteration 89, loss = 0.01543059
Iteration 90, loss = 0.01529282
Iteration 91, loss = 0.01519363
Iteration 92, loss = 0.01527911
Iteration 93, loss = 0.01530548
Iteration 94, loss = 0.01520999
Iteration 95, loss = 0.01500307
Iteration 96, loss = 0.01528509
Iteration 97, loss = 0.01520329
Iteration 98, loss = 0.01517815
Iteration 99, loss = 0.01518480
Iteration 100, loss = 0.01508384
Iteration 101, loss = 0.01509732
Iteration 102, loss = 0.01512466
Iteration 103, loss = 0.01504924
Iteration 104, loss = 0.01512830
Iteration 105, loss = 0.01498242
Iteration 106, loss = 0.01509162
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67848993
Iteration 2, loss = 0.64611936
Iteration 3, loss = 0.60940427
Iteration 4, loss = 0.56351929
Iteration 5, loss = 0.51026488
Iteration 6, loss = 0.45399950
Iteration 7, loss = 0.39983787
Iteration 8, loss = 0.34990363
Iteration 9, loss = 0.30482662
Iteration 10, loss = 0.26502975
Iteration 11, loss = 0.23057604
Iteration 12, loss = 0.20099208
Iteration 13, loss = 0.17575473
Iteration 14, loss = 0.15447463
Iteration 15, loss = 0.13663830
Iteration 16, loss = 0.12154108
Iteration 17, loss = 0.10883125
Iteration 18, loss = 0.09800483
Iteration 19, loss = 0.08882026
Iteration 20, loss = 0.08094461
Iteration 21, loss = 0.07415904
Iteration 22, loss = 0.06828843
Iteration 23, loss = 0.06335940
Iteration 24, loss = 0.05888009
Iteration 25, loss = 0.05502848
Iteration 26, loss = 0.05163105
Iteration 27, loss = 0.04849282
Iteration 28, loss = 0.04573879
Iteration 29, loss = 0.04340909
Iteration 30, loss = 0.04111966
Iteration 31, loss = 0.03935591
Iteration 32, loss = 0.03748369
Iteration 33, loss = 0.03592017
Iteration 34, loss = 0.03444894
Iteration 35, loss = 0.03300954
Iteration 36, loss = 0.03173135
Iteration 37, loss = 0.03055543
Iteration 38, loss = 0.02953072
Iteration 39, loss = 0.02856768
Iteration 40, loss = 0.02770056
Iteration 41, loss = 0.02679094
Iteration 42, loss = 0.02612057
Iteration 43, loss = 0.02541680
Iteration 44, loss = 0.02483190
Iteration 45, loss = 0.02427670
Iteration 46, loss = 0.02373429
Iteration 47, loss = 0.02330076
Iteration 48, loss = 0.02273248
Iteration 49, loss = 0.02247987
Iteration 50, loss = 0.02193061
Iteration 51, loss = 0.02137238
Iteration 52, loss = 0.02117113
Iteration 53, loss = 0.02082984
Iteration 54, loss = 0.02032019
Iteration 55, loss = 0.02002597
Iteration 56, loss = 0.01958435
Iteration 57, loss = 0.01920784
Iteration 58, loss = 0.01891925
Iteration 59, loss = 0.01864983
Iteration 60, loss = 0.01838471
Iteration 61, loss = 0.01809514
Iteration 62, loss = 0.01790184
Iteration 63, loss = 0.01768248
Iteration 64, loss = 0.01747880
Iteration 65, loss = 0.01732272
Iteration 66, loss = 0.01710904
Iteration 67, loss = 0.01701926
Iteration 68, loss = 0.01727003
Iteration 69, loss = 0.01712363
Iteration 70, loss = 0.01705385
Iteration 71, loss = 0.01683374
Iteration 72, loss = 0.01657986
Iteration 73, loss = 0.01664321
Iteration 74, loss = 0.01655024
Iteration 75, loss = 0.01630695
Iteration 76, loss = 0.01599448
Iteration 77, loss = 0.01575836
Iteration 78, loss = 0.01562195
Iteration 79, loss = 0.01540867
Iteration 80, loss = 0.01530841
Iteration 81, loss = 0.01517522
Iteration 82, loss = 0.01524386
Iteration 83, loss = 0.01501256
Iteration 84, loss = 0.01515511
Iteration 85, loss = 0.01500303
Iteration 86, loss = 0.01497664
Iteration 87, loss = 0.01503672
Iteration 88, loss = 0.01487078
Iteration 89, loss = 0.01484703
Iteration 90, loss = 0.01474123
Iteration 91, loss = 0.01456098
Iteration 92, loss = 0.01452506
Iteration 93, loss = 0.01447521
Iteration 94, loss = 0.01435345
Iteration 95, loss = 0.01417644
Iteration 96, loss = 0.01410823
Iteration 97, loss = 0.01410508
Iteration 98, loss = 0.01406823
Iteration 99, loss = 0.01399536
Iteration 100, loss = 0.01415102
Iteration 101, loss = 0.01401013
Iteration 102, loss = 0.01392628
Iteration 103, loss = 0.01383190
Iteration 104, loss = 0.01395978
Iteration 105, loss = 0.01355356
Iteration 106, loss = 0.01361673
Iteration 107, loss = 0.01365123
Iteration 108, loss = 0.01368934
Iteration 109, loss = 0.01345909
Iteration 110, loss = 0.01341540
Iteration 111, loss = 0.01343244
Iteration 112, loss = 0.01337709
Iteration 113, loss = 0.01335606
Iteration 114, loss = 0.01346749
Iteration 115, loss = 0.01370890
Iteration 116, loss = 0.01367546
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67798532
Iteration 2, loss = 0.62854225
Iteration 3, loss = 0.55473596
Iteration 4, loss = 0.46521784
Iteration 5, loss = 0.37603693
Iteration 6, loss = 0.29691539
Iteration 7, loss = 0.23274864
Iteration 8, loss = 0.18373784
Iteration 9, loss = 0.14719102
Iteration 10, loss = 0.12021402
Iteration 11, loss = 0.10018785
Iteration 12, loss = 0.08510858
Iteration 13, loss = 0.07379242
Iteration 14, loss = 0.06481597
Iteration 15, loss = 0.05764389
Iteration 16, loss = 0.05206215
Iteration 17, loss = 0.04738224
Iteration 18, loss = 0.04370098
Iteration 19, loss = 0.04051664
Iteration 20, loss = 0.03784956
Iteration 21, loss = 0.03575579
Iteration 22, loss = 0.03375666
Iteration 23, loss = 0.03204849
Iteration 24, loss = 0.03051155
Iteration 25, loss = 0.02929078
Iteration 26, loss = 0.02815944
Iteration 27, loss = 0.02739621
Iteration 28, loss = 0.02644045
Iteration 29, loss = 0.02580186
Iteration 30, loss = 0.02507791
Iteration 31, loss = 0.02426300
Iteration 32, loss = 0.02376217
Iteration 33, loss = 0.02319541
Iteration 34, loss = 0.02289050
Iteration 35, loss = 0.02235566
Iteration 36, loss = 0.02206307
Iteration 37, loss = 0.02166007
Iteration 38, loss = 0.02134754
Iteration 39, loss = 0.02108449
Iteration 40, loss = 0.02080710
Iteration 41, loss = 0.02070430
Iteration 42, loss = 0.02049402
Iteration 43, loss = 0.02017480
Iteration 44, loss = 0.01997486
Iteration 45, loss = 0.01981521
Iteration 46, loss = 0.01965057
Iteration 47, loss = 0.01968997
Iteration 48, loss = 0.01935132
Iteration 49, loss = 0.01915114
Iteration 50, loss = 0.01927153
Iteration 51, loss = 0.01888702
Iteration 52, loss = 0.01880462
Iteration 53, loss = 0.01874764
Iteration 54, loss = 0.01869389
Iteration 55, loss = 0.01847625
Iteration 56, loss = 0.01851547
Iteration 57, loss = 0.01852753
Iteration 58, loss = 0.01818142
Iteration 59, loss = 0.01821491
Iteration 60, loss = 0.01800504
Iteration 61, loss = 0.01830922
Iteration 62, loss = 0.01802540
Iteration 63, loss = 0.01793618
Iteration 64, loss = 0.01779799
Iteration 65, loss = 0.01808027
Iteration 66, loss = 0.01775634
Iteration 67, loss = 0.01785106
Iteration 68, loss = 0.01766127
Iteration 69, loss = 0.01769555
Iteration 70, loss = 0.01765249
Iteration 71, loss = 0.01753889
Iteration 72, loss = 0.01770165
Iteration 73, loss = 0.01735589
Iteration 74, loss = 0.01778768
Iteration 75, loss = 0.01744572
Iteration 76, loss = 0.01734943
Iteration 77, loss = 0.01726922
Iteration 78, loss = 0.01740459
Iteration 79, loss = 0.01730788
Iteration 80, loss = 0.01722775
Iteration 81, loss = 0.01742092
Iteration 82, loss = 0.01726708
Iteration 83, loss = 0.01711411
Iteration 84, loss = 0.01712786
Iteration 85, loss = 0.01710141
Iteration 86, loss = 0.01715435
Iteration 87, loss = 0.01723931
Iteration 88, loss = 0.01709572
Iteration 89, loss = 0.01724413
Iteration 90, loss = 0.01714553
Iteration 91, loss = 0.01718271
Iteration 92, loss = 0.01708337
Iteration 93, loss = 0.01690443
Iteration 94, loss = 0.01707203
Iteration 95, loss = 0.01698308
Iteration 96, loss = 0.01691399
Iteration 97, loss = 0.01679322
Iteration 98, loss = 0.01692385
Iteration 99, loss = 0.01725114
Iteration 100, loss = 0.01702025
Iteration 101, loss = 0.01713061
Iteration 102, loss = 0.01688391
Iteration 103, loss = 0.01687662
Iteration 104, loss = 0.01713274
Iteration 105, loss = 0.01680179
Iteration 106, loss = 0.01684801
Iteration 107, loss = 0.01674886
Iteration 108, loss = 0.01674888
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68867090
Iteration 2, loss = 0.65951851
Iteration 3, loss = 0.61591247
Iteration 4, loss = 0.55346922
Iteration 5, loss = 0.48120090
Iteration 6, loss = 0.40912438
Iteration 7, loss = 0.34167519
Iteration 8, loss = 0.28216805
Iteration 9, loss = 0.23237717
Iteration 10, loss = 0.19169796
Iteration 11, loss = 0.15945432
Iteration 12, loss = 0.13399728
Iteration 13, loss = 0.11403489
Iteration 14, loss = 0.09818927
Iteration 15, loss = 0.08569113
Iteration 16, loss = 0.07565666
Iteration 17, loss = 0.06757063
Iteration 18, loss = 0.06088507
Iteration 19, loss = 0.05523360
Iteration 20, loss = 0.05069225
Iteration 21, loss = 0.04690736
Iteration 22, loss = 0.04335046
Iteration 23, loss = 0.04050151
Iteration 24, loss = 0.03805507
Iteration 25, loss = 0.03602156
Iteration 26, loss = 0.03415497
Iteration 27, loss = 0.03243190
Iteration 28, loss = 0.03103172
Iteration 29, loss = 0.02955628
Iteration 30, loss = 0.02843132
Iteration 31, loss = 0.02743983
Iteration 32, loss = 0.02642029
Iteration 33, loss = 0.02572985
Iteration 34, loss = 0.02481486
Iteration 35, loss = 0.02414712
Iteration 36, loss = 0.02357224
Iteration 37, loss = 0.02303421
Iteration 38, loss = 0.02245643
Iteration 39, loss = 0.02191022
Iteration 40, loss = 0.02146597
Iteration 41, loss = 0.02107926
Iteration 42, loss = 0.02070458
Iteration 43, loss = 0.02038186
Iteration 44, loss = 0.01993559
Iteration 45, loss = 0.01983546
Iteration 46, loss = 0.01937281
Iteration 47, loss = 0.01930907
Iteration 48, loss = 0.01888983
Iteration 49, loss = 0.01861201
Iteration 50, loss = 0.01851831
Iteration 51, loss = 0.01827065
Iteration 52, loss = 0.01801022
Iteration 53, loss = 0.01798574
Iteration 54, loss = 0.01780446
Iteration 55, loss = 0.01759995
Iteration 56, loss = 0.01736490
Iteration 57, loss = 0.01724017
Iteration 58, loss = 0.01721578
Iteration 59, loss = 0.01697318
Iteration 60, loss = 0.01689384
Iteration 61, loss = 0.01674748
Iteration 62, loss = 0.01672810
Iteration 63, loss = 0.01657282
Iteration 64, loss = 0.01645783
Iteration 65, loss = 0.01642451
Iteration 66, loss = 0.01635302
Iteration 67, loss = 0.01610694
Iteration 68, loss = 0.01614280
Iteration 69, loss = 0.01595622
Iteration 70, loss = 0.01596220
Iteration 71, loss = 0.01591477
Iteration 72, loss = 0.01581372
Iteration 73, loss = 0.01580429
Iteration 74, loss = 0.01566510
Iteration 75, loss = 0.01565528
Iteration 76, loss = 0.01560245
Iteration 77, loss = 0.01548796
Iteration 78, loss = 0.01555451
Iteration 79, loss = 0.01536377
Iteration 80, loss = 0.01553835
Iteration 81, loss = 0.01534127
Iteration 82, loss = 0.01519398
Iteration 83, loss = 0.01544034
Iteration 84, loss = 0.01523582
Iteration 85, loss = 0.01499909
Iteration 86, loss = 0.01507696
Iteration 87, loss = 0.01505383
Iteration 88, loss = 0.01497280
Iteration 89, loss = 0.01492675
Iteration 90, loss = 0.01490443
Iteration 91, loss = 0.01485978
Iteration 92, loss = 0.01492942
Iteration 93, loss = 0.01481153
Iteration 94, loss = 0.01487383
Iteration 95, loss = 0.01490079
Iteration 96, loss = 0.01471795
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68353674
Iteration 2, loss = 0.64128240
Iteration 3, loss = 0.57930517
Iteration 4, loss = 0.50247506
Iteration 5, loss = 0.42086139
Iteration 6, loss = 0.34264172
Iteration 7, loss = 0.27471516
Iteration 8, loss = 0.21976759
Iteration 9, loss = 0.17688077
Iteration 10, loss = 0.14430277
Iteration 11, loss = 0.11934499
Iteration 12, loss = 0.10062654
Iteration 13, loss = 0.08636088
Iteration 14, loss = 0.07512711
Iteration 15, loss = 0.06621427
Iteration 16, loss = 0.05918962
Iteration 17, loss = 0.05341861
Iteration 18, loss = 0.04870757
Iteration 19, loss = 0.04473784
Iteration 20, loss = 0.04149622
Iteration 21, loss = 0.03859229
Iteration 22, loss = 0.03647449
Iteration 23, loss = 0.03420973
Iteration 24, loss = 0.03239182
Iteration 25, loss = 0.03099462
Iteration 26, loss = 0.02946015
Iteration 27, loss = 0.02826479
Iteration 28, loss = 0.02732987
Iteration 29, loss = 0.02626390
Iteration 30, loss = 0.02553553
Iteration 31, loss = 0.02477646
Iteration 32, loss = 0.02410063
Iteration 33, loss = 0.02329670
Iteration 34, loss = 0.02283746
Iteration 35, loss = 0.02220852
Iteration 36, loss = 0.02189328
Iteration 37, loss = 0.02141086
Iteration 38, loss = 0.02102426
Iteration 39, loss = 0.02071218
Iteration 40, loss = 0.02039183
Iteration 41, loss = 0.02007350
Iteration 42, loss = 0.01970874
Iteration 43, loss = 0.01957774
Iteration 44, loss = 0.01916439
Iteration 45, loss = 0.01907072
Iteration 46, loss = 0.01880881
Iteration 47, loss = 0.01854463
Iteration 48, loss = 0.01860053
Iteration 49, loss = 0.01839491
Iteration 50, loss = 0.01810354
Iteration 51, loss = 0.01793061
Iteration 52, loss = 0.01774444
Iteration 53, loss = 0.01767104
Iteration 54, loss = 0.01764346
Iteration 55, loss = 0.01753436
Iteration 56, loss = 0.01733682
Iteration 57, loss = 0.01709981
Iteration 58, loss = 0.01698871
Iteration 59, loss = 0.01679742
Iteration 60, loss = 0.01685940
Iteration 61, loss = 0.01673837
Iteration 62, loss = 0.01669751
Iteration 63, loss = 0.01670538
Iteration 64, loss = 0.01678423
Iteration 65, loss = 0.01675081
Iteration 66, loss = 0.01653319
Iteration 67, loss = 0.01643932
Iteration 68, loss = 0.01631766
Iteration 69, loss = 0.01622129
Iteration 70, loss = 0.01615318
Iteration 71, loss = 0.01623309
Iteration 72, loss = 0.01609257
Iteration 73, loss = 0.01600814
Iteration 74, loss = 0.01612082
Iteration 75, loss = 0.01606459
Iteration 76, loss = 0.01611188
Iteration 77, loss = 0.01584155
Iteration 78, loss = 0.01568492
Iteration 79, loss = 0.01589630
Iteration 80, loss = 0.01588968
Iteration 81, loss = 0.01550514
Iteration 82, loss = 0.01575641
Iteration 83, loss = 0.01582731
Iteration 84, loss = 0.01613188
Iteration 85, loss = 0.01580240
Iteration 86, loss = 0.01554251
Iteration 87, loss = 0.01557736
Iteration 88, loss = 0.01536986
Iteration 89, loss = 0.01540287
Iteration 90, loss = 0.01538528
Iteration 91, loss = 0.01538868
Iteration 92, loss = 0.01572532
Iteration 93, loss = 0.01553621
Iteration 94, loss = 0.01549768
Iteration 95, loss = 0.01515801
Iteration 96, loss = 0.01534696
Iteration 97, loss = 0.01547240
Iteration 98, loss = 0.01547753
Iteration 99, loss = 0.01551064
Iteration 100, loss = 0.01519887
Iteration 101, loss = 0.01518205
Iteration 102, loss = 0.01546838
Iteration 103, loss = 0.01520577
Iteration 104, loss = 0.01508382
Iteration 105, loss = 0.01531055
Iteration 106, loss = 0.01512393
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67391538
Iteration 2, loss = 0.65021994
Iteration 3, loss = 0.62770153
Iteration 4, loss = 0.60233301
Iteration 5, loss = 0.57338106
Iteration 6, loss = 0.54104274
Iteration 7, loss = 0.50556835
Iteration 8, loss = 0.46814201
Iteration 9, loss = 0.43099892
Iteration 10, loss = 0.39401252
Iteration 11, loss = 0.35906305
Iteration 12, loss = 0.32592589
Iteration 13, loss = 0.29527971
Iteration 14, loss = 0.26706413
Iteration 15, loss = 0.24153036
Iteration 16, loss = 0.21839942
Iteration 17, loss = 0.19753190
Iteration 18, loss = 0.17898614
Iteration 19, loss = 0.16263711
Iteration 20, loss = 0.14798412
Iteration 21, loss = 0.13510797
Iteration 22, loss = 0.12375676
Iteration 23, loss = 0.11371775
Iteration 24, loss = 0.10472376
Iteration 25, loss = 0.09684775
Iteration 26, loss = 0.08982438
Iteration 27, loss = 0.08365079
Iteration 28, loss = 0.07803491
Iteration 29, loss = 0.07300249
Iteration 30, loss = 0.06854958
Iteration 31, loss = 0.06456945
Iteration 32, loss = 0.06095624
Iteration 33, loss = 0.05762802
Iteration 34, loss = 0.05468706
Iteration 35, loss = 0.05197024
Iteration 36, loss = 0.04943846
Iteration 37, loss = 0.04716041
Iteration 38, loss = 0.04515522
Iteration 39, loss = 0.04321450
Iteration 40, loss = 0.04145673
Iteration 41, loss = 0.03979456
Iteration 42, loss = 0.03836211
Iteration 43, loss = 0.03694014
Iteration 44, loss = 0.03565937
Iteration 45, loss = 0.03443904
Iteration 46, loss = 0.03333064
Iteration 47, loss = 0.03230468
Iteration 48, loss = 0.03138660
Iteration 49, loss = 0.03046854
Iteration 50, loss = 0.02967822
Iteration 51, loss = 0.02879432
Iteration 52, loss = 0.02800851
Iteration 53, loss = 0.02728196
Iteration 54, loss = 0.02662071
Iteration 55, loss = 0.02603836
Iteration 56, loss = 0.02549517
Iteration 57, loss = 0.02486093
Iteration 58, loss = 0.02434869
Iteration 59, loss = 0.02388011
Iteration 60, loss = 0.02343841
Iteration 61, loss = 0.02291684
Iteration 62, loss = 0.02255500
Iteration 63, loss = 0.02212447
Iteration 64, loss = 0.02169447
Iteration 65, loss = 0.02133891
Iteration 66, loss = 0.02101106
Iteration 67, loss = 0.02069598
Iteration 68, loss = 0.02037974
Iteration 69, loss = 0.02004659
Iteration 70, loss = 0.01972552
Iteration 71, loss = 0.01944485
Iteration 72, loss = 0.01917015
Iteration 73, loss = 0.01899701
Iteration 74, loss = 0.01870270
Iteration 75, loss = 0.01845250
Iteration 76, loss = 0.01826353
Iteration 77, loss = 0.01801771
Iteration 78, loss = 0.01782473
Iteration 79, loss = 0.01764027
Iteration 80, loss = 0.01746169
Iteration 81, loss = 0.01720448
Iteration 82, loss = 0.01708047
Iteration 83, loss = 0.01694313
Iteration 84, loss = 0.01672290
Iteration 85, loss = 0.01661597
Iteration 86, loss = 0.01652700
Iteration 87, loss = 0.01624576
Iteration 88, loss = 0.01610839
Iteration 89, loss = 0.01610217
Iteration 90, loss = 0.01589791
Iteration 91, loss = 0.01574433
Iteration 92, loss = 0.01564626
Iteration 93, loss = 0.01550499
Iteration 94, loss = 0.01536972
Iteration 95, loss = 0.01527464
Iteration 96, loss = 0.01514455
Iteration 97, loss = 0.01502761
Iteration 98, loss = 0.01500896
Iteration 99, loss = 0.01488717
Iteration 100, loss = 0.01477984
Iteration 101, loss = 0.01466032
Iteration 102, loss = 0.01458778
Iteration 103, loss = 0.01451897
Iteration 104, loss = 0.01444044
Iteration 105, loss = 0.01433366
Iteration 106, loss = 0.01424337
Iteration 107, loss = 0.01415979
Iteration 108, loss = 0.01408091
Iteration 109, loss = 0.01401381
Iteration 110, loss = 0.01396276
Iteration 111, loss = 0.01385766
Iteration 112, loss = 0.01381343
Iteration 113, loss = 0.01373314
Iteration 114, loss = 0.01379090
Iteration 115, loss = 0.01360172
Iteration 116, loss = 0.01357814
Iteration 117, loss = 0.01354293
Iteration 118, loss = 0.01347998
Iteration 119, loss = 0.01335908
Iteration 120, loss = 0.01330669
Iteration 121, loss = 0.01326037
Iteration 122, loss = 0.01325703
Iteration 123, loss = 0.01320644
Iteration 124, loss = 0.01312300
Iteration 125, loss = 0.01307385
Iteration 126, loss = 0.01300866
Iteration 127, loss = 0.01308314
Iteration 128, loss = 0.01300382
Iteration 129, loss = 0.01293556
Iteration 130, loss = 0.01287326
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70375707
Iteration 2, loss = 0.61012493
Iteration 3, loss = 0.48583083
Iteration 4, loss = 0.36104532
Iteration 5, loss = 0.26243044
Iteration 6, loss = 0.19203543
Iteration 7, loss = 0.14450354
Iteration 8, loss = 0.11281009
Iteration 9, loss = 0.09111765
Iteration 10, loss = 0.07584840
Iteration 11, loss = 0.06501791
Iteration 12, loss = 0.05688063
Iteration 13, loss = 0.05058112
Iteration 14, loss = 0.04574264
Iteration 15, loss = 0.04171997
Iteration 16, loss = 0.03855723
Iteration 17, loss = 0.03627173
Iteration 18, loss = 0.03413541
Iteration 19, loss = 0.03224716
Iteration 20, loss = 0.03073744
Iteration 21, loss = 0.02956607
Iteration 22, loss = 0.02859115
Iteration 23, loss = 0.02774900
Iteration 24, loss = 0.02667604
Iteration 25, loss = 0.02601110
Iteration 26, loss = 0.02535860
Iteration 27, loss = 0.02479595
Iteration 28, loss = 0.02443401
Iteration 29, loss = 0.02394167
Iteration 30, loss = 0.02348780
Iteration 31, loss = 0.02311741
Iteration 32, loss = 0.02297669
Iteration 33, loss = 0.02264693
Iteration 34, loss = 0.02228509
Iteration 35, loss = 0.02199456
Iteration 36, loss = 0.02168622
Iteration 37, loss = 0.02194126
Iteration 38, loss = 0.02145163
Iteration 39, loss = 0.02117697
Iteration 40, loss = 0.02113706
Iteration 41, loss = 0.02091476
Iteration 42, loss = 0.02093507
Iteration 43, loss = 0.02081782
Iteration 44, loss = 0.02081218
Iteration 45, loss = 0.02037214
Iteration 46, loss = 0.02052066
Iteration 47, loss = 0.02019415
Iteration 48, loss = 0.02024826
Iteration 49, loss = 0.02013833
Iteration 50, loss = 0.02007847
Iteration 51, loss = 0.01982482
Iteration 52, loss = 0.01991430
Iteration 53, loss = 0.01973741
Iteration 54, loss = 0.01982140
Iteration 55, loss = 0.01975195
Iteration 56, loss = 0.01944451
Iteration 57, loss = 0.01936781
Iteration 58, loss = 0.01959005
Iteration 59, loss = 0.01962581
Iteration 60, loss = 0.01958542
Iteration 61, loss = 0.01943108
Iteration 62, loss = 0.01954116
Iteration 63, loss = 0.01924739
Iteration 64, loss = 0.01903772
Iteration 65, loss = 0.01915279
Iteration 66, loss = 0.01958072
Iteration 67, loss = 0.01902413
Iteration 68, loss = 0.01912062
Iteration 69, loss = 0.01927866
Iteration 70, loss = 0.01923727
Iteration 71, loss = 0.01897367
Iteration 72, loss = 0.01947636
Iteration 73, loss = 0.01889937
Iteration 74, loss = 0.01911651
Iteration 75, loss = 0.01915860
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70261432
Iteration 2, loss = 0.63395319
Iteration 3, loss = 0.52782527
Iteration 4, loss = 0.40533498
Iteration 5, loss = 0.29800188
Iteration 6, loss = 0.21842587
Iteration 7, loss = 0.16389433
Iteration 8, loss = 0.12664667
Iteration 9, loss = 0.10225601
Iteration 10, loss = 0.08471769
Iteration 11, loss = 0.07259631
Iteration 12, loss = 0.06318709
Iteration 13, loss = 0.05664903
Iteration 14, loss = 0.05115793
Iteration 15, loss = 0.04710349
Iteration 16, loss = 0.04376690
Iteration 17, loss = 0.04088590
Iteration 18, loss = 0.03888690
Iteration 19, loss = 0.03711722
Iteration 20, loss = 0.03543170
Iteration 21, loss = 0.03410876
Iteration 22, loss = 0.03298757
Iteration 23, loss = 0.03204101
Iteration 24, loss = 0.03117661
Iteration 25, loss = 0.03026795
Iteration 26, loss = 0.02989494
Iteration 27, loss = 0.02900927
Iteration 28, loss = 0.02857241
Iteration 29, loss = 0.02821653
Iteration 30, loss = 0.02803244
Iteration 31, loss = 0.02770934
Iteration 32, loss = 0.02705837
Iteration 33, loss = 0.02692899
Iteration 34, loss = 0.02649781
Iteration 35, loss = 0.02609252
Iteration 36, loss = 0.02605700
Iteration 37, loss = 0.02579184
Iteration 38, loss = 0.02545987
Iteration 39, loss = 0.02532570
Iteration 40, loss = 0.02532630
Iteration 41, loss = 0.02502069
Iteration 42, loss = 0.02499386
Iteration 43, loss = 0.02491914
Iteration 44, loss = 0.02462658
Iteration 45, loss = 0.02484467
Iteration 46, loss = 0.02464916
Iteration 47, loss = 0.02457313
Iteration 48, loss = 0.02445370
Iteration 49, loss = 0.02459232
Iteration 50, loss = 0.02423669
Iteration 51, loss = 0.02431195
Iteration 52, loss = 0.02434659
Iteration 53, loss = 0.02414338
Iteration 54, loss = 0.02386866
Iteration 55, loss = 0.02375443
Iteration 56, loss = 0.02361195
Iteration 57, loss = 0.02350248
Iteration 58, loss = 0.02358147
Iteration 59, loss = 0.02358810
Iteration 60, loss = 0.02341428
Iteration 61, loss = 0.02318042
Iteration 62, loss = 0.02352089
Iteration 63, loss = 0.02328087
Iteration 64, loss = 0.02371909
Iteration 65, loss = 0.02349498
Iteration 66, loss = 0.02330249
Iteration 67, loss = 0.02328691
Iteration 68, loss = 0.02331555
Iteration 69, loss = 0.02333079
Iteration 70, loss = 0.02301818
Iteration 71, loss = 0.02306397
Iteration 72, loss = 0.02349125
Iteration 73, loss = 0.02321121
Iteration 74, loss = 0.02343910
Iteration 75, loss = 0.02304336
Iteration 76, loss = 0.02319108
Iteration 77, loss = 0.02309650
Iteration 78, loss = 0.02298624
Iteration 79, loss = 0.02317644
Iteration 80, loss = 0.02298495
Iteration 81, loss = 0.02319000
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67540705
Iteration 2, loss = 0.60397350
Iteration 3, loss = 0.49837562
Iteration 4, loss = 0.38393836
Iteration 5, loss = 0.28460819
Iteration 6, loss = 0.20942708
Iteration 7, loss = 0.15744825
Iteration 8, loss = 0.12227189
Iteration 9, loss = 0.09862313
Iteration 10, loss = 0.08173279
Iteration 11, loss = 0.06975665
Iteration 12, loss = 0.06100124
Iteration 13, loss = 0.05440982
Iteration 14, loss = 0.04905865
Iteration 15, loss = 0.04513880
Iteration 16, loss = 0.04210834
Iteration 17, loss = 0.03916725
Iteration 18, loss = 0.03699268
Iteration 19, loss = 0.03519505
Iteration 20, loss = 0.03379424
Iteration 21, loss = 0.03217712
Iteration 22, loss = 0.03121081
Iteration 23, loss = 0.03018662
Iteration 24, loss = 0.02939793
Iteration 25, loss = 0.02885869
Iteration 26, loss = 0.02814988
Iteration 27, loss = 0.02745706
Iteration 28, loss = 0.02677052
Iteration 29, loss = 0.02630295
Iteration 30, loss = 0.02594278
Iteration 31, loss = 0.02572679
Iteration 32, loss = 0.02546122
Iteration 33, loss = 0.02501874
Iteration 34, loss = 0.02497856
Iteration 35, loss = 0.02469906
Iteration 36, loss = 0.02433131
Iteration 37, loss = 0.02408326
Iteration 38, loss = 0.02383298
Iteration 39, loss = 0.02367340
Iteration 40, loss = 0.02369683
Iteration 41, loss = 0.02352199
Iteration 42, loss = 0.02314441
Iteration 43, loss = 0.02327190
Iteration 44, loss = 0.02294948
Iteration 45, loss = 0.02315134
Iteration 46, loss = 0.02303778
Iteration 47, loss = 0.02281314
Iteration 48, loss = 0.02273637
Iteration 49, loss = 0.02276807
Iteration 50, loss = 0.02275280
Iteration 51, loss = 0.02255962
Iteration 52, loss = 0.02269195
Iteration 53, loss = 0.02243172
Iteration 54, loss = 0.02242290
Iteration 55, loss = 0.02233635
Iteration 56, loss = 0.02219052
Iteration 57, loss = 0.02209635
Iteration 58, loss = 0.02205721
Iteration 59, loss = 0.02237974
Iteration 60, loss = 0.02187438
Iteration 61, loss = 0.02207571
Iteration 62, loss = 0.02232483
Iteration 63, loss = 0.02176384
Iteration 64, loss = 0.02172827
Iteration 65, loss = 0.02172104
Iteration 66, loss = 0.02194838
Iteration 67, loss = 0.02178182
Iteration 68, loss = 0.02214132
Iteration 69, loss = 0.02168402
Iteration 70, loss = 0.02157922
Iteration 71, loss = 0.02153945
Iteration 72, loss = 0.02160320
Iteration 73, loss = 0.02181148
Iteration 74, loss = 0.02126889
Iteration 75, loss = 0.02139328
Iteration 76, loss = 0.02146620
Iteration 77, loss = 0.02184006
Iteration 78, loss = 0.02121192
Iteration 79, loss = 0.02135932
Iteration 80, loss = 0.02151899
Iteration 81, loss = 0.02133616
Iteration 82, loss = 0.02132068
Iteration 83, loss = 0.02128781
Iteration 84, loss = 0.02152527
Iteration 85, loss = 0.02160018
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67773131
Iteration 2, loss = 0.61309116
Iteration 3, loss = 0.51176002
Iteration 4, loss = 0.40179734
Iteration 5, loss = 0.30534978
Iteration 6, loss = 0.22952303
Iteration 7, loss = 0.17479366
Iteration 8, loss = 0.13644547
Iteration 9, loss = 0.10945923
Iteration 10, loss = 0.09064773
Iteration 11, loss = 0.07684959
Iteration 12, loss = 0.06683410
Iteration 13, loss = 0.05895183
Iteration 14, loss = 0.05284789
Iteration 15, loss = 0.04846171
Iteration 16, loss = 0.04440437
Iteration 17, loss = 0.04121937
Iteration 18, loss = 0.03853518
Iteration 19, loss = 0.03641610
Iteration 20, loss = 0.03462795
Iteration 21, loss = 0.03328723
Iteration 22, loss = 0.03181116
Iteration 23, loss = 0.03084083
Iteration 24, loss = 0.02975290
Iteration 25, loss = 0.02875665
Iteration 26, loss = 0.02804799
Iteration 27, loss = 0.02720241
Iteration 28, loss = 0.02679575
Iteration 29, loss = 0.02616846
Iteration 30, loss = 0.02558753
Iteration 31, loss = 0.02561692
Iteration 32, loss = 0.02488205
Iteration 33, loss = 0.02471278
Iteration 34, loss = 0.02413335
Iteration 35, loss = 0.02392257
Iteration 36, loss = 0.02348676
Iteration 37, loss = 0.02323278
Iteration 38, loss = 0.02325608
Iteration 39, loss = 0.02318430
Iteration 40, loss = 0.02299986
Iteration 41, loss = 0.02258051
Iteration 42, loss = 0.02265202
Iteration 43, loss = 0.02226802
Iteration 44, loss = 0.02232878
Iteration 45, loss = 0.02190960
Iteration 46, loss = 0.02194948
Iteration 47, loss = 0.02185401
Iteration 48, loss = 0.02170175
Iteration 49, loss = 0.02153756
Iteration 50, loss = 0.02157903
Iteration 51, loss = 0.02138276
Iteration 52, loss = 0.02136104
Iteration 53, loss = 0.02150845
Iteration 54, loss = 0.02127168
Iteration 55, loss = 0.02135087
Iteration 56, loss = 0.02095584
Iteration 57, loss = 0.02102972
Iteration 58, loss = 0.02093199
Iteration 59, loss = 0.02117493
Iteration 60, loss = 0.02076672
Iteration 61, loss = 0.02089973
Iteration 62, loss = 0.02070062
Iteration 63, loss = 0.02053133
Iteration 64, loss = 0.02126393
Iteration 65, loss = 0.02075314
Iteration 66, loss = 0.02078725
Iteration 67, loss = 0.02036648
Iteration 68, loss = 0.02068887
Iteration 69, loss = 0.02025880
Iteration 70, loss = 0.02045423
Iteration 71, loss = 0.02060907
Iteration 72, loss = 0.02033772
Iteration 73, loss = 0.02023348
Iteration 74, loss = 0.02040192
Iteration 75, loss = 0.02029394
Iteration 76, loss = 0.02009429
Iteration 77, loss = 0.02011426
Iteration 78, loss = 0.02024222
Iteration 79, loss = 0.02015036
Iteration 80, loss = 0.02018283
Iteration 81, loss = 0.02009323
Iteration 82, loss = 0.02043075
Iteration 83, loss = 0.02010313
Iteration 84, loss = 0.01994095
Iteration 85, loss = 0.02024304
Iteration 86, loss = 0.01977973
Iteration 87, loss = 0.02007232
Iteration 88, loss = 0.02029319
Iteration 89, loss = 0.01999700
Iteration 90, loss = 0.01990077
Iteration 91, loss = 0.02007693
Iteration 92, loss = 0.01979344
Iteration 93, loss = 0.02032708
Iteration 94, loss = 0.01973373
Iteration 95, loss = 0.01995911
Iteration 96, loss = 0.01984750
Iteration 97, loss = 0.01974088
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67618052
Iteration 2, loss = 0.61873537
Iteration 3, loss = 0.52799932
Iteration 4, loss = 0.42633764
Iteration 5, loss = 0.33172321
Iteration 6, loss = 0.25368816
Iteration 7, loss = 0.19514989
Iteration 8, loss = 0.15241243
Iteration 9, loss = 0.12214476
Iteration 10, loss = 0.10035276
Iteration 11, loss = 0.08435369
Iteration 12, loss = 0.07250054
Iteration 13, loss = 0.06352033
Iteration 14, loss = 0.05652946
Iteration 15, loss = 0.05097527
Iteration 16, loss = 0.04658338
Iteration 17, loss = 0.04274518
Iteration 18, loss = 0.04000529
Iteration 19, loss = 0.03735616
Iteration 20, loss = 0.03502534
Iteration 21, loss = 0.03334709
Iteration 22, loss = 0.03174471
Iteration 23, loss = 0.03066905
Iteration 24, loss = 0.02939366
Iteration 25, loss = 0.02822230
Iteration 26, loss = 0.02732018
Iteration 27, loss = 0.02683676
Iteration 28, loss = 0.02601222
Iteration 29, loss = 0.02551976
Iteration 30, loss = 0.02470399
Iteration 31, loss = 0.02414108
Iteration 32, loss = 0.02373070
Iteration 33, loss = 0.02326511
Iteration 34, loss = 0.02284079
Iteration 35, loss = 0.02256811
Iteration 36, loss = 0.02256177
Iteration 37, loss = 0.02214307
Iteration 38, loss = 0.02162894
Iteration 39, loss = 0.02132966
Iteration 40, loss = 0.02152484
Iteration 41, loss = 0.02276302
Iteration 42, loss = 0.02358374
Iteration 43, loss = 0.02301311
Iteration 44, loss = 0.02220480
Iteration 45, loss = 0.02153776
Iteration 46, loss = 0.02118443
Iteration 47, loss = 0.02072214
Iteration 48, loss = 0.02045243
Iteration 49, loss = 0.02019492
Iteration 50, loss = 0.01989973
Iteration 51, loss = 0.01982032
Iteration 52, loss = 0.01954585
Iteration 53, loss = 0.01938487
Iteration 54, loss = 0.01929475
Iteration 55, loss = 0.01916117
Iteration 56, loss = 0.01903886
Iteration 57, loss = 0.01897847
Iteration 58, loss = 0.01883400
Iteration 59, loss = 0.01893470
Iteration 60, loss = 0.01864505
Iteration 61, loss = 0.01862060
Iteration 62, loss = 0.01850319
Iteration 63, loss = 0.01854996
Iteration 64, loss = 0.01838872
Iteration 65, loss = 0.01839191
Iteration 66, loss = 0.01839338
Iteration 67, loss = 0.01817362
Iteration 68, loss = 0.01845697
Iteration 69, loss = 0.01828117
Iteration 70, loss = 0.01817451
Iteration 71, loss = 0.01806057
Iteration 72, loss = 0.01794511
Iteration 73, loss = 0.01792579
Iteration 74, loss = 0.01790449
Iteration 75, loss = 0.01795821
Iteration 76, loss = 0.01795369
Iteration 77, loss = 0.01791833
Iteration 78, loss = 0.01764705
Iteration 79, loss = 0.01769332
Iteration 80, loss = 0.01806644
Iteration 81, loss = 0.01793756
Iteration 82, loss = 0.01788524
Iteration 83, loss = 0.01770852
Iteration 84, loss = 0.01756842
Iteration 85, loss = 0.01765622
Iteration 86, loss = 0.01762270
Iteration 87, loss = 0.01740710
Iteration 88, loss = 0.01743845
Iteration 89, loss = 0.01745488
Iteration 90, loss = 0.01745209
Iteration 91, loss = 0.01737104
Iteration 92, loss = 0.01757635
Iteration 93, loss = 0.01721263
Iteration 94, loss = 0.01744549
Iteration 95, loss = 0.01733651
Iteration 96, loss = 0.01759555
Iteration 97, loss = 0.01751951
Iteration 98, loss = 0.01760988
Iteration 99, loss = 0.01710123
Iteration 100, loss = 0.01723165
Iteration 101, loss = 0.01725687
Iteration 102, loss = 0.01722311
Iteration 103, loss = 0.01712340
Iteration 104, loss = 0.01699785
Iteration 105, loss = 0.01699787
Iteration 106, loss = 0.01712449
Iteration 107, loss = 0.01712455
Iteration 108, loss = 0.01712285
Iteration 109, loss = 0.01708744
Iteration 110, loss = 0.01708265
Iteration 111, loss = 0.01697500
Iteration 112, loss = 0.01747700
Iteration 113, loss = 0.01675780
Iteration 114, loss = 0.01704008
Iteration 115, loss = 0.01712180
Iteration 116, loss = 0.01721323
Iteration 117, loss = 0.01857865
Iteration 118, loss = 0.01858053
Iteration 119, loss = 0.01803551
Iteration 120, loss = 0.01797167
Iteration 121, loss = 0.01795358
Iteration 122, loss = 0.01777387
Iteration 123, loss = 0.01804491
Iteration 124, loss = 0.01746912
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68499683
Iteration 2, loss = 0.65895169
Iteration 3, loss = 0.62950991
Iteration 4, loss = 0.59349810
Iteration 5, loss = 0.55092691
Iteration 6, loss = 0.50285515
Iteration 7, loss = 0.45280936
Iteration 8, loss = 0.40350316
Iteration 9, loss = 0.35592686
Iteration 10, loss = 0.31200852
Iteration 11, loss = 0.27256478
Iteration 12, loss = 0.23782041
Iteration 13, loss = 0.20780572
Iteration 14, loss = 0.18210603
Iteration 15, loss = 0.16027500
Iteration 16, loss = 0.14179683
Iteration 17, loss = 0.12610103
Iteration 18, loss = 0.11301976
Iteration 19, loss = 0.10199245
Iteration 20, loss = 0.09202818
Iteration 21, loss = 0.08413812
Iteration 22, loss = 0.07720527
Iteration 23, loss = 0.07097505
Iteration 24, loss = 0.06571069
Iteration 25, loss = 0.06119909
Iteration 26, loss = 0.05717526
Iteration 27, loss = 0.05363440
Iteration 28, loss = 0.05054927
Iteration 29, loss = 0.04777570
Iteration 30, loss = 0.04533336
Iteration 31, loss = 0.04306610
Iteration 32, loss = 0.04121216
Iteration 33, loss = 0.03937817
Iteration 34, loss = 0.03768752
Iteration 35, loss = 0.03646095
Iteration 36, loss = 0.03486721
Iteration 37, loss = 0.03369508
Iteration 38, loss = 0.03263970
Iteration 39, loss = 0.03147548
Iteration 40, loss = 0.03067392
Iteration 41, loss = 0.02989374
Iteration 42, loss = 0.02896438
Iteration 43, loss = 0.02814475
Iteration 44, loss = 0.02743765
Iteration 45, loss = 0.02664879
Iteration 46, loss = 0.02610895
Iteration 47, loss = 0.02562360
Iteration 48, loss = 0.02511611
Iteration 49, loss = 0.02459064
Iteration 50, loss = 0.02402295
Iteration 51, loss = 0.02379717
Iteration 52, loss = 0.02347957
Iteration 53, loss = 0.02311036
Iteration 54, loss = 0.02266712
Iteration 55, loss = 0.02223602
Iteration 56, loss = 0.02227922
Iteration 57, loss = 0.02216573
Iteration 58, loss = 0.02218468
Iteration 59, loss = 0.02163994
Iteration 60, loss = 0.02096741
Iteration 61, loss = 0.02066251
Iteration 62, loss = 0.02031407
Iteration 63, loss = 0.02006588
Iteration 64, loss = 0.01986203
Iteration 65, loss = 0.01963130
Iteration 66, loss = 0.01959522
Iteration 67, loss = 0.01931272
Iteration 68, loss = 0.01927686
Iteration 69, loss = 0.01907156
Iteration 70, loss = 0.01892551
Iteration 71, loss = 0.01880208
Iteration 72, loss = 0.01855264
Iteration 73, loss = 0.01838480
Iteration 74, loss = 0.01823399
Iteration 75, loss = 0.01835246
Iteration 76, loss = 0.01817248
Iteration 77, loss = 0.01819554
Iteration 78, loss = 0.01801074
Iteration 79, loss = 0.01812294
Iteration 80, loss = 0.01810809
Iteration 81, loss = 0.01793305
Iteration 82, loss = 0.01788235
Iteration 83, loss = 0.01804448
Iteration 84, loss = 0.01776958
Iteration 85, loss = 0.01778197
Iteration 86, loss = 0.01737553
Iteration 87, loss = 0.01733298
Iteration 88, loss = 0.01718455
Iteration 89, loss = 0.01746532
Iteration 90, loss = 0.01730577
Iteration 91, loss = 0.01726042
Iteration 92, loss = 0.01698958
Iteration 93, loss = 0.01701058
Iteration 94, loss = 0.01653164
Iteration 95, loss = 0.01662153
Iteration 96, loss = 0.01651178
Iteration 97, loss = 0.01635337
Iteration 98, loss = 0.01639668
Iteration 99, loss = 0.01686558
Iteration 100, loss = 0.01704453
Iteration 101, loss = 0.01696940
Iteration 102, loss = 0.01666726
Iteration 103, loss = 0.01657855
Iteration 104, loss = 0.01640711
Iteration 105, loss = 0.01618006
Iteration 106, loss = 0.01594142
Iteration 107, loss = 0.01591999
Iteration 108, loss = 0.01582562
Iteration 109, loss = 0.01572199
Iteration 110, loss = 0.01588877
Iteration 111, loss = 0.01587525
Iteration 112, loss = 0.01572934
Iteration 113, loss = 0.01584491
Iteration 114, loss = 0.01557177
Iteration 115, loss = 0.01557408
Iteration 116, loss = 0.01557413
Iteration 117, loss = 0.01552990
Iteration 118, loss = 0.01550127
Iteration 119, loss = 0.01540381
Iteration 120, loss = 0.01536370
Iteration 121, loss = 0.01550047
Iteration 122, loss = 0.01557070
Iteration 123, loss = 0.01555046
Iteration 124, loss = 0.01532392
Iteration 125, loss = 0.01526177
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67945671
Iteration 2, loss = 0.61077391
Iteration 3, loss = 0.50060028
Iteration 4, loss = 0.38381454
Iteration 5, loss = 0.28572843
Iteration 6, loss = 0.21293863
Iteration 7, loss = 0.16136203
Iteration 8, loss = 0.12573481
Iteration 9, loss = 0.10105264
Iteration 10, loss = 0.08371800
Iteration 11, loss = 0.07100569
Iteration 12, loss = 0.06177183
Iteration 13, loss = 0.05430223
Iteration 14, loss = 0.04875385
Iteration 15, loss = 0.04436380
Iteration 16, loss = 0.04094866
Iteration 17, loss = 0.03797577
Iteration 18, loss = 0.03524192
Iteration 19, loss = 0.03349977
Iteration 20, loss = 0.03187513
Iteration 21, loss = 0.03031364
Iteration 22, loss = 0.02915153
Iteration 23, loss = 0.02806657
Iteration 24, loss = 0.02689373
Iteration 25, loss = 0.02592718
Iteration 26, loss = 0.02540819
Iteration 27, loss = 0.02485384
Iteration 28, loss = 0.02429133
Iteration 29, loss = 0.02373409
Iteration 30, loss = 0.02340325
Iteration 31, loss = 0.02298016
Iteration 32, loss = 0.02235355
Iteration 33, loss = 0.02223530
Iteration 34, loss = 0.02183488
Iteration 35, loss = 0.02167544
Iteration 36, loss = 0.02142869
Iteration 37, loss = 0.02113902
Iteration 38, loss = 0.02105770
Iteration 39, loss = 0.02078907
Iteration 40, loss = 0.02055235
Iteration 41, loss = 0.02024929
Iteration 42, loss = 0.02030614
Iteration 43, loss = 0.02000330
Iteration 44, loss = 0.01998680
Iteration 45, loss = 0.01966361
Iteration 46, loss = 0.01962026
Iteration 47, loss = 0.01944777
Iteration 48, loss = 0.01932442
Iteration 49, loss = 0.01921163
Iteration 50, loss = 0.01958759
Iteration 51, loss = 0.01912958
Iteration 52, loss = 0.01913410
Iteration 53, loss = 0.01904753
Iteration 54, loss = 0.01907514
Iteration 55, loss = 0.01906161
Iteration 56, loss = 0.01891708
Iteration 57, loss = 0.01882434
Iteration 58, loss = 0.01868212
Iteration 59, loss = 0.01888354
Iteration 60, loss = 0.01868678
Iteration 61, loss = 0.01853566
Iteration 62, loss = 0.01864799
Iteration 63, loss = 0.01844846
Iteration 64, loss = 0.01838525
Iteration 65, loss = 0.01851870
Iteration 66, loss = 0.01838390
Iteration 67, loss = 0.01851903
Iteration 68, loss = 0.01807673
Iteration 69, loss = 0.01876705
Iteration 70, loss = 0.01836674
Iteration 71, loss = 0.01804638
Iteration 72, loss = 0.01808756
Iteration 73, loss = 0.01804495
Iteration 74, loss = 0.01826086
Iteration 75, loss = 0.01840466
Iteration 76, loss = 0.01803235
Iteration 77, loss = 0.01806353
Iteration 78, loss = 0.01781416
Iteration 79, loss = 0.01801395
Iteration 80, loss = 0.01795301
Iteration 81, loss = 0.01814544
Iteration 82, loss = 0.01843356
Iteration 83, loss = 0.01784375
Iteration 84, loss = 0.01774324
Iteration 85, loss = 0.01765830
Iteration 86, loss = 0.01785106
Iteration 87, loss = 0.01793564
Iteration 88, loss = 0.01786748
Iteration 89, loss = 0.01783200
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67783783
Iteration 2, loss = 0.65512271
Iteration 3, loss = 0.63281090
Iteration 4, loss = 0.60724397
Iteration 5, loss = 0.57727685
Iteration 6, loss = 0.54320583
Iteration 7, loss = 0.50643674
Iteration 8, loss = 0.46800974
Iteration 9, loss = 0.42968567
Iteration 10, loss = 0.39285370
Iteration 11, loss = 0.35758101
Iteration 12, loss = 0.32472817
Iteration 13, loss = 0.29395148
Iteration 14, loss = 0.26575974
Iteration 15, loss = 0.24012009
Iteration 16, loss = 0.21694035
Iteration 17, loss = 0.19623373
Iteration 18, loss = 0.17744560
Iteration 19, loss = 0.16095934
Iteration 20, loss = 0.14620138
Iteration 21, loss = 0.13322411
Iteration 22, loss = 0.12170256
Iteration 23, loss = 0.11149602
Iteration 24, loss = 0.10244902
Iteration 25, loss = 0.09445922
Iteration 26, loss = 0.08748457
Iteration 27, loss = 0.08114750
Iteration 28, loss = 0.07550783
Iteration 29, loss = 0.07047065
Iteration 30, loss = 0.06597832
Iteration 31, loss = 0.06192948
Iteration 32, loss = 0.05833954
Iteration 33, loss = 0.05502567
Iteration 34, loss = 0.05210664
Iteration 35, loss = 0.04934594
Iteration 36, loss = 0.04689146
Iteration 37, loss = 0.04459978
Iteration 38, loss = 0.04258026
Iteration 39, loss = 0.04071342
Iteration 40, loss = 0.03892313
Iteration 41, loss = 0.03737402
Iteration 42, loss = 0.03589919
Iteration 43, loss = 0.03453760
Iteration 44, loss = 0.03323590
Iteration 45, loss = 0.03212458
Iteration 46, loss = 0.03100012
Iteration 47, loss = 0.03001808
Iteration 48, loss = 0.02903852
Iteration 49, loss = 0.02812552
Iteration 50, loss = 0.02734676
Iteration 51, loss = 0.02652480
Iteration 52, loss = 0.02586675
Iteration 53, loss = 0.02516526
Iteration 54, loss = 0.02450626
Iteration 55, loss = 0.02390289
Iteration 56, loss = 0.02332261
Iteration 57, loss = 0.02277899
Iteration 58, loss = 0.02226052
Iteration 59, loss = 0.02179896
Iteration 60, loss = 0.02131217
Iteration 61, loss = 0.02091517
Iteration 62, loss = 0.02048709
Iteration 63, loss = 0.02008950
Iteration 64, loss = 0.01980270
Iteration 65, loss = 0.01941204
Iteration 66, loss = 0.01906586
Iteration 67, loss = 0.01871022
Iteration 68, loss = 0.01843436
Iteration 69, loss = 0.01815191
Iteration 70, loss = 0.01786997
Iteration 71, loss = 0.01759255
Iteration 72, loss = 0.01730512
Iteration 73, loss = 0.01707070
Iteration 74, loss = 0.01685559
Iteration 75, loss = 0.01660789
Iteration 76, loss = 0.01639281
Iteration 77, loss = 0.01618396
Iteration 78, loss = 0.01597796
Iteration 79, loss = 0.01581971
Iteration 80, loss = 0.01574643
Iteration 81, loss = 0.01544258
Iteration 82, loss = 0.01527869
Iteration 83, loss = 0.01514618
Iteration 84, loss = 0.01496090
Iteration 85, loss = 0.01482883
Iteration 86, loss = 0.01470855
Iteration 87, loss = 0.01461310
Iteration 88, loss = 0.01443868
Iteration 89, loss = 0.01427406
Iteration 90, loss = 0.01414481
Iteration 91, loss = 0.01404402
Iteration 92, loss = 0.01390537
Iteration 93, loss = 0.01379455
Iteration 94, loss = 0.01368303
Iteration 95, loss = 0.01360252
Iteration 96, loss = 0.01344101
Iteration 97, loss = 0.01334514
Iteration 98, loss = 0.01324248
Iteration 99, loss = 0.01318893
Iteration 100, loss = 0.01309610
Iteration 101, loss = 0.01300022
Iteration 102, loss = 0.01290384
Iteration 103, loss = 0.01281784
Iteration 104, loss = 0.01272382
Iteration 105, loss = 0.01276017
Iteration 106, loss = 0.01263373
Iteration 107, loss = 0.01250196
Iteration 108, loss = 0.01246911
Iteration 109, loss = 0.01238089
Iteration 110, loss = 0.01230070
Iteration 111, loss = 0.01224299
Iteration 112, loss = 0.01224787
Iteration 113, loss = 0.01209616
Iteration 114, loss = 0.01208825
Iteration 115, loss = 0.01195813
Iteration 116, loss = 0.01193630
Iteration 117, loss = 0.01189476
Iteration 118, loss = 0.01185670
Iteration 119, loss = 0.01177963
Iteration 120, loss = 0.01174608
Iteration 121, loss = 0.01173530
Iteration 122, loss = 0.01164233
Iteration 123, loss = 0.01158864
Iteration 124, loss = 0.01150560
Iteration 125, loss = 0.01147677
Iteration 126, loss = 0.01140475
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67661809
Iteration 2, loss = 0.63868257
Iteration 3, loss = 0.59222848
Iteration 4, loss = 0.53409243
Iteration 5, loss = 0.47169690
Iteration 6, loss = 0.41086549
Iteration 7, loss = 0.35468879
Iteration 8, loss = 0.30473586
Iteration 9, loss = 0.26136308
Iteration 10, loss = 0.22428780
Iteration 11, loss = 0.19326997
Iteration 12, loss = 0.16719729
Iteration 13, loss = 0.14557167
Iteration 14, loss = 0.12746772
Iteration 15, loss = 0.11240487
Iteration 16, loss = 0.09971293
Iteration 17, loss = 0.08903349
Iteration 18, loss = 0.08013386
Iteration 19, loss = 0.07242651
Iteration 20, loss = 0.06598783
Iteration 21, loss = 0.06036339
Iteration 22, loss = 0.05556408
Iteration 23, loss = 0.05136693
Iteration 24, loss = 0.04781829
Iteration 25, loss = 0.04454805
Iteration 26, loss = 0.04170507
Iteration 27, loss = 0.03930393
Iteration 28, loss = 0.03701472
Iteration 29, loss = 0.03506760
Iteration 30, loss = 0.03326731
Iteration 31, loss = 0.03174323
Iteration 32, loss = 0.03030024
Iteration 33, loss = 0.02894272
Iteration 34, loss = 0.02789233
Iteration 35, loss = 0.02673517
Iteration 36, loss = 0.02580704
Iteration 37, loss = 0.02490918
Iteration 38, loss = 0.02406853
Iteration 39, loss = 0.02336381
Iteration 40, loss = 0.02262583
Iteration 41, loss = 0.02196172
Iteration 42, loss = 0.02142892
Iteration 43, loss = 0.02086186
Iteration 44, loss = 0.02033963
Iteration 45, loss = 0.01987741
Iteration 46, loss = 0.01942894
Iteration 47, loss = 0.01905745
Iteration 48, loss = 0.01861679
Iteration 49, loss = 0.01830498
Iteration 50, loss = 0.01798349
Iteration 51, loss = 0.01761620
Iteration 52, loss = 0.01729442
Iteration 53, loss = 0.01701498
Iteration 54, loss = 0.01685359
Iteration 55, loss = 0.01660911
Iteration 56, loss = 0.01635362
Iteration 57, loss = 0.01616672
Iteration 58, loss = 0.01599992
Iteration 59, loss = 0.01570955
Iteration 60, loss = 0.01547545
Iteration 61, loss = 0.01533022
Iteration 62, loss = 0.01515836
Iteration 63, loss = 0.01500182
Iteration 64, loss = 0.01482416
Iteration 65, loss = 0.01471592
Iteration 66, loss = 0.01455992
Iteration 67, loss = 0.01451242
Iteration 68, loss = 0.01446377
Iteration 69, loss = 0.01426872
Iteration 70, loss = 0.01405744
Iteration 71, loss = 0.01399421
Iteration 72, loss = 0.01403438
Iteration 73, loss = 0.01382131
Iteration 74, loss = 0.01377600
Iteration 75, loss = 0.01359290
Iteration 76, loss = 0.01357569
Iteration 77, loss = 0.01342852
Iteration 78, loss = 0.01330442
Iteration 79, loss = 0.01321806
Iteration 80, loss = 0.01323121
Iteration 81, loss = 0.01312328
Iteration 82, loss = 0.01312292
Iteration 83, loss = 0.01297664
Iteration 84, loss = 0.01288323
Iteration 85, loss = 0.01292189
Iteration 86, loss = 0.01282359
Iteration 87, loss = 0.01278768
Iteration 88, loss = 0.01268077
Iteration 89, loss = 0.01263471
Iteration 90, loss = 0.01270319
Iteration 91, loss = 0.01257319
Iteration 92, loss = 0.01243952
Iteration 93, loss = 0.01249401
Iteration 94, loss = 0.01245715
Iteration 95, loss = 0.01238584
Iteration 96, loss = 0.01230064
Iteration 97, loss = 0.01220382
Iteration 98, loss = 0.01222858
Iteration 99, loss = 0.01220902
Iteration 100, loss = 0.01224136
Iteration 101, loss = 0.01213206
Iteration 102, loss = 0.01217292
Iteration 103, loss = 0.01200170
Iteration 104, loss = 0.01206124
Iteration 105, loss = 0.01205412
Iteration 106, loss = 0.01203142
Iteration 107, loss = 0.01187097
Iteration 108, loss = 0.01193187
Iteration 109, loss = 0.01186940
Iteration 110, loss = 0.01186447
Iteration 111, loss = 0.01181030
Iteration 112, loss = 0.01179141
Iteration 113, loss = 0.01176588
Iteration 114, loss = 0.01172482
Iteration 115, loss = 0.01169351
Iteration 116, loss = 0.01174262
Iteration 117, loss = 0.01166724
Iteration 118, loss = 0.01169612
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67451635
Iteration 2, loss = 0.60189840
Iteration 3, loss = 0.49735286
Iteration 4, loss = 0.38639405
Iteration 5, loss = 0.29095622
Iteration 6, loss = 0.21763318
Iteration 7, loss = 0.16541083
Iteration 8, loss = 0.12910956
Iteration 9, loss = 0.10391981
Iteration 10, loss = 0.08562129
Iteration 11, loss = 0.07262275
Iteration 12, loss = 0.06284010
Iteration 13, loss = 0.05541428
Iteration 14, loss = 0.04935509
Iteration 15, loss = 0.04493872
Iteration 16, loss = 0.04136001
Iteration 17, loss = 0.03838181
Iteration 18, loss = 0.03583664
Iteration 19, loss = 0.03401952
Iteration 20, loss = 0.03196737
Iteration 21, loss = 0.03047070
Iteration 22, loss = 0.02950081
Iteration 23, loss = 0.02801110
Iteration 24, loss = 0.02733447
Iteration 25, loss = 0.02655759
Iteration 26, loss = 0.02550552
Iteration 27, loss = 0.02506105
Iteration 28, loss = 0.02445536
Iteration 29, loss = 0.02371873
Iteration 30, loss = 0.02374060
Iteration 31, loss = 0.02311672
Iteration 32, loss = 0.02261208
Iteration 33, loss = 0.02230555
Iteration 34, loss = 0.02205675
Iteration 35, loss = 0.02147872
Iteration 36, loss = 0.02148891
Iteration 37, loss = 0.02121302
Iteration 38, loss = 0.02103690
Iteration 39, loss = 0.02082515
Iteration 40, loss = 0.02049339
Iteration 41, loss = 0.02019466
Iteration 42, loss = 0.02033230
Iteration 43, loss = 0.02002952
Iteration 44, loss = 0.01976061
Iteration 45, loss = 0.01968963
Iteration 46, loss = 0.01961108
Iteration 47, loss = 0.01948820
Iteration 48, loss = 0.01938306
Iteration 49, loss = 0.01947973
Iteration 50, loss = 0.01941923
Iteration 51, loss = 0.01926948
Iteration 52, loss = 0.01927835
Iteration 53, loss = 0.01901009
Iteration 54, loss = 0.01894571
Iteration 55, loss = 0.01890929
Iteration 56, loss = 0.01875087
Iteration 57, loss = 0.01860255
Iteration 58, loss = 0.01852906
Iteration 59, loss = 0.01880244
Iteration 60, loss = 0.01860319
Iteration 61, loss = 0.01842291
Iteration 62, loss = 0.01856454
Iteration 63, loss = 0.01869430
Iteration 64, loss = 0.01838704
Iteration 65, loss = 0.01866037
Iteration 66, loss = 0.01868759
Iteration 67, loss = 0.01857668
Iteration 68, loss = 0.01852841
Iteration 69, loss = 0.01844039
Iteration 70, loss = 0.01804907
Iteration 71, loss = 0.01813775
Iteration 72, loss = 0.01809692
Iteration 73, loss = 0.01828880
Iteration 74, loss = 0.01789691
Iteration 75, loss = 0.01780644
Iteration 76, loss = 0.01799371
Iteration 77, loss = 0.01807373
Iteration 78, loss = 0.01804652
Iteration 79, loss = 0.01783939
Iteration 80, loss = 0.01801084
Iteration 81, loss = 0.01790001
Iteration 82, loss = 0.01798654
Iteration 83, loss = 0.01802011
Iteration 84, loss = 0.01793780
Iteration 85, loss = 0.01800783
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71699053
Iteration 2, loss = 0.67847418
Iteration 3, loss = 0.62785316
Iteration 4, loss = 0.56696165
Iteration 5, loss = 0.49530315
Iteration 6, loss = 0.41949087
Iteration 7, loss = 0.34786273
Iteration 8, loss = 0.28567203
Iteration 9, loss = 0.23432406
Iteration 10, loss = 0.19325783
Iteration 11, loss = 0.16074557
Iteration 12, loss = 0.13539637
Iteration 13, loss = 0.11546703
Iteration 14, loss = 0.09987420
Iteration 15, loss = 0.08741672
Iteration 16, loss = 0.07745036
Iteration 17, loss = 0.06940675
Iteration 18, loss = 0.06263620
Iteration 19, loss = 0.05721708
Iteration 20, loss = 0.05248934
Iteration 21, loss = 0.04860645
Iteration 22, loss = 0.04535163
Iteration 23, loss = 0.04256305
Iteration 24, loss = 0.03999920
Iteration 25, loss = 0.03777011
Iteration 26, loss = 0.03596440
Iteration 27, loss = 0.03415014
Iteration 28, loss = 0.03271690
Iteration 29, loss = 0.03146891
Iteration 30, loss = 0.03032876
Iteration 31, loss = 0.02928589
Iteration 32, loss = 0.02828209
Iteration 33, loss = 0.02748033
Iteration 34, loss = 0.02668516
Iteration 35, loss = 0.02589454
Iteration 36, loss = 0.02529523
Iteration 37, loss = 0.02476551
Iteration 38, loss = 0.02422571
Iteration 39, loss = 0.02377502
Iteration 40, loss = 0.02320512
Iteration 41, loss = 0.02289502
Iteration 42, loss = 0.02246919
Iteration 43, loss = 0.02202982
Iteration 44, loss = 0.02188259
Iteration 45, loss = 0.02145849
Iteration 46, loss = 0.02138078
Iteration 47, loss = 0.02099227
Iteration 48, loss = 0.02078959
Iteration 49, loss = 0.02055114
Iteration 50, loss = 0.02021889
Iteration 51, loss = 0.02000341
Iteration 52, loss = 0.02009525
Iteration 53, loss = 0.01961116
Iteration 54, loss = 0.01938509
Iteration 55, loss = 0.01927287
Iteration 56, loss = 0.01914406
Iteration 57, loss = 0.01899973
Iteration 58, loss = 0.01900635
Iteration 59, loss = 0.01878623
Iteration 60, loss = 0.01857881
Iteration 61, loss = 0.01855809
Iteration 62, loss = 0.01839199
Iteration 63, loss = 0.01830676
Iteration 64, loss = 0.01835639
Iteration 65, loss = 0.01830853
Iteration 66, loss = 0.01843397
Iteration 67, loss = 0.01818662
Iteration 68, loss = 0.01788480
Iteration 69, loss = 0.01773945
Iteration 70, loss = 0.01780383
Iteration 71, loss = 0.01762601
Iteration 72, loss = 0.01748018
Iteration 73, loss = 0.01754956
Iteration 74, loss = 0.01750188
Iteration 75, loss = 0.01752296
Iteration 76, loss = 0.01739234
Iteration 77, loss = 0.01734931
Iteration 78, loss = 0.01731738
Iteration 79, loss = 0.01709813
Iteration 80, loss = 0.01723405
Iteration 81, loss = 0.01710073
Iteration 82, loss = 0.01708846
Iteration 83, loss = 0.01712882
Iteration 84, loss = 0.01712144
Iteration 85, loss = 0.01689278
Iteration 86, loss = 0.01684042
Iteration 87, loss = 0.01690463
Iteration 88, loss = 0.01680496
Iteration 89, loss = 0.01677309
Iteration 90, loss = 0.01699341
Iteration 91, loss = 0.01668663
Iteration 92, loss = 0.01673830
Iteration 93, loss = 0.01667460
Iteration 94, loss = 0.01679354
Iteration 95, loss = 0.01661521
Iteration 96, loss = 0.01647126
Iteration 97, loss = 0.01650350
Iteration 98, loss = 0.01658061
Iteration 99, loss = 0.01650643
Iteration 100, loss = 0.01641915
Iteration 101, loss = 0.01631494
Iteration 102, loss = 0.01636124
Iteration 103, loss = 0.01632528
Iteration 104, loss = 0.01650962
Iteration 105, loss = 0.01628221
Iteration 106, loss = 0.01639116
Iteration 107, loss = 0.01619501
Iteration 108, loss = 0.01640280
Iteration 109, loss = 0.01623771
Iteration 110, loss = 0.01631365
Iteration 111, loss = 0.01623310
Iteration 112, loss = 0.01621176
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70042164
Iteration 2, loss = 0.63462727
Iteration 3, loss = 0.54379553
Iteration 4, loss = 0.43229982
Iteration 5, loss = 0.33230867
Iteration 6, loss = 0.25284529
Iteration 7, loss = 0.19411882
Iteration 8, loss = 0.15168060
Iteration 9, loss = 0.12174335
Iteration 10, loss = 0.10014383
Iteration 11, loss = 0.08466477
Iteration 12, loss = 0.07301361
Iteration 13, loss = 0.06422451
Iteration 14, loss = 0.05751164
Iteration 15, loss = 0.05213782
Iteration 16, loss = 0.04769297
Iteration 17, loss = 0.04430192
Iteration 18, loss = 0.04131668
Iteration 19, loss = 0.03873952
Iteration 20, loss = 0.03657607
Iteration 21, loss = 0.03479761
Iteration 22, loss = 0.03310523
Iteration 23, loss = 0.03163806
Iteration 24, loss = 0.03050987
Iteration 25, loss = 0.02909852
Iteration 26, loss = 0.02888274
Iteration 27, loss = 0.02901102
Iteration 28, loss = 0.02791566
Iteration 29, loss = 0.02701913
Iteration 30, loss = 0.02629885
Iteration 31, loss = 0.02553393
Iteration 32, loss = 0.02490497
Iteration 33, loss = 0.02446382
Iteration 34, loss = 0.02385998
Iteration 35, loss = 0.02358463
Iteration 36, loss = 0.02318215
Iteration 37, loss = 0.02269109
Iteration 38, loss = 0.02242979
Iteration 39, loss = 0.02211143
Iteration 40, loss = 0.02183872
Iteration 41, loss = 0.02157012
Iteration 42, loss = 0.02142542
Iteration 43, loss = 0.02125975
Iteration 44, loss = 0.02139515
Iteration 45, loss = 0.02141380
Iteration 46, loss = 0.02088416
Iteration 47, loss = 0.02064115
Iteration 48, loss = 0.02044288
Iteration 49, loss = 0.02071761
Iteration 50, loss = 0.02042332
Iteration 51, loss = 0.02013488
Iteration 52, loss = 0.02001058
Iteration 53, loss = 0.01989019
Iteration 54, loss = 0.01974080
Iteration 55, loss = 0.01991222
Iteration 56, loss = 0.01957386
Iteration 57, loss = 0.01947206
Iteration 58, loss = 0.01939161
Iteration 59, loss = 0.01927176
Iteration 60, loss = 0.01929672
Iteration 61, loss = 0.01912899
Iteration 62, loss = 0.01925790
Iteration 63, loss = 0.01904258
Iteration 64, loss = 0.01896057
Iteration 65, loss = 0.01900495
Iteration 66, loss = 0.01869573
Iteration 67, loss = 0.01897831
Iteration 68, loss = 0.01850661
Iteration 69, loss = 0.01862235
Iteration 70, loss = 0.01853412
Iteration 71, loss = 0.01856590
Iteration 72, loss = 0.01875819
Iteration 73, loss = 0.01835094
Iteration 74, loss = 0.01861813
Iteration 75, loss = 0.01826432
Iteration 76, loss = 0.01824478
Iteration 77, loss = 0.01828145
Iteration 78, loss = 0.01808657
Iteration 79, loss = 0.01824751
Iteration 80, loss = 0.01811181
Iteration 81, loss = 0.01814251
Iteration 82, loss = 0.01823594
Iteration 83, loss = 0.01810458
Iteration 84, loss = 0.01807485
Iteration 85, loss = 0.01811827
Iteration 86, loss = 0.01788836
Iteration 87, loss = 0.01782436
Iteration 88, loss = 0.01790569
Iteration 89, loss = 0.01786520
Iteration 90, loss = 0.01865510
Iteration 91, loss = 0.01858299
Iteration 92, loss = 0.01808749
Iteration 93, loss = 0.01812679
Iteration 94, loss = 0.01766099
Iteration 95, loss = 0.01762179
Iteration 96, loss = 0.01768312
Iteration 97, loss = 0.01751546
Iteration 98, loss = 0.01763501
Iteration 99, loss = 0.01750114
Iteration 100, loss = 0.01752133
Iteration 101, loss = 0.01850638
Iteration 102, loss = 0.01883780
Iteration 103, loss = 0.01839136
Iteration 104, loss = 0.01806047
Iteration 105, loss = 0.01805108
Iteration 106, loss = 0.01809183
Iteration 107, loss = 0.01830919
Iteration 108, loss = 0.01978734
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68052645
Iteration 2, loss = 0.66475237
Iteration 3, loss = 0.65147602
Iteration 4, loss = 0.63841179
Iteration 5, loss = 0.62454471
Iteration 6, loss = 0.60965181
Iteration 7, loss = 0.59349231
Iteration 8, loss = 0.57555050
Iteration 9, loss = 0.55616570
Iteration 10, loss = 0.53506068
Iteration 11, loss = 0.51244131
Iteration 12, loss = 0.48865401
Iteration 13, loss = 0.46394864
Iteration 14, loss = 0.43877226
Iteration 15, loss = 0.41377029
Iteration 16, loss = 0.38900458
Iteration 17, loss = 0.36485888
Iteration 18, loss = 0.34172417
Iteration 19, loss = 0.31955317
Iteration 20, loss = 0.29843820
Iteration 21, loss = 0.27843354
Iteration 22, loss = 0.25971666
Iteration 23, loss = 0.24209806
Iteration 24, loss = 0.22565437
Iteration 25, loss = 0.21029939
Iteration 26, loss = 0.19609172
Iteration 27, loss = 0.18288644
Iteration 28, loss = 0.17066998
Iteration 29, loss = 0.15941759
Iteration 30, loss = 0.14905349
Iteration 31, loss = 0.13949939
Iteration 32, loss = 0.13067783
Iteration 33, loss = 0.12260003
Iteration 34, loss = 0.11512758
Iteration 35, loss = 0.10833371
Iteration 36, loss = 0.10202671
Iteration 37, loss = 0.09623551
Iteration 38, loss = 0.09086042
Iteration 39, loss = 0.08591668
Iteration 40, loss = 0.08140026
Iteration 41, loss = 0.07717866
Iteration 42, loss = 0.07329028
Iteration 43, loss = 0.06965915
Iteration 44, loss = 0.06632072
Iteration 45, loss = 0.06322298
Iteration 46, loss = 0.06028976
Iteration 47, loss = 0.05760856
Iteration 48, loss = 0.05509232
Iteration 49, loss = 0.05276414
Iteration 50, loss = 0.05059310
Iteration 51, loss = 0.04853765
Iteration 52, loss = 0.04663075
Iteration 53, loss = 0.04485533
Iteration 54, loss = 0.04318295
Iteration 55, loss = 0.04157917
Iteration 56, loss = 0.04010103
Iteration 57, loss = 0.03869176
Iteration 58, loss = 0.03737727
Iteration 59, loss = 0.03613878
Iteration 60, loss = 0.03494984
Iteration 61, loss = 0.03384682
Iteration 62, loss = 0.03277088
Iteration 63, loss = 0.03178029
Iteration 64, loss = 0.03081994
Iteration 65, loss = 0.02992873
Iteration 66, loss = 0.02905858
Iteration 67, loss = 0.02823779
Iteration 68, loss = 0.02745887
Iteration 69, loss = 0.02671796
Iteration 70, loss = 0.02600028
Iteration 71, loss = 0.02534631
Iteration 72, loss = 0.02467456
Iteration 73, loss = 0.02406090
Iteration 74, loss = 0.02347795
Iteration 75, loss = 0.02291279
Iteration 76, loss = 0.02236446
Iteration 77, loss = 0.02185117
Iteration 78, loss = 0.02136735
Iteration 79, loss = 0.02085804
Iteration 80, loss = 0.02045082
Iteration 81, loss = 0.01997253
Iteration 82, loss = 0.01955491
Iteration 83, loss = 0.01915215
Iteration 84, loss = 0.01879704
Iteration 85, loss = 0.01840366
Iteration 86, loss = 0.01805396
Iteration 87, loss = 0.01771137
Iteration 88, loss = 0.01738647
Iteration 89, loss = 0.01705795
Iteration 90, loss = 0.01676273
Iteration 91, loss = 0.01647812
Iteration 92, loss = 0.01618439
Iteration 93, loss = 0.01591027
Iteration 94, loss = 0.01563682
Iteration 95, loss = 0.01537780
Iteration 96, loss = 0.01513333
Iteration 97, loss = 0.01488173
Iteration 98, loss = 0.01465382
Iteration 99, loss = 0.01440532
Iteration 100, loss = 0.01417873
Iteration 101, loss = 0.01397857
Iteration 102, loss = 0.01377697
Iteration 103, loss = 0.01357738
Iteration 104, loss = 0.01337832
Iteration 105, loss = 0.01319422
Iteration 106, loss = 0.01298987
Iteration 107, loss = 0.01280445
Iteration 108, loss = 0.01264354
Iteration 109, loss = 0.01245209
Iteration 110, loss = 0.01231453
Iteration 111, loss = 0.01212525
Iteration 112, loss = 0.01198873
Iteration 113, loss = 0.01184501
Iteration 114, loss = 0.01168610
Iteration 115, loss = 0.01156423
Iteration 116, loss = 0.01143322
Iteration 117, loss = 0.01130770
Iteration 118, loss = 0.01119281
Iteration 119, loss = 0.01107453
Iteration 120, loss = 0.01093596
Iteration 121, loss = 0.01082393
Iteration 122, loss = 0.01068563
Iteration 123, loss = 0.01056498
Iteration 124, loss = 0.01045128
Iteration 125, loss = 0.01033707
Iteration 126, loss = 0.01022424
Iteration 127, loss = 0.01014569
Iteration 128, loss = 0.01002639
Iteration 129, loss = 0.00989335
Iteration 130, loss = 0.00981625
Iteration 131, loss = 0.00970092
Iteration 132, loss = 0.00962367
Iteration 133, loss = 0.00954560
Iteration 134, loss = 0.00942997
Iteration 135, loss = 0.00934689
Iteration 136, loss = 0.00928433
Iteration 137, loss = 0.00918606
Iteration 138, loss = 0.00910766
Iteration 139, loss = 0.00905250
Iteration 140, loss = 0.00895504
Iteration 141, loss = 0.00889791
Iteration 142, loss = 0.00880533
Iteration 143, loss = 0.00873510
Iteration 144, loss = 0.00867264
Iteration 145, loss = 0.00861169
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67178067
Iteration 2, loss = 0.65210019
Iteration 3, loss = 0.63708919
Iteration 4, loss = 0.62287256
Iteration 5, loss = 0.60815258
Iteration 6, loss = 0.59238831
Iteration 7, loss = 0.57523524
Iteration 8, loss = 0.55704771
Iteration 9, loss = 0.53775248
Iteration 10, loss = 0.51727370
Iteration 11, loss = 0.49622774
Iteration 12, loss = 0.47437975
Iteration 13, loss = 0.45252814
Iteration 14, loss = 0.43017391
Iteration 15, loss = 0.40821963
Iteration 16, loss = 0.38676670
Iteration 17, loss = 0.36573419
Iteration 18, loss = 0.34530304
Iteration 19, loss = 0.32590787
Iteration 20, loss = 0.30755797
Iteration 21, loss = 0.29015424
Iteration 22, loss = 0.27363194
Iteration 23, loss = 0.25820597
Iteration 24, loss = 0.24362692
Iteration 25, loss = 0.22997477
Iteration 26, loss = 0.21721516
Iteration 27, loss = 0.20526553
Iteration 28, loss = 0.19408235
Iteration 29, loss = 0.18363645
Iteration 30, loss = 0.17389628
Iteration 31, loss = 0.16474849
Iteration 32, loss = 0.15625376
Iteration 33, loss = 0.14831937
Iteration 34, loss = 0.14085450
Iteration 35, loss = 0.13386834
Iteration 36, loss = 0.12738571
Iteration 37, loss = 0.12127849
Iteration 38, loss = 0.11554239
Iteration 39, loss = 0.11020629
Iteration 40, loss = 0.10516164
Iteration 41, loss = 0.10043416
Iteration 42, loss = 0.09600579
Iteration 43, loss = 0.09186887
Iteration 44, loss = 0.08800434
Iteration 45, loss = 0.08431126
Iteration 46, loss = 0.08087087
Iteration 47, loss = 0.07765340
Iteration 48, loss = 0.07454776
Iteration 49, loss = 0.07165935
Iteration 50, loss = 0.06894944
Iteration 51, loss = 0.06635076
Iteration 52, loss = 0.06394681
Iteration 53, loss = 0.06164456
Iteration 54, loss = 0.05949014
Iteration 55, loss = 0.05741592
Iteration 56, loss = 0.05544207
Iteration 57, loss = 0.05360336
Iteration 58, loss = 0.05184294
Iteration 59, loss = 0.05019781
Iteration 60, loss = 0.04858574
Iteration 61, loss = 0.04711586
Iteration 62, loss = 0.04565118
Iteration 63, loss = 0.04429397
Iteration 64, loss = 0.04299670
Iteration 65, loss = 0.04171491
Iteration 66, loss = 0.04056624
Iteration 67, loss = 0.03940777
Iteration 68, loss = 0.03831443
Iteration 69, loss = 0.03730321
Iteration 70, loss = 0.03628970
Iteration 71, loss = 0.03533698
Iteration 72, loss = 0.03444515
Iteration 73, loss = 0.03355174
Iteration 74, loss = 0.03273955
Iteration 75, loss = 0.03194078
Iteration 76, loss = 0.03116504
Iteration 77, loss = 0.03046136
Iteration 78, loss = 0.02973979
Iteration 79, loss = 0.02907250
Iteration 80, loss = 0.02840567
Iteration 81, loss = 0.02777749
Iteration 82, loss = 0.02718302
Iteration 83, loss = 0.02660347
Iteration 84, loss = 0.02603707
Iteration 85, loss = 0.02551514
Iteration 86, loss = 0.02496953
Iteration 87, loss = 0.02447729
Iteration 88, loss = 0.02396945
Iteration 89, loss = 0.02350083
Iteration 90, loss = 0.02305471
Iteration 91, loss = 0.02260354
Iteration 92, loss = 0.02218464
Iteration 93, loss = 0.02180069
Iteration 94, loss = 0.02140388
Iteration 95, loss = 0.02101051
Iteration 96, loss = 0.02064535
Iteration 97, loss = 0.02028687
Iteration 98, loss = 0.01997701
Iteration 99, loss = 0.01957039
Iteration 100, loss = 0.01925555
Iteration 101, loss = 0.01892768
Iteration 102, loss = 0.01864295
Iteration 103, loss = 0.01834062
Iteration 104, loss = 0.01803761
Iteration 105, loss = 0.01777663
Iteration 106, loss = 0.01750967
Iteration 107, loss = 0.01724181
Iteration 108, loss = 0.01697773
Iteration 109, loss = 0.01673421
Iteration 110, loss = 0.01649711
Iteration 111, loss = 0.01624548
Iteration 112, loss = 0.01604333
Iteration 113, loss = 0.01579214
Iteration 114, loss = 0.01558917
Iteration 115, loss = 0.01536365
Iteration 116, loss = 0.01515725
Iteration 117, loss = 0.01495979
Iteration 118, loss = 0.01475473
Iteration 119, loss = 0.01458158
Iteration 120, loss = 0.01446594
Iteration 121, loss = 0.01425329
Iteration 122, loss = 0.01405420
Iteration 123, loss = 0.01389291
Iteration 124, loss = 0.01372085
Iteration 125, loss = 0.01359048
Iteration 126, loss = 0.01341916
Iteration 127, loss = 0.01326818
Iteration 128, loss = 0.01309204
Iteration 129, loss = 0.01292179
Iteration 130, loss = 0.01277691
Iteration 131, loss = 0.01263183
Iteration 132, loss = 0.01250981
Iteration 133, loss = 0.01236032
Iteration 134, loss = 0.01224881
Iteration 135, loss = 0.01210298
Iteration 136, loss = 0.01198308
Iteration 137, loss = 0.01185473
Iteration 138, loss = 0.01172445
Iteration 139, loss = 0.01160564
Iteration 140, loss = 0.01148650
Iteration 141, loss = 0.01138504
Iteration 142, loss = 0.01128381
Iteration 143, loss = 0.01114748
Iteration 144, loss = 0.01105109
Iteration 145, loss = 0.01095431
Iteration 146, loss = 0.01085603
Iteration 147, loss = 0.01076748
Iteration 148, loss = 0.01066674
Iteration 149, loss = 0.01058417
Iteration 150, loss = 0.01049611
Iteration 151, loss = 0.01041374
Iteration 152, loss = 0.01032470
Iteration 153, loss = 0.01022849
Iteration 154, loss = 0.01014662
Iteration 155, loss = 0.01007038
Iteration 156, loss = 0.00996621
Iteration 157, loss = 0.00990010
Iteration 158, loss = 0.00980277
Iteration 159, loss = 0.00972456
Iteration 160, loss = 0.00964653
Iteration 161, loss = 0.00957113
Iteration 162, loss = 0.00951080
Iteration 163, loss = 0.00943351
Iteration 164, loss = 0.00938292
Iteration 165, loss = 0.00928067
Iteration 166, loss = 0.00921443
Iteration 167, loss = 0.00916291
Iteration 168, loss = 0.00909656
Iteration 169, loss = 0.00904028
Iteration 170, loss = 0.00895302
Iteration 171, loss = 0.00889596
Iteration 172, loss = 0.00883166
Iteration 173, loss = 0.00877500
Iteration 174, loss = 0.00871563
Iteration 175, loss = 0.00867202
Iteration 176, loss = 0.00859700
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67709931
Iteration 2, loss = 0.65307030
Iteration 3, loss = 0.62462055
Iteration 4, loss = 0.58655362
Iteration 5, loss = 0.54037742
Iteration 6, loss = 0.48965883
Iteration 7, loss = 0.43826965
Iteration 8, loss = 0.38874313
Iteration 9, loss = 0.34212125
Iteration 10, loss = 0.29968804
Iteration 11, loss = 0.26173163
Iteration 12, loss = 0.22836658
Iteration 13, loss = 0.19950976
Iteration 14, loss = 0.17480273
Iteration 15, loss = 0.15361601
Iteration 16, loss = 0.13589220
Iteration 17, loss = 0.12103778
Iteration 18, loss = 0.10828991
Iteration 19, loss = 0.09757849
Iteration 20, loss = 0.08834836
Iteration 21, loss = 0.08038855
Iteration 22, loss = 0.07376817
Iteration 23, loss = 0.06787588
Iteration 24, loss = 0.06267997
Iteration 25, loss = 0.05829189
Iteration 26, loss = 0.05440546
Iteration 27, loss = 0.05095817
Iteration 28, loss = 0.04807814
Iteration 29, loss = 0.04527815
Iteration 30, loss = 0.04293505
Iteration 31, loss = 0.04070136
Iteration 32, loss = 0.03868451
Iteration 33, loss = 0.03678607
Iteration 34, loss = 0.03543241
Iteration 35, loss = 0.03408599
Iteration 36, loss = 0.03278552
Iteration 37, loss = 0.03143010
Iteration 38, loss = 0.03029071
Iteration 39, loss = 0.02924118
Iteration 40, loss = 0.02825885
Iteration 41, loss = 0.02746439
Iteration 42, loss = 0.02666818
Iteration 43, loss = 0.02584834
Iteration 44, loss = 0.02510724
Iteration 45, loss = 0.02438696
Iteration 46, loss = 0.02384178
Iteration 47, loss = 0.02326673
Iteration 48, loss = 0.02269041
Iteration 49, loss = 0.02222833
Iteration 50, loss = 0.02172597
Iteration 51, loss = 0.02126459
Iteration 52, loss = 0.02090594
Iteration 53, loss = 0.02056238
Iteration 54, loss = 0.02014786
Iteration 55, loss = 0.02002831
Iteration 56, loss = 0.01975214
Iteration 57, loss = 0.01931829
Iteration 58, loss = 0.01906047
Iteration 59, loss = 0.01869884
Iteration 60, loss = 0.01850746
Iteration 61, loss = 0.01811443
Iteration 62, loss = 0.01788490
Iteration 63, loss = 0.01768970
Iteration 64, loss = 0.01743559
Iteration 65, loss = 0.01717640
Iteration 66, loss = 0.01704012
Iteration 67, loss = 0.01697678
Iteration 68, loss = 0.01680600
Iteration 69, loss = 0.01663270
Iteration 70, loss = 0.01641811
Iteration 71, loss = 0.01616002
Iteration 72, loss = 0.01616535
Iteration 73, loss = 0.01596829
Iteration 74, loss = 0.01588947
Iteration 75, loss = 0.01569810
Iteration 76, loss = 0.01563368
Iteration 77, loss = 0.01547950
Iteration 78, loss = 0.01528446
Iteration 79, loss = 0.01521338
Iteration 80, loss = 0.01510735
Iteration 81, loss = 0.01496456
Iteration 82, loss = 0.01486356
Iteration 83, loss = 0.01478414
Iteration 84, loss = 0.01473349
Iteration 85, loss = 0.01461497
Iteration 86, loss = 0.01450380
Iteration 87, loss = 0.01493308
Iteration 88, loss = 0.01495659
Iteration 89, loss = 0.01452516
Iteration 90, loss = 0.01458696
Iteration 91, loss = 0.01460691
Iteration 92, loss = 0.01448504
Iteration 93, loss = 0.01429714
Iteration 94, loss = 0.01416748
Iteration 95, loss = 0.01404239
Iteration 96, loss = 0.01390856
Iteration 97, loss = 0.01446058
Iteration 98, loss = 0.01434662
Iteration 99, loss = 0.01419841
Iteration 100, loss = 0.01411247
Iteration 101, loss = 0.01378046
Iteration 102, loss = 0.01361561
Iteration 103, loss = 0.01359229
Iteration 104, loss = 0.01345592
Iteration 105, loss = 0.01330886
Iteration 106, loss = 0.01326705
Iteration 107, loss = 0.01323217
Iteration 108, loss = 0.01320596
Iteration 109, loss = 0.01316439
Iteration 110, loss = 0.01310426
Iteration 111, loss = 0.01309933
Iteration 112, loss = 0.01308183
Iteration 113, loss = 0.01324584
Iteration 114, loss = 0.01347312
Iteration 115, loss = 0.01340490
Iteration 116, loss = 0.01342534
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67592012
Iteration 2, loss = 0.64545273
Iteration 3, loss = 0.61023121
Iteration 4, loss = 0.56681755
Iteration 5, loss = 0.51798915
Iteration 6, loss = 0.46762102
Iteration 7, loss = 0.41767469
Iteration 8, loss = 0.37012849
Iteration 9, loss = 0.32575793
Iteration 10, loss = 0.28565911
Iteration 11, loss = 0.24958192
Iteration 12, loss = 0.21825019
Iteration 13, loss = 0.19112810
Iteration 14, loss = 0.16800026
Iteration 15, loss = 0.14805763
Iteration 16, loss = 0.13139297
Iteration 17, loss = 0.11710602
Iteration 18, loss = 0.10497559
Iteration 19, loss = 0.09448282
Iteration 20, loss = 0.08555333
Iteration 21, loss = 0.07793713
Iteration 22, loss = 0.07135934
Iteration 23, loss = 0.06568780
Iteration 24, loss = 0.06069746
Iteration 25, loss = 0.05631744
Iteration 26, loss = 0.05242743
Iteration 27, loss = 0.04899028
Iteration 28, loss = 0.04615106
Iteration 29, loss = 0.04330230
Iteration 30, loss = 0.04082580
Iteration 31, loss = 0.03859798
Iteration 32, loss = 0.03664062
Iteration 33, loss = 0.03480573
Iteration 34, loss = 0.03318807
Iteration 35, loss = 0.03172949
Iteration 36, loss = 0.03038838
Iteration 37, loss = 0.02917556
Iteration 38, loss = 0.02808300
Iteration 39, loss = 0.02705402
Iteration 40, loss = 0.02613212
Iteration 41, loss = 0.02531610
Iteration 42, loss = 0.02450733
Iteration 43, loss = 0.02376418
Iteration 44, loss = 0.02313321
Iteration 45, loss = 0.02236851
Iteration 46, loss = 0.02181543
Iteration 47, loss = 0.02121567
Iteration 48, loss = 0.02061976
Iteration 49, loss = 0.02012465
Iteration 50, loss = 0.01970037
Iteration 51, loss = 0.01924103
Iteration 52, loss = 0.01884799
Iteration 53, loss = 0.01847209
Iteration 54, loss = 0.01806241
Iteration 55, loss = 0.01773991
Iteration 56, loss = 0.01743551
Iteration 57, loss = 0.01710297
Iteration 58, loss = 0.01679765
Iteration 59, loss = 0.01656500
Iteration 60, loss = 0.01624725
Iteration 61, loss = 0.01604560
Iteration 62, loss = 0.01579049
Iteration 63, loss = 0.01553128
Iteration 64, loss = 0.01537291
Iteration 65, loss = 0.01514393
Iteration 66, loss = 0.01497786
Iteration 67, loss = 0.01492998
Iteration 68, loss = 0.01490595
Iteration 69, loss = 0.01475095
Iteration 70, loss = 0.01449900
Iteration 71, loss = 0.01425325
Iteration 72, loss = 0.01407664
Iteration 73, loss = 0.01391532
Iteration 74, loss = 0.01385090
Iteration 75, loss = 0.01368694
Iteration 76, loss = 0.01354947
Iteration 77, loss = 0.01347015
Iteration 78, loss = 0.01329524
Iteration 79, loss = 0.01318695
Iteration 80, loss = 0.01304418
Iteration 81, loss = 0.01295230
Iteration 82, loss = 0.01283244
Iteration 83, loss = 0.01275793
Iteration 84, loss = 0.01265902
Iteration 85, loss = 0.01255881
Iteration 86, loss = 0.01248301
Iteration 87, loss = 0.01242676
Iteration 88, loss = 0.01245469
Iteration 89, loss = 0.01234706
Iteration 90, loss = 0.01225437
Iteration 91, loss = 0.01222998
Iteration 92, loss = 0.01211708
Iteration 93, loss = 0.01203018
Iteration 94, loss = 0.01185333
Iteration 95, loss = 0.01216439
Iteration 96, loss = 0.01206274
Iteration 97, loss = 0.01203558
Iteration 98, loss = 0.01175013
Iteration 99, loss = 0.01168382
Iteration 100, loss = 0.01168904
Iteration 101, loss = 0.01166409
Iteration 102, loss = 0.01163114
Iteration 103, loss = 0.01148948
Iteration 104, loss = 0.01147495
Iteration 105, loss = 0.01139361
Iteration 106, loss = 0.01132098
Iteration 107, loss = 0.01125288
Iteration 108, loss = 0.01124870
Iteration 109, loss = 0.01124744
Iteration 110, loss = 0.01120155
Iteration 111, loss = 0.01112969
Iteration 112, loss = 0.01101735
Iteration 113, loss = 0.01105736
Iteration 114, loss = 0.01118266
Iteration 115, loss = 0.01107737
Iteration 116, loss = 0.01098515
Iteration 117, loss = 0.01097003
Iteration 118, loss = 0.01092511
Iteration 119, loss = 0.01089343
Iteration 120, loss = 0.01081673
Iteration 121, loss = 0.01076132
Iteration 122, loss = 0.01072847
Iteration 123, loss = 0.01072106
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67214820
Iteration 2, loss = 0.59056743
Iteration 3, loss = 0.47619414
Iteration 4, loss = 0.35867737
Iteration 5, loss = 0.26284801
Iteration 6, loss = 0.19369858
Iteration 7, loss = 0.14631882
Iteration 8, loss = 0.11443401
Iteration 9, loss = 0.09251116
Iteration 10, loss = 0.07715168
Iteration 11, loss = 0.06590214
Iteration 12, loss = 0.05767415
Iteration 13, loss = 0.05138129
Iteration 14, loss = 0.04658029
Iteration 15, loss = 0.04255727
Iteration 16, loss = 0.03944436
Iteration 17, loss = 0.03676929
Iteration 18, loss = 0.03456429
Iteration 19, loss = 0.03293481
Iteration 20, loss = 0.03155022
Iteration 21, loss = 0.03011209
Iteration 22, loss = 0.02896967
Iteration 23, loss = 0.02797075
Iteration 24, loss = 0.02725022
Iteration 25, loss = 0.02652903
Iteration 26, loss = 0.02574212
Iteration 27, loss = 0.02534094
Iteration 28, loss = 0.02464126
Iteration 29, loss = 0.02433942
Iteration 30, loss = 0.02384153
Iteration 31, loss = 0.02362442
Iteration 32, loss = 0.02317132
Iteration 33, loss = 0.02272658
Iteration 34, loss = 0.02250867
Iteration 35, loss = 0.02240271
Iteration 36, loss = 0.02231264
Iteration 37, loss = 0.02202872
Iteration 38, loss = 0.02171294
Iteration 39, loss = 0.02165666
Iteration 40, loss = 0.02138770
Iteration 41, loss = 0.02133363
Iteration 42, loss = 0.02121166
Iteration 43, loss = 0.02092524
Iteration 44, loss = 0.02064441
Iteration 45, loss = 0.02092783
Iteration 46, loss = 0.02080024
Iteration 47, loss = 0.02066752
Iteration 48, loss = 0.02048557
Iteration 49, loss = 0.02035743
Iteration 50, loss = 0.02031295
Iteration 51, loss = 0.02019394
Iteration 52, loss = 0.01992633
Iteration 53, loss = 0.01996583
Iteration 54, loss = 0.02007895
Iteration 55, loss = 0.01985887
Iteration 56, loss = 0.01972943
Iteration 57, loss = 0.02002052
Iteration 58, loss = 0.01971747
Iteration 59, loss = 0.01976551
Iteration 60, loss = 0.01961527
Iteration 61, loss = 0.01959924
Iteration 62, loss = 0.01957309
Iteration 63, loss = 0.01967670
Iteration 64, loss = 0.01975017
Iteration 65, loss = 0.01979092
Iteration 66, loss = 0.01958176
Iteration 67, loss = 0.01931275
Iteration 68, loss = 0.01944989
Iteration 69, loss = 0.01940800
Iteration 70, loss = 0.01953845
Iteration 71, loss = 0.01925174
Iteration 72, loss = 0.01937806
Iteration 73, loss = 0.01926823
Iteration 74, loss = 0.01944858
Iteration 75, loss = 0.01925356
Iteration 76, loss = 0.01926989
Iteration 77, loss = 0.01907512
Iteration 78, loss = 0.01943086
Iteration 79, loss = 0.01902899
Iteration 80, loss = 0.01924738
Iteration 81, loss = 0.01899496
Iteration 82, loss = 0.01908744
Iteration 83, loss = 0.01916392
Iteration 84, loss = 0.01922573
Iteration 85, loss = 0.01895419
Iteration 86, loss = 0.01889489
Iteration 87, loss = 0.01887560
Iteration 88, loss = 0.01920496
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70490058
Iteration 2, loss = 0.67272361
Iteration 3, loss = 0.62829256
Iteration 4, loss = 0.57349424
Iteration 5, loss = 0.51216603
Iteration 6, loss = 0.44897198
Iteration 7, loss = 0.38752954
Iteration 8, loss = 0.33142507
Iteration 9, loss = 0.28138991
Iteration 10, loss = 0.23837537
Iteration 11, loss = 0.20214353
Iteration 12, loss = 0.17215748
Iteration 13, loss = 0.14744544
Iteration 14, loss = 0.12742959
Iteration 15, loss = 0.11088223
Iteration 16, loss = 0.09744927
Iteration 17, loss = 0.08634999
Iteration 18, loss = 0.07710903
Iteration 19, loss = 0.06944359
Iteration 20, loss = 0.06303196
Iteration 21, loss = 0.05764835
Iteration 22, loss = 0.05309338
Iteration 23, loss = 0.04901250
Iteration 24, loss = 0.04567418
Iteration 25, loss = 0.04260020
Iteration 26, loss = 0.04005898
Iteration 27, loss = 0.03765380
Iteration 28, loss = 0.03572647
Iteration 29, loss = 0.03393933
Iteration 30, loss = 0.03228381
Iteration 31, loss = 0.03085058
Iteration 32, loss = 0.02962683
Iteration 33, loss = 0.02837120
Iteration 34, loss = 0.02743908
Iteration 35, loss = 0.02633355
Iteration 36, loss = 0.02558056
Iteration 37, loss = 0.02465999
Iteration 38, loss = 0.02404752
Iteration 39, loss = 0.02336160
Iteration 40, loss = 0.02265653
Iteration 41, loss = 0.02211680
Iteration 42, loss = 0.02160745
Iteration 43, loss = 0.02121243
Iteration 44, loss = 0.02076560
Iteration 45, loss = 0.02024125
Iteration 46, loss = 0.01989372
Iteration 47, loss = 0.01958224
Iteration 48, loss = 0.01926440
Iteration 49, loss = 0.01900485
Iteration 50, loss = 0.01867336
Iteration 51, loss = 0.01843785
Iteration 52, loss = 0.01820427
Iteration 53, loss = 0.01788474
Iteration 54, loss = 0.01764068
Iteration 55, loss = 0.01738722
Iteration 56, loss = 0.01720494
Iteration 57, loss = 0.01698025
Iteration 58, loss = 0.01687277
Iteration 59, loss = 0.01664483
Iteration 60, loss = 0.01653863
Iteration 61, loss = 0.01631703
Iteration 62, loss = 0.01616866
Iteration 63, loss = 0.01605708
Iteration 64, loss = 0.01600789
Iteration 65, loss = 0.01576197
Iteration 66, loss = 0.01568578
Iteration 67, loss = 0.01550928
Iteration 68, loss = 0.01549191
Iteration 69, loss = 0.01547521
Iteration 70, loss = 0.01535827
Iteration 71, loss = 0.01526473
Iteration 72, loss = 0.01503382
Iteration 73, loss = 0.01513352
Iteration 74, loss = 0.01488955
Iteration 75, loss = 0.01498868
Iteration 76, loss = 0.01470310
Iteration 77, loss = 0.01468135
Iteration 78, loss = 0.01461764
Iteration 79, loss = 0.01448283
Iteration 80, loss = 0.01455930
Iteration 81, loss = 0.01444320
Iteration 82, loss = 0.01434884
Iteration 83, loss = 0.01427503
Iteration 84, loss = 0.01417716
Iteration 85, loss = 0.01418320
Iteration 86, loss = 0.01409968
Iteration 87, loss = 0.01409643
Iteration 88, loss = 0.01402966
Iteration 89, loss = 0.01413120
Iteration 90, loss = 0.01393730
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67287206
Iteration 2, loss = 0.59539249
Iteration 3, loss = 0.48307685
Iteration 4, loss = 0.36679965
Iteration 5, loss = 0.26946235
Iteration 6, loss = 0.19766715
Iteration 7, loss = 0.14852471
Iteration 8, loss = 0.11552650
Iteration 9, loss = 0.09297092
Iteration 10, loss = 0.07735034
Iteration 11, loss = 0.06579004
Iteration 12, loss = 0.05749962
Iteration 13, loss = 0.05099672
Iteration 14, loss = 0.04617383
Iteration 15, loss = 0.04220026
Iteration 16, loss = 0.03908556
Iteration 17, loss = 0.03661390
Iteration 18, loss = 0.03444901
Iteration 19, loss = 0.03252270
Iteration 20, loss = 0.03115833
Iteration 21, loss = 0.02986997
Iteration 22, loss = 0.02899590
Iteration 23, loss = 0.02777970
Iteration 24, loss = 0.02712402
Iteration 25, loss = 0.02649099
Iteration 26, loss = 0.02585798
Iteration 27, loss = 0.02532679
Iteration 28, loss = 0.02471041
Iteration 29, loss = 0.02425265
Iteration 30, loss = 0.02397190
Iteration 31, loss = 0.02373617
Iteration 32, loss = 0.02329688
Iteration 33, loss = 0.02303863
Iteration 34, loss = 0.02260179
Iteration 35, loss = 0.02236520
Iteration 36, loss = 0.02227469
Iteration 37, loss = 0.02206565
Iteration 38, loss = 0.02194900
Iteration 39, loss = 0.02161406
Iteration 40, loss = 0.02147410
Iteration 41, loss = 0.02133574
Iteration 42, loss = 0.02117380
Iteration 43, loss = 0.02102057
Iteration 44, loss = 0.02083694
Iteration 45, loss = 0.02089796
Iteration 46, loss = 0.02066284
Iteration 47, loss = 0.02046367
Iteration 48, loss = 0.02045538
Iteration 49, loss = 0.02079619
Iteration 50, loss = 0.02033152
Iteration 51, loss = 0.02047969
Iteration 52, loss = 0.02013209
Iteration 53, loss = 0.01998601
Iteration 54, loss = 0.02036078
Iteration 55, loss = 0.02004690
Iteration 56, loss = 0.01993644
Iteration 57, loss = 0.01996609
Iteration 58, loss = 0.01985126
Iteration 59, loss = 0.02016779
Iteration 60, loss = 0.01999111
Iteration 61, loss = 0.01970867
Iteration 62, loss = 0.01992541
Iteration 63, loss = 0.01963398
Iteration 64, loss = 0.01958859
Iteration 65, loss = 0.01961746
Iteration 66, loss = 0.01959952
Iteration 67, loss = 0.01954266
Iteration 68, loss = 0.01936144
Iteration 69, loss = 0.01934810
Iteration 70, loss = 0.01933492
Iteration 71, loss = 0.01952835
Iteration 72, loss = 0.01937434
Iteration 73, loss = 0.01938923
Iteration 74, loss = 0.01920762
Iteration 75, loss = 0.01932169
Iteration 76, loss = 0.01922675
Iteration 77, loss = 0.01916564
Iteration 78, loss = 0.01929934
Iteration 79, loss = 0.01902910
Iteration 80, loss = 0.01928573
Iteration 81, loss = 0.01921611
Iteration 82, loss = 0.01904887
Iteration 83, loss = 0.01913956
Iteration 84, loss = 0.01894465
Iteration 85, loss = 0.01902983
Iteration 86, loss = 0.01909113
Iteration 87, loss = 0.01892626
Iteration 88, loss = 0.01895783
Iteration 89, loss = 0.01889223
Iteration 90, loss = 0.01876066
Iteration 91, loss = 0.01882865
Iteration 92, loss = 0.01928451
Iteration 93, loss = 0.01867264
Iteration 94, loss = 0.01894562
Iteration 95, loss = 0.01893116
Iteration 96, loss = 0.01931033
Iteration 97, loss = 0.01865187
Iteration 98, loss = 0.01896378
Iteration 99, loss = 0.01874952
Iteration 100, loss = 0.01855032
Iteration 101, loss = 0.01880186
Iteration 102, loss = 0.01850448
Iteration 103, loss = 0.01898819
Iteration 104, loss = 0.01900859
Iteration 105, loss = 0.01885647
Iteration 106, loss = 0.01883647
Iteration 107, loss = 0.01861938
Iteration 108, loss = 0.01897786
Iteration 109, loss = 0.01883905
Iteration 110, loss = 0.01897214
Iteration 111, loss = 0.01871459
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	92m44.052s
user	392m39.350s
sys	42m51.027s
Elapsed time: 5564 seconds
