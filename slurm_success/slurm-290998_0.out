Processing dataset: dropped_v1_basic_clean

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: dropped_v1_basic_clean (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on dropped_v1_basic_clean...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on dropped_v1_basic_clean...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on dropped_v1_basic_clean...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on dropped_v1_basic_clean...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on dropped_v1_basic_clean...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.71594381
Iteration 2, loss = 0.65804528
Iteration 3, loss = 0.56349327
Iteration 4, loss = 0.44965125
Iteration 5, loss = 0.34649277
Iteration 6, loss = 0.26416693
Iteration 7, loss = 0.20268439
Iteration 8, loss = 0.15835870
Iteration 9, loss = 0.12686685
Iteration 10, loss = 0.10462272
Iteration 11, loss = 0.08809390
Iteration 12, loss = 0.07605914
Iteration 13, loss = 0.06695679
Iteration 14, loss = 0.05949104
Iteration 15, loss = 0.05381289
Iteration 16, loss = 0.04914583
Iteration 17, loss = 0.04568408
Iteration 18, loss = 0.04257122
Iteration 19, loss = 0.03999705
Iteration 20, loss = 0.03798586
Iteration 21, loss = 0.03597765
Iteration 22, loss = 0.03467556
Iteration 23, loss = 0.03304063
Iteration 24, loss = 0.03200452
Iteration 25, loss = 0.03101697
Iteration 26, loss = 0.03018446
Iteration 27, loss = 0.02950688
Iteration 28, loss = 0.02844427
Iteration 29, loss = 0.02799129
Iteration 30, loss = 0.02731031
Iteration 31, loss = 0.02713165
Iteration 32, loss = 0.02632970
Iteration 33, loss = 0.02601127
Iteration 34, loss = 0.02572395
Iteration 35, loss = 0.02517198
Iteration 36, loss = 0.02501235
Iteration 37, loss = 0.02488824
Iteration 38, loss = 0.02444803
Iteration 39, loss = 0.02417403
Iteration 40, loss = 0.02404112
Iteration 41, loss = 0.02404153
Iteration 42, loss = 0.02354013
Iteration 43, loss = 0.02347111
Iteration 44, loss = 0.02339445
Iteration 45, loss = 0.02338666
Iteration 46, loss = 0.02319626
Iteration 47, loss = 0.02301161
Iteration 48, loss = 0.02303631
Iteration 49, loss = 0.02266356
Iteration 50, loss = 0.02281470
Iteration 51, loss = 0.02255959
Iteration 52, loss = 0.02246823
Iteration 53, loss = 0.02222821
Iteration 54, loss = 0.02236010
Iteration 55, loss = 0.02216865
Iteration 56, loss = 0.02218824
Iteration 57, loss = 0.02203059
Iteration 58, loss = 0.02235357
Iteration 59, loss = 0.02202976
Iteration 60, loss = 0.02207741
Iteration 61, loss = 0.02171542
Iteration 62, loss = 0.02170270
Iteration 63, loss = 0.02163420
Iteration 64, loss = 0.02191197
Iteration 65, loss = 0.02175603
Iteration 66, loss = 0.02161897
Iteration 67, loss = 0.02161442
Iteration 68, loss = 0.02151186
Iteration 69, loss = 0.02128626
Iteration 70, loss = 0.02158561
Iteration 71, loss = 0.02158335
Iteration 72, loss = 0.02144003
Iteration 73, loss = 0.02133628
Iteration 74, loss = 0.02135232
Iteration 75, loss = 0.02132815
Iteration 76, loss = 0.02135489
Iteration 77, loss = 0.02117281
Iteration 78, loss = 0.02101100
Iteration 79, loss = 0.02116508
Iteration 80, loss = 0.02124700
Iteration 81, loss = 0.02146478
Iteration 82, loss = 0.02134902
Iteration 83, loss = 0.02119813
Iteration 84, loss = 0.02108538
Iteration 85, loss = 0.02114303
Iteration 86, loss = 0.02102935
Iteration 87, loss = 0.02097113
Iteration 88, loss = 0.02080812
Iteration 89, loss = 0.02086540
Iteration 90, loss = 0.02096851
Iteration 91, loss = 0.02108952
Iteration 92, loss = 0.02081228
Iteration 93, loss = 0.02104623
Iteration 94, loss = 0.02109574
Iteration 95, loss = 0.02086443
Iteration 96, loss = 0.02059921
Iteration 97, loss = 0.02087870
Iteration 98, loss = 0.02092223
Iteration 99, loss = 0.02084849
Iteration 100, loss = 0.02074236
Iteration 101, loss = 0.02074968
Iteration 102, loss = 0.02091411
Iteration 103, loss = 0.02097885
Iteration 104, loss = 0.02073182
Iteration 105, loss = 0.02076859
Iteration 106, loss = 0.02063423
Iteration 107, loss = 0.02063300
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67705519
Iteration 2, loss = 0.60587953
Iteration 3, loss = 0.49703542
Iteration 4, loss = 0.38242763
Iteration 5, loss = 0.28566823
Iteration 6, loss = 0.21278244
Iteration 7, loss = 0.16190506
Iteration 8, loss = 0.12720691
Iteration 9, loss = 0.10306611
Iteration 10, loss = 0.08632105
Iteration 11, loss = 0.07407780
Iteration 12, loss = 0.06498270
Iteration 13, loss = 0.05806905
Iteration 14, loss = 0.05265726
Iteration 15, loss = 0.04864060
Iteration 16, loss = 0.04523859
Iteration 17, loss = 0.04234902
Iteration 18, loss = 0.04009698
Iteration 19, loss = 0.03815043
Iteration 20, loss = 0.03626105
Iteration 21, loss = 0.03519958
Iteration 22, loss = 0.03409293
Iteration 23, loss = 0.03280665
Iteration 24, loss = 0.03207648
Iteration 25, loss = 0.03117390
Iteration 26, loss = 0.03056442
Iteration 27, loss = 0.02982668
Iteration 28, loss = 0.02952043
Iteration 29, loss = 0.02908141
Iteration 30, loss = 0.02840729
Iteration 31, loss = 0.02777614
Iteration 32, loss = 0.02772023
Iteration 33, loss = 0.02731741
Iteration 34, loss = 0.02750972
Iteration 35, loss = 0.02673950
Iteration 36, loss = 0.02643744
Iteration 37, loss = 0.02662801
Iteration 38, loss = 0.02622322
Iteration 39, loss = 0.02591269
Iteration 40, loss = 0.02586809
Iteration 41, loss = 0.02560205
Iteration 42, loss = 0.02556235
Iteration 43, loss = 0.02517265
Iteration 44, loss = 0.02528793
Iteration 45, loss = 0.02521561
Iteration 46, loss = 0.02494816
Iteration 47, loss = 0.02475307
Iteration 48, loss = 0.02492749
Iteration 49, loss = 0.02477339
Iteration 50, loss = 0.02458798
Iteration 51, loss = 0.02473257
Iteration 52, loss = 0.02444347
Iteration 53, loss = 0.02462507
Iteration 54, loss = 0.02467792
Iteration 55, loss = 0.02440626
Iteration 56, loss = 0.02409007
Iteration 57, loss = 0.02434950
Iteration 58, loss = 0.02402099
Iteration 59, loss = 0.02395781
Iteration 60, loss = 0.02395378
Iteration 61, loss = 0.02416257
Iteration 62, loss = 0.02400457
Iteration 63, loss = 0.02384794
Iteration 64, loss = 0.02399090
Iteration 65, loss = 0.02372655
Iteration 66, loss = 0.02393145
Iteration 67, loss = 0.02372594
Iteration 68, loss = 0.02365084
Iteration 69, loss = 0.02359031
Iteration 70, loss = 0.02372282
Iteration 71, loss = 0.02363361
Iteration 72, loss = 0.02384148
Iteration 73, loss = 0.02364335
Iteration 74, loss = 0.02344628
Iteration 75, loss = 0.02362067
Iteration 76, loss = 0.02365410
Iteration 77, loss = 0.02347632
Iteration 78, loss = 0.02328989
Iteration 79, loss = 0.02342824
Iteration 80, loss = 0.02351654
Iteration 81, loss = 0.02361221
Iteration 82, loss = 0.02362104
Iteration 83, loss = 0.02318055
Iteration 84, loss = 0.02339435
Iteration 85, loss = 0.02378057
Iteration 86, loss = 0.02297571
Iteration 87, loss = 0.02334269
Iteration 88, loss = 0.02364284
Iteration 89, loss = 0.02331598
Iteration 90, loss = 0.02314288
Iteration 91, loss = 0.02297257
Iteration 92, loss = 0.02358924
Iteration 93, loss = 0.02326220
Iteration 94, loss = 0.02342044
Iteration 95, loss = 0.02305848
Iteration 96, loss = 0.02343083
Iteration 97, loss = 0.02306967
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68907837
Iteration 2, loss = 0.61257710
Iteration 3, loss = 0.50030427
Iteration 4, loss = 0.38486673
Iteration 5, loss = 0.28632486
Iteration 6, loss = 0.21204168
Iteration 7, loss = 0.15995103
Iteration 8, loss = 0.12463863
Iteration 9, loss = 0.10046011
Iteration 10, loss = 0.08318117
Iteration 11, loss = 0.07102041
Iteration 12, loss = 0.06176998
Iteration 13, loss = 0.05494790
Iteration 14, loss = 0.04953166
Iteration 15, loss = 0.04523622
Iteration 16, loss = 0.04167285
Iteration 17, loss = 0.03870800
Iteration 18, loss = 0.03646458
Iteration 19, loss = 0.03461647
Iteration 20, loss = 0.03279985
Iteration 21, loss = 0.03150687
Iteration 22, loss = 0.03014737
Iteration 23, loss = 0.02902102
Iteration 24, loss = 0.02826709
Iteration 25, loss = 0.02750791
Iteration 26, loss = 0.02675310
Iteration 27, loss = 0.02602940
Iteration 28, loss = 0.02577614
Iteration 29, loss = 0.02505017
Iteration 30, loss = 0.02462018
Iteration 31, loss = 0.02422263
Iteration 32, loss = 0.02375598
Iteration 33, loss = 0.02333931
Iteration 34, loss = 0.02308435
Iteration 35, loss = 0.02288810
Iteration 36, loss = 0.02257112
Iteration 37, loss = 0.02232510
Iteration 38, loss = 0.02216179
Iteration 39, loss = 0.02186038
Iteration 40, loss = 0.02172007
Iteration 41, loss = 0.02167479
Iteration 42, loss = 0.02147794
Iteration 43, loss = 0.02131476
Iteration 44, loss = 0.02109311
Iteration 45, loss = 0.02111796
Iteration 46, loss = 0.02092864
Iteration 47, loss = 0.02081756
Iteration 48, loss = 0.02091670
Iteration 49, loss = 0.02045392
Iteration 50, loss = 0.02096666
Iteration 51, loss = 0.02046418
Iteration 52, loss = 0.02063331
Iteration 53, loss = 0.02046198
Iteration 54, loss = 0.02031027
Iteration 55, loss = 0.02015699
Iteration 56, loss = 0.02019650
Iteration 57, loss = 0.02021420
Iteration 58, loss = 0.01989398
Iteration 59, loss = 0.02021209
Iteration 60, loss = 0.01969014
Iteration 61, loss = 0.01992149
Iteration 62, loss = 0.01972395
Iteration 63, loss = 0.01971574
Iteration 64, loss = 0.01966274
Iteration 65, loss = 0.01977511
Iteration 66, loss = 0.01964611
Iteration 67, loss = 0.01982262
Iteration 68, loss = 0.01960886
Iteration 69, loss = 0.01936652
Iteration 70, loss = 0.01943645
Iteration 71, loss = 0.01920469
Iteration 72, loss = 0.01976975
Iteration 73, loss = 0.01948847
Iteration 74, loss = 0.01943097
Iteration 75, loss = 0.01941466
Iteration 76, loss = 0.01927779
Iteration 77, loss = 0.01930643
Iteration 78, loss = 0.01930697
Iteration 79, loss = 0.01931367
Iteration 80, loss = 0.01919493
Iteration 81, loss = 0.01919229
Iteration 82, loss = 0.01943582
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68903920
Iteration 2, loss = 0.60466235
Iteration 3, loss = 0.49113455
Iteration 4, loss = 0.37445522
Iteration 5, loss = 0.27807912
Iteration 6, loss = 0.20687897
Iteration 7, loss = 0.15710121
Iteration 8, loss = 0.12303284
Iteration 9, loss = 0.09945667
Iteration 10, loss = 0.08263686
Iteration 11, loss = 0.07037989
Iteration 12, loss = 0.06160695
Iteration 13, loss = 0.05447490
Iteration 14, loss = 0.04911785
Iteration 15, loss = 0.04477736
Iteration 16, loss = 0.04148041
Iteration 17, loss = 0.03882516
Iteration 18, loss = 0.03621395
Iteration 19, loss = 0.03433566
Iteration 20, loss = 0.03264852
Iteration 21, loss = 0.03106438
Iteration 22, loss = 0.02998908
Iteration 23, loss = 0.02893728
Iteration 24, loss = 0.02818261
Iteration 25, loss = 0.02717418
Iteration 26, loss = 0.02655789
Iteration 27, loss = 0.02591751
Iteration 28, loss = 0.02536064
Iteration 29, loss = 0.02495685
Iteration 30, loss = 0.02439100
Iteration 31, loss = 0.02390044
Iteration 32, loss = 0.02342920
Iteration 33, loss = 0.02312814
Iteration 34, loss = 0.02292731
Iteration 35, loss = 0.02273029
Iteration 36, loss = 0.02243381
Iteration 37, loss = 0.02238346
Iteration 38, loss = 0.02198596
Iteration 39, loss = 0.02188684
Iteration 40, loss = 0.02155051
Iteration 41, loss = 0.02143266
Iteration 42, loss = 0.02135708
Iteration 43, loss = 0.02125122
Iteration 44, loss = 0.02075056
Iteration 45, loss = 0.02098017
Iteration 46, loss = 0.02065993
Iteration 47, loss = 0.02081539
Iteration 48, loss = 0.02050478
Iteration 49, loss = 0.02026072
Iteration 50, loss = 0.02014897
Iteration 51, loss = 0.02029674
Iteration 52, loss = 0.02037027
Iteration 53, loss = 0.02002349
Iteration 54, loss = 0.02015812
Iteration 55, loss = 0.02002698
Iteration 56, loss = 0.01998469
Iteration 57, loss = 0.01989085
Iteration 58, loss = 0.01991625
Iteration 59, loss = 0.01976587
Iteration 60, loss = 0.01966191
Iteration 61, loss = 0.01962751
Iteration 62, loss = 0.01968324
Iteration 63, loss = 0.01953373
Iteration 64, loss = 0.01957658
Iteration 65, loss = 0.01936968
Iteration 66, loss = 0.01936878
Iteration 67, loss = 0.01939798
Iteration 68, loss = 0.01957307
Iteration 69, loss = 0.01959258
Iteration 70, loss = 0.01936762
Iteration 71, loss = 0.01937959
Iteration 72, loss = 0.01924958
Iteration 73, loss = 0.01930706
Iteration 74, loss = 0.01929339
Iteration 75, loss = 0.01915493
Iteration 76, loss = 0.01893840
Iteration 77, loss = 0.01912549
Iteration 78, loss = 0.01917999
Iteration 79, loss = 0.01879366
Iteration 80, loss = 0.01925784
Iteration 81, loss = 0.01887195
Iteration 82, loss = 0.01902777
Iteration 83, loss = 0.01876543
Iteration 84, loss = 0.01890355
Iteration 85, loss = 0.01905445
Iteration 86, loss = 0.01895714
Iteration 87, loss = 0.01893909
Iteration 88, loss = 0.01893360
Iteration 89, loss = 0.01861200
Iteration 90, loss = 0.01901067
Iteration 91, loss = 0.01874750
Iteration 92, loss = 0.01912099
Iteration 93, loss = 0.01877576
Iteration 94, loss = 0.01874611
Iteration 95, loss = 0.01899538
Iteration 96, loss = 0.01847764
Iteration 97, loss = 0.01871149
Iteration 98, loss = 0.01896897
Iteration 99, loss = 0.01852036
Iteration 100, loss = 0.01866606
Iteration 101, loss = 0.01856534
Iteration 102, loss = 0.01859659
Iteration 103, loss = 0.01887997
Iteration 104, loss = 0.01869491
Iteration 105, loss = 0.01880960
Iteration 106, loss = 0.01864513
Iteration 107, loss = 0.01847766
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67813856
Iteration 2, loss = 0.60381997
Iteration 3, loss = 0.48548868
Iteration 4, loss = 0.36631602
Iteration 5, loss = 0.27028090
Iteration 6, loss = 0.20050616
Iteration 7, loss = 0.15254715
Iteration 8, loss = 0.11999766
Iteration 9, loss = 0.09750309
Iteration 10, loss = 0.08139691
Iteration 11, loss = 0.06999990
Iteration 12, loss = 0.06115072
Iteration 13, loss = 0.05460896
Iteration 14, loss = 0.04918401
Iteration 15, loss = 0.04506271
Iteration 16, loss = 0.04174741
Iteration 17, loss = 0.03903284
Iteration 18, loss = 0.03671263
Iteration 19, loss = 0.03464150
Iteration 20, loss = 0.03296005
Iteration 21, loss = 0.03147332
Iteration 22, loss = 0.03027462
Iteration 23, loss = 0.02925642
Iteration 24, loss = 0.02832279
Iteration 25, loss = 0.02758091
Iteration 26, loss = 0.02687110
Iteration 27, loss = 0.02614301
Iteration 28, loss = 0.02581292
Iteration 29, loss = 0.02512875
Iteration 30, loss = 0.02475579
Iteration 31, loss = 0.02426321
Iteration 32, loss = 0.02382505
Iteration 33, loss = 0.02355420
Iteration 34, loss = 0.02318477
Iteration 35, loss = 0.02338284
Iteration 36, loss = 0.02288933
Iteration 37, loss = 0.02273403
Iteration 38, loss = 0.02227536
Iteration 39, loss = 0.02199569
Iteration 40, loss = 0.02193855
Iteration 41, loss = 0.02174039
Iteration 42, loss = 0.02169615
Iteration 43, loss = 0.02145320
Iteration 44, loss = 0.02137516
Iteration 45, loss = 0.02133459
Iteration 46, loss = 0.02115371
Iteration 47, loss = 0.02112001
Iteration 48, loss = 0.02096952
Iteration 49, loss = 0.02084761
Iteration 50, loss = 0.02088559
Iteration 51, loss = 0.02058396
Iteration 52, loss = 0.02045561
Iteration 53, loss = 0.02056006
Iteration 54, loss = 0.02049055
Iteration 55, loss = 0.02032776
Iteration 56, loss = 0.02026150
Iteration 57, loss = 0.02006317
Iteration 58, loss = 0.02032760
Iteration 59, loss = 0.02019352
Iteration 60, loss = 0.02025685
Iteration 61, loss = 0.02019290
Iteration 62, loss = 0.02004242
Iteration 63, loss = 0.02014285
Iteration 64, loss = 0.01990797
Iteration 65, loss = 0.01963289
Iteration 66, loss = 0.01999736
Iteration 67, loss = 0.02006169
Iteration 68, loss = 0.01990628
Iteration 69, loss = 0.01974567
Iteration 70, loss = 0.01981781
Iteration 71, loss = 0.01966005
Iteration 72, loss = 0.01961106
Iteration 73, loss = 0.01959424
Iteration 74, loss = 0.01953737
Iteration 75, loss = 0.01957583
Iteration 76, loss = 0.01955646
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71594381
Iteration 2, loss = 0.65804528
Iteration 3, loss = 0.56349327
Iteration 4, loss = 0.44965125
Iteration 5, loss = 0.34649277
Iteration 6, loss = 0.26416693
Iteration 7, loss = 0.20268439
Iteration 8, loss = 0.15835870
Iteration 9, loss = 0.12686685
Iteration 10, loss = 0.10462272
Iteration 11, loss = 0.08809390
Iteration 12, loss = 0.07605914
Iteration 13, loss = 0.06695679
Iteration 14, loss = 0.05949104
Iteration 15, loss = 0.05381289
Iteration 16, loss = 0.04914583
Iteration 17, loss = 0.04568408
Iteration 18, loss = 0.04257122
Iteration 19, loss = 0.03999705
Iteration 20, loss = 0.03798586
Iteration 21, loss = 0.03597765
Iteration 22, loss = 0.03467556
Iteration 23, loss = 0.03304063
Iteration 24, loss = 0.03200452
Iteration 25, loss = 0.03101697
Iteration 26, loss = 0.03018446
Iteration 27, loss = 0.02950688
Iteration 28, loss = 0.02844427
Iteration 29, loss = 0.02799129
Iteration 30, loss = 0.02731031
Iteration 31, loss = 0.02713165
Iteration 32, loss = 0.02632970
Iteration 33, loss = 0.02601127
Iteration 34, loss = 0.02572395
Iteration 35, loss = 0.02517198
Iteration 36, loss = 0.02501235
Iteration 37, loss = 0.02488824
Iteration 38, loss = 0.02444803
Iteration 39, loss = 0.02417403
Iteration 40, loss = 0.02404112
Iteration 41, loss = 0.02404153
Iteration 42, loss = 0.02354013
Iteration 43, loss = 0.02347111
Iteration 44, loss = 0.02339445
Iteration 45, loss = 0.02338666
Iteration 46, loss = 0.02319626
Iteration 47, loss = 0.02301161
Iteration 48, loss = 0.02303631
Iteration 49, loss = 0.02266356
Iteration 50, loss = 0.02281470
Iteration 51, loss = 0.02255959
Iteration 52, loss = 0.02246823
Iteration 53, loss = 0.02222821
Iteration 54, loss = 0.02236010
Iteration 55, loss = 0.02216865
Iteration 56, loss = 0.02218824
Iteration 57, loss = 0.02203059
Iteration 58, loss = 0.02235357
Iteration 59, loss = 0.02202976
Iteration 60, loss = 0.02207741
Iteration 61, loss = 0.02171542
Iteration 62, loss = 0.02170270
Iteration 63, loss = 0.02163420
Iteration 64, loss = 0.02191197
Iteration 65, loss = 0.02175603
Iteration 66, loss = 0.02161897
Iteration 67, loss = 0.02161442
Iteration 68, loss = 0.02151186
Iteration 69, loss = 0.02128626
Iteration 70, loss = 0.02158561
Iteration 71, loss = 0.02158335
Iteration 72, loss = 0.02144003
Iteration 73, loss = 0.02133628
Iteration 74, loss = 0.02135232
Iteration 75, loss = 0.02132815
Iteration 76, loss = 0.02135489
Iteration 77, loss = 0.02117281
Iteration 78, loss = 0.02101100
Iteration 79, loss = 0.02116508
Iteration 80, loss = 0.02124700
Iteration 81, loss = 0.02146478
Iteration 82, loss = 0.02134902
Iteration 83, loss = 0.02119813
Iteration 84, loss = 0.02108538
Iteration 85, loss = 0.02114303
Iteration 86, loss = 0.02102935
Iteration 87, loss = 0.02097113
Iteration 88, loss = 0.02080812
Iteration 89, loss = 0.02086540
Iteration 90, loss = 0.02096851
Iteration 91, loss = 0.02108952
Iteration 92, loss = 0.02081228
Iteration 93, loss = 0.02104623
Iteration 94, loss = 0.02109574
Iteration 95, loss = 0.02086443
Iteration 96, loss = 0.02059921
Iteration 97, loss = 0.02087870
Iteration 98, loss = 0.02092223
Iteration 99, loss = 0.02084849
Iteration 100, loss = 0.02074236
Iteration 101, loss = 0.02074968
Iteration 102, loss = 0.02091411
Iteration 103, loss = 0.02097885
Iteration 104, loss = 0.02073182
Iteration 105, loss = 0.02076859
Iteration 106, loss = 0.02063423
Iteration 107, loss = 0.02063300
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67705519
Iteration 2, loss = 0.60587953
Iteration 3, loss = 0.49703542
Iteration 4, loss = 0.38242763
Iteration 5, loss = 0.28566823
Iteration 6, loss = 0.21278244
Iteration 7, loss = 0.16190506
Iteration 8, loss = 0.12720691
Iteration 9, loss = 0.10306611
Iteration 10, loss = 0.08632105
Iteration 11, loss = 0.07407780
Iteration 12, loss = 0.06498270
Iteration 13, loss = 0.05806905
Iteration 14, loss = 0.05265726
Iteration 15, loss = 0.04864060
Iteration 16, loss = 0.04523859
Iteration 17, loss = 0.04234902
Iteration 18, loss = 0.04009698
Iteration 19, loss = 0.03815043
Iteration 20, loss = 0.03626105
Iteration 21, loss = 0.03519958
Iteration 22, loss = 0.03409293
Iteration 23, loss = 0.03280665
Iteration 24, loss = 0.03207648
Iteration 25, loss = 0.03117390
Iteration 26, loss = 0.03056442
Iteration 27, loss = 0.02982668
Iteration 28, loss = 0.02952043
Iteration 29, loss = 0.02908141
Iteration 30, loss = 0.02840729
Iteration 31, loss = 0.02777614
Iteration 32, loss = 0.02772023
Iteration 33, loss = 0.02731741
Iteration 34, loss = 0.02750972
Iteration 35, loss = 0.02673950
Iteration 36, loss = 0.02643744
Iteration 37, loss = 0.02662801
Iteration 38, loss = 0.02622322
Iteration 39, loss = 0.02591269
Iteration 40, loss = 0.02586809
Iteration 41, loss = 0.02560205
Iteration 42, loss = 0.02556235
Iteration 43, loss = 0.02517265
Iteration 44, loss = 0.02528793
Iteration 45, loss = 0.02521561
Iteration 46, loss = 0.02494816
Iteration 47, loss = 0.02475307
Iteration 48, loss = 0.02492749
Iteration 49, loss = 0.02477339
Iteration 50, loss = 0.02458798
Iteration 51, loss = 0.02473257
Iteration 52, loss = 0.02444347
Iteration 53, loss = 0.02462507
Iteration 54, loss = 0.02467792
Iteration 55, loss = 0.02440626
Iteration 56, loss = 0.02409007
Iteration 57, loss = 0.02434950
Iteration 58, loss = 0.02402099
Iteration 59, loss = 0.02395781
Iteration 60, loss = 0.02395378
Iteration 61, loss = 0.02416257
Iteration 62, loss = 0.02400457
Iteration 63, loss = 0.02384794
Iteration 64, loss = 0.02399090
Iteration 65, loss = 0.02372655
Iteration 66, loss = 0.02393145
Iteration 67, loss = 0.02372594
Iteration 68, loss = 0.02365084
Iteration 69, loss = 0.02359031
Iteration 70, loss = 0.02372282
Iteration 71, loss = 0.02363361
Iteration 72, loss = 0.02384148
Iteration 73, loss = 0.02364335
Iteration 74, loss = 0.02344628
Iteration 75, loss = 0.02362067
Iteration 76, loss = 0.02365410
Iteration 77, loss = 0.02347632
Iteration 78, loss = 0.02328989
Iteration 79, loss = 0.02342824
Iteration 80, loss = 0.02351654
Iteration 81, loss = 0.02361221
Iteration 82, loss = 0.02362104
Iteration 83, loss = 0.02318055
Iteration 84, loss = 0.02339435
Iteration 85, loss = 0.02378057
Iteration 86, loss = 0.02297571
Iteration 87, loss = 0.02334269
Iteration 88, loss = 0.02364284
Iteration 89, loss = 0.02331598
Iteration 90, loss = 0.02314288
Iteration 91, loss = 0.02297257
Iteration 92, loss = 0.02358924
Iteration 93, loss = 0.02326220
Iteration 94, loss = 0.02342044
Iteration 95, loss = 0.02305848
Iteration 96, loss = 0.02343083
Iteration 97, loss = 0.02306967
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68907837
Iteration 2, loss = 0.61257710
Iteration 3, loss = 0.50030427
Iteration 4, loss = 0.38486673
Iteration 5, loss = 0.28632486
Iteration 6, loss = 0.21204168
Iteration 7, loss = 0.15995103
Iteration 8, loss = 0.12463863
Iteration 9, loss = 0.10046011
Iteration 10, loss = 0.08318117
Iteration 11, loss = 0.07102041
Iteration 12, loss = 0.06176998
Iteration 13, loss = 0.05494790
Iteration 14, loss = 0.04953166
Iteration 15, loss = 0.04523622
Iteration 16, loss = 0.04167285
Iteration 17, loss = 0.03870800
Iteration 18, loss = 0.03646458
Iteration 19, loss = 0.03461647
Iteration 20, loss = 0.03279985
Iteration 21, loss = 0.03150687
Iteration 22, loss = 0.03014737
Iteration 23, loss = 0.02902102
Iteration 24, loss = 0.02826709
Iteration 25, loss = 0.02750791
Iteration 26, loss = 0.02675310
Iteration 27, loss = 0.02602940
Iteration 28, loss = 0.02577614
Iteration 29, loss = 0.02505017
Iteration 30, loss = 0.02462018
Iteration 31, loss = 0.02422263
Iteration 32, loss = 0.02375598
Iteration 33, loss = 0.02333931
Iteration 34, loss = 0.02308435
Iteration 35, loss = 0.02288810
Iteration 36, loss = 0.02257112
Iteration 37, loss = 0.02232510
Iteration 38, loss = 0.02216179
Iteration 39, loss = 0.02186038
Iteration 40, loss = 0.02172007
Iteration 41, loss = 0.02167479
Iteration 42, loss = 0.02147794
Iteration 43, loss = 0.02131476
Iteration 44, loss = 0.02109311
Iteration 45, loss = 0.02111796
Iteration 46, loss = 0.02092864
Iteration 47, loss = 0.02081756
Iteration 48, loss = 0.02091670
Iteration 49, loss = 0.02045392
Iteration 50, loss = 0.02096666
Iteration 51, loss = 0.02046418
Iteration 52, loss = 0.02063331
Iteration 53, loss = 0.02046198
Iteration 54, loss = 0.02031027
Iteration 55, loss = 0.02015699
Iteration 56, loss = 0.02019650
Iteration 57, loss = 0.02021420
Iteration 58, loss = 0.01989398
Iteration 59, loss = 0.02021209
Iteration 60, loss = 0.01969014
Iteration 61, loss = 0.01992149
Iteration 62, loss = 0.01972395
Iteration 63, loss = 0.01971574
Iteration 64, loss = 0.01966274
Iteration 65, loss = 0.01977511
Iteration 66, loss = 0.01964611
Iteration 67, loss = 0.01982262
Iteration 68, loss = 0.01960886
Iteration 69, loss = 0.01936652
Iteration 70, loss = 0.01943645
Iteration 71, loss = 0.01920469
Iteration 72, loss = 0.01976975
Iteration 73, loss = 0.01948847
Iteration 74, loss = 0.01943097
Iteration 75, loss = 0.01941466
Iteration 76, loss = 0.01927779
Iteration 77, loss = 0.01930643
Iteration 78, loss = 0.01930697
Iteration 79, loss = 0.01931367
Iteration 80, loss = 0.01919493
Iteration 81, loss = 0.01919229
Iteration 82, loss = 0.01943582
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68903920
Iteration 2, loss = 0.60466235
Iteration 3, loss = 0.49113455
Iteration 4, loss = 0.37445522
Iteration 5, loss = 0.27807912
Iteration 6, loss = 0.20687897
Iteration 7, loss = 0.15710121
Iteration 8, loss = 0.12303284
Iteration 9, loss = 0.09945667
Iteration 10, loss = 0.08263686
Iteration 11, loss = 0.07037989
Iteration 12, loss = 0.06160695
Iteration 13, loss = 0.05447490
Iteration 14, loss = 0.04911785
Iteration 15, loss = 0.04477736
Iteration 16, loss = 0.04148041
Iteration 17, loss = 0.03882516
Iteration 18, loss = 0.03621395
Iteration 19, loss = 0.03433566
Iteration 20, loss = 0.03264852
Iteration 21, loss = 0.03106438
Iteration 22, loss = 0.02998908
Iteration 23, loss = 0.02893728
Iteration 24, loss = 0.02818261
Iteration 25, loss = 0.02717418
Iteration 26, loss = 0.02655789
Iteration 27, loss = 0.02591751
Iteration 28, loss = 0.02536064
Iteration 29, loss = 0.02495685
Iteration 30, loss = 0.02439100
Iteration 31, loss = 0.02390044
Iteration 32, loss = 0.02342920
Iteration 33, loss = 0.02312814
Iteration 34, loss = 0.02292731
Iteration 35, loss = 0.02273029
Iteration 36, loss = 0.02243381
Iteration 37, loss = 0.02238346
Iteration 38, loss = 0.02198596
Iteration 39, loss = 0.02188684
Iteration 40, loss = 0.02155051
Iteration 41, loss = 0.02143266
Iteration 42, loss = 0.02135708
Iteration 43, loss = 0.02125122
Iteration 44, loss = 0.02075056
Iteration 45, loss = 0.02098017
Iteration 46, loss = 0.02065993
Iteration 47, loss = 0.02081539
Iteration 48, loss = 0.02050478
Iteration 49, loss = 0.02026072
Iteration 50, loss = 0.02014897
Iteration 51, loss = 0.02029674
Iteration 52, loss = 0.02037027
Iteration 53, loss = 0.02002349
Iteration 54, loss = 0.02015812
Iteration 55, loss = 0.02002698
Iteration 56, loss = 0.01998469
Iteration 57, loss = 0.01989085
Iteration 58, loss = 0.01991625
Iteration 59, loss = 0.01976587
Iteration 60, loss = 0.01966191
Iteration 61, loss = 0.01962751
Iteration 62, loss = 0.01968324
Iteration 63, loss = 0.01953373
Iteration 64, loss = 0.01957658
Iteration 65, loss = 0.01936968
Iteration 66, loss = 0.01936878
Iteration 67, loss = 0.01939798
Iteration 68, loss = 0.01957307
Iteration 69, loss = 0.01959258
Iteration 70, loss = 0.01936762
Iteration 71, loss = 0.01937959
Iteration 72, loss = 0.01924958
Iteration 73, loss = 0.01930706
Iteration 74, loss = 0.01929339
Iteration 75, loss = 0.01915493
Iteration 76, loss = 0.01893840
Iteration 77, loss = 0.01912549
Iteration 78, loss = 0.01917999
Iteration 79, loss = 0.01879366
Iteration 80, loss = 0.01925784
Iteration 81, loss = 0.01887195
Iteration 82, loss = 0.01902777
Iteration 83, loss = 0.01876543
Iteration 84, loss = 0.01890355
Iteration 85, loss = 0.01905445
Iteration 86, loss = 0.01895714
Iteration 87, loss = 0.01893909
Iteration 88, loss = 0.01893360
Iteration 89, loss = 0.01861200
Iteration 90, loss = 0.01901067
Iteration 91, loss = 0.01874750
Iteration 92, loss = 0.01912099
Iteration 93, loss = 0.01877576
Iteration 94, loss = 0.01874611
Iteration 95, loss = 0.01899538
Iteration 96, loss = 0.01847764
Iteration 97, loss = 0.01871149
Iteration 98, loss = 0.01896897
Iteration 99, loss = 0.01852036
Iteration 100, loss = 0.01866606
Iteration 101, loss = 0.01856534
Iteration 102, loss = 0.01859659
Iteration 103, loss = 0.01887997
Iteration 104, loss = 0.01869491
Iteration 105, loss = 0.01880960
Iteration 106, loss = 0.01864513
Iteration 107, loss = 0.01847766
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67813856
Iteration 2, loss = 0.60381997
Iteration 3, loss = 0.48548868
Iteration 4, loss = 0.36631602
Iteration 5, loss = 0.27028090
Iteration 6, loss = 0.20050616
Iteration 7, loss = 0.15254715
Iteration 8, loss = 0.11999766
Iteration 9, loss = 0.09750309
Iteration 10, loss = 0.08139691
Iteration 11, loss = 0.06999990
Iteration 12, loss = 0.06115072
Iteration 13, loss = 0.05460896
Iteration 14, loss = 0.04918401
Iteration 15, loss = 0.04506271
Iteration 16, loss = 0.04174741
Iteration 17, loss = 0.03903284
Iteration 18, loss = 0.03671263
Iteration 19, loss = 0.03464150
Iteration 20, loss = 0.03296005
Iteration 21, loss = 0.03147332
Iteration 22, loss = 0.03027462
Iteration 23, loss = 0.02925642
Iteration 24, loss = 0.02832279
Iteration 25, loss = 0.02758091
Iteration 26, loss = 0.02687110
Iteration 27, loss = 0.02614301
Iteration 28, loss = 0.02581292
Iteration 29, loss = 0.02512875
Iteration 30, loss = 0.02475579
Iteration 31, loss = 0.02426321
Iteration 32, loss = 0.02382505
Iteration 33, loss = 0.02355420
Iteration 34, loss = 0.02318477
Iteration 35, loss = 0.02338284
Iteration 36, loss = 0.02288933
Iteration 37, loss = 0.02273403
Iteration 38, loss = 0.02227536
Iteration 39, loss = 0.02199569
Iteration 40, loss = 0.02193855
Iteration 41, loss = 0.02174039
Iteration 42, loss = 0.02169615
Iteration 43, loss = 0.02145320
Iteration 44, loss = 0.02137516
Iteration 45, loss = 0.02133459
Iteration 46, loss = 0.02115371
Iteration 47, loss = 0.02112001
Iteration 48, loss = 0.02096952
Iteration 49, loss = 0.02084761
Iteration 50, loss = 0.02088559
Iteration 51, loss = 0.02058396
Iteration 52, loss = 0.02045561
Iteration 53, loss = 0.02056006
Iteration 54, loss = 0.02049055
Iteration 55, loss = 0.02032776
Iteration 56, loss = 0.02026150
Iteration 57, loss = 0.02006317
Iteration 58, loss = 0.02032760
Iteration 59, loss = 0.02019352
Iteration 60, loss = 0.02025685
Iteration 61, loss = 0.02019290
Iteration 62, loss = 0.02004242
Iteration 63, loss = 0.02014285
Iteration 64, loss = 0.01990797
Iteration 65, loss = 0.01963289
Iteration 66, loss = 0.01999736
Iteration 67, loss = 0.02006169
Iteration 68, loss = 0.01990628
Iteration 69, loss = 0.01974567
Iteration 70, loss = 0.01981781
Iteration 71, loss = 0.01966005
Iteration 72, loss = 0.01961106
Iteration 73, loss = 0.01959424
Iteration 74, loss = 0.01953737
Iteration 75, loss = 0.01957583
Iteration 76, loss = 0.01955646
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71594381
Iteration 2, loss = 0.65804528
Iteration 3, loss = 0.56349327
Iteration 4, loss = 0.44965125
Iteration 5, loss = 0.34649277
Iteration 6, loss = 0.26416693
Iteration 7, loss = 0.20268439
Iteration 8, loss = 0.15835870
Iteration 9, loss = 0.12686685
Iteration 10, loss = 0.10462272
Iteration 11, loss = 0.08809390
Iteration 12, loss = 0.07605914
Iteration 13, loss = 0.06695679
Iteration 14, loss = 0.05949104
Iteration 15, loss = 0.05381289
Iteration 16, loss = 0.04914583
Iteration 17, loss = 0.04568408
Iteration 18, loss = 0.04257122
Iteration 19, loss = 0.03999705
Iteration 20, loss = 0.03798586
Iteration 21, loss = 0.03597765
Iteration 22, loss = 0.03467556
Iteration 23, loss = 0.03304063
Iteration 24, loss = 0.03200452
Iteration 25, loss = 0.03101697
Iteration 26, loss = 0.03018446
Iteration 27, loss = 0.02950688
Iteration 28, loss = 0.02844427
Iteration 29, loss = 0.02799129
Iteration 30, loss = 0.02731031
Iteration 31, loss = 0.02713165
Iteration 32, loss = 0.02632970
Iteration 33, loss = 0.02601127
Iteration 34, loss = 0.02572395
Iteration 35, loss = 0.02517198
Iteration 36, loss = 0.02501235
Iteration 37, loss = 0.02488824
Iteration 38, loss = 0.02444803
Iteration 39, loss = 0.02417403
Iteration 40, loss = 0.02404112
Iteration 41, loss = 0.02404153
Iteration 42, loss = 0.02354013
Iteration 43, loss = 0.02347111
Iteration 44, loss = 0.02339445
Iteration 45, loss = 0.02338666
Iteration 46, loss = 0.02319626
Iteration 47, loss = 0.02301161
Iteration 48, loss = 0.02303631
Iteration 49, loss = 0.02266356
Iteration 50, loss = 0.02281470
Iteration 51, loss = 0.02255959
Iteration 52, loss = 0.02246823
Iteration 53, loss = 0.02222821
Iteration 54, loss = 0.02236010
Iteration 55, loss = 0.02216865
Iteration 56, loss = 0.02218824
Iteration 57, loss = 0.02203059
Iteration 58, loss = 0.02235357
Iteration 59, loss = 0.02202976
Iteration 60, loss = 0.02207741
Iteration 61, loss = 0.02171542
Iteration 62, loss = 0.02170270
Iteration 63, loss = 0.02163420
Iteration 64, loss = 0.02191197
Iteration 65, loss = 0.02175603
Iteration 66, loss = 0.02161897
Iteration 67, loss = 0.02161442
Iteration 68, loss = 0.02151186
Iteration 69, loss = 0.02128626
Iteration 70, loss = 0.02158561
Iteration 71, loss = 0.02158335
Iteration 72, loss = 0.02144003
Iteration 73, loss = 0.02133628
Iteration 74, loss = 0.02135232
Iteration 75, loss = 0.02132815
Iteration 76, loss = 0.02135489
Iteration 77, loss = 0.02117281
Iteration 78, loss = 0.02101100
Iteration 79, loss = 0.02116508
Iteration 80, loss = 0.02124700
Iteration 81, loss = 0.02146478
Iteration 82, loss = 0.02134902
Iteration 83, loss = 0.02119813
Iteration 84, loss = 0.02108538
Iteration 85, loss = 0.02114303
Iteration 86, loss = 0.02102935
Iteration 87, loss = 0.02097113
Iteration 88, loss = 0.02080812
Iteration 89, loss = 0.02086540
Iteration 90, loss = 0.02096851
Iteration 91, loss = 0.02108952
Iteration 92, loss = 0.02081228
Iteration 93, loss = 0.02104623
Iteration 94, loss = 0.02109574
Iteration 95, loss = 0.02086443
Iteration 96, loss = 0.02059921
Iteration 97, loss = 0.02087870
Iteration 98, loss = 0.02092223
Iteration 99, loss = 0.02084849
Iteration 100, loss = 0.02074236
Iteration 101, loss = 0.02074968
Iteration 102, loss = 0.02091411
Iteration 103, loss = 0.02097885
Iteration 104, loss = 0.02073182
Iteration 105, loss = 0.02076859
Iteration 106, loss = 0.02063423
Iteration 107, loss = 0.02063300
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67705519
Iteration 2, loss = 0.60587953
Iteration 3, loss = 0.49703542
Iteration 4, loss = 0.38242763
Iteration 5, loss = 0.28566823
Iteration 6, loss = 0.21278244
Iteration 7, loss = 0.16190506
Iteration 8, loss = 0.12720691
Iteration 9, loss = 0.10306611
Iteration 10, loss = 0.08632105
Iteration 11, loss = 0.07407780
Iteration 12, loss = 0.06498270
Iteration 13, loss = 0.05806905
Iteration 14, loss = 0.05265726
Iteration 15, loss = 0.04864060
Iteration 16, loss = 0.04523859
Iteration 17, loss = 0.04234902
Iteration 18, loss = 0.04009698
Iteration 19, loss = 0.03815043
Iteration 20, loss = 0.03626105
Iteration 21, loss = 0.03519958
Iteration 22, loss = 0.03409293
Iteration 23, loss = 0.03280665
Iteration 24, loss = 0.03207648
Iteration 25, loss = 0.03117390
Iteration 26, loss = 0.03056442
Iteration 27, loss = 0.02982668
Iteration 28, loss = 0.02952043
Iteration 29, loss = 0.02908141
Iteration 30, loss = 0.02840729
Iteration 31, loss = 0.02777614
Iteration 32, loss = 0.02772023
Iteration 33, loss = 0.02731741
Iteration 34, loss = 0.02750972
Iteration 35, loss = 0.02673950
Iteration 36, loss = 0.02643744
Iteration 37, loss = 0.02662801
Iteration 38, loss = 0.02622322
Iteration 39, loss = 0.02591269
Iteration 40, loss = 0.02586809
Iteration 41, loss = 0.02560205
Iteration 42, loss = 0.02556235
Iteration 43, loss = 0.02517265
Iteration 44, loss = 0.02528793
Iteration 45, loss = 0.02521561
Iteration 46, loss = 0.02494816
Iteration 47, loss = 0.02475307
Iteration 48, loss = 0.02492749
Iteration 49, loss = 0.02477339
Iteration 50, loss = 0.02458798
Iteration 51, loss = 0.02473257
Iteration 52, loss = 0.02444347
Iteration 53, loss = 0.02462507
Iteration 54, loss = 0.02467792
Iteration 55, loss = 0.02440626
Iteration 56, loss = 0.02409007
Iteration 57, loss = 0.02434950
Iteration 58, loss = 0.02402099
Iteration 59, loss = 0.02395781
Iteration 60, loss = 0.02395378
Iteration 61, loss = 0.02416257
Iteration 62, loss = 0.02400457
Iteration 63, loss = 0.02384794
Iteration 64, loss = 0.02399090
Iteration 65, loss = 0.02372655
Iteration 66, loss = 0.02393145
Iteration 67, loss = 0.02372594
Iteration 68, loss = 0.02365084
Iteration 69, loss = 0.02359031
Iteration 70, loss = 0.02372282
Iteration 71, loss = 0.02363361
Iteration 72, loss = 0.02384148
Iteration 73, loss = 0.02364335
Iteration 74, loss = 0.02344628
Iteration 75, loss = 0.02362067
Iteration 76, loss = 0.02365410
Iteration 77, loss = 0.02347632
Iteration 78, loss = 0.02328989
Iteration 79, loss = 0.02342824
Iteration 80, loss = 0.02351654
Iteration 81, loss = 0.02361221
Iteration 82, loss = 0.02362104
Iteration 83, loss = 0.02318055
Iteration 84, loss = 0.02339435
Iteration 85, loss = 0.02378057
Iteration 86, loss = 0.02297571
Iteration 87, loss = 0.02334269
Iteration 88, loss = 0.02364284
Iteration 89, loss = 0.02331598
Iteration 90, loss = 0.02314288
Iteration 91, loss = 0.02297257
Iteration 92, loss = 0.02358924
Iteration 93, loss = 0.02326220
Iteration 94, loss = 0.02342044
Iteration 95, loss = 0.02305848
Iteration 96, loss = 0.02343083
Iteration 97, loss = 0.02306967
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68907837
Iteration 2, loss = 0.61257710
Iteration 3, loss = 0.50030427
Iteration 4, loss = 0.38486673
Iteration 5, loss = 0.28632486
Iteration 6, loss = 0.21204168
Iteration 7, loss = 0.15995103
Iteration 8, loss = 0.12463863
Iteration 9, loss = 0.10046011
Iteration 10, loss = 0.08318117
Iteration 11, loss = 0.07102041
Iteration 12, loss = 0.06176998
Iteration 13, loss = 0.05494790
Iteration 14, loss = 0.04953166
Iteration 15, loss = 0.04523622
Iteration 16, loss = 0.04167285
Iteration 17, loss = 0.03870800
Iteration 18, loss = 0.03646458
Iteration 19, loss = 0.03461647
Iteration 20, loss = 0.03279985
Iteration 21, loss = 0.03150687
Iteration 22, loss = 0.03014737
Iteration 23, loss = 0.02902102
Iteration 24, loss = 0.02826709
Iteration 25, loss = 0.02750791
Iteration 26, loss = 0.02675310
Iteration 27, loss = 0.02602940
Iteration 28, loss = 0.02577614
Iteration 29, loss = 0.02505017
Iteration 30, loss = 0.02462018
Iteration 31, loss = 0.02422263
Iteration 32, loss = 0.02375598
Iteration 33, loss = 0.02333931
Iteration 34, loss = 0.02308435
Iteration 35, loss = 0.02288810
Iteration 36, loss = 0.02257112
Iteration 37, loss = 0.02232510
Iteration 38, loss = 0.02216179
Iteration 39, loss = 0.02186038
Iteration 40, loss = 0.02172007
Iteration 41, loss = 0.02167479
Iteration 42, loss = 0.02147794
Iteration 43, loss = 0.02131476
Iteration 44, loss = 0.02109311
Iteration 45, loss = 0.02111796
Iteration 46, loss = 0.02092864
Iteration 47, loss = 0.02081756
Iteration 48, loss = 0.02091670
Iteration 49, loss = 0.02045392
Iteration 50, loss = 0.02096666
Iteration 51, loss = 0.02046418
Iteration 52, loss = 0.02063331
Iteration 53, loss = 0.02046198
Iteration 54, loss = 0.02031027
Iteration 55, loss = 0.02015699
Iteration 56, loss = 0.02019650
Iteration 57, loss = 0.02021420
Iteration 58, loss = 0.01989398
Iteration 59, loss = 0.02021209
Iteration 60, loss = 0.01969014
Iteration 61, loss = 0.01992149
Iteration 62, loss = 0.01972395
Iteration 63, loss = 0.01971574
Iteration 64, loss = 0.01966274
Iteration 65, loss = 0.01977511
Iteration 66, loss = 0.01964611
Iteration 67, loss = 0.01982262
Iteration 68, loss = 0.01960886
Iteration 69, loss = 0.01936652
Iteration 70, loss = 0.01943645
Iteration 71, loss = 0.01920469
Iteration 72, loss = 0.01976975
Iteration 73, loss = 0.01948847
Iteration 74, loss = 0.01943097
Iteration 75, loss = 0.01941466
Iteration 76, loss = 0.01927779
Iteration 77, loss = 0.01930643
Iteration 78, loss = 0.01930697
Iteration 79, loss = 0.01931367
Iteration 80, loss = 0.01919493
Iteration 81, loss = 0.01919229
Iteration 82, loss = 0.01943582
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68903920
Iteration 2, loss = 0.60466235
Iteration 3, loss = 0.49113455
Iteration 4, loss = 0.37445522
Iteration 5, loss = 0.27807912
Iteration 6, loss = 0.20687897
Iteration 7, loss = 0.15710121
Iteration 8, loss = 0.12303284
Iteration 9, loss = 0.09945667
Iteration 10, loss = 0.08263686
Iteration 11, loss = 0.07037989
Iteration 12, loss = 0.06160695
Iteration 13, loss = 0.05447490
Iteration 14, loss = 0.04911785
Iteration 15, loss = 0.04477736
Iteration 16, loss = 0.04148041
Iteration 17, loss = 0.03882516
Iteration 18, loss = 0.03621395
Iteration 19, loss = 0.03433566
Iteration 20, loss = 0.03264852
Iteration 21, loss = 0.03106438
Iteration 22, loss = 0.02998908
Iteration 23, loss = 0.02893728
Iteration 24, loss = 0.02818261
Iteration 25, loss = 0.02717418
Iteration 26, loss = 0.02655789
Iteration 27, loss = 0.02591751
Iteration 28, loss = 0.02536064
Iteration 29, loss = 0.02495685
Iteration 30, loss = 0.02439100
Iteration 31, loss = 0.02390044
Iteration 32, loss = 0.02342920
Iteration 33, loss = 0.02312814
Iteration 34, loss = 0.02292731
Iteration 35, loss = 0.02273029
Iteration 36, loss = 0.02243381
Iteration 37, loss = 0.02238346
Iteration 38, loss = 0.02198596
Iteration 39, loss = 0.02188684
Iteration 40, loss = 0.02155051
Iteration 41, loss = 0.02143266
Iteration 42, loss = 0.02135708
Iteration 43, loss = 0.02125122
Iteration 44, loss = 0.02075056
Iteration 45, loss = 0.02098017
Iteration 46, loss = 0.02065993
Iteration 47, loss = 0.02081539
Iteration 48, loss = 0.02050478
Iteration 49, loss = 0.02026072
Iteration 50, loss = 0.02014897
Iteration 51, loss = 0.02029674
Iteration 52, loss = 0.02037027
Iteration 53, loss = 0.02002349
Iteration 54, loss = 0.02015812
Iteration 55, loss = 0.02002698
Iteration 56, loss = 0.01998469
Iteration 57, loss = 0.01989085
Iteration 58, loss = 0.01991625
Iteration 59, loss = 0.01976587
Iteration 60, loss = 0.01966191
Iteration 61, loss = 0.01962751
Iteration 62, loss = 0.01968324
Iteration 63, loss = 0.01953373
Iteration 64, loss = 0.01957658
Iteration 65, loss = 0.01936968
Iteration 66, loss = 0.01936878
Iteration 67, loss = 0.01939798
Iteration 68, loss = 0.01957307
Iteration 69, loss = 0.01959258
Iteration 70, loss = 0.01936762
Iteration 71, loss = 0.01937959
Iteration 72, loss = 0.01924958
Iteration 73, loss = 0.01930706
Iteration 74, loss = 0.01929339
Iteration 75, loss = 0.01915493
Iteration 76, loss = 0.01893840
Iteration 77, loss = 0.01912549
Iteration 78, loss = 0.01917999
Iteration 79, loss = 0.01879366
Iteration 80, loss = 0.01925784
Iteration 81, loss = 0.01887195
Iteration 82, loss = 0.01902777
Iteration 83, loss = 0.01876543
Iteration 84, loss = 0.01890355
Iteration 85, loss = 0.01905445
Iteration 86, loss = 0.01895714
Iteration 87, loss = 0.01893909
Iteration 88, loss = 0.01893360
Iteration 89, loss = 0.01861200
Iteration 90, loss = 0.01901067
Iteration 91, loss = 0.01874750
Iteration 92, loss = 0.01912099
Iteration 93, loss = 0.01877576
Iteration 94, loss = 0.01874611
Iteration 95, loss = 0.01899538
Iteration 96, loss = 0.01847764
Iteration 97, loss = 0.01871149
Iteration 98, loss = 0.01896897
Iteration 99, loss = 0.01852036
Iteration 100, loss = 0.01866606
Iteration 101, loss = 0.01856534
Iteration 102, loss = 0.01859659
Iteration 103, loss = 0.01887997
Iteration 104, loss = 0.01869491
Iteration 105, loss = 0.01880960
Iteration 106, loss = 0.01864513
Iteration 107, loss = 0.01847766
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67813856
Iteration 2, loss = 0.60381997
Iteration 3, loss = 0.48548868
Iteration 4, loss = 0.36631602
Iteration 5, loss = 0.27028090
Iteration 6, loss = 0.20050616
Iteration 7, loss = 0.15254715
Iteration 8, loss = 0.11999766
Iteration 9, loss = 0.09750309
Iteration 10, loss = 0.08139691
Iteration 11, loss = 0.06999990
Iteration 12, loss = 0.06115072
Iteration 13, loss = 0.05460896
Iteration 14, loss = 0.04918401
Iteration 15, loss = 0.04506271
Iteration 16, loss = 0.04174741
Iteration 17, loss = 0.03903284
Iteration 18, loss = 0.03671263
Iteration 19, loss = 0.03464150
Iteration 20, loss = 0.03296005
Iteration 21, loss = 0.03147332
Iteration 22, loss = 0.03027462
Iteration 23, loss = 0.02925642
Iteration 24, loss = 0.02832279
Iteration 25, loss = 0.02758091
Iteration 26, loss = 0.02687110
Iteration 27, loss = 0.02614301
Iteration 28, loss = 0.02581292
Iteration 29, loss = 0.02512875
Iteration 30, loss = 0.02475579
Iteration 31, loss = 0.02426321
Iteration 32, loss = 0.02382505
Iteration 33, loss = 0.02355420
Iteration 34, loss = 0.02318477
Iteration 35, loss = 0.02338284
Iteration 36, loss = 0.02288933
Iteration 37, loss = 0.02273403
Iteration 38, loss = 0.02227536
Iteration 39, loss = 0.02199569
Iteration 40, loss = 0.02193855
Iteration 41, loss = 0.02174039
Iteration 42, loss = 0.02169615
Iteration 43, loss = 0.02145320
Iteration 44, loss = 0.02137516
Iteration 45, loss = 0.02133459
Iteration 46, loss = 0.02115371
Iteration 47, loss = 0.02112001
Iteration 48, loss = 0.02096952
Iteration 49, loss = 0.02084761
Iteration 50, loss = 0.02088559
Iteration 51, loss = 0.02058396
Iteration 52, loss = 0.02045561
Iteration 53, loss = 0.02056006
Iteration 54, loss = 0.02049055
Iteration 55, loss = 0.02032776
Iteration 56, loss = 0.02026150
Iteration 57, loss = 0.02006317
Iteration 58, loss = 0.02032760
Iteration 59, loss = 0.02019352
Iteration 60, loss = 0.02025685
Iteration 61, loss = 0.02019290
Iteration 62, loss = 0.02004242
Iteration 63, loss = 0.02014285
Iteration 64, loss = 0.01990797
Iteration 65, loss = 0.01963289
Iteration 66, loss = 0.01999736
Iteration 67, loss = 0.02006169
Iteration 68, loss = 0.01990628
Iteration 69, loss = 0.01974567
Iteration 70, loss = 0.01981781
Iteration 71, loss = 0.01966005
Iteration 72, loss = 0.01961106
Iteration 73, loss = 0.01959424
Iteration 74, loss = 0.01953737
Iteration 75, loss = 0.01957583
Iteration 76, loss = 0.01955646
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71594381
Iteration 2, loss = 0.65804528
Iteration 3, loss = 0.56349327
Iteration 4, loss = 0.44965125
Iteration 5, loss = 0.34649277
Iteration 6, loss = 0.26416693
Iteration 7, loss = 0.20268439
Iteration 8, loss = 0.15835870
Iteration 9, loss = 0.12686685
Iteration 10, loss = 0.10462272
Iteration 11, loss = 0.08809390
Iteration 12, loss = 0.07605914
Iteration 13, loss = 0.06695679
Iteration 14, loss = 0.05949104
Iteration 15, loss = 0.05381289
Iteration 16, loss = 0.04914583
Iteration 17, loss = 0.04568408
Iteration 18, loss = 0.04257122
Iteration 19, loss = 0.03999705
Iteration 20, loss = 0.03798586
Iteration 21, loss = 0.03597765
Iteration 22, loss = 0.03467556
Iteration 23, loss = 0.03304063
Iteration 24, loss = 0.03200452
Iteration 25, loss = 0.03101697
Iteration 26, loss = 0.03018446
Iteration 27, loss = 0.02950688
Iteration 28, loss = 0.02844427
Iteration 29, loss = 0.02799129
Iteration 30, loss = 0.02731031
Iteration 31, loss = 0.02713165
Iteration 32, loss = 0.02632970
Iteration 33, loss = 0.02601127
Iteration 34, loss = 0.02572395
Iteration 35, loss = 0.02517198
Iteration 36, loss = 0.02501235
Iteration 37, loss = 0.02488824
Iteration 38, loss = 0.02444803
Iteration 39, loss = 0.02417403
Iteration 40, loss = 0.02404112
Iteration 41, loss = 0.02404153
Iteration 42, loss = 0.02354013
Iteration 43, loss = 0.02347111
Iteration 44, loss = 0.02339445
Iteration 45, loss = 0.02338666
Iteration 46, loss = 0.02319626
Iteration 47, loss = 0.02301161
Iteration 48, loss = 0.02303631
Iteration 49, loss = 0.02266356
Iteration 50, loss = 0.02281470
Iteration 51, loss = 0.02255959
Iteration 52, loss = 0.02246823
Iteration 53, loss = 0.02222821
Iteration 54, loss = 0.02236010
Iteration 55, loss = 0.02216865
Iteration 56, loss = 0.02218824
Iteration 57, loss = 0.02203059
Iteration 58, loss = 0.02235357
Iteration 59, loss = 0.02202976
Iteration 60, loss = 0.02207741
Iteration 61, loss = 0.02171542
Iteration 62, loss = 0.02170270
Iteration 63, loss = 0.02163420
Iteration 64, loss = 0.02191197
Iteration 65, loss = 0.02175603
Iteration 66, loss = 0.02161897
Iteration 67, loss = 0.02161442
Iteration 68, loss = 0.02151186
Iteration 69, loss = 0.02128626
Iteration 70, loss = 0.02158561
Iteration 71, loss = 0.02158335
Iteration 72, loss = 0.02144003
Iteration 73, loss = 0.02133628
Iteration 74, loss = 0.02135232
Iteration 75, loss = 0.02132815
Iteration 76, loss = 0.02135489
Iteration 77, loss = 0.02117281
Iteration 78, loss = 0.02101100
Iteration 79, loss = 0.02116508
Iteration 80, loss = 0.02124700
Iteration 81, loss = 0.02146478
Iteration 82, loss = 0.02134902
Iteration 83, loss = 0.02119813
Iteration 84, loss = 0.02108538
Iteration 85, loss = 0.02114303
Iteration 86, loss = 0.02102935
Iteration 87, loss = 0.02097113
Iteration 88, loss = 0.02080812
Iteration 89, loss = 0.02086540
Iteration 90, loss = 0.02096851
Iteration 91, loss = 0.02108952
Iteration 92, loss = 0.02081228
Iteration 93, loss = 0.02104623
Iteration 94, loss = 0.02109574
Iteration 95, loss = 0.02086443
Iteration 96, loss = 0.02059921
Iteration 97, loss = 0.02087870
Iteration 98, loss = 0.02092223
Iteration 99, loss = 0.02084849
Iteration 100, loss = 0.02074236
Iteration 101, loss = 0.02074968
Iteration 102, loss = 0.02091411
Iteration 103, loss = 0.02097885
Iteration 104, loss = 0.02073182
Iteration 105, loss = 0.02076859
Iteration 106, loss = 0.02063423
Iteration 107, loss = 0.02063300
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67705519
Iteration 2, loss = 0.60587953
Iteration 3, loss = 0.49703542
Iteration 4, loss = 0.38242763
Iteration 5, loss = 0.28566823
Iteration 6, loss = 0.21278244
Iteration 7, loss = 0.16190506
Iteration 8, loss = 0.12720691
Iteration 9, loss = 0.10306611
Iteration 10, loss = 0.08632105
Iteration 11, loss = 0.07407780
Iteration 12, loss = 0.06498270
Iteration 13, loss = 0.05806905
Iteration 14, loss = 0.05265726
Iteration 15, loss = 0.04864060
Iteration 16, loss = 0.04523859
Iteration 17, loss = 0.04234902
Iteration 18, loss = 0.04009698
Iteration 19, loss = 0.03815043
Iteration 20, loss = 0.03626105
Iteration 21, loss = 0.03519958
Iteration 22, loss = 0.03409293
Iteration 23, loss = 0.03280665
Iteration 24, loss = 0.03207648
Iteration 25, loss = 0.03117390
Iteration 26, loss = 0.03056442
Iteration 27, loss = 0.02982668
Iteration 28, loss = 0.02952043
Iteration 29, loss = 0.02908141
Iteration 30, loss = 0.02840729
Iteration 31, loss = 0.02777614
Iteration 32, loss = 0.02772023
Iteration 33, loss = 0.02731741
Iteration 34, loss = 0.02750972
Iteration 35, loss = 0.02673950
Iteration 36, loss = 0.02643744
Iteration 37, loss = 0.02662801
Iteration 38, loss = 0.02622322
Iteration 39, loss = 0.02591269
Iteration 40, loss = 0.02586809
Iteration 41, loss = 0.02560205
Iteration 42, loss = 0.02556235
Iteration 43, loss = 0.02517265
Iteration 44, loss = 0.02528793
Iteration 45, loss = 0.02521561
Iteration 46, loss = 0.02494816
Iteration 47, loss = 0.02475307
Iteration 48, loss = 0.02492749
Iteration 49, loss = 0.02477339
Iteration 50, loss = 0.02458798
Iteration 51, loss = 0.02473257
Iteration 52, loss = 0.02444347
Iteration 53, loss = 0.02462507
Iteration 54, loss = 0.02467792
Iteration 55, loss = 0.02440626
Iteration 56, loss = 0.02409007
Iteration 57, loss = 0.02434950
Iteration 58, loss = 0.02402099
Iteration 59, loss = 0.02395781
Iteration 60, loss = 0.02395378
Iteration 61, loss = 0.02416257
Iteration 62, loss = 0.02400457
Iteration 63, loss = 0.02384794
Iteration 64, loss = 0.02399090
Iteration 65, loss = 0.02372655
Iteration 66, loss = 0.02393145
Iteration 67, loss = 0.02372594
Iteration 68, loss = 0.02365084
Iteration 69, loss = 0.02359031
Iteration 70, loss = 0.02372282
Iteration 71, loss = 0.02363361
Iteration 72, loss = 0.02384148
Iteration 73, loss = 0.02364335
Iteration 74, loss = 0.02344628
Iteration 75, loss = 0.02362067
Iteration 76, loss = 0.02365410
Iteration 77, loss = 0.02347632
Iteration 78, loss = 0.02328989
Iteration 79, loss = 0.02342824
Iteration 80, loss = 0.02351654
Iteration 81, loss = 0.02361221
Iteration 82, loss = 0.02362104
Iteration 83, loss = 0.02318055
Iteration 84, loss = 0.02339435
Iteration 85, loss = 0.02378057
Iteration 86, loss = 0.02297571
Iteration 87, loss = 0.02334269
Iteration 88, loss = 0.02364284
Iteration 89, loss = 0.02331598
Iteration 90, loss = 0.02314288
Iteration 91, loss = 0.02297257
Iteration 92, loss = 0.02358924
Iteration 93, loss = 0.02326220
Iteration 94, loss = 0.02342044
Iteration 95, loss = 0.02305848
Iteration 96, loss = 0.02343083
Iteration 97, loss = 0.02306967
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68907837
Iteration 2, loss = 0.61257710
Iteration 3, loss = 0.50030427
Iteration 4, loss = 0.38486673
Iteration 5, loss = 0.28632486
Iteration 6, loss = 0.21204168
Iteration 7, loss = 0.15995103
Iteration 8, loss = 0.12463863
Iteration 9, loss = 0.10046011
Iteration 10, loss = 0.08318117
Iteration 11, loss = 0.07102041
Iteration 12, loss = 0.06176998
Iteration 13, loss = 0.05494790
Iteration 14, loss = 0.04953166
Iteration 15, loss = 0.04523622
Iteration 16, loss = 0.04167285
Iteration 17, loss = 0.03870800
Iteration 18, loss = 0.03646458
Iteration 19, loss = 0.03461647
Iteration 20, loss = 0.03279985
Iteration 21, loss = 0.03150687
Iteration 22, loss = 0.03014737
Iteration 23, loss = 0.02902102
Iteration 24, loss = 0.02826709
Iteration 25, loss = 0.02750791
Iteration 26, loss = 0.02675310
Iteration 27, loss = 0.02602940
Iteration 28, loss = 0.02577614
Iteration 29, loss = 0.02505017
Iteration 30, loss = 0.02462018
Iteration 31, loss = 0.02422263
Iteration 32, loss = 0.02375598
Iteration 33, loss = 0.02333931
Iteration 34, loss = 0.02308435
Iteration 35, loss = 0.02288810
Iteration 36, loss = 0.02257112
Iteration 37, loss = 0.02232510
Iteration 38, loss = 0.02216179
Iteration 39, loss = 0.02186038
Iteration 40, loss = 0.02172007
Iteration 41, loss = 0.02167479
Iteration 42, loss = 0.02147794
Iteration 43, loss = 0.02131476
Iteration 44, loss = 0.02109311
Iteration 45, loss = 0.02111796
Iteration 46, loss = 0.02092864
Iteration 47, loss = 0.02081756
Iteration 48, loss = 0.02091670
Iteration 49, loss = 0.02045392
Iteration 50, loss = 0.02096666
Iteration 51, loss = 0.02046418
Iteration 52, loss = 0.02063331
Iteration 53, loss = 0.02046198
Iteration 54, loss = 0.02031027
Iteration 55, loss = 0.02015699
Iteration 56, loss = 0.02019650
Iteration 57, loss = 0.02021420
Iteration 58, loss = 0.01989398
Iteration 59, loss = 0.02021209
Iteration 60, loss = 0.01969014
Iteration 61, loss = 0.01992149
Iteration 62, loss = 0.01972395
Iteration 63, loss = 0.01971574
Iteration 64, loss = 0.01966274
Iteration 65, loss = 0.01977511
Iteration 66, loss = 0.01964611
Iteration 67, loss = 0.01982262
Iteration 68, loss = 0.01960886
Iteration 69, loss = 0.01936652
Iteration 70, loss = 0.01943645
Iteration 71, loss = 0.01920469
Iteration 72, loss = 0.01976975
Iteration 73, loss = 0.01948847
Iteration 74, loss = 0.01943097
Iteration 75, loss = 0.01941466
Iteration 76, loss = 0.01927779
Iteration 77, loss = 0.01930643
Iteration 78, loss = 0.01930697
Iteration 79, loss = 0.01931367
Iteration 80, loss = 0.01919493
Iteration 81, loss = 0.01919229
Iteration 82, loss = 0.01943582
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68903920
Iteration 2, loss = 0.60466235
Iteration 3, loss = 0.49113455
Iteration 4, loss = 0.37445522
Iteration 5, loss = 0.27807912
Iteration 6, loss = 0.20687897
Iteration 7, loss = 0.15710121
Iteration 8, loss = 0.12303284
Iteration 9, loss = 0.09945667
Iteration 10, loss = 0.08263686
Iteration 11, loss = 0.07037989
Iteration 12, loss = 0.06160695
Iteration 13, loss = 0.05447490
Iteration 14, loss = 0.04911785
Iteration 15, loss = 0.04477736
Iteration 16, loss = 0.04148041
Iteration 17, loss = 0.03882516
Iteration 18, loss = 0.03621395
Iteration 19, loss = 0.03433566
Iteration 20, loss = 0.03264852
Iteration 21, loss = 0.03106438
Iteration 22, loss = 0.02998908
Iteration 23, loss = 0.02893728
Iteration 24, loss = 0.02818261
Iteration 25, loss = 0.02717418
Iteration 26, loss = 0.02655789
Iteration 27, loss = 0.02591751
Iteration 28, loss = 0.02536064
Iteration 29, loss = 0.02495685
Iteration 30, loss = 0.02439100
Iteration 31, loss = 0.02390044
Iteration 32, loss = 0.02342920
Iteration 33, loss = 0.02312814
Iteration 34, loss = 0.02292731
Iteration 35, loss = 0.02273029
Iteration 36, loss = 0.02243381
Iteration 37, loss = 0.02238346
Iteration 38, loss = 0.02198596
Iteration 39, loss = 0.02188684
Iteration 40, loss = 0.02155051
Iteration 41, loss = 0.02143266
Iteration 42, loss = 0.02135708
Iteration 43, loss = 0.02125122
Iteration 44, loss = 0.02075056
Iteration 45, loss = 0.02098017
Iteration 46, loss = 0.02065993
Iteration 47, loss = 0.02081539
Iteration 48, loss = 0.02050478
Iteration 49, loss = 0.02026072
Iteration 50, loss = 0.02014897
Iteration 51, loss = 0.02029674
Iteration 52, loss = 0.02037027
Iteration 53, loss = 0.02002349
Iteration 54, loss = 0.02015812
Iteration 55, loss = 0.02002698
Iteration 56, loss = 0.01998469
Iteration 57, loss = 0.01989085
Iteration 58, loss = 0.01991625
Iteration 59, loss = 0.01976587
Iteration 60, loss = 0.01966191
Iteration 61, loss = 0.01962751
Iteration 62, loss = 0.01968324
Iteration 63, loss = 0.01953373
Iteration 64, loss = 0.01957658
Iteration 65, loss = 0.01936968
Iteration 66, loss = 0.01936878
Iteration 67, loss = 0.01939798
Iteration 68, loss = 0.01957307
Iteration 69, loss = 0.01959258
Iteration 70, loss = 0.01936762
Iteration 71, loss = 0.01937959
Iteration 72, loss = 0.01924958
Iteration 73, loss = 0.01930706
Iteration 74, loss = 0.01929339
Iteration 75, loss = 0.01915493
Iteration 76, loss = 0.01893840
Iteration 77, loss = 0.01912549
Iteration 78, loss = 0.01917999
Iteration 79, loss = 0.01879366
Iteration 80, loss = 0.01925784
Iteration 81, loss = 0.01887195
Iteration 82, loss = 0.01902777
Iteration 83, loss = 0.01876543
Iteration 84, loss = 0.01890355
Iteration 85, loss = 0.01905445
Iteration 86, loss = 0.01895714
Iteration 87, loss = 0.01893909
Iteration 88, loss = 0.01893360
Iteration 89, loss = 0.01861200
Iteration 90, loss = 0.01901067
Iteration 91, loss = 0.01874750
Iteration 92, loss = 0.01912099
Iteration 93, loss = 0.01877576
Iteration 94, loss = 0.01874611
Iteration 95, loss = 0.01899538
Iteration 96, loss = 0.01847764
Iteration 97, loss = 0.01871149
Iteration 98, loss = 0.01896897
Iteration 99, loss = 0.01852036
Iteration 100, loss = 0.01866606
Iteration 101, loss = 0.01856534
Iteration 102, loss = 0.01859659
Iteration 103, loss = 0.01887997
Iteration 104, loss = 0.01869491
Iteration 105, loss = 0.01880960
Iteration 106, loss = 0.01864513
Iteration 107, loss = 0.01847766
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67813856
Iteration 2, loss = 0.60381997
Iteration 3, loss = 0.48548868
Iteration 4, loss = 0.36631602
Iteration 5, loss = 0.27028090
Iteration 6, loss = 0.20050616
Iteration 7, loss = 0.15254715
Iteration 8, loss = 0.11999766
Iteration 9, loss = 0.09750309
Iteration 10, loss = 0.08139691
Iteration 11, loss = 0.06999990
Iteration 12, loss = 0.06115072
Iteration 13, loss = 0.05460896
Iteration 14, loss = 0.04918401
Iteration 15, loss = 0.04506271
Iteration 16, loss = 0.04174741
Iteration 17, loss = 0.03903284
Iteration 18, loss = 0.03671263
Iteration 19, loss = 0.03464150
Iteration 20, loss = 0.03296005
Iteration 21, loss = 0.03147332
Iteration 22, loss = 0.03027462
Iteration 23, loss = 0.02925642
Iteration 24, loss = 0.02832279
Iteration 25, loss = 0.02758091
Iteration 26, loss = 0.02687110
Iteration 27, loss = 0.02614301
Iteration 28, loss = 0.02581292
Iteration 29, loss = 0.02512875
Iteration 30, loss = 0.02475579
Iteration 31, loss = 0.02426321
Iteration 32, loss = 0.02382505
Iteration 33, loss = 0.02355420
Iteration 34, loss = 0.02318477
Iteration 35, loss = 0.02338284
Iteration 36, loss = 0.02288933
Iteration 37, loss = 0.02273403
Iteration 38, loss = 0.02227536
Iteration 39, loss = 0.02199569
Iteration 40, loss = 0.02193855
Iteration 41, loss = 0.02174039
Iteration 42, loss = 0.02169615
Iteration 43, loss = 0.02145320
Iteration 44, loss = 0.02137516
Iteration 45, loss = 0.02133459
Iteration 46, loss = 0.02115371
Iteration 47, loss = 0.02112001
Iteration 48, loss = 0.02096952
Iteration 49, loss = 0.02084761
Iteration 50, loss = 0.02088559
Iteration 51, loss = 0.02058396
Iteration 52, loss = 0.02045561
Iteration 53, loss = 0.02056006
Iteration 54, loss = 0.02049055
Iteration 55, loss = 0.02032776
Iteration 56, loss = 0.02026150
Iteration 57, loss = 0.02006317
Iteration 58, loss = 0.02032760
Iteration 59, loss = 0.02019352
Iteration 60, loss = 0.02025685
Iteration 61, loss = 0.02019290
Iteration 62, loss = 0.02004242
Iteration 63, loss = 0.02014285
Iteration 64, loss = 0.01990797
Iteration 65, loss = 0.01963289
Iteration 66, loss = 0.01999736
Iteration 67, loss = 0.02006169
Iteration 68, loss = 0.01990628
Iteration 69, loss = 0.01974567
Iteration 70, loss = 0.01981781
Iteration 71, loss = 0.01966005
Iteration 72, loss = 0.01961106
Iteration 73, loss = 0.01959424
Iteration 74, loss = 0.01953737
Iteration 75, loss = 0.01957583
Iteration 76, loss = 0.01955646
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67034748
Iteration 2, loss = 0.57620014
Iteration 3, loss = 0.44824244
Iteration 4, loss = 0.32747329
Iteration 5, loss = 0.23429872
Iteration 6, loss = 0.17071462
Iteration 7, loss = 0.12927994
Iteration 8, loss = 0.10172662
Iteration 9, loss = 0.08337637
Iteration 10, loss = 0.07062210
Iteration 11, loss = 0.06147125
Iteration 12, loss = 0.05466883
Iteration 13, loss = 0.04930027
Iteration 14, loss = 0.04549222
Iteration 15, loss = 0.04223083
Iteration 16, loss = 0.03955242
Iteration 17, loss = 0.03724478
Iteration 18, loss = 0.03563614
Iteration 19, loss = 0.03447480
Iteration 20, loss = 0.03302821
Iteration 21, loss = 0.03197487
Iteration 22, loss = 0.03093891
Iteration 23, loss = 0.03023167
Iteration 24, loss = 0.02940941
Iteration 25, loss = 0.02891277
Iteration 26, loss = 0.02840187
Iteration 27, loss = 0.02800480
Iteration 28, loss = 0.02777906
Iteration 29, loss = 0.02710849
Iteration 30, loss = 0.02660129
Iteration 31, loss = 0.02668210
Iteration 32, loss = 0.02629450
Iteration 33, loss = 0.02578449
Iteration 34, loss = 0.02589054
Iteration 35, loss = 0.02581707
Iteration 36, loss = 0.02548805
Iteration 37, loss = 0.02538592
Iteration 38, loss = 0.02525233
Iteration 39, loss = 0.02500650
Iteration 40, loss = 0.02487374
Iteration 41, loss = 0.02485280
Iteration 42, loss = 0.02449569
Iteration 43, loss = 0.02447398
Iteration 44, loss = 0.02446834
Iteration 45, loss = 0.02443755
Iteration 46, loss = 0.02449249
Iteration 47, loss = 0.02407459
Iteration 48, loss = 0.02391998
Iteration 49, loss = 0.02399367
Iteration 50, loss = 0.02397664
Iteration 51, loss = 0.02391011
Iteration 52, loss = 0.02423545
Iteration 53, loss = 0.02353983
Iteration 54, loss = 0.02360058
Iteration 55, loss = 0.02370656
Iteration 56, loss = 0.02366691
Iteration 57, loss = 0.02366470
Iteration 58, loss = 0.02340471
Iteration 59, loss = 0.02365178
Iteration 60, loss = 0.02348784
Iteration 61, loss = 0.02359441
Iteration 62, loss = 0.02322551
Iteration 63, loss = 0.02335830
Iteration 64, loss = 0.02333803
Iteration 65, loss = 0.02346307
Iteration 66, loss = 0.02316112
Iteration 67, loss = 0.02302845
Iteration 68, loss = 0.02321564
Iteration 69, loss = 0.02313725
Iteration 70, loss = 0.02319417
Iteration 71, loss = 0.02298349
Iteration 72, loss = 0.02298774
Iteration 73, loss = 0.02321594
Iteration 74, loss = 0.02299571
Iteration 75, loss = 0.02311613
Iteration 76, loss = 0.02280767
Iteration 77, loss = 0.02296577
Iteration 78, loss = 0.02334211
Iteration 79, loss = 0.02301158
Iteration 80, loss = 0.02309166
Iteration 81, loss = 0.02296361
Iteration 82, loss = 0.02293318
Iteration 83, loss = 0.02283899
Iteration 84, loss = 0.02308043
Iteration 85, loss = 0.02323869
Iteration 86, loss = 0.02270339
Iteration 87, loss = 0.02270071
Iteration 88, loss = 0.02311821
Iteration 89, loss = 0.02261652
Iteration 90, loss = 0.02268040
Iteration 91, loss = 0.02270010
Iteration 92, loss = 0.02259358
Iteration 93, loss = 0.02272887
Iteration 94, loss = 0.02282548
Iteration 95, loss = 0.02262218
Iteration 96, loss = 0.02270682
Iteration 97, loss = 0.02258021
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on dropped_v1_basic_clean...
Iteration 1, loss = 0.68930861
Iteration 2, loss = 0.64614531
Iteration 3, loss = 0.58184652
Iteration 4, loss = 0.50419766
Iteration 5, loss = 0.42530140
Iteration 6, loss = 0.35170730
Iteration 7, loss = 0.28738727
Iteration 8, loss = 0.23414505
Iteration 9, loss = 0.19121665
Iteration 10, loss = 0.15778226
Iteration 11, loss = 0.13215556
Iteration 12, loss = 0.11229578
Iteration 13, loss = 0.09666422
Iteration 14, loss = 0.08464517
Iteration 15, loss = 0.07498315
Iteration 16, loss = 0.06717174
Iteration 17, loss = 0.06124684
Iteration 18, loss = 0.05609286
Iteration 19, loss = 0.05155368
Iteration 20, loss = 0.04801112
Iteration 21, loss = 0.04526078
Iteration 22, loss = 0.04248242
Iteration 23, loss = 0.04023130
Iteration 24, loss = 0.03829581
Iteration 25, loss = 0.03677575
Iteration 26, loss = 0.03534421
Iteration 27, loss = 0.03387677
Iteration 28, loss = 0.03264551
Iteration 29, loss = 0.03170633
Iteration 30, loss = 0.03089028
Iteration 31, loss = 0.02990424
Iteration 32, loss = 0.02954648
Iteration 33, loss = 0.02876954
Iteration 34, loss = 0.02797955
Iteration 35, loss = 0.02726919
Iteration 36, loss = 0.02695696
Iteration 37, loss = 0.02652130
Iteration 38, loss = 0.02608935
Iteration 39, loss = 0.02554572
Iteration 40, loss = 0.02536516
Iteration 41, loss = 0.02491027
Iteration 42, loss = 0.02483784
Iteration 43, loss = 0.02446352
Iteration 44, loss = 0.02411519
Iteration 45, loss = 0.02403055
Iteration 46, loss = 0.02377785
Iteration 47, loss = 0.02358713
Iteration 48, loss = 0.02341699
Iteration 49, loss = 0.02318503
Iteration 50, loss = 0.02300298
Iteration 51, loss = 0.02287522
Iteration 52, loss = 0.02308695
Iteration 53, loss = 0.02253050
Iteration 54, loss = 0.02262723
Iteration 55, loss = 0.02229839
Iteration 56, loss = 0.02201873
Iteration 57, loss = 0.02199022
Iteration 58, loss = 0.02199251
Iteration 59, loss = 0.02162749
Iteration 60, loss = 0.02163624
Iteration 61, loss = 0.02152430
Iteration 62, loss = 0.02139409
Iteration 63, loss = 0.02143818
Iteration 64, loss = 0.02133076
Iteration 65, loss = 0.02151787
Iteration 66, loss = 0.02136731
Iteration 67, loss = 0.02112197
Iteration 68, loss = 0.02100212
Iteration 69, loss = 0.02100602
Iteration 70, loss = 0.02076521
Iteration 71, loss = 0.02071744
Iteration 72, loss = 0.02066239
Iteration 73, loss = 0.02087066
Iteration 74, loss = 0.02060065
Iteration 75, loss = 0.02074030
Iteration 76, loss = 0.02069761
Iteration 77, loss = 0.02078717
Iteration 78, loss = 0.02061971
Iteration 79, loss = 0.02066488
Iteration 80, loss = 0.02050217
Iteration 81, loss = 0.02038973
Iteration 82, loss = 0.02083029
Iteration 83, loss = 0.02019582
Iteration 84, loss = 0.02055251
Iteration 85, loss = 0.02028260
Iteration 86, loss = 0.02042964
Iteration 87, loss = 0.02024303
Iteration 88, loss = 0.01986753
Iteration 89, loss = 0.02032829
Iteration 90, loss = 0.02003717
Iteration 91, loss = 0.02008155
Iteration 92, loss = 0.02019031
Iteration 93, loss = 0.02004615
Iteration 94, loss = 0.02008887
Iteration 95, loss = 0.02009179
Iteration 96, loss = 0.01996226
Iteration 97, loss = 0.01996633
Iteration 98, loss = 0.01987987
Iteration 99, loss = 0.01981118
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71633851
Iteration 2, loss = 0.67479570
Iteration 3, loss = 0.61082112
Iteration 4, loss = 0.52456063
Iteration 5, loss = 0.43378364
Iteration 6, loss = 0.34816139
Iteration 7, loss = 0.27563770
Iteration 8, loss = 0.21797693
Iteration 9, loss = 0.17402741
Iteration 10, loss = 0.14172406
Iteration 11, loss = 0.11730193
Iteration 12, loss = 0.09900401
Iteration 13, loss = 0.08525533
Iteration 14, loss = 0.07437870
Iteration 15, loss = 0.06622953
Iteration 16, loss = 0.05929264
Iteration 17, loss = 0.05391698
Iteration 18, loss = 0.04935505
Iteration 19, loss = 0.04561873
Iteration 20, loss = 0.04256668
Iteration 21, loss = 0.03969514
Iteration 22, loss = 0.03750841
Iteration 23, loss = 0.03565776
Iteration 24, loss = 0.03396245
Iteration 25, loss = 0.03233888
Iteration 26, loss = 0.03101824
Iteration 27, loss = 0.02983685
Iteration 28, loss = 0.02894136
Iteration 29, loss = 0.02810993
Iteration 30, loss = 0.02717660
Iteration 31, loss = 0.02670268
Iteration 32, loss = 0.02584770
Iteration 33, loss = 0.02540450
Iteration 34, loss = 0.02455835
Iteration 35, loss = 0.02397799
Iteration 36, loss = 0.02370843
Iteration 37, loss = 0.02319212
Iteration 38, loss = 0.02308023
Iteration 39, loss = 0.02255335
Iteration 40, loss = 0.02225357
Iteration 41, loss = 0.02184259
Iteration 42, loss = 0.02156879
Iteration 43, loss = 0.02121798
Iteration 44, loss = 0.02116088
Iteration 45, loss = 0.02094713
Iteration 46, loss = 0.02094159
Iteration 47, loss = 0.02068706
Iteration 48, loss = 0.02069834
Iteration 49, loss = 0.02024768
Iteration 50, loss = 0.01999406
Iteration 51, loss = 0.01994986
Iteration 52, loss = 0.01971322
Iteration 53, loss = 0.01949657
Iteration 54, loss = 0.01954778
Iteration 55, loss = 0.01948299
Iteration 56, loss = 0.01930223
Iteration 57, loss = 0.01925411
Iteration 58, loss = 0.01897817
Iteration 59, loss = 0.01887381
Iteration 60, loss = 0.01924159
Iteration 61, loss = 0.01870819
Iteration 62, loss = 0.01861068
Iteration 63, loss = 0.01871025
Iteration 64, loss = 0.01839636
Iteration 65, loss = 0.01846439
Iteration 66, loss = 0.01847759
Iteration 67, loss = 0.01838064
Iteration 68, loss = 0.01818465
Iteration 69, loss = 0.01810461
Iteration 70, loss = 0.01805873
Iteration 71, loss = 0.01808092
Iteration 72, loss = 0.01809444
Iteration 73, loss = 0.01806412
Iteration 74, loss = 0.01800540
Iteration 75, loss = 0.01791878
Iteration 76, loss = 0.01817533
Iteration 77, loss = 0.01788020
Iteration 78, loss = 0.01784681
Iteration 79, loss = 0.01789224
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68836854
Iteration 2, loss = 0.66881354
Iteration 3, loss = 0.65345586
Iteration 4, loss = 0.63897043
Iteration 5, loss = 0.62436987
Iteration 6, loss = 0.60893176
Iteration 7, loss = 0.59243606
Iteration 8, loss = 0.57471116
Iteration 9, loss = 0.55600972
Iteration 10, loss = 0.53624771
Iteration 11, loss = 0.51553378
Iteration 12, loss = 0.49413931
Iteration 13, loss = 0.47222148
Iteration 14, loss = 0.44980058
Iteration 15, loss = 0.42723614
Iteration 16, loss = 0.40466558
Iteration 17, loss = 0.38202560
Iteration 18, loss = 0.35979299
Iteration 19, loss = 0.33814991
Iteration 20, loss = 0.31729381
Iteration 21, loss = 0.29753520
Iteration 22, loss = 0.27875162
Iteration 23, loss = 0.26105770
Iteration 24, loss = 0.24452980
Iteration 25, loss = 0.22902188
Iteration 26, loss = 0.21466639
Iteration 27, loss = 0.20129746
Iteration 28, loss = 0.18880691
Iteration 29, loss = 0.17735943
Iteration 30, loss = 0.16668292
Iteration 31, loss = 0.15683132
Iteration 32, loss = 0.14774469
Iteration 33, loss = 0.13926706
Iteration 34, loss = 0.13145856
Iteration 35, loss = 0.12423089
Iteration 36, loss = 0.11756941
Iteration 37, loss = 0.11127109
Iteration 38, loss = 0.10551148
Iteration 39, loss = 0.10011806
Iteration 40, loss = 0.09509274
Iteration 41, loss = 0.09045417
Iteration 42, loss = 0.08610008
Iteration 43, loss = 0.08206690
Iteration 44, loss = 0.07827419
Iteration 45, loss = 0.07473989
Iteration 46, loss = 0.07145758
Iteration 47, loss = 0.06838625
Iteration 48, loss = 0.06551120
Iteration 49, loss = 0.06288816
Iteration 50, loss = 0.06032024
Iteration 51, loss = 0.05797257
Iteration 52, loss = 0.05570789
Iteration 53, loss = 0.05368309
Iteration 54, loss = 0.05169470
Iteration 55, loss = 0.04984596
Iteration 56, loss = 0.04810033
Iteration 57, loss = 0.04643146
Iteration 58, loss = 0.04489037
Iteration 59, loss = 0.04343372
Iteration 60, loss = 0.04202211
Iteration 61, loss = 0.04070637
Iteration 62, loss = 0.03944028
Iteration 63, loss = 0.03826979
Iteration 64, loss = 0.03712134
Iteration 65, loss = 0.03604866
Iteration 66, loss = 0.03504876
Iteration 67, loss = 0.03406724
Iteration 68, loss = 0.03310764
Iteration 69, loss = 0.03224830
Iteration 70, loss = 0.03138408
Iteration 71, loss = 0.03056582
Iteration 72, loss = 0.02980184
Iteration 73, loss = 0.02902275
Iteration 74, loss = 0.02832792
Iteration 75, loss = 0.02762541
Iteration 76, loss = 0.02694460
Iteration 77, loss = 0.02629717
Iteration 78, loss = 0.02569533
Iteration 79, loss = 0.02510250
Iteration 80, loss = 0.02458896
Iteration 81, loss = 0.02401913
Iteration 82, loss = 0.02355011
Iteration 83, loss = 0.02303586
Iteration 84, loss = 0.02256918
Iteration 85, loss = 0.02210575
Iteration 86, loss = 0.02167276
Iteration 87, loss = 0.02126455
Iteration 88, loss = 0.02081858
Iteration 89, loss = 0.02042434
Iteration 90, loss = 0.02005844
Iteration 91, loss = 0.01966169
Iteration 92, loss = 0.01930322
Iteration 93, loss = 0.01894781
Iteration 94, loss = 0.01860041
Iteration 95, loss = 0.01829351
Iteration 96, loss = 0.01797578
Iteration 97, loss = 0.01767019
Iteration 98, loss = 0.01734385
Iteration 99, loss = 0.01708661
Iteration 100, loss = 0.01678433
Iteration 101, loss = 0.01651844
Iteration 102, loss = 0.01626464
Iteration 103, loss = 0.01600644
Iteration 104, loss = 0.01579303
Iteration 105, loss = 0.01553385
Iteration 106, loss = 0.01532846
Iteration 107, loss = 0.01508844
Iteration 108, loss = 0.01488052
Iteration 109, loss = 0.01468813
Iteration 110, loss = 0.01446862
Iteration 111, loss = 0.01428459
Iteration 112, loss = 0.01407468
Iteration 113, loss = 0.01388450
Iteration 114, loss = 0.01370038
Iteration 115, loss = 0.01353038
Iteration 116, loss = 0.01332992
Iteration 117, loss = 0.01314519
Iteration 118, loss = 0.01301625
Iteration 119, loss = 0.01284242
Iteration 120, loss = 0.01266486
Iteration 121, loss = 0.01251963
Iteration 122, loss = 0.01235803
Iteration 123, loss = 0.01221810
Iteration 124, loss = 0.01208531
Iteration 125, loss = 0.01196911
Iteration 126, loss = 0.01183898
Iteration 127, loss = 0.01168357
Iteration 128, loss = 0.01155655
Iteration 129, loss = 0.01146259
Iteration 130, loss = 0.01131106
Iteration 131, loss = 0.01118333
Iteration 132, loss = 0.01108577
Iteration 133, loss = 0.01098415
Iteration 134, loss = 0.01086971
Iteration 135, loss = 0.01079436
Iteration 136, loss = 0.01070128
Iteration 137, loss = 0.01061450
Iteration 138, loss = 0.01051003
Iteration 139, loss = 0.01041807
Iteration 140, loss = 0.01031042
Iteration 141, loss = 0.01022123
Iteration 142, loss = 0.01007788
Iteration 143, loss = 0.00998596
Iteration 144, loss = 0.00992338
Iteration 145, loss = 0.00979812
Iteration 146, loss = 0.00973951
Iteration 147, loss = 0.00963858
Iteration 148, loss = 0.00956544
Iteration 149, loss = 0.00948352
Iteration 150, loss = 0.00943444
Iteration 151, loss = 0.00935767
Iteration 152, loss = 0.00927701
Iteration 153, loss = 0.00920155
Iteration 154, loss = 0.00913897
Iteration 155, loss = 0.00905921
Iteration 156, loss = 0.00899595
Iteration 157, loss = 0.00891644
Iteration 158, loss = 0.00884321
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67701807
Iteration 2, loss = 0.64905196
Iteration 3, loss = 0.61927874
Iteration 4, loss = 0.58344264
Iteration 5, loss = 0.54196886
Iteration 6, loss = 0.49651239
Iteration 7, loss = 0.44974671
Iteration 8, loss = 0.40339785
Iteration 9, loss = 0.35866802
Iteration 10, loss = 0.31697482
Iteration 11, loss = 0.27938694
Iteration 12, loss = 0.24553606
Iteration 13, loss = 0.21596822
Iteration 14, loss = 0.19040440
Iteration 15, loss = 0.16836251
Iteration 16, loss = 0.14966464
Iteration 17, loss = 0.13359693
Iteration 18, loss = 0.11990415
Iteration 19, loss = 0.10818990
Iteration 20, loss = 0.09820736
Iteration 21, loss = 0.08935482
Iteration 22, loss = 0.08203758
Iteration 23, loss = 0.07548509
Iteration 24, loss = 0.06986500
Iteration 25, loss = 0.06491979
Iteration 26, loss = 0.06049852
Iteration 27, loss = 0.05656942
Iteration 28, loss = 0.05312891
Iteration 29, loss = 0.05011381
Iteration 30, loss = 0.04730179
Iteration 31, loss = 0.04480815
Iteration 32, loss = 0.04262469
Iteration 33, loss = 0.04067890
Iteration 34, loss = 0.03900270
Iteration 35, loss = 0.03726096
Iteration 36, loss = 0.03586956
Iteration 37, loss = 0.03454954
Iteration 38, loss = 0.03313050
Iteration 39, loss = 0.03189510
Iteration 40, loss = 0.03080169
Iteration 41, loss = 0.02960019
Iteration 42, loss = 0.02878111
Iteration 43, loss = 0.02789094
Iteration 44, loss = 0.02717872
Iteration 45, loss = 0.02633452
Iteration 46, loss = 0.02565196
Iteration 47, loss = 0.02495763
Iteration 48, loss = 0.02424930
Iteration 49, loss = 0.02372231
Iteration 50, loss = 0.02320766
Iteration 51, loss = 0.02270946
Iteration 52, loss = 0.02222433
Iteration 53, loss = 0.02178875
Iteration 54, loss = 0.02135307
Iteration 55, loss = 0.02097261
Iteration 56, loss = 0.02062876
Iteration 57, loss = 0.02025831
Iteration 58, loss = 0.01992835
Iteration 59, loss = 0.01954004
Iteration 60, loss = 0.01944013
Iteration 61, loss = 0.01924797
Iteration 62, loss = 0.01907316
Iteration 63, loss = 0.01889661
Iteration 64, loss = 0.01860006
Iteration 65, loss = 0.01832483
Iteration 66, loss = 0.01810232
Iteration 67, loss = 0.01777980
Iteration 68, loss = 0.01758686
Iteration 69, loss = 0.01763701
Iteration 70, loss = 0.01743648
Iteration 71, loss = 0.01721905
Iteration 72, loss = 0.01698528
Iteration 73, loss = 0.01673355
Iteration 74, loss = 0.01657938
Iteration 75, loss = 0.01633694
Iteration 76, loss = 0.01613632
Iteration 77, loss = 0.01596506
Iteration 78, loss = 0.01587649
Iteration 79, loss = 0.01575283
Iteration 80, loss = 0.01573840
Iteration 81, loss = 0.01550391
Iteration 82, loss = 0.01538784
Iteration 83, loss = 0.01538978
Iteration 84, loss = 0.01525246
Iteration 85, loss = 0.01502353
Iteration 86, loss = 0.01486551
Iteration 87, loss = 0.01483873
Iteration 88, loss = 0.01478379
Iteration 89, loss = 0.01463064
Iteration 90, loss = 0.01462308
Iteration 91, loss = 0.01451766
Iteration 92, loss = 0.01450910
Iteration 93, loss = 0.01443324
Iteration 94, loss = 0.01439312
Iteration 95, loss = 0.01432129
Iteration 96, loss = 0.01424004
Iteration 97, loss = 0.01411322
Iteration 98, loss = 0.01402958
Iteration 99, loss = 0.01395243
Iteration 100, loss = 0.01388193
Iteration 101, loss = 0.01387910
Iteration 102, loss = 0.01386246
Iteration 103, loss = 0.01374111
Iteration 104, loss = 0.01365552
Iteration 105, loss = 0.01355732
Iteration 106, loss = 0.01359970
Iteration 107, loss = 0.01356208
Iteration 108, loss = 0.01348515
Iteration 109, loss = 0.01346044
Iteration 110, loss = 0.01346574
Iteration 111, loss = 0.01347629
Iteration 112, loss = 0.01341938
Iteration 113, loss = 0.01330772
Iteration 114, loss = 0.01323481
Iteration 115, loss = 0.01323462
Iteration 116, loss = 0.01311155
Iteration 117, loss = 0.01319251
Iteration 118, loss = 0.01311687
Iteration 119, loss = 0.01303823
Iteration 120, loss = 0.01305591
Iteration 121, loss = 0.01296666
Iteration 122, loss = 0.01313852
Iteration 123, loss = 0.01299267
Iteration 124, loss = 0.01320500
Iteration 125, loss = 0.01326611
Iteration 126, loss = 0.01325674
Iteration 127, loss = 0.01318791
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70334277
Iteration 2, loss = 0.67952927
Iteration 3, loss = 0.65215442
Iteration 4, loss = 0.61747110
Iteration 5, loss = 0.57577466
Iteration 6, loss = 0.52851909
Iteration 7, loss = 0.47879730
Iteration 8, loss = 0.42905352
Iteration 9, loss = 0.38093854
Iteration 10, loss = 0.33605651
Iteration 11, loss = 0.29502745
Iteration 12, loss = 0.25842184
Iteration 13, loss = 0.22622793
Iteration 14, loss = 0.19851584
Iteration 15, loss = 0.17471484
Iteration 16, loss = 0.15448448
Iteration 17, loss = 0.13728668
Iteration 18, loss = 0.12265116
Iteration 19, loss = 0.11016931
Iteration 20, loss = 0.09955886
Iteration 21, loss = 0.09039722
Iteration 22, loss = 0.08252296
Iteration 23, loss = 0.07568079
Iteration 24, loss = 0.06953552
Iteration 25, loss = 0.06461184
Iteration 26, loss = 0.05997205
Iteration 27, loss = 0.05610454
Iteration 28, loss = 0.05240541
Iteration 29, loss = 0.04929275
Iteration 30, loss = 0.04636440
Iteration 31, loss = 0.04386065
Iteration 32, loss = 0.04159460
Iteration 33, loss = 0.03944598
Iteration 34, loss = 0.03758867
Iteration 35, loss = 0.03589132
Iteration 36, loss = 0.03419527
Iteration 37, loss = 0.03292300
Iteration 38, loss = 0.03154739
Iteration 39, loss = 0.03058455
Iteration 40, loss = 0.02939003
Iteration 41, loss = 0.02833991
Iteration 42, loss = 0.02741569
Iteration 43, loss = 0.02637913
Iteration 44, loss = 0.02556397
Iteration 45, loss = 0.02466594
Iteration 46, loss = 0.02393346
Iteration 47, loss = 0.02337402
Iteration 48, loss = 0.02260623
Iteration 49, loss = 0.02212722
Iteration 50, loss = 0.02156683
Iteration 51, loss = 0.02115043
Iteration 52, loss = 0.02069284
Iteration 53, loss = 0.02028753
Iteration 54, loss = 0.01988193
Iteration 55, loss = 0.01939631
Iteration 56, loss = 0.01898772
Iteration 57, loss = 0.01852904
Iteration 58, loss = 0.01815107
Iteration 59, loss = 0.01793128
Iteration 60, loss = 0.01764787
Iteration 61, loss = 0.01728857
Iteration 62, loss = 0.01691574
Iteration 63, loss = 0.01711628
Iteration 64, loss = 0.01653364
Iteration 65, loss = 0.01629424
Iteration 66, loss = 0.01604925
Iteration 67, loss = 0.01595432
Iteration 68, loss = 0.01587467
Iteration 69, loss = 0.01564586
Iteration 70, loss = 0.01520744
Iteration 71, loss = 0.01508283
Iteration 72, loss = 0.01491641
Iteration 73, loss = 0.01471808
Iteration 74, loss = 0.01462182
Iteration 75, loss = 0.01440449
Iteration 76, loss = 0.01431482
Iteration 77, loss = 0.01431242
Iteration 78, loss = 0.01402276
Iteration 79, loss = 0.01444086
Iteration 80, loss = 0.01453938
Iteration 81, loss = 0.01443470
Iteration 82, loss = 0.01419611
Iteration 83, loss = 0.01384869
Iteration 84, loss = 0.01361401
Iteration 85, loss = 0.01335598
Iteration 86, loss = 0.01322403
Iteration 87, loss = 0.01304029
Iteration 88, loss = 0.01299432
Iteration 89, loss = 0.01286323
Iteration 90, loss = 0.01276138
Iteration 91, loss = 0.01263209
Iteration 92, loss = 0.01261414
Iteration 93, loss = 0.01255545
Iteration 94, loss = 0.01252035
Iteration 95, loss = 0.01256217
Iteration 96, loss = 0.01245704
Iteration 97, loss = 0.01236712
Iteration 98, loss = 0.01235213
Iteration 99, loss = 0.01228099
Iteration 100, loss = 0.01214942
Iteration 101, loss = 0.01211165
Iteration 102, loss = 0.01195136
Iteration 103, loss = 0.01194381
Iteration 104, loss = 0.01187249
Iteration 105, loss = 0.01177056
Iteration 106, loss = 0.01177242
Iteration 107, loss = 0.01174711
Iteration 108, loss = 0.01179529
Iteration 109, loss = 0.01164432
Iteration 110, loss = 0.01167455
Iteration 111, loss = 0.01159163
Iteration 112, loss = 0.01151924
Iteration 113, loss = 0.01145321
Iteration 114, loss = 0.01143174
Iteration 115, loss = 0.01134981
Iteration 116, loss = 0.01135692
Iteration 117, loss = 0.01125586
Iteration 118, loss = 0.01120533
Iteration 119, loss = 0.01119580
Iteration 120, loss = 0.01113734
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67694945
Iteration 2, loss = 0.62547952
Iteration 3, loss = 0.54240918
Iteration 4, loss = 0.44507636
Iteration 5, loss = 0.35550355
Iteration 6, loss = 0.27970264
Iteration 7, loss = 0.21990991
Iteration 8, loss = 0.17491169
Iteration 9, loss = 0.14154833
Iteration 10, loss = 0.11700815
Iteration 11, loss = 0.09873226
Iteration 12, loss = 0.08498949
Iteration 13, loss = 0.07432235
Iteration 14, loss = 0.06609060
Iteration 15, loss = 0.05982669
Iteration 16, loss = 0.05438721
Iteration 17, loss = 0.05003278
Iteration 18, loss = 0.04659410
Iteration 19, loss = 0.04355264
Iteration 20, loss = 0.04098142
Iteration 21, loss = 0.03875058
Iteration 22, loss = 0.03690814
Iteration 23, loss = 0.03545558
Iteration 24, loss = 0.03410731
Iteration 25, loss = 0.03275983
Iteration 26, loss = 0.03180326
Iteration 27, loss = 0.03094557
Iteration 28, loss = 0.03013354
Iteration 29, loss = 0.02932816
Iteration 30, loss = 0.02848423
Iteration 31, loss = 0.02796291
Iteration 32, loss = 0.02749548
Iteration 33, loss = 0.02691938
Iteration 34, loss = 0.02648099
Iteration 35, loss = 0.02606469
Iteration 36, loss = 0.02561950
Iteration 37, loss = 0.02534142
Iteration 38, loss = 0.02513118
Iteration 39, loss = 0.02500172
Iteration 40, loss = 0.02464116
Iteration 41, loss = 0.02415806
Iteration 42, loss = 0.02391692
Iteration 43, loss = 0.02379419
Iteration 44, loss = 0.02368682
Iteration 45, loss = 0.02377078
Iteration 46, loss = 0.02356030
Iteration 47, loss = 0.02317685
Iteration 48, loss = 0.02297099
Iteration 49, loss = 0.02285243
Iteration 50, loss = 0.02288917
Iteration 51, loss = 0.02275066
Iteration 52, loss = 0.02253320
Iteration 53, loss = 0.02237690
Iteration 54, loss = 0.02235279
Iteration 55, loss = 0.02211240
Iteration 56, loss = 0.02215869
Iteration 57, loss = 0.02218046
Iteration 58, loss = 0.02202749
Iteration 59, loss = 0.02188422
Iteration 60, loss = 0.02181085
Iteration 61, loss = 0.02199346
Iteration 62, loss = 0.02173654
Iteration 63, loss = 0.02175604
Iteration 64, loss = 0.02154268
Iteration 65, loss = 0.02157214
Iteration 66, loss = 0.02142394
Iteration 67, loss = 0.02139457
Iteration 68, loss = 0.02162785
Iteration 69, loss = 0.02161195
Iteration 70, loss = 0.02132731
Iteration 71, loss = 0.02152314
Iteration 72, loss = 0.02113745
Iteration 73, loss = 0.02136391
Iteration 74, loss = 0.02124531
Iteration 75, loss = 0.02142784
Iteration 76, loss = 0.02116904
Iteration 77, loss = 0.02114897
Iteration 78, loss = 0.02086603
Iteration 79, loss = 0.02107178
Iteration 80, loss = 0.02097777
Iteration 81, loss = 0.02113600
Iteration 82, loss = 0.02092389
Iteration 83, loss = 0.02110385
Iteration 84, loss = 0.02078753
Iteration 85, loss = 0.02070979
Iteration 86, loss = 0.02086813
Iteration 87, loss = 0.02089952
Iteration 88, loss = 0.02080294
Iteration 89, loss = 0.02082020
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72092811
Iteration 2, loss = 0.67818027
Iteration 3, loss = 0.60851642
Iteration 4, loss = 0.50822005
Iteration 5, loss = 0.40643781
Iteration 6, loss = 0.31430509
Iteration 7, loss = 0.24047123
Iteration 8, loss = 0.18650506
Iteration 9, loss = 0.14740869
Iteration 10, loss = 0.11996323
Iteration 11, loss = 0.09996421
Iteration 12, loss = 0.08529607
Iteration 13, loss = 0.07428357
Iteration 14, loss = 0.06582683
Iteration 15, loss = 0.05926700
Iteration 16, loss = 0.05400243
Iteration 17, loss = 0.04955055
Iteration 18, loss = 0.04603695
Iteration 19, loss = 0.04316276
Iteration 20, loss = 0.04048736
Iteration 21, loss = 0.03858381
Iteration 22, loss = 0.03659267
Iteration 23, loss = 0.03528782
Iteration 24, loss = 0.03381185
Iteration 25, loss = 0.03246582
Iteration 26, loss = 0.03154911
Iteration 27, loss = 0.03062491
Iteration 28, loss = 0.02975767
Iteration 29, loss = 0.02894200
Iteration 30, loss = 0.02839956
Iteration 31, loss = 0.02771864
Iteration 32, loss = 0.02715290
Iteration 33, loss = 0.02697794
Iteration 34, loss = 0.02643017
Iteration 35, loss = 0.02616715
Iteration 36, loss = 0.02583863
Iteration 37, loss = 0.02518936
Iteration 38, loss = 0.02508121
Iteration 39, loss = 0.02481721
Iteration 40, loss = 0.02438819
Iteration 41, loss = 0.02419808
Iteration 42, loss = 0.02386916
Iteration 43, loss = 0.02360464
Iteration 44, loss = 0.02340683
Iteration 45, loss = 0.02333223
Iteration 46, loss = 0.02300422
Iteration 47, loss = 0.02290633
Iteration 48, loss = 0.02272194
Iteration 49, loss = 0.02258713
Iteration 50, loss = 0.02250309
Iteration 51, loss = 0.02211394
Iteration 52, loss = 0.02238319
Iteration 53, loss = 0.02206262
Iteration 54, loss = 0.02212013
Iteration 55, loss = 0.02217595
Iteration 56, loss = 0.02182545
Iteration 57, loss = 0.02169268
Iteration 58, loss = 0.02180694
Iteration 59, loss = 0.02145432
Iteration 60, loss = 0.02131967
Iteration 61, loss = 0.02163342
Iteration 62, loss = 0.02121760
Iteration 63, loss = 0.02125917
Iteration 64, loss = 0.02114893
Iteration 65, loss = 0.02100494
Iteration 66, loss = 0.02110557
Iteration 67, loss = 0.02113512
Iteration 68, loss = 0.02144908
Iteration 69, loss = 0.02145753
Iteration 70, loss = 0.02104769
Iteration 71, loss = 0.02092322
Iteration 72, loss = 0.02093985
Iteration 73, loss = 0.02305866
Iteration 74, loss = 0.02393380
Iteration 75, loss = 0.02348421
Iteration 76, loss = 0.02303809
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68092522
Iteration 2, loss = 0.66433413
Iteration 3, loss = 0.64775339
Iteration 4, loss = 0.62807885
Iteration 5, loss = 0.60413099
Iteration 6, loss = 0.57605609
Iteration 7, loss = 0.54394099
Iteration 8, loss = 0.50885132
Iteration 9, loss = 0.47246230
Iteration 10, loss = 0.43589079
Iteration 11, loss = 0.39991761
Iteration 12, loss = 0.36576446
Iteration 13, loss = 0.33315392
Iteration 14, loss = 0.30283106
Iteration 15, loss = 0.27472372
Iteration 16, loss = 0.24903798
Iteration 17, loss = 0.22564578
Iteration 18, loss = 0.20451379
Iteration 19, loss = 0.18570776
Iteration 20, loss = 0.16884409
Iteration 21, loss = 0.15383511
Iteration 22, loss = 0.14056779
Iteration 23, loss = 0.12875788
Iteration 24, loss = 0.11831601
Iteration 25, loss = 0.10903052
Iteration 26, loss = 0.10072184
Iteration 27, loss = 0.09343055
Iteration 28, loss = 0.08691137
Iteration 29, loss = 0.08104764
Iteration 30, loss = 0.07581190
Iteration 31, loss = 0.07112111
Iteration 32, loss = 0.06685067
Iteration 33, loss = 0.06305071
Iteration 34, loss = 0.05952159
Iteration 35, loss = 0.05637411
Iteration 36, loss = 0.05352965
Iteration 37, loss = 0.05086632
Iteration 38, loss = 0.04844860
Iteration 39, loss = 0.04624528
Iteration 40, loss = 0.04427770
Iteration 41, loss = 0.04238586
Iteration 42, loss = 0.04064294
Iteration 43, loss = 0.03900306
Iteration 44, loss = 0.03752426
Iteration 45, loss = 0.03612410
Iteration 46, loss = 0.03487026
Iteration 47, loss = 0.03364606
Iteration 48, loss = 0.03250152
Iteration 49, loss = 0.03148230
Iteration 50, loss = 0.03050296
Iteration 51, loss = 0.02953910
Iteration 52, loss = 0.02869195
Iteration 53, loss = 0.02787089
Iteration 54, loss = 0.02712809
Iteration 55, loss = 0.02637765
Iteration 56, loss = 0.02572157
Iteration 57, loss = 0.02509120
Iteration 58, loss = 0.02445238
Iteration 59, loss = 0.02387156
Iteration 60, loss = 0.02336942
Iteration 61, loss = 0.02291321
Iteration 62, loss = 0.02233814
Iteration 63, loss = 0.02182538
Iteration 64, loss = 0.02146175
Iteration 65, loss = 0.02096805
Iteration 66, loss = 0.02060078
Iteration 67, loss = 0.02018296
Iteration 68, loss = 0.01985027
Iteration 69, loss = 0.01948869
Iteration 70, loss = 0.01912647
Iteration 71, loss = 0.01882564
Iteration 72, loss = 0.01855107
Iteration 73, loss = 0.01818099
Iteration 74, loss = 0.01793019
Iteration 75, loss = 0.01767110
Iteration 76, loss = 0.01742392
Iteration 77, loss = 0.01718649
Iteration 78, loss = 0.01693161
Iteration 79, loss = 0.01670048
Iteration 80, loss = 0.01653811
Iteration 81, loss = 0.01623164
Iteration 82, loss = 0.01605923
Iteration 83, loss = 0.01583497
Iteration 84, loss = 0.01566072
Iteration 85, loss = 0.01550379
Iteration 86, loss = 0.01530553
Iteration 87, loss = 0.01513401
Iteration 88, loss = 0.01498268
Iteration 89, loss = 0.01486493
Iteration 90, loss = 0.01471616
Iteration 91, loss = 0.01453996
Iteration 92, loss = 0.01438444
Iteration 93, loss = 0.01425769
Iteration 94, loss = 0.01411179
Iteration 95, loss = 0.01400456
Iteration 96, loss = 0.01386554
Iteration 97, loss = 0.01378615
Iteration 98, loss = 0.01361383
Iteration 99, loss = 0.01356222
Iteration 100, loss = 0.01341454
Iteration 101, loss = 0.01331013
Iteration 102, loss = 0.01322274
Iteration 103, loss = 0.01315770
Iteration 104, loss = 0.01303171
Iteration 105, loss = 0.01294279
Iteration 106, loss = 0.01284737
Iteration 107, loss = 0.01277379
Iteration 108, loss = 0.01265793
Iteration 109, loss = 0.01256294
Iteration 110, loss = 0.01249159
Iteration 111, loss = 0.01250088
Iteration 112, loss = 0.01236488
Iteration 113, loss = 0.01229532
Iteration 114, loss = 0.01217308
Iteration 115, loss = 0.01212819
Iteration 116, loss = 0.01203569
Iteration 117, loss = 0.01201644
Iteration 118, loss = 0.01190661
Iteration 119, loss = 0.01188923
Iteration 120, loss = 0.01179710
Iteration 121, loss = 0.01177222
Iteration 122, loss = 0.01164950
Iteration 123, loss = 0.01165089
Iteration 124, loss = 0.01159049
Iteration 125, loss = 0.01151200
Iteration 126, loss = 0.01143503
Iteration 127, loss = 0.01141599
Iteration 128, loss = 0.01137284
Iteration 129, loss = 0.01133021
Iteration 130, loss = 0.01127380
Iteration 131, loss = 0.01122245
Iteration 132, loss = 0.01124188
Iteration 133, loss = 0.01113893
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67836821
Iteration 2, loss = 0.64070583
Iteration 3, loss = 0.58907918
Iteration 4, loss = 0.52222414
Iteration 5, loss = 0.45175437
Iteration 6, loss = 0.38400323
Iteration 7, loss = 0.32239056
Iteration 8, loss = 0.26886107
Iteration 9, loss = 0.22427455
Iteration 10, loss = 0.18779408
Iteration 11, loss = 0.15853128
Iteration 12, loss = 0.13509702
Iteration 13, loss = 0.11630322
Iteration 14, loss = 0.10130803
Iteration 15, loss = 0.08925140
Iteration 16, loss = 0.07926985
Iteration 17, loss = 0.07113845
Iteration 18, loss = 0.06436173
Iteration 19, loss = 0.05872646
Iteration 20, loss = 0.05396976
Iteration 21, loss = 0.04980778
Iteration 22, loss = 0.04638722
Iteration 23, loss = 0.04327051
Iteration 24, loss = 0.04055294
Iteration 25, loss = 0.03837647
Iteration 26, loss = 0.03634201
Iteration 27, loss = 0.03442705
Iteration 28, loss = 0.03293082
Iteration 29, loss = 0.03150564
Iteration 30, loss = 0.03019344
Iteration 31, loss = 0.02902451
Iteration 32, loss = 0.02794278
Iteration 33, loss = 0.02711673
Iteration 34, loss = 0.02625187
Iteration 35, loss = 0.02536091
Iteration 36, loss = 0.02465127
Iteration 37, loss = 0.02404532
Iteration 38, loss = 0.02341310
Iteration 39, loss = 0.02294809
Iteration 40, loss = 0.02245434
Iteration 41, loss = 0.02203175
Iteration 42, loss = 0.02144588
Iteration 43, loss = 0.02104209
Iteration 44, loss = 0.02072289
Iteration 45, loss = 0.02033760
Iteration 46, loss = 0.02006844
Iteration 47, loss = 0.01986365
Iteration 48, loss = 0.01946508
Iteration 49, loss = 0.01927560
Iteration 50, loss = 0.01898839
Iteration 51, loss = 0.01873468
Iteration 52, loss = 0.01849312
Iteration 53, loss = 0.01840713
Iteration 54, loss = 0.01807918
Iteration 55, loss = 0.01799577
Iteration 56, loss = 0.01761727
Iteration 57, loss = 0.01779663
Iteration 58, loss = 0.01739111
Iteration 59, loss = 0.01724875
Iteration 60, loss = 0.01726340
Iteration 61, loss = 0.01698525
Iteration 62, loss = 0.01680095
Iteration 63, loss = 0.01672750
Iteration 64, loss = 0.01672105
Iteration 65, loss = 0.01659438
Iteration 66, loss = 0.01642736
Iteration 67, loss = 0.01638200
Iteration 68, loss = 0.01630537
Iteration 69, loss = 0.01601482
Iteration 70, loss = 0.01599886
Iteration 71, loss = 0.01594436
Iteration 72, loss = 0.01590256
Iteration 73, loss = 0.01576599
Iteration 74, loss = 0.01578999
Iteration 75, loss = 0.01574417
Iteration 76, loss = 0.01562252
Iteration 77, loss = 0.01585980
Iteration 78, loss = 0.01547843
Iteration 79, loss = 0.01552732
Iteration 80, loss = 0.01545902
Iteration 81, loss = 0.01529513
Iteration 82, loss = 0.01520521
Iteration 83, loss = 0.01514412
Iteration 84, loss = 0.01507971
Iteration 85, loss = 0.01512414
Iteration 86, loss = 0.01513275
Iteration 87, loss = 0.01497785
Iteration 88, loss = 0.01501463
Iteration 89, loss = 0.01485481
Iteration 90, loss = 0.01484330
Iteration 91, loss = 0.01482790
Iteration 92, loss = 0.01489251
Iteration 93, loss = 0.01470461
Iteration 94, loss = 0.01473575
Iteration 95, loss = 0.01475438
Iteration 96, loss = 0.01465658
Iteration 97, loss = 0.01464914
Iteration 98, loss = 0.01451296
Iteration 99, loss = 0.01459994
Iteration 100, loss = 0.01461506
Iteration 101, loss = 0.01443270
Iteration 102, loss = 0.01460095
Iteration 103, loss = 0.01451801
Iteration 104, loss = 0.01437093
Iteration 105, loss = 0.01443804
Iteration 106, loss = 0.01437925
Iteration 107, loss = 0.01436537
Iteration 108, loss = 0.01435718
Iteration 109, loss = 0.01421129
Iteration 110, loss = 0.01426663
Iteration 111, loss = 0.01437693
Iteration 112, loss = 0.01432574
Iteration 113, loss = 0.01420835
Iteration 114, loss = 0.01433530
Iteration 115, loss = 0.01422254
Iteration 116, loss = 0.01424646
Iteration 117, loss = 0.01407720
Iteration 118, loss = 0.01428391
Iteration 119, loss = 0.01431999
Iteration 120, loss = 0.01408751
Iteration 121, loss = 0.01414603
Iteration 122, loss = 0.01404528
Iteration 123, loss = 0.01403469
Iteration 124, loss = 0.01403936
Iteration 125, loss = 0.01412238
Iteration 126, loss = 0.01392833
Iteration 127, loss = 0.01433550
Iteration 128, loss = 0.01396116
Iteration 129, loss = 0.01388755
Iteration 130, loss = 0.01379248
Iteration 131, loss = 0.01389321
Iteration 132, loss = 0.01391647
Iteration 133, loss = 0.01382257
Iteration 134, loss = 0.01382933
Iteration 135, loss = 0.01383396
Iteration 136, loss = 0.01395537
Iteration 137, loss = 0.01385091
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70793670
Iteration 2, loss = 0.66462683
Iteration 3, loss = 0.60649275
Iteration 4, loss = 0.53650112
Iteration 5, loss = 0.46161421
Iteration 6, loss = 0.39004575
Iteration 7, loss = 0.32564671
Iteration 8, loss = 0.27030400
Iteration 9, loss = 0.22414687
Iteration 10, loss = 0.18695382
Iteration 11, loss = 0.15721225
Iteration 12, loss = 0.13363801
Iteration 13, loss = 0.11506043
Iteration 14, loss = 0.10025496
Iteration 15, loss = 0.08822049
Iteration 16, loss = 0.07882826
Iteration 17, loss = 0.07069094
Iteration 18, loss = 0.06406615
Iteration 19, loss = 0.05861697
Iteration 20, loss = 0.05410749
Iteration 21, loss = 0.05013738
Iteration 22, loss = 0.04673299
Iteration 23, loss = 0.04395797
Iteration 24, loss = 0.04145919
Iteration 25, loss = 0.03932648
Iteration 26, loss = 0.03730760
Iteration 27, loss = 0.03561747
Iteration 28, loss = 0.03409645
Iteration 29, loss = 0.03271677
Iteration 30, loss = 0.03161252
Iteration 31, loss = 0.03051902
Iteration 32, loss = 0.02953986
Iteration 33, loss = 0.02878271
Iteration 34, loss = 0.02794874
Iteration 35, loss = 0.02713415
Iteration 36, loss = 0.02654169
Iteration 37, loss = 0.02614704
Iteration 38, loss = 0.02533397
Iteration 39, loss = 0.02501973
Iteration 40, loss = 0.02439223
Iteration 41, loss = 0.02396310
Iteration 42, loss = 0.02353655
Iteration 43, loss = 0.02328587
Iteration 44, loss = 0.02283111
Iteration 45, loss = 0.02250627
Iteration 46, loss = 0.02215268
Iteration 47, loss = 0.02201901
Iteration 48, loss = 0.02172088
Iteration 49, loss = 0.02133166
Iteration 50, loss = 0.02125647
Iteration 51, loss = 0.02096656
Iteration 52, loss = 0.02083397
Iteration 53, loss = 0.02057157
Iteration 54, loss = 0.02038451
Iteration 55, loss = 0.02021265
Iteration 56, loss = 0.02007685
Iteration 57, loss = 0.02009793
Iteration 58, loss = 0.01969653
Iteration 59, loss = 0.01960300
Iteration 60, loss = 0.01942724
Iteration 61, loss = 0.01953725
Iteration 62, loss = 0.01922964
Iteration 63, loss = 0.01928212
Iteration 64, loss = 0.01908512
Iteration 65, loss = 0.01900944
Iteration 66, loss = 0.01888506
Iteration 67, loss = 0.01883137
Iteration 68, loss = 0.01875953
Iteration 69, loss = 0.01869665
Iteration 70, loss = 0.01867161
Iteration 71, loss = 0.01854449
Iteration 72, loss = 0.01855303
Iteration 73, loss = 0.01843527
Iteration 74, loss = 0.01817175
Iteration 75, loss = 0.01824788
Iteration 76, loss = 0.01817206
Iteration 77, loss = 0.01819113
Iteration 78, loss = 0.01807461
Iteration 79, loss = 0.01820675
Iteration 80, loss = 0.01804736
Iteration 81, loss = 0.01795042
Iteration 82, loss = 0.01804796
Iteration 83, loss = 0.01795725
Iteration 84, loss = 0.01771334
Iteration 85, loss = 0.01777607
Iteration 86, loss = 0.01771129
Iteration 87, loss = 0.01768349
Iteration 88, loss = 0.01752562
Iteration 89, loss = 0.01765911
Iteration 90, loss = 0.01765223
Iteration 91, loss = 0.01760770
Iteration 92, loss = 0.01753255
Iteration 93, loss = 0.01750398
Iteration 94, loss = 0.01746322
Iteration 95, loss = 0.01747639
Iteration 96, loss = 0.01749192
Iteration 97, loss = 0.01739402
Iteration 98, loss = 0.01740830
Iteration 99, loss = 0.01743619
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70610859
Iteration 2, loss = 0.66194021
Iteration 3, loss = 0.60672399
Iteration 4, loss = 0.54231760
Iteration 5, loss = 0.47152790
Iteration 6, loss = 0.40018192
Iteration 7, loss = 0.33363375
Iteration 8, loss = 0.27571449
Iteration 9, loss = 0.22790320
Iteration 10, loss = 0.18872822
Iteration 11, loss = 0.15790690
Iteration 12, loss = 0.13359456
Iteration 13, loss = 0.11439168
Iteration 14, loss = 0.09901417
Iteration 15, loss = 0.08671341
Iteration 16, loss = 0.07694136
Iteration 17, loss = 0.06874262
Iteration 18, loss = 0.06206662
Iteration 19, loss = 0.05635649
Iteration 20, loss = 0.05172235
Iteration 21, loss = 0.04767807
Iteration 22, loss = 0.04421619
Iteration 23, loss = 0.04122851
Iteration 24, loss = 0.03864395
Iteration 25, loss = 0.03646350
Iteration 26, loss = 0.03435104
Iteration 27, loss = 0.03268499
Iteration 28, loss = 0.03127850
Iteration 29, loss = 0.02975662
Iteration 30, loss = 0.02857400
Iteration 31, loss = 0.02763369
Iteration 32, loss = 0.02647894
Iteration 33, loss = 0.02547338
Iteration 34, loss = 0.02463396
Iteration 35, loss = 0.02379189
Iteration 36, loss = 0.02314904
Iteration 37, loss = 0.02260659
Iteration 38, loss = 0.02198842
Iteration 39, loss = 0.02151059
Iteration 40, loss = 0.02115376
Iteration 41, loss = 0.02048371
Iteration 42, loss = 0.02000698
Iteration 43, loss = 0.01979786
Iteration 44, loss = 0.01934675
Iteration 45, loss = 0.01896633
Iteration 46, loss = 0.01868802
Iteration 47, loss = 0.01831352
Iteration 48, loss = 0.01816825
Iteration 49, loss = 0.01786535
Iteration 50, loss = 0.01756304
Iteration 51, loss = 0.01742552
Iteration 52, loss = 0.01731068
Iteration 53, loss = 0.01694357
Iteration 54, loss = 0.01679949
Iteration 55, loss = 0.01675777
Iteration 56, loss = 0.01644865
Iteration 57, loss = 0.01641026
Iteration 58, loss = 0.01611716
Iteration 59, loss = 0.01601281
Iteration 60, loss = 0.01581730
Iteration 61, loss = 0.01590684
Iteration 62, loss = 0.01564868
Iteration 63, loss = 0.01543453
Iteration 64, loss = 0.01540998
Iteration 65, loss = 0.01530836
Iteration 66, loss = 0.01514572
Iteration 67, loss = 0.01505222
Iteration 68, loss = 0.01497625
Iteration 69, loss = 0.01495402
Iteration 70, loss = 0.01479229
Iteration 71, loss = 0.01467176
Iteration 72, loss = 0.01457453
Iteration 73, loss = 0.01464238
Iteration 74, loss = 0.01462755
Iteration 75, loss = 0.01458493
Iteration 76, loss = 0.01442414
Iteration 77, loss = 0.01448979
Iteration 78, loss = 0.01436033
Iteration 79, loss = 0.01440113
Iteration 80, loss = 0.01422672
Iteration 81, loss = 0.01407257
Iteration 82, loss = 0.01406246
Iteration 83, loss = 0.01399633
Iteration 84, loss = 0.01390357
Iteration 85, loss = 0.01383316
Iteration 86, loss = 0.01388735
Iteration 87, loss = 0.01399664
Iteration 88, loss = 0.01377312
Iteration 89, loss = 0.01377687
Iteration 90, loss = 0.01377443
Iteration 91, loss = 0.01367990
Iteration 92, loss = 0.01369176
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67635637
Iteration 2, loss = 0.61780853
Iteration 3, loss = 0.53718088
Iteration 4, loss = 0.44520377
Iteration 5, loss = 0.35865581
Iteration 6, loss = 0.28435418
Iteration 7, loss = 0.22454062
Iteration 8, loss = 0.17894776
Iteration 9, loss = 0.14453158
Iteration 10, loss = 0.11938745
Iteration 11, loss = 0.10054224
Iteration 12, loss = 0.08604301
Iteration 13, loss = 0.07535173
Iteration 14, loss = 0.06661032
Iteration 15, loss = 0.05981046
Iteration 16, loss = 0.05433781
Iteration 17, loss = 0.04967358
Iteration 18, loss = 0.04580650
Iteration 19, loss = 0.04273932
Iteration 20, loss = 0.04002865
Iteration 21, loss = 0.03798699
Iteration 22, loss = 0.03588776
Iteration 23, loss = 0.03424768
Iteration 24, loss = 0.03282984
Iteration 25, loss = 0.03172552
Iteration 26, loss = 0.03033702
Iteration 27, loss = 0.02945186
Iteration 28, loss = 0.02843366
Iteration 29, loss = 0.02767452
Iteration 30, loss = 0.02715836
Iteration 31, loss = 0.02644584
Iteration 32, loss = 0.02583219
Iteration 33, loss = 0.02514813
Iteration 34, loss = 0.02470988
Iteration 35, loss = 0.02432179
Iteration 36, loss = 0.02409431
Iteration 37, loss = 0.02362359
Iteration 38, loss = 0.02324445
Iteration 39, loss = 0.02290434
Iteration 40, loss = 0.02266634
Iteration 41, loss = 0.02243631
Iteration 42, loss = 0.02208938
Iteration 43, loss = 0.02189510
Iteration 44, loss = 0.02183353
Iteration 45, loss = 0.02150274
Iteration 46, loss = 0.02153412
Iteration 47, loss = 0.02131523
Iteration 48, loss = 0.02111664
Iteration 49, loss = 0.02085397
Iteration 50, loss = 0.02091839
Iteration 51, loss = 0.02048010
Iteration 52, loss = 0.02039860
Iteration 53, loss = 0.02048552
Iteration 54, loss = 0.02021508
Iteration 55, loss = 0.02022124
Iteration 56, loss = 0.02018117
Iteration 57, loss = 0.02007806
Iteration 58, loss = 0.01986967
Iteration 59, loss = 0.02006696
Iteration 60, loss = 0.01975361
Iteration 61, loss = 0.01951215
Iteration 62, loss = 0.01953742
Iteration 63, loss = 0.01937828
Iteration 64, loss = 0.01958421
Iteration 65, loss = 0.01949575
Iteration 66, loss = 0.01939144
Iteration 67, loss = 0.01937144
Iteration 68, loss = 0.01897214
Iteration 69, loss = 0.01922676
Iteration 70, loss = 0.01911280
Iteration 71, loss = 0.01912988
Iteration 72, loss = 0.01895295
Iteration 73, loss = 0.01897729
Iteration 74, loss = 0.01927027
Iteration 75, loss = 0.01893943
Iteration 76, loss = 0.01898391
Iteration 77, loss = 0.01891074
Iteration 78, loss = 0.01906297
Iteration 79, loss = 0.01886764
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68979043
Iteration 2, loss = 0.66626214
Iteration 3, loss = 0.63245404
Iteration 4, loss = 0.58704786
Iteration 5, loss = 0.52983876
Iteration 6, loss = 0.46803887
Iteration 7, loss = 0.40686509
Iteration 8, loss = 0.34860523
Iteration 9, loss = 0.29602867
Iteration 10, loss = 0.25022290
Iteration 11, loss = 0.21131149
Iteration 12, loss = 0.17900326
Iteration 13, loss = 0.15239402
Iteration 14, loss = 0.13074194
Iteration 15, loss = 0.11320111
Iteration 16, loss = 0.09881572
Iteration 17, loss = 0.08699892
Iteration 18, loss = 0.07730452
Iteration 19, loss = 0.06918375
Iteration 20, loss = 0.06242651
Iteration 21, loss = 0.05672337
Iteration 22, loss = 0.05193930
Iteration 23, loss = 0.04767574
Iteration 24, loss = 0.04415232
Iteration 25, loss = 0.04100062
Iteration 26, loss = 0.03835961
Iteration 27, loss = 0.03594512
Iteration 28, loss = 0.03384458
Iteration 29, loss = 0.03194860
Iteration 30, loss = 0.03030649
Iteration 31, loss = 0.02884050
Iteration 32, loss = 0.02745114
Iteration 33, loss = 0.02633142
Iteration 34, loss = 0.02533260
Iteration 35, loss = 0.02424467
Iteration 36, loss = 0.02335062
Iteration 37, loss = 0.02253530
Iteration 38, loss = 0.02182583
Iteration 39, loss = 0.02113816
Iteration 40, loss = 0.02045283
Iteration 41, loss = 0.01989623
Iteration 42, loss = 0.01939328
Iteration 43, loss = 0.01882655
Iteration 44, loss = 0.01832878
Iteration 45, loss = 0.01790835
Iteration 46, loss = 0.01766193
Iteration 47, loss = 0.01719047
Iteration 48, loss = 0.01688578
Iteration 49, loss = 0.01655862
Iteration 50, loss = 0.01621912
Iteration 51, loss = 0.01602322
Iteration 52, loss = 0.01567178
Iteration 53, loss = 0.01549093
Iteration 54, loss = 0.01521549
Iteration 55, loss = 0.01495748
Iteration 56, loss = 0.01476878
Iteration 57, loss = 0.01452503
Iteration 58, loss = 0.01443312
Iteration 59, loss = 0.01420258
Iteration 60, loss = 0.01410866
Iteration 61, loss = 0.01397154
Iteration 62, loss = 0.01373896
Iteration 63, loss = 0.01371597
Iteration 64, loss = 0.01351528
Iteration 65, loss = 0.01335369
Iteration 66, loss = 0.01332787
Iteration 67, loss = 0.01315923
Iteration 68, loss = 0.01297033
Iteration 69, loss = 0.01292010
Iteration 70, loss = 0.01276352
Iteration 71, loss = 0.01273092
Iteration 72, loss = 0.01262877
Iteration 73, loss = 0.01247122
Iteration 74, loss = 0.01262863
Iteration 75, loss = 0.01237006
Iteration 76, loss = 0.01230957
Iteration 77, loss = 0.01220629
Iteration 78, loss = 0.01225499
Iteration 79, loss = 0.01212565
Iteration 80, loss = 0.01205898
Iteration 81, loss = 0.01189706
Iteration 82, loss = 0.01179495
Iteration 83, loss = 0.01179203
Iteration 84, loss = 0.01168674
Iteration 85, loss = 0.01170279
Iteration 86, loss = 0.01169411
Iteration 87, loss = 0.01158573
Iteration 88, loss = 0.01148368
Iteration 89, loss = 0.01145840
Iteration 90, loss = 0.01137136
Iteration 91, loss = 0.01135765
Iteration 92, loss = 0.01131797
Iteration 93, loss = 0.01133934
Iteration 94, loss = 0.01115871
Iteration 95, loss = 0.01116658
Iteration 96, loss = 0.01115607
Iteration 97, loss = 0.01112603
Iteration 98, loss = 0.01102169
Iteration 99, loss = 0.01103324
Iteration 100, loss = 0.01111370
Iteration 101, loss = 0.01101511
Iteration 102, loss = 0.01095712
Iteration 103, loss = 0.01088510
Iteration 104, loss = 0.01089899
Iteration 105, loss = 0.01095896
Iteration 106, loss = 0.01086229
Iteration 107, loss = 0.01087856
Iteration 108, loss = 0.01085382
Iteration 109, loss = 0.01082331
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69353400
Iteration 2, loss = 0.66722419
Iteration 3, loss = 0.63101787
Iteration 4, loss = 0.58293577
Iteration 5, loss = 0.52557276
Iteration 6, loss = 0.46435790
Iteration 7, loss = 0.40293329
Iteration 8, loss = 0.34568970
Iteration 9, loss = 0.29402843
Iteration 10, loss = 0.24895124
Iteration 11, loss = 0.21095082
Iteration 12, loss = 0.17928039
Iteration 13, loss = 0.15321938
Iteration 14, loss = 0.13204404
Iteration 15, loss = 0.11465618
Iteration 16, loss = 0.10046321
Iteration 17, loss = 0.08874112
Iteration 18, loss = 0.07906178
Iteration 19, loss = 0.07095156
Iteration 20, loss = 0.06418899
Iteration 21, loss = 0.05842270
Iteration 22, loss = 0.05344800
Iteration 23, loss = 0.04926719
Iteration 24, loss = 0.04552886
Iteration 25, loss = 0.04231912
Iteration 26, loss = 0.03955075
Iteration 27, loss = 0.03707307
Iteration 28, loss = 0.03491477
Iteration 29, loss = 0.03301683
Iteration 30, loss = 0.03119424
Iteration 31, loss = 0.02969272
Iteration 32, loss = 0.02831908
Iteration 33, loss = 0.02700032
Iteration 34, loss = 0.02579615
Iteration 35, loss = 0.02485262
Iteration 36, loss = 0.02393494
Iteration 37, loss = 0.02292396
Iteration 38, loss = 0.02208858
Iteration 39, loss = 0.02140588
Iteration 40, loss = 0.02075845
Iteration 41, loss = 0.02016609
Iteration 42, loss = 0.01949681
Iteration 43, loss = 0.01903510
Iteration 44, loss = 0.01845774
Iteration 45, loss = 0.01803120
Iteration 46, loss = 0.01762268
Iteration 47, loss = 0.01725161
Iteration 48, loss = 0.01680276
Iteration 49, loss = 0.01645444
Iteration 50, loss = 0.01617993
Iteration 51, loss = 0.01582690
Iteration 52, loss = 0.01555337
Iteration 53, loss = 0.01528174
Iteration 54, loss = 0.01497675
Iteration 55, loss = 0.01479363
Iteration 56, loss = 0.01452366
Iteration 57, loss = 0.01428630
Iteration 58, loss = 0.01413909
Iteration 59, loss = 0.01389861
Iteration 60, loss = 0.01373659
Iteration 61, loss = 0.01356063
Iteration 62, loss = 0.01338986
Iteration 63, loss = 0.01319402
Iteration 64, loss = 0.01307137
Iteration 65, loss = 0.01306813
Iteration 66, loss = 0.01276668
Iteration 67, loss = 0.01271232
Iteration 68, loss = 0.01263000
Iteration 69, loss = 0.01246962
Iteration 70, loss = 0.01237420
Iteration 71, loss = 0.01230574
Iteration 72, loss = 0.01213295
Iteration 73, loss = 0.01209929
Iteration 74, loss = 0.01198127
Iteration 75, loss = 0.01193272
Iteration 76, loss = 0.01192589
Iteration 77, loss = 0.01166017
Iteration 78, loss = 0.01162017
Iteration 79, loss = 0.01147645
Iteration 80, loss = 0.01150112
Iteration 81, loss = 0.01134700
Iteration 82, loss = 0.01128800
Iteration 83, loss = 0.01130781
Iteration 84, loss = 0.01123497
Iteration 85, loss = 0.01114805
Iteration 86, loss = 0.01101483
Iteration 87, loss = 0.01098088
Iteration 88, loss = 0.01094965
Iteration 89, loss = 0.01088458
Iteration 90, loss = 0.01083014
Iteration 91, loss = 0.01077249
Iteration 92, loss = 0.01077480
Iteration 93, loss = 0.01073656
Iteration 94, loss = 0.01061068
Iteration 95, loss = 0.01072319
Iteration 96, loss = 0.01056719
Iteration 97, loss = 0.01050603
Iteration 98, loss = 0.01053847
Iteration 99, loss = 0.01050498
Iteration 100, loss = 0.01046337
Iteration 101, loss = 0.01036240
Iteration 102, loss = 0.01032117
Iteration 103, loss = 0.01039928
Iteration 104, loss = 0.01030212
Iteration 105, loss = 0.01030558
Iteration 106, loss = 0.01029441
Iteration 107, loss = 0.01026961
Iteration 108, loss = 0.01014616
Iteration 109, loss = 0.01021762
Iteration 110, loss = 0.01008600
Iteration 111, loss = 0.01008630
Iteration 112, loss = 0.01012419
Iteration 113, loss = 0.01008071
Iteration 114, loss = 0.01001842
Iteration 115, loss = 0.01000347
Iteration 116, loss = 0.01001614
Iteration 117, loss = 0.00998576
Iteration 118, loss = 0.00990412
Iteration 119, loss = 0.00994175
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68636284
Iteration 2, loss = 0.64103485
Iteration 3, loss = 0.56325285
Iteration 4, loss = 0.46109569
Iteration 5, loss = 0.36362678
Iteration 6, loss = 0.28070886
Iteration 7, loss = 0.21627084
Iteration 8, loss = 0.16892166
Iteration 9, loss = 0.13512134
Iteration 10, loss = 0.11049440
Iteration 11, loss = 0.09275623
Iteration 12, loss = 0.07948996
Iteration 13, loss = 0.06944238
Iteration 14, loss = 0.06163085
Iteration 15, loss = 0.05570289
Iteration 16, loss = 0.05078687
Iteration 17, loss = 0.04626162
Iteration 18, loss = 0.04323588
Iteration 19, loss = 0.04050422
Iteration 20, loss = 0.03803525
Iteration 21, loss = 0.03585635
Iteration 22, loss = 0.03415823
Iteration 23, loss = 0.03252975
Iteration 24, loss = 0.03151086
Iteration 25, loss = 0.03017572
Iteration 26, loss = 0.02939643
Iteration 27, loss = 0.02828801
Iteration 28, loss = 0.02745542
Iteration 29, loss = 0.02668444
Iteration 30, loss = 0.02617113
Iteration 31, loss = 0.02537843
Iteration 32, loss = 0.02487167
Iteration 33, loss = 0.02454664
Iteration 34, loss = 0.02445423
Iteration 35, loss = 0.02383533
Iteration 36, loss = 0.02338404
Iteration 37, loss = 0.02334834
Iteration 38, loss = 0.02292054
Iteration 39, loss = 0.02255005
Iteration 40, loss = 0.02218078
Iteration 41, loss = 0.02202725
Iteration 42, loss = 0.02167753
Iteration 43, loss = 0.02167941
Iteration 44, loss = 0.02122503
Iteration 45, loss = 0.02126055
Iteration 46, loss = 0.02088735
Iteration 47, loss = 0.02065218
Iteration 48, loss = 0.02067516
Iteration 49, loss = 0.02053603
Iteration 50, loss = 0.02056433
Iteration 51, loss = 0.02031186
Iteration 52, loss = 0.02040646
Iteration 53, loss = 0.02005435
Iteration 54, loss = 0.02009535
Iteration 55, loss = 0.01974720
Iteration 56, loss = 0.01959159
Iteration 57, loss = 0.01981956
Iteration 58, loss = 0.01958837
Iteration 59, loss = 0.01950770
Iteration 60, loss = 0.01940104
Iteration 61, loss = 0.01917968
Iteration 62, loss = 0.01913902
Iteration 63, loss = 0.01930086
Iteration 64, loss = 0.01902473
Iteration 65, loss = 0.01893110
Iteration 66, loss = 0.01894113
Iteration 67, loss = 0.02019313
Iteration 68, loss = 0.02034511
Iteration 69, loss = 0.01991503
Iteration 70, loss = 0.01962364
Iteration 71, loss = 0.01943029
Iteration 72, loss = 0.01948860
Iteration 73, loss = 0.01943663
Iteration 74, loss = 0.01959480
Iteration 75, loss = 0.01910425
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67862202
Iteration 2, loss = 0.63039298
Iteration 3, loss = 0.56284574
Iteration 4, loss = 0.48190524
Iteration 5, loss = 0.40227938
Iteration 6, loss = 0.32940730
Iteration 7, loss = 0.26696150
Iteration 8, loss = 0.21632856
Iteration 9, loss = 0.17621996
Iteration 10, loss = 0.14543823
Iteration 11, loss = 0.12184181
Iteration 12, loss = 0.10391354
Iteration 13, loss = 0.08974182
Iteration 14, loss = 0.07874142
Iteration 15, loss = 0.06986449
Iteration 16, loss = 0.06296850
Iteration 17, loss = 0.05705763
Iteration 18, loss = 0.05210049
Iteration 19, loss = 0.04817036
Iteration 20, loss = 0.04465387
Iteration 21, loss = 0.04175361
Iteration 22, loss = 0.03958188
Iteration 23, loss = 0.03708162
Iteration 24, loss = 0.03559224
Iteration 25, loss = 0.03386475
Iteration 26, loss = 0.03243479
Iteration 27, loss = 0.03113826
Iteration 28, loss = 0.03008241
Iteration 29, loss = 0.02875938
Iteration 30, loss = 0.02818634
Iteration 31, loss = 0.02735363
Iteration 32, loss = 0.02658755
Iteration 33, loss = 0.02585605
Iteration 34, loss = 0.02524938
Iteration 35, loss = 0.02479190
Iteration 36, loss = 0.02417055
Iteration 37, loss = 0.02365537
Iteration 38, loss = 0.02313085
Iteration 39, loss = 0.02318931
Iteration 40, loss = 0.02240614
Iteration 41, loss = 0.02225959
Iteration 42, loss = 0.02192517
Iteration 43, loss = 0.02167208
Iteration 44, loss = 0.02156543
Iteration 45, loss = 0.02118733
Iteration 46, loss = 0.02101915
Iteration 47, loss = 0.02069579
Iteration 48, loss = 0.02029093
Iteration 49, loss = 0.02035547
Iteration 50, loss = 0.02009709
Iteration 51, loss = 0.01997618
Iteration 52, loss = 0.02000238
Iteration 53, loss = 0.01976255
Iteration 54, loss = 0.01971068
Iteration 55, loss = 0.01959654
Iteration 56, loss = 0.01930103
Iteration 57, loss = 0.01920463
Iteration 58, loss = 0.01892085
Iteration 59, loss = 0.01889602
Iteration 60, loss = 0.01890169
Iteration 61, loss = 0.01907797
Iteration 62, loss = 0.01884558
Iteration 63, loss = 0.01870995
Iteration 64, loss = 0.01842633
Iteration 65, loss = 0.01837860
Iteration 66, loss = 0.01825634
Iteration 67, loss = 0.01833007
Iteration 68, loss = 0.01847107
Iteration 69, loss = 0.01840296
Iteration 70, loss = 0.01828771
Iteration 71, loss = 0.01806415
Iteration 72, loss = 0.01786629
Iteration 73, loss = 0.01781630
Iteration 74, loss = 0.01789714
Iteration 75, loss = 0.01803400
Iteration 76, loss = 0.01794642
Iteration 77, loss = 0.01785216
Iteration 78, loss = 0.01778359
Iteration 79, loss = 0.01754249
Iteration 80, loss = 0.01748298
Iteration 81, loss = 0.01750856
Iteration 82, loss = 0.01762728
Iteration 83, loss = 0.01747783
Iteration 84, loss = 0.01745547
Iteration 85, loss = 0.01740852
Iteration 86, loss = 0.01746051
Iteration 87, loss = 0.01743375
Iteration 88, loss = 0.01728522
Iteration 89, loss = 0.01732830
Iteration 90, loss = 0.01769441
Iteration 91, loss = 0.01733870
Iteration 92, loss = 0.01746566
Iteration 93, loss = 0.01742740
Iteration 94, loss = 0.01731557
Iteration 95, loss = 0.01727604
Iteration 96, loss = 0.01728129
Iteration 97, loss = 0.01706762
Iteration 98, loss = 0.01703419
Iteration 99, loss = 0.01702007
Iteration 100, loss = 0.01686024
Iteration 101, loss = 0.01710359
Iteration 102, loss = 0.01706833
Iteration 103, loss = 0.01727277
Iteration 104, loss = 0.01691007
Iteration 105, loss = 0.01719233
Iteration 106, loss = 0.01713391
Iteration 107, loss = 0.01715065
Iteration 108, loss = 0.01707162
Iteration 109, loss = 0.01690864
Iteration 110, loss = 0.01681105
Iteration 111, loss = 0.01705857
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68413718
Iteration 2, loss = 0.62320797
Iteration 3, loss = 0.53731848
Iteration 4, loss = 0.43781672
Iteration 5, loss = 0.34425725
Iteration 6, loss = 0.26582575
Iteration 7, loss = 0.20601440
Iteration 8, loss = 0.16261168
Iteration 9, loss = 0.13144510
Iteration 10, loss = 0.10904546
Iteration 11, loss = 0.09274524
Iteration 12, loss = 0.08038369
Iteration 13, loss = 0.07089989
Iteration 14, loss = 0.06354269
Iteration 15, loss = 0.05763746
Iteration 16, loss = 0.05289306
Iteration 17, loss = 0.04934610
Iteration 18, loss = 0.04619287
Iteration 19, loss = 0.04348240
Iteration 20, loss = 0.04110393
Iteration 21, loss = 0.03911199
Iteration 22, loss = 0.03772477
Iteration 23, loss = 0.03641638
Iteration 24, loss = 0.03502660
Iteration 25, loss = 0.03421964
Iteration 26, loss = 0.03309865
Iteration 27, loss = 0.03237169
Iteration 28, loss = 0.03136986
Iteration 29, loss = 0.03087377
Iteration 30, loss = 0.03010081
Iteration 31, loss = 0.02972018
Iteration 32, loss = 0.02931062
Iteration 33, loss = 0.02889867
Iteration 34, loss = 0.02847935
Iteration 35, loss = 0.02825895
Iteration 36, loss = 0.02781880
Iteration 37, loss = 0.02819102
Iteration 38, loss = 0.02793600
Iteration 39, loss = 0.02736927
Iteration 40, loss = 0.02696413
Iteration 41, loss = 0.02673769
Iteration 42, loss = 0.02648548
Iteration 43, loss = 0.02623081
Iteration 44, loss = 0.02608619
Iteration 45, loss = 0.02578328
Iteration 46, loss = 0.02562260
Iteration 47, loss = 0.02605795
Iteration 48, loss = 0.02612220
Iteration 49, loss = 0.02570728
Iteration 50, loss = 0.02545367
Iteration 51, loss = 0.02548240
Iteration 52, loss = 0.02531787
Iteration 53, loss = 0.02484817
Iteration 54, loss = 0.02471712
Iteration 55, loss = 0.02445094
Iteration 56, loss = 0.02436124
Iteration 57, loss = 0.02418042
Iteration 58, loss = 0.02407830
Iteration 59, loss = 0.02408701
Iteration 60, loss = 0.02420046
Iteration 61, loss = 0.02389558
Iteration 62, loss = 0.02380763
Iteration 63, loss = 0.02349517
Iteration 64, loss = 0.02380813
Iteration 65, loss = 0.02378242
Iteration 66, loss = 0.02351136
Iteration 67, loss = 0.02387114
Iteration 68, loss = 0.02355827
Iteration 69, loss = 0.02333127
Iteration 70, loss = 0.02343418
Iteration 71, loss = 0.02324886
Iteration 72, loss = 0.02344625
Iteration 73, loss = 0.02316291
Iteration 74, loss = 0.02298560
Iteration 75, loss = 0.02307922
Iteration 76, loss = 0.02311028
Iteration 77, loss = 0.02309168
Iteration 78, loss = 0.02326097
Iteration 79, loss = 0.02293949
Iteration 80, loss = 0.02399812
Iteration 81, loss = 0.02426838
Iteration 82, loss = 0.02405491
Iteration 83, loss = 0.02367317
Iteration 84, loss = 0.02343897
Iteration 85, loss = 0.02361786
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73397157
Iteration 2, loss = 0.71513265
Iteration 3, loss = 0.69966955
Iteration 4, loss = 0.68487385
Iteration 5, loss = 0.66958199
Iteration 6, loss = 0.65358068
Iteration 7, loss = 0.63638476
Iteration 8, loss = 0.61804474
Iteration 9, loss = 0.59860613
Iteration 10, loss = 0.57768575
Iteration 11, loss = 0.55590758
Iteration 12, loss = 0.53312269
Iteration 13, loss = 0.50993462
Iteration 14, loss = 0.48588557
Iteration 15, loss = 0.46181950
Iteration 16, loss = 0.43742567
Iteration 17, loss = 0.41298394
Iteration 18, loss = 0.38897116
Iteration 19, loss = 0.36542104
Iteration 20, loss = 0.34276543
Iteration 21, loss = 0.32112147
Iteration 22, loss = 0.30049240
Iteration 23, loss = 0.28104865
Iteration 24, loss = 0.26278855
Iteration 25, loss = 0.24569126
Iteration 26, loss = 0.22968757
Iteration 27, loss = 0.21479111
Iteration 28, loss = 0.20095463
Iteration 29, loss = 0.18808587
Iteration 30, loss = 0.17624367
Iteration 31, loss = 0.16526539
Iteration 32, loss = 0.15517272
Iteration 33, loss = 0.14577762
Iteration 34, loss = 0.13709617
Iteration 35, loss = 0.12911212
Iteration 36, loss = 0.12173958
Iteration 37, loss = 0.11489667
Iteration 38, loss = 0.10851532
Iteration 39, loss = 0.10268854
Iteration 40, loss = 0.09726781
Iteration 41, loss = 0.09230121
Iteration 42, loss = 0.08765504
Iteration 43, loss = 0.08335938
Iteration 44, loss = 0.07941013
Iteration 45, loss = 0.07569495
Iteration 46, loss = 0.07223567
Iteration 47, loss = 0.06899768
Iteration 48, loss = 0.06602026
Iteration 49, loss = 0.06316577
Iteration 50, loss = 0.06053588
Iteration 51, loss = 0.05808749
Iteration 52, loss = 0.05574349
Iteration 53, loss = 0.05354711
Iteration 54, loss = 0.05149657
Iteration 55, loss = 0.04955241
Iteration 56, loss = 0.04775198
Iteration 57, loss = 0.04600656
Iteration 58, loss = 0.04435512
Iteration 59, loss = 0.04276597
Iteration 60, loss = 0.04132223
Iteration 61, loss = 0.03992736
Iteration 62, loss = 0.03857530
Iteration 63, loss = 0.03732422
Iteration 64, loss = 0.03612077
Iteration 65, loss = 0.03496312
Iteration 66, loss = 0.03387875
Iteration 67, loss = 0.03285471
Iteration 68, loss = 0.03184653
Iteration 69, loss = 0.03089566
Iteration 70, loss = 0.03001645
Iteration 71, loss = 0.02914179
Iteration 72, loss = 0.02833016
Iteration 73, loss = 0.02755508
Iteration 74, loss = 0.02680519
Iteration 75, loss = 0.02608706
Iteration 76, loss = 0.02540904
Iteration 77, loss = 0.02475189
Iteration 78, loss = 0.02411365
Iteration 79, loss = 0.02349375
Iteration 80, loss = 0.02291440
Iteration 81, loss = 0.02234846
Iteration 82, loss = 0.02181073
Iteration 83, loss = 0.02128263
Iteration 84, loss = 0.02078940
Iteration 85, loss = 0.02030635
Iteration 86, loss = 0.01984884
Iteration 87, loss = 0.01939643
Iteration 88, loss = 0.01895883
Iteration 89, loss = 0.01854398
Iteration 90, loss = 0.01813824
Iteration 91, loss = 0.01774267
Iteration 92, loss = 0.01735065
Iteration 93, loss = 0.01699843
Iteration 94, loss = 0.01664072
Iteration 95, loss = 0.01629460
Iteration 96, loss = 0.01597316
Iteration 97, loss = 0.01564753
Iteration 98, loss = 0.01533854
Iteration 99, loss = 0.01503745
Iteration 100, loss = 0.01475330
Iteration 101, loss = 0.01446486
Iteration 102, loss = 0.01418852
Iteration 103, loss = 0.01393096
Iteration 104, loss = 0.01367165
Iteration 105, loss = 0.01341673
Iteration 106, loss = 0.01317437
Iteration 107, loss = 0.01293915
Iteration 108, loss = 0.01271453
Iteration 109, loss = 0.01249349
Iteration 110, loss = 0.01227699
Iteration 111, loss = 0.01206629
Iteration 112, loss = 0.01186199
Iteration 113, loss = 0.01166912
Iteration 114, loss = 0.01147284
Iteration 115, loss = 0.01127872
Iteration 116, loss = 0.01109933
Iteration 117, loss = 0.01091726
Iteration 118, loss = 0.01074679
Iteration 119, loss = 0.01057303
Iteration 120, loss = 0.01041458
Iteration 121, loss = 0.01025412
Iteration 122, loss = 0.01009357
Iteration 123, loss = 0.00994343
Iteration 124, loss = 0.00979609
Iteration 125, loss = 0.00964807
Iteration 126, loss = 0.00950320
Iteration 127, loss = 0.00937018
Iteration 128, loss = 0.00923236
Iteration 129, loss = 0.00909650
Iteration 130, loss = 0.00896636
Iteration 131, loss = 0.00883844
Iteration 132, loss = 0.00871262
Iteration 133, loss = 0.00859643
Iteration 134, loss = 0.00848021
Iteration 135, loss = 0.00836723
Iteration 136, loss = 0.00825473
Iteration 137, loss = 0.00814591
Iteration 138, loss = 0.00804057
Iteration 139, loss = 0.00793802
Iteration 140, loss = 0.00783608
Iteration 141, loss = 0.00773668
Iteration 142, loss = 0.00763955
Iteration 143, loss = 0.00754403
Iteration 144, loss = 0.00744999
Iteration 145, loss = 0.00735688
Iteration 146, loss = 0.00726841
Iteration 147, loss = 0.00717991
Iteration 148, loss = 0.00709543
Iteration 149, loss = 0.00701012
Iteration 150, loss = 0.00692848
Iteration 151, loss = 0.00684600
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71878292
Iteration 2, loss = 0.69920027
Iteration 3, loss = 0.68127817
Iteration 4, loss = 0.66085094
Iteration 5, loss = 0.63749993
Iteration 6, loss = 0.61095295
Iteration 7, loss = 0.58090119
Iteration 8, loss = 0.54815232
Iteration 9, loss = 0.51384018
Iteration 10, loss = 0.47832726
Iteration 11, loss = 0.44240835
Iteration 12, loss = 0.40718445
Iteration 13, loss = 0.37325423
Iteration 14, loss = 0.34099591
Iteration 15, loss = 0.31074667
Iteration 16, loss = 0.28271214
Iteration 17, loss = 0.25710246
Iteration 18, loss = 0.23383157
Iteration 19, loss = 0.21259861
Iteration 20, loss = 0.19371023
Iteration 21, loss = 0.17673097
Iteration 22, loss = 0.16161305
Iteration 23, loss = 0.14810764
Iteration 24, loss = 0.13609990
Iteration 25, loss = 0.12548991
Iteration 26, loss = 0.11591662
Iteration 27, loss = 0.10747513
Iteration 28, loss = 0.09990234
Iteration 29, loss = 0.09311215
Iteration 30, loss = 0.08708785
Iteration 31, loss = 0.08162744
Iteration 32, loss = 0.07673066
Iteration 33, loss = 0.07217915
Iteration 34, loss = 0.06829403
Iteration 35, loss = 0.06462235
Iteration 36, loss = 0.06126837
Iteration 37, loss = 0.05829249
Iteration 38, loss = 0.05545371
Iteration 39, loss = 0.05293258
Iteration 40, loss = 0.05059533
Iteration 41, loss = 0.04850204
Iteration 42, loss = 0.04651762
Iteration 43, loss = 0.04475304
Iteration 44, loss = 0.04300442
Iteration 45, loss = 0.04139308
Iteration 46, loss = 0.03993550
Iteration 47, loss = 0.03856778
Iteration 48, loss = 0.03732806
Iteration 49, loss = 0.03606938
Iteration 50, loss = 0.03502530
Iteration 51, loss = 0.03395811
Iteration 52, loss = 0.03295184
Iteration 53, loss = 0.03206323
Iteration 54, loss = 0.03116831
Iteration 55, loss = 0.03037696
Iteration 56, loss = 0.02961956
Iteration 57, loss = 0.02887196
Iteration 58, loss = 0.02818866
Iteration 59, loss = 0.02753816
Iteration 60, loss = 0.02692154
Iteration 61, loss = 0.02633635
Iteration 62, loss = 0.02578386
Iteration 63, loss = 0.02529986
Iteration 64, loss = 0.02479340
Iteration 65, loss = 0.02432988
Iteration 66, loss = 0.02388667
Iteration 67, loss = 0.02341796
Iteration 68, loss = 0.02300702
Iteration 69, loss = 0.02266017
Iteration 70, loss = 0.02227222
Iteration 71, loss = 0.02193058
Iteration 72, loss = 0.02155882
Iteration 73, loss = 0.02126642
Iteration 74, loss = 0.02096428
Iteration 75, loss = 0.02067094
Iteration 76, loss = 0.02033210
Iteration 77, loss = 0.02005995
Iteration 78, loss = 0.01980056
Iteration 79, loss = 0.01961660
Iteration 80, loss = 0.01927105
Iteration 81, loss = 0.01906849
Iteration 82, loss = 0.01885100
Iteration 83, loss = 0.01865026
Iteration 84, loss = 0.01840714
Iteration 85, loss = 0.01822357
Iteration 86, loss = 0.01806034
Iteration 87, loss = 0.01783994
Iteration 88, loss = 0.01767273
Iteration 89, loss = 0.01751924
Iteration 90, loss = 0.01737281
Iteration 91, loss = 0.01721928
Iteration 92, loss = 0.01699681
Iteration 93, loss = 0.01687587
Iteration 94, loss = 0.01676454
Iteration 95, loss = 0.01660360
Iteration 96, loss = 0.01642642
Iteration 97, loss = 0.01633971
Iteration 98, loss = 0.01618094
Iteration 99, loss = 0.01606112
Iteration 100, loss = 0.01590632
Iteration 101, loss = 0.01582625
Iteration 102, loss = 0.01573469
Iteration 103, loss = 0.01555619
Iteration 104, loss = 0.01548485
Iteration 105, loss = 0.01540030
Iteration 106, loss = 0.01528276
Iteration 107, loss = 0.01517707
Iteration 108, loss = 0.01511302
Iteration 109, loss = 0.01503333
Iteration 110, loss = 0.01491450
Iteration 111, loss = 0.01482365
Iteration 112, loss = 0.01474821
Iteration 113, loss = 0.01464329
Iteration 114, loss = 0.01459166
Iteration 115, loss = 0.01450030
Iteration 116, loss = 0.01443053
Iteration 117, loss = 0.01433950
Iteration 118, loss = 0.01431288
Iteration 119, loss = 0.01422787
Iteration 120, loss = 0.01415268
Iteration 121, loss = 0.01412746
Iteration 122, loss = 0.01401261
Iteration 123, loss = 0.01394847
Iteration 124, loss = 0.01393032
Iteration 125, loss = 0.01387745
Iteration 126, loss = 0.01378344
Iteration 127, loss = 0.01373114
Iteration 128, loss = 0.01366191
Iteration 129, loss = 0.01359380
Iteration 130, loss = 0.01352650
Iteration 131, loss = 0.01346966
Iteration 132, loss = 0.01342996
Iteration 133, loss = 0.01341148
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68057389
Iteration 2, loss = 0.65770957
Iteration 3, loss = 0.63515773
Iteration 4, loss = 0.61052124
Iteration 5, loss = 0.58243876
Iteration 6, loss = 0.55131824
Iteration 7, loss = 0.51713219
Iteration 8, loss = 0.48139951
Iteration 9, loss = 0.44528191
Iteration 10, loss = 0.40988455
Iteration 11, loss = 0.37547834
Iteration 12, loss = 0.34300739
Iteration 13, loss = 0.31270762
Iteration 14, loss = 0.28421916
Iteration 15, loss = 0.25815019
Iteration 16, loss = 0.23448307
Iteration 17, loss = 0.21304448
Iteration 18, loss = 0.19358734
Iteration 19, loss = 0.17638591
Iteration 20, loss = 0.16092891
Iteration 21, loss = 0.14717559
Iteration 22, loss = 0.13493401
Iteration 23, loss = 0.12411963
Iteration 24, loss = 0.11442885
Iteration 25, loss = 0.10587942
Iteration 26, loss = 0.09809952
Iteration 27, loss = 0.09136175
Iteration 28, loss = 0.08525112
Iteration 29, loss = 0.07974443
Iteration 30, loss = 0.07481230
Iteration 31, loss = 0.07037823
Iteration 32, loss = 0.06626221
Iteration 33, loss = 0.06269651
Iteration 34, loss = 0.05940262
Iteration 35, loss = 0.05634347
Iteration 36, loss = 0.05359624
Iteration 37, loss = 0.05105205
Iteration 38, loss = 0.04875137
Iteration 39, loss = 0.04660775
Iteration 40, loss = 0.04465578
Iteration 41, loss = 0.04283780
Iteration 42, loss = 0.04117574
Iteration 43, loss = 0.03961411
Iteration 44, loss = 0.03818154
Iteration 45, loss = 0.03687171
Iteration 46, loss = 0.03559346
Iteration 47, loss = 0.03439063
Iteration 48, loss = 0.03332465
Iteration 49, loss = 0.03231599
Iteration 50, loss = 0.03131991
Iteration 51, loss = 0.03042739
Iteration 52, loss = 0.02959282
Iteration 53, loss = 0.02878869
Iteration 54, loss = 0.02807765
Iteration 55, loss = 0.02732595
Iteration 56, loss = 0.02667142
Iteration 57, loss = 0.02602391
Iteration 58, loss = 0.02544733
Iteration 59, loss = 0.02491135
Iteration 60, loss = 0.02430548
Iteration 61, loss = 0.02385010
Iteration 62, loss = 0.02333758
Iteration 63, loss = 0.02289262
Iteration 64, loss = 0.02248616
Iteration 65, loss = 0.02204201
Iteration 66, loss = 0.02164785
Iteration 67, loss = 0.02124853
Iteration 68, loss = 0.02092680
Iteration 69, loss = 0.02057722
Iteration 70, loss = 0.02024229
Iteration 71, loss = 0.01992529
Iteration 72, loss = 0.01959963
Iteration 73, loss = 0.01932143
Iteration 74, loss = 0.01902054
Iteration 75, loss = 0.01872963
Iteration 76, loss = 0.01848987
Iteration 77, loss = 0.01829250
Iteration 78, loss = 0.01805661
Iteration 79, loss = 0.01781107
Iteration 80, loss = 0.01762014
Iteration 81, loss = 0.01741073
Iteration 82, loss = 0.01720737
Iteration 83, loss = 0.01697895
Iteration 84, loss = 0.01679803
Iteration 85, loss = 0.01665796
Iteration 86, loss = 0.01646033
Iteration 87, loss = 0.01630961
Iteration 88, loss = 0.01613704
Iteration 89, loss = 0.01595034
Iteration 90, loss = 0.01581629
Iteration 91, loss = 0.01568321
Iteration 92, loss = 0.01556007
Iteration 93, loss = 0.01539634
Iteration 94, loss = 0.01524980
Iteration 95, loss = 0.01520031
Iteration 96, loss = 0.01504640
Iteration 97, loss = 0.01492524
Iteration 98, loss = 0.01477038
Iteration 99, loss = 0.01467069
Iteration 100, loss = 0.01454765
Iteration 101, loss = 0.01450291
Iteration 102, loss = 0.01436019
Iteration 103, loss = 0.01427410
Iteration 104, loss = 0.01415171
Iteration 105, loss = 0.01408691
Iteration 106, loss = 0.01398538
Iteration 107, loss = 0.01395029
Iteration 108, loss = 0.01383829
Iteration 109, loss = 0.01376636
Iteration 110, loss = 0.01373888
Iteration 111, loss = 0.01362572
Iteration 112, loss = 0.01353691
Iteration 113, loss = 0.01346221
Iteration 114, loss = 0.01333618
Iteration 115, loss = 0.01333056
Iteration 116, loss = 0.01320215
Iteration 117, loss = 0.01319037
Iteration 118, loss = 0.01307739
Iteration 119, loss = 0.01306674
Iteration 120, loss = 0.01297096
Iteration 121, loss = 0.01292312
Iteration 122, loss = 0.01285522
Iteration 123, loss = 0.01280311
Iteration 124, loss = 0.01277030
Iteration 125, loss = 0.01266714
Iteration 126, loss = 0.01263056
Iteration 127, loss = 0.01257142
Iteration 128, loss = 0.01251017
Iteration 129, loss = 0.01247647
Iteration 130, loss = 0.01241336
Iteration 131, loss = 0.01236330
Iteration 132, loss = 0.01235650
Iteration 133, loss = 0.01229801
Iteration 134, loss = 0.01225998
Iteration 135, loss = 0.01219814
Iteration 136, loss = 0.01215439
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72055747
Iteration 2, loss = 0.67695823
Iteration 3, loss = 0.61283633
Iteration 4, loss = 0.53254733
Iteration 5, loss = 0.44327862
Iteration 6, loss = 0.35888829
Iteration 7, loss = 0.28678771
Iteration 8, loss = 0.22932357
Iteration 9, loss = 0.18474872
Iteration 10, loss = 0.15074935
Iteration 11, loss = 0.12514022
Iteration 12, loss = 0.10565956
Iteration 13, loss = 0.09072203
Iteration 14, loss = 0.07894823
Iteration 15, loss = 0.06961845
Iteration 16, loss = 0.06214594
Iteration 17, loss = 0.05603724
Iteration 18, loss = 0.05113853
Iteration 19, loss = 0.04679605
Iteration 20, loss = 0.04337384
Iteration 21, loss = 0.04046359
Iteration 22, loss = 0.03792090
Iteration 23, loss = 0.03569985
Iteration 24, loss = 0.03373747
Iteration 25, loss = 0.03207837
Iteration 26, loss = 0.03061130
Iteration 27, loss = 0.02933655
Iteration 28, loss = 0.02811393
Iteration 29, loss = 0.02709411
Iteration 30, loss = 0.02610424
Iteration 31, loss = 0.02560351
Iteration 32, loss = 0.02481471
Iteration 33, loss = 0.02385970
Iteration 34, loss = 0.02323732
Iteration 35, loss = 0.02291310
Iteration 36, loss = 0.02222469
Iteration 37, loss = 0.02176527
Iteration 38, loss = 0.02123526
Iteration 39, loss = 0.02093835
Iteration 40, loss = 0.02070968
Iteration 41, loss = 0.02047067
Iteration 42, loss = 0.01992634
Iteration 43, loss = 0.01967816
Iteration 44, loss = 0.01953622
Iteration 45, loss = 0.01923399
Iteration 46, loss = 0.01890400
Iteration 47, loss = 0.01872829
Iteration 48, loss = 0.01852463
Iteration 49, loss = 0.01831443
Iteration 50, loss = 0.01820912
Iteration 51, loss = 0.01829004
Iteration 52, loss = 0.01771904
Iteration 53, loss = 0.01763899
Iteration 54, loss = 0.01772139
Iteration 55, loss = 0.01750151
Iteration 56, loss = 0.01736886
Iteration 57, loss = 0.01718094
Iteration 58, loss = 0.01713498
Iteration 59, loss = 0.01700384
Iteration 60, loss = 0.01700766
Iteration 61, loss = 0.01659395
Iteration 62, loss = 0.01683170
Iteration 63, loss = 0.01657311
Iteration 64, loss = 0.01659721
Iteration 65, loss = 0.01642631
Iteration 66, loss = 0.01627159
Iteration 67, loss = 0.01633337
Iteration 68, loss = 0.01603852
Iteration 69, loss = 0.01625114
Iteration 70, loss = 0.01612259
Iteration 71, loss = 0.01595952
Iteration 72, loss = 0.01594245
Iteration 73, loss = 0.01586057
Iteration 74, loss = 0.01587143
Iteration 75, loss = 0.01604044
Iteration 76, loss = 0.01583090
Iteration 77, loss = 0.01572570
Iteration 78, loss = 0.01571735
Iteration 79, loss = 0.01582968
Iteration 80, loss = 0.01581895
Iteration 81, loss = 0.01586705
Iteration 82, loss = 0.01557394
Iteration 83, loss = 0.01551795
Iteration 84, loss = 0.01512055
Iteration 85, loss = 0.01565355
Iteration 86, loss = 0.01540284
Iteration 87, loss = 0.01534743
Iteration 88, loss = 0.01558235
Iteration 89, loss = 0.01510928
Iteration 90, loss = 0.01533007
Iteration 91, loss = 0.01535882
Iteration 92, loss = 0.01527657
Iteration 93, loss = 0.01508992
Iteration 94, loss = 0.01516701
Iteration 95, loss = 0.01498701
Iteration 96, loss = 0.01511489
Iteration 97, loss = 0.01512121
Iteration 98, loss = 0.01507790
Iteration 99, loss = 0.01505273
Iteration 100, loss = 0.01509937
Iteration 101, loss = 0.01501551
Iteration 102, loss = 0.01496113
Iteration 103, loss = 0.01503054
Iteration 104, loss = 0.01503183
Iteration 105, loss = 0.01508785
Iteration 106, loss = 0.01495443
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67943236
Iteration 2, loss = 0.64196861
Iteration 3, loss = 0.58656038
Iteration 4, loss = 0.51575689
Iteration 5, loss = 0.44294480
Iteration 6, loss = 0.37427646
Iteration 7, loss = 0.31298592
Iteration 8, loss = 0.26036464
Iteration 9, loss = 0.21627995
Iteration 10, loss = 0.18063951
Iteration 11, loss = 0.15222771
Iteration 12, loss = 0.12965420
Iteration 13, loss = 0.11170508
Iteration 14, loss = 0.09736354
Iteration 15, loss = 0.08580766
Iteration 16, loss = 0.07653988
Iteration 17, loss = 0.06872202
Iteration 18, loss = 0.06226987
Iteration 19, loss = 0.05694950
Iteration 20, loss = 0.05249054
Iteration 21, loss = 0.04883825
Iteration 22, loss = 0.04545081
Iteration 23, loss = 0.04266106
Iteration 24, loss = 0.04011340
Iteration 25, loss = 0.03806732
Iteration 26, loss = 0.03617529
Iteration 27, loss = 0.03463425
Iteration 28, loss = 0.03308656
Iteration 29, loss = 0.03171059
Iteration 30, loss = 0.03056918
Iteration 31, loss = 0.02951411
Iteration 32, loss = 0.02852443
Iteration 33, loss = 0.02767165
Iteration 34, loss = 0.02692587
Iteration 35, loss = 0.02622811
Iteration 36, loss = 0.02565866
Iteration 37, loss = 0.02506514
Iteration 38, loss = 0.02453963
Iteration 39, loss = 0.02399566
Iteration 40, loss = 0.02347121
Iteration 41, loss = 0.02303910
Iteration 42, loss = 0.02277922
Iteration 43, loss = 0.02230426
Iteration 44, loss = 0.02197474
Iteration 45, loss = 0.02149523
Iteration 46, loss = 0.02145323
Iteration 47, loss = 0.02106125
Iteration 48, loss = 0.02072011
Iteration 49, loss = 0.02052268
Iteration 50, loss = 0.02044633
Iteration 51, loss = 0.02015162
Iteration 52, loss = 0.01990679
Iteration 53, loss = 0.01973370
Iteration 54, loss = 0.01949661
Iteration 55, loss = 0.01938410
Iteration 56, loss = 0.01931076
Iteration 57, loss = 0.01910454
Iteration 58, loss = 0.01883930
Iteration 59, loss = 0.01885651
Iteration 60, loss = 0.01875743
Iteration 61, loss = 0.01852802
Iteration 62, loss = 0.01835010
Iteration 63, loss = 0.01825149
Iteration 64, loss = 0.01826245
Iteration 65, loss = 0.01820286
Iteration 66, loss = 0.01795404
Iteration 67, loss = 0.01805012
Iteration 68, loss = 0.01789088
Iteration 69, loss = 0.01794355
Iteration 70, loss = 0.01760985
Iteration 71, loss = 0.01766903
Iteration 72, loss = 0.01757463
Iteration 73, loss = 0.01746636
Iteration 74, loss = 0.01743575
Iteration 75, loss = 0.01759565
Iteration 76, loss = 0.01725858
Iteration 77, loss = 0.01734391
Iteration 78, loss = 0.01709230
Iteration 79, loss = 0.01713662
Iteration 80, loss = 0.01717698
Iteration 81, loss = 0.01708913
Iteration 82, loss = 0.01701917
Iteration 83, loss = 0.01697582
Iteration 84, loss = 0.01693866
Iteration 85, loss = 0.01690658
Iteration 86, loss = 0.01682580
Iteration 87, loss = 0.01684165
Iteration 88, loss = 0.01673867
Iteration 89, loss = 0.01668050
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70183394
Iteration 2, loss = 0.65185972
Iteration 3, loss = 0.58463160
Iteration 4, loss = 0.50293713
Iteration 5, loss = 0.41812142
Iteration 6, loss = 0.34068904
Iteration 7, loss = 0.27481193
Iteration 8, loss = 0.22194840
Iteration 9, loss = 0.18034275
Iteration 10, loss = 0.14850072
Iteration 11, loss = 0.12394121
Iteration 12, loss = 0.10534236
Iteration 13, loss = 0.09073472
Iteration 14, loss = 0.07921407
Iteration 15, loss = 0.07021169
Iteration 16, loss = 0.06276222
Iteration 17, loss = 0.05684426
Iteration 18, loss = 0.05187256
Iteration 19, loss = 0.04781624
Iteration 20, loss = 0.04433155
Iteration 21, loss = 0.04119949
Iteration 22, loss = 0.03884448
Iteration 23, loss = 0.03671723
Iteration 24, loss = 0.03440873
Iteration 25, loss = 0.03295167
Iteration 26, loss = 0.03160405
Iteration 27, loss = 0.03045966
Iteration 28, loss = 0.02902263
Iteration 29, loss = 0.02814019
Iteration 30, loss = 0.02707283
Iteration 31, loss = 0.02617401
Iteration 32, loss = 0.02551253
Iteration 33, loss = 0.02460013
Iteration 34, loss = 0.02424093
Iteration 35, loss = 0.02344891
Iteration 36, loss = 0.02303400
Iteration 37, loss = 0.02259140
Iteration 38, loss = 0.02225054
Iteration 39, loss = 0.02193808
Iteration 40, loss = 0.02134934
Iteration 41, loss = 0.02102665
Iteration 42, loss = 0.02085256
Iteration 43, loss = 0.02026052
Iteration 44, loss = 0.02011552
Iteration 45, loss = 0.01982068
Iteration 46, loss = 0.01963763
Iteration 47, loss = 0.01960283
Iteration 48, loss = 0.01961917
Iteration 49, loss = 0.01921151
Iteration 50, loss = 0.01883407
Iteration 51, loss = 0.01866593
Iteration 52, loss = 0.01848279
Iteration 53, loss = 0.01844239
Iteration 54, loss = 0.01815255
Iteration 55, loss = 0.01820266
Iteration 56, loss = 0.01808587
Iteration 57, loss = 0.01792863
Iteration 58, loss = 0.01755676
Iteration 59, loss = 0.01760786
Iteration 60, loss = 0.01748866
Iteration 61, loss = 0.01740387
Iteration 62, loss = 0.01719403
Iteration 63, loss = 0.01711553
Iteration 64, loss = 0.01709496
Iteration 65, loss = 0.01702357
Iteration 66, loss = 0.01693522
Iteration 67, loss = 0.01695106
Iteration 68, loss = 0.01693554
Iteration 69, loss = 0.01674373
Iteration 70, loss = 0.01670086
Iteration 71, loss = 0.01660272
Iteration 72, loss = 0.01658313
Iteration 73, loss = 0.01671600
Iteration 74, loss = 0.01652665
Iteration 75, loss = 0.01667435
Iteration 76, loss = 0.01645244
Iteration 77, loss = 0.01640114
Iteration 78, loss = 0.01631559
Iteration 79, loss = 0.01611259
Iteration 80, loss = 0.01618230
Iteration 81, loss = 0.01618943
Iteration 82, loss = 0.01623866
Iteration 83, loss = 0.01602184
Iteration 84, loss = 0.01618418
Iteration 85, loss = 0.01603219
Iteration 86, loss = 0.01613626
Iteration 87, loss = 0.01589252
Iteration 88, loss = 0.01604574
Iteration 89, loss = 0.01589335
Iteration 90, loss = 0.01626434
Iteration 91, loss = 0.01603061
Iteration 92, loss = 0.01601089
Iteration 93, loss = 0.01589293
Iteration 94, loss = 0.01599445
Iteration 95, loss = 0.01606739
Iteration 96, loss = 0.01581504
Iteration 97, loss = 0.01572124
Iteration 98, loss = 0.01566135
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68494867
Iteration 2, loss = 0.61304782
Iteration 3, loss = 0.51099796
Iteration 4, loss = 0.39976143
Iteration 5, loss = 0.30323617
Iteration 6, loss = 0.22865095
Iteration 7, loss = 0.17455817
Iteration 8, loss = 0.13692331
Iteration 9, loss = 0.11044065
Iteration 10, loss = 0.09162631
Iteration 11, loss = 0.07804319
Iteration 12, loss = 0.06794095
Iteration 13, loss = 0.06019068
Iteration 14, loss = 0.05417399
Iteration 15, loss = 0.04922138
Iteration 16, loss = 0.04521211
Iteration 17, loss = 0.04234523
Iteration 18, loss = 0.03937462
Iteration 19, loss = 0.03743773
Iteration 20, loss = 0.03539249
Iteration 21, loss = 0.03385821
Iteration 22, loss = 0.03261319
Iteration 23, loss = 0.03124315
Iteration 24, loss = 0.03047984
Iteration 25, loss = 0.02946157
Iteration 26, loss = 0.02879420
Iteration 27, loss = 0.02806875
Iteration 28, loss = 0.02716970
Iteration 29, loss = 0.02655119
Iteration 30, loss = 0.02644281
Iteration 31, loss = 0.02555832
Iteration 32, loss = 0.02553377
Iteration 33, loss = 0.02529047
Iteration 34, loss = 0.02462835
Iteration 35, loss = 0.02449921
Iteration 36, loss = 0.02402417
Iteration 37, loss = 0.02400075
Iteration 38, loss = 0.02361450
Iteration 39, loss = 0.02335882
Iteration 40, loss = 0.02314815
Iteration 41, loss = 0.02315822
Iteration 42, loss = 0.02281068
Iteration 43, loss = 0.02281549
Iteration 44, loss = 0.02274110
Iteration 45, loss = 0.02238168
Iteration 46, loss = 0.02259861
Iteration 47, loss = 0.02237566
Iteration 48, loss = 0.02221697
Iteration 49, loss = 0.02188830
Iteration 50, loss = 0.02190210
Iteration 51, loss = 0.02181123
Iteration 52, loss = 0.02157081
Iteration 53, loss = 0.02153258
Iteration 54, loss = 0.02177296
Iteration 55, loss = 0.02135696
Iteration 56, loss = 0.02144038
Iteration 57, loss = 0.02176818
Iteration 58, loss = 0.02151863
Iteration 59, loss = 0.02121346
Iteration 60, loss = 0.02111900
Iteration 61, loss = 0.02104124
Iteration 62, loss = 0.02115987
Iteration 63, loss = 0.02098442
Iteration 64, loss = 0.02084667
Iteration 65, loss = 0.02114418
Iteration 66, loss = 0.02077338
Iteration 67, loss = 0.02111852
Iteration 68, loss = 0.02093684
Iteration 69, loss = 0.02079105
Iteration 70, loss = 0.02089960
Iteration 71, loss = 0.02072792
Iteration 72, loss = 0.02065872
Iteration 73, loss = 0.02057604
Iteration 74, loss = 0.02047060
Iteration 75, loss = 0.02050517
Iteration 76, loss = 0.02037866
Iteration 77, loss = 0.02051916
Iteration 78, loss = 0.02073947
Iteration 79, loss = 0.02033166
Iteration 80, loss = 0.02027450
Iteration 81, loss = 0.02034698
Iteration 82, loss = 0.02046470
Iteration 83, loss = 0.02050923
Iteration 84, loss = 0.02050520
Iteration 85, loss = 0.02039569
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67621671
Iteration 2, loss = 0.61425582
Iteration 3, loss = 0.50628093
Iteration 4, loss = 0.39508454
Iteration 5, loss = 0.29959657
Iteration 6, loss = 0.22612865
Iteration 7, loss = 0.17327394
Iteration 8, loss = 0.13627533
Iteration 9, loss = 0.11056828
Iteration 10, loss = 0.09241242
Iteration 11, loss = 0.07904827
Iteration 12, loss = 0.06888814
Iteration 13, loss = 0.06125195
Iteration 14, loss = 0.05531190
Iteration 15, loss = 0.05068308
Iteration 16, loss = 0.04682703
Iteration 17, loss = 0.04383653
Iteration 18, loss = 0.04103055
Iteration 19, loss = 0.03895162
Iteration 20, loss = 0.03722158
Iteration 21, loss = 0.03547080
Iteration 22, loss = 0.03424368
Iteration 23, loss = 0.03336697
Iteration 24, loss = 0.03197596
Iteration 25, loss = 0.03112104
Iteration 26, loss = 0.03023300
Iteration 27, loss = 0.02979261
Iteration 28, loss = 0.02926210
Iteration 29, loss = 0.02856646
Iteration 30, loss = 0.02826625
Iteration 31, loss = 0.02749106
Iteration 32, loss = 0.02738367
Iteration 33, loss = 0.02690635
Iteration 34, loss = 0.02675088
Iteration 35, loss = 0.02636114
Iteration 36, loss = 0.02609444
Iteration 37, loss = 0.02593128
Iteration 38, loss = 0.02565787
Iteration 39, loss = 0.02546116
Iteration 40, loss = 0.02497777
Iteration 41, loss = 0.02506303
Iteration 42, loss = 0.02479348
Iteration 43, loss = 0.02453904
Iteration 44, loss = 0.02427686
Iteration 45, loss = 0.02443819
Iteration 46, loss = 0.02411932
Iteration 47, loss = 0.02395396
Iteration 48, loss = 0.02392973
Iteration 49, loss = 0.02398830
Iteration 50, loss = 0.02394705
Iteration 51, loss = 0.02389160
Iteration 52, loss = 0.02380866
Iteration 53, loss = 0.02379615
Iteration 54, loss = 0.02346301
Iteration 55, loss = 0.02351793
Iteration 56, loss = 0.02325991
Iteration 57, loss = 0.02349799
Iteration 58, loss = 0.02318456
Iteration 59, loss = 0.02305683
Iteration 60, loss = 0.02306166
Iteration 61, loss = 0.02318892
Iteration 62, loss = 0.02319493
Iteration 63, loss = 0.02303766
Iteration 64, loss = 0.02283947
Iteration 65, loss = 0.02291215
Iteration 66, loss = 0.02294681
Iteration 67, loss = 0.02292082
Iteration 68, loss = 0.02316636
Iteration 69, loss = 0.02291524
Iteration 70, loss = 0.02265448
Iteration 71, loss = 0.02268022
Iteration 72, loss = 0.02270358
Iteration 73, loss = 0.02275681
Iteration 74, loss = 0.02258696
Iteration 75, loss = 0.02257620
Iteration 76, loss = 0.02276137
Iteration 77, loss = 0.02278143
Iteration 78, loss = 0.02267270
Iteration 79, loss = 0.02260238
Iteration 80, loss = 0.02238835
Iteration 81, loss = 0.02227923
Iteration 82, loss = 0.02248069
Iteration 83, loss = 0.02239231
Iteration 84, loss = 0.02230900
Iteration 85, loss = 0.02252186
Iteration 86, loss = 0.02240625
Iteration 87, loss = 0.02222419
Iteration 88, loss = 0.02248708
Iteration 89, loss = 0.02267535
Iteration 90, loss = 0.02219836
Iteration 91, loss = 0.02224572
Iteration 92, loss = 0.02224724
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68315392
Iteration 2, loss = 0.65180852
Iteration 3, loss = 0.61816705
Iteration 4, loss = 0.57707672
Iteration 5, loss = 0.52978918
Iteration 6, loss = 0.47967471
Iteration 7, loss = 0.42961455
Iteration 8, loss = 0.38157507
Iteration 9, loss = 0.33677586
Iteration 10, loss = 0.29620477
Iteration 11, loss = 0.26010300
Iteration 12, loss = 0.22848170
Iteration 13, loss = 0.20112193
Iteration 14, loss = 0.17756973
Iteration 15, loss = 0.15759022
Iteration 16, loss = 0.14082462
Iteration 17, loss = 0.12624114
Iteration 18, loss = 0.11380463
Iteration 19, loss = 0.10321845
Iteration 20, loss = 0.09419312
Iteration 21, loss = 0.08623079
Iteration 22, loss = 0.07951533
Iteration 23, loss = 0.07364869
Iteration 24, loss = 0.06820916
Iteration 25, loss = 0.06374073
Iteration 26, loss = 0.05972710
Iteration 27, loss = 0.05608207
Iteration 28, loss = 0.05296696
Iteration 29, loss = 0.05007960
Iteration 30, loss = 0.04753435
Iteration 31, loss = 0.04529314
Iteration 32, loss = 0.04335152
Iteration 33, loss = 0.04144003
Iteration 34, loss = 0.04028799
Iteration 35, loss = 0.03874150
Iteration 36, loss = 0.03754493
Iteration 37, loss = 0.03597772
Iteration 38, loss = 0.03462510
Iteration 39, loss = 0.03350290
Iteration 40, loss = 0.03214018
Iteration 41, loss = 0.03117732
Iteration 42, loss = 0.03023050
Iteration 43, loss = 0.02946596
Iteration 44, loss = 0.02872349
Iteration 45, loss = 0.02785640
Iteration 46, loss = 0.02709298
Iteration 47, loss = 0.02690874
Iteration 48, loss = 0.02637704
Iteration 49, loss = 0.02605524
Iteration 50, loss = 0.02558270
Iteration 51, loss = 0.02495921
Iteration 52, loss = 0.02424362
Iteration 53, loss = 0.02377924
Iteration 54, loss = 0.02335707
Iteration 55, loss = 0.02297881
Iteration 56, loss = 0.02296160
Iteration 57, loss = 0.02238214
Iteration 58, loss = 0.02213410
Iteration 59, loss = 0.02209700
Iteration 60, loss = 0.02168763
Iteration 61, loss = 0.02138870
Iteration 62, loss = 0.02102569
Iteration 63, loss = 0.02069579
Iteration 64, loss = 0.02047667
Iteration 65, loss = 0.02018984
Iteration 66, loss = 0.01993252
Iteration 67, loss = 0.01980116
Iteration 68, loss = 0.01968881
Iteration 69, loss = 0.01930482
Iteration 70, loss = 0.01946214
Iteration 71, loss = 0.01927208
Iteration 72, loss = 0.01902578
Iteration 73, loss = 0.01883846
Iteration 74, loss = 0.01863450
Iteration 75, loss = 0.01846696
Iteration 76, loss = 0.01833081
Iteration 77, loss = 0.01820260
Iteration 78, loss = 0.01813337
Iteration 79, loss = 0.01794334
Iteration 80, loss = 0.01807044
Iteration 81, loss = 0.01825310
Iteration 82, loss = 0.01825275
Iteration 83, loss = 0.01795042
Iteration 84, loss = 0.01763057
Iteration 85, loss = 0.01749327
Iteration 86, loss = 0.01747267
Iteration 87, loss = 0.01774142
Iteration 88, loss = 0.01776720
Iteration 89, loss = 0.01750800
Iteration 90, loss = 0.01736847
Iteration 91, loss = 0.01694243
Iteration 92, loss = 0.01687712
Iteration 93, loss = 0.01685935
Iteration 94, loss = 0.01696406
Iteration 95, loss = 0.01693827
Iteration 96, loss = 0.01687271
Iteration 97, loss = 0.01672988
Iteration 98, loss = 0.01657792
Iteration 99, loss = 0.01642938
Iteration 100, loss = 0.01670332
Iteration 101, loss = 0.01662738
Iteration 102, loss = 0.01621293
Iteration 103, loss = 0.01625028
Iteration 104, loss = 0.01642513
Iteration 105, loss = 0.01636161
Iteration 106, loss = 0.01625292
Iteration 107, loss = 0.01610409
Iteration 108, loss = 0.01596134
Iteration 109, loss = 0.01596234
Iteration 110, loss = 0.01584885
Iteration 111, loss = 0.01592569
Iteration 112, loss = 0.01614849
Iteration 113, loss = 0.01602724
Iteration 114, loss = 0.01601236
Iteration 115, loss = 0.01577761
Iteration 116, loss = 0.01564280
Iteration 117, loss = 0.01568854
Iteration 118, loss = 0.01561378
Iteration 119, loss = 0.01556010
Iteration 120, loss = 0.01562673
Iteration 121, loss = 0.01549311
Iteration 122, loss = 0.01558190
Iteration 123, loss = 0.01556318
Iteration 124, loss = 0.01553196
Iteration 125, loss = 0.01569570
Iteration 126, loss = 0.01553871
Iteration 127, loss = 0.01541727
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70621236
Iteration 2, loss = 0.65736273
Iteration 3, loss = 0.58908100
Iteration 4, loss = 0.49778094
Iteration 5, loss = 0.40476894
Iteration 6, loss = 0.32232700
Iteration 7, loss = 0.25469538
Iteration 8, loss = 0.20211730
Iteration 9, loss = 0.16259535
Iteration 10, loss = 0.13315036
Iteration 11, loss = 0.11099372
Iteration 12, loss = 0.09437716
Iteration 13, loss = 0.08162972
Iteration 14, loss = 0.07171228
Iteration 15, loss = 0.06376273
Iteration 16, loss = 0.05732355
Iteration 17, loss = 0.05224588
Iteration 18, loss = 0.04788157
Iteration 19, loss = 0.04437781
Iteration 20, loss = 0.04121267
Iteration 21, loss = 0.03866028
Iteration 22, loss = 0.03642888
Iteration 23, loss = 0.03449842
Iteration 24, loss = 0.03302881
Iteration 25, loss = 0.03156429
Iteration 26, loss = 0.03016592
Iteration 27, loss = 0.02900077
Iteration 28, loss = 0.02807093
Iteration 29, loss = 0.02737361
Iteration 30, loss = 0.02632347
Iteration 31, loss = 0.02575844
Iteration 32, loss = 0.02493809
Iteration 33, loss = 0.02429771
Iteration 34, loss = 0.02393084
Iteration 35, loss = 0.02344416
Iteration 36, loss = 0.02287102
Iteration 37, loss = 0.02243170
Iteration 38, loss = 0.02208679
Iteration 39, loss = 0.02162739
Iteration 40, loss = 0.02159025
Iteration 41, loss = 0.02125876
Iteration 42, loss = 0.02094539
Iteration 43, loss = 0.02072138
Iteration 44, loss = 0.02052627
Iteration 45, loss = 0.02026102
Iteration 46, loss = 0.02012926
Iteration 47, loss = 0.01988209
Iteration 48, loss = 0.01975159
Iteration 49, loss = 0.01957684
Iteration 50, loss = 0.01932937
Iteration 51, loss = 0.01916757
Iteration 52, loss = 0.01891439
Iteration 53, loss = 0.01899772
Iteration 54, loss = 0.01895777
Iteration 55, loss = 0.01889041
Iteration 56, loss = 0.01865844
Iteration 57, loss = 0.01853419
Iteration 58, loss = 0.01842250
Iteration 59, loss = 0.01828360
Iteration 60, loss = 0.01828807
Iteration 61, loss = 0.01809211
Iteration 62, loss = 0.01808652
Iteration 63, loss = 0.01796222
Iteration 64, loss = 0.01788234
Iteration 65, loss = 0.01806573
Iteration 66, loss = 0.01790551
Iteration 67, loss = 0.01765333
Iteration 68, loss = 0.01779387
Iteration 69, loss = 0.01765735
Iteration 70, loss = 0.01785915
Iteration 71, loss = 0.01753917
Iteration 72, loss = 0.01743505
Iteration 73, loss = 0.01736685
Iteration 74, loss = 0.01740492
Iteration 75, loss = 0.01727822
Iteration 76, loss = 0.01751248
Iteration 77, loss = 0.01724184
Iteration 78, loss = 0.01717912
Iteration 79, loss = 0.01726557
Iteration 80, loss = 0.01732301
Iteration 81, loss = 0.01726166
Iteration 82, loss = 0.01705428
Iteration 83, loss = 0.01715933
Iteration 84, loss = 0.01743974
Iteration 85, loss = 0.01694104
Iteration 86, loss = 0.01694701
Iteration 87, loss = 0.01706532
Iteration 88, loss = 0.01693358
Iteration 89, loss = 0.01717682
Iteration 90, loss = 0.01708813
Iteration 91, loss = 0.01714066
Iteration 92, loss = 0.01693149
Iteration 93, loss = 0.01676845
Iteration 94, loss = 0.01677376
Iteration 95, loss = 0.01671451
Iteration 96, loss = 0.01672790
Iteration 97, loss = 0.01655496
Iteration 98, loss = 0.01682572
Iteration 99, loss = 0.01671922
Iteration 100, loss = 0.01677439
Iteration 101, loss = 0.01677161
Iteration 102, loss = 0.01658707
Iteration 103, loss = 0.01662466
Iteration 104, loss = 0.01682587
Iteration 105, loss = 0.01655756
Iteration 106, loss = 0.01659227
Iteration 107, loss = 0.01649347
Iteration 108, loss = 0.01646101
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67956099
Iteration 2, loss = 0.65675027
Iteration 3, loss = 0.62899782
Iteration 4, loss = 0.58926280
Iteration 5, loss = 0.53928544
Iteration 6, loss = 0.48518286
Iteration 7, loss = 0.43196664
Iteration 8, loss = 0.38123978
Iteration 9, loss = 0.33440221
Iteration 10, loss = 0.29234202
Iteration 11, loss = 0.25524457
Iteration 12, loss = 0.22313931
Iteration 13, loss = 0.19556981
Iteration 14, loss = 0.17184950
Iteration 15, loss = 0.15206041
Iteration 16, loss = 0.13516143
Iteration 17, loss = 0.12098116
Iteration 18, loss = 0.10883336
Iteration 19, loss = 0.09839217
Iteration 20, loss = 0.08932385
Iteration 21, loss = 0.08171793
Iteration 22, loss = 0.07519700
Iteration 23, loss = 0.06938481
Iteration 24, loss = 0.06433585
Iteration 25, loss = 0.05997647
Iteration 26, loss = 0.05601184
Iteration 27, loss = 0.05266691
Iteration 28, loss = 0.04959467
Iteration 29, loss = 0.04687924
Iteration 30, loss = 0.04450017
Iteration 31, loss = 0.04227175
Iteration 32, loss = 0.04023574
Iteration 33, loss = 0.03842303
Iteration 34, loss = 0.03662423
Iteration 35, loss = 0.03509533
Iteration 36, loss = 0.03366014
Iteration 37, loss = 0.03234351
Iteration 38, loss = 0.03123138
Iteration 39, loss = 0.03021894
Iteration 40, loss = 0.02923482
Iteration 41, loss = 0.02831030
Iteration 42, loss = 0.02750091
Iteration 43, loss = 0.02696357
Iteration 44, loss = 0.02610135
Iteration 45, loss = 0.02552747
Iteration 46, loss = 0.02489165
Iteration 47, loss = 0.02423966
Iteration 48, loss = 0.02370909
Iteration 49, loss = 0.02318129
Iteration 50, loss = 0.02268322
Iteration 51, loss = 0.02221856
Iteration 52, loss = 0.02179889
Iteration 53, loss = 0.02134773
Iteration 54, loss = 0.02102816
Iteration 55, loss = 0.02063795
Iteration 56, loss = 0.02035275
Iteration 57, loss = 0.02003965
Iteration 58, loss = 0.01979069
Iteration 59, loss = 0.01943605
Iteration 60, loss = 0.01914130
Iteration 61, loss = 0.01886824
Iteration 62, loss = 0.01859280
Iteration 63, loss = 0.01836818
Iteration 64, loss = 0.01815486
Iteration 65, loss = 0.01797053
Iteration 66, loss = 0.01773618
Iteration 67, loss = 0.01753514
Iteration 68, loss = 0.01739750
Iteration 69, loss = 0.01708438
Iteration 70, loss = 0.01721095
Iteration 71, loss = 0.01698542
Iteration 72, loss = 0.01677775
Iteration 73, loss = 0.01663806
Iteration 74, loss = 0.01652405
Iteration 75, loss = 0.01638425
Iteration 76, loss = 0.01668964
Iteration 77, loss = 0.01655019
Iteration 78, loss = 0.01635871
Iteration 79, loss = 0.01610966
Iteration 80, loss = 0.01578053
Iteration 81, loss = 0.01566159
Iteration 82, loss = 0.01559462
Iteration 83, loss = 0.01535271
Iteration 84, loss = 0.01544584
Iteration 85, loss = 0.01536917
Iteration 86, loss = 0.01534968
Iteration 87, loss = 0.01514293
Iteration 88, loss = 0.01502805
Iteration 89, loss = 0.01487710
Iteration 90, loss = 0.01473062
Iteration 91, loss = 0.01477645
Iteration 92, loss = 0.01453693
Iteration 93, loss = 0.01449748
Iteration 94, loss = 0.01443189
Iteration 95, loss = 0.01442049
Iteration 96, loss = 0.01427165
Iteration 97, loss = 0.01426198
Iteration 98, loss = 0.01430971
Iteration 99, loss = 0.01434380
Iteration 100, loss = 0.01389799
Iteration 101, loss = 0.01474013
Iteration 102, loss = 0.01488891
Iteration 103, loss = 0.01484033
Iteration 104, loss = 0.01456472
Iteration 105, loss = 0.01431177
Iteration 106, loss = 0.01435459
Iteration 107, loss = 0.01413746
Iteration 108, loss = 0.01403454
Iteration 109, loss = 0.01388594
Iteration 110, loss = 0.01393588
Iteration 111, loss = 0.01378236
Iteration 112, loss = 0.01369351
Iteration 113, loss = 0.01361386
Iteration 114, loss = 0.01355762
Iteration 115, loss = 0.01342175
Iteration 116, loss = 0.01339447
Iteration 117, loss = 0.01334975
Iteration 118, loss = 0.01338530
Iteration 119, loss = 0.01349942
Iteration 120, loss = 0.01367270
Iteration 121, loss = 0.01355874
Iteration 122, loss = 0.01341986
Iteration 123, loss = 0.01341336
Iteration 124, loss = 0.01355426
Iteration 125, loss = 0.01339761
Iteration 126, loss = 0.01323285
Iteration 127, loss = 0.01336020
Iteration 128, loss = 0.01328167
Iteration 129, loss = 0.01336848
Iteration 130, loss = 0.01350260
Iteration 131, loss = 0.01345742
Iteration 132, loss = 0.01334814
Iteration 133, loss = 0.01314183
Iteration 134, loss = 0.01296945
Iteration 135, loss = 0.01297901
Iteration 136, loss = 0.01313403
Iteration 137, loss = 0.01322732
Iteration 138, loss = 0.01307641
Iteration 139, loss = 0.01292825
Iteration 140, loss = 0.01283949
Iteration 141, loss = 0.01287258
Iteration 142, loss = 0.01266101
Iteration 143, loss = 0.01270537
Iteration 144, loss = 0.01297527
Iteration 145, loss = 0.01316538
Iteration 146, loss = 0.01329652
Iteration 147, loss = 0.01313042
Iteration 148, loss = 0.01303957
Iteration 149, loss = 0.01297591
Iteration 150, loss = 0.01278053
Iteration 151, loss = 0.01268820
Iteration 152, loss = 0.01263720
Iteration 153, loss = 0.01267560
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69876164
Iteration 2, loss = 0.63654453
Iteration 3, loss = 0.54951758
Iteration 4, loss = 0.44251382
Iteration 5, loss = 0.34514875
Iteration 6, loss = 0.26460220
Iteration 7, loss = 0.20322052
Iteration 8, loss = 0.15932753
Iteration 9, loss = 0.12800971
Iteration 10, loss = 0.10564619
Iteration 11, loss = 0.08934196
Iteration 12, loss = 0.07686847
Iteration 13, loss = 0.06741282
Iteration 14, loss = 0.06011792
Iteration 15, loss = 0.05433026
Iteration 16, loss = 0.04945524
Iteration 17, loss = 0.04578955
Iteration 18, loss = 0.04258058
Iteration 19, loss = 0.03992810
Iteration 20, loss = 0.03766937
Iteration 21, loss = 0.03575143
Iteration 22, loss = 0.03400360
Iteration 23, loss = 0.03257021
Iteration 24, loss = 0.03129963
Iteration 25, loss = 0.03033895
Iteration 26, loss = 0.02937903
Iteration 27, loss = 0.02839887
Iteration 28, loss = 0.02775843
Iteration 29, loss = 0.02695950
Iteration 30, loss = 0.02632322
Iteration 31, loss = 0.02565889
Iteration 32, loss = 0.02534401
Iteration 33, loss = 0.02488085
Iteration 34, loss = 0.02456447
Iteration 35, loss = 0.02409682
Iteration 36, loss = 0.02405570
Iteration 37, loss = 0.02363644
Iteration 38, loss = 0.02302045
Iteration 39, loss = 0.02267969
Iteration 40, loss = 0.02237425
Iteration 41, loss = 0.02223136
Iteration 42, loss = 0.02202492
Iteration 43, loss = 0.02180652
Iteration 44, loss = 0.02143902
Iteration 45, loss = 0.02140116
Iteration 46, loss = 0.02106373
Iteration 47, loss = 0.02109122
Iteration 48, loss = 0.02122832
Iteration 49, loss = 0.02065614
Iteration 50, loss = 0.02071271
Iteration 51, loss = 0.02052138
Iteration 52, loss = 0.02048473
Iteration 53, loss = 0.02022631
Iteration 54, loss = 0.02019040
Iteration 55, loss = 0.02000688
Iteration 56, loss = 0.01997339
Iteration 57, loss = 0.01986228
Iteration 58, loss = 0.01969557
Iteration 59, loss = 0.01983777
Iteration 60, loss = 0.01995473
Iteration 61, loss = 0.01961040
Iteration 62, loss = 0.01981671
Iteration 63, loss = 0.01949895
Iteration 64, loss = 0.01942175
Iteration 65, loss = 0.01922286
Iteration 66, loss = 0.01926925
Iteration 67, loss = 0.01925478
Iteration 68, loss = 0.01915460
Iteration 69, loss = 0.01910420
Iteration 70, loss = 0.01911618
Iteration 71, loss = 0.01909069
Iteration 72, loss = 0.01919253
Iteration 73, loss = 0.01909608
Iteration 74, loss = 0.01897924
Iteration 75, loss = 0.01902625
Iteration 76, loss = 0.01892956
Iteration 77, loss = 0.02017593
Iteration 78, loss = 0.02043767
Iteration 79, loss = 0.02005211
Iteration 80, loss = 0.01953633
Iteration 81, loss = 0.01956469
Iteration 82, loss = 0.02063361
Iteration 83, loss = 0.02062337
Iteration 84, loss = 0.01996855
Iteration 85, loss = 0.01976870
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72043396
Iteration 2, loss = 0.68196470
Iteration 3, loss = 0.63531855
Iteration 4, loss = 0.58216317
Iteration 5, loss = 0.52262107
Iteration 6, loss = 0.45982110
Iteration 7, loss = 0.39796786
Iteration 8, loss = 0.34065722
Iteration 9, loss = 0.28959772
Iteration 10, loss = 0.24545104
Iteration 11, loss = 0.20826670
Iteration 12, loss = 0.17718393
Iteration 13, loss = 0.15181731
Iteration 14, loss = 0.13118703
Iteration 15, loss = 0.11425456
Iteration 16, loss = 0.10054595
Iteration 17, loss = 0.08919031
Iteration 18, loss = 0.07980102
Iteration 19, loss = 0.07189534
Iteration 20, loss = 0.06539669
Iteration 21, loss = 0.05976270
Iteration 22, loss = 0.05503020
Iteration 23, loss = 0.05092325
Iteration 24, loss = 0.04731088
Iteration 25, loss = 0.04426757
Iteration 26, loss = 0.04162104
Iteration 27, loss = 0.03924366
Iteration 28, loss = 0.03715221
Iteration 29, loss = 0.03528660
Iteration 30, loss = 0.03356832
Iteration 31, loss = 0.03209462
Iteration 32, loss = 0.03081046
Iteration 33, loss = 0.02953293
Iteration 34, loss = 0.02837836
Iteration 35, loss = 0.02731295
Iteration 36, loss = 0.02652391
Iteration 37, loss = 0.02564384
Iteration 38, loss = 0.02496603
Iteration 39, loss = 0.02427226
Iteration 40, loss = 0.02352196
Iteration 41, loss = 0.02301522
Iteration 42, loss = 0.02242761
Iteration 43, loss = 0.02191380
Iteration 44, loss = 0.02138491
Iteration 45, loss = 0.02094209
Iteration 46, loss = 0.02047497
Iteration 47, loss = 0.02018126
Iteration 48, loss = 0.01975264
Iteration 49, loss = 0.01945064
Iteration 50, loss = 0.01915204
Iteration 51, loss = 0.01877989
Iteration 52, loss = 0.01861135
Iteration 53, loss = 0.01821886
Iteration 54, loss = 0.01797065
Iteration 55, loss = 0.01782155
Iteration 56, loss = 0.01758165
Iteration 57, loss = 0.01734772
Iteration 58, loss = 0.01725738
Iteration 59, loss = 0.01697582
Iteration 60, loss = 0.01681725
Iteration 61, loss = 0.01669790
Iteration 62, loss = 0.01649992
Iteration 63, loss = 0.01637198
Iteration 64, loss = 0.01616770
Iteration 65, loss = 0.01605023
Iteration 66, loss = 0.01591923
Iteration 67, loss = 0.01581298
Iteration 68, loss = 0.01571414
Iteration 69, loss = 0.01558953
Iteration 70, loss = 0.01559438
Iteration 71, loss = 0.01545453
Iteration 72, loss = 0.01530091
Iteration 73, loss = 0.01515989
Iteration 74, loss = 0.01515543
Iteration 75, loss = 0.01502152
Iteration 76, loss = 0.01501154
Iteration 77, loss = 0.01497195
Iteration 78, loss = 0.01490389
Iteration 79, loss = 0.01478812
Iteration 80, loss = 0.01469376
Iteration 81, loss = 0.01464790
Iteration 82, loss = 0.01456891
Iteration 83, loss = 0.01442348
Iteration 84, loss = 0.01439828
Iteration 85, loss = 0.01435920
Iteration 86, loss = 0.01428878
Iteration 87, loss = 0.01427210
Iteration 88, loss = 0.01423388
Iteration 89, loss = 0.01410979
Iteration 90, loss = 0.01399040
Iteration 91, loss = 0.01397968
Iteration 92, loss = 0.01405336
Iteration 93, loss = 0.01396361
Iteration 94, loss = 0.01386580
Iteration 95, loss = 0.01381919
Iteration 96, loss = 0.01382884
Iteration 97, loss = 0.01382156
Iteration 98, loss = 0.01385585
Iteration 99, loss = 0.01377465
Iteration 100, loss = 0.01361533
Iteration 101, loss = 0.01357241
Iteration 102, loss = 0.01359707
Iteration 103, loss = 0.01358621
Iteration 104, loss = 0.01375871
Iteration 105, loss = 0.01347544
Iteration 106, loss = 0.01355874
Iteration 107, loss = 0.01345572
Iteration 108, loss = 0.01338780
Iteration 109, loss = 0.01343772
Iteration 110, loss = 0.01328697
Iteration 111, loss = 0.01327475
Iteration 112, loss = 0.01332374
Iteration 113, loss = 0.01324174
Iteration 114, loss = 0.01344525
Iteration 115, loss = 0.01330625
Iteration 116, loss = 0.01319732
Iteration 117, loss = 0.01333293
Iteration 118, loss = 0.01322175
Iteration 119, loss = 0.01319478
Iteration 120, loss = 0.01317698
Iteration 121, loss = 0.01327120
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69919608
Iteration 2, loss = 0.68108545
Iteration 3, loss = 0.66624111
Iteration 4, loss = 0.65169117
Iteration 5, loss = 0.63653890
Iteration 6, loss = 0.62035244
Iteration 7, loss = 0.60277120
Iteration 8, loss = 0.58390654
Iteration 9, loss = 0.56339063
Iteration 10, loss = 0.54140018
Iteration 11, loss = 0.51813361
Iteration 12, loss = 0.49385340
Iteration 13, loss = 0.46889417
Iteration 14, loss = 0.44366411
Iteration 15, loss = 0.41840023
Iteration 16, loss = 0.39368199
Iteration 17, loss = 0.36975148
Iteration 18, loss = 0.34646267
Iteration 19, loss = 0.32428681
Iteration 20, loss = 0.30328430
Iteration 21, loss = 0.28332259
Iteration 22, loss = 0.26447145
Iteration 23, loss = 0.24686782
Iteration 24, loss = 0.23033489
Iteration 25, loss = 0.21497948
Iteration 26, loss = 0.20077185
Iteration 27, loss = 0.18753668
Iteration 28, loss = 0.17521743
Iteration 29, loss = 0.16398633
Iteration 30, loss = 0.15364381
Iteration 31, loss = 0.14403773
Iteration 32, loss = 0.13524192
Iteration 33, loss = 0.12713779
Iteration 34, loss = 0.11970652
Iteration 35, loss = 0.11286377
Iteration 36, loss = 0.10651657
Iteration 37, loss = 0.10071939
Iteration 38, loss = 0.09534037
Iteration 39, loss = 0.09042391
Iteration 40, loss = 0.08589777
Iteration 41, loss = 0.08166518
Iteration 42, loss = 0.07774618
Iteration 43, loss = 0.07417075
Iteration 44, loss = 0.07081325
Iteration 45, loss = 0.06768619
Iteration 46, loss = 0.06477184
Iteration 47, loss = 0.06205905
Iteration 48, loss = 0.05956606
Iteration 49, loss = 0.05720140
Iteration 50, loss = 0.05500373
Iteration 51, loss = 0.05296347
Iteration 52, loss = 0.05102261
Iteration 53, loss = 0.04920833
Iteration 54, loss = 0.04749772
Iteration 55, loss = 0.04589809
Iteration 56, loss = 0.04439786
Iteration 57, loss = 0.04301178
Iteration 58, loss = 0.04163685
Iteration 59, loss = 0.04038353
Iteration 60, loss = 0.03914962
Iteration 61, loss = 0.03798384
Iteration 62, loss = 0.03690498
Iteration 63, loss = 0.03588485
Iteration 64, loss = 0.03490442
Iteration 65, loss = 0.03397764
Iteration 66, loss = 0.03311383
Iteration 67, loss = 0.03226150
Iteration 68, loss = 0.03146274
Iteration 69, loss = 0.03070402
Iteration 70, loss = 0.02995875
Iteration 71, loss = 0.02926054
Iteration 72, loss = 0.02860264
Iteration 73, loss = 0.02795099
Iteration 74, loss = 0.02733562
Iteration 75, loss = 0.02674080
Iteration 76, loss = 0.02618290
Iteration 77, loss = 0.02563532
Iteration 78, loss = 0.02514451
Iteration 79, loss = 0.02461339
Iteration 80, loss = 0.02415140
Iteration 81, loss = 0.02370566
Iteration 82, loss = 0.02328083
Iteration 83, loss = 0.02283271
Iteration 84, loss = 0.02242246
Iteration 85, loss = 0.02202964
Iteration 86, loss = 0.02165206
Iteration 87, loss = 0.02131221
Iteration 88, loss = 0.02093337
Iteration 89, loss = 0.02059093
Iteration 90, loss = 0.02027957
Iteration 91, loss = 0.01998636
Iteration 92, loss = 0.01970037
Iteration 93, loss = 0.01939607
Iteration 94, loss = 0.01912331
Iteration 95, loss = 0.01884347
Iteration 96, loss = 0.01858708
Iteration 97, loss = 0.01837113
Iteration 98, loss = 0.01805914
Iteration 99, loss = 0.01781777
Iteration 100, loss = 0.01760503
Iteration 101, loss = 0.01737092
Iteration 102, loss = 0.01715035
Iteration 103, loss = 0.01695954
Iteration 104, loss = 0.01673511
Iteration 105, loss = 0.01653243
Iteration 106, loss = 0.01633976
Iteration 107, loss = 0.01613104
Iteration 108, loss = 0.01595146
Iteration 109, loss = 0.01575832
Iteration 110, loss = 0.01559425
Iteration 111, loss = 0.01548231
Iteration 112, loss = 0.01533900
Iteration 113, loss = 0.01513750
Iteration 114, loss = 0.01501028
Iteration 115, loss = 0.01486992
Iteration 116, loss = 0.01468412
Iteration 117, loss = 0.01453008
Iteration 118, loss = 0.01438751
Iteration 119, loss = 0.01436224
Iteration 120, loss = 0.01417514
Iteration 121, loss = 0.01403114
Iteration 122, loss = 0.01389530
Iteration 123, loss = 0.01377167
Iteration 124, loss = 0.01363436
Iteration 125, loss = 0.01356820
Iteration 126, loss = 0.01341525
Iteration 127, loss = 0.01330779
Iteration 128, loss = 0.01323192
Iteration 129, loss = 0.01310744
Iteration 130, loss = 0.01298985
Iteration 131, loss = 0.01289257
Iteration 132, loss = 0.01282934
Iteration 133, loss = 0.01275331
Iteration 134, loss = 0.01263244
Iteration 135, loss = 0.01257139
Iteration 136, loss = 0.01245200
Iteration 137, loss = 0.01236829
Iteration 138, loss = 0.01230677
Iteration 139, loss = 0.01219189
Iteration 140, loss = 0.01212022
Iteration 141, loss = 0.01204210
Iteration 142, loss = 0.01202435
Iteration 143, loss = 0.01190066
Iteration 144, loss = 0.01181622
Iteration 145, loss = 0.01175681
Iteration 146, loss = 0.01168416
Iteration 147, loss = 0.01160262
Iteration 148, loss = 0.01157250
Iteration 149, loss = 0.01144691
Iteration 150, loss = 0.01138781
Iteration 151, loss = 0.01134538
Iteration 152, loss = 0.01124561
Iteration 153, loss = 0.01120795
Iteration 154, loss = 0.01114927
Iteration 155, loss = 0.01108881
Iteration 156, loss = 0.01106727
Iteration 157, loss = 0.01099581
Iteration 158, loss = 0.01093506
Iteration 159, loss = 0.01091285
Iteration 160, loss = 0.01084692
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67289186
Iteration 2, loss = 0.65007975
Iteration 3, loss = 0.62855378
Iteration 4, loss = 0.60433021
Iteration 5, loss = 0.57604309
Iteration 6, loss = 0.54333519
Iteration 7, loss = 0.50740950
Iteration 8, loss = 0.46960805
Iteration 9, loss = 0.43169868
Iteration 10, loss = 0.39520152
Iteration 11, loss = 0.36043879
Iteration 12, loss = 0.32793897
Iteration 13, loss = 0.29791943
Iteration 14, loss = 0.27029484
Iteration 15, loss = 0.24523148
Iteration 16, loss = 0.22243876
Iteration 17, loss = 0.20189305
Iteration 18, loss = 0.18355880
Iteration 19, loss = 0.16711795
Iteration 20, loss = 0.15251922
Iteration 21, loss = 0.13947068
Iteration 22, loss = 0.12796399
Iteration 23, loss = 0.11770993
Iteration 24, loss = 0.10852682
Iteration 25, loss = 0.10040290
Iteration 26, loss = 0.09312708
Iteration 27, loss = 0.08663856
Iteration 28, loss = 0.08083669
Iteration 29, loss = 0.07566221
Iteration 30, loss = 0.07097676
Iteration 31, loss = 0.06673019
Iteration 32, loss = 0.06291000
Iteration 33, loss = 0.05948343
Iteration 34, loss = 0.05631626
Iteration 35, loss = 0.05349750
Iteration 36, loss = 0.05083567
Iteration 37, loss = 0.04844929
Iteration 38, loss = 0.04629729
Iteration 39, loss = 0.04424522
Iteration 40, loss = 0.04238771
Iteration 41, loss = 0.04065848
Iteration 42, loss = 0.03912034
Iteration 43, loss = 0.03763038
Iteration 44, loss = 0.03625852
Iteration 45, loss = 0.03504137
Iteration 46, loss = 0.03380229
Iteration 47, loss = 0.03271892
Iteration 48, loss = 0.03167872
Iteration 49, loss = 0.03072248
Iteration 50, loss = 0.02981629
Iteration 51, loss = 0.02904338
Iteration 52, loss = 0.02820396
Iteration 53, loss = 0.02744415
Iteration 54, loss = 0.02677529
Iteration 55, loss = 0.02607446
Iteration 56, loss = 0.02545265
Iteration 57, loss = 0.02486729
Iteration 58, loss = 0.02431559
Iteration 59, loss = 0.02381698
Iteration 60, loss = 0.02327862
Iteration 61, loss = 0.02280284
Iteration 62, loss = 0.02233256
Iteration 63, loss = 0.02192979
Iteration 64, loss = 0.02152498
Iteration 65, loss = 0.02113921
Iteration 66, loss = 0.02077487
Iteration 67, loss = 0.02041820
Iteration 68, loss = 0.02008949
Iteration 69, loss = 0.01975967
Iteration 70, loss = 0.01945970
Iteration 71, loss = 0.01915773
Iteration 72, loss = 0.01894504
Iteration 73, loss = 0.01861183
Iteration 74, loss = 0.01836002
Iteration 75, loss = 0.01818407
Iteration 76, loss = 0.01787514
Iteration 77, loss = 0.01766087
Iteration 78, loss = 0.01743406
Iteration 79, loss = 0.01725387
Iteration 80, loss = 0.01699566
Iteration 81, loss = 0.01681239
Iteration 82, loss = 0.01663255
Iteration 83, loss = 0.01647095
Iteration 84, loss = 0.01628341
Iteration 85, loss = 0.01614023
Iteration 86, loss = 0.01596716
Iteration 87, loss = 0.01580988
Iteration 88, loss = 0.01565590
Iteration 89, loss = 0.01553536
Iteration 90, loss = 0.01536579
Iteration 91, loss = 0.01522350
Iteration 92, loss = 0.01519216
Iteration 93, loss = 0.01503428
Iteration 94, loss = 0.01487207
Iteration 95, loss = 0.01479173
Iteration 96, loss = 0.01472048
Iteration 97, loss = 0.01452417
Iteration 98, loss = 0.01442334
Iteration 99, loss = 0.01438528
Iteration 100, loss = 0.01423717
Iteration 101, loss = 0.01417358
Iteration 102, loss = 0.01402415
Iteration 103, loss = 0.01396477
Iteration 104, loss = 0.01387398
Iteration 105, loss = 0.01379157
Iteration 106, loss = 0.01373978
Iteration 107, loss = 0.01361670
Iteration 108, loss = 0.01354049
Iteration 109, loss = 0.01343237
Iteration 110, loss = 0.01340212
Iteration 111, loss = 0.01333121
Iteration 112, loss = 0.01323712
Iteration 113, loss = 0.01322029
Iteration 114, loss = 0.01310066
Iteration 115, loss = 0.01308509
Iteration 116, loss = 0.01299138
Iteration 117, loss = 0.01293415
Iteration 118, loss = 0.01285401
Iteration 119, loss = 0.01281762
Iteration 120, loss = 0.01275615
Iteration 121, loss = 0.01272237
Iteration 122, loss = 0.01266464
Iteration 123, loss = 0.01265096
Iteration 124, loss = 0.01255673
Iteration 125, loss = 0.01248734
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72160666
Iteration 2, loss = 0.66368715
Iteration 3, loss = 0.58626971
Iteration 4, loss = 0.49071169
Iteration 5, loss = 0.39596888
Iteration 6, loss = 0.31227569
Iteration 7, loss = 0.24455013
Iteration 8, loss = 0.19295644
Iteration 9, loss = 0.15480238
Iteration 10, loss = 0.12659514
Iteration 11, loss = 0.10590394
Iteration 12, loss = 0.09005188
Iteration 13, loss = 0.07803768
Iteration 14, loss = 0.06871743
Iteration 15, loss = 0.06131760
Iteration 16, loss = 0.05509290
Iteration 17, loss = 0.05038590
Iteration 18, loss = 0.04632532
Iteration 19, loss = 0.04298617
Iteration 20, loss = 0.04001631
Iteration 21, loss = 0.03782057
Iteration 22, loss = 0.03572035
Iteration 23, loss = 0.03370537
Iteration 24, loss = 0.03225419
Iteration 25, loss = 0.03089658
Iteration 26, loss = 0.02969927
Iteration 27, loss = 0.02840838
Iteration 28, loss = 0.02754682
Iteration 29, loss = 0.02665957
Iteration 30, loss = 0.02590219
Iteration 31, loss = 0.02530481
Iteration 32, loss = 0.02453151
Iteration 33, loss = 0.02410964
Iteration 34, loss = 0.02373659
Iteration 35, loss = 0.02305314
Iteration 36, loss = 0.02262196
Iteration 37, loss = 0.02217641
Iteration 38, loss = 0.02185612
Iteration 39, loss = 0.02152876
Iteration 40, loss = 0.02117982
Iteration 41, loss = 0.02103880
Iteration 42, loss = 0.02071737
Iteration 43, loss = 0.02052678
Iteration 44, loss = 0.02040166
Iteration 45, loss = 0.01996491
Iteration 46, loss = 0.01981211
Iteration 47, loss = 0.01983837
Iteration 48, loss = 0.01965139
Iteration 49, loss = 0.01935054
Iteration 50, loss = 0.01915142
Iteration 51, loss = 0.01902649
Iteration 52, loss = 0.01876157
Iteration 53, loss = 0.01870413
Iteration 54, loss = 0.01883591
Iteration 55, loss = 0.01853443
Iteration 56, loss = 0.01846169
Iteration 57, loss = 0.01839810
Iteration 58, loss = 0.01833740
Iteration 59, loss = 0.01835133
Iteration 60, loss = 0.01819055
Iteration 61, loss = 0.01806776
Iteration 62, loss = 0.01804179
Iteration 63, loss = 0.01786504
Iteration 64, loss = 0.01786434
Iteration 65, loss = 0.01774432
Iteration 66, loss = 0.01749696
Iteration 67, loss = 0.01768508
Iteration 68, loss = 0.01752980
Iteration 69, loss = 0.01767085
Iteration 70, loss = 0.01744979
Iteration 71, loss = 0.01786280
Iteration 72, loss = 0.01741262
Iteration 73, loss = 0.01733353
Iteration 74, loss = 0.01752003
Iteration 75, loss = 0.01738635
Iteration 76, loss = 0.01733897
Iteration 77, loss = 0.01718621
Iteration 78, loss = 0.01729381
Iteration 79, loss = 0.01718273
Iteration 80, loss = 0.01714871
Iteration 81, loss = 0.01720858
Iteration 82, loss = 0.01711641
Iteration 83, loss = 0.01704832
Iteration 84, loss = 0.01710357
Iteration 85, loss = 0.01688305
Iteration 86, loss = 0.01701166
Iteration 87, loss = 0.01692562
Iteration 88, loss = 0.01697607
Iteration 89, loss = 0.01684487
Iteration 90, loss = 0.01690570
Iteration 91, loss = 0.01666510
Iteration 92, loss = 0.01678943
Iteration 93, loss = 0.01677991
Iteration 94, loss = 0.01651982
Iteration 95, loss = 0.01669746
Iteration 96, loss = 0.01667368
Iteration 97, loss = 0.01660039
Iteration 98, loss = 0.01651650
Iteration 99, loss = 0.01666865
Iteration 100, loss = 0.01679366
Iteration 101, loss = 0.01643692
Iteration 102, loss = 0.01670019
Iteration 103, loss = 0.01659653
Iteration 104, loss = 0.01650824
Iteration 105, loss = 0.01665577
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71661176
Iteration 2, loss = 0.63357927
Iteration 3, loss = 0.52561305
Iteration 4, loss = 0.40884489
Iteration 5, loss = 0.30852650
Iteration 6, loss = 0.23149877
Iteration 7, loss = 0.17577782
Iteration 8, loss = 0.13706890
Iteration 9, loss = 0.10994137
Iteration 10, loss = 0.09103761
Iteration 11, loss = 0.07682875
Iteration 12, loss = 0.06661237
Iteration 13, loss = 0.05853775
Iteration 14, loss = 0.05213205
Iteration 15, loss = 0.04761778
Iteration 16, loss = 0.04360174
Iteration 17, loss = 0.04006507
Iteration 18, loss = 0.03734007
Iteration 19, loss = 0.03516169
Iteration 20, loss = 0.03340943
Iteration 21, loss = 0.03158252
Iteration 22, loss = 0.03018248
Iteration 23, loss = 0.02873005
Iteration 24, loss = 0.02767225
Iteration 25, loss = 0.02684387
Iteration 26, loss = 0.02596526
Iteration 27, loss = 0.02517457
Iteration 28, loss = 0.02484141
Iteration 29, loss = 0.02401386
Iteration 30, loss = 0.02361822
Iteration 31, loss = 0.02321048
Iteration 32, loss = 0.02284685
Iteration 33, loss = 0.02241645
Iteration 34, loss = 0.02210264
Iteration 35, loss = 0.02164427
Iteration 36, loss = 0.02136958
Iteration 37, loss = 0.02119806
Iteration 38, loss = 0.02088762
Iteration 39, loss = 0.02088381
Iteration 40, loss = 0.02068850
Iteration 41, loss = 0.02020199
Iteration 42, loss = 0.02008120
Iteration 43, loss = 0.01995215
Iteration 44, loss = 0.02000553
Iteration 45, loss = 0.01977541
Iteration 46, loss = 0.01925918
Iteration 47, loss = 0.01969271
Iteration 48, loss = 0.01917005
Iteration 49, loss = 0.01916778
Iteration 50, loss = 0.01928542
Iteration 51, loss = 0.01908323
Iteration 52, loss = 0.01891613
Iteration 53, loss = 0.01886116
Iteration 54, loss = 0.01886290
Iteration 55, loss = 0.01898950
Iteration 56, loss = 0.01853584
Iteration 57, loss = 0.01851144
Iteration 58, loss = 0.01846619
Iteration 59, loss = 0.01840510
Iteration 60, loss = 0.01839540
Iteration 61, loss = 0.01847337
Iteration 62, loss = 0.01845187
Iteration 63, loss = 0.01829538
Iteration 64, loss = 0.01813801
Iteration 65, loss = 0.01841192
Iteration 66, loss = 0.01813649
Iteration 67, loss = 0.01798257
Iteration 68, loss = 0.01768021
Iteration 69, loss = 0.01781167
Iteration 70, loss = 0.01805981
Iteration 71, loss = 0.01773963
Iteration 72, loss = 0.01798818
Iteration 73, loss = 0.01775909
Iteration 74, loss = 0.01778854
Iteration 75, loss = 0.01776916
Iteration 76, loss = 0.01768584
Iteration 77, loss = 0.01779470
Iteration 78, loss = 0.01744611
Iteration 79, loss = 0.01757844
Iteration 80, loss = 0.01749661
Iteration 81, loss = 0.01772818
Iteration 82, loss = 0.01795944
Iteration 83, loss = 0.01744031
Iteration 84, loss = 0.01751710
Iteration 85, loss = 0.01746819
Iteration 86, loss = 0.01779502
Iteration 87, loss = 0.01751158
Iteration 88, loss = 0.01755329
Iteration 89, loss = 0.01771823
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67490182
Iteration 2, loss = 0.65141040
Iteration 3, loss = 0.62298420
Iteration 4, loss = 0.58321076
Iteration 5, loss = 0.53469098
Iteration 6, loss = 0.48243359
Iteration 7, loss = 0.43054895
Iteration 8, loss = 0.38096909
Iteration 9, loss = 0.33536206
Iteration 10, loss = 0.29345164
Iteration 11, loss = 0.25688272
Iteration 12, loss = 0.22438173
Iteration 13, loss = 0.19654009
Iteration 14, loss = 0.17295055
Iteration 15, loss = 0.15258254
Iteration 16, loss = 0.13537312
Iteration 17, loss = 0.12109929
Iteration 18, loss = 0.10865748
Iteration 19, loss = 0.09810072
Iteration 20, loss = 0.08901292
Iteration 21, loss = 0.08118204
Iteration 22, loss = 0.07462997
Iteration 23, loss = 0.06862081
Iteration 24, loss = 0.06348971
Iteration 25, loss = 0.05893583
Iteration 26, loss = 0.05497678
Iteration 27, loss = 0.05151659
Iteration 28, loss = 0.04856752
Iteration 29, loss = 0.04594333
Iteration 30, loss = 0.04344564
Iteration 31, loss = 0.04131764
Iteration 32, loss = 0.03920073
Iteration 33, loss = 0.03732292
Iteration 34, loss = 0.03565619
Iteration 35, loss = 0.03431645
Iteration 36, loss = 0.03296066
Iteration 37, loss = 0.03175770
Iteration 38, loss = 0.03040479
Iteration 39, loss = 0.02939674
Iteration 40, loss = 0.02842693
Iteration 41, loss = 0.02754487
Iteration 42, loss = 0.02681346
Iteration 43, loss = 0.02587495
Iteration 44, loss = 0.02540558
Iteration 45, loss = 0.02476316
Iteration 46, loss = 0.02413850
Iteration 47, loss = 0.02343517
Iteration 48, loss = 0.02291196
Iteration 49, loss = 0.02246536
Iteration 50, loss = 0.02201454
Iteration 51, loss = 0.02147056
Iteration 52, loss = 0.02103340
Iteration 53, loss = 0.02061771
Iteration 54, loss = 0.02026795
Iteration 55, loss = 0.01989816
Iteration 56, loss = 0.01953442
Iteration 57, loss = 0.01926061
Iteration 58, loss = 0.01902032
Iteration 59, loss = 0.01859983
Iteration 60, loss = 0.01861037
Iteration 61, loss = 0.01838218
Iteration 62, loss = 0.01810849
Iteration 63, loss = 0.01785469
Iteration 64, loss = 0.01751165
Iteration 65, loss = 0.01739372
Iteration 66, loss = 0.01723615
Iteration 67, loss = 0.01699758
Iteration 68, loss = 0.01681785
Iteration 69, loss = 0.01665341
Iteration 70, loss = 0.01648281
Iteration 71, loss = 0.01635246
Iteration 72, loss = 0.01624084
Iteration 73, loss = 0.01620620
Iteration 74, loss = 0.01610280
Iteration 75, loss = 0.01596711
Iteration 76, loss = 0.01578705
Iteration 77, loss = 0.01558023
Iteration 78, loss = 0.01538351
Iteration 79, loss = 0.01528252
Iteration 80, loss = 0.01510681
Iteration 81, loss = 0.01507447
Iteration 82, loss = 0.01492167
Iteration 83, loss = 0.01475995
Iteration 84, loss = 0.01482152
Iteration 85, loss = 0.01477594
Iteration 86, loss = 0.01461804
Iteration 87, loss = 0.01451966
Iteration 88, loss = 0.01440929
Iteration 89, loss = 0.01439108
Iteration 90, loss = 0.01425868
Iteration 91, loss = 0.01416372
Iteration 92, loss = 0.01412158
Iteration 93, loss = 0.01403822
Iteration 94, loss = 0.01400958
Iteration 95, loss = 0.01390700
Iteration 96, loss = 0.01396172
Iteration 97, loss = 0.01396722
Iteration 98, loss = 0.01378611
Iteration 99, loss = 0.01377728
Iteration 100, loss = 0.01379971
Iteration 101, loss = 0.01380849
Iteration 102, loss = 0.01366984
Iteration 103, loss = 0.01343823
Iteration 104, loss = 0.01386404
Iteration 105, loss = 0.01372425
Iteration 106, loss = 0.01372330
Iteration 107, loss = 0.01351809
Iteration 108, loss = 0.01341171
Iteration 109, loss = 0.01362607
Iteration 110, loss = 0.01372752
Iteration 111, loss = 0.01376701
Iteration 112, loss = 0.01356371
Iteration 113, loss = 0.01332061
Iteration 114, loss = 0.01313500
Iteration 115, loss = 0.01296359
Iteration 116, loss = 0.01298970
Iteration 117, loss = 0.01298645
Iteration 118, loss = 0.01300605
Iteration 119, loss = 0.01307838
Iteration 120, loss = 0.01296449
Iteration 121, loss = 0.01318244
Iteration 122, loss = 0.01319107
Iteration 123, loss = 0.01331940
Iteration 124, loss = 0.01345556
Iteration 125, loss = 0.01329553
Iteration 126, loss = 0.01293323
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70900506
Iteration 2, loss = 0.64233869
Iteration 3, loss = 0.54221647
Iteration 4, loss = 0.42533734
Iteration 5, loss = 0.32127721
Iteration 6, loss = 0.23991081
Iteration 7, loss = 0.18166453
Iteration 8, loss = 0.14100159
Iteration 9, loss = 0.11276086
Iteration 10, loss = 0.09267286
Iteration 11, loss = 0.07848766
Iteration 12, loss = 0.06764482
Iteration 13, loss = 0.05929288
Iteration 14, loss = 0.05306354
Iteration 15, loss = 0.04808890
Iteration 16, loss = 0.04385885
Iteration 17, loss = 0.04064296
Iteration 18, loss = 0.03781641
Iteration 19, loss = 0.03558032
Iteration 20, loss = 0.03365840
Iteration 21, loss = 0.03192364
Iteration 22, loss = 0.03045247
Iteration 23, loss = 0.02927493
Iteration 24, loss = 0.02825953
Iteration 25, loss = 0.02717701
Iteration 26, loss = 0.02656530
Iteration 27, loss = 0.02557177
Iteration 28, loss = 0.02502524
Iteration 29, loss = 0.02463683
Iteration 30, loss = 0.02421518
Iteration 31, loss = 0.02336507
Iteration 32, loss = 0.02309564
Iteration 33, loss = 0.02269892
Iteration 34, loss = 0.02253210
Iteration 35, loss = 0.02207047
Iteration 36, loss = 0.02175647
Iteration 37, loss = 0.02177939
Iteration 38, loss = 0.02120246
Iteration 39, loss = 0.02122425
Iteration 40, loss = 0.02100372
Iteration 41, loss = 0.02066900
Iteration 42, loss = 0.02041592
Iteration 43, loss = 0.02033499
Iteration 44, loss = 0.02000281
Iteration 45, loss = 0.02000446
Iteration 46, loss = 0.01990687
Iteration 47, loss = 0.01991905
Iteration 48, loss = 0.01949187
Iteration 49, loss = 0.01942335
Iteration 50, loss = 0.01982068
Iteration 51, loss = 0.01965158
Iteration 52, loss = 0.01936148
Iteration 53, loss = 0.01923070
Iteration 54, loss = 0.01906464
Iteration 55, loss = 0.01898427
Iteration 56, loss = 0.01885651
Iteration 57, loss = 0.01893059
Iteration 58, loss = 0.01874485
Iteration 59, loss = 0.01882954
Iteration 60, loss = 0.01871355
Iteration 61, loss = 0.01838175
Iteration 62, loss = 0.01849445
Iteration 63, loss = 0.01854222
Iteration 64, loss = 0.01850466
Iteration 65, loss = 0.01874210
Iteration 66, loss = 0.01832579
Iteration 67, loss = 0.01830060
Iteration 68, loss = 0.01814435
Iteration 69, loss = 0.01843860
Iteration 70, loss = 0.01818686
Iteration 71, loss = 0.01814710
Iteration 72, loss = 0.01812484
Iteration 73, loss = 0.01775222
Iteration 74, loss = 0.01791509
Iteration 75, loss = 0.01851816
Iteration 76, loss = 0.01801870
Iteration 77, loss = 0.01798900
Iteration 78, loss = 0.01796253
Iteration 79, loss = 0.01798762
Iteration 80, loss = 0.01810723
Iteration 81, loss = 0.01810099
Iteration 82, loss = 0.01805457
Iteration 83, loss = 0.01780652
Iteration 84, loss = 0.01794398
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71748238
Iteration 2, loss = 0.68196446
Iteration 3, loss = 0.63539548
Iteration 4, loss = 0.58051134
Iteration 5, loss = 0.51982969
Iteration 6, loss = 0.45753425
Iteration 7, loss = 0.39655533
Iteration 8, loss = 0.33968491
Iteration 9, loss = 0.28932162
Iteration 10, loss = 0.24559506
Iteration 11, loss = 0.20869206
Iteration 12, loss = 0.17804983
Iteration 13, loss = 0.15272684
Iteration 14, loss = 0.13207488
Iteration 15, loss = 0.11501760
Iteration 16, loss = 0.10107179
Iteration 17, loss = 0.08947997
Iteration 18, loss = 0.07995627
Iteration 19, loss = 0.07201129
Iteration 20, loss = 0.06515627
Iteration 21, loss = 0.05945109
Iteration 22, loss = 0.05462808
Iteration 23, loss = 0.05034209
Iteration 24, loss = 0.04676423
Iteration 25, loss = 0.04354445
Iteration 26, loss = 0.04078806
Iteration 27, loss = 0.03830810
Iteration 28, loss = 0.03621853
Iteration 29, loss = 0.03422245
Iteration 30, loss = 0.03256203
Iteration 31, loss = 0.03098321
Iteration 32, loss = 0.02954962
Iteration 33, loss = 0.02831560
Iteration 34, loss = 0.02716286
Iteration 35, loss = 0.02611425
Iteration 36, loss = 0.02532244
Iteration 37, loss = 0.02434829
Iteration 38, loss = 0.02365115
Iteration 39, loss = 0.02287946
Iteration 40, loss = 0.02227795
Iteration 41, loss = 0.02162491
Iteration 42, loss = 0.02113070
Iteration 43, loss = 0.02045113
Iteration 44, loss = 0.01999760
Iteration 45, loss = 0.01963660
Iteration 46, loss = 0.01914946
Iteration 47, loss = 0.01879138
Iteration 48, loss = 0.01836294
Iteration 49, loss = 0.01801652
Iteration 50, loss = 0.01769406
Iteration 51, loss = 0.01733818
Iteration 52, loss = 0.01716807
Iteration 53, loss = 0.01692767
Iteration 54, loss = 0.01667666
Iteration 55, loss = 0.01638283
Iteration 56, loss = 0.01618738
Iteration 57, loss = 0.01602696
Iteration 58, loss = 0.01577814
Iteration 59, loss = 0.01555374
Iteration 60, loss = 0.01532830
Iteration 61, loss = 0.01525972
Iteration 62, loss = 0.01504784
Iteration 63, loss = 0.01489305
Iteration 64, loss = 0.01471544
Iteration 65, loss = 0.01471010
Iteration 66, loss = 0.01445680
Iteration 67, loss = 0.01429978
Iteration 68, loss = 0.01423729
Iteration 69, loss = 0.01406235
Iteration 70, loss = 0.01401424
Iteration 71, loss = 0.01391771
Iteration 72, loss = 0.01386028
Iteration 73, loss = 0.01365916
Iteration 74, loss = 0.01366511
Iteration 75, loss = 0.01354419
Iteration 76, loss = 0.01345525
Iteration 77, loss = 0.01333227
Iteration 78, loss = 0.01337657
Iteration 79, loss = 0.01317498
Iteration 80, loss = 0.01319704
Iteration 81, loss = 0.01305945
Iteration 82, loss = 0.01296909
Iteration 83, loss = 0.01286043
Iteration 84, loss = 0.01287605
Iteration 85, loss = 0.01280427
Iteration 86, loss = 0.01275065
Iteration 87, loss = 0.01270911
Iteration 88, loss = 0.01259053
Iteration 89, loss = 0.01262937
Iteration 90, loss = 0.01255424
Iteration 91, loss = 0.01259891
Iteration 92, loss = 0.01251464
Iteration 93, loss = 0.01241851
Iteration 94, loss = 0.01259933
Iteration 95, loss = 0.01237907
Iteration 96, loss = 0.01230516
Iteration 97, loss = 0.01227020
Iteration 98, loss = 0.01229112
Iteration 99, loss = 0.01229255
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67694556
Iteration 2, loss = 0.61836686
Iteration 3, loss = 0.52815826
Iteration 4, loss = 0.43042236
Iteration 5, loss = 0.33986111
Iteration 6, loss = 0.26381435
Iteration 7, loss = 0.20502543
Iteration 8, loss = 0.16187144
Iteration 9, loss = 0.13044960
Iteration 10, loss = 0.10764634
Iteration 11, loss = 0.09129137
Iteration 12, loss = 0.07842386
Iteration 13, loss = 0.06881264
Iteration 14, loss = 0.06123635
Iteration 15, loss = 0.05537707
Iteration 16, loss = 0.05053707
Iteration 17, loss = 0.04671634
Iteration 18, loss = 0.04349796
Iteration 19, loss = 0.04052920
Iteration 20, loss = 0.03817327
Iteration 21, loss = 0.03621299
Iteration 22, loss = 0.03467801
Iteration 23, loss = 0.03284045
Iteration 24, loss = 0.03169467
Iteration 25, loss = 0.03059450
Iteration 26, loss = 0.02965589
Iteration 27, loss = 0.02852535
Iteration 28, loss = 0.02778319
Iteration 29, loss = 0.02747575
Iteration 30, loss = 0.02711730
Iteration 31, loss = 0.02618338
Iteration 32, loss = 0.02566064
Iteration 33, loss = 0.02507223
Iteration 34, loss = 0.02468614
Iteration 35, loss = 0.02421799
Iteration 36, loss = 0.02398771
Iteration 37, loss = 0.02366667
Iteration 38, loss = 0.02328825
Iteration 39, loss = 0.02299975
Iteration 40, loss = 0.02277708
Iteration 41, loss = 0.02244795
Iteration 42, loss = 0.02247121
Iteration 43, loss = 0.02210475
Iteration 44, loss = 0.02162561
Iteration 45, loss = 0.02188575
Iteration 46, loss = 0.02139210
Iteration 47, loss = 0.02121692
Iteration 48, loss = 0.02100374
Iteration 49, loss = 0.02197787
Iteration 50, loss = 0.02217870
Iteration 51, loss = 0.02175623
Iteration 52, loss = 0.02135694
Iteration 53, loss = 0.02097717
Iteration 54, loss = 0.02082427
Iteration 55, loss = 0.02051139
Iteration 56, loss = 0.02047301
Iteration 57, loss = 0.02069333
Iteration 58, loss = 0.02039879
Iteration 59, loss = 0.02003864
Iteration 60, loss = 0.02018080
Iteration 61, loss = 0.02006510
Iteration 62, loss = 0.01991802
Iteration 63, loss = 0.01969558
Iteration 64, loss = 0.01966048
Iteration 65, loss = 0.01960192
Iteration 66, loss = 0.01959612
Iteration 67, loss = 0.01939732
Iteration 68, loss = 0.01942438
Iteration 69, loss = 0.01919497
Iteration 70, loss = 0.01919510
Iteration 71, loss = 0.01942218
Iteration 72, loss = 0.01918860
Iteration 73, loss = 0.01939318
Iteration 74, loss = 0.01910941
Iteration 75, loss = 0.01902291
Iteration 76, loss = 0.01878700
Iteration 77, loss = 0.01885216
Iteration 78, loss = 0.01869175
Iteration 79, loss = 0.01875632
Iteration 80, loss = 0.01848529
Iteration 81, loss = 0.01879239
Iteration 82, loss = 0.01858283
Iteration 83, loss = 0.01856586
Iteration 84, loss = 0.01857143
Iteration 85, loss = 0.01849927
Iteration 86, loss = 0.01853622
Iteration 87, loss = 0.01874493
Iteration 88, loss = 0.01840132
Iteration 89, loss = 0.01852523
Iteration 90, loss = 0.01832281
Iteration 91, loss = 0.01833920
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71594381
Iteration 2, loss = 0.65804528
Iteration 3, loss = 0.56349327
Iteration 4, loss = 0.44965125
Iteration 5, loss = 0.34649277
Iteration 6, loss = 0.26416693
Iteration 7, loss = 0.20268439
Iteration 8, loss = 0.15835870
Iteration 9, loss = 0.12686685
Iteration 10, loss = 0.10462272
Iteration 11, loss = 0.08809390
Iteration 12, loss = 0.07605914
Iteration 13, loss = 0.06695679
Iteration 14, loss = 0.05949104
Iteration 15, loss = 0.05381289
Iteration 16, loss = 0.04914583
Iteration 17, loss = 0.04568408
Iteration 18, loss = 0.04257122
Iteration 19, loss = 0.03999705
Iteration 20, loss = 0.03798586
Iteration 21, loss = 0.03597765
Iteration 22, loss = 0.03467556
Iteration 23, loss = 0.03304063
Iteration 24, loss = 0.03200452
Iteration 25, loss = 0.03101697
Iteration 26, loss = 0.03018446
Iteration 27, loss = 0.02950688
Iteration 28, loss = 0.02844427
Iteration 29, loss = 0.02799129
Iteration 30, loss = 0.02731031
Iteration 31, loss = 0.02713165
Iteration 32, loss = 0.02632970
Iteration 33, loss = 0.02601127
Iteration 34, loss = 0.02572395
Iteration 35, loss = 0.02517198
Iteration 36, loss = 0.02501235
Iteration 37, loss = 0.02488824
Iteration 38, loss = 0.02444803
Iteration 39, loss = 0.02417403
Iteration 40, loss = 0.02404112
Iteration 41, loss = 0.02404153
Iteration 42, loss = 0.02354013
Iteration 43, loss = 0.02347111
Iteration 44, loss = 0.02339445
Iteration 45, loss = 0.02338666
Iteration 46, loss = 0.02319626
Iteration 47, loss = 0.02301161
Iteration 48, loss = 0.02303631
Iteration 49, loss = 0.02266356
Iteration 50, loss = 0.02281470
Iteration 51, loss = 0.02255959
Iteration 52, loss = 0.02246823
Iteration 53, loss = 0.02222821
Iteration 54, loss = 0.02236010
Iteration 55, loss = 0.02216865
Iteration 56, loss = 0.02218824
Iteration 57, loss = 0.02203059
Iteration 58, loss = 0.02235357
Iteration 59, loss = 0.02202976
Iteration 60, loss = 0.02207741
Iteration 61, loss = 0.02171542
Iteration 62, loss = 0.02170270
Iteration 63, loss = 0.02163420
Iteration 64, loss = 0.02191197
Iteration 65, loss = 0.02175603
Iteration 66, loss = 0.02161897
Iteration 67, loss = 0.02161442
Iteration 68, loss = 0.02151186
Iteration 69, loss = 0.02128626
Iteration 70, loss = 0.02158561
Iteration 71, loss = 0.02158335
Iteration 72, loss = 0.02144003
Iteration 73, loss = 0.02133628
Iteration 74, loss = 0.02135232
Iteration 75, loss = 0.02132815
Iteration 76, loss = 0.02135489
Iteration 77, loss = 0.02117281
Iteration 78, loss = 0.02101100
Iteration 79, loss = 0.02116508
Iteration 80, loss = 0.02124700
Iteration 81, loss = 0.02146478
Iteration 82, loss = 0.02134902
Iteration 83, loss = 0.02119813
Iteration 84, loss = 0.02108538
Iteration 85, loss = 0.02114303
Iteration 86, loss = 0.02102935
Iteration 87, loss = 0.02097113
Iteration 88, loss = 0.02080812
Iteration 89, loss = 0.02086540
Iteration 90, loss = 0.02096851
Iteration 91, loss = 0.02108952
Iteration 92, loss = 0.02081228
Iteration 93, loss = 0.02104623
Iteration 94, loss = 0.02109574
Iteration 95, loss = 0.02086443
Iteration 96, loss = 0.02059921
Iteration 97, loss = 0.02087870
Iteration 98, loss = 0.02092223
Iteration 99, loss = 0.02084849
Iteration 100, loss = 0.02074236
Iteration 101, loss = 0.02074968
Iteration 102, loss = 0.02091411
Iteration 103, loss = 0.02097885
Iteration 104, loss = 0.02073182
Iteration 105, loss = 0.02076859
Iteration 106, loss = 0.02063423
Iteration 107, loss = 0.02063300
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67705519
Iteration 2, loss = 0.60587953
Iteration 3, loss = 0.49703542
Iteration 4, loss = 0.38242763
Iteration 5, loss = 0.28566823
Iteration 6, loss = 0.21278244
Iteration 7, loss = 0.16190506
Iteration 8, loss = 0.12720691
Iteration 9, loss = 0.10306611
Iteration 10, loss = 0.08632105
Iteration 11, loss = 0.07407780
Iteration 12, loss = 0.06498270
Iteration 13, loss = 0.05806905
Iteration 14, loss = 0.05265726
Iteration 15, loss = 0.04864060
Iteration 16, loss = 0.04523859
Iteration 17, loss = 0.04234902
Iteration 18, loss = 0.04009698
Iteration 19, loss = 0.03815043
Iteration 20, loss = 0.03626105
Iteration 21, loss = 0.03519958
Iteration 22, loss = 0.03409293
Iteration 23, loss = 0.03280665
Iteration 24, loss = 0.03207648
Iteration 25, loss = 0.03117390
Iteration 26, loss = 0.03056442
Iteration 27, loss = 0.02982668
Iteration 28, loss = 0.02952043
Iteration 29, loss = 0.02908141
Iteration 30, loss = 0.02840729
Iteration 31, loss = 0.02777614
Iteration 32, loss = 0.02772023
Iteration 33, loss = 0.02731741
Iteration 34, loss = 0.02750972
Iteration 35, loss = 0.02673950
Iteration 36, loss = 0.02643744
Iteration 37, loss = 0.02662801
Iteration 38, loss = 0.02622322
Iteration 39, loss = 0.02591269
Iteration 40, loss = 0.02586809
Iteration 41, loss = 0.02560205
Iteration 42, loss = 0.02556235
Iteration 43, loss = 0.02517265
Iteration 44, loss = 0.02528793
Iteration 45, loss = 0.02521561
Iteration 46, loss = 0.02494816
Iteration 47, loss = 0.02475307
Iteration 48, loss = 0.02492749
Iteration 49, loss = 0.02477339
Iteration 50, loss = 0.02458798
Iteration 51, loss = 0.02473257
Iteration 52, loss = 0.02444347
Iteration 53, loss = 0.02462507
Iteration 54, loss = 0.02467792
Iteration 55, loss = 0.02440626
Iteration 56, loss = 0.02409007
Iteration 57, loss = 0.02434950
Iteration 58, loss = 0.02402099
Iteration 59, loss = 0.02395781
Iteration 60, loss = 0.02395378
Iteration 61, loss = 0.02416257
Iteration 62, loss = 0.02400457
Iteration 63, loss = 0.02384794
Iteration 64, loss = 0.02399090
Iteration 65, loss = 0.02372655
Iteration 66, loss = 0.02393145
Iteration 67, loss = 0.02372594
Iteration 68, loss = 0.02365084
Iteration 69, loss = 0.02359031
Iteration 70, loss = 0.02372282
Iteration 71, loss = 0.02363361
Iteration 72, loss = 0.02384148
Iteration 73, loss = 0.02364335
Iteration 74, loss = 0.02344628
Iteration 75, loss = 0.02362067
Iteration 76, loss = 0.02365410
Iteration 77, loss = 0.02347632
Iteration 78, loss = 0.02328989
Iteration 79, loss = 0.02342824
Iteration 80, loss = 0.02351654
Iteration 81, loss = 0.02361221
Iteration 82, loss = 0.02362104
Iteration 83, loss = 0.02318055
Iteration 84, loss = 0.02339435
Iteration 85, loss = 0.02378057
Iteration 86, loss = 0.02297571
Iteration 87, loss = 0.02334269
Iteration 88, loss = 0.02364284
Iteration 89, loss = 0.02331598
Iteration 90, loss = 0.02314288
Iteration 91, loss = 0.02297257
Iteration 92, loss = 0.02358924
Iteration 93, loss = 0.02326220
Iteration 94, loss = 0.02342044
Iteration 95, loss = 0.02305848
Iteration 96, loss = 0.02343083
Iteration 97, loss = 0.02306967
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69933403
Iteration 2, loss = 0.68116558
Iteration 3, loss = 0.66648200
Iteration 4, loss = 0.65210503
Iteration 5, loss = 0.63721083
Iteration 6, loss = 0.62126433
Iteration 7, loss = 0.60433360
Iteration 8, loss = 0.58585357
Iteration 9, loss = 0.56607678
Iteration 10, loss = 0.54451829
Iteration 11, loss = 0.52144239
Iteration 12, loss = 0.49752345
Iteration 13, loss = 0.47242326
Iteration 14, loss = 0.44699573
Iteration 15, loss = 0.42152057
Iteration 16, loss = 0.39634904
Iteration 17, loss = 0.37187372
Iteration 18, loss = 0.34833978
Iteration 19, loss = 0.32583278
Iteration 20, loss = 0.30467748
Iteration 21, loss = 0.28455850
Iteration 22, loss = 0.26576248
Iteration 23, loss = 0.24813789
Iteration 24, loss = 0.23183894
Iteration 25, loss = 0.21656203
Iteration 26, loss = 0.20249264
Iteration 27, loss = 0.18951584
Iteration 28, loss = 0.17738463
Iteration 29, loss = 0.16630982
Iteration 30, loss = 0.15604147
Iteration 31, loss = 0.14655369
Iteration 32, loss = 0.13788512
Iteration 33, loss = 0.12981969
Iteration 34, loss = 0.12246667
Iteration 35, loss = 0.11562857
Iteration 36, loss = 0.10936621
Iteration 37, loss = 0.10357476
Iteration 38, loss = 0.09820993
Iteration 39, loss = 0.09324849
Iteration 40, loss = 0.08865764
Iteration 41, loss = 0.08444146
Iteration 42, loss = 0.08051283
Iteration 43, loss = 0.07689791
Iteration 44, loss = 0.07350615
Iteration 45, loss = 0.07039344
Iteration 46, loss = 0.06739068
Iteration 47, loss = 0.06467947
Iteration 48, loss = 0.06212580
Iteration 49, loss = 0.05974734
Iteration 50, loss = 0.05750175
Iteration 51, loss = 0.05538787
Iteration 52, loss = 0.05341893
Iteration 53, loss = 0.05160369
Iteration 54, loss = 0.04982782
Iteration 55, loss = 0.04816219
Iteration 56, loss = 0.04662085
Iteration 57, loss = 0.04516279
Iteration 58, loss = 0.04377235
Iteration 59, loss = 0.04248781
Iteration 60, loss = 0.04122430
Iteration 61, loss = 0.04007096
Iteration 62, loss = 0.03895719
Iteration 63, loss = 0.03788126
Iteration 64, loss = 0.03686233
Iteration 65, loss = 0.03593339
Iteration 66, loss = 0.03495252
Iteration 67, loss = 0.03411209
Iteration 68, loss = 0.03329121
Iteration 69, loss = 0.03250866
Iteration 70, loss = 0.03177871
Iteration 71, loss = 0.03101124
Iteration 72, loss = 0.03033254
Iteration 73, loss = 0.02966864
Iteration 74, loss = 0.02903020
Iteration 75, loss = 0.02842144
Iteration 76, loss = 0.02781958
Iteration 77, loss = 0.02725741
Iteration 78, loss = 0.02671340
Iteration 79, loss = 0.02618256
Iteration 80, loss = 0.02570419
Iteration 81, loss = 0.02519380
Iteration 82, loss = 0.02476173
Iteration 83, loss = 0.02428494
Iteration 84, loss = 0.02387440
Iteration 85, loss = 0.02344100
Iteration 86, loss = 0.02301259
Iteration 87, loss = 0.02263150
Iteration 88, loss = 0.02230853
Iteration 89, loss = 0.02190932
Iteration 90, loss = 0.02153565
Iteration 91, loss = 0.02124294
Iteration 92, loss = 0.02096218
Iteration 93, loss = 0.02060796
Iteration 94, loss = 0.02030041
Iteration 95, loss = 0.02000618
Iteration 96, loss = 0.01970241
Iteration 97, loss = 0.01945907
Iteration 98, loss = 0.01918108
Iteration 99, loss = 0.01887993
Iteration 100, loss = 0.01866172
Iteration 101, loss = 0.01840652
Iteration 102, loss = 0.01815763
Iteration 103, loss = 0.01794332
Iteration 104, loss = 0.01768157
Iteration 105, loss = 0.01750579
Iteration 106, loss = 0.01727737
Iteration 107, loss = 0.01708028
Iteration 108, loss = 0.01689139
Iteration 109, loss = 0.01673464
Iteration 110, loss = 0.01653268
Iteration 111, loss = 0.01638366
Iteration 112, loss = 0.01618173
Iteration 113, loss = 0.01599631
Iteration 114, loss = 0.01585476
Iteration 115, loss = 0.01569347
Iteration 116, loss = 0.01555378
Iteration 117, loss = 0.01534640
Iteration 118, loss = 0.01517287
Iteration 119, loss = 0.01507584
Iteration 120, loss = 0.01501479
Iteration 121, loss = 0.01479856
Iteration 122, loss = 0.01467060
Iteration 123, loss = 0.01450077
Iteration 124, loss = 0.01434720
Iteration 125, loss = 0.01424720
Iteration 126, loss = 0.01414296
Iteration 127, loss = 0.01400591
Iteration 128, loss = 0.01391366
Iteration 129, loss = 0.01380537
Iteration 130, loss = 0.01367459
Iteration 131, loss = 0.01356830
Iteration 132, loss = 0.01346347
Iteration 133, loss = 0.01335419
Iteration 134, loss = 0.01326080
Iteration 135, loss = 0.01317722
Iteration 136, loss = 0.01306852
Iteration 137, loss = 0.01301607
Iteration 138, loss = 0.01288936
Iteration 139, loss = 0.01279573
Iteration 140, loss = 0.01271052
Iteration 141, loss = 0.01260563
Iteration 142, loss = 0.01252397
Iteration 143, loss = 0.01244797
Iteration 144, loss = 0.01234695
Iteration 145, loss = 0.01228007
Iteration 146, loss = 0.01219960
Iteration 147, loss = 0.01211563
Iteration 148, loss = 0.01209022
Iteration 149, loss = 0.01201250
Iteration 150, loss = 0.01191525
Iteration 151, loss = 0.01187546
Iteration 152, loss = 0.01181132
Iteration 153, loss = 0.01176977
Iteration 154, loss = 0.01169479
Iteration 155, loss = 0.01162550
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67506637
Iteration 2, loss = 0.65376593
Iteration 3, loss = 0.63624359
Iteration 4, loss = 0.61925897
Iteration 5, loss = 0.60190202
Iteration 6, loss = 0.58401798
Iteration 7, loss = 0.56521909
Iteration 8, loss = 0.54548916
Iteration 9, loss = 0.52470174
Iteration 10, loss = 0.50315424
Iteration 11, loss = 0.48108613
Iteration 12, loss = 0.45858454
Iteration 13, loss = 0.43563247
Iteration 14, loss = 0.41271625
Iteration 15, loss = 0.39020654
Iteration 16, loss = 0.36795758
Iteration 17, loss = 0.34635885
Iteration 18, loss = 0.32537309
Iteration 19, loss = 0.30543256
Iteration 20, loss = 0.28614285
Iteration 21, loss = 0.26812248
Iteration 22, loss = 0.25096684
Iteration 23, loss = 0.23483054
Iteration 24, loss = 0.21969127
Iteration 25, loss = 0.20555643
Iteration 26, loss = 0.19235098
Iteration 27, loss = 0.18004755
Iteration 28, loss = 0.16864848
Iteration 29, loss = 0.15814450
Iteration 30, loss = 0.14827498
Iteration 31, loss = 0.13916842
Iteration 32, loss = 0.13084393
Iteration 33, loss = 0.12314439
Iteration 34, loss = 0.11596536
Iteration 35, loss = 0.10938728
Iteration 36, loss = 0.10322506
Iteration 37, loss = 0.09763853
Iteration 38, loss = 0.09240203
Iteration 39, loss = 0.08759633
Iteration 40, loss = 0.08311483
Iteration 41, loss = 0.07898614
Iteration 42, loss = 0.07513272
Iteration 43, loss = 0.07157251
Iteration 44, loss = 0.06825809
Iteration 45, loss = 0.06513487
Iteration 46, loss = 0.06225406
Iteration 47, loss = 0.05957146
Iteration 48, loss = 0.05701726
Iteration 49, loss = 0.05466243
Iteration 50, loss = 0.05242939
Iteration 51, loss = 0.05036875
Iteration 52, loss = 0.04844029
Iteration 53, loss = 0.04658994
Iteration 54, loss = 0.04489324
Iteration 55, loss = 0.04326911
Iteration 56, loss = 0.04175307
Iteration 57, loss = 0.04032849
Iteration 58, loss = 0.03898759
Iteration 59, loss = 0.03768730
Iteration 60, loss = 0.03646880
Iteration 61, loss = 0.03533043
Iteration 62, loss = 0.03422484
Iteration 63, loss = 0.03320005
Iteration 64, loss = 0.03220716
Iteration 65, loss = 0.03127429
Iteration 66, loss = 0.03038624
Iteration 67, loss = 0.02953857
Iteration 68, loss = 0.02873247
Iteration 69, loss = 0.02794829
Iteration 70, loss = 0.02722620
Iteration 71, loss = 0.02650779
Iteration 72, loss = 0.02581964
Iteration 73, loss = 0.02518328
Iteration 74, loss = 0.02456292
Iteration 75, loss = 0.02397043
Iteration 76, loss = 0.02340765
Iteration 77, loss = 0.02283999
Iteration 78, loss = 0.02239771
Iteration 79, loss = 0.02185561
Iteration 80, loss = 0.02144909
Iteration 81, loss = 0.02095046
Iteration 82, loss = 0.02051240
Iteration 83, loss = 0.02009569
Iteration 84, loss = 0.01966646
Iteration 85, loss = 0.01927534
Iteration 86, loss = 0.01889218
Iteration 87, loss = 0.01853379
Iteration 88, loss = 0.01816532
Iteration 89, loss = 0.01784274
Iteration 90, loss = 0.01752234
Iteration 91, loss = 0.01717725
Iteration 92, loss = 0.01690613
Iteration 93, loss = 0.01659701
Iteration 94, loss = 0.01630743
Iteration 95, loss = 0.01601089
Iteration 96, loss = 0.01574756
Iteration 97, loss = 0.01550206
Iteration 98, loss = 0.01525076
Iteration 99, loss = 0.01500072
Iteration 100, loss = 0.01477573
Iteration 101, loss = 0.01454848
Iteration 102, loss = 0.01432999
Iteration 103, loss = 0.01412226
Iteration 104, loss = 0.01391028
Iteration 105, loss = 0.01370635
Iteration 106, loss = 0.01351146
Iteration 107, loss = 0.01332871
Iteration 108, loss = 0.01314217
Iteration 109, loss = 0.01296228
Iteration 110, loss = 0.01280796
Iteration 111, loss = 0.01262224
Iteration 112, loss = 0.01246335
Iteration 113, loss = 0.01230914
Iteration 114, loss = 0.01215894
Iteration 115, loss = 0.01201258
Iteration 116, loss = 0.01186833
Iteration 117, loss = 0.01173021
Iteration 118, loss = 0.01160317
Iteration 119, loss = 0.01146825
Iteration 120, loss = 0.01132620
Iteration 121, loss = 0.01119967
Iteration 122, loss = 0.01108064
Iteration 123, loss = 0.01096030
Iteration 124, loss = 0.01084084
Iteration 125, loss = 0.01071636
Iteration 126, loss = 0.01059663
Iteration 127, loss = 0.01048427
Iteration 128, loss = 0.01037562
Iteration 129, loss = 0.01026969
Iteration 130, loss = 0.01016729
Iteration 131, loss = 0.01008026
Iteration 132, loss = 0.00996592
Iteration 133, loss = 0.00987325
Iteration 134, loss = 0.00978150
Iteration 135, loss = 0.00968984
Iteration 136, loss = 0.00961303
Iteration 137, loss = 0.00952627
Iteration 138, loss = 0.00943154
Iteration 139, loss = 0.00935760
Iteration 140, loss = 0.00926632
Iteration 141, loss = 0.00920393
Iteration 142, loss = 0.00910936
Iteration 143, loss = 0.00903570
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68888788
Iteration 2, loss = 0.61219208
Iteration 3, loss = 0.50051962
Iteration 4, loss = 0.38397229
Iteration 5, loss = 0.28504279
Iteration 6, loss = 0.21108450
Iteration 7, loss = 0.15945325
Iteration 8, loss = 0.12446993
Iteration 9, loss = 0.10051911
Iteration 10, loss = 0.08318730
Iteration 11, loss = 0.07097251
Iteration 12, loss = 0.06211076
Iteration 13, loss = 0.05491678
Iteration 14, loss = 0.04963665
Iteration 15, loss = 0.04544635
Iteration 16, loss = 0.04196646
Iteration 17, loss = 0.03861814
Iteration 18, loss = 0.03657868
Iteration 19, loss = 0.03455219
Iteration 20, loss = 0.03277631
Iteration 21, loss = 0.03130533
Iteration 22, loss = 0.03024228
Iteration 23, loss = 0.02927440
Iteration 24, loss = 0.02828524
Iteration 25, loss = 0.02710544
Iteration 26, loss = 0.02659754
Iteration 27, loss = 0.02625899
Iteration 28, loss = 0.02568632
Iteration 29, loss = 0.02489268
Iteration 30, loss = 0.02462896
Iteration 31, loss = 0.02401915
Iteration 32, loss = 0.02378176
Iteration 33, loss = 0.02339312
Iteration 34, loss = 0.02313048
Iteration 35, loss = 0.02290013
Iteration 36, loss = 0.02247033
Iteration 37, loss = 0.02231638
Iteration 38, loss = 0.02214708
Iteration 39, loss = 0.02192169
Iteration 40, loss = 0.02189571
Iteration 41, loss = 0.02158034
Iteration 42, loss = 0.02140255
Iteration 43, loss = 0.02143642
Iteration 44, loss = 0.02128377
Iteration 45, loss = 0.02078940
Iteration 46, loss = 0.02099503
Iteration 47, loss = 0.02073048
Iteration 48, loss = 0.02058447
Iteration 49, loss = 0.02043833
Iteration 50, loss = 0.02063359
Iteration 51, loss = 0.02039661
Iteration 52, loss = 0.02040101
Iteration 53, loss = 0.02044869
Iteration 54, loss = 0.02021942
Iteration 55, loss = 0.02001002
Iteration 56, loss = 0.01992074
Iteration 57, loss = 0.01990603
Iteration 58, loss = 0.01999661
Iteration 59, loss = 0.01998570
Iteration 60, loss = 0.02006992
Iteration 61, loss = 0.01986952
Iteration 62, loss = 0.02002861
Iteration 63, loss = 0.02008381
Iteration 64, loss = 0.01963169
Iteration 65, loss = 0.01954739
Iteration 66, loss = 0.01980449
Iteration 67, loss = 0.01922558
Iteration 68, loss = 0.01939891
Iteration 69, loss = 0.01937899
Iteration 70, loss = 0.01941422
Iteration 71, loss = 0.01943909
Iteration 72, loss = 0.01922548
Iteration 73, loss = 0.01945436
Iteration 74, loss = 0.01950688
Iteration 75, loss = 0.01906452
Iteration 76, loss = 0.01930595
Iteration 77, loss = 0.01907888
Iteration 78, loss = 0.01929962
Iteration 79, loss = 0.01920158
Iteration 80, loss = 0.01915511
Iteration 81, loss = 0.01899902
Iteration 82, loss = 0.01924448
Iteration 83, loss = 0.01913502
Iteration 84, loss = 0.01919560
Iteration 85, loss = 0.01888736
Iteration 86, loss = 0.01901702
Iteration 87, loss = 0.01904291
Iteration 88, loss = 0.01895010
Iteration 89, loss = 0.01876295
Iteration 90, loss = 0.01877200
Iteration 91, loss = 0.01902317
Iteration 92, loss = 0.01912317
Iteration 93, loss = 0.01927511
Iteration 94, loss = 0.01879948
Iteration 95, loss = 0.01879425
Iteration 96, loss = 0.01897600
Iteration 97, loss = 0.01873305
Iteration 98, loss = 0.01884387
Iteration 99, loss = 0.01875747
Iteration 100, loss = 0.01877135
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71348608
Iteration 2, loss = 0.66550390
Iteration 3, loss = 0.59893331
Iteration 4, loss = 0.51775296
Iteration 5, loss = 0.43163450
Iteration 6, loss = 0.35146740
Iteration 7, loss = 0.28308222
Iteration 8, loss = 0.22790404
Iteration 9, loss = 0.18487539
Iteration 10, loss = 0.15210272
Iteration 11, loss = 0.12726693
Iteration 12, loss = 0.10797288
Iteration 13, loss = 0.09316987
Iteration 14, loss = 0.08157041
Iteration 15, loss = 0.07249905
Iteration 16, loss = 0.06509154
Iteration 17, loss = 0.05891411
Iteration 18, loss = 0.05408649
Iteration 19, loss = 0.04991941
Iteration 20, loss = 0.04655976
Iteration 21, loss = 0.04357695
Iteration 22, loss = 0.04107733
Iteration 23, loss = 0.03892057
Iteration 24, loss = 0.03686132
Iteration 25, loss = 0.03529545
Iteration 26, loss = 0.03387906
Iteration 27, loss = 0.03260183
Iteration 28, loss = 0.03148483
Iteration 29, loss = 0.03057638
Iteration 30, loss = 0.02962085
Iteration 31, loss = 0.02864910
Iteration 32, loss = 0.02809564
Iteration 33, loss = 0.02742096
Iteration 34, loss = 0.02661925
Iteration 35, loss = 0.02623812
Iteration 36, loss = 0.02563360
Iteration 37, loss = 0.02551492
Iteration 38, loss = 0.02491925
Iteration 39, loss = 0.02451747
Iteration 40, loss = 0.02432763
Iteration 41, loss = 0.02386788
Iteration 42, loss = 0.02379777
Iteration 43, loss = 0.02328061
Iteration 44, loss = 0.02302658
Iteration 45, loss = 0.02270634
Iteration 46, loss = 0.02252536
Iteration 47, loss = 0.02221448
Iteration 48, loss = 0.02196256
Iteration 49, loss = 0.02176122
Iteration 50, loss = 0.02171055
Iteration 51, loss = 0.02167848
Iteration 52, loss = 0.02130722
Iteration 53, loss = 0.02129050
Iteration 54, loss = 0.02132926
Iteration 55, loss = 0.02129503
Iteration 56, loss = 0.02107046
Iteration 57, loss = 0.02076405
Iteration 58, loss = 0.02080047
Iteration 59, loss = 0.02063426
Iteration 60, loss = 0.02052346
Iteration 61, loss = 0.02025063
Iteration 62, loss = 0.02031955
Iteration 63, loss = 0.02005488
Iteration 64, loss = 0.02000798
Iteration 65, loss = 0.01986017
Iteration 66, loss = 0.01992652
Iteration 67, loss = 0.02017265
Iteration 68, loss = 0.01993872
Iteration 69, loss = 0.01994422
Iteration 70, loss = 0.01970265
Iteration 71, loss = 0.01974375
Iteration 72, loss = 0.01952812
Iteration 73, loss = 0.01965936
Iteration 74, loss = 0.01946374
Iteration 75, loss = 0.01943589
Iteration 76, loss = 0.01930871
Iteration 77, loss = 0.01937466
Iteration 78, loss = 0.01938250
Iteration 79, loss = 0.01913267
Iteration 80, loss = 0.01923154
Iteration 81, loss = 0.01913509
Iteration 82, loss = 0.01905925
Iteration 83, loss = 0.01897429
Iteration 84, loss = 0.01916293
Iteration 85, loss = 0.01934359
Iteration 86, loss = 0.01927048
Iteration 87, loss = 0.01877449
Iteration 88, loss = 0.01892660
Iteration 89, loss = 0.01882800
Iteration 90, loss = 0.01869285
Iteration 91, loss = 0.01886708
Iteration 92, loss = 0.01891305
Iteration 93, loss = 0.01885696
Iteration 94, loss = 0.01874612
Iteration 95, loss = 0.01877422
Iteration 96, loss = 0.01874071
Iteration 97, loss = 0.01885582
Iteration 98, loss = 0.01883292
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67514884
Iteration 2, loss = 0.61066208
Iteration 3, loss = 0.50889231
Iteration 4, loss = 0.39559652
Iteration 5, loss = 0.29773492
Iteration 6, loss = 0.22289069
Iteration 7, loss = 0.16964664
Iteration 8, loss = 0.13272873
Iteration 9, loss = 0.10708738
Iteration 10, loss = 0.08866671
Iteration 11, loss = 0.07542199
Iteration 12, loss = 0.06525856
Iteration 13, loss = 0.05772351
Iteration 14, loss = 0.05176398
Iteration 15, loss = 0.04700763
Iteration 16, loss = 0.04321724
Iteration 17, loss = 0.03998397
Iteration 18, loss = 0.03747302
Iteration 19, loss = 0.03506598
Iteration 20, loss = 0.03341385
Iteration 21, loss = 0.03177265
Iteration 22, loss = 0.03036174
Iteration 23, loss = 0.02927755
Iteration 24, loss = 0.02831715
Iteration 25, loss = 0.02730974
Iteration 26, loss = 0.02645042
Iteration 27, loss = 0.02584617
Iteration 28, loss = 0.02501769
Iteration 29, loss = 0.02461752
Iteration 30, loss = 0.02415832
Iteration 31, loss = 0.02388111
Iteration 32, loss = 0.02319516
Iteration 33, loss = 0.02279743
Iteration 34, loss = 0.02238785
Iteration 35, loss = 0.02217659
Iteration 36, loss = 0.02214933
Iteration 37, loss = 0.02159232
Iteration 38, loss = 0.02156029
Iteration 39, loss = 0.02135312
Iteration 40, loss = 0.02101761
Iteration 41, loss = 0.02097495
Iteration 42, loss = 0.02079367
Iteration 43, loss = 0.02039027
Iteration 44, loss = 0.02052185
Iteration 45, loss = 0.02029657
Iteration 46, loss = 0.02020833
Iteration 47, loss = 0.01995168
Iteration 48, loss = 0.01979470
Iteration 49, loss = 0.01991473
Iteration 50, loss = 0.02001501
Iteration 51, loss = 0.01963245
Iteration 52, loss = 0.01948105
Iteration 53, loss = 0.01947464
Iteration 54, loss = 0.01941957
Iteration 55, loss = 0.01937155
Iteration 56, loss = 0.01924978
Iteration 57, loss = 0.01923508
Iteration 58, loss = 0.01883412
Iteration 59, loss = 0.01917312
Iteration 60, loss = 0.01893647
Iteration 61, loss = 0.01870291
Iteration 62, loss = 0.01861992
Iteration 63, loss = 0.01861358
Iteration 64, loss = 0.01878564
Iteration 65, loss = 0.01870172
Iteration 66, loss = 0.01901792
Iteration 67, loss = 0.01852080
Iteration 68, loss = 0.01868321
Iteration 69, loss = 0.01895866
Iteration 70, loss = 0.01869482
Iteration 71, loss = 0.01860084
Iteration 72, loss = 0.01839588
Iteration 73, loss = 0.01842415
Iteration 74, loss = 0.01838935
Iteration 75, loss = 0.01849112
Iteration 76, loss = 0.01822906
Iteration 77, loss = 0.01844620
Iteration 78, loss = 0.01837680
Iteration 79, loss = 0.01822527
Iteration 80, loss = 0.01828605
Iteration 81, loss = 0.01813088
Iteration 82, loss = 0.01814959
Iteration 83, loss = 0.01807122
Iteration 84, loss = 0.01791607
Iteration 85, loss = 0.01822625
Iteration 86, loss = 0.01850824
Iteration 87, loss = 0.01810280
Iteration 88, loss = 0.01800068
Iteration 89, loss = 0.01789817
Iteration 90, loss = 0.01807637
Iteration 91, loss = 0.01805745
Iteration 92, loss = 0.01785601
Iteration 93, loss = 0.01821150
Iteration 94, loss = 0.01856375
Iteration 95, loss = 0.01813154
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67814280
Iteration 2, loss = 0.65808358
Iteration 3, loss = 0.63825025
Iteration 4, loss = 0.61502622
Iteration 5, loss = 0.58735503
Iteration 6, loss = 0.55554020
Iteration 7, loss = 0.52039753
Iteration 8, loss = 0.48349954
Iteration 9, loss = 0.44601485
Iteration 10, loss = 0.40967764
Iteration 11, loss = 0.37438635
Iteration 12, loss = 0.34092267
Iteration 13, loss = 0.30965572
Iteration 14, loss = 0.28086002
Iteration 15, loss = 0.25414252
Iteration 16, loss = 0.23027317
Iteration 17, loss = 0.20840429
Iteration 18, loss = 0.18899911
Iteration 19, loss = 0.17152752
Iteration 20, loss = 0.15602760
Iteration 21, loss = 0.14226621
Iteration 22, loss = 0.13000524
Iteration 23, loss = 0.11922692
Iteration 24, loss = 0.10960711
Iteration 25, loss = 0.10120536
Iteration 26, loss = 0.09360129
Iteration 27, loss = 0.08685460
Iteration 28, loss = 0.08081846
Iteration 29, loss = 0.07543862
Iteration 30, loss = 0.07067423
Iteration 31, loss = 0.06629863
Iteration 32, loss = 0.06239886
Iteration 33, loss = 0.05889199
Iteration 34, loss = 0.05565328
Iteration 35, loss = 0.05280336
Iteration 36, loss = 0.05012928
Iteration 37, loss = 0.04767460
Iteration 38, loss = 0.04547698
Iteration 39, loss = 0.04342000
Iteration 40, loss = 0.04155392
Iteration 41, loss = 0.03983884
Iteration 42, loss = 0.03826964
Iteration 43, loss = 0.03677632
Iteration 44, loss = 0.03539934
Iteration 45, loss = 0.03417205
Iteration 46, loss = 0.03304361
Iteration 47, loss = 0.03187685
Iteration 48, loss = 0.03089618
Iteration 49, loss = 0.02993145
Iteration 50, loss = 0.02898657
Iteration 51, loss = 0.02818415
Iteration 52, loss = 0.02735521
Iteration 53, loss = 0.02667578
Iteration 54, loss = 0.02593864
Iteration 55, loss = 0.02527576
Iteration 56, loss = 0.02468214
Iteration 57, loss = 0.02403423
Iteration 58, loss = 0.02355685
Iteration 59, loss = 0.02298141
Iteration 60, loss = 0.02252371
Iteration 61, loss = 0.02201408
Iteration 62, loss = 0.02153306
Iteration 63, loss = 0.02116447
Iteration 64, loss = 0.02073837
Iteration 65, loss = 0.02034379
Iteration 66, loss = 0.02000387
Iteration 67, loss = 0.01963260
Iteration 68, loss = 0.01929359
Iteration 69, loss = 0.01896468
Iteration 70, loss = 0.01866491
Iteration 71, loss = 0.01838466
Iteration 72, loss = 0.01808818
Iteration 73, loss = 0.01790998
Iteration 74, loss = 0.01757156
Iteration 75, loss = 0.01734455
Iteration 76, loss = 0.01710062
Iteration 77, loss = 0.01687059
Iteration 78, loss = 0.01671415
Iteration 79, loss = 0.01648656
Iteration 80, loss = 0.01627636
Iteration 81, loss = 0.01607583
Iteration 82, loss = 0.01588345
Iteration 83, loss = 0.01574240
Iteration 84, loss = 0.01555030
Iteration 85, loss = 0.01540725
Iteration 86, loss = 0.01519617
Iteration 87, loss = 0.01508581
Iteration 88, loss = 0.01493040
Iteration 89, loss = 0.01481497
Iteration 90, loss = 0.01464336
Iteration 91, loss = 0.01454648
Iteration 92, loss = 0.01445160
Iteration 93, loss = 0.01426093
Iteration 94, loss = 0.01416185
Iteration 95, loss = 0.01405064
Iteration 96, loss = 0.01390190
Iteration 97, loss = 0.01378589
Iteration 98, loss = 0.01370952
Iteration 99, loss = 0.01356442
Iteration 100, loss = 0.01353717
Iteration 101, loss = 0.01338104
Iteration 102, loss = 0.01329853
Iteration 103, loss = 0.01323567
Iteration 104, loss = 0.01314068
Iteration 105, loss = 0.01309377
Iteration 106, loss = 0.01297107
Iteration 107, loss = 0.01285589
Iteration 108, loss = 0.01278725
Iteration 109, loss = 0.01272506
Iteration 110, loss = 0.01266190
Iteration 111, loss = 0.01256288
Iteration 112, loss = 0.01251269
Iteration 113, loss = 0.01247076
Iteration 114, loss = 0.01234051
Iteration 115, loss = 0.01232473
Iteration 116, loss = 0.01227167
Iteration 117, loss = 0.01215134
Iteration 118, loss = 0.01217413
Iteration 119, loss = 0.01202882
Iteration 120, loss = 0.01202563
Iteration 121, loss = 0.01194584
Iteration 122, loss = 0.01188907
Iteration 123, loss = 0.01186876
Iteration 124, loss = 0.01176814
Iteration 125, loss = 0.01175068
Iteration 126, loss = 0.01166528
Iteration 127, loss = 0.01165920
Iteration 128, loss = 0.01158505
Iteration 129, loss = 0.01158180
Iteration 130, loss = 0.01151073
Iteration 131, loss = 0.01146513
Iteration 132, loss = 0.01145414
Iteration 133, loss = 0.01138889
Iteration 134, loss = 0.01137907
Iteration 135, loss = 0.01129135
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71956983
Iteration 2, loss = 0.67346008
Iteration 3, loss = 0.61070636
Iteration 4, loss = 0.53720312
Iteration 5, loss = 0.46008816
Iteration 6, loss = 0.38693118
Iteration 7, loss = 0.32152933
Iteration 8, loss = 0.26584541
Iteration 9, loss = 0.21991000
Iteration 10, loss = 0.18301377
Iteration 11, loss = 0.15363776
Iteration 12, loss = 0.13014129
Iteration 13, loss = 0.11177336
Iteration 14, loss = 0.09707737
Iteration 15, loss = 0.08519028
Iteration 16, loss = 0.07561431
Iteration 17, loss = 0.06774813
Iteration 18, loss = 0.06123225
Iteration 19, loss = 0.05585047
Iteration 20, loss = 0.05125194
Iteration 21, loss = 0.04720380
Iteration 22, loss = 0.04386830
Iteration 23, loss = 0.04103486
Iteration 24, loss = 0.03854738
Iteration 25, loss = 0.03620680
Iteration 26, loss = 0.03432515
Iteration 27, loss = 0.03262936
Iteration 28, loss = 0.03113751
Iteration 29, loss = 0.02981665
Iteration 30, loss = 0.02845420
Iteration 31, loss = 0.02754972
Iteration 32, loss = 0.02645123
Iteration 33, loss = 0.02550823
Iteration 34, loss = 0.02468811
Iteration 35, loss = 0.02396048
Iteration 36, loss = 0.02329277
Iteration 37, loss = 0.02270511
Iteration 38, loss = 0.02214218
Iteration 39, loss = 0.02165570
Iteration 40, loss = 0.02138666
Iteration 41, loss = 0.02083411
Iteration 42, loss = 0.02046846
Iteration 43, loss = 0.01987613
Iteration 44, loss = 0.01955005
Iteration 45, loss = 0.01932368
Iteration 46, loss = 0.01903495
Iteration 47, loss = 0.01869413
Iteration 48, loss = 0.01839307
Iteration 49, loss = 0.01826071
Iteration 50, loss = 0.01797120
Iteration 51, loss = 0.01770010
Iteration 52, loss = 0.01747686
Iteration 53, loss = 0.01730784
Iteration 54, loss = 0.01701931
Iteration 55, loss = 0.01696146
Iteration 56, loss = 0.01675379
Iteration 57, loss = 0.01657811
Iteration 58, loss = 0.01644317
Iteration 59, loss = 0.01639440
Iteration 60, loss = 0.01623263
Iteration 61, loss = 0.01622933
Iteration 62, loss = 0.01594037
Iteration 63, loss = 0.01590174
Iteration 64, loss = 0.01575462
Iteration 65, loss = 0.01575841
Iteration 66, loss = 0.01562030
Iteration 67, loss = 0.01541111
Iteration 68, loss = 0.01529294
Iteration 69, loss = 0.01527391
Iteration 70, loss = 0.01528077
Iteration 71, loss = 0.01495949
Iteration 72, loss = 0.01495530
Iteration 73, loss = 0.01497180
Iteration 74, loss = 0.01486259
Iteration 75, loss = 0.01488993
Iteration 76, loss = 0.01481567
Iteration 77, loss = 0.01488232
Iteration 78, loss = 0.01463678
Iteration 79, loss = 0.01457286
Iteration 80, loss = 0.01453541
Iteration 81, loss = 0.01436329
Iteration 82, loss = 0.01447680
Iteration 83, loss = 0.01440183
Iteration 84, loss = 0.01433292
Iteration 85, loss = 0.01434778
Iteration 86, loss = 0.01430544
Iteration 87, loss = 0.01432593
Iteration 88, loss = 0.01425007
Iteration 89, loss = 0.01411243
Iteration 90, loss = 0.01419824
Iteration 91, loss = 0.01409468
Iteration 92, loss = 0.01421357
Iteration 93, loss = 0.01386681
Iteration 94, loss = 0.01423273
Iteration 95, loss = 0.01409197
Iteration 96, loss = 0.01413765
Iteration 97, loss = 0.01389675
Iteration 98, loss = 0.01376356
Iteration 99, loss = 0.01386429
Iteration 100, loss = 0.01383653
Iteration 101, loss = 0.01389287
Iteration 102, loss = 0.01368674
Iteration 103, loss = 0.01378279
Iteration 104, loss = 0.01391656
Iteration 105, loss = 0.01374832
Iteration 106, loss = 0.01365398
Iteration 107, loss = 0.01370959
Iteration 108, loss = 0.01363174
Iteration 109, loss = 0.01372649
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67787805
Iteration 2, loss = 0.60366458
Iteration 3, loss = 0.48664193
Iteration 4, loss = 0.36871442
Iteration 5, loss = 0.27126681
Iteration 6, loss = 0.20086887
Iteration 7, loss = 0.15269617
Iteration 8, loss = 0.11989421
Iteration 9, loss = 0.09735918
Iteration 10, loss = 0.08148260
Iteration 11, loss = 0.06968365
Iteration 12, loss = 0.06088436
Iteration 13, loss = 0.05421767
Iteration 14, loss = 0.04907633
Iteration 15, loss = 0.04484431
Iteration 16, loss = 0.04157352
Iteration 17, loss = 0.03877935
Iteration 18, loss = 0.03647049
Iteration 19, loss = 0.03466344
Iteration 20, loss = 0.03291861
Iteration 21, loss = 0.03156379
Iteration 22, loss = 0.03038935
Iteration 23, loss = 0.02919529
Iteration 24, loss = 0.02837332
Iteration 25, loss = 0.02736463
Iteration 26, loss = 0.02671318
Iteration 27, loss = 0.02636320
Iteration 28, loss = 0.02543531
Iteration 29, loss = 0.02511137
Iteration 30, loss = 0.02458193
Iteration 31, loss = 0.02405576
Iteration 32, loss = 0.02391625
Iteration 33, loss = 0.02332142
Iteration 34, loss = 0.02304825
Iteration 35, loss = 0.02311891
Iteration 36, loss = 0.02260962
Iteration 37, loss = 0.02241817
Iteration 38, loss = 0.02241194
Iteration 39, loss = 0.02200107
Iteration 40, loss = 0.02189814
Iteration 41, loss = 0.02179911
Iteration 42, loss = 0.02143370
Iteration 43, loss = 0.02133489
Iteration 44, loss = 0.02143647
Iteration 45, loss = 0.02110304
Iteration 46, loss = 0.02118759
Iteration 47, loss = 0.02094597
Iteration 48, loss = 0.02073117
Iteration 49, loss = 0.02087211
Iteration 50, loss = 0.02058742
Iteration 51, loss = 0.02043788
Iteration 52, loss = 0.02068846
Iteration 53, loss = 0.02063174
Iteration 54, loss = 0.02038661
Iteration 55, loss = 0.02018341
Iteration 56, loss = 0.02037667
Iteration 57, loss = 0.01998388
Iteration 58, loss = 0.02035864
Iteration 59, loss = 0.02029943
Iteration 60, loss = 0.01999302
Iteration 61, loss = 0.02001014
Iteration 62, loss = 0.01989244
Iteration 63, loss = 0.02008308
Iteration 64, loss = 0.01977895
Iteration 65, loss = 0.01974693
Iteration 66, loss = 0.01969607
Iteration 67, loss = 0.01956368
Iteration 68, loss = 0.01964249
Iteration 69, loss = 0.01972788
Iteration 70, loss = 0.01982623
Iteration 71, loss = 0.01994597
Iteration 72, loss = 0.01962646
Iteration 73, loss = 0.01977608
Iteration 74, loss = 0.01966309
Iteration 75, loss = 0.01954395
Iteration 76, loss = 0.01947426
Iteration 77, loss = 0.01947175
Iteration 78, loss = 0.01935130
Iteration 79, loss = 0.01972464
Iteration 80, loss = 0.01941167
Iteration 81, loss = 0.01945561
Iteration 82, loss = 0.01955079
Iteration 83, loss = 0.01916880
Iteration 84, loss = 0.01917580
Iteration 85, loss = 0.01931369
Iteration 86, loss = 0.01921125
Iteration 87, loss = 0.01936287
Iteration 88, loss = 0.01932044
Iteration 89, loss = 0.01952268
Iteration 90, loss = 0.01910831
Iteration 91, loss = 0.01924294
Iteration 92, loss = 0.01911415
Iteration 93, loss = 0.01922762
Iteration 94, loss = 0.01920450
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68190285
Iteration 2, loss = 0.65659168
Iteration 3, loss = 0.61986166
Iteration 4, loss = 0.56831795
Iteration 5, loss = 0.50749975
Iteration 6, loss = 0.44401579
Iteration 7, loss = 0.38257679
Iteration 8, loss = 0.32657159
Iteration 9, loss = 0.27706214
Iteration 10, loss = 0.23465791
Iteration 11, loss = 0.19920829
Iteration 12, loss = 0.16988325
Iteration 13, loss = 0.14602425
Iteration 14, loss = 0.12649432
Iteration 15, loss = 0.11060653
Iteration 16, loss = 0.09754055
Iteration 17, loss = 0.08681742
Iteration 18, loss = 0.07801605
Iteration 19, loss = 0.07046150
Iteration 20, loss = 0.06421748
Iteration 21, loss = 0.05909803
Iteration 22, loss = 0.05451638
Iteration 23, loss = 0.05054217
Iteration 24, loss = 0.04715111
Iteration 25, loss = 0.04424432
Iteration 26, loss = 0.04157788
Iteration 27, loss = 0.03932073
Iteration 28, loss = 0.03733524
Iteration 29, loss = 0.03563382
Iteration 30, loss = 0.03401147
Iteration 31, loss = 0.03248600
Iteration 32, loss = 0.03118419
Iteration 33, loss = 0.03003031
Iteration 34, loss = 0.02900981
Iteration 35, loss = 0.02808348
Iteration 36, loss = 0.02727220
Iteration 37, loss = 0.02636063
Iteration 38, loss = 0.02568109
Iteration 39, loss = 0.02498829
Iteration 40, loss = 0.02437727
Iteration 41, loss = 0.02389801
Iteration 42, loss = 0.02334875
Iteration 43, loss = 0.02282579
Iteration 44, loss = 0.02231266
Iteration 45, loss = 0.02204598
Iteration 46, loss = 0.02163604
Iteration 47, loss = 0.02122613
Iteration 48, loss = 0.02080494
Iteration 49, loss = 0.02054510
Iteration 50, loss = 0.02017307
Iteration 51, loss = 0.01994013
Iteration 52, loss = 0.01975563
Iteration 53, loss = 0.01941727
Iteration 54, loss = 0.01925230
Iteration 55, loss = 0.01900243
Iteration 56, loss = 0.01881117
Iteration 57, loss = 0.01857256
Iteration 58, loss = 0.01853179
Iteration 59, loss = 0.01841697
Iteration 60, loss = 0.01807846
Iteration 61, loss = 0.01795918
Iteration 62, loss = 0.01779459
Iteration 63, loss = 0.01756495
Iteration 64, loss = 0.01753705
Iteration 65, loss = 0.01725437
Iteration 66, loss = 0.01724828
Iteration 67, loss = 0.01726081
Iteration 68, loss = 0.01702782
Iteration 69, loss = 0.01700744
Iteration 70, loss = 0.01688240
Iteration 71, loss = 0.01672281
Iteration 72, loss = 0.01668084
Iteration 73, loss = 0.01660636
Iteration 74, loss = 0.01649877
Iteration 75, loss = 0.01635985
Iteration 76, loss = 0.01628748
Iteration 77, loss = 0.01633453
Iteration 78, loss = 0.01637846
Iteration 79, loss = 0.01619369
Iteration 80, loss = 0.01603417
Iteration 81, loss = 0.01614686
Iteration 82, loss = 0.01594077
Iteration 83, loss = 0.01588696
Iteration 84, loss = 0.01577406
Iteration 85, loss = 0.01567316
Iteration 86, loss = 0.01566546
Iteration 87, loss = 0.01559921
Iteration 88, loss = 0.01572174
Iteration 89, loss = 0.01549298
Iteration 90, loss = 0.01542111
Iteration 91, loss = 0.01544491
Iteration 92, loss = 0.01531207
Iteration 93, loss = 0.01545070
Iteration 94, loss = 0.01546536
Iteration 95, loss = 0.01545490
Iteration 96, loss = 0.01521686
Iteration 97, loss = 0.01529615
Iteration 98, loss = 0.01532694
Iteration 99, loss = 0.01517940
Iteration 100, loss = 0.01514700
Iteration 101, loss = 0.01506784
Iteration 102, loss = 0.01507094
Iteration 103, loss = 0.01505821
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68932735
Iteration 2, loss = 0.60508584
Iteration 3, loss = 0.49160075
Iteration 4, loss = 0.37571141
Iteration 5, loss = 0.27885057
Iteration 6, loss = 0.20754624
Iteration 7, loss = 0.15760656
Iteration 8, loss = 0.12345241
Iteration 9, loss = 0.09993661
Iteration 10, loss = 0.08296571
Iteration 11, loss = 0.07073119
Iteration 12, loss = 0.06171565
Iteration 13, loss = 0.05442369
Iteration 14, loss = 0.04944704
Iteration 15, loss = 0.04480241
Iteration 16, loss = 0.04136486
Iteration 17, loss = 0.03867542
Iteration 18, loss = 0.03609648
Iteration 19, loss = 0.03407351
Iteration 20, loss = 0.03240815
Iteration 21, loss = 0.03097307
Iteration 22, loss = 0.02987632
Iteration 23, loss = 0.02915872
Iteration 24, loss = 0.02792289
Iteration 25, loss = 0.02706837
Iteration 26, loss = 0.02625234
Iteration 27, loss = 0.02591118
Iteration 28, loss = 0.02534467
Iteration 29, loss = 0.02453522
Iteration 30, loss = 0.02428923
Iteration 31, loss = 0.02385082
Iteration 32, loss = 0.02336376
Iteration 33, loss = 0.02328695
Iteration 34, loss = 0.02280331
Iteration 35, loss = 0.02254311
Iteration 36, loss = 0.02261940
Iteration 37, loss = 0.02212654
Iteration 38, loss = 0.02197946
Iteration 39, loss = 0.02164474
Iteration 40, loss = 0.02165870
Iteration 41, loss = 0.02134446
Iteration 42, loss = 0.02104576
Iteration 43, loss = 0.02101110
Iteration 44, loss = 0.02097798
Iteration 45, loss = 0.02061793
Iteration 46, loss = 0.02077266
Iteration 47, loss = 0.02054931
Iteration 48, loss = 0.02042055
Iteration 49, loss = 0.02029214
Iteration 50, loss = 0.02025513
Iteration 51, loss = 0.02020853
Iteration 52, loss = 0.02017942
Iteration 53, loss = 0.02001756
Iteration 54, loss = 0.01981231
Iteration 55, loss = 0.02010268
Iteration 56, loss = 0.01996112
Iteration 57, loss = 0.01967447
Iteration 58, loss = 0.01944893
Iteration 59, loss = 0.01987407
Iteration 60, loss = 0.01926512
Iteration 61, loss = 0.01961124
Iteration 62, loss = 0.01951579
Iteration 63, loss = 0.01943673
Iteration 64, loss = 0.01944815
Iteration 65, loss = 0.01974767
Iteration 66, loss = 0.01915297
Iteration 67, loss = 0.01945482
Iteration 68, loss = 0.01918906
Iteration 69, loss = 0.01927284
Iteration 70, loss = 0.01921755
Iteration 71, loss = 0.01902709
Iteration 72, loss = 0.01931804
Iteration 73, loss = 0.01899680
Iteration 74, loss = 0.01887325
Iteration 75, loss = 0.01912134
Iteration 76, loss = 0.01936064
Iteration 77, loss = 0.01887877
Iteration 78, loss = 0.01920697
Iteration 79, loss = 0.01901709
Iteration 80, loss = 0.01926308
Iteration 81, loss = 0.01898084
Iteration 82, loss = 0.01896262
Iteration 83, loss = 0.01869051
Iteration 84, loss = 0.01887913
Iteration 85, loss = 0.01892350
Iteration 86, loss = 0.01912906
Iteration 87, loss = 0.01876765
Iteration 88, loss = 0.01865451
Iteration 89, loss = 0.01880142
Iteration 90, loss = 0.01868361
Iteration 91, loss = 0.01865436
Iteration 92, loss = 0.01879441
Iteration 93, loss = 0.01879260
Iteration 94, loss = 0.01873021
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	94m42.148s
user	374m14.874s
sys	38m42.728s
Elapsed time: 5682 seconds
