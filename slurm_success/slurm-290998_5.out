Processing dataset: dropped_v6_custom_stopwords

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: dropped_v6_custom_stopwords (7424 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on dropped_v6_custom_stopwords...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on dropped_v6_custom_stopwords...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on dropped_v6_custom_stopwords...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on dropped_v6_custom_stopwords...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on dropped_v6_custom_stopwords...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.68441217
Iteration 2, loss = 0.61570533
Iteration 3, loss = 0.51047754
Iteration 4, loss = 0.39772273
Iteration 5, loss = 0.30042581
Iteration 6, loss = 0.22603164
Iteration 7, loss = 0.17314156
Iteration 8, loss = 0.13670285
Iteration 9, loss = 0.11141591
Iteration 10, loss = 0.09344384
Iteration 11, loss = 0.08034027
Iteration 12, loss = 0.07010822
Iteration 13, loss = 0.06272036
Iteration 14, loss = 0.05699403
Iteration 15, loss = 0.05231077
Iteration 16, loss = 0.04817383
Iteration 17, loss = 0.04533950
Iteration 18, loss = 0.04261545
Iteration 19, loss = 0.04057774
Iteration 20, loss = 0.03900289
Iteration 21, loss = 0.03721754
Iteration 22, loss = 0.03575939
Iteration 23, loss = 0.03460635
Iteration 24, loss = 0.03348998
Iteration 25, loss = 0.03259995
Iteration 26, loss = 0.03172561
Iteration 27, loss = 0.03110218
Iteration 28, loss = 0.03054906
Iteration 29, loss = 0.03005285
Iteration 30, loss = 0.02954616
Iteration 31, loss = 0.02918887
Iteration 32, loss = 0.02864198
Iteration 33, loss = 0.02822768
Iteration 34, loss = 0.02794751
Iteration 35, loss = 0.02747889
Iteration 36, loss = 0.02727879
Iteration 37, loss = 0.02698339
Iteration 38, loss = 0.02693098
Iteration 39, loss = 0.02665579
Iteration 40, loss = 0.02658516
Iteration 41, loss = 0.02626905
Iteration 42, loss = 0.02620335
Iteration 43, loss = 0.02603712
Iteration 44, loss = 0.02588494
Iteration 45, loss = 0.02575265
Iteration 46, loss = 0.02528053
Iteration 47, loss = 0.02528148
Iteration 48, loss = 0.02535665
Iteration 49, loss = 0.02511594
Iteration 50, loss = 0.02525254
Iteration 51, loss = 0.02490748
Iteration 52, loss = 0.02503837
Iteration 53, loss = 0.02483557
Iteration 54, loss = 0.02468256
Iteration 55, loss = 0.02465114
Iteration 56, loss = 0.02471535
Iteration 57, loss = 0.02448523
Iteration 58, loss = 0.02436779
Iteration 59, loss = 0.02434361
Iteration 60, loss = 0.02431282
Iteration 61, loss = 0.02442215
Iteration 62, loss = 0.02437556
Iteration 63, loss = 0.02421349
Iteration 64, loss = 0.02413981
Iteration 65, loss = 0.02426928
Iteration 66, loss = 0.02433246
Iteration 67, loss = 0.02439272
Iteration 68, loss = 0.02406937
Iteration 69, loss = 0.02417778
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67313730
Iteration 2, loss = 0.59670972
Iteration 3, loss = 0.48178335
Iteration 4, loss = 0.36976908
Iteration 5, loss = 0.27807960
Iteration 6, loss = 0.20925907
Iteration 7, loss = 0.16140690
Iteration 8, loss = 0.12798964
Iteration 9, loss = 0.10470692
Iteration 10, loss = 0.08801087
Iteration 11, loss = 0.07540462
Iteration 12, loss = 0.06599624
Iteration 13, loss = 0.05881480
Iteration 14, loss = 0.05332410
Iteration 15, loss = 0.04860617
Iteration 16, loss = 0.04495087
Iteration 17, loss = 0.04173476
Iteration 18, loss = 0.03929108
Iteration 19, loss = 0.03717488
Iteration 20, loss = 0.03548151
Iteration 21, loss = 0.03377590
Iteration 22, loss = 0.03251916
Iteration 23, loss = 0.03117936
Iteration 24, loss = 0.03010461
Iteration 25, loss = 0.02963473
Iteration 26, loss = 0.02866065
Iteration 27, loss = 0.02804414
Iteration 28, loss = 0.02717483
Iteration 29, loss = 0.02701625
Iteration 30, loss = 0.02645403
Iteration 31, loss = 0.02590050
Iteration 32, loss = 0.02555141
Iteration 33, loss = 0.02502619
Iteration 34, loss = 0.02490645
Iteration 35, loss = 0.02433200
Iteration 36, loss = 0.02439241
Iteration 37, loss = 0.02381115
Iteration 38, loss = 0.02371976
Iteration 39, loss = 0.02329075
Iteration 40, loss = 0.02333734
Iteration 41, loss = 0.02323940
Iteration 42, loss = 0.02275514
Iteration 43, loss = 0.02269963
Iteration 44, loss = 0.02260522
Iteration 45, loss = 0.02267483
Iteration 46, loss = 0.02254808
Iteration 47, loss = 0.02212738
Iteration 48, loss = 0.02228085
Iteration 49, loss = 0.02184016
Iteration 50, loss = 0.02195305
Iteration 51, loss = 0.02185229
Iteration 52, loss = 0.02170162
Iteration 53, loss = 0.02148481
Iteration 54, loss = 0.02130076
Iteration 55, loss = 0.02150192
Iteration 56, loss = 0.02164995
Iteration 57, loss = 0.02111634
Iteration 58, loss = 0.02148512
Iteration 59, loss = 0.02145567
Iteration 60, loss = 0.02129991
Iteration 61, loss = 0.02105939
Iteration 62, loss = 0.02094658
Iteration 63, loss = 0.02104241
Iteration 64, loss = 0.02070778
Iteration 65, loss = 0.02082709
Iteration 66, loss = 0.02080911
Iteration 67, loss = 0.02125705
Iteration 68, loss = 0.02057593
Iteration 69, loss = 0.02083957
Iteration 70, loss = 0.02077113
Iteration 71, loss = 0.02070515
Iteration 72, loss = 0.02074654
Iteration 73, loss = 0.02053987
Iteration 74, loss = 0.02042952
Iteration 75, loss = 0.02073708
Iteration 76, loss = 0.02042747
Iteration 77, loss = 0.02034272
Iteration 78, loss = 0.02037874
Iteration 79, loss = 0.02050645
Iteration 80, loss = 0.02043134
Iteration 81, loss = 0.02056173
Iteration 82, loss = 0.02018694
Iteration 83, loss = 0.02003050
Iteration 84, loss = 0.02012164
Iteration 85, loss = 0.02038010
Iteration 86, loss = 0.02012893
Iteration 87, loss = 0.02014447
Iteration 88, loss = 0.02011695
Iteration 89, loss = 0.02028854
Iteration 90, loss = 0.02017918
Iteration 91, loss = 0.02009745
Iteration 92, loss = 0.02006846
Iteration 93, loss = 0.02017348
Iteration 94, loss = 0.02005830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67499308
Iteration 2, loss = 0.60420324
Iteration 3, loss = 0.49625393
Iteration 4, loss = 0.38526624
Iteration 5, loss = 0.29125186
Iteration 6, loss = 0.22052796
Iteration 7, loss = 0.17013597
Iteration 8, loss = 0.13537861
Iteration 9, loss = 0.11078684
Iteration 10, loss = 0.09353007
Iteration 11, loss = 0.08048973
Iteration 12, loss = 0.07093580
Iteration 13, loss = 0.06340488
Iteration 14, loss = 0.05755283
Iteration 15, loss = 0.05270258
Iteration 16, loss = 0.04915031
Iteration 17, loss = 0.04584384
Iteration 18, loss = 0.04342406
Iteration 19, loss = 0.04146800
Iteration 20, loss = 0.03962009
Iteration 21, loss = 0.03778861
Iteration 22, loss = 0.03639491
Iteration 23, loss = 0.03550211
Iteration 24, loss = 0.03432779
Iteration 25, loss = 0.03348245
Iteration 26, loss = 0.03256978
Iteration 27, loss = 0.03183680
Iteration 28, loss = 0.03140865
Iteration 29, loss = 0.03075283
Iteration 30, loss = 0.03044403
Iteration 31, loss = 0.02972575
Iteration 32, loss = 0.02930480
Iteration 33, loss = 0.02897114
Iteration 34, loss = 0.02886235
Iteration 35, loss = 0.02829401
Iteration 36, loss = 0.02816403
Iteration 37, loss = 0.02793596
Iteration 38, loss = 0.02774669
Iteration 39, loss = 0.02785375
Iteration 40, loss = 0.02713121
Iteration 41, loss = 0.02705852
Iteration 42, loss = 0.02708384
Iteration 43, loss = 0.02648850
Iteration 44, loss = 0.02656188
Iteration 45, loss = 0.02651307
Iteration 46, loss = 0.02637137
Iteration 47, loss = 0.02611293
Iteration 48, loss = 0.02617951
Iteration 49, loss = 0.02581096
Iteration 50, loss = 0.02571244
Iteration 51, loss = 0.02566290
Iteration 52, loss = 0.02547356
Iteration 53, loss = 0.02564351
Iteration 54, loss = 0.02572187
Iteration 55, loss = 0.02537763
Iteration 56, loss = 0.02558738
Iteration 57, loss = 0.02547245
Iteration 58, loss = 0.02520345
Iteration 59, loss = 0.02539464
Iteration 60, loss = 0.02518532
Iteration 61, loss = 0.02513062
Iteration 62, loss = 0.02548728
Iteration 63, loss = 0.02507330
Iteration 64, loss = 0.02518310
Iteration 65, loss = 0.02490475
Iteration 66, loss = 0.02485946
Iteration 67, loss = 0.02477919
Iteration 68, loss = 0.02472127
Iteration 69, loss = 0.02468504
Iteration 70, loss = 0.02464980
Iteration 71, loss = 0.02477481
Iteration 72, loss = 0.02452973
Iteration 73, loss = 0.02480314
Iteration 74, loss = 0.02487015
Iteration 75, loss = 0.02445894
Iteration 76, loss = 0.02452153
Iteration 77, loss = 0.02468328
Iteration 78, loss = 0.02459598
Iteration 79, loss = 0.02460184
Iteration 80, loss = 0.02436222
Iteration 81, loss = 0.02443247
Iteration 82, loss = 0.02455502
Iteration 83, loss = 0.02433352
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68488930
Iteration 2, loss = 0.60851111
Iteration 3, loss = 0.50021480
Iteration 4, loss = 0.38709233
Iteration 5, loss = 0.29125306
Iteration 6, loss = 0.21942520
Iteration 7, loss = 0.16891154
Iteration 8, loss = 0.13398648
Iteration 9, loss = 0.10974630
Iteration 10, loss = 0.09210200
Iteration 11, loss = 0.07954409
Iteration 12, loss = 0.06977302
Iteration 13, loss = 0.06260639
Iteration 14, loss = 0.05672127
Iteration 15, loss = 0.05204096
Iteration 16, loss = 0.04839142
Iteration 17, loss = 0.04538258
Iteration 18, loss = 0.04267198
Iteration 19, loss = 0.04049985
Iteration 20, loss = 0.03888689
Iteration 21, loss = 0.03715835
Iteration 22, loss = 0.03580579
Iteration 23, loss = 0.03471830
Iteration 24, loss = 0.03362979
Iteration 25, loss = 0.03266510
Iteration 26, loss = 0.03211686
Iteration 27, loss = 0.03121891
Iteration 28, loss = 0.03076995
Iteration 29, loss = 0.02998323
Iteration 30, loss = 0.02964574
Iteration 31, loss = 0.02902963
Iteration 32, loss = 0.02897092
Iteration 33, loss = 0.02834690
Iteration 34, loss = 0.02783240
Iteration 35, loss = 0.02768258
Iteration 36, loss = 0.02731800
Iteration 37, loss = 0.02715061
Iteration 38, loss = 0.02703731
Iteration 39, loss = 0.02698745
Iteration 40, loss = 0.02640948
Iteration 41, loss = 0.02627657
Iteration 42, loss = 0.02581965
Iteration 43, loss = 0.02557370
Iteration 44, loss = 0.02566744
Iteration 45, loss = 0.02565853
Iteration 46, loss = 0.02556662
Iteration 47, loss = 0.02559632
Iteration 48, loss = 0.02574638
Iteration 49, loss = 0.02538533
Iteration 50, loss = 0.02521944
Iteration 51, loss = 0.02486090
Iteration 52, loss = 0.02501128
Iteration 53, loss = 0.02476689
Iteration 54, loss = 0.02480661
Iteration 55, loss = 0.02487142
Iteration 56, loss = 0.02480405
Iteration 57, loss = 0.02460015
Iteration 58, loss = 0.02473152
Iteration 59, loss = 0.02459221
Iteration 60, loss = 0.02444667
Iteration 61, loss = 0.02442551
Iteration 62, loss = 0.02436995
Iteration 63, loss = 0.02400145
Iteration 64, loss = 0.02422731
Iteration 65, loss = 0.02433998
Iteration 66, loss = 0.02425653
Iteration 67, loss = 0.02452620
Iteration 68, loss = 0.02433745
Iteration 69, loss = 0.02366938
Iteration 70, loss = 0.02383351
Iteration 71, loss = 0.02383369
Iteration 72, loss = 0.02406340
Iteration 73, loss = 0.02402129
Iteration 74, loss = 0.02365622
Iteration 75, loss = 0.02399407
Iteration 76, loss = 0.02401945
Iteration 77, loss = 0.02368824
Iteration 78, loss = 0.02383079
Iteration 79, loss = 0.02338110
Iteration 80, loss = 0.02388948
Iteration 81, loss = 0.02338429
Iteration 82, loss = 0.02382873
Iteration 83, loss = 0.02362596
Iteration 84, loss = 0.02369822
Iteration 85, loss = 0.02373881
Iteration 86, loss = 0.02383747
Iteration 87, loss = 0.02359905
Iteration 88, loss = 0.02380467
Iteration 89, loss = 0.02345090
Iteration 90, loss = 0.02354269
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70000179
Iteration 2, loss = 0.63338037
Iteration 3, loss = 0.52580854
Iteration 4, loss = 0.40363457
Iteration 5, loss = 0.30193666
Iteration 6, loss = 0.22519457
Iteration 7, loss = 0.17211259
Iteration 8, loss = 0.13586390
Iteration 9, loss = 0.11017752
Iteration 10, loss = 0.09232158
Iteration 11, loss = 0.07910564
Iteration 12, loss = 0.06905853
Iteration 13, loss = 0.06153188
Iteration 14, loss = 0.05565674
Iteration 15, loss = 0.05095545
Iteration 16, loss = 0.04709732
Iteration 17, loss = 0.04399660
Iteration 18, loss = 0.04163886
Iteration 19, loss = 0.03918347
Iteration 20, loss = 0.03739188
Iteration 21, loss = 0.03569609
Iteration 22, loss = 0.03420742
Iteration 23, loss = 0.03300840
Iteration 24, loss = 0.03213629
Iteration 25, loss = 0.03134916
Iteration 26, loss = 0.03044387
Iteration 27, loss = 0.02972534
Iteration 28, loss = 0.02913948
Iteration 29, loss = 0.02852886
Iteration 30, loss = 0.02823977
Iteration 31, loss = 0.02754530
Iteration 32, loss = 0.02713783
Iteration 33, loss = 0.02668894
Iteration 34, loss = 0.02638868
Iteration 35, loss = 0.02603483
Iteration 36, loss = 0.02606379
Iteration 37, loss = 0.02569352
Iteration 38, loss = 0.02527511
Iteration 39, loss = 0.02522626
Iteration 40, loss = 0.02520220
Iteration 41, loss = 0.02471465
Iteration 42, loss = 0.02473249
Iteration 43, loss = 0.02441982
Iteration 44, loss = 0.02432284
Iteration 45, loss = 0.02402288
Iteration 46, loss = 0.02413168
Iteration 47, loss = 0.02414698
Iteration 48, loss = 0.02384959
Iteration 49, loss = 0.02362846
Iteration 50, loss = 0.02372575
Iteration 51, loss = 0.02351953
Iteration 52, loss = 0.02352066
Iteration 53, loss = 0.02342970
Iteration 54, loss = 0.02351756
Iteration 55, loss = 0.02336713
Iteration 56, loss = 0.02303114
Iteration 57, loss = 0.02315975
Iteration 58, loss = 0.02303396
Iteration 59, loss = 0.02273661
Iteration 60, loss = 0.02291933
Iteration 61, loss = 0.02293805
Iteration 62, loss = 0.02290398
Iteration 63, loss = 0.02281968
Iteration 64, loss = 0.02275559
Iteration 65, loss = 0.02264469
Iteration 66, loss = 0.02294594
Iteration 67, loss = 0.02280739
Iteration 68, loss = 0.02275672
Iteration 69, loss = 0.02264273
Iteration 70, loss = 0.02271499
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68441217
Iteration 2, loss = 0.61570533
Iteration 3, loss = 0.51047754
Iteration 4, loss = 0.39772273
Iteration 5, loss = 0.30042581
Iteration 6, loss = 0.22603164
Iteration 7, loss = 0.17314156
Iteration 8, loss = 0.13670285
Iteration 9, loss = 0.11141591
Iteration 10, loss = 0.09344384
Iteration 11, loss = 0.08034027
Iteration 12, loss = 0.07010822
Iteration 13, loss = 0.06272036
Iteration 14, loss = 0.05699403
Iteration 15, loss = 0.05231077
Iteration 16, loss = 0.04817383
Iteration 17, loss = 0.04533950
Iteration 18, loss = 0.04261545
Iteration 19, loss = 0.04057774
Iteration 20, loss = 0.03900289
Iteration 21, loss = 0.03721754
Iteration 22, loss = 0.03575939
Iteration 23, loss = 0.03460635
Iteration 24, loss = 0.03348998
Iteration 25, loss = 0.03259995
Iteration 26, loss = 0.03172561
Iteration 27, loss = 0.03110218
Iteration 28, loss = 0.03054906
Iteration 29, loss = 0.03005285
Iteration 30, loss = 0.02954616
Iteration 31, loss = 0.02918887
Iteration 32, loss = 0.02864198
Iteration 33, loss = 0.02822768
Iteration 34, loss = 0.02794751
Iteration 35, loss = 0.02747889
Iteration 36, loss = 0.02727879
Iteration 37, loss = 0.02698339
Iteration 38, loss = 0.02693098
Iteration 39, loss = 0.02665579
Iteration 40, loss = 0.02658516
Iteration 41, loss = 0.02626905
Iteration 42, loss = 0.02620335
Iteration 43, loss = 0.02603712
Iteration 44, loss = 0.02588494
Iteration 45, loss = 0.02575265
Iteration 46, loss = 0.02528053
Iteration 47, loss = 0.02528148
Iteration 48, loss = 0.02535665
Iteration 49, loss = 0.02511594
Iteration 50, loss = 0.02525254
Iteration 51, loss = 0.02490748
Iteration 52, loss = 0.02503837
Iteration 53, loss = 0.02483557
Iteration 54, loss = 0.02468256
Iteration 55, loss = 0.02465114
Iteration 56, loss = 0.02471535
Iteration 57, loss = 0.02448523
Iteration 58, loss = 0.02436779
Iteration 59, loss = 0.02434361
Iteration 60, loss = 0.02431282
Iteration 61, loss = 0.02442215
Iteration 62, loss = 0.02437556
Iteration 63, loss = 0.02421349
Iteration 64, loss = 0.02413981
Iteration 65, loss = 0.02426928
Iteration 66, loss = 0.02433246
Iteration 67, loss = 0.02439272
Iteration 68, loss = 0.02406937
Iteration 69, loss = 0.02417778
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67313730
Iteration 2, loss = 0.59670972
Iteration 3, loss = 0.48178335
Iteration 4, loss = 0.36976908
Iteration 5, loss = 0.27807960
Iteration 6, loss = 0.20925907
Iteration 7, loss = 0.16140690
Iteration 8, loss = 0.12798964
Iteration 9, loss = 0.10470692
Iteration 10, loss = 0.08801087
Iteration 11, loss = 0.07540462
Iteration 12, loss = 0.06599624
Iteration 13, loss = 0.05881480
Iteration 14, loss = 0.05332410
Iteration 15, loss = 0.04860617
Iteration 16, loss = 0.04495087
Iteration 17, loss = 0.04173476
Iteration 18, loss = 0.03929108
Iteration 19, loss = 0.03717488
Iteration 20, loss = 0.03548151
Iteration 21, loss = 0.03377590
Iteration 22, loss = 0.03251916
Iteration 23, loss = 0.03117936
Iteration 24, loss = 0.03010461
Iteration 25, loss = 0.02963473
Iteration 26, loss = 0.02866065
Iteration 27, loss = 0.02804414
Iteration 28, loss = 0.02717483
Iteration 29, loss = 0.02701625
Iteration 30, loss = 0.02645403
Iteration 31, loss = 0.02590050
Iteration 32, loss = 0.02555141
Iteration 33, loss = 0.02502619
Iteration 34, loss = 0.02490645
Iteration 35, loss = 0.02433200
Iteration 36, loss = 0.02439241
Iteration 37, loss = 0.02381115
Iteration 38, loss = 0.02371976
Iteration 39, loss = 0.02329075
Iteration 40, loss = 0.02333734
Iteration 41, loss = 0.02323940
Iteration 42, loss = 0.02275514
Iteration 43, loss = 0.02269963
Iteration 44, loss = 0.02260522
Iteration 45, loss = 0.02267483
Iteration 46, loss = 0.02254808
Iteration 47, loss = 0.02212738
Iteration 48, loss = 0.02228085
Iteration 49, loss = 0.02184016
Iteration 50, loss = 0.02195305
Iteration 51, loss = 0.02185229
Iteration 52, loss = 0.02170162
Iteration 53, loss = 0.02148481
Iteration 54, loss = 0.02130076
Iteration 55, loss = 0.02150192
Iteration 56, loss = 0.02164995
Iteration 57, loss = 0.02111634
Iteration 58, loss = 0.02148512
Iteration 59, loss = 0.02145567
Iteration 60, loss = 0.02129991
Iteration 61, loss = 0.02105939
Iteration 62, loss = 0.02094658
Iteration 63, loss = 0.02104241
Iteration 64, loss = 0.02070778
Iteration 65, loss = 0.02082709
Iteration 66, loss = 0.02080911
Iteration 67, loss = 0.02125705
Iteration 68, loss = 0.02057593
Iteration 69, loss = 0.02083957
Iteration 70, loss = 0.02077113
Iteration 71, loss = 0.02070515
Iteration 72, loss = 0.02074654
Iteration 73, loss = 0.02053987
Iteration 74, loss = 0.02042952
Iteration 75, loss = 0.02073708
Iteration 76, loss = 0.02042747
Iteration 77, loss = 0.02034272
Iteration 78, loss = 0.02037874
Iteration 79, loss = 0.02050645
Iteration 80, loss = 0.02043134
Iteration 81, loss = 0.02056173
Iteration 82, loss = 0.02018694
Iteration 83, loss = 0.02003050
Iteration 84, loss = 0.02012164
Iteration 85, loss = 0.02038010
Iteration 86, loss = 0.02012893
Iteration 87, loss = 0.02014447
Iteration 88, loss = 0.02011695
Iteration 89, loss = 0.02028854
Iteration 90, loss = 0.02017918
Iteration 91, loss = 0.02009745
Iteration 92, loss = 0.02006846
Iteration 93, loss = 0.02017348
Iteration 94, loss = 0.02005830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67499308
Iteration 2, loss = 0.60420324
Iteration 3, loss = 0.49625393
Iteration 4, loss = 0.38526624
Iteration 5, loss = 0.29125186
Iteration 6, loss = 0.22052796
Iteration 7, loss = 0.17013597
Iteration 8, loss = 0.13537861
Iteration 9, loss = 0.11078684
Iteration 10, loss = 0.09353007
Iteration 11, loss = 0.08048973
Iteration 12, loss = 0.07093580
Iteration 13, loss = 0.06340488
Iteration 14, loss = 0.05755283
Iteration 15, loss = 0.05270258
Iteration 16, loss = 0.04915031
Iteration 17, loss = 0.04584384
Iteration 18, loss = 0.04342406
Iteration 19, loss = 0.04146800
Iteration 20, loss = 0.03962009
Iteration 21, loss = 0.03778861
Iteration 22, loss = 0.03639491
Iteration 23, loss = 0.03550211
Iteration 24, loss = 0.03432779
Iteration 25, loss = 0.03348245
Iteration 26, loss = 0.03256978
Iteration 27, loss = 0.03183680
Iteration 28, loss = 0.03140865
Iteration 29, loss = 0.03075283
Iteration 30, loss = 0.03044403
Iteration 31, loss = 0.02972575
Iteration 32, loss = 0.02930480
Iteration 33, loss = 0.02897114
Iteration 34, loss = 0.02886235
Iteration 35, loss = 0.02829401
Iteration 36, loss = 0.02816403
Iteration 37, loss = 0.02793596
Iteration 38, loss = 0.02774669
Iteration 39, loss = 0.02785375
Iteration 40, loss = 0.02713121
Iteration 41, loss = 0.02705852
Iteration 42, loss = 0.02708384
Iteration 43, loss = 0.02648850
Iteration 44, loss = 0.02656188
Iteration 45, loss = 0.02651307
Iteration 46, loss = 0.02637137
Iteration 47, loss = 0.02611293
Iteration 48, loss = 0.02617951
Iteration 49, loss = 0.02581096
Iteration 50, loss = 0.02571244
Iteration 51, loss = 0.02566290
Iteration 52, loss = 0.02547356
Iteration 53, loss = 0.02564351
Iteration 54, loss = 0.02572187
Iteration 55, loss = 0.02537763
Iteration 56, loss = 0.02558738
Iteration 57, loss = 0.02547245
Iteration 58, loss = 0.02520345
Iteration 59, loss = 0.02539464
Iteration 60, loss = 0.02518532
Iteration 61, loss = 0.02513062
Iteration 62, loss = 0.02548728
Iteration 63, loss = 0.02507330
Iteration 64, loss = 0.02518310
Iteration 65, loss = 0.02490475
Iteration 66, loss = 0.02485946
Iteration 67, loss = 0.02477919
Iteration 68, loss = 0.02472127
Iteration 69, loss = 0.02468504
Iteration 70, loss = 0.02464980
Iteration 71, loss = 0.02477481
Iteration 72, loss = 0.02452973
Iteration 73, loss = 0.02480314
Iteration 74, loss = 0.02487015
Iteration 75, loss = 0.02445894
Iteration 76, loss = 0.02452153
Iteration 77, loss = 0.02468328
Iteration 78, loss = 0.02459598
Iteration 79, loss = 0.02460184
Iteration 80, loss = 0.02436222
Iteration 81, loss = 0.02443247
Iteration 82, loss = 0.02455502
Iteration 83, loss = 0.02433352
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68488930
Iteration 2, loss = 0.60851111
Iteration 3, loss = 0.50021480
Iteration 4, loss = 0.38709233
Iteration 5, loss = 0.29125306
Iteration 6, loss = 0.21942520
Iteration 7, loss = 0.16891154
Iteration 8, loss = 0.13398648
Iteration 9, loss = 0.10974630
Iteration 10, loss = 0.09210200
Iteration 11, loss = 0.07954409
Iteration 12, loss = 0.06977302
Iteration 13, loss = 0.06260639
Iteration 14, loss = 0.05672127
Iteration 15, loss = 0.05204096
Iteration 16, loss = 0.04839142
Iteration 17, loss = 0.04538258
Iteration 18, loss = 0.04267198
Iteration 19, loss = 0.04049985
Iteration 20, loss = 0.03888689
Iteration 21, loss = 0.03715835
Iteration 22, loss = 0.03580579
Iteration 23, loss = 0.03471830
Iteration 24, loss = 0.03362979
Iteration 25, loss = 0.03266510
Iteration 26, loss = 0.03211686
Iteration 27, loss = 0.03121891
Iteration 28, loss = 0.03076995
Iteration 29, loss = 0.02998323
Iteration 30, loss = 0.02964574
Iteration 31, loss = 0.02902963
Iteration 32, loss = 0.02897092
Iteration 33, loss = 0.02834690
Iteration 34, loss = 0.02783240
Iteration 35, loss = 0.02768258
Iteration 36, loss = 0.02731800
Iteration 37, loss = 0.02715061
Iteration 38, loss = 0.02703731
Iteration 39, loss = 0.02698745
Iteration 40, loss = 0.02640948
Iteration 41, loss = 0.02627657
Iteration 42, loss = 0.02581965
Iteration 43, loss = 0.02557370
Iteration 44, loss = 0.02566744
Iteration 45, loss = 0.02565853
Iteration 46, loss = 0.02556662
Iteration 47, loss = 0.02559632
Iteration 48, loss = 0.02574638
Iteration 49, loss = 0.02538533
Iteration 50, loss = 0.02521944
Iteration 51, loss = 0.02486090
Iteration 52, loss = 0.02501128
Iteration 53, loss = 0.02476689
Iteration 54, loss = 0.02480661
Iteration 55, loss = 0.02487142
Iteration 56, loss = 0.02480405
Iteration 57, loss = 0.02460015
Iteration 58, loss = 0.02473152
Iteration 59, loss = 0.02459221
Iteration 60, loss = 0.02444667
Iteration 61, loss = 0.02442551
Iteration 62, loss = 0.02436995
Iteration 63, loss = 0.02400145
Iteration 64, loss = 0.02422731
Iteration 65, loss = 0.02433998
Iteration 66, loss = 0.02425653
Iteration 67, loss = 0.02452620
Iteration 68, loss = 0.02433745
Iteration 69, loss = 0.02366938
Iteration 70, loss = 0.02383351
Iteration 71, loss = 0.02383369
Iteration 72, loss = 0.02406340
Iteration 73, loss = 0.02402129
Iteration 74, loss = 0.02365622
Iteration 75, loss = 0.02399407
Iteration 76, loss = 0.02401945
Iteration 77, loss = 0.02368824
Iteration 78, loss = 0.02383079
Iteration 79, loss = 0.02338110
Iteration 80, loss = 0.02388948
Iteration 81, loss = 0.02338429
Iteration 82, loss = 0.02382873
Iteration 83, loss = 0.02362596
Iteration 84, loss = 0.02369822
Iteration 85, loss = 0.02373881
Iteration 86, loss = 0.02383747
Iteration 87, loss = 0.02359905
Iteration 88, loss = 0.02380467
Iteration 89, loss = 0.02345090
Iteration 90, loss = 0.02354269
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70000179
Iteration 2, loss = 0.63338037
Iteration 3, loss = 0.52580854
Iteration 4, loss = 0.40363457
Iteration 5, loss = 0.30193666
Iteration 6, loss = 0.22519457
Iteration 7, loss = 0.17211259
Iteration 8, loss = 0.13586390
Iteration 9, loss = 0.11017752
Iteration 10, loss = 0.09232158
Iteration 11, loss = 0.07910564
Iteration 12, loss = 0.06905853
Iteration 13, loss = 0.06153188
Iteration 14, loss = 0.05565674
Iteration 15, loss = 0.05095545
Iteration 16, loss = 0.04709732
Iteration 17, loss = 0.04399660
Iteration 18, loss = 0.04163886
Iteration 19, loss = 0.03918347
Iteration 20, loss = 0.03739188
Iteration 21, loss = 0.03569609
Iteration 22, loss = 0.03420742
Iteration 23, loss = 0.03300840
Iteration 24, loss = 0.03213629
Iteration 25, loss = 0.03134916
Iteration 26, loss = 0.03044387
Iteration 27, loss = 0.02972534
Iteration 28, loss = 0.02913948
Iteration 29, loss = 0.02852886
Iteration 30, loss = 0.02823977
Iteration 31, loss = 0.02754530
Iteration 32, loss = 0.02713783
Iteration 33, loss = 0.02668894
Iteration 34, loss = 0.02638868
Iteration 35, loss = 0.02603483
Iteration 36, loss = 0.02606379
Iteration 37, loss = 0.02569352
Iteration 38, loss = 0.02527511
Iteration 39, loss = 0.02522626
Iteration 40, loss = 0.02520220
Iteration 41, loss = 0.02471465
Iteration 42, loss = 0.02473249
Iteration 43, loss = 0.02441982
Iteration 44, loss = 0.02432284
Iteration 45, loss = 0.02402288
Iteration 46, loss = 0.02413168
Iteration 47, loss = 0.02414698
Iteration 48, loss = 0.02384959
Iteration 49, loss = 0.02362846
Iteration 50, loss = 0.02372575
Iteration 51, loss = 0.02351953
Iteration 52, loss = 0.02352066
Iteration 53, loss = 0.02342970
Iteration 54, loss = 0.02351756
Iteration 55, loss = 0.02336713
Iteration 56, loss = 0.02303114
Iteration 57, loss = 0.02315975
Iteration 58, loss = 0.02303396
Iteration 59, loss = 0.02273661
Iteration 60, loss = 0.02291933
Iteration 61, loss = 0.02293805
Iteration 62, loss = 0.02290398
Iteration 63, loss = 0.02281968
Iteration 64, loss = 0.02275559
Iteration 65, loss = 0.02264469
Iteration 66, loss = 0.02294594
Iteration 67, loss = 0.02280739
Iteration 68, loss = 0.02275672
Iteration 69, loss = 0.02264273
Iteration 70, loss = 0.02271499
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68441217
Iteration 2, loss = 0.61570533
Iteration 3, loss = 0.51047754
Iteration 4, loss = 0.39772273
Iteration 5, loss = 0.30042581
Iteration 6, loss = 0.22603164
Iteration 7, loss = 0.17314156
Iteration 8, loss = 0.13670285
Iteration 9, loss = 0.11141591
Iteration 10, loss = 0.09344384
Iteration 11, loss = 0.08034027
Iteration 12, loss = 0.07010822
Iteration 13, loss = 0.06272036
Iteration 14, loss = 0.05699403
Iteration 15, loss = 0.05231077
Iteration 16, loss = 0.04817383
Iteration 17, loss = 0.04533950
Iteration 18, loss = 0.04261545
Iteration 19, loss = 0.04057774
Iteration 20, loss = 0.03900289
Iteration 21, loss = 0.03721754
Iteration 22, loss = 0.03575939
Iteration 23, loss = 0.03460635
Iteration 24, loss = 0.03348998
Iteration 25, loss = 0.03259995
Iteration 26, loss = 0.03172561
Iteration 27, loss = 0.03110218
Iteration 28, loss = 0.03054906
Iteration 29, loss = 0.03005285
Iteration 30, loss = 0.02954616
Iteration 31, loss = 0.02918887
Iteration 32, loss = 0.02864198
Iteration 33, loss = 0.02822768
Iteration 34, loss = 0.02794751
Iteration 35, loss = 0.02747889
Iteration 36, loss = 0.02727879
Iteration 37, loss = 0.02698339
Iteration 38, loss = 0.02693098
Iteration 39, loss = 0.02665579
Iteration 40, loss = 0.02658516
Iteration 41, loss = 0.02626905
Iteration 42, loss = 0.02620335
Iteration 43, loss = 0.02603712
Iteration 44, loss = 0.02588494
Iteration 45, loss = 0.02575265
Iteration 46, loss = 0.02528053
Iteration 47, loss = 0.02528148
Iteration 48, loss = 0.02535665
Iteration 49, loss = 0.02511594
Iteration 50, loss = 0.02525254
Iteration 51, loss = 0.02490748
Iteration 52, loss = 0.02503837
Iteration 53, loss = 0.02483557
Iteration 54, loss = 0.02468256
Iteration 55, loss = 0.02465114
Iteration 56, loss = 0.02471535
Iteration 57, loss = 0.02448523
Iteration 58, loss = 0.02436779
Iteration 59, loss = 0.02434361
Iteration 60, loss = 0.02431282
Iteration 61, loss = 0.02442215
Iteration 62, loss = 0.02437556
Iteration 63, loss = 0.02421349
Iteration 64, loss = 0.02413981
Iteration 65, loss = 0.02426928
Iteration 66, loss = 0.02433246
Iteration 67, loss = 0.02439272
Iteration 68, loss = 0.02406937
Iteration 69, loss = 0.02417778
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67313730
Iteration 2, loss = 0.59670972
Iteration 3, loss = 0.48178335
Iteration 4, loss = 0.36976908
Iteration 5, loss = 0.27807960
Iteration 6, loss = 0.20925907
Iteration 7, loss = 0.16140690
Iteration 8, loss = 0.12798964
Iteration 9, loss = 0.10470692
Iteration 10, loss = 0.08801087
Iteration 11, loss = 0.07540462
Iteration 12, loss = 0.06599624
Iteration 13, loss = 0.05881480
Iteration 14, loss = 0.05332410
Iteration 15, loss = 0.04860617
Iteration 16, loss = 0.04495087
Iteration 17, loss = 0.04173476
Iteration 18, loss = 0.03929108
Iteration 19, loss = 0.03717488
Iteration 20, loss = 0.03548151
Iteration 21, loss = 0.03377590
Iteration 22, loss = 0.03251916
Iteration 23, loss = 0.03117936
Iteration 24, loss = 0.03010461
Iteration 25, loss = 0.02963473
Iteration 26, loss = 0.02866065
Iteration 27, loss = 0.02804414
Iteration 28, loss = 0.02717483
Iteration 29, loss = 0.02701625
Iteration 30, loss = 0.02645403
Iteration 31, loss = 0.02590050
Iteration 32, loss = 0.02555141
Iteration 33, loss = 0.02502619
Iteration 34, loss = 0.02490645
Iteration 35, loss = 0.02433200
Iteration 36, loss = 0.02439241
Iteration 37, loss = 0.02381115
Iteration 38, loss = 0.02371976
Iteration 39, loss = 0.02329075
Iteration 40, loss = 0.02333734
Iteration 41, loss = 0.02323940
Iteration 42, loss = 0.02275514
Iteration 43, loss = 0.02269963
Iteration 44, loss = 0.02260522
Iteration 45, loss = 0.02267483
Iteration 46, loss = 0.02254808
Iteration 47, loss = 0.02212738
Iteration 48, loss = 0.02228085
Iteration 49, loss = 0.02184016
Iteration 50, loss = 0.02195305
Iteration 51, loss = 0.02185229
Iteration 52, loss = 0.02170162
Iteration 53, loss = 0.02148481
Iteration 54, loss = 0.02130076
Iteration 55, loss = 0.02150192
Iteration 56, loss = 0.02164995
Iteration 57, loss = 0.02111634
Iteration 58, loss = 0.02148512
Iteration 59, loss = 0.02145567
Iteration 60, loss = 0.02129991
Iteration 61, loss = 0.02105939
Iteration 62, loss = 0.02094658
Iteration 63, loss = 0.02104241
Iteration 64, loss = 0.02070778
Iteration 65, loss = 0.02082709
Iteration 66, loss = 0.02080911
Iteration 67, loss = 0.02125705
Iteration 68, loss = 0.02057593
Iteration 69, loss = 0.02083957
Iteration 70, loss = 0.02077113
Iteration 71, loss = 0.02070515
Iteration 72, loss = 0.02074654
Iteration 73, loss = 0.02053987
Iteration 74, loss = 0.02042952
Iteration 75, loss = 0.02073708
Iteration 76, loss = 0.02042747
Iteration 77, loss = 0.02034272
Iteration 78, loss = 0.02037874
Iteration 79, loss = 0.02050645
Iteration 80, loss = 0.02043134
Iteration 81, loss = 0.02056173
Iteration 82, loss = 0.02018694
Iteration 83, loss = 0.02003050
Iteration 84, loss = 0.02012164
Iteration 85, loss = 0.02038010
Iteration 86, loss = 0.02012893
Iteration 87, loss = 0.02014447
Iteration 88, loss = 0.02011695
Iteration 89, loss = 0.02028854
Iteration 90, loss = 0.02017918
Iteration 91, loss = 0.02009745
Iteration 92, loss = 0.02006846
Iteration 93, loss = 0.02017348
Iteration 94, loss = 0.02005830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67499308
Iteration 2, loss = 0.60420324
Iteration 3, loss = 0.49625393
Iteration 4, loss = 0.38526624
Iteration 5, loss = 0.29125186
Iteration 6, loss = 0.22052796
Iteration 7, loss = 0.17013597
Iteration 8, loss = 0.13537861
Iteration 9, loss = 0.11078684
Iteration 10, loss = 0.09353007
Iteration 11, loss = 0.08048973
Iteration 12, loss = 0.07093580
Iteration 13, loss = 0.06340488
Iteration 14, loss = 0.05755283
Iteration 15, loss = 0.05270258
Iteration 16, loss = 0.04915031
Iteration 17, loss = 0.04584384
Iteration 18, loss = 0.04342406
Iteration 19, loss = 0.04146800
Iteration 20, loss = 0.03962009
Iteration 21, loss = 0.03778861
Iteration 22, loss = 0.03639491
Iteration 23, loss = 0.03550211
Iteration 24, loss = 0.03432779
Iteration 25, loss = 0.03348245
Iteration 26, loss = 0.03256978
Iteration 27, loss = 0.03183680
Iteration 28, loss = 0.03140865
Iteration 29, loss = 0.03075283
Iteration 30, loss = 0.03044403
Iteration 31, loss = 0.02972575
Iteration 32, loss = 0.02930480
Iteration 33, loss = 0.02897114
Iteration 34, loss = 0.02886235
Iteration 35, loss = 0.02829401
Iteration 36, loss = 0.02816403
Iteration 37, loss = 0.02793596
Iteration 38, loss = 0.02774669
Iteration 39, loss = 0.02785375
Iteration 40, loss = 0.02713121
Iteration 41, loss = 0.02705852
Iteration 42, loss = 0.02708384
Iteration 43, loss = 0.02648850
Iteration 44, loss = 0.02656188
Iteration 45, loss = 0.02651307
Iteration 46, loss = 0.02637137
Iteration 47, loss = 0.02611293
Iteration 48, loss = 0.02617951
Iteration 49, loss = 0.02581096
Iteration 50, loss = 0.02571244
Iteration 51, loss = 0.02566290
Iteration 52, loss = 0.02547356
Iteration 53, loss = 0.02564351
Iteration 54, loss = 0.02572187
Iteration 55, loss = 0.02537763
Iteration 56, loss = 0.02558738
Iteration 57, loss = 0.02547245
Iteration 58, loss = 0.02520345
Iteration 59, loss = 0.02539464
Iteration 60, loss = 0.02518532
Iteration 61, loss = 0.02513062
Iteration 62, loss = 0.02548728
Iteration 63, loss = 0.02507330
Iteration 64, loss = 0.02518310
Iteration 65, loss = 0.02490475
Iteration 66, loss = 0.02485946
Iteration 67, loss = 0.02477919
Iteration 68, loss = 0.02472127
Iteration 69, loss = 0.02468504
Iteration 70, loss = 0.02464980
Iteration 71, loss = 0.02477481
Iteration 72, loss = 0.02452973
Iteration 73, loss = 0.02480314
Iteration 74, loss = 0.02487015
Iteration 75, loss = 0.02445894
Iteration 76, loss = 0.02452153
Iteration 77, loss = 0.02468328
Iteration 78, loss = 0.02459598
Iteration 79, loss = 0.02460184
Iteration 80, loss = 0.02436222
Iteration 81, loss = 0.02443247
Iteration 82, loss = 0.02455502
Iteration 83, loss = 0.02433352
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68488930
Iteration 2, loss = 0.60851111
Iteration 3, loss = 0.50021480
Iteration 4, loss = 0.38709233
Iteration 5, loss = 0.29125306
Iteration 6, loss = 0.21942520
Iteration 7, loss = 0.16891154
Iteration 8, loss = 0.13398648
Iteration 9, loss = 0.10974630
Iteration 10, loss = 0.09210200
Iteration 11, loss = 0.07954409
Iteration 12, loss = 0.06977302
Iteration 13, loss = 0.06260639
Iteration 14, loss = 0.05672127
Iteration 15, loss = 0.05204096
Iteration 16, loss = 0.04839142
Iteration 17, loss = 0.04538258
Iteration 18, loss = 0.04267198
Iteration 19, loss = 0.04049985
Iteration 20, loss = 0.03888689
Iteration 21, loss = 0.03715835
Iteration 22, loss = 0.03580579
Iteration 23, loss = 0.03471830
Iteration 24, loss = 0.03362979
Iteration 25, loss = 0.03266510
Iteration 26, loss = 0.03211686
Iteration 27, loss = 0.03121891
Iteration 28, loss = 0.03076995
Iteration 29, loss = 0.02998323
Iteration 30, loss = 0.02964574
Iteration 31, loss = 0.02902963
Iteration 32, loss = 0.02897092
Iteration 33, loss = 0.02834690
Iteration 34, loss = 0.02783240
Iteration 35, loss = 0.02768258
Iteration 36, loss = 0.02731800
Iteration 37, loss = 0.02715061
Iteration 38, loss = 0.02703731
Iteration 39, loss = 0.02698745
Iteration 40, loss = 0.02640948
Iteration 41, loss = 0.02627657
Iteration 42, loss = 0.02581965
Iteration 43, loss = 0.02557370
Iteration 44, loss = 0.02566744
Iteration 45, loss = 0.02565853
Iteration 46, loss = 0.02556662
Iteration 47, loss = 0.02559632
Iteration 48, loss = 0.02574638
Iteration 49, loss = 0.02538533
Iteration 50, loss = 0.02521944
Iteration 51, loss = 0.02486090
Iteration 52, loss = 0.02501128
Iteration 53, loss = 0.02476689
Iteration 54, loss = 0.02480661
Iteration 55, loss = 0.02487142
Iteration 56, loss = 0.02480405
Iteration 57, loss = 0.02460015
Iteration 58, loss = 0.02473152
Iteration 59, loss = 0.02459221
Iteration 60, loss = 0.02444667
Iteration 61, loss = 0.02442551
Iteration 62, loss = 0.02436995
Iteration 63, loss = 0.02400145
Iteration 64, loss = 0.02422731
Iteration 65, loss = 0.02433998
Iteration 66, loss = 0.02425653
Iteration 67, loss = 0.02452620
Iteration 68, loss = 0.02433745
Iteration 69, loss = 0.02366938
Iteration 70, loss = 0.02383351
Iteration 71, loss = 0.02383369
Iteration 72, loss = 0.02406340
Iteration 73, loss = 0.02402129
Iteration 74, loss = 0.02365622
Iteration 75, loss = 0.02399407
Iteration 76, loss = 0.02401945
Iteration 77, loss = 0.02368824
Iteration 78, loss = 0.02383079
Iteration 79, loss = 0.02338110
Iteration 80, loss = 0.02388948
Iteration 81, loss = 0.02338429
Iteration 82, loss = 0.02382873
Iteration 83, loss = 0.02362596
Iteration 84, loss = 0.02369822
Iteration 85, loss = 0.02373881
Iteration 86, loss = 0.02383747
Iteration 87, loss = 0.02359905
Iteration 88, loss = 0.02380467
Iteration 89, loss = 0.02345090
Iteration 90, loss = 0.02354269
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70000179
Iteration 2, loss = 0.63338037
Iteration 3, loss = 0.52580854
Iteration 4, loss = 0.40363457
Iteration 5, loss = 0.30193666
Iteration 6, loss = 0.22519457
Iteration 7, loss = 0.17211259
Iteration 8, loss = 0.13586390
Iteration 9, loss = 0.11017752
Iteration 10, loss = 0.09232158
Iteration 11, loss = 0.07910564
Iteration 12, loss = 0.06905853
Iteration 13, loss = 0.06153188
Iteration 14, loss = 0.05565674
Iteration 15, loss = 0.05095545
Iteration 16, loss = 0.04709732
Iteration 17, loss = 0.04399660
Iteration 18, loss = 0.04163886
Iteration 19, loss = 0.03918347
Iteration 20, loss = 0.03739188
Iteration 21, loss = 0.03569609
Iteration 22, loss = 0.03420742
Iteration 23, loss = 0.03300840
Iteration 24, loss = 0.03213629
Iteration 25, loss = 0.03134916
Iteration 26, loss = 0.03044387
Iteration 27, loss = 0.02972534
Iteration 28, loss = 0.02913948
Iteration 29, loss = 0.02852886
Iteration 30, loss = 0.02823977
Iteration 31, loss = 0.02754530
Iteration 32, loss = 0.02713783
Iteration 33, loss = 0.02668894
Iteration 34, loss = 0.02638868
Iteration 35, loss = 0.02603483
Iteration 36, loss = 0.02606379
Iteration 37, loss = 0.02569352
Iteration 38, loss = 0.02527511
Iteration 39, loss = 0.02522626
Iteration 40, loss = 0.02520220
Iteration 41, loss = 0.02471465
Iteration 42, loss = 0.02473249
Iteration 43, loss = 0.02441982
Iteration 44, loss = 0.02432284
Iteration 45, loss = 0.02402288
Iteration 46, loss = 0.02413168
Iteration 47, loss = 0.02414698
Iteration 48, loss = 0.02384959
Iteration 49, loss = 0.02362846
Iteration 50, loss = 0.02372575
Iteration 51, loss = 0.02351953
Iteration 52, loss = 0.02352066
Iteration 53, loss = 0.02342970
Iteration 54, loss = 0.02351756
Iteration 55, loss = 0.02336713
Iteration 56, loss = 0.02303114
Iteration 57, loss = 0.02315975
Iteration 58, loss = 0.02303396
Iteration 59, loss = 0.02273661
Iteration 60, loss = 0.02291933
Iteration 61, loss = 0.02293805
Iteration 62, loss = 0.02290398
Iteration 63, loss = 0.02281968
Iteration 64, loss = 0.02275559
Iteration 65, loss = 0.02264469
Iteration 66, loss = 0.02294594
Iteration 67, loss = 0.02280739
Iteration 68, loss = 0.02275672
Iteration 69, loss = 0.02264273
Iteration 70, loss = 0.02271499
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68441217
Iteration 2, loss = 0.61570533
Iteration 3, loss = 0.51047754
Iteration 4, loss = 0.39772273
Iteration 5, loss = 0.30042581
Iteration 6, loss = 0.22603164
Iteration 7, loss = 0.17314156
Iteration 8, loss = 0.13670285
Iteration 9, loss = 0.11141591
Iteration 10, loss = 0.09344384
Iteration 11, loss = 0.08034027
Iteration 12, loss = 0.07010822
Iteration 13, loss = 0.06272036
Iteration 14, loss = 0.05699403
Iteration 15, loss = 0.05231077
Iteration 16, loss = 0.04817383
Iteration 17, loss = 0.04533950
Iteration 18, loss = 0.04261545
Iteration 19, loss = 0.04057774
Iteration 20, loss = 0.03900289
Iteration 21, loss = 0.03721754
Iteration 22, loss = 0.03575939
Iteration 23, loss = 0.03460635
Iteration 24, loss = 0.03348998
Iteration 25, loss = 0.03259995
Iteration 26, loss = 0.03172561
Iteration 27, loss = 0.03110218
Iteration 28, loss = 0.03054906
Iteration 29, loss = 0.03005285
Iteration 30, loss = 0.02954616
Iteration 31, loss = 0.02918887
Iteration 32, loss = 0.02864198
Iteration 33, loss = 0.02822768
Iteration 34, loss = 0.02794751
Iteration 35, loss = 0.02747889
Iteration 36, loss = 0.02727879
Iteration 37, loss = 0.02698339
Iteration 38, loss = 0.02693098
Iteration 39, loss = 0.02665579
Iteration 40, loss = 0.02658516
Iteration 41, loss = 0.02626905
Iteration 42, loss = 0.02620335
Iteration 43, loss = 0.02603712
Iteration 44, loss = 0.02588494
Iteration 45, loss = 0.02575265
Iteration 46, loss = 0.02528053
Iteration 47, loss = 0.02528148
Iteration 48, loss = 0.02535665
Iteration 49, loss = 0.02511594
Iteration 50, loss = 0.02525254
Iteration 51, loss = 0.02490748
Iteration 52, loss = 0.02503837
Iteration 53, loss = 0.02483557
Iteration 54, loss = 0.02468256
Iteration 55, loss = 0.02465114
Iteration 56, loss = 0.02471535
Iteration 57, loss = 0.02448523
Iteration 58, loss = 0.02436779
Iteration 59, loss = 0.02434361
Iteration 60, loss = 0.02431282
Iteration 61, loss = 0.02442215
Iteration 62, loss = 0.02437556
Iteration 63, loss = 0.02421349
Iteration 64, loss = 0.02413981
Iteration 65, loss = 0.02426928
Iteration 66, loss = 0.02433246
Iteration 67, loss = 0.02439272
Iteration 68, loss = 0.02406937
Iteration 69, loss = 0.02417778
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67313730
Iteration 2, loss = 0.59670972
Iteration 3, loss = 0.48178335
Iteration 4, loss = 0.36976908
Iteration 5, loss = 0.27807960
Iteration 6, loss = 0.20925907
Iteration 7, loss = 0.16140690
Iteration 8, loss = 0.12798964
Iteration 9, loss = 0.10470692
Iteration 10, loss = 0.08801087
Iteration 11, loss = 0.07540462
Iteration 12, loss = 0.06599624
Iteration 13, loss = 0.05881480
Iteration 14, loss = 0.05332410
Iteration 15, loss = 0.04860617
Iteration 16, loss = 0.04495087
Iteration 17, loss = 0.04173476
Iteration 18, loss = 0.03929108
Iteration 19, loss = 0.03717488
Iteration 20, loss = 0.03548151
Iteration 21, loss = 0.03377590
Iteration 22, loss = 0.03251916
Iteration 23, loss = 0.03117936
Iteration 24, loss = 0.03010461
Iteration 25, loss = 0.02963473
Iteration 26, loss = 0.02866065
Iteration 27, loss = 0.02804414
Iteration 28, loss = 0.02717483
Iteration 29, loss = 0.02701625
Iteration 30, loss = 0.02645403
Iteration 31, loss = 0.02590050
Iteration 32, loss = 0.02555141
Iteration 33, loss = 0.02502619
Iteration 34, loss = 0.02490645
Iteration 35, loss = 0.02433200
Iteration 36, loss = 0.02439241
Iteration 37, loss = 0.02381115
Iteration 38, loss = 0.02371976
Iteration 39, loss = 0.02329075
Iteration 40, loss = 0.02333734
Iteration 41, loss = 0.02323940
Iteration 42, loss = 0.02275514
Iteration 43, loss = 0.02269963
Iteration 44, loss = 0.02260522
Iteration 45, loss = 0.02267483
Iteration 46, loss = 0.02254808
Iteration 47, loss = 0.02212738
Iteration 48, loss = 0.02228085
Iteration 49, loss = 0.02184016
Iteration 50, loss = 0.02195305
Iteration 51, loss = 0.02185229
Iteration 52, loss = 0.02170162
Iteration 53, loss = 0.02148481
Iteration 54, loss = 0.02130076
Iteration 55, loss = 0.02150192
Iteration 56, loss = 0.02164995
Iteration 57, loss = 0.02111634
Iteration 58, loss = 0.02148512
Iteration 59, loss = 0.02145567
Iteration 60, loss = 0.02129991
Iteration 61, loss = 0.02105939
Iteration 62, loss = 0.02094658
Iteration 63, loss = 0.02104241
Iteration 64, loss = 0.02070778
Iteration 65, loss = 0.02082709
Iteration 66, loss = 0.02080911
Iteration 67, loss = 0.02125705
Iteration 68, loss = 0.02057593
Iteration 69, loss = 0.02083957
Iteration 70, loss = 0.02077113
Iteration 71, loss = 0.02070515
Iteration 72, loss = 0.02074654
Iteration 73, loss = 0.02053987
Iteration 74, loss = 0.02042952
Iteration 75, loss = 0.02073708
Iteration 76, loss = 0.02042747
Iteration 77, loss = 0.02034272
Iteration 78, loss = 0.02037874
Iteration 79, loss = 0.02050645
Iteration 80, loss = 0.02043134
Iteration 81, loss = 0.02056173
Iteration 82, loss = 0.02018694
Iteration 83, loss = 0.02003050
Iteration 84, loss = 0.02012164
Iteration 85, loss = 0.02038010
Iteration 86, loss = 0.02012893
Iteration 87, loss = 0.02014447
Iteration 88, loss = 0.02011695
Iteration 89, loss = 0.02028854
Iteration 90, loss = 0.02017918
Iteration 91, loss = 0.02009745
Iteration 92, loss = 0.02006846
Iteration 93, loss = 0.02017348
Iteration 94, loss = 0.02005830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67499308
Iteration 2, loss = 0.60420324
Iteration 3, loss = 0.49625393
Iteration 4, loss = 0.38526624
Iteration 5, loss = 0.29125186
Iteration 6, loss = 0.22052796
Iteration 7, loss = 0.17013597
Iteration 8, loss = 0.13537861
Iteration 9, loss = 0.11078684
Iteration 10, loss = 0.09353007
Iteration 11, loss = 0.08048973
Iteration 12, loss = 0.07093580
Iteration 13, loss = 0.06340488
Iteration 14, loss = 0.05755283
Iteration 15, loss = 0.05270258
Iteration 16, loss = 0.04915031
Iteration 17, loss = 0.04584384
Iteration 18, loss = 0.04342406
Iteration 19, loss = 0.04146800
Iteration 20, loss = 0.03962009
Iteration 21, loss = 0.03778861
Iteration 22, loss = 0.03639491
Iteration 23, loss = 0.03550211
Iteration 24, loss = 0.03432779
Iteration 25, loss = 0.03348245
Iteration 26, loss = 0.03256978
Iteration 27, loss = 0.03183680
Iteration 28, loss = 0.03140865
Iteration 29, loss = 0.03075283
Iteration 30, loss = 0.03044403
Iteration 31, loss = 0.02972575
Iteration 32, loss = 0.02930480
Iteration 33, loss = 0.02897114
Iteration 34, loss = 0.02886235
Iteration 35, loss = 0.02829401
Iteration 36, loss = 0.02816403
Iteration 37, loss = 0.02793596
Iteration 38, loss = 0.02774669
Iteration 39, loss = 0.02785375
Iteration 40, loss = 0.02713121
Iteration 41, loss = 0.02705852
Iteration 42, loss = 0.02708384
Iteration 43, loss = 0.02648850
Iteration 44, loss = 0.02656188
Iteration 45, loss = 0.02651307
Iteration 46, loss = 0.02637137
Iteration 47, loss = 0.02611293
Iteration 48, loss = 0.02617951
Iteration 49, loss = 0.02581096
Iteration 50, loss = 0.02571244
Iteration 51, loss = 0.02566290
Iteration 52, loss = 0.02547356
Iteration 53, loss = 0.02564351
Iteration 54, loss = 0.02572187
Iteration 55, loss = 0.02537763
Iteration 56, loss = 0.02558738
Iteration 57, loss = 0.02547245
Iteration 58, loss = 0.02520345
Iteration 59, loss = 0.02539464
Iteration 60, loss = 0.02518532
Iteration 61, loss = 0.02513062
Iteration 62, loss = 0.02548728
Iteration 63, loss = 0.02507330
Iteration 64, loss = 0.02518310
Iteration 65, loss = 0.02490475
Iteration 66, loss = 0.02485946
Iteration 67, loss = 0.02477919
Iteration 68, loss = 0.02472127
Iteration 69, loss = 0.02468504
Iteration 70, loss = 0.02464980
Iteration 71, loss = 0.02477481
Iteration 72, loss = 0.02452973
Iteration 73, loss = 0.02480314
Iteration 74, loss = 0.02487015
Iteration 75, loss = 0.02445894
Iteration 76, loss = 0.02452153
Iteration 77, loss = 0.02468328
Iteration 78, loss = 0.02459598
Iteration 79, loss = 0.02460184
Iteration 80, loss = 0.02436222
Iteration 81, loss = 0.02443247
Iteration 82, loss = 0.02455502
Iteration 83, loss = 0.02433352
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68488930
Iteration 2, loss = 0.60851111
Iteration 3, loss = 0.50021480
Iteration 4, loss = 0.38709233
Iteration 5, loss = 0.29125306
Iteration 6, loss = 0.21942520
Iteration 7, loss = 0.16891154
Iteration 8, loss = 0.13398648
Iteration 9, loss = 0.10974630
Iteration 10, loss = 0.09210200
Iteration 11, loss = 0.07954409
Iteration 12, loss = 0.06977302
Iteration 13, loss = 0.06260639
Iteration 14, loss = 0.05672127
Iteration 15, loss = 0.05204096
Iteration 16, loss = 0.04839142
Iteration 17, loss = 0.04538258
Iteration 18, loss = 0.04267198
Iteration 19, loss = 0.04049985
Iteration 20, loss = 0.03888689
Iteration 21, loss = 0.03715835
Iteration 22, loss = 0.03580579
Iteration 23, loss = 0.03471830
Iteration 24, loss = 0.03362979
Iteration 25, loss = 0.03266510
Iteration 26, loss = 0.03211686
Iteration 27, loss = 0.03121891
Iteration 28, loss = 0.03076995
Iteration 29, loss = 0.02998323
Iteration 30, loss = 0.02964574
Iteration 31, loss = 0.02902963
Iteration 32, loss = 0.02897092
Iteration 33, loss = 0.02834690
Iteration 34, loss = 0.02783240
Iteration 35, loss = 0.02768258
Iteration 36, loss = 0.02731800
Iteration 37, loss = 0.02715061
Iteration 38, loss = 0.02703731
Iteration 39, loss = 0.02698745
Iteration 40, loss = 0.02640948
Iteration 41, loss = 0.02627657
Iteration 42, loss = 0.02581965
Iteration 43, loss = 0.02557370
Iteration 44, loss = 0.02566744
Iteration 45, loss = 0.02565853
Iteration 46, loss = 0.02556662
Iteration 47, loss = 0.02559632
Iteration 48, loss = 0.02574638
Iteration 49, loss = 0.02538533
Iteration 50, loss = 0.02521944
Iteration 51, loss = 0.02486090
Iteration 52, loss = 0.02501128
Iteration 53, loss = 0.02476689
Iteration 54, loss = 0.02480661
Iteration 55, loss = 0.02487142
Iteration 56, loss = 0.02480405
Iteration 57, loss = 0.02460015
Iteration 58, loss = 0.02473152
Iteration 59, loss = 0.02459221
Iteration 60, loss = 0.02444667
Iteration 61, loss = 0.02442551
Iteration 62, loss = 0.02436995
Iteration 63, loss = 0.02400145
Iteration 64, loss = 0.02422731
Iteration 65, loss = 0.02433998
Iteration 66, loss = 0.02425653
Iteration 67, loss = 0.02452620
Iteration 68, loss = 0.02433745
Iteration 69, loss = 0.02366938
Iteration 70, loss = 0.02383351
Iteration 71, loss = 0.02383369
Iteration 72, loss = 0.02406340
Iteration 73, loss = 0.02402129
Iteration 74, loss = 0.02365622
Iteration 75, loss = 0.02399407
Iteration 76, loss = 0.02401945
Iteration 77, loss = 0.02368824
Iteration 78, loss = 0.02383079
Iteration 79, loss = 0.02338110
Iteration 80, loss = 0.02388948
Iteration 81, loss = 0.02338429
Iteration 82, loss = 0.02382873
Iteration 83, loss = 0.02362596
Iteration 84, loss = 0.02369822
Iteration 85, loss = 0.02373881
Iteration 86, loss = 0.02383747
Iteration 87, loss = 0.02359905
Iteration 88, loss = 0.02380467
Iteration 89, loss = 0.02345090
Iteration 90, loss = 0.02354269
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70000179
Iteration 2, loss = 0.63338037
Iteration 3, loss = 0.52580854
Iteration 4, loss = 0.40363457
Iteration 5, loss = 0.30193666
Iteration 6, loss = 0.22519457
Iteration 7, loss = 0.17211259
Iteration 8, loss = 0.13586390
Iteration 9, loss = 0.11017752
Iteration 10, loss = 0.09232158
Iteration 11, loss = 0.07910564
Iteration 12, loss = 0.06905853
Iteration 13, loss = 0.06153188
Iteration 14, loss = 0.05565674
Iteration 15, loss = 0.05095545
Iteration 16, loss = 0.04709732
Iteration 17, loss = 0.04399660
Iteration 18, loss = 0.04163886
Iteration 19, loss = 0.03918347
Iteration 20, loss = 0.03739188
Iteration 21, loss = 0.03569609
Iteration 22, loss = 0.03420742
Iteration 23, loss = 0.03300840
Iteration 24, loss = 0.03213629
Iteration 25, loss = 0.03134916
Iteration 26, loss = 0.03044387
Iteration 27, loss = 0.02972534
Iteration 28, loss = 0.02913948
Iteration 29, loss = 0.02852886
Iteration 30, loss = 0.02823977
Iteration 31, loss = 0.02754530
Iteration 32, loss = 0.02713783
Iteration 33, loss = 0.02668894
Iteration 34, loss = 0.02638868
Iteration 35, loss = 0.02603483
Iteration 36, loss = 0.02606379
Iteration 37, loss = 0.02569352
Iteration 38, loss = 0.02527511
Iteration 39, loss = 0.02522626
Iteration 40, loss = 0.02520220
Iteration 41, loss = 0.02471465
Iteration 42, loss = 0.02473249
Iteration 43, loss = 0.02441982
Iteration 44, loss = 0.02432284
Iteration 45, loss = 0.02402288
Iteration 46, loss = 0.02413168
Iteration 47, loss = 0.02414698
Iteration 48, loss = 0.02384959
Iteration 49, loss = 0.02362846
Iteration 50, loss = 0.02372575
Iteration 51, loss = 0.02351953
Iteration 52, loss = 0.02352066
Iteration 53, loss = 0.02342970
Iteration 54, loss = 0.02351756
Iteration 55, loss = 0.02336713
Iteration 56, loss = 0.02303114
Iteration 57, loss = 0.02315975
Iteration 58, loss = 0.02303396
Iteration 59, loss = 0.02273661
Iteration 60, loss = 0.02291933
Iteration 61, loss = 0.02293805
Iteration 62, loss = 0.02290398
Iteration 63, loss = 0.02281968
Iteration 64, loss = 0.02275559
Iteration 65, loss = 0.02264469
Iteration 66, loss = 0.02294594
Iteration 67, loss = 0.02280739
Iteration 68, loss = 0.02275672
Iteration 69, loss = 0.02264273
Iteration 70, loss = 0.02271499
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67053832
Iteration 2, loss = 0.57711994
Iteration 3, loss = 0.44648907
Iteration 4, loss = 0.32596750
Iteration 5, loss = 0.23502618
Iteration 6, loss = 0.17316902
Iteration 7, loss = 0.13327160
Iteration 8, loss = 0.10640436
Iteration 9, loss = 0.08876418
Iteration 10, loss = 0.07560492
Iteration 11, loss = 0.06626292
Iteration 12, loss = 0.05942650
Iteration 13, loss = 0.05409526
Iteration 14, loss = 0.04981174
Iteration 15, loss = 0.04631868
Iteration 16, loss = 0.04360936
Iteration 17, loss = 0.04136486
Iteration 18, loss = 0.03920118
Iteration 19, loss = 0.03804896
Iteration 20, loss = 0.03689621
Iteration 21, loss = 0.03537104
Iteration 22, loss = 0.03436012
Iteration 23, loss = 0.03369781
Iteration 24, loss = 0.03294774
Iteration 25, loss = 0.03217551
Iteration 26, loss = 0.03166626
Iteration 27, loss = 0.03102708
Iteration 28, loss = 0.03067120
Iteration 29, loss = 0.03004252
Iteration 30, loss = 0.03012758
Iteration 31, loss = 0.02997887
Iteration 32, loss = 0.02943967
Iteration 33, loss = 0.02895088
Iteration 34, loss = 0.02886097
Iteration 35, loss = 0.02865592
Iteration 36, loss = 0.02840713
Iteration 37, loss = 0.02871216
Iteration 38, loss = 0.02828309
Iteration 39, loss = 0.02827405
Iteration 40, loss = 0.02776511
Iteration 41, loss = 0.02757240
Iteration 42, loss = 0.02722208
Iteration 43, loss = 0.02732475
Iteration 44, loss = 0.02738143
Iteration 45, loss = 0.02715601
Iteration 46, loss = 0.02706171
Iteration 47, loss = 0.02701696
Iteration 48, loss = 0.02669806
Iteration 49, loss = 0.02695974
Iteration 50, loss = 0.02689325
Iteration 51, loss = 0.02673451
Iteration 52, loss = 0.02693839
Iteration 53, loss = 0.02704480
Iteration 54, loss = 0.02673155
Iteration 55, loss = 0.02663873
Iteration 56, loss = 0.02668153
Iteration 57, loss = 0.02655806
Iteration 58, loss = 0.02636860
Iteration 59, loss = 0.02634406
Iteration 60, loss = 0.02639267
Iteration 61, loss = 0.02646816
Iteration 62, loss = 0.02636746
Iteration 63, loss = 0.02634246
Iteration 64, loss = 0.02616651
Iteration 65, loss = 0.02631969
Iteration 66, loss = 0.02601906
Iteration 67, loss = 0.02602344
Iteration 68, loss = 0.02615049
Iteration 69, loss = 0.02610099
Iteration 70, loss = 0.02586275
Iteration 71, loss = 0.02582412
Iteration 72, loss = 0.02566808
Iteration 73, loss = 0.02603294
Iteration 74, loss = 0.02612839
Iteration 75, loss = 0.02569092
Iteration 76, loss = 0.02572866
Iteration 77, loss = 0.02587810
Iteration 78, loss = 0.02573814
Iteration 79, loss = 0.02587442
Iteration 80, loss = 0.02573258
Iteration 81, loss = 0.02566682
Iteration 82, loss = 0.02567970
Iteration 83, loss = 0.02574788
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on dropped_v6_custom_stopwords...
Iteration 1, loss = 0.72734112
Iteration 2, loss = 0.68380226
Iteration 3, loss = 0.63108643
Iteration 4, loss = 0.56969685
Iteration 5, loss = 0.50100271
Iteration 6, loss = 0.43149584
Iteration 7, loss = 0.36575131
Iteration 8, loss = 0.30720953
Iteration 9, loss = 0.25711582
Iteration 10, loss = 0.21572462
Iteration 11, loss = 0.18222041
Iteration 12, loss = 0.15520737
Iteration 13, loss = 0.13366579
Iteration 14, loss = 0.11638142
Iteration 15, loss = 0.10253211
Iteration 16, loss = 0.09110854
Iteration 17, loss = 0.08185873
Iteration 18, loss = 0.07415518
Iteration 19, loss = 0.06769155
Iteration 20, loss = 0.06226169
Iteration 21, loss = 0.05764499
Iteration 22, loss = 0.05362203
Iteration 23, loss = 0.05029979
Iteration 24, loss = 0.04724335
Iteration 25, loss = 0.04449968
Iteration 26, loss = 0.04234873
Iteration 27, loss = 0.04017943
Iteration 28, loss = 0.03839859
Iteration 29, loss = 0.03673156
Iteration 30, loss = 0.03540069
Iteration 31, loss = 0.03395643
Iteration 32, loss = 0.03279060
Iteration 33, loss = 0.03168436
Iteration 34, loss = 0.03066256
Iteration 35, loss = 0.02985813
Iteration 36, loss = 0.02902887
Iteration 37, loss = 0.02821634
Iteration 38, loss = 0.02772148
Iteration 39, loss = 0.02695676
Iteration 40, loss = 0.02641115
Iteration 41, loss = 0.02592919
Iteration 42, loss = 0.02536048
Iteration 43, loss = 0.02497078
Iteration 44, loss = 0.02449440
Iteration 45, loss = 0.02397681
Iteration 46, loss = 0.02366815
Iteration 47, loss = 0.02321884
Iteration 48, loss = 0.02303482
Iteration 49, loss = 0.02268031
Iteration 50, loss = 0.02229443
Iteration 51, loss = 0.02214259
Iteration 52, loss = 0.02201780
Iteration 53, loss = 0.02161086
Iteration 54, loss = 0.02130297
Iteration 55, loss = 0.02120257
Iteration 56, loss = 0.02102638
Iteration 57, loss = 0.02076008
Iteration 58, loss = 0.02058242
Iteration 59, loss = 0.02043471
Iteration 60, loss = 0.02031838
Iteration 61, loss = 0.02015297
Iteration 62, loss = 0.01991259
Iteration 63, loss = 0.01982312
Iteration 64, loss = 0.01971269
Iteration 65, loss = 0.01967274
Iteration 66, loss = 0.01950571
Iteration 67, loss = 0.01944005
Iteration 68, loss = 0.01924286
Iteration 69, loss = 0.01929896
Iteration 70, loss = 0.01905135
Iteration 71, loss = 0.01892237
Iteration 72, loss = 0.01889998
Iteration 73, loss = 0.01879472
Iteration 74, loss = 0.01872140
Iteration 75, loss = 0.01861803
Iteration 76, loss = 0.01857249
Iteration 77, loss = 0.01828184
Iteration 78, loss = 0.01837583
Iteration 79, loss = 0.01824987
Iteration 80, loss = 0.01824786
Iteration 81, loss = 0.01826494
Iteration 82, loss = 0.01812820
Iteration 83, loss = 0.01825256
Iteration 84, loss = 0.01811925
Iteration 85, loss = 0.01807952
Iteration 86, loss = 0.01789244
Iteration 87, loss = 0.01795230
Iteration 88, loss = 0.01785979
Iteration 89, loss = 0.01789118
Iteration 90, loss = 0.01782637
Iteration 91, loss = 0.01762400
Iteration 92, loss = 0.01761273
Iteration 93, loss = 0.01764953
Iteration 94, loss = 0.01756042
Iteration 95, loss = 0.01747082
Iteration 96, loss = 0.01754931
Iteration 97, loss = 0.01745433
Iteration 98, loss = 0.01752121
Iteration 99, loss = 0.01728942
Iteration 100, loss = 0.01739868
Iteration 101, loss = 0.01751752
Iteration 102, loss = 0.01737841
Iteration 103, loss = 0.01742749
Iteration 104, loss = 0.01725823
Iteration 105, loss = 0.01732853
Iteration 106, loss = 0.01710976
Iteration 107, loss = 0.01724282
Iteration 108, loss = 0.01720528
Iteration 109, loss = 0.01712964
Iteration 110, loss = 0.01723473
Iteration 111, loss = 0.01700376
Iteration 112, loss = 0.01698390
Iteration 113, loss = 0.01694896
Iteration 114, loss = 0.01695278
Iteration 115, loss = 0.01695894
Iteration 116, loss = 0.01691755
Iteration 117, loss = 0.01698847
Iteration 118, loss = 0.01691078
Iteration 119, loss = 0.01693513
Iteration 120, loss = 0.01696693
Iteration 121, loss = 0.01680370
Iteration 122, loss = 0.01690550
Iteration 123, loss = 0.01693099
Iteration 124, loss = 0.01677099
Iteration 125, loss = 0.01698042
Iteration 126, loss = 0.01675238
Iteration 127, loss = 0.01678863
Iteration 128, loss = 0.01683495
Iteration 129, loss = 0.01669917
Iteration 130, loss = 0.01673586
Iteration 131, loss = 0.01676312
Iteration 132, loss = 0.01669671
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67613481
Iteration 2, loss = 0.63479663
Iteration 3, loss = 0.57389461
Iteration 4, loss = 0.49573843
Iteration 5, loss = 0.41611373
Iteration 6, loss = 0.34300654
Iteration 7, loss = 0.28051826
Iteration 8, loss = 0.22915202
Iteration 9, loss = 0.18877733
Iteration 10, loss = 0.15766071
Iteration 11, loss = 0.13352999
Iteration 12, loss = 0.11475613
Iteration 13, loss = 0.10019491
Iteration 14, loss = 0.08845085
Iteration 15, loss = 0.07917594
Iteration 16, loss = 0.07139811
Iteration 17, loss = 0.06513389
Iteration 18, loss = 0.05996773
Iteration 19, loss = 0.05560607
Iteration 20, loss = 0.05194641
Iteration 21, loss = 0.04865550
Iteration 22, loss = 0.04602362
Iteration 23, loss = 0.04363365
Iteration 24, loss = 0.04139648
Iteration 25, loss = 0.04006976
Iteration 26, loss = 0.03816558
Iteration 27, loss = 0.03675524
Iteration 28, loss = 0.03554068
Iteration 29, loss = 0.03462324
Iteration 30, loss = 0.03356432
Iteration 31, loss = 0.03257345
Iteration 32, loss = 0.03175842
Iteration 33, loss = 0.03081229
Iteration 34, loss = 0.03020653
Iteration 35, loss = 0.02939710
Iteration 36, loss = 0.02892454
Iteration 37, loss = 0.02835848
Iteration 38, loss = 0.02779679
Iteration 39, loss = 0.02761554
Iteration 40, loss = 0.02723645
Iteration 41, loss = 0.02673044
Iteration 42, loss = 0.02673025
Iteration 43, loss = 0.02614946
Iteration 44, loss = 0.02594943
Iteration 45, loss = 0.02542621
Iteration 46, loss = 0.02528777
Iteration 47, loss = 0.02486404
Iteration 48, loss = 0.02481330
Iteration 49, loss = 0.02444779
Iteration 50, loss = 0.02431930
Iteration 51, loss = 0.02406863
Iteration 52, loss = 0.02387847
Iteration 53, loss = 0.02357359
Iteration 54, loss = 0.02378743
Iteration 55, loss = 0.02354066
Iteration 56, loss = 0.02337637
Iteration 57, loss = 0.02319417
Iteration 58, loss = 0.02316873
Iteration 59, loss = 0.02298360
Iteration 60, loss = 0.02294238
Iteration 61, loss = 0.02278516
Iteration 62, loss = 0.02272904
Iteration 63, loss = 0.02252292
Iteration 64, loss = 0.02260509
Iteration 65, loss = 0.02236302
Iteration 66, loss = 0.02260135
Iteration 67, loss = 0.02228338
Iteration 68, loss = 0.02189641
Iteration 69, loss = 0.02206861
Iteration 70, loss = 0.02213270
Iteration 71, loss = 0.02206000
Iteration 72, loss = 0.02198633
Iteration 73, loss = 0.02183390
Iteration 74, loss = 0.02193485
Iteration 75, loss = 0.02188859
Iteration 76, loss = 0.02164170
Iteration 77, loss = 0.02146405
Iteration 78, loss = 0.02143906
Iteration 79, loss = 0.02154808
Iteration 80, loss = 0.02126586
Iteration 81, loss = 0.02098279
Iteration 82, loss = 0.02121348
Iteration 83, loss = 0.02121319
Iteration 84, loss = 0.02124567
Iteration 85, loss = 0.02125422
Iteration 86, loss = 0.02108262
Iteration 87, loss = 0.02116389
Iteration 88, loss = 0.02117837
Iteration 89, loss = 0.02106896
Iteration 90, loss = 0.02103825
Iteration 91, loss = 0.02093860
Iteration 92, loss = 0.02084641
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70450437
Iteration 2, loss = 0.65821211
Iteration 3, loss = 0.58512494
Iteration 4, loss = 0.48992025
Iteration 5, loss = 0.39475051
Iteration 6, loss = 0.31077684
Iteration 7, loss = 0.24364965
Iteration 8, loss = 0.19274699
Iteration 9, loss = 0.15555549
Iteration 10, loss = 0.12832556
Iteration 11, loss = 0.10814544
Iteration 12, loss = 0.09301036
Iteration 13, loss = 0.08130443
Iteration 14, loss = 0.07235027
Iteration 15, loss = 0.06497133
Iteration 16, loss = 0.05913999
Iteration 17, loss = 0.05457786
Iteration 18, loss = 0.05050925
Iteration 19, loss = 0.04720347
Iteration 20, loss = 0.04441747
Iteration 21, loss = 0.04211479
Iteration 22, loss = 0.04014183
Iteration 23, loss = 0.03837115
Iteration 24, loss = 0.03675605
Iteration 25, loss = 0.03551778
Iteration 26, loss = 0.03403360
Iteration 27, loss = 0.03327103
Iteration 28, loss = 0.03208463
Iteration 29, loss = 0.03134130
Iteration 30, loss = 0.03055942
Iteration 31, loss = 0.02988575
Iteration 32, loss = 0.02946790
Iteration 33, loss = 0.02862325
Iteration 34, loss = 0.02796271
Iteration 35, loss = 0.02783643
Iteration 36, loss = 0.02729342
Iteration 37, loss = 0.02697698
Iteration 38, loss = 0.02649214
Iteration 39, loss = 0.02628900
Iteration 40, loss = 0.02596322
Iteration 41, loss = 0.02558611
Iteration 42, loss = 0.02546738
Iteration 43, loss = 0.02520105
Iteration 44, loss = 0.02483488
Iteration 45, loss = 0.02500417
Iteration 46, loss = 0.02454983
Iteration 47, loss = 0.02448112
Iteration 48, loss = 0.02414724
Iteration 49, loss = 0.02410356
Iteration 50, loss = 0.02387069
Iteration 51, loss = 0.02370388
Iteration 52, loss = 0.02379868
Iteration 53, loss = 0.02354499
Iteration 54, loss = 0.02327807
Iteration 55, loss = 0.02329249
Iteration 56, loss = 0.02316495
Iteration 57, loss = 0.02324513
Iteration 58, loss = 0.02331396
Iteration 59, loss = 0.02277802
Iteration 60, loss = 0.02299812
Iteration 61, loss = 0.02273936
Iteration 62, loss = 0.02260126
Iteration 63, loss = 0.02273566
Iteration 64, loss = 0.02265510
Iteration 65, loss = 0.02249518
Iteration 66, loss = 0.02237558
Iteration 67, loss = 0.02232534
Iteration 68, loss = 0.02234464
Iteration 69, loss = 0.02208122
Iteration 70, loss = 0.02227481
Iteration 71, loss = 0.02208592
Iteration 72, loss = 0.02209369
Iteration 73, loss = 0.02216707
Iteration 74, loss = 0.02236477
Iteration 75, loss = 0.02188381
Iteration 76, loss = 0.02191931
Iteration 77, loss = 0.02184426
Iteration 78, loss = 0.02176502
Iteration 79, loss = 0.02193232
Iteration 80, loss = 0.02168013
Iteration 81, loss = 0.02189607
Iteration 82, loss = 0.02194568
Iteration 83, loss = 0.02171861
Iteration 84, loss = 0.02175379
Iteration 85, loss = 0.02163972
Iteration 86, loss = 0.02177073
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70880368
Iteration 2, loss = 0.68150326
Iteration 3, loss = 0.64965686
Iteration 4, loss = 0.61101455
Iteration 5, loss = 0.56698544
Iteration 6, loss = 0.51883574
Iteration 7, loss = 0.46885764
Iteration 8, loss = 0.41998372
Iteration 9, loss = 0.37368942
Iteration 10, loss = 0.33081598
Iteration 11, loss = 0.29205782
Iteration 12, loss = 0.25726532
Iteration 13, loss = 0.22707336
Iteration 14, loss = 0.20075476
Iteration 15, loss = 0.17829139
Iteration 16, loss = 0.15900651
Iteration 17, loss = 0.14239187
Iteration 18, loss = 0.12810506
Iteration 19, loss = 0.11610726
Iteration 20, loss = 0.10565669
Iteration 21, loss = 0.09649654
Iteration 22, loss = 0.08863089
Iteration 23, loss = 0.08177156
Iteration 24, loss = 0.07586318
Iteration 25, loss = 0.07065576
Iteration 26, loss = 0.06598463
Iteration 27, loss = 0.06171361
Iteration 28, loss = 0.05797173
Iteration 29, loss = 0.05472400
Iteration 30, loss = 0.05162817
Iteration 31, loss = 0.04896281
Iteration 32, loss = 0.04650236
Iteration 33, loss = 0.04441264
Iteration 34, loss = 0.04252777
Iteration 35, loss = 0.04072948
Iteration 36, loss = 0.03909933
Iteration 37, loss = 0.03776557
Iteration 38, loss = 0.03625967
Iteration 39, loss = 0.03504019
Iteration 40, loss = 0.03377592
Iteration 41, loss = 0.03267166
Iteration 42, loss = 0.03160532
Iteration 43, loss = 0.03072846
Iteration 44, loss = 0.02982099
Iteration 45, loss = 0.02897830
Iteration 46, loss = 0.02822207
Iteration 47, loss = 0.02752467
Iteration 48, loss = 0.02678515
Iteration 49, loss = 0.02611255
Iteration 50, loss = 0.02571316
Iteration 51, loss = 0.02520022
Iteration 52, loss = 0.02472511
Iteration 53, loss = 0.02416178
Iteration 54, loss = 0.02369594
Iteration 55, loss = 0.02305998
Iteration 56, loss = 0.02265282
Iteration 57, loss = 0.02223962
Iteration 58, loss = 0.02183646
Iteration 59, loss = 0.02156940
Iteration 60, loss = 0.02120977
Iteration 61, loss = 0.02092552
Iteration 62, loss = 0.02073843
Iteration 63, loss = 0.02039363
Iteration 64, loss = 0.02018414
Iteration 65, loss = 0.01980367
Iteration 66, loss = 0.01955890
Iteration 67, loss = 0.01924065
Iteration 68, loss = 0.01900906
Iteration 69, loss = 0.01878451
Iteration 70, loss = 0.01858455
Iteration 71, loss = 0.01838909
Iteration 72, loss = 0.01822479
Iteration 73, loss = 0.01826024
Iteration 74, loss = 0.01802742
Iteration 75, loss = 0.01770011
Iteration 76, loss = 0.01778778
Iteration 77, loss = 0.01773581
Iteration 78, loss = 0.01751690
Iteration 79, loss = 0.01723150
Iteration 80, loss = 0.01743970
Iteration 81, loss = 0.01725784
Iteration 82, loss = 0.01696552
Iteration 83, loss = 0.01673669
Iteration 84, loss = 0.01653016
Iteration 85, loss = 0.01663475
Iteration 86, loss = 0.01650237
Iteration 87, loss = 0.01634498
Iteration 88, loss = 0.01607833
Iteration 89, loss = 0.01590855
Iteration 90, loss = 0.01570197
Iteration 91, loss = 0.01557116
Iteration 92, loss = 0.01547861
Iteration 93, loss = 0.01541717
Iteration 94, loss = 0.01525756
Iteration 95, loss = 0.01518528
Iteration 96, loss = 0.01509334
Iteration 97, loss = 0.01502479
Iteration 98, loss = 0.01486121
Iteration 99, loss = 0.01492138
Iteration 100, loss = 0.01477820
Iteration 101, loss = 0.01493241
Iteration 102, loss = 0.01473878
Iteration 103, loss = 0.01464825
Iteration 104, loss = 0.01454920
Iteration 105, loss = 0.01447695
Iteration 106, loss = 0.01437040
Iteration 107, loss = 0.01434453
Iteration 108, loss = 0.01423664
Iteration 109, loss = 0.01421918
Iteration 110, loss = 0.01420315
Iteration 111, loss = 0.01409879
Iteration 112, loss = 0.01407602
Iteration 113, loss = 0.01399096
Iteration 114, loss = 0.01396474
Iteration 115, loss = 0.01388419
Iteration 116, loss = 0.01388054
Iteration 117, loss = 0.01376507
Iteration 118, loss = 0.01375868
Iteration 119, loss = 0.01376646
Iteration 120, loss = 0.01379797
Iteration 121, loss = 0.01381353
Iteration 122, loss = 0.01358894
Iteration 123, loss = 0.01374734
Iteration 124, loss = 0.01379549
Iteration 125, loss = 0.01368318
Iteration 126, loss = 0.01355338
Iteration 127, loss = 0.01350953
Iteration 128, loss = 0.01342399
Iteration 129, loss = 0.01325083
Iteration 130, loss = 0.01326914
Iteration 131, loss = 0.01326092
Iteration 132, loss = 0.01353563
Iteration 133, loss = 0.01327433
Iteration 134, loss = 0.01325210
Iteration 135, loss = 0.01326164
Iteration 136, loss = 0.01316736
Iteration 137, loss = 0.01314503
Iteration 138, loss = 0.01303170
Iteration 139, loss = 0.01298317
Iteration 140, loss = 0.01291264
Iteration 141, loss = 0.01298922
Iteration 142, loss = 0.01300145
Iteration 143, loss = 0.01312926
Iteration 144, loss = 0.01305503
Iteration 145, loss = 0.01301612
Iteration 146, loss = 0.01288466
Iteration 147, loss = 0.01282065
Iteration 148, loss = 0.01303300
Iteration 149, loss = 0.01284809
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70876617
Iteration 2, loss = 0.68786418
Iteration 3, loss = 0.67034987
Iteration 4, loss = 0.65358543
Iteration 5, loss = 0.63625466
Iteration 6, loss = 0.61790920
Iteration 7, loss = 0.59824105
Iteration 8, loss = 0.57715222
Iteration 9, loss = 0.55482488
Iteration 10, loss = 0.53134597
Iteration 11, loss = 0.50709169
Iteration 12, loss = 0.48210350
Iteration 13, loss = 0.45684932
Iteration 14, loss = 0.43147186
Iteration 15, loss = 0.40644880
Iteration 16, loss = 0.38197707
Iteration 17, loss = 0.35815082
Iteration 18, loss = 0.33546326
Iteration 19, loss = 0.31372442
Iteration 20, loss = 0.29318271
Iteration 21, loss = 0.27379559
Iteration 22, loss = 0.25571374
Iteration 23, loss = 0.23878665
Iteration 24, loss = 0.22297900
Iteration 25, loss = 0.20830915
Iteration 26, loss = 0.19472003
Iteration 27, loss = 0.18214012
Iteration 28, loss = 0.17043961
Iteration 29, loss = 0.15970702
Iteration 30, loss = 0.14975531
Iteration 31, loss = 0.14054831
Iteration 32, loss = 0.13213984
Iteration 33, loss = 0.12432542
Iteration 34, loss = 0.11712003
Iteration 35, loss = 0.11047804
Iteration 36, loss = 0.10438518
Iteration 37, loss = 0.09871846
Iteration 38, loss = 0.09346249
Iteration 39, loss = 0.08866035
Iteration 40, loss = 0.08419158
Iteration 41, loss = 0.08004133
Iteration 42, loss = 0.07616388
Iteration 43, loss = 0.07255002
Iteration 44, loss = 0.06918952
Iteration 45, loss = 0.06606082
Iteration 46, loss = 0.06318083
Iteration 47, loss = 0.06045884
Iteration 48, loss = 0.05791955
Iteration 49, loss = 0.05554024
Iteration 50, loss = 0.05333699
Iteration 51, loss = 0.05123694
Iteration 52, loss = 0.04926068
Iteration 53, loss = 0.04740345
Iteration 54, loss = 0.04569884
Iteration 55, loss = 0.04406385
Iteration 56, loss = 0.04252410
Iteration 57, loss = 0.04104105
Iteration 58, loss = 0.03966221
Iteration 59, loss = 0.03838019
Iteration 60, loss = 0.03715372
Iteration 61, loss = 0.03595196
Iteration 62, loss = 0.03482935
Iteration 63, loss = 0.03378561
Iteration 64, loss = 0.03277652
Iteration 65, loss = 0.03184131
Iteration 66, loss = 0.03092063
Iteration 67, loss = 0.03007545
Iteration 68, loss = 0.02923327
Iteration 69, loss = 0.02845163
Iteration 70, loss = 0.02772722
Iteration 71, loss = 0.02707749
Iteration 72, loss = 0.02640938
Iteration 73, loss = 0.02574924
Iteration 74, loss = 0.02512174
Iteration 75, loss = 0.02453724
Iteration 76, loss = 0.02389508
Iteration 77, loss = 0.02339678
Iteration 78, loss = 0.02279592
Iteration 79, loss = 0.02231785
Iteration 80, loss = 0.02181656
Iteration 81, loss = 0.02134584
Iteration 82, loss = 0.02087135
Iteration 83, loss = 0.02045064
Iteration 84, loss = 0.02003299
Iteration 85, loss = 0.01963024
Iteration 86, loss = 0.01927676
Iteration 87, loss = 0.01887553
Iteration 88, loss = 0.01850999
Iteration 89, loss = 0.01819646
Iteration 90, loss = 0.01785004
Iteration 91, loss = 0.01752063
Iteration 92, loss = 0.01720941
Iteration 93, loss = 0.01690455
Iteration 94, loss = 0.01658988
Iteration 95, loss = 0.01626604
Iteration 96, loss = 0.01597861
Iteration 97, loss = 0.01584371
Iteration 98, loss = 0.01560209
Iteration 99, loss = 0.01534312
Iteration 100, loss = 0.01513916
Iteration 101, loss = 0.01489910
Iteration 102, loss = 0.01468704
Iteration 103, loss = 0.01444804
Iteration 104, loss = 0.01423160
Iteration 105, loss = 0.01402214
Iteration 106, loss = 0.01388294
Iteration 107, loss = 0.01360783
Iteration 108, loss = 0.01339657
Iteration 109, loss = 0.01324239
Iteration 110, loss = 0.01302908
Iteration 111, loss = 0.01286174
Iteration 112, loss = 0.01270125
Iteration 113, loss = 0.01256827
Iteration 114, loss = 0.01241370
Iteration 115, loss = 0.01226896
Iteration 116, loss = 0.01209701
Iteration 117, loss = 0.01195786
Iteration 118, loss = 0.01181726
Iteration 119, loss = 0.01167465
Iteration 120, loss = 0.01153661
Iteration 121, loss = 0.01141345
Iteration 122, loss = 0.01126734
Iteration 123, loss = 0.01114493
Iteration 124, loss = 0.01103396
Iteration 125, loss = 0.01089694
Iteration 126, loss = 0.01079369
Iteration 127, loss = 0.01067224
Iteration 128, loss = 0.01053542
Iteration 129, loss = 0.01042190
Iteration 130, loss = 0.01031519
Iteration 131, loss = 0.01020683
Iteration 132, loss = 0.01014479
Iteration 133, loss = 0.01000070
Iteration 134, loss = 0.00990394
Iteration 135, loss = 0.00981394
Iteration 136, loss = 0.00973606
Iteration 137, loss = 0.00963372
Iteration 138, loss = 0.00955935
Iteration 139, loss = 0.00946616
Iteration 140, loss = 0.00937968
Iteration 141, loss = 0.00930658
Iteration 142, loss = 0.00923728
Iteration 143, loss = 0.00915515
Iteration 144, loss = 0.00907922
Iteration 145, loss = 0.00899323
Iteration 146, loss = 0.00892726
Iteration 147, loss = 0.00884074
Iteration 148, loss = 0.00876850
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69543547
Iteration 2, loss = 0.66947240
Iteration 3, loss = 0.64052145
Iteration 4, loss = 0.60551021
Iteration 5, loss = 0.56389358
Iteration 6, loss = 0.51737046
Iteration 7, loss = 0.46820870
Iteration 8, loss = 0.41971045
Iteration 9, loss = 0.37348395
Iteration 10, loss = 0.33081383
Iteration 11, loss = 0.29232421
Iteration 12, loss = 0.25828494
Iteration 13, loss = 0.22851517
Iteration 14, loss = 0.20266174
Iteration 15, loss = 0.18045620
Iteration 16, loss = 0.16148401
Iteration 17, loss = 0.14521084
Iteration 18, loss = 0.13140054
Iteration 19, loss = 0.11927194
Iteration 20, loss = 0.10899212
Iteration 21, loss = 0.09997596
Iteration 22, loss = 0.09220463
Iteration 23, loss = 0.08522954
Iteration 24, loss = 0.07918143
Iteration 25, loss = 0.07394704
Iteration 26, loss = 0.06922114
Iteration 27, loss = 0.06521091
Iteration 28, loss = 0.06148586
Iteration 29, loss = 0.05823446
Iteration 30, loss = 0.05508461
Iteration 31, loss = 0.05238795
Iteration 32, loss = 0.04985729
Iteration 33, loss = 0.04767608
Iteration 34, loss = 0.04556318
Iteration 35, loss = 0.04378999
Iteration 36, loss = 0.04206179
Iteration 37, loss = 0.04081992
Iteration 38, loss = 0.03919265
Iteration 39, loss = 0.03785215
Iteration 40, loss = 0.03699508
Iteration 41, loss = 0.03591963
Iteration 42, loss = 0.03478340
Iteration 43, loss = 0.03377559
Iteration 44, loss = 0.03276475
Iteration 45, loss = 0.03195280
Iteration 46, loss = 0.03123339
Iteration 47, loss = 0.03048523
Iteration 48, loss = 0.02990479
Iteration 49, loss = 0.02924911
Iteration 50, loss = 0.02859917
Iteration 51, loss = 0.02803130
Iteration 52, loss = 0.02737592
Iteration 53, loss = 0.02699610
Iteration 54, loss = 0.02649693
Iteration 55, loss = 0.02605354
Iteration 56, loss = 0.02562103
Iteration 57, loss = 0.02531316
Iteration 58, loss = 0.02495972
Iteration 59, loss = 0.02469917
Iteration 60, loss = 0.02422290
Iteration 61, loss = 0.02387529
Iteration 62, loss = 0.02346359
Iteration 63, loss = 0.02316303
Iteration 64, loss = 0.02303361
Iteration 65, loss = 0.02258694
Iteration 66, loss = 0.02222795
Iteration 67, loss = 0.02229259
Iteration 68, loss = 0.02207421
Iteration 69, loss = 0.02204018
Iteration 70, loss = 0.02193450
Iteration 71, loss = 0.02164567
Iteration 72, loss = 0.02125535
Iteration 73, loss = 0.02108777
Iteration 74, loss = 0.02079267
Iteration 75, loss = 0.02057730
Iteration 76, loss = 0.02033543
Iteration 77, loss = 0.02023046
Iteration 78, loss = 0.02019060
Iteration 79, loss = 0.01998829
Iteration 80, loss = 0.01996705
Iteration 81, loss = 0.01978170
Iteration 82, loss = 0.01958827
Iteration 83, loss = 0.01945285
Iteration 84, loss = 0.01953774
Iteration 85, loss = 0.01933938
Iteration 86, loss = 0.01931238
Iteration 87, loss = 0.01918266
Iteration 88, loss = 0.01913260
Iteration 89, loss = 0.01900881
Iteration 90, loss = 0.01884342
Iteration 91, loss = 0.01877866
Iteration 92, loss = 0.01875987
Iteration 93, loss = 0.01865304
Iteration 94, loss = 0.01855873
Iteration 95, loss = 0.01843470
Iteration 96, loss = 0.01821225
Iteration 97, loss = 0.01832203
Iteration 98, loss = 0.01842079
Iteration 99, loss = 0.01832354
Iteration 100, loss = 0.01774287
Iteration 101, loss = 0.01796169
Iteration 102, loss = 0.01810173
Iteration 103, loss = 0.01792240
Iteration 104, loss = 0.01787050
Iteration 105, loss = 0.01783169
Iteration 106, loss = 0.01772268
Iteration 107, loss = 0.01757334
Iteration 108, loss = 0.01739833
Iteration 109, loss = 0.01726510
Iteration 110, loss = 0.01723942
Iteration 111, loss = 0.01726626
Iteration 112, loss = 0.01694646
Iteration 113, loss = 0.01703969
Iteration 114, loss = 0.01702056
Iteration 115, loss = 0.01702674
Iteration 116, loss = 0.01694724
Iteration 117, loss = 0.01667050
Iteration 118, loss = 0.01703450
Iteration 119, loss = 0.01699909
Iteration 120, loss = 0.01703553
Iteration 121, loss = 0.01706878
Iteration 122, loss = 0.01698367
Iteration 123, loss = 0.01699551
Iteration 124, loss = 0.01680438
Iteration 125, loss = 0.01695900
Iteration 126, loss = 0.01684601
Iteration 127, loss = 0.01681523
Iteration 128, loss = 0.01676004
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70348684
Iteration 2, loss = 0.68214014
Iteration 3, loss = 0.66169808
Iteration 4, loss = 0.63848358
Iteration 5, loss = 0.61188671
Iteration 6, loss = 0.58245968
Iteration 7, loss = 0.54989655
Iteration 8, loss = 0.51520808
Iteration 9, loss = 0.47947677
Iteration 10, loss = 0.44310798
Iteration 11, loss = 0.40726742
Iteration 12, loss = 0.37275123
Iteration 13, loss = 0.33989806
Iteration 14, loss = 0.30920213
Iteration 15, loss = 0.28064629
Iteration 16, loss = 0.25450171
Iteration 17, loss = 0.23065378
Iteration 18, loss = 0.20913397
Iteration 19, loss = 0.18972629
Iteration 20, loss = 0.17242993
Iteration 21, loss = 0.15692512
Iteration 22, loss = 0.14312338
Iteration 23, loss = 0.13091052
Iteration 24, loss = 0.12007647
Iteration 25, loss = 0.11039112
Iteration 26, loss = 0.10168032
Iteration 27, loss = 0.09405830
Iteration 28, loss = 0.08718087
Iteration 29, loss = 0.08108550
Iteration 30, loss = 0.07548438
Iteration 31, loss = 0.07055814
Iteration 32, loss = 0.06611492
Iteration 33, loss = 0.06203852
Iteration 34, loss = 0.05841472
Iteration 35, loss = 0.05503443
Iteration 36, loss = 0.05196674
Iteration 37, loss = 0.04920931
Iteration 38, loss = 0.04665697
Iteration 39, loss = 0.04433358
Iteration 40, loss = 0.04215034
Iteration 41, loss = 0.04024014
Iteration 42, loss = 0.03837104
Iteration 43, loss = 0.03666650
Iteration 44, loss = 0.03509136
Iteration 45, loss = 0.03364649
Iteration 46, loss = 0.03228410
Iteration 47, loss = 0.03099871
Iteration 48, loss = 0.02985557
Iteration 49, loss = 0.02870528
Iteration 50, loss = 0.02767303
Iteration 51, loss = 0.02670904
Iteration 52, loss = 0.02583681
Iteration 53, loss = 0.02496079
Iteration 54, loss = 0.02414418
Iteration 55, loss = 0.02341729
Iteration 56, loss = 0.02267959
Iteration 57, loss = 0.02201662
Iteration 58, loss = 0.02138771
Iteration 59, loss = 0.02073845
Iteration 60, loss = 0.02019129
Iteration 61, loss = 0.01967457
Iteration 62, loss = 0.01914719
Iteration 63, loss = 0.01865599
Iteration 64, loss = 0.01815720
Iteration 65, loss = 0.01770992
Iteration 66, loss = 0.01729740
Iteration 67, loss = 0.01690691
Iteration 68, loss = 0.01651593
Iteration 69, loss = 0.01616494
Iteration 70, loss = 0.01584046
Iteration 71, loss = 0.01550533
Iteration 72, loss = 0.01514039
Iteration 73, loss = 0.01483673
Iteration 74, loss = 0.01456275
Iteration 75, loss = 0.01428832
Iteration 76, loss = 0.01400678
Iteration 77, loss = 0.01374451
Iteration 78, loss = 0.01351845
Iteration 79, loss = 0.01324920
Iteration 80, loss = 0.01303661
Iteration 81, loss = 0.01283581
Iteration 82, loss = 0.01261186
Iteration 83, loss = 0.01239150
Iteration 84, loss = 0.01223694
Iteration 85, loss = 0.01202905
Iteration 86, loss = 0.01182959
Iteration 87, loss = 0.01165081
Iteration 88, loss = 0.01149738
Iteration 89, loss = 0.01133765
Iteration 90, loss = 0.01116767
Iteration 91, loss = 0.01105963
Iteration 92, loss = 0.01089389
Iteration 93, loss = 0.01074722
Iteration 94, loss = 0.01058929
Iteration 95, loss = 0.01045782
Iteration 96, loss = 0.01033995
Iteration 97, loss = 0.01022472
Iteration 98, loss = 0.01013284
Iteration 99, loss = 0.01002634
Iteration 100, loss = 0.00988576
Iteration 101, loss = 0.00976827
Iteration 102, loss = 0.00964337
Iteration 103, loss = 0.00954387
Iteration 104, loss = 0.00944614
Iteration 105, loss = 0.00934790
Iteration 106, loss = 0.00929470
Iteration 107, loss = 0.00917175
Iteration 108, loss = 0.00909020
Iteration 109, loss = 0.00896704
Iteration 110, loss = 0.00892160
Iteration 111, loss = 0.00881680
Iteration 112, loss = 0.00874556
Iteration 113, loss = 0.00865858
Iteration 114, loss = 0.00858680
Iteration 115, loss = 0.00851347
Iteration 116, loss = 0.00850360
Iteration 117, loss = 0.00838784
Iteration 118, loss = 0.00833290
Iteration 119, loss = 0.00825096
Iteration 120, loss = 0.00818090
Iteration 121, loss = 0.00812482
Iteration 122, loss = 0.00803954
Iteration 123, loss = 0.00798636
Iteration 124, loss = 0.00794779
Iteration 125, loss = 0.00789668
Iteration 126, loss = 0.00784854
Iteration 127, loss = 0.00777315
Iteration 128, loss = 0.00773545
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67647886
Iteration 2, loss = 0.63922584
Iteration 3, loss = 0.58822189
Iteration 4, loss = 0.52508331
Iteration 5, loss = 0.45840073
Iteration 6, loss = 0.39420421
Iteration 7, loss = 0.33478411
Iteration 8, loss = 0.28227047
Iteration 9, loss = 0.23778557
Iteration 10, loss = 0.20078201
Iteration 11, loss = 0.17066466
Iteration 12, loss = 0.14638446
Iteration 13, loss = 0.12681563
Iteration 14, loss = 0.11105376
Iteration 15, loss = 0.09823626
Iteration 16, loss = 0.08775290
Iteration 17, loss = 0.07913257
Iteration 18, loss = 0.07188703
Iteration 19, loss = 0.06599118
Iteration 20, loss = 0.06091089
Iteration 21, loss = 0.05651710
Iteration 22, loss = 0.05271297
Iteration 23, loss = 0.04956768
Iteration 24, loss = 0.04696190
Iteration 25, loss = 0.04451919
Iteration 26, loss = 0.04211060
Iteration 27, loss = 0.04027307
Iteration 28, loss = 0.03848764
Iteration 29, loss = 0.03717587
Iteration 30, loss = 0.03563973
Iteration 31, loss = 0.03450579
Iteration 32, loss = 0.03332133
Iteration 33, loss = 0.03222717
Iteration 34, loss = 0.03138199
Iteration 35, loss = 0.03054058
Iteration 36, loss = 0.02991261
Iteration 37, loss = 0.02921914
Iteration 38, loss = 0.02856972
Iteration 39, loss = 0.02802731
Iteration 40, loss = 0.02744540
Iteration 41, loss = 0.02674568
Iteration 42, loss = 0.02628816
Iteration 43, loss = 0.02580616
Iteration 44, loss = 0.02556493
Iteration 45, loss = 0.02517905
Iteration 46, loss = 0.02472062
Iteration 47, loss = 0.02443634
Iteration 48, loss = 0.02415480
Iteration 49, loss = 0.02385968
Iteration 50, loss = 0.02361736
Iteration 51, loss = 0.02339609
Iteration 52, loss = 0.02319945
Iteration 53, loss = 0.02298924
Iteration 54, loss = 0.02272611
Iteration 55, loss = 0.02245801
Iteration 56, loss = 0.02231149
Iteration 57, loss = 0.02211360
Iteration 58, loss = 0.02210435
Iteration 59, loss = 0.02184856
Iteration 60, loss = 0.02179275
Iteration 61, loss = 0.02152300
Iteration 62, loss = 0.02150384
Iteration 63, loss = 0.02132742
Iteration 64, loss = 0.02103642
Iteration 65, loss = 0.02095478
Iteration 66, loss = 0.02088914
Iteration 67, loss = 0.02109176
Iteration 68, loss = 0.02076819
Iteration 69, loss = 0.02066488
Iteration 70, loss = 0.02057245
Iteration 71, loss = 0.02048945
Iteration 72, loss = 0.02024813
Iteration 73, loss = 0.02030139
Iteration 74, loss = 0.02021795
Iteration 75, loss = 0.02012371
Iteration 76, loss = 0.02004030
Iteration 77, loss = 0.02005415
Iteration 78, loss = 0.01990991
Iteration 79, loss = 0.01984875
Iteration 80, loss = 0.01975792
Iteration 81, loss = 0.01992878
Iteration 82, loss = 0.01972697
Iteration 83, loss = 0.01969999
Iteration 84, loss = 0.01959103
Iteration 85, loss = 0.01963345
Iteration 86, loss = 0.01960416
Iteration 87, loss = 0.01934375
Iteration 88, loss = 0.01948658
Iteration 89, loss = 0.01939153
Iteration 90, loss = 0.01933379
Iteration 91, loss = 0.01934888
Iteration 92, loss = 0.01912104
Iteration 93, loss = 0.01929911
Iteration 94, loss = 0.01921777
Iteration 95, loss = 0.01904050
Iteration 96, loss = 0.01923515
Iteration 97, loss = 0.01921072
Iteration 98, loss = 0.01905526
Iteration 99, loss = 0.01897738
Iteration 100, loss = 0.01903703
Iteration 101, loss = 0.01904920
Iteration 102, loss = 0.01919231
Iteration 103, loss = 0.01900338
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72423729
Iteration 2, loss = 0.69413702
Iteration 3, loss = 0.65944114
Iteration 4, loss = 0.61640536
Iteration 5, loss = 0.56856706
Iteration 6, loss = 0.51692436
Iteration 7, loss = 0.46509952
Iteration 8, loss = 0.41440215
Iteration 9, loss = 0.36723148
Iteration 10, loss = 0.32393177
Iteration 11, loss = 0.28519286
Iteration 12, loss = 0.25144169
Iteration 13, loss = 0.22204784
Iteration 14, loss = 0.19679261
Iteration 15, loss = 0.17519537
Iteration 16, loss = 0.15661297
Iteration 17, loss = 0.14074510
Iteration 18, loss = 0.12723499
Iteration 19, loss = 0.11567760
Iteration 20, loss = 0.10574482
Iteration 21, loss = 0.09709039
Iteration 22, loss = 0.08970826
Iteration 23, loss = 0.08317654
Iteration 24, loss = 0.07739298
Iteration 25, loss = 0.07240891
Iteration 26, loss = 0.06802680
Iteration 27, loss = 0.06398045
Iteration 28, loss = 0.06033999
Iteration 29, loss = 0.05729574
Iteration 30, loss = 0.05428176
Iteration 31, loss = 0.05205217
Iteration 32, loss = 0.04972689
Iteration 33, loss = 0.04766214
Iteration 34, loss = 0.04578782
Iteration 35, loss = 0.04396253
Iteration 36, loss = 0.04241072
Iteration 37, loss = 0.04096277
Iteration 38, loss = 0.03966110
Iteration 39, loss = 0.03818004
Iteration 40, loss = 0.03699128
Iteration 41, loss = 0.03594500
Iteration 42, loss = 0.03484094
Iteration 43, loss = 0.03381851
Iteration 44, loss = 0.03296268
Iteration 45, loss = 0.03215334
Iteration 46, loss = 0.03146047
Iteration 47, loss = 0.03065188
Iteration 48, loss = 0.02999402
Iteration 49, loss = 0.02954784
Iteration 50, loss = 0.02886489
Iteration 51, loss = 0.02845083
Iteration 52, loss = 0.02795176
Iteration 53, loss = 0.02745626
Iteration 54, loss = 0.02694746
Iteration 55, loss = 0.02640903
Iteration 56, loss = 0.02601406
Iteration 57, loss = 0.02558146
Iteration 58, loss = 0.02515363
Iteration 59, loss = 0.02473142
Iteration 60, loss = 0.02464139
Iteration 61, loss = 0.02443425
Iteration 62, loss = 0.02413157
Iteration 63, loss = 0.02376012
Iteration 64, loss = 0.02335389
Iteration 65, loss = 0.02332729
Iteration 66, loss = 0.02302518
Iteration 67, loss = 0.02281927
Iteration 68, loss = 0.02257681
Iteration 69, loss = 0.02220370
Iteration 70, loss = 0.02223210
Iteration 71, loss = 0.02214426
Iteration 72, loss = 0.02196552
Iteration 73, loss = 0.02173983
Iteration 74, loss = 0.02154280
Iteration 75, loss = 0.02133633
Iteration 76, loss = 0.02117001
Iteration 77, loss = 0.02117846
Iteration 78, loss = 0.02111400
Iteration 79, loss = 0.02086002
Iteration 80, loss = 0.02095099
Iteration 81, loss = 0.02071486
Iteration 82, loss = 0.02040809
Iteration 83, loss = 0.02039491
Iteration 84, loss = 0.02019639
Iteration 85, loss = 0.01987351
Iteration 86, loss = 0.01985699
Iteration 87, loss = 0.01957347
Iteration 88, loss = 0.01971474
Iteration 89, loss = 0.01974624
Iteration 90, loss = 0.01965465
Iteration 91, loss = 0.01945990
Iteration 92, loss = 0.01925233
Iteration 93, loss = 0.01920762
Iteration 94, loss = 0.01891838
Iteration 95, loss = 0.01881605
Iteration 96, loss = 0.01873114
Iteration 97, loss = 0.01870300
Iteration 98, loss = 0.01861396
Iteration 99, loss = 0.01869723
Iteration 100, loss = 0.01860310
Iteration 101, loss = 0.01859383
Iteration 102, loss = 0.01877705
Iteration 103, loss = 0.01862203
Iteration 104, loss = 0.01844185
Iteration 105, loss = 0.01836600
Iteration 106, loss = 0.01836702
Iteration 107, loss = 0.01833352
Iteration 108, loss = 0.01817840
Iteration 109, loss = 0.01813616
Iteration 110, loss = 0.01868739
Iteration 111, loss = 0.01833110
Iteration 112, loss = 0.01824709
Iteration 113, loss = 0.01814638
Iteration 114, loss = 0.01802222
Iteration 115, loss = 0.01780465
Iteration 116, loss = 0.01777084
Iteration 117, loss = 0.01763819
Iteration 118, loss = 0.01757373
Iteration 119, loss = 0.01758614
Iteration 120, loss = 0.01752510
Iteration 121, loss = 0.01740494
Iteration 122, loss = 0.01733529
Iteration 123, loss = 0.01754590
Iteration 124, loss = 0.01739610
Iteration 125, loss = 0.01799289
Iteration 126, loss = 0.01812241
Iteration 127, loss = 0.01811237
Iteration 128, loss = 0.01777559
Iteration 129, loss = 0.01771468
Iteration 130, loss = 0.01748602
Iteration 131, loss = 0.01738699
Iteration 132, loss = 0.01752343
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67704295
Iteration 2, loss = 0.63253792
Iteration 3, loss = 0.56365108
Iteration 4, loss = 0.48068147
Iteration 5, loss = 0.39993672
Iteration 6, loss = 0.32784933
Iteration 7, loss = 0.26652639
Iteration 8, loss = 0.21742136
Iteration 9, loss = 0.17884525
Iteration 10, loss = 0.14906003
Iteration 11, loss = 0.12594447
Iteration 12, loss = 0.10823890
Iteration 13, loss = 0.09428489
Iteration 14, loss = 0.08311458
Iteration 15, loss = 0.07437812
Iteration 16, loss = 0.06699036
Iteration 17, loss = 0.06098602
Iteration 18, loss = 0.05590121
Iteration 19, loss = 0.05173780
Iteration 20, loss = 0.04820475
Iteration 21, loss = 0.04514776
Iteration 22, loss = 0.04275397
Iteration 23, loss = 0.04038185
Iteration 24, loss = 0.03845578
Iteration 25, loss = 0.03657325
Iteration 26, loss = 0.03521198
Iteration 27, loss = 0.03388179
Iteration 28, loss = 0.03250009
Iteration 29, loss = 0.03154950
Iteration 30, loss = 0.03053662
Iteration 31, loss = 0.02958455
Iteration 32, loss = 0.02870471
Iteration 33, loss = 0.02805195
Iteration 34, loss = 0.02730206
Iteration 35, loss = 0.02670610
Iteration 36, loss = 0.02630423
Iteration 37, loss = 0.02576900
Iteration 38, loss = 0.02524278
Iteration 39, loss = 0.02500912
Iteration 40, loss = 0.02469548
Iteration 41, loss = 0.02434091
Iteration 42, loss = 0.02373047
Iteration 43, loss = 0.02357014
Iteration 44, loss = 0.02320094
Iteration 45, loss = 0.02300280
Iteration 46, loss = 0.02278842
Iteration 47, loss = 0.02263303
Iteration 48, loss = 0.02240254
Iteration 49, loss = 0.02238585
Iteration 50, loss = 0.02198981
Iteration 51, loss = 0.02183953
Iteration 52, loss = 0.02144086
Iteration 53, loss = 0.02138750
Iteration 54, loss = 0.02136744
Iteration 55, loss = 0.02101400
Iteration 56, loss = 0.02099977
Iteration 57, loss = 0.02076181
Iteration 58, loss = 0.02062186
Iteration 59, loss = 0.02075736
Iteration 60, loss = 0.02058590
Iteration 61, loss = 0.02039531
Iteration 62, loss = 0.02017591
Iteration 63, loss = 0.02021743
Iteration 64, loss = 0.01999301
Iteration 65, loss = 0.01987054
Iteration 66, loss = 0.01990723
Iteration 67, loss = 0.01980067
Iteration 68, loss = 0.01985930
Iteration 69, loss = 0.01951354
Iteration 70, loss = 0.01956264
Iteration 71, loss = 0.01940419
Iteration 72, loss = 0.01943621
Iteration 73, loss = 0.01962976
Iteration 74, loss = 0.01955242
Iteration 75, loss = 0.01944291
Iteration 76, loss = 0.01939858
Iteration 77, loss = 0.01931251
Iteration 78, loss = 0.01937884
Iteration 79, loss = 0.01910671
Iteration 80, loss = 0.01899996
Iteration 81, loss = 0.01898765
Iteration 82, loss = 0.01919765
Iteration 83, loss = 0.01919221
Iteration 84, loss = 0.01894499
Iteration 85, loss = 0.01902285
Iteration 86, loss = 0.01892118
Iteration 87, loss = 0.01879009
Iteration 88, loss = 0.01876349
Iteration 89, loss = 0.01878495
Iteration 90, loss = 0.01884641
Iteration 91, loss = 0.01891981
Iteration 92, loss = 0.01883196
Iteration 93, loss = 0.01882162
Iteration 94, loss = 0.01865862
Iteration 95, loss = 0.01872175
Iteration 96, loss = 0.01860157
Iteration 97, loss = 0.01870742
Iteration 98, loss = 0.01852737
Iteration 99, loss = 0.01864147
Iteration 100, loss = 0.01858348
Iteration 101, loss = 0.01858448
Iteration 102, loss = 0.01856098
Iteration 103, loss = 0.01840513
Iteration 104, loss = 0.01854772
Iteration 105, loss = 0.01841666
Iteration 106, loss = 0.01845241
Iteration 107, loss = 0.01848841
Iteration 108, loss = 0.01823824
Iteration 109, loss = 0.01867284
Iteration 110, loss = 0.01853080
Iteration 111, loss = 0.01825668
Iteration 112, loss = 0.01823893
Iteration 113, loss = 0.01832392
Iteration 114, loss = 0.01815956
Iteration 115, loss = 0.01826149
Iteration 116, loss = 0.01825295
Iteration 117, loss = 0.01831666
Iteration 118, loss = 0.01819435
Iteration 119, loss = 0.01826767
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70228836
Iteration 2, loss = 0.66686405
Iteration 3, loss = 0.61599328
Iteration 4, loss = 0.55308561
Iteration 5, loss = 0.48362960
Iteration 6, loss = 0.41293658
Iteration 7, loss = 0.34742194
Iteration 8, loss = 0.29037088
Iteration 9, loss = 0.24212490
Iteration 10, loss = 0.20312227
Iteration 11, loss = 0.17144763
Iteration 12, loss = 0.14625424
Iteration 13, loss = 0.12637861
Iteration 14, loss = 0.11037304
Iteration 15, loss = 0.09746100
Iteration 16, loss = 0.08694719
Iteration 17, loss = 0.07839432
Iteration 18, loss = 0.07114179
Iteration 19, loss = 0.06522350
Iteration 20, loss = 0.06002572
Iteration 21, loss = 0.05579788
Iteration 22, loss = 0.05199529
Iteration 23, loss = 0.04868011
Iteration 24, loss = 0.04583817
Iteration 25, loss = 0.04346715
Iteration 26, loss = 0.04127137
Iteration 27, loss = 0.03916228
Iteration 28, loss = 0.03771349
Iteration 29, loss = 0.03604177
Iteration 30, loss = 0.03472081
Iteration 31, loss = 0.03338229
Iteration 32, loss = 0.03223663
Iteration 33, loss = 0.03121677
Iteration 34, loss = 0.03043898
Iteration 35, loss = 0.02952799
Iteration 36, loss = 0.02865840
Iteration 37, loss = 0.02797856
Iteration 38, loss = 0.02737905
Iteration 39, loss = 0.02680220
Iteration 40, loss = 0.02619665
Iteration 41, loss = 0.02568434
Iteration 42, loss = 0.02530087
Iteration 43, loss = 0.02479348
Iteration 44, loss = 0.02456311
Iteration 45, loss = 0.02398283
Iteration 46, loss = 0.02379780
Iteration 47, loss = 0.02332049
Iteration 48, loss = 0.02316909
Iteration 49, loss = 0.02290986
Iteration 50, loss = 0.02263737
Iteration 51, loss = 0.02222166
Iteration 52, loss = 0.02210325
Iteration 53, loss = 0.02171183
Iteration 54, loss = 0.02140351
Iteration 55, loss = 0.02126139
Iteration 56, loss = 0.02119616
Iteration 57, loss = 0.02094161
Iteration 58, loss = 0.02077325
Iteration 59, loss = 0.02075120
Iteration 60, loss = 0.02064865
Iteration 61, loss = 0.02044074
Iteration 62, loss = 0.02019031
Iteration 63, loss = 0.02015645
Iteration 64, loss = 0.01994833
Iteration 65, loss = 0.01982278
Iteration 66, loss = 0.01968114
Iteration 67, loss = 0.01952700
Iteration 68, loss = 0.01942290
Iteration 69, loss = 0.01934331
Iteration 70, loss = 0.01927578
Iteration 71, loss = 0.01914891
Iteration 72, loss = 0.01908548
Iteration 73, loss = 0.01902374
Iteration 74, loss = 0.01896928
Iteration 75, loss = 0.01898205
Iteration 76, loss = 0.01892970
Iteration 77, loss = 0.01876791
Iteration 78, loss = 0.01868328
Iteration 79, loss = 0.01849936
Iteration 80, loss = 0.01847309
Iteration 81, loss = 0.01863801
Iteration 82, loss = 0.01839998
Iteration 83, loss = 0.01848975
Iteration 84, loss = 0.01825355
Iteration 85, loss = 0.01834856
Iteration 86, loss = 0.01827002
Iteration 87, loss = 0.01813697
Iteration 88, loss = 0.01814691
Iteration 89, loss = 0.01807279
Iteration 90, loss = 0.01815975
Iteration 91, loss = 0.01807382
Iteration 92, loss = 0.01788018
Iteration 93, loss = 0.01797073
Iteration 94, loss = 0.01787377
Iteration 95, loss = 0.01785879
Iteration 96, loss = 0.01775853
Iteration 97, loss = 0.01777557
Iteration 98, loss = 0.01786411
Iteration 99, loss = 0.01784159
Iteration 100, loss = 0.01774977
Iteration 101, loss = 0.01784427
Iteration 102, loss = 0.01760644
Iteration 103, loss = 0.01755458
Iteration 104, loss = 0.01749338
Iteration 105, loss = 0.01757286
Iteration 106, loss = 0.01750272
Iteration 107, loss = 0.01765795
Iteration 108, loss = 0.01754683
Iteration 109, loss = 0.01729621
Iteration 110, loss = 0.01745052
Iteration 111, loss = 0.01727987
Iteration 112, loss = 0.01735401
Iteration 113, loss = 0.01746943
Iteration 114, loss = 0.01722702
Iteration 115, loss = 0.01728187
Iteration 116, loss = 0.01741072
Iteration 117, loss = 0.01741948
Iteration 118, loss = 0.01734120
Iteration 119, loss = 0.01719381
Iteration 120, loss = 0.01715019
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67990809
Iteration 2, loss = 0.64678974
Iteration 3, loss = 0.59767977
Iteration 4, loss = 0.53115591
Iteration 5, loss = 0.45829217
Iteration 6, loss = 0.38859877
Iteration 7, loss = 0.32628091
Iteration 8, loss = 0.27259947
Iteration 9, loss = 0.22822275
Iteration 10, loss = 0.19191561
Iteration 11, loss = 0.16326756
Iteration 12, loss = 0.14001321
Iteration 13, loss = 0.12174937
Iteration 14, loss = 0.10668796
Iteration 15, loss = 0.09463378
Iteration 16, loss = 0.08490565
Iteration 17, loss = 0.07670255
Iteration 18, loss = 0.06996597
Iteration 19, loss = 0.06418688
Iteration 20, loss = 0.05925110
Iteration 21, loss = 0.05510089
Iteration 22, loss = 0.05157476
Iteration 23, loss = 0.04856067
Iteration 24, loss = 0.04579138
Iteration 25, loss = 0.04341522
Iteration 26, loss = 0.04126597
Iteration 27, loss = 0.03950270
Iteration 28, loss = 0.03778782
Iteration 29, loss = 0.03630325
Iteration 30, loss = 0.03498748
Iteration 31, loss = 0.03387008
Iteration 32, loss = 0.03268104
Iteration 33, loss = 0.03167775
Iteration 34, loss = 0.03085388
Iteration 35, loss = 0.03007159
Iteration 36, loss = 0.02932302
Iteration 37, loss = 0.02844578
Iteration 38, loss = 0.02798652
Iteration 39, loss = 0.02745228
Iteration 40, loss = 0.02685238
Iteration 41, loss = 0.02629699
Iteration 42, loss = 0.02606282
Iteration 43, loss = 0.02547738
Iteration 44, loss = 0.02516635
Iteration 45, loss = 0.02466691
Iteration 46, loss = 0.02429506
Iteration 47, loss = 0.02409218
Iteration 48, loss = 0.02386837
Iteration 49, loss = 0.02350360
Iteration 50, loss = 0.02323283
Iteration 51, loss = 0.02307371
Iteration 52, loss = 0.02273904
Iteration 53, loss = 0.02260372
Iteration 54, loss = 0.02236914
Iteration 55, loss = 0.02213114
Iteration 56, loss = 0.02189869
Iteration 57, loss = 0.02166979
Iteration 58, loss = 0.02165406
Iteration 59, loss = 0.02147749
Iteration 60, loss = 0.02138124
Iteration 61, loss = 0.02113191
Iteration 62, loss = 0.02112265
Iteration 63, loss = 0.02091520
Iteration 64, loss = 0.02081067
Iteration 65, loss = 0.02059929
Iteration 66, loss = 0.02054369
Iteration 67, loss = 0.02052770
Iteration 68, loss = 0.02048537
Iteration 69, loss = 0.02029268
Iteration 70, loss = 0.02024759
Iteration 71, loss = 0.02013490
Iteration 72, loss = 0.02004253
Iteration 73, loss = 0.01998769
Iteration 74, loss = 0.02012471
Iteration 75, loss = 0.01970320
Iteration 76, loss = 0.01972084
Iteration 77, loss = 0.01963124
Iteration 78, loss = 0.01959248
Iteration 79, loss = 0.01953115
Iteration 80, loss = 0.01948099
Iteration 81, loss = 0.01940888
Iteration 82, loss = 0.01940102
Iteration 83, loss = 0.01940349
Iteration 84, loss = 0.01924469
Iteration 85, loss = 0.01918826
Iteration 86, loss = 0.01928252
Iteration 87, loss = 0.01910242
Iteration 88, loss = 0.01920223
Iteration 89, loss = 0.01918682
Iteration 90, loss = 0.01900924
Iteration 91, loss = 0.01894806
Iteration 92, loss = 0.01906372
Iteration 93, loss = 0.01882537
Iteration 94, loss = 0.01884624
Iteration 95, loss = 0.01884174
Iteration 96, loss = 0.01884047
Iteration 97, loss = 0.01877063
Iteration 98, loss = 0.01883090
Iteration 99, loss = 0.01861208
Iteration 100, loss = 0.01869971
Iteration 101, loss = 0.01859894
Iteration 102, loss = 0.01861137
Iteration 103, loss = 0.01867035
Iteration 104, loss = 0.01862087
Iteration 105, loss = 0.01859937
Iteration 106, loss = 0.01865462
Iteration 107, loss = 0.01859337
Iteration 108, loss = 0.01862672
Iteration 109, loss = 0.01840756
Iteration 110, loss = 0.01838783
Iteration 111, loss = 0.01858081
Iteration 112, loss = 0.01848784
Iteration 113, loss = 0.01862473
Iteration 114, loss = 0.01844848
Iteration 115, loss = 0.01830059
Iteration 116, loss = 0.01846244
Iteration 117, loss = 0.01828821
Iteration 118, loss = 0.01848902
Iteration 119, loss = 0.01826460
Iteration 120, loss = 0.01833117
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70221894
Iteration 2, loss = 0.66897061
Iteration 3, loss = 0.61660286
Iteration 4, loss = 0.54472293
Iteration 5, loss = 0.46628440
Iteration 6, loss = 0.38908042
Iteration 7, loss = 0.31970411
Iteration 8, loss = 0.26133388
Iteration 9, loss = 0.21379202
Iteration 10, loss = 0.17688377
Iteration 11, loss = 0.14807740
Iteration 12, loss = 0.12614778
Iteration 13, loss = 0.10889459
Iteration 14, loss = 0.09542103
Iteration 15, loss = 0.08471863
Iteration 16, loss = 0.07596634
Iteration 17, loss = 0.06875592
Iteration 18, loss = 0.06286427
Iteration 19, loss = 0.05796054
Iteration 20, loss = 0.05385320
Iteration 21, loss = 0.05030713
Iteration 22, loss = 0.04723899
Iteration 23, loss = 0.04468980
Iteration 24, loss = 0.04230004
Iteration 25, loss = 0.04047166
Iteration 26, loss = 0.03869172
Iteration 27, loss = 0.03712960
Iteration 28, loss = 0.03556881
Iteration 29, loss = 0.03439247
Iteration 30, loss = 0.03342801
Iteration 31, loss = 0.03237811
Iteration 32, loss = 0.03147902
Iteration 33, loss = 0.03053658
Iteration 34, loss = 0.02982416
Iteration 35, loss = 0.02920403
Iteration 36, loss = 0.02843526
Iteration 37, loss = 0.02800684
Iteration 38, loss = 0.02750497
Iteration 39, loss = 0.02693780
Iteration 40, loss = 0.02654501
Iteration 41, loss = 0.02591798
Iteration 42, loss = 0.02569767
Iteration 43, loss = 0.02536769
Iteration 44, loss = 0.02483255
Iteration 45, loss = 0.02456227
Iteration 46, loss = 0.02445118
Iteration 47, loss = 0.02415938
Iteration 48, loss = 0.02404289
Iteration 49, loss = 0.02370804
Iteration 50, loss = 0.02347185
Iteration 51, loss = 0.02337366
Iteration 52, loss = 0.02297679
Iteration 53, loss = 0.02267366
Iteration 54, loss = 0.02289099
Iteration 55, loss = 0.02243432
Iteration 56, loss = 0.02233760
Iteration 57, loss = 0.02220576
Iteration 58, loss = 0.02183341
Iteration 59, loss = 0.02190256
Iteration 60, loss = 0.02174306
Iteration 61, loss = 0.02165791
Iteration 62, loss = 0.02148112
Iteration 63, loss = 0.02155351
Iteration 64, loss = 0.02144162
Iteration 65, loss = 0.02130502
Iteration 66, loss = 0.02121372
Iteration 67, loss = 0.02105433
Iteration 68, loss = 0.02098721
Iteration 69, loss = 0.02093747
Iteration 70, loss = 0.02083984
Iteration 71, loss = 0.02069081
Iteration 72, loss = 0.02064891
Iteration 73, loss = 0.02052958
Iteration 74, loss = 0.02063384
Iteration 75, loss = 0.02056512
Iteration 76, loss = 0.02031400
Iteration 77, loss = 0.02045361
Iteration 78, loss = 0.02023629
Iteration 79, loss = 0.02043949
Iteration 80, loss = 0.02027706
Iteration 81, loss = 0.02020489
Iteration 82, loss = 0.02016312
Iteration 83, loss = 0.02002561
Iteration 84, loss = 0.02004428
Iteration 85, loss = 0.02024116
Iteration 86, loss = 0.02010212
Iteration 87, loss = 0.02019398
Iteration 88, loss = 0.02006053
Iteration 89, loss = 0.01997768
Iteration 90, loss = 0.01999741
Iteration 91, loss = 0.01979025
Iteration 92, loss = 0.02006205
Iteration 93, loss = 0.01961623
Iteration 94, loss = 0.01966076
Iteration 95, loss = 0.01979639
Iteration 96, loss = 0.01977916
Iteration 97, loss = 0.01959398
Iteration 98, loss = 0.01947956
Iteration 99, loss = 0.01939520
Iteration 100, loss = 0.01980708
Iteration 101, loss = 0.01961999
Iteration 102, loss = 0.01955282
Iteration 103, loss = 0.01966041
Iteration 104, loss = 0.01957350
Iteration 105, loss = 0.01931716
Iteration 106, loss = 0.01935991
Iteration 107, loss = 0.01938477
Iteration 108, loss = 0.01928072
Iteration 109, loss = 0.01933831
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70885784
Iteration 2, loss = 0.67123427
Iteration 3, loss = 0.60688647
Iteration 4, loss = 0.51549251
Iteration 5, loss = 0.41702880
Iteration 6, loss = 0.32794279
Iteration 7, loss = 0.25528119
Iteration 8, loss = 0.20034440
Iteration 9, loss = 0.16016389
Iteration 10, loss = 0.13116327
Iteration 11, loss = 0.10979460
Iteration 12, loss = 0.09420719
Iteration 13, loss = 0.08187291
Iteration 14, loss = 0.07275762
Iteration 15, loss = 0.06542122
Iteration 16, loss = 0.05948960
Iteration 17, loss = 0.05469661
Iteration 18, loss = 0.05068937
Iteration 19, loss = 0.04755368
Iteration 20, loss = 0.04440480
Iteration 21, loss = 0.04236068
Iteration 22, loss = 0.04029867
Iteration 23, loss = 0.03848804
Iteration 24, loss = 0.03701079
Iteration 25, loss = 0.03574990
Iteration 26, loss = 0.03434357
Iteration 27, loss = 0.03340939
Iteration 28, loss = 0.03246239
Iteration 29, loss = 0.03157495
Iteration 30, loss = 0.03115241
Iteration 31, loss = 0.03043791
Iteration 32, loss = 0.02982090
Iteration 33, loss = 0.02933984
Iteration 34, loss = 0.02861248
Iteration 35, loss = 0.02808906
Iteration 36, loss = 0.02770757
Iteration 37, loss = 0.02748601
Iteration 38, loss = 0.02722885
Iteration 39, loss = 0.02685377
Iteration 40, loss = 0.02627431
Iteration 41, loss = 0.02615592
Iteration 42, loss = 0.02610871
Iteration 43, loss = 0.02560259
Iteration 44, loss = 0.02570400
Iteration 45, loss = 0.02550302
Iteration 46, loss = 0.02511775
Iteration 47, loss = 0.02509931
Iteration 48, loss = 0.02474797
Iteration 49, loss = 0.02460400
Iteration 50, loss = 0.02439492
Iteration 51, loss = 0.02424068
Iteration 52, loss = 0.02442168
Iteration 53, loss = 0.02410992
Iteration 54, loss = 0.02406152
Iteration 55, loss = 0.02401036
Iteration 56, loss = 0.02369334
Iteration 57, loss = 0.02384485
Iteration 58, loss = 0.02381179
Iteration 59, loss = 0.02328880
Iteration 60, loss = 0.02365786
Iteration 61, loss = 0.02330680
Iteration 62, loss = 0.02328643
Iteration 63, loss = 0.02320916
Iteration 64, loss = 0.02326834
Iteration 65, loss = 0.02322531
Iteration 66, loss = 0.02324110
Iteration 67, loss = 0.02294401
Iteration 68, loss = 0.02302661
Iteration 69, loss = 0.02281440
Iteration 70, loss = 0.02302304
Iteration 71, loss = 0.02258796
Iteration 72, loss = 0.02281801
Iteration 73, loss = 0.02289958
Iteration 74, loss = 0.02285830
Iteration 75, loss = 0.02261179
Iteration 76, loss = 0.02260999
Iteration 77, loss = 0.02261723
Iteration 78, loss = 0.02247914
Iteration 79, loss = 0.02246641
Iteration 80, loss = 0.02248948
Iteration 81, loss = 0.02257518
Iteration 82, loss = 0.02231581
Iteration 83, loss = 0.02243096
Iteration 84, loss = 0.02261396
Iteration 85, loss = 0.02224008
Iteration 86, loss = 0.02235699
Iteration 87, loss = 0.02218687
Iteration 88, loss = 0.02247472
Iteration 89, loss = 0.02227736
Iteration 90, loss = 0.02209886
Iteration 91, loss = 0.02201812
Iteration 92, loss = 0.02230591
Iteration 93, loss = 0.02220995
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67814713
Iteration 2, loss = 0.64618733
Iteration 3, loss = 0.60457361
Iteration 4, loss = 0.55004358
Iteration 5, loss = 0.48962478
Iteration 6, loss = 0.42935134
Iteration 7, loss = 0.37283068
Iteration 8, loss = 0.32123291
Iteration 9, loss = 0.27587065
Iteration 10, loss = 0.23672582
Iteration 11, loss = 0.20362354
Iteration 12, loss = 0.17630486
Iteration 13, loss = 0.15371320
Iteration 14, loss = 0.13508508
Iteration 15, loss = 0.11953460
Iteration 16, loss = 0.10670651
Iteration 17, loss = 0.09613426
Iteration 18, loss = 0.08710253
Iteration 19, loss = 0.07949372
Iteration 20, loss = 0.07304150
Iteration 21, loss = 0.06759898
Iteration 22, loss = 0.06280747
Iteration 23, loss = 0.05881220
Iteration 24, loss = 0.05502266
Iteration 25, loss = 0.05181745
Iteration 26, loss = 0.04904441
Iteration 27, loss = 0.04653789
Iteration 28, loss = 0.04432654
Iteration 29, loss = 0.04239046
Iteration 30, loss = 0.04057549
Iteration 31, loss = 0.03901032
Iteration 32, loss = 0.03761521
Iteration 33, loss = 0.03614111
Iteration 34, loss = 0.03496470
Iteration 35, loss = 0.03391329
Iteration 36, loss = 0.03299178
Iteration 37, loss = 0.03197503
Iteration 38, loss = 0.03101645
Iteration 39, loss = 0.03032001
Iteration 40, loss = 0.02962286
Iteration 41, loss = 0.02891272
Iteration 42, loss = 0.02827227
Iteration 43, loss = 0.02769925
Iteration 44, loss = 0.02719099
Iteration 45, loss = 0.02671657
Iteration 46, loss = 0.02624932
Iteration 47, loss = 0.02589894
Iteration 48, loss = 0.02547860
Iteration 49, loss = 0.02504513
Iteration 50, loss = 0.02467892
Iteration 51, loss = 0.02430938
Iteration 52, loss = 0.02401473
Iteration 53, loss = 0.02380217
Iteration 54, loss = 0.02351253
Iteration 55, loss = 0.02326114
Iteration 56, loss = 0.02293006
Iteration 57, loss = 0.02280036
Iteration 58, loss = 0.02257223
Iteration 59, loss = 0.02220446
Iteration 60, loss = 0.02199721
Iteration 61, loss = 0.02180103
Iteration 62, loss = 0.02165109
Iteration 63, loss = 0.02139375
Iteration 64, loss = 0.02143108
Iteration 65, loss = 0.02127548
Iteration 66, loss = 0.02102019
Iteration 67, loss = 0.02077290
Iteration 68, loss = 0.02078179
Iteration 69, loss = 0.02054517
Iteration 70, loss = 0.02048213
Iteration 71, loss = 0.02027557
Iteration 72, loss = 0.02032293
Iteration 73, loss = 0.02003929
Iteration 74, loss = 0.02002104
Iteration 75, loss = 0.01989181
Iteration 76, loss = 0.01982522
Iteration 77, loss = 0.01970995
Iteration 78, loss = 0.01971105
Iteration 79, loss = 0.01966234
Iteration 80, loss = 0.01954839
Iteration 81, loss = 0.01952108
Iteration 82, loss = 0.01927713
Iteration 83, loss = 0.01935489
Iteration 84, loss = 0.01913651
Iteration 85, loss = 0.01907757
Iteration 86, loss = 0.01910348
Iteration 87, loss = 0.01911472
Iteration 88, loss = 0.01899255
Iteration 89, loss = 0.01891746
Iteration 90, loss = 0.01870619
Iteration 91, loss = 0.01885251
Iteration 92, loss = 0.01872843
Iteration 93, loss = 0.01878538
Iteration 94, loss = 0.01864391
Iteration 95, loss = 0.01865858
Iteration 96, loss = 0.01862245
Iteration 97, loss = 0.01856250
Iteration 98, loss = 0.01853188
Iteration 99, loss = 0.01844929
Iteration 100, loss = 0.01847689
Iteration 101, loss = 0.01829468
Iteration 102, loss = 0.01838081
Iteration 103, loss = 0.01813904
Iteration 104, loss = 0.01814513
Iteration 105, loss = 0.01824139
Iteration 106, loss = 0.01816302
Iteration 107, loss = 0.01814389
Iteration 108, loss = 0.01799905
Iteration 109, loss = 0.01794991
Iteration 110, loss = 0.01803535
Iteration 111, loss = 0.01818757
Iteration 112, loss = 0.01790213
Iteration 113, loss = 0.01800908
Iteration 114, loss = 0.01784153
Iteration 115, loss = 0.01802129
Iteration 116, loss = 0.01782143
Iteration 117, loss = 0.01783831
Iteration 118, loss = 0.01777036
Iteration 119, loss = 0.01777905
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67931326
Iteration 2, loss = 0.64032261
Iteration 3, loss = 0.57417266
Iteration 4, loss = 0.48924785
Iteration 5, loss = 0.40314424
Iteration 6, loss = 0.32490124
Iteration 7, loss = 0.25867332
Iteration 8, loss = 0.20674839
Iteration 9, loss = 0.16724621
Iteration 10, loss = 0.13760038
Iteration 11, loss = 0.11539746
Iteration 12, loss = 0.09843693
Iteration 13, loss = 0.08564104
Iteration 14, loss = 0.07545744
Iteration 15, loss = 0.06739041
Iteration 16, loss = 0.06066351
Iteration 17, loss = 0.05539997
Iteration 18, loss = 0.05105683
Iteration 19, loss = 0.04728636
Iteration 20, loss = 0.04420340
Iteration 21, loss = 0.04172928
Iteration 22, loss = 0.03926836
Iteration 23, loss = 0.03719299
Iteration 24, loss = 0.03556023
Iteration 25, loss = 0.03397771
Iteration 26, loss = 0.03280146
Iteration 27, loss = 0.03158893
Iteration 28, loss = 0.03062768
Iteration 29, loss = 0.02950920
Iteration 30, loss = 0.02864217
Iteration 31, loss = 0.02804684
Iteration 32, loss = 0.02737794
Iteration 33, loss = 0.02655879
Iteration 34, loss = 0.02615312
Iteration 35, loss = 0.02564643
Iteration 36, loss = 0.02495797
Iteration 37, loss = 0.02494474
Iteration 38, loss = 0.02428957
Iteration 39, loss = 0.02393399
Iteration 40, loss = 0.02361667
Iteration 41, loss = 0.02332765
Iteration 42, loss = 0.02306305
Iteration 43, loss = 0.02279679
Iteration 44, loss = 0.02253261
Iteration 45, loss = 0.02231370
Iteration 46, loss = 0.02217193
Iteration 47, loss = 0.02188727
Iteration 48, loss = 0.02196238
Iteration 49, loss = 0.02150026
Iteration 50, loss = 0.02150418
Iteration 51, loss = 0.02127914
Iteration 52, loss = 0.02112118
Iteration 53, loss = 0.02093766
Iteration 54, loss = 0.02084179
Iteration 55, loss = 0.02075388
Iteration 56, loss = 0.02056058
Iteration 57, loss = 0.02057522
Iteration 58, loss = 0.02049904
Iteration 59, loss = 0.02045097
Iteration 60, loss = 0.02031310
Iteration 61, loss = 0.02014338
Iteration 62, loss = 0.02006663
Iteration 63, loss = 0.02027034
Iteration 64, loss = 0.01997315
Iteration 65, loss = 0.01986908
Iteration 66, loss = 0.01970584
Iteration 67, loss = 0.01980363
Iteration 68, loss = 0.01972601
Iteration 69, loss = 0.01961326
Iteration 70, loss = 0.01933654
Iteration 71, loss = 0.01948232
Iteration 72, loss = 0.01951352
Iteration 73, loss = 0.01930177
Iteration 74, loss = 0.01925628
Iteration 75, loss = 0.01939743
Iteration 76, loss = 0.01944700
Iteration 77, loss = 0.01971987
Iteration 78, loss = 0.01925003
Iteration 79, loss = 0.01916097
Iteration 80, loss = 0.01903814
Iteration 81, loss = 0.01908994
Iteration 82, loss = 0.01916971
Iteration 83, loss = 0.01893012
Iteration 84, loss = 0.01883872
Iteration 85, loss = 0.01893429
Iteration 86, loss = 0.01899609
Iteration 87, loss = 0.01877902
Iteration 88, loss = 0.01880673
Iteration 89, loss = 0.01879085
Iteration 90, loss = 0.01878105
Iteration 91, loss = 0.01867349
Iteration 92, loss = 0.01873315
Iteration 93, loss = 0.01862527
Iteration 94, loss = 0.01860899
Iteration 95, loss = 0.01867836
Iteration 96, loss = 0.01877665
Iteration 97, loss = 0.01869413
Iteration 98, loss = 0.01852353
Iteration 99, loss = 0.01858841
Iteration 100, loss = 0.01851274
Iteration 101, loss = 0.01847922
Iteration 102, loss = 0.01846221
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67595860
Iteration 2, loss = 0.61988552
Iteration 3, loss = 0.53809609
Iteration 4, loss = 0.44719695
Iteration 5, loss = 0.36402972
Iteration 6, loss = 0.29245382
Iteration 7, loss = 0.23484253
Iteration 8, loss = 0.19048166
Iteration 9, loss = 0.15658030
Iteration 10, loss = 0.13082654
Iteration 11, loss = 0.11134407
Iteration 12, loss = 0.09628414
Iteration 13, loss = 0.08443711
Iteration 14, loss = 0.07505120
Iteration 15, loss = 0.06754186
Iteration 16, loss = 0.06125405
Iteration 17, loss = 0.05611608
Iteration 18, loss = 0.05200921
Iteration 19, loss = 0.04836032
Iteration 20, loss = 0.04546713
Iteration 21, loss = 0.04272912
Iteration 22, loss = 0.04062889
Iteration 23, loss = 0.03852679
Iteration 24, loss = 0.03689539
Iteration 25, loss = 0.03531543
Iteration 26, loss = 0.03411568
Iteration 27, loss = 0.03271894
Iteration 28, loss = 0.03182199
Iteration 29, loss = 0.03088946
Iteration 30, loss = 0.03014935
Iteration 31, loss = 0.02920022
Iteration 32, loss = 0.02845040
Iteration 33, loss = 0.02783908
Iteration 34, loss = 0.02730688
Iteration 35, loss = 0.02684889
Iteration 36, loss = 0.02619915
Iteration 37, loss = 0.02586298
Iteration 38, loss = 0.02572083
Iteration 39, loss = 0.02528803
Iteration 40, loss = 0.02479206
Iteration 41, loss = 0.02456103
Iteration 42, loss = 0.02441724
Iteration 43, loss = 0.02396712
Iteration 44, loss = 0.02371886
Iteration 45, loss = 0.02356169
Iteration 46, loss = 0.02325457
Iteration 47, loss = 0.02319418
Iteration 48, loss = 0.02278129
Iteration 49, loss = 0.02281019
Iteration 50, loss = 0.02241857
Iteration 51, loss = 0.02244698
Iteration 52, loss = 0.02225739
Iteration 53, loss = 0.02213995
Iteration 54, loss = 0.02212829
Iteration 55, loss = 0.02188456
Iteration 56, loss = 0.02184030
Iteration 57, loss = 0.02162137
Iteration 58, loss = 0.02167209
Iteration 59, loss = 0.02146463
Iteration 60, loss = 0.02152439
Iteration 61, loss = 0.02128166
Iteration 62, loss = 0.02115308
Iteration 63, loss = 0.02147199
Iteration 64, loss = 0.02099716
Iteration 65, loss = 0.02106166
Iteration 66, loss = 0.02074669
Iteration 67, loss = 0.02084158
Iteration 68, loss = 0.02058753
Iteration 69, loss = 0.02072316
Iteration 70, loss = 0.02046910
Iteration 71, loss = 0.02070624
Iteration 72, loss = 0.02069555
Iteration 73, loss = 0.02068201
Iteration 74, loss = 0.02049036
Iteration 75, loss = 0.02038287
Iteration 76, loss = 0.02035296
Iteration 77, loss = 0.02040634
Iteration 78, loss = 0.02040969
Iteration 79, loss = 0.02027855
Iteration 80, loss = 0.02005672
Iteration 81, loss = 0.02023262
Iteration 82, loss = 0.02031122
Iteration 83, loss = 0.02030657
Iteration 84, loss = 0.02000773
Iteration 85, loss = 0.01992552
Iteration 86, loss = 0.02000494
Iteration 87, loss = 0.01994568
Iteration 88, loss = 0.01982843
Iteration 89, loss = 0.01972410
Iteration 90, loss = 0.02015822
Iteration 91, loss = 0.01989503
Iteration 92, loss = 0.01994126
Iteration 93, loss = 0.01979392
Iteration 94, loss = 0.01978167
Iteration 95, loss = 0.01985289
Iteration 96, loss = 0.01984462
Iteration 97, loss = 0.01992879
Iteration 98, loss = 0.02012091
Iteration 99, loss = 0.01991537
Iteration 100, loss = 0.01959704
Iteration 101, loss = 0.01978326
Iteration 102, loss = 0.01958306
Iteration 103, loss = 0.01955787
Iteration 104, loss = 0.01965938
Iteration 105, loss = 0.01954747
Iteration 106, loss = 0.01959659
Iteration 107, loss = 0.01965754
Iteration 108, loss = 0.01936747
Iteration 109, loss = 0.01952710
Iteration 110, loss = 0.01949304
Iteration 111, loss = 0.01951709
Iteration 112, loss = 0.01953447
Iteration 113, loss = 0.01943759
Iteration 114, loss = 0.01942180
Iteration 115, loss = 0.01937434
Iteration 116, loss = 0.01959350
Iteration 117, loss = 0.01931988
Iteration 118, loss = 0.01946230
Iteration 119, loss = 0.01946814
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67637114
Iteration 2, loss = 0.61937948
Iteration 3, loss = 0.52654936
Iteration 4, loss = 0.42415364
Iteration 5, loss = 0.32953847
Iteration 6, loss = 0.25353477
Iteration 7, loss = 0.19659084
Iteration 8, loss = 0.15557558
Iteration 9, loss = 0.12670363
Iteration 10, loss = 0.10574789
Iteration 11, loss = 0.09009955
Iteration 12, loss = 0.07841953
Iteration 13, loss = 0.06944151
Iteration 14, loss = 0.06233674
Iteration 15, loss = 0.05676965
Iteration 16, loss = 0.05199664
Iteration 17, loss = 0.04834860
Iteration 18, loss = 0.04509285
Iteration 19, loss = 0.04254129
Iteration 20, loss = 0.04045370
Iteration 21, loss = 0.03852041
Iteration 22, loss = 0.03675539
Iteration 23, loss = 0.03542005
Iteration 24, loss = 0.03443762
Iteration 25, loss = 0.03329621
Iteration 26, loss = 0.03233366
Iteration 27, loss = 0.03145768
Iteration 28, loss = 0.03056479
Iteration 29, loss = 0.03027603
Iteration 30, loss = 0.02921207
Iteration 31, loss = 0.02895515
Iteration 32, loss = 0.02833999
Iteration 33, loss = 0.02788417
Iteration 34, loss = 0.02750794
Iteration 35, loss = 0.02710023
Iteration 36, loss = 0.02708413
Iteration 37, loss = 0.02688829
Iteration 38, loss = 0.02623536
Iteration 39, loss = 0.02618883
Iteration 40, loss = 0.02567631
Iteration 41, loss = 0.02554101
Iteration 42, loss = 0.02550047
Iteration 43, loss = 0.02526912
Iteration 44, loss = 0.02515818
Iteration 45, loss = 0.02473663
Iteration 46, loss = 0.02472002
Iteration 47, loss = 0.02451496
Iteration 48, loss = 0.02436386
Iteration 49, loss = 0.02422455
Iteration 50, loss = 0.02408638
Iteration 51, loss = 0.02418367
Iteration 52, loss = 0.02411769
Iteration 53, loss = 0.02406779
Iteration 54, loss = 0.02393033
Iteration 55, loss = 0.02370417
Iteration 56, loss = 0.02376435
Iteration 57, loss = 0.02348006
Iteration 58, loss = 0.02341110
Iteration 59, loss = 0.02337929
Iteration 60, loss = 0.02341153
Iteration 61, loss = 0.02314280
Iteration 62, loss = 0.02332623
Iteration 63, loss = 0.02307929
Iteration 64, loss = 0.02317823
Iteration 65, loss = 0.02314235
Iteration 66, loss = 0.02304426
Iteration 67, loss = 0.02292186
Iteration 68, loss = 0.02300364
Iteration 69, loss = 0.02274328
Iteration 70, loss = 0.02288058
Iteration 71, loss = 0.02337534
Iteration 72, loss = 0.02276390
Iteration 73, loss = 0.02277724
Iteration 74, loss = 0.02268124
Iteration 75, loss = 0.02253669
Iteration 76, loss = 0.02253246
Iteration 77, loss = 0.02259867
Iteration 78, loss = 0.02269327
Iteration 79, loss = 0.02270924
Iteration 80, loss = 0.02246909
Iteration 81, loss = 0.02245290
Iteration 82, loss = 0.02247287
Iteration 83, loss = 0.02263259
Iteration 84, loss = 0.02250532
Iteration 85, loss = 0.02257165
Iteration 86, loss = 0.02262928
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68005661
Iteration 2, loss = 0.61339968
Iteration 3, loss = 0.51012160
Iteration 4, loss = 0.39970553
Iteration 5, loss = 0.30578178
Iteration 6, loss = 0.23306153
Iteration 7, loss = 0.18041022
Iteration 8, loss = 0.14335249
Iteration 9, loss = 0.11715264
Iteration 10, loss = 0.09834299
Iteration 11, loss = 0.08415454
Iteration 12, loss = 0.07370515
Iteration 13, loss = 0.06554265
Iteration 14, loss = 0.05908552
Iteration 15, loss = 0.05422704
Iteration 16, loss = 0.04986851
Iteration 17, loss = 0.04658547
Iteration 18, loss = 0.04368647
Iteration 19, loss = 0.04146933
Iteration 20, loss = 0.03934647
Iteration 21, loss = 0.03764540
Iteration 22, loss = 0.03610586
Iteration 23, loss = 0.03488564
Iteration 24, loss = 0.03361527
Iteration 25, loss = 0.03293654
Iteration 26, loss = 0.03183886
Iteration 27, loss = 0.03105231
Iteration 28, loss = 0.03016598
Iteration 29, loss = 0.02975755
Iteration 30, loss = 0.02919298
Iteration 31, loss = 0.02884788
Iteration 32, loss = 0.02819981
Iteration 33, loss = 0.02783927
Iteration 34, loss = 0.02741429
Iteration 35, loss = 0.02717441
Iteration 36, loss = 0.02666565
Iteration 37, loss = 0.02668631
Iteration 38, loss = 0.02643879
Iteration 39, loss = 0.02573498
Iteration 40, loss = 0.02570082
Iteration 41, loss = 0.02552592
Iteration 42, loss = 0.02517285
Iteration 43, loss = 0.02500878
Iteration 44, loss = 0.02513804
Iteration 45, loss = 0.02472070
Iteration 46, loss = 0.02482240
Iteration 47, loss = 0.02463081
Iteration 48, loss = 0.02443587
Iteration 49, loss = 0.02415260
Iteration 50, loss = 0.02417327
Iteration 51, loss = 0.02398149
Iteration 52, loss = 0.02393515
Iteration 53, loss = 0.02421646
Iteration 54, loss = 0.02392637
Iteration 55, loss = 0.02341604
Iteration 56, loss = 0.02359292
Iteration 57, loss = 0.02357384
Iteration 58, loss = 0.02344680
Iteration 59, loss = 0.02363827
Iteration 60, loss = 0.02333841
Iteration 61, loss = 0.02313787
Iteration 62, loss = 0.02368378
Iteration 63, loss = 0.02327471
Iteration 64, loss = 0.02313641
Iteration 65, loss = 0.02299703
Iteration 66, loss = 0.02290590
Iteration 67, loss = 0.02318729
Iteration 68, loss = 0.02289174
Iteration 69, loss = 0.02295824
Iteration 70, loss = 0.02288431
Iteration 71, loss = 0.02289077
Iteration 72, loss = 0.02295766
Iteration 73, loss = 0.02274473
Iteration 74, loss = 0.02266002
Iteration 75, loss = 0.02276715
Iteration 76, loss = 0.02265376
Iteration 77, loss = 0.02276617
Iteration 78, loss = 0.02277558
Iteration 79, loss = 0.02268821
Iteration 80, loss = 0.02239625
Iteration 81, loss = 0.02235308
Iteration 82, loss = 0.02285122
Iteration 83, loss = 0.02263043
Iteration 84, loss = 0.02241572
Iteration 85, loss = 0.02249880
Iteration 86, loss = 0.02308623
Iteration 87, loss = 0.02220331
Iteration 88, loss = 0.02221059
Iteration 89, loss = 0.02236677
Iteration 90, loss = 0.02252873
Iteration 91, loss = 0.02230482
Iteration 92, loss = 0.02274165
Iteration 93, loss = 0.02237306
Iteration 94, loss = 0.02254938
Iteration 95, loss = 0.02236104
Iteration 96, loss = 0.02220912
Iteration 97, loss = 0.02201008
Iteration 98, loss = 0.02217922
Iteration 99, loss = 0.02240469
Iteration 100, loss = 0.02223789
Iteration 101, loss = 0.02201917
Iteration 102, loss = 0.02218127
Iteration 103, loss = 0.02211333
Iteration 104, loss = 0.02200843
Iteration 105, loss = 0.02219825
Iteration 106, loss = 0.02201542
Iteration 107, loss = 0.02206755
Iteration 108, loss = 0.02202471
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70775218
Iteration 2, loss = 0.68858202
Iteration 3, loss = 0.67115023
Iteration 4, loss = 0.65113638
Iteration 5, loss = 0.62724336
Iteration 6, loss = 0.59894951
Iteration 7, loss = 0.56710879
Iteration 8, loss = 0.53286828
Iteration 9, loss = 0.49739351
Iteration 10, loss = 0.46130093
Iteration 11, loss = 0.42596205
Iteration 12, loss = 0.39158514
Iteration 13, loss = 0.35891420
Iteration 14, loss = 0.32806360
Iteration 15, loss = 0.29930647
Iteration 16, loss = 0.27287120
Iteration 17, loss = 0.24859464
Iteration 18, loss = 0.22647349
Iteration 19, loss = 0.20649053
Iteration 20, loss = 0.18852754
Iteration 21, loss = 0.17234076
Iteration 22, loss = 0.15788230
Iteration 23, loss = 0.14497650
Iteration 24, loss = 0.13342413
Iteration 25, loss = 0.12314580
Iteration 26, loss = 0.11394828
Iteration 27, loss = 0.10577342
Iteration 28, loss = 0.09831794
Iteration 29, loss = 0.09168308
Iteration 30, loss = 0.08567001
Iteration 31, loss = 0.08034962
Iteration 32, loss = 0.07555158
Iteration 33, loss = 0.07110189
Iteration 34, loss = 0.06704757
Iteration 35, loss = 0.06340980
Iteration 36, loss = 0.06011066
Iteration 37, loss = 0.05704001
Iteration 38, loss = 0.05425497
Iteration 39, loss = 0.05173251
Iteration 40, loss = 0.04934374
Iteration 41, loss = 0.04721012
Iteration 42, loss = 0.04515868
Iteration 43, loss = 0.04333838
Iteration 44, loss = 0.04159929
Iteration 45, loss = 0.03997572
Iteration 46, loss = 0.03848838
Iteration 47, loss = 0.03708509
Iteration 48, loss = 0.03582815
Iteration 49, loss = 0.03455675
Iteration 50, loss = 0.03351561
Iteration 51, loss = 0.03237383
Iteration 52, loss = 0.03137355
Iteration 53, loss = 0.03042663
Iteration 54, loss = 0.02957117
Iteration 55, loss = 0.02869208
Iteration 56, loss = 0.02790806
Iteration 57, loss = 0.02718515
Iteration 58, loss = 0.02644366
Iteration 59, loss = 0.02577097
Iteration 60, loss = 0.02518870
Iteration 61, loss = 0.02451176
Iteration 62, loss = 0.02394822
Iteration 63, loss = 0.02342876
Iteration 64, loss = 0.02295526
Iteration 65, loss = 0.02241100
Iteration 66, loss = 0.02194264
Iteration 67, loss = 0.02149768
Iteration 68, loss = 0.02108491
Iteration 69, loss = 0.02068844
Iteration 70, loss = 0.02027327
Iteration 71, loss = 0.01985823
Iteration 72, loss = 0.01950878
Iteration 73, loss = 0.01915862
Iteration 74, loss = 0.01885772
Iteration 75, loss = 0.01852890
Iteration 76, loss = 0.01826529
Iteration 77, loss = 0.01795839
Iteration 78, loss = 0.01772821
Iteration 79, loss = 0.01742438
Iteration 80, loss = 0.01716357
Iteration 81, loss = 0.01689276
Iteration 82, loss = 0.01666693
Iteration 83, loss = 0.01644955
Iteration 84, loss = 0.01620166
Iteration 85, loss = 0.01598222
Iteration 86, loss = 0.01578038
Iteration 87, loss = 0.01559171
Iteration 88, loss = 0.01535990
Iteration 89, loss = 0.01526751
Iteration 90, loss = 0.01510162
Iteration 91, loss = 0.01480600
Iteration 92, loss = 0.01466076
Iteration 93, loss = 0.01447679
Iteration 94, loss = 0.01435360
Iteration 95, loss = 0.01419049
Iteration 96, loss = 0.01399046
Iteration 97, loss = 0.01393534
Iteration 98, loss = 0.01379197
Iteration 99, loss = 0.01365761
Iteration 100, loss = 0.01346616
Iteration 101, loss = 0.01336925
Iteration 102, loss = 0.01326121
Iteration 103, loss = 0.01309357
Iteration 104, loss = 0.01296494
Iteration 105, loss = 0.01287133
Iteration 106, loss = 0.01276783
Iteration 107, loss = 0.01262022
Iteration 108, loss = 0.01252040
Iteration 109, loss = 0.01242447
Iteration 110, loss = 0.01231619
Iteration 111, loss = 0.01229046
Iteration 112, loss = 0.01220018
Iteration 113, loss = 0.01206659
Iteration 114, loss = 0.01194311
Iteration 115, loss = 0.01188451
Iteration 116, loss = 0.01176398
Iteration 117, loss = 0.01171858
Iteration 118, loss = 0.01161155
Iteration 119, loss = 0.01152449
Iteration 120, loss = 0.01149392
Iteration 121, loss = 0.01138584
Iteration 122, loss = 0.01128970
Iteration 123, loss = 0.01136164
Iteration 124, loss = 0.01115205
Iteration 125, loss = 0.01109778
Iteration 126, loss = 0.01105147
Iteration 127, loss = 0.01094626
Iteration 128, loss = 0.01093507
Iteration 129, loss = 0.01081251
Iteration 130, loss = 0.01078242
Iteration 131, loss = 0.01069962
Iteration 132, loss = 0.01068409
Iteration 133, loss = 0.01058986
Iteration 134, loss = 0.01050020
Iteration 135, loss = 0.01046173
Iteration 136, loss = 0.01044021
Iteration 137, loss = 0.01039223
Iteration 138, loss = 0.01038376
Iteration 139, loss = 0.01028981
Iteration 140, loss = 0.01027017
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69317578
Iteration 2, loss = 0.66652105
Iteration 3, loss = 0.63627513
Iteration 4, loss = 0.59854721
Iteration 5, loss = 0.55484698
Iteration 6, loss = 0.50735200
Iteration 7, loss = 0.45910423
Iteration 8, loss = 0.41211166
Iteration 9, loss = 0.36756110
Iteration 10, loss = 0.32645820
Iteration 11, loss = 0.28887047
Iteration 12, loss = 0.25521650
Iteration 13, loss = 0.22555904
Iteration 14, loss = 0.19974793
Iteration 15, loss = 0.17736123
Iteration 16, loss = 0.15791166
Iteration 17, loss = 0.14185183
Iteration 18, loss = 0.12774408
Iteration 19, loss = 0.11563603
Iteration 20, loss = 0.10521230
Iteration 21, loss = 0.09652645
Iteration 22, loss = 0.08888728
Iteration 23, loss = 0.08217099
Iteration 24, loss = 0.07641744
Iteration 25, loss = 0.07136829
Iteration 26, loss = 0.06672504
Iteration 27, loss = 0.06263931
Iteration 28, loss = 0.05903363
Iteration 29, loss = 0.05584865
Iteration 30, loss = 0.05295532
Iteration 31, loss = 0.05023874
Iteration 32, loss = 0.04811091
Iteration 33, loss = 0.04610965
Iteration 34, loss = 0.04404212
Iteration 35, loss = 0.04222690
Iteration 36, loss = 0.04072552
Iteration 37, loss = 0.03918497
Iteration 38, loss = 0.03780084
Iteration 39, loss = 0.03653115
Iteration 40, loss = 0.03543204
Iteration 41, loss = 0.03448461
Iteration 42, loss = 0.03352857
Iteration 43, loss = 0.03268681
Iteration 44, loss = 0.03179318
Iteration 45, loss = 0.03092102
Iteration 46, loss = 0.03011297
Iteration 47, loss = 0.02961768
Iteration 48, loss = 0.02905324
Iteration 49, loss = 0.02818608
Iteration 50, loss = 0.02779784
Iteration 51, loss = 0.02728313
Iteration 52, loss = 0.02665739
Iteration 53, loss = 0.02620497
Iteration 54, loss = 0.02563853
Iteration 55, loss = 0.02535127
Iteration 56, loss = 0.02509677
Iteration 57, loss = 0.02464840
Iteration 58, loss = 0.02444322
Iteration 59, loss = 0.02406934
Iteration 60, loss = 0.02377719
Iteration 61, loss = 0.02332896
Iteration 62, loss = 0.02309349
Iteration 63, loss = 0.02280917
Iteration 64, loss = 0.02272563
Iteration 65, loss = 0.02247968
Iteration 66, loss = 0.02216967
Iteration 67, loss = 0.02186784
Iteration 68, loss = 0.02172082
Iteration 69, loss = 0.02141151
Iteration 70, loss = 0.02113148
Iteration 71, loss = 0.02078718
Iteration 72, loss = 0.02062632
Iteration 73, loss = 0.02056445
Iteration 74, loss = 0.02032859
Iteration 75, loss = 0.02034555
Iteration 76, loss = 0.02025575
Iteration 77, loss = 0.01994160
Iteration 78, loss = 0.01971743
Iteration 79, loss = 0.01955563
Iteration 80, loss = 0.01929509
Iteration 81, loss = 0.01915796
Iteration 82, loss = 0.01905105
Iteration 83, loss = 0.01899570
Iteration 84, loss = 0.01884326
Iteration 85, loss = 0.01877138
Iteration 86, loss = 0.01863296
Iteration 87, loss = 0.01850354
Iteration 88, loss = 0.01846767
Iteration 89, loss = 0.01837838
Iteration 90, loss = 0.01830676
Iteration 91, loss = 0.01831340
Iteration 92, loss = 0.01823787
Iteration 93, loss = 0.01812953
Iteration 94, loss = 0.01794989
Iteration 95, loss = 0.01787851
Iteration 96, loss = 0.01782592
Iteration 97, loss = 0.01766758
Iteration 98, loss = 0.01761416
Iteration 99, loss = 0.01754930
Iteration 100, loss = 0.01748567
Iteration 101, loss = 0.01727006
Iteration 102, loss = 0.01726892
Iteration 103, loss = 0.01735441
Iteration 104, loss = 0.01732129
Iteration 105, loss = 0.01720185
Iteration 106, loss = 0.01720677
Iteration 107, loss = 0.01707850
Iteration 108, loss = 0.01726644
Iteration 109, loss = 0.01718531
Iteration 110, loss = 0.01705944
Iteration 111, loss = 0.01701364
Iteration 112, loss = 0.01683968
Iteration 113, loss = 0.01689264
Iteration 114, loss = 0.01692696
Iteration 115, loss = 0.01664758
Iteration 116, loss = 0.01660471
Iteration 117, loss = 0.01664311
Iteration 118, loss = 0.01649879
Iteration 119, loss = 0.01645188
Iteration 120, loss = 0.01661199
Iteration 121, loss = 0.01653663
Iteration 122, loss = 0.01648701
Iteration 123, loss = 0.01632901
Iteration 124, loss = 0.01632006
Iteration 125, loss = 0.01626103
Iteration 126, loss = 0.01610868
Iteration 127, loss = 0.01620317
Iteration 128, loss = 0.01627337
Iteration 129, loss = 0.01618052
Iteration 130, loss = 0.01670337
Iteration 131, loss = 0.01674862
Iteration 132, loss = 0.01651600
Iteration 133, loss = 0.01627906
Iteration 134, loss = 0.01605626
Iteration 135, loss = 0.01589926
Iteration 136, loss = 0.01574072
Iteration 137, loss = 0.01579369
Iteration 138, loss = 0.01569175
Iteration 139, loss = 0.01573322
Iteration 140, loss = 0.01576226
Iteration 141, loss = 0.01591030
Iteration 142, loss = 0.01585586
Iteration 143, loss = 0.01570245
Iteration 144, loss = 0.01576903
Iteration 145, loss = 0.01559885
Iteration 146, loss = 0.01559081
Iteration 147, loss = 0.01549486
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72136676
Iteration 2, loss = 0.68755827
Iteration 3, loss = 0.64559851
Iteration 4, loss = 0.59478198
Iteration 5, loss = 0.53749398
Iteration 6, loss = 0.47826061
Iteration 7, loss = 0.41950991
Iteration 8, loss = 0.36352018
Iteration 9, loss = 0.31278913
Iteration 10, loss = 0.26850416
Iteration 11, loss = 0.23069255
Iteration 12, loss = 0.19893513
Iteration 13, loss = 0.17252608
Iteration 14, loss = 0.15081272
Iteration 15, loss = 0.13277086
Iteration 16, loss = 0.11776599
Iteration 17, loss = 0.10536565
Iteration 18, loss = 0.09508863
Iteration 19, loss = 0.08633271
Iteration 20, loss = 0.07898516
Iteration 21, loss = 0.07285712
Iteration 22, loss = 0.06747997
Iteration 23, loss = 0.06282156
Iteration 24, loss = 0.05874591
Iteration 25, loss = 0.05521282
Iteration 26, loss = 0.05223883
Iteration 27, loss = 0.04945664
Iteration 28, loss = 0.04702721
Iteration 29, loss = 0.04477919
Iteration 30, loss = 0.04284267
Iteration 31, loss = 0.04126374
Iteration 32, loss = 0.03973030
Iteration 33, loss = 0.03818786
Iteration 34, loss = 0.03698555
Iteration 35, loss = 0.03588414
Iteration 36, loss = 0.03468330
Iteration 37, loss = 0.03380740
Iteration 38, loss = 0.03308354
Iteration 39, loss = 0.03203702
Iteration 40, loss = 0.03139373
Iteration 41, loss = 0.03063622
Iteration 42, loss = 0.02992732
Iteration 43, loss = 0.02937881
Iteration 44, loss = 0.02897095
Iteration 45, loss = 0.02837187
Iteration 46, loss = 0.02799251
Iteration 47, loss = 0.02738967
Iteration 48, loss = 0.02691993
Iteration 49, loss = 0.02652568
Iteration 50, loss = 0.02621174
Iteration 51, loss = 0.02573070
Iteration 52, loss = 0.02581264
Iteration 53, loss = 0.02507778
Iteration 54, loss = 0.02497850
Iteration 55, loss = 0.02475942
Iteration 56, loss = 0.02453081
Iteration 57, loss = 0.02418445
Iteration 58, loss = 0.02396111
Iteration 59, loss = 0.02384131
Iteration 60, loss = 0.02361298
Iteration 61, loss = 0.02321909
Iteration 62, loss = 0.02316253
Iteration 63, loss = 0.02282811
Iteration 64, loss = 0.02280380
Iteration 65, loss = 0.02267364
Iteration 66, loss = 0.02255015
Iteration 67, loss = 0.02239398
Iteration 68, loss = 0.02224035
Iteration 69, loss = 0.02212205
Iteration 70, loss = 0.02194338
Iteration 71, loss = 0.02177389
Iteration 72, loss = 0.02166410
Iteration 73, loss = 0.02159707
Iteration 74, loss = 0.02158155
Iteration 75, loss = 0.02138722
Iteration 76, loss = 0.02135318
Iteration 77, loss = 0.02131373
Iteration 78, loss = 0.02122976
Iteration 79, loss = 0.02110247
Iteration 80, loss = 0.02103055
Iteration 81, loss = 0.02105346
Iteration 82, loss = 0.02105200
Iteration 83, loss = 0.02083537
Iteration 84, loss = 0.02069659
Iteration 85, loss = 0.02058533
Iteration 86, loss = 0.02052350
Iteration 87, loss = 0.02043697
Iteration 88, loss = 0.02029763
Iteration 89, loss = 0.02033549
Iteration 90, loss = 0.02031815
Iteration 91, loss = 0.02028387
Iteration 92, loss = 0.02022829
Iteration 93, loss = 0.02010252
Iteration 94, loss = 0.02005796
Iteration 95, loss = 0.02016880
Iteration 96, loss = 0.02027492
Iteration 97, loss = 0.02004133
Iteration 98, loss = 0.01984391
Iteration 99, loss = 0.01995532
Iteration 100, loss = 0.01987160
Iteration 101, loss = 0.01980677
Iteration 102, loss = 0.01962811
Iteration 103, loss = 0.01966476
Iteration 104, loss = 0.01960869
Iteration 105, loss = 0.01955660
Iteration 106, loss = 0.01959874
Iteration 107, loss = 0.01951303
Iteration 108, loss = 0.01960021
Iteration 109, loss = 0.01944044
Iteration 110, loss = 0.01949669
Iteration 111, loss = 0.01940709
Iteration 112, loss = 0.01940708
Iteration 113, loss = 0.01930676
Iteration 114, loss = 0.01942516
Iteration 115, loss = 0.01938804
Iteration 116, loss = 0.01925095
Iteration 117, loss = 0.01929504
Iteration 118, loss = 0.01932200
Iteration 119, loss = 0.01910444
Iteration 120, loss = 0.01930917
Iteration 121, loss = 0.01921495
Iteration 122, loss = 0.01907152
Iteration 123, loss = 0.01922773
Iteration 124, loss = 0.01903642
Iteration 125, loss = 0.01916877
Iteration 126, loss = 0.01915694
Iteration 127, loss = 0.01909627
Iteration 128, loss = 0.01910895
Iteration 129, loss = 0.01919880
Iteration 130, loss = 0.01904455
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69029724
Iteration 2, loss = 0.67572829
Iteration 3, loss = 0.66274242
Iteration 4, loss = 0.65005840
Iteration 5, loss = 0.63673304
Iteration 6, loss = 0.62271904
Iteration 7, loss = 0.60754790
Iteration 8, loss = 0.59120075
Iteration 9, loss = 0.57381104
Iteration 10, loss = 0.55532207
Iteration 11, loss = 0.53587999
Iteration 12, loss = 0.51580721
Iteration 13, loss = 0.49491977
Iteration 14, loss = 0.47361184
Iteration 15, loss = 0.45208233
Iteration 16, loss = 0.43044506
Iteration 17, loss = 0.40895235
Iteration 18, loss = 0.38764209
Iteration 19, loss = 0.36675189
Iteration 20, loss = 0.34647959
Iteration 21, loss = 0.32688605
Iteration 22, loss = 0.30805583
Iteration 23, loss = 0.28987595
Iteration 24, loss = 0.27266115
Iteration 25, loss = 0.25625149
Iteration 26, loss = 0.24073701
Iteration 27, loss = 0.22598849
Iteration 28, loss = 0.21232103
Iteration 29, loss = 0.19939513
Iteration 30, loss = 0.18729682
Iteration 31, loss = 0.17609302
Iteration 32, loss = 0.16552672
Iteration 33, loss = 0.15585010
Iteration 34, loss = 0.14676807
Iteration 35, loss = 0.13833115
Iteration 36, loss = 0.13060900
Iteration 37, loss = 0.12336690
Iteration 38, loss = 0.11669641
Iteration 39, loss = 0.11049763
Iteration 40, loss = 0.10471817
Iteration 41, loss = 0.09944587
Iteration 42, loss = 0.09455439
Iteration 43, loss = 0.08994411
Iteration 44, loss = 0.08571046
Iteration 45, loss = 0.08174046
Iteration 46, loss = 0.07805683
Iteration 47, loss = 0.07465106
Iteration 48, loss = 0.07146381
Iteration 49, loss = 0.06844797
Iteration 50, loss = 0.06568114
Iteration 51, loss = 0.06307079
Iteration 52, loss = 0.06062333
Iteration 53, loss = 0.05829502
Iteration 54, loss = 0.05612954
Iteration 55, loss = 0.05413099
Iteration 56, loss = 0.05220281
Iteration 57, loss = 0.05044012
Iteration 58, loss = 0.04873099
Iteration 59, loss = 0.04723950
Iteration 60, loss = 0.04564621
Iteration 61, loss = 0.04420077
Iteration 62, loss = 0.04285354
Iteration 63, loss = 0.04158831
Iteration 64, loss = 0.04038110
Iteration 65, loss = 0.03924216
Iteration 66, loss = 0.03816429
Iteration 67, loss = 0.03713220
Iteration 68, loss = 0.03617807
Iteration 69, loss = 0.03521264
Iteration 70, loss = 0.03434053
Iteration 71, loss = 0.03344978
Iteration 72, loss = 0.03264477
Iteration 73, loss = 0.03188172
Iteration 74, loss = 0.03110279
Iteration 75, loss = 0.03042798
Iteration 76, loss = 0.02969963
Iteration 77, loss = 0.02909338
Iteration 78, loss = 0.02847089
Iteration 79, loss = 0.02785329
Iteration 80, loss = 0.02732203
Iteration 81, loss = 0.02672250
Iteration 82, loss = 0.02619234
Iteration 83, loss = 0.02568453
Iteration 84, loss = 0.02523469
Iteration 85, loss = 0.02472612
Iteration 86, loss = 0.02429887
Iteration 87, loss = 0.02385888
Iteration 88, loss = 0.02348402
Iteration 89, loss = 0.02312946
Iteration 90, loss = 0.02264786
Iteration 91, loss = 0.02228776
Iteration 92, loss = 0.02193156
Iteration 93, loss = 0.02161843
Iteration 94, loss = 0.02131335
Iteration 95, loss = 0.02098830
Iteration 96, loss = 0.02060777
Iteration 97, loss = 0.02029424
Iteration 98, loss = 0.01996915
Iteration 99, loss = 0.01969760
Iteration 100, loss = 0.01943111
Iteration 101, loss = 0.01921344
Iteration 102, loss = 0.01898252
Iteration 103, loss = 0.01875731
Iteration 104, loss = 0.01851333
Iteration 105, loss = 0.01826932
Iteration 106, loss = 0.01811429
Iteration 107, loss = 0.01783980
Iteration 108, loss = 0.01767135
Iteration 109, loss = 0.01742773
Iteration 110, loss = 0.01713882
Iteration 111, loss = 0.01695917
Iteration 112, loss = 0.01674066
Iteration 113, loss = 0.01653786
Iteration 114, loss = 0.01637074
Iteration 115, loss = 0.01621000
Iteration 116, loss = 0.01600430
Iteration 117, loss = 0.01588391
Iteration 118, loss = 0.01569659
Iteration 119, loss = 0.01555134
Iteration 120, loss = 0.01537424
Iteration 121, loss = 0.01523683
Iteration 122, loss = 0.01508550
Iteration 123, loss = 0.01493273
Iteration 124, loss = 0.01481101
Iteration 125, loss = 0.01468905
Iteration 126, loss = 0.01455405
Iteration 127, loss = 0.01445853
Iteration 128, loss = 0.01430627
Iteration 129, loss = 0.01428445
Iteration 130, loss = 0.01411686
Iteration 131, loss = 0.01405320
Iteration 132, loss = 0.01387382
Iteration 133, loss = 0.01381529
Iteration 134, loss = 0.01362727
Iteration 135, loss = 0.01352742
Iteration 136, loss = 0.01343613
Iteration 137, loss = 0.01332629
Iteration 138, loss = 0.01325349
Iteration 139, loss = 0.01314572
Iteration 140, loss = 0.01310903
Iteration 141, loss = 0.01296869
Iteration 142, loss = 0.01290348
Iteration 143, loss = 0.01277300
Iteration 144, loss = 0.01268611
Iteration 145, loss = 0.01261425
Iteration 146, loss = 0.01252918
Iteration 147, loss = 0.01248854
Iteration 148, loss = 0.01235683
Iteration 149, loss = 0.01237060
Iteration 150, loss = 0.01228210
Iteration 151, loss = 0.01219201
Iteration 152, loss = 0.01213571
Iteration 153, loss = 0.01207606
Iteration 154, loss = 0.01200022
Iteration 155, loss = 0.01191619
Iteration 156, loss = 0.01185089
Iteration 157, loss = 0.01177548
Iteration 158, loss = 0.01168241
Iteration 159, loss = 0.01162394
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69201507
Iteration 2, loss = 0.67685190
Iteration 3, loss = 0.66409476
Iteration 4, loss = 0.65150010
Iteration 5, loss = 0.63832403
Iteration 6, loss = 0.62423854
Iteration 7, loss = 0.60898646
Iteration 8, loss = 0.59226060
Iteration 9, loss = 0.57418172
Iteration 10, loss = 0.55463770
Iteration 11, loss = 0.53386915
Iteration 12, loss = 0.51197491
Iteration 13, loss = 0.48937728
Iteration 14, loss = 0.46609268
Iteration 15, loss = 0.44268907
Iteration 16, loss = 0.41947816
Iteration 17, loss = 0.39644898
Iteration 18, loss = 0.37387515
Iteration 19, loss = 0.35208119
Iteration 20, loss = 0.33107485
Iteration 21, loss = 0.31095453
Iteration 22, loss = 0.29184880
Iteration 23, loss = 0.27376618
Iteration 24, loss = 0.25656531
Iteration 25, loss = 0.24041100
Iteration 26, loss = 0.22539378
Iteration 27, loss = 0.21123525
Iteration 28, loss = 0.19807688
Iteration 29, loss = 0.18583748
Iteration 30, loss = 0.17442811
Iteration 31, loss = 0.16380285
Iteration 32, loss = 0.15405254
Iteration 33, loss = 0.14492659
Iteration 34, loss = 0.13655407
Iteration 35, loss = 0.12875122
Iteration 36, loss = 0.12152556
Iteration 37, loss = 0.11496876
Iteration 38, loss = 0.10874176
Iteration 39, loss = 0.10305608
Iteration 40, loss = 0.09780810
Iteration 41, loss = 0.09292346
Iteration 42, loss = 0.08836062
Iteration 43, loss = 0.08419326
Iteration 44, loss = 0.08028540
Iteration 45, loss = 0.07660936
Iteration 46, loss = 0.07321238
Iteration 47, loss = 0.06998758
Iteration 48, loss = 0.06707337
Iteration 49, loss = 0.06430931
Iteration 50, loss = 0.06172922
Iteration 51, loss = 0.05931992
Iteration 52, loss = 0.05711290
Iteration 53, loss = 0.05493850
Iteration 54, loss = 0.05298616
Iteration 55, loss = 0.05105292
Iteration 56, loss = 0.04926023
Iteration 57, loss = 0.04761768
Iteration 58, loss = 0.04603201
Iteration 59, loss = 0.04454351
Iteration 60, loss = 0.04310374
Iteration 61, loss = 0.04179237
Iteration 62, loss = 0.04052812
Iteration 63, loss = 0.03936398
Iteration 64, loss = 0.03832414
Iteration 65, loss = 0.03725808
Iteration 66, loss = 0.03617234
Iteration 67, loss = 0.03519262
Iteration 68, loss = 0.03426900
Iteration 69, loss = 0.03341689
Iteration 70, loss = 0.03257141
Iteration 71, loss = 0.03183748
Iteration 72, loss = 0.03106502
Iteration 73, loss = 0.03035634
Iteration 74, loss = 0.02966589
Iteration 75, loss = 0.02901957
Iteration 76, loss = 0.02836562
Iteration 77, loss = 0.02769080
Iteration 78, loss = 0.02708123
Iteration 79, loss = 0.02652467
Iteration 80, loss = 0.02611461
Iteration 81, loss = 0.02553719
Iteration 82, loss = 0.02503945
Iteration 83, loss = 0.02457427
Iteration 84, loss = 0.02414498
Iteration 85, loss = 0.02370204
Iteration 86, loss = 0.02327424
Iteration 87, loss = 0.02285679
Iteration 88, loss = 0.02248704
Iteration 89, loss = 0.02209022
Iteration 90, loss = 0.02171445
Iteration 91, loss = 0.02141798
Iteration 92, loss = 0.02110314
Iteration 93, loss = 0.02082405
Iteration 94, loss = 0.02053077
Iteration 95, loss = 0.02020034
Iteration 96, loss = 0.01985647
Iteration 97, loss = 0.01952007
Iteration 98, loss = 0.01927959
Iteration 99, loss = 0.01895465
Iteration 100, loss = 0.01874229
Iteration 101, loss = 0.01845032
Iteration 102, loss = 0.01822745
Iteration 103, loss = 0.01800206
Iteration 104, loss = 0.01774880
Iteration 105, loss = 0.01752811
Iteration 106, loss = 0.01731070
Iteration 107, loss = 0.01711428
Iteration 108, loss = 0.01691051
Iteration 109, loss = 0.01669914
Iteration 110, loss = 0.01647364
Iteration 111, loss = 0.01628911
Iteration 112, loss = 0.01619645
Iteration 113, loss = 0.01594678
Iteration 114, loss = 0.01574808
Iteration 115, loss = 0.01556961
Iteration 116, loss = 0.01538213
Iteration 117, loss = 0.01542689
Iteration 118, loss = 0.01518346
Iteration 119, loss = 0.01508400
Iteration 120, loss = 0.01490456
Iteration 121, loss = 0.01479383
Iteration 122, loss = 0.01465407
Iteration 123, loss = 0.01453172
Iteration 124, loss = 0.01438120
Iteration 125, loss = 0.01421465
Iteration 126, loss = 0.01403909
Iteration 127, loss = 0.01396116
Iteration 128, loss = 0.01380333
Iteration 129, loss = 0.01369750
Iteration 130, loss = 0.01369745
Iteration 131, loss = 0.01353424
Iteration 132, loss = 0.01346558
Iteration 133, loss = 0.01333646
Iteration 134, loss = 0.01331301
Iteration 135, loss = 0.01316429
Iteration 136, loss = 0.01306411
Iteration 137, loss = 0.01297247
Iteration 138, loss = 0.01283237
Iteration 139, loss = 0.01270935
Iteration 140, loss = 0.01278547
Iteration 141, loss = 0.01264151
Iteration 142, loss = 0.01256772
Iteration 143, loss = 0.01245792
Iteration 144, loss = 0.01235949
Iteration 145, loss = 0.01227216
Iteration 146, loss = 0.01219838
Iteration 147, loss = 0.01217094
Iteration 148, loss = 0.01206672
Iteration 149, loss = 0.01198474
Iteration 150, loss = 0.01192514
Iteration 151, loss = 0.01186508
Iteration 152, loss = 0.01176053
Iteration 153, loss = 0.01172503
Iteration 154, loss = 0.01166207
Iteration 155, loss = 0.01175347
Iteration 156, loss = 0.01159779
Iteration 157, loss = 0.01150459
Iteration 158, loss = 0.01146613
Iteration 159, loss = 0.01131329
Iteration 160, loss = 0.01122858
Iteration 161, loss = 0.01118995
Iteration 162, loss = 0.01105432
Iteration 163, loss = 0.01109161
Iteration 164, loss = 0.01098914
Iteration 165, loss = 0.01096712
Iteration 166, loss = 0.01091341
Iteration 167, loss = 0.01088560
Iteration 168, loss = 0.01084024
Iteration 169, loss = 0.01080307
Iteration 170, loss = 0.01074037
Iteration 171, loss = 0.01070819
Iteration 172, loss = 0.01069431
Iteration 173, loss = 0.01063731
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67742565
Iteration 2, loss = 0.64932889
Iteration 3, loss = 0.61283879
Iteration 4, loss = 0.56419663
Iteration 5, loss = 0.50838082
Iteration 6, loss = 0.45145282
Iteration 7, loss = 0.39530082
Iteration 8, loss = 0.34224495
Iteration 9, loss = 0.29440835
Iteration 10, loss = 0.25264298
Iteration 11, loss = 0.21644817
Iteration 12, loss = 0.18628814
Iteration 13, loss = 0.16137549
Iteration 14, loss = 0.14073499
Iteration 15, loss = 0.12374199
Iteration 16, loss = 0.10973891
Iteration 17, loss = 0.09807065
Iteration 18, loss = 0.08846771
Iteration 19, loss = 0.08016259
Iteration 20, loss = 0.07332734
Iteration 21, loss = 0.06739311
Iteration 22, loss = 0.06246413
Iteration 23, loss = 0.05808531
Iteration 24, loss = 0.05421274
Iteration 25, loss = 0.05102087
Iteration 26, loss = 0.04813114
Iteration 27, loss = 0.04553640
Iteration 28, loss = 0.04322800
Iteration 29, loss = 0.04120024
Iteration 30, loss = 0.03928551
Iteration 31, loss = 0.03764547
Iteration 32, loss = 0.03624303
Iteration 33, loss = 0.03490318
Iteration 34, loss = 0.03371010
Iteration 35, loss = 0.03257493
Iteration 36, loss = 0.03167429
Iteration 37, loss = 0.03070109
Iteration 38, loss = 0.02982322
Iteration 39, loss = 0.02906597
Iteration 40, loss = 0.02837549
Iteration 41, loss = 0.02788578
Iteration 42, loss = 0.02709134
Iteration 43, loss = 0.02650538
Iteration 44, loss = 0.02597481
Iteration 45, loss = 0.02565472
Iteration 46, loss = 0.02510594
Iteration 47, loss = 0.02450476
Iteration 48, loss = 0.02423506
Iteration 49, loss = 0.02378506
Iteration 50, loss = 0.02347138
Iteration 51, loss = 0.02326990
Iteration 52, loss = 0.02298801
Iteration 53, loss = 0.02248423
Iteration 54, loss = 0.02226083
Iteration 55, loss = 0.02203202
Iteration 56, loss = 0.02170465
Iteration 57, loss = 0.02149611
Iteration 58, loss = 0.02131020
Iteration 59, loss = 0.02106847
Iteration 60, loss = 0.02081149
Iteration 61, loss = 0.02070216
Iteration 62, loss = 0.02076872
Iteration 63, loss = 0.02047908
Iteration 64, loss = 0.02022703
Iteration 65, loss = 0.02016571
Iteration 66, loss = 0.02005721
Iteration 67, loss = 0.01972301
Iteration 68, loss = 0.01965587
Iteration 69, loss = 0.01945502
Iteration 70, loss = 0.01938975
Iteration 71, loss = 0.01926095
Iteration 72, loss = 0.01919820
Iteration 73, loss = 0.01903762
Iteration 74, loss = 0.01904233
Iteration 75, loss = 0.01897560
Iteration 76, loss = 0.01879523
Iteration 77, loss = 0.01860186
Iteration 78, loss = 0.01861216
Iteration 79, loss = 0.01855849
Iteration 80, loss = 0.01851571
Iteration 81, loss = 0.01832105
Iteration 82, loss = 0.01820143
Iteration 83, loss = 0.01818291
Iteration 84, loss = 0.01814464
Iteration 85, loss = 0.01803623
Iteration 86, loss = 0.01800152
Iteration 87, loss = 0.01784669
Iteration 88, loss = 0.01782689
Iteration 89, loss = 0.01779245
Iteration 90, loss = 0.01779703
Iteration 91, loss = 0.01780358
Iteration 92, loss = 0.01778391
Iteration 93, loss = 0.01774211
Iteration 94, loss = 0.01760462
Iteration 95, loss = 0.01751975
Iteration 96, loss = 0.01740901
Iteration 97, loss = 0.01741948
Iteration 98, loss = 0.01745228
Iteration 99, loss = 0.01744229
Iteration 100, loss = 0.01720952
Iteration 101, loss = 0.01739806
Iteration 102, loss = 0.01728787
Iteration 103, loss = 0.01713199
Iteration 104, loss = 0.01718558
Iteration 105, loss = 0.01703919
Iteration 106, loss = 0.01708559
Iteration 107, loss = 0.01709065
Iteration 108, loss = 0.01698483
Iteration 109, loss = 0.01705847
Iteration 110, loss = 0.01709527
Iteration 111, loss = 0.01701520
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68226845
Iteration 2, loss = 0.62688276
Iteration 3, loss = 0.54903884
Iteration 4, loss = 0.45560430
Iteration 5, loss = 0.36695610
Iteration 6, loss = 0.29243681
Iteration 7, loss = 0.23385805
Iteration 8, loss = 0.18882843
Iteration 9, loss = 0.15522842
Iteration 10, loss = 0.12974989
Iteration 11, loss = 0.11054237
Iteration 12, loss = 0.09560334
Iteration 13, loss = 0.08390879
Iteration 14, loss = 0.07491237
Iteration 15, loss = 0.06749748
Iteration 16, loss = 0.06142783
Iteration 17, loss = 0.05648094
Iteration 18, loss = 0.05224724
Iteration 19, loss = 0.04886860
Iteration 20, loss = 0.04571986
Iteration 21, loss = 0.04324128
Iteration 22, loss = 0.04124239
Iteration 23, loss = 0.03896026
Iteration 24, loss = 0.03738119
Iteration 25, loss = 0.03596255
Iteration 26, loss = 0.03473478
Iteration 27, loss = 0.03367124
Iteration 28, loss = 0.03245560
Iteration 29, loss = 0.03147032
Iteration 30, loss = 0.03069867
Iteration 31, loss = 0.02988647
Iteration 32, loss = 0.02927707
Iteration 33, loss = 0.02864425
Iteration 34, loss = 0.02802580
Iteration 35, loss = 0.02760851
Iteration 36, loss = 0.02725265
Iteration 37, loss = 0.02652757
Iteration 38, loss = 0.02623360
Iteration 39, loss = 0.02584526
Iteration 40, loss = 0.02573288
Iteration 41, loss = 0.02545765
Iteration 42, loss = 0.02506368
Iteration 43, loss = 0.02508085
Iteration 44, loss = 0.02477002
Iteration 45, loss = 0.02427451
Iteration 46, loss = 0.02423405
Iteration 47, loss = 0.02392196
Iteration 48, loss = 0.02376229
Iteration 49, loss = 0.02349137
Iteration 50, loss = 0.02354619
Iteration 51, loss = 0.02324681
Iteration 52, loss = 0.02296827
Iteration 53, loss = 0.02300166
Iteration 54, loss = 0.02281478
Iteration 55, loss = 0.02265504
Iteration 56, loss = 0.02267084
Iteration 57, loss = 0.02269495
Iteration 58, loss = 0.02230362
Iteration 59, loss = 0.02228165
Iteration 60, loss = 0.02223331
Iteration 61, loss = 0.02203813
Iteration 62, loss = 0.02203037
Iteration 63, loss = 0.02208022
Iteration 64, loss = 0.02202795
Iteration 65, loss = 0.02162582
Iteration 66, loss = 0.02160361
Iteration 67, loss = 0.02160012
Iteration 68, loss = 0.02169185
Iteration 69, loss = 0.02169000
Iteration 70, loss = 0.02129810
Iteration 71, loss = 0.02135857
Iteration 72, loss = 0.02131421
Iteration 73, loss = 0.02140300
Iteration 74, loss = 0.02120799
Iteration 75, loss = 0.02123394
Iteration 76, loss = 0.02133177
Iteration 77, loss = 0.02140978
Iteration 78, loss = 0.02105132
Iteration 79, loss = 0.02114337
Iteration 80, loss = 0.02102290
Iteration 81, loss = 0.02096447
Iteration 82, loss = 0.02098941
Iteration 83, loss = 0.02086498
Iteration 84, loss = 0.02082754
Iteration 85, loss = 0.02089166
Iteration 86, loss = 0.02087151
Iteration 87, loss = 0.02084135
Iteration 88, loss = 0.02078768
Iteration 89, loss = 0.02070400
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69743138
Iteration 2, loss = 0.68206082
Iteration 3, loss = 0.66889306
Iteration 4, loss = 0.65583703
Iteration 5, loss = 0.64207349
Iteration 6, loss = 0.62674573
Iteration 7, loss = 0.61012790
Iteration 8, loss = 0.59169344
Iteration 9, loss = 0.57184226
Iteration 10, loss = 0.55039843
Iteration 11, loss = 0.52790120
Iteration 12, loss = 0.50429728
Iteration 13, loss = 0.48015484
Iteration 14, loss = 0.45600083
Iteration 15, loss = 0.43177030
Iteration 16, loss = 0.40800202
Iteration 17, loss = 0.38462115
Iteration 18, loss = 0.36211880
Iteration 19, loss = 0.34052974
Iteration 20, loss = 0.31979666
Iteration 21, loss = 0.30015461
Iteration 22, loss = 0.28138389
Iteration 23, loss = 0.26381201
Iteration 24, loss = 0.24732141
Iteration 25, loss = 0.23188519
Iteration 26, loss = 0.21745129
Iteration 27, loss = 0.20397409
Iteration 28, loss = 0.19154225
Iteration 29, loss = 0.17988268
Iteration 30, loss = 0.16908989
Iteration 31, loss = 0.15911786
Iteration 32, loss = 0.14978298
Iteration 33, loss = 0.14125526
Iteration 34, loss = 0.13337113
Iteration 35, loss = 0.12604302
Iteration 36, loss = 0.11926965
Iteration 37, loss = 0.11302325
Iteration 38, loss = 0.10727325
Iteration 39, loss = 0.10194390
Iteration 40, loss = 0.09693657
Iteration 41, loss = 0.09238782
Iteration 42, loss = 0.08807441
Iteration 43, loss = 0.08415070
Iteration 44, loss = 0.08039149
Iteration 45, loss = 0.07688877
Iteration 46, loss = 0.07368138
Iteration 47, loss = 0.07067238
Iteration 48, loss = 0.06782633
Iteration 49, loss = 0.06524439
Iteration 50, loss = 0.06281376
Iteration 51, loss = 0.06059198
Iteration 52, loss = 0.05839204
Iteration 53, loss = 0.05634854
Iteration 54, loss = 0.05454564
Iteration 55, loss = 0.05275006
Iteration 56, loss = 0.05111264
Iteration 57, loss = 0.04945363
Iteration 58, loss = 0.04802031
Iteration 59, loss = 0.04653568
Iteration 60, loss = 0.04518952
Iteration 61, loss = 0.04389538
Iteration 62, loss = 0.04269457
Iteration 63, loss = 0.04152021
Iteration 64, loss = 0.04049067
Iteration 65, loss = 0.03943650
Iteration 66, loss = 0.03849602
Iteration 67, loss = 0.03755110
Iteration 68, loss = 0.03666223
Iteration 69, loss = 0.03577484
Iteration 70, loss = 0.03497760
Iteration 71, loss = 0.03418761
Iteration 72, loss = 0.03350699
Iteration 73, loss = 0.03278450
Iteration 74, loss = 0.03204617
Iteration 75, loss = 0.03141974
Iteration 76, loss = 0.03080811
Iteration 77, loss = 0.03021973
Iteration 78, loss = 0.02969674
Iteration 79, loss = 0.02921442
Iteration 80, loss = 0.02875766
Iteration 81, loss = 0.02814862
Iteration 82, loss = 0.02767779
Iteration 83, loss = 0.02725615
Iteration 84, loss = 0.02681306
Iteration 85, loss = 0.02633132
Iteration 86, loss = 0.02593018
Iteration 87, loss = 0.02553079
Iteration 88, loss = 0.02515589
Iteration 89, loss = 0.02475787
Iteration 90, loss = 0.02441551
Iteration 91, loss = 0.02413138
Iteration 92, loss = 0.02375713
Iteration 93, loss = 0.02341225
Iteration 94, loss = 0.02308275
Iteration 95, loss = 0.02283119
Iteration 96, loss = 0.02255334
Iteration 97, loss = 0.02224247
Iteration 98, loss = 0.02198679
Iteration 99, loss = 0.02170830
Iteration 100, loss = 0.02147599
Iteration 101, loss = 0.02116042
Iteration 102, loss = 0.02097469
Iteration 103, loss = 0.02075638
Iteration 104, loss = 0.02042626
Iteration 105, loss = 0.02030973
Iteration 106, loss = 0.02007471
Iteration 107, loss = 0.01991556
Iteration 108, loss = 0.01967787
Iteration 109, loss = 0.01949506
Iteration 110, loss = 0.01937839
Iteration 111, loss = 0.01912725
Iteration 112, loss = 0.01896033
Iteration 113, loss = 0.01878597
Iteration 114, loss = 0.01864414
Iteration 115, loss = 0.01846264
Iteration 116, loss = 0.01825950
Iteration 117, loss = 0.01820110
Iteration 118, loss = 0.01800473
Iteration 119, loss = 0.01782049
Iteration 120, loss = 0.01774525
Iteration 121, loss = 0.01757049
Iteration 122, loss = 0.01739753
Iteration 123, loss = 0.01730327
Iteration 124, loss = 0.01715335
Iteration 125, loss = 0.01702304
Iteration 126, loss = 0.01689414
Iteration 127, loss = 0.01685691
Iteration 128, loss = 0.01675310
Iteration 129, loss = 0.01659737
Iteration 130, loss = 0.01652975
Iteration 131, loss = 0.01650142
Iteration 132, loss = 0.01631214
Iteration 133, loss = 0.01618615
Iteration 134, loss = 0.01607710
Iteration 135, loss = 0.01594803
Iteration 136, loss = 0.01588713
Iteration 137, loss = 0.01579950
Iteration 138, loss = 0.01565096
Iteration 139, loss = 0.01560037
Iteration 140, loss = 0.01555385
Iteration 141, loss = 0.01538603
Iteration 142, loss = 0.01532492
Iteration 143, loss = 0.01530557
Iteration 144, loss = 0.01510673
Iteration 145, loss = 0.01504803
Iteration 146, loss = 0.01497893
Iteration 147, loss = 0.01493329
Iteration 148, loss = 0.01488436
Iteration 149, loss = 0.01486603
Iteration 150, loss = 0.01476562
Iteration 151, loss = 0.01469529
Iteration 152, loss = 0.01460437
Iteration 153, loss = 0.01457060
Iteration 154, loss = 0.01452529
Iteration 155, loss = 0.01441752
Iteration 156, loss = 0.01432963
Iteration 157, loss = 0.01424602
Iteration 158, loss = 0.01417154
Iteration 159, loss = 0.01412225
Iteration 160, loss = 0.01408135
Iteration 161, loss = 0.01400890
Iteration 162, loss = 0.01396537
Iteration 163, loss = 0.01397810
Iteration 164, loss = 0.01393329
Iteration 165, loss = 0.01389361
Iteration 166, loss = 0.01382126
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69218276
Iteration 2, loss = 0.67141902
Iteration 3, loss = 0.65152691
Iteration 4, loss = 0.62850452
Iteration 5, loss = 0.60150565
Iteration 6, loss = 0.57061426
Iteration 7, loss = 0.53630352
Iteration 8, loss = 0.49959802
Iteration 9, loss = 0.46212920
Iteration 10, loss = 0.42492971
Iteration 11, loss = 0.38899729
Iteration 12, loss = 0.35494665
Iteration 13, loss = 0.32304635
Iteration 14, loss = 0.29358737
Iteration 15, loss = 0.26666259
Iteration 16, loss = 0.24222628
Iteration 17, loss = 0.22023386
Iteration 18, loss = 0.20044343
Iteration 19, loss = 0.18287742
Iteration 20, loss = 0.16706965
Iteration 21, loss = 0.15308824
Iteration 22, loss = 0.14079364
Iteration 23, loss = 0.12966270
Iteration 24, loss = 0.11988427
Iteration 25, loss = 0.11113756
Iteration 26, loss = 0.10329999
Iteration 27, loss = 0.09643077
Iteration 28, loss = 0.09020915
Iteration 29, loss = 0.08462900
Iteration 30, loss = 0.07962301
Iteration 31, loss = 0.07506441
Iteration 32, loss = 0.07093784
Iteration 33, loss = 0.06715058
Iteration 34, loss = 0.06379590
Iteration 35, loss = 0.06069107
Iteration 36, loss = 0.05791242
Iteration 37, loss = 0.05528856
Iteration 38, loss = 0.05295460
Iteration 39, loss = 0.05068401
Iteration 40, loss = 0.04865104
Iteration 41, loss = 0.04687087
Iteration 42, loss = 0.04511139
Iteration 43, loss = 0.04343894
Iteration 44, loss = 0.04208482
Iteration 45, loss = 0.04058686
Iteration 46, loss = 0.03932481
Iteration 47, loss = 0.03804282
Iteration 48, loss = 0.03696474
Iteration 49, loss = 0.03589662
Iteration 50, loss = 0.03491830
Iteration 51, loss = 0.03402589
Iteration 52, loss = 0.03310084
Iteration 53, loss = 0.03227196
Iteration 54, loss = 0.03146621
Iteration 55, loss = 0.03071057
Iteration 56, loss = 0.03003144
Iteration 57, loss = 0.02938498
Iteration 58, loss = 0.02874748
Iteration 59, loss = 0.02822509
Iteration 60, loss = 0.02761955
Iteration 61, loss = 0.02711444
Iteration 62, loss = 0.02656351
Iteration 63, loss = 0.02614083
Iteration 64, loss = 0.02567044
Iteration 65, loss = 0.02520817
Iteration 66, loss = 0.02478499
Iteration 67, loss = 0.02442284
Iteration 68, loss = 0.02399541
Iteration 69, loss = 0.02363369
Iteration 70, loss = 0.02326181
Iteration 71, loss = 0.02297344
Iteration 72, loss = 0.02264974
Iteration 73, loss = 0.02234835
Iteration 74, loss = 0.02205682
Iteration 75, loss = 0.02169855
Iteration 76, loss = 0.02147484
Iteration 77, loss = 0.02128276
Iteration 78, loss = 0.02099766
Iteration 79, loss = 0.02073382
Iteration 80, loss = 0.02056471
Iteration 81, loss = 0.02029822
Iteration 82, loss = 0.02011479
Iteration 83, loss = 0.01994000
Iteration 84, loss = 0.01965565
Iteration 85, loss = 0.01944415
Iteration 86, loss = 0.01931075
Iteration 87, loss = 0.01915532
Iteration 88, loss = 0.01903532
Iteration 89, loss = 0.01876612
Iteration 90, loss = 0.01865810
Iteration 91, loss = 0.01853879
Iteration 92, loss = 0.01832628
Iteration 93, loss = 0.01819408
Iteration 94, loss = 0.01802919
Iteration 95, loss = 0.01791392
Iteration 96, loss = 0.01778450
Iteration 97, loss = 0.01760942
Iteration 98, loss = 0.01755072
Iteration 99, loss = 0.01743717
Iteration 100, loss = 0.01733553
Iteration 101, loss = 0.01720353
Iteration 102, loss = 0.01711100
Iteration 103, loss = 0.01702935
Iteration 104, loss = 0.01680351
Iteration 105, loss = 0.01675733
Iteration 106, loss = 0.01669777
Iteration 107, loss = 0.01654613
Iteration 108, loss = 0.01647830
Iteration 109, loss = 0.01642908
Iteration 110, loss = 0.01626221
Iteration 111, loss = 0.01620730
Iteration 112, loss = 0.01618862
Iteration 113, loss = 0.01610456
Iteration 114, loss = 0.01597036
Iteration 115, loss = 0.01590381
Iteration 116, loss = 0.01588054
Iteration 117, loss = 0.01574038
Iteration 118, loss = 0.01572007
Iteration 119, loss = 0.01561157
Iteration 120, loss = 0.01556100
Iteration 121, loss = 0.01550086
Iteration 122, loss = 0.01542668
Iteration 123, loss = 0.01546346
Iteration 124, loss = 0.01527537
Iteration 125, loss = 0.01531962
Iteration 126, loss = 0.01517155
Iteration 127, loss = 0.01519037
Iteration 128, loss = 0.01511227
Iteration 129, loss = 0.01506126
Iteration 130, loss = 0.01504849
Iteration 131, loss = 0.01492641
Iteration 132, loss = 0.01491291
Iteration 133, loss = 0.01493906
Iteration 134, loss = 0.01485458
Iteration 135, loss = 0.01481262
Iteration 136, loss = 0.01474281
Iteration 137, loss = 0.01465210
Iteration 138, loss = 0.01461757
Iteration 139, loss = 0.01462788
Iteration 140, loss = 0.01468095
Iteration 141, loss = 0.01452551
Iteration 142, loss = 0.01449641
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67499308
Iteration 2, loss = 0.60420324
Iteration 3, loss = 0.49625393
Iteration 4, loss = 0.38526624
Iteration 5, loss = 0.29125186
Iteration 6, loss = 0.22052796
Iteration 7, loss = 0.17013597
Iteration 8, loss = 0.13537861
Iteration 9, loss = 0.11078684
Iteration 10, loss = 0.09353007
Iteration 11, loss = 0.08048973
Iteration 12, loss = 0.07093580
Iteration 13, loss = 0.06340488
Iteration 14, loss = 0.05755283
Iteration 15, loss = 0.05270258
Iteration 16, loss = 0.04915031
Iteration 17, loss = 0.04584384
Iteration 18, loss = 0.04342406
Iteration 19, loss = 0.04146800
Iteration 20, loss = 0.03962009
Iteration 21, loss = 0.03778861
Iteration 22, loss = 0.03639491
Iteration 23, loss = 0.03550211
Iteration 24, loss = 0.03432779
Iteration 25, loss = 0.03348245
Iteration 26, loss = 0.03256978
Iteration 27, loss = 0.03183680
Iteration 28, loss = 0.03140865
Iteration 29, loss = 0.03075283
Iteration 30, loss = 0.03044403
Iteration 31, loss = 0.02972575
Iteration 32, loss = 0.02930480
Iteration 33, loss = 0.02897114
Iteration 34, loss = 0.02886235
Iteration 35, loss = 0.02829401
Iteration 36, loss = 0.02816403
Iteration 37, loss = 0.02793596
Iteration 38, loss = 0.02774669
Iteration 39, loss = 0.02785375
Iteration 40, loss = 0.02713121
Iteration 41, loss = 0.02705852
Iteration 42, loss = 0.02708384
Iteration 43, loss = 0.02648850
Iteration 44, loss = 0.02656188
Iteration 45, loss = 0.02651307
Iteration 46, loss = 0.02637137
Iteration 47, loss = 0.02611293
Iteration 48, loss = 0.02617951
Iteration 49, loss = 0.02581096
Iteration 50, loss = 0.02571244
Iteration 51, loss = 0.02566290
Iteration 52, loss = 0.02547356
Iteration 53, loss = 0.02564351
Iteration 54, loss = 0.02572187
Iteration 55, loss = 0.02537763
Iteration 56, loss = 0.02558738
Iteration 57, loss = 0.02547245
Iteration 58, loss = 0.02520345
Iteration 59, loss = 0.02539464
Iteration 60, loss = 0.02518532
Iteration 61, loss = 0.02513062
Iteration 62, loss = 0.02548728
Iteration 63, loss = 0.02507330
Iteration 64, loss = 0.02518310
Iteration 65, loss = 0.02490475
Iteration 66, loss = 0.02485946
Iteration 67, loss = 0.02477919
Iteration 68, loss = 0.02472127
Iteration 69, loss = 0.02468504
Iteration 70, loss = 0.02464980
Iteration 71, loss = 0.02477481
Iteration 72, loss = 0.02452973
Iteration 73, loss = 0.02480314
Iteration 74, loss = 0.02487015
Iteration 75, loss = 0.02445894
Iteration 76, loss = 0.02452153
Iteration 77, loss = 0.02468328
Iteration 78, loss = 0.02459598
Iteration 79, loss = 0.02460184
Iteration 80, loss = 0.02436222
Iteration 81, loss = 0.02443247
Iteration 82, loss = 0.02455502
Iteration 83, loss = 0.02433352
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69523041
Iteration 2, loss = 0.64556953
Iteration 3, loss = 0.57598815
Iteration 4, loss = 0.48572068
Iteration 5, loss = 0.39536440
Iteration 6, loss = 0.31588177
Iteration 7, loss = 0.25113879
Iteration 8, loss = 0.20112178
Iteration 9, loss = 0.16288895
Iteration 10, loss = 0.13453802
Iteration 11, loss = 0.11286548
Iteration 12, loss = 0.09652957
Iteration 13, loss = 0.08388825
Iteration 14, loss = 0.07367638
Iteration 15, loss = 0.06571235
Iteration 16, loss = 0.05922966
Iteration 17, loss = 0.05386476
Iteration 18, loss = 0.04940036
Iteration 19, loss = 0.04551042
Iteration 20, loss = 0.04234572
Iteration 21, loss = 0.03998578
Iteration 22, loss = 0.03760332
Iteration 23, loss = 0.03547341
Iteration 24, loss = 0.03369372
Iteration 25, loss = 0.03204578
Iteration 26, loss = 0.03052329
Iteration 27, loss = 0.02946835
Iteration 28, loss = 0.02840516
Iteration 29, loss = 0.02735373
Iteration 30, loss = 0.02652965
Iteration 31, loss = 0.02573219
Iteration 32, loss = 0.02504309
Iteration 33, loss = 0.02437708
Iteration 34, loss = 0.02380238
Iteration 35, loss = 0.02337271
Iteration 36, loss = 0.02294770
Iteration 37, loss = 0.02234382
Iteration 38, loss = 0.02192953
Iteration 39, loss = 0.02166346
Iteration 40, loss = 0.02133818
Iteration 41, loss = 0.02106269
Iteration 42, loss = 0.02058849
Iteration 43, loss = 0.02029508
Iteration 44, loss = 0.02000835
Iteration 45, loss = 0.02005579
Iteration 46, loss = 0.01963707
Iteration 47, loss = 0.01930418
Iteration 48, loss = 0.01917198
Iteration 49, loss = 0.01897269
Iteration 50, loss = 0.01878586
Iteration 51, loss = 0.01868884
Iteration 52, loss = 0.01839622
Iteration 53, loss = 0.01840571
Iteration 54, loss = 0.01821973
Iteration 55, loss = 0.01816157
Iteration 56, loss = 0.01797377
Iteration 57, loss = 0.01786883
Iteration 58, loss = 0.01758996
Iteration 59, loss = 0.01766169
Iteration 60, loss = 0.01761272
Iteration 61, loss = 0.01759280
Iteration 62, loss = 0.01743627
Iteration 63, loss = 0.01740022
Iteration 64, loss = 0.01726848
Iteration 65, loss = 0.01695205
Iteration 66, loss = 0.01703009
Iteration 67, loss = 0.01704162
Iteration 68, loss = 0.01705014
Iteration 69, loss = 0.01703126
Iteration 70, loss = 0.01676605
Iteration 71, loss = 0.01662779
Iteration 72, loss = 0.01670714
Iteration 73, loss = 0.01652590
Iteration 74, loss = 0.01650742
Iteration 75, loss = 0.01656434
Iteration 76, loss = 0.01674588
Iteration 77, loss = 0.01646560
Iteration 78, loss = 0.01644814
Iteration 79, loss = 0.01628297
Iteration 80, loss = 0.01618380
Iteration 81, loss = 0.01612348
Iteration 82, loss = 0.01615786
Iteration 83, loss = 0.01611474
Iteration 84, loss = 0.01613150
Iteration 85, loss = 0.01610499
Iteration 86, loss = 0.01606935
Iteration 87, loss = 0.01619456
Iteration 88, loss = 0.01605117
Iteration 89, loss = 0.01620072
Iteration 90, loss = 0.01584098
Iteration 91, loss = 0.01582178
Iteration 92, loss = 0.01582544
Iteration 93, loss = 0.01571140
Iteration 94, loss = 0.01578374
Iteration 95, loss = 0.01569387
Iteration 96, loss = 0.01562238
Iteration 97, loss = 0.01571259
Iteration 98, loss = 0.01557732
Iteration 99, loss = 0.01567125
Iteration 100, loss = 0.01558737
Iteration 101, loss = 0.01567625
Iteration 102, loss = 0.01556838
Iteration 103, loss = 0.01561477
Iteration 104, loss = 0.01547398
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68441217
Iteration 2, loss = 0.61570533
Iteration 3, loss = 0.51047754
Iteration 4, loss = 0.39772273
Iteration 5, loss = 0.30042581
Iteration 6, loss = 0.22603164
Iteration 7, loss = 0.17314156
Iteration 8, loss = 0.13670285
Iteration 9, loss = 0.11141591
Iteration 10, loss = 0.09344384
Iteration 11, loss = 0.08034027
Iteration 12, loss = 0.07010822
Iteration 13, loss = 0.06272036
Iteration 14, loss = 0.05699403
Iteration 15, loss = 0.05231077
Iteration 16, loss = 0.04817383
Iteration 17, loss = 0.04533950
Iteration 18, loss = 0.04261545
Iteration 19, loss = 0.04057774
Iteration 20, loss = 0.03900289
Iteration 21, loss = 0.03721754
Iteration 22, loss = 0.03575939
Iteration 23, loss = 0.03460635
Iteration 24, loss = 0.03348998
Iteration 25, loss = 0.03259995
Iteration 26, loss = 0.03172561
Iteration 27, loss = 0.03110218
Iteration 28, loss = 0.03054906
Iteration 29, loss = 0.03005285
Iteration 30, loss = 0.02954616
Iteration 31, loss = 0.02918887
Iteration 32, loss = 0.02864198
Iteration 33, loss = 0.02822768
Iteration 34, loss = 0.02794751
Iteration 35, loss = 0.02747889
Iteration 36, loss = 0.02727879
Iteration 37, loss = 0.02698339
Iteration 38, loss = 0.02693098
Iteration 39, loss = 0.02665579
Iteration 40, loss = 0.02658516
Iteration 41, loss = 0.02626905
Iteration 42, loss = 0.02620335
Iteration 43, loss = 0.02603712
Iteration 44, loss = 0.02588494
Iteration 45, loss = 0.02575265
Iteration 46, loss = 0.02528053
Iteration 47, loss = 0.02528148
Iteration 48, loss = 0.02535665
Iteration 49, loss = 0.02511594
Iteration 50, loss = 0.02525254
Iteration 51, loss = 0.02490748
Iteration 52, loss = 0.02503837
Iteration 53, loss = 0.02483557
Iteration 54, loss = 0.02468256
Iteration 55, loss = 0.02465114
Iteration 56, loss = 0.02471535
Iteration 57, loss = 0.02448523
Iteration 58, loss = 0.02436779
Iteration 59, loss = 0.02434361
Iteration 60, loss = 0.02431282
Iteration 61, loss = 0.02442215
Iteration 62, loss = 0.02437556
Iteration 63, loss = 0.02421349
Iteration 64, loss = 0.02413981
Iteration 65, loss = 0.02426928
Iteration 66, loss = 0.02433246
Iteration 67, loss = 0.02439272
Iteration 68, loss = 0.02406937
Iteration 69, loss = 0.02417778
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67797423
Iteration 2, loss = 0.61838617
Iteration 3, loss = 0.53264477
Iteration 4, loss = 0.43312084
Iteration 5, loss = 0.34304313
Iteration 6, loss = 0.26856030
Iteration 7, loss = 0.21098317
Iteration 8, loss = 0.16857583
Iteration 9, loss = 0.13710255
Iteration 10, loss = 0.11394443
Iteration 11, loss = 0.09632434
Iteration 12, loss = 0.08332195
Iteration 13, loss = 0.07292035
Iteration 14, loss = 0.06503742
Iteration 15, loss = 0.05836200
Iteration 16, loss = 0.05293979
Iteration 17, loss = 0.04873416
Iteration 18, loss = 0.04482179
Iteration 19, loss = 0.04195577
Iteration 20, loss = 0.03949566
Iteration 21, loss = 0.03715043
Iteration 22, loss = 0.03542602
Iteration 23, loss = 0.03366631
Iteration 24, loss = 0.03235278
Iteration 25, loss = 0.03093032
Iteration 26, loss = 0.02968455
Iteration 27, loss = 0.02891591
Iteration 28, loss = 0.02802878
Iteration 29, loss = 0.02714193
Iteration 30, loss = 0.02629294
Iteration 31, loss = 0.02568599
Iteration 32, loss = 0.02516428
Iteration 33, loss = 0.02462968
Iteration 34, loss = 0.02427760
Iteration 35, loss = 0.02359305
Iteration 36, loss = 0.02321311
Iteration 37, loss = 0.02298742
Iteration 38, loss = 0.02267838
Iteration 39, loss = 0.02224694
Iteration 40, loss = 0.02193068
Iteration 41, loss = 0.02174850
Iteration 42, loss = 0.02164989
Iteration 43, loss = 0.02128951
Iteration 44, loss = 0.02098542
Iteration 45, loss = 0.02066223
Iteration 46, loss = 0.02067661
Iteration 47, loss = 0.02042020
Iteration 48, loss = 0.02016277
Iteration 49, loss = 0.02020577
Iteration 50, loss = 0.02005208
Iteration 51, loss = 0.01989460
Iteration 52, loss = 0.01972702
Iteration 53, loss = 0.01950235
Iteration 54, loss = 0.01965956
Iteration 55, loss = 0.01917834
Iteration 56, loss = 0.01923801
Iteration 57, loss = 0.01909408
Iteration 58, loss = 0.01930986
Iteration 59, loss = 0.01917626
Iteration 60, loss = 0.01883464
Iteration 61, loss = 0.01901433
Iteration 62, loss = 0.01863746
Iteration 63, loss = 0.01871013
Iteration 64, loss = 0.01858702
Iteration 65, loss = 0.01846241
Iteration 66, loss = 0.01858195
Iteration 67, loss = 0.01854958
Iteration 68, loss = 0.01849594
Iteration 69, loss = 0.01812178
Iteration 70, loss = 0.01810146
Iteration 71, loss = 0.01829194
Iteration 72, loss = 0.01829466
Iteration 73, loss = 0.01800869
Iteration 74, loss = 0.01790738
Iteration 75, loss = 0.01809629
Iteration 76, loss = 0.01789004
Iteration 77, loss = 0.01789918
Iteration 78, loss = 0.01785455
Iteration 79, loss = 0.01771014
Iteration 80, loss = 0.01767554
Iteration 81, loss = 0.01785893
Iteration 82, loss = 0.01779350
Iteration 83, loss = 0.01779595
Iteration 84, loss = 0.01778647
Iteration 85, loss = 0.01769157
Iteration 86, loss = 0.01757757
Iteration 87, loss = 0.01744385
Iteration 88, loss = 0.01752878
Iteration 89, loss = 0.01745669
Iteration 90, loss = 0.01735838
Iteration 91, loss = 0.01746338
Iteration 92, loss = 0.01750002
Iteration 93, loss = 0.01743176
Iteration 94, loss = 0.01744266
Iteration 95, loss = 0.01724535
Iteration 96, loss = 0.01734808
Iteration 97, loss = 0.01741511
Iteration 98, loss = 0.01727180
Iteration 99, loss = 0.01720384
Iteration 100, loss = 0.01726319
Iteration 101, loss = 0.01721699
Iteration 102, loss = 0.01720224
Iteration 103, loss = 0.01734785
Iteration 104, loss = 0.01729096
Iteration 105, loss = 0.01715247
Iteration 106, loss = 0.01706538
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67949180
Iteration 2, loss = 0.63572424
Iteration 3, loss = 0.56436303
Iteration 4, loss = 0.47868379
Iteration 5, loss = 0.39306172
Iteration 6, loss = 0.31609028
Iteration 7, loss = 0.25240586
Iteration 8, loss = 0.20214694
Iteration 9, loss = 0.16428846
Iteration 10, loss = 0.13556757
Iteration 11, loss = 0.11429830
Iteration 12, loss = 0.09797368
Iteration 13, loss = 0.08529460
Iteration 14, loss = 0.07540334
Iteration 15, loss = 0.06755904
Iteration 16, loss = 0.06114583
Iteration 17, loss = 0.05583415
Iteration 18, loss = 0.05149543
Iteration 19, loss = 0.04789641
Iteration 20, loss = 0.04477507
Iteration 21, loss = 0.04217292
Iteration 22, loss = 0.03994184
Iteration 23, loss = 0.03809361
Iteration 24, loss = 0.03620573
Iteration 25, loss = 0.03470707
Iteration 26, loss = 0.03358224
Iteration 27, loss = 0.03229763
Iteration 28, loss = 0.03125585
Iteration 29, loss = 0.03030676
Iteration 30, loss = 0.02958195
Iteration 31, loss = 0.02868968
Iteration 32, loss = 0.02816230
Iteration 33, loss = 0.02752300
Iteration 34, loss = 0.02687672
Iteration 35, loss = 0.02637574
Iteration 36, loss = 0.02599248
Iteration 37, loss = 0.02555701
Iteration 38, loss = 0.02493225
Iteration 39, loss = 0.02474588
Iteration 40, loss = 0.02447290
Iteration 41, loss = 0.02406247
Iteration 42, loss = 0.02373419
Iteration 43, loss = 0.02352095
Iteration 44, loss = 0.02326060
Iteration 45, loss = 0.02297717
Iteration 46, loss = 0.02279235
Iteration 47, loss = 0.02264991
Iteration 48, loss = 0.02256087
Iteration 49, loss = 0.02232341
Iteration 50, loss = 0.02208112
Iteration 51, loss = 0.02194096
Iteration 52, loss = 0.02190000
Iteration 53, loss = 0.02179010
Iteration 54, loss = 0.02148581
Iteration 55, loss = 0.02143267
Iteration 56, loss = 0.02132292
Iteration 57, loss = 0.02128649
Iteration 58, loss = 0.02116200
Iteration 59, loss = 0.02096604
Iteration 60, loss = 0.02102257
Iteration 61, loss = 0.02094878
Iteration 62, loss = 0.02065411
Iteration 63, loss = 0.02086063
Iteration 64, loss = 0.02065476
Iteration 65, loss = 0.02050422
Iteration 66, loss = 0.02038616
Iteration 67, loss = 0.02060207
Iteration 68, loss = 0.02031618
Iteration 69, loss = 0.02039079
Iteration 70, loss = 0.02026310
Iteration 71, loss = 0.02008906
Iteration 72, loss = 0.02029856
Iteration 73, loss = 0.02011621
Iteration 74, loss = 0.01981513
Iteration 75, loss = 0.01996627
Iteration 76, loss = 0.01991642
Iteration 77, loss = 0.01999509
Iteration 78, loss = 0.01975744
Iteration 79, loss = 0.01980740
Iteration 80, loss = 0.01969802
Iteration 81, loss = 0.01980964
Iteration 82, loss = 0.01966968
Iteration 83, loss = 0.01971416
Iteration 84, loss = 0.01960344
Iteration 85, loss = 0.01955736
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67819568
Iteration 2, loss = 0.63630064
Iteration 3, loss = 0.57277551
Iteration 4, loss = 0.49529251
Iteration 5, loss = 0.41634344
Iteration 6, loss = 0.34303162
Iteration 7, loss = 0.28021309
Iteration 8, loss = 0.22913815
Iteration 9, loss = 0.18888744
Iteration 10, loss = 0.15759365
Iteration 11, loss = 0.13333460
Iteration 12, loss = 0.11466449
Iteration 13, loss = 0.09997642
Iteration 14, loss = 0.08828858
Iteration 15, loss = 0.07898713
Iteration 16, loss = 0.07129463
Iteration 17, loss = 0.06504212
Iteration 18, loss = 0.05998156
Iteration 19, loss = 0.05555222
Iteration 20, loss = 0.05194423
Iteration 21, loss = 0.04874895
Iteration 22, loss = 0.04594772
Iteration 23, loss = 0.04353587
Iteration 24, loss = 0.04154454
Iteration 25, loss = 0.03987329
Iteration 26, loss = 0.03819195
Iteration 27, loss = 0.03671179
Iteration 28, loss = 0.03544219
Iteration 29, loss = 0.03420951
Iteration 30, loss = 0.03335966
Iteration 31, loss = 0.03237039
Iteration 32, loss = 0.03171590
Iteration 33, loss = 0.03107099
Iteration 34, loss = 0.03025385
Iteration 35, loss = 0.02974616
Iteration 36, loss = 0.02924519
Iteration 37, loss = 0.02853128
Iteration 38, loss = 0.02802488
Iteration 39, loss = 0.02771094
Iteration 40, loss = 0.02739035
Iteration 41, loss = 0.02705993
Iteration 42, loss = 0.02634590
Iteration 43, loss = 0.02627406
Iteration 44, loss = 0.02587455
Iteration 45, loss = 0.02563232
Iteration 46, loss = 0.02539734
Iteration 47, loss = 0.02517505
Iteration 48, loss = 0.02497326
Iteration 49, loss = 0.02478456
Iteration 50, loss = 0.02471418
Iteration 51, loss = 0.02444613
Iteration 52, loss = 0.02439342
Iteration 53, loss = 0.02395910
Iteration 54, loss = 0.02397697
Iteration 55, loss = 0.02383787
Iteration 56, loss = 0.02359264
Iteration 57, loss = 0.02396457
Iteration 58, loss = 0.02367704
Iteration 59, loss = 0.02330855
Iteration 60, loss = 0.02331067
Iteration 61, loss = 0.02304068
Iteration 62, loss = 0.02340822
Iteration 63, loss = 0.02306280
Iteration 64, loss = 0.02277824
Iteration 65, loss = 0.02266685
Iteration 66, loss = 0.02260573
Iteration 67, loss = 0.02256730
Iteration 68, loss = 0.02266434
Iteration 69, loss = 0.02258017
Iteration 70, loss = 0.02278150
Iteration 71, loss = 0.02235483
Iteration 72, loss = 0.02223956
Iteration 73, loss = 0.02223873
Iteration 74, loss = 0.02194682
Iteration 75, loss = 0.02191434
Iteration 76, loss = 0.02209651
Iteration 77, loss = 0.02187695
Iteration 78, loss = 0.02194898
Iteration 79, loss = 0.02183952
Iteration 80, loss = 0.02184760
Iteration 81, loss = 0.02173215
Iteration 82, loss = 0.02181301
Iteration 83, loss = 0.02169569
Iteration 84, loss = 0.02168425
Iteration 85, loss = 0.02199766
Iteration 86, loss = 0.02134521
Iteration 87, loss = 0.02149743
Iteration 88, loss = 0.02149472
Iteration 89, loss = 0.02153117
Iteration 90, loss = 0.02138856
Iteration 91, loss = 0.02121986
Iteration 92, loss = 0.02146717
Iteration 93, loss = 0.02138000
Iteration 94, loss = 0.02180134
Iteration 95, loss = 0.02170010
Iteration 96, loss = 0.02150876
Iteration 97, loss = 0.02106690
Iteration 98, loss = 0.02106219
Iteration 99, loss = 0.02135288
Iteration 100, loss = 0.02127559
Iteration 101, loss = 0.02119545
Iteration 102, loss = 0.02128569
Iteration 103, loss = 0.02090566
Iteration 104, loss = 0.02113432
Iteration 105, loss = 0.02095943
Iteration 106, loss = 0.02084984
Iteration 107, loss = 0.02092441
Iteration 108, loss = 0.02132819
Iteration 109, loss = 0.02140036
Iteration 110, loss = 0.02117576
Iteration 111, loss = 0.02086454
Iteration 112, loss = 0.02141715
Iteration 113, loss = 0.02133137
Iteration 114, loss = 0.02099357
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67675394
Iteration 2, loss = 0.64790032
Iteration 3, loss = 0.61098259
Iteration 4, loss = 0.56164402
Iteration 5, loss = 0.50417000
Iteration 6, loss = 0.44575284
Iteration 7, loss = 0.38951259
Iteration 8, loss = 0.33720495
Iteration 9, loss = 0.29032091
Iteration 10, loss = 0.24951227
Iteration 11, loss = 0.21448327
Iteration 12, loss = 0.18505519
Iteration 13, loss = 0.16084343
Iteration 14, loss = 0.14040211
Iteration 15, loss = 0.12370759
Iteration 16, loss = 0.10988433
Iteration 17, loss = 0.09833966
Iteration 18, loss = 0.08857422
Iteration 19, loss = 0.08052560
Iteration 20, loss = 0.07352222
Iteration 21, loss = 0.06751335
Iteration 22, loss = 0.06241349
Iteration 23, loss = 0.05813128
Iteration 24, loss = 0.05421555
Iteration 25, loss = 0.05090794
Iteration 26, loss = 0.04791188
Iteration 27, loss = 0.04530606
Iteration 28, loss = 0.04310172
Iteration 29, loss = 0.04085286
Iteration 30, loss = 0.03914493
Iteration 31, loss = 0.03738735
Iteration 32, loss = 0.03592587
Iteration 33, loss = 0.03452315
Iteration 34, loss = 0.03330594
Iteration 35, loss = 0.03222404
Iteration 36, loss = 0.03123007
Iteration 37, loss = 0.03037262
Iteration 38, loss = 0.02938134
Iteration 39, loss = 0.02850003
Iteration 40, loss = 0.02770416
Iteration 41, loss = 0.02713636
Iteration 42, loss = 0.02651182
Iteration 43, loss = 0.02593765
Iteration 44, loss = 0.02539104
Iteration 45, loss = 0.02477330
Iteration 46, loss = 0.02433568
Iteration 47, loss = 0.02392640
Iteration 48, loss = 0.02343822
Iteration 49, loss = 0.02310660
Iteration 50, loss = 0.02274673
Iteration 51, loss = 0.02243534
Iteration 52, loss = 0.02207558
Iteration 53, loss = 0.02185033
Iteration 54, loss = 0.02145781
Iteration 55, loss = 0.02126030
Iteration 56, loss = 0.02097014
Iteration 57, loss = 0.02074172
Iteration 58, loss = 0.02053531
Iteration 59, loss = 0.02035482
Iteration 60, loss = 0.02019767
Iteration 61, loss = 0.01983399
Iteration 62, loss = 0.01966306
Iteration 63, loss = 0.01949846
Iteration 64, loss = 0.01952976
Iteration 65, loss = 0.01928846
Iteration 66, loss = 0.01903889
Iteration 67, loss = 0.01893400
Iteration 68, loss = 0.01889841
Iteration 69, loss = 0.01870879
Iteration 70, loss = 0.01857629
Iteration 71, loss = 0.01840574
Iteration 72, loss = 0.01834145
Iteration 73, loss = 0.01831358
Iteration 74, loss = 0.01812577
Iteration 75, loss = 0.01805621
Iteration 76, loss = 0.01798511
Iteration 77, loss = 0.01791582
Iteration 78, loss = 0.01773579
Iteration 79, loss = 0.01755037
Iteration 80, loss = 0.01751411
Iteration 81, loss = 0.01742811
Iteration 82, loss = 0.01738257
Iteration 83, loss = 0.01737833
Iteration 84, loss = 0.01722118
Iteration 85, loss = 0.01725824
Iteration 86, loss = 0.01700857
Iteration 87, loss = 0.01705523
Iteration 88, loss = 0.01703171
Iteration 89, loss = 0.01698876
Iteration 90, loss = 0.01694252
Iteration 91, loss = 0.01689635
Iteration 92, loss = 0.01677882
Iteration 93, loss = 0.01670014
Iteration 94, loss = 0.01673701
Iteration 95, loss = 0.01663056
Iteration 96, loss = 0.01652187
Iteration 97, loss = 0.01659586
Iteration 98, loss = 0.01641442
Iteration 99, loss = 0.01663350
Iteration 100, loss = 0.01642556
Iteration 101, loss = 0.01634304
Iteration 102, loss = 0.01641034
Iteration 103, loss = 0.01629360
Iteration 104, loss = 0.01628152
Iteration 105, loss = 0.01618957
Iteration 106, loss = 0.01632438
Iteration 107, loss = 0.01639463
Iteration 108, loss = 0.01622724
Iteration 109, loss = 0.01601739
Iteration 110, loss = 0.01608118
Iteration 111, loss = 0.01615078
Iteration 112, loss = 0.01596261
Iteration 113, loss = 0.01588779
Iteration 114, loss = 0.01604451
Iteration 115, loss = 0.01598136
Iteration 116, loss = 0.01593735
Iteration 117, loss = 0.01585528
Iteration 118, loss = 0.01581080
Iteration 119, loss = 0.01586741
Iteration 120, loss = 0.01584825
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68682364
Iteration 2, loss = 0.62959295
Iteration 3, loss = 0.54464850
Iteration 4, loss = 0.44728106
Iteration 5, loss = 0.35592284
Iteration 6, loss = 0.27919019
Iteration 7, loss = 0.21962876
Iteration 8, loss = 0.17517377
Iteration 9, loss = 0.14255911
Iteration 10, loss = 0.11888964
Iteration 11, loss = 0.10069712
Iteration 12, loss = 0.08737363
Iteration 13, loss = 0.07683466
Iteration 14, loss = 0.06872421
Iteration 15, loss = 0.06217218
Iteration 16, loss = 0.05698522
Iteration 17, loss = 0.05252778
Iteration 18, loss = 0.04903274
Iteration 19, loss = 0.04581371
Iteration 20, loss = 0.04329693
Iteration 21, loss = 0.04143855
Iteration 22, loss = 0.03940443
Iteration 23, loss = 0.03769580
Iteration 24, loss = 0.03639493
Iteration 25, loss = 0.03518587
Iteration 26, loss = 0.03384577
Iteration 27, loss = 0.03319704
Iteration 28, loss = 0.03219767
Iteration 29, loss = 0.03150384
Iteration 30, loss = 0.03048757
Iteration 31, loss = 0.03019057
Iteration 32, loss = 0.02955616
Iteration 33, loss = 0.02916073
Iteration 34, loss = 0.02875984
Iteration 35, loss = 0.02816820
Iteration 36, loss = 0.02764268
Iteration 37, loss = 0.02762531
Iteration 38, loss = 0.02708239
Iteration 39, loss = 0.02710064
Iteration 40, loss = 0.02668308
Iteration 41, loss = 0.02643465
Iteration 42, loss = 0.02622020
Iteration 43, loss = 0.02595206
Iteration 44, loss = 0.02570380
Iteration 45, loss = 0.02542506
Iteration 46, loss = 0.02530642
Iteration 47, loss = 0.02525628
Iteration 48, loss = 0.02487937
Iteration 49, loss = 0.02489398
Iteration 50, loss = 0.02478202
Iteration 51, loss = 0.02470285
Iteration 52, loss = 0.02463588
Iteration 53, loss = 0.02463720
Iteration 54, loss = 0.02451309
Iteration 55, loss = 0.02395639
Iteration 56, loss = 0.02430837
Iteration 57, loss = 0.02409004
Iteration 58, loss = 0.02404602
Iteration 59, loss = 0.02381185
Iteration 60, loss = 0.02384452
Iteration 61, loss = 0.02392357
Iteration 62, loss = 0.02376323
Iteration 63, loss = 0.02353090
Iteration 64, loss = 0.02356118
Iteration 65, loss = 0.02360639
Iteration 66, loss = 0.02358070
Iteration 67, loss = 0.02335615
Iteration 68, loss = 0.02344826
Iteration 69, loss = 0.02325364
Iteration 70, loss = 0.02297316
Iteration 71, loss = 0.02321656
Iteration 72, loss = 0.02313896
Iteration 73, loss = 0.02312276
Iteration 74, loss = 0.02336406
Iteration 75, loss = 0.02296485
Iteration 76, loss = 0.02320763
Iteration 77, loss = 0.02266054
Iteration 78, loss = 0.02288408
Iteration 79, loss = 0.02271216
Iteration 80, loss = 0.02275991
Iteration 81, loss = 0.02272792
Iteration 82, loss = 0.02298883
Iteration 83, loss = 0.02294651
Iteration 84, loss = 0.02279462
Iteration 85, loss = 0.02298939
Iteration 86, loss = 0.02275654
Iteration 87, loss = 0.02287142
Iteration 88, loss = 0.02266350
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72585790
Iteration 2, loss = 0.70653400
Iteration 3, loss = 0.69005285
Iteration 4, loss = 0.67421211
Iteration 5, loss = 0.65792545
Iteration 6, loss = 0.64058625
Iteration 7, loss = 0.62193054
Iteration 8, loss = 0.60194454
Iteration 9, loss = 0.58065557
Iteration 10, loss = 0.55798818
Iteration 11, loss = 0.53467066
Iteration 12, loss = 0.50995152
Iteration 13, loss = 0.48506643
Iteration 14, loss = 0.45967924
Iteration 15, loss = 0.43452287
Iteration 16, loss = 0.40971798
Iteration 17, loss = 0.38540101
Iteration 18, loss = 0.36191389
Iteration 19, loss = 0.33937493
Iteration 20, loss = 0.31780478
Iteration 21, loss = 0.29732789
Iteration 22, loss = 0.27805359
Iteration 23, loss = 0.25987698
Iteration 24, loss = 0.24281230
Iteration 25, loss = 0.22686252
Iteration 26, loss = 0.21200952
Iteration 27, loss = 0.19814468
Iteration 28, loss = 0.18530305
Iteration 29, loss = 0.17340129
Iteration 30, loss = 0.16233949
Iteration 31, loss = 0.15213120
Iteration 32, loss = 0.14271214
Iteration 33, loss = 0.13395429
Iteration 34, loss = 0.12589150
Iteration 35, loss = 0.11843695
Iteration 36, loss = 0.11155381
Iteration 37, loss = 0.10521735
Iteration 38, loss = 0.09929130
Iteration 39, loss = 0.09390441
Iteration 40, loss = 0.08882928
Iteration 41, loss = 0.08417603
Iteration 42, loss = 0.07981964
Iteration 43, loss = 0.07581665
Iteration 44, loss = 0.07208355
Iteration 45, loss = 0.06862640
Iteration 46, loss = 0.06536415
Iteration 47, loss = 0.06235774
Iteration 48, loss = 0.05954070
Iteration 49, loss = 0.05690582
Iteration 50, loss = 0.05444837
Iteration 51, loss = 0.05212575
Iteration 52, loss = 0.04998448
Iteration 53, loss = 0.04793559
Iteration 54, loss = 0.04603610
Iteration 55, loss = 0.04423287
Iteration 56, loss = 0.04255574
Iteration 57, loss = 0.04095055
Iteration 58, loss = 0.03945822
Iteration 59, loss = 0.03803201
Iteration 60, loss = 0.03669291
Iteration 61, loss = 0.03541032
Iteration 62, loss = 0.03422052
Iteration 63, loss = 0.03305869
Iteration 64, loss = 0.03197616
Iteration 65, loss = 0.03094291
Iteration 66, loss = 0.02995631
Iteration 67, loss = 0.02902291
Iteration 68, loss = 0.02813943
Iteration 69, loss = 0.02728265
Iteration 70, loss = 0.02646782
Iteration 71, loss = 0.02569637
Iteration 72, loss = 0.02495362
Iteration 73, loss = 0.02425163
Iteration 74, loss = 0.02356321
Iteration 75, loss = 0.02292202
Iteration 76, loss = 0.02229497
Iteration 77, loss = 0.02170570
Iteration 78, loss = 0.02112950
Iteration 79, loss = 0.02058385
Iteration 80, loss = 0.02005352
Iteration 81, loss = 0.01955094
Iteration 82, loss = 0.01906050
Iteration 83, loss = 0.01859226
Iteration 84, loss = 0.01813841
Iteration 85, loss = 0.01770624
Iteration 86, loss = 0.01728013
Iteration 87, loss = 0.01688065
Iteration 88, loss = 0.01649203
Iteration 89, loss = 0.01611970
Iteration 90, loss = 0.01576015
Iteration 91, loss = 0.01541603
Iteration 92, loss = 0.01508172
Iteration 93, loss = 0.01475359
Iteration 94, loss = 0.01444311
Iteration 95, loss = 0.01414121
Iteration 96, loss = 0.01384874
Iteration 97, loss = 0.01356796
Iteration 98, loss = 0.01329401
Iteration 99, loss = 0.01302903
Iteration 100, loss = 0.01277658
Iteration 101, loss = 0.01253012
Iteration 102, loss = 0.01228966
Iteration 103, loss = 0.01205704
Iteration 104, loss = 0.01183231
Iteration 105, loss = 0.01161406
Iteration 106, loss = 0.01140257
Iteration 107, loss = 0.01119759
Iteration 108, loss = 0.01099702
Iteration 109, loss = 0.01080550
Iteration 110, loss = 0.01061988
Iteration 111, loss = 0.01043696
Iteration 112, loss = 0.01025684
Iteration 113, loss = 0.01008624
Iteration 114, loss = 0.00991610
Iteration 115, loss = 0.00975109
Iteration 116, loss = 0.00959346
Iteration 117, loss = 0.00943885
Iteration 118, loss = 0.00928679
Iteration 119, loss = 0.00913917
Iteration 120, loss = 0.00899406
Iteration 121, loss = 0.00885348
Iteration 122, loss = 0.00871580
Iteration 123, loss = 0.00858487
Iteration 124, loss = 0.00845313
Iteration 125, loss = 0.00832818
Iteration 126, loss = 0.00820502
Iteration 127, loss = 0.00808540
Iteration 128, loss = 0.00796890
Iteration 129, loss = 0.00785667
Iteration 130, loss = 0.00774701
Iteration 131, loss = 0.00763764
Iteration 132, loss = 0.00753218
Iteration 133, loss = 0.00742759
Iteration 134, loss = 0.00732843
Iteration 135, loss = 0.00722848
Iteration 136, loss = 0.00713306
Iteration 137, loss = 0.00703864
Iteration 138, loss = 0.00694693
Iteration 139, loss = 0.00685640
Iteration 140, loss = 0.00676927
Iteration 141, loss = 0.00668333
Iteration 142, loss = 0.00660079
Iteration 143, loss = 0.00651844
Iteration 144, loss = 0.00643810
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68225879
Iteration 2, loss = 0.66393381
Iteration 3, loss = 0.64525890
Iteration 4, loss = 0.62315274
Iteration 5, loss = 0.59693289
Iteration 6, loss = 0.56647848
Iteration 7, loss = 0.53249252
Iteration 8, loss = 0.49618842
Iteration 9, loss = 0.45891076
Iteration 10, loss = 0.42215582
Iteration 11, loss = 0.38631427
Iteration 12, loss = 0.35242228
Iteration 13, loss = 0.32074376
Iteration 14, loss = 0.29137946
Iteration 15, loss = 0.26454691
Iteration 16, loss = 0.23993339
Iteration 17, loss = 0.21787891
Iteration 18, loss = 0.19818975
Iteration 19, loss = 0.18044620
Iteration 20, loss = 0.16473515
Iteration 21, loss = 0.15071290
Iteration 22, loss = 0.13825225
Iteration 23, loss = 0.12724490
Iteration 24, loss = 0.11743614
Iteration 25, loss = 0.10873545
Iteration 26, loss = 0.10101115
Iteration 27, loss = 0.09410992
Iteration 28, loss = 0.08788132
Iteration 29, loss = 0.08240680
Iteration 30, loss = 0.07734635
Iteration 31, loss = 0.07295142
Iteration 32, loss = 0.06883337
Iteration 33, loss = 0.06511259
Iteration 34, loss = 0.06188449
Iteration 35, loss = 0.05885639
Iteration 36, loss = 0.05597687
Iteration 37, loss = 0.05352958
Iteration 38, loss = 0.05112315
Iteration 39, loss = 0.04899910
Iteration 40, loss = 0.04699113
Iteration 41, loss = 0.04521238
Iteration 42, loss = 0.04349033
Iteration 43, loss = 0.04200052
Iteration 44, loss = 0.04051336
Iteration 45, loss = 0.03914135
Iteration 46, loss = 0.03789460
Iteration 47, loss = 0.03676085
Iteration 48, loss = 0.03566169
Iteration 49, loss = 0.03463163
Iteration 50, loss = 0.03363564
Iteration 51, loss = 0.03275136
Iteration 52, loss = 0.03191791
Iteration 53, loss = 0.03110936
Iteration 54, loss = 0.03034922
Iteration 55, loss = 0.02963478
Iteration 56, loss = 0.02897963
Iteration 57, loss = 0.02839934
Iteration 58, loss = 0.02768029
Iteration 59, loss = 0.02713928
Iteration 60, loss = 0.02655772
Iteration 61, loss = 0.02613144
Iteration 62, loss = 0.02563617
Iteration 63, loss = 0.02518228
Iteration 64, loss = 0.02471546
Iteration 65, loss = 0.02434000
Iteration 66, loss = 0.02389799
Iteration 67, loss = 0.02352316
Iteration 68, loss = 0.02311967
Iteration 69, loss = 0.02283296
Iteration 70, loss = 0.02241815
Iteration 71, loss = 0.02209283
Iteration 72, loss = 0.02178514
Iteration 73, loss = 0.02152040
Iteration 74, loss = 0.02122468
Iteration 75, loss = 0.02092679
Iteration 76, loss = 0.02072318
Iteration 77, loss = 0.02048059
Iteration 78, loss = 0.02024556
Iteration 79, loss = 0.02002476
Iteration 80, loss = 0.01982115
Iteration 81, loss = 0.01960317
Iteration 82, loss = 0.01934562
Iteration 83, loss = 0.01912894
Iteration 84, loss = 0.01890062
Iteration 85, loss = 0.01874838
Iteration 86, loss = 0.01861346
Iteration 87, loss = 0.01845825
Iteration 88, loss = 0.01830142
Iteration 89, loss = 0.01807934
Iteration 90, loss = 0.01785942
Iteration 91, loss = 0.01775524
Iteration 92, loss = 0.01765284
Iteration 93, loss = 0.01746779
Iteration 94, loss = 0.01735099
Iteration 95, loss = 0.01731433
Iteration 96, loss = 0.01707027
Iteration 97, loss = 0.01693771
Iteration 98, loss = 0.01683518
Iteration 99, loss = 0.01671897
Iteration 100, loss = 0.01661694
Iteration 101, loss = 0.01656012
Iteration 102, loss = 0.01645694
Iteration 103, loss = 0.01631938
Iteration 104, loss = 0.01616069
Iteration 105, loss = 0.01608733
Iteration 106, loss = 0.01600206
Iteration 107, loss = 0.01592995
Iteration 108, loss = 0.01581032
Iteration 109, loss = 0.01574720
Iteration 110, loss = 0.01573827
Iteration 111, loss = 0.01557904
Iteration 112, loss = 0.01557598
Iteration 113, loss = 0.01542469
Iteration 114, loss = 0.01537697
Iteration 115, loss = 0.01522904
Iteration 116, loss = 0.01522250
Iteration 117, loss = 0.01516189
Iteration 118, loss = 0.01507594
Iteration 119, loss = 0.01495760
Iteration 120, loss = 0.01491047
Iteration 121, loss = 0.01487369
Iteration 122, loss = 0.01476961
Iteration 123, loss = 0.01467095
Iteration 124, loss = 0.01474045
Iteration 125, loss = 0.01462714
Iteration 126, loss = 0.01455199
Iteration 127, loss = 0.01454003
Iteration 128, loss = 0.01442293
Iteration 129, loss = 0.01437284
Iteration 130, loss = 0.01434120
Iteration 131, loss = 0.01425373
Iteration 132, loss = 0.01426439
Iteration 133, loss = 0.01420500
Iteration 134, loss = 0.01415280
Iteration 135, loss = 0.01407311
Iteration 136, loss = 0.01399786
Iteration 137, loss = 0.01406619
Iteration 138, loss = 0.01401651
Iteration 139, loss = 0.01387367
Iteration 140, loss = 0.01395573
Iteration 141, loss = 0.01383013
Iteration 142, loss = 0.01377600
Iteration 143, loss = 0.01372520
Iteration 144, loss = 0.01372086
Iteration 145, loss = 0.01361802
Iteration 146, loss = 0.01358394
Iteration 147, loss = 0.01360314
Iteration 148, loss = 0.01351366
Iteration 149, loss = 0.01361254
Iteration 150, loss = 0.01350804
Iteration 151, loss = 0.01349990
Iteration 152, loss = 0.01339019
Iteration 153, loss = 0.01342777
Iteration 154, loss = 0.01335653
Iteration 155, loss = 0.01333106
Iteration 156, loss = 0.01328922
Iteration 157, loss = 0.01330634
Iteration 158, loss = 0.01320847
Iteration 159, loss = 0.01318784
Iteration 160, loss = 0.01316673
Iteration 161, loss = 0.01312795
Iteration 162, loss = 0.01313546
Iteration 163, loss = 0.01313221
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71991815
Iteration 2, loss = 0.66386292
Iteration 3, loss = 0.58200951
Iteration 4, loss = 0.48271181
Iteration 5, loss = 0.38686265
Iteration 6, loss = 0.30352139
Iteration 7, loss = 0.23722330
Iteration 8, loss = 0.18734330
Iteration 9, loss = 0.15093913
Iteration 10, loss = 0.12435681
Iteration 11, loss = 0.10485007
Iteration 12, loss = 0.09014904
Iteration 13, loss = 0.07924086
Iteration 14, loss = 0.07038040
Iteration 15, loss = 0.06350445
Iteration 16, loss = 0.05792969
Iteration 17, loss = 0.05320632
Iteration 18, loss = 0.04966027
Iteration 19, loss = 0.04633736
Iteration 20, loss = 0.04381993
Iteration 21, loss = 0.04152158
Iteration 22, loss = 0.03958448
Iteration 23, loss = 0.03767568
Iteration 24, loss = 0.03629981
Iteration 25, loss = 0.03499437
Iteration 26, loss = 0.03365691
Iteration 27, loss = 0.03283326
Iteration 28, loss = 0.03179970
Iteration 29, loss = 0.03102548
Iteration 30, loss = 0.03028461
Iteration 31, loss = 0.02964942
Iteration 32, loss = 0.02902266
Iteration 33, loss = 0.02858087
Iteration 34, loss = 0.02808990
Iteration 35, loss = 0.02781104
Iteration 36, loss = 0.02706207
Iteration 37, loss = 0.02670630
Iteration 38, loss = 0.02653587
Iteration 39, loss = 0.02634081
Iteration 40, loss = 0.02587635
Iteration 41, loss = 0.02561556
Iteration 42, loss = 0.02544236
Iteration 43, loss = 0.02528044
Iteration 44, loss = 0.02484384
Iteration 45, loss = 0.02501777
Iteration 46, loss = 0.02462049
Iteration 47, loss = 0.02461305
Iteration 48, loss = 0.02418228
Iteration 49, loss = 0.02407351
Iteration 50, loss = 0.02404653
Iteration 51, loss = 0.02384806
Iteration 52, loss = 0.02376889
Iteration 53, loss = 0.02373004
Iteration 54, loss = 0.02358193
Iteration 55, loss = 0.02329987
Iteration 56, loss = 0.02352664
Iteration 57, loss = 0.02314073
Iteration 58, loss = 0.02294591
Iteration 59, loss = 0.02302449
Iteration 60, loss = 0.02286782
Iteration 61, loss = 0.02275591
Iteration 62, loss = 0.02283952
Iteration 63, loss = 0.02258345
Iteration 64, loss = 0.02276346
Iteration 65, loss = 0.02272312
Iteration 66, loss = 0.02249686
Iteration 67, loss = 0.02239826
Iteration 68, loss = 0.02232771
Iteration 69, loss = 0.02219844
Iteration 70, loss = 0.02241175
Iteration 71, loss = 0.02222995
Iteration 72, loss = 0.02213434
Iteration 73, loss = 0.02203222
Iteration 74, loss = 0.02209626
Iteration 75, loss = 0.02206846
Iteration 76, loss = 0.02218401
Iteration 77, loss = 0.02195658
Iteration 78, loss = 0.02207705
Iteration 79, loss = 0.02190246
Iteration 80, loss = 0.02192317
Iteration 81, loss = 0.02172404
Iteration 82, loss = 0.02166456
Iteration 83, loss = 0.02181875
Iteration 84, loss = 0.02190395
Iteration 85, loss = 0.02176687
Iteration 86, loss = 0.02175216
Iteration 87, loss = 0.02164681
Iteration 88, loss = 0.02172325
Iteration 89, loss = 0.02167467
Iteration 90, loss = 0.02177423
Iteration 91, loss = 0.02172281
Iteration 92, loss = 0.02170785
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67313730
Iteration 2, loss = 0.59670972
Iteration 3, loss = 0.48178335
Iteration 4, loss = 0.36976908
Iteration 5, loss = 0.27807960
Iteration 6, loss = 0.20925907
Iteration 7, loss = 0.16140690
Iteration 8, loss = 0.12798964
Iteration 9, loss = 0.10470692
Iteration 10, loss = 0.08801087
Iteration 11, loss = 0.07540462
Iteration 12, loss = 0.06599624
Iteration 13, loss = 0.05881480
Iteration 14, loss = 0.05332410
Iteration 15, loss = 0.04860617
Iteration 16, loss = 0.04495087
Iteration 17, loss = 0.04173476
Iteration 18, loss = 0.03929108
Iteration 19, loss = 0.03717488
Iteration 20, loss = 0.03548151
Iteration 21, loss = 0.03377590
Iteration 22, loss = 0.03251916
Iteration 23, loss = 0.03117936
Iteration 24, loss = 0.03010461
Iteration 25, loss = 0.02963473
Iteration 26, loss = 0.02866065
Iteration 27, loss = 0.02804414
Iteration 28, loss = 0.02717483
Iteration 29, loss = 0.02701625
Iteration 30, loss = 0.02645403
Iteration 31, loss = 0.02590050
Iteration 32, loss = 0.02555141
Iteration 33, loss = 0.02502619
Iteration 34, loss = 0.02490645
Iteration 35, loss = 0.02433200
Iteration 36, loss = 0.02439241
Iteration 37, loss = 0.02381115
Iteration 38, loss = 0.02371976
Iteration 39, loss = 0.02329075
Iteration 40, loss = 0.02333734
Iteration 41, loss = 0.02323940
Iteration 42, loss = 0.02275514
Iteration 43, loss = 0.02269963
Iteration 44, loss = 0.02260522
Iteration 45, loss = 0.02267483
Iteration 46, loss = 0.02254808
Iteration 47, loss = 0.02212738
Iteration 48, loss = 0.02228085
Iteration 49, loss = 0.02184016
Iteration 50, loss = 0.02195305
Iteration 51, loss = 0.02185229
Iteration 52, loss = 0.02170162
Iteration 53, loss = 0.02148481
Iteration 54, loss = 0.02130076
Iteration 55, loss = 0.02150192
Iteration 56, loss = 0.02164995
Iteration 57, loss = 0.02111634
Iteration 58, loss = 0.02148512
Iteration 59, loss = 0.02145567
Iteration 60, loss = 0.02129991
Iteration 61, loss = 0.02105939
Iteration 62, loss = 0.02094658
Iteration 63, loss = 0.02104241
Iteration 64, loss = 0.02070778
Iteration 65, loss = 0.02082709
Iteration 66, loss = 0.02080911
Iteration 67, loss = 0.02125705
Iteration 68, loss = 0.02057593
Iteration 69, loss = 0.02083957
Iteration 70, loss = 0.02077113
Iteration 71, loss = 0.02070515
Iteration 72, loss = 0.02074654
Iteration 73, loss = 0.02053987
Iteration 74, loss = 0.02042952
Iteration 75, loss = 0.02073708
Iteration 76, loss = 0.02042747
Iteration 77, loss = 0.02034272
Iteration 78, loss = 0.02037874
Iteration 79, loss = 0.02050645
Iteration 80, loss = 0.02043134
Iteration 81, loss = 0.02056173
Iteration 82, loss = 0.02018694
Iteration 83, loss = 0.02003050
Iteration 84, loss = 0.02012164
Iteration 85, loss = 0.02038010
Iteration 86, loss = 0.02012893
Iteration 87, loss = 0.02014447
Iteration 88, loss = 0.02011695
Iteration 89, loss = 0.02028854
Iteration 90, loss = 0.02017918
Iteration 91, loss = 0.02009745
Iteration 92, loss = 0.02006846
Iteration 93, loss = 0.02017348
Iteration 94, loss = 0.02005830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69324920
Iteration 2, loss = 0.66788361
Iteration 3, loss = 0.63780527
Iteration 4, loss = 0.59941187
Iteration 5, loss = 0.55342707
Iteration 6, loss = 0.50300078
Iteration 7, loss = 0.45232491
Iteration 8, loss = 0.40333321
Iteration 9, loss = 0.35724187
Iteration 10, loss = 0.31532818
Iteration 11, loss = 0.27792206
Iteration 12, loss = 0.24476388
Iteration 13, loss = 0.21579611
Iteration 14, loss = 0.19080158
Iteration 15, loss = 0.16945858
Iteration 16, loss = 0.15119721
Iteration 17, loss = 0.13560801
Iteration 18, loss = 0.12216828
Iteration 19, loss = 0.11083543
Iteration 20, loss = 0.10083958
Iteration 21, loss = 0.09227940
Iteration 22, loss = 0.08512679
Iteration 23, loss = 0.07863791
Iteration 24, loss = 0.07291774
Iteration 25, loss = 0.06799869
Iteration 26, loss = 0.06347358
Iteration 27, loss = 0.05950951
Iteration 28, loss = 0.05597072
Iteration 29, loss = 0.05280749
Iteration 30, loss = 0.04993870
Iteration 31, loss = 0.04777693
Iteration 32, loss = 0.04568040
Iteration 33, loss = 0.04361765
Iteration 34, loss = 0.04168230
Iteration 35, loss = 0.03984003
Iteration 36, loss = 0.03825804
Iteration 37, loss = 0.03689542
Iteration 38, loss = 0.03551487
Iteration 39, loss = 0.03409143
Iteration 40, loss = 0.03291793
Iteration 41, loss = 0.03195949
Iteration 42, loss = 0.03087593
Iteration 43, loss = 0.02993069
Iteration 44, loss = 0.02921658
Iteration 45, loss = 0.02843115
Iteration 46, loss = 0.02782029
Iteration 47, loss = 0.02710698
Iteration 48, loss = 0.02642533
Iteration 49, loss = 0.02576889
Iteration 50, loss = 0.02509070
Iteration 51, loss = 0.02459519
Iteration 52, loss = 0.02394852
Iteration 53, loss = 0.02343092
Iteration 54, loss = 0.02301212
Iteration 55, loss = 0.02256532
Iteration 56, loss = 0.02219714
Iteration 57, loss = 0.02181208
Iteration 58, loss = 0.02163829
Iteration 59, loss = 0.02134355
Iteration 60, loss = 0.02102744
Iteration 61, loss = 0.02064443
Iteration 62, loss = 0.02054972
Iteration 63, loss = 0.02039189
Iteration 64, loss = 0.02011571
Iteration 65, loss = 0.01983611
Iteration 66, loss = 0.01953092
Iteration 67, loss = 0.01936013
Iteration 68, loss = 0.01923923
Iteration 69, loss = 0.01897565
Iteration 70, loss = 0.01870049
Iteration 71, loss = 0.01845801
Iteration 72, loss = 0.01819680
Iteration 73, loss = 0.01820203
Iteration 74, loss = 0.01811987
Iteration 75, loss = 0.01780641
Iteration 76, loss = 0.01792179
Iteration 77, loss = 0.01786116
Iteration 78, loss = 0.01737422
Iteration 79, loss = 0.01711987
Iteration 80, loss = 0.01694755
Iteration 81, loss = 0.01682340
Iteration 82, loss = 0.01671576
Iteration 83, loss = 0.01661628
Iteration 84, loss = 0.01642390
Iteration 85, loss = 0.01626753
Iteration 86, loss = 0.01619600
Iteration 87, loss = 0.01604748
Iteration 88, loss = 0.01615839
Iteration 89, loss = 0.01613285
Iteration 90, loss = 0.01600201
Iteration 91, loss = 0.01587245
Iteration 92, loss = 0.01582096
Iteration 93, loss = 0.01567277
Iteration 94, loss = 0.01559723
Iteration 95, loss = 0.01535812
Iteration 96, loss = 0.01543503
Iteration 97, loss = 0.01536616
Iteration 98, loss = 0.01535377
Iteration 99, loss = 0.01538559
Iteration 100, loss = 0.01534036
Iteration 101, loss = 0.01525113
Iteration 102, loss = 0.01527129
Iteration 103, loss = 0.01483885
Iteration 104, loss = 0.01501518
Iteration 105, loss = 0.01495822
Iteration 106, loss = 0.01490431
Iteration 107, loss = 0.01474145
Iteration 108, loss = 0.01470099
Iteration 109, loss = 0.01477013
Iteration 110, loss = 0.01464583
Iteration 111, loss = 0.01447963
Iteration 112, loss = 0.01446922
Iteration 113, loss = 0.01438924
Iteration 114, loss = 0.01423834
Iteration 115, loss = 0.01411023
Iteration 116, loss = 0.01407144
Iteration 117, loss = 0.01400855
Iteration 118, loss = 0.01406089
Iteration 119, loss = 0.01387619
Iteration 120, loss = 0.01432990
Iteration 121, loss = 0.01416735
Iteration 122, loss = 0.01416080
Iteration 123, loss = 0.01412151
Iteration 124, loss = 0.01394148
Iteration 125, loss = 0.01385618
Iteration 126, loss = 0.01382372
Iteration 127, loss = 0.01374613
Iteration 128, loss = 0.01371045
Iteration 129, loss = 0.01358187
Iteration 130, loss = 0.01355457
Iteration 131, loss = 0.01351723
Iteration 132, loss = 0.01369775
Iteration 133, loss = 0.01370545
Iteration 134, loss = 0.01374222
Iteration 135, loss = 0.01392108
Iteration 136, loss = 0.01372031
Iteration 137, loss = 0.01371221
Iteration 138, loss = 0.01372399
Iteration 139, loss = 0.01365338
Iteration 140, loss = 0.01357601
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67605071
Iteration 2, loss = 0.60925078
Iteration 3, loss = 0.50413251
Iteration 4, loss = 0.39483933
Iteration 5, loss = 0.30073938
Iteration 6, loss = 0.22889816
Iteration 7, loss = 0.17717000
Iteration 8, loss = 0.14098754
Iteration 9, loss = 0.11526294
Iteration 10, loss = 0.09694380
Iteration 11, loss = 0.08310889
Iteration 12, loss = 0.07297241
Iteration 13, loss = 0.06487307
Iteration 14, loss = 0.05861230
Iteration 15, loss = 0.05359145
Iteration 16, loss = 0.04943118
Iteration 17, loss = 0.04604793
Iteration 18, loss = 0.04317151
Iteration 19, loss = 0.04081199
Iteration 20, loss = 0.03876027
Iteration 21, loss = 0.03714793
Iteration 22, loss = 0.03593118
Iteration 23, loss = 0.03439634
Iteration 24, loss = 0.03326340
Iteration 25, loss = 0.03228293
Iteration 26, loss = 0.03136450
Iteration 27, loss = 0.03050060
Iteration 28, loss = 0.02986709
Iteration 29, loss = 0.02941773
Iteration 30, loss = 0.02876202
Iteration 31, loss = 0.02880483
Iteration 32, loss = 0.02775696
Iteration 33, loss = 0.02730543
Iteration 34, loss = 0.02713885
Iteration 35, loss = 0.02667343
Iteration 36, loss = 0.02632175
Iteration 37, loss = 0.02618381
Iteration 38, loss = 0.02586512
Iteration 39, loss = 0.02568065
Iteration 40, loss = 0.02530156
Iteration 41, loss = 0.02490670
Iteration 42, loss = 0.02462779
Iteration 43, loss = 0.02459979
Iteration 44, loss = 0.02452217
Iteration 45, loss = 0.02447293
Iteration 46, loss = 0.02423597
Iteration 47, loss = 0.02433288
Iteration 48, loss = 0.02371946
Iteration 49, loss = 0.02378418
Iteration 50, loss = 0.02373471
Iteration 51, loss = 0.02329518
Iteration 52, loss = 0.02375971
Iteration 53, loss = 0.02359038
Iteration 54, loss = 0.02323716
Iteration 55, loss = 0.02306751
Iteration 56, loss = 0.02307956
Iteration 57, loss = 0.02295148
Iteration 58, loss = 0.02291673
Iteration 59, loss = 0.02302844
Iteration 60, loss = 0.02296105
Iteration 61, loss = 0.02290036
Iteration 62, loss = 0.02265700
Iteration 63, loss = 0.02288620
Iteration 64, loss = 0.02254902
Iteration 65, loss = 0.02251503
Iteration 66, loss = 0.02254093
Iteration 67, loss = 0.02261802
Iteration 68, loss = 0.02258661
Iteration 69, loss = 0.02227780
Iteration 70, loss = 0.02237181
Iteration 71, loss = 0.02241796
Iteration 72, loss = 0.02236423
Iteration 73, loss = 0.02230256
Iteration 74, loss = 0.02265373
Iteration 75, loss = 0.02237422
Iteration 76, loss = 0.02235527
Iteration 77, loss = 0.02221295
Iteration 78, loss = 0.02213760
Iteration 79, loss = 0.02186360
Iteration 80, loss = 0.02194166
Iteration 81, loss = 0.02208689
Iteration 82, loss = 0.02194166
Iteration 83, loss = 0.02188211
Iteration 84, loss = 0.02201901
Iteration 85, loss = 0.02168354
Iteration 86, loss = 0.02170965
Iteration 87, loss = 0.02202387
Iteration 88, loss = 0.02233113
Iteration 89, loss = 0.02189477
Iteration 90, loss = 0.02172612
Iteration 91, loss = 0.02198518
Iteration 92, loss = 0.02155263
Iteration 93, loss = 0.02157311
Iteration 94, loss = 0.02200205
Iteration 95, loss = 0.02193421
Iteration 96, loss = 0.02195822
Iteration 97, loss = 0.02179151
Iteration 98, loss = 0.02154030
Iteration 99, loss = 0.02194527
Iteration 100, loss = 0.02125678
Iteration 101, loss = 0.02158603
Iteration 102, loss = 0.02173455
Iteration 103, loss = 0.02144299
Iteration 104, loss = 0.02134652
Iteration 105, loss = 0.02144528
Iteration 106, loss = 0.02160803
Iteration 107, loss = 0.02176150
Iteration 108, loss = 0.02146494
Iteration 109, loss = 0.02161648
Iteration 110, loss = 0.02138177
Iteration 111, loss = 0.02160333
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69143025
Iteration 2, loss = 0.65583405
Iteration 3, loss = 0.61351412
Iteration 4, loss = 0.56242934
Iteration 5, loss = 0.50434831
Iteration 6, loss = 0.44512578
Iteration 7, loss = 0.38801476
Iteration 8, loss = 0.33607835
Iteration 9, loss = 0.28961289
Iteration 10, loss = 0.24934587
Iteration 11, loss = 0.21494013
Iteration 12, loss = 0.18589230
Iteration 13, loss = 0.16186923
Iteration 14, loss = 0.14183912
Iteration 15, loss = 0.12521868
Iteration 16, loss = 0.11134993
Iteration 17, loss = 0.09961848
Iteration 18, loss = 0.08998496
Iteration 19, loss = 0.08172105
Iteration 20, loss = 0.07471422
Iteration 21, loss = 0.06869212
Iteration 22, loss = 0.06356568
Iteration 23, loss = 0.05904770
Iteration 24, loss = 0.05518937
Iteration 25, loss = 0.05184929
Iteration 26, loss = 0.04878585
Iteration 27, loss = 0.04617923
Iteration 28, loss = 0.04372388
Iteration 29, loss = 0.04162267
Iteration 30, loss = 0.03984525
Iteration 31, loss = 0.03801518
Iteration 32, loss = 0.03657462
Iteration 33, loss = 0.03503990
Iteration 34, loss = 0.03378366
Iteration 35, loss = 0.03258827
Iteration 36, loss = 0.03156531
Iteration 37, loss = 0.03057791
Iteration 38, loss = 0.02970045
Iteration 39, loss = 0.02882921
Iteration 40, loss = 0.02806675
Iteration 41, loss = 0.02739992
Iteration 42, loss = 0.02666718
Iteration 43, loss = 0.02610107
Iteration 44, loss = 0.02559697
Iteration 45, loss = 0.02501716
Iteration 46, loss = 0.02456010
Iteration 47, loss = 0.02407376
Iteration 48, loss = 0.02355274
Iteration 49, loss = 0.02318571
Iteration 50, loss = 0.02288244
Iteration 51, loss = 0.02252745
Iteration 52, loss = 0.02212408
Iteration 53, loss = 0.02191273
Iteration 54, loss = 0.02163042
Iteration 55, loss = 0.02127464
Iteration 56, loss = 0.02098798
Iteration 57, loss = 0.02078033
Iteration 58, loss = 0.02050155
Iteration 59, loss = 0.02027685
Iteration 60, loss = 0.02007843
Iteration 61, loss = 0.01992558
Iteration 62, loss = 0.01972488
Iteration 63, loss = 0.01946120
Iteration 64, loss = 0.01920054
Iteration 65, loss = 0.01907347
Iteration 66, loss = 0.01892840
Iteration 67, loss = 0.01886995
Iteration 68, loss = 0.01874370
Iteration 69, loss = 0.01851197
Iteration 70, loss = 0.01834773
Iteration 71, loss = 0.01822113
Iteration 72, loss = 0.01825414
Iteration 73, loss = 0.01805639
Iteration 74, loss = 0.01786081
Iteration 75, loss = 0.01768720
Iteration 76, loss = 0.01761248
Iteration 77, loss = 0.01763956
Iteration 78, loss = 0.01744986
Iteration 79, loss = 0.01731804
Iteration 80, loss = 0.01728448
Iteration 81, loss = 0.01722262
Iteration 82, loss = 0.01707517
Iteration 83, loss = 0.01707186
Iteration 84, loss = 0.01694620
Iteration 85, loss = 0.01682153
Iteration 86, loss = 0.01674671
Iteration 87, loss = 0.01678465
Iteration 88, loss = 0.01665141
Iteration 89, loss = 0.01666537
Iteration 90, loss = 0.01649838
Iteration 91, loss = 0.01645224
Iteration 92, loss = 0.01639163
Iteration 93, loss = 0.01658222
Iteration 94, loss = 0.01627134
Iteration 95, loss = 0.01617563
Iteration 96, loss = 0.01613435
Iteration 97, loss = 0.01612295
Iteration 98, loss = 0.01608051
Iteration 99, loss = 0.01607601
Iteration 100, loss = 0.01611094
Iteration 101, loss = 0.01600284
Iteration 102, loss = 0.01581509
Iteration 103, loss = 0.01576332
Iteration 104, loss = 0.01577891
Iteration 105, loss = 0.01577522
Iteration 106, loss = 0.01572164
Iteration 107, loss = 0.01566319
Iteration 108, loss = 0.01561594
Iteration 109, loss = 0.01555533
Iteration 110, loss = 0.01549811
Iteration 111, loss = 0.01549231
Iteration 112, loss = 0.01538040
Iteration 113, loss = 0.01541810
Iteration 114, loss = 0.01540773
Iteration 115, loss = 0.01543457
Iteration 116, loss = 0.01549945
Iteration 117, loss = 0.01530792
Iteration 118, loss = 0.01527277
Iteration 119, loss = 0.01534893
Iteration 120, loss = 0.01528525
Iteration 121, loss = 0.01520041
Iteration 122, loss = 0.01529403
Iteration 123, loss = 0.01535771
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68488930
Iteration 2, loss = 0.60851111
Iteration 3, loss = 0.50021480
Iteration 4, loss = 0.38709233
Iteration 5, loss = 0.29125306
Iteration 6, loss = 0.21942520
Iteration 7, loss = 0.16891154
Iteration 8, loss = 0.13398648
Iteration 9, loss = 0.10974630
Iteration 10, loss = 0.09210200
Iteration 11, loss = 0.07954409
Iteration 12, loss = 0.06977302
Iteration 13, loss = 0.06260639
Iteration 14, loss = 0.05672127
Iteration 15, loss = 0.05204096
Iteration 16, loss = 0.04839142
Iteration 17, loss = 0.04538258
Iteration 18, loss = 0.04267198
Iteration 19, loss = 0.04049985
Iteration 20, loss = 0.03888689
Iteration 21, loss = 0.03715835
Iteration 22, loss = 0.03580579
Iteration 23, loss = 0.03471830
Iteration 24, loss = 0.03362979
Iteration 25, loss = 0.03266510
Iteration 26, loss = 0.03211686
Iteration 27, loss = 0.03121891
Iteration 28, loss = 0.03076995
Iteration 29, loss = 0.02998323
Iteration 30, loss = 0.02964574
Iteration 31, loss = 0.02902963
Iteration 32, loss = 0.02897092
Iteration 33, loss = 0.02834690
Iteration 34, loss = 0.02783240
Iteration 35, loss = 0.02768258
Iteration 36, loss = 0.02731800
Iteration 37, loss = 0.02715061
Iteration 38, loss = 0.02703731
Iteration 39, loss = 0.02698745
Iteration 40, loss = 0.02640948
Iteration 41, loss = 0.02627657
Iteration 42, loss = 0.02581965
Iteration 43, loss = 0.02557370
Iteration 44, loss = 0.02566744
Iteration 45, loss = 0.02565853
Iteration 46, loss = 0.02556662
Iteration 47, loss = 0.02559632
Iteration 48, loss = 0.02574638
Iteration 49, loss = 0.02538533
Iteration 50, loss = 0.02521944
Iteration 51, loss = 0.02486090
Iteration 52, loss = 0.02501128
Iteration 53, loss = 0.02476689
Iteration 54, loss = 0.02480661
Iteration 55, loss = 0.02487142
Iteration 56, loss = 0.02480405
Iteration 57, loss = 0.02460015
Iteration 58, loss = 0.02473152
Iteration 59, loss = 0.02459221
Iteration 60, loss = 0.02444667
Iteration 61, loss = 0.02442551
Iteration 62, loss = 0.02436995
Iteration 63, loss = 0.02400145
Iteration 64, loss = 0.02422731
Iteration 65, loss = 0.02433998
Iteration 66, loss = 0.02425653
Iteration 67, loss = 0.02452620
Iteration 68, loss = 0.02433745
Iteration 69, loss = 0.02366938
Iteration 70, loss = 0.02383351
Iteration 71, loss = 0.02383369
Iteration 72, loss = 0.02406340
Iteration 73, loss = 0.02402129
Iteration 74, loss = 0.02365622
Iteration 75, loss = 0.02399407
Iteration 76, loss = 0.02401945
Iteration 77, loss = 0.02368824
Iteration 78, loss = 0.02383079
Iteration 79, loss = 0.02338110
Iteration 80, loss = 0.02388948
Iteration 81, loss = 0.02338429
Iteration 82, loss = 0.02382873
Iteration 83, loss = 0.02362596
Iteration 84, loss = 0.02369822
Iteration 85, loss = 0.02373881
Iteration 86, loss = 0.02383747
Iteration 87, loss = 0.02359905
Iteration 88, loss = 0.02380467
Iteration 89, loss = 0.02345090
Iteration 90, loss = 0.02354269
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70126605
Iteration 2, loss = 0.64412253
Iteration 3, loss = 0.55742643
Iteration 4, loss = 0.44892793
Iteration 5, loss = 0.34865217
Iteration 6, loss = 0.26725530
Iteration 7, loss = 0.20580132
Iteration 8, loss = 0.16180777
Iteration 9, loss = 0.13003915
Iteration 10, loss = 0.10750899
Iteration 11, loss = 0.09075257
Iteration 12, loss = 0.07836108
Iteration 13, loss = 0.06861999
Iteration 14, loss = 0.06109806
Iteration 15, loss = 0.05500599
Iteration 16, loss = 0.05012141
Iteration 17, loss = 0.04617775
Iteration 18, loss = 0.04287246
Iteration 19, loss = 0.04014074
Iteration 20, loss = 0.03781789
Iteration 21, loss = 0.03571079
Iteration 22, loss = 0.03410394
Iteration 23, loss = 0.03250736
Iteration 24, loss = 0.03117374
Iteration 25, loss = 0.03019900
Iteration 26, loss = 0.02902660
Iteration 27, loss = 0.02823521
Iteration 28, loss = 0.02750268
Iteration 29, loss = 0.02660351
Iteration 30, loss = 0.02604766
Iteration 31, loss = 0.02573970
Iteration 32, loss = 0.02483175
Iteration 33, loss = 0.02453807
Iteration 34, loss = 0.02416838
Iteration 35, loss = 0.02357699
Iteration 36, loss = 0.02313712
Iteration 37, loss = 0.02303333
Iteration 38, loss = 0.02256916
Iteration 39, loss = 0.02231256
Iteration 40, loss = 0.02215440
Iteration 41, loss = 0.02197417
Iteration 42, loss = 0.02191896
Iteration 43, loss = 0.02161424
Iteration 44, loss = 0.02141879
Iteration 45, loss = 0.02096202
Iteration 46, loss = 0.02101380
Iteration 47, loss = 0.02111943
Iteration 48, loss = 0.02097737
Iteration 49, loss = 0.02046679
Iteration 50, loss = 0.02035817
Iteration 51, loss = 0.02035280
Iteration 52, loss = 0.02011141
Iteration 53, loss = 0.02014629
Iteration 54, loss = 0.01999549
Iteration 55, loss = 0.01967096
Iteration 56, loss = 0.01985575
Iteration 57, loss = 0.01971221
Iteration 58, loss = 0.01962421
Iteration 59, loss = 0.01961861
Iteration 60, loss = 0.01951868
Iteration 61, loss = 0.01916655
Iteration 62, loss = 0.01918830
Iteration 63, loss = 0.01928964
Iteration 64, loss = 0.01898098
Iteration 65, loss = 0.01911407
Iteration 66, loss = 0.01928705
Iteration 67, loss = 0.01904902
Iteration 68, loss = 0.01895056
Iteration 69, loss = 0.01906877
Iteration 70, loss = 0.01877818
Iteration 71, loss = 0.01895523
Iteration 72, loss = 0.01883758
Iteration 73, loss = 0.01887225
Iteration 74, loss = 0.01863816
Iteration 75, loss = 0.01866564
Iteration 76, loss = 0.01871474
Iteration 77, loss = 0.01875389
Iteration 78, loss = 0.01839603
Iteration 79, loss = 0.01839440
Iteration 80, loss = 0.01857816
Iteration 81, loss = 0.01845470
Iteration 82, loss = 0.01831553
Iteration 83, loss = 0.01862711
Iteration 84, loss = 0.01822895
Iteration 85, loss = 0.01834956
Iteration 86, loss = 0.01841689
Iteration 87, loss = 0.01826282
Iteration 88, loss = 0.01803462
Iteration 89, loss = 0.01849154
Iteration 90, loss = 0.01804759
Iteration 91, loss = 0.01840486
Iteration 92, loss = 0.01843941
Iteration 93, loss = 0.01787526
Iteration 94, loss = 0.01825475
Iteration 95, loss = 0.01798442
Iteration 96, loss = 0.01800975
Iteration 97, loss = 0.01807025
Iteration 98, loss = 0.01814284
Iteration 99, loss = 0.01797682
Iteration 100, loss = 0.01812126
Iteration 101, loss = 0.01798072
Iteration 102, loss = 0.01797104
Iteration 103, loss = 0.01812489
Iteration 104, loss = 0.01812858
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71144962
Iteration 2, loss = 0.67287137
Iteration 3, loss = 0.62190119
Iteration 4, loss = 0.55905176
Iteration 5, loss = 0.48686061
Iteration 6, loss = 0.41491578
Iteration 7, loss = 0.34823189
Iteration 8, loss = 0.29042773
Iteration 9, loss = 0.24205050
Iteration 10, loss = 0.20231300
Iteration 11, loss = 0.17066375
Iteration 12, loss = 0.14527946
Iteration 13, loss = 0.12514012
Iteration 14, loss = 0.10890612
Iteration 15, loss = 0.09585697
Iteration 16, loss = 0.08513368
Iteration 17, loss = 0.07646501
Iteration 18, loss = 0.06909905
Iteration 19, loss = 0.06292381
Iteration 20, loss = 0.05774224
Iteration 21, loss = 0.05340857
Iteration 22, loss = 0.04948976
Iteration 23, loss = 0.04630181
Iteration 24, loss = 0.04350027
Iteration 25, loss = 0.04088363
Iteration 26, loss = 0.03877367
Iteration 27, loss = 0.03665538
Iteration 28, loss = 0.03508986
Iteration 29, loss = 0.03332793
Iteration 30, loss = 0.03199972
Iteration 31, loss = 0.03069320
Iteration 32, loss = 0.02954192
Iteration 33, loss = 0.02847189
Iteration 34, loss = 0.02752296
Iteration 35, loss = 0.02670350
Iteration 36, loss = 0.02593213
Iteration 37, loss = 0.02518330
Iteration 38, loss = 0.02449710
Iteration 39, loss = 0.02397870
Iteration 40, loss = 0.02345879
Iteration 41, loss = 0.02293889
Iteration 42, loss = 0.02239092
Iteration 43, loss = 0.02206826
Iteration 44, loss = 0.02153476
Iteration 45, loss = 0.02102947
Iteration 46, loss = 0.02077726
Iteration 47, loss = 0.02034869
Iteration 48, loss = 0.02000515
Iteration 49, loss = 0.01981236
Iteration 50, loss = 0.01947238
Iteration 51, loss = 0.01925227
Iteration 52, loss = 0.01904003
Iteration 53, loss = 0.01865625
Iteration 54, loss = 0.01850545
Iteration 55, loss = 0.01830794
Iteration 56, loss = 0.01817989
Iteration 57, loss = 0.01805158
Iteration 58, loss = 0.01770291
Iteration 59, loss = 0.01749178
Iteration 60, loss = 0.01740139
Iteration 61, loss = 0.01726137
Iteration 62, loss = 0.01711626
Iteration 63, loss = 0.01691729
Iteration 64, loss = 0.01682749
Iteration 65, loss = 0.01666824
Iteration 66, loss = 0.01683546
Iteration 67, loss = 0.01653433
Iteration 68, loss = 0.01637227
Iteration 69, loss = 0.01637310
Iteration 70, loss = 0.01612176
Iteration 71, loss = 0.01613238
Iteration 72, loss = 0.01603851
Iteration 73, loss = 0.01591912
Iteration 74, loss = 0.01571047
Iteration 75, loss = 0.01570833
Iteration 76, loss = 0.01568938
Iteration 77, loss = 0.01548927
Iteration 78, loss = 0.01560342
Iteration 79, loss = 0.01556378
Iteration 80, loss = 0.01537432
Iteration 81, loss = 0.01529242
Iteration 82, loss = 0.01535942
Iteration 83, loss = 0.01514601
Iteration 84, loss = 0.01513013
Iteration 85, loss = 0.01497004
Iteration 86, loss = 0.01507302
Iteration 87, loss = 0.01491728
Iteration 88, loss = 0.01481104
Iteration 89, loss = 0.01479974
Iteration 90, loss = 0.01479787
Iteration 91, loss = 0.01472214
Iteration 92, loss = 0.01476013
Iteration 93, loss = 0.01458330
Iteration 94, loss = 0.01461882
Iteration 95, loss = 0.01461981
Iteration 96, loss = 0.01454694
Iteration 97, loss = 0.01445597
Iteration 98, loss = 0.01456798
Iteration 99, loss = 0.01440671
Iteration 100, loss = 0.01443704
Iteration 101, loss = 0.01432774
Iteration 102, loss = 0.01427505
Iteration 103, loss = 0.01437261
Iteration 104, loss = 0.01429006
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70021780
Iteration 2, loss = 0.67578236
Iteration 3, loss = 0.65222234
Iteration 4, loss = 0.62508747
Iteration 5, loss = 0.59428135
Iteration 6, loss = 0.56018705
Iteration 7, loss = 0.52430826
Iteration 8, loss = 0.48751626
Iteration 9, loss = 0.45124898
Iteration 10, loss = 0.41607562
Iteration 11, loss = 0.38251001
Iteration 12, loss = 0.35067799
Iteration 13, loss = 0.32093050
Iteration 14, loss = 0.29332849
Iteration 15, loss = 0.26794174
Iteration 16, loss = 0.24469275
Iteration 17, loss = 0.22355493
Iteration 18, loss = 0.20446505
Iteration 19, loss = 0.18714301
Iteration 20, loss = 0.17155200
Iteration 21, loss = 0.15759568
Iteration 22, loss = 0.14519548
Iteration 23, loss = 0.13398440
Iteration 24, loss = 0.12394730
Iteration 25, loss = 0.11506067
Iteration 26, loss = 0.10692347
Iteration 27, loss = 0.09973226
Iteration 28, loss = 0.09320002
Iteration 29, loss = 0.08743331
Iteration 30, loss = 0.08218158
Iteration 31, loss = 0.07741266
Iteration 32, loss = 0.07311585
Iteration 33, loss = 0.06917817
Iteration 34, loss = 0.06563242
Iteration 35, loss = 0.06236208
Iteration 36, loss = 0.05941565
Iteration 37, loss = 0.05665689
Iteration 38, loss = 0.05418940
Iteration 39, loss = 0.05196481
Iteration 40, loss = 0.04978265
Iteration 41, loss = 0.04780955
Iteration 42, loss = 0.04603811
Iteration 43, loss = 0.04432123
Iteration 44, loss = 0.04281665
Iteration 45, loss = 0.04133939
Iteration 46, loss = 0.04002748
Iteration 47, loss = 0.03872116
Iteration 48, loss = 0.03750881
Iteration 49, loss = 0.03647924
Iteration 50, loss = 0.03548338
Iteration 51, loss = 0.03443510
Iteration 52, loss = 0.03358602
Iteration 53, loss = 0.03267463
Iteration 54, loss = 0.03192242
Iteration 55, loss = 0.03107327
Iteration 56, loss = 0.03041058
Iteration 57, loss = 0.02978166
Iteration 58, loss = 0.02916176
Iteration 59, loss = 0.02852305
Iteration 60, loss = 0.02791275
Iteration 61, loss = 0.02737473
Iteration 62, loss = 0.02685325
Iteration 63, loss = 0.02635087
Iteration 64, loss = 0.02597975
Iteration 65, loss = 0.02545495
Iteration 66, loss = 0.02503917
Iteration 67, loss = 0.02466541
Iteration 68, loss = 0.02423607
Iteration 69, loss = 0.02385422
Iteration 70, loss = 0.02349850
Iteration 71, loss = 0.02318110
Iteration 72, loss = 0.02288983
Iteration 73, loss = 0.02249003
Iteration 74, loss = 0.02223464
Iteration 75, loss = 0.02199012
Iteration 76, loss = 0.02165628
Iteration 77, loss = 0.02141156
Iteration 78, loss = 0.02116371
Iteration 79, loss = 0.02090896
Iteration 80, loss = 0.02067616
Iteration 81, loss = 0.02046504
Iteration 82, loss = 0.02029095
Iteration 83, loss = 0.02003393
Iteration 84, loss = 0.01990216
Iteration 85, loss = 0.01963622
Iteration 86, loss = 0.01943514
Iteration 87, loss = 0.01925216
Iteration 88, loss = 0.01907818
Iteration 89, loss = 0.01897274
Iteration 90, loss = 0.01878577
Iteration 91, loss = 0.01862008
Iteration 92, loss = 0.01843263
Iteration 93, loss = 0.01831556
Iteration 94, loss = 0.01811973
Iteration 95, loss = 0.01804364
Iteration 96, loss = 0.01789737
Iteration 97, loss = 0.01783959
Iteration 98, loss = 0.01765648
Iteration 99, loss = 0.01751558
Iteration 100, loss = 0.01742273
Iteration 101, loss = 0.01730270
Iteration 102, loss = 0.01717907
Iteration 103, loss = 0.01712279
Iteration 104, loss = 0.01694131
Iteration 105, loss = 0.01690137
Iteration 106, loss = 0.01677521
Iteration 107, loss = 0.01674425
Iteration 108, loss = 0.01656120
Iteration 109, loss = 0.01654428
Iteration 110, loss = 0.01644873
Iteration 111, loss = 0.01631437
Iteration 112, loss = 0.01624682
Iteration 113, loss = 0.01618058
Iteration 114, loss = 0.01612982
Iteration 115, loss = 0.01603402
Iteration 116, loss = 0.01598277
Iteration 117, loss = 0.01585156
Iteration 118, loss = 0.01579900
Iteration 119, loss = 0.01568942
Iteration 120, loss = 0.01564866
Iteration 121, loss = 0.01563599
Iteration 122, loss = 0.01556494
Iteration 123, loss = 0.01544127
Iteration 124, loss = 0.01540292
Iteration 125, loss = 0.01528772
Iteration 126, loss = 0.01527223
Iteration 127, loss = 0.01524444
Iteration 128, loss = 0.01519048
Iteration 129, loss = 0.01514468
Iteration 130, loss = 0.01506420
Iteration 131, loss = 0.01503899
Iteration 132, loss = 0.01492740
Iteration 133, loss = 0.01495572
Iteration 134, loss = 0.01482808
Iteration 135, loss = 0.01480120
Iteration 136, loss = 0.01483226
Iteration 137, loss = 0.01467123
Iteration 138, loss = 0.01475016
Iteration 139, loss = 0.01459907
Iteration 140, loss = 0.01458628
Iteration 141, loss = 0.01455746
Iteration 142, loss = 0.01453437
Iteration 143, loss = 0.01444165
Iteration 144, loss = 0.01442976
Iteration 145, loss = 0.01439386
Iteration 146, loss = 0.01434248
Iteration 147, loss = 0.01428826
Iteration 148, loss = 0.01420829
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67650292
Iteration 2, loss = 0.61035680
Iteration 3, loss = 0.51261339
Iteration 4, loss = 0.40838900
Iteration 5, loss = 0.31538176
Iteration 6, loss = 0.24133260
Iteration 7, loss = 0.18754922
Iteration 8, loss = 0.14846718
Iteration 9, loss = 0.12119484
Iteration 10, loss = 0.10106308
Iteration 11, loss = 0.08638301
Iteration 12, loss = 0.07565906
Iteration 13, loss = 0.06697542
Iteration 14, loss = 0.06031296
Iteration 15, loss = 0.05492842
Iteration 16, loss = 0.05071526
Iteration 17, loss = 0.04713676
Iteration 18, loss = 0.04390259
Iteration 19, loss = 0.04154423
Iteration 20, loss = 0.03933158
Iteration 21, loss = 0.03746029
Iteration 22, loss = 0.03623426
Iteration 23, loss = 0.03480580
Iteration 24, loss = 0.03383472
Iteration 25, loss = 0.03262690
Iteration 26, loss = 0.03177647
Iteration 27, loss = 0.03080014
Iteration 28, loss = 0.03022498
Iteration 29, loss = 0.02947609
Iteration 30, loss = 0.02910099
Iteration 31, loss = 0.02835427
Iteration 32, loss = 0.02803847
Iteration 33, loss = 0.02754546
Iteration 34, loss = 0.02717368
Iteration 35, loss = 0.02686786
Iteration 36, loss = 0.02638799
Iteration 37, loss = 0.02625821
Iteration 38, loss = 0.02616708
Iteration 39, loss = 0.02577294
Iteration 40, loss = 0.02558077
Iteration 41, loss = 0.02533742
Iteration 42, loss = 0.02541477
Iteration 43, loss = 0.02534682
Iteration 44, loss = 0.02505343
Iteration 45, loss = 0.02483081
Iteration 46, loss = 0.02465596
Iteration 47, loss = 0.02441726
Iteration 48, loss = 0.02430401
Iteration 49, loss = 0.02409272
Iteration 50, loss = 0.02397329
Iteration 51, loss = 0.02406596
Iteration 52, loss = 0.02352888
Iteration 53, loss = 0.02367539
Iteration 54, loss = 0.02369770
Iteration 55, loss = 0.02372102
Iteration 56, loss = 0.02358112
Iteration 57, loss = 0.02340903
Iteration 58, loss = 0.02355158
Iteration 59, loss = 0.02337968
Iteration 60, loss = 0.02345403
Iteration 61, loss = 0.02320413
Iteration 62, loss = 0.02323329
Iteration 63, loss = 0.02289973
Iteration 64, loss = 0.02302956
Iteration 65, loss = 0.02282579
Iteration 66, loss = 0.02289228
Iteration 67, loss = 0.02288684
Iteration 68, loss = 0.02280043
Iteration 69, loss = 0.02297904
Iteration 70, loss = 0.02290300
Iteration 71, loss = 0.02271506
Iteration 72, loss = 0.02261478
Iteration 73, loss = 0.02265172
Iteration 74, loss = 0.02288483
Iteration 75, loss = 0.02284720
Iteration 76, loss = 0.02261615
Iteration 77, loss = 0.02248216
Iteration 78, loss = 0.02248018
Iteration 79, loss = 0.02260928
Iteration 80, loss = 0.02245332
Iteration 81, loss = 0.02245318
Iteration 82, loss = 0.02230450
Iteration 83, loss = 0.02243491
Iteration 84, loss = 0.02229098
Iteration 85, loss = 0.02228886
Iteration 86, loss = 0.02239852
Iteration 87, loss = 0.02229842
Iteration 88, loss = 0.02225963
Iteration 89, loss = 0.02241306
Iteration 90, loss = 0.02232048
Iteration 91, loss = 0.02209119
Iteration 92, loss = 0.02246903
Iteration 93, loss = 0.02182490
Iteration 94, loss = 0.02204513
Iteration 95, loss = 0.02229390
Iteration 96, loss = 0.02179200
Iteration 97, loss = 0.02219545
Iteration 98, loss = 0.02210065
Iteration 99, loss = 0.02230461
Iteration 100, loss = 0.02206138
Iteration 101, loss = 0.02202663
Iteration 102, loss = 0.02214487
Iteration 103, loss = 0.02200283
Iteration 104, loss = 0.02176344
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69487583
Iteration 2, loss = 0.65579473
Iteration 3, loss = 0.59814781
Iteration 4, loss = 0.51952422
Iteration 5, loss = 0.43761362
Iteration 6, loss = 0.36210006
Iteration 7, loss = 0.29712241
Iteration 8, loss = 0.24360183
Iteration 9, loss = 0.20092897
Iteration 10, loss = 0.16747056
Iteration 11, loss = 0.14137955
Iteration 12, loss = 0.12107956
Iteration 13, loss = 0.10507558
Iteration 14, loss = 0.09234023
Iteration 15, loss = 0.08182661
Iteration 16, loss = 0.07347417
Iteration 17, loss = 0.06647059
Iteration 18, loss = 0.06073863
Iteration 19, loss = 0.05584744
Iteration 20, loss = 0.05172804
Iteration 21, loss = 0.04817901
Iteration 22, loss = 0.04512381
Iteration 23, loss = 0.04244972
Iteration 24, loss = 0.04010020
Iteration 25, loss = 0.03810770
Iteration 26, loss = 0.03628117
Iteration 27, loss = 0.03465051
Iteration 28, loss = 0.03325624
Iteration 29, loss = 0.03198220
Iteration 30, loss = 0.03099401
Iteration 31, loss = 0.02993959
Iteration 32, loss = 0.02904647
Iteration 33, loss = 0.02869888
Iteration 34, loss = 0.02737571
Iteration 35, loss = 0.02714242
Iteration 36, loss = 0.02630687
Iteration 37, loss = 0.02579880
Iteration 38, loss = 0.02498606
Iteration 39, loss = 0.02449345
Iteration 40, loss = 0.02411788
Iteration 41, loss = 0.02389076
Iteration 42, loss = 0.02314656
Iteration 43, loss = 0.02288273
Iteration 44, loss = 0.02249897
Iteration 45, loss = 0.02225615
Iteration 46, loss = 0.02193822
Iteration 47, loss = 0.02164006
Iteration 48, loss = 0.02137954
Iteration 49, loss = 0.02133809
Iteration 50, loss = 0.02107217
Iteration 51, loss = 0.02079749
Iteration 52, loss = 0.02050080
Iteration 53, loss = 0.02035121
Iteration 54, loss = 0.02010739
Iteration 55, loss = 0.01984538
Iteration 56, loss = 0.01993719
Iteration 57, loss = 0.01975983
Iteration 58, loss = 0.01947166
Iteration 59, loss = 0.01950841
Iteration 60, loss = 0.01923247
Iteration 61, loss = 0.01921951
Iteration 62, loss = 0.01903341
Iteration 63, loss = 0.01903504
Iteration 64, loss = 0.01883550
Iteration 65, loss = 0.01875824
Iteration 66, loss = 0.01856314
Iteration 67, loss = 0.01872828
Iteration 68, loss = 0.01833337
Iteration 69, loss = 0.01830118
Iteration 70, loss = 0.01833935
Iteration 71, loss = 0.01817425
Iteration 72, loss = 0.01797700
Iteration 73, loss = 0.01811090
Iteration 74, loss = 0.01790774
Iteration 75, loss = 0.01803046
Iteration 76, loss = 0.01784162
Iteration 77, loss = 0.01791396
Iteration 78, loss = 0.01785685
Iteration 79, loss = 0.01786130
Iteration 80, loss = 0.01770085
Iteration 81, loss = 0.01765954
Iteration 82, loss = 0.01748624
Iteration 83, loss = 0.01756139
Iteration 84, loss = 0.01758565
Iteration 85, loss = 0.01740611
Iteration 86, loss = 0.01737744
Iteration 87, loss = 0.01725289
Iteration 88, loss = 0.01735132
Iteration 89, loss = 0.01723530
Iteration 90, loss = 0.01732165
Iteration 91, loss = 0.01744164
Iteration 92, loss = 0.01731535
Iteration 93, loss = 0.01694718
Iteration 94, loss = 0.01712060
Iteration 95, loss = 0.01689890
Iteration 96, loss = 0.01682644
Iteration 97, loss = 0.01695321
Iteration 98, loss = 0.01685143
Iteration 99, loss = 0.01669089
Iteration 100, loss = 0.01692755
Iteration 101, loss = 0.01698495
Iteration 102, loss = 0.01669270
Iteration 103, loss = 0.01676904
Iteration 104, loss = 0.01666489
Iteration 105, loss = 0.01684233
Iteration 106, loss = 0.01670451
Iteration 107, loss = 0.01665048
Iteration 108, loss = 0.01680384
Iteration 109, loss = 0.01668199
Iteration 110, loss = 0.01656555
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67965794
Iteration 2, loss = 0.60432809
Iteration 3, loss = 0.49480198
Iteration 4, loss = 0.38293306
Iteration 5, loss = 0.28882923
Iteration 6, loss = 0.21787356
Iteration 7, loss = 0.16761176
Iteration 8, loss = 0.13272078
Iteration 9, loss = 0.10845860
Iteration 10, loss = 0.09100417
Iteration 11, loss = 0.07828392
Iteration 12, loss = 0.06861040
Iteration 13, loss = 0.06111461
Iteration 14, loss = 0.05551711
Iteration 15, loss = 0.05069902
Iteration 16, loss = 0.04682695
Iteration 17, loss = 0.04391102
Iteration 18, loss = 0.04121547
Iteration 19, loss = 0.03909618
Iteration 20, loss = 0.03719923
Iteration 21, loss = 0.03570992
Iteration 22, loss = 0.03426059
Iteration 23, loss = 0.03317160
Iteration 24, loss = 0.03209025
Iteration 25, loss = 0.03125582
Iteration 26, loss = 0.03052210
Iteration 27, loss = 0.02992711
Iteration 28, loss = 0.02920294
Iteration 29, loss = 0.02861327
Iteration 30, loss = 0.02812310
Iteration 31, loss = 0.02761980
Iteration 32, loss = 0.02734998
Iteration 33, loss = 0.02704395
Iteration 34, loss = 0.02679218
Iteration 35, loss = 0.02626284
Iteration 36, loss = 0.02616945
Iteration 37, loss = 0.02552946
Iteration 38, loss = 0.02554518
Iteration 39, loss = 0.02541609
Iteration 40, loss = 0.02497404
Iteration 41, loss = 0.02482696
Iteration 42, loss = 0.02475153
Iteration 43, loss = 0.02485265
Iteration 44, loss = 0.02464399
Iteration 45, loss = 0.02457194
Iteration 46, loss = 0.02422460
Iteration 47, loss = 0.02408907
Iteration 48, loss = 0.02398462
Iteration 49, loss = 0.02398024
Iteration 50, loss = 0.02365169
Iteration 51, loss = 0.02396431
Iteration 52, loss = 0.02362762
Iteration 53, loss = 0.02344999
Iteration 54, loss = 0.02341863
Iteration 55, loss = 0.02343099
Iteration 56, loss = 0.02352310
Iteration 57, loss = 0.02359819
Iteration 58, loss = 0.02376370
Iteration 59, loss = 0.02311474
Iteration 60, loss = 0.02305468
Iteration 61, loss = 0.02321323
Iteration 62, loss = 0.02283079
Iteration 63, loss = 0.02314769
Iteration 64, loss = 0.02305787
Iteration 65, loss = 0.02280278
Iteration 66, loss = 0.02300078
Iteration 67, loss = 0.02290991
Iteration 68, loss = 0.02294985
Iteration 69, loss = 0.02277607
Iteration 70, loss = 0.02268705
Iteration 71, loss = 0.02280520
Iteration 72, loss = 0.02302005
Iteration 73, loss = 0.02262688
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	92m11.402s
user	708m33.536s
sys	80m18.004s
Elapsed time: 5532 seconds
