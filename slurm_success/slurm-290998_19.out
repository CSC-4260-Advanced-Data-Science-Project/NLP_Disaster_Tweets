Processing dataset: prepended_v2_no_emojis_mentions

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: prepended_v2_no_emojis_mentions (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on prepended_v2_no_emojis_mentions...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on prepended_v2_no_emojis_mentions...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on prepended_v2_no_emojis_mentions...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on prepended_v2_no_emojis_mentions...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on prepended_v2_no_emojis_mentions...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.69842083
Iteration 2, loss = 0.63971242
Iteration 3, loss = 0.53805897
Iteration 4, loss = 0.42354834
Iteration 5, loss = 0.32155611
Iteration 6, loss = 0.24232675
Iteration 7, loss = 0.18443533
Iteration 8, loss = 0.14399882
Iteration 9, loss = 0.11589103
Iteration 10, loss = 0.09616670
Iteration 11, loss = 0.08189010
Iteration 12, loss = 0.07100074
Iteration 13, loss = 0.06326821
Iteration 14, loss = 0.05738124
Iteration 15, loss = 0.05221736
Iteration 16, loss = 0.04813702
Iteration 17, loss = 0.04491609
Iteration 18, loss = 0.04256871
Iteration 19, loss = 0.03989368
Iteration 20, loss = 0.03807280
Iteration 21, loss = 0.03674248
Iteration 22, loss = 0.03506608
Iteration 23, loss = 0.03385942
Iteration 24, loss = 0.03262706
Iteration 25, loss = 0.03212139
Iteration 26, loss = 0.03121664
Iteration 27, loss = 0.03045566
Iteration 28, loss = 0.02975271
Iteration 29, loss = 0.02908522
Iteration 30, loss = 0.02866675
Iteration 31, loss = 0.02845647
Iteration 32, loss = 0.02787890
Iteration 33, loss = 0.02743601
Iteration 34, loss = 0.02723078
Iteration 35, loss = 0.02689063
Iteration 36, loss = 0.02671999
Iteration 37, loss = 0.02627811
Iteration 38, loss = 0.02598363
Iteration 39, loss = 0.02595976
Iteration 40, loss = 0.02577371
Iteration 41, loss = 0.02535531
Iteration 42, loss = 0.02544189
Iteration 43, loss = 0.02509391
Iteration 44, loss = 0.02512491
Iteration 45, loss = 0.02471335
Iteration 46, loss = 0.02470337
Iteration 47, loss = 0.02471608
Iteration 48, loss = 0.02460631
Iteration 49, loss = 0.02446389
Iteration 50, loss = 0.02437192
Iteration 51, loss = 0.02446294
Iteration 52, loss = 0.02411753
Iteration 53, loss = 0.02399040
Iteration 54, loss = 0.02427771
Iteration 55, loss = 0.02388258
Iteration 56, loss = 0.02413166
Iteration 57, loss = 0.02401920
Iteration 58, loss = 0.02392941
Iteration 59, loss = 0.02392002
Iteration 60, loss = 0.02341001
Iteration 61, loss = 0.02344846
Iteration 62, loss = 0.02357152
Iteration 63, loss = 0.02345349
Iteration 64, loss = 0.02334535
Iteration 65, loss = 0.02327226
Iteration 66, loss = 0.02331085
Iteration 67, loss = 0.02338279
Iteration 68, loss = 0.02320180
Iteration 69, loss = 0.02320624
Iteration 70, loss = 0.02311264
Iteration 71, loss = 0.02318320
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67500719
Iteration 2, loss = 0.59955049
Iteration 3, loss = 0.49090799
Iteration 4, loss = 0.37920933
Iteration 5, loss = 0.28261578
Iteration 6, loss = 0.21061248
Iteration 7, loss = 0.16067330
Iteration 8, loss = 0.12649104
Iteration 9, loss = 0.10298304
Iteration 10, loss = 0.08660178
Iteration 11, loss = 0.07441360
Iteration 12, loss = 0.06559500
Iteration 13, loss = 0.05894212
Iteration 14, loss = 0.05372013
Iteration 15, loss = 0.04929241
Iteration 16, loss = 0.04617802
Iteration 17, loss = 0.04328988
Iteration 18, loss = 0.04087657
Iteration 19, loss = 0.03880015
Iteration 20, loss = 0.03728818
Iteration 21, loss = 0.03580225
Iteration 22, loss = 0.03471277
Iteration 23, loss = 0.03351126
Iteration 24, loss = 0.03274608
Iteration 25, loss = 0.03191637
Iteration 26, loss = 0.03108340
Iteration 27, loss = 0.03040936
Iteration 28, loss = 0.02990656
Iteration 29, loss = 0.02959552
Iteration 30, loss = 0.02887011
Iteration 31, loss = 0.02863898
Iteration 32, loss = 0.02850899
Iteration 33, loss = 0.02816736
Iteration 34, loss = 0.02759649
Iteration 35, loss = 0.02723349
Iteration 36, loss = 0.02716092
Iteration 37, loss = 0.02707928
Iteration 38, loss = 0.02661867
Iteration 39, loss = 0.02664499
Iteration 40, loss = 0.02643193
Iteration 41, loss = 0.02629062
Iteration 42, loss = 0.02627614
Iteration 43, loss = 0.02591364
Iteration 44, loss = 0.02565436
Iteration 45, loss = 0.02558076
Iteration 46, loss = 0.02528727
Iteration 47, loss = 0.02543011
Iteration 48, loss = 0.02543345
Iteration 49, loss = 0.02510866
Iteration 50, loss = 0.02506084
Iteration 51, loss = 0.02507656
Iteration 52, loss = 0.02476779
Iteration 53, loss = 0.02478150
Iteration 54, loss = 0.02481449
Iteration 55, loss = 0.02486495
Iteration 56, loss = 0.02456673
Iteration 57, loss = 0.02458446
Iteration 58, loss = 0.02489502
Iteration 59, loss = 0.02462627
Iteration 60, loss = 0.02456353
Iteration 61, loss = 0.02468566
Iteration 62, loss = 0.02415161
Iteration 63, loss = 0.02429172
Iteration 64, loss = 0.02440867
Iteration 65, loss = 0.02452589
Iteration 66, loss = 0.02450995
Iteration 67, loss = 0.02442551
Iteration 68, loss = 0.02425323
Iteration 69, loss = 0.02414071
Iteration 70, loss = 0.02415255
Iteration 71, loss = 0.02402109
Iteration 72, loss = 0.02397553
Iteration 73, loss = 0.02406846
Iteration 74, loss = 0.02389659
Iteration 75, loss = 0.02382127
Iteration 76, loss = 0.02374280
Iteration 77, loss = 0.02380545
Iteration 78, loss = 0.02394683
Iteration 79, loss = 0.02402509
Iteration 80, loss = 0.02401172
Iteration 81, loss = 0.02379589
Iteration 82, loss = 0.02374854
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68228639
Iteration 2, loss = 0.60169143
Iteration 3, loss = 0.48391228
Iteration 4, loss = 0.36630233
Iteration 5, loss = 0.27107879
Iteration 6, loss = 0.20101338
Iteration 7, loss = 0.15292988
Iteration 8, loss = 0.12021133
Iteration 9, loss = 0.09784116
Iteration 10, loss = 0.08197458
Iteration 11, loss = 0.07028520
Iteration 12, loss = 0.06147916
Iteration 13, loss = 0.05478902
Iteration 14, loss = 0.04969163
Iteration 15, loss = 0.04547720
Iteration 16, loss = 0.04231365
Iteration 17, loss = 0.03945884
Iteration 18, loss = 0.03705968
Iteration 19, loss = 0.03542705
Iteration 20, loss = 0.03388296
Iteration 21, loss = 0.03212488
Iteration 22, loss = 0.03103555
Iteration 23, loss = 0.02992529
Iteration 24, loss = 0.02918169
Iteration 25, loss = 0.02848242
Iteration 26, loss = 0.02795984
Iteration 27, loss = 0.02719363
Iteration 28, loss = 0.02663108
Iteration 29, loss = 0.02612906
Iteration 30, loss = 0.02594933
Iteration 31, loss = 0.02561395
Iteration 32, loss = 0.02489826
Iteration 33, loss = 0.02484438
Iteration 34, loss = 0.02423428
Iteration 35, loss = 0.02398842
Iteration 36, loss = 0.02406430
Iteration 37, loss = 0.02361854
Iteration 38, loss = 0.02357049
Iteration 39, loss = 0.02340537
Iteration 40, loss = 0.02334746
Iteration 41, loss = 0.02291446
Iteration 42, loss = 0.02278038
Iteration 43, loss = 0.02277328
Iteration 44, loss = 0.02252822
Iteration 45, loss = 0.02274944
Iteration 46, loss = 0.02221432
Iteration 47, loss = 0.02219029
Iteration 48, loss = 0.02227974
Iteration 49, loss = 0.02218076
Iteration 50, loss = 0.02198909
Iteration 51, loss = 0.02176567
Iteration 52, loss = 0.02157071
Iteration 53, loss = 0.02204230
Iteration 54, loss = 0.02201911
Iteration 55, loss = 0.02175060
Iteration 56, loss = 0.02162987
Iteration 57, loss = 0.02143022
Iteration 58, loss = 0.02139231
Iteration 59, loss = 0.02168016
Iteration 60, loss = 0.02155900
Iteration 61, loss = 0.02146327
Iteration 62, loss = 0.02122142
Iteration 63, loss = 0.02132292
Iteration 64, loss = 0.02104978
Iteration 65, loss = 0.02122428
Iteration 66, loss = 0.02121513
Iteration 67, loss = 0.02095187
Iteration 68, loss = 0.02091182
Iteration 69, loss = 0.02158626
Iteration 70, loss = 0.02091550
Iteration 71, loss = 0.02092070
Iteration 72, loss = 0.02085701
Iteration 73, loss = 0.02086374
Iteration 74, loss = 0.02082098
Iteration 75, loss = 0.02097870
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68082492
Iteration 2, loss = 0.59959057
Iteration 3, loss = 0.48838198
Iteration 4, loss = 0.37712033
Iteration 5, loss = 0.28403860
Iteration 6, loss = 0.21319285
Iteration 7, loss = 0.16294352
Iteration 8, loss = 0.12796597
Iteration 9, loss = 0.10393138
Iteration 10, loss = 0.08638129
Iteration 11, loss = 0.07382385
Iteration 12, loss = 0.06450719
Iteration 13, loss = 0.05710486
Iteration 14, loss = 0.05160976
Iteration 15, loss = 0.04720025
Iteration 16, loss = 0.04370008
Iteration 17, loss = 0.04089740
Iteration 18, loss = 0.03860634
Iteration 19, loss = 0.03644826
Iteration 20, loss = 0.03478054
Iteration 21, loss = 0.03316608
Iteration 22, loss = 0.03208054
Iteration 23, loss = 0.03074161
Iteration 24, loss = 0.03015671
Iteration 25, loss = 0.02894582
Iteration 26, loss = 0.02832318
Iteration 27, loss = 0.02792208
Iteration 28, loss = 0.02729710
Iteration 29, loss = 0.02662268
Iteration 30, loss = 0.02618884
Iteration 31, loss = 0.02583338
Iteration 32, loss = 0.02555042
Iteration 33, loss = 0.02512349
Iteration 34, loss = 0.02496122
Iteration 35, loss = 0.02476216
Iteration 36, loss = 0.02422445
Iteration 37, loss = 0.02409639
Iteration 38, loss = 0.02384797
Iteration 39, loss = 0.02371146
Iteration 40, loss = 0.02345838
Iteration 41, loss = 0.02324415
Iteration 42, loss = 0.02355279
Iteration 43, loss = 0.02290380
Iteration 44, loss = 0.02286499
Iteration 45, loss = 0.02282751
Iteration 46, loss = 0.02254924
Iteration 47, loss = 0.02248979
Iteration 48, loss = 0.02245954
Iteration 49, loss = 0.02229320
Iteration 50, loss = 0.02233889
Iteration 51, loss = 0.02241866
Iteration 52, loss = 0.02195543
Iteration 53, loss = 0.02201176
Iteration 54, loss = 0.02191490
Iteration 55, loss = 0.02192879
Iteration 56, loss = 0.02196861
Iteration 57, loss = 0.02172804
Iteration 58, loss = 0.02165691
Iteration 59, loss = 0.02179848
Iteration 60, loss = 0.02164378
Iteration 61, loss = 0.02200728
Iteration 62, loss = 0.02134558
Iteration 63, loss = 0.02147949
Iteration 64, loss = 0.02150513
Iteration 65, loss = 0.02142190
Iteration 66, loss = 0.02150007
Iteration 67, loss = 0.02134700
Iteration 68, loss = 0.02130203
Iteration 69, loss = 0.02134257
Iteration 70, loss = 0.02133492
Iteration 71, loss = 0.02103227
Iteration 72, loss = 0.02103311
Iteration 73, loss = 0.02092125
Iteration 74, loss = 0.02099420
Iteration 75, loss = 0.02116140
Iteration 76, loss = 0.02106794
Iteration 77, loss = 0.02102478
Iteration 78, loss = 0.02093623
Iteration 79, loss = 0.02082979
Iteration 80, loss = 0.02124841
Iteration 81, loss = 0.02090418
Iteration 82, loss = 0.02095877
Iteration 83, loss = 0.02094822
Iteration 84, loss = 0.02073044
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71175204
Iteration 2, loss = 0.63459968
Iteration 3, loss = 0.52927735
Iteration 4, loss = 0.40636963
Iteration 5, loss = 0.29938092
Iteration 6, loss = 0.21971667
Iteration 7, loss = 0.16477802
Iteration 8, loss = 0.12810680
Iteration 9, loss = 0.10293308
Iteration 10, loss = 0.08574576
Iteration 11, loss = 0.07306082
Iteration 12, loss = 0.06365267
Iteration 13, loss = 0.05662244
Iteration 14, loss = 0.05079794
Iteration 15, loss = 0.04656375
Iteration 16, loss = 0.04307753
Iteration 17, loss = 0.04021425
Iteration 18, loss = 0.03779561
Iteration 19, loss = 0.03589124
Iteration 20, loss = 0.03410662
Iteration 21, loss = 0.03285420
Iteration 22, loss = 0.03147523
Iteration 23, loss = 0.03056202
Iteration 24, loss = 0.02953402
Iteration 25, loss = 0.02858851
Iteration 26, loss = 0.02786166
Iteration 27, loss = 0.02745496
Iteration 28, loss = 0.02681093
Iteration 29, loss = 0.02629127
Iteration 30, loss = 0.02589914
Iteration 31, loss = 0.02542971
Iteration 32, loss = 0.02526753
Iteration 33, loss = 0.02470573
Iteration 34, loss = 0.02450992
Iteration 35, loss = 0.02405483
Iteration 36, loss = 0.02382254
Iteration 37, loss = 0.02355326
Iteration 38, loss = 0.02358923
Iteration 39, loss = 0.02374261
Iteration 40, loss = 0.02308505
Iteration 41, loss = 0.02307128
Iteration 42, loss = 0.02280727
Iteration 43, loss = 0.02277279
Iteration 44, loss = 0.02254407
Iteration 45, loss = 0.02236803
Iteration 46, loss = 0.02229552
Iteration 47, loss = 0.02222924
Iteration 48, loss = 0.02211678
Iteration 49, loss = 0.02219451
Iteration 50, loss = 0.02226628
Iteration 51, loss = 0.02204824
Iteration 52, loss = 0.02178126
Iteration 53, loss = 0.02205515
Iteration 54, loss = 0.02180644
Iteration 55, loss = 0.02184839
Iteration 56, loss = 0.02142583
Iteration 57, loss = 0.02139456
Iteration 58, loss = 0.02137457
Iteration 59, loss = 0.02152965
Iteration 60, loss = 0.02133556
Iteration 61, loss = 0.02142682
Iteration 62, loss = 0.02140817
Iteration 63, loss = 0.02106801
Iteration 64, loss = 0.02130554
Iteration 65, loss = 0.02148176
Iteration 66, loss = 0.02118100
Iteration 67, loss = 0.02128322
Iteration 68, loss = 0.02133433
Iteration 69, loss = 0.02090574
Iteration 70, loss = 0.02108894
Iteration 71, loss = 0.02107190
Iteration 72, loss = 0.02072864
Iteration 73, loss = 0.02100727
Iteration 74, loss = 0.02071710
Iteration 75, loss = 0.02079647
Iteration 76, loss = 0.02084945
Iteration 77, loss = 0.02072114
Iteration 78, loss = 0.02108174
Iteration 79, loss = 0.02112836
Iteration 80, loss = 0.02077721
Iteration 81, loss = 0.02074100
Iteration 82, loss = 0.02046661
Iteration 83, loss = 0.02067152
Iteration 84, loss = 0.02062481
Iteration 85, loss = 0.02057449
Iteration 86, loss = 0.02053992
Iteration 87, loss = 0.02067118
Iteration 88, loss = 0.02073643
Iteration 89, loss = 0.02060700
Iteration 90, loss = 0.02052605
Iteration 91, loss = 0.02085735
Iteration 92, loss = 0.02070574
Iteration 93, loss = 0.02049856
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69842083
Iteration 2, loss = 0.63971242
Iteration 3, loss = 0.53805897
Iteration 4, loss = 0.42354834
Iteration 5, loss = 0.32155611
Iteration 6, loss = 0.24232675
Iteration 7, loss = 0.18443533
Iteration 8, loss = 0.14399882
Iteration 9, loss = 0.11589103
Iteration 10, loss = 0.09616670
Iteration 11, loss = 0.08189010
Iteration 12, loss = 0.07100074
Iteration 13, loss = 0.06326821
Iteration 14, loss = 0.05738124
Iteration 15, loss = 0.05221736
Iteration 16, loss = 0.04813702
Iteration 17, loss = 0.04491609
Iteration 18, loss = 0.04256871
Iteration 19, loss = 0.03989368
Iteration 20, loss = 0.03807280
Iteration 21, loss = 0.03674248
Iteration 22, loss = 0.03506608
Iteration 23, loss = 0.03385942
Iteration 24, loss = 0.03262706
Iteration 25, loss = 0.03212139
Iteration 26, loss = 0.03121664
Iteration 27, loss = 0.03045566
Iteration 28, loss = 0.02975271
Iteration 29, loss = 0.02908522
Iteration 30, loss = 0.02866675
Iteration 31, loss = 0.02845647
Iteration 32, loss = 0.02787890
Iteration 33, loss = 0.02743601
Iteration 34, loss = 0.02723078
Iteration 35, loss = 0.02689063
Iteration 36, loss = 0.02671999
Iteration 37, loss = 0.02627811
Iteration 38, loss = 0.02598363
Iteration 39, loss = 0.02595976
Iteration 40, loss = 0.02577371
Iteration 41, loss = 0.02535531
Iteration 42, loss = 0.02544189
Iteration 43, loss = 0.02509391
Iteration 44, loss = 0.02512491
Iteration 45, loss = 0.02471335
Iteration 46, loss = 0.02470337
Iteration 47, loss = 0.02471608
Iteration 48, loss = 0.02460631
Iteration 49, loss = 0.02446389
Iteration 50, loss = 0.02437192
Iteration 51, loss = 0.02446294
Iteration 52, loss = 0.02411753
Iteration 53, loss = 0.02399040
Iteration 54, loss = 0.02427771
Iteration 55, loss = 0.02388258
Iteration 56, loss = 0.02413166
Iteration 57, loss = 0.02401920
Iteration 58, loss = 0.02392941
Iteration 59, loss = 0.02392002
Iteration 60, loss = 0.02341001
Iteration 61, loss = 0.02344846
Iteration 62, loss = 0.02357152
Iteration 63, loss = 0.02345349
Iteration 64, loss = 0.02334535
Iteration 65, loss = 0.02327226
Iteration 66, loss = 0.02331085
Iteration 67, loss = 0.02338279
Iteration 68, loss = 0.02320180
Iteration 69, loss = 0.02320624
Iteration 70, loss = 0.02311264
Iteration 71, loss = 0.02318320
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67500719
Iteration 2, loss = 0.59955049
Iteration 3, loss = 0.49090799
Iteration 4, loss = 0.37920933
Iteration 5, loss = 0.28261578
Iteration 6, loss = 0.21061248
Iteration 7, loss = 0.16067330
Iteration 8, loss = 0.12649104
Iteration 9, loss = 0.10298304
Iteration 10, loss = 0.08660178
Iteration 11, loss = 0.07441360
Iteration 12, loss = 0.06559500
Iteration 13, loss = 0.05894212
Iteration 14, loss = 0.05372013
Iteration 15, loss = 0.04929241
Iteration 16, loss = 0.04617802
Iteration 17, loss = 0.04328988
Iteration 18, loss = 0.04087657
Iteration 19, loss = 0.03880015
Iteration 20, loss = 0.03728818
Iteration 21, loss = 0.03580225
Iteration 22, loss = 0.03471277
Iteration 23, loss = 0.03351126
Iteration 24, loss = 0.03274608
Iteration 25, loss = 0.03191637
Iteration 26, loss = 0.03108340
Iteration 27, loss = 0.03040936
Iteration 28, loss = 0.02990656
Iteration 29, loss = 0.02959552
Iteration 30, loss = 0.02887011
Iteration 31, loss = 0.02863898
Iteration 32, loss = 0.02850899
Iteration 33, loss = 0.02816736
Iteration 34, loss = 0.02759649
Iteration 35, loss = 0.02723349
Iteration 36, loss = 0.02716092
Iteration 37, loss = 0.02707928
Iteration 38, loss = 0.02661867
Iteration 39, loss = 0.02664499
Iteration 40, loss = 0.02643193
Iteration 41, loss = 0.02629062
Iteration 42, loss = 0.02627614
Iteration 43, loss = 0.02591364
Iteration 44, loss = 0.02565436
Iteration 45, loss = 0.02558076
Iteration 46, loss = 0.02528727
Iteration 47, loss = 0.02543011
Iteration 48, loss = 0.02543345
Iteration 49, loss = 0.02510866
Iteration 50, loss = 0.02506084
Iteration 51, loss = 0.02507656
Iteration 52, loss = 0.02476779
Iteration 53, loss = 0.02478150
Iteration 54, loss = 0.02481449
Iteration 55, loss = 0.02486495
Iteration 56, loss = 0.02456673
Iteration 57, loss = 0.02458446
Iteration 58, loss = 0.02489502
Iteration 59, loss = 0.02462627
Iteration 60, loss = 0.02456353
Iteration 61, loss = 0.02468566
Iteration 62, loss = 0.02415161
Iteration 63, loss = 0.02429172
Iteration 64, loss = 0.02440867
Iteration 65, loss = 0.02452589
Iteration 66, loss = 0.02450995
Iteration 67, loss = 0.02442551
Iteration 68, loss = 0.02425323
Iteration 69, loss = 0.02414071
Iteration 70, loss = 0.02415255
Iteration 71, loss = 0.02402109
Iteration 72, loss = 0.02397553
Iteration 73, loss = 0.02406846
Iteration 74, loss = 0.02389659
Iteration 75, loss = 0.02382127
Iteration 76, loss = 0.02374280
Iteration 77, loss = 0.02380545
Iteration 78, loss = 0.02394683
Iteration 79, loss = 0.02402509
Iteration 80, loss = 0.02401172
Iteration 81, loss = 0.02379589
Iteration 82, loss = 0.02374854
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68228639
Iteration 2, loss = 0.60169143
Iteration 3, loss = 0.48391228
Iteration 4, loss = 0.36630233
Iteration 5, loss = 0.27107879
Iteration 6, loss = 0.20101338
Iteration 7, loss = 0.15292988
Iteration 8, loss = 0.12021133
Iteration 9, loss = 0.09784116
Iteration 10, loss = 0.08197458
Iteration 11, loss = 0.07028520
Iteration 12, loss = 0.06147916
Iteration 13, loss = 0.05478902
Iteration 14, loss = 0.04969163
Iteration 15, loss = 0.04547720
Iteration 16, loss = 0.04231365
Iteration 17, loss = 0.03945884
Iteration 18, loss = 0.03705968
Iteration 19, loss = 0.03542705
Iteration 20, loss = 0.03388296
Iteration 21, loss = 0.03212488
Iteration 22, loss = 0.03103555
Iteration 23, loss = 0.02992529
Iteration 24, loss = 0.02918169
Iteration 25, loss = 0.02848242
Iteration 26, loss = 0.02795984
Iteration 27, loss = 0.02719363
Iteration 28, loss = 0.02663108
Iteration 29, loss = 0.02612906
Iteration 30, loss = 0.02594933
Iteration 31, loss = 0.02561395
Iteration 32, loss = 0.02489826
Iteration 33, loss = 0.02484438
Iteration 34, loss = 0.02423428
Iteration 35, loss = 0.02398842
Iteration 36, loss = 0.02406430
Iteration 37, loss = 0.02361854
Iteration 38, loss = 0.02357049
Iteration 39, loss = 0.02340537
Iteration 40, loss = 0.02334746
Iteration 41, loss = 0.02291446
Iteration 42, loss = 0.02278038
Iteration 43, loss = 0.02277328
Iteration 44, loss = 0.02252822
Iteration 45, loss = 0.02274944
Iteration 46, loss = 0.02221432
Iteration 47, loss = 0.02219029
Iteration 48, loss = 0.02227974
Iteration 49, loss = 0.02218076
Iteration 50, loss = 0.02198909
Iteration 51, loss = 0.02176567
Iteration 52, loss = 0.02157071
Iteration 53, loss = 0.02204230
Iteration 54, loss = 0.02201911
Iteration 55, loss = 0.02175060
Iteration 56, loss = 0.02162987
Iteration 57, loss = 0.02143022
Iteration 58, loss = 0.02139231
Iteration 59, loss = 0.02168016
Iteration 60, loss = 0.02155900
Iteration 61, loss = 0.02146327
Iteration 62, loss = 0.02122142
Iteration 63, loss = 0.02132292
Iteration 64, loss = 0.02104978
Iteration 65, loss = 0.02122428
Iteration 66, loss = 0.02121513
Iteration 67, loss = 0.02095187
Iteration 68, loss = 0.02091182
Iteration 69, loss = 0.02158626
Iteration 70, loss = 0.02091550
Iteration 71, loss = 0.02092070
Iteration 72, loss = 0.02085701
Iteration 73, loss = 0.02086374
Iteration 74, loss = 0.02082098
Iteration 75, loss = 0.02097870
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68082492
Iteration 2, loss = 0.59959057
Iteration 3, loss = 0.48838198
Iteration 4, loss = 0.37712033
Iteration 5, loss = 0.28403860
Iteration 6, loss = 0.21319285
Iteration 7, loss = 0.16294352
Iteration 8, loss = 0.12796597
Iteration 9, loss = 0.10393138
Iteration 10, loss = 0.08638129
Iteration 11, loss = 0.07382385
Iteration 12, loss = 0.06450719
Iteration 13, loss = 0.05710486
Iteration 14, loss = 0.05160976
Iteration 15, loss = 0.04720025
Iteration 16, loss = 0.04370008
Iteration 17, loss = 0.04089740
Iteration 18, loss = 0.03860634
Iteration 19, loss = 0.03644826
Iteration 20, loss = 0.03478054
Iteration 21, loss = 0.03316608
Iteration 22, loss = 0.03208054
Iteration 23, loss = 0.03074161
Iteration 24, loss = 0.03015671
Iteration 25, loss = 0.02894582
Iteration 26, loss = 0.02832318
Iteration 27, loss = 0.02792208
Iteration 28, loss = 0.02729710
Iteration 29, loss = 0.02662268
Iteration 30, loss = 0.02618884
Iteration 31, loss = 0.02583338
Iteration 32, loss = 0.02555042
Iteration 33, loss = 0.02512349
Iteration 34, loss = 0.02496122
Iteration 35, loss = 0.02476216
Iteration 36, loss = 0.02422445
Iteration 37, loss = 0.02409639
Iteration 38, loss = 0.02384797
Iteration 39, loss = 0.02371146
Iteration 40, loss = 0.02345838
Iteration 41, loss = 0.02324415
Iteration 42, loss = 0.02355279
Iteration 43, loss = 0.02290380
Iteration 44, loss = 0.02286499
Iteration 45, loss = 0.02282751
Iteration 46, loss = 0.02254924
Iteration 47, loss = 0.02248979
Iteration 48, loss = 0.02245954
Iteration 49, loss = 0.02229320
Iteration 50, loss = 0.02233889
Iteration 51, loss = 0.02241866
Iteration 52, loss = 0.02195543
Iteration 53, loss = 0.02201176
Iteration 54, loss = 0.02191490
Iteration 55, loss = 0.02192879
Iteration 56, loss = 0.02196861
Iteration 57, loss = 0.02172804
Iteration 58, loss = 0.02165691
Iteration 59, loss = 0.02179848
Iteration 60, loss = 0.02164378
Iteration 61, loss = 0.02200728
Iteration 62, loss = 0.02134558
Iteration 63, loss = 0.02147949
Iteration 64, loss = 0.02150513
Iteration 65, loss = 0.02142190
Iteration 66, loss = 0.02150007
Iteration 67, loss = 0.02134700
Iteration 68, loss = 0.02130203
Iteration 69, loss = 0.02134257
Iteration 70, loss = 0.02133492
Iteration 71, loss = 0.02103227
Iteration 72, loss = 0.02103311
Iteration 73, loss = 0.02092125
Iteration 74, loss = 0.02099420
Iteration 75, loss = 0.02116140
Iteration 76, loss = 0.02106794
Iteration 77, loss = 0.02102478
Iteration 78, loss = 0.02093623
Iteration 79, loss = 0.02082979
Iteration 80, loss = 0.02124841
Iteration 81, loss = 0.02090418
Iteration 82, loss = 0.02095877
Iteration 83, loss = 0.02094822
Iteration 84, loss = 0.02073044
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71175204
Iteration 2, loss = 0.63459968
Iteration 3, loss = 0.52927735
Iteration 4, loss = 0.40636963
Iteration 5, loss = 0.29938092
Iteration 6, loss = 0.21971667
Iteration 7, loss = 0.16477802
Iteration 8, loss = 0.12810680
Iteration 9, loss = 0.10293308
Iteration 10, loss = 0.08574576
Iteration 11, loss = 0.07306082
Iteration 12, loss = 0.06365267
Iteration 13, loss = 0.05662244
Iteration 14, loss = 0.05079794
Iteration 15, loss = 0.04656375
Iteration 16, loss = 0.04307753
Iteration 17, loss = 0.04021425
Iteration 18, loss = 0.03779561
Iteration 19, loss = 0.03589124
Iteration 20, loss = 0.03410662
Iteration 21, loss = 0.03285420
Iteration 22, loss = 0.03147523
Iteration 23, loss = 0.03056202
Iteration 24, loss = 0.02953402
Iteration 25, loss = 0.02858851
Iteration 26, loss = 0.02786166
Iteration 27, loss = 0.02745496
Iteration 28, loss = 0.02681093
Iteration 29, loss = 0.02629127
Iteration 30, loss = 0.02589914
Iteration 31, loss = 0.02542971
Iteration 32, loss = 0.02526753
Iteration 33, loss = 0.02470573
Iteration 34, loss = 0.02450992
Iteration 35, loss = 0.02405483
Iteration 36, loss = 0.02382254
Iteration 37, loss = 0.02355326
Iteration 38, loss = 0.02358923
Iteration 39, loss = 0.02374261
Iteration 40, loss = 0.02308505
Iteration 41, loss = 0.02307128
Iteration 42, loss = 0.02280727
Iteration 43, loss = 0.02277279
Iteration 44, loss = 0.02254407
Iteration 45, loss = 0.02236803
Iteration 46, loss = 0.02229552
Iteration 47, loss = 0.02222924
Iteration 48, loss = 0.02211678
Iteration 49, loss = 0.02219451
Iteration 50, loss = 0.02226628
Iteration 51, loss = 0.02204824
Iteration 52, loss = 0.02178126
Iteration 53, loss = 0.02205515
Iteration 54, loss = 0.02180644
Iteration 55, loss = 0.02184839
Iteration 56, loss = 0.02142583
Iteration 57, loss = 0.02139456
Iteration 58, loss = 0.02137457
Iteration 59, loss = 0.02152965
Iteration 60, loss = 0.02133556
Iteration 61, loss = 0.02142682
Iteration 62, loss = 0.02140817
Iteration 63, loss = 0.02106801
Iteration 64, loss = 0.02130554
Iteration 65, loss = 0.02148176
Iteration 66, loss = 0.02118100
Iteration 67, loss = 0.02128322
Iteration 68, loss = 0.02133433
Iteration 69, loss = 0.02090574
Iteration 70, loss = 0.02108894
Iteration 71, loss = 0.02107190
Iteration 72, loss = 0.02072864
Iteration 73, loss = 0.02100727
Iteration 74, loss = 0.02071710
Iteration 75, loss = 0.02079647
Iteration 76, loss = 0.02084945
Iteration 77, loss = 0.02072114
Iteration 78, loss = 0.02108174
Iteration 79, loss = 0.02112836
Iteration 80, loss = 0.02077721
Iteration 81, loss = 0.02074100
Iteration 82, loss = 0.02046661
Iteration 83, loss = 0.02067152
Iteration 84, loss = 0.02062481
Iteration 85, loss = 0.02057449
Iteration 86, loss = 0.02053992
Iteration 87, loss = 0.02067118
Iteration 88, loss = 0.02073643
Iteration 89, loss = 0.02060700
Iteration 90, loss = 0.02052605
Iteration 91, loss = 0.02085735
Iteration 92, loss = 0.02070574
Iteration 93, loss = 0.02049856
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69842083
Iteration 2, loss = 0.63971242
Iteration 3, loss = 0.53805897
Iteration 4, loss = 0.42354834
Iteration 5, loss = 0.32155611
Iteration 6, loss = 0.24232675
Iteration 7, loss = 0.18443533
Iteration 8, loss = 0.14399882
Iteration 9, loss = 0.11589103
Iteration 10, loss = 0.09616670
Iteration 11, loss = 0.08189010
Iteration 12, loss = 0.07100074
Iteration 13, loss = 0.06326821
Iteration 14, loss = 0.05738124
Iteration 15, loss = 0.05221736
Iteration 16, loss = 0.04813702
Iteration 17, loss = 0.04491609
Iteration 18, loss = 0.04256871
Iteration 19, loss = 0.03989368
Iteration 20, loss = 0.03807280
Iteration 21, loss = 0.03674248
Iteration 22, loss = 0.03506608
Iteration 23, loss = 0.03385942
Iteration 24, loss = 0.03262706
Iteration 25, loss = 0.03212139
Iteration 26, loss = 0.03121664
Iteration 27, loss = 0.03045566
Iteration 28, loss = 0.02975271
Iteration 29, loss = 0.02908522
Iteration 30, loss = 0.02866675
Iteration 31, loss = 0.02845647
Iteration 32, loss = 0.02787890
Iteration 33, loss = 0.02743601
Iteration 34, loss = 0.02723078
Iteration 35, loss = 0.02689063
Iteration 36, loss = 0.02671999
Iteration 37, loss = 0.02627811
Iteration 38, loss = 0.02598363
Iteration 39, loss = 0.02595976
Iteration 40, loss = 0.02577371
Iteration 41, loss = 0.02535531
Iteration 42, loss = 0.02544189
Iteration 43, loss = 0.02509391
Iteration 44, loss = 0.02512491
Iteration 45, loss = 0.02471335
Iteration 46, loss = 0.02470337
Iteration 47, loss = 0.02471608
Iteration 48, loss = 0.02460631
Iteration 49, loss = 0.02446389
Iteration 50, loss = 0.02437192
Iteration 51, loss = 0.02446294
Iteration 52, loss = 0.02411753
Iteration 53, loss = 0.02399040
Iteration 54, loss = 0.02427771
Iteration 55, loss = 0.02388258
Iteration 56, loss = 0.02413166
Iteration 57, loss = 0.02401920
Iteration 58, loss = 0.02392941
Iteration 59, loss = 0.02392002
Iteration 60, loss = 0.02341001
Iteration 61, loss = 0.02344846
Iteration 62, loss = 0.02357152
Iteration 63, loss = 0.02345349
Iteration 64, loss = 0.02334535
Iteration 65, loss = 0.02327226
Iteration 66, loss = 0.02331085
Iteration 67, loss = 0.02338279
Iteration 68, loss = 0.02320180
Iteration 69, loss = 0.02320624
Iteration 70, loss = 0.02311264
Iteration 71, loss = 0.02318320
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67500719
Iteration 2, loss = 0.59955049
Iteration 3, loss = 0.49090799
Iteration 4, loss = 0.37920933
Iteration 5, loss = 0.28261578
Iteration 6, loss = 0.21061248
Iteration 7, loss = 0.16067330
Iteration 8, loss = 0.12649104
Iteration 9, loss = 0.10298304
Iteration 10, loss = 0.08660178
Iteration 11, loss = 0.07441360
Iteration 12, loss = 0.06559500
Iteration 13, loss = 0.05894212
Iteration 14, loss = 0.05372013
Iteration 15, loss = 0.04929241
Iteration 16, loss = 0.04617802
Iteration 17, loss = 0.04328988
Iteration 18, loss = 0.04087657
Iteration 19, loss = 0.03880015
Iteration 20, loss = 0.03728818
Iteration 21, loss = 0.03580225
Iteration 22, loss = 0.03471277
Iteration 23, loss = 0.03351126
Iteration 24, loss = 0.03274608
Iteration 25, loss = 0.03191637
Iteration 26, loss = 0.03108340
Iteration 27, loss = 0.03040936
Iteration 28, loss = 0.02990656
Iteration 29, loss = 0.02959552
Iteration 30, loss = 0.02887011
Iteration 31, loss = 0.02863898
Iteration 32, loss = 0.02850899
Iteration 33, loss = 0.02816736
Iteration 34, loss = 0.02759649
Iteration 35, loss = 0.02723349
Iteration 36, loss = 0.02716092
Iteration 37, loss = 0.02707928
Iteration 38, loss = 0.02661867
Iteration 39, loss = 0.02664499
Iteration 40, loss = 0.02643193
Iteration 41, loss = 0.02629062
Iteration 42, loss = 0.02627614
Iteration 43, loss = 0.02591364
Iteration 44, loss = 0.02565436
Iteration 45, loss = 0.02558076
Iteration 46, loss = 0.02528727
Iteration 47, loss = 0.02543011
Iteration 48, loss = 0.02543345
Iteration 49, loss = 0.02510866
Iteration 50, loss = 0.02506084
Iteration 51, loss = 0.02507656
Iteration 52, loss = 0.02476779
Iteration 53, loss = 0.02478150
Iteration 54, loss = 0.02481449
Iteration 55, loss = 0.02486495
Iteration 56, loss = 0.02456673
Iteration 57, loss = 0.02458446
Iteration 58, loss = 0.02489502
Iteration 59, loss = 0.02462627
Iteration 60, loss = 0.02456353
Iteration 61, loss = 0.02468566
Iteration 62, loss = 0.02415161
Iteration 63, loss = 0.02429172
Iteration 64, loss = 0.02440867
Iteration 65, loss = 0.02452589
Iteration 66, loss = 0.02450995
Iteration 67, loss = 0.02442551
Iteration 68, loss = 0.02425323
Iteration 69, loss = 0.02414071
Iteration 70, loss = 0.02415255
Iteration 71, loss = 0.02402109
Iteration 72, loss = 0.02397553
Iteration 73, loss = 0.02406846
Iteration 74, loss = 0.02389659
Iteration 75, loss = 0.02382127
Iteration 76, loss = 0.02374280
Iteration 77, loss = 0.02380545
Iteration 78, loss = 0.02394683
Iteration 79, loss = 0.02402509
Iteration 80, loss = 0.02401172
Iteration 81, loss = 0.02379589
Iteration 82, loss = 0.02374854
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68228639
Iteration 2, loss = 0.60169143
Iteration 3, loss = 0.48391228
Iteration 4, loss = 0.36630233
Iteration 5, loss = 0.27107879
Iteration 6, loss = 0.20101338
Iteration 7, loss = 0.15292988
Iteration 8, loss = 0.12021133
Iteration 9, loss = 0.09784116
Iteration 10, loss = 0.08197458
Iteration 11, loss = 0.07028520
Iteration 12, loss = 0.06147916
Iteration 13, loss = 0.05478902
Iteration 14, loss = 0.04969163
Iteration 15, loss = 0.04547720
Iteration 16, loss = 0.04231365
Iteration 17, loss = 0.03945884
Iteration 18, loss = 0.03705968
Iteration 19, loss = 0.03542705
Iteration 20, loss = 0.03388296
Iteration 21, loss = 0.03212488
Iteration 22, loss = 0.03103555
Iteration 23, loss = 0.02992529
Iteration 24, loss = 0.02918169
Iteration 25, loss = 0.02848242
Iteration 26, loss = 0.02795984
Iteration 27, loss = 0.02719363
Iteration 28, loss = 0.02663108
Iteration 29, loss = 0.02612906
Iteration 30, loss = 0.02594933
Iteration 31, loss = 0.02561395
Iteration 32, loss = 0.02489826
Iteration 33, loss = 0.02484438
Iteration 34, loss = 0.02423428
Iteration 35, loss = 0.02398842
Iteration 36, loss = 0.02406430
Iteration 37, loss = 0.02361854
Iteration 38, loss = 0.02357049
Iteration 39, loss = 0.02340537
Iteration 40, loss = 0.02334746
Iteration 41, loss = 0.02291446
Iteration 42, loss = 0.02278038
Iteration 43, loss = 0.02277328
Iteration 44, loss = 0.02252822
Iteration 45, loss = 0.02274944
Iteration 46, loss = 0.02221432
Iteration 47, loss = 0.02219029
Iteration 48, loss = 0.02227974
Iteration 49, loss = 0.02218076
Iteration 50, loss = 0.02198909
Iteration 51, loss = 0.02176567
Iteration 52, loss = 0.02157071
Iteration 53, loss = 0.02204230
Iteration 54, loss = 0.02201911
Iteration 55, loss = 0.02175060
Iteration 56, loss = 0.02162987
Iteration 57, loss = 0.02143022
Iteration 58, loss = 0.02139231
Iteration 59, loss = 0.02168016
Iteration 60, loss = 0.02155900
Iteration 61, loss = 0.02146327
Iteration 62, loss = 0.02122142
Iteration 63, loss = 0.02132292
Iteration 64, loss = 0.02104978
Iteration 65, loss = 0.02122428
Iteration 66, loss = 0.02121513
Iteration 67, loss = 0.02095187
Iteration 68, loss = 0.02091182
Iteration 69, loss = 0.02158626
Iteration 70, loss = 0.02091550
Iteration 71, loss = 0.02092070
Iteration 72, loss = 0.02085701
Iteration 73, loss = 0.02086374
Iteration 74, loss = 0.02082098
Iteration 75, loss = 0.02097870
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68082492
Iteration 2, loss = 0.59959057
Iteration 3, loss = 0.48838198
Iteration 4, loss = 0.37712033
Iteration 5, loss = 0.28403860
Iteration 6, loss = 0.21319285
Iteration 7, loss = 0.16294352
Iteration 8, loss = 0.12796597
Iteration 9, loss = 0.10393138
Iteration 10, loss = 0.08638129
Iteration 11, loss = 0.07382385
Iteration 12, loss = 0.06450719
Iteration 13, loss = 0.05710486
Iteration 14, loss = 0.05160976
Iteration 15, loss = 0.04720025
Iteration 16, loss = 0.04370008
Iteration 17, loss = 0.04089740
Iteration 18, loss = 0.03860634
Iteration 19, loss = 0.03644826
Iteration 20, loss = 0.03478054
Iteration 21, loss = 0.03316608
Iteration 22, loss = 0.03208054
Iteration 23, loss = 0.03074161
Iteration 24, loss = 0.03015671
Iteration 25, loss = 0.02894582
Iteration 26, loss = 0.02832318
Iteration 27, loss = 0.02792208
Iteration 28, loss = 0.02729710
Iteration 29, loss = 0.02662268
Iteration 30, loss = 0.02618884
Iteration 31, loss = 0.02583338
Iteration 32, loss = 0.02555042
Iteration 33, loss = 0.02512349
Iteration 34, loss = 0.02496122
Iteration 35, loss = 0.02476216
Iteration 36, loss = 0.02422445
Iteration 37, loss = 0.02409639
Iteration 38, loss = 0.02384797
Iteration 39, loss = 0.02371146
Iteration 40, loss = 0.02345838
Iteration 41, loss = 0.02324415
Iteration 42, loss = 0.02355279
Iteration 43, loss = 0.02290380
Iteration 44, loss = 0.02286499
Iteration 45, loss = 0.02282751
Iteration 46, loss = 0.02254924
Iteration 47, loss = 0.02248979
Iteration 48, loss = 0.02245954
Iteration 49, loss = 0.02229320
Iteration 50, loss = 0.02233889
Iteration 51, loss = 0.02241866
Iteration 52, loss = 0.02195543
Iteration 53, loss = 0.02201176
Iteration 54, loss = 0.02191490
Iteration 55, loss = 0.02192879
Iteration 56, loss = 0.02196861
Iteration 57, loss = 0.02172804
Iteration 58, loss = 0.02165691
Iteration 59, loss = 0.02179848
Iteration 60, loss = 0.02164378
Iteration 61, loss = 0.02200728
Iteration 62, loss = 0.02134558
Iteration 63, loss = 0.02147949
Iteration 64, loss = 0.02150513
Iteration 65, loss = 0.02142190
Iteration 66, loss = 0.02150007
Iteration 67, loss = 0.02134700
Iteration 68, loss = 0.02130203
Iteration 69, loss = 0.02134257
Iteration 70, loss = 0.02133492
Iteration 71, loss = 0.02103227
Iteration 72, loss = 0.02103311
Iteration 73, loss = 0.02092125
Iteration 74, loss = 0.02099420
Iteration 75, loss = 0.02116140
Iteration 76, loss = 0.02106794
Iteration 77, loss = 0.02102478
Iteration 78, loss = 0.02093623
Iteration 79, loss = 0.02082979
Iteration 80, loss = 0.02124841
Iteration 81, loss = 0.02090418
Iteration 82, loss = 0.02095877
Iteration 83, loss = 0.02094822
Iteration 84, loss = 0.02073044
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71175204
Iteration 2, loss = 0.63459968
Iteration 3, loss = 0.52927735
Iteration 4, loss = 0.40636963
Iteration 5, loss = 0.29938092
Iteration 6, loss = 0.21971667
Iteration 7, loss = 0.16477802
Iteration 8, loss = 0.12810680
Iteration 9, loss = 0.10293308
Iteration 10, loss = 0.08574576
Iteration 11, loss = 0.07306082
Iteration 12, loss = 0.06365267
Iteration 13, loss = 0.05662244
Iteration 14, loss = 0.05079794
Iteration 15, loss = 0.04656375
Iteration 16, loss = 0.04307753
Iteration 17, loss = 0.04021425
Iteration 18, loss = 0.03779561
Iteration 19, loss = 0.03589124
Iteration 20, loss = 0.03410662
Iteration 21, loss = 0.03285420
Iteration 22, loss = 0.03147523
Iteration 23, loss = 0.03056202
Iteration 24, loss = 0.02953402
Iteration 25, loss = 0.02858851
Iteration 26, loss = 0.02786166
Iteration 27, loss = 0.02745496
Iteration 28, loss = 0.02681093
Iteration 29, loss = 0.02629127
Iteration 30, loss = 0.02589914
Iteration 31, loss = 0.02542971
Iteration 32, loss = 0.02526753
Iteration 33, loss = 0.02470573
Iteration 34, loss = 0.02450992
Iteration 35, loss = 0.02405483
Iteration 36, loss = 0.02382254
Iteration 37, loss = 0.02355326
Iteration 38, loss = 0.02358923
Iteration 39, loss = 0.02374261
Iteration 40, loss = 0.02308505
Iteration 41, loss = 0.02307128
Iteration 42, loss = 0.02280727
Iteration 43, loss = 0.02277279
Iteration 44, loss = 0.02254407
Iteration 45, loss = 0.02236803
Iteration 46, loss = 0.02229552
Iteration 47, loss = 0.02222924
Iteration 48, loss = 0.02211678
Iteration 49, loss = 0.02219451
Iteration 50, loss = 0.02226628
Iteration 51, loss = 0.02204824
Iteration 52, loss = 0.02178126
Iteration 53, loss = 0.02205515
Iteration 54, loss = 0.02180644
Iteration 55, loss = 0.02184839
Iteration 56, loss = 0.02142583
Iteration 57, loss = 0.02139456
Iteration 58, loss = 0.02137457
Iteration 59, loss = 0.02152965
Iteration 60, loss = 0.02133556
Iteration 61, loss = 0.02142682
Iteration 62, loss = 0.02140817
Iteration 63, loss = 0.02106801
Iteration 64, loss = 0.02130554
Iteration 65, loss = 0.02148176
Iteration 66, loss = 0.02118100
Iteration 67, loss = 0.02128322
Iteration 68, loss = 0.02133433
Iteration 69, loss = 0.02090574
Iteration 70, loss = 0.02108894
Iteration 71, loss = 0.02107190
Iteration 72, loss = 0.02072864
Iteration 73, loss = 0.02100727
Iteration 74, loss = 0.02071710
Iteration 75, loss = 0.02079647
Iteration 76, loss = 0.02084945
Iteration 77, loss = 0.02072114
Iteration 78, loss = 0.02108174
Iteration 79, loss = 0.02112836
Iteration 80, loss = 0.02077721
Iteration 81, loss = 0.02074100
Iteration 82, loss = 0.02046661
Iteration 83, loss = 0.02067152
Iteration 84, loss = 0.02062481
Iteration 85, loss = 0.02057449
Iteration 86, loss = 0.02053992
Iteration 87, loss = 0.02067118
Iteration 88, loss = 0.02073643
Iteration 89, loss = 0.02060700
Iteration 90, loss = 0.02052605
Iteration 91, loss = 0.02085735
Iteration 92, loss = 0.02070574
Iteration 93, loss = 0.02049856
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69842083
Iteration 2, loss = 0.63971242
Iteration 3, loss = 0.53805897
Iteration 4, loss = 0.42354834
Iteration 5, loss = 0.32155611
Iteration 6, loss = 0.24232675
Iteration 7, loss = 0.18443533
Iteration 8, loss = 0.14399882
Iteration 9, loss = 0.11589103
Iteration 10, loss = 0.09616670
Iteration 11, loss = 0.08189010
Iteration 12, loss = 0.07100074
Iteration 13, loss = 0.06326821
Iteration 14, loss = 0.05738124
Iteration 15, loss = 0.05221736
Iteration 16, loss = 0.04813702
Iteration 17, loss = 0.04491609
Iteration 18, loss = 0.04256871
Iteration 19, loss = 0.03989368
Iteration 20, loss = 0.03807280
Iteration 21, loss = 0.03674248
Iteration 22, loss = 0.03506608
Iteration 23, loss = 0.03385942
Iteration 24, loss = 0.03262706
Iteration 25, loss = 0.03212139
Iteration 26, loss = 0.03121664
Iteration 27, loss = 0.03045566
Iteration 28, loss = 0.02975271
Iteration 29, loss = 0.02908522
Iteration 30, loss = 0.02866675
Iteration 31, loss = 0.02845647
Iteration 32, loss = 0.02787890
Iteration 33, loss = 0.02743601
Iteration 34, loss = 0.02723078
Iteration 35, loss = 0.02689063
Iteration 36, loss = 0.02671999
Iteration 37, loss = 0.02627811
Iteration 38, loss = 0.02598363
Iteration 39, loss = 0.02595976
Iteration 40, loss = 0.02577371
Iteration 41, loss = 0.02535531
Iteration 42, loss = 0.02544189
Iteration 43, loss = 0.02509391
Iteration 44, loss = 0.02512491
Iteration 45, loss = 0.02471335
Iteration 46, loss = 0.02470337
Iteration 47, loss = 0.02471608
Iteration 48, loss = 0.02460631
Iteration 49, loss = 0.02446389
Iteration 50, loss = 0.02437192
Iteration 51, loss = 0.02446294
Iteration 52, loss = 0.02411753
Iteration 53, loss = 0.02399040
Iteration 54, loss = 0.02427771
Iteration 55, loss = 0.02388258
Iteration 56, loss = 0.02413166
Iteration 57, loss = 0.02401920
Iteration 58, loss = 0.02392941
Iteration 59, loss = 0.02392002
Iteration 60, loss = 0.02341001
Iteration 61, loss = 0.02344846
Iteration 62, loss = 0.02357152
Iteration 63, loss = 0.02345349
Iteration 64, loss = 0.02334535
Iteration 65, loss = 0.02327226
Iteration 66, loss = 0.02331085
Iteration 67, loss = 0.02338279
Iteration 68, loss = 0.02320180
Iteration 69, loss = 0.02320624
Iteration 70, loss = 0.02311264
Iteration 71, loss = 0.02318320
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67500719
Iteration 2, loss = 0.59955049
Iteration 3, loss = 0.49090799
Iteration 4, loss = 0.37920933
Iteration 5, loss = 0.28261578
Iteration 6, loss = 0.21061248
Iteration 7, loss = 0.16067330
Iteration 8, loss = 0.12649104
Iteration 9, loss = 0.10298304
Iteration 10, loss = 0.08660178
Iteration 11, loss = 0.07441360
Iteration 12, loss = 0.06559500
Iteration 13, loss = 0.05894212
Iteration 14, loss = 0.05372013
Iteration 15, loss = 0.04929241
Iteration 16, loss = 0.04617802
Iteration 17, loss = 0.04328988
Iteration 18, loss = 0.04087657
Iteration 19, loss = 0.03880015
Iteration 20, loss = 0.03728818
Iteration 21, loss = 0.03580225
Iteration 22, loss = 0.03471277
Iteration 23, loss = 0.03351126
Iteration 24, loss = 0.03274608
Iteration 25, loss = 0.03191637
Iteration 26, loss = 0.03108340
Iteration 27, loss = 0.03040936
Iteration 28, loss = 0.02990656
Iteration 29, loss = 0.02959552
Iteration 30, loss = 0.02887011
Iteration 31, loss = 0.02863898
Iteration 32, loss = 0.02850899
Iteration 33, loss = 0.02816736
Iteration 34, loss = 0.02759649
Iteration 35, loss = 0.02723349
Iteration 36, loss = 0.02716092
Iteration 37, loss = 0.02707928
Iteration 38, loss = 0.02661867
Iteration 39, loss = 0.02664499
Iteration 40, loss = 0.02643193
Iteration 41, loss = 0.02629062
Iteration 42, loss = 0.02627614
Iteration 43, loss = 0.02591364
Iteration 44, loss = 0.02565436
Iteration 45, loss = 0.02558076
Iteration 46, loss = 0.02528727
Iteration 47, loss = 0.02543011
Iteration 48, loss = 0.02543345
Iteration 49, loss = 0.02510866
Iteration 50, loss = 0.02506084
Iteration 51, loss = 0.02507656
Iteration 52, loss = 0.02476779
Iteration 53, loss = 0.02478150
Iteration 54, loss = 0.02481449
Iteration 55, loss = 0.02486495
Iteration 56, loss = 0.02456673
Iteration 57, loss = 0.02458446
Iteration 58, loss = 0.02489502
Iteration 59, loss = 0.02462627
Iteration 60, loss = 0.02456353
Iteration 61, loss = 0.02468566
Iteration 62, loss = 0.02415161
Iteration 63, loss = 0.02429172
Iteration 64, loss = 0.02440867
Iteration 65, loss = 0.02452589
Iteration 66, loss = 0.02450995
Iteration 67, loss = 0.02442551
Iteration 68, loss = 0.02425323
Iteration 69, loss = 0.02414071
Iteration 70, loss = 0.02415255
Iteration 71, loss = 0.02402109
Iteration 72, loss = 0.02397553
Iteration 73, loss = 0.02406846
Iteration 74, loss = 0.02389659
Iteration 75, loss = 0.02382127
Iteration 76, loss = 0.02374280
Iteration 77, loss = 0.02380545
Iteration 78, loss = 0.02394683
Iteration 79, loss = 0.02402509
Iteration 80, loss = 0.02401172
Iteration 81, loss = 0.02379589
Iteration 82, loss = 0.02374854
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68228639
Iteration 2, loss = 0.60169143
Iteration 3, loss = 0.48391228
Iteration 4, loss = 0.36630233
Iteration 5, loss = 0.27107879
Iteration 6, loss = 0.20101338
Iteration 7, loss = 0.15292988
Iteration 8, loss = 0.12021133
Iteration 9, loss = 0.09784116
Iteration 10, loss = 0.08197458
Iteration 11, loss = 0.07028520
Iteration 12, loss = 0.06147916
Iteration 13, loss = 0.05478902
Iteration 14, loss = 0.04969163
Iteration 15, loss = 0.04547720
Iteration 16, loss = 0.04231365
Iteration 17, loss = 0.03945884
Iteration 18, loss = 0.03705968
Iteration 19, loss = 0.03542705
Iteration 20, loss = 0.03388296
Iteration 21, loss = 0.03212488
Iteration 22, loss = 0.03103555
Iteration 23, loss = 0.02992529
Iteration 24, loss = 0.02918169
Iteration 25, loss = 0.02848242
Iteration 26, loss = 0.02795984
Iteration 27, loss = 0.02719363
Iteration 28, loss = 0.02663108
Iteration 29, loss = 0.02612906
Iteration 30, loss = 0.02594933
Iteration 31, loss = 0.02561395
Iteration 32, loss = 0.02489826
Iteration 33, loss = 0.02484438
Iteration 34, loss = 0.02423428
Iteration 35, loss = 0.02398842
Iteration 36, loss = 0.02406430
Iteration 37, loss = 0.02361854
Iteration 38, loss = 0.02357049
Iteration 39, loss = 0.02340537
Iteration 40, loss = 0.02334746
Iteration 41, loss = 0.02291446
Iteration 42, loss = 0.02278038
Iteration 43, loss = 0.02277328
Iteration 44, loss = 0.02252822
Iteration 45, loss = 0.02274944
Iteration 46, loss = 0.02221432
Iteration 47, loss = 0.02219029
Iteration 48, loss = 0.02227974
Iteration 49, loss = 0.02218076
Iteration 50, loss = 0.02198909
Iteration 51, loss = 0.02176567
Iteration 52, loss = 0.02157071
Iteration 53, loss = 0.02204230
Iteration 54, loss = 0.02201911
Iteration 55, loss = 0.02175060
Iteration 56, loss = 0.02162987
Iteration 57, loss = 0.02143022
Iteration 58, loss = 0.02139231
Iteration 59, loss = 0.02168016
Iteration 60, loss = 0.02155900
Iteration 61, loss = 0.02146327
Iteration 62, loss = 0.02122142
Iteration 63, loss = 0.02132292
Iteration 64, loss = 0.02104978
Iteration 65, loss = 0.02122428
Iteration 66, loss = 0.02121513
Iteration 67, loss = 0.02095187
Iteration 68, loss = 0.02091182
Iteration 69, loss = 0.02158626
Iteration 70, loss = 0.02091550
Iteration 71, loss = 0.02092070
Iteration 72, loss = 0.02085701
Iteration 73, loss = 0.02086374
Iteration 74, loss = 0.02082098
Iteration 75, loss = 0.02097870
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68082492
Iteration 2, loss = 0.59959057
Iteration 3, loss = 0.48838198
Iteration 4, loss = 0.37712033
Iteration 5, loss = 0.28403860
Iteration 6, loss = 0.21319285
Iteration 7, loss = 0.16294352
Iteration 8, loss = 0.12796597
Iteration 9, loss = 0.10393138
Iteration 10, loss = 0.08638129
Iteration 11, loss = 0.07382385
Iteration 12, loss = 0.06450719
Iteration 13, loss = 0.05710486
Iteration 14, loss = 0.05160976
Iteration 15, loss = 0.04720025
Iteration 16, loss = 0.04370008
Iteration 17, loss = 0.04089740
Iteration 18, loss = 0.03860634
Iteration 19, loss = 0.03644826
Iteration 20, loss = 0.03478054
Iteration 21, loss = 0.03316608
Iteration 22, loss = 0.03208054
Iteration 23, loss = 0.03074161
Iteration 24, loss = 0.03015671
Iteration 25, loss = 0.02894582
Iteration 26, loss = 0.02832318
Iteration 27, loss = 0.02792208
Iteration 28, loss = 0.02729710
Iteration 29, loss = 0.02662268
Iteration 30, loss = 0.02618884
Iteration 31, loss = 0.02583338
Iteration 32, loss = 0.02555042
Iteration 33, loss = 0.02512349
Iteration 34, loss = 0.02496122
Iteration 35, loss = 0.02476216
Iteration 36, loss = 0.02422445
Iteration 37, loss = 0.02409639
Iteration 38, loss = 0.02384797
Iteration 39, loss = 0.02371146
Iteration 40, loss = 0.02345838
Iteration 41, loss = 0.02324415
Iteration 42, loss = 0.02355279
Iteration 43, loss = 0.02290380
Iteration 44, loss = 0.02286499
Iteration 45, loss = 0.02282751
Iteration 46, loss = 0.02254924
Iteration 47, loss = 0.02248979
Iteration 48, loss = 0.02245954
Iteration 49, loss = 0.02229320
Iteration 50, loss = 0.02233889
Iteration 51, loss = 0.02241866
Iteration 52, loss = 0.02195543
Iteration 53, loss = 0.02201176
Iteration 54, loss = 0.02191490
Iteration 55, loss = 0.02192879
Iteration 56, loss = 0.02196861
Iteration 57, loss = 0.02172804
Iteration 58, loss = 0.02165691
Iteration 59, loss = 0.02179848
Iteration 60, loss = 0.02164378
Iteration 61, loss = 0.02200728
Iteration 62, loss = 0.02134558
Iteration 63, loss = 0.02147949
Iteration 64, loss = 0.02150513
Iteration 65, loss = 0.02142190
Iteration 66, loss = 0.02150007
Iteration 67, loss = 0.02134700
Iteration 68, loss = 0.02130203
Iteration 69, loss = 0.02134257
Iteration 70, loss = 0.02133492
Iteration 71, loss = 0.02103227
Iteration 72, loss = 0.02103311
Iteration 73, loss = 0.02092125
Iteration 74, loss = 0.02099420
Iteration 75, loss = 0.02116140
Iteration 76, loss = 0.02106794
Iteration 77, loss = 0.02102478
Iteration 78, loss = 0.02093623
Iteration 79, loss = 0.02082979
Iteration 80, loss = 0.02124841
Iteration 81, loss = 0.02090418
Iteration 82, loss = 0.02095877
Iteration 83, loss = 0.02094822
Iteration 84, loss = 0.02073044
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71175204
Iteration 2, loss = 0.63459968
Iteration 3, loss = 0.52927735
Iteration 4, loss = 0.40636963
Iteration 5, loss = 0.29938092
Iteration 6, loss = 0.21971667
Iteration 7, loss = 0.16477802
Iteration 8, loss = 0.12810680
Iteration 9, loss = 0.10293308
Iteration 10, loss = 0.08574576
Iteration 11, loss = 0.07306082
Iteration 12, loss = 0.06365267
Iteration 13, loss = 0.05662244
Iteration 14, loss = 0.05079794
Iteration 15, loss = 0.04656375
Iteration 16, loss = 0.04307753
Iteration 17, loss = 0.04021425
Iteration 18, loss = 0.03779561
Iteration 19, loss = 0.03589124
Iteration 20, loss = 0.03410662
Iteration 21, loss = 0.03285420
Iteration 22, loss = 0.03147523
Iteration 23, loss = 0.03056202
Iteration 24, loss = 0.02953402
Iteration 25, loss = 0.02858851
Iteration 26, loss = 0.02786166
Iteration 27, loss = 0.02745496
Iteration 28, loss = 0.02681093
Iteration 29, loss = 0.02629127
Iteration 30, loss = 0.02589914
Iteration 31, loss = 0.02542971
Iteration 32, loss = 0.02526753
Iteration 33, loss = 0.02470573
Iteration 34, loss = 0.02450992
Iteration 35, loss = 0.02405483
Iteration 36, loss = 0.02382254
Iteration 37, loss = 0.02355326
Iteration 38, loss = 0.02358923
Iteration 39, loss = 0.02374261
Iteration 40, loss = 0.02308505
Iteration 41, loss = 0.02307128
Iteration 42, loss = 0.02280727
Iteration 43, loss = 0.02277279
Iteration 44, loss = 0.02254407
Iteration 45, loss = 0.02236803
Iteration 46, loss = 0.02229552
Iteration 47, loss = 0.02222924
Iteration 48, loss = 0.02211678
Iteration 49, loss = 0.02219451
Iteration 50, loss = 0.02226628
Iteration 51, loss = 0.02204824
Iteration 52, loss = 0.02178126
Iteration 53, loss = 0.02205515
Iteration 54, loss = 0.02180644
Iteration 55, loss = 0.02184839
Iteration 56, loss = 0.02142583
Iteration 57, loss = 0.02139456
Iteration 58, loss = 0.02137457
Iteration 59, loss = 0.02152965
Iteration 60, loss = 0.02133556
Iteration 61, loss = 0.02142682
Iteration 62, loss = 0.02140817
Iteration 63, loss = 0.02106801
Iteration 64, loss = 0.02130554
Iteration 65, loss = 0.02148176
Iteration 66, loss = 0.02118100
Iteration 67, loss = 0.02128322
Iteration 68, loss = 0.02133433
Iteration 69, loss = 0.02090574
Iteration 70, loss = 0.02108894
Iteration 71, loss = 0.02107190
Iteration 72, loss = 0.02072864
Iteration 73, loss = 0.02100727
Iteration 74, loss = 0.02071710
Iteration 75, loss = 0.02079647
Iteration 76, loss = 0.02084945
Iteration 77, loss = 0.02072114
Iteration 78, loss = 0.02108174
Iteration 79, loss = 0.02112836
Iteration 80, loss = 0.02077721
Iteration 81, loss = 0.02074100
Iteration 82, loss = 0.02046661
Iteration 83, loss = 0.02067152
Iteration 84, loss = 0.02062481
Iteration 85, loss = 0.02057449
Iteration 86, loss = 0.02053992
Iteration 87, loss = 0.02067118
Iteration 88, loss = 0.02073643
Iteration 89, loss = 0.02060700
Iteration 90, loss = 0.02052605
Iteration 91, loss = 0.02085735
Iteration 92, loss = 0.02070574
Iteration 93, loss = 0.02049856
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67805123
Iteration 2, loss = 0.58538575
Iteration 3, loss = 0.44615335
Iteration 4, loss = 0.31794999
Iteration 5, loss = 0.22389712
Iteration 6, loss = 0.16281220
Iteration 7, loss = 0.12373450
Iteration 8, loss = 0.09826631
Iteration 9, loss = 0.08170962
Iteration 10, loss = 0.06972668
Iteration 11, loss = 0.06135698
Iteration 12, loss = 0.05452736
Iteration 13, loss = 0.04985652
Iteration 14, loss = 0.04592994
Iteration 15, loss = 0.04324766
Iteration 16, loss = 0.04045284
Iteration 17, loss = 0.03833188
Iteration 18, loss = 0.03690246
Iteration 19, loss = 0.03519732
Iteration 20, loss = 0.03414195
Iteration 21, loss = 0.03303811
Iteration 22, loss = 0.03231807
Iteration 23, loss = 0.03148767
Iteration 24, loss = 0.03098371
Iteration 25, loss = 0.03008411
Iteration 26, loss = 0.02952417
Iteration 27, loss = 0.02946636
Iteration 28, loss = 0.02906409
Iteration 29, loss = 0.02870672
Iteration 30, loss = 0.02825319
Iteration 31, loss = 0.02810131
Iteration 32, loss = 0.02775559
Iteration 33, loss = 0.02736446
Iteration 34, loss = 0.02712688
Iteration 35, loss = 0.02704252
Iteration 36, loss = 0.02701472
Iteration 37, loss = 0.02666908
Iteration 38, loss = 0.02678810
Iteration 39, loss = 0.02666818
Iteration 40, loss = 0.02635887
Iteration 41, loss = 0.02627637
Iteration 42, loss = 0.02614170
Iteration 43, loss = 0.02588157
Iteration 44, loss = 0.02611581
Iteration 45, loss = 0.02555447
Iteration 46, loss = 0.02586062
Iteration 47, loss = 0.02546049
Iteration 48, loss = 0.02574832
Iteration 49, loss = 0.02562757
Iteration 50, loss = 0.02557637
Iteration 51, loss = 0.02526186
Iteration 52, loss = 0.02529941
Iteration 53, loss = 0.02550794
Iteration 54, loss = 0.02523631
Iteration 55, loss = 0.02508221
Iteration 56, loss = 0.02500535
Iteration 57, loss = 0.02534385
Iteration 58, loss = 0.02495017
Iteration 59, loss = 0.02475601
Iteration 60, loss = 0.02507836
Iteration 61, loss = 0.02491971
Iteration 62, loss = 0.02471888
Iteration 63, loss = 0.02465411
Iteration 64, loss = 0.02468770
Iteration 65, loss = 0.02503635
Iteration 66, loss = 0.02459664
Iteration 67, loss = 0.02472358
Iteration 68, loss = 0.02464840
Iteration 69, loss = 0.02454599
Iteration 70, loss = 0.02443704
Iteration 71, loss = 0.02457061
Iteration 72, loss = 0.02488777
Iteration 73, loss = 0.02441175
Iteration 74, loss = 0.02450725
Iteration 75, loss = 0.02479848
Iteration 76, loss = 0.02434834
Iteration 77, loss = 0.02435871
Iteration 78, loss = 0.02429928
Iteration 79, loss = 0.02428217
Iteration 80, loss = 0.02442356
Iteration 81, loss = 0.02439838
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on prepended_v2_no_emojis_mentions...
Iteration 1, loss = 0.66531501
Iteration 2, loss = 0.65030412
Iteration 3, loss = 0.63735801
Iteration 4, loss = 0.62421137
Iteration 5, loss = 0.61052499
Iteration 6, loss = 0.59582446
Iteration 7, loss = 0.58002387
Iteration 8, loss = 0.56287829
Iteration 9, loss = 0.54456374
Iteration 10, loss = 0.52506158
Iteration 11, loss = 0.50447340
Iteration 12, loss = 0.48305126
Iteration 13, loss = 0.46117189
Iteration 14, loss = 0.43877714
Iteration 15, loss = 0.41636787
Iteration 16, loss = 0.39410912
Iteration 17, loss = 0.37231709
Iteration 18, loss = 0.35092064
Iteration 19, loss = 0.33036813
Iteration 20, loss = 0.31046895
Iteration 21, loss = 0.29144581
Iteration 22, loss = 0.27329823
Iteration 23, loss = 0.25627284
Iteration 24, loss = 0.24015067
Iteration 25, loss = 0.22484244
Iteration 26, loss = 0.21076357
Iteration 27, loss = 0.19739459
Iteration 28, loss = 0.18508844
Iteration 29, loss = 0.17334503
Iteration 30, loss = 0.16273435
Iteration 31, loss = 0.15290961
Iteration 32, loss = 0.14367749
Iteration 33, loss = 0.13514963
Iteration 34, loss = 0.12724421
Iteration 35, loss = 0.12001524
Iteration 36, loss = 0.11322431
Iteration 37, loss = 0.10698096
Iteration 38, loss = 0.10115218
Iteration 39, loss = 0.09575747
Iteration 40, loss = 0.09084679
Iteration 41, loss = 0.08621552
Iteration 42, loss = 0.08195228
Iteration 43, loss = 0.07801979
Iteration 44, loss = 0.07431146
Iteration 45, loss = 0.07088788
Iteration 46, loss = 0.06764593
Iteration 47, loss = 0.06467540
Iteration 48, loss = 0.06186122
Iteration 49, loss = 0.05927119
Iteration 50, loss = 0.05680496
Iteration 51, loss = 0.05454497
Iteration 52, loss = 0.05245808
Iteration 53, loss = 0.05036852
Iteration 54, loss = 0.04848747
Iteration 55, loss = 0.04672481
Iteration 56, loss = 0.04504391
Iteration 57, loss = 0.04346982
Iteration 58, loss = 0.04196953
Iteration 59, loss = 0.04057638
Iteration 60, loss = 0.03925621
Iteration 61, loss = 0.03801428
Iteration 62, loss = 0.03684254
Iteration 63, loss = 0.03568399
Iteration 64, loss = 0.03462489
Iteration 65, loss = 0.03363469
Iteration 66, loss = 0.03266873
Iteration 67, loss = 0.03176976
Iteration 68, loss = 0.03088379
Iteration 69, loss = 0.03006062
Iteration 70, loss = 0.02925891
Iteration 71, loss = 0.02849878
Iteration 72, loss = 0.02778255
Iteration 73, loss = 0.02708043
Iteration 74, loss = 0.02643415
Iteration 75, loss = 0.02579785
Iteration 76, loss = 0.02518791
Iteration 77, loss = 0.02457204
Iteration 78, loss = 0.02399877
Iteration 79, loss = 0.02345876
Iteration 80, loss = 0.02292868
Iteration 81, loss = 0.02244589
Iteration 82, loss = 0.02198304
Iteration 83, loss = 0.02149472
Iteration 84, loss = 0.02105959
Iteration 85, loss = 0.02062315
Iteration 86, loss = 0.02025727
Iteration 87, loss = 0.01984136
Iteration 88, loss = 0.01945334
Iteration 89, loss = 0.01911104
Iteration 90, loss = 0.01873124
Iteration 91, loss = 0.01839022
Iteration 92, loss = 0.01807036
Iteration 93, loss = 0.01777780
Iteration 94, loss = 0.01744744
Iteration 95, loss = 0.01716166
Iteration 96, loss = 0.01684398
Iteration 97, loss = 0.01663190
Iteration 98, loss = 0.01633878
Iteration 99, loss = 0.01608175
Iteration 100, loss = 0.01583362
Iteration 101, loss = 0.01559603
Iteration 102, loss = 0.01536752
Iteration 103, loss = 0.01513952
Iteration 104, loss = 0.01493260
Iteration 105, loss = 0.01471053
Iteration 106, loss = 0.01446170
Iteration 107, loss = 0.01430037
Iteration 108, loss = 0.01405428
Iteration 109, loss = 0.01387701
Iteration 110, loss = 0.01365359
Iteration 111, loss = 0.01347237
Iteration 112, loss = 0.01328758
Iteration 113, loss = 0.01312204
Iteration 114, loss = 0.01295192
Iteration 115, loss = 0.01278620
Iteration 116, loss = 0.01262182
Iteration 117, loss = 0.01247741
Iteration 118, loss = 0.01231257
Iteration 119, loss = 0.01217742
Iteration 120, loss = 0.01203542
Iteration 121, loss = 0.01188864
Iteration 122, loss = 0.01177834
Iteration 123, loss = 0.01161819
Iteration 124, loss = 0.01149278
Iteration 125, loss = 0.01140699
Iteration 126, loss = 0.01124805
Iteration 127, loss = 0.01116141
Iteration 128, loss = 0.01100592
Iteration 129, loss = 0.01091280
Iteration 130, loss = 0.01078453
Iteration 131, loss = 0.01069012
Iteration 132, loss = 0.01056333
Iteration 133, loss = 0.01044936
Iteration 134, loss = 0.01036037
Iteration 135, loss = 0.01031713
Iteration 136, loss = 0.01018500
Iteration 137, loss = 0.01008627
Iteration 138, loss = 0.00998977
Iteration 139, loss = 0.00989483
Iteration 140, loss = 0.00981963
Iteration 141, loss = 0.00971947
Iteration 142, loss = 0.00966453
Iteration 143, loss = 0.00957180
Iteration 144, loss = 0.00946726
Iteration 145, loss = 0.00939663
Iteration 146, loss = 0.00931000
Iteration 147, loss = 0.00923708
Iteration 148, loss = 0.00916221
Iteration 149, loss = 0.00910956
Iteration 150, loss = 0.00901811
Iteration 151, loss = 0.00895380
Iteration 152, loss = 0.00888966
Iteration 153, loss = 0.00882609
Iteration 154, loss = 0.00876429
Iteration 155, loss = 0.00870376
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71797477
Iteration 2, loss = 0.66303646
Iteration 3, loss = 0.59438984
Iteration 4, loss = 0.52030384
Iteration 5, loss = 0.44473321
Iteration 6, loss = 0.37408346
Iteration 7, loss = 0.31228377
Iteration 8, loss = 0.26007421
Iteration 9, loss = 0.21708313
Iteration 10, loss = 0.18172258
Iteration 11, loss = 0.15331724
Iteration 12, loss = 0.13056410
Iteration 13, loss = 0.11251901
Iteration 14, loss = 0.09783391
Iteration 15, loss = 0.08601477
Iteration 16, loss = 0.07636546
Iteration 17, loss = 0.06848565
Iteration 18, loss = 0.06203130
Iteration 19, loss = 0.05646660
Iteration 20, loss = 0.05183906
Iteration 21, loss = 0.04803208
Iteration 22, loss = 0.04441847
Iteration 23, loss = 0.04158794
Iteration 24, loss = 0.03909990
Iteration 25, loss = 0.03693743
Iteration 26, loss = 0.03488790
Iteration 27, loss = 0.03330381
Iteration 28, loss = 0.03186634
Iteration 29, loss = 0.03049695
Iteration 30, loss = 0.02932420
Iteration 31, loss = 0.02808213
Iteration 32, loss = 0.02709938
Iteration 33, loss = 0.02625602
Iteration 34, loss = 0.02548076
Iteration 35, loss = 0.02467465
Iteration 36, loss = 0.02408393
Iteration 37, loss = 0.02345077
Iteration 38, loss = 0.02294582
Iteration 39, loss = 0.02240353
Iteration 40, loss = 0.02191047
Iteration 41, loss = 0.02148105
Iteration 42, loss = 0.02118221
Iteration 43, loss = 0.02081217
Iteration 44, loss = 0.02046820
Iteration 45, loss = 0.02010375
Iteration 46, loss = 0.01982563
Iteration 47, loss = 0.01953926
Iteration 48, loss = 0.01922422
Iteration 49, loss = 0.01912943
Iteration 50, loss = 0.01875874
Iteration 51, loss = 0.01854958
Iteration 52, loss = 0.01849806
Iteration 53, loss = 0.01825061
Iteration 54, loss = 0.01808369
Iteration 55, loss = 0.01788122
Iteration 56, loss = 0.01775100
Iteration 57, loss = 0.01774964
Iteration 58, loss = 0.01748792
Iteration 59, loss = 0.01729043
Iteration 60, loss = 0.01733413
Iteration 61, loss = 0.01698947
Iteration 62, loss = 0.01718067
Iteration 63, loss = 0.01674628
Iteration 64, loss = 0.01681901
Iteration 65, loss = 0.01667974
Iteration 66, loss = 0.01662309
Iteration 67, loss = 0.01641045
Iteration 68, loss = 0.01634710
Iteration 69, loss = 0.01640621
Iteration 70, loss = 0.01621333
Iteration 71, loss = 0.01622060
Iteration 72, loss = 0.01620393
Iteration 73, loss = 0.01599121
Iteration 74, loss = 0.01589802
Iteration 75, loss = 0.01581957
Iteration 76, loss = 0.01584330
Iteration 77, loss = 0.01573284
Iteration 78, loss = 0.01578789
Iteration 79, loss = 0.01557821
Iteration 80, loss = 0.01566420
Iteration 81, loss = 0.01556881
Iteration 82, loss = 0.01537438
Iteration 83, loss = 0.01544739
Iteration 84, loss = 0.01546032
Iteration 85, loss = 0.01530789
Iteration 86, loss = 0.01539030
Iteration 87, loss = 0.01532216
Iteration 88, loss = 0.01520492
Iteration 89, loss = 0.01523577
Iteration 90, loss = 0.01525682
Iteration 91, loss = 0.01524414
Iteration 92, loss = 0.01520246
Iteration 93, loss = 0.01511057
Iteration 94, loss = 0.01505161
Iteration 95, loss = 0.01511075
Iteration 96, loss = 0.01518827
Iteration 97, loss = 0.01506889
Iteration 98, loss = 0.01499442
Iteration 99, loss = 0.01488169
Iteration 100, loss = 0.01491884
Iteration 101, loss = 0.01492386
Iteration 102, loss = 0.01486256
Iteration 103, loss = 0.01495230
Iteration 104, loss = 0.01488539
Iteration 105, loss = 0.01478709
Iteration 106, loss = 0.01477310
Iteration 107, loss = 0.01483360
Iteration 108, loss = 0.01480969
Iteration 109, loss = 0.01471530
Iteration 110, loss = 0.01482203
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67730216
Iteration 2, loss = 0.62730553
Iteration 3, loss = 0.54945885
Iteration 4, loss = 0.45885981
Iteration 5, loss = 0.37250432
Iteration 6, loss = 0.29741944
Iteration 7, loss = 0.23640200
Iteration 8, loss = 0.18917835
Iteration 9, loss = 0.15341602
Iteration 10, loss = 0.12696510
Iteration 11, loss = 0.10678451
Iteration 12, loss = 0.09180684
Iteration 13, loss = 0.08011383
Iteration 14, loss = 0.07080795
Iteration 15, loss = 0.06385569
Iteration 16, loss = 0.05800598
Iteration 17, loss = 0.05309143
Iteration 18, loss = 0.04927273
Iteration 19, loss = 0.04615146
Iteration 20, loss = 0.04332989
Iteration 21, loss = 0.04091750
Iteration 22, loss = 0.03891898
Iteration 23, loss = 0.03714623
Iteration 24, loss = 0.03577379
Iteration 25, loss = 0.03428986
Iteration 26, loss = 0.03337099
Iteration 27, loss = 0.03228104
Iteration 28, loss = 0.03144910
Iteration 29, loss = 0.03053451
Iteration 30, loss = 0.03004439
Iteration 31, loss = 0.02920445
Iteration 32, loss = 0.02879198
Iteration 33, loss = 0.02804361
Iteration 34, loss = 0.02756664
Iteration 35, loss = 0.02721312
Iteration 36, loss = 0.02682757
Iteration 37, loss = 0.02649458
Iteration 38, loss = 0.02647595
Iteration 39, loss = 0.02576370
Iteration 40, loss = 0.02595486
Iteration 41, loss = 0.02542587
Iteration 42, loss = 0.02537102
Iteration 43, loss = 0.02500841
Iteration 44, loss = 0.02453730
Iteration 45, loss = 0.02470465
Iteration 46, loss = 0.02423754
Iteration 47, loss = 0.02399975
Iteration 48, loss = 0.02408567
Iteration 49, loss = 0.02399832
Iteration 50, loss = 0.02354766
Iteration 51, loss = 0.02351990
Iteration 52, loss = 0.02338626
Iteration 53, loss = 0.02356411
Iteration 54, loss = 0.02326472
Iteration 55, loss = 0.02323008
Iteration 56, loss = 0.02315624
Iteration 57, loss = 0.02287971
Iteration 58, loss = 0.02279205
Iteration 59, loss = 0.02281207
Iteration 60, loss = 0.02274668
Iteration 61, loss = 0.02263729
Iteration 62, loss = 0.02262240
Iteration 63, loss = 0.02261940
Iteration 64, loss = 0.02232648
Iteration 65, loss = 0.02243110
Iteration 66, loss = 0.02240387
Iteration 67, loss = 0.02244158
Iteration 68, loss = 0.02223074
Iteration 69, loss = 0.02199149
Iteration 70, loss = 0.02237800
Iteration 71, loss = 0.02186099
Iteration 72, loss = 0.02201697
Iteration 73, loss = 0.02190757
Iteration 74, loss = 0.02202932
Iteration 75, loss = 0.02197533
Iteration 76, loss = 0.02205073
Iteration 77, loss = 0.02178001
Iteration 78, loss = 0.02200350
Iteration 79, loss = 0.02181179
Iteration 80, loss = 0.02169921
Iteration 81, loss = 0.02172950
Iteration 82, loss = 0.02177235
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67907483
Iteration 2, loss = 0.63436908
Iteration 3, loss = 0.56694101
Iteration 4, loss = 0.48359757
Iteration 5, loss = 0.40036724
Iteration 6, loss = 0.32507993
Iteration 7, loss = 0.26222743
Iteration 8, loss = 0.21201204
Iteration 9, loss = 0.17314484
Iteration 10, loss = 0.14349020
Iteration 11, loss = 0.12099602
Iteration 12, loss = 0.10373667
Iteration 13, loss = 0.08998811
Iteration 14, loss = 0.07925977
Iteration 15, loss = 0.07113893
Iteration 16, loss = 0.06423923
Iteration 17, loss = 0.05867287
Iteration 18, loss = 0.05390145
Iteration 19, loss = 0.05009685
Iteration 20, loss = 0.04691721
Iteration 21, loss = 0.04414917
Iteration 22, loss = 0.04153103
Iteration 23, loss = 0.03967596
Iteration 24, loss = 0.03777249
Iteration 25, loss = 0.03610989
Iteration 26, loss = 0.03480100
Iteration 27, loss = 0.03371564
Iteration 28, loss = 0.03247116
Iteration 29, loss = 0.03135855
Iteration 30, loss = 0.03061566
Iteration 31, loss = 0.02967501
Iteration 32, loss = 0.02912788
Iteration 33, loss = 0.02833326
Iteration 34, loss = 0.02781982
Iteration 35, loss = 0.02758293
Iteration 36, loss = 0.02687258
Iteration 37, loss = 0.02649658
Iteration 38, loss = 0.02608552
Iteration 39, loss = 0.02549242
Iteration 40, loss = 0.02511351
Iteration 41, loss = 0.02477076
Iteration 42, loss = 0.02449717
Iteration 43, loss = 0.02426435
Iteration 44, loss = 0.02392835
Iteration 45, loss = 0.02373921
Iteration 46, loss = 0.02341454
Iteration 47, loss = 0.02362516
Iteration 48, loss = 0.02354316
Iteration 49, loss = 0.02313513
Iteration 50, loss = 0.02273716
Iteration 51, loss = 0.02255899
Iteration 52, loss = 0.02243249
Iteration 53, loss = 0.02240101
Iteration 54, loss = 0.02205437
Iteration 55, loss = 0.02191254
Iteration 56, loss = 0.02194527
Iteration 57, loss = 0.02190606
Iteration 58, loss = 0.02171175
Iteration 59, loss = 0.02147402
Iteration 60, loss = 0.02168293
Iteration 61, loss = 0.02156682
Iteration 62, loss = 0.02145523
Iteration 63, loss = 0.02133003
Iteration 64, loss = 0.02103091
Iteration 65, loss = 0.02120647
Iteration 66, loss = 0.02112189
Iteration 67, loss = 0.02102110
Iteration 68, loss = 0.02091958
Iteration 69, loss = 0.02101127
Iteration 70, loss = 0.02081781
Iteration 71, loss = 0.02077617
Iteration 72, loss = 0.02050416
Iteration 73, loss = 0.02058196
Iteration 74, loss = 0.02037269
Iteration 75, loss = 0.02053704
Iteration 76, loss = 0.02033089
Iteration 77, loss = 0.02044745
Iteration 78, loss = 0.02044753
Iteration 79, loss = 0.02040332
Iteration 80, loss = 0.02044027
Iteration 81, loss = 0.02037989
Iteration 82, loss = 0.02017185
Iteration 83, loss = 0.02030890
Iteration 84, loss = 0.02007849
Iteration 85, loss = 0.02038668
Iteration 86, loss = 0.02030555
Iteration 87, loss = 0.02009719
Iteration 88, loss = 0.02016106
Iteration 89, loss = 0.02003453
Iteration 90, loss = 0.01998620
Iteration 91, loss = 0.01969018
Iteration 92, loss = 0.02013678
Iteration 93, loss = 0.02013527
Iteration 94, loss = 0.01987362
Iteration 95, loss = 0.02026961
Iteration 96, loss = 0.01996002
Iteration 97, loss = 0.01968019
Iteration 98, loss = 0.01973037
Iteration 99, loss = 0.01964123
Iteration 100, loss = 0.01977371
Iteration 101, loss = 0.01952031
Iteration 102, loss = 0.01972286
Iteration 103, loss = 0.01960492
Iteration 104, loss = 0.01978439
Iteration 105, loss = 0.01939612
Iteration 106, loss = 0.01971393
Iteration 107, loss = 0.01959209
Iteration 108, loss = 0.01955645
Iteration 109, loss = 0.01954278
Iteration 110, loss = 0.01941298
Iteration 111, loss = 0.01952554
Iteration 112, loss = 0.01952505
Iteration 113, loss = 0.01946259
Iteration 114, loss = 0.01941267
Iteration 115, loss = 0.01970720
Iteration 116, loss = 0.01972396
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70873111
Iteration 2, loss = 0.64780757
Iteration 3, loss = 0.57303906
Iteration 4, loss = 0.48681807
Iteration 5, loss = 0.39995009
Iteration 6, loss = 0.32174470
Iteration 7, loss = 0.25751881
Iteration 8, loss = 0.20755937
Iteration 9, loss = 0.16880832
Iteration 10, loss = 0.13987604
Iteration 11, loss = 0.11775516
Iteration 12, loss = 0.10066510
Iteration 13, loss = 0.08783303
Iteration 14, loss = 0.07735962
Iteration 15, loss = 0.06928859
Iteration 16, loss = 0.06260439
Iteration 17, loss = 0.05711935
Iteration 18, loss = 0.05268850
Iteration 19, loss = 0.04908915
Iteration 20, loss = 0.04593154
Iteration 21, loss = 0.04332943
Iteration 22, loss = 0.04094312
Iteration 23, loss = 0.03900365
Iteration 24, loss = 0.03728533
Iteration 25, loss = 0.03577273
Iteration 26, loss = 0.03453426
Iteration 27, loss = 0.03335468
Iteration 28, loss = 0.03210267
Iteration 29, loss = 0.03157563
Iteration 30, loss = 0.03043331
Iteration 31, loss = 0.02969300
Iteration 32, loss = 0.02918305
Iteration 33, loss = 0.02835339
Iteration 34, loss = 0.02799592
Iteration 35, loss = 0.02744738
Iteration 36, loss = 0.02689190
Iteration 37, loss = 0.02646444
Iteration 38, loss = 0.02617954
Iteration 39, loss = 0.02579108
Iteration 40, loss = 0.02582327
Iteration 41, loss = 0.02528855
Iteration 42, loss = 0.02477929
Iteration 43, loss = 0.02460562
Iteration 44, loss = 0.02462679
Iteration 45, loss = 0.02422022
Iteration 46, loss = 0.02390855
Iteration 47, loss = 0.02362247
Iteration 48, loss = 0.02341326
Iteration 49, loss = 0.02311445
Iteration 50, loss = 0.02313379
Iteration 51, loss = 0.02287705
Iteration 52, loss = 0.02290088
Iteration 53, loss = 0.02254661
Iteration 54, loss = 0.02251831
Iteration 55, loss = 0.02260517
Iteration 56, loss = 0.02236774
Iteration 57, loss = 0.02232959
Iteration 58, loss = 0.02206354
Iteration 59, loss = 0.02218636
Iteration 60, loss = 0.02183491
Iteration 61, loss = 0.02173359
Iteration 62, loss = 0.02189315
Iteration 63, loss = 0.02162543
Iteration 64, loss = 0.02164501
Iteration 65, loss = 0.02158713
Iteration 66, loss = 0.02170544
Iteration 67, loss = 0.02158051
Iteration 68, loss = 0.02153399
Iteration 69, loss = 0.02143139
Iteration 70, loss = 0.02159230
Iteration 71, loss = 0.02171074
Iteration 72, loss = 0.02117433
Iteration 73, loss = 0.02106178
Iteration 74, loss = 0.02095126
Iteration 75, loss = 0.02101907
Iteration 76, loss = 0.02093095
Iteration 77, loss = 0.02090797
Iteration 78, loss = 0.02070579
Iteration 79, loss = 0.02070658
Iteration 80, loss = 0.02086638
Iteration 81, loss = 0.02091977
Iteration 82, loss = 0.02088412
Iteration 83, loss = 0.02087805
Iteration 84, loss = 0.02073664
Iteration 85, loss = 0.02056837
Iteration 86, loss = 0.02075418
Iteration 87, loss = 0.02071323
Iteration 88, loss = 0.02075852
Iteration 89, loss = 0.02063187
Iteration 90, loss = 0.02053474
Iteration 91, loss = 0.02048004
Iteration 92, loss = 0.02045959
Iteration 93, loss = 0.02037116
Iteration 94, loss = 0.02016111
Iteration 95, loss = 0.02050826
Iteration 96, loss = 0.02060219
Iteration 97, loss = 0.02038770
Iteration 98, loss = 0.02055250
Iteration 99, loss = 0.02042933
Iteration 100, loss = 0.02031614
Iteration 101, loss = 0.02038467
Iteration 102, loss = 0.02039442
Iteration 103, loss = 0.02032931
Iteration 104, loss = 0.02063243
Iteration 105, loss = 0.02023318
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67620043
Iteration 2, loss = 0.62074841
Iteration 3, loss = 0.54312277
Iteration 4, loss = 0.45545439
Iteration 5, loss = 0.37142978
Iteration 6, loss = 0.29768773
Iteration 7, loss = 0.23753534
Iteration 8, loss = 0.19074224
Iteration 9, loss = 0.15524053
Iteration 10, loss = 0.12859422
Iteration 11, loss = 0.10851401
Iteration 12, loss = 0.09311356
Iteration 13, loss = 0.08127234
Iteration 14, loss = 0.07193012
Iteration 15, loss = 0.06447841
Iteration 16, loss = 0.05873324
Iteration 17, loss = 0.05382318
Iteration 18, loss = 0.04975635
Iteration 19, loss = 0.04641584
Iteration 20, loss = 0.04345985
Iteration 21, loss = 0.04135583
Iteration 22, loss = 0.03918550
Iteration 23, loss = 0.03724731
Iteration 24, loss = 0.03568063
Iteration 25, loss = 0.03439609
Iteration 26, loss = 0.03314105
Iteration 27, loss = 0.03209647
Iteration 28, loss = 0.03101982
Iteration 29, loss = 0.03048831
Iteration 30, loss = 0.02953934
Iteration 31, loss = 0.02866289
Iteration 32, loss = 0.02807995
Iteration 33, loss = 0.02746923
Iteration 34, loss = 0.02721652
Iteration 35, loss = 0.02669661
Iteration 36, loss = 0.02628253
Iteration 37, loss = 0.02579794
Iteration 38, loss = 0.02543598
Iteration 39, loss = 0.02521215
Iteration 40, loss = 0.02473238
Iteration 41, loss = 0.02454064
Iteration 42, loss = 0.02397132
Iteration 43, loss = 0.02400028
Iteration 44, loss = 0.02367296
Iteration 45, loss = 0.02349087
Iteration 46, loss = 0.02350523
Iteration 47, loss = 0.02334730
Iteration 48, loss = 0.02331728
Iteration 49, loss = 0.02283184
Iteration 50, loss = 0.02315398
Iteration 51, loss = 0.02266293
Iteration 52, loss = 0.02237318
Iteration 53, loss = 0.02222287
Iteration 54, loss = 0.02251119
Iteration 55, loss = 0.02209483
Iteration 56, loss = 0.02229301
Iteration 57, loss = 0.02194085
Iteration 58, loss = 0.02204614
Iteration 59, loss = 0.02176641
Iteration 60, loss = 0.02161194
Iteration 61, loss = 0.02174275
Iteration 62, loss = 0.02191182
Iteration 63, loss = 0.02140042
Iteration 64, loss = 0.02150369
Iteration 65, loss = 0.02114737
Iteration 66, loss = 0.02130562
Iteration 67, loss = 0.02121598
Iteration 68, loss = 0.02105226
Iteration 69, loss = 0.02107075
Iteration 70, loss = 0.02120637
Iteration 71, loss = 0.02113757
Iteration 72, loss = 0.02091435
Iteration 73, loss = 0.02085351
Iteration 74, loss = 0.02084029
Iteration 75, loss = 0.02071801
Iteration 76, loss = 0.02058872
Iteration 77, loss = 0.02079067
Iteration 78, loss = 0.02065062
Iteration 79, loss = 0.02082378
Iteration 80, loss = 0.02068924
Iteration 81, loss = 0.02052664
Iteration 82, loss = 0.02058903
Iteration 83, loss = 0.02046947
Iteration 84, loss = 0.02054147
Iteration 85, loss = 0.02037375
Iteration 86, loss = 0.02045637
Iteration 87, loss = 0.02063810
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73008992
Iteration 2, loss = 0.70529285
Iteration 3, loss = 0.68076565
Iteration 4, loss = 0.65341106
Iteration 5, loss = 0.62280091
Iteration 6, loss = 0.58971341
Iteration 7, loss = 0.55447582
Iteration 8, loss = 0.51787076
Iteration 9, loss = 0.48081509
Iteration 10, loss = 0.44396304
Iteration 11, loss = 0.40781742
Iteration 12, loss = 0.37339650
Iteration 13, loss = 0.34064807
Iteration 14, loss = 0.31006077
Iteration 15, loss = 0.28177942
Iteration 16, loss = 0.25589101
Iteration 17, loss = 0.23224811
Iteration 18, loss = 0.21104977
Iteration 19, loss = 0.19191496
Iteration 20, loss = 0.17482288
Iteration 21, loss = 0.15958218
Iteration 22, loss = 0.14603258
Iteration 23, loss = 0.13398987
Iteration 24, loss = 0.12325857
Iteration 25, loss = 0.11373613
Iteration 26, loss = 0.10529018
Iteration 27, loss = 0.09774611
Iteration 28, loss = 0.09091560
Iteration 29, loss = 0.08487630
Iteration 30, loss = 0.07946640
Iteration 31, loss = 0.07462860
Iteration 32, loss = 0.07022066
Iteration 33, loss = 0.06626124
Iteration 34, loss = 0.06261308
Iteration 35, loss = 0.05936310
Iteration 36, loss = 0.05641960
Iteration 37, loss = 0.05372911
Iteration 38, loss = 0.05121526
Iteration 39, loss = 0.04895228
Iteration 40, loss = 0.04681860
Iteration 41, loss = 0.04490885
Iteration 42, loss = 0.04310934
Iteration 43, loss = 0.04144042
Iteration 44, loss = 0.03996023
Iteration 45, loss = 0.03849665
Iteration 46, loss = 0.03718692
Iteration 47, loss = 0.03596015
Iteration 48, loss = 0.03479201
Iteration 49, loss = 0.03374912
Iteration 50, loss = 0.03274081
Iteration 51, loss = 0.03182182
Iteration 52, loss = 0.03091319
Iteration 53, loss = 0.03013316
Iteration 54, loss = 0.02926622
Iteration 55, loss = 0.02859819
Iteration 56, loss = 0.02789016
Iteration 57, loss = 0.02727143
Iteration 58, loss = 0.02659566
Iteration 59, loss = 0.02604073
Iteration 60, loss = 0.02544243
Iteration 61, loss = 0.02498122
Iteration 62, loss = 0.02447705
Iteration 63, loss = 0.02394595
Iteration 64, loss = 0.02349380
Iteration 65, loss = 0.02312457
Iteration 66, loss = 0.02267735
Iteration 67, loss = 0.02228474
Iteration 68, loss = 0.02190013
Iteration 69, loss = 0.02159204
Iteration 70, loss = 0.02119472
Iteration 71, loss = 0.02092190
Iteration 72, loss = 0.02061667
Iteration 73, loss = 0.02032511
Iteration 74, loss = 0.02001050
Iteration 75, loss = 0.01976400
Iteration 76, loss = 0.01948428
Iteration 77, loss = 0.01923007
Iteration 78, loss = 0.01897764
Iteration 79, loss = 0.01880431
Iteration 80, loss = 0.01856847
Iteration 81, loss = 0.01834303
Iteration 82, loss = 0.01814215
Iteration 83, loss = 0.01792434
Iteration 84, loss = 0.01773871
Iteration 85, loss = 0.01765746
Iteration 86, loss = 0.01745764
Iteration 87, loss = 0.01722569
Iteration 88, loss = 0.01705917
Iteration 89, loss = 0.01686887
Iteration 90, loss = 0.01675699
Iteration 91, loss = 0.01663118
Iteration 92, loss = 0.01646302
Iteration 93, loss = 0.01631497
Iteration 94, loss = 0.01622173
Iteration 95, loss = 0.01606641
Iteration 96, loss = 0.01595909
Iteration 97, loss = 0.01583859
Iteration 98, loss = 0.01569042
Iteration 99, loss = 0.01562413
Iteration 100, loss = 0.01549867
Iteration 101, loss = 0.01541842
Iteration 102, loss = 0.01526538
Iteration 103, loss = 0.01520449
Iteration 104, loss = 0.01509685
Iteration 105, loss = 0.01501810
Iteration 106, loss = 0.01491927
Iteration 107, loss = 0.01478340
Iteration 108, loss = 0.01473866
Iteration 109, loss = 0.01464014
Iteration 110, loss = 0.01455722
Iteration 111, loss = 0.01455747
Iteration 112, loss = 0.01443057
Iteration 113, loss = 0.01432704
Iteration 114, loss = 0.01427268
Iteration 115, loss = 0.01419069
Iteration 116, loss = 0.01413013
Iteration 117, loss = 0.01407351
Iteration 118, loss = 0.01401060
Iteration 119, loss = 0.01397996
Iteration 120, loss = 0.01386306
Iteration 121, loss = 0.01379390
Iteration 122, loss = 0.01374199
Iteration 123, loss = 0.01369259
Iteration 124, loss = 0.01362993
Iteration 125, loss = 0.01359849
Iteration 126, loss = 0.01357980
Iteration 127, loss = 0.01348245
Iteration 128, loss = 0.01347148
Iteration 129, loss = 0.01342162
Iteration 130, loss = 0.01338257
Iteration 131, loss = 0.01327370
Iteration 132, loss = 0.01326694
Iteration 133, loss = 0.01323328
Iteration 134, loss = 0.01312476
Iteration 135, loss = 0.01316009
Iteration 136, loss = 0.01305116
Iteration 137, loss = 0.01306760
Iteration 138, loss = 0.01296467
Iteration 139, loss = 0.01295324
Iteration 140, loss = 0.01297181
Iteration 141, loss = 0.01286237
Iteration 142, loss = 0.01283926
Iteration 143, loss = 0.01286191
Iteration 144, loss = 0.01280063
Iteration 145, loss = 0.01271866
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67735832
Iteration 2, loss = 0.65552303
Iteration 3, loss = 0.62883080
Iteration 4, loss = 0.59237144
Iteration 5, loss = 0.54770571
Iteration 6, loss = 0.49865978
Iteration 7, loss = 0.44863757
Iteration 8, loss = 0.39976289
Iteration 9, loss = 0.35355797
Iteration 10, loss = 0.31133353
Iteration 11, loss = 0.27337378
Iteration 12, loss = 0.24005993
Iteration 13, loss = 0.21094609
Iteration 14, loss = 0.18604339
Iteration 15, loss = 0.16450816
Iteration 16, loss = 0.14632259
Iteration 17, loss = 0.13076498
Iteration 18, loss = 0.11755103
Iteration 19, loss = 0.10625513
Iteration 20, loss = 0.09643203
Iteration 21, loss = 0.08807206
Iteration 22, loss = 0.08072879
Iteration 23, loss = 0.07450838
Iteration 24, loss = 0.06911518
Iteration 25, loss = 0.06423437
Iteration 26, loss = 0.06004657
Iteration 27, loss = 0.05626524
Iteration 28, loss = 0.05285393
Iteration 29, loss = 0.04990064
Iteration 30, loss = 0.04717611
Iteration 31, loss = 0.04483488
Iteration 32, loss = 0.04263493
Iteration 33, loss = 0.04071153
Iteration 34, loss = 0.03914696
Iteration 35, loss = 0.03795974
Iteration 36, loss = 0.03661812
Iteration 37, loss = 0.03522420
Iteration 38, loss = 0.03382512
Iteration 39, loss = 0.03247532
Iteration 40, loss = 0.03134896
Iteration 41, loss = 0.03022991
Iteration 42, loss = 0.02936890
Iteration 43, loss = 0.02843032
Iteration 44, loss = 0.02769888
Iteration 45, loss = 0.02691502
Iteration 46, loss = 0.02627434
Iteration 47, loss = 0.02565187
Iteration 48, loss = 0.02503332
Iteration 49, loss = 0.02448344
Iteration 50, loss = 0.02419377
Iteration 51, loss = 0.02371261
Iteration 52, loss = 0.02320983
Iteration 53, loss = 0.02285131
Iteration 54, loss = 0.02236310
Iteration 55, loss = 0.02204971
Iteration 56, loss = 0.02162499
Iteration 57, loss = 0.02131008
Iteration 58, loss = 0.02106181
Iteration 59, loss = 0.02068164
Iteration 60, loss = 0.02042237
Iteration 61, loss = 0.02017134
Iteration 62, loss = 0.01991953
Iteration 63, loss = 0.01965455
Iteration 64, loss = 0.01936968
Iteration 65, loss = 0.01915657
Iteration 66, loss = 0.01898168
Iteration 67, loss = 0.01875368
Iteration 68, loss = 0.01859686
Iteration 69, loss = 0.01846942
Iteration 70, loss = 0.01831264
Iteration 71, loss = 0.01816607
Iteration 72, loss = 0.01797706
Iteration 73, loss = 0.01798783
Iteration 74, loss = 0.01783708
Iteration 75, loss = 0.01767423
Iteration 76, loss = 0.01755140
Iteration 77, loss = 0.01739597
Iteration 78, loss = 0.01734391
Iteration 79, loss = 0.01721165
Iteration 80, loss = 0.01706812
Iteration 81, loss = 0.01710676
Iteration 82, loss = 0.01685862
Iteration 83, loss = 0.01692333
Iteration 84, loss = 0.01679174
Iteration 85, loss = 0.01663942
Iteration 86, loss = 0.01652166
Iteration 87, loss = 0.01636410
Iteration 88, loss = 0.01629000
Iteration 89, loss = 0.01634348
Iteration 90, loss = 0.01624492
Iteration 91, loss = 0.01609360
Iteration 92, loss = 0.01603284
Iteration 93, loss = 0.01587429
Iteration 94, loss = 0.01587165
Iteration 95, loss = 0.01593368
Iteration 96, loss = 0.01586592
Iteration 97, loss = 0.01579067
Iteration 98, loss = 0.01567003
Iteration 99, loss = 0.01554210
Iteration 100, loss = 0.01551342
Iteration 101, loss = 0.01537304
Iteration 102, loss = 0.01529131
Iteration 103, loss = 0.01520809
Iteration 104, loss = 0.01510778
Iteration 105, loss = 0.01513418
Iteration 106, loss = 0.01503262
Iteration 107, loss = 0.01499099
Iteration 108, loss = 0.01481869
Iteration 109, loss = 0.01495227
Iteration 110, loss = 0.01500790
Iteration 111, loss = 0.01505787
Iteration 112, loss = 0.01509060
Iteration 113, loss = 0.01517970
Iteration 114, loss = 0.01519073
Iteration 115, loss = 0.01511843
Iteration 116, loss = 0.01497046
Iteration 117, loss = 0.01475341
Iteration 118, loss = 0.01461906
Iteration 119, loss = 0.01459624
Iteration 120, loss = 0.01457935
Iteration 121, loss = 0.01446691
Iteration 122, loss = 0.01449501
Iteration 123, loss = 0.01438756
Iteration 124, loss = 0.01436088
Iteration 125, loss = 0.01438460
Iteration 126, loss = 0.01435707
Iteration 127, loss = 0.01433069
Iteration 128, loss = 0.01429541
Iteration 129, loss = 0.01430633
Iteration 130, loss = 0.01428354
Iteration 131, loss = 0.01432897
Iteration 132, loss = 0.01424756
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69363770
Iteration 2, loss = 0.65921378
Iteration 3, loss = 0.61328242
Iteration 4, loss = 0.55693815
Iteration 5, loss = 0.49385794
Iteration 6, loss = 0.43073754
Iteration 7, loss = 0.37059211
Iteration 8, loss = 0.31569924
Iteration 9, loss = 0.26759209
Iteration 10, loss = 0.22640999
Iteration 11, loss = 0.19215733
Iteration 12, loss = 0.16378504
Iteration 13, loss = 0.14078441
Iteration 14, loss = 0.12186420
Iteration 15, loss = 0.10652405
Iteration 16, loss = 0.09385478
Iteration 17, loss = 0.08330734
Iteration 18, loss = 0.07460169
Iteration 19, loss = 0.06738718
Iteration 20, loss = 0.06133885
Iteration 21, loss = 0.05587344
Iteration 22, loss = 0.05151086
Iteration 23, loss = 0.04760928
Iteration 24, loss = 0.04434435
Iteration 25, loss = 0.04120729
Iteration 26, loss = 0.03861613
Iteration 27, loss = 0.03627242
Iteration 28, loss = 0.03430034
Iteration 29, loss = 0.03251020
Iteration 30, loss = 0.03084404
Iteration 31, loss = 0.02939766
Iteration 32, loss = 0.02814506
Iteration 33, loss = 0.02690264
Iteration 34, loss = 0.02581992
Iteration 35, loss = 0.02480372
Iteration 36, loss = 0.02388871
Iteration 37, loss = 0.02311373
Iteration 38, loss = 0.02238596
Iteration 39, loss = 0.02172720
Iteration 40, loss = 0.02096529
Iteration 41, loss = 0.02041129
Iteration 42, loss = 0.01992618
Iteration 43, loss = 0.01942205
Iteration 44, loss = 0.01894152
Iteration 45, loss = 0.01851880
Iteration 46, loss = 0.01807712
Iteration 47, loss = 0.01771186
Iteration 48, loss = 0.01742661
Iteration 49, loss = 0.01706783
Iteration 50, loss = 0.01673405
Iteration 51, loss = 0.01644282
Iteration 52, loss = 0.01615210
Iteration 53, loss = 0.01593282
Iteration 54, loss = 0.01563790
Iteration 55, loss = 0.01550546
Iteration 56, loss = 0.01529647
Iteration 57, loss = 0.01506158
Iteration 58, loss = 0.01486349
Iteration 59, loss = 0.01464592
Iteration 60, loss = 0.01456879
Iteration 61, loss = 0.01445262
Iteration 62, loss = 0.01427899
Iteration 63, loss = 0.01401027
Iteration 64, loss = 0.01393201
Iteration 65, loss = 0.01370088
Iteration 66, loss = 0.01361266
Iteration 67, loss = 0.01353521
Iteration 68, loss = 0.01345989
Iteration 69, loss = 0.01328795
Iteration 70, loss = 0.01319588
Iteration 71, loss = 0.01304260
Iteration 72, loss = 0.01305696
Iteration 73, loss = 0.01296406
Iteration 74, loss = 0.01279890
Iteration 75, loss = 0.01270237
Iteration 76, loss = 0.01272567
Iteration 77, loss = 0.01256020
Iteration 78, loss = 0.01248513
Iteration 79, loss = 0.01249061
Iteration 80, loss = 0.01244970
Iteration 81, loss = 0.01231862
Iteration 82, loss = 0.01224117
Iteration 83, loss = 0.01211240
Iteration 84, loss = 0.01212442
Iteration 85, loss = 0.01204614
Iteration 86, loss = 0.01197734
Iteration 87, loss = 0.01192293
Iteration 88, loss = 0.01183966
Iteration 89, loss = 0.01186729
Iteration 90, loss = 0.01180630
Iteration 91, loss = 0.01182382
Iteration 92, loss = 0.01174694
Iteration 93, loss = 0.01178565
Iteration 94, loss = 0.01160027
Iteration 95, loss = 0.01161893
Iteration 96, loss = 0.01167416
Iteration 97, loss = 0.01151908
Iteration 98, loss = 0.01152902
Iteration 99, loss = 0.01144332
Iteration 100, loss = 0.01137782
Iteration 101, loss = 0.01131949
Iteration 102, loss = 0.01140987
Iteration 103, loss = 0.01124788
Iteration 104, loss = 0.01124329
Iteration 105, loss = 0.01125208
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70479446
Iteration 2, loss = 0.68552112
Iteration 3, loss = 0.66866079
Iteration 4, loss = 0.65228565
Iteration 5, loss = 0.63556577
Iteration 6, loss = 0.61804704
Iteration 7, loss = 0.59928640
Iteration 8, loss = 0.57952325
Iteration 9, loss = 0.55861681
Iteration 10, loss = 0.53728417
Iteration 11, loss = 0.51496541
Iteration 12, loss = 0.49196338
Iteration 13, loss = 0.46893469
Iteration 14, loss = 0.44589532
Iteration 15, loss = 0.42301719
Iteration 16, loss = 0.40042335
Iteration 17, loss = 0.37856182
Iteration 18, loss = 0.35716107
Iteration 19, loss = 0.33667810
Iteration 20, loss = 0.31705613
Iteration 21, loss = 0.29842112
Iteration 22, loss = 0.28062629
Iteration 23, loss = 0.26386428
Iteration 24, loss = 0.24802080
Iteration 25, loss = 0.23303885
Iteration 26, loss = 0.21909356
Iteration 27, loss = 0.20593548
Iteration 28, loss = 0.19362234
Iteration 29, loss = 0.18217866
Iteration 30, loss = 0.17153628
Iteration 31, loss = 0.16152347
Iteration 32, loss = 0.15226095
Iteration 33, loss = 0.14357070
Iteration 34, loss = 0.13551018
Iteration 35, loss = 0.12812826
Iteration 36, loss = 0.12117909
Iteration 37, loss = 0.11471298
Iteration 38, loss = 0.10877522
Iteration 39, loss = 0.10317144
Iteration 40, loss = 0.09799647
Iteration 41, loss = 0.09322178
Iteration 42, loss = 0.08874691
Iteration 43, loss = 0.08460197
Iteration 44, loss = 0.08077066
Iteration 45, loss = 0.07709123
Iteration 46, loss = 0.07378886
Iteration 47, loss = 0.07060982
Iteration 48, loss = 0.06765017
Iteration 49, loss = 0.06494592
Iteration 50, loss = 0.06239181
Iteration 51, loss = 0.05998633
Iteration 52, loss = 0.05774023
Iteration 53, loss = 0.05563727
Iteration 54, loss = 0.05360663
Iteration 55, loss = 0.05175546
Iteration 56, loss = 0.05002138
Iteration 57, loss = 0.04833609
Iteration 58, loss = 0.04674544
Iteration 59, loss = 0.04530340
Iteration 60, loss = 0.04386859
Iteration 61, loss = 0.04250185
Iteration 62, loss = 0.04127777
Iteration 63, loss = 0.04003695
Iteration 64, loss = 0.03890879
Iteration 65, loss = 0.03784513
Iteration 66, loss = 0.03681766
Iteration 67, loss = 0.03587247
Iteration 68, loss = 0.03489819
Iteration 69, loss = 0.03399965
Iteration 70, loss = 0.03316774
Iteration 71, loss = 0.03233677
Iteration 72, loss = 0.03159171
Iteration 73, loss = 0.03083324
Iteration 74, loss = 0.03015009
Iteration 75, loss = 0.02946952
Iteration 76, loss = 0.02884333
Iteration 77, loss = 0.02822767
Iteration 78, loss = 0.02761319
Iteration 79, loss = 0.02705376
Iteration 80, loss = 0.02653406
Iteration 81, loss = 0.02600696
Iteration 82, loss = 0.02551567
Iteration 83, loss = 0.02509179
Iteration 84, loss = 0.02462154
Iteration 85, loss = 0.02412949
Iteration 86, loss = 0.02370556
Iteration 87, loss = 0.02332043
Iteration 88, loss = 0.02294858
Iteration 89, loss = 0.02255242
Iteration 90, loss = 0.02219707
Iteration 91, loss = 0.02184502
Iteration 92, loss = 0.02150668
Iteration 93, loss = 0.02123621
Iteration 94, loss = 0.02087419
Iteration 95, loss = 0.02056720
Iteration 96, loss = 0.02024054
Iteration 97, loss = 0.01995215
Iteration 98, loss = 0.01964057
Iteration 99, loss = 0.01936940
Iteration 100, loss = 0.01906172
Iteration 101, loss = 0.01880421
Iteration 102, loss = 0.01857994
Iteration 103, loss = 0.01832314
Iteration 104, loss = 0.01815285
Iteration 105, loss = 0.01786688
Iteration 106, loss = 0.01771442
Iteration 107, loss = 0.01747102
Iteration 108, loss = 0.01726693
Iteration 109, loss = 0.01708852
Iteration 110, loss = 0.01686699
Iteration 111, loss = 0.01668661
Iteration 112, loss = 0.01652478
Iteration 113, loss = 0.01632504
Iteration 114, loss = 0.01619277
Iteration 115, loss = 0.01600959
Iteration 116, loss = 0.01582610
Iteration 117, loss = 0.01570833
Iteration 118, loss = 0.01554512
Iteration 119, loss = 0.01539135
Iteration 120, loss = 0.01522312
Iteration 121, loss = 0.01506343
Iteration 122, loss = 0.01495036
Iteration 123, loss = 0.01481478
Iteration 124, loss = 0.01466656
Iteration 125, loss = 0.01453026
Iteration 126, loss = 0.01440980
Iteration 127, loss = 0.01429055
Iteration 128, loss = 0.01416119
Iteration 129, loss = 0.01403778
Iteration 130, loss = 0.01397567
Iteration 131, loss = 0.01383970
Iteration 132, loss = 0.01375691
Iteration 133, loss = 0.01364685
Iteration 134, loss = 0.01356411
Iteration 135, loss = 0.01343611
Iteration 136, loss = 0.01333358
Iteration 137, loss = 0.01323659
Iteration 138, loss = 0.01316347
Iteration 139, loss = 0.01299211
Iteration 140, loss = 0.01287557
Iteration 141, loss = 0.01279170
Iteration 142, loss = 0.01267900
Iteration 143, loss = 0.01263502
Iteration 144, loss = 0.01258120
Iteration 145, loss = 0.01247290
Iteration 146, loss = 0.01243219
Iteration 147, loss = 0.01233557
Iteration 148, loss = 0.01226557
Iteration 149, loss = 0.01221457
Iteration 150, loss = 0.01213138
Iteration 151, loss = 0.01204562
Iteration 152, loss = 0.01198342
Iteration 153, loss = 0.01195446
Iteration 154, loss = 0.01183235
Iteration 155, loss = 0.01176075
Iteration 156, loss = 0.01168752
Iteration 157, loss = 0.01160195
Iteration 158, loss = 0.01153625
Iteration 159, loss = 0.01147612
Iteration 160, loss = 0.01141541
Iteration 161, loss = 0.01139210
Iteration 162, loss = 0.01133310
Iteration 163, loss = 0.01125563
Iteration 164, loss = 0.01120324
Iteration 165, loss = 0.01120265
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69969398
Iteration 2, loss = 0.65536893
Iteration 3, loss = 0.59311107
Iteration 4, loss = 0.50944443
Iteration 5, loss = 0.42254572
Iteration 6, loss = 0.34345699
Iteration 7, loss = 0.27645739
Iteration 8, loss = 0.22273629
Iteration 9, loss = 0.18111539
Iteration 10, loss = 0.14908188
Iteration 11, loss = 0.12465019
Iteration 12, loss = 0.10608343
Iteration 13, loss = 0.09173785
Iteration 14, loss = 0.08025551
Iteration 15, loss = 0.07129090
Iteration 16, loss = 0.06387680
Iteration 17, loss = 0.05819440
Iteration 18, loss = 0.05315568
Iteration 19, loss = 0.04912835
Iteration 20, loss = 0.04571707
Iteration 21, loss = 0.04271289
Iteration 22, loss = 0.04035999
Iteration 23, loss = 0.03797298
Iteration 24, loss = 0.03635256
Iteration 25, loss = 0.03443385
Iteration 26, loss = 0.03304071
Iteration 27, loss = 0.03176622
Iteration 28, loss = 0.03053349
Iteration 29, loss = 0.02950934
Iteration 30, loss = 0.02855731
Iteration 31, loss = 0.02785815
Iteration 32, loss = 0.02729539
Iteration 33, loss = 0.02639272
Iteration 34, loss = 0.02581206
Iteration 35, loss = 0.02524563
Iteration 36, loss = 0.02469003
Iteration 37, loss = 0.02435618
Iteration 38, loss = 0.02373770
Iteration 39, loss = 0.02346968
Iteration 40, loss = 0.02302197
Iteration 41, loss = 0.02264743
Iteration 42, loss = 0.02233632
Iteration 43, loss = 0.02212974
Iteration 44, loss = 0.02182808
Iteration 45, loss = 0.02158502
Iteration 46, loss = 0.02129110
Iteration 47, loss = 0.02094248
Iteration 48, loss = 0.02067854
Iteration 49, loss = 0.02087709
Iteration 50, loss = 0.02056533
Iteration 51, loss = 0.02039034
Iteration 52, loss = 0.02003807
Iteration 53, loss = 0.02007029
Iteration 54, loss = 0.01981825
Iteration 55, loss = 0.01986327
Iteration 56, loss = 0.01957281
Iteration 57, loss = 0.01955130
Iteration 58, loss = 0.01926014
Iteration 59, loss = 0.01927629
Iteration 60, loss = 0.01932921
Iteration 61, loss = 0.01922334
Iteration 62, loss = 0.01908717
Iteration 63, loss = 0.01898598
Iteration 64, loss = 0.01873639
Iteration 65, loss = 0.01900409
Iteration 66, loss = 0.01882942
Iteration 67, loss = 0.01864416
Iteration 68, loss = 0.01853591
Iteration 69, loss = 0.01852548
Iteration 70, loss = 0.01858965
Iteration 71, loss = 0.01824589
Iteration 72, loss = 0.01815486
Iteration 73, loss = 0.01828137
Iteration 74, loss = 0.01836982
Iteration 75, loss = 0.01818720
Iteration 76, loss = 0.01809696
Iteration 77, loss = 0.01814548
Iteration 78, loss = 0.01795776
Iteration 79, loss = 0.01772965
Iteration 80, loss = 0.01774190
Iteration 81, loss = 0.01784539
Iteration 82, loss = 0.01780180
Iteration 83, loss = 0.01767028
Iteration 84, loss = 0.01766183
Iteration 85, loss = 0.01793794
Iteration 86, loss = 0.01778473
Iteration 87, loss = 0.01758000
Iteration 88, loss = 0.01769820
Iteration 89, loss = 0.01765755
Iteration 90, loss = 0.01740410
Iteration 91, loss = 0.01766033
Iteration 92, loss = 0.01759612
Iteration 93, loss = 0.01745787
Iteration 94, loss = 0.01738068
Iteration 95, loss = 0.01779190
Iteration 96, loss = 0.01762435
Iteration 97, loss = 0.01724664
Iteration 98, loss = 0.01749028
Iteration 99, loss = 0.01720093
Iteration 100, loss = 0.01752737
Iteration 101, loss = 0.01731671
Iteration 102, loss = 0.01715150
Iteration 103, loss = 0.01722687
Iteration 104, loss = 0.01738707
Iteration 105, loss = 0.01753714
Iteration 106, loss = 0.01721446
Iteration 107, loss = 0.01766411
Iteration 108, loss = 0.01694137
Iteration 109, loss = 0.01711455
Iteration 110, loss = 0.01714632
Iteration 111, loss = 0.01711184
Iteration 112, loss = 0.01744250
Iteration 113, loss = 0.01714307
Iteration 114, loss = 0.01726763
Iteration 115, loss = 0.01707515
Iteration 116, loss = 0.01720235
Iteration 117, loss = 0.01719185
Iteration 118, loss = 0.01716560
Iteration 119, loss = 0.01715139
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68200680
Iteration 2, loss = 0.63388155
Iteration 3, loss = 0.55480112
Iteration 4, loss = 0.45713986
Iteration 5, loss = 0.36425823
Iteration 6, loss = 0.28398186
Iteration 7, loss = 0.22160480
Iteration 8, loss = 0.17511532
Iteration 9, loss = 0.14135211
Iteration 10, loss = 0.11692982
Iteration 11, loss = 0.09897875
Iteration 12, loss = 0.08571945
Iteration 13, loss = 0.07537884
Iteration 14, loss = 0.06761006
Iteration 15, loss = 0.06124102
Iteration 16, loss = 0.05631223
Iteration 17, loss = 0.05218750
Iteration 18, loss = 0.04873944
Iteration 19, loss = 0.04599390
Iteration 20, loss = 0.04386445
Iteration 21, loss = 0.04156529
Iteration 22, loss = 0.03992777
Iteration 23, loss = 0.03860372
Iteration 24, loss = 0.03834118
Iteration 25, loss = 0.03748402
Iteration 26, loss = 0.03615869
Iteration 27, loss = 0.03504242
Iteration 28, loss = 0.03389296
Iteration 29, loss = 0.03316992
Iteration 30, loss = 0.03229880
Iteration 31, loss = 0.03168089
Iteration 32, loss = 0.03109409
Iteration 33, loss = 0.03066565
Iteration 34, loss = 0.03015757
Iteration 35, loss = 0.02956127
Iteration 36, loss = 0.02931795
Iteration 37, loss = 0.02938378
Iteration 38, loss = 0.02849993
Iteration 39, loss = 0.02821158
Iteration 40, loss = 0.02797242
Iteration 41, loss = 0.02767471
Iteration 42, loss = 0.02749645
Iteration 43, loss = 0.02708753
Iteration 44, loss = 0.02692852
Iteration 45, loss = 0.02681569
Iteration 46, loss = 0.02655325
Iteration 47, loss = 0.02644030
Iteration 48, loss = 0.02658012
Iteration 49, loss = 0.02671984
Iteration 50, loss = 0.02621889
Iteration 51, loss = 0.02621130
Iteration 52, loss = 0.02605895
Iteration 53, loss = 0.02596216
Iteration 54, loss = 0.02583443
Iteration 55, loss = 0.02568380
Iteration 56, loss = 0.02555633
Iteration 57, loss = 0.02522496
Iteration 58, loss = 0.02515888
Iteration 59, loss = 0.02518585
Iteration 60, loss = 0.02739127
Iteration 61, loss = 0.02786475
Iteration 62, loss = 0.02673386
Iteration 63, loss = 0.02637041
Iteration 64, loss = 0.02614726
Iteration 65, loss = 0.02569046
Iteration 66, loss = 0.02706863
Iteration 67, loss = 0.02780454
Iteration 68, loss = 0.02661618
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71988302
Iteration 2, loss = 0.69740751
Iteration 3, loss = 0.67870144
Iteration 4, loss = 0.66071884
Iteration 5, loss = 0.64201655
Iteration 6, loss = 0.62211196
Iteration 7, loss = 0.60056835
Iteration 8, loss = 0.57773830
Iteration 9, loss = 0.55363679
Iteration 10, loss = 0.52843443
Iteration 11, loss = 0.50283211
Iteration 12, loss = 0.47637297
Iteration 13, loss = 0.44995247
Iteration 14, loss = 0.42370606
Iteration 15, loss = 0.39791164
Iteration 16, loss = 0.37284287
Iteration 17, loss = 0.34875186
Iteration 18, loss = 0.32591153
Iteration 19, loss = 0.30417452
Iteration 20, loss = 0.28373012
Iteration 21, loss = 0.26460383
Iteration 22, loss = 0.24665803
Iteration 23, loss = 0.23005734
Iteration 24, loss = 0.21453443
Iteration 25, loss = 0.20032010
Iteration 26, loss = 0.18707993
Iteration 27, loss = 0.17495083
Iteration 28, loss = 0.16374185
Iteration 29, loss = 0.15335816
Iteration 30, loss = 0.14379792
Iteration 31, loss = 0.13508002
Iteration 32, loss = 0.12699378
Iteration 33, loss = 0.11954467
Iteration 34, loss = 0.11262295
Iteration 35, loss = 0.10634671
Iteration 36, loss = 0.10048813
Iteration 37, loss = 0.09506850
Iteration 38, loss = 0.09002836
Iteration 39, loss = 0.08543680
Iteration 40, loss = 0.08112520
Iteration 41, loss = 0.07711970
Iteration 42, loss = 0.07340153
Iteration 43, loss = 0.06993727
Iteration 44, loss = 0.06667114
Iteration 45, loss = 0.06369216
Iteration 46, loss = 0.06086480
Iteration 47, loss = 0.05824437
Iteration 48, loss = 0.05581186
Iteration 49, loss = 0.05351090
Iteration 50, loss = 0.05132476
Iteration 51, loss = 0.04930592
Iteration 52, loss = 0.04742869
Iteration 53, loss = 0.04559603
Iteration 54, loss = 0.04391461
Iteration 55, loss = 0.04229205
Iteration 56, loss = 0.04078906
Iteration 57, loss = 0.03937405
Iteration 58, loss = 0.03801949
Iteration 59, loss = 0.03672500
Iteration 60, loss = 0.03551210
Iteration 61, loss = 0.03435040
Iteration 62, loss = 0.03323319
Iteration 63, loss = 0.03219619
Iteration 64, loss = 0.03119689
Iteration 65, loss = 0.03025889
Iteration 66, loss = 0.02935035
Iteration 67, loss = 0.02848392
Iteration 68, loss = 0.02768265
Iteration 69, loss = 0.02686603
Iteration 70, loss = 0.02612372
Iteration 71, loss = 0.02539102
Iteration 72, loss = 0.02469236
Iteration 73, loss = 0.02401895
Iteration 74, loss = 0.02337355
Iteration 75, loss = 0.02277373
Iteration 76, loss = 0.02217559
Iteration 77, loss = 0.02161810
Iteration 78, loss = 0.02106964
Iteration 79, loss = 0.02053900
Iteration 80, loss = 0.02005149
Iteration 81, loss = 0.01956497
Iteration 82, loss = 0.01911424
Iteration 83, loss = 0.01865874
Iteration 84, loss = 0.01823760
Iteration 85, loss = 0.01782905
Iteration 86, loss = 0.01743066
Iteration 87, loss = 0.01704900
Iteration 88, loss = 0.01668037
Iteration 89, loss = 0.01632256
Iteration 90, loss = 0.01597128
Iteration 91, loss = 0.01564170
Iteration 92, loss = 0.01531940
Iteration 93, loss = 0.01500617
Iteration 94, loss = 0.01470410
Iteration 95, loss = 0.01442098
Iteration 96, loss = 0.01412903
Iteration 97, loss = 0.01386026
Iteration 98, loss = 0.01359553
Iteration 99, loss = 0.01334427
Iteration 100, loss = 0.01309414
Iteration 101, loss = 0.01285610
Iteration 102, loss = 0.01262087
Iteration 103, loss = 0.01238672
Iteration 104, loss = 0.01216616
Iteration 105, loss = 0.01194194
Iteration 106, loss = 0.01172342
Iteration 107, loss = 0.01151606
Iteration 108, loss = 0.01132000
Iteration 109, loss = 0.01112885
Iteration 110, loss = 0.01093496
Iteration 111, loss = 0.01075872
Iteration 112, loss = 0.01058103
Iteration 113, loss = 0.01040323
Iteration 114, loss = 0.01023361
Iteration 115, loss = 0.01006877
Iteration 116, loss = 0.00990548
Iteration 117, loss = 0.00975252
Iteration 118, loss = 0.00959933
Iteration 119, loss = 0.00944791
Iteration 120, loss = 0.00930717
Iteration 121, loss = 0.00916171
Iteration 122, loss = 0.00902933
Iteration 123, loss = 0.00889779
Iteration 124, loss = 0.00876720
Iteration 125, loss = 0.00864146
Iteration 126, loss = 0.00851897
Iteration 127, loss = 0.00839709
Iteration 128, loss = 0.00827993
Iteration 129, loss = 0.00816392
Iteration 130, loss = 0.00805438
Iteration 131, loss = 0.00794275
Iteration 132, loss = 0.00783422
Iteration 133, loss = 0.00772890
Iteration 134, loss = 0.00762832
Iteration 135, loss = 0.00752596
Iteration 136, loss = 0.00742487
Iteration 137, loss = 0.00733229
Iteration 138, loss = 0.00723431
Iteration 139, loss = 0.00714379
Iteration 140, loss = 0.00704958
Iteration 141, loss = 0.00695871
Iteration 142, loss = 0.00687070
Iteration 143, loss = 0.00678804
Iteration 144, loss = 0.00670501
Iteration 145, loss = 0.00662272
Iteration 146, loss = 0.00654282
Iteration 147, loss = 0.00646571
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68086019
Iteration 2, loss = 0.63105462
Iteration 3, loss = 0.55609627
Iteration 4, loss = 0.46499650
Iteration 5, loss = 0.37670946
Iteration 6, loss = 0.29875920
Iteration 7, loss = 0.23504471
Iteration 8, loss = 0.18641866
Iteration 9, loss = 0.14996973
Iteration 10, loss = 0.12310180
Iteration 11, loss = 0.10295170
Iteration 12, loss = 0.08782454
Iteration 13, loss = 0.07615533
Iteration 14, loss = 0.06718634
Iteration 15, loss = 0.05999759
Iteration 16, loss = 0.05441677
Iteration 17, loss = 0.04960185
Iteration 18, loss = 0.04581064
Iteration 19, loss = 0.04257169
Iteration 20, loss = 0.03994112
Iteration 21, loss = 0.03771766
Iteration 22, loss = 0.03578647
Iteration 23, loss = 0.03410237
Iteration 24, loss = 0.03260088
Iteration 25, loss = 0.03122873
Iteration 26, loss = 0.03024646
Iteration 27, loss = 0.02916102
Iteration 28, loss = 0.02819379
Iteration 29, loss = 0.02740054
Iteration 30, loss = 0.02675183
Iteration 31, loss = 0.02617234
Iteration 32, loss = 0.02581624
Iteration 33, loss = 0.02501765
Iteration 34, loss = 0.02466664
Iteration 35, loss = 0.02418456
Iteration 36, loss = 0.02373591
Iteration 37, loss = 0.02320201
Iteration 38, loss = 0.02329592
Iteration 39, loss = 0.02285280
Iteration 40, loss = 0.02249814
Iteration 41, loss = 0.02224289
Iteration 42, loss = 0.02186925
Iteration 43, loss = 0.02150078
Iteration 44, loss = 0.02155938
Iteration 45, loss = 0.02126202
Iteration 46, loss = 0.02131909
Iteration 47, loss = 0.02116637
Iteration 48, loss = 0.02092925
Iteration 49, loss = 0.02078660
Iteration 50, loss = 0.02050818
Iteration 51, loss = 0.02037776
Iteration 52, loss = 0.02008198
Iteration 53, loss = 0.02016363
Iteration 54, loss = 0.02015667
Iteration 55, loss = 0.02019644
Iteration 56, loss = 0.01996804
Iteration 57, loss = 0.01963300
Iteration 58, loss = 0.01976811
Iteration 59, loss = 0.01955915
Iteration 60, loss = 0.01946290
Iteration 61, loss = 0.01968063
Iteration 62, loss = 0.01922808
Iteration 63, loss = 0.01930031
Iteration 64, loss = 0.01918015
Iteration 65, loss = 0.01931317
Iteration 66, loss = 0.01938349
Iteration 67, loss = 0.01904012
Iteration 68, loss = 0.01916059
Iteration 69, loss = 0.01916246
Iteration 70, loss = 0.01886236
Iteration 71, loss = 0.01869261
Iteration 72, loss = 0.01896069
Iteration 73, loss = 0.01889275
Iteration 74, loss = 0.01878012
Iteration 75, loss = 0.01866405
Iteration 76, loss = 0.01890037
Iteration 77, loss = 0.01873915
Iteration 78, loss = 0.01863270
Iteration 79, loss = 0.01899426
Iteration 80, loss = 0.01862531
Iteration 81, loss = 0.01843590
Iteration 82, loss = 0.01878501
Iteration 83, loss = 0.01832503
Iteration 84, loss = 0.01886623
Iteration 85, loss = 0.01840863
Iteration 86, loss = 0.01855161
Iteration 87, loss = 0.01844447
Iteration 88, loss = 0.01829686
Iteration 89, loss = 0.01846142
Iteration 90, loss = 0.01820420
Iteration 91, loss = 0.01822962
Iteration 92, loss = 0.01837414
Iteration 93, loss = 0.01829657
Iteration 94, loss = 0.01846035
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67832383
Iteration 2, loss = 0.65192388
Iteration 3, loss = 0.62221839
Iteration 4, loss = 0.58446143
Iteration 5, loss = 0.53893093
Iteration 6, loss = 0.49027455
Iteration 7, loss = 0.44076369
Iteration 8, loss = 0.39259559
Iteration 9, loss = 0.34742215
Iteration 10, loss = 0.30543668
Iteration 11, loss = 0.26799754
Iteration 12, loss = 0.23516778
Iteration 13, loss = 0.20669094
Iteration 14, loss = 0.18207119
Iteration 15, loss = 0.16116612
Iteration 16, loss = 0.14342634
Iteration 17, loss = 0.12807863
Iteration 18, loss = 0.11540970
Iteration 19, loss = 0.10440270
Iteration 20, loss = 0.09503938
Iteration 21, loss = 0.08670061
Iteration 22, loss = 0.07985226
Iteration 23, loss = 0.07365528
Iteration 24, loss = 0.06825110
Iteration 25, loss = 0.06368902
Iteration 26, loss = 0.05960421
Iteration 27, loss = 0.05593829
Iteration 28, loss = 0.05270934
Iteration 29, loss = 0.04984638
Iteration 30, loss = 0.04729988
Iteration 31, loss = 0.04511364
Iteration 32, loss = 0.04291191
Iteration 33, loss = 0.04105290
Iteration 34, loss = 0.03940297
Iteration 35, loss = 0.03779101
Iteration 36, loss = 0.03637188
Iteration 37, loss = 0.03512651
Iteration 38, loss = 0.03353700
Iteration 39, loss = 0.03388251
Iteration 40, loss = 0.03236954
Iteration 41, loss = 0.03112305
Iteration 42, loss = 0.03011919
Iteration 43, loss = 0.02926589
Iteration 44, loss = 0.02858830
Iteration 45, loss = 0.02784232
Iteration 46, loss = 0.02714247
Iteration 47, loss = 0.02641627
Iteration 48, loss = 0.02595103
Iteration 49, loss = 0.02550082
Iteration 50, loss = 0.02500036
Iteration 51, loss = 0.02451399
Iteration 52, loss = 0.02399917
Iteration 53, loss = 0.02377940
Iteration 54, loss = 0.02354618
Iteration 55, loss = 0.02322528
Iteration 56, loss = 0.02281037
Iteration 57, loss = 0.02248338
Iteration 58, loss = 0.02195530
Iteration 59, loss = 0.02160971
Iteration 60, loss = 0.02136529
Iteration 61, loss = 0.02125042
Iteration 62, loss = 0.02098831
Iteration 63, loss = 0.02072802
Iteration 64, loss = 0.02039125
Iteration 65, loss = 0.02034860
Iteration 66, loss = 0.02012906
Iteration 67, loss = 0.01995244
Iteration 68, loss = 0.01961850
Iteration 69, loss = 0.01941739
Iteration 70, loss = 0.01948333
Iteration 71, loss = 0.01944511
Iteration 72, loss = 0.01941131
Iteration 73, loss = 0.01930785
Iteration 74, loss = 0.01890078
Iteration 75, loss = 0.01865454
Iteration 76, loss = 0.01847999
Iteration 77, loss = 0.01832321
Iteration 78, loss = 0.01829823
Iteration 79, loss = 0.01798379
Iteration 80, loss = 0.01807159
Iteration 81, loss = 0.01803194
Iteration 82, loss = 0.01794266
Iteration 83, loss = 0.01781729
Iteration 84, loss = 0.01763750
Iteration 85, loss = 0.01754496
Iteration 86, loss = 0.01745996
Iteration 87, loss = 0.01748731
Iteration 88, loss = 0.01738913
Iteration 89, loss = 0.01724040
Iteration 90, loss = 0.01746881
Iteration 91, loss = 0.01776445
Iteration 92, loss = 0.01740854
Iteration 93, loss = 0.01723591
Iteration 94, loss = 0.01694008
Iteration 95, loss = 0.01669448
Iteration 96, loss = 0.01666851
Iteration 97, loss = 0.01660636
Iteration 98, loss = 0.01668694
Iteration 99, loss = 0.01668034
Iteration 100, loss = 0.01673892
Iteration 101, loss = 0.01661989
Iteration 102, loss = 0.01645463
Iteration 103, loss = 0.01638420
Iteration 104, loss = 0.01630238
Iteration 105, loss = 0.01628442
Iteration 106, loss = 0.01632491
Iteration 107, loss = 0.01649329
Iteration 108, loss = 0.01633473
Iteration 109, loss = 0.01629102
Iteration 110, loss = 0.01607865
Iteration 111, loss = 0.01586674
Iteration 112, loss = 0.01588229
Iteration 113, loss = 0.01583782
Iteration 114, loss = 0.01583902
Iteration 115, loss = 0.01568934
Iteration 116, loss = 0.01588098
Iteration 117, loss = 0.01585974
Iteration 118, loss = 0.01591822
Iteration 119, loss = 0.01560275
Iteration 120, loss = 0.01578976
Iteration 121, loss = 0.01580362
Iteration 122, loss = 0.01571675
Iteration 123, loss = 0.01551991
Iteration 124, loss = 0.01546424
Iteration 125, loss = 0.01543362
Iteration 126, loss = 0.01546330
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69551819
Iteration 2, loss = 0.64160084
Iteration 3, loss = 0.56232584
Iteration 4, loss = 0.46796695
Iteration 5, loss = 0.37819633
Iteration 6, loss = 0.29997652
Iteration 7, loss = 0.23663651
Iteration 8, loss = 0.18809698
Iteration 9, loss = 0.15176938
Iteration 10, loss = 0.12478055
Iteration 11, loss = 0.10464891
Iteration 12, loss = 0.08938741
Iteration 13, loss = 0.07779770
Iteration 14, loss = 0.06861630
Iteration 15, loss = 0.06149870
Iteration 16, loss = 0.05551785
Iteration 17, loss = 0.05071682
Iteration 18, loss = 0.04686323
Iteration 19, loss = 0.04349443
Iteration 20, loss = 0.04091075
Iteration 21, loss = 0.03855602
Iteration 22, loss = 0.03664744
Iteration 23, loss = 0.03475511
Iteration 24, loss = 0.03307845
Iteration 25, loss = 0.03178974
Iteration 26, loss = 0.03075312
Iteration 27, loss = 0.02974183
Iteration 28, loss = 0.02894997
Iteration 29, loss = 0.02815426
Iteration 30, loss = 0.02731445
Iteration 31, loss = 0.02657282
Iteration 32, loss = 0.02609509
Iteration 33, loss = 0.02558241
Iteration 34, loss = 0.02503886
Iteration 35, loss = 0.02460871
Iteration 36, loss = 0.02417435
Iteration 37, loss = 0.02375726
Iteration 38, loss = 0.02344221
Iteration 39, loss = 0.02308245
Iteration 40, loss = 0.02287347
Iteration 41, loss = 0.02264667
Iteration 42, loss = 0.02236520
Iteration 43, loss = 0.02194409
Iteration 44, loss = 0.02202496
Iteration 45, loss = 0.02185608
Iteration 46, loss = 0.02182238
Iteration 47, loss = 0.02146237
Iteration 48, loss = 0.02144337
Iteration 49, loss = 0.02115775
Iteration 50, loss = 0.02104009
Iteration 51, loss = 0.02083915
Iteration 52, loss = 0.02092968
Iteration 53, loss = 0.02056431
Iteration 54, loss = 0.02059815
Iteration 55, loss = 0.02047895
Iteration 56, loss = 0.02041101
Iteration 57, loss = 0.02012529
Iteration 58, loss = 0.02044839
Iteration 59, loss = 0.02018150
Iteration 60, loss = 0.02011730
Iteration 61, loss = 0.01990133
Iteration 62, loss = 0.01982105
Iteration 63, loss = 0.02001081
Iteration 64, loss = 0.01986891
Iteration 65, loss = 0.01952775
Iteration 66, loss = 0.01943857
Iteration 67, loss = 0.01972432
Iteration 68, loss = 0.01948670
Iteration 69, loss = 0.01941308
Iteration 70, loss = 0.01950079
Iteration 71, loss = 0.01932034
Iteration 72, loss = 0.01940620
Iteration 73, loss = 0.01937415
Iteration 74, loss = 0.01940313
Iteration 75, loss = 0.01936978
Iteration 76, loss = 0.01922260
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71403090
Iteration 2, loss = 0.68791994
Iteration 3, loss = 0.65936820
Iteration 4, loss = 0.62512585
Iteration 5, loss = 0.58468644
Iteration 6, loss = 0.53864026
Iteration 7, loss = 0.49006209
Iteration 8, loss = 0.44051335
Iteration 9, loss = 0.39316588
Iteration 10, loss = 0.34857770
Iteration 11, loss = 0.30776877
Iteration 12, loss = 0.27081533
Iteration 13, loss = 0.23853924
Iteration 14, loss = 0.21011952
Iteration 15, loss = 0.18561478
Iteration 16, loss = 0.16437745
Iteration 17, loss = 0.14630154
Iteration 18, loss = 0.13090934
Iteration 19, loss = 0.11779130
Iteration 20, loss = 0.10667714
Iteration 21, loss = 0.09698479
Iteration 22, loss = 0.08858711
Iteration 23, loss = 0.08139667
Iteration 24, loss = 0.07513108
Iteration 25, loss = 0.06958945
Iteration 26, loss = 0.06481655
Iteration 27, loss = 0.06055321
Iteration 28, loss = 0.05682930
Iteration 29, loss = 0.05355506
Iteration 30, loss = 0.05055926
Iteration 31, loss = 0.04790450
Iteration 32, loss = 0.04552411
Iteration 33, loss = 0.04339982
Iteration 34, loss = 0.04146107
Iteration 35, loss = 0.03972484
Iteration 36, loss = 0.03813351
Iteration 37, loss = 0.03653142
Iteration 38, loss = 0.03525586
Iteration 39, loss = 0.03405043
Iteration 40, loss = 0.03293827
Iteration 41, loss = 0.03184954
Iteration 42, loss = 0.03091483
Iteration 43, loss = 0.03015545
Iteration 44, loss = 0.02942530
Iteration 45, loss = 0.02854504
Iteration 46, loss = 0.02794915
Iteration 47, loss = 0.02707363
Iteration 48, loss = 0.02681566
Iteration 49, loss = 0.02614245
Iteration 50, loss = 0.02561014
Iteration 51, loss = 0.02517712
Iteration 52, loss = 0.02465341
Iteration 53, loss = 0.02429725
Iteration 54, loss = 0.02388873
Iteration 55, loss = 0.02352805
Iteration 56, loss = 0.02311461
Iteration 57, loss = 0.02266217
Iteration 58, loss = 0.02225892
Iteration 59, loss = 0.02211405
Iteration 60, loss = 0.02185951
Iteration 61, loss = 0.02153181
Iteration 62, loss = 0.02140860
Iteration 63, loss = 0.02099503
Iteration 64, loss = 0.02070469
Iteration 65, loss = 0.02035438
Iteration 66, loss = 0.02043705
Iteration 67, loss = 0.02011693
Iteration 68, loss = 0.02015614
Iteration 69, loss = 0.01997712
Iteration 70, loss = 0.01992085
Iteration 71, loss = 0.01954637
Iteration 72, loss = 0.01935109
Iteration 73, loss = 0.01911164
Iteration 74, loss = 0.01868005
Iteration 75, loss = 0.01856607
Iteration 76, loss = 0.01842566
Iteration 77, loss = 0.01827863
Iteration 78, loss = 0.01823311
Iteration 79, loss = 0.01822599
Iteration 80, loss = 0.01796716
Iteration 81, loss = 0.01795777
Iteration 82, loss = 0.01794473
Iteration 83, loss = 0.01767993
Iteration 84, loss = 0.01767458
Iteration 85, loss = 0.01737477
Iteration 86, loss = 0.01778152
Iteration 87, loss = 0.01760757
Iteration 88, loss = 0.01779414
Iteration 89, loss = 0.01760318
Iteration 90, loss = 0.01742137
Iteration 91, loss = 0.01715908
Iteration 92, loss = 0.01697172
Iteration 93, loss = 0.01690333
Iteration 94, loss = 0.01689086
Iteration 95, loss = 0.01675979
Iteration 96, loss = 0.01663784
Iteration 97, loss = 0.01650640
Iteration 98, loss = 0.01633352
Iteration 99, loss = 0.01629764
Iteration 100, loss = 0.01630787
Iteration 101, loss = 0.01627075
Iteration 102, loss = 0.01623213
Iteration 103, loss = 0.01619766
Iteration 104, loss = 0.01637345
Iteration 105, loss = 0.01633643
Iteration 106, loss = 0.01626483
Iteration 107, loss = 0.01619900
Iteration 108, loss = 0.01599246
Iteration 109, loss = 0.01591076
Iteration 110, loss = 0.01586781
Iteration 111, loss = 0.01578287
Iteration 112, loss = 0.01572755
Iteration 113, loss = 0.01565872
Iteration 114, loss = 0.01560684
Iteration 115, loss = 0.01558656
Iteration 116, loss = 0.01563819
Iteration 117, loss = 0.01565149
Iteration 118, loss = 0.01573723
Iteration 119, loss = 0.01615000
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67767038
Iteration 2, loss = 0.62657416
Iteration 3, loss = 0.55230241
Iteration 4, loss = 0.46600520
Iteration 5, loss = 0.38324976
Iteration 6, loss = 0.31043525
Iteration 7, loss = 0.24982691
Iteration 8, loss = 0.20178145
Iteration 9, loss = 0.16463561
Iteration 10, loss = 0.13609730
Iteration 11, loss = 0.11456953
Iteration 12, loss = 0.09780038
Iteration 13, loss = 0.08480516
Iteration 14, loss = 0.07455608
Iteration 15, loss = 0.06638344
Iteration 16, loss = 0.05968059
Iteration 17, loss = 0.05431035
Iteration 18, loss = 0.04964128
Iteration 19, loss = 0.04593957
Iteration 20, loss = 0.04253721
Iteration 21, loss = 0.03992455
Iteration 22, loss = 0.03745961
Iteration 23, loss = 0.03541042
Iteration 24, loss = 0.03367390
Iteration 25, loss = 0.03217881
Iteration 26, loss = 0.03081010
Iteration 27, loss = 0.02982400
Iteration 28, loss = 0.02852705
Iteration 29, loss = 0.02788428
Iteration 30, loss = 0.02675457
Iteration 31, loss = 0.02587897
Iteration 32, loss = 0.02531286
Iteration 33, loss = 0.02467726
Iteration 34, loss = 0.02402879
Iteration 35, loss = 0.02379907
Iteration 36, loss = 0.02305447
Iteration 37, loss = 0.02261887
Iteration 38, loss = 0.02208951
Iteration 39, loss = 0.02194295
Iteration 40, loss = 0.02156609
Iteration 41, loss = 0.02106942
Iteration 42, loss = 0.02087587
Iteration 43, loss = 0.02073313
Iteration 44, loss = 0.02035792
Iteration 45, loss = 0.02030882
Iteration 46, loss = 0.02003147
Iteration 47, loss = 0.01954274
Iteration 48, loss = 0.01943276
Iteration 49, loss = 0.01956308
Iteration 50, loss = 0.01916601
Iteration 51, loss = 0.01896624
Iteration 52, loss = 0.01905760
Iteration 53, loss = 0.01874361
Iteration 54, loss = 0.01871644
Iteration 55, loss = 0.01851533
Iteration 56, loss = 0.01842785
Iteration 57, loss = 0.01824081
Iteration 58, loss = 0.01825088
Iteration 59, loss = 0.01805896
Iteration 60, loss = 0.01823196
Iteration 61, loss = 0.01819633
Iteration 62, loss = 0.01797807
Iteration 63, loss = 0.01784771
Iteration 64, loss = 0.01762562
Iteration 65, loss = 0.01755017
Iteration 66, loss = 0.01754448
Iteration 67, loss = 0.01762019
Iteration 68, loss = 0.01746680
Iteration 69, loss = 0.01741713
Iteration 70, loss = 0.01728880
Iteration 71, loss = 0.01726069
Iteration 72, loss = 0.01712932
Iteration 73, loss = 0.01719566
Iteration 74, loss = 0.01712720
Iteration 75, loss = 0.01691976
Iteration 76, loss = 0.01690317
Iteration 77, loss = 0.01711984
Iteration 78, loss = 0.01697960
Iteration 79, loss = 0.01685800
Iteration 80, loss = 0.01681641
Iteration 81, loss = 0.01690261
Iteration 82, loss = 0.01664330
Iteration 83, loss = 0.01665885
Iteration 84, loss = 0.01668405
Iteration 85, loss = 0.01678074
Iteration 86, loss = 0.01663866
Iteration 87, loss = 0.01670202
Iteration 88, loss = 0.01641824
Iteration 89, loss = 0.01641617
Iteration 90, loss = 0.01639425
Iteration 91, loss = 0.01646356
Iteration 92, loss = 0.01651341
Iteration 93, loss = 0.01624691
Iteration 94, loss = 0.01639788
Iteration 95, loss = 0.01628004
Iteration 96, loss = 0.01625083
Iteration 97, loss = 0.01618463
Iteration 98, loss = 0.01618848
Iteration 99, loss = 0.01621648
Iteration 100, loss = 0.01627072
Iteration 101, loss = 0.01612679
Iteration 102, loss = 0.01612826
Iteration 103, loss = 0.01641680
Iteration 104, loss = 0.01619694
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72361442
Iteration 2, loss = 0.68445677
Iteration 3, loss = 0.63544372
Iteration 4, loss = 0.57769078
Iteration 5, loss = 0.51491120
Iteration 6, loss = 0.45085040
Iteration 7, loss = 0.38996890
Iteration 8, loss = 0.33339878
Iteration 9, loss = 0.28304015
Iteration 10, loss = 0.23996481
Iteration 11, loss = 0.20373722
Iteration 12, loss = 0.17376433
Iteration 13, loss = 0.14927004
Iteration 14, loss = 0.12926376
Iteration 15, loss = 0.11300777
Iteration 16, loss = 0.09956749
Iteration 17, loss = 0.08864609
Iteration 18, loss = 0.07945256
Iteration 19, loss = 0.07164183
Iteration 20, loss = 0.06547425
Iteration 21, loss = 0.06003251
Iteration 22, loss = 0.05541592
Iteration 23, loss = 0.05141033
Iteration 24, loss = 0.04793506
Iteration 25, loss = 0.04484387
Iteration 26, loss = 0.04216960
Iteration 27, loss = 0.03987856
Iteration 28, loss = 0.03775586
Iteration 29, loss = 0.03598051
Iteration 30, loss = 0.03441104
Iteration 31, loss = 0.03283906
Iteration 32, loss = 0.03157924
Iteration 33, loss = 0.03039597
Iteration 34, loss = 0.02943030
Iteration 35, loss = 0.02860846
Iteration 36, loss = 0.02738347
Iteration 37, loss = 0.02676356
Iteration 38, loss = 0.02596040
Iteration 39, loss = 0.02535642
Iteration 40, loss = 0.02468115
Iteration 41, loss = 0.02413776
Iteration 42, loss = 0.02359418
Iteration 43, loss = 0.02297693
Iteration 44, loss = 0.02264237
Iteration 45, loss = 0.02215321
Iteration 46, loss = 0.02182688
Iteration 47, loss = 0.02147001
Iteration 48, loss = 0.02110347
Iteration 49, loss = 0.02079997
Iteration 50, loss = 0.02048196
Iteration 51, loss = 0.02021164
Iteration 52, loss = 0.01989365
Iteration 53, loss = 0.01962062
Iteration 54, loss = 0.01979877
Iteration 55, loss = 0.01918729
Iteration 56, loss = 0.01896884
Iteration 57, loss = 0.01893003
Iteration 58, loss = 0.01857721
Iteration 59, loss = 0.01838034
Iteration 60, loss = 0.01829449
Iteration 61, loss = 0.01824981
Iteration 62, loss = 0.01789489
Iteration 63, loss = 0.01781131
Iteration 64, loss = 0.01764426
Iteration 65, loss = 0.01754821
Iteration 66, loss = 0.01743474
Iteration 67, loss = 0.01730554
Iteration 68, loss = 0.01719938
Iteration 69, loss = 0.01705781
Iteration 70, loss = 0.01697647
Iteration 71, loss = 0.01691613
Iteration 72, loss = 0.01685031
Iteration 73, loss = 0.01666923
Iteration 74, loss = 0.01651139
Iteration 75, loss = 0.01655308
Iteration 76, loss = 0.01643573
Iteration 77, loss = 0.01636556
Iteration 78, loss = 0.01631076
Iteration 79, loss = 0.01631820
Iteration 80, loss = 0.01624495
Iteration 81, loss = 0.01624286
Iteration 82, loss = 0.01609592
Iteration 83, loss = 0.01617094
Iteration 84, loss = 0.01599115
Iteration 85, loss = 0.01596389
Iteration 86, loss = 0.01584273
Iteration 87, loss = 0.01563232
Iteration 88, loss = 0.01578305
Iteration 89, loss = 0.01595022
Iteration 90, loss = 0.01585110
Iteration 91, loss = 0.01557163
Iteration 92, loss = 0.01547949
Iteration 93, loss = 0.01555450
Iteration 94, loss = 0.01548639
Iteration 95, loss = 0.01545771
Iteration 96, loss = 0.01536735
Iteration 97, loss = 0.01543974
Iteration 98, loss = 0.01523382
Iteration 99, loss = 0.01527839
Iteration 100, loss = 0.01529113
Iteration 101, loss = 0.01509656
Iteration 102, loss = 0.01512169
Iteration 103, loss = 0.01523582
Iteration 104, loss = 0.01506786
Iteration 105, loss = 0.01502475
Iteration 106, loss = 0.01504654
Iteration 107, loss = 0.01504927
Iteration 108, loss = 0.01504297
Iteration 109, loss = 0.01491557
Iteration 110, loss = 0.01500090
Iteration 111, loss = 0.01500249
Iteration 112, loss = 0.01499397
Iteration 113, loss = 0.01496277
Iteration 114, loss = 0.01489363
Iteration 115, loss = 0.01492116
Iteration 116, loss = 0.01497318
Iteration 117, loss = 0.01485975
Iteration 118, loss = 0.01488324
Iteration 119, loss = 0.01483174
Iteration 120, loss = 0.01494616
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67669432
Iteration 2, loss = 0.64000322
Iteration 3, loss = 0.58579231
Iteration 4, loss = 0.51530821
Iteration 5, loss = 0.44098358
Iteration 6, loss = 0.37129943
Iteration 7, loss = 0.30861599
Iteration 8, loss = 0.25482911
Iteration 9, loss = 0.21078537
Iteration 10, loss = 0.17552963
Iteration 11, loss = 0.14729488
Iteration 12, loss = 0.12496308
Iteration 13, loss = 0.10751882
Iteration 14, loss = 0.09363800
Iteration 15, loss = 0.08223739
Iteration 16, loss = 0.07302435
Iteration 17, loss = 0.06541420
Iteration 18, loss = 0.05930983
Iteration 19, loss = 0.05407185
Iteration 20, loss = 0.04972527
Iteration 21, loss = 0.04595952
Iteration 22, loss = 0.04290866
Iteration 23, loss = 0.03998439
Iteration 24, loss = 0.03765521
Iteration 25, loss = 0.03549746
Iteration 26, loss = 0.03380423
Iteration 27, loss = 0.03222391
Iteration 28, loss = 0.03058638
Iteration 29, loss = 0.02931072
Iteration 30, loss = 0.02831515
Iteration 31, loss = 0.02737659
Iteration 32, loss = 0.02623611
Iteration 33, loss = 0.02547884
Iteration 34, loss = 0.02455995
Iteration 35, loss = 0.02386526
Iteration 36, loss = 0.02332452
Iteration 37, loss = 0.02265691
Iteration 38, loss = 0.02211576
Iteration 39, loss = 0.02164604
Iteration 40, loss = 0.02122707
Iteration 41, loss = 0.02071872
Iteration 42, loss = 0.02046542
Iteration 43, loss = 0.02010335
Iteration 44, loss = 0.01974681
Iteration 45, loss = 0.01945672
Iteration 46, loss = 0.01908708
Iteration 47, loss = 0.01880762
Iteration 48, loss = 0.01867026
Iteration 49, loss = 0.01848667
Iteration 50, loss = 0.01806290
Iteration 51, loss = 0.01794466
Iteration 52, loss = 0.01785954
Iteration 53, loss = 0.01766514
Iteration 54, loss = 0.01757226
Iteration 55, loss = 0.01724417
Iteration 56, loss = 0.01714156
Iteration 57, loss = 0.01695428
Iteration 58, loss = 0.01671321
Iteration 59, loss = 0.01671520
Iteration 60, loss = 0.01651099
Iteration 61, loss = 0.01649983
Iteration 62, loss = 0.01629782
Iteration 63, loss = 0.01625362
Iteration 64, loss = 0.01629014
Iteration 65, loss = 0.01605750
Iteration 66, loss = 0.01599564
Iteration 67, loss = 0.01607800
Iteration 68, loss = 0.01585962
Iteration 69, loss = 0.01568951
Iteration 70, loss = 0.01569024
Iteration 71, loss = 0.01565650
Iteration 72, loss = 0.01553723
Iteration 73, loss = 0.01537474
Iteration 74, loss = 0.01536564
Iteration 75, loss = 0.01548437
Iteration 76, loss = 0.01524753
Iteration 77, loss = 0.01523421
Iteration 78, loss = 0.01523295
Iteration 79, loss = 0.01525956
Iteration 80, loss = 0.01515092
Iteration 81, loss = 0.01511501
Iteration 82, loss = 0.01491313
Iteration 83, loss = 0.01490425
Iteration 84, loss = 0.01477913
Iteration 85, loss = 0.01495976
Iteration 86, loss = 0.01486061
Iteration 87, loss = 0.01489101
Iteration 88, loss = 0.01468887
Iteration 89, loss = 0.01468452
Iteration 90, loss = 0.01473881
Iteration 91, loss = 0.01450737
Iteration 92, loss = 0.01457706
Iteration 93, loss = 0.01448750
Iteration 94, loss = 0.01461036
Iteration 95, loss = 0.01443439
Iteration 96, loss = 0.01456700
Iteration 97, loss = 0.01450518
Iteration 98, loss = 0.01439170
Iteration 99, loss = 0.01442666
Iteration 100, loss = 0.01433755
Iteration 101, loss = 0.01426224
Iteration 102, loss = 0.01433447
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70243762
Iteration 2, loss = 0.63521083
Iteration 3, loss = 0.54761777
Iteration 4, loss = 0.44589137
Iteration 5, loss = 0.35120704
Iteration 6, loss = 0.27207394
Iteration 7, loss = 0.21139413
Iteration 8, loss = 0.16609862
Iteration 9, loss = 0.13352929
Iteration 10, loss = 0.10991718
Iteration 11, loss = 0.09275655
Iteration 12, loss = 0.07958424
Iteration 13, loss = 0.07002549
Iteration 14, loss = 0.06234424
Iteration 15, loss = 0.05606080
Iteration 16, loss = 0.05134090
Iteration 17, loss = 0.04731019
Iteration 18, loss = 0.04414924
Iteration 19, loss = 0.04146702
Iteration 20, loss = 0.03927863
Iteration 21, loss = 0.03713013
Iteration 22, loss = 0.03561378
Iteration 23, loss = 0.03396045
Iteration 24, loss = 0.03323134
Iteration 25, loss = 0.03201339
Iteration 26, loss = 0.03090146
Iteration 27, loss = 0.03018948
Iteration 28, loss = 0.02928374
Iteration 29, loss = 0.02864607
Iteration 30, loss = 0.02772737
Iteration 31, loss = 0.02729712
Iteration 32, loss = 0.02679726
Iteration 33, loss = 0.02630781
Iteration 34, loss = 0.02577846
Iteration 35, loss = 0.02551380
Iteration 36, loss = 0.02497702
Iteration 37, loss = 0.02460907
Iteration 38, loss = 0.02429932
Iteration 39, loss = 0.02389514
Iteration 40, loss = 0.02370370
Iteration 41, loss = 0.02352958
Iteration 42, loss = 0.02347824
Iteration 43, loss = 0.02324325
Iteration 44, loss = 0.02293748
Iteration 45, loss = 0.02285618
Iteration 46, loss = 0.02259198
Iteration 47, loss = 0.02229765
Iteration 48, loss = 0.02233725
Iteration 49, loss = 0.02193624
Iteration 50, loss = 0.02178051
Iteration 51, loss = 0.02171660
Iteration 52, loss = 0.02170495
Iteration 53, loss = 0.02145324
Iteration 54, loss = 0.02130869
Iteration 55, loss = 0.02127021
Iteration 56, loss = 0.02121285
Iteration 57, loss = 0.02107785
Iteration 58, loss = 0.02089765
Iteration 59, loss = 0.02091310
Iteration 60, loss = 0.02097586
Iteration 61, loss = 0.02053512
Iteration 62, loss = 0.02078886
Iteration 63, loss = 0.02043108
Iteration 64, loss = 0.02043498
Iteration 65, loss = 0.02035936
Iteration 66, loss = 0.02055980
Iteration 67, loss = 0.02004328
Iteration 68, loss = 0.02032534
Iteration 69, loss = 0.02016878
Iteration 70, loss = 0.02032217
Iteration 71, loss = 0.01998240
Iteration 72, loss = 0.01997620
Iteration 73, loss = 0.02146390
Iteration 74, loss = 0.02123441
Iteration 75, loss = 0.02056792
Iteration 76, loss = 0.02046593
Iteration 77, loss = 0.02052137
Iteration 78, loss = 0.02030066
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68249397
Iteration 2, loss = 0.62203045
Iteration 3, loss = 0.53364417
Iteration 4, loss = 0.43479505
Iteration 5, loss = 0.34330563
Iteration 6, loss = 0.26719896
Iteration 7, loss = 0.20897407
Iteration 8, loss = 0.16601010
Iteration 9, loss = 0.13478030
Iteration 10, loss = 0.11206256
Iteration 11, loss = 0.09517217
Iteration 12, loss = 0.08240995
Iteration 13, loss = 0.07291385
Iteration 14, loss = 0.06527702
Iteration 15, loss = 0.05939193
Iteration 16, loss = 0.05454881
Iteration 17, loss = 0.05041646
Iteration 18, loss = 0.04715869
Iteration 19, loss = 0.04451426
Iteration 20, loss = 0.04218703
Iteration 21, loss = 0.04040446
Iteration 22, loss = 0.03938459
Iteration 23, loss = 0.03784468
Iteration 24, loss = 0.03606234
Iteration 25, loss = 0.03494058
Iteration 26, loss = 0.03383281
Iteration 27, loss = 0.03290237
Iteration 28, loss = 0.03210763
Iteration 29, loss = 0.03191637
Iteration 30, loss = 0.03144761
Iteration 31, loss = 0.03106928
Iteration 32, loss = 0.03078803
Iteration 33, loss = 0.03004602
Iteration 34, loss = 0.02937301
Iteration 35, loss = 0.02891795
Iteration 36, loss = 0.02839199
Iteration 37, loss = 0.02791551
Iteration 38, loss = 0.02748185
Iteration 39, loss = 0.02713995
Iteration 40, loss = 0.02700374
Iteration 41, loss = 0.02658212
Iteration 42, loss = 0.02630317
Iteration 43, loss = 0.02615239
Iteration 44, loss = 0.02576942
Iteration 45, loss = 0.02562808
Iteration 46, loss = 0.02519891
Iteration 47, loss = 0.02503821
Iteration 48, loss = 0.02484807
Iteration 49, loss = 0.02469206
Iteration 50, loss = 0.02464214
Iteration 51, loss = 0.02452984
Iteration 52, loss = 0.02431596
Iteration 53, loss = 0.02430108
Iteration 54, loss = 0.02408722
Iteration 55, loss = 0.02402240
Iteration 56, loss = 0.02394626
Iteration 57, loss = 0.02392555
Iteration 58, loss = 0.02376653
Iteration 59, loss = 0.02375330
Iteration 60, loss = 0.02356538
Iteration 61, loss = 0.02347248
Iteration 62, loss = 0.02424612
Iteration 63, loss = 0.02422295
Iteration 64, loss = 0.02367905
Iteration 65, loss = 0.02387858
Iteration 66, loss = 0.02363076
Iteration 67, loss = 0.02362345
Iteration 68, loss = 0.02335813
Iteration 69, loss = 0.02326050
Iteration 70, loss = 0.02315182
Iteration 71, loss = 0.02332178
Iteration 72, loss = 0.02293957
Iteration 73, loss = 0.02289472
Iteration 74, loss = 0.02287152
Iteration 75, loss = 0.02276741
Iteration 76, loss = 0.02246802
Iteration 77, loss = 0.02250285
Iteration 78, loss = 0.02358455
Iteration 79, loss = 0.02416333
Iteration 80, loss = 0.02375794
Iteration 81, loss = 0.02373518
Iteration 82, loss = 0.02317988
Iteration 83, loss = 0.02318082
Iteration 84, loss = 0.02306092
Iteration 85, loss = 0.02302752
Iteration 86, loss = 0.02271846
Iteration 87, loss = 0.02272931
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70471951
Iteration 2, loss = 0.64235451
Iteration 3, loss = 0.54920070
Iteration 4, loss = 0.43486551
Iteration 5, loss = 0.33009800
Iteration 6, loss = 0.24729743
Iteration 7, loss = 0.18738122
Iteration 8, loss = 0.14595037
Iteration 9, loss = 0.11717195
Iteration 10, loss = 0.09713504
Iteration 11, loss = 0.08250082
Iteration 12, loss = 0.07191858
Iteration 13, loss = 0.06370850
Iteration 14, loss = 0.05772494
Iteration 15, loss = 0.05261079
Iteration 16, loss = 0.04865235
Iteration 17, loss = 0.04522674
Iteration 18, loss = 0.04257990
Iteration 19, loss = 0.04034072
Iteration 20, loss = 0.03859132
Iteration 21, loss = 0.03709745
Iteration 22, loss = 0.03560314
Iteration 23, loss = 0.03437082
Iteration 24, loss = 0.03329639
Iteration 25, loss = 0.03248316
Iteration 26, loss = 0.03145454
Iteration 27, loss = 0.03088288
Iteration 28, loss = 0.03024255
Iteration 29, loss = 0.02964235
Iteration 30, loss = 0.02930793
Iteration 31, loss = 0.02893595
Iteration 32, loss = 0.02836611
Iteration 33, loss = 0.02782797
Iteration 34, loss = 0.02760003
Iteration 35, loss = 0.02721897
Iteration 36, loss = 0.02704599
Iteration 37, loss = 0.02664011
Iteration 38, loss = 0.02651427
Iteration 39, loss = 0.02636245
Iteration 40, loss = 0.02601210
Iteration 41, loss = 0.02603999
Iteration 42, loss = 0.02562844
Iteration 43, loss = 0.02534556
Iteration 44, loss = 0.02544660
Iteration 45, loss = 0.02530038
Iteration 46, loss = 0.02540791
Iteration 47, loss = 0.02526913
Iteration 48, loss = 0.02532225
Iteration 49, loss = 0.02453364
Iteration 50, loss = 0.02461996
Iteration 51, loss = 0.02474775
Iteration 52, loss = 0.02448360
Iteration 53, loss = 0.02433443
Iteration 54, loss = 0.02433980
Iteration 55, loss = 0.02431751
Iteration 56, loss = 0.02450080
Iteration 57, loss = 0.02442087
Iteration 58, loss = 0.02412749
Iteration 59, loss = 0.02392386
Iteration 60, loss = 0.02382451
Iteration 61, loss = 0.02383762
Iteration 62, loss = 0.02357579
Iteration 63, loss = 0.02377923
Iteration 64, loss = 0.02384926
Iteration 65, loss = 0.02351483
Iteration 66, loss = 0.02333633
Iteration 67, loss = 0.02352180
Iteration 68, loss = 0.02344428
Iteration 69, loss = 0.02369384
Iteration 70, loss = 0.02333502
Iteration 71, loss = 0.02360481
Iteration 72, loss = 0.02350438
Iteration 73, loss = 0.02329814
Iteration 74, loss = 0.02336501
Iteration 75, loss = 0.02344491
Iteration 76, loss = 0.02356586
Iteration 77, loss = 0.02334787
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70221747
Iteration 2, loss = 0.63041410
Iteration 3, loss = 0.52935483
Iteration 4, loss = 0.41514410
Iteration 5, loss = 0.31264799
Iteration 6, loss = 0.23412367
Iteration 7, loss = 0.17828542
Iteration 8, loss = 0.13938123
Iteration 9, loss = 0.11265396
Iteration 10, loss = 0.09368364
Iteration 11, loss = 0.08012748
Iteration 12, loss = 0.06972989
Iteration 13, loss = 0.06215462
Iteration 14, loss = 0.05602593
Iteration 15, loss = 0.05123891
Iteration 16, loss = 0.04745999
Iteration 17, loss = 0.04411061
Iteration 18, loss = 0.04158837
Iteration 19, loss = 0.03930954
Iteration 20, loss = 0.03747552
Iteration 21, loss = 0.03590156
Iteration 22, loss = 0.03435980
Iteration 23, loss = 0.03340265
Iteration 24, loss = 0.03215565
Iteration 25, loss = 0.03138578
Iteration 26, loss = 0.03046026
Iteration 27, loss = 0.02964135
Iteration 28, loss = 0.02903529
Iteration 29, loss = 0.02890571
Iteration 30, loss = 0.02802054
Iteration 31, loss = 0.02789999
Iteration 32, loss = 0.02712560
Iteration 33, loss = 0.02683873
Iteration 34, loss = 0.02632386
Iteration 35, loss = 0.02614869
Iteration 36, loss = 0.02588709
Iteration 37, loss = 0.02558002
Iteration 38, loss = 0.02554574
Iteration 39, loss = 0.02529976
Iteration 40, loss = 0.02488460
Iteration 41, loss = 0.02483767
Iteration 42, loss = 0.02482141
Iteration 43, loss = 0.02437014
Iteration 44, loss = 0.02412713
Iteration 45, loss = 0.02405343
Iteration 46, loss = 0.02407703
Iteration 47, loss = 0.02396989
Iteration 48, loss = 0.02394613
Iteration 49, loss = 0.02372852
Iteration 50, loss = 0.02357468
Iteration 51, loss = 0.02365112
Iteration 52, loss = 0.02344295
Iteration 53, loss = 0.02328295
Iteration 54, loss = 0.02337424
Iteration 55, loss = 0.02343142
Iteration 56, loss = 0.02298800
Iteration 57, loss = 0.02290585
Iteration 58, loss = 0.02289369
Iteration 59, loss = 0.02281524
Iteration 60, loss = 0.02294305
Iteration 61, loss = 0.02278451
Iteration 62, loss = 0.02261713
Iteration 63, loss = 0.02279614
Iteration 64, loss = 0.02247429
Iteration 65, loss = 0.02249610
Iteration 66, loss = 0.02246624
Iteration 67, loss = 0.02263749
Iteration 68, loss = 0.02225158
Iteration 69, loss = 0.02216413
Iteration 70, loss = 0.02250178
Iteration 71, loss = 0.02242833
Iteration 72, loss = 0.02220969
Iteration 73, loss = 0.02238531
Iteration 74, loss = 0.02207769
Iteration 75, loss = 0.02210944
Iteration 76, loss = 0.02201172
Iteration 77, loss = 0.02219798
Iteration 78, loss = 0.02202760
Iteration 79, loss = 0.02224141
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70575841
Iteration 2, loss = 0.67193813
Iteration 3, loss = 0.62467266
Iteration 4, loss = 0.56413644
Iteration 5, loss = 0.49365565
Iteration 6, loss = 0.42102197
Iteration 7, loss = 0.35329549
Iteration 8, loss = 0.29375158
Iteration 9, loss = 0.24328948
Iteration 10, loss = 0.20222825
Iteration 11, loss = 0.16939879
Iteration 12, loss = 0.14328189
Iteration 13, loss = 0.12263814
Iteration 14, loss = 0.10621612
Iteration 15, loss = 0.09284934
Iteration 16, loss = 0.08216383
Iteration 17, loss = 0.07355311
Iteration 18, loss = 0.06630023
Iteration 19, loss = 0.06030124
Iteration 20, loss = 0.05529765
Iteration 21, loss = 0.05108174
Iteration 22, loss = 0.04731600
Iteration 23, loss = 0.04412278
Iteration 24, loss = 0.04149957
Iteration 25, loss = 0.03913945
Iteration 26, loss = 0.03696898
Iteration 27, loss = 0.03505012
Iteration 28, loss = 0.03354987
Iteration 29, loss = 0.03190366
Iteration 30, loss = 0.03081076
Iteration 31, loss = 0.02949226
Iteration 32, loss = 0.02855232
Iteration 33, loss = 0.02756058
Iteration 34, loss = 0.02667029
Iteration 35, loss = 0.02607207
Iteration 36, loss = 0.02514938
Iteration 37, loss = 0.02451621
Iteration 38, loss = 0.02388863
Iteration 39, loss = 0.02346024
Iteration 40, loss = 0.02287820
Iteration 41, loss = 0.02253104
Iteration 42, loss = 0.02200778
Iteration 43, loss = 0.02174196
Iteration 44, loss = 0.02122173
Iteration 45, loss = 0.02099200
Iteration 46, loss = 0.02053357
Iteration 47, loss = 0.02035881
Iteration 48, loss = 0.02010026
Iteration 49, loss = 0.01980765
Iteration 50, loss = 0.01950990
Iteration 51, loss = 0.01928549
Iteration 52, loss = 0.01913948
Iteration 53, loss = 0.01898119
Iteration 54, loss = 0.01867505
Iteration 55, loss = 0.01851368
Iteration 56, loss = 0.01843426
Iteration 57, loss = 0.01826334
Iteration 58, loss = 0.01803630
Iteration 59, loss = 0.01799619
Iteration 60, loss = 0.01768678
Iteration 61, loss = 0.01745012
Iteration 62, loss = 0.01738593
Iteration 63, loss = 0.01742994
Iteration 64, loss = 0.01733228
Iteration 65, loss = 0.01709363
Iteration 66, loss = 0.01698760
Iteration 67, loss = 0.01686587
Iteration 68, loss = 0.01676830
Iteration 69, loss = 0.01656383
Iteration 70, loss = 0.01665775
Iteration 71, loss = 0.01673690
Iteration 72, loss = 0.01654150
Iteration 73, loss = 0.01653769
Iteration 74, loss = 0.01643977
Iteration 75, loss = 0.01630025
Iteration 76, loss = 0.01616705
Iteration 77, loss = 0.01606707
Iteration 78, loss = 0.01604549
Iteration 79, loss = 0.01607091
Iteration 80, loss = 0.01596549
Iteration 81, loss = 0.01595729
Iteration 82, loss = 0.01592146
Iteration 83, loss = 0.01580174
Iteration 84, loss = 0.01590969
Iteration 85, loss = 0.01572438
Iteration 86, loss = 0.01576750
Iteration 87, loss = 0.01555334
Iteration 88, loss = 0.01570018
Iteration 89, loss = 0.01555904
Iteration 90, loss = 0.01546514
Iteration 91, loss = 0.01544814
Iteration 92, loss = 0.01537916
Iteration 93, loss = 0.01545324
Iteration 94, loss = 0.01530743
Iteration 95, loss = 0.01539222
Iteration 96, loss = 0.01533213
Iteration 97, loss = 0.01539527
Iteration 98, loss = 0.01526213
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68088000
Iteration 2, loss = 0.63731611
Iteration 3, loss = 0.57358118
Iteration 4, loss = 0.48788724
Iteration 5, loss = 0.40274104
Iteration 6, loss = 0.32635187
Iteration 7, loss = 0.26272881
Iteration 8, loss = 0.21215910
Iteration 9, loss = 0.17284126
Iteration 10, loss = 0.14285727
Iteration 11, loss = 0.11963445
Iteration 12, loss = 0.10204569
Iteration 13, loss = 0.08830303
Iteration 14, loss = 0.07733800
Iteration 15, loss = 0.06851168
Iteration 16, loss = 0.06143899
Iteration 17, loss = 0.05569419
Iteration 18, loss = 0.05088175
Iteration 19, loss = 0.04695680
Iteration 20, loss = 0.04353836
Iteration 21, loss = 0.04062898
Iteration 22, loss = 0.03812981
Iteration 23, loss = 0.03598619
Iteration 24, loss = 0.03415891
Iteration 25, loss = 0.03254382
Iteration 26, loss = 0.03110415
Iteration 27, loss = 0.02985865
Iteration 28, loss = 0.02872216
Iteration 29, loss = 0.02775554
Iteration 30, loss = 0.02671621
Iteration 31, loss = 0.02608516
Iteration 32, loss = 0.02526966
Iteration 33, loss = 0.02462505
Iteration 34, loss = 0.02410513
Iteration 35, loss = 0.02345543
Iteration 36, loss = 0.02302149
Iteration 37, loss = 0.02248340
Iteration 38, loss = 0.02218165
Iteration 39, loss = 0.02177046
Iteration 40, loss = 0.02135617
Iteration 41, loss = 0.02104934
Iteration 42, loss = 0.02064240
Iteration 43, loss = 0.02042278
Iteration 44, loss = 0.02019568
Iteration 45, loss = 0.01996075
Iteration 46, loss = 0.01973377
Iteration 47, loss = 0.01958866
Iteration 48, loss = 0.01937274
Iteration 49, loss = 0.01924573
Iteration 50, loss = 0.01902046
Iteration 51, loss = 0.01883432
Iteration 52, loss = 0.01873712
Iteration 53, loss = 0.01857317
Iteration 54, loss = 0.01863375
Iteration 55, loss = 0.01857911
Iteration 56, loss = 0.01820469
Iteration 57, loss = 0.01816793
Iteration 58, loss = 0.01830226
Iteration 59, loss = 0.01820261
Iteration 60, loss = 0.01784952
Iteration 61, loss = 0.01769803
Iteration 62, loss = 0.01764158
Iteration 63, loss = 0.01747530
Iteration 64, loss = 0.01743558
Iteration 65, loss = 0.01729980
Iteration 66, loss = 0.01718278
Iteration 67, loss = 0.01715600
Iteration 68, loss = 0.01733066
Iteration 69, loss = 0.01708123
Iteration 70, loss = 0.01701194
Iteration 71, loss = 0.01686860
Iteration 72, loss = 0.01677216
Iteration 73, loss = 0.01675624
Iteration 74, loss = 0.01674762
Iteration 75, loss = 0.01690113
Iteration 76, loss = 0.01661116
Iteration 77, loss = 0.01656734
Iteration 78, loss = 0.01639326
Iteration 79, loss = 0.01651259
Iteration 80, loss = 0.01657043
Iteration 81, loss = 0.01642103
Iteration 82, loss = 0.01653271
Iteration 83, loss = 0.01680118
Iteration 84, loss = 0.01635241
Iteration 85, loss = 0.01640717
Iteration 86, loss = 0.01632721
Iteration 87, loss = 0.01634803
Iteration 88, loss = 0.01621120
Iteration 89, loss = 0.01624949
Iteration 90, loss = 0.01622785
Iteration 91, loss = 0.01615646
Iteration 92, loss = 0.01605634
Iteration 93, loss = 0.01601262
Iteration 94, loss = 0.01647512
Iteration 95, loss = 0.01616442
Iteration 96, loss = 0.01593821
Iteration 97, loss = 0.01596181
Iteration 98, loss = 0.01592763
Iteration 99, loss = 0.01607227
Iteration 100, loss = 0.01623525
Iteration 101, loss = 0.01619557
Iteration 102, loss = 0.01592703
Iteration 103, loss = 0.01589127
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69842083
Iteration 2, loss = 0.63971242
Iteration 3, loss = 0.53805897
Iteration 4, loss = 0.42354834
Iteration 5, loss = 0.32155611
Iteration 6, loss = 0.24232675
Iteration 7, loss = 0.18443533
Iteration 8, loss = 0.14399882
Iteration 9, loss = 0.11589103
Iteration 10, loss = 0.09616670
Iteration 11, loss = 0.08189010
Iteration 12, loss = 0.07100074
Iteration 13, loss = 0.06326821
Iteration 14, loss = 0.05738124
Iteration 15, loss = 0.05221736
Iteration 16, loss = 0.04813702
Iteration 17, loss = 0.04491609
Iteration 18, loss = 0.04256871
Iteration 19, loss = 0.03989368
Iteration 20, loss = 0.03807280
Iteration 21, loss = 0.03674248
Iteration 22, loss = 0.03506608
Iteration 23, loss = 0.03385942
Iteration 24, loss = 0.03262706
Iteration 25, loss = 0.03212139
Iteration 26, loss = 0.03121664
Iteration 27, loss = 0.03045566
Iteration 28, loss = 0.02975271
Iteration 29, loss = 0.02908522
Iteration 30, loss = 0.02866675
Iteration 31, loss = 0.02845647
Iteration 32, loss = 0.02787890
Iteration 33, loss = 0.02743601
Iteration 34, loss = 0.02723078
Iteration 35, loss = 0.02689063
Iteration 36, loss = 0.02671999
Iteration 37, loss = 0.02627811
Iteration 38, loss = 0.02598363
Iteration 39, loss = 0.02595976
Iteration 40, loss = 0.02577371
Iteration 41, loss = 0.02535531
Iteration 42, loss = 0.02544189
Iteration 43, loss = 0.02509391
Iteration 44, loss = 0.02512491
Iteration 45, loss = 0.02471335
Iteration 46, loss = 0.02470337
Iteration 47, loss = 0.02471608
Iteration 48, loss = 0.02460631
Iteration 49, loss = 0.02446389
Iteration 50, loss = 0.02437192
Iteration 51, loss = 0.02446294
Iteration 52, loss = 0.02411753
Iteration 53, loss = 0.02399040
Iteration 54, loss = 0.02427771
Iteration 55, loss = 0.02388258
Iteration 56, loss = 0.02413166
Iteration 57, loss = 0.02401920
Iteration 58, loss = 0.02392941
Iteration 59, loss = 0.02392002
Iteration 60, loss = 0.02341001
Iteration 61, loss = 0.02344846
Iteration 62, loss = 0.02357152
Iteration 63, loss = 0.02345349
Iteration 64, loss = 0.02334535
Iteration 65, loss = 0.02327226
Iteration 66, loss = 0.02331085
Iteration 67, loss = 0.02338279
Iteration 68, loss = 0.02320180
Iteration 69, loss = 0.02320624
Iteration 70, loss = 0.02311264
Iteration 71, loss = 0.02318320
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68134186
Iteration 2, loss = 0.66196026
Iteration 3, loss = 0.64297463
Iteration 4, loss = 0.62114961
Iteration 5, loss = 0.59548335
Iteration 6, loss = 0.56578824
Iteration 7, loss = 0.53214889
Iteration 8, loss = 0.49556220
Iteration 9, loss = 0.45706360
Iteration 10, loss = 0.41872370
Iteration 11, loss = 0.38146814
Iteration 12, loss = 0.34626061
Iteration 13, loss = 0.31343801
Iteration 14, loss = 0.28358507
Iteration 15, loss = 0.25625451
Iteration 16, loss = 0.23176369
Iteration 17, loss = 0.20964765
Iteration 18, loss = 0.19012394
Iteration 19, loss = 0.17276586
Iteration 20, loss = 0.15739622
Iteration 21, loss = 0.14378087
Iteration 22, loss = 0.13174103
Iteration 23, loss = 0.12109639
Iteration 24, loss = 0.11168521
Iteration 25, loss = 0.10329418
Iteration 26, loss = 0.09584697
Iteration 27, loss = 0.08926257
Iteration 28, loss = 0.08334692
Iteration 29, loss = 0.07799615
Iteration 30, loss = 0.07332415
Iteration 31, loss = 0.06901390
Iteration 32, loss = 0.06509591
Iteration 33, loss = 0.06160321
Iteration 34, loss = 0.05844063
Iteration 35, loss = 0.05550023
Iteration 36, loss = 0.05282476
Iteration 37, loss = 0.05041485
Iteration 38, loss = 0.04819126
Iteration 39, loss = 0.04618849
Iteration 40, loss = 0.04429334
Iteration 41, loss = 0.04251960
Iteration 42, loss = 0.04090107
Iteration 43, loss = 0.03942764
Iteration 44, loss = 0.03809717
Iteration 45, loss = 0.03676897
Iteration 46, loss = 0.03561571
Iteration 47, loss = 0.03444752
Iteration 48, loss = 0.03337598
Iteration 49, loss = 0.03242028
Iteration 50, loss = 0.03149725
Iteration 51, loss = 0.03061061
Iteration 52, loss = 0.02980613
Iteration 53, loss = 0.02902964
Iteration 54, loss = 0.02836272
Iteration 55, loss = 0.02762872
Iteration 56, loss = 0.02706586
Iteration 57, loss = 0.02644207
Iteration 58, loss = 0.02585047
Iteration 59, loss = 0.02531523
Iteration 60, loss = 0.02477685
Iteration 61, loss = 0.02432664
Iteration 62, loss = 0.02384662
Iteration 63, loss = 0.02338620
Iteration 64, loss = 0.02298619
Iteration 65, loss = 0.02259391
Iteration 66, loss = 0.02220816
Iteration 67, loss = 0.02181623
Iteration 68, loss = 0.02152114
Iteration 69, loss = 0.02119478
Iteration 70, loss = 0.02090243
Iteration 71, loss = 0.02057427
Iteration 72, loss = 0.02025845
Iteration 73, loss = 0.01999956
Iteration 74, loss = 0.01972066
Iteration 75, loss = 0.01947375
Iteration 76, loss = 0.01924944
Iteration 77, loss = 0.01901325
Iteration 78, loss = 0.01875441
Iteration 79, loss = 0.01855350
Iteration 80, loss = 0.01833216
Iteration 81, loss = 0.01813319
Iteration 82, loss = 0.01796193
Iteration 83, loss = 0.01774319
Iteration 84, loss = 0.01761076
Iteration 85, loss = 0.01745789
Iteration 86, loss = 0.01724594
Iteration 87, loss = 0.01710701
Iteration 88, loss = 0.01696321
Iteration 89, loss = 0.01676388
Iteration 90, loss = 0.01672217
Iteration 91, loss = 0.01649029
Iteration 92, loss = 0.01635843
Iteration 93, loss = 0.01631025
Iteration 94, loss = 0.01618278
Iteration 95, loss = 0.01603011
Iteration 96, loss = 0.01592073
Iteration 97, loss = 0.01576873
Iteration 98, loss = 0.01568742
Iteration 99, loss = 0.01556909
Iteration 100, loss = 0.01544296
Iteration 101, loss = 0.01538647
Iteration 102, loss = 0.01525986
Iteration 103, loss = 0.01519106
Iteration 104, loss = 0.01504047
Iteration 105, loss = 0.01502780
Iteration 106, loss = 0.01491120
Iteration 107, loss = 0.01489732
Iteration 108, loss = 0.01474605
Iteration 109, loss = 0.01471607
Iteration 110, loss = 0.01455854
Iteration 111, loss = 0.01450218
Iteration 112, loss = 0.01447174
Iteration 113, loss = 0.01441903
Iteration 114, loss = 0.01428595
Iteration 115, loss = 0.01420830
Iteration 116, loss = 0.01418249
Iteration 117, loss = 0.01408776
Iteration 118, loss = 0.01409298
Iteration 119, loss = 0.01400530
Iteration 120, loss = 0.01397872
Iteration 121, loss = 0.01392556
Iteration 122, loss = 0.01389347
Iteration 123, loss = 0.01374766
Iteration 124, loss = 0.01370089
Iteration 125, loss = 0.01365080
Iteration 126, loss = 0.01359230
Iteration 127, loss = 0.01351396
Iteration 128, loss = 0.01352326
Iteration 129, loss = 0.01346422
Iteration 130, loss = 0.01343217
Iteration 131, loss = 0.01337834
Iteration 132, loss = 0.01334307
Iteration 133, loss = 0.01327682
Iteration 134, loss = 0.01328437
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71044869
Iteration 2, loss = 0.65103046
Iteration 3, loss = 0.56394110
Iteration 4, loss = 0.45915434
Iteration 5, loss = 0.35997905
Iteration 6, loss = 0.27723259
Iteration 7, loss = 0.21413775
Iteration 8, loss = 0.16825928
Iteration 9, loss = 0.13536559
Iteration 10, loss = 0.11172692
Iteration 11, loss = 0.09453474
Iteration 12, loss = 0.08162257
Iteration 13, loss = 0.07138992
Iteration 14, loss = 0.06392366
Iteration 15, loss = 0.05773788
Iteration 16, loss = 0.05283699
Iteration 17, loss = 0.04886145
Iteration 18, loss = 0.04556095
Iteration 19, loss = 0.04266781
Iteration 20, loss = 0.04044473
Iteration 21, loss = 0.03946120
Iteration 22, loss = 0.03789666
Iteration 23, loss = 0.03634520
Iteration 24, loss = 0.03493755
Iteration 25, loss = 0.03348572
Iteration 26, loss = 0.03372029
Iteration 27, loss = 0.03272357
Iteration 28, loss = 0.03185118
Iteration 29, loss = 0.03071672
Iteration 30, loss = 0.02996422
Iteration 31, loss = 0.02929362
Iteration 32, loss = 0.02880075
Iteration 33, loss = 0.02782127
Iteration 34, loss = 0.02756400
Iteration 35, loss = 0.02702800
Iteration 36, loss = 0.02665261
Iteration 37, loss = 0.02624331
Iteration 38, loss = 0.02577237
Iteration 39, loss = 0.02611104
Iteration 40, loss = 0.02600981
Iteration 41, loss = 0.02538473
Iteration 42, loss = 0.02520498
Iteration 43, loss = 0.02497785
Iteration 44, loss = 0.02538191
Iteration 45, loss = 0.02547903
Iteration 46, loss = 0.02543209
Iteration 47, loss = 0.02509900
Iteration 48, loss = 0.02507724
Iteration 49, loss = 0.02497975
Iteration 50, loss = 0.02538709
Iteration 51, loss = 0.02479985
Iteration 52, loss = 0.02427462
Iteration 53, loss = 0.02427780
Iteration 54, loss = 0.02396712
Iteration 55, loss = 0.02365565
Iteration 56, loss = 0.02362006
Iteration 57, loss = 0.02342139
Iteration 58, loss = 0.02323751
Iteration 59, loss = 0.02335384
Iteration 60, loss = 0.02318242
Iteration 61, loss = 0.02277796
Iteration 62, loss = 0.02292611
Iteration 63, loss = 0.02269333
Iteration 64, loss = 0.02256175
Iteration 65, loss = 0.02260945
Iteration 66, loss = 0.02286878
Iteration 67, loss = 0.02232257
Iteration 68, loss = 0.02236281
Iteration 69, loss = 0.02215604
Iteration 70, loss = 0.02225504
Iteration 71, loss = 0.02190383
Iteration 72, loss = 0.02193176
Iteration 73, loss = 0.02214207
Iteration 74, loss = 0.02181728
Iteration 75, loss = 0.02193407
Iteration 76, loss = 0.02157480
Iteration 77, loss = 0.02170782
Iteration 78, loss = 0.02157212
Iteration 79, loss = 0.02163133
Iteration 80, loss = 0.02131409
Iteration 81, loss = 0.02151398
Iteration 82, loss = 0.02127527
Iteration 83, loss = 0.02220519
Iteration 84, loss = 0.02200020
Iteration 85, loss = 0.02197478
Iteration 86, loss = 0.02204898
Iteration 87, loss = 0.02192202
Iteration 88, loss = 0.02214582
Iteration 89, loss = 0.02321657
Iteration 90, loss = 0.02300150
Iteration 91, loss = 0.02253372
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70066976
Iteration 2, loss = 0.67708022
Iteration 3, loss = 0.65397855
Iteration 4, loss = 0.62804327
Iteration 5, loss = 0.59856927
Iteration 6, loss = 0.56566725
Iteration 7, loss = 0.53069506
Iteration 8, loss = 0.49427089
Iteration 9, loss = 0.45742930
Iteration 10, loss = 0.42098956
Iteration 11, loss = 0.38572327
Iteration 12, loss = 0.35224106
Iteration 13, loss = 0.32078829
Iteration 14, loss = 0.29175459
Iteration 15, loss = 0.26476167
Iteration 16, loss = 0.24037459
Iteration 17, loss = 0.21824120
Iteration 18, loss = 0.19830553
Iteration 19, loss = 0.18040386
Iteration 20, loss = 0.16441375
Iteration 21, loss = 0.15018031
Iteration 22, loss = 0.13746435
Iteration 23, loss = 0.12626148
Iteration 24, loss = 0.11618656
Iteration 25, loss = 0.10734488
Iteration 26, loss = 0.09936587
Iteration 27, loss = 0.09232390
Iteration 28, loss = 0.08595011
Iteration 29, loss = 0.08029624
Iteration 30, loss = 0.07528407
Iteration 31, loss = 0.07067833
Iteration 32, loss = 0.06653433
Iteration 33, loss = 0.06276346
Iteration 34, loss = 0.05939956
Iteration 35, loss = 0.05632534
Iteration 36, loss = 0.05354531
Iteration 37, loss = 0.05096017
Iteration 38, loss = 0.04866385
Iteration 39, loss = 0.04649519
Iteration 40, loss = 0.04452586
Iteration 41, loss = 0.04260949
Iteration 42, loss = 0.04091446
Iteration 43, loss = 0.03941227
Iteration 44, loss = 0.03790872
Iteration 45, loss = 0.03661701
Iteration 46, loss = 0.03533525
Iteration 47, loss = 0.03412688
Iteration 48, loss = 0.03308886
Iteration 49, loss = 0.03206645
Iteration 50, loss = 0.03106814
Iteration 51, loss = 0.03024213
Iteration 52, loss = 0.02933041
Iteration 53, loss = 0.02857189
Iteration 54, loss = 0.02781285
Iteration 55, loss = 0.02714859
Iteration 56, loss = 0.02645792
Iteration 57, loss = 0.02583604
Iteration 58, loss = 0.02526588
Iteration 59, loss = 0.02465069
Iteration 60, loss = 0.02414832
Iteration 61, loss = 0.02362084
Iteration 62, loss = 0.02314052
Iteration 63, loss = 0.02269582
Iteration 64, loss = 0.02226281
Iteration 65, loss = 0.02187879
Iteration 66, loss = 0.02156080
Iteration 67, loss = 0.02111204
Iteration 68, loss = 0.02073894
Iteration 69, loss = 0.02041642
Iteration 70, loss = 0.02012238
Iteration 71, loss = 0.01979406
Iteration 72, loss = 0.01950847
Iteration 73, loss = 0.01920784
Iteration 74, loss = 0.01894228
Iteration 75, loss = 0.01868984
Iteration 76, loss = 0.01844001
Iteration 77, loss = 0.01819571
Iteration 78, loss = 0.01797086
Iteration 79, loss = 0.01772295
Iteration 80, loss = 0.01757937
Iteration 81, loss = 0.01734473
Iteration 82, loss = 0.01718313
Iteration 83, loss = 0.01693971
Iteration 84, loss = 0.01673220
Iteration 85, loss = 0.01660102
Iteration 86, loss = 0.01640540
Iteration 87, loss = 0.01631525
Iteration 88, loss = 0.01610956
Iteration 89, loss = 0.01597935
Iteration 90, loss = 0.01580605
Iteration 91, loss = 0.01567884
Iteration 92, loss = 0.01555299
Iteration 93, loss = 0.01540232
Iteration 94, loss = 0.01527078
Iteration 95, loss = 0.01515208
Iteration 96, loss = 0.01502985
Iteration 97, loss = 0.01490751
Iteration 98, loss = 0.01479456
Iteration 99, loss = 0.01473679
Iteration 100, loss = 0.01461626
Iteration 101, loss = 0.01450078
Iteration 102, loss = 0.01440139
Iteration 103, loss = 0.01435106
Iteration 104, loss = 0.01424363
Iteration 105, loss = 0.01414951
Iteration 106, loss = 0.01406433
Iteration 107, loss = 0.01396400
Iteration 108, loss = 0.01389067
Iteration 109, loss = 0.01377701
Iteration 110, loss = 0.01370299
Iteration 111, loss = 0.01365336
Iteration 112, loss = 0.01356805
Iteration 113, loss = 0.01352290
Iteration 114, loss = 0.01344552
Iteration 115, loss = 0.01338978
Iteration 116, loss = 0.01334839
Iteration 117, loss = 0.01325599
Iteration 118, loss = 0.01317534
Iteration 119, loss = 0.01312141
Iteration 120, loss = 0.01306949
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67135060
Iteration 2, loss = 0.59023824
Iteration 3, loss = 0.48449462
Iteration 4, loss = 0.37774326
Iteration 5, loss = 0.28636305
Iteration 6, loss = 0.21672465
Iteration 7, loss = 0.16596984
Iteration 8, loss = 0.13054117
Iteration 9, loss = 0.10593256
Iteration 10, loss = 0.08777180
Iteration 11, loss = 0.07494143
Iteration 12, loss = 0.06499394
Iteration 13, loss = 0.05746367
Iteration 14, loss = 0.05163105
Iteration 15, loss = 0.04708649
Iteration 16, loss = 0.04313981
Iteration 17, loss = 0.04011584
Iteration 18, loss = 0.03733284
Iteration 19, loss = 0.03536395
Iteration 20, loss = 0.03336299
Iteration 21, loss = 0.03190142
Iteration 22, loss = 0.03051467
Iteration 23, loss = 0.02948514
Iteration 24, loss = 0.02852685
Iteration 25, loss = 0.02747135
Iteration 26, loss = 0.02672785
Iteration 27, loss = 0.02616886
Iteration 28, loss = 0.02539508
Iteration 29, loss = 0.02474932
Iteration 30, loss = 0.02440571
Iteration 31, loss = 0.02389714
Iteration 32, loss = 0.02371146
Iteration 33, loss = 0.02348529
Iteration 34, loss = 0.02287352
Iteration 35, loss = 0.02284125
Iteration 36, loss = 0.02229189
Iteration 37, loss = 0.02200768
Iteration 38, loss = 0.02182932
Iteration 39, loss = 0.02151803
Iteration 40, loss = 0.02129371
Iteration 41, loss = 0.02131859
Iteration 42, loss = 0.02103693
Iteration 43, loss = 0.02076385
Iteration 44, loss = 0.02080555
Iteration 45, loss = 0.02035730
Iteration 46, loss = 0.02072761
Iteration 47, loss = 0.02024918
Iteration 48, loss = 0.02020086
Iteration 49, loss = 0.02034642
Iteration 50, loss = 0.01998799
Iteration 51, loss = 0.01975511
Iteration 52, loss = 0.01999668
Iteration 53, loss = 0.01988377
Iteration 54, loss = 0.01957983
Iteration 55, loss = 0.01971863
Iteration 56, loss = 0.01972675
Iteration 57, loss = 0.01955989
Iteration 58, loss = 0.01945839
Iteration 59, loss = 0.01928958
Iteration 60, loss = 0.01925692
Iteration 61, loss = 0.01954926
Iteration 62, loss = 0.01938504
Iteration 63, loss = 0.01911833
Iteration 64, loss = 0.01925832
Iteration 65, loss = 0.01917156
Iteration 66, loss = 0.01882612
Iteration 67, loss = 0.01901962
Iteration 68, loss = 0.01907643
Iteration 69, loss = 0.01884556
Iteration 70, loss = 0.01879635
Iteration 71, loss = 0.01897466
Iteration 72, loss = 0.01887438
Iteration 73, loss = 0.01890710
Iteration 74, loss = 0.01878317
Iteration 75, loss = 0.01868744
Iteration 76, loss = 0.01891412
Iteration 77, loss = 0.01857002
Iteration 78, loss = 0.01845337
Iteration 79, loss = 0.01893892
Iteration 80, loss = 0.01861110
Iteration 81, loss = 0.01854648
Iteration 82, loss = 0.01882804
Iteration 83, loss = 0.01839578
Iteration 84, loss = 0.01847248
Iteration 85, loss = 0.01833596
Iteration 86, loss = 0.01833988
Iteration 87, loss = 0.01847523
Iteration 88, loss = 0.01855073
Iteration 89, loss = 0.01837955
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67500719
Iteration 2, loss = 0.59955049
Iteration 3, loss = 0.49090799
Iteration 4, loss = 0.37920933
Iteration 5, loss = 0.28261578
Iteration 6, loss = 0.21061248
Iteration 7, loss = 0.16067330
Iteration 8, loss = 0.12649104
Iteration 9, loss = 0.10298304
Iteration 10, loss = 0.08660178
Iteration 11, loss = 0.07441360
Iteration 12, loss = 0.06559500
Iteration 13, loss = 0.05894212
Iteration 14, loss = 0.05372013
Iteration 15, loss = 0.04929241
Iteration 16, loss = 0.04617802
Iteration 17, loss = 0.04328988
Iteration 18, loss = 0.04087657
Iteration 19, loss = 0.03880015
Iteration 20, loss = 0.03728818
Iteration 21, loss = 0.03580225
Iteration 22, loss = 0.03471277
Iteration 23, loss = 0.03351126
Iteration 24, loss = 0.03274608
Iteration 25, loss = 0.03191637
Iteration 26, loss = 0.03108340
Iteration 27, loss = 0.03040936
Iteration 28, loss = 0.02990656
Iteration 29, loss = 0.02959552
Iteration 30, loss = 0.02887011
Iteration 31, loss = 0.02863898
Iteration 32, loss = 0.02850899
Iteration 33, loss = 0.02816736
Iteration 34, loss = 0.02759649
Iteration 35, loss = 0.02723349
Iteration 36, loss = 0.02716092
Iteration 37, loss = 0.02707928
Iteration 38, loss = 0.02661867
Iteration 39, loss = 0.02664499
Iteration 40, loss = 0.02643193
Iteration 41, loss = 0.02629062
Iteration 42, loss = 0.02627614
Iteration 43, loss = 0.02591364
Iteration 44, loss = 0.02565436
Iteration 45, loss = 0.02558076
Iteration 46, loss = 0.02528727
Iteration 47, loss = 0.02543011
Iteration 48, loss = 0.02543345
Iteration 49, loss = 0.02510866
Iteration 50, loss = 0.02506084
Iteration 51, loss = 0.02507656
Iteration 52, loss = 0.02476779
Iteration 53, loss = 0.02478150
Iteration 54, loss = 0.02481449
Iteration 55, loss = 0.02486495
Iteration 56, loss = 0.02456673
Iteration 57, loss = 0.02458446
Iteration 58, loss = 0.02489502
Iteration 59, loss = 0.02462627
Iteration 60, loss = 0.02456353
Iteration 61, loss = 0.02468566
Iteration 62, loss = 0.02415161
Iteration 63, loss = 0.02429172
Iteration 64, loss = 0.02440867
Iteration 65, loss = 0.02452589
Iteration 66, loss = 0.02450995
Iteration 67, loss = 0.02442551
Iteration 68, loss = 0.02425323
Iteration 69, loss = 0.02414071
Iteration 70, loss = 0.02415255
Iteration 71, loss = 0.02402109
Iteration 72, loss = 0.02397553
Iteration 73, loss = 0.02406846
Iteration 74, loss = 0.02389659
Iteration 75, loss = 0.02382127
Iteration 76, loss = 0.02374280
Iteration 77, loss = 0.02380545
Iteration 78, loss = 0.02394683
Iteration 79, loss = 0.02402509
Iteration 80, loss = 0.02401172
Iteration 81, loss = 0.02379589
Iteration 82, loss = 0.02374854
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69697063
Iteration 2, loss = 0.65743849
Iteration 3, loss = 0.60414151
Iteration 4, loss = 0.53851698
Iteration 5, loss = 0.46597690
Iteration 6, loss = 0.39387810
Iteration 7, loss = 0.32837486
Iteration 8, loss = 0.27162129
Iteration 9, loss = 0.22476313
Iteration 10, loss = 0.18692731
Iteration 11, loss = 0.15680732
Iteration 12, loss = 0.13302867
Iteration 13, loss = 0.11408752
Iteration 14, loss = 0.09926186
Iteration 15, loss = 0.08742880
Iteration 16, loss = 0.07774377
Iteration 17, loss = 0.06980179
Iteration 18, loss = 0.06337221
Iteration 19, loss = 0.05800502
Iteration 20, loss = 0.05339183
Iteration 21, loss = 0.04954063
Iteration 22, loss = 0.04630448
Iteration 23, loss = 0.04342170
Iteration 24, loss = 0.04094716
Iteration 25, loss = 0.03890775
Iteration 26, loss = 0.03700650
Iteration 27, loss = 0.03522629
Iteration 28, loss = 0.03382416
Iteration 29, loss = 0.03251969
Iteration 30, loss = 0.03135213
Iteration 31, loss = 0.03040221
Iteration 32, loss = 0.02950298
Iteration 33, loss = 0.02859965
Iteration 34, loss = 0.02767611
Iteration 35, loss = 0.02711603
Iteration 36, loss = 0.02639825
Iteration 37, loss = 0.02586172
Iteration 38, loss = 0.02526496
Iteration 39, loss = 0.02487238
Iteration 40, loss = 0.02432043
Iteration 41, loss = 0.02385713
Iteration 42, loss = 0.02360417
Iteration 43, loss = 0.02311817
Iteration 44, loss = 0.02287468
Iteration 45, loss = 0.02259143
Iteration 46, loss = 0.02225998
Iteration 47, loss = 0.02198145
Iteration 48, loss = 0.02181692
Iteration 49, loss = 0.02164807
Iteration 50, loss = 0.02125576
Iteration 51, loss = 0.02109370
Iteration 52, loss = 0.02088482
Iteration 53, loss = 0.02067046
Iteration 54, loss = 0.02049678
Iteration 55, loss = 0.02051974
Iteration 56, loss = 0.02030396
Iteration 57, loss = 0.02013863
Iteration 58, loss = 0.01985263
Iteration 59, loss = 0.01974421
Iteration 60, loss = 0.01967932
Iteration 61, loss = 0.01955916
Iteration 62, loss = 0.01943372
Iteration 63, loss = 0.01930925
Iteration 64, loss = 0.01919967
Iteration 65, loss = 0.01912058
Iteration 66, loss = 0.01897116
Iteration 67, loss = 0.01895207
Iteration 68, loss = 0.01899370
Iteration 69, loss = 0.01876050
Iteration 70, loss = 0.01876699
Iteration 71, loss = 0.01855216
Iteration 72, loss = 0.01872682
Iteration 73, loss = 0.01848023
Iteration 74, loss = 0.01842326
Iteration 75, loss = 0.01839174
Iteration 76, loss = 0.01840315
Iteration 77, loss = 0.01822842
Iteration 78, loss = 0.01823022
Iteration 79, loss = 0.01826453
Iteration 80, loss = 0.01817527
Iteration 81, loss = 0.01827023
Iteration 82, loss = 0.01796598
Iteration 83, loss = 0.01811895
Iteration 84, loss = 0.01804370
Iteration 85, loss = 0.01783149
Iteration 86, loss = 0.01782368
Iteration 87, loss = 0.01796676
Iteration 88, loss = 0.01775671
Iteration 89, loss = 0.01799240
Iteration 90, loss = 0.01770499
Iteration 91, loss = 0.01763552
Iteration 92, loss = 0.01764165
Iteration 93, loss = 0.01761947
Iteration 94, loss = 0.01765563
Iteration 95, loss = 0.01765148
Iteration 96, loss = 0.01786883
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67975321
Iteration 2, loss = 0.66130822
Iteration 3, loss = 0.64275202
Iteration 4, loss = 0.62135279
Iteration 5, loss = 0.59582508
Iteration 6, loss = 0.56630570
Iteration 7, loss = 0.53269602
Iteration 8, loss = 0.49712347
Iteration 9, loss = 0.46087201
Iteration 10, loss = 0.42523902
Iteration 11, loss = 0.39076621
Iteration 12, loss = 0.35782854
Iteration 13, loss = 0.32705922
Iteration 14, loss = 0.29824110
Iteration 15, loss = 0.27156035
Iteration 16, loss = 0.24729622
Iteration 17, loss = 0.22505170
Iteration 18, loss = 0.20495208
Iteration 19, loss = 0.18688248
Iteration 20, loss = 0.17055479
Iteration 21, loss = 0.15597490
Iteration 22, loss = 0.14293308
Iteration 23, loss = 0.13119552
Iteration 24, loss = 0.12091747
Iteration 25, loss = 0.11170138
Iteration 26, loss = 0.10335843
Iteration 27, loss = 0.09602170
Iteration 28, loss = 0.08938085
Iteration 29, loss = 0.08345959
Iteration 30, loss = 0.07810967
Iteration 31, loss = 0.07331264
Iteration 32, loss = 0.06899472
Iteration 33, loss = 0.06508709
Iteration 34, loss = 0.06154780
Iteration 35, loss = 0.05840117
Iteration 36, loss = 0.05534240
Iteration 37, loss = 0.05268298
Iteration 38, loss = 0.05022744
Iteration 39, loss = 0.04799140
Iteration 40, loss = 0.04597408
Iteration 41, loss = 0.04401311
Iteration 42, loss = 0.04224471
Iteration 43, loss = 0.04061590
Iteration 44, loss = 0.03916466
Iteration 45, loss = 0.03775069
Iteration 46, loss = 0.03645260
Iteration 47, loss = 0.03522434
Iteration 48, loss = 0.03413535
Iteration 49, loss = 0.03309199
Iteration 50, loss = 0.03205905
Iteration 51, loss = 0.03116842
Iteration 52, loss = 0.03029256
Iteration 53, loss = 0.02952800
Iteration 54, loss = 0.02872416
Iteration 55, loss = 0.02805525
Iteration 56, loss = 0.02740243
Iteration 57, loss = 0.02674182
Iteration 58, loss = 0.02615893
Iteration 59, loss = 0.02555638
Iteration 60, loss = 0.02499368
Iteration 61, loss = 0.02447530
Iteration 62, loss = 0.02400070
Iteration 63, loss = 0.02354663
Iteration 64, loss = 0.02308998
Iteration 65, loss = 0.02270312
Iteration 66, loss = 0.02228901
Iteration 67, loss = 0.02193638
Iteration 68, loss = 0.02165018
Iteration 69, loss = 0.02126217
Iteration 70, loss = 0.02091534
Iteration 71, loss = 0.02060137
Iteration 72, loss = 0.02031614
Iteration 73, loss = 0.02002074
Iteration 74, loss = 0.01978241
Iteration 75, loss = 0.01947629
Iteration 76, loss = 0.01924517
Iteration 77, loss = 0.01905248
Iteration 78, loss = 0.01883690
Iteration 79, loss = 0.01856385
Iteration 80, loss = 0.01836833
Iteration 81, loss = 0.01815907
Iteration 82, loss = 0.01799170
Iteration 83, loss = 0.01780452
Iteration 84, loss = 0.01763985
Iteration 85, loss = 0.01741639
Iteration 86, loss = 0.01729099
Iteration 87, loss = 0.01709019
Iteration 88, loss = 0.01696714
Iteration 89, loss = 0.01677368
Iteration 90, loss = 0.01666888
Iteration 91, loss = 0.01653957
Iteration 92, loss = 0.01640390
Iteration 93, loss = 0.01624521
Iteration 94, loss = 0.01611377
Iteration 95, loss = 0.01607852
Iteration 96, loss = 0.01587001
Iteration 97, loss = 0.01576449
Iteration 98, loss = 0.01567313
Iteration 99, loss = 0.01557625
Iteration 100, loss = 0.01544974
Iteration 101, loss = 0.01533537
Iteration 102, loss = 0.01524533
Iteration 103, loss = 0.01520881
Iteration 104, loss = 0.01505446
Iteration 105, loss = 0.01499198
Iteration 106, loss = 0.01491986
Iteration 107, loss = 0.01485408
Iteration 108, loss = 0.01476204
Iteration 109, loss = 0.01466499
Iteration 110, loss = 0.01461258
Iteration 111, loss = 0.01449892
Iteration 112, loss = 0.01444377
Iteration 113, loss = 0.01436124
Iteration 114, loss = 0.01431310
Iteration 115, loss = 0.01423985
Iteration 116, loss = 0.01416357
Iteration 117, loss = 0.01411678
Iteration 118, loss = 0.01405535
Iteration 119, loss = 0.01398709
Iteration 120, loss = 0.01396412
Iteration 121, loss = 0.01388232
Iteration 122, loss = 0.01383887
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70393374
Iteration 2, loss = 0.65214540
Iteration 3, loss = 0.58029679
Iteration 4, loss = 0.48809064
Iteration 5, loss = 0.39348753
Iteration 6, loss = 0.30985679
Iteration 7, loss = 0.24266298
Iteration 8, loss = 0.19120653
Iteration 9, loss = 0.15326459
Iteration 10, loss = 0.12522734
Iteration 11, loss = 0.10471534
Iteration 12, loss = 0.08906402
Iteration 13, loss = 0.07725780
Iteration 14, loss = 0.06806287
Iteration 15, loss = 0.06090421
Iteration 16, loss = 0.05473533
Iteration 17, loss = 0.05018992
Iteration 18, loss = 0.04605439
Iteration 19, loss = 0.04274221
Iteration 20, loss = 0.04013373
Iteration 21, loss = 0.03740931
Iteration 22, loss = 0.03554065
Iteration 23, loss = 0.03370708
Iteration 24, loss = 0.03227884
Iteration 25, loss = 0.03079652
Iteration 26, loss = 0.02975455
Iteration 27, loss = 0.02870608
Iteration 28, loss = 0.02779045
Iteration 29, loss = 0.02696639
Iteration 30, loss = 0.02616401
Iteration 31, loss = 0.02564291
Iteration 32, loss = 0.02505337
Iteration 33, loss = 0.02435854
Iteration 34, loss = 0.02388677
Iteration 35, loss = 0.02343582
Iteration 36, loss = 0.02310674
Iteration 37, loss = 0.02271518
Iteration 38, loss = 0.02244530
Iteration 39, loss = 0.02228195
Iteration 40, loss = 0.02193886
Iteration 41, loss = 0.02155079
Iteration 42, loss = 0.02142774
Iteration 43, loss = 0.02108121
Iteration 44, loss = 0.02085589
Iteration 45, loss = 0.02068867
Iteration 46, loss = 0.02041791
Iteration 47, loss = 0.02047152
Iteration 48, loss = 0.02022168
Iteration 49, loss = 0.02028187
Iteration 50, loss = 0.01996597
Iteration 51, loss = 0.01981659
Iteration 52, loss = 0.01986911
Iteration 53, loss = 0.01951886
Iteration 54, loss = 0.01940435
Iteration 55, loss = 0.01936302
Iteration 56, loss = 0.01916793
Iteration 57, loss = 0.01922725
Iteration 58, loss = 0.01914258
Iteration 59, loss = 0.01904606
Iteration 60, loss = 0.01899918
Iteration 61, loss = 0.01896694
Iteration 62, loss = 0.01882670
Iteration 63, loss = 0.01874392
Iteration 64, loss = 0.01870733
Iteration 65, loss = 0.01871581
Iteration 66, loss = 0.01849291
Iteration 67, loss = 0.01861638
Iteration 68, loss = 0.01849581
Iteration 69, loss = 0.01841596
Iteration 70, loss = 0.01826053
Iteration 71, loss = 0.01814465
Iteration 72, loss = 0.01826445
Iteration 73, loss = 0.01839688
Iteration 74, loss = 0.01830380
Iteration 75, loss = 0.01802164
Iteration 76, loss = 0.01810736
Iteration 77, loss = 0.01817173
Iteration 78, loss = 0.01804490
Iteration 79, loss = 0.01801878
Iteration 80, loss = 0.01807972
Iteration 81, loss = 0.01791153
Iteration 82, loss = 0.01790835
Iteration 83, loss = 0.01783503
Iteration 84, loss = 0.01781552
Iteration 85, loss = 0.01792543
Iteration 86, loss = 0.01783271
Iteration 87, loss = 0.01790044
Iteration 88, loss = 0.01792119
Iteration 89, loss = 0.01767405
Iteration 90, loss = 0.01787858
Iteration 91, loss = 0.01800196
Iteration 92, loss = 0.01748637
Iteration 93, loss = 0.01764566
Iteration 94, loss = 0.01764973
Iteration 95, loss = 0.01769987
Iteration 96, loss = 0.01766567
Iteration 97, loss = 0.01765653
Iteration 98, loss = 0.01786292
Iteration 99, loss = 0.01751982
Iteration 100, loss = 0.01765922
Iteration 101, loss = 0.01759423
Iteration 102, loss = 0.01765745
Iteration 103, loss = 0.01744240
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71722589
Iteration 2, loss = 0.67749985
Iteration 3, loss = 0.61936048
Iteration 4, loss = 0.55076504
Iteration 5, loss = 0.47719881
Iteration 6, loss = 0.40498090
Iteration 7, loss = 0.33912283
Iteration 8, loss = 0.28203431
Iteration 9, loss = 0.23407462
Iteration 10, loss = 0.19495478
Iteration 11, loss = 0.16368335
Iteration 12, loss = 0.13878405
Iteration 13, loss = 0.11900887
Iteration 14, loss = 0.10339231
Iteration 15, loss = 0.09067331
Iteration 16, loss = 0.08061052
Iteration 17, loss = 0.07223201
Iteration 18, loss = 0.06531874
Iteration 19, loss = 0.05975505
Iteration 20, loss = 0.05484173
Iteration 21, loss = 0.05107154
Iteration 22, loss = 0.04741623
Iteration 23, loss = 0.04450775
Iteration 24, loss = 0.04194508
Iteration 25, loss = 0.03989210
Iteration 26, loss = 0.03774246
Iteration 27, loss = 0.03592887
Iteration 28, loss = 0.03442412
Iteration 29, loss = 0.03319175
Iteration 30, loss = 0.03204393
Iteration 31, loss = 0.03103109
Iteration 32, loss = 0.03006376
Iteration 33, loss = 0.02900688
Iteration 34, loss = 0.02833917
Iteration 35, loss = 0.02750837
Iteration 36, loss = 0.02684406
Iteration 37, loss = 0.02627674
Iteration 38, loss = 0.02584667
Iteration 39, loss = 0.02526769
Iteration 40, loss = 0.02468533
Iteration 41, loss = 0.02423489
Iteration 42, loss = 0.02392320
Iteration 43, loss = 0.02379966
Iteration 44, loss = 0.02322684
Iteration 45, loss = 0.02281906
Iteration 46, loss = 0.02248725
Iteration 47, loss = 0.02231745
Iteration 48, loss = 0.02210409
Iteration 49, loss = 0.02181431
Iteration 50, loss = 0.02162388
Iteration 51, loss = 0.02142313
Iteration 52, loss = 0.02120926
Iteration 53, loss = 0.02098918
Iteration 54, loss = 0.02083529
Iteration 55, loss = 0.02081241
Iteration 56, loss = 0.02054391
Iteration 57, loss = 0.02031137
Iteration 58, loss = 0.02012261
Iteration 59, loss = 0.01993980
Iteration 60, loss = 0.01986437
Iteration 61, loss = 0.01985127
Iteration 62, loss = 0.01978675
Iteration 63, loss = 0.01958165
Iteration 64, loss = 0.01949251
Iteration 65, loss = 0.01950688
Iteration 66, loss = 0.01928281
Iteration 67, loss = 0.01913142
Iteration 68, loss = 0.01926309
Iteration 69, loss = 0.01903193
Iteration 70, loss = 0.01923358
Iteration 71, loss = 0.01896083
Iteration 72, loss = 0.01884434
Iteration 73, loss = 0.01894576
Iteration 74, loss = 0.01882423
Iteration 75, loss = 0.01884705
Iteration 76, loss = 0.01892342
Iteration 77, loss = 0.01859078
Iteration 78, loss = 0.01854817
Iteration 79, loss = 0.01851730
Iteration 80, loss = 0.01847168
Iteration 81, loss = 0.01833968
Iteration 82, loss = 0.01833019
Iteration 83, loss = 0.01838144
Iteration 84, loss = 0.01827517
Iteration 85, loss = 0.01818886
Iteration 86, loss = 0.01821383
Iteration 87, loss = 0.01841072
Iteration 88, loss = 0.01824085
Iteration 89, loss = 0.01804940
Iteration 90, loss = 0.01801926
Iteration 91, loss = 0.01792681
Iteration 92, loss = 0.01791510
Iteration 93, loss = 0.01782353
Iteration 94, loss = 0.01782251
Iteration 95, loss = 0.01799423
Iteration 96, loss = 0.01794543
Iteration 97, loss = 0.01771011
Iteration 98, loss = 0.01771027
Iteration 99, loss = 0.01788513
Iteration 100, loss = 0.01784361
Iteration 101, loss = 0.01777679
Iteration 102, loss = 0.01776432
Iteration 103, loss = 0.01783382
Iteration 104, loss = 0.01759208
Iteration 105, loss = 0.01777084
Iteration 106, loss = 0.01760054
Iteration 107, loss = 0.01769531
Iteration 108, loss = 0.01745527
Iteration 109, loss = 0.01756460
Iteration 110, loss = 0.01751333
Iteration 111, loss = 0.01755000
Iteration 112, loss = 0.01757151
Iteration 113, loss = 0.01752840
Iteration 114, loss = 0.01747770
Iteration 115, loss = 0.01738978
Iteration 116, loss = 0.01736560
Iteration 117, loss = 0.01735715
Iteration 118, loss = 0.01741744
Iteration 119, loss = 0.01738406
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67621813
Iteration 2, loss = 0.61585600
Iteration 3, loss = 0.52244632
Iteration 4, loss = 0.41876556
Iteration 5, loss = 0.32672428
Iteration 6, loss = 0.25152724
Iteration 7, loss = 0.19521497
Iteration 8, loss = 0.15469377
Iteration 9, loss = 0.12562101
Iteration 10, loss = 0.10515373
Iteration 11, loss = 0.08949587
Iteration 12, loss = 0.07757470
Iteration 13, loss = 0.06841707
Iteration 14, loss = 0.06123505
Iteration 15, loss = 0.05536847
Iteration 16, loss = 0.05066720
Iteration 17, loss = 0.04690708
Iteration 18, loss = 0.04358843
Iteration 19, loss = 0.04095321
Iteration 20, loss = 0.03874495
Iteration 21, loss = 0.03679848
Iteration 22, loss = 0.03505138
Iteration 23, loss = 0.03360328
Iteration 24, loss = 0.03220119
Iteration 25, loss = 0.03091681
Iteration 26, loss = 0.03015559
Iteration 27, loss = 0.02922411
Iteration 28, loss = 0.02886944
Iteration 29, loss = 0.02835933
Iteration 30, loss = 0.02724535
Iteration 31, loss = 0.02679573
Iteration 32, loss = 0.02617515
Iteration 33, loss = 0.02564342
Iteration 34, loss = 0.02529387
Iteration 35, loss = 0.02495779
Iteration 36, loss = 0.02455548
Iteration 37, loss = 0.02410751
Iteration 38, loss = 0.02395143
Iteration 39, loss = 0.02365327
Iteration 40, loss = 0.02448698
Iteration 41, loss = 0.02530221
Iteration 42, loss = 0.02494457
Iteration 43, loss = 0.02448467
Iteration 44, loss = 0.02376790
Iteration 45, loss = 0.02346888
Iteration 46, loss = 0.02297368
Iteration 47, loss = 0.02273421
Iteration 48, loss = 0.02249874
Iteration 49, loss = 0.02233076
Iteration 50, loss = 0.02190602
Iteration 51, loss = 0.02191907
Iteration 52, loss = 0.02161159
Iteration 53, loss = 0.02129391
Iteration 54, loss = 0.02123276
Iteration 55, loss = 0.02105426
Iteration 56, loss = 0.02098053
Iteration 57, loss = 0.02086764
Iteration 58, loss = 0.02079164
Iteration 59, loss = 0.02083051
Iteration 60, loss = 0.02061449
Iteration 61, loss = 0.02058176
Iteration 62, loss = 0.02021415
Iteration 63, loss = 0.02025026
Iteration 64, loss = 0.02120711
Iteration 65, loss = 0.02136794
Iteration 66, loss = 0.02073686
Iteration 67, loss = 0.02055102
Iteration 68, loss = 0.02050021
Iteration 69, loss = 0.02017637
Iteration 70, loss = 0.02024826
Iteration 71, loss = 0.02017600
Iteration 72, loss = 0.02016076
Iteration 73, loss = 0.02003406
Iteration 74, loss = 0.02011548
Iteration 75, loss = 0.02157938
Iteration 76, loss = 0.02262356
Iteration 77, loss = 0.02185855
Iteration 78, loss = 0.02146719
Iteration 79, loss = 0.02115412
Iteration 80, loss = 0.02075389
Iteration 81, loss = 0.02062691
Iteration 82, loss = 0.02058394
Iteration 83, loss = 0.02025239
Iteration 84, loss = 0.02009002
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71347725
Iteration 2, loss = 0.68245508
Iteration 3, loss = 0.64592112
Iteration 4, loss = 0.60271769
Iteration 5, loss = 0.55483358
Iteration 6, loss = 0.50289326
Iteration 7, loss = 0.44974401
Iteration 8, loss = 0.39777748
Iteration 9, loss = 0.34911105
Iteration 10, loss = 0.30496743
Iteration 11, loss = 0.26599429
Iteration 12, loss = 0.23168333
Iteration 13, loss = 0.20244172
Iteration 14, loss = 0.17726197
Iteration 15, loss = 0.15591366
Iteration 16, loss = 0.13785720
Iteration 17, loss = 0.12267139
Iteration 18, loss = 0.10994189
Iteration 19, loss = 0.09887186
Iteration 20, loss = 0.08968739
Iteration 21, loss = 0.08159192
Iteration 22, loss = 0.07503490
Iteration 23, loss = 0.06918908
Iteration 24, loss = 0.06398257
Iteration 25, loss = 0.05947714
Iteration 26, loss = 0.05555291
Iteration 27, loss = 0.05200204
Iteration 28, loss = 0.04882939
Iteration 29, loss = 0.04607209
Iteration 30, loss = 0.04360627
Iteration 31, loss = 0.04140332
Iteration 32, loss = 0.03942149
Iteration 33, loss = 0.03762827
Iteration 34, loss = 0.03607418
Iteration 35, loss = 0.03457992
Iteration 36, loss = 0.03328231
Iteration 37, loss = 0.03202948
Iteration 38, loss = 0.03084028
Iteration 39, loss = 0.02981188
Iteration 40, loss = 0.02911585
Iteration 41, loss = 0.02821150
Iteration 42, loss = 0.02724203
Iteration 43, loss = 0.02649887
Iteration 44, loss = 0.02578637
Iteration 45, loss = 0.02511816
Iteration 46, loss = 0.02443246
Iteration 47, loss = 0.02398015
Iteration 48, loss = 0.02353522
Iteration 49, loss = 0.02329409
Iteration 50, loss = 0.02288627
Iteration 51, loss = 0.02213829
Iteration 52, loss = 0.02183583
Iteration 53, loss = 0.02135971
Iteration 54, loss = 0.02091369
Iteration 55, loss = 0.02064229
Iteration 56, loss = 0.02037636
Iteration 57, loss = 0.02024627
Iteration 58, loss = 0.02010963
Iteration 59, loss = 0.01980146
Iteration 60, loss = 0.01935912
Iteration 61, loss = 0.01923552
Iteration 62, loss = 0.01891729
Iteration 63, loss = 0.01870977
Iteration 64, loss = 0.01847922
Iteration 65, loss = 0.01809442
Iteration 66, loss = 0.01796042
Iteration 67, loss = 0.01763674
Iteration 68, loss = 0.01755852
Iteration 69, loss = 0.01734953
Iteration 70, loss = 0.01720995
Iteration 71, loss = 0.01702844
Iteration 72, loss = 0.01683550
Iteration 73, loss = 0.01674853
Iteration 74, loss = 0.01658444
Iteration 75, loss = 0.01641705
Iteration 76, loss = 0.01633698
Iteration 77, loss = 0.01625455
Iteration 78, loss = 0.01611779
Iteration 79, loss = 0.01596736
Iteration 80, loss = 0.01663955
Iteration 81, loss = 0.01653771
Iteration 82, loss = 0.01630867
Iteration 83, loss = 0.01607673
Iteration 84, loss = 0.01568228
Iteration 85, loss = 0.01559616
Iteration 86, loss = 0.01552242
Iteration 87, loss = 0.01548413
Iteration 88, loss = 0.01550126
Iteration 89, loss = 0.01489731
Iteration 90, loss = 0.01534091
Iteration 91, loss = 0.01567349
Iteration 92, loss = 0.01534078
Iteration 93, loss = 0.01515257
Iteration 94, loss = 0.01496299
Iteration 95, loss = 0.01486972
Iteration 96, loss = 0.01472241
Iteration 97, loss = 0.01466030
Iteration 98, loss = 0.01458839
Iteration 99, loss = 0.01454189
Iteration 100, loss = 0.01446624
Iteration 101, loss = 0.01446249
Iteration 102, loss = 0.01445608
Iteration 103, loss = 0.01443372
Iteration 104, loss = 0.01440321
Iteration 105, loss = 0.01436041
Iteration 106, loss = 0.01438602
Iteration 107, loss = 0.01446598
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68051336
Iteration 2, loss = 0.61579670
Iteration 3, loss = 0.51397892
Iteration 4, loss = 0.40264389
Iteration 5, loss = 0.30599363
Iteration 6, loss = 0.23167967
Iteration 7, loss = 0.17769561
Iteration 8, loss = 0.14010587
Iteration 9, loss = 0.11324029
Iteration 10, loss = 0.09404574
Iteration 11, loss = 0.08020939
Iteration 12, loss = 0.06953778
Iteration 13, loss = 0.06136627
Iteration 14, loss = 0.05529588
Iteration 15, loss = 0.05012677
Iteration 16, loss = 0.04626377
Iteration 17, loss = 0.04291218
Iteration 18, loss = 0.04018690
Iteration 19, loss = 0.03787179
Iteration 20, loss = 0.03585045
Iteration 21, loss = 0.03417722
Iteration 22, loss = 0.03274893
Iteration 23, loss = 0.03143099
Iteration 24, loss = 0.03025426
Iteration 25, loss = 0.02962307
Iteration 26, loss = 0.02871754
Iteration 27, loss = 0.02775716
Iteration 28, loss = 0.02733535
Iteration 29, loss = 0.02635834
Iteration 30, loss = 0.02602438
Iteration 31, loss = 0.02549794
Iteration 32, loss = 0.02532562
Iteration 33, loss = 0.02469012
Iteration 34, loss = 0.02441832
Iteration 35, loss = 0.02396374
Iteration 36, loss = 0.02368317
Iteration 37, loss = 0.02352820
Iteration 38, loss = 0.02312091
Iteration 39, loss = 0.02310137
Iteration 40, loss = 0.02280251
Iteration 41, loss = 0.02266984
Iteration 42, loss = 0.02233855
Iteration 43, loss = 0.02217730
Iteration 44, loss = 0.02203646
Iteration 45, loss = 0.02179782
Iteration 46, loss = 0.02197159
Iteration 47, loss = 0.02163409
Iteration 48, loss = 0.02147536
Iteration 49, loss = 0.02152395
Iteration 50, loss = 0.02173659
Iteration 51, loss = 0.02126674
Iteration 52, loss = 0.02093519
Iteration 53, loss = 0.02110516
Iteration 54, loss = 0.02111994
Iteration 55, loss = 0.02098952
Iteration 56, loss = 0.02071928
Iteration 57, loss = 0.02066983
Iteration 58, loss = 0.02078456
Iteration 59, loss = 0.02083317
Iteration 60, loss = 0.02060235
Iteration 61, loss = 0.02035827
Iteration 62, loss = 0.02031351
Iteration 63, loss = 0.02039993
Iteration 64, loss = 0.02049733
Iteration 65, loss = 0.02047157
Iteration 66, loss = 0.02056259
Iteration 67, loss = 0.02035839
Iteration 68, loss = 0.01994208
Iteration 69, loss = 0.02039030
Iteration 70, loss = 0.02002742
Iteration 71, loss = 0.02013626
Iteration 72, loss = 0.02003568
Iteration 73, loss = 0.02004329
Iteration 74, loss = 0.01996180
Iteration 75, loss = 0.01996522
Iteration 76, loss = 0.01989364
Iteration 77, loss = 0.02004179
Iteration 78, loss = 0.01964781
Iteration 79, loss = 0.02006920
Iteration 80, loss = 0.01984748
Iteration 81, loss = 0.01998998
Iteration 82, loss = 0.01982752
Iteration 83, loss = 0.01966373
Iteration 84, loss = 0.01966975
Iteration 85, loss = 0.01939059
Iteration 86, loss = 0.01960093
Iteration 87, loss = 0.01944948
Iteration 88, loss = 0.01959153
Iteration 89, loss = 0.02004920
Iteration 90, loss = 0.01985460
Iteration 91, loss = 0.01978909
Iteration 92, loss = 0.01951106
Iteration 93, loss = 0.01956603
Iteration 94, loss = 0.01963807
Iteration 95, loss = 0.02010011
Iteration 96, loss = 0.01971453
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68190582
Iteration 2, loss = 0.65163561
Iteration 3, loss = 0.60923831
Iteration 4, loss = 0.55329184
Iteration 5, loss = 0.49194813
Iteration 6, loss = 0.43045907
Iteration 7, loss = 0.37159320
Iteration 8, loss = 0.31830069
Iteration 9, loss = 0.27084405
Iteration 10, loss = 0.23041584
Iteration 11, loss = 0.19625255
Iteration 12, loss = 0.16796973
Iteration 13, loss = 0.14478535
Iteration 14, loss = 0.12576312
Iteration 15, loss = 0.11008593
Iteration 16, loss = 0.09736814
Iteration 17, loss = 0.08674899
Iteration 18, loss = 0.07800034
Iteration 19, loss = 0.07056233
Iteration 20, loss = 0.06434394
Iteration 21, loss = 0.05897672
Iteration 22, loss = 0.05439476
Iteration 23, loss = 0.05061328
Iteration 24, loss = 0.04715638
Iteration 25, loss = 0.04414038
Iteration 26, loss = 0.04171601
Iteration 27, loss = 0.03928537
Iteration 28, loss = 0.03718645
Iteration 29, loss = 0.03547216
Iteration 30, loss = 0.03383238
Iteration 31, loss = 0.03235226
Iteration 32, loss = 0.03115768
Iteration 33, loss = 0.02996312
Iteration 34, loss = 0.02888888
Iteration 35, loss = 0.02795275
Iteration 36, loss = 0.02703808
Iteration 37, loss = 0.02614515
Iteration 38, loss = 0.02552618
Iteration 39, loss = 0.02491555
Iteration 40, loss = 0.02427829
Iteration 41, loss = 0.02373253
Iteration 42, loss = 0.02314724
Iteration 43, loss = 0.02261559
Iteration 44, loss = 0.02221050
Iteration 45, loss = 0.02178965
Iteration 46, loss = 0.02141151
Iteration 47, loss = 0.02101349
Iteration 48, loss = 0.02071786
Iteration 49, loss = 0.02032524
Iteration 50, loss = 0.02016679
Iteration 51, loss = 0.01972281
Iteration 52, loss = 0.01955321
Iteration 53, loss = 0.01936006
Iteration 54, loss = 0.01907544
Iteration 55, loss = 0.01888779
Iteration 56, loss = 0.01876191
Iteration 57, loss = 0.01837198
Iteration 58, loss = 0.01828825
Iteration 59, loss = 0.01807612
Iteration 60, loss = 0.01783197
Iteration 61, loss = 0.01782281
Iteration 62, loss = 0.01768899
Iteration 63, loss = 0.01750072
Iteration 64, loss = 0.01729124
Iteration 65, loss = 0.01716552
Iteration 66, loss = 0.01712199
Iteration 67, loss = 0.01698656
Iteration 68, loss = 0.01683153
Iteration 69, loss = 0.01683788
Iteration 70, loss = 0.01666019
Iteration 71, loss = 0.01662751
Iteration 72, loss = 0.01681796
Iteration 73, loss = 0.01640596
Iteration 74, loss = 0.01645042
Iteration 75, loss = 0.01631290
Iteration 76, loss = 0.01617593
Iteration 77, loss = 0.01603269
Iteration 78, loss = 0.01603638
Iteration 79, loss = 0.01590101
Iteration 80, loss = 0.01577534
Iteration 81, loss = 0.01592009
Iteration 82, loss = 0.01579777
Iteration 83, loss = 0.01573113
Iteration 84, loss = 0.01556326
Iteration 85, loss = 0.01549174
Iteration 86, loss = 0.01550591
Iteration 87, loss = 0.01547564
Iteration 88, loss = 0.01532425
Iteration 89, loss = 0.01540551
Iteration 90, loss = 0.01530256
Iteration 91, loss = 0.01530913
Iteration 92, loss = 0.01521527
Iteration 93, loss = 0.01519512
Iteration 94, loss = 0.01510297
Iteration 95, loss = 0.01512427
Iteration 96, loss = 0.01505861
Iteration 97, loss = 0.01508999
Iteration 98, loss = 0.01500266
Iteration 99, loss = 0.01496883
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68056943
Iteration 2, loss = 0.59829359
Iteration 3, loss = 0.48621447
Iteration 4, loss = 0.37533927
Iteration 5, loss = 0.28243416
Iteration 6, loss = 0.21222586
Iteration 7, loss = 0.16208259
Iteration 8, loss = 0.12747069
Iteration 9, loss = 0.10345216
Iteration 10, loss = 0.08623252
Iteration 11, loss = 0.07398880
Iteration 12, loss = 0.06410798
Iteration 13, loss = 0.05698455
Iteration 14, loss = 0.05143284
Iteration 15, loss = 0.04713511
Iteration 16, loss = 0.04346856
Iteration 17, loss = 0.04074224
Iteration 18, loss = 0.03821668
Iteration 19, loss = 0.03636876
Iteration 20, loss = 0.03442323
Iteration 21, loss = 0.03295594
Iteration 22, loss = 0.03182950
Iteration 23, loss = 0.03062022
Iteration 24, loss = 0.02968830
Iteration 25, loss = 0.02895033
Iteration 26, loss = 0.02808283
Iteration 27, loss = 0.02748623
Iteration 28, loss = 0.02700634
Iteration 29, loss = 0.02659604
Iteration 30, loss = 0.02611547
Iteration 31, loss = 0.02579082
Iteration 32, loss = 0.02530988
Iteration 33, loss = 0.02490356
Iteration 34, loss = 0.02479822
Iteration 35, loss = 0.02455745
Iteration 36, loss = 0.02431341
Iteration 37, loss = 0.02418436
Iteration 38, loss = 0.02392046
Iteration 39, loss = 0.02337184
Iteration 40, loss = 0.02339835
Iteration 41, loss = 0.02324295
Iteration 42, loss = 0.02317872
Iteration 43, loss = 0.02293022
Iteration 44, loss = 0.02304775
Iteration 45, loss = 0.02280122
Iteration 46, loss = 0.02271508
Iteration 47, loss = 0.02237633
Iteration 48, loss = 0.02254654
Iteration 49, loss = 0.02225968
Iteration 50, loss = 0.02211968
Iteration 51, loss = 0.02223897
Iteration 52, loss = 0.02223914
Iteration 53, loss = 0.02185438
Iteration 54, loss = 0.02173602
Iteration 55, loss = 0.02161542
Iteration 56, loss = 0.02162193
Iteration 57, loss = 0.02169309
Iteration 58, loss = 0.02156073
Iteration 59, loss = 0.02145502
Iteration 60, loss = 0.02123330
Iteration 61, loss = 0.02146347
Iteration 62, loss = 0.02111634
Iteration 63, loss = 0.02156161
Iteration 64, loss = 0.02138318
Iteration 65, loss = 0.02138807
Iteration 66, loss = 0.02127296
Iteration 67, loss = 0.02101454
Iteration 68, loss = 0.02118374
Iteration 69, loss = 0.02103733
Iteration 70, loss = 0.02100859
Iteration 71, loss = 0.02112115
Iteration 72, loss = 0.02101762
Iteration 73, loss = 0.02102894
Iteration 74, loss = 0.02129429
Iteration 75, loss = 0.02112146
Iteration 76, loss = 0.02091006
Iteration 77, loss = 0.02090261
Iteration 78, loss = 0.02095059
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70964370
Iteration 2, loss = 0.68776277
Iteration 3, loss = 0.66610181
Iteration 4, loss = 0.64110834
Iteration 5, loss = 0.61314306
Iteration 6, loss = 0.58137078
Iteration 7, loss = 0.54666560
Iteration 8, loss = 0.50989318
Iteration 9, loss = 0.47243003
Iteration 10, loss = 0.43517391
Iteration 11, loss = 0.39866721
Iteration 12, loss = 0.36355886
Iteration 13, loss = 0.33047712
Iteration 14, loss = 0.29959703
Iteration 15, loss = 0.27127470
Iteration 16, loss = 0.24533515
Iteration 17, loss = 0.22209899
Iteration 18, loss = 0.20103433
Iteration 19, loss = 0.18229228
Iteration 20, loss = 0.16565916
Iteration 21, loss = 0.15081466
Iteration 22, loss = 0.13769630
Iteration 23, loss = 0.12612913
Iteration 24, loss = 0.11584827
Iteration 25, loss = 0.10667658
Iteration 26, loss = 0.09858031
Iteration 27, loss = 0.09144177
Iteration 28, loss = 0.08501681
Iteration 29, loss = 0.07922813
Iteration 30, loss = 0.07416391
Iteration 31, loss = 0.06953779
Iteration 32, loss = 0.06544499
Iteration 33, loss = 0.06164148
Iteration 34, loss = 0.05828917
Iteration 35, loss = 0.05519738
Iteration 36, loss = 0.05242706
Iteration 37, loss = 0.04986579
Iteration 38, loss = 0.04752994
Iteration 39, loss = 0.04539114
Iteration 40, loss = 0.04347009
Iteration 41, loss = 0.04165554
Iteration 42, loss = 0.03996486
Iteration 43, loss = 0.03845386
Iteration 44, loss = 0.03701657
Iteration 45, loss = 0.03570874
Iteration 46, loss = 0.03452752
Iteration 47, loss = 0.03333274
Iteration 48, loss = 0.03233814
Iteration 49, loss = 0.03133310
Iteration 50, loss = 0.03036113
Iteration 51, loss = 0.02949082
Iteration 52, loss = 0.02875573
Iteration 53, loss = 0.02792303
Iteration 54, loss = 0.02718730
Iteration 55, loss = 0.02654684
Iteration 56, loss = 0.02590889
Iteration 57, loss = 0.02529283
Iteration 58, loss = 0.02474361
Iteration 59, loss = 0.02417827
Iteration 60, loss = 0.02366891
Iteration 61, loss = 0.02319601
Iteration 62, loss = 0.02275955
Iteration 63, loss = 0.02237519
Iteration 64, loss = 0.02190863
Iteration 65, loss = 0.02150733
Iteration 66, loss = 0.02116108
Iteration 67, loss = 0.02084270
Iteration 68, loss = 0.02047522
Iteration 69, loss = 0.02015889
Iteration 70, loss = 0.01983491
Iteration 71, loss = 0.01953316
Iteration 72, loss = 0.01926412
Iteration 73, loss = 0.01897388
Iteration 74, loss = 0.01877971
Iteration 75, loss = 0.01849074
Iteration 76, loss = 0.01826866
Iteration 77, loss = 0.01806491
Iteration 78, loss = 0.01783595
Iteration 79, loss = 0.01763242
Iteration 80, loss = 0.01748013
Iteration 81, loss = 0.01721514
Iteration 82, loss = 0.01704722
Iteration 83, loss = 0.01689893
Iteration 84, loss = 0.01670672
Iteration 85, loss = 0.01652558
Iteration 86, loss = 0.01636275
Iteration 87, loss = 0.01620695
Iteration 88, loss = 0.01605166
Iteration 89, loss = 0.01592055
Iteration 90, loss = 0.01578574
Iteration 91, loss = 0.01562632
Iteration 92, loss = 0.01553393
Iteration 93, loss = 0.01544283
Iteration 94, loss = 0.01527800
Iteration 95, loss = 0.01516459
Iteration 96, loss = 0.01510946
Iteration 97, loss = 0.01495585
Iteration 98, loss = 0.01485505
Iteration 99, loss = 0.01480553
Iteration 100, loss = 0.01465005
Iteration 101, loss = 0.01460448
Iteration 102, loss = 0.01448760
Iteration 103, loss = 0.01440448
Iteration 104, loss = 0.01430734
Iteration 105, loss = 0.01422388
Iteration 106, loss = 0.01421539
Iteration 107, loss = 0.01407597
Iteration 108, loss = 0.01397975
Iteration 109, loss = 0.01394535
Iteration 110, loss = 0.01384238
Iteration 111, loss = 0.01375013
Iteration 112, loss = 0.01368771
Iteration 113, loss = 0.01362543
Iteration 114, loss = 0.01361377
Iteration 115, loss = 0.01347034
Iteration 116, loss = 0.01345709
Iteration 117, loss = 0.01337964
Iteration 118, loss = 0.01335649
Iteration 119, loss = 0.01328178
Iteration 120, loss = 0.01325373
Iteration 121, loss = 0.01312672
Iteration 122, loss = 0.01314505
Iteration 123, loss = 0.01306332
Iteration 124, loss = 0.01299616
Iteration 125, loss = 0.01298028
Iteration 126, loss = 0.01295268
Iteration 127, loss = 0.01286288
Iteration 128, loss = 0.01281509
Iteration 129, loss = 0.01276546
Iteration 130, loss = 0.01277506
Iteration 131, loss = 0.01269941
Iteration 132, loss = 0.01261845
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68821901
Iteration 2, loss = 0.65069790
Iteration 3, loss = 0.60590896
Iteration 4, loss = 0.55202555
Iteration 5, loss = 0.48928434
Iteration 6, loss = 0.42611189
Iteration 7, loss = 0.36607018
Iteration 8, loss = 0.31201868
Iteration 9, loss = 0.26462854
Iteration 10, loss = 0.22422298
Iteration 11, loss = 0.19063721
Iteration 12, loss = 0.16277443
Iteration 13, loss = 0.14004529
Iteration 14, loss = 0.12149523
Iteration 15, loss = 0.10634446
Iteration 16, loss = 0.09377579
Iteration 17, loss = 0.08362808
Iteration 18, loss = 0.07504690
Iteration 19, loss = 0.06787701
Iteration 20, loss = 0.06172144
Iteration 21, loss = 0.05665153
Iteration 22, loss = 0.05219192
Iteration 23, loss = 0.04836826
Iteration 24, loss = 0.04503934
Iteration 25, loss = 0.04211124
Iteration 26, loss = 0.03963476
Iteration 27, loss = 0.03739722
Iteration 28, loss = 0.03550423
Iteration 29, loss = 0.03363140
Iteration 30, loss = 0.03204271
Iteration 31, loss = 0.03060433
Iteration 32, loss = 0.02941783
Iteration 33, loss = 0.02823112
Iteration 34, loss = 0.02722316
Iteration 35, loss = 0.02620417
Iteration 36, loss = 0.02547306
Iteration 37, loss = 0.02459962
Iteration 38, loss = 0.02395038
Iteration 39, loss = 0.02331092
Iteration 40, loss = 0.02259024
Iteration 41, loss = 0.02201072
Iteration 42, loss = 0.02141803
Iteration 43, loss = 0.02098322
Iteration 44, loss = 0.02058951
Iteration 45, loss = 0.02019779
Iteration 46, loss = 0.01988179
Iteration 47, loss = 0.01944352
Iteration 48, loss = 0.01900780
Iteration 49, loss = 0.01876619
Iteration 50, loss = 0.01845069
Iteration 51, loss = 0.01813011
Iteration 52, loss = 0.01796869
Iteration 53, loss = 0.01775957
Iteration 54, loss = 0.01732102
Iteration 55, loss = 0.01729249
Iteration 56, loss = 0.01701473
Iteration 57, loss = 0.01683954
Iteration 58, loss = 0.01666822
Iteration 59, loss = 0.01649672
Iteration 60, loss = 0.01632020
Iteration 61, loss = 0.01607336
Iteration 62, loss = 0.01599989
Iteration 63, loss = 0.01591298
Iteration 64, loss = 0.01570931
Iteration 65, loss = 0.01553776
Iteration 66, loss = 0.01543011
Iteration 67, loss = 0.01525706
Iteration 68, loss = 0.01517126
Iteration 69, loss = 0.01509988
Iteration 70, loss = 0.01496605
Iteration 71, loss = 0.01493267
Iteration 72, loss = 0.01483108
Iteration 73, loss = 0.01476904
Iteration 74, loss = 0.01463662
Iteration 75, loss = 0.01466256
Iteration 76, loss = 0.01439045
Iteration 77, loss = 0.01434118
Iteration 78, loss = 0.01434107
Iteration 79, loss = 0.01432079
Iteration 80, loss = 0.01433423
Iteration 81, loss = 0.01421442
Iteration 82, loss = 0.01408520
Iteration 83, loss = 0.01407032
Iteration 84, loss = 0.01391278
Iteration 85, loss = 0.01394772
Iteration 86, loss = 0.01384591
Iteration 87, loss = 0.01391186
Iteration 88, loss = 0.01391389
Iteration 89, loss = 0.01371554
Iteration 90, loss = 0.01371008
Iteration 91, loss = 0.01362172
Iteration 92, loss = 0.01360995
Iteration 93, loss = 0.01356004
Iteration 94, loss = 0.01344884
Iteration 95, loss = 0.01344978
Iteration 96, loss = 0.01353802
Iteration 97, loss = 0.01343709
Iteration 98, loss = 0.01340550
Iteration 99, loss = 0.01336394
Iteration 100, loss = 0.01322951
Iteration 101, loss = 0.01321475
Iteration 102, loss = 0.01315353
Iteration 103, loss = 0.01313754
Iteration 104, loss = 0.01318578
Iteration 105, loss = 0.01301738
Iteration 106, loss = 0.01312765
Iteration 107, loss = 0.01306341
Iteration 108, loss = 0.01301248
Iteration 109, loss = 0.01308056
Iteration 110, loss = 0.01290458
Iteration 111, loss = 0.01298029
Iteration 112, loss = 0.01298318
Iteration 113, loss = 0.01294093
Iteration 114, loss = 0.01301704
Iteration 115, loss = 0.01290174
Iteration 116, loss = 0.01300979
Iteration 117, loss = 0.01293424
Iteration 118, loss = 0.01292103
Iteration 119, loss = 0.01277702
Iteration 120, loss = 0.01279850
Iteration 121, loss = 0.01279833
Iteration 122, loss = 0.01279066
Iteration 123, loss = 0.01275688
Iteration 124, loss = 0.01281737
Iteration 125, loss = 0.01272404
Iteration 126, loss = 0.01268217
Iteration 127, loss = 0.01269329
Iteration 128, loss = 0.01278559
Iteration 129, loss = 0.01264377
Iteration 130, loss = 0.01272188
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67657375
Iteration 2, loss = 0.60889372
Iteration 3, loss = 0.50528231
Iteration 4, loss = 0.39547047
Iteration 5, loss = 0.29954545
Iteration 6, loss = 0.22498318
Iteration 7, loss = 0.17152715
Iteration 8, loss = 0.13407496
Iteration 9, loss = 0.10804767
Iteration 10, loss = 0.08983848
Iteration 11, loss = 0.07608494
Iteration 12, loss = 0.06623839
Iteration 13, loss = 0.05853984
Iteration 14, loss = 0.05241861
Iteration 15, loss = 0.04768707
Iteration 16, loss = 0.04365602
Iteration 17, loss = 0.04046587
Iteration 18, loss = 0.03803944
Iteration 19, loss = 0.03568998
Iteration 20, loss = 0.03387677
Iteration 21, loss = 0.03239756
Iteration 22, loss = 0.03110941
Iteration 23, loss = 0.03001479
Iteration 24, loss = 0.02893944
Iteration 25, loss = 0.02806007
Iteration 26, loss = 0.02728965
Iteration 27, loss = 0.02652805
Iteration 28, loss = 0.02607538
Iteration 29, loss = 0.02536746
Iteration 30, loss = 0.02486146
Iteration 31, loss = 0.02461742
Iteration 32, loss = 0.02401561
Iteration 33, loss = 0.02402449
Iteration 34, loss = 0.02331619
Iteration 35, loss = 0.02326155
Iteration 36, loss = 0.02262492
Iteration 37, loss = 0.02273074
Iteration 38, loss = 0.02253870
Iteration 39, loss = 0.02229367
Iteration 40, loss = 0.02183201
Iteration 41, loss = 0.02176012
Iteration 42, loss = 0.02159030
Iteration 43, loss = 0.02122785
Iteration 44, loss = 0.02116032
Iteration 45, loss = 0.02097378
Iteration 46, loss = 0.02073387
Iteration 47, loss = 0.02083335
Iteration 48, loss = 0.02068047
Iteration 49, loss = 0.02062112
Iteration 50, loss = 0.02048788
Iteration 51, loss = 0.02048771
Iteration 52, loss = 0.02032813
Iteration 53, loss = 0.02016824
Iteration 54, loss = 0.02023806
Iteration 55, loss = 0.02020824
Iteration 56, loss = 0.02031566
Iteration 57, loss = 0.02016031
Iteration 58, loss = 0.02036823
Iteration 59, loss = 0.01999515
Iteration 60, loss = 0.01987512
Iteration 61, loss = 0.01962699
Iteration 62, loss = 0.01971540
Iteration 63, loss = 0.01986182
Iteration 64, loss = 0.01964065
Iteration 65, loss = 0.01985956
Iteration 66, loss = 0.01950025
Iteration 67, loss = 0.01934216
Iteration 68, loss = 0.01965564
Iteration 69, loss = 0.01948611
Iteration 70, loss = 0.01929948
Iteration 71, loss = 0.01946123
Iteration 72, loss = 0.01931319
Iteration 73, loss = 0.01961505
Iteration 74, loss = 0.01920350
Iteration 75, loss = 0.01945904
Iteration 76, loss = 0.01950866
Iteration 77, loss = 0.01932915
Iteration 78, loss = 0.01916988
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69788041
Iteration 2, loss = 0.66516287
Iteration 3, loss = 0.62174017
Iteration 4, loss = 0.56603494
Iteration 5, loss = 0.50323524
Iteration 6, loss = 0.43924044
Iteration 7, loss = 0.37728324
Iteration 8, loss = 0.32081012
Iteration 9, loss = 0.27121960
Iteration 10, loss = 0.22903363
Iteration 11, loss = 0.19368844
Iteration 12, loss = 0.16472981
Iteration 13, loss = 0.14101566
Iteration 14, loss = 0.12175745
Iteration 15, loss = 0.10595083
Iteration 16, loss = 0.09306440
Iteration 17, loss = 0.08243550
Iteration 18, loss = 0.07378049
Iteration 19, loss = 0.06630330
Iteration 20, loss = 0.06014271
Iteration 21, loss = 0.05483652
Iteration 22, loss = 0.05027963
Iteration 23, loss = 0.04651438
Iteration 24, loss = 0.04311861
Iteration 25, loss = 0.04019001
Iteration 26, loss = 0.03764179
Iteration 27, loss = 0.03537093
Iteration 28, loss = 0.03342324
Iteration 29, loss = 0.03158162
Iteration 30, loss = 0.03002638
Iteration 31, loss = 0.02871930
Iteration 32, loss = 0.02734130
Iteration 33, loss = 0.02622494
Iteration 34, loss = 0.02514666
Iteration 35, loss = 0.02425734
Iteration 36, loss = 0.02336578
Iteration 37, loss = 0.02255275
Iteration 38, loss = 0.02180954
Iteration 39, loss = 0.02120510
Iteration 40, loss = 0.02050142
Iteration 41, loss = 0.02001137
Iteration 42, loss = 0.01954303
Iteration 43, loss = 0.01904160
Iteration 44, loss = 0.01867111
Iteration 45, loss = 0.01811939
Iteration 46, loss = 0.01778481
Iteration 47, loss = 0.01744427
Iteration 48, loss = 0.01706685
Iteration 49, loss = 0.01682789
Iteration 50, loss = 0.01644244
Iteration 51, loss = 0.01619636
Iteration 52, loss = 0.01590924
Iteration 53, loss = 0.01575234
Iteration 54, loss = 0.01553139
Iteration 55, loss = 0.01525041
Iteration 56, loss = 0.01511943
Iteration 57, loss = 0.01482168
Iteration 58, loss = 0.01472581
Iteration 59, loss = 0.01451949
Iteration 60, loss = 0.01444288
Iteration 61, loss = 0.01419506
Iteration 62, loss = 0.01397025
Iteration 63, loss = 0.01383654
Iteration 64, loss = 0.01390415
Iteration 65, loss = 0.01368834
Iteration 66, loss = 0.01356087
Iteration 67, loss = 0.01341915
Iteration 68, loss = 0.01348927
Iteration 69, loss = 0.01335173
Iteration 70, loss = 0.01313357
Iteration 71, loss = 0.01303247
Iteration 72, loss = 0.01294055
Iteration 73, loss = 0.01281519
Iteration 74, loss = 0.01279015
Iteration 75, loss = 0.01266227
Iteration 76, loss = 0.01254013
Iteration 77, loss = 0.01249981
Iteration 78, loss = 0.01249604
Iteration 79, loss = 0.01236326
Iteration 80, loss = 0.01245036
Iteration 81, loss = 0.01235115
Iteration 82, loss = 0.01226539
Iteration 83, loss = 0.01217461
Iteration 84, loss = 0.01219923
Iteration 85, loss = 0.01213552
Iteration 86, loss = 0.01199493
Iteration 87, loss = 0.01196658
Iteration 88, loss = 0.01184582
Iteration 89, loss = 0.01179125
Iteration 90, loss = 0.01175768
Iteration 91, loss = 0.01180671
Iteration 92, loss = 0.01177902
Iteration 93, loss = 0.01175129
Iteration 94, loss = 0.01165218
Iteration 95, loss = 0.01170301
Iteration 96, loss = 0.01171149
Iteration 97, loss = 0.01159500
Iteration 98, loss = 0.01161583
Iteration 99, loss = 0.01155243
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71035524
Iteration 2, loss = 0.69538274
Iteration 3, loss = 0.68226676
Iteration 4, loss = 0.66932390
Iteration 5, loss = 0.65571606
Iteration 6, loss = 0.64121435
Iteration 7, loss = 0.62559293
Iteration 8, loss = 0.60879805
Iteration 9, loss = 0.59072284
Iteration 10, loss = 0.57146001
Iteration 11, loss = 0.55102743
Iteration 12, loss = 0.52952058
Iteration 13, loss = 0.50701201
Iteration 14, loss = 0.48387760
Iteration 15, loss = 0.46023585
Iteration 16, loss = 0.43637101
Iteration 17, loss = 0.41236977
Iteration 18, loss = 0.38868969
Iteration 19, loss = 0.36541568
Iteration 20, loss = 0.34288096
Iteration 21, loss = 0.32126325
Iteration 22, loss = 0.30064842
Iteration 23, loss = 0.28118438
Iteration 24, loss = 0.26287624
Iteration 25, loss = 0.24565472
Iteration 26, loss = 0.22975106
Iteration 27, loss = 0.21477973
Iteration 28, loss = 0.20094001
Iteration 29, loss = 0.18810558
Iteration 30, loss = 0.17618245
Iteration 31, loss = 0.16524834
Iteration 32, loss = 0.15509432
Iteration 33, loss = 0.14579790
Iteration 34, loss = 0.13709522
Iteration 35, loss = 0.12911142
Iteration 36, loss = 0.12175267
Iteration 37, loss = 0.11494993
Iteration 38, loss = 0.10865844
Iteration 39, loss = 0.10289162
Iteration 40, loss = 0.09755583
Iteration 41, loss = 0.09260360
Iteration 42, loss = 0.08802950
Iteration 43, loss = 0.08381938
Iteration 44, loss = 0.07982799
Iteration 45, loss = 0.07621835
Iteration 46, loss = 0.07282817
Iteration 47, loss = 0.06963330
Iteration 48, loss = 0.06668067
Iteration 49, loss = 0.06394865
Iteration 50, loss = 0.06136711
Iteration 51, loss = 0.05895598
Iteration 52, loss = 0.05667025
Iteration 53, loss = 0.05455191
Iteration 54, loss = 0.05255755
Iteration 55, loss = 0.05071219
Iteration 56, loss = 0.04893244
Iteration 57, loss = 0.04729509
Iteration 58, loss = 0.04572983
Iteration 59, loss = 0.04425192
Iteration 60, loss = 0.04285255
Iteration 61, loss = 0.04156014
Iteration 62, loss = 0.04027934
Iteration 63, loss = 0.03910625
Iteration 64, loss = 0.03799104
Iteration 65, loss = 0.03691735
Iteration 66, loss = 0.03590924
Iteration 67, loss = 0.03494226
Iteration 68, loss = 0.03404990
Iteration 69, loss = 0.03314330
Iteration 70, loss = 0.03232954
Iteration 71, loss = 0.03153521
Iteration 72, loss = 0.03077477
Iteration 73, loss = 0.03006845
Iteration 74, loss = 0.02934314
Iteration 75, loss = 0.02870843
Iteration 76, loss = 0.02807867
Iteration 77, loss = 0.02751918
Iteration 78, loss = 0.02696771
Iteration 79, loss = 0.02635784
Iteration 80, loss = 0.02585432
Iteration 81, loss = 0.02530655
Iteration 82, loss = 0.02484780
Iteration 83, loss = 0.02435139
Iteration 84, loss = 0.02389911
Iteration 85, loss = 0.02348233
Iteration 86, loss = 0.02305820
Iteration 87, loss = 0.02267230
Iteration 88, loss = 0.02228032
Iteration 89, loss = 0.02193654
Iteration 90, loss = 0.02153911
Iteration 91, loss = 0.02120438
Iteration 92, loss = 0.02087538
Iteration 93, loss = 0.02053171
Iteration 94, loss = 0.02023910
Iteration 95, loss = 0.01991909
Iteration 96, loss = 0.01962132
Iteration 97, loss = 0.01934031
Iteration 98, loss = 0.01906900
Iteration 99, loss = 0.01885371
Iteration 100, loss = 0.01858211
Iteration 101, loss = 0.01832300
Iteration 102, loss = 0.01806065
Iteration 103, loss = 0.01785599
Iteration 104, loss = 0.01762444
Iteration 105, loss = 0.01739257
Iteration 106, loss = 0.01720801
Iteration 107, loss = 0.01697986
Iteration 108, loss = 0.01676020
Iteration 109, loss = 0.01664387
Iteration 110, loss = 0.01639762
Iteration 111, loss = 0.01620867
Iteration 112, loss = 0.01610621
Iteration 113, loss = 0.01589091
Iteration 114, loss = 0.01571129
Iteration 115, loss = 0.01556706
Iteration 116, loss = 0.01541221
Iteration 117, loss = 0.01526052
Iteration 118, loss = 0.01511078
Iteration 119, loss = 0.01495957
Iteration 120, loss = 0.01482415
Iteration 121, loss = 0.01469307
Iteration 122, loss = 0.01454854
Iteration 123, loss = 0.01442834
Iteration 124, loss = 0.01436046
Iteration 125, loss = 0.01416993
Iteration 126, loss = 0.01404410
Iteration 127, loss = 0.01388210
Iteration 128, loss = 0.01376733
Iteration 129, loss = 0.01374149
Iteration 130, loss = 0.01357910
Iteration 131, loss = 0.01346686
Iteration 132, loss = 0.01339852
Iteration 133, loss = 0.01332125
Iteration 134, loss = 0.01319103
Iteration 135, loss = 0.01308455
Iteration 136, loss = 0.01299927
Iteration 137, loss = 0.01291356
Iteration 138, loss = 0.01279556
Iteration 139, loss = 0.01273657
Iteration 140, loss = 0.01263690
Iteration 141, loss = 0.01255186
Iteration 142, loss = 0.01241402
Iteration 143, loss = 0.01238792
Iteration 144, loss = 0.01227246
Iteration 145, loss = 0.01221876
Iteration 146, loss = 0.01218410
Iteration 147, loss = 0.01212617
Iteration 148, loss = 0.01205172
Iteration 149, loss = 0.01198129
Iteration 150, loss = 0.01189966
Iteration 151, loss = 0.01190348
Iteration 152, loss = 0.01179281
Iteration 153, loss = 0.01173804
Iteration 154, loss = 0.01166129
Iteration 155, loss = 0.01162354
Iteration 156, loss = 0.01154672
Iteration 157, loss = 0.01146494
Iteration 158, loss = 0.01139703
Iteration 159, loss = 0.01136846
Iteration 160, loss = 0.01128322
Iteration 161, loss = 0.01121976
Iteration 162, loss = 0.01118502
Iteration 163, loss = 0.01108840
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67595154
Iteration 2, loss = 0.65017665
Iteration 3, loss = 0.62206003
Iteration 4, loss = 0.58676570
Iteration 5, loss = 0.54465810
Iteration 6, loss = 0.49818657
Iteration 7, loss = 0.44982355
Iteration 8, loss = 0.40255228
Iteration 9, loss = 0.35729852
Iteration 10, loss = 0.31547278
Iteration 11, loss = 0.27699937
Iteration 12, loss = 0.24303557
Iteration 13, loss = 0.21287216
Iteration 14, loss = 0.18715186
Iteration 15, loss = 0.16497931
Iteration 16, loss = 0.14596990
Iteration 17, loss = 0.12982964
Iteration 18, loss = 0.11626167
Iteration 19, loss = 0.10454611
Iteration 20, loss = 0.09470391
Iteration 21, loss = 0.08621988
Iteration 22, loss = 0.07887863
Iteration 23, loss = 0.07244299
Iteration 24, loss = 0.06697909
Iteration 25, loss = 0.06218307
Iteration 26, loss = 0.05799558
Iteration 27, loss = 0.05425264
Iteration 28, loss = 0.05081764
Iteration 29, loss = 0.04776466
Iteration 30, loss = 0.04502999
Iteration 31, loss = 0.04257160
Iteration 32, loss = 0.04043190
Iteration 33, loss = 0.03847275
Iteration 34, loss = 0.03670476
Iteration 35, loss = 0.03506998
Iteration 36, loss = 0.03362871
Iteration 37, loss = 0.03230186
Iteration 38, loss = 0.03108484
Iteration 39, loss = 0.02991650
Iteration 40, loss = 0.02889599
Iteration 41, loss = 0.02793893
Iteration 42, loss = 0.02713831
Iteration 43, loss = 0.02630277
Iteration 44, loss = 0.02554139
Iteration 45, loss = 0.02483754
Iteration 46, loss = 0.02423702
Iteration 47, loss = 0.02367625
Iteration 48, loss = 0.02300193
Iteration 49, loss = 0.02248030
Iteration 50, loss = 0.02191118
Iteration 51, loss = 0.02156196
Iteration 52, loss = 0.02114647
Iteration 53, loss = 0.02076968
Iteration 54, loss = 0.02040783
Iteration 55, loss = 0.01995771
Iteration 56, loss = 0.01961309
Iteration 57, loss = 0.01920659
Iteration 58, loss = 0.01899450
Iteration 59, loss = 0.01862390
Iteration 60, loss = 0.01839832
Iteration 61, loss = 0.01815941
Iteration 62, loss = 0.01788656
Iteration 63, loss = 0.01762048
Iteration 64, loss = 0.01736835
Iteration 65, loss = 0.01756247
Iteration 66, loss = 0.01726464
Iteration 67, loss = 0.01701595
Iteration 68, loss = 0.01671534
Iteration 69, loss = 0.01651427
Iteration 70, loss = 0.01635481
Iteration 71, loss = 0.01619863
Iteration 72, loss = 0.01599825
Iteration 73, loss = 0.01583021
Iteration 74, loss = 0.01566347
Iteration 75, loss = 0.01553061
Iteration 76, loss = 0.01542609
Iteration 77, loss = 0.01542648
Iteration 78, loss = 0.01518382
Iteration 79, loss = 0.01511473
Iteration 80, loss = 0.01511842
Iteration 81, loss = 0.01511752
Iteration 82, loss = 0.01528467
Iteration 83, loss = 0.01546098
Iteration 84, loss = 0.01539749
Iteration 85, loss = 0.01531617
Iteration 86, loss = 0.01462389
Iteration 87, loss = 0.01471136
Iteration 88, loss = 0.01462060
Iteration 89, loss = 0.01459579
Iteration 90, loss = 0.01439072
Iteration 91, loss = 0.01424693
Iteration 92, loss = 0.01403869
Iteration 93, loss = 0.01398815
Iteration 94, loss = 0.01372921
Iteration 95, loss = 0.01374828
Iteration 96, loss = 0.01363756
Iteration 97, loss = 0.01360749
Iteration 98, loss = 0.01352410
Iteration 99, loss = 0.01343211
Iteration 100, loss = 0.01339814
Iteration 101, loss = 0.01336276
Iteration 102, loss = 0.01335991
Iteration 103, loss = 0.01333851
Iteration 104, loss = 0.01333968
Iteration 105, loss = 0.01304246
Iteration 106, loss = 0.01325381
Iteration 107, loss = 0.01333452
Iteration 108, loss = 0.01327834
Iteration 109, loss = 0.01313031
Iteration 110, loss = 0.01314709
Iteration 111, loss = 0.01295135
Iteration 112, loss = 0.01291429
Iteration 113, loss = 0.01302004
Iteration 114, loss = 0.01290005
Iteration 115, loss = 0.01280882
Iteration 116, loss = 0.01275757
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71169424
Iteration 2, loss = 0.63464017
Iteration 3, loss = 0.52798057
Iteration 4, loss = 0.40557062
Iteration 5, loss = 0.29866435
Iteration 6, loss = 0.21908924
Iteration 7, loss = 0.16434447
Iteration 8, loss = 0.12784756
Iteration 9, loss = 0.10278645
Iteration 10, loss = 0.08528059
Iteration 11, loss = 0.07280114
Iteration 12, loss = 0.06339870
Iteration 13, loss = 0.05625668
Iteration 14, loss = 0.05093416
Iteration 15, loss = 0.04630079
Iteration 16, loss = 0.04283547
Iteration 17, loss = 0.03988141
Iteration 18, loss = 0.03765310
Iteration 19, loss = 0.03568605
Iteration 20, loss = 0.03375216
Iteration 21, loss = 0.03244790
Iteration 22, loss = 0.03121548
Iteration 23, loss = 0.02995672
Iteration 24, loss = 0.02922514
Iteration 25, loss = 0.02861409
Iteration 26, loss = 0.02798220
Iteration 27, loss = 0.02694116
Iteration 28, loss = 0.02653037
Iteration 29, loss = 0.02576701
Iteration 30, loss = 0.02561199
Iteration 31, loss = 0.02533662
Iteration 32, loss = 0.02475160
Iteration 33, loss = 0.02461596
Iteration 34, loss = 0.02428116
Iteration 35, loss = 0.02400194
Iteration 36, loss = 0.02377420
Iteration 37, loss = 0.02357130
Iteration 38, loss = 0.02318081
Iteration 39, loss = 0.02309958
Iteration 40, loss = 0.02281605
Iteration 41, loss = 0.02270048
Iteration 42, loss = 0.02259850
Iteration 43, loss = 0.02284369
Iteration 44, loss = 0.02225150
Iteration 45, loss = 0.02216963
Iteration 46, loss = 0.02198615
Iteration 47, loss = 0.02200869
Iteration 48, loss = 0.02184100
Iteration 49, loss = 0.02170658
Iteration 50, loss = 0.02158116
Iteration 51, loss = 0.02151527
Iteration 52, loss = 0.02156492
Iteration 53, loss = 0.02137157
Iteration 54, loss = 0.02144709
Iteration 55, loss = 0.02153432
Iteration 56, loss = 0.02136153
Iteration 57, loss = 0.02146082
Iteration 58, loss = 0.02106853
Iteration 59, loss = 0.02107048
Iteration 60, loss = 0.02105116
Iteration 61, loss = 0.02123517
Iteration 62, loss = 0.02100062
Iteration 63, loss = 0.02101154
Iteration 64, loss = 0.02085000
Iteration 65, loss = 0.02062926
Iteration 66, loss = 0.02076053
Iteration 67, loss = 0.02070713
Iteration 68, loss = 0.02070635
Iteration 69, loss = 0.02079317
Iteration 70, loss = 0.02050947
Iteration 71, loss = 0.02063341
Iteration 72, loss = 0.02090882
Iteration 73, loss = 0.02063026
Iteration 74, loss = 0.02117871
Iteration 75, loss = 0.02058994
Iteration 76, loss = 0.02061998
Iteration 77, loss = 0.02051425
Iteration 78, loss = 0.02067761
Iteration 79, loss = 0.02062610
Iteration 80, loss = 0.02069180
Iteration 81, loss = 0.02058559
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67430007
Iteration 2, loss = 0.66026022
Iteration 3, loss = 0.64854234
Iteration 4, loss = 0.63674601
Iteration 5, loss = 0.62408276
Iteration 6, loss = 0.61025650
Iteration 7, loss = 0.59472509
Iteration 8, loss = 0.57767899
Iteration 9, loss = 0.55902163
Iteration 10, loss = 0.53891929
Iteration 11, loss = 0.51746133
Iteration 12, loss = 0.49490220
Iteration 13, loss = 0.47152122
Iteration 14, loss = 0.44777796
Iteration 15, loss = 0.42388218
Iteration 16, loss = 0.40005784
Iteration 17, loss = 0.37689143
Iteration 18, loss = 0.35419687
Iteration 19, loss = 0.33242608
Iteration 20, loss = 0.31156191
Iteration 21, loss = 0.29158846
Iteration 22, loss = 0.27279197
Iteration 23, loss = 0.25499675
Iteration 24, loss = 0.23832683
Iteration 25, loss = 0.22268322
Iteration 26, loss = 0.20802881
Iteration 27, loss = 0.19453804
Iteration 28, loss = 0.18194007
Iteration 29, loss = 0.17013274
Iteration 30, loss = 0.15930621
Iteration 31, loss = 0.14932313
Iteration 32, loss = 0.14004698
Iteration 33, loss = 0.13154512
Iteration 34, loss = 0.12365377
Iteration 35, loss = 0.11638087
Iteration 36, loss = 0.10968156
Iteration 37, loss = 0.10352956
Iteration 38, loss = 0.09779705
Iteration 39, loss = 0.09256473
Iteration 40, loss = 0.08772975
Iteration 41, loss = 0.08320922
Iteration 42, loss = 0.07902634
Iteration 43, loss = 0.07517075
Iteration 44, loss = 0.07160279
Iteration 45, loss = 0.06827709
Iteration 46, loss = 0.06515260
Iteration 47, loss = 0.06223431
Iteration 48, loss = 0.05952121
Iteration 49, loss = 0.05698591
Iteration 50, loss = 0.05463832
Iteration 51, loss = 0.05240819
Iteration 52, loss = 0.05032182
Iteration 53, loss = 0.04840268
Iteration 54, loss = 0.04656155
Iteration 55, loss = 0.04486177
Iteration 56, loss = 0.04325012
Iteration 57, loss = 0.04170598
Iteration 58, loss = 0.04028143
Iteration 59, loss = 0.03891221
Iteration 60, loss = 0.03764776
Iteration 61, loss = 0.03642880
Iteration 62, loss = 0.03527093
Iteration 63, loss = 0.03416965
Iteration 64, loss = 0.03313776
Iteration 65, loss = 0.03214080
Iteration 66, loss = 0.03120692
Iteration 67, loss = 0.03037887
Iteration 68, loss = 0.02948371
Iteration 69, loss = 0.02869214
Iteration 70, loss = 0.02794336
Iteration 71, loss = 0.02719379
Iteration 72, loss = 0.02649521
Iteration 73, loss = 0.02580092
Iteration 74, loss = 0.02517302
Iteration 75, loss = 0.02454497
Iteration 76, loss = 0.02398486
Iteration 77, loss = 0.02341795
Iteration 78, loss = 0.02286099
Iteration 79, loss = 0.02234345
Iteration 80, loss = 0.02185405
Iteration 81, loss = 0.02135861
Iteration 82, loss = 0.02090971
Iteration 83, loss = 0.02046136
Iteration 84, loss = 0.02005725
Iteration 85, loss = 0.01965322
Iteration 86, loss = 0.01926770
Iteration 87, loss = 0.01890226
Iteration 88, loss = 0.01853256
Iteration 89, loss = 0.01818387
Iteration 90, loss = 0.01783834
Iteration 91, loss = 0.01751631
Iteration 92, loss = 0.01721042
Iteration 93, loss = 0.01688840
Iteration 94, loss = 0.01661935
Iteration 95, loss = 0.01632386
Iteration 96, loss = 0.01606213
Iteration 97, loss = 0.01578394
Iteration 98, loss = 0.01553540
Iteration 99, loss = 0.01529478
Iteration 100, loss = 0.01504907
Iteration 101, loss = 0.01483688
Iteration 102, loss = 0.01461427
Iteration 103, loss = 0.01438905
Iteration 104, loss = 0.01418059
Iteration 105, loss = 0.01395755
Iteration 106, loss = 0.01373487
Iteration 107, loss = 0.01351460
Iteration 108, loss = 0.01333805
Iteration 109, loss = 0.01314295
Iteration 110, loss = 0.01299270
Iteration 111, loss = 0.01281639
Iteration 112, loss = 0.01264809
Iteration 113, loss = 0.01248676
Iteration 114, loss = 0.01231620
Iteration 115, loss = 0.01217827
Iteration 116, loss = 0.01201513
Iteration 117, loss = 0.01187828
Iteration 118, loss = 0.01175189
Iteration 119, loss = 0.01161008
Iteration 120, loss = 0.01149055
Iteration 121, loss = 0.01134879
Iteration 122, loss = 0.01123590
Iteration 123, loss = 0.01110229
Iteration 124, loss = 0.01097998
Iteration 125, loss = 0.01086612
Iteration 126, loss = 0.01072949
Iteration 127, loss = 0.01065370
Iteration 128, loss = 0.01052366
Iteration 129, loss = 0.01039328
Iteration 130, loss = 0.01028492
Iteration 131, loss = 0.01018047
Iteration 132, loss = 0.01008271
Iteration 133, loss = 0.00997523
Iteration 134, loss = 0.00988685
Iteration 135, loss = 0.00982110
Iteration 136, loss = 0.00971303
Iteration 137, loss = 0.00963476
Iteration 138, loss = 0.00953121
Iteration 139, loss = 0.00945718
Iteration 140, loss = 0.00937955
Iteration 141, loss = 0.00929761
Iteration 142, loss = 0.00920804
Iteration 143, loss = 0.00913939
Iteration 144, loss = 0.00906101
Iteration 145, loss = 0.00900092
Iteration 146, loss = 0.00892962
Iteration 147, loss = 0.00885041
Iteration 148, loss = 0.00878728
Iteration 149, loss = 0.00871354
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68266008
Iteration 2, loss = 0.60441598
Iteration 3, loss = 0.48774883
Iteration 4, loss = 0.36950243
Iteration 5, loss = 0.27294118
Iteration 6, loss = 0.20253150
Iteration 7, loss = 0.15384259
Iteration 8, loss = 0.12095447
Iteration 9, loss = 0.09839726
Iteration 10, loss = 0.08206348
Iteration 11, loss = 0.07073403
Iteration 12, loss = 0.06182779
Iteration 13, loss = 0.05500614
Iteration 14, loss = 0.04968665
Iteration 15, loss = 0.04569702
Iteration 16, loss = 0.04222046
Iteration 17, loss = 0.03944425
Iteration 18, loss = 0.03695914
Iteration 19, loss = 0.03526105
Iteration 20, loss = 0.03354663
Iteration 21, loss = 0.03235695
Iteration 22, loss = 0.03106489
Iteration 23, loss = 0.03002142
Iteration 24, loss = 0.02922328
Iteration 25, loss = 0.02831246
Iteration 26, loss = 0.02776381
Iteration 27, loss = 0.02694148
Iteration 28, loss = 0.02658447
Iteration 29, loss = 0.02604868
Iteration 30, loss = 0.02560409
Iteration 31, loss = 0.02542721
Iteration 32, loss = 0.02473900
Iteration 33, loss = 0.02462473
Iteration 34, loss = 0.02394832
Iteration 35, loss = 0.02383427
Iteration 36, loss = 0.02359232
Iteration 37, loss = 0.02370059
Iteration 38, loss = 0.02345123
Iteration 39, loss = 0.02371462
Iteration 40, loss = 0.02306237
Iteration 41, loss = 0.02276833
Iteration 42, loss = 0.02276271
Iteration 43, loss = 0.02279420
Iteration 44, loss = 0.02240015
Iteration 45, loss = 0.02256595
Iteration 46, loss = 0.02238345
Iteration 47, loss = 0.02225502
Iteration 48, loss = 0.02212626
Iteration 49, loss = 0.02194272
Iteration 50, loss = 0.02187876
Iteration 51, loss = 0.02188654
Iteration 52, loss = 0.02165645
Iteration 53, loss = 0.02174246
Iteration 54, loss = 0.02142156
Iteration 55, loss = 0.02158116
Iteration 56, loss = 0.02163780
Iteration 57, loss = 0.02134681
Iteration 58, loss = 0.02136430
Iteration 59, loss = 0.02140134
Iteration 60, loss = 0.02114673
Iteration 61, loss = 0.02122536
Iteration 62, loss = 0.02124939
Iteration 63, loss = 0.02087282
Iteration 64, loss = 0.02117417
Iteration 65, loss = 0.02137395
Iteration 66, loss = 0.02093646
Iteration 67, loss = 0.02104894
Iteration 68, loss = 0.02105624
Iteration 69, loss = 0.02093894
Iteration 70, loss = 0.02117745
Iteration 71, loss = 0.02077564
Iteration 72, loss = 0.02072265
Iteration 73, loss = 0.02061701
Iteration 74, loss = 0.02057045
Iteration 75, loss = 0.02077284
Iteration 76, loss = 0.02080574
Iteration 77, loss = 0.02067246
Iteration 78, loss = 0.02060533
Iteration 79, loss = 0.02047107
Iteration 80, loss = 0.02075924
Iteration 81, loss = 0.02052043
Iteration 82, loss = 0.02051699
Iteration 83, loss = 0.02064210
Iteration 84, loss = 0.02030818
Iteration 85, loss = 0.02072918
Iteration 86, loss = 0.02029572
Iteration 87, loss = 0.02028065
Iteration 88, loss = 0.02065704
Iteration 89, loss = 0.02071681
Iteration 90, loss = 0.02051994
Iteration 91, loss = 0.02038581
Iteration 92, loss = 0.02057533
Iteration 93, loss = 0.02059014
Iteration 94, loss = 0.02052012
Iteration 95, loss = 0.02021485
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	86m31.783s
user	365m49.277s
sys	37m18.068s
Elapsed time: 5192 seconds
