{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå™Ô∏è Natural Disaster Tweet Classification Project\n",
    "\n",
    "This notebook explores and models tweet data to classify whether a tweet is about a real disaster or not. We'll clean and preprocess the data in several ways, visualize it, generate multiple datasets, and run various NLP models to compare performance.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Table of Contents\n",
    "1. [Import Data](#import-data)\n",
    "2. [Initial Data Cleaning](#initial-data-cleaning)\n",
    "   - Detect Non-ASCII Characters\n",
    "   - Apply Replacement for Known Corrupted Sequences\n",
    "   - Normalize Remaining Unicode Characters\n",
    "   - Data Preprocessing\n",
    "      - Drop Unnecessary Columns\n",
    "      - Handle Missing Values\n",
    "      - Remove Conflicting Labels\n",
    "      - Remove Duplicates\n",
    "   - Save Cleaned Data Sets\n",
    "3. [Exploratory Data Analysis (EDA)](#eda)\n",
    "   - Dataset Overview\n",
    "      - Dataset Shape\n",
    "      - Column Data Types\n",
    "      - Unique Value Counts per Column\n",
    "      - Basic Text Length Statistics\n",
    "   - Target distribution\n",
    "      - Count and Proportion of Target Class\n",
    "      - Bar Chart and Target Frequencies\n",
    "      - Brief Comment on Class Balance\n",
    "   - Tweet Length Analysis\n",
    "      - Histogram of tweet lengths\n",
    "      - Length distribution per class (0 vs. 1)\n",
    "      - Boxplots\n",
    "      - Sample of longest tweets\n",
    "   - Mention (@) Analysis\n",
    "      - Frequency of @mentions by class\n",
    "      - Top mentions visualized\n",
    "      - Comments on usage trends\n",
    "   - URL Analysis\n",
    "      - Frequency and examples of URLs\n",
    "      - Distribution by target class\n",
    "      - Bar chart of top URLs\n",
    "   - Detect Rows with Emojis or ASCII Emoticons\n",
    "      - Detect and count emojis\n",
    "      - Analyze distribution by class\n",
    "      - Visualize presence of common special characters (e.g., ‚Äò!‚Äô, ‚Äò#‚Äô, ‚Äò:‚Äô, etc.)\n",
    "   - Sample Tweet Inspection\n",
    "      - Display example tweets by class\n",
    "      - Include edge cases (very short, very long, only special characters, etc.)\n",
    "4. [Text Preprocessing Variants](#preprocessing-variants)\n",
    "   - Define Preprocessing Variants\n",
    "     - Basic Cleaning\n",
    "     - Remove Emojis and Mentions\n",
    "     - Lemmatization\n",
    "     - Stemming\n",
    "     - Lemmatization + Stemming\n",
    "     - Lemmatization + Custom Stopwords\n",
    "     - Lowercase Alphabetic Tokens Only\n",
    "     - Lemmatization + Keep Hashtags\n",
    "     - Minimal Processing\n",
    "     - Lemmatization + Stemming + Custom Stopwords\n",
    "   - Save Final Processed Datasets\n",
    "5. [Word Clouds & Comparison](#wordclouds)\n",
    "6. [Baseline Modeling Pipeline](#baseline-modeling)\n",
    "   - Load Preprocessed Datasets\n",
    "   - Define Baseline Classifier Pipelines\n",
    "   - Apply TF-IDF Vectorization\n",
    "   - Train/Test Split (80/20)\n",
    "   - Train Baseline Models:\n",
    "     - Multinomial Naive Bayes\n",
    "     - Logistic Regression\n",
    "     - Passive Aggressive Classifier\n",
    "     - Support Vector Machine\n",
    "     - K-Nearest Neighbors\n",
    "     - Neural Network (MLP)\n",
    "   - Evaluate Models (Cross-Validation + Test Set)\n",
    "   - Save Performance Metrics\n",
    "7. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "   - Define GridSearchCV Search Spaces\n",
    "   - Run Hypertuning on Selected Models\n",
    "   - Evaluate Best-Tuned Models\n",
    "   - Save Best Model Summaries\n",
    "   - Save Tuned Performance Metrics\n",
    "\n",
    "8. [Performance Aggregation and Visualization](#performance-comparison)\n",
    "   - Merge All Result Files\n",
    "   - Visualize Best F1 Macro Scores\n",
    "   - Compare Model Performances Across Variants\n",
    "   - Analyze Impact of Preprocessing Strategies\n",
    "\n",
    "9. [Conclusions & Next Steps](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. üì• Import Data <a name=\"import-data\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Warning Handling\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# üì¶ Core Dependencies\n",
    "import chardet\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "from collections import Counter\n",
    "from unidecode import unidecode\n",
    "\n",
    "# üìä Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm  # for colormap\n",
    "from matplotlib.colors import to_hex\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "# For inline plotting in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# üß† Natural Language Processing (NLTK)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# ‚öôÔ∏è Machine Learning - Models & Evaluation\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, \n",
    "    f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, cross_val_score, cross_validate, learning_curve, train_test_split, GridSearchCV\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# üî¢ Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "# Spacy\n",
    "# !python -m spacy download en_core_web_md\n",
    "# nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load and Inspect Data\n",
    "\n",
    "# Detect encoding of the training CSV file to avoid character corruption\n",
    "with open(\"data/train.csv\", \"rb\") as f:\n",
    "    raw_data = f.read(100_000)  # Read the first 100KB\n",
    "    result = chardet.detect(raw_data)\n",
    "    detected_encoding = result['encoding']\n",
    "\n",
    "# Fallback in case encoding couldn't be detected\n",
    "if detected_encoding is None:\n",
    "    detected_encoding = 'utf-8'\n",
    "\n",
    "print(f\"Detected Encoding: {detected_encoding}\")\n",
    "\n",
    "# Load training and test datasets using the detected encoding\n",
    "train_data = pd.read_csv(\"data/train.csv\", encoding=detected_encoding)\n",
    "test_data = pd.read_csv(\"data/test.csv\", encoding=detected_encoding)\n",
    "\n",
    "# Reset index to ensure consistency\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# üîç Preview the first few rows of the training data\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. üßº Initial Data Cleaning <a name=\"initial-data-cleaning\"></a>\n",
    "- Fix known corrupted sequences\n",
    "- Remove duplicates\n",
    "- Fill or drop null values\n",
    "- Normalize text (e.g., remove non-ASCII)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 üîç Detect Non-ASCII Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrupted_char_counts(series):\n",
    "    \"\"\"\n",
    "    Count non-ASCII characters in a Pandas Series of text.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): Column of text to inspect.\n",
    "        \n",
    "    Returns:\n",
    "        Counter: Dictionary of non-ASCII characters and their frequencies.\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "    for text in series:\n",
    "        if isinstance(text, str):\n",
    "            for char in re.findall(r\"[^\\x00-\\x7F]\", text):\n",
    "                counts[char] += 1\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contains_non_ascii(text):\n",
    "    \"\"\"\n",
    "    Check if a text contains any non-ASCII characters.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if non-ASCII characters found, False otherwise\n",
    "    \"\"\"\n",
    "    return bool(re.search(r'[^\\x00-\\x7F]', str(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run corrupted character detection\n",
    "corrupted_counts = get_corrupted_char_counts(train_data['text'])\n",
    "print(\"Top corrupted characters:\", corrupted_counts.most_common(10))\n",
    "\n",
    "# Identify rows containing non-ASCII characters in any object column\n",
    "df_corrupted = train_data[train_data.select_dtypes(include='object').applymap(contains_non_ascii).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrupted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 üßπ Apply Replacement for Known Corrupted Sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace known misencoded sequences in specified text columns\n",
    "corrections = {\n",
    "    \"¬â√õ√í\": \"-\", \"\\x89√õ√ì\": \"-\", \"@\\x89√õ_\": \"@_\", \"\\x89√õ_\": \"_\",\n",
    "    \"¬â√£¬¢\": \"TM\", \"\\x89√õ√è\": '\"', \"\\x89√õ\\x9d\": '\"', \"\\x89√õ¬™\": \"'\",\n",
    "    \"√•√ä\": \" \", \"\\x89√õ√∑\": \"'\", \"√•¬£\": \"¬£\", \"# \": \"#\", \"√•¬§}\": \"\",\n",
    "    \"√•_?\": \"\", \"√•√á\": \"...\", \"√•√à\": \".\", \"√å√ë1\": \"#1\", \"√å√ë\": \"x\",\n",
    "    \"√å√º\": \"\", \"√å¬¥\": \"'\", \"\\n\\n\": \" \", \"√å¬©\": \"e\", \"√å¬º\": \"u\",\n",
    "    \"\\x89√õ¬¢\": '\"', \"√•¬°\": \"!\", \"&amp;\": \" \", \"√•¬´\": \"'\",\n",
    "    \"\\x89√¢√Ç\": \"\", \"√•¬®\": \"\", \"√•¬¨\": \"\", \"√å¬§\": \"c\", \"√å_n\": \"an\",\n",
    "    \"√å_\": \"i\", \"N√•¬º\": \"number\", \"√•_\": \"\", \"√©\": \"e\", \"√≥n\": \"an\",\n",
    "    \"√ß\": \"c\", \"√≠\": \"i\", \"√å¬¢\": \"\", \"\\n\": \"\", \"√•¬©\": \" @\", \"¬∫\": \"o\",\n",
    "}\n",
    "\n",
    "def apply_text_corrections(df, columns, corrections):\n",
    "    for bad, good in corrections.items():\n",
    "        for col in columns:\n",
    "            df[col] = df[col].str.replace(bad, good, regex=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = apply_text_corrections(train_data, ['text', 'location'], corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 üî† Normalize Remaining Unicode Characters (e.g., Accents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column_text(df, columns):\n",
    "    \"\"\"\n",
    "    Apply unidecode normalization to specific columns of a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "        columns (list): List of text columns to normalize.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with normalized text columns.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda x: unidecode(str(x)) if isinstance(x, str) else x)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization\n",
    "train_data = normalize_column_text(train_data, ['text', 'location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 üßπ Data Preprocessing\n",
    "- üì§ Drop Unnecessary Columns  \n",
    "    - Remove columns that are not useful for modeling (e.g., `location`, `id`).\n",
    "- üï≥Ô∏è Handle Missing Values  \n",
    "    - Detect and remove rows containing `NaN` values after dropping unused columns.\n",
    "- ‚ö†Ô∏è Remove Conflicting Labels  \n",
    "    - Exclude tweets that appear with the same text but different target labels to reduce noise.\n",
    "- üìã Remove Duplicates  \n",
    "    - Drop duplicate rows based on meaningful features like `text`, `target`, and optionally `keyword`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df: pd.DataFrame, keyword_strategy: str = \"prepend\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and deduplicates the tweet dataset with optional keyword handling.\n",
    "\n",
    "    Steps:\n",
    "    - Drop 'id' and 'location' columns\n",
    "    - Drop rows with NaN values after column drops\n",
    "    - Remove rows with same text but conflicting target labels\n",
    "    - Drop duplicates based on relevant columns (before modifying 'text')\n",
    "    - Handle 'keyword' based on the strategy:\n",
    "        - 'prepend': Prepend keyword to the text and drop the column\n",
    "        - 'keep': Keep keyword column\n",
    "        - 'drop': Drop keyword column\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Original DataFrame with columns: id, keyword, location, text, target.\n",
    "\n",
    "    keyword_strategy : str\n",
    "        How to handle the 'keyword' column. One of:\n",
    "            - 'prepend': Prepend keyword to text and drop keyword column\n",
    "            - 'keep': Keep the keyword column as-is\n",
    "            - 'drop': Drop the keyword column completely\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A cleaned and deduplicated DataFrame ready for modeling.\n",
    "    \"\"\"\n",
    "    assert keyword_strategy in ['prepend', 'keep', 'drop'], \"Invalid keyword_strategy. Choose from 'prepend', 'keep', or 'drop'.\"\n",
    "\n",
    "    # Drop 'id' and 'location' columns\n",
    "    df_cleaned = df.drop(columns=['id', 'location'], errors='ignore').copy()\n",
    "\n",
    "    # Drop any rows with NaN values\n",
    "    before_nan = len(df_cleaned)\n",
    "    df_cleaned = df_cleaned.dropna().reset_index(drop=True)\n",
    "    after_nan = len(df_cleaned)\n",
    "    print(f\"Dropped {before_nan - after_nan} rows with NaN values after dropping unused columns.\")\n",
    "\n",
    "    # Remove rows with same text and different target values (conflicts)\n",
    "    conflict_texts = (\n",
    "        df_cleaned.groupby('text')['target']\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "        .query('target > 1')['text']\n",
    "    )\n",
    "\n",
    "    before_conflict = len(df_cleaned)\n",
    "    df_cleaned = df_cleaned[~df_cleaned['text'].isin(conflict_texts)].reset_index(drop=True)\n",
    "    after_conflict = len(df_cleaned)\n",
    "    print(f\"Removed {before_conflict - after_conflict} rows with conflicting labels on identical text.\")\n",
    "\n",
    "    # Deduplicate BEFORE keyword modification\n",
    "    dedup_cols = ['text', 'target']\n",
    "    if keyword_strategy == 'keep' and 'keyword' in df_cleaned.columns:\n",
    "        dedup_cols.insert(1, 'keyword')\n",
    "\n",
    "    before_dupes = len(df_cleaned)\n",
    "    df_deduped = df_cleaned.drop_duplicates(subset=dedup_cols).reset_index(drop=True)\n",
    "    after_dupes = len(df_deduped)\n",
    "    print(f\"Dropped {before_dupes - after_dupes} duplicates based on: {dedup_cols}\")\n",
    "\n",
    "    # Handle keyword column after deduplication\n",
    "    if keyword_strategy == 'prepend' and 'keyword' in df_deduped.columns:\n",
    "        df_deduped['text'] = df_deduped.apply(\n",
    "            lambda row: f\"{row['keyword']} {row['text']}\" if pd.notnull(row['keyword']) else row['text'],\n",
    "            axis=1\n",
    "        )\n",
    "        df_deduped.drop(columns=['keyword'], inplace=True)\n",
    "\n",
    "    elif keyword_strategy == 'drop' and 'keyword' in df_deduped.columns:\n",
    "        df_deduped.drop(columns=['keyword'], inplace=True)\n",
    "\n",
    "    return df_deduped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepend keyword to text, then drop keyword column\n",
    "df_keyword_prepended = prepare_dataset(train_data, keyword_strategy=\"prepend\")\n",
    "\n",
    "# Drop keyword completely\n",
    "df_keyword_dropped = prepare_dataset(train_data, keyword_strategy=\"drop\")\n",
    "\n",
    "# Keep keyword as a separate column\n",
    "df_keyword_kept = prepare_dataset(train_data, keyword_strategy=\"keep\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 üíæ Save Cleaned Datasets for Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file paths\n",
    "output_dir = \"processed_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Save datasets to CSV\n",
    "df_keyword_prepended.to_csv(f\"{output_dir}/train_keyword_prepended.csv\", index=False)\n",
    "df_keyword_dropped.to_csv(f\"{output_dir}/train_keyword_dropped.csv\", index=False)\n",
    "df_keyword_kept.to_csv(f\"{output_dir}/train_keyword_kept.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ All cleaned datasets saved to 'processed_data/' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3. üìä Exploratory Data Analysis (EDA) <a name=\"eda\"></a>\n",
    "- Dataset Overview\n",
    "- Distribution of target labels (0/1)\n",
    "- Tweet length histogram\n",
    "- Hashtag Analysis\n",
    "- @mention Analysis\n",
    "- URL Analysis\n",
    "- Emoji presence\n",
    "- Sample Tweet Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 üìà Dataset Overview\n",
    "- Dataset shape (rows, columns)\n",
    "- Column data types\n",
    "- Missing value summary\n",
    "- Unique value count per column\n",
    "- Basic text length statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_overview(df: pd.DataFrame, text_column: str = \"text\") -> None:\n",
    "    \"\"\"\n",
    "    Displays key statistics and information about the structure of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The dataset to summarize.\n",
    "    text_column : str, default=\"text\"\n",
    "        Name of the column containing tweet text for length analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üîç Dataset Overview\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Shape\n",
    "    print(f\"Shape (rows, columns): {df.shape}\")\n",
    "    \n",
    "    # Column data types\n",
    "    print(\"\\nüì¶ Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Null values\n",
    "    print(\"\\nüï≥Ô∏è Missing Values per Column:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Unique values\n",
    "    print(\"\\nüî¢ Unique Values per Column:\")\n",
    "    print(df.nunique())\n",
    "    \n",
    "    # Text length stats\n",
    "    if text_column in df.columns:\n",
    "        df['text_length'] = df[text_column].apply(len)\n",
    "        print(\"\\n‚úèÔ∏è Text Length Statistics:\")\n",
    "        print(df['text_length'].describe())\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Text column '{text_column}' not found in DataFrame.\")\n",
    "\n",
    "    print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_overview(df_keyword_prepended)\n",
    "dataset_overview(df_keyword_dropped)\n",
    "dataset_overview(df_keyword_kept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 üéØ Target Distribution\n",
    "- Count and proportion of each target class (0 = Non-Disaster, 1 = Disaster)\n",
    "- Bar chart of target frequencies\n",
    "- Brief comment on class balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution(df: pd.DataFrame, title_suffix: str = \"\"):\n",
    "    \"\"\"\n",
    "    Displays the count, proportion, and bar plot of target classes.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The dataset with a 'target' column.\n",
    "\n",
    "    title_suffix : str, optional\n",
    "        Optional label to append to the plot title (e.g., ' (with keyword)')\n",
    "    \"\"\"\n",
    "    # Count and percentage\n",
    "    counts = df['target'].value_counts().sort_index()\n",
    "    percentages = df['target'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "    # Display stats\n",
    "    print(\"Target Class Counts:\")\n",
    "    print(counts)\n",
    "    print(\"\\nTarget Class Proportions:\")\n",
    "    print(percentages.round(2).astype(str) + \" %\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.countplot(x='target', data=df, palette=sns.color_palette(\"viridis\", 2))\n",
    "    plt.title(f'Target Class Distribution{title_suffix}')\n",
    "    plt.xlabel('Target')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1], ['Non-Disaster (0)', 'Disaster (1)'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target class distribution for each version of the dataset\n",
    "\n",
    "plot_target_distribution(df_keyword_prepended, title_suffix=\" (Keyword Prepended)\")\n",
    "plot_target_distribution(df_keyword_dropped, title_suffix=\" (Keyword Dropped)\")\n",
    "plot_target_distribution(df_keyword_kept, title_suffix=\" (Keyword Kept)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ‚öñÔ∏è **Class Balance Consideration**  \n",
    "> A brief review of best practices suggests that datasets with class ratios between **50/50 and 40/60** are generally considered **balanced**.  \n",
    ">  \n",
    "> Since our target distribution falls within this range, we will **proceed without applying any class balancing techniques**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3 üìù Tweet Length Analysis\n",
    "- Histogram of tweet lengths\n",
    "- Length distribution per class (0 vs. 1)\n",
    "- Boxplots\n",
    "- Sample of longest tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tweet_length_distribution(df: pd.DataFrame, dataset_name: str = \"Dataset\"):\n",
    "    \"\"\"\n",
    "    Plots the distribution of tweet lengths overall and by target class.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset containing at least 'text' and 'target' columns.\n",
    "    dataset_name : str\n",
    "        Name of the dataset (used in titles).\n",
    "    \"\"\"\n",
    "    # Calculate tweet lengths if not already present\n",
    "    if 'length' not in df.columns:\n",
    "        df['length'] = df['text'].apply(len)\n",
    "\n",
    "    # Overall Length Distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df['length'], bins=50, kde=False, color='mediumpurple')\n",
    "    plt.title(f'{dataset_name}: Distribution of Tweet Lengths')\n",
    "    plt.xlabel('Tweet Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Length Distribution by Target\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x='length',\n",
    "        hue='target',\n",
    "        bins=50,\n",
    "        palette='viridis',\n",
    "        multiple='stack'\n",
    "    )\n",
    "    plt.title(f'{dataset_name}: Tweet Length by Class (0 = Non-Disaster, 1 = Disaster)')\n",
    "    plt.xlabel('Tweet Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(title='Target', labels=['Non-Disaster (0)', 'Disaster (1)'])  # Explicit legend\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Boxplot by Target\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x='target', y='length', data=df, palette='Set2')\n",
    "    plt.title(f'{dataset_name}: Boxplot of Tweet Lengths by Class')\n",
    "    plt.xlabel('Target')\n",
    "    plt.ylabel('Tweet Length')\n",
    "    plt.xticks([0, 1], ['Non-Disaster', 'Disaster'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tweet_length_distribution(df_keyword_prepended, \"Keyword Prepended\")\n",
    "plot_tweet_length_distribution(df_keyword_dropped, \"Keyword Dropped\")\n",
    "plot_tweet_length_distribution(df_keyword_kept, \"Keyword Kept\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 üîñ Hashtag Analysis\n",
    "- Frequency distribution of hashtags per class\n",
    "- Bar charts for top hashtags in each target group\n",
    "- Shared hashtags across classes\n",
    "- Heatmap comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hashtags(df: pd.DataFrame, text_col: str = \"text\", target_col: str = \"target\", top_n: int = 15):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes hashtag frequency across target classes.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing tweet text and binary target labels.\n",
    "\n",
    "    text_col : str, default=\"text\"\n",
    "        Name of the column containing the tweet text.\n",
    "\n",
    "    target_col : str, default=\"target\"\n",
    "        Name of the column containing binary labels (0 or 1).\n",
    "\n",
    "    top_n : int, default=15\n",
    "        Number of top hashtags to visualize for each class.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[pd.Series, pd.Series, pd.DataFrame]\n",
    "        - Hashtag counts for target = 1\n",
    "        - Hashtag counts for target = 0\n",
    "        - Common hashtags DataFrame with side-by-side counts\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import re\n",
    "\n",
    "    counter_1 = Counter()\n",
    "    counter_0 = Counter()\n",
    "    hashtag_pattern = re.compile(r\"#\\w+\")\n",
    "\n",
    "    for text, target in zip(df[text_col], df[target_col]):\n",
    "        hashtags = hashtag_pattern.findall(str(text).lower())\n",
    "        if target == 1:\n",
    "            counter_1.update(hashtags)\n",
    "        elif target == 0:\n",
    "            counter_0.update(hashtags)\n",
    "\n",
    "    hashtags_1 = pd.Series(counter_1).sort_values(ascending=False)\n",
    "    hashtags_0 = pd.Series(counter_0).sort_values(ascending=False)\n",
    "\n",
    "    # Plotting\n",
    "    viridis = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "    color1 = viridis(0.3)\n",
    "    color2 = viridis(0.8)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    hashtags_1.head(top_n).plot(kind=\"bar\", color=color2)\n",
    "    plt.title(\"Top Hashtags (Disaster Tweets)\")\n",
    "    plt.xlabel(\"Hashtag\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    hashtags_0.head(top_n).plot(kind=\"bar\", color=color1)\n",
    "    plt.title(\"Top Hashtags (Non-Disaster Tweets)\")\n",
    "    plt.xlabel(\"Hashtag\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Shared hashtags\n",
    "    common_tags = hashtags_1.index.intersection(hashtags_0.index)\n",
    "    df_common = pd.DataFrame({\n",
    "        \"Disaster\": hashtags_1.loc[common_tags],\n",
    "        \"Non-Disaster\": hashtags_0.loc[common_tags]\n",
    "    })\n",
    "\n",
    "    df_common[\"total\"] = df_common.sum(axis=1)\n",
    "    df_common = df_common.sort_values(\"total\", ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df_common[[\"Disaster\", \"Non-Disaster\"]], annot=True, cmap=\"viridis\", fmt=\".0f\")\n",
    "    plt.title(\"Top Shared Hashtags: Heatmap Comparison\")\n",
    "    plt.ylabel(\"Hashtag\")\n",
    "    plt.xlabel(\"Target Class\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ‚úÖ Print total hashtag counts\n",
    "    print(f\"Total hashtags in target=1 (Disaster): {sum(counter_1.values())}\")\n",
    "    print(f\"Total hashtags in target=0 (Non-Disaster): {sum(counter_0.values())}\")\n",
    "\n",
    "    return hashtags_1, hashtags_0, df_common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hashtag analysis on all dataset variations\n",
    "# hashtags_prepended_1, hashtags_prepended_0, common_hashtags_prepended = analyze_hashtags(df_keyword_prepended)\n",
    "# hashtags_dropped_1, hashtags_dropped_0, common_hashtags_dropped = analyze_hashtags(df_keyword_dropped)\n",
    "hashtags_kept_1, hashtags_kept_0, common_hashtags_kept = analyze_hashtags(df_keyword_kept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 üë• Mention (@) Analysis\n",
    "- Frequency of @mentions by class\n",
    "- Top mentions visualized\n",
    "- Comments on usage trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mentions(df, dataset_label=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Analyze and visualize @mention usage in tweets, grouped by target class.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing 'text' and 'target' columns.\n",
    "    dataset_label : str\n",
    "        Label used in plot titles to distinguish datasets.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays plots and summary stats)\n",
    "    \"\"\"\n",
    "    counter_1 = Counter()\n",
    "    counter_0 = Counter()\n",
    "\n",
    "    # Compile pattern for mentions\n",
    "    mention_pattern = re.compile(r\"@\\w+\")\n",
    "\n",
    "    for text, target in zip(df[\"text\"], df[\"target\"]):\n",
    "        mentions = mention_pattern.findall(str(text).lower())\n",
    "        for mention in mentions:\n",
    "            if target == 1:\n",
    "                counter_1[mention] += 1\n",
    "            else:\n",
    "                counter_0[mention] += 1\n",
    "\n",
    "    mentions_target1 = pd.Series(counter_1)\n",
    "    mentions_target0 = pd.Series(counter_0)\n",
    "\n",
    "    top_mentions_target1 = mentions_target1.sort_values(ascending=False).head(15)\n",
    "    top_mentions_target0 = mentions_target0.sort_values(ascending=False).head(15)\n",
    "\n",
    "    max_count = max(top_mentions_target1.max(), top_mentions_target0.max())\n",
    "    viridis = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "    color1 = viridis(0.3)\n",
    "    color2 = viridis(0.7)\n",
    "\n",
    "    # Plot for target = 1\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_mentions_target1.plot(kind=\"bar\", color=color1)\n",
    "    plt.title(f\"Top 15 Mentions in Disaster Tweets (Target=1) - {dataset_label}\")\n",
    "    plt.xlabel(\"Mention\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.ylim(0, max_count)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot for target = 0\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_mentions_target0.plot(kind=\"bar\", color=color2)\n",
    "    plt.title(f\"Top 15 Mentions in Non-Disaster Tweets (Target=0) - {dataset_label}\")\n",
    "    plt.xlabel(\"Mention\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.ylim(0, max_count)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ‚úÖ Print summary stats\n",
    "    total_mentions_1 = sum(counter_1.values())\n",
    "    total_mentions_0 = sum(counter_0.values())\n",
    "    print(f\"Total @mentions in target=1 (Disaster): {total_mentions_1}\")\n",
    "    print(f\"Total @mentions in target=0 (Non-Disaster): {total_mentions_0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üîç Analyze mentions in the dataset with keyword prepended to text\n",
    "# analyze_mentions(df_keyword_prepended, dataset_label=\"Keyword Prepended\")\n",
    "\n",
    "# # üîç Analyze mentions in the dataset with keyword dropped\n",
    "# analyze_mentions(df_keyword_dropped, dataset_label=\"Keyword Dropped\")\n",
    "\n",
    "# üîç Analyze mentions in the dataset with keyword kept\n",
    "analyze_mentions(df_keyword_kept, dataset_label=\"Keyword Kept\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 üîó URL Analysis\n",
    "- Frequency and examples of URLs\n",
    "- Distribution by target class\n",
    "- Bar chart of top URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_urls(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Analyzes URL usage in tweets, visualizing frequency by target class and top URLs.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with at least 'text' and 'target' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize counters for each target\n",
    "    url_counter_1 = Counter()\n",
    "    url_counter_0 = Counter()\n",
    "\n",
    "    # URL regex\n",
    "    url_pattern = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
    "\n",
    "    # Collect URLs by target\n",
    "    for text, target in zip(df['text'], df['target']):\n",
    "        urls = url_pattern.findall(str(text).lower())\n",
    "        for url in urls:\n",
    "            if target == 1:\n",
    "                url_counter_1[url] += 1\n",
    "            elif target == 0:\n",
    "                url_counter_0[url] += 1\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    df_urls_1 = pd.Series(url_counter_1).sort_values(ascending=False).head(15)\n",
    "    df_urls_0 = pd.Series(url_counter_0).sort_values(ascending=False).head(15)\n",
    "\n",
    "    # Get unified y-axis limit\n",
    "    max_count = max(df_urls_1.max(), df_urls_0.max())\n",
    "\n",
    "    # Define shared color palette\n",
    "    viridis = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "    color1 = viridis(0.3)\n",
    "    color2 = viridis(0.8)\n",
    "\n",
    "    # Plot target = 1\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    df_urls_1.plot(kind='bar', color=color1)\n",
    "    plt.title('Top 15 URLs in Disaster-Related Tweets (target = 1)')\n",
    "    plt.xlabel('URL')\n",
    "    plt.ylabel('Count')\n",
    "    plt.ylim(0, max_count)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot target = 0\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    df_urls_0.plot(kind='bar', color=color2)\n",
    "    plt.title('Top 15 URLs in Non-Disaster Tweets (target = 0)')\n",
    "    plt.xlabel('URL')\n",
    "    plt.ylabel('Count')\n",
    "    plt.ylim(0, max_count)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary stats\n",
    "    total_urls_1 = sum(url_counter_1.values())\n",
    "    total_urls_0 = sum(url_counter_0.values())\n",
    "    print(f\"Total URLs in target=1 (Disaster): {total_urls_1}\")\n",
    "    print(f\"Total URLs in target=0 (Non-Disaster): {total_urls_0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üîó URL Analysis for each cleaned dataset\n",
    "# analyze_urls(df_keyword_prepended)\n",
    "# analyze_urls(df_keyword_dropped)\n",
    "analyze_urls(df_keyword_kept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 üòÖ Detect Rows with Emojis or ASCII Emoticons\n",
    "- Detect and count emojis\n",
    "- Analyze distribution by class\n",
    "- Visualize presence of common special characters (e.g., ‚Äò!‚Äô, ‚Äò#‚Äô, ‚Äò:‚Äô, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_emojis_and_emoticons(df, text_col=\"text\", target_col=\"target\", dataset_label=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Detects emoji and emoticon usage in text and visualizes distribution by target class.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the tweet text and binary labels.\n",
    "\n",
    "    text_col : str, default=\"text\"\n",
    "        Name of the column containing tweet text.\n",
    "\n",
    "    target_col : str, default=\"target\"\n",
    "        Name of the column containing target class.\n",
    "\n",
    "    dataset_label : str\n",
    "        Label for titles/plots to distinguish datasets.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from collections import Counter\n",
    "\n",
    "    emoji_pattern = re.compile(\"[\" \n",
    "        \"\\U0001F600-\\U0001F64F\"\n",
    "        \"\\U0001F300-\\U0001F5FF\"\n",
    "        \"\\U0001F680-\\U0001F6FF\"\n",
    "        \"\\U0001F700-\\U0001F77F\"\n",
    "        \"\\U0001F780-\\U0001F7FF\"\n",
    "        \"\\U0001F800-\\U0001F8FF\"\n",
    "        \"\\U0001F900-\\U0001F9FF\"\n",
    "        \"\\U0001FA00-\\U0001FA6F\"\n",
    "        \"\\U0001FA70-\\U0001FAFF\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    emoticon_pattern = re.compile(r\"[:;=xX8][\\-^oO']?[\\)D\\(\\]/\\\\OpP]\")\n",
    "\n",
    "    def contains_special(text):\n",
    "        text = str(text)\n",
    "        return bool(emoji_pattern.search(text)) or bool(emoticon_pattern.search(text))\n",
    "\n",
    "    df[\"has_special\"] = df[text_col].apply(contains_special)\n",
    "\n",
    "    # Count by target\n",
    "    counts = df.groupby(target_col)[\"has_special\"].value_counts().unstack().fillna(0)\n",
    "    counts.index = counts.index.map({0: \"Non-Disaster\", 1: \"Disaster\"})\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    counts.plot(kind=\"bar\", stacked=True, colormap=\"viridis\")\n",
    "    plt.title(f\"Emoji/Emoticon Usage by Target - {dataset_label}\")\n",
    "    plt.ylabel(\"Tweet Count\")\n",
    "    plt.xlabel(\"Target\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend([\"No Emoji/Emoticon\", \"Has Emoji/Emoticon\"], title=\"Contains\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print counts\n",
    "    print(f\"Total tweets with emoji/emoticon: {df['has_special'].sum()} out of {len(df)} ({df['has_special'].mean()*100:.2f}%)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze_emojis_and_emoticons(df_keyword_prepended, dataset_label=\"Keyword Prepended\")\n",
    "# analyze_emojis_and_emoticons(df_keyword_dropped, dataset_label=\"Keyword Dropped\")\n",
    "analyze_emojis_and_emoticons(df_keyword_kept, dataset_label=\"Keyword Kept\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 üßµ Sample Tweet Inspection\n",
    "- Display example tweets by class\n",
    "- Include edge cases (very short, very long, only special characters, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_sample_tweets(df: pd.DataFrame, dataset_label: str = \"Dataset\", n_samples: int = 3):\n",
    "    \"\"\"\n",
    "    Display random and edge case tweet examples by target class.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame containing 'text' and 'target' columns.\n",
    "\n",
    "    dataset_label : str\n",
    "        A label for the dataset to include in printed headers.\n",
    "\n",
    "    n_samples : int, default=3\n",
    "        Number of random tweets to sample from each class.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìÑ Sample Tweets from {dataset_label}:\\n\")\n",
    "\n",
    "    for label, name in [(0, \"Non-Disaster\"), (1, \"Disaster\")]:\n",
    "        print(f\"--- {name} Tweets ---\\n\")\n",
    "        samples = df[df[\"target\"] == label][\"text\"].sample(n=n_samples, random_state=42)\n",
    "        for idx, tweet in enumerate(samples, 1):\n",
    "            print(f\"{idx}. {tweet}\\n\")\n",
    "\n",
    "    print(\"\\nüîé Edge Case Examples:\\n\")\n",
    "\n",
    "    # Shortest tweet\n",
    "    df[\"text_length\"] = df[\"text\"].apply(len)\n",
    "    print(f\"üßµ Shortest Tweet ({df['text_length'].min()} chars):\\n{df.loc[df['text_length'].idxmin(), 'text']}\\n\")\n",
    "\n",
    "    # Longest tweet\n",
    "    print(f\"üßµ Longest Tweet ({df['text_length'].max()} chars):\\n{df.loc[df['text_length'].idxmax(), 'text']}\\n\")\n",
    "\n",
    "    # Only special characters\n",
    "    special_only = df[df[\"text\"].str.match(r\"^[^\\w\\s]+$\", na=False)]\n",
    "    if not special_only.empty:\n",
    "        print(\"üßµ Tweet with only special characters:\\n\", special_only.iloc[0][\"text\"], \"\\n\")\n",
    "\n",
    "    # Only whitespace or empty\n",
    "    whitespace_only = df[df[\"text\"].str.strip() == \"\"]\n",
    "    if not whitespace_only.empty:\n",
    "        print(\"üßµ Empty or whitespace-only tweet:\\n\", whitespace_only.iloc[0][\"text\"], \"\\n\")\n",
    "\n",
    "    # Only emoji or emoticon\n",
    "    emoji_emoticon_only = df[df[\"text\"].apply(lambda x: bool(re.fullmatch(r\"[\\W\\d_]+\", str(x).strip())))]\n",
    "    if not emoji_emoticon_only.empty:\n",
    "        print(\"üßµ Emoji/Emoticon-only tweet:\\n\", emoji_emoticon_only.iloc[0][\"text\"], \"\\n\")\n",
    "\n",
    "    # Drop helper column\n",
    "    df.drop(columns=\"text_length\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect_sample_tweets(df_keyword_prepended, dataset_label=\"Keyword Prepended\")\n",
    "inspect_sample_tweets(df_keyword_dropped, dataset_label=\"Keyword Dropped\")\n",
    "# inspect_sample_tweets(df_keyword_kept, dataset_label=\"Keyword Kept\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. üîÑ Text Preprocessing Variants <a name=\"preprocessing-variants\"></a>\n",
    "\n",
    "We apply multiple preprocessing strategies to generate diverse versions of the tweet dataset for model comparison. Each variant is saved independently and labeled for later use.\n",
    "\n",
    "### ‚úÖ Preprocessing Base Versions\n",
    "Each variant is generated from three base datasets:\n",
    "- **Keyword Prepended** ‚Äì `df_keyword_prepended`\n",
    "- **Keyword Dropped** ‚Äì `df_keyword_dropped`\n",
    "- **Keyword Kept** ‚Äì `df_keyword_kept`\n",
    "\n",
    "### üß™ Variant Descriptions\n",
    "\n",
    "| Variant | Description |\n",
    "|:--------|:------------|\n",
    "| **v1_basic_clean** | Lowercased, punctuation removed, stopwords removed. Emojis and mentions are *kept*. |\n",
    "| **v2_no_emojis_mentions** | v1 + Emojis and @mentions are removed. |\n",
    "| **v3_lemmatized** | v1 + Lemmatization applied. |\n",
    "| **v4_stemmed** | v1 + Stemming applied. |\n",
    "| **v5_lemma_stem** | v1 + Lemmatization and stemming applied sequentially. |\n",
    "| **v6_custom_stopwords** | v1 + Lemmatization + expanded stopword list (common Twitter words like 'rt', 'im', etc.). |\n",
    "| **v7_lowercase_words_only** | v1 + Numbers removed + keep only lowercase alphabetic tokens. |\n",
    "| **v8_keep_hashtags** | v1 + Lemmatization applied + hashtags preserved instead of stripped. |\n",
    "| **v9_minimal_processing** | Minimal cleaning: No stopword removal, no emoji or mention removal, no URL stripping, no lemmatization or stemming (baseline variant). |\n",
    "| **v10_lemma_stem_custom_stopwords** | Lemmatization + stemming + extended stopword list. Emojis and mentions are *kept*. |\n",
    "\n",
    "> *Variants were designed to test the impact of incremental text cleaning steps on model performance.*\n",
    "\n",
    "Each variant is stored in the `final_processed/` directory with a filename structure like:\n",
    "\n",
    "```\n",
    "final_processed/{df_label}v{variant_number}{variant_name}.csv\n",
    "```\n",
    "\n",
    "\n",
    "**Examples:**\n",
    "- `final_processed/prepended_v3_lemmatized.csv`\n",
    "- `final_processed/dropped_v6_custom_stopwords.csv`\n",
    "- `final_processed/kept_v10_lemma_stem_custom_stopwords.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "base_stopwords = set(stopwords.words('english'))\n",
    "extra_stopwords = {\n",
    "    \"get\", \"he\", \"that\", \"didnt\", \"doesnt\", \"dont\", \"isnt\", \"ive\", \"know\",\n",
    "    \"least\", \"let\", \"like\",\"may\", \"much\", \"must\", \"never\", \"one\", \"other\", \"rt\",\n",
    "    \"said\", \"see\", \"seem\", \"she\", \"take\", \"thank\", \"there\", \"though\", \"still\",\n",
    "    \"us\", \"use\", \"want\", \"wasnt\", \"way\", \"well\", \"what\", \"without\", \"wont\",\n",
    "    \"would\", \"yet\", \"your\", \"im\"\n",
    "}\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "def preprocess_text(\n",
    "    text,\n",
    "    lowercase=True,\n",
    "    remove_stopwords=True,\n",
    "    remove_emojis=True,\n",
    "    remove_mentions=True,\n",
    "    remove_urls=True,\n",
    "    remove_numbers=True,\n",
    "    lemmatize=False,\n",
    "    stem=False,\n",
    "    keep_hashtags=False,\n",
    "    use_extra_stopwords=False,\n",
    "    alpha_only=False, \n",
    "    tokenizer=tweet_tokenizer\n",
    "):\n",
    "    try:\n",
    "        if not isinstance(text, str) or text.strip() == \"\":\n",
    "            return \"\"\n",
    "\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "\n",
    "        if remove_emojis:\n",
    "            emoji_pattern = re.compile(\"[\" \n",
    "                \"\\U0001F600-\\U0001F64F\"\n",
    "                \"\\U0001F300-\\U0001F5FF\"\n",
    "                \"\\U0001F680-\\U0001F6FF\"\n",
    "                \"\\U0001F700-\\U0001F77F\"\n",
    "                \"\\U0001F780-\\U0001F7FF\"\n",
    "                \"\\U0001F800-\\U0001F8FF\"\n",
    "                \"\\U0001F900-\\U0001F9FF\"\n",
    "                \"\\U0001FA00-\\U0001FA6F\"\n",
    "                \"\\U0001FA70-\\U0001FAFF\"\n",
    "                \"\\U00002702-\\U000027B0\"\n",
    "                \"\\U000024C2-\\U0001F251\"\n",
    "                \"]+\", flags=re.UNICODE)\n",
    "            text = emoji_pattern.sub('', text)\n",
    "\n",
    "        if remove_mentions:\n",
    "            text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "        if remove_urls:\n",
    "            text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "        if not keep_hashtags:\n",
    "            text = re.sub(r\"#\", \"\", text)\n",
    "\n",
    "        if remove_numbers:\n",
    "            text = re.sub(r'\\S*\\d\\S*', '', text)\n",
    "\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        if alpha_only:\n",
    "            tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "        if remove_stopwords:\n",
    "            stop_set = base_stopwords | punctuation\n",
    "            if use_extra_stopwords:\n",
    "                stop_set = stop_set | extra_stopwords\n",
    "            tokens = [t for t in tokens if t not in stop_set]\n",
    "\n",
    "        if lemmatize:\n",
    "            tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "        if stem:\n",
    "            tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Preprocessing failed for: {repr(text)}\\n‚Üí {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_variants(df: pd.DataFrame, df_label: str = \"prepended\", output_dir: str = \"processed\"):\n",
    "    \"\"\"\n",
    "    Generates and saves multiple preprocessed versions of a tweet dataset.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame containing a 'text' column.\n",
    "\n",
    "    df_label : str, default=\"prepended\"\n",
    "        Label prefix to use in saved filenames (e.g., \"prepended_v1_basic_clean.csv\").\n",
    "\n",
    "    output_dir : str, default=\"processed\"\n",
    "        Directory to save the processed datasets.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    variants = {\n",
    "        \"v1_basic_clean\": {\"remove_emojis\": False, \"remove_mentions\": False},\n",
    "        \"v2_no_emojis_mentions\": {\"remove_emojis\": True, \"remove_mentions\": True},\n",
    "        \"v3_lemmatized\": {\"lemmatize\": True},\n",
    "        \"v4_stemmed\": {\"stem\": True},\n",
    "        \"v5_lemma_stem\": {\"lemmatize\": True, \"stem\": True},\n",
    "        \"v6_custom_stopwords\": {\"lemmatize\": True, \"use_extra_stopwords\": True}, \n",
    "        \"v7_lowercase_words_only\": {\"remove_numbers\": True, \"alpha_only\": True},\n",
    "        \"v8_keep_hashtags\": {\"lemmatize\": True, \"keep_hashtags\": True},\n",
    "        \"v9_minimal_processing\": {\n",
    "            \"remove_stopwords\": False,\n",
    "            \"remove_emojis\": False,\n",
    "            \"remove_mentions\": False,\n",
    "            \"remove_urls\": False,\n",
    "            \"lemmatize\": False,\n",
    "            \"stem\": False\n",
    "        },\n",
    "        \"v10_lemma_stem_custom_stopwords\": {\n",
    "            \"lemmatize\": True,\n",
    "            \"stem\": True,\n",
    "            \"use_extra_stopwords\": True,\n",
    "            \"remove_emojis\": False,\n",
    "            \"remove_mentions\": False\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    for name, options in variants.items():\n",
    "        variant_df = df.copy()\n",
    "        variant_df[\"processed_text\"] = variant_df[\"text\"].apply(lambda x: preprocess_text(x, **options))\n",
    "        save_path = f\"{output_dir}/{df_label}_{name}.csv\"\n",
    "        variant_df.to_csv(save_path, index=False)\n",
    "        print(f\"‚úÖ Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_variants(df_keyword_prepended, df_label=\"prepended\", output_dir=\"final_processed\")\n",
    "generate_text_variants(df_keyword_dropped, df_label=\"dropped\", output_dir=\"final_processed\")\n",
    "generate_text_variants(df_keyword_kept, df_label=\"kept\", output_dir=\"final_processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. üå•Ô∏è Word Clouds & Frequency Comparison <a name=\"wordclouds\"></a>\n",
    "\n",
    "This section generates word clouds and frequency bar charts for each preprocessed dataset to visualize word usage patterns. For each variant, we analyze:\n",
    "\n",
    "- **Overall Word Cloud**: Most frequent words across all tweets.\n",
    "- **Class-Wise Word Clouds**: Separate word clouds for disaster (`target = 1`) and non-disaster (`target = 0`) tweets.\n",
    "- **Top Word Frequency Bar Charts**: Comparing high-frequency tokens across classes.\n",
    "\n",
    "These visualizations help identify which preprocessing strategies retain the most meaningful features and how class distinctions are reflected in word usage.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_datasets(folder_path=\"final_processed\"):\n",
    "    \"\"\"\n",
    "    Load all processed datasets from a folder into a dictionary.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict: {filename_without_ext: DataFrame}\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            name = fname.replace(\".csv\", \"\")\n",
    "            datasets[name] = pd.read_csv(os.path.join(folder_path, fname))\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_word_stats(df, dataset_label, text_col=\"processed_text\", target_col=\"target\", top_n=20, output_dir=\"wordclouds\"):\n",
    "    \"\"\"\n",
    "    Generate and save word clouds and frequency bar plots to disk.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The processed dataset containing at least 'processed_text' and 'target' columns.\n",
    "    dataset_label : str\n",
    "        A unique label identifying this dataset (used as the folder name).\n",
    "    text_col : str\n",
    "        Column name containing processed tweet text.\n",
    "    target_col : str\n",
    "        Column containing binary labels (0 or 1).\n",
    "    top_n : int\n",
    "        Number of top words to show in bar plots.\n",
    "    output_dir : str\n",
    "        Parent directory to save word clouds and plots.\n",
    "    \"\"\"\n",
    "    subdir = os.path.join(output_dir, dataset_label)\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "    \n",
    "    # Frequency counts\n",
    "    tokens_all = \" \".join(df[text_col].astype(str)).split()\n",
    "    tokens_0 = \" \".join(df[df[target_col] == 0][text_col].astype(str)).split()\n",
    "    tokens_1 = \" \".join(df[df[target_col] == 1][text_col].astype(str)).split()\n",
    "\n",
    "    counter_all = Counter(tokens_all)\n",
    "    counter_0 = Counter(tokens_0)\n",
    "    counter_1 = Counter(tokens_1)\n",
    "\n",
    "    viridis = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "\n",
    "    # Word Cloud Generation\n",
    "    for counter, label in zip(\n",
    "        [counter_all, counter_0, counter_1],\n",
    "        [\"all\", \"target0\", \"target1\"]\n",
    "    ):\n",
    "        wc = WordCloud(width=800, height=400, background_color=\"white\", colormap=\"viridis\", max_words=1000)\n",
    "        wc.generate_from_frequencies(counter)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wc, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{label.capitalize()} - {dataset_label}\")\n",
    "        plt.tight_layout()\n",
    "        path = os.path.join(subdir, f\"wordcloud_{label}.png\")\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "\n",
    "    # Frequency Bar Charts\n",
    "    for counter, label in zip([counter_0, counter_1], [\"target0\", \"target1\"]):\n",
    "        top_words = counter.most_common(top_n)\n",
    "        words, freqs = zip(*top_words)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        sns.barplot(x=list(words), y=list(freqs), palette=\"viridis\")\n",
    "        plt.title(f\"Top {top_n} Words for {label} - {dataset_label}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        path = os.path.join(subdir, f\"freqplot_{label}.png\")\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"Saved word cloud and frequency plots for: {dataset_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"final_processed\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(\"final_processed\", file))\n",
    "        label = file.replace(\".csv\", \"\")\n",
    "        save_word_stats(df, dataset_label=label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ü§ñ Baseline Modeling Pipeline <a name=\"baseline-modeling\"></a>\n",
    "\n",
    "This section documents the Baseline Modeling Pipeline used for the initial HPC workflow.  \n",
    "Because this pipeline processes 30 different datasets across six classifiers with cross-validation and learning curve generation, it is computationally intensive and is **not recommended to run on a typical laptop or desktop machine**.  \n",
    "All baseline models were trained and evaluated on an HPC system using batch scheduling to ensure timely completion.\n",
    "\n",
    "- Load Preprocessed Datasets\n",
    "- Define Baseline Classifier Pipelines\n",
    "- Apply TF-IDF Vectorization\n",
    "- Train/Test Split (80/20)\n",
    "- Train Baseline Models:\n",
    "  - Multinomial Naive Bayes\n",
    "  - Logistic Regression\n",
    "  - Passive Aggressive Classifier\n",
    "  - Support Vector Machine\n",
    "  - K-Nearest Neighbors\n",
    "  - Neural Network (MLP)\n",
    "- Evaluate Models (Cross-Validation + Test Set)\n",
    "- Save Performance Metrics\n",
    "\n",
    "Each model's performance is evaluated using cross-validation and on a held-out test set.\n",
    "\n",
    "### üìÇ Source Files\n",
    "- [pipeline_modules.py](./pipeline_modules.py) ‚Äî Baseline modeling code\n",
    "- [run_pipeline.py](./run_pipeline.py) ‚Äî Script to execute baseline workflows\n",
    "- [run_pipeline.sh](./run_pipeline.sh) ‚Äî Batch script to coordinate HPC array runs\n",
    "- [merge_results.py](./merge_results.py) ‚Äî Merge evaluation results across datasets and models\n",
    "- [dataset_list.txt](./dataset_list.txt) ‚Äî List of all dataset variants to process\n",
    "\n",
    "The HPC jobs were scheduled on the Impulse cluster using 28 CPU cores and 16 GB memory per task, with a 4-hour walltime limit, and executed as an array job over 30 datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6.1 üìÇ Load Preprocessed Datasets  \n",
    "Load all text-processed CSVs and prepare `(X, y)` pairs for modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xy_datasets(folder_path=\"final_processed\", text_col=\"processed_text\", target_col=\"target\"):\n",
    "    \"\"\"\n",
    "    Load processed datasets and return (X, y) tuples for modeling.\n",
    "    Drops rows with NaN values in either the text or target columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    folder_path : str\n",
    "        Directory containing the processed CSV files.\n",
    "    text_col : str\n",
    "        Name of the column containing cleaned text.\n",
    "    target_col : str\n",
    "        Name of the column containing target labels.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping file names to (X, y) tuples.\n",
    "    \"\"\"\n",
    "    xy_datasets = {}\n",
    "    nan_report = {}\n",
    "\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.endswith(\".csv\"):\n",
    "            path = os.path.join(folder_path, fname)\n",
    "            name = fname.replace(\".csv\", \"\")\n",
    "            df = pd.read_csv(path)\n",
    "\n",
    "            # Count initial NaNs\n",
    "            nans_before = df[[text_col, target_col]].isna().sum().sum()\n",
    "\n",
    "            # Drop NaNs in either column\n",
    "            df = df.dropna(subset=[text_col, target_col]).reset_index(drop=True)\n",
    "\n",
    "            nans_after = df[[text_col, target_col]].isna().sum().sum()\n",
    "            if nans_before > 0:\n",
    "                nan_report[name] = nans_before\n",
    "\n",
    "            if text_col in df.columns and target_col in df.columns:\n",
    "                xy_datasets[name] = (df[text_col], df[target_col])\n",
    "\n",
    "    print(f\"\\nüì• Loaded {len(xy_datasets)} datasets.\")\n",
    "\n",
    "    if nan_report:\n",
    "        print(\"\\n‚ö†Ô∏è Datasets with NaNs (before cleaning):\")\n",
    "        for name, count in nan_report.items():\n",
    "            print(f\" - {name}: {count} NaNs removed\")\n",
    "\n",
    "    return xy_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6.2 üîß Define Baseline Classifier Pipelines  \n",
    "Set up pipelines for each classifier with TF-IDF + model combinations.\n",
    "- Apply TF-IDF Vectorization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common TF-IDF config for all models\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "\n",
    "# Define pipelines\n",
    "pipelines = {\n",
    "    'MultinomialNB': Pipeline([\n",
    "        ('tfidf', tfidf),\n",
    "        ('clf', MultinomialNB())\n",
    "    ]),\n",
    "    'LogisticRegression': Pipeline([\n",
    "        ('tfidf', tfidf),\n",
    "        ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "    ]),\n",
    "    'PassiveAggressive': Pipeline([\n",
    "        ('tfidf', tfidf),\n",
    "        ('clf', PassiveAggressiveClassifier(max_iter=1000, random_state=42))\n",
    "    ]),\n",
    "    'SVM': Pipeline([\n",
    "        ('tfidf', tfidf),\n",
    "        ('clf', SVC(kernel='linear', C=1.0, probability=True, random_state=42))\n",
    "    ]),\n",
    "    'KNN': Pipeline([\n",
    "        ('tfidf', tfidf),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors=5))\n",
    "    ]),\n",
    "    'NeuralNetwork': Pipeline([\n",
    "        ('tfidf', tfidf),\n",
    "        ('clf', MLPClassifier(hidden_layer_sizes=(50,), max_iter=300, verbose=True, \n",
    "        random_state=42))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 üöÄ Train and Evaluate Baseline Models\n",
    "\n",
    "We now train and evaluate each classifier on every processed dataset using 5-fold Stratified Cross-Validation, followed by evaluation on a held-out test set. Metrics such as Accuracy, Precision, Recall, F1 Score, and ROC AUC are recorded.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, cv, scoring='accuracy', n_jobs=-1, save_path=None):\n",
    "\n",
    "    # Use viridis colormap to get consistent, soft shades\n",
    "    viridis = cm.get_cmap('viridis')\n",
    "    train_color = to_hex(viridis(0.25))  # bluish-teal\n",
    "    test_color = to_hex(viridis(0.65))   # greenish\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(facecolor='white')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(scoring.capitalize())\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Soft shading\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2, color=train_color)\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2, color=test_color)\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=train_color, label=\"Training Score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=test_color, label=\"Validation Score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(xy_datasets, pipelines, cv_folds=5, random_state=42, output_dir=\"performance_metrics3\"):\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    viridis = cm.get_cmap('viridis', 256)\n",
    "    viridis_light = ListedColormap(viridis(np.linspace(0.2, 0.8)))  # focus on mid-range\n",
    "\n",
    "    \n",
    "    for dataset_name, (X, y) in xy_datasets.items():\n",
    "        print(f\"\\nüì¶ Dataset: {dataset_name} ({len(X)} samples)\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=random_state\n",
    "        )\n",
    "\n",
    "        for model_name, pipeline in pipelines.items():\n",
    "            print(f\"  üîç Evaluating: {model_name}\")\n",
    "            combo_name = f\"{dataset_name}_{model_name}\"\n",
    "            save_dir = os.path.join(output_dir, combo_name)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "            acc = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='accuracy').mean()\n",
    "            prec = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='precision').mean()\n",
    "            rec = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='recall').mean()\n",
    "            f1 = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1').mean()\n",
    "\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "\n",
    "            try:\n",
    "                if hasattr(pipeline.named_steps['clf'], 'predict_proba'):\n",
    "                    y_scores = pipeline.predict_proba(X_test)[:, 1]\n",
    "                else:\n",
    "                    y_scores = pipeline.decision_function(X_test)\n",
    "                roc = roc_auc_score(y_test, y_scores)\n",
    "            except Exception:\n",
    "                roc = np.nan\n",
    "                y_scores = None\n",
    "\n",
    "            results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Model': model_name,\n",
    "                'Accuracy': acc,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec,\n",
    "                'F1 Score': f1,\n",
    "                'ROC AUC': roc\n",
    "            })\n",
    "\n",
    "            # ROC Curve\n",
    "            if y_scores is not None:\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc:.2f})', color=to_hex(viridis(0.6)))\n",
    "                plt.plot([0, 1], [0, 1], 'k--', alpha=0.4)\n",
    "                plt.grid(alpha=0.3)\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'ROC Curve ‚Äî {model_name} on {dataset_name}')\n",
    "                plt.legend(loc='lower right')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(save_dir, \"roc_curve.png\"))\n",
    "                plt.close()\n",
    "\n",
    "            # Confusion Matrix\n",
    "            cmatrix = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(6, 5))\n",
    "            sns.heatmap(\n",
    "                cmatrix,\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                # cmap=sns.light_palette(\"seagreen\", as_cmap=True),\n",
    "                cmap=viridis_light,\n",
    "                cbar=False,\n",
    "                linewidths=0.5,\n",
    "                linecolor='white',\n",
    "                square=True,\n",
    "                annot_kws={\"size\": 12},\n",
    "                xticklabels=pipeline.classes_,\n",
    "                yticklabels=pipeline.classes_\n",
    "            )\n",
    "            plt.title(f'Confusion Matrix ‚Äî {model_name} on {dataset_name}')\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"Actual\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(save_dir, \"confusion_matrix.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # Learning Curve\n",
    "            print(f\"üìà Plotting learning curve for {model_name} on {dataset_name}...\")\n",
    "            plot_learning_curve(\n",
    "                pipeline,\n",
    "                f\"Learning Curve ‚Äî {model_name} on {dataset_name}\",\n",
    "                X_train,\n",
    "                y_train,\n",
    "                cv=cv,\n",
    "                save_path=os.path.join(save_dir, \"learning_curve.png\")\n",
    "            )\n",
    "\n",
    "    # Sort by F1 Score in descending order\n",
    "    results_df = pd.DataFrame(results).sort_values(by=\"F1 Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 üíæ Run Baseline Workflow & Save Evaluation Metrics\n",
    "\n",
    "After model evaluation, the performance metrics for each (dataset, model) pair are saved as CSV files under the applicable `performance_metrics/` directory for further analysis and visualization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the dataset name from the command-line\n",
    "dataset_name = sys.argv[1]\n",
    "\n",
    "xy_all = load_xy_datasets(\"final_processed\")\n",
    "\n",
    "if dataset_name not in xy_all:\n",
    "    print(dataset_name)\n",
    "    raise ValueError(f\"{dataset_name} not found in final_processed/\")\n",
    "\n",
    "xy_subset = {dataset_name: xy_all[dataset_name]}\n",
    "\n",
    "# Only returns results_df now\n",
    "results_df = evaluate_models(xy_subset, pipelines)\n",
    "\n",
    "# Save raw results to CSV\n",
    "results_df.to_csv(f\"performance_metrics3/{dataset_name}_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üîß Model Hypertuning Function\n",
    "\n",
    "In this section, we define a reusable function `run_grid_searches()` that leverages `GridSearchCV` to tune hyperparameters for key models in our pipeline:\n",
    "\n",
    "Each model has its own tailored search space, and the function prints out the best hyperparameters and corresponding F1 Macro scores using cross-validation. This allows for automated, performance-driven tuning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Multinomial Naive Bayes (MultinomialNB)**\n",
    "- `alpha`: Smoothing parameter (Laplace smoothing) to handle zero counts for unseen words.\n",
    "\n",
    "---\n",
    "\n",
    "### **Passive Aggressive Classifier**\n",
    "- `C`: Regularization strength; lower values imply stronger regularization.\n",
    "- `max_iter`: Maximum number of training iterations.\n",
    "- `tol`: Tolerance for stopping criteria (used for convergence).\n",
    "\n",
    "---\n",
    "\n",
    "### **Neural Network (MLPClassifier)**\n",
    "- `hidden_layer_sizes`: Number of neurons per hidden layer (e.g., `(50, 50)` = 2 layers of 50).\n",
    "- `activation`: Activation function such as `'relu'` or `'tanh'`.\n",
    "- `solver`: Optimization algorithm; `'adam'` is adaptive and efficient.\n",
    "- `alpha`: L2 regularization parameter (helps prevent overfitting).\n",
    "- `learning_rate`: Strategy for adjusting the learning rate during training.\n",
    "- `early_stopping`: Stop training when validation score stops improving.\n",
    "- `validation_fraction`: Percentage of training data used for validation during early stopping.\n",
    "- `n_iter_no_change`: Number of epochs without improvement before stopping.\n",
    "\n",
    "---\n",
    "\n",
    "### **Logistic Regression**\n",
    "- `C`: Inverse of regularization strength (smaller values specify stronger regularization).\n",
    "- `penalty`: Type of regularization term (`'l2'` for Ridge-style regularization).\n",
    "- `solver`: Optimization algorithm (`'lbfgs'` is efficient for multiclass problems).\n",
    "- `max_iter`: Maximum number of iterations for the optimization algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### **Support Vector Machine (SVM)**\n",
    "- `C`: Regularization strength (higher values reduce regularization).\n",
    "- `kernel`: Specifies the kernel type to be used in the algorithm (`'linear'` or `'rbf'`).\n",
    "- `gamma`: Kernel coefficient for `'rbf'`; controls the influence of single training examples.\n",
    "\n",
    "---\n",
    "\n",
    "### **K-Nearest Neighbors (KNN)**\n",
    "- `n_neighbors`: Number of neighbors to use for prediction.\n",
    "- `weights`: Weight function used in prediction (`'uniform'` or `'distance'`).\n",
    "- `metric`: Distance metric to use (e.g., `'euclidean'`, `'manhattan'`).\n",
    "\n",
    "These comprehensive tuning options ensure that every model is given the opportunity to perform optimally on our NLP classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üîç Run just on one dataset and two models\n",
    "xy_datasets = load_xy_datasets(\"final_processed\")\n",
    "xy_datasets = {'prepended_v3_lemmatized': xy_datasets['prepended_v3_lemmatized']}  # pick one\n",
    "\n",
    "# Select and split dataset for hypertuning\n",
    "X, y = xy_datasets['prepended_v3_lemmatized']  # Replace with desired dataset key\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "def run_grid_searches(X_train, y_train, pipelines):\n",
    "    param_grids = {\n",
    "        'MultinomialNB': {\n",
    "            'clf__alpha': [0.1, 0.5, 1.0, 1.5, 2.0],\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'clf__C': [0.01, 0.1, 1.0, 10.0],\n",
    "            'clf__penalty': ['l2'],\n",
    "            'clf__solver': ['lbfgs'],\n",
    "            'clf__max_iter': [300, 500, 1000]\n",
    "        },\n",
    "        'PassiveAggressive': {\n",
    "            'clf__C': [0.01, 0.1, 1.0, 10.0],\n",
    "            'clf__max_iter': [500, 1000, 2000],\n",
    "            'clf__tol': [1e-4, 1e-3, 1e-2]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'clf__C': [0.1, 1.0, 10.0],\n",
    "            'clf__kernel': ['linear', 'rbf'],\n",
    "            'clf__gamma': ['scale', 'auto']\n",
    "        },\n",
    "        'KNN': {\n",
    "            'clf__n_neighbors': [3, 5, 7, 9],\n",
    "            'clf__weights': ['uniform', 'distance'],\n",
    "            'clf__metric': ['euclidean', 'manhattan']\n",
    "        },\n",
    "        'NeuralNetwork': {\n",
    "            'clf__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "            'clf__activation': ['relu', 'tanh'],\n",
    "            'clf__solver': ['adam', 'sgd'],\n",
    "            'clf__alpha': [0.0001, 0.001, 0.01],\n",
    "            'clf__learning_rate': ['constant', 'adaptive'],\n",
    "            'clf__early_stopping': [True],\n",
    "            'clf__n_iter_no_change': [5, 10],\n",
    "            'clf__validation_fraction': [0.1, 0.2]\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    best_models = {}\n",
    "\n",
    "    for model_name, pipeline in pipelines.items():\n",
    "        if model_name not in param_grids:\n",
    "            print(f\"Skipping model: {model_name}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîç Tuning hyperparameters for: {model_name}...\")\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=param_grids[model_name],\n",
    "            cv=5,\n",
    "            scoring='f1_macro',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid.fit(X_train, y_train)\n",
    "        best_models[model_name] = grid\n",
    "\n",
    "        print(f\"\\n‚úÖ Best Params for {model_name}: {grid.best_params_}\")\n",
    "        print(f\"üèÜ Best F1 Macro Score: {grid.best_score_:.4f}\")\n",
    "\n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ‚ñ∂Ô∏è Run Grid Search on Pipelines\n",
    "\n",
    "This section runs `GridSearchCV` for the three selected models using the training data and prints:\n",
    "- Best hyperparameters for each model\n",
    "- Best F1 Macro score from cross-validation\n",
    "\n",
    "This enables informed model comparison and selection based on empirical performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the hypertuning function\n",
    "best_models = run_grid_searches(X_train, y_train, pipelines)\n",
    "\n",
    "# Evaluate best models on test set\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\nüìä Evaluation Report for {name}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìä Visualize Grid Search Results\n",
    "\n",
    "We generate a bar chart of the best F1 Macro scores across all hypertuned models for quick visual comparison. This allows us to:\n",
    "- Evaluate which models performed best under optimal conditions\n",
    "- Make informed decisions about which model(s) to deploy or further refine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize best F1 scores\n",
    "model_names = []\n",
    "f1_scores = []\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    model_names.append(name)\n",
    "    f1_scores.append(model.best_score_)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(model_names, f1_scores)\n",
    "plt.title(\"Best F1 Macro Scores by Model\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"F1 Macro Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### 7.3 üß™ Train/Test Split & Cross-Validation  \n",
    "\n",
    "Split each dataset into training and test sets (80/20), and apply Stratified K-Fold cross-validation.  \n",
    "**Note:** We now use `GridSearchCV` to first identify the best hyperparameter settings for each model. These tuned models are then passed into the evaluation pipeline.\n",
    "\n",
    "This ensures:\n",
    "- Only the best version of each model (with optimized hyperparameters) is used in evaluation\n",
    "- Consistency across training, validation, and test performance\n",
    "- Improved generalization and model comparison\n",
    "\n",
    "We support both raw pipelines and GridSearchCV objects. If a `GridSearchCV` object is passed, the evaluation automatically uses its `.best_estimator_`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, cv, scoring='accuracy', n_jobs=-1, save_path=None):\n",
    "\n",
    "    # Use viridis colormap to get consistent, soft shades\n",
    "    viridis = cm.get_cmap('viridis')\n",
    "    train_color = to_hex(viridis(0.25))  # bluish-teal\n",
    "    test_color = to_hex(viridis(0.65))   # greenish\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(facecolor='white')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(scoring.capitalize())\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Soft shading\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2, color=train_color)\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2, color=test_color)\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=train_color, label=\"Training Score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=test_color, label=\"Validation Score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(xy_datasets, best_models, cv_folds=5, random_state=42, output_dir=\"performance_metrics\"):\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    viridis = cm.get_cmap('viridis', 256)\n",
    "    viridis_light = ListedColormap(viridis(np.linspace(0.2, 0.8)))  # focus on mid-range\n",
    "\n",
    "    \n",
    "    for dataset_name, (X, y) in xy_datasets.items():\n",
    "        print(f\"\\nüì¶ Dataset: {dataset_name} ({len(X)} samples)\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=random_state\n",
    "        )\n",
    "\n",
    "        for model_name, model in pipelines.items():\n",
    "            pipeline = model.best_estimator_ if hasattr(model, 'best_estimator_') else model\n",
    "            print(f\"  üîç Evaluating: {model_name}\")\n",
    "            combo_name = f\"{dataset_name}_{model_name}\"\n",
    "            save_dir = os.path.join(output_dir, combo_name)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "            acc = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='accuracy').mean()\n",
    "            prec = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='precision').mean()\n",
    "            rec = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='recall').mean()\n",
    "            f1 = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1').mean()\n",
    "\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "\n",
    "            try:\n",
    "                if hasattr(pipeline.named_steps['clf'], 'predict_proba'):\n",
    "                    y_scores = pipeline.predict_proba(X_test)[:, 1]\n",
    "                else:\n",
    "                    y_scores = pipeline.decision_function(X_test)\n",
    "                roc = roc_auc_score(y_test, y_scores)\n",
    "            except Exception:\n",
    "                roc = np.nan\n",
    "                y_scores = None\n",
    "\n",
    "            results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Model': model_name,\n",
    "                'Accuracy': acc,\n",
    "                'Precision': prec,\n",
    "                'Recall': rec,\n",
    "                'F1 Score': f1,\n",
    "                'ROC AUC': roc\n",
    "            })\n",
    "\n",
    "            # ROC Curve\n",
    "            if y_scores is not None:\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "                plt.figure()\n",
    "                plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc:.2f})', color=to_hex(viridis(0.6)))\n",
    "                plt.plot([0, 1], [0, 1], 'k--', alpha=0.4)\n",
    "                plt.grid(alpha=0.3)\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title(f'ROC Curve ‚Äî {model_name} on {dataset_name}')\n",
    "                plt.legend(loc='lower right')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(save_dir, \"roc_curve.png\"))\n",
    "                plt.close()\n",
    "\n",
    "            # Confusion Matrix\n",
    "            cmatrix = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(6, 5))\n",
    "            sns.heatmap(\n",
    "                cmatrix,\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                # cmap=sns.light_palette(\"seagreen\", as_cmap=True),\n",
    "                cmap=viridis_light,\n",
    "                cbar=False,\n",
    "                linewidths=0.5,\n",
    "                linecolor='white',\n",
    "                square=True,\n",
    "                annot_kws={\"size\": 12},\n",
    "                xticklabels=pipeline.classes_,\n",
    "                yticklabels=pipeline.classes_\n",
    "            )\n",
    "            plt.title(f'Confusion Matrix ‚Äî {model_name} on {dataset_name}')\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"Actual\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(save_dir, \"confusion_matrix.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # Learning Curve\n",
    "            print(f\"üìà Plotting learning curve for {model_name} on {dataset_name}...\")\n",
    "            plot_learning_curve(\n",
    "                pipeline,\n",
    "                f\"Learning Curve ‚Äî {model_name} on {dataset_name}\",\n",
    "                X_train,\n",
    "                y_train,\n",
    "                cv=cv,\n",
    "                save_path=os.path.join(save_dir, \"learning_curve.png\")\n",
    "            )\n",
    "\n",
    "    # Sort by F1 Score in descending order\n",
    "    results_df = pd.DataFrame(results).sort_values(by=\"F1 Score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    styled = results_df.style.highlight_max(axis=0, color='green').set_table_styles([\n",
    "        {'selector': 'th', 'props': [('font-size', '14px'), ('color', 'black'), ('background-color', 'white')]}\n",
    "    ])\n",
    "\n",
    "    return styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # üîç Run just on one dataset and two models\n",
    "# xy_datasets = load_xy_datasets(\"final_processed\")\n",
    "# xy_datasets = {'prepended_v3_lemmatized': xy_datasets['prepended_v3_lemmatized']}  # pick one\n",
    "\n",
    "# styled_results = evaluate_models(xy_datasets, pipelines)\n",
    "styled_results = evaluate_models(xy_datasets, best_models)\n",
    "styled_results  # <- this shows in Jupyter as styled HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7.4 üöÄ Run Models & Collect Metrics  \n",
    "Loop through each pipeline and dataset, collecting evaluation metrics:\n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F1 Score  \n",
    "- ROC AUC\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7.5 üìä Display Evaluation Results  \n",
    "Aggregate and display model performance metrics in a sortable and styled table.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Load and Inspect Data\n",
    "filepath = \"performance_metrics3/ALL_results.csv\"\n",
    "# Detect encoding of the training CSV file to avoid character corruption\n",
    "with open(filepath, \"rb\") as f:\n",
    "    raw_data = f.read(100_000)  # Read the first 100KB\n",
    "    result = chardet.detect(raw_data)\n",
    "    detected_encoding = result['encoding']\n",
    "\n",
    "# Fallback in case encoding couldn't be detected\n",
    "if detected_encoding is None:\n",
    "    detected_encoding = 'utf-8'\n",
    "\n",
    "print(f\"Detected Encoding: {detected_encoding}\")\n",
    "\n",
    "# Load the CSV file with the detected encoding\n",
    "# df_results = pd.read_csv(filepath, encoding=detected_encoding).sort_values(by=\"F1 Score\", ascending=False)\n",
    "# df_results = pd.read_csv(filepath, encoding=detected_encoding).sort_values(by=\"Accuracy\", ascending=False)\n",
    "df_results = pd.read_csv(filepath, encoding=detected_encoding).sort_values(by=\"Recall\", ascending=False)\n",
    "\n",
    "\n",
    "df_results.head(20)\n",
    "\n",
    "# Reset index to ensure consistency\n",
    "# train_data.reset_index(drop=True, inplace=True)\n",
    "# test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# üîç Preview the first few rows of the training data\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. üìà Performance Comparison <a name=\"comparison\"></a>\n",
    "\n",
    "In this section, we compare model performance across all preprocessing variants.\n",
    "This helps us understand:\n",
    "\n",
    "- Which text preprocessing techniques boost performance?\n",
    "- Which classifiers are more resilient to noisy or minimal processing?\n",
    "\n",
    "We visualize:\n",
    "- Metric comparisons (bar plots, line plots)\n",
    "- Heatmaps of performance across models and variants\n",
    "- Highlighting the best-performing pipeline\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9. ‚úÖ Conclusions & Next Steps <a name=\"conclusion\"></a>\n",
    "- Which dataset version performed best?\n",
    "- Which model was most effective?\n",
    "- What preprocessing steps helped or hurt performance?\n",
    "- Future improvements (hyperparameter tuning, embeddings, etc.)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
