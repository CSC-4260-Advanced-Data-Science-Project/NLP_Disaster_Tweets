Processing dataset: kept_v10_lemma_stem_custom_stopwords

üì• Loaded 30 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - prepended_v10_lemma_stem_custom_stopwords: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - dropped_v10_lemma_stem_custom_stopwords: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - kept_v10_lemma_stem_custom_stopwords: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: kept_v10_lemma_stem_custom_stopwords (7443 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on kept_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on kept_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on kept_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on kept_v10_lemma_stem_custom_stopwords...
/opt/ohpc/pub/spack/v0.21.1/opt/spack/linux-rocky8-x86_64_v3/gcc-8.5.0/py-joblib-1.2.0-g6vh2usiiivwucfr57aoxwqvljmoxokc/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on kept_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.68284097
Iteration 2, loss = 0.61820352
Iteration 3, loss = 0.51351853
Iteration 4, loss = 0.39858386
Iteration 5, loss = 0.29959230
Iteration 6, loss = 0.22463388
Iteration 7, loss = 0.17175407
Iteration 8, loss = 0.13514591
Iteration 9, loss = 0.10993029
Iteration 10, loss = 0.09221291
Iteration 11, loss = 0.07925527
Iteration 12, loss = 0.06942862
Iteration 13, loss = 0.06204128
Iteration 14, loss = 0.05642974
Iteration 15, loss = 0.05195843
Iteration 16, loss = 0.04834929
Iteration 17, loss = 0.04533728
Iteration 18, loss = 0.04266816
Iteration 19, loss = 0.04086629
Iteration 20, loss = 0.03881111
Iteration 21, loss = 0.03759809
Iteration 22, loss = 0.03637813
Iteration 23, loss = 0.03534829
Iteration 24, loss = 0.03443763
Iteration 25, loss = 0.03332649
Iteration 26, loss = 0.03272203
Iteration 27, loss = 0.03205188
Iteration 28, loss = 0.03178865
Iteration 29, loss = 0.03107210
Iteration 30, loss = 0.03068128
Iteration 31, loss = 0.03027433
Iteration 32, loss = 0.02969234
Iteration 33, loss = 0.02910561
Iteration 34, loss = 0.02925042
Iteration 35, loss = 0.02866484
Iteration 36, loss = 0.02874179
Iteration 37, loss = 0.02828291
Iteration 38, loss = 0.02820872
Iteration 39, loss = 0.02808329
Iteration 40, loss = 0.02782100
Iteration 41, loss = 0.02780462
Iteration 42, loss = 0.02734749
Iteration 43, loss = 0.02727761
Iteration 44, loss = 0.02721605
Iteration 45, loss = 0.02711727
Iteration 46, loss = 0.02696144
Iteration 47, loss = 0.02663952
Iteration 48, loss = 0.02670605
Iteration 49, loss = 0.02680777
Iteration 50, loss = 0.02655798
Iteration 51, loss = 0.02629357
Iteration 52, loss = 0.02632008
Iteration 53, loss = 0.02631975
Iteration 54, loss = 0.02646662
Iteration 55, loss = 0.02617318
Iteration 56, loss = 0.02619271
Iteration 57, loss = 0.02586951
Iteration 58, loss = 0.02601265
Iteration 59, loss = 0.02615067
Iteration 60, loss = 0.02575575
Iteration 61, loss = 0.02594693
Iteration 62, loss = 0.02546084
Iteration 63, loss = 0.02559757
Iteration 64, loss = 0.02548247
Iteration 65, loss = 0.02541829
Iteration 66, loss = 0.02535221
Iteration 67, loss = 0.02564259
Iteration 68, loss = 0.02549236
Iteration 69, loss = 0.02535090
Iteration 70, loss = 0.02555978
Iteration 71, loss = 0.02563311
Iteration 72, loss = 0.02552289
Iteration 73, loss = 0.02599612
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70767382
Iteration 2, loss = 0.62854521
Iteration 3, loss = 0.52508624
Iteration 4, loss = 0.40572258
Iteration 5, loss = 0.30249587
Iteration 6, loss = 0.22459017
Iteration 7, loss = 0.17010288
Iteration 8, loss = 0.13306203
Iteration 9, loss = 0.10714775
Iteration 10, loss = 0.08902152
Iteration 11, loss = 0.07623871
Iteration 12, loss = 0.06648958
Iteration 13, loss = 0.05907017
Iteration 14, loss = 0.05332978
Iteration 15, loss = 0.04874520
Iteration 16, loss = 0.04507343
Iteration 17, loss = 0.04205243
Iteration 18, loss = 0.03971263
Iteration 19, loss = 0.03743704
Iteration 20, loss = 0.03570709
Iteration 21, loss = 0.03410925
Iteration 22, loss = 0.03302491
Iteration 23, loss = 0.03177033
Iteration 24, loss = 0.03093691
Iteration 25, loss = 0.02986386
Iteration 26, loss = 0.02916781
Iteration 27, loss = 0.02854674
Iteration 28, loss = 0.02778284
Iteration 29, loss = 0.02769555
Iteration 30, loss = 0.02707239
Iteration 31, loss = 0.02653775
Iteration 32, loss = 0.02620387
Iteration 33, loss = 0.02618886
Iteration 34, loss = 0.02552218
Iteration 35, loss = 0.02531066
Iteration 36, loss = 0.02494826
Iteration 37, loss = 0.02494488
Iteration 38, loss = 0.02447515
Iteration 39, loss = 0.02415549
Iteration 40, loss = 0.02413571
Iteration 41, loss = 0.02395441
Iteration 42, loss = 0.02388012
Iteration 43, loss = 0.02362330
Iteration 44, loss = 0.02338429
Iteration 45, loss = 0.02344135
Iteration 46, loss = 0.02332782
Iteration 47, loss = 0.02325794
Iteration 48, loss = 0.02279099
Iteration 49, loss = 0.02308789
Iteration 50, loss = 0.02286470
Iteration 51, loss = 0.02267164
Iteration 52, loss = 0.02264688
Iteration 53, loss = 0.02263942
Iteration 54, loss = 0.02241762
Iteration 55, loss = 0.02247446
Iteration 56, loss = 0.02238997
Iteration 57, loss = 0.02221066
Iteration 58, loss = 0.02214850
Iteration 59, loss = 0.02227086
Iteration 60, loss = 0.02217720
Iteration 61, loss = 0.02229495
Iteration 62, loss = 0.02202479
Iteration 63, loss = 0.02194925
Iteration 64, loss = 0.02199266
Iteration 65, loss = 0.02187501
Iteration 66, loss = 0.02168042
Iteration 67, loss = 0.02164434
Iteration 68, loss = 0.02181599
Iteration 69, loss = 0.02196218
Iteration 70, loss = 0.02155668
Iteration 71, loss = 0.02165601
Iteration 72, loss = 0.02147934
Iteration 73, loss = 0.02155960
Iteration 74, loss = 0.02171979
Iteration 75, loss = 0.02133377
Iteration 76, loss = 0.02153662
Iteration 77, loss = 0.02157650
Iteration 78, loss = 0.02143194
Iteration 79, loss = 0.02163556
Iteration 80, loss = 0.02128084
Iteration 81, loss = 0.02135407
Iteration 82, loss = 0.02153343
Iteration 83, loss = 0.02141234
Iteration 84, loss = 0.02141263
Iteration 85, loss = 0.02110095
Iteration 86, loss = 0.02116370
Iteration 87, loss = 0.02160462
Iteration 88, loss = 0.02139628
Iteration 89, loss = 0.02132336
Iteration 90, loss = 0.02127973
Iteration 91, loss = 0.02153720
Iteration 92, loss = 0.02140256
Iteration 93, loss = 0.02116272
Iteration 94, loss = 0.02131689
Iteration 95, loss = 0.02158801
Iteration 96, loss = 0.02088427
Iteration 97, loss = 0.02124533
Iteration 98, loss = 0.02106416
Iteration 99, loss = 0.02126638
Iteration 100, loss = 0.02078848
Iteration 101, loss = 0.02118827
Iteration 102, loss = 0.02124836
Iteration 103, loss = 0.02120623
Iteration 104, loss = 0.02127716
Iteration 105, loss = 0.02106710
Iteration 106, loss = 0.02121945
Iteration 107, loss = 0.02094391
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67606798
Iteration 2, loss = 0.60260181
Iteration 3, loss = 0.49151418
Iteration 4, loss = 0.37710520
Iteration 5, loss = 0.28150081
Iteration 6, loss = 0.20998031
Iteration 7, loss = 0.15993940
Iteration 8, loss = 0.12570222
Iteration 9, loss = 0.10178590
Iteration 10, loss = 0.08516545
Iteration 11, loss = 0.07276139
Iteration 12, loss = 0.06362407
Iteration 13, loss = 0.05646245
Iteration 14, loss = 0.05099592
Iteration 15, loss = 0.04672130
Iteration 16, loss = 0.04315278
Iteration 17, loss = 0.04011478
Iteration 18, loss = 0.03771175
Iteration 19, loss = 0.03546299
Iteration 20, loss = 0.03399159
Iteration 21, loss = 0.03261036
Iteration 22, loss = 0.03113684
Iteration 23, loss = 0.02987309
Iteration 24, loss = 0.02907104
Iteration 25, loss = 0.02816336
Iteration 26, loss = 0.02759316
Iteration 27, loss = 0.02705105
Iteration 28, loss = 0.02674078
Iteration 29, loss = 0.02588397
Iteration 30, loss = 0.02561297
Iteration 31, loss = 0.02516937
Iteration 32, loss = 0.02471624
Iteration 33, loss = 0.02438494
Iteration 34, loss = 0.02392548
Iteration 35, loss = 0.02368052
Iteration 36, loss = 0.02352882
Iteration 37, loss = 0.02307279
Iteration 38, loss = 0.02287382
Iteration 39, loss = 0.02276094
Iteration 40, loss = 0.02270222
Iteration 41, loss = 0.02253140
Iteration 42, loss = 0.02208432
Iteration 43, loss = 0.02217322
Iteration 44, loss = 0.02245148
Iteration 45, loss = 0.02201443
Iteration 46, loss = 0.02173797
Iteration 47, loss = 0.02156237
Iteration 48, loss = 0.02175912
Iteration 49, loss = 0.02142554
Iteration 50, loss = 0.02145036
Iteration 51, loss = 0.02145148
Iteration 52, loss = 0.02112820
Iteration 53, loss = 0.02113135
Iteration 54, loss = 0.02127871
Iteration 55, loss = 0.02100596
Iteration 56, loss = 0.02111109
Iteration 57, loss = 0.02086029
Iteration 58, loss = 0.02078880
Iteration 59, loss = 0.02075246
Iteration 60, loss = 0.02064356
Iteration 61, loss = 0.02078929
Iteration 62, loss = 0.02077066
Iteration 63, loss = 0.02055649
Iteration 64, loss = 0.02047961
Iteration 65, loss = 0.02061124
Iteration 66, loss = 0.02045450
Iteration 67, loss = 0.02038953
Iteration 68, loss = 0.02056976
Iteration 69, loss = 0.02041519
Iteration 70, loss = 0.02034317
Iteration 71, loss = 0.02017412
Iteration 72, loss = 0.02033357
Iteration 73, loss = 0.02024955
Iteration 74, loss = 0.02000242
Iteration 75, loss = 0.02035123
Iteration 76, loss = 0.02031987
Iteration 77, loss = 0.02005596
Iteration 78, loss = 0.02007625
Iteration 79, loss = 0.02020801
Iteration 80, loss = 0.02026930
Iteration 81, loss = 0.02020915
Iteration 82, loss = 0.02011148
Iteration 83, loss = 0.02024076
Iteration 84, loss = 0.02034902
Iteration 85, loss = 0.02013754
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67629104
Iteration 2, loss = 0.61626883
Iteration 3, loss = 0.51609063
Iteration 4, loss = 0.40605599
Iteration 5, loss = 0.30693492
Iteration 6, loss = 0.22987802
Iteration 7, loss = 0.17494479
Iteration 8, loss = 0.13727984
Iteration 9, loss = 0.11136403
Iteration 10, loss = 0.09305900
Iteration 11, loss = 0.07966311
Iteration 12, loss = 0.06982015
Iteration 13, loss = 0.06231150
Iteration 14, loss = 0.05652933
Iteration 15, loss = 0.05178911
Iteration 16, loss = 0.04802513
Iteration 17, loss = 0.04486734
Iteration 18, loss = 0.04241157
Iteration 19, loss = 0.04021112
Iteration 20, loss = 0.03860239
Iteration 21, loss = 0.03705761
Iteration 22, loss = 0.03589499
Iteration 23, loss = 0.03448625
Iteration 24, loss = 0.03389040
Iteration 25, loss = 0.03279152
Iteration 26, loss = 0.03193778
Iteration 27, loss = 0.03132523
Iteration 28, loss = 0.03068504
Iteration 29, loss = 0.03009859
Iteration 30, loss = 0.02981866
Iteration 31, loss = 0.02928058
Iteration 32, loss = 0.02902302
Iteration 33, loss = 0.02860774
Iteration 34, loss = 0.02836636
Iteration 35, loss = 0.02786138
Iteration 36, loss = 0.02788969
Iteration 37, loss = 0.02739717
Iteration 38, loss = 0.02739715
Iteration 39, loss = 0.02708759
Iteration 40, loss = 0.02680074
Iteration 41, loss = 0.02679215
Iteration 42, loss = 0.02652936
Iteration 43, loss = 0.02655078
Iteration 44, loss = 0.02618876
Iteration 45, loss = 0.02620065
Iteration 46, loss = 0.02591384
Iteration 47, loss = 0.02584227
Iteration 48, loss = 0.02563129
Iteration 49, loss = 0.02551102
Iteration 50, loss = 0.02551638
Iteration 51, loss = 0.02532993
Iteration 52, loss = 0.02542719
Iteration 53, loss = 0.02549008
Iteration 54, loss = 0.02506622
Iteration 55, loss = 0.02515876
Iteration 56, loss = 0.02494717
Iteration 57, loss = 0.02484000
Iteration 58, loss = 0.02482018
Iteration 59, loss = 0.02481256
Iteration 60, loss = 0.02462491
Iteration 61, loss = 0.02479170
Iteration 62, loss = 0.02481111
Iteration 63, loss = 0.02463873
Iteration 64, loss = 0.02452174
Iteration 65, loss = 0.02449568
Iteration 66, loss = 0.02451736
Iteration 67, loss = 0.02433456
Iteration 68, loss = 0.02456149
Iteration 69, loss = 0.02460645
Iteration 70, loss = 0.02433245
Iteration 71, loss = 0.02440870
Iteration 72, loss = 0.02452980
Iteration 73, loss = 0.02441610
Iteration 74, loss = 0.02433259
Iteration 75, loss = 0.02440254
Iteration 76, loss = 0.02423250
Iteration 77, loss = 0.02442886
Iteration 78, loss = 0.02415586
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67458183
Iteration 2, loss = 0.59658289
Iteration 3, loss = 0.48804198
Iteration 4, loss = 0.37884533
Iteration 5, loss = 0.28532406
Iteration 6, loss = 0.21446515
Iteration 7, loss = 0.16432812
Iteration 8, loss = 0.12980065
Iteration 9, loss = 0.10581690
Iteration 10, loss = 0.08874788
Iteration 11, loss = 0.07619381
Iteration 12, loss = 0.06699489
Iteration 13, loss = 0.05987539
Iteration 14, loss = 0.05441918
Iteration 15, loss = 0.05005174
Iteration 16, loss = 0.04644195
Iteration 17, loss = 0.04356628
Iteration 18, loss = 0.04091673
Iteration 19, loss = 0.03912819
Iteration 20, loss = 0.03761520
Iteration 21, loss = 0.03586567
Iteration 22, loss = 0.03472683
Iteration 23, loss = 0.03366141
Iteration 24, loss = 0.03293929
Iteration 25, loss = 0.03195003
Iteration 26, loss = 0.03122351
Iteration 27, loss = 0.03065292
Iteration 28, loss = 0.02995166
Iteration 29, loss = 0.02950193
Iteration 30, loss = 0.02892252
Iteration 31, loss = 0.02870211
Iteration 32, loss = 0.02827129
Iteration 33, loss = 0.02787507
Iteration 34, loss = 0.02765962
Iteration 35, loss = 0.02744493
Iteration 36, loss = 0.02707064
Iteration 37, loss = 0.02679255
Iteration 38, loss = 0.02636060
Iteration 39, loss = 0.02639406
Iteration 40, loss = 0.02605621
Iteration 41, loss = 0.02595233
Iteration 42, loss = 0.02582264
Iteration 43, loss = 0.02584781
Iteration 44, loss = 0.02564311
Iteration 45, loss = 0.02520797
Iteration 46, loss = 0.02517559
Iteration 47, loss = 0.02517388
Iteration 48, loss = 0.02519092
Iteration 49, loss = 0.02496218
Iteration 50, loss = 0.02465390
Iteration 51, loss = 0.02479320
Iteration 52, loss = 0.02480458
Iteration 53, loss = 0.02494053
Iteration 54, loss = 0.02451444
Iteration 55, loss = 0.02450277
Iteration 56, loss = 0.02446760
Iteration 57, loss = 0.02414881
Iteration 58, loss = 0.02428452
Iteration 59, loss = 0.02440671
Iteration 60, loss = 0.02423174
Iteration 61, loss = 0.02409584
Iteration 62, loss = 0.02415820
Iteration 63, loss = 0.02387286
Iteration 64, loss = 0.02371630
Iteration 65, loss = 0.02399029
Iteration 66, loss = 0.02360704
Iteration 67, loss = 0.02354765
Iteration 68, loss = 0.02377275
Iteration 69, loss = 0.02388807
Iteration 70, loss = 0.02400956
Iteration 71, loss = 0.02425911
Iteration 72, loss = 0.02384872
Iteration 73, loss = 0.02360939
Iteration 74, loss = 0.02343776
Iteration 75, loss = 0.02364294
Iteration 76, loss = 0.02340640
Iteration 77, loss = 0.02356072
Iteration 78, loss = 0.02373116
Iteration 79, loss = 0.02349978
Iteration 80, loss = 0.02349426
Iteration 81, loss = 0.02335670
Iteration 82, loss = 0.02322947
Iteration 83, loss = 0.02347720
Iteration 84, loss = 0.02365910
Iteration 85, loss = 0.02335320
Iteration 86, loss = 0.02353345
Iteration 87, loss = 0.02301855
Iteration 88, loss = 0.02365358
Iteration 89, loss = 0.02328099
Iteration 90, loss = 0.02351904
Iteration 91, loss = 0.02327179
Iteration 92, loss = 0.02329953
Iteration 93, loss = 0.02305167
Iteration 94, loss = 0.02310102
Iteration 95, loss = 0.02342851
Iteration 96, loss = 0.02319018
Iteration 97, loss = 0.02315272
Iteration 98, loss = 0.02315012
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68284097
Iteration 2, loss = 0.61820352
Iteration 3, loss = 0.51351853
Iteration 4, loss = 0.39858386
Iteration 5, loss = 0.29959230
Iteration 6, loss = 0.22463388
Iteration 7, loss = 0.17175407
Iteration 8, loss = 0.13514591
Iteration 9, loss = 0.10993029
Iteration 10, loss = 0.09221291
Iteration 11, loss = 0.07925527
Iteration 12, loss = 0.06942862
Iteration 13, loss = 0.06204128
Iteration 14, loss = 0.05642974
Iteration 15, loss = 0.05195843
Iteration 16, loss = 0.04834929
Iteration 17, loss = 0.04533728
Iteration 18, loss = 0.04266816
Iteration 19, loss = 0.04086629
Iteration 20, loss = 0.03881111
Iteration 21, loss = 0.03759809
Iteration 22, loss = 0.03637813
Iteration 23, loss = 0.03534829
Iteration 24, loss = 0.03443763
Iteration 25, loss = 0.03332649
Iteration 26, loss = 0.03272203
Iteration 27, loss = 0.03205188
Iteration 28, loss = 0.03178865
Iteration 29, loss = 0.03107210
Iteration 30, loss = 0.03068128
Iteration 31, loss = 0.03027433
Iteration 32, loss = 0.02969234
Iteration 33, loss = 0.02910561
Iteration 34, loss = 0.02925042
Iteration 35, loss = 0.02866484
Iteration 36, loss = 0.02874179
Iteration 37, loss = 0.02828291
Iteration 38, loss = 0.02820872
Iteration 39, loss = 0.02808329
Iteration 40, loss = 0.02782100
Iteration 41, loss = 0.02780462
Iteration 42, loss = 0.02734749
Iteration 43, loss = 0.02727761
Iteration 44, loss = 0.02721605
Iteration 45, loss = 0.02711727
Iteration 46, loss = 0.02696144
Iteration 47, loss = 0.02663952
Iteration 48, loss = 0.02670605
Iteration 49, loss = 0.02680777
Iteration 50, loss = 0.02655798
Iteration 51, loss = 0.02629357
Iteration 52, loss = 0.02632008
Iteration 53, loss = 0.02631975
Iteration 54, loss = 0.02646662
Iteration 55, loss = 0.02617318
Iteration 56, loss = 0.02619271
Iteration 57, loss = 0.02586951
Iteration 58, loss = 0.02601265
Iteration 59, loss = 0.02615067
Iteration 60, loss = 0.02575575
Iteration 61, loss = 0.02594693
Iteration 62, loss = 0.02546084
Iteration 63, loss = 0.02559757
Iteration 64, loss = 0.02548247
Iteration 65, loss = 0.02541829
Iteration 66, loss = 0.02535221
Iteration 67, loss = 0.02564259
Iteration 68, loss = 0.02549236
Iteration 69, loss = 0.02535090
Iteration 70, loss = 0.02555978
Iteration 71, loss = 0.02563311
Iteration 72, loss = 0.02552289
Iteration 73, loss = 0.02599612
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70767382
Iteration 2, loss = 0.62854521
Iteration 3, loss = 0.52508624
Iteration 4, loss = 0.40572258
Iteration 5, loss = 0.30249587
Iteration 6, loss = 0.22459017
Iteration 7, loss = 0.17010288
Iteration 8, loss = 0.13306203
Iteration 9, loss = 0.10714775
Iteration 10, loss = 0.08902152
Iteration 11, loss = 0.07623871
Iteration 12, loss = 0.06648958
Iteration 13, loss = 0.05907017
Iteration 14, loss = 0.05332978
Iteration 15, loss = 0.04874520
Iteration 16, loss = 0.04507343
Iteration 17, loss = 0.04205243
Iteration 18, loss = 0.03971263
Iteration 19, loss = 0.03743704
Iteration 20, loss = 0.03570709
Iteration 21, loss = 0.03410925
Iteration 22, loss = 0.03302491
Iteration 23, loss = 0.03177033
Iteration 24, loss = 0.03093691
Iteration 25, loss = 0.02986386
Iteration 26, loss = 0.02916781
Iteration 27, loss = 0.02854674
Iteration 28, loss = 0.02778284
Iteration 29, loss = 0.02769555
Iteration 30, loss = 0.02707239
Iteration 31, loss = 0.02653775
Iteration 32, loss = 0.02620387
Iteration 33, loss = 0.02618886
Iteration 34, loss = 0.02552218
Iteration 35, loss = 0.02531066
Iteration 36, loss = 0.02494826
Iteration 37, loss = 0.02494488
Iteration 38, loss = 0.02447515
Iteration 39, loss = 0.02415549
Iteration 40, loss = 0.02413571
Iteration 41, loss = 0.02395441
Iteration 42, loss = 0.02388012
Iteration 43, loss = 0.02362330
Iteration 44, loss = 0.02338429
Iteration 45, loss = 0.02344135
Iteration 46, loss = 0.02332782
Iteration 47, loss = 0.02325794
Iteration 48, loss = 0.02279099
Iteration 49, loss = 0.02308789
Iteration 50, loss = 0.02286470
Iteration 51, loss = 0.02267164
Iteration 52, loss = 0.02264688
Iteration 53, loss = 0.02263942
Iteration 54, loss = 0.02241762
Iteration 55, loss = 0.02247446
Iteration 56, loss = 0.02238997
Iteration 57, loss = 0.02221066
Iteration 58, loss = 0.02214850
Iteration 59, loss = 0.02227086
Iteration 60, loss = 0.02217720
Iteration 61, loss = 0.02229495
Iteration 62, loss = 0.02202479
Iteration 63, loss = 0.02194925
Iteration 64, loss = 0.02199266
Iteration 65, loss = 0.02187501
Iteration 66, loss = 0.02168042
Iteration 67, loss = 0.02164434
Iteration 68, loss = 0.02181599
Iteration 69, loss = 0.02196218
Iteration 70, loss = 0.02155668
Iteration 71, loss = 0.02165601
Iteration 72, loss = 0.02147934
Iteration 73, loss = 0.02155960
Iteration 74, loss = 0.02171979
Iteration 75, loss = 0.02133377
Iteration 76, loss = 0.02153662
Iteration 77, loss = 0.02157650
Iteration 78, loss = 0.02143194
Iteration 79, loss = 0.02163556
Iteration 80, loss = 0.02128084
Iteration 81, loss = 0.02135407
Iteration 82, loss = 0.02153343
Iteration 83, loss = 0.02141234
Iteration 84, loss = 0.02141263
Iteration 85, loss = 0.02110095
Iteration 86, loss = 0.02116370
Iteration 87, loss = 0.02160462
Iteration 88, loss = 0.02139628
Iteration 89, loss = 0.02132336
Iteration 90, loss = 0.02127973
Iteration 91, loss = 0.02153720
Iteration 92, loss = 0.02140256
Iteration 93, loss = 0.02116272
Iteration 94, loss = 0.02131689
Iteration 95, loss = 0.02158801
Iteration 96, loss = 0.02088427
Iteration 97, loss = 0.02124533
Iteration 98, loss = 0.02106416
Iteration 99, loss = 0.02126638
Iteration 100, loss = 0.02078848
Iteration 101, loss = 0.02118827
Iteration 102, loss = 0.02124836
Iteration 103, loss = 0.02120623
Iteration 104, loss = 0.02127716
Iteration 105, loss = 0.02106710
Iteration 106, loss = 0.02121945
Iteration 107, loss = 0.02094391
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67606798
Iteration 2, loss = 0.60260181
Iteration 3, loss = 0.49151418
Iteration 4, loss = 0.37710520
Iteration 5, loss = 0.28150081
Iteration 6, loss = 0.20998031
Iteration 7, loss = 0.15993940
Iteration 8, loss = 0.12570222
Iteration 9, loss = 0.10178590
Iteration 10, loss = 0.08516545
Iteration 11, loss = 0.07276139
Iteration 12, loss = 0.06362407
Iteration 13, loss = 0.05646245
Iteration 14, loss = 0.05099592
Iteration 15, loss = 0.04672130
Iteration 16, loss = 0.04315278
Iteration 17, loss = 0.04011478
Iteration 18, loss = 0.03771175
Iteration 19, loss = 0.03546299
Iteration 20, loss = 0.03399159
Iteration 21, loss = 0.03261036
Iteration 22, loss = 0.03113684
Iteration 23, loss = 0.02987309
Iteration 24, loss = 0.02907104
Iteration 25, loss = 0.02816336
Iteration 26, loss = 0.02759316
Iteration 27, loss = 0.02705105
Iteration 28, loss = 0.02674078
Iteration 29, loss = 0.02588397
Iteration 30, loss = 0.02561297
Iteration 31, loss = 0.02516937
Iteration 32, loss = 0.02471624
Iteration 33, loss = 0.02438494
Iteration 34, loss = 0.02392548
Iteration 35, loss = 0.02368052
Iteration 36, loss = 0.02352882
Iteration 37, loss = 0.02307279
Iteration 38, loss = 0.02287382
Iteration 39, loss = 0.02276094
Iteration 40, loss = 0.02270222
Iteration 41, loss = 0.02253140
Iteration 42, loss = 0.02208432
Iteration 43, loss = 0.02217322
Iteration 44, loss = 0.02245148
Iteration 45, loss = 0.02201443
Iteration 46, loss = 0.02173797
Iteration 47, loss = 0.02156237
Iteration 48, loss = 0.02175912
Iteration 49, loss = 0.02142554
Iteration 50, loss = 0.02145036
Iteration 51, loss = 0.02145148
Iteration 52, loss = 0.02112820
Iteration 53, loss = 0.02113135
Iteration 54, loss = 0.02127871
Iteration 55, loss = 0.02100596
Iteration 56, loss = 0.02111109
Iteration 57, loss = 0.02086029
Iteration 58, loss = 0.02078880
Iteration 59, loss = 0.02075246
Iteration 60, loss = 0.02064356
Iteration 61, loss = 0.02078929
Iteration 62, loss = 0.02077066
Iteration 63, loss = 0.02055649
Iteration 64, loss = 0.02047961
Iteration 65, loss = 0.02061124
Iteration 66, loss = 0.02045450
Iteration 67, loss = 0.02038953
Iteration 68, loss = 0.02056976
Iteration 69, loss = 0.02041519
Iteration 70, loss = 0.02034317
Iteration 71, loss = 0.02017412
Iteration 72, loss = 0.02033357
Iteration 73, loss = 0.02024955
Iteration 74, loss = 0.02000242
Iteration 75, loss = 0.02035123
Iteration 76, loss = 0.02031987
Iteration 77, loss = 0.02005596
Iteration 78, loss = 0.02007625
Iteration 79, loss = 0.02020801
Iteration 80, loss = 0.02026930
Iteration 81, loss = 0.02020915
Iteration 82, loss = 0.02011148
Iteration 83, loss = 0.02024076
Iteration 84, loss = 0.02034902
Iteration 85, loss = 0.02013754
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67629104
Iteration 2, loss = 0.61626883
Iteration 3, loss = 0.51609063
Iteration 4, loss = 0.40605599
Iteration 5, loss = 0.30693492
Iteration 6, loss = 0.22987802
Iteration 7, loss = 0.17494479
Iteration 8, loss = 0.13727984
Iteration 9, loss = 0.11136403
Iteration 10, loss = 0.09305900
Iteration 11, loss = 0.07966311
Iteration 12, loss = 0.06982015
Iteration 13, loss = 0.06231150
Iteration 14, loss = 0.05652933
Iteration 15, loss = 0.05178911
Iteration 16, loss = 0.04802513
Iteration 17, loss = 0.04486734
Iteration 18, loss = 0.04241157
Iteration 19, loss = 0.04021112
Iteration 20, loss = 0.03860239
Iteration 21, loss = 0.03705761
Iteration 22, loss = 0.03589499
Iteration 23, loss = 0.03448625
Iteration 24, loss = 0.03389040
Iteration 25, loss = 0.03279152
Iteration 26, loss = 0.03193778
Iteration 27, loss = 0.03132523
Iteration 28, loss = 0.03068504
Iteration 29, loss = 0.03009859
Iteration 30, loss = 0.02981866
Iteration 31, loss = 0.02928058
Iteration 32, loss = 0.02902302
Iteration 33, loss = 0.02860774
Iteration 34, loss = 0.02836636
Iteration 35, loss = 0.02786138
Iteration 36, loss = 0.02788969
Iteration 37, loss = 0.02739717
Iteration 38, loss = 0.02739715
Iteration 39, loss = 0.02708759
Iteration 40, loss = 0.02680074
Iteration 41, loss = 0.02679215
Iteration 42, loss = 0.02652936
Iteration 43, loss = 0.02655078
Iteration 44, loss = 0.02618876
Iteration 45, loss = 0.02620065
Iteration 46, loss = 0.02591384
Iteration 47, loss = 0.02584227
Iteration 48, loss = 0.02563129
Iteration 49, loss = 0.02551102
Iteration 50, loss = 0.02551638
Iteration 51, loss = 0.02532993
Iteration 52, loss = 0.02542719
Iteration 53, loss = 0.02549008
Iteration 54, loss = 0.02506622
Iteration 55, loss = 0.02515876
Iteration 56, loss = 0.02494717
Iteration 57, loss = 0.02484000
Iteration 58, loss = 0.02482018
Iteration 59, loss = 0.02481256
Iteration 60, loss = 0.02462491
Iteration 61, loss = 0.02479170
Iteration 62, loss = 0.02481111
Iteration 63, loss = 0.02463873
Iteration 64, loss = 0.02452174
Iteration 65, loss = 0.02449568
Iteration 66, loss = 0.02451736
Iteration 67, loss = 0.02433456
Iteration 68, loss = 0.02456149
Iteration 69, loss = 0.02460645
Iteration 70, loss = 0.02433245
Iteration 71, loss = 0.02440870
Iteration 72, loss = 0.02452980
Iteration 73, loss = 0.02441610
Iteration 74, loss = 0.02433259
Iteration 75, loss = 0.02440254
Iteration 76, loss = 0.02423250
Iteration 77, loss = 0.02442886
Iteration 78, loss = 0.02415586
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67458183
Iteration 2, loss = 0.59658289
Iteration 3, loss = 0.48804198
Iteration 4, loss = 0.37884533
Iteration 5, loss = 0.28532406
Iteration 6, loss = 0.21446515
Iteration 7, loss = 0.16432812
Iteration 8, loss = 0.12980065
Iteration 9, loss = 0.10581690
Iteration 10, loss = 0.08874788
Iteration 11, loss = 0.07619381
Iteration 12, loss = 0.06699489
Iteration 13, loss = 0.05987539
Iteration 14, loss = 0.05441918
Iteration 15, loss = 0.05005174
Iteration 16, loss = 0.04644195
Iteration 17, loss = 0.04356628
Iteration 18, loss = 0.04091673
Iteration 19, loss = 0.03912819
Iteration 20, loss = 0.03761520
Iteration 21, loss = 0.03586567
Iteration 22, loss = 0.03472683
Iteration 23, loss = 0.03366141
Iteration 24, loss = 0.03293929
Iteration 25, loss = 0.03195003
Iteration 26, loss = 0.03122351
Iteration 27, loss = 0.03065292
Iteration 28, loss = 0.02995166
Iteration 29, loss = 0.02950193
Iteration 30, loss = 0.02892252
Iteration 31, loss = 0.02870211
Iteration 32, loss = 0.02827129
Iteration 33, loss = 0.02787507
Iteration 34, loss = 0.02765962
Iteration 35, loss = 0.02744493
Iteration 36, loss = 0.02707064
Iteration 37, loss = 0.02679255
Iteration 38, loss = 0.02636060
Iteration 39, loss = 0.02639406
Iteration 40, loss = 0.02605621
Iteration 41, loss = 0.02595233
Iteration 42, loss = 0.02582264
Iteration 43, loss = 0.02584781
Iteration 44, loss = 0.02564311
Iteration 45, loss = 0.02520797
Iteration 46, loss = 0.02517559
Iteration 47, loss = 0.02517388
Iteration 48, loss = 0.02519092
Iteration 49, loss = 0.02496218
Iteration 50, loss = 0.02465390
Iteration 51, loss = 0.02479320
Iteration 52, loss = 0.02480458
Iteration 53, loss = 0.02494053
Iteration 54, loss = 0.02451444
Iteration 55, loss = 0.02450277
Iteration 56, loss = 0.02446760
Iteration 57, loss = 0.02414881
Iteration 58, loss = 0.02428452
Iteration 59, loss = 0.02440671
Iteration 60, loss = 0.02423174
Iteration 61, loss = 0.02409584
Iteration 62, loss = 0.02415820
Iteration 63, loss = 0.02387286
Iteration 64, loss = 0.02371630
Iteration 65, loss = 0.02399029
Iteration 66, loss = 0.02360704
Iteration 67, loss = 0.02354765
Iteration 68, loss = 0.02377275
Iteration 69, loss = 0.02388807
Iteration 70, loss = 0.02400956
Iteration 71, loss = 0.02425911
Iteration 72, loss = 0.02384872
Iteration 73, loss = 0.02360939
Iteration 74, loss = 0.02343776
Iteration 75, loss = 0.02364294
Iteration 76, loss = 0.02340640
Iteration 77, loss = 0.02356072
Iteration 78, loss = 0.02373116
Iteration 79, loss = 0.02349978
Iteration 80, loss = 0.02349426
Iteration 81, loss = 0.02335670
Iteration 82, loss = 0.02322947
Iteration 83, loss = 0.02347720
Iteration 84, loss = 0.02365910
Iteration 85, loss = 0.02335320
Iteration 86, loss = 0.02353345
Iteration 87, loss = 0.02301855
Iteration 88, loss = 0.02365358
Iteration 89, loss = 0.02328099
Iteration 90, loss = 0.02351904
Iteration 91, loss = 0.02327179
Iteration 92, loss = 0.02329953
Iteration 93, loss = 0.02305167
Iteration 94, loss = 0.02310102
Iteration 95, loss = 0.02342851
Iteration 96, loss = 0.02319018
Iteration 97, loss = 0.02315272
Iteration 98, loss = 0.02315012
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68284097
Iteration 2, loss = 0.61820352
Iteration 3, loss = 0.51351853
Iteration 4, loss = 0.39858386
Iteration 5, loss = 0.29959230
Iteration 6, loss = 0.22463388
Iteration 7, loss = 0.17175407
Iteration 8, loss = 0.13514591
Iteration 9, loss = 0.10993029
Iteration 10, loss = 0.09221291
Iteration 11, loss = 0.07925527
Iteration 12, loss = 0.06942862
Iteration 13, loss = 0.06204128
Iteration 14, loss = 0.05642974
Iteration 15, loss = 0.05195843
Iteration 16, loss = 0.04834929
Iteration 17, loss = 0.04533728
Iteration 18, loss = 0.04266816
Iteration 19, loss = 0.04086629
Iteration 20, loss = 0.03881111
Iteration 21, loss = 0.03759809
Iteration 22, loss = 0.03637813
Iteration 23, loss = 0.03534829
Iteration 24, loss = 0.03443763
Iteration 25, loss = 0.03332649
Iteration 26, loss = 0.03272203
Iteration 27, loss = 0.03205188
Iteration 28, loss = 0.03178865
Iteration 29, loss = 0.03107210
Iteration 30, loss = 0.03068128
Iteration 31, loss = 0.03027433
Iteration 32, loss = 0.02969234
Iteration 33, loss = 0.02910561
Iteration 34, loss = 0.02925042
Iteration 35, loss = 0.02866484
Iteration 36, loss = 0.02874179
Iteration 37, loss = 0.02828291
Iteration 38, loss = 0.02820872
Iteration 39, loss = 0.02808329
Iteration 40, loss = 0.02782100
Iteration 41, loss = 0.02780462
Iteration 42, loss = 0.02734749
Iteration 43, loss = 0.02727761
Iteration 44, loss = 0.02721605
Iteration 45, loss = 0.02711727
Iteration 46, loss = 0.02696144
Iteration 47, loss = 0.02663952
Iteration 48, loss = 0.02670605
Iteration 49, loss = 0.02680777
Iteration 50, loss = 0.02655798
Iteration 51, loss = 0.02629357
Iteration 52, loss = 0.02632008
Iteration 53, loss = 0.02631975
Iteration 54, loss = 0.02646662
Iteration 55, loss = 0.02617318
Iteration 56, loss = 0.02619271
Iteration 57, loss = 0.02586951
Iteration 58, loss = 0.02601265
Iteration 59, loss = 0.02615067
Iteration 60, loss = 0.02575575
Iteration 61, loss = 0.02594693
Iteration 62, loss = 0.02546084
Iteration 63, loss = 0.02559757
Iteration 64, loss = 0.02548247
Iteration 65, loss = 0.02541829
Iteration 66, loss = 0.02535221
Iteration 67, loss = 0.02564259
Iteration 68, loss = 0.02549236
Iteration 69, loss = 0.02535090
Iteration 70, loss = 0.02555978
Iteration 71, loss = 0.02563311
Iteration 72, loss = 0.02552289
Iteration 73, loss = 0.02599612
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70767382
Iteration 2, loss = 0.62854521
Iteration 3, loss = 0.52508624
Iteration 4, loss = 0.40572258
Iteration 5, loss = 0.30249587
Iteration 6, loss = 0.22459017
Iteration 7, loss = 0.17010288
Iteration 8, loss = 0.13306203
Iteration 9, loss = 0.10714775
Iteration 10, loss = 0.08902152
Iteration 11, loss = 0.07623871
Iteration 12, loss = 0.06648958
Iteration 13, loss = 0.05907017
Iteration 14, loss = 0.05332978
Iteration 15, loss = 0.04874520
Iteration 16, loss = 0.04507343
Iteration 17, loss = 0.04205243
Iteration 18, loss = 0.03971263
Iteration 19, loss = 0.03743704
Iteration 20, loss = 0.03570709
Iteration 21, loss = 0.03410925
Iteration 22, loss = 0.03302491
Iteration 23, loss = 0.03177033
Iteration 24, loss = 0.03093691
Iteration 25, loss = 0.02986386
Iteration 26, loss = 0.02916781
Iteration 27, loss = 0.02854674
Iteration 28, loss = 0.02778284
Iteration 29, loss = 0.02769555
Iteration 30, loss = 0.02707239
Iteration 31, loss = 0.02653775
Iteration 32, loss = 0.02620387
Iteration 33, loss = 0.02618886
Iteration 34, loss = 0.02552218
Iteration 35, loss = 0.02531066
Iteration 36, loss = 0.02494826
Iteration 37, loss = 0.02494488
Iteration 38, loss = 0.02447515
Iteration 39, loss = 0.02415549
Iteration 40, loss = 0.02413571
Iteration 41, loss = 0.02395441
Iteration 42, loss = 0.02388012
Iteration 43, loss = 0.02362330
Iteration 44, loss = 0.02338429
Iteration 45, loss = 0.02344135
Iteration 46, loss = 0.02332782
Iteration 47, loss = 0.02325794
Iteration 48, loss = 0.02279099
Iteration 49, loss = 0.02308789
Iteration 50, loss = 0.02286470
Iteration 51, loss = 0.02267164
Iteration 52, loss = 0.02264688
Iteration 53, loss = 0.02263942
Iteration 54, loss = 0.02241762
Iteration 55, loss = 0.02247446
Iteration 56, loss = 0.02238997
Iteration 57, loss = 0.02221066
Iteration 58, loss = 0.02214850
Iteration 59, loss = 0.02227086
Iteration 60, loss = 0.02217720
Iteration 61, loss = 0.02229495
Iteration 62, loss = 0.02202479
Iteration 63, loss = 0.02194925
Iteration 64, loss = 0.02199266
Iteration 65, loss = 0.02187501
Iteration 66, loss = 0.02168042
Iteration 67, loss = 0.02164434
Iteration 68, loss = 0.02181599
Iteration 69, loss = 0.02196218
Iteration 70, loss = 0.02155668
Iteration 71, loss = 0.02165601
Iteration 72, loss = 0.02147934
Iteration 73, loss = 0.02155960
Iteration 74, loss = 0.02171979
Iteration 75, loss = 0.02133377
Iteration 76, loss = 0.02153662
Iteration 77, loss = 0.02157650
Iteration 78, loss = 0.02143194
Iteration 79, loss = 0.02163556
Iteration 80, loss = 0.02128084
Iteration 81, loss = 0.02135407
Iteration 82, loss = 0.02153343
Iteration 83, loss = 0.02141234
Iteration 84, loss = 0.02141263
Iteration 85, loss = 0.02110095
Iteration 86, loss = 0.02116370
Iteration 87, loss = 0.02160462
Iteration 88, loss = 0.02139628
Iteration 89, loss = 0.02132336
Iteration 90, loss = 0.02127973
Iteration 91, loss = 0.02153720
Iteration 92, loss = 0.02140256
Iteration 93, loss = 0.02116272
Iteration 94, loss = 0.02131689
Iteration 95, loss = 0.02158801
Iteration 96, loss = 0.02088427
Iteration 97, loss = 0.02124533
Iteration 98, loss = 0.02106416
Iteration 99, loss = 0.02126638
Iteration 100, loss = 0.02078848
Iteration 101, loss = 0.02118827
Iteration 102, loss = 0.02124836
Iteration 103, loss = 0.02120623
Iteration 104, loss = 0.02127716
Iteration 105, loss = 0.02106710
Iteration 106, loss = 0.02121945
Iteration 107, loss = 0.02094391
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67606798
Iteration 2, loss = 0.60260181
Iteration 3, loss = 0.49151418
Iteration 4, loss = 0.37710520
Iteration 5, loss = 0.28150081
Iteration 6, loss = 0.20998031
Iteration 7, loss = 0.15993940
Iteration 8, loss = 0.12570222
Iteration 9, loss = 0.10178590
Iteration 10, loss = 0.08516545
Iteration 11, loss = 0.07276139
Iteration 12, loss = 0.06362407
Iteration 13, loss = 0.05646245
Iteration 14, loss = 0.05099592
Iteration 15, loss = 0.04672130
Iteration 16, loss = 0.04315278
Iteration 17, loss = 0.04011478
Iteration 18, loss = 0.03771175
Iteration 19, loss = 0.03546299
Iteration 20, loss = 0.03399159
Iteration 21, loss = 0.03261036
Iteration 22, loss = 0.03113684
Iteration 23, loss = 0.02987309
Iteration 24, loss = 0.02907104
Iteration 25, loss = 0.02816336
Iteration 26, loss = 0.02759316
Iteration 27, loss = 0.02705105
Iteration 28, loss = 0.02674078
Iteration 29, loss = 0.02588397
Iteration 30, loss = 0.02561297
Iteration 31, loss = 0.02516937
Iteration 32, loss = 0.02471624
Iteration 33, loss = 0.02438494
Iteration 34, loss = 0.02392548
Iteration 35, loss = 0.02368052
Iteration 36, loss = 0.02352882
Iteration 37, loss = 0.02307279
Iteration 38, loss = 0.02287382
Iteration 39, loss = 0.02276094
Iteration 40, loss = 0.02270222
Iteration 41, loss = 0.02253140
Iteration 42, loss = 0.02208432
Iteration 43, loss = 0.02217322
Iteration 44, loss = 0.02245148
Iteration 45, loss = 0.02201443
Iteration 46, loss = 0.02173797
Iteration 47, loss = 0.02156237
Iteration 48, loss = 0.02175912
Iteration 49, loss = 0.02142554
Iteration 50, loss = 0.02145036
Iteration 51, loss = 0.02145148
Iteration 52, loss = 0.02112820
Iteration 53, loss = 0.02113135
Iteration 54, loss = 0.02127871
Iteration 55, loss = 0.02100596
Iteration 56, loss = 0.02111109
Iteration 57, loss = 0.02086029
Iteration 58, loss = 0.02078880
Iteration 59, loss = 0.02075246
Iteration 60, loss = 0.02064356
Iteration 61, loss = 0.02078929
Iteration 62, loss = 0.02077066
Iteration 63, loss = 0.02055649
Iteration 64, loss = 0.02047961
Iteration 65, loss = 0.02061124
Iteration 66, loss = 0.02045450
Iteration 67, loss = 0.02038953
Iteration 68, loss = 0.02056976
Iteration 69, loss = 0.02041519
Iteration 70, loss = 0.02034317
Iteration 71, loss = 0.02017412
Iteration 72, loss = 0.02033357
Iteration 73, loss = 0.02024955
Iteration 74, loss = 0.02000242
Iteration 75, loss = 0.02035123
Iteration 76, loss = 0.02031987
Iteration 77, loss = 0.02005596
Iteration 78, loss = 0.02007625
Iteration 79, loss = 0.02020801
Iteration 80, loss = 0.02026930
Iteration 81, loss = 0.02020915
Iteration 82, loss = 0.02011148
Iteration 83, loss = 0.02024076
Iteration 84, loss = 0.02034902
Iteration 85, loss = 0.02013754
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67629104
Iteration 2, loss = 0.61626883
Iteration 3, loss = 0.51609063
Iteration 4, loss = 0.40605599
Iteration 5, loss = 0.30693492
Iteration 6, loss = 0.22987802
Iteration 7, loss = 0.17494479
Iteration 8, loss = 0.13727984
Iteration 9, loss = 0.11136403
Iteration 10, loss = 0.09305900
Iteration 11, loss = 0.07966311
Iteration 12, loss = 0.06982015
Iteration 13, loss = 0.06231150
Iteration 14, loss = 0.05652933
Iteration 15, loss = 0.05178911
Iteration 16, loss = 0.04802513
Iteration 17, loss = 0.04486734
Iteration 18, loss = 0.04241157
Iteration 19, loss = 0.04021112
Iteration 20, loss = 0.03860239
Iteration 21, loss = 0.03705761
Iteration 22, loss = 0.03589499
Iteration 23, loss = 0.03448625
Iteration 24, loss = 0.03389040
Iteration 25, loss = 0.03279152
Iteration 26, loss = 0.03193778
Iteration 27, loss = 0.03132523
Iteration 28, loss = 0.03068504
Iteration 29, loss = 0.03009859
Iteration 30, loss = 0.02981866
Iteration 31, loss = 0.02928058
Iteration 32, loss = 0.02902302
Iteration 33, loss = 0.02860774
Iteration 34, loss = 0.02836636
Iteration 35, loss = 0.02786138
Iteration 36, loss = 0.02788969
Iteration 37, loss = 0.02739717
Iteration 38, loss = 0.02739715
Iteration 39, loss = 0.02708759
Iteration 40, loss = 0.02680074
Iteration 41, loss = 0.02679215
Iteration 42, loss = 0.02652936
Iteration 43, loss = 0.02655078
Iteration 44, loss = 0.02618876
Iteration 45, loss = 0.02620065
Iteration 46, loss = 0.02591384
Iteration 47, loss = 0.02584227
Iteration 48, loss = 0.02563129
Iteration 49, loss = 0.02551102
Iteration 50, loss = 0.02551638
Iteration 51, loss = 0.02532993
Iteration 52, loss = 0.02542719
Iteration 53, loss = 0.02549008
Iteration 54, loss = 0.02506622
Iteration 55, loss = 0.02515876
Iteration 56, loss = 0.02494717
Iteration 57, loss = 0.02484000
Iteration 58, loss = 0.02482018
Iteration 59, loss = 0.02481256
Iteration 60, loss = 0.02462491
Iteration 61, loss = 0.02479170
Iteration 62, loss = 0.02481111
Iteration 63, loss = 0.02463873
Iteration 64, loss = 0.02452174
Iteration 65, loss = 0.02449568
Iteration 66, loss = 0.02451736
Iteration 67, loss = 0.02433456
Iteration 68, loss = 0.02456149
Iteration 69, loss = 0.02460645
Iteration 70, loss = 0.02433245
Iteration 71, loss = 0.02440870
Iteration 72, loss = 0.02452980
Iteration 73, loss = 0.02441610
Iteration 74, loss = 0.02433259
Iteration 75, loss = 0.02440254
Iteration 76, loss = 0.02423250
Iteration 77, loss = 0.02442886
Iteration 78, loss = 0.02415586
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67458183
Iteration 2, loss = 0.59658289
Iteration 3, loss = 0.48804198
Iteration 4, loss = 0.37884533
Iteration 5, loss = 0.28532406
Iteration 6, loss = 0.21446515
Iteration 7, loss = 0.16432812
Iteration 8, loss = 0.12980065
Iteration 9, loss = 0.10581690
Iteration 10, loss = 0.08874788
Iteration 11, loss = 0.07619381
Iteration 12, loss = 0.06699489
Iteration 13, loss = 0.05987539
Iteration 14, loss = 0.05441918
Iteration 15, loss = 0.05005174
Iteration 16, loss = 0.04644195
Iteration 17, loss = 0.04356628
Iteration 18, loss = 0.04091673
Iteration 19, loss = 0.03912819
Iteration 20, loss = 0.03761520
Iteration 21, loss = 0.03586567
Iteration 22, loss = 0.03472683
Iteration 23, loss = 0.03366141
Iteration 24, loss = 0.03293929
Iteration 25, loss = 0.03195003
Iteration 26, loss = 0.03122351
Iteration 27, loss = 0.03065292
Iteration 28, loss = 0.02995166
Iteration 29, loss = 0.02950193
Iteration 30, loss = 0.02892252
Iteration 31, loss = 0.02870211
Iteration 32, loss = 0.02827129
Iteration 33, loss = 0.02787507
Iteration 34, loss = 0.02765962
Iteration 35, loss = 0.02744493
Iteration 36, loss = 0.02707064
Iteration 37, loss = 0.02679255
Iteration 38, loss = 0.02636060
Iteration 39, loss = 0.02639406
Iteration 40, loss = 0.02605621
Iteration 41, loss = 0.02595233
Iteration 42, loss = 0.02582264
Iteration 43, loss = 0.02584781
Iteration 44, loss = 0.02564311
Iteration 45, loss = 0.02520797
Iteration 46, loss = 0.02517559
Iteration 47, loss = 0.02517388
Iteration 48, loss = 0.02519092
Iteration 49, loss = 0.02496218
Iteration 50, loss = 0.02465390
Iteration 51, loss = 0.02479320
Iteration 52, loss = 0.02480458
Iteration 53, loss = 0.02494053
Iteration 54, loss = 0.02451444
Iteration 55, loss = 0.02450277
Iteration 56, loss = 0.02446760
Iteration 57, loss = 0.02414881
Iteration 58, loss = 0.02428452
Iteration 59, loss = 0.02440671
Iteration 60, loss = 0.02423174
Iteration 61, loss = 0.02409584
Iteration 62, loss = 0.02415820
Iteration 63, loss = 0.02387286
Iteration 64, loss = 0.02371630
Iteration 65, loss = 0.02399029
Iteration 66, loss = 0.02360704
Iteration 67, loss = 0.02354765
Iteration 68, loss = 0.02377275
Iteration 69, loss = 0.02388807
Iteration 70, loss = 0.02400956
Iteration 71, loss = 0.02425911
Iteration 72, loss = 0.02384872
Iteration 73, loss = 0.02360939
Iteration 74, loss = 0.02343776
Iteration 75, loss = 0.02364294
Iteration 76, loss = 0.02340640
Iteration 77, loss = 0.02356072
Iteration 78, loss = 0.02373116
Iteration 79, loss = 0.02349978
Iteration 80, loss = 0.02349426
Iteration 81, loss = 0.02335670
Iteration 82, loss = 0.02322947
Iteration 83, loss = 0.02347720
Iteration 84, loss = 0.02365910
Iteration 85, loss = 0.02335320
Iteration 86, loss = 0.02353345
Iteration 87, loss = 0.02301855
Iteration 88, loss = 0.02365358
Iteration 89, loss = 0.02328099
Iteration 90, loss = 0.02351904
Iteration 91, loss = 0.02327179
Iteration 92, loss = 0.02329953
Iteration 93, loss = 0.02305167
Iteration 94, loss = 0.02310102
Iteration 95, loss = 0.02342851
Iteration 96, loss = 0.02319018
Iteration 97, loss = 0.02315272
Iteration 98, loss = 0.02315012
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68284097
Iteration 2, loss = 0.61820352
Iteration 3, loss = 0.51351853
Iteration 4, loss = 0.39858386
Iteration 5, loss = 0.29959230
Iteration 6, loss = 0.22463388
Iteration 7, loss = 0.17175407
Iteration 8, loss = 0.13514591
Iteration 9, loss = 0.10993029
Iteration 10, loss = 0.09221291
Iteration 11, loss = 0.07925527
Iteration 12, loss = 0.06942862
Iteration 13, loss = 0.06204128
Iteration 14, loss = 0.05642974
Iteration 15, loss = 0.05195843
Iteration 16, loss = 0.04834929
Iteration 17, loss = 0.04533728
Iteration 18, loss = 0.04266816
Iteration 19, loss = 0.04086629
Iteration 20, loss = 0.03881111
Iteration 21, loss = 0.03759809
Iteration 22, loss = 0.03637813
Iteration 23, loss = 0.03534829
Iteration 24, loss = 0.03443763
Iteration 25, loss = 0.03332649
Iteration 26, loss = 0.03272203
Iteration 27, loss = 0.03205188
Iteration 28, loss = 0.03178865
Iteration 29, loss = 0.03107210
Iteration 30, loss = 0.03068128
Iteration 31, loss = 0.03027433
Iteration 32, loss = 0.02969234
Iteration 33, loss = 0.02910561
Iteration 34, loss = 0.02925042
Iteration 35, loss = 0.02866484
Iteration 36, loss = 0.02874179
Iteration 37, loss = 0.02828291
Iteration 38, loss = 0.02820872
Iteration 39, loss = 0.02808329
Iteration 40, loss = 0.02782100
Iteration 41, loss = 0.02780462
Iteration 42, loss = 0.02734749
Iteration 43, loss = 0.02727761
Iteration 44, loss = 0.02721605
Iteration 45, loss = 0.02711727
Iteration 46, loss = 0.02696144
Iteration 47, loss = 0.02663952
Iteration 48, loss = 0.02670605
Iteration 49, loss = 0.02680777
Iteration 50, loss = 0.02655798
Iteration 51, loss = 0.02629357
Iteration 52, loss = 0.02632008
Iteration 53, loss = 0.02631975
Iteration 54, loss = 0.02646662
Iteration 55, loss = 0.02617318
Iteration 56, loss = 0.02619271
Iteration 57, loss = 0.02586951
Iteration 58, loss = 0.02601265
Iteration 59, loss = 0.02615067
Iteration 60, loss = 0.02575575
Iteration 61, loss = 0.02594693
Iteration 62, loss = 0.02546084
Iteration 63, loss = 0.02559757
Iteration 64, loss = 0.02548247
Iteration 65, loss = 0.02541829
Iteration 66, loss = 0.02535221
Iteration 67, loss = 0.02564259
Iteration 68, loss = 0.02549236
Iteration 69, loss = 0.02535090
Iteration 70, loss = 0.02555978
Iteration 71, loss = 0.02563311
Iteration 72, loss = 0.02552289
Iteration 73, loss = 0.02599612
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70767382
Iteration 2, loss = 0.62854521
Iteration 3, loss = 0.52508624
Iteration 4, loss = 0.40572258
Iteration 5, loss = 0.30249587
Iteration 6, loss = 0.22459017
Iteration 7, loss = 0.17010288
Iteration 8, loss = 0.13306203
Iteration 9, loss = 0.10714775
Iteration 10, loss = 0.08902152
Iteration 11, loss = 0.07623871
Iteration 12, loss = 0.06648958
Iteration 13, loss = 0.05907017
Iteration 14, loss = 0.05332978
Iteration 15, loss = 0.04874520
Iteration 16, loss = 0.04507343
Iteration 17, loss = 0.04205243
Iteration 18, loss = 0.03971263
Iteration 19, loss = 0.03743704
Iteration 20, loss = 0.03570709
Iteration 21, loss = 0.03410925
Iteration 22, loss = 0.03302491
Iteration 23, loss = 0.03177033
Iteration 24, loss = 0.03093691
Iteration 25, loss = 0.02986386
Iteration 26, loss = 0.02916781
Iteration 27, loss = 0.02854674
Iteration 28, loss = 0.02778284
Iteration 29, loss = 0.02769555
Iteration 30, loss = 0.02707239
Iteration 31, loss = 0.02653775
Iteration 32, loss = 0.02620387
Iteration 33, loss = 0.02618886
Iteration 34, loss = 0.02552218
Iteration 35, loss = 0.02531066
Iteration 36, loss = 0.02494826
Iteration 37, loss = 0.02494488
Iteration 38, loss = 0.02447515
Iteration 39, loss = 0.02415549
Iteration 40, loss = 0.02413571
Iteration 41, loss = 0.02395441
Iteration 42, loss = 0.02388012
Iteration 43, loss = 0.02362330
Iteration 44, loss = 0.02338429
Iteration 45, loss = 0.02344135
Iteration 46, loss = 0.02332782
Iteration 47, loss = 0.02325794
Iteration 48, loss = 0.02279099
Iteration 49, loss = 0.02308789
Iteration 50, loss = 0.02286470
Iteration 51, loss = 0.02267164
Iteration 52, loss = 0.02264688
Iteration 53, loss = 0.02263942
Iteration 54, loss = 0.02241762
Iteration 55, loss = 0.02247446
Iteration 56, loss = 0.02238997
Iteration 57, loss = 0.02221066
Iteration 58, loss = 0.02214850
Iteration 59, loss = 0.02227086
Iteration 60, loss = 0.02217720
Iteration 61, loss = 0.02229495
Iteration 62, loss = 0.02202479
Iteration 63, loss = 0.02194925
Iteration 64, loss = 0.02199266
Iteration 65, loss = 0.02187501
Iteration 66, loss = 0.02168042
Iteration 67, loss = 0.02164434
Iteration 68, loss = 0.02181599
Iteration 69, loss = 0.02196218
Iteration 70, loss = 0.02155668
Iteration 71, loss = 0.02165601
Iteration 72, loss = 0.02147934
Iteration 73, loss = 0.02155960
Iteration 74, loss = 0.02171979
Iteration 75, loss = 0.02133377
Iteration 76, loss = 0.02153662
Iteration 77, loss = 0.02157650
Iteration 78, loss = 0.02143194
Iteration 79, loss = 0.02163556
Iteration 80, loss = 0.02128084
Iteration 81, loss = 0.02135407
Iteration 82, loss = 0.02153343
Iteration 83, loss = 0.02141234
Iteration 84, loss = 0.02141263
Iteration 85, loss = 0.02110095
Iteration 86, loss = 0.02116370
Iteration 87, loss = 0.02160462
Iteration 88, loss = 0.02139628
Iteration 89, loss = 0.02132336
Iteration 90, loss = 0.02127973
Iteration 91, loss = 0.02153720
Iteration 92, loss = 0.02140256
Iteration 93, loss = 0.02116272
Iteration 94, loss = 0.02131689
Iteration 95, loss = 0.02158801
Iteration 96, loss = 0.02088427
Iteration 97, loss = 0.02124533
Iteration 98, loss = 0.02106416
Iteration 99, loss = 0.02126638
Iteration 100, loss = 0.02078848
Iteration 101, loss = 0.02118827
Iteration 102, loss = 0.02124836
Iteration 103, loss = 0.02120623
Iteration 104, loss = 0.02127716
Iteration 105, loss = 0.02106710
Iteration 106, loss = 0.02121945
Iteration 107, loss = 0.02094391
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67606798
Iteration 2, loss = 0.60260181
Iteration 3, loss = 0.49151418
Iteration 4, loss = 0.37710520
Iteration 5, loss = 0.28150081
Iteration 6, loss = 0.20998031
Iteration 7, loss = 0.15993940
Iteration 8, loss = 0.12570222
Iteration 9, loss = 0.10178590
Iteration 10, loss = 0.08516545
Iteration 11, loss = 0.07276139
Iteration 12, loss = 0.06362407
Iteration 13, loss = 0.05646245
Iteration 14, loss = 0.05099592
Iteration 15, loss = 0.04672130
Iteration 16, loss = 0.04315278
Iteration 17, loss = 0.04011478
Iteration 18, loss = 0.03771175
Iteration 19, loss = 0.03546299
Iteration 20, loss = 0.03399159
Iteration 21, loss = 0.03261036
Iteration 22, loss = 0.03113684
Iteration 23, loss = 0.02987309
Iteration 24, loss = 0.02907104
Iteration 25, loss = 0.02816336
Iteration 26, loss = 0.02759316
Iteration 27, loss = 0.02705105
Iteration 28, loss = 0.02674078
Iteration 29, loss = 0.02588397
Iteration 30, loss = 0.02561297
Iteration 31, loss = 0.02516937
Iteration 32, loss = 0.02471624
Iteration 33, loss = 0.02438494
Iteration 34, loss = 0.02392548
Iteration 35, loss = 0.02368052
Iteration 36, loss = 0.02352882
Iteration 37, loss = 0.02307279
Iteration 38, loss = 0.02287382
Iteration 39, loss = 0.02276094
Iteration 40, loss = 0.02270222
Iteration 41, loss = 0.02253140
Iteration 42, loss = 0.02208432
Iteration 43, loss = 0.02217322
Iteration 44, loss = 0.02245148
Iteration 45, loss = 0.02201443
Iteration 46, loss = 0.02173797
Iteration 47, loss = 0.02156237
Iteration 48, loss = 0.02175912
Iteration 49, loss = 0.02142554
Iteration 50, loss = 0.02145036
Iteration 51, loss = 0.02145148
Iteration 52, loss = 0.02112820
Iteration 53, loss = 0.02113135
Iteration 54, loss = 0.02127871
Iteration 55, loss = 0.02100596
Iteration 56, loss = 0.02111109
Iteration 57, loss = 0.02086029
Iteration 58, loss = 0.02078880
Iteration 59, loss = 0.02075246
Iteration 60, loss = 0.02064356
Iteration 61, loss = 0.02078929
Iteration 62, loss = 0.02077066
Iteration 63, loss = 0.02055649
Iteration 64, loss = 0.02047961
Iteration 65, loss = 0.02061124
Iteration 66, loss = 0.02045450
Iteration 67, loss = 0.02038953
Iteration 68, loss = 0.02056976
Iteration 69, loss = 0.02041519
Iteration 70, loss = 0.02034317
Iteration 71, loss = 0.02017412
Iteration 72, loss = 0.02033357
Iteration 73, loss = 0.02024955
Iteration 74, loss = 0.02000242
Iteration 75, loss = 0.02035123
Iteration 76, loss = 0.02031987
Iteration 77, loss = 0.02005596
Iteration 78, loss = 0.02007625
Iteration 79, loss = 0.02020801
Iteration 80, loss = 0.02026930
Iteration 81, loss = 0.02020915
Iteration 82, loss = 0.02011148
Iteration 83, loss = 0.02024076
Iteration 84, loss = 0.02034902
Iteration 85, loss = 0.02013754
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67629104
Iteration 2, loss = 0.61626883
Iteration 3, loss = 0.51609063
Iteration 4, loss = 0.40605599
Iteration 5, loss = 0.30693492
Iteration 6, loss = 0.22987802
Iteration 7, loss = 0.17494479
Iteration 8, loss = 0.13727984
Iteration 9, loss = 0.11136403
Iteration 10, loss = 0.09305900
Iteration 11, loss = 0.07966311
Iteration 12, loss = 0.06982015
Iteration 13, loss = 0.06231150
Iteration 14, loss = 0.05652933
Iteration 15, loss = 0.05178911
Iteration 16, loss = 0.04802513
Iteration 17, loss = 0.04486734
Iteration 18, loss = 0.04241157
Iteration 19, loss = 0.04021112
Iteration 20, loss = 0.03860239
Iteration 21, loss = 0.03705761
Iteration 22, loss = 0.03589499
Iteration 23, loss = 0.03448625
Iteration 24, loss = 0.03389040
Iteration 25, loss = 0.03279152
Iteration 26, loss = 0.03193778
Iteration 27, loss = 0.03132523
Iteration 28, loss = 0.03068504
Iteration 29, loss = 0.03009859
Iteration 30, loss = 0.02981866
Iteration 31, loss = 0.02928058
Iteration 32, loss = 0.02902302
Iteration 33, loss = 0.02860774
Iteration 34, loss = 0.02836636
Iteration 35, loss = 0.02786138
Iteration 36, loss = 0.02788969
Iteration 37, loss = 0.02739717
Iteration 38, loss = 0.02739715
Iteration 39, loss = 0.02708759
Iteration 40, loss = 0.02680074
Iteration 41, loss = 0.02679215
Iteration 42, loss = 0.02652936
Iteration 43, loss = 0.02655078
Iteration 44, loss = 0.02618876
Iteration 45, loss = 0.02620065
Iteration 46, loss = 0.02591384
Iteration 47, loss = 0.02584227
Iteration 48, loss = 0.02563129
Iteration 49, loss = 0.02551102
Iteration 50, loss = 0.02551638
Iteration 51, loss = 0.02532993
Iteration 52, loss = 0.02542719
Iteration 53, loss = 0.02549008
Iteration 54, loss = 0.02506622
Iteration 55, loss = 0.02515876
Iteration 56, loss = 0.02494717
Iteration 57, loss = 0.02484000
Iteration 58, loss = 0.02482018
Iteration 59, loss = 0.02481256
Iteration 60, loss = 0.02462491
Iteration 61, loss = 0.02479170
Iteration 62, loss = 0.02481111
Iteration 63, loss = 0.02463873
Iteration 64, loss = 0.02452174
Iteration 65, loss = 0.02449568
Iteration 66, loss = 0.02451736
Iteration 67, loss = 0.02433456
Iteration 68, loss = 0.02456149
Iteration 69, loss = 0.02460645
Iteration 70, loss = 0.02433245
Iteration 71, loss = 0.02440870
Iteration 72, loss = 0.02452980
Iteration 73, loss = 0.02441610
Iteration 74, loss = 0.02433259
Iteration 75, loss = 0.02440254
Iteration 76, loss = 0.02423250
Iteration 77, loss = 0.02442886
Iteration 78, loss = 0.02415586
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67458183
Iteration 2, loss = 0.59658289
Iteration 3, loss = 0.48804198
Iteration 4, loss = 0.37884533
Iteration 5, loss = 0.28532406
Iteration 6, loss = 0.21446515
Iteration 7, loss = 0.16432812
Iteration 8, loss = 0.12980065
Iteration 9, loss = 0.10581690
Iteration 10, loss = 0.08874788
Iteration 11, loss = 0.07619381
Iteration 12, loss = 0.06699489
Iteration 13, loss = 0.05987539
Iteration 14, loss = 0.05441918
Iteration 15, loss = 0.05005174
Iteration 16, loss = 0.04644195
Iteration 17, loss = 0.04356628
Iteration 18, loss = 0.04091673
Iteration 19, loss = 0.03912819
Iteration 20, loss = 0.03761520
Iteration 21, loss = 0.03586567
Iteration 22, loss = 0.03472683
Iteration 23, loss = 0.03366141
Iteration 24, loss = 0.03293929
Iteration 25, loss = 0.03195003
Iteration 26, loss = 0.03122351
Iteration 27, loss = 0.03065292
Iteration 28, loss = 0.02995166
Iteration 29, loss = 0.02950193
Iteration 30, loss = 0.02892252
Iteration 31, loss = 0.02870211
Iteration 32, loss = 0.02827129
Iteration 33, loss = 0.02787507
Iteration 34, loss = 0.02765962
Iteration 35, loss = 0.02744493
Iteration 36, loss = 0.02707064
Iteration 37, loss = 0.02679255
Iteration 38, loss = 0.02636060
Iteration 39, loss = 0.02639406
Iteration 40, loss = 0.02605621
Iteration 41, loss = 0.02595233
Iteration 42, loss = 0.02582264
Iteration 43, loss = 0.02584781
Iteration 44, loss = 0.02564311
Iteration 45, loss = 0.02520797
Iteration 46, loss = 0.02517559
Iteration 47, loss = 0.02517388
Iteration 48, loss = 0.02519092
Iteration 49, loss = 0.02496218
Iteration 50, loss = 0.02465390
Iteration 51, loss = 0.02479320
Iteration 52, loss = 0.02480458
Iteration 53, loss = 0.02494053
Iteration 54, loss = 0.02451444
Iteration 55, loss = 0.02450277
Iteration 56, loss = 0.02446760
Iteration 57, loss = 0.02414881
Iteration 58, loss = 0.02428452
Iteration 59, loss = 0.02440671
Iteration 60, loss = 0.02423174
Iteration 61, loss = 0.02409584
Iteration 62, loss = 0.02415820
Iteration 63, loss = 0.02387286
Iteration 64, loss = 0.02371630
Iteration 65, loss = 0.02399029
Iteration 66, loss = 0.02360704
Iteration 67, loss = 0.02354765
Iteration 68, loss = 0.02377275
Iteration 69, loss = 0.02388807
Iteration 70, loss = 0.02400956
Iteration 71, loss = 0.02425911
Iteration 72, loss = 0.02384872
Iteration 73, loss = 0.02360939
Iteration 74, loss = 0.02343776
Iteration 75, loss = 0.02364294
Iteration 76, loss = 0.02340640
Iteration 77, loss = 0.02356072
Iteration 78, loss = 0.02373116
Iteration 79, loss = 0.02349978
Iteration 80, loss = 0.02349426
Iteration 81, loss = 0.02335670
Iteration 82, loss = 0.02322947
Iteration 83, loss = 0.02347720
Iteration 84, loss = 0.02365910
Iteration 85, loss = 0.02335320
Iteration 86, loss = 0.02353345
Iteration 87, loss = 0.02301855
Iteration 88, loss = 0.02365358
Iteration 89, loss = 0.02328099
Iteration 90, loss = 0.02351904
Iteration 91, loss = 0.02327179
Iteration 92, loss = 0.02329953
Iteration 93, loss = 0.02305167
Iteration 94, loss = 0.02310102
Iteration 95, loss = 0.02342851
Iteration 96, loss = 0.02319018
Iteration 97, loss = 0.02315272
Iteration 98, loss = 0.02315012
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.66811766
Iteration 2, loss = 0.56103270
Iteration 3, loss = 0.42774270
Iteration 4, loss = 0.31096352
Iteration 5, loss = 0.22398932
Iteration 6, loss = 0.16547047
Iteration 7, loss = 0.12700343
Iteration 8, loss = 0.10167070
Iteration 9, loss = 0.08437438
Iteration 10, loss = 0.07214633
Iteration 11, loss = 0.06332646
Iteration 12, loss = 0.05666436
Iteration 13, loss = 0.05178842
Iteration 14, loss = 0.04791717
Iteration 15, loss = 0.04460095
Iteration 16, loss = 0.04167890
Iteration 17, loss = 0.03987803
Iteration 18, loss = 0.03824780
Iteration 19, loss = 0.03671579
Iteration 20, loss = 0.03527639
Iteration 21, loss = 0.03432240
Iteration 22, loss = 0.03367283
Iteration 23, loss = 0.03301994
Iteration 24, loss = 0.03210182
Iteration 25, loss = 0.03192243
Iteration 26, loss = 0.03127835
Iteration 27, loss = 0.03049719
Iteration 28, loss = 0.03028086
Iteration 29, loss = 0.02969925
Iteration 30, loss = 0.02943126
Iteration 31, loss = 0.02879406
Iteration 32, loss = 0.02885140
Iteration 33, loss = 0.02864602
Iteration 34, loss = 0.02833258
Iteration 35, loss = 0.02843996
Iteration 36, loss = 0.02793585
Iteration 37, loss = 0.02766570
Iteration 38, loss = 0.02734213
Iteration 39, loss = 0.02750442
Iteration 40, loss = 0.02722126
Iteration 41, loss = 0.02748751
Iteration 42, loss = 0.02749626
Iteration 43, loss = 0.02701075
Iteration 44, loss = 0.02696261
Iteration 45, loss = 0.02643291
Iteration 46, loss = 0.02652757
Iteration 47, loss = 0.02665946
Iteration 48, loss = 0.02679239
Iteration 49, loss = 0.02638791
Iteration 50, loss = 0.02636884
Iteration 51, loss = 0.02640243
Iteration 52, loss = 0.02624576
Iteration 53, loss = 0.02641540
Iteration 54, loss = 0.02609986
Iteration 55, loss = 0.02622570
Iteration 56, loss = 0.02605395
Iteration 57, loss = 0.02586442
Iteration 58, loss = 0.02601322
Iteration 59, loss = 0.02592850
Iteration 60, loss = 0.02585087
Iteration 61, loss = 0.02555395
Iteration 62, loss = 0.02604692
Iteration 63, loss = 0.02570857
Iteration 64, loss = 0.02566966
Iteration 65, loss = 0.02600249
Iteration 66, loss = 0.02593140
Iteration 67, loss = 0.02581233
Iteration 68, loss = 0.02561799
Iteration 69, loss = 0.02551661
Iteration 70, loss = 0.02575975
Iteration 71, loss = 0.02543090
Iteration 72, loss = 0.02568128
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on kept_v10_lemma_stem_custom_stopwords...
Iteration 1, loss = 0.70168559
Iteration 2, loss = 0.66234456
Iteration 3, loss = 0.60309839
Iteration 4, loss = 0.52464807
Iteration 5, loss = 0.43953992
Iteration 6, loss = 0.35984505
Iteration 7, loss = 0.29073206
Iteration 8, loss = 0.23446879
Iteration 9, loss = 0.19002103
Iteration 10, loss = 0.15605480
Iteration 11, loss = 0.13013970
Iteration 12, loss = 0.11028009
Iteration 13, loss = 0.09495619
Iteration 14, loss = 0.08279227
Iteration 15, loss = 0.07317251
Iteration 16, loss = 0.06547713
Iteration 17, loss = 0.05911279
Iteration 18, loss = 0.05393674
Iteration 19, loss = 0.04964410
Iteration 20, loss = 0.04599037
Iteration 21, loss = 0.04276656
Iteration 22, loss = 0.04012868
Iteration 23, loss = 0.03789264
Iteration 24, loss = 0.03574193
Iteration 25, loss = 0.03400773
Iteration 26, loss = 0.03252993
Iteration 27, loss = 0.03123682
Iteration 28, loss = 0.02997109
Iteration 29, loss = 0.02880549
Iteration 30, loss = 0.02789742
Iteration 31, loss = 0.02707950
Iteration 32, loss = 0.02639203
Iteration 33, loss = 0.02570131
Iteration 34, loss = 0.02506014
Iteration 35, loss = 0.02448527
Iteration 36, loss = 0.02392406
Iteration 37, loss = 0.02337387
Iteration 38, loss = 0.02279294
Iteration 39, loss = 0.02237404
Iteration 40, loss = 0.02219243
Iteration 41, loss = 0.02177350
Iteration 42, loss = 0.02131728
Iteration 43, loss = 0.02126516
Iteration 44, loss = 0.02086810
Iteration 45, loss = 0.02054865
Iteration 46, loss = 0.02022519
Iteration 47, loss = 0.02006411
Iteration 48, loss = 0.01982371
Iteration 49, loss = 0.01958567
Iteration 50, loss = 0.01940397
Iteration 51, loss = 0.01928356
Iteration 52, loss = 0.01919860
Iteration 53, loss = 0.01905191
Iteration 54, loss = 0.01881968
Iteration 55, loss = 0.01856933
Iteration 56, loss = 0.01865197
Iteration 57, loss = 0.01851888
Iteration 58, loss = 0.01829479
Iteration 59, loss = 0.01815984
Iteration 60, loss = 0.01821063
Iteration 61, loss = 0.01798181
Iteration 62, loss = 0.01799644
Iteration 63, loss = 0.01798970
Iteration 64, loss = 0.01781309
Iteration 65, loss = 0.01781004
Iteration 66, loss = 0.01762128
Iteration 67, loss = 0.01736091
Iteration 68, loss = 0.01734867
Iteration 69, loss = 0.01733000
Iteration 70, loss = 0.01724735
Iteration 71, loss = 0.01740910
Iteration 72, loss = 0.01717405
Iteration 73, loss = 0.01721475
Iteration 74, loss = 0.01720063
Iteration 75, loss = 0.01704808
Iteration 76, loss = 0.01703763
Iteration 77, loss = 0.01686623
Iteration 78, loss = 0.01695285
Iteration 79, loss = 0.01667666
Iteration 80, loss = 0.01672422
Iteration 81, loss = 0.01665543
Iteration 82, loss = 0.01663604
Iteration 83, loss = 0.01663454
Iteration 84, loss = 0.01664789
Iteration 85, loss = 0.01676705
Iteration 86, loss = 0.01639308
Iteration 87, loss = 0.01619228
Iteration 88, loss = 0.01647861
Iteration 89, loss = 0.01631002
Iteration 90, loss = 0.01628471
Iteration 91, loss = 0.01639448
Iteration 92, loss = 0.01642083
Iteration 93, loss = 0.01629172
Iteration 94, loss = 0.01627606
Iteration 95, loss = 0.01626911
Iteration 96, loss = 0.01615319
Iteration 97, loss = 0.01636406
Iteration 98, loss = 0.01630522
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68346424
Iteration 2, loss = 0.65492652
Iteration 3, loss = 0.61758164
Iteration 4, loss = 0.56830257
Iteration 5, loss = 0.51116480
Iteration 6, loss = 0.45189262
Iteration 7, loss = 0.39312542
Iteration 8, loss = 0.33876114
Iteration 9, loss = 0.29020576
Iteration 10, loss = 0.24805905
Iteration 11, loss = 0.21236086
Iteration 12, loss = 0.18248506
Iteration 13, loss = 0.15786567
Iteration 14, loss = 0.13764114
Iteration 15, loss = 0.12090240
Iteration 16, loss = 0.10705165
Iteration 17, loss = 0.09556024
Iteration 18, loss = 0.08599964
Iteration 19, loss = 0.07794497
Iteration 20, loss = 0.07117449
Iteration 21, loss = 0.06523471
Iteration 22, loss = 0.06013773
Iteration 23, loss = 0.05581100
Iteration 24, loss = 0.05207456
Iteration 25, loss = 0.04873750
Iteration 26, loss = 0.04593035
Iteration 27, loss = 0.04333753
Iteration 28, loss = 0.04094927
Iteration 29, loss = 0.03889448
Iteration 30, loss = 0.03709250
Iteration 31, loss = 0.03549147
Iteration 32, loss = 0.03387318
Iteration 33, loss = 0.03263712
Iteration 34, loss = 0.03132210
Iteration 35, loss = 0.03034094
Iteration 36, loss = 0.02924532
Iteration 37, loss = 0.02836730
Iteration 38, loss = 0.02745078
Iteration 39, loss = 0.02669810
Iteration 40, loss = 0.02606019
Iteration 41, loss = 0.02532797
Iteration 42, loss = 0.02463993
Iteration 43, loss = 0.02415662
Iteration 44, loss = 0.02361991
Iteration 45, loss = 0.02323534
Iteration 46, loss = 0.02263950
Iteration 47, loss = 0.02215092
Iteration 48, loss = 0.02183631
Iteration 49, loss = 0.02137216
Iteration 50, loss = 0.02114894
Iteration 51, loss = 0.02069008
Iteration 52, loss = 0.02038512
Iteration 53, loss = 0.02010145
Iteration 54, loss = 0.01993999
Iteration 55, loss = 0.01976495
Iteration 56, loss = 0.01942129
Iteration 57, loss = 0.01919376
Iteration 58, loss = 0.01894294
Iteration 59, loss = 0.01870824
Iteration 60, loss = 0.01842475
Iteration 61, loss = 0.01827302
Iteration 62, loss = 0.01811037
Iteration 63, loss = 0.01789377
Iteration 64, loss = 0.01768969
Iteration 65, loss = 0.01764213
Iteration 66, loss = 0.01758764
Iteration 67, loss = 0.01729000
Iteration 68, loss = 0.01729634
Iteration 69, loss = 0.01707988
Iteration 70, loss = 0.01693586
Iteration 71, loss = 0.01684567
Iteration 72, loss = 0.01662450
Iteration 73, loss = 0.01647514
Iteration 74, loss = 0.01653311
Iteration 75, loss = 0.01637952
Iteration 76, loss = 0.01627412
Iteration 77, loss = 0.01623420
Iteration 78, loss = 0.01606235
Iteration 79, loss = 0.01605810
Iteration 80, loss = 0.01585961
Iteration 81, loss = 0.01576465
Iteration 82, loss = 0.01578228
Iteration 83, loss = 0.01569131
Iteration 84, loss = 0.01574410
Iteration 85, loss = 0.01557993
Iteration 86, loss = 0.01556820
Iteration 87, loss = 0.01543523
Iteration 88, loss = 0.01534993
Iteration 89, loss = 0.01535231
Iteration 90, loss = 0.01517919
Iteration 91, loss = 0.01513375
Iteration 92, loss = 0.01514447
Iteration 93, loss = 0.01502500
Iteration 94, loss = 0.01504079
Iteration 95, loss = 0.01495554
Iteration 96, loss = 0.01500294
Iteration 97, loss = 0.01498830
Iteration 98, loss = 0.01486867
Iteration 99, loss = 0.01475682
Iteration 100, loss = 0.01483073
Iteration 101, loss = 0.01477027
Iteration 102, loss = 0.01469226
Iteration 103, loss = 0.01467532
Iteration 104, loss = 0.01459587
Iteration 105, loss = 0.01455387
Iteration 106, loss = 0.01459560
Iteration 107, loss = 0.01454756
Iteration 108, loss = 0.01448501
Iteration 109, loss = 0.01436872
Iteration 110, loss = 0.01449456
Iteration 111, loss = 0.01428968
Iteration 112, loss = 0.01443029
Iteration 113, loss = 0.01434736
Iteration 114, loss = 0.01432910
Iteration 115, loss = 0.01430946
Iteration 116, loss = 0.01428804
Iteration 117, loss = 0.01431246
Iteration 118, loss = 0.01426185
Iteration 119, loss = 0.01407290
Iteration 120, loss = 0.01413391
Iteration 121, loss = 0.01404446
Iteration 122, loss = 0.01409307
Iteration 123, loss = 0.01412696
Iteration 124, loss = 0.01419520
Iteration 125, loss = 0.01401814
Iteration 126, loss = 0.01395649
Iteration 127, loss = 0.01399273
Iteration 128, loss = 0.01400339
Iteration 129, loss = 0.01398415
Iteration 130, loss = 0.01400918
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68593138
Iteration 2, loss = 0.66372365
Iteration 3, loss = 0.63732332
Iteration 4, loss = 0.60319955
Iteration 5, loss = 0.56120224
Iteration 6, loss = 0.51418376
Iteration 7, loss = 0.46508001
Iteration 8, loss = 0.41653409
Iteration 9, loss = 0.36991678
Iteration 10, loss = 0.32652602
Iteration 11, loss = 0.28721679
Iteration 12, loss = 0.25211440
Iteration 13, loss = 0.22156308
Iteration 14, loss = 0.19504287
Iteration 15, loss = 0.17227819
Iteration 16, loss = 0.15298043
Iteration 17, loss = 0.13652609
Iteration 18, loss = 0.12233957
Iteration 19, loss = 0.11034766
Iteration 20, loss = 0.10020073
Iteration 21, loss = 0.09151514
Iteration 22, loss = 0.08374757
Iteration 23, loss = 0.07720751
Iteration 24, loss = 0.07142024
Iteration 25, loss = 0.06623051
Iteration 26, loss = 0.06176665
Iteration 27, loss = 0.05788565
Iteration 28, loss = 0.05431212
Iteration 29, loss = 0.05112962
Iteration 30, loss = 0.04835980
Iteration 31, loss = 0.04583791
Iteration 32, loss = 0.04368562
Iteration 33, loss = 0.04167559
Iteration 34, loss = 0.03974744
Iteration 35, loss = 0.03811270
Iteration 36, loss = 0.03655663
Iteration 37, loss = 0.03514282
Iteration 38, loss = 0.03374285
Iteration 39, loss = 0.03262545
Iteration 40, loss = 0.03143986
Iteration 41, loss = 0.03041664
Iteration 42, loss = 0.02943253
Iteration 43, loss = 0.02858395
Iteration 44, loss = 0.02779571
Iteration 45, loss = 0.02689212
Iteration 46, loss = 0.02620338
Iteration 47, loss = 0.02560473
Iteration 48, loss = 0.02501903
Iteration 49, loss = 0.02439292
Iteration 50, loss = 0.02389154
Iteration 51, loss = 0.02341261
Iteration 52, loss = 0.02285158
Iteration 53, loss = 0.02230404
Iteration 54, loss = 0.02183994
Iteration 55, loss = 0.02143257
Iteration 56, loss = 0.02107183
Iteration 57, loss = 0.02058910
Iteration 58, loss = 0.02027531
Iteration 59, loss = 0.01993594
Iteration 60, loss = 0.01966659
Iteration 61, loss = 0.01960423
Iteration 62, loss = 0.01912890
Iteration 63, loss = 0.01901175
Iteration 64, loss = 0.01869159
Iteration 65, loss = 0.01836519
Iteration 66, loss = 0.01814753
Iteration 67, loss = 0.01785442
Iteration 68, loss = 0.01764512
Iteration 69, loss = 0.01746860
Iteration 70, loss = 0.01729909
Iteration 71, loss = 0.01707561
Iteration 72, loss = 0.01704514
Iteration 73, loss = 0.01685115
Iteration 74, loss = 0.01651912
Iteration 75, loss = 0.01641886
Iteration 76, loss = 0.01626201
Iteration 77, loss = 0.01609092
Iteration 78, loss = 0.01583809
Iteration 79, loss = 0.01578464
Iteration 80, loss = 0.01569378
Iteration 81, loss = 0.01554594
Iteration 82, loss = 0.01549214
Iteration 83, loss = 0.01523523
Iteration 84, loss = 0.01506543
Iteration 85, loss = 0.01501905
Iteration 86, loss = 0.01518706
Iteration 87, loss = 0.01502236
Iteration 88, loss = 0.01490448
Iteration 89, loss = 0.01478397
Iteration 90, loss = 0.01452279
Iteration 91, loss = 0.01450798
Iteration 92, loss = 0.01447067
Iteration 93, loss = 0.01419539
Iteration 94, loss = 0.01432737
Iteration 95, loss = 0.01424014
Iteration 96, loss = 0.01414641
Iteration 97, loss = 0.01404832
Iteration 98, loss = 0.01394197
Iteration 99, loss = 0.01384631
Iteration 100, loss = 0.01375704
Iteration 101, loss = 0.01367713
Iteration 102, loss = 0.01348368
Iteration 103, loss = 0.01369849
Iteration 104, loss = 0.01358760
Iteration 105, loss = 0.01347336
Iteration 106, loss = 0.01334865
Iteration 107, loss = 0.01332309
Iteration 108, loss = 0.01346446
Iteration 109, loss = 0.01376525
Iteration 110, loss = 0.01412151
Iteration 111, loss = 0.01390081
Iteration 112, loss = 0.01368269
Iteration 113, loss = 0.01349275
Iteration 114, loss = 0.01326107
Iteration 115, loss = 0.01319654
Iteration 116, loss = 0.01286214
Iteration 117, loss = 0.01282060
Iteration 118, loss = 0.01278057
Iteration 119, loss = 0.01262377
Iteration 120, loss = 0.01273322
Iteration 121, loss = 0.01274701
Iteration 122, loss = 0.01274871
Iteration 123, loss = 0.01266587
Iteration 124, loss = 0.01273606
Iteration 125, loss = 0.01267408
Iteration 126, loss = 0.01265844
Iteration 127, loss = 0.01240248
Iteration 128, loss = 0.01247987
Iteration 129, loss = 0.01256634
Iteration 130, loss = 0.01255599
Iteration 131, loss = 0.01256714
Iteration 132, loss = 0.01249324
Iteration 133, loss = 0.01264266
Iteration 134, loss = 0.01248723
Iteration 135, loss = 0.01230355
Iteration 136, loss = 0.01226813
Iteration 137, loss = 0.01212971
Iteration 138, loss = 0.01209962
Iteration 139, loss = 0.01205458
Iteration 140, loss = 0.01188768
Iteration 141, loss = 0.01195315
Iteration 142, loss = 0.01193474
Iteration 143, loss = 0.01206876
Iteration 144, loss = 0.01210605
Iteration 145, loss = 0.01211482
Iteration 146, loss = 0.01198577
Iteration 147, loss = 0.01197008
Iteration 148, loss = 0.01198098
Iteration 149, loss = 0.01187680
Iteration 150, loss = 0.01180052
Iteration 151, loss = 0.01177918
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68558224
Iteration 2, loss = 0.63743427
Iteration 3, loss = 0.56368854
Iteration 4, loss = 0.47408234
Iteration 5, loss = 0.38752416
Iteration 6, loss = 0.31021786
Iteration 7, loss = 0.24696907
Iteration 8, loss = 0.19744007
Iteration 9, loss = 0.15985682
Iteration 10, loss = 0.13206507
Iteration 11, loss = 0.11115368
Iteration 12, loss = 0.09522349
Iteration 13, loss = 0.08298709
Iteration 14, loss = 0.07341401
Iteration 15, loss = 0.06594576
Iteration 16, loss = 0.05978998
Iteration 17, loss = 0.05476489
Iteration 18, loss = 0.05057725
Iteration 19, loss = 0.04722539
Iteration 20, loss = 0.04427864
Iteration 21, loss = 0.04192801
Iteration 22, loss = 0.03961029
Iteration 23, loss = 0.03773549
Iteration 24, loss = 0.03618413
Iteration 25, loss = 0.03474898
Iteration 26, loss = 0.03348807
Iteration 27, loss = 0.03231088
Iteration 28, loss = 0.03137896
Iteration 29, loss = 0.03047307
Iteration 30, loss = 0.02956269
Iteration 31, loss = 0.02882968
Iteration 32, loss = 0.02838489
Iteration 33, loss = 0.02765317
Iteration 34, loss = 0.02692462
Iteration 35, loss = 0.02654195
Iteration 36, loss = 0.02621026
Iteration 37, loss = 0.02582313
Iteration 38, loss = 0.02542683
Iteration 39, loss = 0.02524244
Iteration 40, loss = 0.02491745
Iteration 41, loss = 0.02444299
Iteration 42, loss = 0.02416327
Iteration 43, loss = 0.02406533
Iteration 44, loss = 0.02383184
Iteration 45, loss = 0.02350219
Iteration 46, loss = 0.02356677
Iteration 47, loss = 0.02317575
Iteration 48, loss = 0.02310870
Iteration 49, loss = 0.02267070
Iteration 50, loss = 0.02279629
Iteration 51, loss = 0.02241212
Iteration 52, loss = 0.02229971
Iteration 53, loss = 0.02244550
Iteration 54, loss = 0.02239221
Iteration 55, loss = 0.02208876
Iteration 56, loss = 0.02183146
Iteration 57, loss = 0.02161070
Iteration 58, loss = 0.02147581
Iteration 59, loss = 0.02161833
Iteration 60, loss = 0.02160755
Iteration 61, loss = 0.02141351
Iteration 62, loss = 0.02124448
Iteration 63, loss = 0.02122114
Iteration 64, loss = 0.02098118
Iteration 65, loss = 0.02125063
Iteration 66, loss = 0.02096086
Iteration 67, loss = 0.02101338
Iteration 68, loss = 0.02073465
Iteration 69, loss = 0.02090212
Iteration 70, loss = 0.02076745
Iteration 71, loss = 0.02050901
Iteration 72, loss = 0.02058400
Iteration 73, loss = 0.02045746
Iteration 74, loss = 0.02043790
Iteration 75, loss = 0.02056348
Iteration 76, loss = 0.02045630
Iteration 77, loss = 0.02050281
Iteration 78, loss = 0.02035263
Iteration 79, loss = 0.02031765
Iteration 80, loss = 0.02044358
Iteration 81, loss = 0.02041799
Iteration 82, loss = 0.02029840
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70789045
Iteration 2, loss = 0.66423346
Iteration 3, loss = 0.60355472
Iteration 4, loss = 0.52370803
Iteration 5, loss = 0.43965959
Iteration 6, loss = 0.36013488
Iteration 7, loss = 0.29285743
Iteration 8, loss = 0.23804090
Iteration 9, loss = 0.19519894
Iteration 10, loss = 0.16162008
Iteration 11, loss = 0.13620906
Iteration 12, loss = 0.11642898
Iteration 13, loss = 0.10105889
Iteration 14, loss = 0.08905654
Iteration 15, loss = 0.07923427
Iteration 16, loss = 0.07148853
Iteration 17, loss = 0.06503838
Iteration 18, loss = 0.05967636
Iteration 19, loss = 0.05524795
Iteration 20, loss = 0.05142642
Iteration 21, loss = 0.04823077
Iteration 22, loss = 0.04553759
Iteration 23, loss = 0.04298679
Iteration 24, loss = 0.04097606
Iteration 25, loss = 0.03918552
Iteration 26, loss = 0.03752360
Iteration 27, loss = 0.03616958
Iteration 28, loss = 0.03474291
Iteration 29, loss = 0.03356527
Iteration 30, loss = 0.03259441
Iteration 31, loss = 0.03162942
Iteration 32, loss = 0.03094443
Iteration 33, loss = 0.03014602
Iteration 34, loss = 0.02937201
Iteration 35, loss = 0.02872620
Iteration 36, loss = 0.02820938
Iteration 37, loss = 0.02741966
Iteration 38, loss = 0.02711922
Iteration 39, loss = 0.02660258
Iteration 40, loss = 0.02628248
Iteration 41, loss = 0.02590690
Iteration 42, loss = 0.02559212
Iteration 43, loss = 0.02523077
Iteration 44, loss = 0.02507456
Iteration 45, loss = 0.02461912
Iteration 46, loss = 0.02421187
Iteration 47, loss = 0.02399812
Iteration 48, loss = 0.02384725
Iteration 49, loss = 0.02360398
Iteration 50, loss = 0.02349236
Iteration 51, loss = 0.02324173
Iteration 52, loss = 0.02310164
Iteration 53, loss = 0.02275327
Iteration 54, loss = 0.02265842
Iteration 55, loss = 0.02254111
Iteration 56, loss = 0.02235266
Iteration 57, loss = 0.02217804
Iteration 58, loss = 0.02216229
Iteration 59, loss = 0.02192279
Iteration 60, loss = 0.02188807
Iteration 61, loss = 0.02179702
Iteration 62, loss = 0.02154134
Iteration 63, loss = 0.02154574
Iteration 64, loss = 0.02137825
Iteration 65, loss = 0.02128339
Iteration 66, loss = 0.02125872
Iteration 67, loss = 0.02117620
Iteration 68, loss = 0.02108428
Iteration 69, loss = 0.02086493
Iteration 70, loss = 0.02100148
Iteration 71, loss = 0.02086197
Iteration 72, loss = 0.02098866
Iteration 73, loss = 0.02083848
Iteration 74, loss = 0.02068769
Iteration 75, loss = 0.02063728
Iteration 76, loss = 0.02071796
Iteration 77, loss = 0.02080278
Iteration 78, loss = 0.02061493
Iteration 79, loss = 0.02036316
Iteration 80, loss = 0.02017501
Iteration 81, loss = 0.02046572
Iteration 82, loss = 0.02004913
Iteration 83, loss = 0.02042304
Iteration 84, loss = 0.02022690
Iteration 85, loss = 0.02012526
Iteration 86, loss = 0.02021314
Iteration 87, loss = 0.02000186
Iteration 88, loss = 0.02019779
Iteration 89, loss = 0.01982981
Iteration 90, loss = 0.01985939
Iteration 91, loss = 0.01978509
Iteration 92, loss = 0.01970856
Iteration 93, loss = 0.02001789
Iteration 94, loss = 0.01969573
Iteration 95, loss = 0.01974191
Iteration 96, loss = 0.01964957
Iteration 97, loss = 0.01942231
Iteration 98, loss = 0.01966220
Iteration 99, loss = 0.01942854
Iteration 100, loss = 0.01984376
Iteration 101, loss = 0.01946504
Iteration 102, loss = 0.01971781
Iteration 103, loss = 0.01942949
Iteration 104, loss = 0.01968133
Iteration 105, loss = 0.01973077
Iteration 106, loss = 0.01972352
Iteration 107, loss = 0.01947349
Iteration 108, loss = 0.01949161
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68427843
Iteration 2, loss = 0.63835415
Iteration 3, loss = 0.57434879
Iteration 4, loss = 0.49759643
Iteration 5, loss = 0.41849424
Iteration 6, loss = 0.34555386
Iteration 7, loss = 0.28248912
Iteration 8, loss = 0.23060158
Iteration 9, loss = 0.18967925
Iteration 10, loss = 0.15782139
Iteration 11, loss = 0.13330927
Iteration 12, loss = 0.11437950
Iteration 13, loss = 0.10002361
Iteration 14, loss = 0.08820311
Iteration 15, loss = 0.07892430
Iteration 16, loss = 0.07126824
Iteration 17, loss = 0.06526703
Iteration 18, loss = 0.06011135
Iteration 19, loss = 0.05613415
Iteration 20, loss = 0.05254503
Iteration 21, loss = 0.04942620
Iteration 22, loss = 0.04675730
Iteration 23, loss = 0.04462662
Iteration 24, loss = 0.04272702
Iteration 25, loss = 0.04089203
Iteration 26, loss = 0.03939040
Iteration 27, loss = 0.03800372
Iteration 28, loss = 0.03679481
Iteration 29, loss = 0.03568057
Iteration 30, loss = 0.03469856
Iteration 31, loss = 0.03385237
Iteration 32, loss = 0.03303967
Iteration 33, loss = 0.03245198
Iteration 34, loss = 0.03164532
Iteration 35, loss = 0.03133135
Iteration 36, loss = 0.03057834
Iteration 37, loss = 0.03017648
Iteration 38, loss = 0.02957159
Iteration 39, loss = 0.02915345
Iteration 40, loss = 0.02870207
Iteration 41, loss = 0.02860008
Iteration 42, loss = 0.02808502
Iteration 43, loss = 0.02775168
Iteration 44, loss = 0.02768974
Iteration 45, loss = 0.02737254
Iteration 46, loss = 0.02705143
Iteration 47, loss = 0.02680133
Iteration 48, loss = 0.02652468
Iteration 49, loss = 0.02650285
Iteration 50, loss = 0.02602929
Iteration 51, loss = 0.02627105
Iteration 52, loss = 0.02599846
Iteration 53, loss = 0.02580825
Iteration 54, loss = 0.02549141
Iteration 55, loss = 0.02537496
Iteration 56, loss = 0.02533048
Iteration 57, loss = 0.02533143
Iteration 58, loss = 0.02517566
Iteration 59, loss = 0.02496683
Iteration 60, loss = 0.02483913
Iteration 61, loss = 0.02472266
Iteration 62, loss = 0.02462969
Iteration 63, loss = 0.02446088
Iteration 64, loss = 0.02470993
Iteration 65, loss = 0.02430199
Iteration 66, loss = 0.02419266
Iteration 67, loss = 0.02406364
Iteration 68, loss = 0.02407268
Iteration 69, loss = 0.02385187
Iteration 70, loss = 0.02447115
Iteration 71, loss = 0.02404795
Iteration 72, loss = 0.02396500
Iteration 73, loss = 0.02397478
Iteration 74, loss = 0.02391726
Iteration 75, loss = 0.02394128
Iteration 76, loss = 0.02346456
Iteration 77, loss = 0.02340130
Iteration 78, loss = 0.02348478
Iteration 79, loss = 0.02353433
Iteration 80, loss = 0.02329326
Iteration 81, loss = 0.02348774
Iteration 82, loss = 0.02317875
Iteration 83, loss = 0.02324722
Iteration 84, loss = 0.02315424
Iteration 85, loss = 0.02331021
Iteration 86, loss = 0.02320070
Iteration 87, loss = 0.02306960
Iteration 88, loss = 0.02299696
Iteration 89, loss = 0.02285865
Iteration 90, loss = 0.02312717
Iteration 91, loss = 0.02312594
Iteration 92, loss = 0.02310899
Iteration 93, loss = 0.02323605
Iteration 94, loss = 0.02301765
Iteration 95, loss = 0.02296696
Iteration 96, loss = 0.02282808
Iteration 97, loss = 0.02260934
Iteration 98, loss = 0.02280835
Iteration 99, loss = 0.02299778
Iteration 100, loss = 0.02270972
Iteration 101, loss = 0.02289174
Iteration 102, loss = 0.02271294
Iteration 103, loss = 0.02286404
Iteration 104, loss = 0.02270462
Iteration 105, loss = 0.02292032
Iteration 106, loss = 0.02273727
Iteration 107, loss = 0.02301171
Iteration 108, loss = 0.02267345
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68266180
Iteration 2, loss = 0.65940488
Iteration 3, loss = 0.63165214
Iteration 4, loss = 0.59391831
Iteration 5, loss = 0.54800694
Iteration 6, loss = 0.49823786
Iteration 7, loss = 0.44831630
Iteration 8, loss = 0.39989651
Iteration 9, loss = 0.35467511
Iteration 10, loss = 0.31274308
Iteration 11, loss = 0.27520946
Iteration 12, loss = 0.24184092
Iteration 13, loss = 0.21273700
Iteration 14, loss = 0.18764753
Iteration 15, loss = 0.16619302
Iteration 16, loss = 0.14790919
Iteration 17, loss = 0.13233191
Iteration 18, loss = 0.11906529
Iteration 19, loss = 0.10786842
Iteration 20, loss = 0.09805761
Iteration 21, loss = 0.09005376
Iteration 22, loss = 0.08276710
Iteration 23, loss = 0.07650846
Iteration 24, loss = 0.07095025
Iteration 25, loss = 0.06602903
Iteration 26, loss = 0.06184433
Iteration 27, loss = 0.05819079
Iteration 28, loss = 0.05475216
Iteration 29, loss = 0.05179867
Iteration 30, loss = 0.04906299
Iteration 31, loss = 0.04672772
Iteration 32, loss = 0.04451156
Iteration 33, loss = 0.04273631
Iteration 34, loss = 0.04075157
Iteration 35, loss = 0.03907850
Iteration 36, loss = 0.03754191
Iteration 37, loss = 0.03608300
Iteration 38, loss = 0.03481150
Iteration 39, loss = 0.03359675
Iteration 40, loss = 0.03254247
Iteration 41, loss = 0.03161810
Iteration 42, loss = 0.03067343
Iteration 43, loss = 0.02989942
Iteration 44, loss = 0.02913456
Iteration 45, loss = 0.02841908
Iteration 46, loss = 0.02779119
Iteration 47, loss = 0.02715083
Iteration 48, loss = 0.02650442
Iteration 49, loss = 0.02593252
Iteration 50, loss = 0.02538580
Iteration 51, loss = 0.02484559
Iteration 52, loss = 0.02471667
Iteration 53, loss = 0.02422214
Iteration 54, loss = 0.02371603
Iteration 55, loss = 0.02341477
Iteration 56, loss = 0.02291530
Iteration 57, loss = 0.02249877
Iteration 58, loss = 0.02236040
Iteration 59, loss = 0.02230426
Iteration 60, loss = 0.02187838
Iteration 61, loss = 0.02153464
Iteration 62, loss = 0.02121474
Iteration 63, loss = 0.02078919
Iteration 64, loss = 0.02054848
Iteration 65, loss = 0.02018312
Iteration 66, loss = 0.01995214
Iteration 67, loss = 0.01971408
Iteration 68, loss = 0.01961737
Iteration 69, loss = 0.01927923
Iteration 70, loss = 0.01969709
Iteration 71, loss = 0.01944557
Iteration 72, loss = 0.01914233
Iteration 73, loss = 0.01882970
Iteration 74, loss = 0.01857187
Iteration 75, loss = 0.01841944
Iteration 76, loss = 0.01814979
Iteration 77, loss = 0.01799383
Iteration 78, loss = 0.01781357
Iteration 79, loss = 0.01788628
Iteration 80, loss = 0.01760977
Iteration 81, loss = 0.01771707
Iteration 82, loss = 0.01770166
Iteration 83, loss = 0.01747961
Iteration 84, loss = 0.01749117
Iteration 85, loss = 0.01730669
Iteration 86, loss = 0.01718455
Iteration 87, loss = 0.01704385
Iteration 88, loss = 0.01675038
Iteration 89, loss = 0.01688008
Iteration 90, loss = 0.01683301
Iteration 91, loss = 0.01672247
Iteration 92, loss = 0.01670266
Iteration 93, loss = 0.01678143
Iteration 94, loss = 0.01678743
Iteration 95, loss = 0.01658053
Iteration 96, loss = 0.01630274
Iteration 97, loss = 0.01620777
Iteration 98, loss = 0.01613637
Iteration 99, loss = 0.01614128
Iteration 100, loss = 0.01604233
Iteration 101, loss = 0.01622693
Iteration 102, loss = 0.01611467
Iteration 103, loss = 0.01600870
Iteration 104, loss = 0.01580903
Iteration 105, loss = 0.01572856
Iteration 106, loss = 0.01563897
Iteration 107, loss = 0.01555787
Iteration 108, loss = 0.01541919
Iteration 109, loss = 0.01535010
Iteration 110, loss = 0.01524743
Iteration 111, loss = 0.01536313
Iteration 112, loss = 0.01530976
Iteration 113, loss = 0.01523552
Iteration 114, loss = 0.01517547
Iteration 115, loss = 0.01512808
Iteration 116, loss = 0.01508416
Iteration 117, loss = 0.01502259
Iteration 118, loss = 0.01492944
Iteration 119, loss = 0.01487192
Iteration 120, loss = 0.01506820
Iteration 121, loss = 0.01492511
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67905547
Iteration 2, loss = 0.63970609
Iteration 3, loss = 0.58075560
Iteration 4, loss = 0.50147663
Iteration 5, loss = 0.42017129
Iteration 6, loss = 0.34504830
Iteration 7, loss = 0.28048500
Iteration 8, loss = 0.22790989
Iteration 9, loss = 0.18662221
Iteration 10, loss = 0.15478816
Iteration 11, loss = 0.13064915
Iteration 12, loss = 0.11179329
Iteration 13, loss = 0.09729011
Iteration 14, loss = 0.08562337
Iteration 15, loss = 0.07632172
Iteration 16, loss = 0.06880364
Iteration 17, loss = 0.06276171
Iteration 18, loss = 0.05785086
Iteration 19, loss = 0.05349203
Iteration 20, loss = 0.04999702
Iteration 21, loss = 0.04699129
Iteration 22, loss = 0.04437977
Iteration 23, loss = 0.04207170
Iteration 24, loss = 0.03991035
Iteration 25, loss = 0.03844627
Iteration 26, loss = 0.03679416
Iteration 27, loss = 0.03555770
Iteration 28, loss = 0.03436150
Iteration 29, loss = 0.03322336
Iteration 30, loss = 0.03235681
Iteration 31, loss = 0.03143449
Iteration 32, loss = 0.03059362
Iteration 33, loss = 0.02982672
Iteration 34, loss = 0.02929318
Iteration 35, loss = 0.02875358
Iteration 36, loss = 0.02828777
Iteration 37, loss = 0.02757788
Iteration 38, loss = 0.02728030
Iteration 39, loss = 0.02680145
Iteration 40, loss = 0.02643688
Iteration 41, loss = 0.02617703
Iteration 42, loss = 0.02595627
Iteration 43, loss = 0.02537705
Iteration 44, loss = 0.02502361
Iteration 45, loss = 0.02493905
Iteration 46, loss = 0.02467652
Iteration 47, loss = 0.02431043
Iteration 48, loss = 0.02417128
Iteration 49, loss = 0.02416518
Iteration 50, loss = 0.02389055
Iteration 51, loss = 0.02348286
Iteration 52, loss = 0.02355038
Iteration 53, loss = 0.02344005
Iteration 54, loss = 0.02338863
Iteration 55, loss = 0.02286142
Iteration 56, loss = 0.02281995
Iteration 57, loss = 0.02263031
Iteration 58, loss = 0.02277671
Iteration 59, loss = 0.02255481
Iteration 60, loss = 0.02246328
Iteration 61, loss = 0.02218351
Iteration 62, loss = 0.02218459
Iteration 63, loss = 0.02222825
Iteration 64, loss = 0.02205583
Iteration 65, loss = 0.02192395
Iteration 66, loss = 0.02195102
Iteration 67, loss = 0.02185703
Iteration 68, loss = 0.02168547
Iteration 69, loss = 0.02168071
Iteration 70, loss = 0.02162787
Iteration 71, loss = 0.02141233
Iteration 72, loss = 0.02137696
Iteration 73, loss = 0.02146372
Iteration 74, loss = 0.02117771
Iteration 75, loss = 0.02102070
Iteration 76, loss = 0.02122139
Iteration 77, loss = 0.02114207
Iteration 78, loss = 0.02101414
Iteration 79, loss = 0.02083393
Iteration 80, loss = 0.02106963
Iteration 81, loss = 0.02114850
Iteration 82, loss = 0.02115247
Iteration 83, loss = 0.02083863
Iteration 84, loss = 0.02074533
Iteration 85, loss = 0.02077122
Iteration 86, loss = 0.02090096
Iteration 87, loss = 0.02069862
Iteration 88, loss = 0.02068435
Iteration 89, loss = 0.02087920
Iteration 90, loss = 0.02083476
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69919853
Iteration 2, loss = 0.68214871
Iteration 3, loss = 0.66727037
Iteration 4, loss = 0.65242605
Iteration 5, loss = 0.63706883
Iteration 6, loss = 0.62061473
Iteration 7, loss = 0.60314469
Iteration 8, loss = 0.58436193
Iteration 9, loss = 0.56451691
Iteration 10, loss = 0.54342244
Iteration 11, loss = 0.52163893
Iteration 12, loss = 0.49885420
Iteration 13, loss = 0.47552169
Iteration 14, loss = 0.45166908
Iteration 15, loss = 0.42764108
Iteration 16, loss = 0.40360751
Iteration 17, loss = 0.37997050
Iteration 18, loss = 0.35696920
Iteration 19, loss = 0.33463349
Iteration 20, loss = 0.31334664
Iteration 21, loss = 0.29312356
Iteration 22, loss = 0.27396153
Iteration 23, loss = 0.25601594
Iteration 24, loss = 0.23921633
Iteration 25, loss = 0.22341980
Iteration 26, loss = 0.20882012
Iteration 27, loss = 0.19523742
Iteration 28, loss = 0.18265496
Iteration 29, loss = 0.17102870
Iteration 30, loss = 0.16026221
Iteration 31, loss = 0.15032805
Iteration 32, loss = 0.14116195
Iteration 33, loss = 0.13274350
Iteration 34, loss = 0.12496987
Iteration 35, loss = 0.11778476
Iteration 36, loss = 0.11116425
Iteration 37, loss = 0.10507302
Iteration 38, loss = 0.09943288
Iteration 39, loss = 0.09419460
Iteration 40, loss = 0.08936135
Iteration 41, loss = 0.08492588
Iteration 42, loss = 0.08072721
Iteration 43, loss = 0.07692503
Iteration 44, loss = 0.07330082
Iteration 45, loss = 0.06998442
Iteration 46, loss = 0.06686202
Iteration 47, loss = 0.06400542
Iteration 48, loss = 0.06126701
Iteration 49, loss = 0.05879785
Iteration 50, loss = 0.05641098
Iteration 51, loss = 0.05419251
Iteration 52, loss = 0.05210838
Iteration 53, loss = 0.05021422
Iteration 54, loss = 0.04837437
Iteration 55, loss = 0.04663906
Iteration 56, loss = 0.04503557
Iteration 57, loss = 0.04349936
Iteration 58, loss = 0.04207892
Iteration 59, loss = 0.04074724
Iteration 60, loss = 0.03941586
Iteration 61, loss = 0.03819283
Iteration 62, loss = 0.03704272
Iteration 63, loss = 0.03590313
Iteration 64, loss = 0.03486092
Iteration 65, loss = 0.03387719
Iteration 66, loss = 0.03291283
Iteration 67, loss = 0.03202537
Iteration 68, loss = 0.03114289
Iteration 69, loss = 0.03032497
Iteration 70, loss = 0.02954703
Iteration 71, loss = 0.02886189
Iteration 72, loss = 0.02813077
Iteration 73, loss = 0.02742555
Iteration 74, loss = 0.02680531
Iteration 75, loss = 0.02619724
Iteration 76, loss = 0.02556609
Iteration 77, loss = 0.02499420
Iteration 78, loss = 0.02444112
Iteration 79, loss = 0.02394904
Iteration 80, loss = 0.02343197
Iteration 81, loss = 0.02293867
Iteration 82, loss = 0.02252846
Iteration 83, loss = 0.02202295
Iteration 84, loss = 0.02158355
Iteration 85, loss = 0.02116635
Iteration 86, loss = 0.02076510
Iteration 87, loss = 0.02037470
Iteration 88, loss = 0.02000803
Iteration 89, loss = 0.01964108
Iteration 90, loss = 0.01929537
Iteration 91, loss = 0.01895718
Iteration 92, loss = 0.01862987
Iteration 93, loss = 0.01834511
Iteration 94, loss = 0.01803364
Iteration 95, loss = 0.01773753
Iteration 96, loss = 0.01743295
Iteration 97, loss = 0.01716902
Iteration 98, loss = 0.01696417
Iteration 99, loss = 0.01667623
Iteration 100, loss = 0.01645055
Iteration 101, loss = 0.01622423
Iteration 102, loss = 0.01596652
Iteration 103, loss = 0.01573527
Iteration 104, loss = 0.01550927
Iteration 105, loss = 0.01529433
Iteration 106, loss = 0.01508110
Iteration 107, loss = 0.01487132
Iteration 108, loss = 0.01467863
Iteration 109, loss = 0.01451477
Iteration 110, loss = 0.01431597
Iteration 111, loss = 0.01416971
Iteration 112, loss = 0.01394939
Iteration 113, loss = 0.01380048
Iteration 114, loss = 0.01363482
Iteration 115, loss = 0.01347503
Iteration 116, loss = 0.01331728
Iteration 117, loss = 0.01316759
Iteration 118, loss = 0.01301692
Iteration 119, loss = 0.01285807
Iteration 120, loss = 0.01273155
Iteration 121, loss = 0.01259633
Iteration 122, loss = 0.01249428
Iteration 123, loss = 0.01232059
Iteration 124, loss = 0.01223165
Iteration 125, loss = 0.01208551
Iteration 126, loss = 0.01200615
Iteration 127, loss = 0.01184256
Iteration 128, loss = 0.01171934
Iteration 129, loss = 0.01160327
Iteration 130, loss = 0.01149524
Iteration 131, loss = 0.01139934
Iteration 132, loss = 0.01134045
Iteration 133, loss = 0.01122165
Iteration 134, loss = 0.01116104
Iteration 135, loss = 0.01106523
Iteration 136, loss = 0.01090407
Iteration 137, loss = 0.01080547
Iteration 138, loss = 0.01074321
Iteration 139, loss = 0.01062362
Iteration 140, loss = 0.01056796
Iteration 141, loss = 0.01046960
Iteration 142, loss = 0.01037549
Iteration 143, loss = 0.01029384
Iteration 144, loss = 0.01021521
Iteration 145, loss = 0.01015302
Iteration 146, loss = 0.01009092
Iteration 147, loss = 0.00997193
Iteration 148, loss = 0.00991176
Iteration 149, loss = 0.00984505
Iteration 150, loss = 0.00978038
Iteration 151, loss = 0.00972671
Iteration 152, loss = 0.00966011
Iteration 153, loss = 0.00958966
Iteration 154, loss = 0.00953235
Iteration 155, loss = 0.00939989
Iteration 156, loss = 0.00936637
Iteration 157, loss = 0.00931009
Iteration 158, loss = 0.00927190
Iteration 159, loss = 0.00922798
Iteration 160, loss = 0.00917671
Iteration 161, loss = 0.00914822
Iteration 162, loss = 0.00906451
Iteration 163, loss = 0.00905618
Iteration 164, loss = 0.00899728
Iteration 165, loss = 0.00893809
Iteration 166, loss = 0.00886872
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71467376
Iteration 2, loss = 0.69529425
Iteration 3, loss = 0.67920587
Iteration 4, loss = 0.66381144
Iteration 5, loss = 0.64782542
Iteration 6, loss = 0.63085547
Iteration 7, loss = 0.61250647
Iteration 8, loss = 0.59286958
Iteration 9, loss = 0.57159687
Iteration 10, loss = 0.54907042
Iteration 11, loss = 0.52535919
Iteration 12, loss = 0.50091970
Iteration 13, loss = 0.47577036
Iteration 14, loss = 0.45039021
Iteration 15, loss = 0.42530202
Iteration 16, loss = 0.40059521
Iteration 17, loss = 0.37654720
Iteration 18, loss = 0.35333596
Iteration 19, loss = 0.33109205
Iteration 20, loss = 0.31003921
Iteration 21, loss = 0.28994995
Iteration 22, loss = 0.27112440
Iteration 23, loss = 0.25339770
Iteration 24, loss = 0.23684133
Iteration 25, loss = 0.22135653
Iteration 26, loss = 0.20694936
Iteration 27, loss = 0.19360333
Iteration 28, loss = 0.18129450
Iteration 29, loss = 0.16982089
Iteration 30, loss = 0.15926496
Iteration 31, loss = 0.14949019
Iteration 32, loss = 0.14045955
Iteration 33, loss = 0.13214509
Iteration 34, loss = 0.12442494
Iteration 35, loss = 0.11734192
Iteration 36, loss = 0.11077645
Iteration 37, loss = 0.10472389
Iteration 38, loss = 0.09908540
Iteration 39, loss = 0.09388280
Iteration 40, loss = 0.08905259
Iteration 41, loss = 0.08458491
Iteration 42, loss = 0.08046480
Iteration 43, loss = 0.07660832
Iteration 44, loss = 0.07306618
Iteration 45, loss = 0.06972189
Iteration 46, loss = 0.06663718
Iteration 47, loss = 0.06376623
Iteration 48, loss = 0.06106608
Iteration 49, loss = 0.05853310
Iteration 50, loss = 0.05617823
Iteration 51, loss = 0.05395442
Iteration 52, loss = 0.05186828
Iteration 53, loss = 0.04989771
Iteration 54, loss = 0.04809321
Iteration 55, loss = 0.04634100
Iteration 56, loss = 0.04470239
Iteration 57, loss = 0.04316868
Iteration 58, loss = 0.04171773
Iteration 59, loss = 0.04034601
Iteration 60, loss = 0.03903641
Iteration 61, loss = 0.03778983
Iteration 62, loss = 0.03663425
Iteration 63, loss = 0.03551743
Iteration 64, loss = 0.03444711
Iteration 65, loss = 0.03345104
Iteration 66, loss = 0.03249093
Iteration 67, loss = 0.03158164
Iteration 68, loss = 0.03072485
Iteration 69, loss = 0.02988572
Iteration 70, loss = 0.02909635
Iteration 71, loss = 0.02834461
Iteration 72, loss = 0.02761393
Iteration 73, loss = 0.02692600
Iteration 74, loss = 0.02625112
Iteration 75, loss = 0.02562409
Iteration 76, loss = 0.02501825
Iteration 77, loss = 0.02443244
Iteration 78, loss = 0.02387031
Iteration 79, loss = 0.02333202
Iteration 80, loss = 0.02281209
Iteration 81, loss = 0.02232231
Iteration 82, loss = 0.02186343
Iteration 83, loss = 0.02138990
Iteration 84, loss = 0.02095090
Iteration 85, loss = 0.02052693
Iteration 86, loss = 0.02012247
Iteration 87, loss = 0.01972899
Iteration 88, loss = 0.01935457
Iteration 89, loss = 0.01898665
Iteration 90, loss = 0.01863302
Iteration 91, loss = 0.01829688
Iteration 92, loss = 0.01796467
Iteration 93, loss = 0.01764252
Iteration 94, loss = 0.01734335
Iteration 95, loss = 0.01703890
Iteration 96, loss = 0.01675238
Iteration 97, loss = 0.01647219
Iteration 98, loss = 0.01620968
Iteration 99, loss = 0.01595013
Iteration 100, loss = 0.01570413
Iteration 101, loss = 0.01545931
Iteration 102, loss = 0.01522375
Iteration 103, loss = 0.01499399
Iteration 104, loss = 0.01477533
Iteration 105, loss = 0.01455593
Iteration 106, loss = 0.01434660
Iteration 107, loss = 0.01415021
Iteration 108, loss = 0.01395941
Iteration 109, loss = 0.01376116
Iteration 110, loss = 0.01356673
Iteration 111, loss = 0.01339076
Iteration 112, loss = 0.01321076
Iteration 113, loss = 0.01303618
Iteration 114, loss = 0.01288986
Iteration 115, loss = 0.01272627
Iteration 116, loss = 0.01255747
Iteration 117, loss = 0.01240962
Iteration 118, loss = 0.01225566
Iteration 119, loss = 0.01212275
Iteration 120, loss = 0.01198015
Iteration 121, loss = 0.01183679
Iteration 122, loss = 0.01170319
Iteration 123, loss = 0.01157116
Iteration 124, loss = 0.01144638
Iteration 125, loss = 0.01132272
Iteration 126, loss = 0.01119770
Iteration 127, loss = 0.01109076
Iteration 128, loss = 0.01096523
Iteration 129, loss = 0.01084766
Iteration 130, loss = 0.01073712
Iteration 131, loss = 0.01062876
Iteration 132, loss = 0.01052046
Iteration 133, loss = 0.01041767
Iteration 134, loss = 0.01031579
Iteration 135, loss = 0.01021447
Iteration 136, loss = 0.01011781
Iteration 137, loss = 0.01002434
Iteration 138, loss = 0.00992863
Iteration 139, loss = 0.00983863
Iteration 140, loss = 0.00975410
Iteration 141, loss = 0.00966603
Iteration 142, loss = 0.00958081
Iteration 143, loss = 0.00949685
Iteration 144, loss = 0.00941309
Iteration 145, loss = 0.00933602
Iteration 146, loss = 0.00925529
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67859052
Iteration 2, loss = 0.65256846
Iteration 3, loss = 0.62283252
Iteration 4, loss = 0.58467273
Iteration 5, loss = 0.53976391
Iteration 6, loss = 0.49251857
Iteration 7, loss = 0.44497476
Iteration 8, loss = 0.39869971
Iteration 9, loss = 0.35480929
Iteration 10, loss = 0.31413712
Iteration 11, loss = 0.27729984
Iteration 12, loss = 0.24418757
Iteration 13, loss = 0.21525954
Iteration 14, loss = 0.18990503
Iteration 15, loss = 0.16813179
Iteration 16, loss = 0.14938350
Iteration 17, loss = 0.13359100
Iteration 18, loss = 0.11989020
Iteration 19, loss = 0.10807477
Iteration 20, loss = 0.09792163
Iteration 21, loss = 0.08926503
Iteration 22, loss = 0.08169792
Iteration 23, loss = 0.07511709
Iteration 24, loss = 0.06926416
Iteration 25, loss = 0.06427629
Iteration 26, loss = 0.05990282
Iteration 27, loss = 0.05596714
Iteration 28, loss = 0.05238997
Iteration 29, loss = 0.04929371
Iteration 30, loss = 0.04648995
Iteration 31, loss = 0.04399242
Iteration 32, loss = 0.04175506
Iteration 33, loss = 0.03962608
Iteration 34, loss = 0.03778877
Iteration 35, loss = 0.03601474
Iteration 36, loss = 0.03438836
Iteration 37, loss = 0.03270395
Iteration 38, loss = 0.03178256
Iteration 39, loss = 0.03050831
Iteration 40, loss = 0.02954179
Iteration 41, loss = 0.02846646
Iteration 42, loss = 0.02750354
Iteration 43, loss = 0.02661354
Iteration 44, loss = 0.02565359
Iteration 45, loss = 0.02486403
Iteration 46, loss = 0.02411762
Iteration 47, loss = 0.02356036
Iteration 48, loss = 0.02290197
Iteration 49, loss = 0.02234591
Iteration 50, loss = 0.02175518
Iteration 51, loss = 0.02119254
Iteration 52, loss = 0.02070536
Iteration 53, loss = 0.02023372
Iteration 54, loss = 0.01981363
Iteration 55, loss = 0.01937578
Iteration 56, loss = 0.01899345
Iteration 57, loss = 0.01862008
Iteration 58, loss = 0.01828412
Iteration 59, loss = 0.01793366
Iteration 60, loss = 0.01764449
Iteration 61, loss = 0.01737006
Iteration 62, loss = 0.01709440
Iteration 63, loss = 0.01685807
Iteration 64, loss = 0.01662384
Iteration 65, loss = 0.01632044
Iteration 66, loss = 0.01622372
Iteration 67, loss = 0.01593711
Iteration 68, loss = 0.01578185
Iteration 69, loss = 0.01572587
Iteration 70, loss = 0.01551140
Iteration 71, loss = 0.01518560
Iteration 72, loss = 0.01486332
Iteration 73, loss = 0.01477969
Iteration 74, loss = 0.01464962
Iteration 75, loss = 0.01443251
Iteration 76, loss = 0.01430503
Iteration 77, loss = 0.01409285
Iteration 78, loss = 0.01394625
Iteration 79, loss = 0.01377402
Iteration 80, loss = 0.01369088
Iteration 81, loss = 0.01351992
Iteration 82, loss = 0.01359620
Iteration 83, loss = 0.01350036
Iteration 84, loss = 0.01335200
Iteration 85, loss = 0.01323214
Iteration 86, loss = 0.01310587
Iteration 87, loss = 0.01286486
Iteration 88, loss = 0.01293086
Iteration 89, loss = 0.01293480
Iteration 90, loss = 0.01278282
Iteration 91, loss = 0.01279830
Iteration 92, loss = 0.01268692
Iteration 93, loss = 0.01250325
Iteration 94, loss = 0.01243795
Iteration 95, loss = 0.01224913
Iteration 96, loss = 0.01218976
Iteration 97, loss = 0.01215800
Iteration 98, loss = 0.01196020
Iteration 99, loss = 0.01205709
Iteration 100, loss = 0.01194943
Iteration 101, loss = 0.01187574
Iteration 102, loss = 0.01174622
Iteration 103, loss = 0.01166584
Iteration 104, loss = 0.01173910
Iteration 105, loss = 0.01179340
Iteration 106, loss = 0.01146570
Iteration 107, loss = 0.01175349
Iteration 108, loss = 0.01165628
Iteration 109, loss = 0.01155270
Iteration 110, loss = 0.01153175
Iteration 111, loss = 0.01150878
Iteration 112, loss = 0.01140447
Iteration 113, loss = 0.01118868
Iteration 114, loss = 0.01168050
Iteration 115, loss = 0.01138569
Iteration 116, loss = 0.01154526
Iteration 117, loss = 0.01129149
Iteration 118, loss = 0.01123746
Iteration 119, loss = 0.01120500
Iteration 120, loss = 0.01101389
Iteration 121, loss = 0.01104691
Iteration 122, loss = 0.01092389
Iteration 123, loss = 0.01087327
Iteration 124, loss = 0.01090562
Iteration 125, loss = 0.01102134
Iteration 126, loss = 0.01099003
Iteration 127, loss = 0.01090549
Iteration 128, loss = 0.01088309
Iteration 129, loss = 0.01071499
Iteration 130, loss = 0.01063270
Iteration 131, loss = 0.01062657
Iteration 132, loss = 0.01054412
Iteration 133, loss = 0.01053678
Iteration 134, loss = 0.01049807
Iteration 135, loss = 0.01046734
Iteration 136, loss = 0.01041731
Iteration 137, loss = 0.01047348
Iteration 138, loss = 0.01038885
Iteration 139, loss = 0.01032808
Iteration 140, loss = 0.01036869
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68775952
Iteration 2, loss = 0.67132645
Iteration 3, loss = 0.65693846
Iteration 4, loss = 0.64249578
Iteration 5, loss = 0.62733293
Iteration 6, loss = 0.61101685
Iteration 7, loss = 0.59322534
Iteration 8, loss = 0.57404869
Iteration 9, loss = 0.55338631
Iteration 10, loss = 0.53144933
Iteration 11, loss = 0.50850801
Iteration 12, loss = 0.48473125
Iteration 13, loss = 0.46038948
Iteration 14, loss = 0.43596891
Iteration 15, loss = 0.41144152
Iteration 16, loss = 0.38746839
Iteration 17, loss = 0.36390999
Iteration 18, loss = 0.34146935
Iteration 19, loss = 0.31985760
Iteration 20, loss = 0.29937468
Iteration 21, loss = 0.27999725
Iteration 22, loss = 0.26177540
Iteration 23, loss = 0.24472635
Iteration 24, loss = 0.22872576
Iteration 25, loss = 0.21381822
Iteration 26, loss = 0.19996626
Iteration 27, loss = 0.18704431
Iteration 28, loss = 0.17508130
Iteration 29, loss = 0.16403461
Iteration 30, loss = 0.15379339
Iteration 31, loss = 0.14436068
Iteration 32, loss = 0.13560414
Iteration 33, loss = 0.12757964
Iteration 34, loss = 0.12011553
Iteration 35, loss = 0.11324534
Iteration 36, loss = 0.10684260
Iteration 37, loss = 0.10096744
Iteration 38, loss = 0.09551362
Iteration 39, loss = 0.09045180
Iteration 40, loss = 0.08579003
Iteration 41, loss = 0.08143754
Iteration 42, loss = 0.07741710
Iteration 43, loss = 0.07365538
Iteration 44, loss = 0.07018144
Iteration 45, loss = 0.06690750
Iteration 46, loss = 0.06387973
Iteration 47, loss = 0.06107586
Iteration 48, loss = 0.05840086
Iteration 49, loss = 0.05595108
Iteration 50, loss = 0.05361930
Iteration 51, loss = 0.05142616
Iteration 52, loss = 0.04939204
Iteration 53, loss = 0.04747051
Iteration 54, loss = 0.04565875
Iteration 55, loss = 0.04397250
Iteration 56, loss = 0.04236676
Iteration 57, loss = 0.04083668
Iteration 58, loss = 0.03942159
Iteration 59, loss = 0.03805788
Iteration 60, loss = 0.03676614
Iteration 61, loss = 0.03554238
Iteration 62, loss = 0.03438117
Iteration 63, loss = 0.03327384
Iteration 64, loss = 0.03224312
Iteration 65, loss = 0.03126240
Iteration 66, loss = 0.03030866
Iteration 67, loss = 0.02941484
Iteration 68, loss = 0.02855866
Iteration 69, loss = 0.02773559
Iteration 70, loss = 0.02695326
Iteration 71, loss = 0.02621080
Iteration 72, loss = 0.02550635
Iteration 73, loss = 0.02482414
Iteration 74, loss = 0.02416513
Iteration 75, loss = 0.02354417
Iteration 76, loss = 0.02294856
Iteration 77, loss = 0.02236681
Iteration 78, loss = 0.02181241
Iteration 79, loss = 0.02128226
Iteration 80, loss = 0.02077566
Iteration 81, loss = 0.02027947
Iteration 82, loss = 0.01979446
Iteration 83, loss = 0.01934075
Iteration 84, loss = 0.01889700
Iteration 85, loss = 0.01846811
Iteration 86, loss = 0.01806059
Iteration 87, loss = 0.01766900
Iteration 88, loss = 0.01729026
Iteration 89, loss = 0.01693055
Iteration 90, loss = 0.01658151
Iteration 91, loss = 0.01624470
Iteration 92, loss = 0.01592680
Iteration 93, loss = 0.01560688
Iteration 94, loss = 0.01530986
Iteration 95, loss = 0.01501240
Iteration 96, loss = 0.01472989
Iteration 97, loss = 0.01445845
Iteration 98, loss = 0.01419408
Iteration 99, loss = 0.01393135
Iteration 100, loss = 0.01368673
Iteration 101, loss = 0.01343961
Iteration 102, loss = 0.01320842
Iteration 103, loss = 0.01297741
Iteration 104, loss = 0.01276023
Iteration 105, loss = 0.01254220
Iteration 106, loss = 0.01233936
Iteration 107, loss = 0.01213476
Iteration 108, loss = 0.01193763
Iteration 109, loss = 0.01174721
Iteration 110, loss = 0.01156220
Iteration 111, loss = 0.01138558
Iteration 112, loss = 0.01120770
Iteration 113, loss = 0.01103874
Iteration 114, loss = 0.01087030
Iteration 115, loss = 0.01070854
Iteration 116, loss = 0.01055410
Iteration 117, loss = 0.01039721
Iteration 118, loss = 0.01025034
Iteration 119, loss = 0.01010412
Iteration 120, loss = 0.00996471
Iteration 121, loss = 0.00982642
Iteration 122, loss = 0.00969351
Iteration 123, loss = 0.00955977
Iteration 124, loss = 0.00943188
Iteration 125, loss = 0.00930729
Iteration 126, loss = 0.00918572
Iteration 127, loss = 0.00906543
Iteration 128, loss = 0.00894861
Iteration 129, loss = 0.00883432
Iteration 130, loss = 0.00872212
Iteration 131, loss = 0.00861480
Iteration 132, loss = 0.00850664
Iteration 133, loss = 0.00840719
Iteration 134, loss = 0.00830174
Iteration 135, loss = 0.00820407
Iteration 136, loss = 0.00810681
Iteration 137, loss = 0.00801111
Iteration 138, loss = 0.00792101
Iteration 139, loss = 0.00782818
Iteration 140, loss = 0.00773890
Iteration 141, loss = 0.00765344
Iteration 142, loss = 0.00756830
Iteration 143, loss = 0.00748756
Iteration 144, loss = 0.00740561
Iteration 145, loss = 0.00732725
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68323507
Iteration 2, loss = 0.64902346
Iteration 3, loss = 0.60705402
Iteration 4, loss = 0.55545598
Iteration 5, loss = 0.49803892
Iteration 6, loss = 0.43901545
Iteration 7, loss = 0.38176311
Iteration 8, loss = 0.32867119
Iteration 9, loss = 0.28117744
Iteration 10, loss = 0.23970668
Iteration 11, loss = 0.20491754
Iteration 12, loss = 0.17575019
Iteration 13, loss = 0.15181389
Iteration 14, loss = 0.13210004
Iteration 15, loss = 0.11582701
Iteration 16, loss = 0.10252471
Iteration 17, loss = 0.09149699
Iteration 18, loss = 0.08232256
Iteration 19, loss = 0.07449864
Iteration 20, loss = 0.06800114
Iteration 21, loss = 0.06229106
Iteration 22, loss = 0.05748840
Iteration 23, loss = 0.05334362
Iteration 24, loss = 0.04976108
Iteration 25, loss = 0.04661710
Iteration 26, loss = 0.04381135
Iteration 27, loss = 0.04145734
Iteration 28, loss = 0.03924616
Iteration 29, loss = 0.03725940
Iteration 30, loss = 0.03559818
Iteration 31, loss = 0.03400215
Iteration 32, loss = 0.03252971
Iteration 33, loss = 0.03116655
Iteration 34, loss = 0.03009224
Iteration 35, loss = 0.02898648
Iteration 36, loss = 0.02807833
Iteration 37, loss = 0.02722705
Iteration 38, loss = 0.02633143
Iteration 39, loss = 0.02567142
Iteration 40, loss = 0.02505121
Iteration 41, loss = 0.02427801
Iteration 42, loss = 0.02372131
Iteration 43, loss = 0.02312414
Iteration 44, loss = 0.02261909
Iteration 45, loss = 0.02204836
Iteration 46, loss = 0.02162088
Iteration 47, loss = 0.02123328
Iteration 48, loss = 0.02093115
Iteration 49, loss = 0.02057665
Iteration 50, loss = 0.02023587
Iteration 51, loss = 0.01994595
Iteration 52, loss = 0.01951082
Iteration 53, loss = 0.01918450
Iteration 54, loss = 0.01903767
Iteration 55, loss = 0.01870182
Iteration 56, loss = 0.01853403
Iteration 57, loss = 0.01828185
Iteration 58, loss = 0.01812605
Iteration 59, loss = 0.01792597
Iteration 60, loss = 0.01782334
Iteration 61, loss = 0.01743883
Iteration 62, loss = 0.01739767
Iteration 63, loss = 0.01712976
Iteration 64, loss = 0.01708772
Iteration 65, loss = 0.01690343
Iteration 66, loss = 0.01671058
Iteration 67, loss = 0.01666588
Iteration 68, loss = 0.01648079
Iteration 69, loss = 0.01644248
Iteration 70, loss = 0.01621215
Iteration 71, loss = 0.01614782
Iteration 72, loss = 0.01611712
Iteration 73, loss = 0.01610383
Iteration 74, loss = 0.01568501
Iteration 75, loss = 0.01566857
Iteration 76, loss = 0.01562946
Iteration 77, loss = 0.01561596
Iteration 78, loss = 0.01553862
Iteration 79, loss = 0.01548319
Iteration 80, loss = 0.01528229
Iteration 81, loss = 0.01529643
Iteration 82, loss = 0.01518178
Iteration 83, loss = 0.01506926
Iteration 84, loss = 0.01498450
Iteration 85, loss = 0.01495646
Iteration 86, loss = 0.01488508
Iteration 87, loss = 0.01478434
Iteration 88, loss = 0.01486488
Iteration 89, loss = 0.01474772
Iteration 90, loss = 0.01467107
Iteration 91, loss = 0.01456855
Iteration 92, loss = 0.01472725
Iteration 93, loss = 0.01455180
Iteration 94, loss = 0.01454089
Iteration 95, loss = 0.01444131
Iteration 96, loss = 0.01442767
Iteration 97, loss = 0.01432407
Iteration 98, loss = 0.01429555
Iteration 99, loss = 0.01430320
Iteration 100, loss = 0.01435828
Iteration 101, loss = 0.01422291
Iteration 102, loss = 0.01413340
Iteration 103, loss = 0.01417581
Iteration 104, loss = 0.01410624
Iteration 105, loss = 0.01403399
Iteration 106, loss = 0.01397605
Iteration 107, loss = 0.01390064
Iteration 108, loss = 0.01397424
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70847114
Iteration 2, loss = 0.65989577
Iteration 3, loss = 0.59070384
Iteration 4, loss = 0.49931440
Iteration 5, loss = 0.40695202
Iteration 6, loss = 0.32570078
Iteration 7, loss = 0.25920641
Iteration 8, loss = 0.20668557
Iteration 9, loss = 0.16705839
Iteration 10, loss = 0.13708100
Iteration 11, loss = 0.11444153
Iteration 12, loss = 0.09709464
Iteration 13, loss = 0.08391559
Iteration 14, loss = 0.07360676
Iteration 15, loss = 0.06509318
Iteration 16, loss = 0.05843452
Iteration 17, loss = 0.05308339
Iteration 18, loss = 0.04852795
Iteration 19, loss = 0.04481431
Iteration 20, loss = 0.04169678
Iteration 21, loss = 0.03895472
Iteration 22, loss = 0.03679329
Iteration 23, loss = 0.03474260
Iteration 24, loss = 0.03297892
Iteration 25, loss = 0.03148587
Iteration 26, loss = 0.03011387
Iteration 27, loss = 0.02889189
Iteration 28, loss = 0.02793958
Iteration 29, loss = 0.02697895
Iteration 30, loss = 0.02611643
Iteration 31, loss = 0.02540468
Iteration 32, loss = 0.02484679
Iteration 33, loss = 0.02409158
Iteration 34, loss = 0.02353007
Iteration 35, loss = 0.02315973
Iteration 36, loss = 0.02259049
Iteration 37, loss = 0.02214192
Iteration 38, loss = 0.02178295
Iteration 39, loss = 0.02143299
Iteration 40, loss = 0.02098060
Iteration 41, loss = 0.02074864
Iteration 42, loss = 0.02072767
Iteration 43, loss = 0.02027502
Iteration 44, loss = 0.01999125
Iteration 45, loss = 0.01987880
Iteration 46, loss = 0.01954194
Iteration 47, loss = 0.01942193
Iteration 48, loss = 0.01924003
Iteration 49, loss = 0.01890094
Iteration 50, loss = 0.01892951
Iteration 51, loss = 0.01850908
Iteration 52, loss = 0.01851361
Iteration 53, loss = 0.01844133
Iteration 54, loss = 0.01804719
Iteration 55, loss = 0.01809386
Iteration 56, loss = 0.01797581
Iteration 57, loss = 0.01801121
Iteration 58, loss = 0.01784238
Iteration 59, loss = 0.01767252
Iteration 60, loss = 0.01781213
Iteration 61, loss = 0.01741937
Iteration 62, loss = 0.01756963
Iteration 63, loss = 0.01736518
Iteration 64, loss = 0.01736978
Iteration 65, loss = 0.01722356
Iteration 66, loss = 0.01716668
Iteration 67, loss = 0.01718319
Iteration 68, loss = 0.01725423
Iteration 69, loss = 0.01697929
Iteration 70, loss = 0.01726592
Iteration 71, loss = 0.01686970
Iteration 72, loss = 0.01683661
Iteration 73, loss = 0.01677101
Iteration 74, loss = 0.01687794
Iteration 75, loss = 0.01691742
Iteration 76, loss = 0.01694583
Iteration 77, loss = 0.01661915
Iteration 78, loss = 0.01681097
Iteration 79, loss = 0.01643088
Iteration 80, loss = 0.01672509
Iteration 81, loss = 0.01642131
Iteration 82, loss = 0.01675811
Iteration 83, loss = 0.01646780
Iteration 84, loss = 0.01630254
Iteration 85, loss = 0.01651616
Iteration 86, loss = 0.01631404
Iteration 87, loss = 0.01626419
Iteration 88, loss = 0.01633982
Iteration 89, loss = 0.01619412
Iteration 90, loss = 0.01613623
Iteration 91, loss = 0.01628182
Iteration 92, loss = 0.01618118
Iteration 93, loss = 0.01607190
Iteration 94, loss = 0.01613935
Iteration 95, loss = 0.01612951
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67783372
Iteration 2, loss = 0.62390923
Iteration 3, loss = 0.53230276
Iteration 4, loss = 0.43230351
Iteration 5, loss = 0.34032728
Iteration 6, loss = 0.26456797
Iteration 7, loss = 0.20625126
Iteration 8, loss = 0.16338275
Iteration 9, loss = 0.13227171
Iteration 10, loss = 0.10981693
Iteration 11, loss = 0.09336428
Iteration 12, loss = 0.08064713
Iteration 13, loss = 0.07105983
Iteration 14, loss = 0.06354108
Iteration 15, loss = 0.05753833
Iteration 16, loss = 0.05269365
Iteration 17, loss = 0.04868610
Iteration 18, loss = 0.04562088
Iteration 19, loss = 0.04263848
Iteration 20, loss = 0.04041364
Iteration 21, loss = 0.03820367
Iteration 22, loss = 0.03690951
Iteration 23, loss = 0.03576082
Iteration 24, loss = 0.03415828
Iteration 25, loss = 0.03300269
Iteration 26, loss = 0.03178597
Iteration 27, loss = 0.03100404
Iteration 28, loss = 0.03005521
Iteration 29, loss = 0.02923272
Iteration 30, loss = 0.02925878
Iteration 31, loss = 0.02890185
Iteration 32, loss = 0.02848215
Iteration 33, loss = 0.02757850
Iteration 34, loss = 0.02725031
Iteration 35, loss = 0.02653712
Iteration 36, loss = 0.02590689
Iteration 37, loss = 0.02552542
Iteration 38, loss = 0.02550251
Iteration 39, loss = 0.02509197
Iteration 40, loss = 0.02499522
Iteration 41, loss = 0.02457062
Iteration 42, loss = 0.02428726
Iteration 43, loss = 0.02397854
Iteration 44, loss = 0.02378846
Iteration 45, loss = 0.02356721
Iteration 46, loss = 0.02343352
Iteration 47, loss = 0.02310581
Iteration 48, loss = 0.02291095
Iteration 49, loss = 0.02309121
Iteration 50, loss = 0.02310297
Iteration 51, loss = 0.02305083
Iteration 52, loss = 0.02295624
Iteration 53, loss = 0.02269333
Iteration 54, loss = 0.02234347
Iteration 55, loss = 0.02212963
Iteration 56, loss = 0.02230844
Iteration 57, loss = 0.02221670
Iteration 58, loss = 0.02203907
Iteration 59, loss = 0.02195761
Iteration 60, loss = 0.02195941
Iteration 61, loss = 0.02198843
Iteration 62, loss = 0.02192830
Iteration 63, loss = 0.02151745
Iteration 64, loss = 0.02146251
Iteration 65, loss = 0.02387330
Iteration 66, loss = 0.02327763
Iteration 67, loss = 0.02211075
Iteration 68, loss = 0.02169027
Iteration 69, loss = 0.02124373
Iteration 70, loss = 0.02095907
Iteration 71, loss = 0.02088858
Iteration 72, loss = 0.02082979
Iteration 73, loss = 0.02244182
Iteration 74, loss = 0.02305603
Iteration 75, loss = 0.02242022
Iteration 76, loss = 0.02171277
Iteration 77, loss = 0.02114594
Iteration 78, loss = 0.02104141
Iteration 79, loss = 0.02076936
Iteration 80, loss = 0.02078794
Iteration 81, loss = 0.02089620
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71071174
Iteration 2, loss = 0.66743558
Iteration 3, loss = 0.60212783
Iteration 4, loss = 0.51559269
Iteration 5, loss = 0.42421428
Iteration 6, loss = 0.33959152
Iteration 7, loss = 0.26963147
Iteration 8, loss = 0.21489950
Iteration 9, loss = 0.17372503
Iteration 10, loss = 0.14283902
Iteration 11, loss = 0.11966555
Iteration 12, loss = 0.10235972
Iteration 13, loss = 0.08889001
Iteration 14, loss = 0.07853828
Iteration 15, loss = 0.07022450
Iteration 16, loss = 0.06358389
Iteration 17, loss = 0.05807644
Iteration 18, loss = 0.05365120
Iteration 19, loss = 0.04980694
Iteration 20, loss = 0.04684856
Iteration 21, loss = 0.04410667
Iteration 22, loss = 0.04178701
Iteration 23, loss = 0.03998994
Iteration 24, loss = 0.03813238
Iteration 25, loss = 0.03668860
Iteration 26, loss = 0.03548259
Iteration 27, loss = 0.03412055
Iteration 28, loss = 0.03312459
Iteration 29, loss = 0.03218726
Iteration 30, loss = 0.03154163
Iteration 31, loss = 0.03070281
Iteration 32, loss = 0.02996963
Iteration 33, loss = 0.02950723
Iteration 34, loss = 0.02904067
Iteration 35, loss = 0.02838908
Iteration 36, loss = 0.02788541
Iteration 37, loss = 0.02751386
Iteration 38, loss = 0.02722680
Iteration 39, loss = 0.02667478
Iteration 40, loss = 0.02662650
Iteration 41, loss = 0.02615609
Iteration 42, loss = 0.02594199
Iteration 43, loss = 0.02564928
Iteration 44, loss = 0.02529115
Iteration 45, loss = 0.02508083
Iteration 46, loss = 0.02498022
Iteration 47, loss = 0.02468132
Iteration 48, loss = 0.02451497
Iteration 49, loss = 0.02446245
Iteration 50, loss = 0.02411511
Iteration 51, loss = 0.02405384
Iteration 52, loss = 0.02381542
Iteration 53, loss = 0.02385622
Iteration 54, loss = 0.02338678
Iteration 55, loss = 0.02346137
Iteration 56, loss = 0.02338210
Iteration 57, loss = 0.02320053
Iteration 58, loss = 0.02311019
Iteration 59, loss = 0.02299518
Iteration 60, loss = 0.02300671
Iteration 61, loss = 0.02328637
Iteration 62, loss = 0.02268728
Iteration 63, loss = 0.02278756
Iteration 64, loss = 0.02267446
Iteration 65, loss = 0.02246945
Iteration 66, loss = 0.02260404
Iteration 67, loss = 0.02246949
Iteration 68, loss = 0.02210678
Iteration 69, loss = 0.02233051
Iteration 70, loss = 0.02239528
Iteration 71, loss = 0.02245895
Iteration 72, loss = 0.02217530
Iteration 73, loss = 0.02200485
Iteration 74, loss = 0.02202648
Iteration 75, loss = 0.02203057
Iteration 76, loss = 0.02189641
Iteration 77, loss = 0.02197268
Iteration 78, loss = 0.02197273
Iteration 79, loss = 0.02178994
Iteration 80, loss = 0.02180899
Iteration 81, loss = 0.02176500
Iteration 82, loss = 0.02156969
Iteration 83, loss = 0.02176505
Iteration 84, loss = 0.02153925
Iteration 85, loss = 0.02154968
Iteration 86, loss = 0.02156510
Iteration 87, loss = 0.02153485
Iteration 88, loss = 0.02149597
Iteration 89, loss = 0.02167248
Iteration 90, loss = 0.02153594
Iteration 91, loss = 0.02178121
Iteration 92, loss = 0.02129073
Iteration 93, loss = 0.02142016
Iteration 94, loss = 0.02131661
Iteration 95, loss = 0.02135341
Iteration 96, loss = 0.02140999
Iteration 97, loss = 0.02133754
Iteration 98, loss = 0.02117949
Iteration 99, loss = 0.02129541
Iteration 100, loss = 0.02150239
Iteration 101, loss = 0.02153849
Iteration 102, loss = 0.02097844
Iteration 103, loss = 0.02120742
Iteration 104, loss = 0.02111777
Iteration 105, loss = 0.02125547
Iteration 106, loss = 0.02120525
Iteration 107, loss = 0.02105060
Iteration 108, loss = 0.02123386
Iteration 109, loss = 0.02118019
Iteration 110, loss = 0.02118808
Iteration 111, loss = 0.02125732
Iteration 112, loss = 0.02103837
Iteration 113, loss = 0.02101635
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72117898
Iteration 2, loss = 0.68001666
Iteration 3, loss = 0.62383257
Iteration 4, loss = 0.55794354
Iteration 5, loss = 0.48584517
Iteration 6, loss = 0.41463493
Iteration 7, loss = 0.34946751
Iteration 8, loss = 0.29310896
Iteration 9, loss = 0.24521102
Iteration 10, loss = 0.20610386
Iteration 11, loss = 0.17440495
Iteration 12, loss = 0.14906030
Iteration 13, loss = 0.12860827
Iteration 14, loss = 0.11237397
Iteration 15, loss = 0.09916604
Iteration 16, loss = 0.08851446
Iteration 17, loss = 0.07977798
Iteration 18, loss = 0.07242966
Iteration 19, loss = 0.06628350
Iteration 20, loss = 0.06096700
Iteration 21, loss = 0.05677629
Iteration 22, loss = 0.05307208
Iteration 23, loss = 0.04980546
Iteration 24, loss = 0.04687532
Iteration 25, loss = 0.04437363
Iteration 26, loss = 0.04228415
Iteration 27, loss = 0.04043304
Iteration 28, loss = 0.03870860
Iteration 29, loss = 0.03728888
Iteration 30, loss = 0.03594734
Iteration 31, loss = 0.03452246
Iteration 32, loss = 0.03336767
Iteration 33, loss = 0.03238481
Iteration 34, loss = 0.03147999
Iteration 35, loss = 0.03057425
Iteration 36, loss = 0.02984153
Iteration 37, loss = 0.02920705
Iteration 38, loss = 0.02849375
Iteration 39, loss = 0.02784505
Iteration 40, loss = 0.02739336
Iteration 41, loss = 0.02691108
Iteration 42, loss = 0.02646789
Iteration 43, loss = 0.02596921
Iteration 44, loss = 0.02572573
Iteration 45, loss = 0.02531107
Iteration 46, loss = 0.02499597
Iteration 47, loss = 0.02447056
Iteration 48, loss = 0.02418602
Iteration 49, loss = 0.02395025
Iteration 50, loss = 0.02372859
Iteration 51, loss = 0.02346417
Iteration 52, loss = 0.02319926
Iteration 53, loss = 0.02304118
Iteration 54, loss = 0.02274185
Iteration 55, loss = 0.02256447
Iteration 56, loss = 0.02225996
Iteration 57, loss = 0.02221442
Iteration 58, loss = 0.02191293
Iteration 59, loss = 0.02202980
Iteration 60, loss = 0.02194687
Iteration 61, loss = 0.02154696
Iteration 62, loss = 0.02140901
Iteration 63, loss = 0.02122682
Iteration 64, loss = 0.02125991
Iteration 65, loss = 0.02093854
Iteration 66, loss = 0.02090877
Iteration 67, loss = 0.02075170
Iteration 68, loss = 0.02073427
Iteration 69, loss = 0.02074082
Iteration 70, loss = 0.02056285
Iteration 71, loss = 0.02053430
Iteration 72, loss = 0.02041642
Iteration 73, loss = 0.02039295
Iteration 74, loss = 0.02021664
Iteration 75, loss = 0.02007600
Iteration 76, loss = 0.02006527
Iteration 77, loss = 0.02011934
Iteration 78, loss = 0.02007761
Iteration 79, loss = 0.01990439
Iteration 80, loss = 0.01969834
Iteration 81, loss = 0.01960382
Iteration 82, loss = 0.01970477
Iteration 83, loss = 0.01970594
Iteration 84, loss = 0.01972685
Iteration 85, loss = 0.01958019
Iteration 86, loss = 0.01965949
Iteration 87, loss = 0.01951276
Iteration 88, loss = 0.01940597
Iteration 89, loss = 0.01928369
Iteration 90, loss = 0.01942958
Iteration 91, loss = 0.01924241
Iteration 92, loss = 0.01931947
Iteration 93, loss = 0.01920043
Iteration 94, loss = 0.01916210
Iteration 95, loss = 0.01914178
Iteration 96, loss = 0.01909235
Iteration 97, loss = 0.01900686
Iteration 98, loss = 0.01906166
Iteration 99, loss = 0.01901157
Iteration 100, loss = 0.01893849
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70266680
Iteration 2, loss = 0.66935742
Iteration 3, loss = 0.62194651
Iteration 4, loss = 0.55958450
Iteration 5, loss = 0.48960196
Iteration 6, loss = 0.41995313
Iteration 7, loss = 0.35446223
Iteration 8, loss = 0.29719971
Iteration 9, loss = 0.24824210
Iteration 10, loss = 0.20796020
Iteration 11, loss = 0.17539090
Iteration 12, loss = 0.14936301
Iteration 13, loss = 0.12854908
Iteration 14, loss = 0.11192956
Iteration 15, loss = 0.09852551
Iteration 16, loss = 0.08761836
Iteration 17, loss = 0.07859141
Iteration 18, loss = 0.07111299
Iteration 19, loss = 0.06501066
Iteration 20, loss = 0.05980201
Iteration 21, loss = 0.05531658
Iteration 22, loss = 0.05157783
Iteration 23, loss = 0.04828011
Iteration 24, loss = 0.04543272
Iteration 25, loss = 0.04286318
Iteration 26, loss = 0.04060480
Iteration 27, loss = 0.03867948
Iteration 28, loss = 0.03701177
Iteration 29, loss = 0.03555824
Iteration 30, loss = 0.03419167
Iteration 31, loss = 0.03293661
Iteration 32, loss = 0.03169206
Iteration 33, loss = 0.03079801
Iteration 34, loss = 0.02986273
Iteration 35, loss = 0.02906320
Iteration 36, loss = 0.02832311
Iteration 37, loss = 0.02757253
Iteration 38, loss = 0.02686655
Iteration 39, loss = 0.02654822
Iteration 40, loss = 0.02596118
Iteration 41, loss = 0.02524335
Iteration 42, loss = 0.02487042
Iteration 43, loss = 0.02436689
Iteration 44, loss = 0.02404133
Iteration 45, loss = 0.02366208
Iteration 46, loss = 0.02321075
Iteration 47, loss = 0.02294053
Iteration 48, loss = 0.02271445
Iteration 49, loss = 0.02235034
Iteration 50, loss = 0.02221090
Iteration 51, loss = 0.02192653
Iteration 52, loss = 0.02163320
Iteration 53, loss = 0.02135059
Iteration 54, loss = 0.02127246
Iteration 55, loss = 0.02096564
Iteration 56, loss = 0.02091719
Iteration 57, loss = 0.02069924
Iteration 58, loss = 0.02039491
Iteration 59, loss = 0.02030061
Iteration 60, loss = 0.02019280
Iteration 61, loss = 0.02011259
Iteration 62, loss = 0.01993016
Iteration 63, loss = 0.01997959
Iteration 64, loss = 0.01961923
Iteration 65, loss = 0.01956848
Iteration 66, loss = 0.01945229
Iteration 67, loss = 0.01929142
Iteration 68, loss = 0.01914213
Iteration 69, loss = 0.01908800
Iteration 70, loss = 0.01896920
Iteration 71, loss = 0.01903936
Iteration 72, loss = 0.01889334
Iteration 73, loss = 0.01874920
Iteration 74, loss = 0.01872483
Iteration 75, loss = 0.01857328
Iteration 76, loss = 0.01849877
Iteration 77, loss = 0.01854439
Iteration 78, loss = 0.01841233
Iteration 79, loss = 0.01830306
Iteration 80, loss = 0.01835664
Iteration 81, loss = 0.01829903
Iteration 82, loss = 0.01833331
Iteration 83, loss = 0.01800901
Iteration 84, loss = 0.01809155
Iteration 85, loss = 0.01809129
Iteration 86, loss = 0.01813621
Iteration 87, loss = 0.01790274
Iteration 88, loss = 0.01799069
Iteration 89, loss = 0.01783857
Iteration 90, loss = 0.01771918
Iteration 91, loss = 0.01764985
Iteration 92, loss = 0.01766206
Iteration 93, loss = 0.01774820
Iteration 94, loss = 0.01771988
Iteration 95, loss = 0.01764026
Iteration 96, loss = 0.01768575
Iteration 97, loss = 0.01742051
Iteration 98, loss = 0.01749538
Iteration 99, loss = 0.01746666
Iteration 100, loss = 0.01746703
Iteration 101, loss = 0.01751241
Iteration 102, loss = 0.01744097
Iteration 103, loss = 0.01742308
Iteration 104, loss = 0.01725033
Iteration 105, loss = 0.01732660
Iteration 106, loss = 0.01743716
Iteration 107, loss = 0.01732464
Iteration 108, loss = 0.01717829
Iteration 109, loss = 0.01712839
Iteration 110, loss = 0.01711532
Iteration 111, loss = 0.01717070
Iteration 112, loss = 0.01699737
Iteration 113, loss = 0.01711538
Iteration 114, loss = 0.01705101
Iteration 115, loss = 0.01713789
Iteration 116, loss = 0.01700684
Iteration 117, loss = 0.01695917
Iteration 118, loss = 0.01705648
Iteration 119, loss = 0.01717859
Iteration 120, loss = 0.01696818
Iteration 121, loss = 0.01696982
Iteration 122, loss = 0.01707868
Iteration 123, loss = 0.01694319
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71602014
Iteration 2, loss = 0.65982376
Iteration 3, loss = 0.57938464
Iteration 4, loss = 0.48016707
Iteration 5, loss = 0.38220628
Iteration 6, loss = 0.29742389
Iteration 7, loss = 0.23160651
Iteration 8, loss = 0.18293606
Iteration 9, loss = 0.14763590
Iteration 10, loss = 0.12204967
Iteration 11, loss = 0.10339343
Iteration 12, loss = 0.08925673
Iteration 13, loss = 0.07838636
Iteration 14, loss = 0.07023560
Iteration 15, loss = 0.06379404
Iteration 16, loss = 0.05823058
Iteration 17, loss = 0.05390421
Iteration 18, loss = 0.05042491
Iteration 19, loss = 0.04717643
Iteration 20, loss = 0.04475199
Iteration 21, loss = 0.04251634
Iteration 22, loss = 0.04080126
Iteration 23, loss = 0.03956344
Iteration 24, loss = 0.03800636
Iteration 25, loss = 0.03652636
Iteration 26, loss = 0.03546605
Iteration 27, loss = 0.03478450
Iteration 28, loss = 0.03380194
Iteration 29, loss = 0.03294749
Iteration 30, loss = 0.03193035
Iteration 31, loss = 0.03156161
Iteration 32, loss = 0.03107164
Iteration 33, loss = 0.03028388
Iteration 34, loss = 0.02963779
Iteration 35, loss = 0.02990258
Iteration 36, loss = 0.02960748
Iteration 37, loss = 0.02877588
Iteration 38, loss = 0.02863702
Iteration 39, loss = 0.02809129
Iteration 40, loss = 0.02790521
Iteration 41, loss = 0.02749268
Iteration 42, loss = 0.02746392
Iteration 43, loss = 0.02711964
Iteration 44, loss = 0.02681724
Iteration 45, loss = 0.02659329
Iteration 46, loss = 0.02662724
Iteration 47, loss = 0.02642472
Iteration 48, loss = 0.02641188
Iteration 49, loss = 0.02597589
Iteration 50, loss = 0.02599668
Iteration 51, loss = 0.02570381
Iteration 52, loss = 0.02609946
Iteration 53, loss = 0.02538690
Iteration 54, loss = 0.02543813
Iteration 55, loss = 0.02547291
Iteration 56, loss = 0.02509188
Iteration 57, loss = 0.02503282
Iteration 58, loss = 0.02515446
Iteration 59, loss = 0.02553640
Iteration 60, loss = 0.02558057
Iteration 61, loss = 0.02525643
Iteration 62, loss = 0.02499449
Iteration 63, loss = 0.02499498
Iteration 64, loss = 0.02498114
Iteration 65, loss = 0.02455032
Iteration 66, loss = 0.02464811
Iteration 67, loss = 0.02437525
Iteration 68, loss = 0.02438179
Iteration 69, loss = 0.02407080
Iteration 70, loss = 0.02412898
Iteration 71, loss = 0.02407609
Iteration 72, loss = 0.02407515
Iteration 73, loss = 0.02399262
Iteration 74, loss = 0.02496261
Iteration 75, loss = 0.02510670
Iteration 76, loss = 0.02466280
Iteration 77, loss = 0.02412074
Iteration 78, loss = 0.02402650
Iteration 79, loss = 0.02395145
Iteration 80, loss = 0.02356575
Iteration 81, loss = 0.02386044
Iteration 82, loss = 0.02380252
Iteration 83, loss = 0.02368440
Iteration 84, loss = 0.02353849
Iteration 85, loss = 0.02442670
Iteration 86, loss = 0.02412737
Iteration 87, loss = 0.02435558
Iteration 88, loss = 0.02434522
Iteration 89, loss = 0.02441326
Iteration 90, loss = 0.02419596
Iteration 91, loss = 0.02414942
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70276011
Iteration 2, loss = 0.64296619
Iteration 3, loss = 0.55518363
Iteration 4, loss = 0.44900702
Iteration 5, loss = 0.35278515
Iteration 6, loss = 0.27214656
Iteration 7, loss = 0.21109425
Iteration 8, loss = 0.16642963
Iteration 9, loss = 0.13411000
Iteration 10, loss = 0.11065636
Iteration 11, loss = 0.09322420
Iteration 12, loss = 0.08055507
Iteration 13, loss = 0.07064113
Iteration 14, loss = 0.06275023
Iteration 15, loss = 0.05635253
Iteration 16, loss = 0.05142007
Iteration 17, loss = 0.04728190
Iteration 18, loss = 0.04387646
Iteration 19, loss = 0.04087454
Iteration 20, loss = 0.03849665
Iteration 21, loss = 0.03654307
Iteration 22, loss = 0.03471414
Iteration 23, loss = 0.03321954
Iteration 24, loss = 0.03174707
Iteration 25, loss = 0.03056017
Iteration 26, loss = 0.02963171
Iteration 27, loss = 0.02920627
Iteration 28, loss = 0.02833571
Iteration 29, loss = 0.02754643
Iteration 30, loss = 0.02711636
Iteration 31, loss = 0.02615201
Iteration 32, loss = 0.02576852
Iteration 33, loss = 0.02527865
Iteration 34, loss = 0.02476864
Iteration 35, loss = 0.02422887
Iteration 36, loss = 0.02387611
Iteration 37, loss = 0.02365966
Iteration 38, loss = 0.02326451
Iteration 39, loss = 0.02294947
Iteration 40, loss = 0.02259350
Iteration 41, loss = 0.02225939
Iteration 42, loss = 0.02202582
Iteration 43, loss = 0.02185924
Iteration 44, loss = 0.02162661
Iteration 45, loss = 0.02156348
Iteration 46, loss = 0.02132493
Iteration 47, loss = 0.02109349
Iteration 48, loss = 0.02095973
Iteration 49, loss = 0.02108572
Iteration 50, loss = 0.02110389
Iteration 51, loss = 0.02082476
Iteration 52, loss = 0.02070258
Iteration 53, loss = 0.02092948
Iteration 54, loss = 0.02053571
Iteration 55, loss = 0.02025114
Iteration 56, loss = 0.02029389
Iteration 57, loss = 0.02032353
Iteration 58, loss = 0.02018799
Iteration 59, loss = 0.01979994
Iteration 60, loss = 0.01969732
Iteration 61, loss = 0.01983950
Iteration 62, loss = 0.02008151
Iteration 63, loss = 0.01980544
Iteration 64, loss = 0.01954204
Iteration 65, loss = 0.01998128
Iteration 66, loss = 0.01983609
Iteration 67, loss = 0.01952520
Iteration 68, loss = 0.01974038
Iteration 69, loss = 0.01942718
Iteration 70, loss = 0.01915407
Iteration 71, loss = 0.01931604
Iteration 72, loss = 0.01944127
Iteration 73, loss = 0.01935631
Iteration 74, loss = 0.01926122
Iteration 75, loss = 0.01918598
Iteration 76, loss = 0.01913474
Iteration 77, loss = 0.01913495
Iteration 78, loss = 0.01915634
Iteration 79, loss = 0.01880484
Iteration 80, loss = 0.01923793
Iteration 81, loss = 0.01928092
Iteration 82, loss = 0.01922494
Iteration 83, loss = 0.01875019
Iteration 84, loss = 0.01866860
Iteration 85, loss = 0.01875177
Iteration 86, loss = 0.02057843
Iteration 87, loss = 0.01950338
Iteration 88, loss = 0.01922125
Iteration 89, loss = 0.01889213
Iteration 90, loss = 0.01874717
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67966841
Iteration 2, loss = 0.64360453
Iteration 3, loss = 0.59209742
Iteration 4, loss = 0.52504148
Iteration 5, loss = 0.45419405
Iteration 6, loss = 0.38696310
Iteration 7, loss = 0.32636427
Iteration 8, loss = 0.27353524
Iteration 9, loss = 0.22947321
Iteration 10, loss = 0.19342070
Iteration 11, loss = 0.16410770
Iteration 12, loss = 0.14061521
Iteration 13, loss = 0.12208148
Iteration 14, loss = 0.10676009
Iteration 15, loss = 0.09460196
Iteration 16, loss = 0.08462498
Iteration 17, loss = 0.07635257
Iteration 18, loss = 0.06948786
Iteration 19, loss = 0.06365290
Iteration 20, loss = 0.05884396
Iteration 21, loss = 0.05463786
Iteration 22, loss = 0.05110897
Iteration 23, loss = 0.04796186
Iteration 24, loss = 0.04527716
Iteration 25, loss = 0.04275776
Iteration 26, loss = 0.04072996
Iteration 27, loss = 0.03906378
Iteration 28, loss = 0.03720924
Iteration 29, loss = 0.03584077
Iteration 30, loss = 0.03450622
Iteration 31, loss = 0.03322045
Iteration 32, loss = 0.03216559
Iteration 33, loss = 0.03120037
Iteration 34, loss = 0.03029613
Iteration 35, loss = 0.02958691
Iteration 36, loss = 0.02862860
Iteration 37, loss = 0.02795946
Iteration 38, loss = 0.02743397
Iteration 39, loss = 0.02682780
Iteration 40, loss = 0.02626018
Iteration 41, loss = 0.02579859
Iteration 42, loss = 0.02532597
Iteration 43, loss = 0.02491994
Iteration 44, loss = 0.02453676
Iteration 45, loss = 0.02419111
Iteration 46, loss = 0.02391789
Iteration 47, loss = 0.02344325
Iteration 48, loss = 0.02321290
Iteration 49, loss = 0.02287371
Iteration 50, loss = 0.02258998
Iteration 51, loss = 0.02238003
Iteration 52, loss = 0.02210115
Iteration 53, loss = 0.02181745
Iteration 54, loss = 0.02169728
Iteration 55, loss = 0.02158112
Iteration 56, loss = 0.02124899
Iteration 57, loss = 0.02112208
Iteration 58, loss = 0.02089978
Iteration 59, loss = 0.02085392
Iteration 60, loss = 0.02064799
Iteration 61, loss = 0.02051335
Iteration 62, loss = 0.02038216
Iteration 63, loss = 0.02024823
Iteration 64, loss = 0.02010868
Iteration 65, loss = 0.01997408
Iteration 66, loss = 0.01982981
Iteration 67, loss = 0.01978259
Iteration 68, loss = 0.01954007
Iteration 69, loss = 0.01963001
Iteration 70, loss = 0.01958929
Iteration 71, loss = 0.01949899
Iteration 72, loss = 0.01950990
Iteration 73, loss = 0.01920015
Iteration 74, loss = 0.01913718
Iteration 75, loss = 0.01903492
Iteration 76, loss = 0.01906116
Iteration 77, loss = 0.01880001
Iteration 78, loss = 0.01887538
Iteration 79, loss = 0.01878631
Iteration 80, loss = 0.01876849
Iteration 81, loss = 0.01875006
Iteration 82, loss = 0.01857215
Iteration 83, loss = 0.01851246
Iteration 84, loss = 0.01855870
Iteration 85, loss = 0.01845450
Iteration 86, loss = 0.01833508
Iteration 87, loss = 0.01823439
Iteration 88, loss = 0.01820968
Iteration 89, loss = 0.01841242
Iteration 90, loss = 0.01819286
Iteration 91, loss = 0.01818331
Iteration 92, loss = 0.01808394
Iteration 93, loss = 0.01807472
Iteration 94, loss = 0.01807791
Iteration 95, loss = 0.01794876
Iteration 96, loss = 0.01781997
Iteration 97, loss = 0.01792561
Iteration 98, loss = 0.01782642
Iteration 99, loss = 0.01796994
Iteration 100, loss = 0.01773508
Iteration 101, loss = 0.01787918
Iteration 102, loss = 0.01771301
Iteration 103, loss = 0.01762774
Iteration 104, loss = 0.01775649
Iteration 105, loss = 0.01772932
Iteration 106, loss = 0.01764063
Iteration 107, loss = 0.01772185
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69292577
Iteration 2, loss = 0.65226406
Iteration 3, loss = 0.59164879
Iteration 4, loss = 0.51196902
Iteration 5, loss = 0.43118059
Iteration 6, loss = 0.35642695
Iteration 7, loss = 0.29182869
Iteration 8, loss = 0.23853003
Iteration 9, loss = 0.19621969
Iteration 10, loss = 0.16313438
Iteration 11, loss = 0.13771302
Iteration 12, loss = 0.11771518
Iteration 13, loss = 0.10223080
Iteration 14, loss = 0.08972384
Iteration 15, loss = 0.07985664
Iteration 16, loss = 0.07176284
Iteration 17, loss = 0.06532626
Iteration 18, loss = 0.05998289
Iteration 19, loss = 0.05524482
Iteration 20, loss = 0.05165718
Iteration 21, loss = 0.04824013
Iteration 22, loss = 0.04546752
Iteration 23, loss = 0.04314497
Iteration 24, loss = 0.04101340
Iteration 25, loss = 0.03914749
Iteration 26, loss = 0.03769860
Iteration 27, loss = 0.03611503
Iteration 28, loss = 0.03485296
Iteration 29, loss = 0.03365145
Iteration 30, loss = 0.03274446
Iteration 31, loss = 0.03173772
Iteration 32, loss = 0.03083618
Iteration 33, loss = 0.03046217
Iteration 34, loss = 0.02948756
Iteration 35, loss = 0.02878736
Iteration 36, loss = 0.02840289
Iteration 37, loss = 0.02781964
Iteration 38, loss = 0.02746610
Iteration 39, loss = 0.02708282
Iteration 40, loss = 0.02653331
Iteration 41, loss = 0.02607705
Iteration 42, loss = 0.02564590
Iteration 43, loss = 0.02519066
Iteration 44, loss = 0.02501745
Iteration 45, loss = 0.02492748
Iteration 46, loss = 0.02456516
Iteration 47, loss = 0.02433004
Iteration 48, loss = 0.02408633
Iteration 49, loss = 0.02384842
Iteration 50, loss = 0.02347387
Iteration 51, loss = 0.02338715
Iteration 52, loss = 0.02380330
Iteration 53, loss = 0.02348488
Iteration 54, loss = 0.02323745
Iteration 55, loss = 0.02288008
Iteration 56, loss = 0.02276642
Iteration 57, loss = 0.02238026
Iteration 58, loss = 0.02229840
Iteration 59, loss = 0.02229286
Iteration 60, loss = 0.02227855
Iteration 61, loss = 0.02206224
Iteration 62, loss = 0.02191848
Iteration 63, loss = 0.02220926
Iteration 64, loss = 0.02179251
Iteration 65, loss = 0.02152005
Iteration 66, loss = 0.02164278
Iteration 67, loss = 0.02165830
Iteration 68, loss = 0.02129541
Iteration 69, loss = 0.02153337
Iteration 70, loss = 0.02133312
Iteration 71, loss = 0.02105562
Iteration 72, loss = 0.02121609
Iteration 73, loss = 0.02114667
Iteration 74, loss = 0.02100468
Iteration 75, loss = 0.02082586
Iteration 76, loss = 0.02085791
Iteration 77, loss = 0.02086041
Iteration 78, loss = 0.02068399
Iteration 79, loss = 0.02061660
Iteration 80, loss = 0.02069096
Iteration 81, loss = 0.02066917
Iteration 82, loss = 0.02066647
Iteration 83, loss = 0.02069827
Iteration 84, loss = 0.02050740
Iteration 85, loss = 0.02058795
Iteration 86, loss = 0.02050251
Iteration 87, loss = 0.02036317
Iteration 88, loss = 0.02033307
Iteration 89, loss = 0.02031754
Iteration 90, loss = 0.02017761
Iteration 91, loss = 0.02022181
Iteration 92, loss = 0.02013870
Iteration 93, loss = 0.02027724
Iteration 94, loss = 0.02026844
Iteration 95, loss = 0.02017683
Iteration 96, loss = 0.02027061
Iteration 97, loss = 0.02025600
Iteration 98, loss = 0.02006687
Iteration 99, loss = 0.02019553
Iteration 100, loss = 0.02014149
Iteration 101, loss = 0.02000128
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68997572
Iteration 2, loss = 0.67669341
Iteration 3, loss = 0.66496327
Iteration 4, loss = 0.65309603
Iteration 5, loss = 0.64046569
Iteration 6, loss = 0.62667937
Iteration 7, loss = 0.61151028
Iteration 8, loss = 0.59488341
Iteration 9, loss = 0.57681179
Iteration 10, loss = 0.55738108
Iteration 11, loss = 0.53674478
Iteration 12, loss = 0.51522967
Iteration 13, loss = 0.49276787
Iteration 14, loss = 0.46986567
Iteration 15, loss = 0.44677728
Iteration 16, loss = 0.42351488
Iteration 17, loss = 0.40085023
Iteration 18, loss = 0.37848104
Iteration 19, loss = 0.35685626
Iteration 20, loss = 0.33621533
Iteration 21, loss = 0.31644014
Iteration 22, loss = 0.29749773
Iteration 23, loss = 0.27963895
Iteration 24, loss = 0.26275436
Iteration 25, loss = 0.24678691
Iteration 26, loss = 0.23184154
Iteration 27, loss = 0.21768396
Iteration 28, loss = 0.20457963
Iteration 29, loss = 0.19221417
Iteration 30, loss = 0.18084394
Iteration 31, loss = 0.17016626
Iteration 32, loss = 0.16022641
Iteration 33, loss = 0.15094458
Iteration 34, loss = 0.14234469
Iteration 35, loss = 0.13434682
Iteration 36, loss = 0.12695429
Iteration 37, loss = 0.12006301
Iteration 38, loss = 0.11368862
Iteration 39, loss = 0.10772031
Iteration 40, loss = 0.10224087
Iteration 41, loss = 0.09709019
Iteration 42, loss = 0.09232945
Iteration 43, loss = 0.08793600
Iteration 44, loss = 0.08381200
Iteration 45, loss = 0.07995707
Iteration 46, loss = 0.07635573
Iteration 47, loss = 0.07305942
Iteration 48, loss = 0.06995983
Iteration 49, loss = 0.06703334
Iteration 50, loss = 0.06430284
Iteration 51, loss = 0.06175087
Iteration 52, loss = 0.05935779
Iteration 53, loss = 0.05712264
Iteration 54, loss = 0.05503476
Iteration 55, loss = 0.05302702
Iteration 56, loss = 0.05117432
Iteration 57, loss = 0.04938384
Iteration 58, loss = 0.04774259
Iteration 59, loss = 0.04613907
Iteration 60, loss = 0.04467237
Iteration 61, loss = 0.04325209
Iteration 62, loss = 0.04193407
Iteration 63, loss = 0.04065422
Iteration 64, loss = 0.03947308
Iteration 65, loss = 0.03837106
Iteration 66, loss = 0.03724943
Iteration 67, loss = 0.03623942
Iteration 68, loss = 0.03530050
Iteration 69, loss = 0.03436907
Iteration 70, loss = 0.03350008
Iteration 71, loss = 0.03264020
Iteration 72, loss = 0.03186536
Iteration 73, loss = 0.03108128
Iteration 74, loss = 0.03035851
Iteration 75, loss = 0.02966197
Iteration 76, loss = 0.02901639
Iteration 77, loss = 0.02837257
Iteration 78, loss = 0.02775131
Iteration 79, loss = 0.02717260
Iteration 80, loss = 0.02662631
Iteration 81, loss = 0.02609389
Iteration 82, loss = 0.02554741
Iteration 83, loss = 0.02506991
Iteration 84, loss = 0.02458967
Iteration 85, loss = 0.02413453
Iteration 86, loss = 0.02371099
Iteration 87, loss = 0.02330398
Iteration 88, loss = 0.02289827
Iteration 89, loss = 0.02249742
Iteration 90, loss = 0.02213613
Iteration 91, loss = 0.02174600
Iteration 92, loss = 0.02141024
Iteration 93, loss = 0.02110361
Iteration 94, loss = 0.02079772
Iteration 95, loss = 0.02052570
Iteration 96, loss = 0.02017212
Iteration 97, loss = 0.01983912
Iteration 98, loss = 0.01953734
Iteration 99, loss = 0.01924998
Iteration 100, loss = 0.01901723
Iteration 101, loss = 0.01872149
Iteration 102, loss = 0.01849370
Iteration 103, loss = 0.01822364
Iteration 104, loss = 0.01801651
Iteration 105, loss = 0.01779827
Iteration 106, loss = 0.01754762
Iteration 107, loss = 0.01732147
Iteration 108, loss = 0.01711762
Iteration 109, loss = 0.01689878
Iteration 110, loss = 0.01677777
Iteration 111, loss = 0.01654305
Iteration 112, loss = 0.01635773
Iteration 113, loss = 0.01618354
Iteration 114, loss = 0.01600634
Iteration 115, loss = 0.01583813
Iteration 116, loss = 0.01565807
Iteration 117, loss = 0.01556984
Iteration 118, loss = 0.01532726
Iteration 119, loss = 0.01517519
Iteration 120, loss = 0.01503649
Iteration 121, loss = 0.01490440
Iteration 122, loss = 0.01475941
Iteration 123, loss = 0.01460989
Iteration 124, loss = 0.01447199
Iteration 125, loss = 0.01436353
Iteration 126, loss = 0.01420986
Iteration 127, loss = 0.01412974
Iteration 128, loss = 0.01398655
Iteration 129, loss = 0.01387247
Iteration 130, loss = 0.01377530
Iteration 131, loss = 0.01365292
Iteration 132, loss = 0.01355534
Iteration 133, loss = 0.01344687
Iteration 134, loss = 0.01330894
Iteration 135, loss = 0.01320873
Iteration 136, loss = 0.01309600
Iteration 137, loss = 0.01307256
Iteration 138, loss = 0.01295040
Iteration 139, loss = 0.01286187
Iteration 140, loss = 0.01283584
Iteration 141, loss = 0.01272876
Iteration 142, loss = 0.01264775
Iteration 143, loss = 0.01256239
Iteration 144, loss = 0.01247547
Iteration 145, loss = 0.01238442
Iteration 146, loss = 0.01230508
Iteration 147, loss = 0.01226569
Iteration 148, loss = 0.01214730
Iteration 149, loss = 0.01205435
Iteration 150, loss = 0.01194826
Iteration 151, loss = 0.01187937
Iteration 152, loss = 0.01180623
Iteration 153, loss = 0.01175982
Iteration 154, loss = 0.01168808
Iteration 155, loss = 0.01161023
Iteration 156, loss = 0.01155646
Iteration 157, loss = 0.01149202
Iteration 158, loss = 0.01142521
Iteration 159, loss = 0.01135245
Iteration 160, loss = 0.01130179
Iteration 161, loss = 0.01125471
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69311293
Iteration 2, loss = 0.67720823
Iteration 3, loss = 0.66125525
Iteration 4, loss = 0.64163224
Iteration 5, loss = 0.61757370
Iteration 6, loss = 0.58906523
Iteration 7, loss = 0.55703668
Iteration 8, loss = 0.52280008
Iteration 9, loss = 0.48750534
Iteration 10, loss = 0.45183022
Iteration 11, loss = 0.41684931
Iteration 12, loss = 0.38271328
Iteration 13, loss = 0.35006764
Iteration 14, loss = 0.31939840
Iteration 15, loss = 0.29065880
Iteration 16, loss = 0.26412699
Iteration 17, loss = 0.23977770
Iteration 18, loss = 0.21772462
Iteration 19, loss = 0.19781789
Iteration 20, loss = 0.17997199
Iteration 21, loss = 0.16400961
Iteration 22, loss = 0.14978365
Iteration 23, loss = 0.13712053
Iteration 24, loss = 0.12586130
Iteration 25, loss = 0.11577487
Iteration 26, loss = 0.10691185
Iteration 27, loss = 0.09896168
Iteration 28, loss = 0.09183882
Iteration 29, loss = 0.08545015
Iteration 30, loss = 0.07976627
Iteration 31, loss = 0.07464654
Iteration 32, loss = 0.07005624
Iteration 33, loss = 0.06590071
Iteration 34, loss = 0.06212487
Iteration 35, loss = 0.05868079
Iteration 36, loss = 0.05553406
Iteration 37, loss = 0.05271769
Iteration 38, loss = 0.05011934
Iteration 39, loss = 0.04769284
Iteration 40, loss = 0.04554556
Iteration 41, loss = 0.04346829
Iteration 42, loss = 0.04163146
Iteration 43, loss = 0.03995967
Iteration 44, loss = 0.03831276
Iteration 45, loss = 0.03685554
Iteration 46, loss = 0.03546821
Iteration 47, loss = 0.03421086
Iteration 48, loss = 0.03299097
Iteration 49, loss = 0.03186342
Iteration 50, loss = 0.03081413
Iteration 51, loss = 0.02986968
Iteration 52, loss = 0.02894530
Iteration 53, loss = 0.02809371
Iteration 54, loss = 0.02727933
Iteration 55, loss = 0.02653077
Iteration 56, loss = 0.02575635
Iteration 57, loss = 0.02513508
Iteration 58, loss = 0.02448554
Iteration 59, loss = 0.02386076
Iteration 60, loss = 0.02328042
Iteration 61, loss = 0.02274420
Iteration 62, loss = 0.02222887
Iteration 63, loss = 0.02176684
Iteration 64, loss = 0.02125199
Iteration 65, loss = 0.02082858
Iteration 66, loss = 0.02040717
Iteration 67, loss = 0.01998758
Iteration 68, loss = 0.01961836
Iteration 69, loss = 0.01929396
Iteration 70, loss = 0.01889586
Iteration 71, loss = 0.01855634
Iteration 72, loss = 0.01826539
Iteration 73, loss = 0.01796211
Iteration 74, loss = 0.01766964
Iteration 75, loss = 0.01735457
Iteration 76, loss = 0.01711788
Iteration 77, loss = 0.01686698
Iteration 78, loss = 0.01662639
Iteration 79, loss = 0.01638921
Iteration 80, loss = 0.01614749
Iteration 81, loss = 0.01590357
Iteration 82, loss = 0.01570686
Iteration 83, loss = 0.01553121
Iteration 84, loss = 0.01532691
Iteration 85, loss = 0.01514082
Iteration 86, loss = 0.01494103
Iteration 87, loss = 0.01477095
Iteration 88, loss = 0.01460666
Iteration 89, loss = 0.01446715
Iteration 90, loss = 0.01428076
Iteration 91, loss = 0.01412970
Iteration 92, loss = 0.01398690
Iteration 93, loss = 0.01385325
Iteration 94, loss = 0.01370041
Iteration 95, loss = 0.01355493
Iteration 96, loss = 0.01348736
Iteration 97, loss = 0.01337217
Iteration 98, loss = 0.01320397
Iteration 99, loss = 0.01309176
Iteration 100, loss = 0.01297873
Iteration 101, loss = 0.01289238
Iteration 102, loss = 0.01279860
Iteration 103, loss = 0.01266878
Iteration 104, loss = 0.01257878
Iteration 105, loss = 0.01247027
Iteration 106, loss = 0.01240789
Iteration 107, loss = 0.01225905
Iteration 108, loss = 0.01220386
Iteration 109, loss = 0.01215501
Iteration 110, loss = 0.01204822
Iteration 111, loss = 0.01197704
Iteration 112, loss = 0.01186776
Iteration 113, loss = 0.01178960
Iteration 114, loss = 0.01172215
Iteration 115, loss = 0.01166809
Iteration 116, loss = 0.01158188
Iteration 117, loss = 0.01155916
Iteration 118, loss = 0.01142611
Iteration 119, loss = 0.01139956
Iteration 120, loss = 0.01132569
Iteration 121, loss = 0.01132390
Iteration 122, loss = 0.01120240
Iteration 123, loss = 0.01116095
Iteration 124, loss = 0.01110923
Iteration 125, loss = 0.01104292
Iteration 126, loss = 0.01099659
Iteration 127, loss = 0.01090592
Iteration 128, loss = 0.01090427
Iteration 129, loss = 0.01083477
Iteration 130, loss = 0.01079783
Iteration 131, loss = 0.01074878
Iteration 132, loss = 0.01067855
Iteration 133, loss = 0.01065934
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68184420
Iteration 2, loss = 0.62952234
Iteration 3, loss = 0.55383020
Iteration 4, loss = 0.46173677
Iteration 5, loss = 0.37353950
Iteration 6, loss = 0.29727573
Iteration 7, loss = 0.23590640
Iteration 8, loss = 0.18867299
Iteration 9, loss = 0.15330833
Iteration 10, loss = 0.12692967
Iteration 11, loss = 0.10714251
Iteration 12, loss = 0.09211264
Iteration 13, loss = 0.08056115
Iteration 14, loss = 0.07124157
Iteration 15, loss = 0.06414467
Iteration 16, loss = 0.05824046
Iteration 17, loss = 0.05343427
Iteration 18, loss = 0.04936851
Iteration 19, loss = 0.04626590
Iteration 20, loss = 0.04329401
Iteration 21, loss = 0.04092822
Iteration 22, loss = 0.03894083
Iteration 23, loss = 0.03696143
Iteration 24, loss = 0.03561293
Iteration 25, loss = 0.03396778
Iteration 26, loss = 0.03289814
Iteration 27, loss = 0.03175917
Iteration 28, loss = 0.03085230
Iteration 29, loss = 0.03010084
Iteration 30, loss = 0.02911746
Iteration 31, loss = 0.02867860
Iteration 32, loss = 0.02792987
Iteration 33, loss = 0.02742674
Iteration 34, loss = 0.02695607
Iteration 35, loss = 0.02633129
Iteration 36, loss = 0.02595478
Iteration 37, loss = 0.02548567
Iteration 38, loss = 0.02529768
Iteration 39, loss = 0.02499110
Iteration 40, loss = 0.02452790
Iteration 41, loss = 0.02429802
Iteration 42, loss = 0.02395242
Iteration 43, loss = 0.02380252
Iteration 44, loss = 0.02403943
Iteration 45, loss = 0.02334653
Iteration 46, loss = 0.02298178
Iteration 47, loss = 0.02290278
Iteration 48, loss = 0.02277518
Iteration 49, loss = 0.02272934
Iteration 50, loss = 0.02265791
Iteration 51, loss = 0.02230492
Iteration 52, loss = 0.02212813
Iteration 53, loss = 0.02217688
Iteration 54, loss = 0.02204447
Iteration 55, loss = 0.02176973
Iteration 56, loss = 0.02178758
Iteration 57, loss = 0.02144983
Iteration 58, loss = 0.02137107
Iteration 59, loss = 0.02143932
Iteration 60, loss = 0.02134834
Iteration 61, loss = 0.02133050
Iteration 62, loss = 0.02113856
Iteration 63, loss = 0.02116652
Iteration 64, loss = 0.02091642
Iteration 65, loss = 0.02111655
Iteration 66, loss = 0.02080304
Iteration 67, loss = 0.02087039
Iteration 68, loss = 0.02101851
Iteration 69, loss = 0.02070005
Iteration 70, loss = 0.02063176
Iteration 71, loss = 0.02086035
Iteration 72, loss = 0.02073643
Iteration 73, loss = 0.02060772
Iteration 74, loss = 0.02050189
Iteration 75, loss = 0.02049830
Iteration 76, loss = 0.02050573
Iteration 77, loss = 0.02052177
Iteration 78, loss = 0.02058405
Iteration 79, loss = 0.02051820
Iteration 80, loss = 0.02025342
Iteration 81, loss = 0.02028192
Iteration 82, loss = 0.02032336
Iteration 83, loss = 0.02006521
Iteration 84, loss = 0.02015386
Iteration 85, loss = 0.02031136
Iteration 86, loss = 0.02016492
Iteration 87, loss = 0.01997190
Iteration 88, loss = 0.01999820
Iteration 89, loss = 0.02004880
Iteration 90, loss = 0.02015378
Iteration 91, loss = 0.02006130
Iteration 92, loss = 0.01986878
Iteration 93, loss = 0.01997316
Iteration 94, loss = 0.02014353
Iteration 95, loss = 0.01992218
Iteration 96, loss = 0.01985808
Iteration 97, loss = 0.01980586
Iteration 98, loss = 0.01981559
Iteration 99, loss = 0.01982519
Iteration 100, loss = 0.01972608
Iteration 101, loss = 0.01977981
Iteration 102, loss = 0.01978875
Iteration 103, loss = 0.01987540
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68284097
Iteration 2, loss = 0.61820352
Iteration 3, loss = 0.51351853
Iteration 4, loss = 0.39858386
Iteration 5, loss = 0.29959230
Iteration 6, loss = 0.22463388
Iteration 7, loss = 0.17175407
Iteration 8, loss = 0.13514591
Iteration 9, loss = 0.10993029
Iteration 10, loss = 0.09221291
Iteration 11, loss = 0.07925527
Iteration 12, loss = 0.06942862
Iteration 13, loss = 0.06204128
Iteration 14, loss = 0.05642974
Iteration 15, loss = 0.05195843
Iteration 16, loss = 0.04834929
Iteration 17, loss = 0.04533728
Iteration 18, loss = 0.04266816
Iteration 19, loss = 0.04086629
Iteration 20, loss = 0.03881111
Iteration 21, loss = 0.03759809
Iteration 22, loss = 0.03637813
Iteration 23, loss = 0.03534829
Iteration 24, loss = 0.03443763
Iteration 25, loss = 0.03332649
Iteration 26, loss = 0.03272203
Iteration 27, loss = 0.03205188
Iteration 28, loss = 0.03178865
Iteration 29, loss = 0.03107210
Iteration 30, loss = 0.03068128
Iteration 31, loss = 0.03027433
Iteration 32, loss = 0.02969234
Iteration 33, loss = 0.02910561
Iteration 34, loss = 0.02925042
Iteration 35, loss = 0.02866484
Iteration 36, loss = 0.02874179
Iteration 37, loss = 0.02828291
Iteration 38, loss = 0.02820872
Iteration 39, loss = 0.02808329
Iteration 40, loss = 0.02782100
Iteration 41, loss = 0.02780462
Iteration 42, loss = 0.02734749
Iteration 43, loss = 0.02727761
Iteration 44, loss = 0.02721605
Iteration 45, loss = 0.02711727
Iteration 46, loss = 0.02696144
Iteration 47, loss = 0.02663952
Iteration 48, loss = 0.02670605
Iteration 49, loss = 0.02680777
Iteration 50, loss = 0.02655798
Iteration 51, loss = 0.02629357
Iteration 52, loss = 0.02632008
Iteration 53, loss = 0.02631975
Iteration 54, loss = 0.02646662
Iteration 55, loss = 0.02617318
Iteration 56, loss = 0.02619271
Iteration 57, loss = 0.02586951
Iteration 58, loss = 0.02601265
Iteration 59, loss = 0.02615067
Iteration 60, loss = 0.02575575
Iteration 61, loss = 0.02594693
Iteration 62, loss = 0.02546084
Iteration 63, loss = 0.02559757
Iteration 64, loss = 0.02548247
Iteration 65, loss = 0.02541829
Iteration 66, loss = 0.02535221
Iteration 67, loss = 0.02564259
Iteration 68, loss = 0.02549236
Iteration 69, loss = 0.02535090
Iteration 70, loss = 0.02555978
Iteration 71, loss = 0.02563311
Iteration 72, loss = 0.02552289
Iteration 73, loss = 0.02599612
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67463749
Iteration 2, loss = 0.61145476
Iteration 3, loss = 0.51445314
Iteration 4, loss = 0.40875508
Iteration 5, loss = 0.31684544
Iteration 6, loss = 0.24335622
Iteration 7, loss = 0.18964244
Iteration 8, loss = 0.15060136
Iteration 9, loss = 0.12288204
Iteration 10, loss = 0.10257147
Iteration 11, loss = 0.08740760
Iteration 12, loss = 0.07635799
Iteration 13, loss = 0.06747860
Iteration 14, loss = 0.06061660
Iteration 15, loss = 0.05527034
Iteration 16, loss = 0.05075730
Iteration 17, loss = 0.04726714
Iteration 18, loss = 0.04404260
Iteration 19, loss = 0.04161404
Iteration 20, loss = 0.03963298
Iteration 21, loss = 0.03755236
Iteration 22, loss = 0.03592825
Iteration 23, loss = 0.03452471
Iteration 24, loss = 0.03358308
Iteration 25, loss = 0.03239954
Iteration 26, loss = 0.03138748
Iteration 27, loss = 0.03042532
Iteration 28, loss = 0.02988667
Iteration 29, loss = 0.02918275
Iteration 30, loss = 0.02867535
Iteration 31, loss = 0.02812988
Iteration 32, loss = 0.02743958
Iteration 33, loss = 0.02731852
Iteration 34, loss = 0.02655942
Iteration 35, loss = 0.02648215
Iteration 36, loss = 0.02635355
Iteration 37, loss = 0.02552679
Iteration 38, loss = 0.02582370
Iteration 39, loss = 0.02530611
Iteration 40, loss = 0.02477617
Iteration 41, loss = 0.02464120
Iteration 42, loss = 0.02445591
Iteration 43, loss = 0.02426415
Iteration 44, loss = 0.02401436
Iteration 45, loss = 0.02408876
Iteration 46, loss = 0.02394236
Iteration 47, loss = 0.02367196
Iteration 48, loss = 0.02370634
Iteration 49, loss = 0.02340005
Iteration 50, loss = 0.02316824
Iteration 51, loss = 0.02304871
Iteration 52, loss = 0.02321393
Iteration 53, loss = 0.02282136
Iteration 54, loss = 0.02284727
Iteration 55, loss = 0.02272969
Iteration 56, loss = 0.02299754
Iteration 57, loss = 0.02289138
Iteration 58, loss = 0.02248121
Iteration 59, loss = 0.02261472
Iteration 60, loss = 0.02236177
Iteration 61, loss = 0.02233757
Iteration 62, loss = 0.02209321
Iteration 63, loss = 0.02253760
Iteration 64, loss = 0.02220182
Iteration 65, loss = 0.02206967
Iteration 66, loss = 0.02223458
Iteration 67, loss = 0.02226444
Iteration 68, loss = 0.02236832
Iteration 69, loss = 0.02232220
Iteration 70, loss = 0.02192616
Iteration 71, loss = 0.02184140
Iteration 72, loss = 0.02208276
Iteration 73, loss = 0.02159979
Iteration 74, loss = 0.02176685
Iteration 75, loss = 0.02202989
Iteration 76, loss = 0.02197576
Iteration 77, loss = 0.02176606
Iteration 78, loss = 0.02170629
Iteration 79, loss = 0.02147086
Iteration 80, loss = 0.02157300
Iteration 81, loss = 0.02140726
Iteration 82, loss = 0.02151476
Iteration 83, loss = 0.02150341
Iteration 84, loss = 0.02154693
Iteration 85, loss = 0.02140377
Iteration 86, loss = 0.02138101
Iteration 87, loss = 0.02146772
Iteration 88, loss = 0.02140986
Iteration 89, loss = 0.02128954
Iteration 90, loss = 0.02126370
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69246532
Iteration 2, loss = 0.67281330
Iteration 3, loss = 0.65445983
Iteration 4, loss = 0.63386659
Iteration 5, loss = 0.60964558
Iteration 6, loss = 0.58183283
Iteration 7, loss = 0.55060450
Iteration 8, loss = 0.51706288
Iteration 9, loss = 0.48192643
Iteration 10, loss = 0.44664486
Iteration 11, loss = 0.41199372
Iteration 12, loss = 0.37854968
Iteration 13, loss = 0.34664865
Iteration 14, loss = 0.31642080
Iteration 15, loss = 0.28832633
Iteration 16, loss = 0.26233194
Iteration 17, loss = 0.23854601
Iteration 18, loss = 0.21691861
Iteration 19, loss = 0.19734521
Iteration 20, loss = 0.17980113
Iteration 21, loss = 0.16399278
Iteration 22, loss = 0.14994898
Iteration 23, loss = 0.13737654
Iteration 24, loss = 0.12611943
Iteration 25, loss = 0.11613068
Iteration 26, loss = 0.10716418
Iteration 27, loss = 0.09920654
Iteration 28, loss = 0.09202533
Iteration 29, loss = 0.08562740
Iteration 30, loss = 0.07988213
Iteration 31, loss = 0.07468097
Iteration 32, loss = 0.07000933
Iteration 33, loss = 0.06575049
Iteration 34, loss = 0.06193508
Iteration 35, loss = 0.05842250
Iteration 36, loss = 0.05524839
Iteration 37, loss = 0.05229446
Iteration 38, loss = 0.04964415
Iteration 39, loss = 0.04719919
Iteration 40, loss = 0.04494502
Iteration 41, loss = 0.04290230
Iteration 42, loss = 0.04101718
Iteration 43, loss = 0.03918026
Iteration 44, loss = 0.03755734
Iteration 45, loss = 0.03603120
Iteration 46, loss = 0.03465926
Iteration 47, loss = 0.03331128
Iteration 48, loss = 0.03205559
Iteration 49, loss = 0.03089897
Iteration 50, loss = 0.02985204
Iteration 51, loss = 0.02886173
Iteration 52, loss = 0.02788923
Iteration 53, loss = 0.02700303
Iteration 54, loss = 0.02619746
Iteration 55, loss = 0.02537937
Iteration 56, loss = 0.02466422
Iteration 57, loss = 0.02396832
Iteration 58, loss = 0.02328939
Iteration 59, loss = 0.02270202
Iteration 60, loss = 0.02207073
Iteration 61, loss = 0.02153847
Iteration 62, loss = 0.02099641
Iteration 63, loss = 0.02044685
Iteration 64, loss = 0.01994864
Iteration 65, loss = 0.01951620
Iteration 66, loss = 0.01908691
Iteration 67, loss = 0.01870171
Iteration 68, loss = 0.01831736
Iteration 69, loss = 0.01792183
Iteration 70, loss = 0.01754408
Iteration 71, loss = 0.01722055
Iteration 72, loss = 0.01689671
Iteration 73, loss = 0.01659959
Iteration 74, loss = 0.01628837
Iteration 75, loss = 0.01597840
Iteration 76, loss = 0.01573653
Iteration 77, loss = 0.01544067
Iteration 78, loss = 0.01519808
Iteration 79, loss = 0.01492508
Iteration 80, loss = 0.01473232
Iteration 81, loss = 0.01446211
Iteration 82, loss = 0.01426205
Iteration 83, loss = 0.01405376
Iteration 84, loss = 0.01386950
Iteration 85, loss = 0.01373349
Iteration 86, loss = 0.01346707
Iteration 87, loss = 0.01330036
Iteration 88, loss = 0.01312806
Iteration 89, loss = 0.01297304
Iteration 90, loss = 0.01279815
Iteration 91, loss = 0.01264006
Iteration 92, loss = 0.01250759
Iteration 93, loss = 0.01234749
Iteration 94, loss = 0.01226037
Iteration 95, loss = 0.01209921
Iteration 96, loss = 0.01197212
Iteration 97, loss = 0.01183180
Iteration 98, loss = 0.01171380
Iteration 99, loss = 0.01159464
Iteration 100, loss = 0.01147417
Iteration 101, loss = 0.01137180
Iteration 102, loss = 0.01126012
Iteration 103, loss = 0.01110754
Iteration 104, loss = 0.01103581
Iteration 105, loss = 0.01091347
Iteration 106, loss = 0.01087862
Iteration 107, loss = 0.01077361
Iteration 108, loss = 0.01071830
Iteration 109, loss = 0.01058086
Iteration 110, loss = 0.01046594
Iteration 111, loss = 0.01042456
Iteration 112, loss = 0.01035677
Iteration 113, loss = 0.01021378
Iteration 114, loss = 0.01021230
Iteration 115, loss = 0.01014142
Iteration 116, loss = 0.01003873
Iteration 117, loss = 0.00992614
Iteration 118, loss = 0.00994198
Iteration 119, loss = 0.00984478
Iteration 120, loss = 0.00975965
Iteration 121, loss = 0.00965179
Iteration 122, loss = 0.00964435
Iteration 123, loss = 0.00956129
Iteration 124, loss = 0.00949812
Iteration 125, loss = 0.00944757
Iteration 126, loss = 0.00940773
Iteration 127, loss = 0.00938750
Iteration 128, loss = 0.00926693
Iteration 129, loss = 0.00925405
Iteration 130, loss = 0.00917796
Iteration 131, loss = 0.00912495
Iteration 132, loss = 0.00910237
Iteration 133, loss = 0.00904052
Iteration 134, loss = 0.00899246
Iteration 135, loss = 0.00893518
Iteration 136, loss = 0.00895421
Iteration 137, loss = 0.00885394
Iteration 138, loss = 0.00883406
Iteration 139, loss = 0.00875942
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68894869
Iteration 2, loss = 0.65857886
Iteration 3, loss = 0.61809534
Iteration 4, loss = 0.56584251
Iteration 5, loss = 0.50573975
Iteration 6, loss = 0.44467048
Iteration 7, loss = 0.38639906
Iteration 8, loss = 0.33262397
Iteration 9, loss = 0.28457445
Iteration 10, loss = 0.24304232
Iteration 11, loss = 0.20751392
Iteration 12, loss = 0.17802644
Iteration 13, loss = 0.15350530
Iteration 14, loss = 0.13336895
Iteration 15, loss = 0.11677716
Iteration 16, loss = 0.10307683
Iteration 17, loss = 0.09182414
Iteration 18, loss = 0.08233639
Iteration 19, loss = 0.07445906
Iteration 20, loss = 0.06774986
Iteration 21, loss = 0.06203623
Iteration 22, loss = 0.05719610
Iteration 23, loss = 0.05296043
Iteration 24, loss = 0.04933379
Iteration 25, loss = 0.04617797
Iteration 26, loss = 0.04336005
Iteration 27, loss = 0.04080786
Iteration 28, loss = 0.03856445
Iteration 29, loss = 0.03670470
Iteration 30, loss = 0.03500956
Iteration 31, loss = 0.03341649
Iteration 32, loss = 0.03196379
Iteration 33, loss = 0.03063934
Iteration 34, loss = 0.02946389
Iteration 35, loss = 0.02835663
Iteration 36, loss = 0.02739770
Iteration 37, loss = 0.02654785
Iteration 38, loss = 0.02570878
Iteration 39, loss = 0.02501193
Iteration 40, loss = 0.02433692
Iteration 41, loss = 0.02371416
Iteration 42, loss = 0.02306011
Iteration 43, loss = 0.02246076
Iteration 44, loss = 0.02200610
Iteration 45, loss = 0.02154893
Iteration 46, loss = 0.02104667
Iteration 47, loss = 0.02073731
Iteration 48, loss = 0.02033647
Iteration 49, loss = 0.01999279
Iteration 50, loss = 0.01967629
Iteration 51, loss = 0.01932027
Iteration 52, loss = 0.01893326
Iteration 53, loss = 0.01877815
Iteration 54, loss = 0.01845087
Iteration 55, loss = 0.01819708
Iteration 56, loss = 0.01790322
Iteration 57, loss = 0.01780507
Iteration 58, loss = 0.01744658
Iteration 59, loss = 0.01735351
Iteration 60, loss = 0.01712003
Iteration 61, loss = 0.01701405
Iteration 62, loss = 0.01672882
Iteration 63, loss = 0.01656719
Iteration 64, loss = 0.01639856
Iteration 65, loss = 0.01640949
Iteration 66, loss = 0.01623822
Iteration 67, loss = 0.01602827
Iteration 68, loss = 0.01595871
Iteration 69, loss = 0.01583699
Iteration 70, loss = 0.01558458
Iteration 71, loss = 0.01552427
Iteration 72, loss = 0.01546986
Iteration 73, loss = 0.01528424
Iteration 74, loss = 0.01521658
Iteration 75, loss = 0.01516290
Iteration 76, loss = 0.01510798
Iteration 77, loss = 0.01497495
Iteration 78, loss = 0.01485562
Iteration 79, loss = 0.01476801
Iteration 80, loss = 0.01476785
Iteration 81, loss = 0.01482040
Iteration 82, loss = 0.01466993
Iteration 83, loss = 0.01451372
Iteration 84, loss = 0.01452361
Iteration 85, loss = 0.01441095
Iteration 86, loss = 0.01436557
Iteration 87, loss = 0.01434403
Iteration 88, loss = 0.01424737
Iteration 89, loss = 0.01427016
Iteration 90, loss = 0.01412417
Iteration 91, loss = 0.01411638
Iteration 92, loss = 0.01401494
Iteration 93, loss = 0.01398693
Iteration 94, loss = 0.01400208
Iteration 95, loss = 0.01397874
Iteration 96, loss = 0.01380070
Iteration 97, loss = 0.01388515
Iteration 98, loss = 0.01375321
Iteration 99, loss = 0.01379937
Iteration 100, loss = 0.01366804
Iteration 101, loss = 0.01363089
Iteration 102, loss = 0.01366949
Iteration 103, loss = 0.01363458
Iteration 104, loss = 0.01350315
Iteration 105, loss = 0.01350747
Iteration 106, loss = 0.01343773
Iteration 107, loss = 0.01344629
Iteration 108, loss = 0.01342525
Iteration 109, loss = 0.01336312
Iteration 110, loss = 0.01329122
Iteration 111, loss = 0.01344675
Iteration 112, loss = 0.01335406
Iteration 113, loss = 0.01328287
Iteration 114, loss = 0.01336226
Iteration 115, loss = 0.01327436
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69472930
Iteration 2, loss = 0.64739460
Iteration 3, loss = 0.57826049
Iteration 4, loss = 0.48971939
Iteration 5, loss = 0.40096672
Iteration 6, loss = 0.31955212
Iteration 7, loss = 0.25290486
Iteration 8, loss = 0.20144557
Iteration 9, loss = 0.16298337
Iteration 10, loss = 0.13446960
Iteration 11, loss = 0.11304005
Iteration 12, loss = 0.09682609
Iteration 13, loss = 0.08436771
Iteration 14, loss = 0.07444308
Iteration 15, loss = 0.06668827
Iteration 16, loss = 0.06052297
Iteration 17, loss = 0.05528755
Iteration 18, loss = 0.05107237
Iteration 19, loss = 0.04753442
Iteration 20, loss = 0.04434902
Iteration 21, loss = 0.04204041
Iteration 22, loss = 0.03994753
Iteration 23, loss = 0.03789768
Iteration 24, loss = 0.03611208
Iteration 25, loss = 0.03487658
Iteration 26, loss = 0.03330489
Iteration 27, loss = 0.03223682
Iteration 28, loss = 0.03150009
Iteration 29, loss = 0.03030492
Iteration 30, loss = 0.02962926
Iteration 31, loss = 0.02898892
Iteration 32, loss = 0.02825329
Iteration 33, loss = 0.02741518
Iteration 34, loss = 0.02711984
Iteration 35, loss = 0.02657215
Iteration 36, loss = 0.02598420
Iteration 37, loss = 0.02574939
Iteration 38, loss = 0.02567694
Iteration 39, loss = 0.02501431
Iteration 40, loss = 0.02477519
Iteration 41, loss = 0.02450750
Iteration 42, loss = 0.02403194
Iteration 43, loss = 0.02409935
Iteration 44, loss = 0.02355344
Iteration 45, loss = 0.02339925
Iteration 46, loss = 0.02315686
Iteration 47, loss = 0.02302945
Iteration 48, loss = 0.02293326
Iteration 49, loss = 0.02275501
Iteration 50, loss = 0.02259225
Iteration 51, loss = 0.02265791
Iteration 52, loss = 0.02235007
Iteration 53, loss = 0.02234717
Iteration 54, loss = 0.02186978
Iteration 55, loss = 0.02173704
Iteration 56, loss = 0.02177402
Iteration 57, loss = 0.02189510
Iteration 58, loss = 0.02166380
Iteration 59, loss = 0.02144925
Iteration 60, loss = 0.02147091
Iteration 61, loss = 0.02144509
Iteration 62, loss = 0.02111104
Iteration 63, loss = 0.02120640
Iteration 64, loss = 0.02135228
Iteration 65, loss = 0.02103118
Iteration 66, loss = 0.02113024
Iteration 67, loss = 0.02105514
Iteration 68, loss = 0.02088753
Iteration 69, loss = 0.02082541
Iteration 70, loss = 0.02095949
Iteration 71, loss = 0.02055583
Iteration 72, loss = 0.02070685
Iteration 73, loss = 0.02069690
Iteration 74, loss = 0.02089983
Iteration 75, loss = 0.02045255
Iteration 76, loss = 0.02031177
Iteration 77, loss = 0.02046780
Iteration 78, loss = 0.02043301
Iteration 79, loss = 0.02026347
Iteration 80, loss = 0.02003816
Iteration 81, loss = 0.02050218
Iteration 82, loss = 0.02016272
Iteration 83, loss = 0.02038547
Iteration 84, loss = 0.02011342
Iteration 85, loss = 0.02038507
Iteration 86, loss = 0.02036135
Iteration 87, loss = 0.02010433
Iteration 88, loss = 0.02040665
Iteration 89, loss = 0.02011819
Iteration 90, loss = 0.02017626
Iteration 91, loss = 0.01998501
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70091409
Iteration 2, loss = 0.63851596
Iteration 3, loss = 0.54330328
Iteration 4, loss = 0.43101758
Iteration 5, loss = 0.32912248
Iteration 6, loss = 0.24800509
Iteration 7, loss = 0.18967977
Iteration 8, loss = 0.14813314
Iteration 9, loss = 0.11902527
Iteration 10, loss = 0.09861749
Iteration 11, loss = 0.08355425
Iteration 12, loss = 0.07221394
Iteration 13, loss = 0.06373067
Iteration 14, loss = 0.05717767
Iteration 15, loss = 0.05190908
Iteration 16, loss = 0.04766174
Iteration 17, loss = 0.04400317
Iteration 18, loss = 0.04109256
Iteration 19, loss = 0.03864740
Iteration 20, loss = 0.03670411
Iteration 21, loss = 0.03490440
Iteration 22, loss = 0.03355174
Iteration 23, loss = 0.03211949
Iteration 24, loss = 0.03093965
Iteration 25, loss = 0.02989520
Iteration 26, loss = 0.02918425
Iteration 27, loss = 0.02844012
Iteration 28, loss = 0.02763913
Iteration 29, loss = 0.02710242
Iteration 30, loss = 0.02657571
Iteration 31, loss = 0.02612296
Iteration 32, loss = 0.02567020
Iteration 33, loss = 0.02522530
Iteration 34, loss = 0.02480479
Iteration 35, loss = 0.02488231
Iteration 36, loss = 0.02421069
Iteration 37, loss = 0.02415291
Iteration 38, loss = 0.02397545
Iteration 39, loss = 0.02350960
Iteration 40, loss = 0.02350405
Iteration 41, loss = 0.02292017
Iteration 42, loss = 0.02289819
Iteration 43, loss = 0.02273413
Iteration 44, loss = 0.02265533
Iteration 45, loss = 0.02238097
Iteration 46, loss = 0.02235989
Iteration 47, loss = 0.02237294
Iteration 48, loss = 0.02195328
Iteration 49, loss = 0.02185827
Iteration 50, loss = 0.02193045
Iteration 51, loss = 0.02176712
Iteration 52, loss = 0.02141731
Iteration 53, loss = 0.02163900
Iteration 54, loss = 0.02159137
Iteration 55, loss = 0.02125896
Iteration 56, loss = 0.02150575
Iteration 57, loss = 0.02119991
Iteration 58, loss = 0.02121942
Iteration 59, loss = 0.02110828
Iteration 60, loss = 0.02087282
Iteration 61, loss = 0.02117883
Iteration 62, loss = 0.02089211
Iteration 63, loss = 0.02086967
Iteration 64, loss = 0.02090206
Iteration 65, loss = 0.02081543
Iteration 66, loss = 0.02073639
Iteration 67, loss = 0.02053030
Iteration 68, loss = 0.02085338
Iteration 69, loss = 0.02055479
Iteration 70, loss = 0.02070993
Iteration 71, loss = 0.02069918
Iteration 72, loss = 0.02055024
Iteration 73, loss = 0.02039055
Iteration 74, loss = 0.02062652
Iteration 75, loss = 0.02027700
Iteration 76, loss = 0.02022612
Iteration 77, loss = 0.02037015
Iteration 78, loss = 0.02040672
Iteration 79, loss = 0.02041455
Iteration 80, loss = 0.02016202
Iteration 81, loss = 0.02047154
Iteration 82, loss = 0.02014951
Iteration 83, loss = 0.02027320
Iteration 84, loss = 0.01998473
Iteration 85, loss = 0.02027327
Iteration 86, loss = 0.02003400
Iteration 87, loss = 0.02041548
Iteration 88, loss = 0.02062312
Iteration 89, loss = 0.02016215
Iteration 90, loss = 0.02007174
Iteration 91, loss = 0.02050902
Iteration 92, loss = 0.02006132
Iteration 93, loss = 0.02004515
Iteration 94, loss = 0.02014311
Iteration 95, loss = 0.02006999
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71028840
Iteration 2, loss = 0.64958794
Iteration 3, loss = 0.55808222
Iteration 4, loss = 0.44545422
Iteration 5, loss = 0.34077193
Iteration 6, loss = 0.25623263
Iteration 7, loss = 0.19585547
Iteration 8, loss = 0.15361206
Iteration 9, loss = 0.12400733
Iteration 10, loss = 0.10305020
Iteration 11, loss = 0.08763607
Iteration 12, loss = 0.07644483
Iteration 13, loss = 0.06783967
Iteration 14, loss = 0.06136647
Iteration 15, loss = 0.05606154
Iteration 16, loss = 0.05175863
Iteration 17, loss = 0.04811966
Iteration 18, loss = 0.04546041
Iteration 19, loss = 0.04294501
Iteration 20, loss = 0.04126844
Iteration 21, loss = 0.03957737
Iteration 22, loss = 0.03782801
Iteration 23, loss = 0.03693536
Iteration 24, loss = 0.03556303
Iteration 25, loss = 0.03458617
Iteration 26, loss = 0.03387237
Iteration 27, loss = 0.03320696
Iteration 28, loss = 0.03231493
Iteration 29, loss = 0.03149553
Iteration 30, loss = 0.03121157
Iteration 31, loss = 0.03050073
Iteration 32, loss = 0.03030665
Iteration 33, loss = 0.02975038
Iteration 34, loss = 0.02934847
Iteration 35, loss = 0.02915316
Iteration 36, loss = 0.02929064
Iteration 37, loss = 0.02850972
Iteration 38, loss = 0.02837223
Iteration 39, loss = 0.02822838
Iteration 40, loss = 0.02776754
Iteration 41, loss = 0.02784165
Iteration 42, loss = 0.02763974
Iteration 43, loss = 0.02724133
Iteration 44, loss = 0.02716270
Iteration 45, loss = 0.02713986
Iteration 46, loss = 0.02704167
Iteration 47, loss = 0.02663820
Iteration 48, loss = 0.02674313
Iteration 49, loss = 0.02626756
Iteration 50, loss = 0.02642240
Iteration 51, loss = 0.02657891
Iteration 52, loss = 0.02597117
Iteration 53, loss = 0.02624963
Iteration 54, loss = 0.02577283
Iteration 55, loss = 0.02583111
Iteration 56, loss = 0.02578068
Iteration 57, loss = 0.02579004
Iteration 58, loss = 0.02568236
Iteration 59, loss = 0.02575055
Iteration 60, loss = 0.02545351
Iteration 61, loss = 0.02556900
Iteration 62, loss = 0.02533434
Iteration 63, loss = 0.02540938
Iteration 64, loss = 0.02526098
Iteration 65, loss = 0.02515717
Iteration 66, loss = 0.02534704
Iteration 67, loss = 0.02526611
Iteration 68, loss = 0.02518993
Iteration 69, loss = 0.02497892
Iteration 70, loss = 0.02535480
Iteration 71, loss = 0.02529267
Iteration 72, loss = 0.02474843
Iteration 73, loss = 0.02496469
Iteration 74, loss = 0.02508325
Iteration 75, loss = 0.02508864
Iteration 76, loss = 0.02509987
Iteration 77, loss = 0.02500836
Iteration 78, loss = 0.02459101
Iteration 79, loss = 0.02464628
Iteration 80, loss = 0.02445825
Iteration 81, loss = 0.02456339
Iteration 82, loss = 0.02482544
Iteration 83, loss = 0.02467967
Iteration 84, loss = 0.02470802
Iteration 85, loss = 0.02483162
Iteration 86, loss = 0.02462839
Iteration 87, loss = 0.02470160
Iteration 88, loss = 0.02481654
Iteration 89, loss = 0.02420968
Iteration 90, loss = 0.02441976
Iteration 91, loss = 0.02447740
Iteration 92, loss = 0.02449047
Iteration 93, loss = 0.02454055
Iteration 94, loss = 0.02425118
Iteration 95, loss = 0.02468316
Iteration 96, loss = 0.02431603
Iteration 97, loss = 0.02454463
Iteration 98, loss = 0.02442949
Iteration 99, loss = 0.02444609
Iteration 100, loss = 0.02432633
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69234631
Iteration 2, loss = 0.67280675
Iteration 3, loss = 0.65361490
Iteration 4, loss = 0.63093743
Iteration 5, loss = 0.60374517
Iteration 6, loss = 0.57236208
Iteration 7, loss = 0.53771374
Iteration 8, loss = 0.50152803
Iteration 9, loss = 0.46491338
Iteration 10, loss = 0.42891438
Iteration 11, loss = 0.39438856
Iteration 12, loss = 0.36124796
Iteration 13, loss = 0.33005832
Iteration 14, loss = 0.30096592
Iteration 15, loss = 0.27412602
Iteration 16, loss = 0.24947400
Iteration 17, loss = 0.22714440
Iteration 18, loss = 0.20686380
Iteration 19, loss = 0.18864749
Iteration 20, loss = 0.17231067
Iteration 21, loss = 0.15769952
Iteration 22, loss = 0.14458596
Iteration 23, loss = 0.13292041
Iteration 24, loss = 0.12262482
Iteration 25, loss = 0.11331161
Iteration 26, loss = 0.10504256
Iteration 27, loss = 0.09767907
Iteration 28, loss = 0.09103809
Iteration 29, loss = 0.08512135
Iteration 30, loss = 0.07966833
Iteration 31, loss = 0.07486182
Iteration 32, loss = 0.07049696
Iteration 33, loss = 0.06649395
Iteration 34, loss = 0.06297704
Iteration 35, loss = 0.05967608
Iteration 36, loss = 0.05667489
Iteration 37, loss = 0.05393102
Iteration 38, loss = 0.05139099
Iteration 39, loss = 0.04906911
Iteration 40, loss = 0.04694878
Iteration 41, loss = 0.04499600
Iteration 42, loss = 0.04314520
Iteration 43, loss = 0.04149263
Iteration 44, loss = 0.03991861
Iteration 45, loss = 0.03844962
Iteration 46, loss = 0.03710670
Iteration 47, loss = 0.03582668
Iteration 48, loss = 0.03464250
Iteration 49, loss = 0.03359019
Iteration 50, loss = 0.03252295
Iteration 51, loss = 0.03162007
Iteration 52, loss = 0.03063115
Iteration 53, loss = 0.02973430
Iteration 54, loss = 0.02895176
Iteration 55, loss = 0.02822195
Iteration 56, loss = 0.02746607
Iteration 57, loss = 0.02678232
Iteration 58, loss = 0.02618301
Iteration 59, loss = 0.02559124
Iteration 60, loss = 0.02495060
Iteration 61, loss = 0.02436442
Iteration 62, loss = 0.02391784
Iteration 63, loss = 0.02341431
Iteration 64, loss = 0.02291046
Iteration 65, loss = 0.02246398
Iteration 66, loss = 0.02199740
Iteration 67, loss = 0.02157274
Iteration 68, loss = 0.02121297
Iteration 69, loss = 0.02086471
Iteration 70, loss = 0.02056232
Iteration 71, loss = 0.02030870
Iteration 72, loss = 0.01985182
Iteration 73, loss = 0.01951340
Iteration 74, loss = 0.01922457
Iteration 75, loss = 0.01896078
Iteration 76, loss = 0.01871506
Iteration 77, loss = 0.01845046
Iteration 78, loss = 0.01815793
Iteration 79, loss = 0.01792616
Iteration 80, loss = 0.01777442
Iteration 81, loss = 0.01745448
Iteration 82, loss = 0.01725386
Iteration 83, loss = 0.01701318
Iteration 84, loss = 0.01684655
Iteration 85, loss = 0.01668683
Iteration 86, loss = 0.01643337
Iteration 87, loss = 0.01630722
Iteration 88, loss = 0.01608363
Iteration 89, loss = 0.01591302
Iteration 90, loss = 0.01573701
Iteration 91, loss = 0.01562229
Iteration 92, loss = 0.01543108
Iteration 93, loss = 0.01527296
Iteration 94, loss = 0.01512366
Iteration 95, loss = 0.01501189
Iteration 96, loss = 0.01492320
Iteration 97, loss = 0.01476938
Iteration 98, loss = 0.01457619
Iteration 99, loss = 0.01449713
Iteration 100, loss = 0.01441673
Iteration 101, loss = 0.01429997
Iteration 102, loss = 0.01416048
Iteration 103, loss = 0.01407791
Iteration 104, loss = 0.01393548
Iteration 105, loss = 0.01387011
Iteration 106, loss = 0.01376580
Iteration 107, loss = 0.01363382
Iteration 108, loss = 0.01355125
Iteration 109, loss = 0.01346272
Iteration 110, loss = 0.01336249
Iteration 111, loss = 0.01327235
Iteration 112, loss = 0.01322366
Iteration 113, loss = 0.01316654
Iteration 114, loss = 0.01303971
Iteration 115, loss = 0.01297171
Iteration 116, loss = 0.01288891
Iteration 117, loss = 0.01281404
Iteration 118, loss = 0.01272845
Iteration 119, loss = 0.01269801
Iteration 120, loss = 0.01258230
Iteration 121, loss = 0.01252083
Iteration 122, loss = 0.01244491
Iteration 123, loss = 0.01238453
Iteration 124, loss = 0.01234373
Iteration 125, loss = 0.01229383
Iteration 126, loss = 0.01220428
Iteration 127, loss = 0.01215172
Iteration 128, loss = 0.01212175
Iteration 129, loss = 0.01209982
Iteration 130, loss = 0.01202423
Iteration 131, loss = 0.01192589
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67606798
Iteration 2, loss = 0.60260181
Iteration 3, loss = 0.49151418
Iteration 4, loss = 0.37710520
Iteration 5, loss = 0.28150081
Iteration 6, loss = 0.20998031
Iteration 7, loss = 0.15993940
Iteration 8, loss = 0.12570222
Iteration 9, loss = 0.10178590
Iteration 10, loss = 0.08516545
Iteration 11, loss = 0.07276139
Iteration 12, loss = 0.06362407
Iteration 13, loss = 0.05646245
Iteration 14, loss = 0.05099592
Iteration 15, loss = 0.04672130
Iteration 16, loss = 0.04315278
Iteration 17, loss = 0.04011478
Iteration 18, loss = 0.03771175
Iteration 19, loss = 0.03546299
Iteration 20, loss = 0.03399159
Iteration 21, loss = 0.03261036
Iteration 22, loss = 0.03113684
Iteration 23, loss = 0.02987309
Iteration 24, loss = 0.02907104
Iteration 25, loss = 0.02816336
Iteration 26, loss = 0.02759316
Iteration 27, loss = 0.02705105
Iteration 28, loss = 0.02674078
Iteration 29, loss = 0.02588397
Iteration 30, loss = 0.02561297
Iteration 31, loss = 0.02516937
Iteration 32, loss = 0.02471624
Iteration 33, loss = 0.02438494
Iteration 34, loss = 0.02392548
Iteration 35, loss = 0.02368052
Iteration 36, loss = 0.02352882
Iteration 37, loss = 0.02307279
Iteration 38, loss = 0.02287382
Iteration 39, loss = 0.02276094
Iteration 40, loss = 0.02270222
Iteration 41, loss = 0.02253140
Iteration 42, loss = 0.02208432
Iteration 43, loss = 0.02217322
Iteration 44, loss = 0.02245148
Iteration 45, loss = 0.02201443
Iteration 46, loss = 0.02173797
Iteration 47, loss = 0.02156237
Iteration 48, loss = 0.02175912
Iteration 49, loss = 0.02142554
Iteration 50, loss = 0.02145036
Iteration 51, loss = 0.02145148
Iteration 52, loss = 0.02112820
Iteration 53, loss = 0.02113135
Iteration 54, loss = 0.02127871
Iteration 55, loss = 0.02100596
Iteration 56, loss = 0.02111109
Iteration 57, loss = 0.02086029
Iteration 58, loss = 0.02078880
Iteration 59, loss = 0.02075246
Iteration 60, loss = 0.02064356
Iteration 61, loss = 0.02078929
Iteration 62, loss = 0.02077066
Iteration 63, loss = 0.02055649
Iteration 64, loss = 0.02047961
Iteration 65, loss = 0.02061124
Iteration 66, loss = 0.02045450
Iteration 67, loss = 0.02038953
Iteration 68, loss = 0.02056976
Iteration 69, loss = 0.02041519
Iteration 70, loss = 0.02034317
Iteration 71, loss = 0.02017412
Iteration 72, loss = 0.02033357
Iteration 73, loss = 0.02024955
Iteration 74, loss = 0.02000242
Iteration 75, loss = 0.02035123
Iteration 76, loss = 0.02031987
Iteration 77, loss = 0.02005596
Iteration 78, loss = 0.02007625
Iteration 79, loss = 0.02020801
Iteration 80, loss = 0.02026930
Iteration 81, loss = 0.02020915
Iteration 82, loss = 0.02011148
Iteration 83, loss = 0.02024076
Iteration 84, loss = 0.02034902
Iteration 85, loss = 0.02013754
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70630756
Iteration 2, loss = 0.67761578
Iteration 3, loss = 0.64623056
Iteration 4, loss = 0.60759112
Iteration 5, loss = 0.56219848
Iteration 6, loss = 0.51205309
Iteration 7, loss = 0.46037179
Iteration 8, loss = 0.40989134
Iteration 9, loss = 0.36212814
Iteration 10, loss = 0.31854732
Iteration 11, loss = 0.27946427
Iteration 12, loss = 0.24494183
Iteration 13, loss = 0.21485670
Iteration 14, loss = 0.18901779
Iteration 15, loss = 0.16695339
Iteration 16, loss = 0.14809960
Iteration 17, loss = 0.13226188
Iteration 18, loss = 0.11857708
Iteration 19, loss = 0.10700646
Iteration 20, loss = 0.09721056
Iteration 21, loss = 0.08857998
Iteration 22, loss = 0.08134596
Iteration 23, loss = 0.07484442
Iteration 24, loss = 0.06929985
Iteration 25, loss = 0.06445111
Iteration 26, loss = 0.06025670
Iteration 27, loss = 0.05635751
Iteration 28, loss = 0.05290028
Iteration 29, loss = 0.04991471
Iteration 30, loss = 0.04712892
Iteration 31, loss = 0.04469462
Iteration 32, loss = 0.04246828
Iteration 33, loss = 0.04044521
Iteration 34, loss = 0.03863596
Iteration 35, loss = 0.03697404
Iteration 36, loss = 0.03545748
Iteration 37, loss = 0.03406105
Iteration 38, loss = 0.03287238
Iteration 39, loss = 0.03160783
Iteration 40, loss = 0.03055338
Iteration 41, loss = 0.02958742
Iteration 42, loss = 0.02864092
Iteration 43, loss = 0.02783404
Iteration 44, loss = 0.02697711
Iteration 45, loss = 0.02625502
Iteration 46, loss = 0.02565635
Iteration 47, loss = 0.02494676
Iteration 48, loss = 0.02438267
Iteration 49, loss = 0.02387311
Iteration 50, loss = 0.02346614
Iteration 51, loss = 0.02289391
Iteration 52, loss = 0.02242722
Iteration 53, loss = 0.02189686
Iteration 54, loss = 0.02150049
Iteration 55, loss = 0.02097994
Iteration 56, loss = 0.02062373
Iteration 57, loss = 0.02028604
Iteration 58, loss = 0.01996116
Iteration 59, loss = 0.01981213
Iteration 60, loss = 0.01942402
Iteration 61, loss = 0.01916893
Iteration 62, loss = 0.01885920
Iteration 63, loss = 0.01853939
Iteration 64, loss = 0.01832727
Iteration 65, loss = 0.01802151
Iteration 66, loss = 0.01777528
Iteration 67, loss = 0.01760439
Iteration 68, loss = 0.01736206
Iteration 69, loss = 0.01714823
Iteration 70, loss = 0.01710174
Iteration 71, loss = 0.01688742
Iteration 72, loss = 0.01680842
Iteration 73, loss = 0.01668855
Iteration 74, loss = 0.01650776
Iteration 75, loss = 0.01629463
Iteration 76, loss = 0.01604450
Iteration 77, loss = 0.01589238
Iteration 78, loss = 0.01575300
Iteration 79, loss = 0.01573668
Iteration 80, loss = 0.01574348
Iteration 81, loss = 0.01563603
Iteration 82, loss = 0.01541596
Iteration 83, loss = 0.01527093
Iteration 84, loss = 0.01504130
Iteration 85, loss = 0.01520324
Iteration 86, loss = 0.01495631
Iteration 87, loss = 0.01486305
Iteration 88, loss = 0.01468166
Iteration 89, loss = 0.01450096
Iteration 90, loss = 0.01446020
Iteration 91, loss = 0.01432321
Iteration 92, loss = 0.01428249
Iteration 93, loss = 0.01430115
Iteration 94, loss = 0.01422067
Iteration 95, loss = 0.01419871
Iteration 96, loss = 0.01411174
Iteration 97, loss = 0.01388928
Iteration 98, loss = 0.01399262
Iteration 99, loss = 0.01392807
Iteration 100, loss = 0.01398305
Iteration 101, loss = 0.01403127
Iteration 102, loss = 0.01384764
Iteration 103, loss = 0.01370827
Iteration 104, loss = 0.01362840
Iteration 105, loss = 0.01349701
Iteration 106, loss = 0.01335937
Iteration 107, loss = 0.01334728
Iteration 108, loss = 0.01329796
Iteration 109, loss = 0.01321782
Iteration 110, loss = 0.01316160
Iteration 111, loss = 0.01307244
Iteration 112, loss = 0.01308284
Iteration 113, loss = 0.01300674
Iteration 114, loss = 0.01301633
Iteration 115, loss = 0.01298888
Iteration 116, loss = 0.01284981
Iteration 117, loss = 0.01280332
Iteration 118, loss = 0.01269484
Iteration 119, loss = 0.01274135
Iteration 120, loss = 0.01273123
Iteration 121, loss = 0.01272862
Iteration 122, loss = 0.01274185
Iteration 123, loss = 0.01261461
Iteration 124, loss = 0.01258568
Iteration 125, loss = 0.01251034
Iteration 126, loss = 0.01246939
Iteration 127, loss = 0.01244800
Iteration 128, loss = 0.01242527
Iteration 129, loss = 0.01238251
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69519022
Iteration 2, loss = 0.63643118
Iteration 3, loss = 0.55095431
Iteration 4, loss = 0.44936876
Iteration 5, loss = 0.35456757
Iteration 6, loss = 0.27516896
Iteration 7, loss = 0.21493213
Iteration 8, loss = 0.17001429
Iteration 9, loss = 0.13784008
Iteration 10, loss = 0.11446842
Iteration 11, loss = 0.09704712
Iteration 12, loss = 0.08410350
Iteration 13, loss = 0.07425354
Iteration 14, loss = 0.06649108
Iteration 15, loss = 0.06031853
Iteration 16, loss = 0.05539024
Iteration 17, loss = 0.05125926
Iteration 18, loss = 0.04819795
Iteration 19, loss = 0.04526314
Iteration 20, loss = 0.04282001
Iteration 21, loss = 0.04075952
Iteration 22, loss = 0.03889317
Iteration 23, loss = 0.03748203
Iteration 24, loss = 0.03595192
Iteration 25, loss = 0.03467626
Iteration 26, loss = 0.03373767
Iteration 27, loss = 0.03301500
Iteration 28, loss = 0.03211210
Iteration 29, loss = 0.03129136
Iteration 30, loss = 0.03087364
Iteration 31, loss = 0.02996174
Iteration 32, loss = 0.02968911
Iteration 33, loss = 0.02904652
Iteration 34, loss = 0.02856716
Iteration 35, loss = 0.02825623
Iteration 36, loss = 0.02789889
Iteration 37, loss = 0.02810030
Iteration 38, loss = 0.02796228
Iteration 39, loss = 0.02752182
Iteration 40, loss = 0.02723030
Iteration 41, loss = 0.02675992
Iteration 42, loss = 0.02656980
Iteration 43, loss = 0.02623912
Iteration 44, loss = 0.02602210
Iteration 45, loss = 0.02560622
Iteration 46, loss = 0.02553842
Iteration 47, loss = 0.02618523
Iteration 48, loss = 0.02563169
Iteration 49, loss = 0.02529562
Iteration 50, loss = 0.02493396
Iteration 51, loss = 0.02477265
Iteration 52, loss = 0.02466602
Iteration 53, loss = 0.02448211
Iteration 54, loss = 0.02416701
Iteration 55, loss = 0.02432629
Iteration 56, loss = 0.02423248
Iteration 57, loss = 0.02412476
Iteration 58, loss = 0.02422689
Iteration 59, loss = 0.02395638
Iteration 60, loss = 0.02371310
Iteration 61, loss = 0.02378062
Iteration 62, loss = 0.02393947
Iteration 63, loss = 0.02337970
Iteration 64, loss = 0.02426171
Iteration 65, loss = 0.02413203
Iteration 66, loss = 0.02405197
Iteration 67, loss = 0.02398931
Iteration 68, loss = 0.02386233
Iteration 69, loss = 0.02345285
Iteration 70, loss = 0.02335364
Iteration 71, loss = 0.02365036
Iteration 72, loss = 0.02311086
Iteration 73, loss = 0.02317576
Iteration 74, loss = 0.02360859
Iteration 75, loss = 0.02434878
Iteration 76, loss = 0.02439783
Iteration 77, loss = 0.02380212
Iteration 78, loss = 0.02337454
Iteration 79, loss = 0.02310166
Iteration 80, loss = 0.02285945
Iteration 81, loss = 0.02290837
Iteration 82, loss = 0.02274342
Iteration 83, loss = 0.02268531
Iteration 84, loss = 0.02307266
Iteration 85, loss = 0.02354346
Iteration 86, loss = 0.02349375
Iteration 87, loss = 0.02312321
Iteration 88, loss = 0.02292312
Iteration 89, loss = 0.02278330
Iteration 90, loss = 0.02255863
Iteration 91, loss = 0.02251612
Iteration 92, loss = 0.02231324
Iteration 93, loss = 0.02233792
Iteration 94, loss = 0.02236622
Iteration 95, loss = 0.02236154
Iteration 96, loss = 0.02214454
Iteration 97, loss = 0.02237611
Iteration 98, loss = 0.02216294
Iteration 99, loss = 0.02220339
Iteration 100, loss = 0.02206634
Iteration 101, loss = 0.02204335
Iteration 102, loss = 0.02202893
Iteration 103, loss = 0.02203936
Iteration 104, loss = 0.02224988
Iteration 105, loss = 0.02234971
Iteration 106, loss = 0.02206865
Iteration 107, loss = 0.02213149
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70390123
Iteration 2, loss = 0.67814471
Iteration 3, loss = 0.64337154
Iteration 4, loss = 0.59775826
Iteration 5, loss = 0.54226488
Iteration 6, loss = 0.48154626
Iteration 7, loss = 0.42091371
Iteration 8, loss = 0.36326636
Iteration 9, loss = 0.31141565
Iteration 10, loss = 0.26605944
Iteration 11, loss = 0.22727564
Iteration 12, loss = 0.19452919
Iteration 13, loss = 0.16759007
Iteration 14, loss = 0.14526580
Iteration 15, loss = 0.12704773
Iteration 16, loss = 0.11203159
Iteration 17, loss = 0.09943075
Iteration 18, loss = 0.08925096
Iteration 19, loss = 0.08041209
Iteration 20, loss = 0.07305458
Iteration 21, loss = 0.06691705
Iteration 22, loss = 0.06155280
Iteration 23, loss = 0.05695234
Iteration 24, loss = 0.05300189
Iteration 25, loss = 0.04953511
Iteration 26, loss = 0.04647623
Iteration 27, loss = 0.04386883
Iteration 28, loss = 0.04146832
Iteration 29, loss = 0.03942413
Iteration 30, loss = 0.03760859
Iteration 31, loss = 0.03582993
Iteration 32, loss = 0.03430771
Iteration 33, loss = 0.03282648
Iteration 34, loss = 0.03161942
Iteration 35, loss = 0.03057408
Iteration 36, loss = 0.02940285
Iteration 37, loss = 0.02847857
Iteration 38, loss = 0.02765453
Iteration 39, loss = 0.02696661
Iteration 40, loss = 0.02612302
Iteration 41, loss = 0.02548910
Iteration 42, loss = 0.02489220
Iteration 43, loss = 0.02437938
Iteration 44, loss = 0.02382410
Iteration 45, loss = 0.02307261
Iteration 46, loss = 0.02273164
Iteration 47, loss = 0.02230788
Iteration 48, loss = 0.02193659
Iteration 49, loss = 0.02161171
Iteration 50, loss = 0.02106683
Iteration 51, loss = 0.02082652
Iteration 52, loss = 0.02049762
Iteration 53, loss = 0.02020685
Iteration 54, loss = 0.01995438
Iteration 55, loss = 0.01961929
Iteration 56, loss = 0.01942857
Iteration 57, loss = 0.01915090
Iteration 58, loss = 0.01896631
Iteration 59, loss = 0.01878392
Iteration 60, loss = 0.01854376
Iteration 61, loss = 0.01825992
Iteration 62, loss = 0.01818503
Iteration 63, loss = 0.01808791
Iteration 64, loss = 0.01793156
Iteration 65, loss = 0.01782619
Iteration 66, loss = 0.01762028
Iteration 67, loss = 0.01738588
Iteration 68, loss = 0.01728146
Iteration 69, loss = 0.01723085
Iteration 70, loss = 0.01691860
Iteration 71, loss = 0.01691665
Iteration 72, loss = 0.01684795
Iteration 73, loss = 0.01671701
Iteration 74, loss = 0.01673085
Iteration 75, loss = 0.01652149
Iteration 76, loss = 0.01637932
Iteration 77, loss = 0.01628675
Iteration 78, loss = 0.01619251
Iteration 79, loss = 0.01610780
Iteration 80, loss = 0.01612981
Iteration 81, loss = 0.01603269
Iteration 82, loss = 0.01609326
Iteration 83, loss = 0.01583103
Iteration 84, loss = 0.01575975
Iteration 85, loss = 0.01571828
Iteration 86, loss = 0.01572512
Iteration 87, loss = 0.01550176
Iteration 88, loss = 0.01552377
Iteration 89, loss = 0.01548899
Iteration 90, loss = 0.01552632
Iteration 91, loss = 0.01542387
Iteration 92, loss = 0.01539979
Iteration 93, loss = 0.01519826
Iteration 94, loss = 0.01531542
Iteration 95, loss = 0.01524942
Iteration 96, loss = 0.01505095
Iteration 97, loss = 0.01527892
Iteration 98, loss = 0.01528952
Iteration 99, loss = 0.01499022
Iteration 100, loss = 0.01509451
Iteration 101, loss = 0.01496145
Iteration 102, loss = 0.01506716
Iteration 103, loss = 0.01482558
Iteration 104, loss = 0.01477020
Iteration 105, loss = 0.01476343
Iteration 106, loss = 0.01479751
Iteration 107, loss = 0.01479838
Iteration 108, loss = 0.01462410
Iteration 109, loss = 0.01464185
Iteration 110, loss = 0.01479488
Iteration 111, loss = 0.01454871
Iteration 112, loss = 0.01459266
Iteration 113, loss = 0.01454073
Iteration 114, loss = 0.01452290
Iteration 115, loss = 0.01463051
Iteration 116, loss = 0.01448186
Iteration 117, loss = 0.01461273
Iteration 118, loss = 0.01442391
Iteration 119, loss = 0.01442985
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68989085
Iteration 2, loss = 0.67317150
Iteration 3, loss = 0.65633160
Iteration 4, loss = 0.63659870
Iteration 5, loss = 0.61276663
Iteration 6, loss = 0.58421532
Iteration 7, loss = 0.55160720
Iteration 8, loss = 0.51615919
Iteration 9, loss = 0.47920540
Iteration 10, loss = 0.44186931
Iteration 11, loss = 0.40511953
Iteration 12, loss = 0.36965009
Iteration 13, loss = 0.33649974
Iteration 14, loss = 0.30535871
Iteration 15, loss = 0.27672696
Iteration 16, loss = 0.25058579
Iteration 17, loss = 0.22682445
Iteration 18, loss = 0.20559997
Iteration 19, loss = 0.18653501
Iteration 20, loss = 0.16951629
Iteration 21, loss = 0.15440655
Iteration 22, loss = 0.14105687
Iteration 23, loss = 0.12914438
Iteration 24, loss = 0.11863239
Iteration 25, loss = 0.10933544
Iteration 26, loss = 0.10098383
Iteration 27, loss = 0.09363987
Iteration 28, loss = 0.08704553
Iteration 29, loss = 0.08113078
Iteration 30, loss = 0.07588890
Iteration 31, loss = 0.07109162
Iteration 32, loss = 0.06686349
Iteration 33, loss = 0.06296806
Iteration 34, loss = 0.05952642
Iteration 35, loss = 0.05630289
Iteration 36, loss = 0.05347198
Iteration 37, loss = 0.05070883
Iteration 38, loss = 0.04834701
Iteration 39, loss = 0.04611074
Iteration 40, loss = 0.04404436
Iteration 41, loss = 0.04219651
Iteration 42, loss = 0.04047897
Iteration 43, loss = 0.03885926
Iteration 44, loss = 0.03736377
Iteration 45, loss = 0.03594989
Iteration 46, loss = 0.03465794
Iteration 47, loss = 0.03345123
Iteration 48, loss = 0.03239078
Iteration 49, loss = 0.03129932
Iteration 50, loss = 0.03028216
Iteration 51, loss = 0.02936366
Iteration 52, loss = 0.02850071
Iteration 53, loss = 0.02770069
Iteration 54, loss = 0.02690956
Iteration 55, loss = 0.02617247
Iteration 56, loss = 0.02549810
Iteration 57, loss = 0.02488227
Iteration 58, loss = 0.02425246
Iteration 59, loss = 0.02366606
Iteration 60, loss = 0.02311557
Iteration 61, loss = 0.02261004
Iteration 62, loss = 0.02209644
Iteration 63, loss = 0.02169426
Iteration 64, loss = 0.02113435
Iteration 65, loss = 0.02077320
Iteration 66, loss = 0.02032906
Iteration 67, loss = 0.01993529
Iteration 68, loss = 0.01957204
Iteration 69, loss = 0.01925032
Iteration 70, loss = 0.01891335
Iteration 71, loss = 0.01854855
Iteration 72, loss = 0.01823209
Iteration 73, loss = 0.01798026
Iteration 74, loss = 0.01772844
Iteration 75, loss = 0.01746051
Iteration 76, loss = 0.01714886
Iteration 77, loss = 0.01691328
Iteration 78, loss = 0.01666592
Iteration 79, loss = 0.01644608
Iteration 80, loss = 0.01617244
Iteration 81, loss = 0.01597564
Iteration 82, loss = 0.01576836
Iteration 83, loss = 0.01556147
Iteration 84, loss = 0.01539682
Iteration 85, loss = 0.01519202
Iteration 86, loss = 0.01505688
Iteration 87, loss = 0.01483903
Iteration 88, loss = 0.01472191
Iteration 89, loss = 0.01459087
Iteration 90, loss = 0.01434365
Iteration 91, loss = 0.01424064
Iteration 92, loss = 0.01403789
Iteration 93, loss = 0.01392296
Iteration 94, loss = 0.01381171
Iteration 95, loss = 0.01369239
Iteration 96, loss = 0.01359178
Iteration 97, loss = 0.01342607
Iteration 98, loss = 0.01327306
Iteration 99, loss = 0.01321830
Iteration 100, loss = 0.01306645
Iteration 101, loss = 0.01296300
Iteration 102, loss = 0.01285337
Iteration 103, loss = 0.01277656
Iteration 104, loss = 0.01266408
Iteration 105, loss = 0.01254835
Iteration 106, loss = 0.01248228
Iteration 107, loss = 0.01233844
Iteration 108, loss = 0.01227437
Iteration 109, loss = 0.01219490
Iteration 110, loss = 0.01212593
Iteration 111, loss = 0.01201524
Iteration 112, loss = 0.01197598
Iteration 113, loss = 0.01186113
Iteration 114, loss = 0.01180287
Iteration 115, loss = 0.01173451
Iteration 116, loss = 0.01165369
Iteration 117, loss = 0.01161719
Iteration 118, loss = 0.01154612
Iteration 119, loss = 0.01146416
Iteration 120, loss = 0.01137330
Iteration 121, loss = 0.01133033
Iteration 122, loss = 0.01122230
Iteration 123, loss = 0.01122742
Iteration 124, loss = 0.01110712
Iteration 125, loss = 0.01109971
Iteration 126, loss = 0.01104840
Iteration 127, loss = 0.01098734
Iteration 128, loss = 0.01092102
Iteration 129, loss = 0.01089257
Iteration 130, loss = 0.01078054
Iteration 131, loss = 0.01076857
Iteration 132, loss = 0.01071876
Iteration 133, loss = 0.01069260
Iteration 134, loss = 0.01066929
Iteration 135, loss = 0.01055532
Iteration 136, loss = 0.01052614
Iteration 137, loss = 0.01047815
Iteration 138, loss = 0.01041650
Iteration 139, loss = 0.01036647
Iteration 140, loss = 0.01036270
Iteration 141, loss = 0.01028309
Iteration 142, loss = 0.01025991
Iteration 143, loss = 0.01024168
Iteration 144, loss = 0.01019603
Iteration 145, loss = 0.01014455
Iteration 146, loss = 0.01011212
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67708055
Iteration 2, loss = 0.61504557
Iteration 3, loss = 0.52286031
Iteration 4, loss = 0.42376128
Iteration 5, loss = 0.33509441
Iteration 6, loss = 0.26129092
Iteration 7, loss = 0.20462987
Iteration 8, loss = 0.16262334
Iteration 9, loss = 0.13258425
Iteration 10, loss = 0.11032451
Iteration 11, loss = 0.09432826
Iteration 12, loss = 0.08175701
Iteration 13, loss = 0.07264135
Iteration 14, loss = 0.06546197
Iteration 15, loss = 0.05952081
Iteration 16, loss = 0.05463237
Iteration 17, loss = 0.05067319
Iteration 18, loss = 0.04738578
Iteration 19, loss = 0.04438465
Iteration 20, loss = 0.04170657
Iteration 21, loss = 0.04000289
Iteration 22, loss = 0.03805464
Iteration 23, loss = 0.03647288
Iteration 24, loss = 0.03508096
Iteration 25, loss = 0.03389750
Iteration 26, loss = 0.03277310
Iteration 27, loss = 0.03202185
Iteration 28, loss = 0.03106799
Iteration 29, loss = 0.03028969
Iteration 30, loss = 0.02965084
Iteration 31, loss = 0.02927766
Iteration 32, loss = 0.02861525
Iteration 33, loss = 0.02871448
Iteration 34, loss = 0.02795830
Iteration 35, loss = 0.02757956
Iteration 36, loss = 0.02717947
Iteration 37, loss = 0.02679789
Iteration 38, loss = 0.02620101
Iteration 39, loss = 0.02586932
Iteration 40, loss = 0.02567655
Iteration 41, loss = 0.02534728
Iteration 42, loss = 0.02510691
Iteration 43, loss = 0.02512124
Iteration 44, loss = 0.02506007
Iteration 45, loss = 0.02453089
Iteration 46, loss = 0.02437535
Iteration 47, loss = 0.02427985
Iteration 48, loss = 0.02387411
Iteration 49, loss = 0.02416748
Iteration 50, loss = 0.02375156
Iteration 51, loss = 0.02382693
Iteration 52, loss = 0.02371580
Iteration 53, loss = 0.02393278
Iteration 54, loss = 0.02490104
Iteration 55, loss = 0.02419855
Iteration 56, loss = 0.02372037
Iteration 57, loss = 0.02341944
Iteration 58, loss = 0.02318760
Iteration 59, loss = 0.02298402
Iteration 60, loss = 0.02307771
Iteration 61, loss = 0.02266414
Iteration 62, loss = 0.02263978
Iteration 63, loss = 0.02253932
Iteration 64, loss = 0.02259899
Iteration 65, loss = 0.02251916
Iteration 66, loss = 0.02248961
Iteration 67, loss = 0.02243755
Iteration 68, loss = 0.02226298
Iteration 69, loss = 0.02232311
Iteration 70, loss = 0.02218193
Iteration 71, loss = 0.02192332
Iteration 72, loss = 0.02204121
Iteration 73, loss = 0.02192943
Iteration 74, loss = 0.02196611
Iteration 75, loss = 0.02197166
Iteration 76, loss = 0.02163842
Iteration 77, loss = 0.02166159
Iteration 78, loss = 0.02182435
Iteration 79, loss = 0.02212005
Iteration 80, loss = 0.02251756
Iteration 81, loss = 0.02281824
Iteration 82, loss = 0.02240331
Iteration 83, loss = 0.02230218
Iteration 84, loss = 0.02261102
Iteration 85, loss = 0.02335807
Iteration 86, loss = 0.02238579
Iteration 87, loss = 0.02189519
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69076851
Iteration 2, loss = 0.67535487
Iteration 3, loss = 0.66198831
Iteration 4, loss = 0.64859519
Iteration 5, loss = 0.63437490
Iteration 6, loss = 0.61908055
Iteration 7, loss = 0.60244613
Iteration 8, loss = 0.58436734
Iteration 9, loss = 0.56459538
Iteration 10, loss = 0.54348380
Iteration 11, loss = 0.52122920
Iteration 12, loss = 0.49781592
Iteration 13, loss = 0.47375701
Iteration 14, loss = 0.44937216
Iteration 15, loss = 0.42519586
Iteration 16, loss = 0.40119078
Iteration 17, loss = 0.37808592
Iteration 18, loss = 0.35555752
Iteration 19, loss = 0.33419620
Iteration 20, loss = 0.31379118
Iteration 21, loss = 0.29433271
Iteration 22, loss = 0.27614000
Iteration 23, loss = 0.25880235
Iteration 24, loss = 0.24264732
Iteration 25, loss = 0.22756563
Iteration 26, loss = 0.21337725
Iteration 27, loss = 0.20021858
Iteration 28, loss = 0.18792744
Iteration 29, loss = 0.17658156
Iteration 30, loss = 0.16595151
Iteration 31, loss = 0.15616832
Iteration 32, loss = 0.14704823
Iteration 33, loss = 0.13865204
Iteration 34, loss = 0.13084700
Iteration 35, loss = 0.12364805
Iteration 36, loss = 0.11692481
Iteration 37, loss = 0.11074458
Iteration 38, loss = 0.10499659
Iteration 39, loss = 0.09976133
Iteration 40, loss = 0.09476287
Iteration 41, loss = 0.09020772
Iteration 42, loss = 0.08599444
Iteration 43, loss = 0.08202587
Iteration 44, loss = 0.07834690
Iteration 45, loss = 0.07488511
Iteration 46, loss = 0.07171275
Iteration 47, loss = 0.06871302
Iteration 48, loss = 0.06594025
Iteration 49, loss = 0.06331415
Iteration 50, loss = 0.06088715
Iteration 51, loss = 0.05866492
Iteration 52, loss = 0.05646904
Iteration 53, loss = 0.05444943
Iteration 54, loss = 0.05255666
Iteration 55, loss = 0.05075649
Iteration 56, loss = 0.04908756
Iteration 57, loss = 0.04752709
Iteration 58, loss = 0.04602634
Iteration 59, loss = 0.04458225
Iteration 60, loss = 0.04324635
Iteration 61, loss = 0.04201822
Iteration 62, loss = 0.04078248
Iteration 63, loss = 0.03969495
Iteration 64, loss = 0.03857356
Iteration 65, loss = 0.03756504
Iteration 66, loss = 0.03659188
Iteration 67, loss = 0.03565198
Iteration 68, loss = 0.03476977
Iteration 69, loss = 0.03390376
Iteration 70, loss = 0.03313629
Iteration 71, loss = 0.03232709
Iteration 72, loss = 0.03160787
Iteration 73, loss = 0.03093146
Iteration 74, loss = 0.03028229
Iteration 75, loss = 0.02960354
Iteration 76, loss = 0.02901991
Iteration 77, loss = 0.02836347
Iteration 78, loss = 0.02781169
Iteration 79, loss = 0.02724285
Iteration 80, loss = 0.02678680
Iteration 81, loss = 0.02625582
Iteration 82, loss = 0.02579245
Iteration 83, loss = 0.02532754
Iteration 84, loss = 0.02492120
Iteration 85, loss = 0.02444962
Iteration 86, loss = 0.02401573
Iteration 87, loss = 0.02363933
Iteration 88, loss = 0.02325222
Iteration 89, loss = 0.02290293
Iteration 90, loss = 0.02250351
Iteration 91, loss = 0.02228411
Iteration 92, loss = 0.02186454
Iteration 93, loss = 0.02157265
Iteration 94, loss = 0.02124487
Iteration 95, loss = 0.02096601
Iteration 96, loss = 0.02064980
Iteration 97, loss = 0.02036500
Iteration 98, loss = 0.02008192
Iteration 99, loss = 0.01981927
Iteration 100, loss = 0.01958731
Iteration 101, loss = 0.01932823
Iteration 102, loss = 0.01909092
Iteration 103, loss = 0.01885803
Iteration 104, loss = 0.01863429
Iteration 105, loss = 0.01843331
Iteration 106, loss = 0.01821789
Iteration 107, loss = 0.01803434
Iteration 108, loss = 0.01785691
Iteration 109, loss = 0.01760003
Iteration 110, loss = 0.01745094
Iteration 111, loss = 0.01722402
Iteration 112, loss = 0.01705393
Iteration 113, loss = 0.01688849
Iteration 114, loss = 0.01676086
Iteration 115, loss = 0.01656856
Iteration 116, loss = 0.01643189
Iteration 117, loss = 0.01626297
Iteration 118, loss = 0.01616973
Iteration 119, loss = 0.01599723
Iteration 120, loss = 0.01585391
Iteration 121, loss = 0.01571089
Iteration 122, loss = 0.01556729
Iteration 123, loss = 0.01544733
Iteration 124, loss = 0.01528739
Iteration 125, loss = 0.01515673
Iteration 126, loss = 0.01503469
Iteration 127, loss = 0.01489650
Iteration 128, loss = 0.01477746
Iteration 129, loss = 0.01465746
Iteration 130, loss = 0.01454639
Iteration 131, loss = 0.01447206
Iteration 132, loss = 0.01432691
Iteration 133, loss = 0.01422881
Iteration 134, loss = 0.01413067
Iteration 135, loss = 0.01403578
Iteration 136, loss = 0.01395031
Iteration 137, loss = 0.01384194
Iteration 138, loss = 0.01376206
Iteration 139, loss = 0.01369675
Iteration 140, loss = 0.01357067
Iteration 141, loss = 0.01351615
Iteration 142, loss = 0.01341913
Iteration 143, loss = 0.01332521
Iteration 144, loss = 0.01325419
Iteration 145, loss = 0.01315847
Iteration 146, loss = 0.01309983
Iteration 147, loss = 0.01301089
Iteration 148, loss = 0.01296302
Iteration 149, loss = 0.01284037
Iteration 150, loss = 0.01277815
Iteration 151, loss = 0.01272992
Iteration 152, loss = 0.01266125
Iteration 153, loss = 0.01260934
Iteration 154, loss = 0.01255239
Iteration 155, loss = 0.01241500
Iteration 156, loss = 0.01241705
Iteration 157, loss = 0.01228767
Iteration 158, loss = 0.01225233
Iteration 159, loss = 0.01223343
Iteration 160, loss = 0.01218313
Iteration 161, loss = 0.01209185
Iteration 162, loss = 0.01201068
Iteration 163, loss = 0.01194112
Iteration 164, loss = 0.01189011
Iteration 165, loss = 0.01183068
Iteration 166, loss = 0.01173556
Iteration 167, loss = 0.01173137
Iteration 168, loss = 0.01181505
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71841416
Iteration 2, loss = 0.68975445
Iteration 3, loss = 0.65258835
Iteration 4, loss = 0.60578949
Iteration 5, loss = 0.54835426
Iteration 6, loss = 0.48637252
Iteration 7, loss = 0.42422943
Iteration 8, loss = 0.36562821
Iteration 9, loss = 0.31311363
Iteration 10, loss = 0.26736978
Iteration 11, loss = 0.22845594
Iteration 12, loss = 0.19583896
Iteration 13, loss = 0.16884259
Iteration 14, loss = 0.14661032
Iteration 15, loss = 0.12838428
Iteration 16, loss = 0.11345057
Iteration 17, loss = 0.10090113
Iteration 18, loss = 0.09054534
Iteration 19, loss = 0.08184652
Iteration 20, loss = 0.07462457
Iteration 21, loss = 0.06829199
Iteration 22, loss = 0.06294659
Iteration 23, loss = 0.05837535
Iteration 24, loss = 0.05435282
Iteration 25, loss = 0.05086521
Iteration 26, loss = 0.04781509
Iteration 27, loss = 0.04525432
Iteration 28, loss = 0.04277564
Iteration 29, loss = 0.04068804
Iteration 30, loss = 0.03877241
Iteration 31, loss = 0.03713021
Iteration 32, loss = 0.03546693
Iteration 33, loss = 0.03414925
Iteration 34, loss = 0.03284544
Iteration 35, loss = 0.03182397
Iteration 36, loss = 0.03065145
Iteration 37, loss = 0.02977374
Iteration 38, loss = 0.02887281
Iteration 39, loss = 0.02816728
Iteration 40, loss = 0.02728924
Iteration 41, loss = 0.02661569
Iteration 42, loss = 0.02603535
Iteration 43, loss = 0.02545529
Iteration 44, loss = 0.02480337
Iteration 45, loss = 0.02434684
Iteration 46, loss = 0.02391858
Iteration 47, loss = 0.02352862
Iteration 48, loss = 0.02300360
Iteration 49, loss = 0.02272328
Iteration 50, loss = 0.02227828
Iteration 51, loss = 0.02188496
Iteration 52, loss = 0.02172894
Iteration 53, loss = 0.02129961
Iteration 54, loss = 0.02105156
Iteration 55, loss = 0.02086334
Iteration 56, loss = 0.02064868
Iteration 57, loss = 0.02041822
Iteration 58, loss = 0.02003160
Iteration 59, loss = 0.01985733
Iteration 60, loss = 0.01960124
Iteration 61, loss = 0.01948337
Iteration 62, loss = 0.01929734
Iteration 63, loss = 0.01925706
Iteration 64, loss = 0.01899625
Iteration 65, loss = 0.01891730
Iteration 66, loss = 0.01862218
Iteration 67, loss = 0.01856828
Iteration 68, loss = 0.01837455
Iteration 69, loss = 0.01815014
Iteration 70, loss = 0.01808666
Iteration 71, loss = 0.01799518
Iteration 72, loss = 0.01774379
Iteration 73, loss = 0.01768820
Iteration 74, loss = 0.01762663
Iteration 75, loss = 0.01757086
Iteration 76, loss = 0.01740131
Iteration 77, loss = 0.01745161
Iteration 78, loss = 0.01733781
Iteration 79, loss = 0.01710742
Iteration 80, loss = 0.01710488
Iteration 81, loss = 0.01690595
Iteration 82, loss = 0.01703621
Iteration 83, loss = 0.01687426
Iteration 84, loss = 0.01692283
Iteration 85, loss = 0.01672528
Iteration 86, loss = 0.01675363
Iteration 87, loss = 0.01662410
Iteration 88, loss = 0.01647530
Iteration 89, loss = 0.01645655
Iteration 90, loss = 0.01641152
Iteration 91, loss = 0.01636998
Iteration 92, loss = 0.01630622
Iteration 93, loss = 0.01638135
Iteration 94, loss = 0.01619499
Iteration 95, loss = 0.01624746
Iteration 96, loss = 0.01615565
Iteration 97, loss = 0.01598806
Iteration 98, loss = 0.01603547
Iteration 99, loss = 0.01604580
Iteration 100, loss = 0.01599878
Iteration 101, loss = 0.01590920
Iteration 102, loss = 0.01594298
Iteration 103, loss = 0.01588855
Iteration 104, loss = 0.01590468
Iteration 105, loss = 0.01587845
Iteration 106, loss = 0.01581426
Iteration 107, loss = 0.01580794
Iteration 108, loss = 0.01569424
Iteration 109, loss = 0.01562888
Iteration 110, loss = 0.01566216
Iteration 111, loss = 0.01565324
Iteration 112, loss = 0.01558207
Iteration 113, loss = 0.01561868
Iteration 114, loss = 0.01554413
Iteration 115, loss = 0.01555534
Iteration 116, loss = 0.01552305
Iteration 117, loss = 0.01543605
Iteration 118, loss = 0.01542383
Iteration 119, loss = 0.01540032
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67629104
Iteration 2, loss = 0.61626883
Iteration 3, loss = 0.51609063
Iteration 4, loss = 0.40605599
Iteration 5, loss = 0.30693492
Iteration 6, loss = 0.22987802
Iteration 7, loss = 0.17494479
Iteration 8, loss = 0.13727984
Iteration 9, loss = 0.11136403
Iteration 10, loss = 0.09305900
Iteration 11, loss = 0.07966311
Iteration 12, loss = 0.06982015
Iteration 13, loss = 0.06231150
Iteration 14, loss = 0.05652933
Iteration 15, loss = 0.05178911
Iteration 16, loss = 0.04802513
Iteration 17, loss = 0.04486734
Iteration 18, loss = 0.04241157
Iteration 19, loss = 0.04021112
Iteration 20, loss = 0.03860239
Iteration 21, loss = 0.03705761
Iteration 22, loss = 0.03589499
Iteration 23, loss = 0.03448625
Iteration 24, loss = 0.03389040
Iteration 25, loss = 0.03279152
Iteration 26, loss = 0.03193778
Iteration 27, loss = 0.03132523
Iteration 28, loss = 0.03068504
Iteration 29, loss = 0.03009859
Iteration 30, loss = 0.02981866
Iteration 31, loss = 0.02928058
Iteration 32, loss = 0.02902302
Iteration 33, loss = 0.02860774
Iteration 34, loss = 0.02836636
Iteration 35, loss = 0.02786138
Iteration 36, loss = 0.02788969
Iteration 37, loss = 0.02739717
Iteration 38, loss = 0.02739715
Iteration 39, loss = 0.02708759
Iteration 40, loss = 0.02680074
Iteration 41, loss = 0.02679215
Iteration 42, loss = 0.02652936
Iteration 43, loss = 0.02655078
Iteration 44, loss = 0.02618876
Iteration 45, loss = 0.02620065
Iteration 46, loss = 0.02591384
Iteration 47, loss = 0.02584227
Iteration 48, loss = 0.02563129
Iteration 49, loss = 0.02551102
Iteration 50, loss = 0.02551638
Iteration 51, loss = 0.02532993
Iteration 52, loss = 0.02542719
Iteration 53, loss = 0.02549008
Iteration 54, loss = 0.02506622
Iteration 55, loss = 0.02515876
Iteration 56, loss = 0.02494717
Iteration 57, loss = 0.02484000
Iteration 58, loss = 0.02482018
Iteration 59, loss = 0.02481256
Iteration 60, loss = 0.02462491
Iteration 61, loss = 0.02479170
Iteration 62, loss = 0.02481111
Iteration 63, loss = 0.02463873
Iteration 64, loss = 0.02452174
Iteration 65, loss = 0.02449568
Iteration 66, loss = 0.02451736
Iteration 67, loss = 0.02433456
Iteration 68, loss = 0.02456149
Iteration 69, loss = 0.02460645
Iteration 70, loss = 0.02433245
Iteration 71, loss = 0.02440870
Iteration 72, loss = 0.02452980
Iteration 73, loss = 0.02441610
Iteration 74, loss = 0.02433259
Iteration 75, loss = 0.02440254
Iteration 76, loss = 0.02423250
Iteration 77, loss = 0.02442886
Iteration 78, loss = 0.02415586
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67881342
Iteration 2, loss = 0.65698628
Iteration 3, loss = 0.63159599
Iteration 4, loss = 0.59681726
Iteration 5, loss = 0.55238436
Iteration 6, loss = 0.50333872
Iteration 7, loss = 0.45359122
Iteration 8, loss = 0.40551050
Iteration 9, loss = 0.35985048
Iteration 10, loss = 0.31818756
Iteration 11, loss = 0.28034299
Iteration 12, loss = 0.24707696
Iteration 13, loss = 0.21775485
Iteration 14, loss = 0.19267266
Iteration 15, loss = 0.17100535
Iteration 16, loss = 0.15234102
Iteration 17, loss = 0.13648844
Iteration 18, loss = 0.12287381
Iteration 19, loss = 0.11115266
Iteration 20, loss = 0.10100041
Iteration 21, loss = 0.09209310
Iteration 22, loss = 0.08454896
Iteration 23, loss = 0.07782796
Iteration 24, loss = 0.07195632
Iteration 25, loss = 0.06681367
Iteration 26, loss = 0.06228147
Iteration 27, loss = 0.05825820
Iteration 28, loss = 0.05467999
Iteration 29, loss = 0.05152938
Iteration 30, loss = 0.04873210
Iteration 31, loss = 0.04604983
Iteration 32, loss = 0.04377537
Iteration 33, loss = 0.04160624
Iteration 34, loss = 0.03984098
Iteration 35, loss = 0.03815052
Iteration 36, loss = 0.03671234
Iteration 37, loss = 0.03518598
Iteration 38, loss = 0.03394449
Iteration 39, loss = 0.03257621
Iteration 40, loss = 0.03142054
Iteration 41, loss = 0.03031483
Iteration 42, loss = 0.02930668
Iteration 43, loss = 0.02826677
Iteration 44, loss = 0.02737698
Iteration 45, loss = 0.02654638
Iteration 46, loss = 0.02577187
Iteration 47, loss = 0.02503629
Iteration 48, loss = 0.02438494
Iteration 49, loss = 0.02376125
Iteration 50, loss = 0.02313361
Iteration 51, loss = 0.02266324
Iteration 52, loss = 0.02211551
Iteration 53, loss = 0.02160673
Iteration 54, loss = 0.02130774
Iteration 55, loss = 0.02088628
Iteration 56, loss = 0.02039179
Iteration 57, loss = 0.02005341
Iteration 58, loss = 0.01966890
Iteration 59, loss = 0.01929917
Iteration 60, loss = 0.01898262
Iteration 61, loss = 0.01869086
Iteration 62, loss = 0.01859038
Iteration 63, loss = 0.01826277
Iteration 64, loss = 0.01810253
Iteration 65, loss = 0.01785973
Iteration 66, loss = 0.01757767
Iteration 67, loss = 0.01734026
Iteration 68, loss = 0.01715873
Iteration 69, loss = 0.01683780
Iteration 70, loss = 0.01662363
Iteration 71, loss = 0.01643694
Iteration 72, loss = 0.01623743
Iteration 73, loss = 0.01587284
Iteration 74, loss = 0.01605763
Iteration 75, loss = 0.01595681
Iteration 76, loss = 0.01579317
Iteration 77, loss = 0.01550707
Iteration 78, loss = 0.01530489
Iteration 79, loss = 0.01516925
Iteration 80, loss = 0.01498594
Iteration 81, loss = 0.01475979
Iteration 82, loss = 0.01484628
Iteration 83, loss = 0.01464104
Iteration 84, loss = 0.01458612
Iteration 85, loss = 0.01431932
Iteration 86, loss = 0.01419386
Iteration 87, loss = 0.01404457
Iteration 88, loss = 0.01399500
Iteration 89, loss = 0.01401054
Iteration 90, loss = 0.01412951
Iteration 91, loss = 0.01410623
Iteration 92, loss = 0.01390580
Iteration 93, loss = 0.01387677
Iteration 94, loss = 0.01368099
Iteration 95, loss = 0.01357883
Iteration 96, loss = 0.01342997
Iteration 97, loss = 0.01310932
Iteration 98, loss = 0.01316863
Iteration 99, loss = 0.01303587
Iteration 100, loss = 0.01307898
Iteration 101, loss = 0.01310537
Iteration 102, loss = 0.01299829
Iteration 103, loss = 0.01286103
Iteration 104, loss = 0.01280439
Iteration 105, loss = 0.01270530
Iteration 106, loss = 0.01280513
Iteration 107, loss = 0.01274442
Iteration 108, loss = 0.01266857
Iteration 109, loss = 0.01262551
Iteration 110, loss = 0.01242864
Iteration 111, loss = 0.01244943
Iteration 112, loss = 0.01239760
Iteration 113, loss = 0.01227503
Iteration 114, loss = 0.01225627
Iteration 115, loss = 0.01215525
Iteration 116, loss = 0.01205729
Iteration 117, loss = 0.01204284
Iteration 118, loss = 0.01196966
Iteration 119, loss = 0.01199262
Iteration 120, loss = 0.01197773
Iteration 121, loss = 0.01188263
Iteration 122, loss = 0.01222930
Iteration 123, loss = 0.01214204
Iteration 124, loss = 0.01212358
Iteration 125, loss = 0.01201106
Iteration 126, loss = 0.01185297
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70959010
Iteration 2, loss = 0.63194263
Iteration 3, loss = 0.53360839
Iteration 4, loss = 0.42568484
Iteration 5, loss = 0.32982581
Iteration 6, loss = 0.25266524
Iteration 7, loss = 0.19478376
Iteration 8, loss = 0.15390101
Iteration 9, loss = 0.12438583
Iteration 10, loss = 0.10360272
Iteration 11, loss = 0.08828919
Iteration 12, loss = 0.07700413
Iteration 13, loss = 0.06810540
Iteration 14, loss = 0.06150524
Iteration 15, loss = 0.05604537
Iteration 16, loss = 0.05188082
Iteration 17, loss = 0.04813073
Iteration 18, loss = 0.04522800
Iteration 19, loss = 0.04261854
Iteration 20, loss = 0.04053437
Iteration 21, loss = 0.03889014
Iteration 22, loss = 0.03723207
Iteration 23, loss = 0.03616855
Iteration 24, loss = 0.03496366
Iteration 25, loss = 0.03402836
Iteration 26, loss = 0.03304299
Iteration 27, loss = 0.03238922
Iteration 28, loss = 0.03158202
Iteration 29, loss = 0.03102091
Iteration 30, loss = 0.03031687
Iteration 31, loss = 0.02991763
Iteration 32, loss = 0.02924921
Iteration 33, loss = 0.02901550
Iteration 34, loss = 0.02863680
Iteration 35, loss = 0.02854227
Iteration 36, loss = 0.02804632
Iteration 37, loss = 0.02780136
Iteration 38, loss = 0.02764951
Iteration 39, loss = 0.02720112
Iteration 40, loss = 0.02702391
Iteration 41, loss = 0.02681039
Iteration 42, loss = 0.02669146
Iteration 43, loss = 0.02632339
Iteration 44, loss = 0.02619587
Iteration 45, loss = 0.02603659
Iteration 46, loss = 0.02591537
Iteration 47, loss = 0.02593093
Iteration 48, loss = 0.02579586
Iteration 49, loss = 0.02566768
Iteration 50, loss = 0.02526758
Iteration 51, loss = 0.02541646
Iteration 52, loss = 0.02535963
Iteration 53, loss = 0.02519955
Iteration 54, loss = 0.02517977
Iteration 55, loss = 0.02509157
Iteration 56, loss = 0.02514758
Iteration 57, loss = 0.02499718
Iteration 58, loss = 0.02478020
Iteration 59, loss = 0.02498976
Iteration 60, loss = 0.02492047
Iteration 61, loss = 0.02456321
Iteration 62, loss = 0.02463019
Iteration 63, loss = 0.02443808
Iteration 64, loss = 0.02454188
Iteration 65, loss = 0.02461425
Iteration 66, loss = 0.02429257
Iteration 67, loss = 0.02451276
Iteration 68, loss = 0.02428332
Iteration 69, loss = 0.02425870
Iteration 70, loss = 0.02423158
Iteration 71, loss = 0.02426866
Iteration 72, loss = 0.02405449
Iteration 73, loss = 0.02417979
Iteration 74, loss = 0.02416279
Iteration 75, loss = 0.02405551
Iteration 76, loss = 0.02411642
Iteration 77, loss = 0.02417410
Iteration 78, loss = 0.02389883
Iteration 79, loss = 0.02394496
Iteration 80, loss = 0.02395324
Iteration 81, loss = 0.02410302
Iteration 82, loss = 0.02416605
Iteration 83, loss = 0.02394513
Iteration 84, loss = 0.02370641
Iteration 85, loss = 0.02383866
Iteration 86, loss = 0.02367836
Iteration 87, loss = 0.02362493
Iteration 88, loss = 0.02361512
Iteration 89, loss = 0.02381367
Iteration 90, loss = 0.02395354
Iteration 91, loss = 0.02386701
Iteration 92, loss = 0.02392240
Iteration 93, loss = 0.02364690
Iteration 94, loss = 0.02357386
Iteration 95, loss = 0.02357782
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70767382
Iteration 2, loss = 0.62854521
Iteration 3, loss = 0.52508624
Iteration 4, loss = 0.40572258
Iteration 5, loss = 0.30249587
Iteration 6, loss = 0.22459017
Iteration 7, loss = 0.17010288
Iteration 8, loss = 0.13306203
Iteration 9, loss = 0.10714775
Iteration 10, loss = 0.08902152
Iteration 11, loss = 0.07623871
Iteration 12, loss = 0.06648958
Iteration 13, loss = 0.05907017
Iteration 14, loss = 0.05332978
Iteration 15, loss = 0.04874520
Iteration 16, loss = 0.04507343
Iteration 17, loss = 0.04205243
Iteration 18, loss = 0.03971263
Iteration 19, loss = 0.03743704
Iteration 20, loss = 0.03570709
Iteration 21, loss = 0.03410925
Iteration 22, loss = 0.03302491
Iteration 23, loss = 0.03177033
Iteration 24, loss = 0.03093691
Iteration 25, loss = 0.02986386
Iteration 26, loss = 0.02916781
Iteration 27, loss = 0.02854674
Iteration 28, loss = 0.02778284
Iteration 29, loss = 0.02769555
Iteration 30, loss = 0.02707239
Iteration 31, loss = 0.02653775
Iteration 32, loss = 0.02620387
Iteration 33, loss = 0.02618886
Iteration 34, loss = 0.02552218
Iteration 35, loss = 0.02531066
Iteration 36, loss = 0.02494826
Iteration 37, loss = 0.02494488
Iteration 38, loss = 0.02447515
Iteration 39, loss = 0.02415549
Iteration 40, loss = 0.02413571
Iteration 41, loss = 0.02395441
Iteration 42, loss = 0.02388012
Iteration 43, loss = 0.02362330
Iteration 44, loss = 0.02338429
Iteration 45, loss = 0.02344135
Iteration 46, loss = 0.02332782
Iteration 47, loss = 0.02325794
Iteration 48, loss = 0.02279099
Iteration 49, loss = 0.02308789
Iteration 50, loss = 0.02286470
Iteration 51, loss = 0.02267164
Iteration 52, loss = 0.02264688
Iteration 53, loss = 0.02263942
Iteration 54, loss = 0.02241762
Iteration 55, loss = 0.02247446
Iteration 56, loss = 0.02238997
Iteration 57, loss = 0.02221066
Iteration 58, loss = 0.02214850
Iteration 59, loss = 0.02227086
Iteration 60, loss = 0.02217720
Iteration 61, loss = 0.02229495
Iteration 62, loss = 0.02202479
Iteration 63, loss = 0.02194925
Iteration 64, loss = 0.02199266
Iteration 65, loss = 0.02187501
Iteration 66, loss = 0.02168042
Iteration 67, loss = 0.02164434
Iteration 68, loss = 0.02181599
Iteration 69, loss = 0.02196218
Iteration 70, loss = 0.02155668
Iteration 71, loss = 0.02165601
Iteration 72, loss = 0.02147934
Iteration 73, loss = 0.02155960
Iteration 74, loss = 0.02171979
Iteration 75, loss = 0.02133377
Iteration 76, loss = 0.02153662
Iteration 77, loss = 0.02157650
Iteration 78, loss = 0.02143194
Iteration 79, loss = 0.02163556
Iteration 80, loss = 0.02128084
Iteration 81, loss = 0.02135407
Iteration 82, loss = 0.02153343
Iteration 83, loss = 0.02141234
Iteration 84, loss = 0.02141263
Iteration 85, loss = 0.02110095
Iteration 86, loss = 0.02116370
Iteration 87, loss = 0.02160462
Iteration 88, loss = 0.02139628
Iteration 89, loss = 0.02132336
Iteration 90, loss = 0.02127973
Iteration 91, loss = 0.02153720
Iteration 92, loss = 0.02140256
Iteration 93, loss = 0.02116272
Iteration 94, loss = 0.02131689
Iteration 95, loss = 0.02158801
Iteration 96, loss = 0.02088427
Iteration 97, loss = 0.02124533
Iteration 98, loss = 0.02106416
Iteration 99, loss = 0.02126638
Iteration 100, loss = 0.02078848
Iteration 101, loss = 0.02118827
Iteration 102, loss = 0.02124836
Iteration 103, loss = 0.02120623
Iteration 104, loss = 0.02127716
Iteration 105, loss = 0.02106710
Iteration 106, loss = 0.02121945
Iteration 107, loss = 0.02094391
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72163860
Iteration 2, loss = 0.67503161
Iteration 3, loss = 0.61556814
Iteration 4, loss = 0.54633185
Iteration 5, loss = 0.47089991
Iteration 6, loss = 0.39774393
Iteration 7, loss = 0.33164810
Iteration 8, loss = 0.27537237
Iteration 9, loss = 0.22855434
Iteration 10, loss = 0.19071717
Iteration 11, loss = 0.16053024
Iteration 12, loss = 0.13635272
Iteration 13, loss = 0.11723584
Iteration 14, loss = 0.10190696
Iteration 15, loss = 0.08954643
Iteration 16, loss = 0.07952845
Iteration 17, loss = 0.07135016
Iteration 18, loss = 0.06451096
Iteration 19, loss = 0.05875628
Iteration 20, loss = 0.05408792
Iteration 21, loss = 0.04985984
Iteration 22, loss = 0.04648392
Iteration 23, loss = 0.04332736
Iteration 24, loss = 0.04076045
Iteration 25, loss = 0.03851414
Iteration 26, loss = 0.03647369
Iteration 27, loss = 0.03463890
Iteration 28, loss = 0.03296043
Iteration 29, loss = 0.03169324
Iteration 30, loss = 0.03032136
Iteration 31, loss = 0.02915534
Iteration 32, loss = 0.02818057
Iteration 33, loss = 0.02736686
Iteration 34, loss = 0.02655448
Iteration 35, loss = 0.02565997
Iteration 36, loss = 0.02507160
Iteration 37, loss = 0.02441536
Iteration 38, loss = 0.02382844
Iteration 39, loss = 0.02306945
Iteration 40, loss = 0.02268039
Iteration 41, loss = 0.02220196
Iteration 42, loss = 0.02190202
Iteration 43, loss = 0.02147369
Iteration 44, loss = 0.02081999
Iteration 45, loss = 0.02058659
Iteration 46, loss = 0.02029652
Iteration 47, loss = 0.02007849
Iteration 48, loss = 0.01961690
Iteration 49, loss = 0.01964009
Iteration 50, loss = 0.01924806
Iteration 51, loss = 0.01900246
Iteration 52, loss = 0.01874519
Iteration 53, loss = 0.01850973
Iteration 54, loss = 0.01837240
Iteration 55, loss = 0.01812091
Iteration 56, loss = 0.01792220
Iteration 57, loss = 0.01788407
Iteration 58, loss = 0.01780372
Iteration 59, loss = 0.01754952
Iteration 60, loss = 0.01742617
Iteration 61, loss = 0.01724638
Iteration 62, loss = 0.01711090
Iteration 63, loss = 0.01696589
Iteration 64, loss = 0.01694860
Iteration 65, loss = 0.01684363
Iteration 66, loss = 0.01676756
Iteration 67, loss = 0.01671575
Iteration 68, loss = 0.01635148
Iteration 69, loss = 0.01646320
Iteration 70, loss = 0.01636876
Iteration 71, loss = 0.01627952
Iteration 72, loss = 0.01607187
Iteration 73, loss = 0.01601758
Iteration 74, loss = 0.01584152
Iteration 75, loss = 0.01582585
Iteration 76, loss = 0.01583501
Iteration 77, loss = 0.01580615
Iteration 78, loss = 0.01570786
Iteration 79, loss = 0.01566552
Iteration 80, loss = 0.01554932
Iteration 81, loss = 0.01568204
Iteration 82, loss = 0.01553180
Iteration 83, loss = 0.01530557
Iteration 84, loss = 0.01537639
Iteration 85, loss = 0.01531093
Iteration 86, loss = 0.01521261
Iteration 87, loss = 0.01522122
Iteration 88, loss = 0.01524554
Iteration 89, loss = 0.01518807
Iteration 90, loss = 0.01506365
Iteration 91, loss = 0.01512209
Iteration 92, loss = 0.01495102
Iteration 93, loss = 0.01516153
Iteration 94, loss = 0.01491357
Iteration 95, loss = 0.01496568
Iteration 96, loss = 0.01505056
Iteration 97, loss = 0.01486342
Iteration 98, loss = 0.01484736
Iteration 99, loss = 0.01472609
Iteration 100, loss = 0.01470891
Iteration 101, loss = 0.01483303
Iteration 102, loss = 0.01469520
Iteration 103, loss = 0.01479317
Iteration 104, loss = 0.01468262
Iteration 105, loss = 0.01469780
Iteration 106, loss = 0.01463287
Iteration 107, loss = 0.01451404
Iteration 108, loss = 0.01447642
Iteration 109, loss = 0.01455811
Iteration 110, loss = 0.01446273
Iteration 111, loss = 0.01473562
Iteration 112, loss = 0.01459201
Iteration 113, loss = 0.01441706
Iteration 114, loss = 0.01457913
Iteration 115, loss = 0.01452435
Iteration 116, loss = 0.01436737
Iteration 117, loss = 0.01438734
Iteration 118, loss = 0.01429266
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67597247
Iteration 2, loss = 0.61226780
Iteration 3, loss = 0.51185828
Iteration 4, loss = 0.40485047
Iteration 5, loss = 0.31019794
Iteration 6, loss = 0.23609747
Iteration 7, loss = 0.18187857
Iteration 8, loss = 0.14385345
Iteration 9, loss = 0.11677626
Iteration 10, loss = 0.09758536
Iteration 11, loss = 0.08361546
Iteration 12, loss = 0.07278891
Iteration 13, loss = 0.06481582
Iteration 14, loss = 0.05884143
Iteration 15, loss = 0.05348437
Iteration 16, loss = 0.04935742
Iteration 17, loss = 0.04614156
Iteration 18, loss = 0.04371779
Iteration 19, loss = 0.04116251
Iteration 20, loss = 0.03907578
Iteration 21, loss = 0.03750806
Iteration 22, loss = 0.03598346
Iteration 23, loss = 0.03489455
Iteration 24, loss = 0.03359961
Iteration 25, loss = 0.03284186
Iteration 26, loss = 0.03208789
Iteration 27, loss = 0.03128842
Iteration 28, loss = 0.03065240
Iteration 29, loss = 0.03013731
Iteration 30, loss = 0.02934960
Iteration 31, loss = 0.02897158
Iteration 32, loss = 0.02859991
Iteration 33, loss = 0.02816139
Iteration 34, loss = 0.02784743
Iteration 35, loss = 0.02724626
Iteration 36, loss = 0.02729516
Iteration 37, loss = 0.02685527
Iteration 38, loss = 0.02652442
Iteration 39, loss = 0.02646548
Iteration 40, loss = 0.02641495
Iteration 41, loss = 0.02607010
Iteration 42, loss = 0.02587942
Iteration 43, loss = 0.02558433
Iteration 44, loss = 0.02553519
Iteration 45, loss = 0.02553180
Iteration 46, loss = 0.02530999
Iteration 47, loss = 0.02490812
Iteration 48, loss = 0.02478249
Iteration 49, loss = 0.02477933
Iteration 50, loss = 0.02490147
Iteration 51, loss = 0.02434606
Iteration 52, loss = 0.02434079
Iteration 53, loss = 0.02433709
Iteration 54, loss = 0.02432022
Iteration 55, loss = 0.02440730
Iteration 56, loss = 0.02419618
Iteration 57, loss = 0.02411817
Iteration 58, loss = 0.02403097
Iteration 59, loss = 0.02410730
Iteration 60, loss = 0.02417325
Iteration 61, loss = 0.02407634
Iteration 62, loss = 0.02385396
Iteration 63, loss = 0.02394392
Iteration 64, loss = 0.02387141
Iteration 65, loss = 0.02365312
Iteration 66, loss = 0.02344456
Iteration 67, loss = 0.02354097
Iteration 68, loss = 0.02366594
Iteration 69, loss = 0.02334136
Iteration 70, loss = 0.02377418
Iteration 71, loss = 0.02324748
Iteration 72, loss = 0.02346601
Iteration 73, loss = 0.02346565
Iteration 74, loss = 0.02319659
Iteration 75, loss = 0.02330442
Iteration 76, loss = 0.02327263
Iteration 77, loss = 0.02344764
Iteration 78, loss = 0.02307232
Iteration 79, loss = 0.02299931
Iteration 80, loss = 0.02314707
Iteration 81, loss = 0.02319561
Iteration 82, loss = 0.02296235
Iteration 83, loss = 0.02327866
Iteration 84, loss = 0.02310865
Iteration 85, loss = 0.02279359
Iteration 86, loss = 0.02293381
Iteration 87, loss = 0.02309460
Iteration 88, loss = 0.02305695
Iteration 89, loss = 0.02334878
Iteration 90, loss = 0.02330866
Iteration 91, loss = 0.02293503
Iteration 92, loss = 0.02277809
Iteration 93, loss = 0.02290877
Iteration 94, loss = 0.02308929
Iteration 95, loss = 0.02299217
Iteration 96, loss = 0.02284212
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68277805
Iteration 2, loss = 0.66044555
Iteration 3, loss = 0.63845790
Iteration 4, loss = 0.61335301
Iteration 5, loss = 0.58459754
Iteration 6, loss = 0.55208224
Iteration 7, loss = 0.51699861
Iteration 8, loss = 0.48060829
Iteration 9, loss = 0.44405285
Iteration 10, loss = 0.40817272
Iteration 11, loss = 0.37371233
Iteration 12, loss = 0.34106127
Iteration 13, loss = 0.31053445
Iteration 14, loss = 0.28221941
Iteration 15, loss = 0.25624666
Iteration 16, loss = 0.23256578
Iteration 17, loss = 0.21106340
Iteration 18, loss = 0.19179449
Iteration 19, loss = 0.17468092
Iteration 20, loss = 0.15936715
Iteration 21, loss = 0.14572724
Iteration 22, loss = 0.13367507
Iteration 23, loss = 0.12293755
Iteration 24, loss = 0.11339041
Iteration 25, loss = 0.10488323
Iteration 26, loss = 0.09732657
Iteration 27, loss = 0.09061311
Iteration 28, loss = 0.08465995
Iteration 29, loss = 0.07920005
Iteration 30, loss = 0.07435681
Iteration 31, loss = 0.07001718
Iteration 32, loss = 0.06605964
Iteration 33, loss = 0.06251483
Iteration 34, loss = 0.05924324
Iteration 35, loss = 0.05628149
Iteration 36, loss = 0.05359633
Iteration 37, loss = 0.05114903
Iteration 38, loss = 0.04889122
Iteration 39, loss = 0.04680696
Iteration 40, loss = 0.04492438
Iteration 41, loss = 0.04308012
Iteration 42, loss = 0.04149420
Iteration 43, loss = 0.03999096
Iteration 44, loss = 0.03854492
Iteration 45, loss = 0.03722927
Iteration 46, loss = 0.03600232
Iteration 47, loss = 0.03490963
Iteration 48, loss = 0.03383159
Iteration 49, loss = 0.03280553
Iteration 50, loss = 0.03190309
Iteration 51, loss = 0.03100054
Iteration 52, loss = 0.03020424
Iteration 53, loss = 0.02946246
Iteration 54, loss = 0.02867429
Iteration 55, loss = 0.02798516
Iteration 56, loss = 0.02733898
Iteration 57, loss = 0.02672268
Iteration 58, loss = 0.02616448
Iteration 59, loss = 0.02559361
Iteration 60, loss = 0.02509613
Iteration 61, loss = 0.02458121
Iteration 62, loss = 0.02408567
Iteration 63, loss = 0.02363667
Iteration 64, loss = 0.02322143
Iteration 65, loss = 0.02281831
Iteration 66, loss = 0.02241982
Iteration 67, loss = 0.02208688
Iteration 68, loss = 0.02174678
Iteration 69, loss = 0.02135963
Iteration 70, loss = 0.02107146
Iteration 71, loss = 0.02073242
Iteration 72, loss = 0.02045309
Iteration 73, loss = 0.02015149
Iteration 74, loss = 0.01990159
Iteration 75, loss = 0.01963708
Iteration 76, loss = 0.01937620
Iteration 77, loss = 0.01908962
Iteration 78, loss = 0.01888415
Iteration 79, loss = 0.01867660
Iteration 80, loss = 0.01849184
Iteration 81, loss = 0.01826295
Iteration 82, loss = 0.01804332
Iteration 83, loss = 0.01788864
Iteration 84, loss = 0.01772078
Iteration 85, loss = 0.01752906
Iteration 86, loss = 0.01734607
Iteration 87, loss = 0.01716676
Iteration 88, loss = 0.01706252
Iteration 89, loss = 0.01687999
Iteration 90, loss = 0.01667472
Iteration 91, loss = 0.01653356
Iteration 92, loss = 0.01641117
Iteration 93, loss = 0.01625884
Iteration 94, loss = 0.01618815
Iteration 95, loss = 0.01604176
Iteration 96, loss = 0.01587754
Iteration 97, loss = 0.01578203
Iteration 98, loss = 0.01566184
Iteration 99, loss = 0.01554362
Iteration 100, loss = 0.01540514
Iteration 101, loss = 0.01539591
Iteration 102, loss = 0.01538781
Iteration 103, loss = 0.01509739
Iteration 104, loss = 0.01506755
Iteration 105, loss = 0.01495712
Iteration 106, loss = 0.01486552
Iteration 107, loss = 0.01478871
Iteration 108, loss = 0.01469365
Iteration 109, loss = 0.01456075
Iteration 110, loss = 0.01455139
Iteration 111, loss = 0.01446434
Iteration 112, loss = 0.01439004
Iteration 113, loss = 0.01425062
Iteration 114, loss = 0.01419750
Iteration 115, loss = 0.01408448
Iteration 116, loss = 0.01407962
Iteration 117, loss = 0.01398599
Iteration 118, loss = 0.01397001
Iteration 119, loss = 0.01394537
Iteration 120, loss = 0.01379181
Iteration 121, loss = 0.01375902
Iteration 122, loss = 0.01367778
Iteration 123, loss = 0.01362897
Iteration 124, loss = 0.01360817
Iteration 125, loss = 0.01358350
Iteration 126, loss = 0.01341564
Iteration 127, loss = 0.01339070
Iteration 128, loss = 0.01336967
Iteration 129, loss = 0.01334457
Iteration 130, loss = 0.01322302
Iteration 131, loss = 0.01317322
Iteration 132, loss = 0.01317340
Iteration 133, loss = 0.01314189
Iteration 134, loss = 0.01312505
Iteration 135, loss = 0.01307479
Iteration 136, loss = 0.01302430
Iteration 137, loss = 0.01290250
Iteration 138, loss = 0.01291111
Iteration 139, loss = 0.01286460
Iteration 140, loss = 0.01281457
Iteration 141, loss = 0.01277115
Iteration 142, loss = 0.01283720
Iteration 143, loss = 0.01268477
Iteration 144, loss = 0.01269639
Iteration 145, loss = 0.01269349
Iteration 146, loss = 0.01264504
Iteration 147, loss = 0.01251723
Iteration 148, loss = 0.01251936
Iteration 149, loss = 0.01245278
Iteration 150, loss = 0.01243140
Iteration 151, loss = 0.01238511
Iteration 152, loss = 0.01239479
Iteration 153, loss = 0.01240553
Iteration 154, loss = 0.01235702
Iteration 155, loss = 0.01227278
Iteration 156, loss = 0.01221959
Iteration 157, loss = 0.01225067
Iteration 158, loss = 0.01225152
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68118947
Iteration 2, loss = 0.64451845
Iteration 3, loss = 0.59389827
Iteration 4, loss = 0.52821708
Iteration 5, loss = 0.45701520
Iteration 6, loss = 0.38731545
Iteration 7, loss = 0.32339516
Iteration 8, loss = 0.26861510
Iteration 9, loss = 0.22316176
Iteration 10, loss = 0.18633642
Iteration 11, loss = 0.15694371
Iteration 12, loss = 0.13363947
Iteration 13, loss = 0.11537051
Iteration 14, loss = 0.10072069
Iteration 15, loss = 0.08877553
Iteration 16, loss = 0.07920424
Iteration 17, loss = 0.07119879
Iteration 18, loss = 0.06465818
Iteration 19, loss = 0.05929904
Iteration 20, loss = 0.05460197
Iteration 21, loss = 0.05074762
Iteration 22, loss = 0.04744745
Iteration 23, loss = 0.04438880
Iteration 24, loss = 0.04188439
Iteration 25, loss = 0.03977960
Iteration 26, loss = 0.03784313
Iteration 27, loss = 0.03598149
Iteration 28, loss = 0.03456195
Iteration 29, loss = 0.03316110
Iteration 30, loss = 0.03192628
Iteration 31, loss = 0.03084558
Iteration 32, loss = 0.02979509
Iteration 33, loss = 0.02888424
Iteration 34, loss = 0.02806131
Iteration 35, loss = 0.02727690
Iteration 36, loss = 0.02663066
Iteration 37, loss = 0.02590923
Iteration 38, loss = 0.02555428
Iteration 39, loss = 0.02503587
Iteration 40, loss = 0.02462830
Iteration 41, loss = 0.02416188
Iteration 42, loss = 0.02358287
Iteration 43, loss = 0.02317408
Iteration 44, loss = 0.02280657
Iteration 45, loss = 0.02240085
Iteration 46, loss = 0.02212319
Iteration 47, loss = 0.02182642
Iteration 48, loss = 0.02148220
Iteration 49, loss = 0.02134095
Iteration 50, loss = 0.02110789
Iteration 51, loss = 0.02084545
Iteration 52, loss = 0.02066155
Iteration 53, loss = 0.02045023
Iteration 54, loss = 0.02026877
Iteration 55, loss = 0.02001677
Iteration 56, loss = 0.01982200
Iteration 57, loss = 0.01981164
Iteration 58, loss = 0.01962153
Iteration 59, loss = 0.01943836
Iteration 60, loss = 0.01926288
Iteration 61, loss = 0.01935380
Iteration 62, loss = 0.01895822
Iteration 63, loss = 0.01900626
Iteration 64, loss = 0.01877264
Iteration 65, loss = 0.01857797
Iteration 66, loss = 0.01855656
Iteration 67, loss = 0.01858143
Iteration 68, loss = 0.01832625
Iteration 69, loss = 0.01849987
Iteration 70, loss = 0.01836080
Iteration 71, loss = 0.01827994
Iteration 72, loss = 0.01810040
Iteration 73, loss = 0.01799983
Iteration 74, loss = 0.01788512
Iteration 75, loss = 0.01782386
Iteration 76, loss = 0.01784097
Iteration 77, loss = 0.01764624
Iteration 78, loss = 0.01768893
Iteration 79, loss = 0.01764841
Iteration 80, loss = 0.01756226
Iteration 81, loss = 0.01758257
Iteration 82, loss = 0.01740111
Iteration 83, loss = 0.01745520
Iteration 84, loss = 0.01721745
Iteration 85, loss = 0.01731726
Iteration 86, loss = 0.01727989
Iteration 87, loss = 0.01733290
Iteration 88, loss = 0.01727336
Iteration 89, loss = 0.01704214
Iteration 90, loss = 0.01718960
Iteration 91, loss = 0.01715519
Iteration 92, loss = 0.01716381
Iteration 93, loss = 0.01693199
Iteration 94, loss = 0.01701034
Iteration 95, loss = 0.01692057
Iteration 96, loss = 0.01703299
Iteration 97, loss = 0.01699698
Iteration 98, loss = 0.01691789
Iteration 99, loss = 0.01684912
Iteration 100, loss = 0.01688164
Iteration 101, loss = 0.01678059
Iteration 102, loss = 0.01669198
Iteration 103, loss = 0.01664955
Iteration 104, loss = 0.01667333
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71703507
Iteration 2, loss = 0.65247607
Iteration 3, loss = 0.55371328
Iteration 4, loss = 0.43312684
Iteration 5, loss = 0.32421013
Iteration 6, loss = 0.24046067
Iteration 7, loss = 0.18177639
Iteration 8, loss = 0.14144860
Iteration 9, loss = 0.11373536
Iteration 10, loss = 0.09440755
Iteration 11, loss = 0.08033216
Iteration 12, loss = 0.07020783
Iteration 13, loss = 0.06261211
Iteration 14, loss = 0.05635230
Iteration 15, loss = 0.05144312
Iteration 16, loss = 0.04761090
Iteration 17, loss = 0.04446225
Iteration 18, loss = 0.04179157
Iteration 19, loss = 0.03989786
Iteration 20, loss = 0.03779006
Iteration 21, loss = 0.03635803
Iteration 22, loss = 0.03500558
Iteration 23, loss = 0.03393147
Iteration 24, loss = 0.03289293
Iteration 25, loss = 0.03219275
Iteration 26, loss = 0.03132153
Iteration 27, loss = 0.03043457
Iteration 28, loss = 0.02993209
Iteration 29, loss = 0.02935405
Iteration 30, loss = 0.02887645
Iteration 31, loss = 0.02854294
Iteration 32, loss = 0.02821231
Iteration 33, loss = 0.02781959
Iteration 34, loss = 0.02750975
Iteration 35, loss = 0.02712171
Iteration 36, loss = 0.02694096
Iteration 37, loss = 0.02672858
Iteration 38, loss = 0.02649311
Iteration 39, loss = 0.02660310
Iteration 40, loss = 0.02602767
Iteration 41, loss = 0.02602603
Iteration 42, loss = 0.02567735
Iteration 43, loss = 0.02563419
Iteration 44, loss = 0.02538594
Iteration 45, loss = 0.02527334
Iteration 46, loss = 0.02512024
Iteration 47, loss = 0.02513125
Iteration 48, loss = 0.02504079
Iteration 49, loss = 0.02510429
Iteration 50, loss = 0.02470523
Iteration 51, loss = 0.02448689
Iteration 52, loss = 0.02485531
Iteration 53, loss = 0.02456197
Iteration 54, loss = 0.02443051
Iteration 55, loss = 0.02437854
Iteration 56, loss = 0.02432044
Iteration 57, loss = 0.02419966
Iteration 58, loss = 0.02431212
Iteration 59, loss = 0.02431701
Iteration 60, loss = 0.02445008
Iteration 61, loss = 0.02386456
Iteration 62, loss = 0.02402409
Iteration 63, loss = 0.02396741
Iteration 64, loss = 0.02386862
Iteration 65, loss = 0.02390124
Iteration 66, loss = 0.02383576
Iteration 67, loss = 0.02367303
Iteration 68, loss = 0.02364656
Iteration 69, loss = 0.02367771
Iteration 70, loss = 0.02354028
Iteration 71, loss = 0.02376096
Iteration 72, loss = 0.02355700
Iteration 73, loss = 0.02389929
Iteration 74, loss = 0.02375586
Iteration 75, loss = 0.02357931
Iteration 76, loss = 0.02378378
Iteration 77, loss = 0.02336083
Iteration 78, loss = 0.02327433
Iteration 79, loss = 0.02351054
Iteration 80, loss = 0.02322982
Iteration 81, loss = 0.02322080
Iteration 82, loss = 0.02325945
Iteration 83, loss = 0.02320512
Iteration 84, loss = 0.02356550
Iteration 85, loss = 0.02299140
Iteration 86, loss = 0.02325033
Iteration 87, loss = 0.02316417
Iteration 88, loss = 0.02359630
Iteration 89, loss = 0.02312343
Iteration 90, loss = 0.02354055
Iteration 91, loss = 0.02320502
Iteration 92, loss = 0.02316294
Iteration 93, loss = 0.02299066
Iteration 94, loss = 0.02335135
Iteration 95, loss = 0.02287361
Iteration 96, loss = 0.02323668
Iteration 97, loss = 0.02324352
Iteration 98, loss = 0.02334380
Iteration 99, loss = 0.02301153
Iteration 100, loss = 0.02328039
Iteration 101, loss = 0.02306664
Iteration 102, loss = 0.02337662
Iteration 103, loss = 0.02310168
Iteration 104, loss = 0.02293109
Iteration 105, loss = 0.02276316
Iteration 106, loss = 0.02291750
Iteration 107, loss = 0.02273737
Iteration 108, loss = 0.02321924
Iteration 109, loss = 0.02304867
Iteration 110, loss = 0.02301432
Iteration 111, loss = 0.02297740
Iteration 112, loss = 0.02325159
Iteration 113, loss = 0.02338401
Iteration 114, loss = 0.02281933
Iteration 115, loss = 0.02323478
Iteration 116, loss = 0.02285156
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	82m58.248s
user	357m51.572s
sys	35m17.845s
Elapsed time: 4979 seconds
