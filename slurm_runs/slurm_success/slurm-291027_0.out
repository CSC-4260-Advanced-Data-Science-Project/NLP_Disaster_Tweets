Processing dataset: dropped_v10_lemma_stem_custom_stopwords

üì• Loaded 30 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - prepended_v10_lemma_stem_custom_stopwords: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - dropped_v10_lemma_stem_custom_stopwords: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - kept_v10_lemma_stem_custom_stopwords: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: dropped_v10_lemma_stem_custom_stopwords (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on dropped_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on dropped_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on dropped_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on dropped_v10_lemma_stem_custom_stopwords...
/opt/ohpc/pub/spack/v0.21.1/opt/spack/linux-rocky8-x86_64_v3/gcc-8.5.0/py-joblib-1.2.0-g6vh2usiiivwucfr57aoxwqvljmoxokc/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on dropped_v10_lemma_stem_custom_stopwords...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67718773
Iteration 2, loss = 0.60933251
Iteration 3, loss = 0.50220342
Iteration 4, loss = 0.38973322
Iteration 5, loss = 0.29167733
Iteration 6, loss = 0.21665341
Iteration 7, loss = 0.16447986
Iteration 8, loss = 0.12899464
Iteration 9, loss = 0.10467696
Iteration 10, loss = 0.08738636
Iteration 11, loss = 0.07494832
Iteration 12, loss = 0.06560487
Iteration 13, loss = 0.05833490
Iteration 14, loss = 0.05281996
Iteration 15, loss = 0.04830822
Iteration 16, loss = 0.04478329
Iteration 17, loss = 0.04199702
Iteration 18, loss = 0.03975816
Iteration 19, loss = 0.03738568
Iteration 20, loss = 0.03578036
Iteration 21, loss = 0.03415997
Iteration 22, loss = 0.03309872
Iteration 23, loss = 0.03185479
Iteration 24, loss = 0.03082168
Iteration 25, loss = 0.03009000
Iteration 26, loss = 0.02950694
Iteration 27, loss = 0.02877701
Iteration 28, loss = 0.02810999
Iteration 29, loss = 0.02765276
Iteration 30, loss = 0.02730501
Iteration 31, loss = 0.02667482
Iteration 32, loss = 0.02653023
Iteration 33, loss = 0.02602669
Iteration 34, loss = 0.02556016
Iteration 35, loss = 0.02546080
Iteration 36, loss = 0.02525349
Iteration 37, loss = 0.02499107
Iteration 38, loss = 0.02460075
Iteration 39, loss = 0.02452899
Iteration 40, loss = 0.02423825
Iteration 41, loss = 0.02421133
Iteration 42, loss = 0.02397124
Iteration 43, loss = 0.02388263
Iteration 44, loss = 0.02387920
Iteration 45, loss = 0.02351158
Iteration 46, loss = 0.02353120
Iteration 47, loss = 0.02339334
Iteration 48, loss = 0.02345047
Iteration 49, loss = 0.02328188
Iteration 50, loss = 0.02307707
Iteration 51, loss = 0.02290674
Iteration 52, loss = 0.02303256
Iteration 53, loss = 0.02286817
Iteration 54, loss = 0.02319546
Iteration 55, loss = 0.02262659
Iteration 56, loss = 0.02254098
Iteration 57, loss = 0.02233155
Iteration 58, loss = 0.02256167
Iteration 59, loss = 0.02247444
Iteration 60, loss = 0.02233188
Iteration 61, loss = 0.02254281
Iteration 62, loss = 0.02241986
Iteration 63, loss = 0.02254454
Iteration 64, loss = 0.02238337
Iteration 65, loss = 0.02232817
Iteration 66, loss = 0.02227168
Iteration 67, loss = 0.02235542
Iteration 68, loss = 0.02187870
Iteration 69, loss = 0.02220786
Iteration 70, loss = 0.02187072
Iteration 71, loss = 0.02181160
Iteration 72, loss = 0.02210190
Iteration 73, loss = 0.02170427
Iteration 74, loss = 0.02203638
Iteration 75, loss = 0.02195073
Iteration 76, loss = 0.02166144
Iteration 77, loss = 0.02188317
Iteration 78, loss = 0.02168901
Iteration 79, loss = 0.02206473
Iteration 80, loss = 0.02159683
Iteration 81, loss = 0.02160490
Iteration 82, loss = 0.02171449
Iteration 83, loss = 0.02166051
Iteration 84, loss = 0.02153351
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67625793
Iteration 2, loss = 0.59859405
Iteration 3, loss = 0.49089414
Iteration 4, loss = 0.38355580
Iteration 5, loss = 0.29056388
Iteration 6, loss = 0.21914865
Iteration 7, loss = 0.16809431
Iteration 8, loss = 0.13275010
Iteration 9, loss = 0.10796518
Iteration 10, loss = 0.09051699
Iteration 11, loss = 0.07752856
Iteration 12, loss = 0.06800839
Iteration 13, loss = 0.06090455
Iteration 14, loss = 0.05547587
Iteration 15, loss = 0.05042154
Iteration 16, loss = 0.04693811
Iteration 17, loss = 0.04378981
Iteration 18, loss = 0.04153665
Iteration 19, loss = 0.03944314
Iteration 20, loss = 0.03774783
Iteration 21, loss = 0.03618198
Iteration 22, loss = 0.03514829
Iteration 23, loss = 0.03378603
Iteration 24, loss = 0.03300322
Iteration 25, loss = 0.03195378
Iteration 26, loss = 0.03111472
Iteration 27, loss = 0.03083590
Iteration 28, loss = 0.03025384
Iteration 29, loss = 0.02968000
Iteration 30, loss = 0.02892834
Iteration 31, loss = 0.02860489
Iteration 32, loss = 0.02847880
Iteration 33, loss = 0.02792527
Iteration 34, loss = 0.02782836
Iteration 35, loss = 0.02739624
Iteration 36, loss = 0.02709766
Iteration 37, loss = 0.02679211
Iteration 38, loss = 0.02679357
Iteration 39, loss = 0.02658101
Iteration 40, loss = 0.02621511
Iteration 41, loss = 0.02607891
Iteration 42, loss = 0.02597887
Iteration 43, loss = 0.02577272
Iteration 44, loss = 0.02553093
Iteration 45, loss = 0.02562357
Iteration 46, loss = 0.02546257
Iteration 47, loss = 0.02507707
Iteration 48, loss = 0.02542933
Iteration 49, loss = 0.02501466
Iteration 50, loss = 0.02514427
Iteration 51, loss = 0.02512183
Iteration 52, loss = 0.02480069
Iteration 53, loss = 0.02457434
Iteration 54, loss = 0.02437648
Iteration 55, loss = 0.02472775
Iteration 56, loss = 0.02430345
Iteration 57, loss = 0.02458017
Iteration 58, loss = 0.02493035
Iteration 59, loss = 0.02426211
Iteration 60, loss = 0.02440781
Iteration 61, loss = 0.02469701
Iteration 62, loss = 0.02425969
Iteration 63, loss = 0.02445069
Iteration 64, loss = 0.02408182
Iteration 65, loss = 0.02404133
Iteration 66, loss = 0.02407081
Iteration 67, loss = 0.02425415
Iteration 68, loss = 0.02390687
Iteration 69, loss = 0.02413344
Iteration 70, loss = 0.02396910
Iteration 71, loss = 0.02395949
Iteration 72, loss = 0.02383952
Iteration 73, loss = 0.02407597
Iteration 74, loss = 0.02400115
Iteration 75, loss = 0.02374866
Iteration 76, loss = 0.02341671
Iteration 77, loss = 0.02376919
Iteration 78, loss = 0.02407301
Iteration 79, loss = 0.02375408
Iteration 80, loss = 0.02409318
Iteration 81, loss = 0.02380423
Iteration 82, loss = 0.02335286
Iteration 83, loss = 0.02380039
Iteration 84, loss = 0.02357568
Iteration 85, loss = 0.02351289
Iteration 86, loss = 0.02362213
Iteration 87, loss = 0.02374975
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69483948
Iteration 2, loss = 0.62240907
Iteration 3, loss = 0.51539190
Iteration 4, loss = 0.39799333
Iteration 5, loss = 0.29776528
Iteration 6, loss = 0.22201385
Iteration 7, loss = 0.16838325
Iteration 8, loss = 0.13171756
Iteration 9, loss = 0.10648928
Iteration 10, loss = 0.08840056
Iteration 11, loss = 0.07533248
Iteration 12, loss = 0.06535050
Iteration 13, loss = 0.05814266
Iteration 14, loss = 0.05227830
Iteration 15, loss = 0.04779139
Iteration 16, loss = 0.04398385
Iteration 17, loss = 0.04094082
Iteration 18, loss = 0.03855418
Iteration 19, loss = 0.03623749
Iteration 20, loss = 0.03445122
Iteration 21, loss = 0.03302211
Iteration 22, loss = 0.03172463
Iteration 23, loss = 0.03049736
Iteration 24, loss = 0.02955855
Iteration 25, loss = 0.02869911
Iteration 26, loss = 0.02789813
Iteration 27, loss = 0.02725878
Iteration 28, loss = 0.02660631
Iteration 29, loss = 0.02596032
Iteration 30, loss = 0.02562029
Iteration 31, loss = 0.02505768
Iteration 32, loss = 0.02474234
Iteration 33, loss = 0.02432136
Iteration 34, loss = 0.02403903
Iteration 35, loss = 0.02387794
Iteration 36, loss = 0.02335014
Iteration 37, loss = 0.02326849
Iteration 38, loss = 0.02319859
Iteration 39, loss = 0.02314519
Iteration 40, loss = 0.02246358
Iteration 41, loss = 0.02232525
Iteration 42, loss = 0.02223634
Iteration 43, loss = 0.02208262
Iteration 44, loss = 0.02183876
Iteration 45, loss = 0.02156065
Iteration 46, loss = 0.02149405
Iteration 47, loss = 0.02155666
Iteration 48, loss = 0.02132070
Iteration 49, loss = 0.02106775
Iteration 50, loss = 0.02098053
Iteration 51, loss = 0.02119299
Iteration 52, loss = 0.02098390
Iteration 53, loss = 0.02113137
Iteration 54, loss = 0.02141765
Iteration 55, loss = 0.02058752
Iteration 56, loss = 0.02066985
Iteration 57, loss = 0.02077077
Iteration 58, loss = 0.02055532
Iteration 59, loss = 0.02050812
Iteration 60, loss = 0.02043083
Iteration 61, loss = 0.02024888
Iteration 62, loss = 0.02044355
Iteration 63, loss = 0.02032801
Iteration 64, loss = 0.02013226
Iteration 65, loss = 0.02001368
Iteration 66, loss = 0.02001017
Iteration 67, loss = 0.02017740
Iteration 68, loss = 0.02006744
Iteration 69, loss = 0.02014563
Iteration 70, loss = 0.01972503
Iteration 71, loss = 0.01993739
Iteration 72, loss = 0.01982156
Iteration 73, loss = 0.01988616
Iteration 74, loss = 0.02002609
Iteration 75, loss = 0.01972687
Iteration 76, loss = 0.01978816
Iteration 77, loss = 0.01980313
Iteration 78, loss = 0.01969813
Iteration 79, loss = 0.01966381
Iteration 80, loss = 0.01972240
Iteration 81, loss = 0.01954705
Iteration 82, loss = 0.01975299
Iteration 83, loss = 0.01958907
Iteration 84, loss = 0.01937088
Iteration 85, loss = 0.01973524
Iteration 86, loss = 0.01981192
Iteration 87, loss = 0.01955172
Iteration 88, loss = 0.01958399
Iteration 89, loss = 0.01926385
Iteration 90, loss = 0.01942294
Iteration 91, loss = 0.01970385
Iteration 92, loss = 0.01950412
Iteration 93, loss = 0.01972894
Iteration 94, loss = 0.01934063
Iteration 95, loss = 0.01955010
Iteration 96, loss = 0.01902208
Iteration 97, loss = 0.01940729
Iteration 98, loss = 0.01924512
Iteration 99, loss = 0.01940775
Iteration 100, loss = 0.01931948
Iteration 101, loss = 0.01925141
Iteration 102, loss = 0.01924246
Iteration 103, loss = 0.01924692
Iteration 104, loss = 0.01898799
Iteration 105, loss = 0.01925614
Iteration 106, loss = 0.01940245
Iteration 107, loss = 0.01917111
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67755054
Iteration 2, loss = 0.60298444
Iteration 3, loss = 0.49777532
Iteration 4, loss = 0.38761836
Iteration 5, loss = 0.29187356
Iteration 6, loss = 0.21902283
Iteration 7, loss = 0.16688062
Iteration 8, loss = 0.13071934
Iteration 9, loss = 0.10549192
Iteration 10, loss = 0.08754109
Iteration 11, loss = 0.07466339
Iteration 12, loss = 0.06467842
Iteration 13, loss = 0.05719986
Iteration 14, loss = 0.05149317
Iteration 15, loss = 0.04695790
Iteration 16, loss = 0.04325487
Iteration 17, loss = 0.04007665
Iteration 18, loss = 0.03779273
Iteration 19, loss = 0.03551761
Iteration 20, loss = 0.03381509
Iteration 21, loss = 0.03208058
Iteration 22, loss = 0.03071623
Iteration 23, loss = 0.02959367
Iteration 24, loss = 0.02862404
Iteration 25, loss = 0.02777974
Iteration 26, loss = 0.02750751
Iteration 27, loss = 0.02625424
Iteration 28, loss = 0.02590851
Iteration 29, loss = 0.02526004
Iteration 30, loss = 0.02478531
Iteration 31, loss = 0.02441406
Iteration 32, loss = 0.02378128
Iteration 33, loss = 0.02371494
Iteration 34, loss = 0.02315132
Iteration 35, loss = 0.02318094
Iteration 36, loss = 0.02281961
Iteration 37, loss = 0.02218928
Iteration 38, loss = 0.02214140
Iteration 39, loss = 0.02203302
Iteration 40, loss = 0.02180394
Iteration 41, loss = 0.02161688
Iteration 42, loss = 0.02150342
Iteration 43, loss = 0.02112150
Iteration 44, loss = 0.02135928
Iteration 45, loss = 0.02124352
Iteration 46, loss = 0.02106591
Iteration 47, loss = 0.02098401
Iteration 48, loss = 0.02062630
Iteration 49, loss = 0.02072736
Iteration 50, loss = 0.02066701
Iteration 51, loss = 0.02062884
Iteration 52, loss = 0.02045674
Iteration 53, loss = 0.02030903
Iteration 54, loss = 0.02051127
Iteration 55, loss = 0.02019947
Iteration 56, loss = 0.02002154
Iteration 57, loss = 0.01982331
Iteration 58, loss = 0.02013998
Iteration 59, loss = 0.02003104
Iteration 60, loss = 0.01961895
Iteration 61, loss = 0.01985022
Iteration 62, loss = 0.01974317
Iteration 63, loss = 0.01942335
Iteration 64, loss = 0.01962555
Iteration 65, loss = 0.01945589
Iteration 66, loss = 0.01963748
Iteration 67, loss = 0.01984591
Iteration 68, loss = 0.01957801
Iteration 69, loss = 0.01952483
Iteration 70, loss = 0.01949324
Iteration 71, loss = 0.01946136
Iteration 72, loss = 0.01941628
Iteration 73, loss = 0.01952036
Iteration 74, loss = 0.01926647
Iteration 75, loss = 0.01961518
Iteration 76, loss = 0.01939943
Iteration 77, loss = 0.01938108
Iteration 78, loss = 0.01936765
Iteration 79, loss = 0.01920743
Iteration 80, loss = 0.01907288
Iteration 81, loss = 0.01909226
Iteration 82, loss = 0.01893997
Iteration 83, loss = 0.01931897
Iteration 84, loss = 0.01911013
Iteration 85, loss = 0.01909602
Iteration 86, loss = 0.01903660
Iteration 87, loss = 0.01881807
Iteration 88, loss = 0.01884552
Iteration 89, loss = 0.01915860
Iteration 90, loss = 0.01868207
Iteration 91, loss = 0.01880099
Iteration 92, loss = 0.01908026
Iteration 93, loss = 0.01928897
Iteration 94, loss = 0.01898414
Iteration 95, loss = 0.01869500
Iteration 96, loss = 0.01879381
Iteration 97, loss = 0.01894425
Iteration 98, loss = 0.01870729
Iteration 99, loss = 0.01872318
Iteration 100, loss = 0.01868774
Iteration 101, loss = 0.01897199
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67319660
Iteration 2, loss = 0.59564420
Iteration 3, loss = 0.48180803
Iteration 4, loss = 0.36892058
Iteration 5, loss = 0.27577611
Iteration 6, loss = 0.20715585
Iteration 7, loss = 0.15926229
Iteration 8, loss = 0.12591340
Iteration 9, loss = 0.10281044
Iteration 10, loss = 0.08600182
Iteration 11, loss = 0.07399431
Iteration 12, loss = 0.06469839
Iteration 13, loss = 0.05765651
Iteration 14, loss = 0.05216236
Iteration 15, loss = 0.04774955
Iteration 16, loss = 0.04417657
Iteration 17, loss = 0.04118218
Iteration 18, loss = 0.03847814
Iteration 19, loss = 0.03626817
Iteration 20, loss = 0.03457743
Iteration 21, loss = 0.03299046
Iteration 22, loss = 0.03177394
Iteration 23, loss = 0.03083071
Iteration 24, loss = 0.02943608
Iteration 25, loss = 0.02878878
Iteration 26, loss = 0.02815924
Iteration 27, loss = 0.02726603
Iteration 28, loss = 0.02661631
Iteration 29, loss = 0.02613034
Iteration 30, loss = 0.02561222
Iteration 31, loss = 0.02514962
Iteration 32, loss = 0.02466135
Iteration 33, loss = 0.02441759
Iteration 34, loss = 0.02402017
Iteration 35, loss = 0.02375118
Iteration 36, loss = 0.02359021
Iteration 37, loss = 0.02360758
Iteration 38, loss = 0.02324569
Iteration 39, loss = 0.02294090
Iteration 40, loss = 0.02255550
Iteration 41, loss = 0.02237937
Iteration 42, loss = 0.02243609
Iteration 43, loss = 0.02186649
Iteration 44, loss = 0.02199022
Iteration 45, loss = 0.02193937
Iteration 46, loss = 0.02177698
Iteration 47, loss = 0.02160190
Iteration 48, loss = 0.02142471
Iteration 49, loss = 0.02151911
Iteration 50, loss = 0.02108967
Iteration 51, loss = 0.02129031
Iteration 52, loss = 0.02104652
Iteration 53, loss = 0.02121521
Iteration 54, loss = 0.02077528
Iteration 55, loss = 0.02057511
Iteration 56, loss = 0.02085786
Iteration 57, loss = 0.02087176
Iteration 58, loss = 0.02084279
Iteration 59, loss = 0.02055860
Iteration 60, loss = 0.02051224
Iteration 61, loss = 0.02032625
Iteration 62, loss = 0.02041066
Iteration 63, loss = 0.02028550
Iteration 64, loss = 0.02007991
Iteration 65, loss = 0.02028432
Iteration 66, loss = 0.02028510
Iteration 67, loss = 0.02019493
Iteration 68, loss = 0.02006374
Iteration 69, loss = 0.02020151
Iteration 70, loss = 0.02009908
Iteration 71, loss = 0.02007071
Iteration 72, loss = 0.02034503
Iteration 73, loss = 0.02021280
Iteration 74, loss = 0.02023921
Iteration 75, loss = 0.02003098
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67718773
Iteration 2, loss = 0.60933251
Iteration 3, loss = 0.50220342
Iteration 4, loss = 0.38973322
Iteration 5, loss = 0.29167733
Iteration 6, loss = 0.21665341
Iteration 7, loss = 0.16447986
Iteration 8, loss = 0.12899464
Iteration 9, loss = 0.10467696
Iteration 10, loss = 0.08738636
Iteration 11, loss = 0.07494832
Iteration 12, loss = 0.06560487
Iteration 13, loss = 0.05833490
Iteration 14, loss = 0.05281996
Iteration 15, loss = 0.04830822
Iteration 16, loss = 0.04478329
Iteration 17, loss = 0.04199702
Iteration 18, loss = 0.03975816
Iteration 19, loss = 0.03738568
Iteration 20, loss = 0.03578036
Iteration 21, loss = 0.03415997
Iteration 22, loss = 0.03309872
Iteration 23, loss = 0.03185479
Iteration 24, loss = 0.03082168
Iteration 25, loss = 0.03009000
Iteration 26, loss = 0.02950694
Iteration 27, loss = 0.02877701
Iteration 28, loss = 0.02810999
Iteration 29, loss = 0.02765276
Iteration 30, loss = 0.02730501
Iteration 31, loss = 0.02667482
Iteration 32, loss = 0.02653023
Iteration 33, loss = 0.02602669
Iteration 34, loss = 0.02556016
Iteration 35, loss = 0.02546080
Iteration 36, loss = 0.02525349
Iteration 37, loss = 0.02499107
Iteration 38, loss = 0.02460075
Iteration 39, loss = 0.02452899
Iteration 40, loss = 0.02423825
Iteration 41, loss = 0.02421133
Iteration 42, loss = 0.02397124
Iteration 43, loss = 0.02388263
Iteration 44, loss = 0.02387920
Iteration 45, loss = 0.02351158
Iteration 46, loss = 0.02353120
Iteration 47, loss = 0.02339334
Iteration 48, loss = 0.02345047
Iteration 49, loss = 0.02328188
Iteration 50, loss = 0.02307707
Iteration 51, loss = 0.02290674
Iteration 52, loss = 0.02303256
Iteration 53, loss = 0.02286817
Iteration 54, loss = 0.02319546
Iteration 55, loss = 0.02262659
Iteration 56, loss = 0.02254098
Iteration 57, loss = 0.02233155
Iteration 58, loss = 0.02256167
Iteration 59, loss = 0.02247444
Iteration 60, loss = 0.02233188
Iteration 61, loss = 0.02254281
Iteration 62, loss = 0.02241986
Iteration 63, loss = 0.02254454
Iteration 64, loss = 0.02238337
Iteration 65, loss = 0.02232817
Iteration 66, loss = 0.02227168
Iteration 67, loss = 0.02235542
Iteration 68, loss = 0.02187870
Iteration 69, loss = 0.02220786
Iteration 70, loss = 0.02187072
Iteration 71, loss = 0.02181160
Iteration 72, loss = 0.02210190
Iteration 73, loss = 0.02170427
Iteration 74, loss = 0.02203638
Iteration 75, loss = 0.02195073
Iteration 76, loss = 0.02166144
Iteration 77, loss = 0.02188317
Iteration 78, loss = 0.02168901
Iteration 79, loss = 0.02206473
Iteration 80, loss = 0.02159683
Iteration 81, loss = 0.02160490
Iteration 82, loss = 0.02171449
Iteration 83, loss = 0.02166051
Iteration 84, loss = 0.02153351
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67625793
Iteration 2, loss = 0.59859405
Iteration 3, loss = 0.49089414
Iteration 4, loss = 0.38355580
Iteration 5, loss = 0.29056388
Iteration 6, loss = 0.21914865
Iteration 7, loss = 0.16809431
Iteration 8, loss = 0.13275010
Iteration 9, loss = 0.10796518
Iteration 10, loss = 0.09051699
Iteration 11, loss = 0.07752856
Iteration 12, loss = 0.06800839
Iteration 13, loss = 0.06090455
Iteration 14, loss = 0.05547587
Iteration 15, loss = 0.05042154
Iteration 16, loss = 0.04693811
Iteration 17, loss = 0.04378981
Iteration 18, loss = 0.04153665
Iteration 19, loss = 0.03944314
Iteration 20, loss = 0.03774783
Iteration 21, loss = 0.03618198
Iteration 22, loss = 0.03514829
Iteration 23, loss = 0.03378603
Iteration 24, loss = 0.03300322
Iteration 25, loss = 0.03195378
Iteration 26, loss = 0.03111472
Iteration 27, loss = 0.03083590
Iteration 28, loss = 0.03025384
Iteration 29, loss = 0.02968000
Iteration 30, loss = 0.02892834
Iteration 31, loss = 0.02860489
Iteration 32, loss = 0.02847880
Iteration 33, loss = 0.02792527
Iteration 34, loss = 0.02782836
Iteration 35, loss = 0.02739624
Iteration 36, loss = 0.02709766
Iteration 37, loss = 0.02679211
Iteration 38, loss = 0.02679357
Iteration 39, loss = 0.02658101
Iteration 40, loss = 0.02621511
Iteration 41, loss = 0.02607891
Iteration 42, loss = 0.02597887
Iteration 43, loss = 0.02577272
Iteration 44, loss = 0.02553093
Iteration 45, loss = 0.02562357
Iteration 46, loss = 0.02546257
Iteration 47, loss = 0.02507707
Iteration 48, loss = 0.02542933
Iteration 49, loss = 0.02501466
Iteration 50, loss = 0.02514427
Iteration 51, loss = 0.02512183
Iteration 52, loss = 0.02480069
Iteration 53, loss = 0.02457434
Iteration 54, loss = 0.02437648
Iteration 55, loss = 0.02472775
Iteration 56, loss = 0.02430345
Iteration 57, loss = 0.02458017
Iteration 58, loss = 0.02493035
Iteration 59, loss = 0.02426211
Iteration 60, loss = 0.02440781
Iteration 61, loss = 0.02469701
Iteration 62, loss = 0.02425969
Iteration 63, loss = 0.02445069
Iteration 64, loss = 0.02408182
Iteration 65, loss = 0.02404133
Iteration 66, loss = 0.02407081
Iteration 67, loss = 0.02425415
Iteration 68, loss = 0.02390687
Iteration 69, loss = 0.02413344
Iteration 70, loss = 0.02396910
Iteration 71, loss = 0.02395949
Iteration 72, loss = 0.02383952
Iteration 73, loss = 0.02407597
Iteration 74, loss = 0.02400115
Iteration 75, loss = 0.02374866
Iteration 76, loss = 0.02341671
Iteration 77, loss = 0.02376919
Iteration 78, loss = 0.02407301
Iteration 79, loss = 0.02375408
Iteration 80, loss = 0.02409318
Iteration 81, loss = 0.02380423
Iteration 82, loss = 0.02335286
Iteration 83, loss = 0.02380039
Iteration 84, loss = 0.02357568
Iteration 85, loss = 0.02351289
Iteration 86, loss = 0.02362213
Iteration 87, loss = 0.02374975
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69483948
Iteration 2, loss = 0.62240907
Iteration 3, loss = 0.51539190
Iteration 4, loss = 0.39799333
Iteration 5, loss = 0.29776528
Iteration 6, loss = 0.22201385
Iteration 7, loss = 0.16838325
Iteration 8, loss = 0.13171756
Iteration 9, loss = 0.10648928
Iteration 10, loss = 0.08840056
Iteration 11, loss = 0.07533248
Iteration 12, loss = 0.06535050
Iteration 13, loss = 0.05814266
Iteration 14, loss = 0.05227830
Iteration 15, loss = 0.04779139
Iteration 16, loss = 0.04398385
Iteration 17, loss = 0.04094082
Iteration 18, loss = 0.03855418
Iteration 19, loss = 0.03623749
Iteration 20, loss = 0.03445122
Iteration 21, loss = 0.03302211
Iteration 22, loss = 0.03172463
Iteration 23, loss = 0.03049736
Iteration 24, loss = 0.02955855
Iteration 25, loss = 0.02869911
Iteration 26, loss = 0.02789813
Iteration 27, loss = 0.02725878
Iteration 28, loss = 0.02660631
Iteration 29, loss = 0.02596032
Iteration 30, loss = 0.02562029
Iteration 31, loss = 0.02505768
Iteration 32, loss = 0.02474234
Iteration 33, loss = 0.02432136
Iteration 34, loss = 0.02403903
Iteration 35, loss = 0.02387794
Iteration 36, loss = 0.02335014
Iteration 37, loss = 0.02326849
Iteration 38, loss = 0.02319859
Iteration 39, loss = 0.02314519
Iteration 40, loss = 0.02246358
Iteration 41, loss = 0.02232525
Iteration 42, loss = 0.02223634
Iteration 43, loss = 0.02208262
Iteration 44, loss = 0.02183876
Iteration 45, loss = 0.02156065
Iteration 46, loss = 0.02149405
Iteration 47, loss = 0.02155666
Iteration 48, loss = 0.02132070
Iteration 49, loss = 0.02106775
Iteration 50, loss = 0.02098053
Iteration 51, loss = 0.02119299
Iteration 52, loss = 0.02098390
Iteration 53, loss = 0.02113137
Iteration 54, loss = 0.02141765
Iteration 55, loss = 0.02058752
Iteration 56, loss = 0.02066985
Iteration 57, loss = 0.02077077
Iteration 58, loss = 0.02055532
Iteration 59, loss = 0.02050812
Iteration 60, loss = 0.02043083
Iteration 61, loss = 0.02024888
Iteration 62, loss = 0.02044355
Iteration 63, loss = 0.02032801
Iteration 64, loss = 0.02013226
Iteration 65, loss = 0.02001368
Iteration 66, loss = 0.02001017
Iteration 67, loss = 0.02017740
Iteration 68, loss = 0.02006744
Iteration 69, loss = 0.02014563
Iteration 70, loss = 0.01972503
Iteration 71, loss = 0.01993739
Iteration 72, loss = 0.01982156
Iteration 73, loss = 0.01988616
Iteration 74, loss = 0.02002609
Iteration 75, loss = 0.01972687
Iteration 76, loss = 0.01978816
Iteration 77, loss = 0.01980313
Iteration 78, loss = 0.01969813
Iteration 79, loss = 0.01966381
Iteration 80, loss = 0.01972240
Iteration 81, loss = 0.01954705
Iteration 82, loss = 0.01975299
Iteration 83, loss = 0.01958907
Iteration 84, loss = 0.01937088
Iteration 85, loss = 0.01973524
Iteration 86, loss = 0.01981192
Iteration 87, loss = 0.01955172
Iteration 88, loss = 0.01958399
Iteration 89, loss = 0.01926385
Iteration 90, loss = 0.01942294
Iteration 91, loss = 0.01970385
Iteration 92, loss = 0.01950412
Iteration 93, loss = 0.01972894
Iteration 94, loss = 0.01934063
Iteration 95, loss = 0.01955010
Iteration 96, loss = 0.01902208
Iteration 97, loss = 0.01940729
Iteration 98, loss = 0.01924512
Iteration 99, loss = 0.01940775
Iteration 100, loss = 0.01931948
Iteration 101, loss = 0.01925141
Iteration 102, loss = 0.01924246
Iteration 103, loss = 0.01924692
Iteration 104, loss = 0.01898799
Iteration 105, loss = 0.01925614
Iteration 106, loss = 0.01940245
Iteration 107, loss = 0.01917111
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67755054
Iteration 2, loss = 0.60298444
Iteration 3, loss = 0.49777532
Iteration 4, loss = 0.38761836
Iteration 5, loss = 0.29187356
Iteration 6, loss = 0.21902283
Iteration 7, loss = 0.16688062
Iteration 8, loss = 0.13071934
Iteration 9, loss = 0.10549192
Iteration 10, loss = 0.08754109
Iteration 11, loss = 0.07466339
Iteration 12, loss = 0.06467842
Iteration 13, loss = 0.05719986
Iteration 14, loss = 0.05149317
Iteration 15, loss = 0.04695790
Iteration 16, loss = 0.04325487
Iteration 17, loss = 0.04007665
Iteration 18, loss = 0.03779273
Iteration 19, loss = 0.03551761
Iteration 20, loss = 0.03381509
Iteration 21, loss = 0.03208058
Iteration 22, loss = 0.03071623
Iteration 23, loss = 0.02959367
Iteration 24, loss = 0.02862404
Iteration 25, loss = 0.02777974
Iteration 26, loss = 0.02750751
Iteration 27, loss = 0.02625424
Iteration 28, loss = 0.02590851
Iteration 29, loss = 0.02526004
Iteration 30, loss = 0.02478531
Iteration 31, loss = 0.02441406
Iteration 32, loss = 0.02378128
Iteration 33, loss = 0.02371494
Iteration 34, loss = 0.02315132
Iteration 35, loss = 0.02318094
Iteration 36, loss = 0.02281961
Iteration 37, loss = 0.02218928
Iteration 38, loss = 0.02214140
Iteration 39, loss = 0.02203302
Iteration 40, loss = 0.02180394
Iteration 41, loss = 0.02161688
Iteration 42, loss = 0.02150342
Iteration 43, loss = 0.02112150
Iteration 44, loss = 0.02135928
Iteration 45, loss = 0.02124352
Iteration 46, loss = 0.02106591
Iteration 47, loss = 0.02098401
Iteration 48, loss = 0.02062630
Iteration 49, loss = 0.02072736
Iteration 50, loss = 0.02066701
Iteration 51, loss = 0.02062884
Iteration 52, loss = 0.02045674
Iteration 53, loss = 0.02030903
Iteration 54, loss = 0.02051127
Iteration 55, loss = 0.02019947
Iteration 56, loss = 0.02002154
Iteration 57, loss = 0.01982331
Iteration 58, loss = 0.02013998
Iteration 59, loss = 0.02003104
Iteration 60, loss = 0.01961895
Iteration 61, loss = 0.01985022
Iteration 62, loss = 0.01974317
Iteration 63, loss = 0.01942335
Iteration 64, loss = 0.01962555
Iteration 65, loss = 0.01945589
Iteration 66, loss = 0.01963748
Iteration 67, loss = 0.01984591
Iteration 68, loss = 0.01957801
Iteration 69, loss = 0.01952483
Iteration 70, loss = 0.01949324
Iteration 71, loss = 0.01946136
Iteration 72, loss = 0.01941628
Iteration 73, loss = 0.01952036
Iteration 74, loss = 0.01926647
Iteration 75, loss = 0.01961518
Iteration 76, loss = 0.01939943
Iteration 77, loss = 0.01938108
Iteration 78, loss = 0.01936765
Iteration 79, loss = 0.01920743
Iteration 80, loss = 0.01907288
Iteration 81, loss = 0.01909226
Iteration 82, loss = 0.01893997
Iteration 83, loss = 0.01931897
Iteration 84, loss = 0.01911013
Iteration 85, loss = 0.01909602
Iteration 86, loss = 0.01903660
Iteration 87, loss = 0.01881807
Iteration 88, loss = 0.01884552
Iteration 89, loss = 0.01915860
Iteration 90, loss = 0.01868207
Iteration 91, loss = 0.01880099
Iteration 92, loss = 0.01908026
Iteration 93, loss = 0.01928897
Iteration 94, loss = 0.01898414
Iteration 95, loss = 0.01869500
Iteration 96, loss = 0.01879381
Iteration 97, loss = 0.01894425
Iteration 98, loss = 0.01870729
Iteration 99, loss = 0.01872318
Iteration 100, loss = 0.01868774
Iteration 101, loss = 0.01897199
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67319660
Iteration 2, loss = 0.59564420
Iteration 3, loss = 0.48180803
Iteration 4, loss = 0.36892058
Iteration 5, loss = 0.27577611
Iteration 6, loss = 0.20715585
Iteration 7, loss = 0.15926229
Iteration 8, loss = 0.12591340
Iteration 9, loss = 0.10281044
Iteration 10, loss = 0.08600182
Iteration 11, loss = 0.07399431
Iteration 12, loss = 0.06469839
Iteration 13, loss = 0.05765651
Iteration 14, loss = 0.05216236
Iteration 15, loss = 0.04774955
Iteration 16, loss = 0.04417657
Iteration 17, loss = 0.04118218
Iteration 18, loss = 0.03847814
Iteration 19, loss = 0.03626817
Iteration 20, loss = 0.03457743
Iteration 21, loss = 0.03299046
Iteration 22, loss = 0.03177394
Iteration 23, loss = 0.03083071
Iteration 24, loss = 0.02943608
Iteration 25, loss = 0.02878878
Iteration 26, loss = 0.02815924
Iteration 27, loss = 0.02726603
Iteration 28, loss = 0.02661631
Iteration 29, loss = 0.02613034
Iteration 30, loss = 0.02561222
Iteration 31, loss = 0.02514962
Iteration 32, loss = 0.02466135
Iteration 33, loss = 0.02441759
Iteration 34, loss = 0.02402017
Iteration 35, loss = 0.02375118
Iteration 36, loss = 0.02359021
Iteration 37, loss = 0.02360758
Iteration 38, loss = 0.02324569
Iteration 39, loss = 0.02294090
Iteration 40, loss = 0.02255550
Iteration 41, loss = 0.02237937
Iteration 42, loss = 0.02243609
Iteration 43, loss = 0.02186649
Iteration 44, loss = 0.02199022
Iteration 45, loss = 0.02193937
Iteration 46, loss = 0.02177698
Iteration 47, loss = 0.02160190
Iteration 48, loss = 0.02142471
Iteration 49, loss = 0.02151911
Iteration 50, loss = 0.02108967
Iteration 51, loss = 0.02129031
Iteration 52, loss = 0.02104652
Iteration 53, loss = 0.02121521
Iteration 54, loss = 0.02077528
Iteration 55, loss = 0.02057511
Iteration 56, loss = 0.02085786
Iteration 57, loss = 0.02087176
Iteration 58, loss = 0.02084279
Iteration 59, loss = 0.02055860
Iteration 60, loss = 0.02051224
Iteration 61, loss = 0.02032625
Iteration 62, loss = 0.02041066
Iteration 63, loss = 0.02028550
Iteration 64, loss = 0.02007991
Iteration 65, loss = 0.02028432
Iteration 66, loss = 0.02028510
Iteration 67, loss = 0.02019493
Iteration 68, loss = 0.02006374
Iteration 69, loss = 0.02020151
Iteration 70, loss = 0.02009908
Iteration 71, loss = 0.02007071
Iteration 72, loss = 0.02034503
Iteration 73, loss = 0.02021280
Iteration 74, loss = 0.02023921
Iteration 75, loss = 0.02003098
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67718773
Iteration 2, loss = 0.60933251
Iteration 3, loss = 0.50220342
Iteration 4, loss = 0.38973322
Iteration 5, loss = 0.29167733
Iteration 6, loss = 0.21665341
Iteration 7, loss = 0.16447986
Iteration 8, loss = 0.12899464
Iteration 9, loss = 0.10467696
Iteration 10, loss = 0.08738636
Iteration 11, loss = 0.07494832
Iteration 12, loss = 0.06560487
Iteration 13, loss = 0.05833490
Iteration 14, loss = 0.05281996
Iteration 15, loss = 0.04830822
Iteration 16, loss = 0.04478329
Iteration 17, loss = 0.04199702
Iteration 18, loss = 0.03975816
Iteration 19, loss = 0.03738568
Iteration 20, loss = 0.03578036
Iteration 21, loss = 0.03415997
Iteration 22, loss = 0.03309872
Iteration 23, loss = 0.03185479
Iteration 24, loss = 0.03082168
Iteration 25, loss = 0.03009000
Iteration 26, loss = 0.02950694
Iteration 27, loss = 0.02877701
Iteration 28, loss = 0.02810999
Iteration 29, loss = 0.02765276
Iteration 30, loss = 0.02730501
Iteration 31, loss = 0.02667482
Iteration 32, loss = 0.02653023
Iteration 33, loss = 0.02602669
Iteration 34, loss = 0.02556016
Iteration 35, loss = 0.02546080
Iteration 36, loss = 0.02525349
Iteration 37, loss = 0.02499107
Iteration 38, loss = 0.02460075
Iteration 39, loss = 0.02452899
Iteration 40, loss = 0.02423825
Iteration 41, loss = 0.02421133
Iteration 42, loss = 0.02397124
Iteration 43, loss = 0.02388263
Iteration 44, loss = 0.02387920
Iteration 45, loss = 0.02351158
Iteration 46, loss = 0.02353120
Iteration 47, loss = 0.02339334
Iteration 48, loss = 0.02345047
Iteration 49, loss = 0.02328188
Iteration 50, loss = 0.02307707
Iteration 51, loss = 0.02290674
Iteration 52, loss = 0.02303256
Iteration 53, loss = 0.02286817
Iteration 54, loss = 0.02319546
Iteration 55, loss = 0.02262659
Iteration 56, loss = 0.02254098
Iteration 57, loss = 0.02233155
Iteration 58, loss = 0.02256167
Iteration 59, loss = 0.02247444
Iteration 60, loss = 0.02233188
Iteration 61, loss = 0.02254281
Iteration 62, loss = 0.02241986
Iteration 63, loss = 0.02254454
Iteration 64, loss = 0.02238337
Iteration 65, loss = 0.02232817
Iteration 66, loss = 0.02227168
Iteration 67, loss = 0.02235542
Iteration 68, loss = 0.02187870
Iteration 69, loss = 0.02220786
Iteration 70, loss = 0.02187072
Iteration 71, loss = 0.02181160
Iteration 72, loss = 0.02210190
Iteration 73, loss = 0.02170427
Iteration 74, loss = 0.02203638
Iteration 75, loss = 0.02195073
Iteration 76, loss = 0.02166144
Iteration 77, loss = 0.02188317
Iteration 78, loss = 0.02168901
Iteration 79, loss = 0.02206473
Iteration 80, loss = 0.02159683
Iteration 81, loss = 0.02160490
Iteration 82, loss = 0.02171449
Iteration 83, loss = 0.02166051
Iteration 84, loss = 0.02153351
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67625793
Iteration 2, loss = 0.59859405
Iteration 3, loss = 0.49089414
Iteration 4, loss = 0.38355580
Iteration 5, loss = 0.29056388
Iteration 6, loss = 0.21914865
Iteration 7, loss = 0.16809431
Iteration 8, loss = 0.13275010
Iteration 9, loss = 0.10796518
Iteration 10, loss = 0.09051699
Iteration 11, loss = 0.07752856
Iteration 12, loss = 0.06800839
Iteration 13, loss = 0.06090455
Iteration 14, loss = 0.05547587
Iteration 15, loss = 0.05042154
Iteration 16, loss = 0.04693811
Iteration 17, loss = 0.04378981
Iteration 18, loss = 0.04153665
Iteration 19, loss = 0.03944314
Iteration 20, loss = 0.03774783
Iteration 21, loss = 0.03618198
Iteration 22, loss = 0.03514829
Iteration 23, loss = 0.03378603
Iteration 24, loss = 0.03300322
Iteration 25, loss = 0.03195378
Iteration 26, loss = 0.03111472
Iteration 27, loss = 0.03083590
Iteration 28, loss = 0.03025384
Iteration 29, loss = 0.02968000
Iteration 30, loss = 0.02892834
Iteration 31, loss = 0.02860489
Iteration 32, loss = 0.02847880
Iteration 33, loss = 0.02792527
Iteration 34, loss = 0.02782836
Iteration 35, loss = 0.02739624
Iteration 36, loss = 0.02709766
Iteration 37, loss = 0.02679211
Iteration 38, loss = 0.02679357
Iteration 39, loss = 0.02658101
Iteration 40, loss = 0.02621511
Iteration 41, loss = 0.02607891
Iteration 42, loss = 0.02597887
Iteration 43, loss = 0.02577272
Iteration 44, loss = 0.02553093
Iteration 45, loss = 0.02562357
Iteration 46, loss = 0.02546257
Iteration 47, loss = 0.02507707
Iteration 48, loss = 0.02542933
Iteration 49, loss = 0.02501466
Iteration 50, loss = 0.02514427
Iteration 51, loss = 0.02512183
Iteration 52, loss = 0.02480069
Iteration 53, loss = 0.02457434
Iteration 54, loss = 0.02437648
Iteration 55, loss = 0.02472775
Iteration 56, loss = 0.02430345
Iteration 57, loss = 0.02458017
Iteration 58, loss = 0.02493035
Iteration 59, loss = 0.02426211
Iteration 60, loss = 0.02440781
Iteration 61, loss = 0.02469701
Iteration 62, loss = 0.02425969
Iteration 63, loss = 0.02445069
Iteration 64, loss = 0.02408182
Iteration 65, loss = 0.02404133
Iteration 66, loss = 0.02407081
Iteration 67, loss = 0.02425415
Iteration 68, loss = 0.02390687
Iteration 69, loss = 0.02413344
Iteration 70, loss = 0.02396910
Iteration 71, loss = 0.02395949
Iteration 72, loss = 0.02383952
Iteration 73, loss = 0.02407597
Iteration 74, loss = 0.02400115
Iteration 75, loss = 0.02374866
Iteration 76, loss = 0.02341671
Iteration 77, loss = 0.02376919
Iteration 78, loss = 0.02407301
Iteration 79, loss = 0.02375408
Iteration 80, loss = 0.02409318
Iteration 81, loss = 0.02380423
Iteration 82, loss = 0.02335286
Iteration 83, loss = 0.02380039
Iteration 84, loss = 0.02357568
Iteration 85, loss = 0.02351289
Iteration 86, loss = 0.02362213
Iteration 87, loss = 0.02374975
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69483948
Iteration 2, loss = 0.62240907
Iteration 3, loss = 0.51539190
Iteration 4, loss = 0.39799333
Iteration 5, loss = 0.29776528
Iteration 6, loss = 0.22201385
Iteration 7, loss = 0.16838325
Iteration 8, loss = 0.13171756
Iteration 9, loss = 0.10648928
Iteration 10, loss = 0.08840056
Iteration 11, loss = 0.07533248
Iteration 12, loss = 0.06535050
Iteration 13, loss = 0.05814266
Iteration 14, loss = 0.05227830
Iteration 15, loss = 0.04779139
Iteration 16, loss = 0.04398385
Iteration 17, loss = 0.04094082
Iteration 18, loss = 0.03855418
Iteration 19, loss = 0.03623749
Iteration 20, loss = 0.03445122
Iteration 21, loss = 0.03302211
Iteration 22, loss = 0.03172463
Iteration 23, loss = 0.03049736
Iteration 24, loss = 0.02955855
Iteration 25, loss = 0.02869911
Iteration 26, loss = 0.02789813
Iteration 27, loss = 0.02725878
Iteration 28, loss = 0.02660631
Iteration 29, loss = 0.02596032
Iteration 30, loss = 0.02562029
Iteration 31, loss = 0.02505768
Iteration 32, loss = 0.02474234
Iteration 33, loss = 0.02432136
Iteration 34, loss = 0.02403903
Iteration 35, loss = 0.02387794
Iteration 36, loss = 0.02335014
Iteration 37, loss = 0.02326849
Iteration 38, loss = 0.02319859
Iteration 39, loss = 0.02314519
Iteration 40, loss = 0.02246358
Iteration 41, loss = 0.02232525
Iteration 42, loss = 0.02223634
Iteration 43, loss = 0.02208262
Iteration 44, loss = 0.02183876
Iteration 45, loss = 0.02156065
Iteration 46, loss = 0.02149405
Iteration 47, loss = 0.02155666
Iteration 48, loss = 0.02132070
Iteration 49, loss = 0.02106775
Iteration 50, loss = 0.02098053
Iteration 51, loss = 0.02119299
Iteration 52, loss = 0.02098390
Iteration 53, loss = 0.02113137
Iteration 54, loss = 0.02141765
Iteration 55, loss = 0.02058752
Iteration 56, loss = 0.02066985
Iteration 57, loss = 0.02077077
Iteration 58, loss = 0.02055532
Iteration 59, loss = 0.02050812
Iteration 60, loss = 0.02043083
Iteration 61, loss = 0.02024888
Iteration 62, loss = 0.02044355
Iteration 63, loss = 0.02032801
Iteration 64, loss = 0.02013226
Iteration 65, loss = 0.02001368
Iteration 66, loss = 0.02001017
Iteration 67, loss = 0.02017740
Iteration 68, loss = 0.02006744
Iteration 69, loss = 0.02014563
Iteration 70, loss = 0.01972503
Iteration 71, loss = 0.01993739
Iteration 72, loss = 0.01982156
Iteration 73, loss = 0.01988616
Iteration 74, loss = 0.02002609
Iteration 75, loss = 0.01972687
Iteration 76, loss = 0.01978816
Iteration 77, loss = 0.01980313
Iteration 78, loss = 0.01969813
Iteration 79, loss = 0.01966381
Iteration 80, loss = 0.01972240
Iteration 81, loss = 0.01954705
Iteration 82, loss = 0.01975299
Iteration 83, loss = 0.01958907
Iteration 84, loss = 0.01937088
Iteration 85, loss = 0.01973524
Iteration 86, loss = 0.01981192
Iteration 87, loss = 0.01955172
Iteration 88, loss = 0.01958399
Iteration 89, loss = 0.01926385
Iteration 90, loss = 0.01942294
Iteration 91, loss = 0.01970385
Iteration 92, loss = 0.01950412
Iteration 93, loss = 0.01972894
Iteration 94, loss = 0.01934063
Iteration 95, loss = 0.01955010
Iteration 96, loss = 0.01902208
Iteration 97, loss = 0.01940729
Iteration 98, loss = 0.01924512
Iteration 99, loss = 0.01940775
Iteration 100, loss = 0.01931948
Iteration 101, loss = 0.01925141
Iteration 102, loss = 0.01924246
Iteration 103, loss = 0.01924692
Iteration 104, loss = 0.01898799
Iteration 105, loss = 0.01925614
Iteration 106, loss = 0.01940245
Iteration 107, loss = 0.01917111
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67755054
Iteration 2, loss = 0.60298444
Iteration 3, loss = 0.49777532
Iteration 4, loss = 0.38761836
Iteration 5, loss = 0.29187356
Iteration 6, loss = 0.21902283
Iteration 7, loss = 0.16688062
Iteration 8, loss = 0.13071934
Iteration 9, loss = 0.10549192
Iteration 10, loss = 0.08754109
Iteration 11, loss = 0.07466339
Iteration 12, loss = 0.06467842
Iteration 13, loss = 0.05719986
Iteration 14, loss = 0.05149317
Iteration 15, loss = 0.04695790
Iteration 16, loss = 0.04325487
Iteration 17, loss = 0.04007665
Iteration 18, loss = 0.03779273
Iteration 19, loss = 0.03551761
Iteration 20, loss = 0.03381509
Iteration 21, loss = 0.03208058
Iteration 22, loss = 0.03071623
Iteration 23, loss = 0.02959367
Iteration 24, loss = 0.02862404
Iteration 25, loss = 0.02777974
Iteration 26, loss = 0.02750751
Iteration 27, loss = 0.02625424
Iteration 28, loss = 0.02590851
Iteration 29, loss = 0.02526004
Iteration 30, loss = 0.02478531
Iteration 31, loss = 0.02441406
Iteration 32, loss = 0.02378128
Iteration 33, loss = 0.02371494
Iteration 34, loss = 0.02315132
Iteration 35, loss = 0.02318094
Iteration 36, loss = 0.02281961
Iteration 37, loss = 0.02218928
Iteration 38, loss = 0.02214140
Iteration 39, loss = 0.02203302
Iteration 40, loss = 0.02180394
Iteration 41, loss = 0.02161688
Iteration 42, loss = 0.02150342
Iteration 43, loss = 0.02112150
Iteration 44, loss = 0.02135928
Iteration 45, loss = 0.02124352
Iteration 46, loss = 0.02106591
Iteration 47, loss = 0.02098401
Iteration 48, loss = 0.02062630
Iteration 49, loss = 0.02072736
Iteration 50, loss = 0.02066701
Iteration 51, loss = 0.02062884
Iteration 52, loss = 0.02045674
Iteration 53, loss = 0.02030903
Iteration 54, loss = 0.02051127
Iteration 55, loss = 0.02019947
Iteration 56, loss = 0.02002154
Iteration 57, loss = 0.01982331
Iteration 58, loss = 0.02013998
Iteration 59, loss = 0.02003104
Iteration 60, loss = 0.01961895
Iteration 61, loss = 0.01985022
Iteration 62, loss = 0.01974317
Iteration 63, loss = 0.01942335
Iteration 64, loss = 0.01962555
Iteration 65, loss = 0.01945589
Iteration 66, loss = 0.01963748
Iteration 67, loss = 0.01984591
Iteration 68, loss = 0.01957801
Iteration 69, loss = 0.01952483
Iteration 70, loss = 0.01949324
Iteration 71, loss = 0.01946136
Iteration 72, loss = 0.01941628
Iteration 73, loss = 0.01952036
Iteration 74, loss = 0.01926647
Iteration 75, loss = 0.01961518
Iteration 76, loss = 0.01939943
Iteration 77, loss = 0.01938108
Iteration 78, loss = 0.01936765
Iteration 79, loss = 0.01920743
Iteration 80, loss = 0.01907288
Iteration 81, loss = 0.01909226
Iteration 82, loss = 0.01893997
Iteration 83, loss = 0.01931897
Iteration 84, loss = 0.01911013
Iteration 85, loss = 0.01909602
Iteration 86, loss = 0.01903660
Iteration 87, loss = 0.01881807
Iteration 88, loss = 0.01884552
Iteration 89, loss = 0.01915860
Iteration 90, loss = 0.01868207
Iteration 91, loss = 0.01880099
Iteration 92, loss = 0.01908026
Iteration 93, loss = 0.01928897
Iteration 94, loss = 0.01898414
Iteration 95, loss = 0.01869500
Iteration 96, loss = 0.01879381
Iteration 97, loss = 0.01894425
Iteration 98, loss = 0.01870729
Iteration 99, loss = 0.01872318
Iteration 100, loss = 0.01868774
Iteration 101, loss = 0.01897199
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67319660
Iteration 2, loss = 0.59564420
Iteration 3, loss = 0.48180803
Iteration 4, loss = 0.36892058
Iteration 5, loss = 0.27577611
Iteration 6, loss = 0.20715585
Iteration 7, loss = 0.15926229
Iteration 8, loss = 0.12591340
Iteration 9, loss = 0.10281044
Iteration 10, loss = 0.08600182
Iteration 11, loss = 0.07399431
Iteration 12, loss = 0.06469839
Iteration 13, loss = 0.05765651
Iteration 14, loss = 0.05216236
Iteration 15, loss = 0.04774955
Iteration 16, loss = 0.04417657
Iteration 17, loss = 0.04118218
Iteration 18, loss = 0.03847814
Iteration 19, loss = 0.03626817
Iteration 20, loss = 0.03457743
Iteration 21, loss = 0.03299046
Iteration 22, loss = 0.03177394
Iteration 23, loss = 0.03083071
Iteration 24, loss = 0.02943608
Iteration 25, loss = 0.02878878
Iteration 26, loss = 0.02815924
Iteration 27, loss = 0.02726603
Iteration 28, loss = 0.02661631
Iteration 29, loss = 0.02613034
Iteration 30, loss = 0.02561222
Iteration 31, loss = 0.02514962
Iteration 32, loss = 0.02466135
Iteration 33, loss = 0.02441759
Iteration 34, loss = 0.02402017
Iteration 35, loss = 0.02375118
Iteration 36, loss = 0.02359021
Iteration 37, loss = 0.02360758
Iteration 38, loss = 0.02324569
Iteration 39, loss = 0.02294090
Iteration 40, loss = 0.02255550
Iteration 41, loss = 0.02237937
Iteration 42, loss = 0.02243609
Iteration 43, loss = 0.02186649
Iteration 44, loss = 0.02199022
Iteration 45, loss = 0.02193937
Iteration 46, loss = 0.02177698
Iteration 47, loss = 0.02160190
Iteration 48, loss = 0.02142471
Iteration 49, loss = 0.02151911
Iteration 50, loss = 0.02108967
Iteration 51, loss = 0.02129031
Iteration 52, loss = 0.02104652
Iteration 53, loss = 0.02121521
Iteration 54, loss = 0.02077528
Iteration 55, loss = 0.02057511
Iteration 56, loss = 0.02085786
Iteration 57, loss = 0.02087176
Iteration 58, loss = 0.02084279
Iteration 59, loss = 0.02055860
Iteration 60, loss = 0.02051224
Iteration 61, loss = 0.02032625
Iteration 62, loss = 0.02041066
Iteration 63, loss = 0.02028550
Iteration 64, loss = 0.02007991
Iteration 65, loss = 0.02028432
Iteration 66, loss = 0.02028510
Iteration 67, loss = 0.02019493
Iteration 68, loss = 0.02006374
Iteration 69, loss = 0.02020151
Iteration 70, loss = 0.02009908
Iteration 71, loss = 0.02007071
Iteration 72, loss = 0.02034503
Iteration 73, loss = 0.02021280
Iteration 74, loss = 0.02023921
Iteration 75, loss = 0.02003098
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67718773
Iteration 2, loss = 0.60933251
Iteration 3, loss = 0.50220342
Iteration 4, loss = 0.38973322
Iteration 5, loss = 0.29167733
Iteration 6, loss = 0.21665341
Iteration 7, loss = 0.16447986
Iteration 8, loss = 0.12899464
Iteration 9, loss = 0.10467696
Iteration 10, loss = 0.08738636
Iteration 11, loss = 0.07494832
Iteration 12, loss = 0.06560487
Iteration 13, loss = 0.05833490
Iteration 14, loss = 0.05281996
Iteration 15, loss = 0.04830822
Iteration 16, loss = 0.04478329
Iteration 17, loss = 0.04199702
Iteration 18, loss = 0.03975816
Iteration 19, loss = 0.03738568
Iteration 20, loss = 0.03578036
Iteration 21, loss = 0.03415997
Iteration 22, loss = 0.03309872
Iteration 23, loss = 0.03185479
Iteration 24, loss = 0.03082168
Iteration 25, loss = 0.03009000
Iteration 26, loss = 0.02950694
Iteration 27, loss = 0.02877701
Iteration 28, loss = 0.02810999
Iteration 29, loss = 0.02765276
Iteration 30, loss = 0.02730501
Iteration 31, loss = 0.02667482
Iteration 32, loss = 0.02653023
Iteration 33, loss = 0.02602669
Iteration 34, loss = 0.02556016
Iteration 35, loss = 0.02546080
Iteration 36, loss = 0.02525349
Iteration 37, loss = 0.02499107
Iteration 38, loss = 0.02460075
Iteration 39, loss = 0.02452899
Iteration 40, loss = 0.02423825
Iteration 41, loss = 0.02421133
Iteration 42, loss = 0.02397124
Iteration 43, loss = 0.02388263
Iteration 44, loss = 0.02387920
Iteration 45, loss = 0.02351158
Iteration 46, loss = 0.02353120
Iteration 47, loss = 0.02339334
Iteration 48, loss = 0.02345047
Iteration 49, loss = 0.02328188
Iteration 50, loss = 0.02307707
Iteration 51, loss = 0.02290674
Iteration 52, loss = 0.02303256
Iteration 53, loss = 0.02286817
Iteration 54, loss = 0.02319546
Iteration 55, loss = 0.02262659
Iteration 56, loss = 0.02254098
Iteration 57, loss = 0.02233155
Iteration 58, loss = 0.02256167
Iteration 59, loss = 0.02247444
Iteration 60, loss = 0.02233188
Iteration 61, loss = 0.02254281
Iteration 62, loss = 0.02241986
Iteration 63, loss = 0.02254454
Iteration 64, loss = 0.02238337
Iteration 65, loss = 0.02232817
Iteration 66, loss = 0.02227168
Iteration 67, loss = 0.02235542
Iteration 68, loss = 0.02187870
Iteration 69, loss = 0.02220786
Iteration 70, loss = 0.02187072
Iteration 71, loss = 0.02181160
Iteration 72, loss = 0.02210190
Iteration 73, loss = 0.02170427
Iteration 74, loss = 0.02203638
Iteration 75, loss = 0.02195073
Iteration 76, loss = 0.02166144
Iteration 77, loss = 0.02188317
Iteration 78, loss = 0.02168901
Iteration 79, loss = 0.02206473
Iteration 80, loss = 0.02159683
Iteration 81, loss = 0.02160490
Iteration 82, loss = 0.02171449
Iteration 83, loss = 0.02166051
Iteration 84, loss = 0.02153351
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67625793
Iteration 2, loss = 0.59859405
Iteration 3, loss = 0.49089414
Iteration 4, loss = 0.38355580
Iteration 5, loss = 0.29056388
Iteration 6, loss = 0.21914865
Iteration 7, loss = 0.16809431
Iteration 8, loss = 0.13275010
Iteration 9, loss = 0.10796518
Iteration 10, loss = 0.09051699
Iteration 11, loss = 0.07752856
Iteration 12, loss = 0.06800839
Iteration 13, loss = 0.06090455
Iteration 14, loss = 0.05547587
Iteration 15, loss = 0.05042154
Iteration 16, loss = 0.04693811
Iteration 17, loss = 0.04378981
Iteration 18, loss = 0.04153665
Iteration 19, loss = 0.03944314
Iteration 20, loss = 0.03774783
Iteration 21, loss = 0.03618198
Iteration 22, loss = 0.03514829
Iteration 23, loss = 0.03378603
Iteration 24, loss = 0.03300322
Iteration 25, loss = 0.03195378
Iteration 26, loss = 0.03111472
Iteration 27, loss = 0.03083590
Iteration 28, loss = 0.03025384
Iteration 29, loss = 0.02968000
Iteration 30, loss = 0.02892834
Iteration 31, loss = 0.02860489
Iteration 32, loss = 0.02847880
Iteration 33, loss = 0.02792527
Iteration 34, loss = 0.02782836
Iteration 35, loss = 0.02739624
Iteration 36, loss = 0.02709766
Iteration 37, loss = 0.02679211
Iteration 38, loss = 0.02679357
Iteration 39, loss = 0.02658101
Iteration 40, loss = 0.02621511
Iteration 41, loss = 0.02607891
Iteration 42, loss = 0.02597887
Iteration 43, loss = 0.02577272
Iteration 44, loss = 0.02553093
Iteration 45, loss = 0.02562357
Iteration 46, loss = 0.02546257
Iteration 47, loss = 0.02507707
Iteration 48, loss = 0.02542933
Iteration 49, loss = 0.02501466
Iteration 50, loss = 0.02514427
Iteration 51, loss = 0.02512183
Iteration 52, loss = 0.02480069
Iteration 53, loss = 0.02457434
Iteration 54, loss = 0.02437648
Iteration 55, loss = 0.02472775
Iteration 56, loss = 0.02430345
Iteration 57, loss = 0.02458017
Iteration 58, loss = 0.02493035
Iteration 59, loss = 0.02426211
Iteration 60, loss = 0.02440781
Iteration 61, loss = 0.02469701
Iteration 62, loss = 0.02425969
Iteration 63, loss = 0.02445069
Iteration 64, loss = 0.02408182
Iteration 65, loss = 0.02404133
Iteration 66, loss = 0.02407081
Iteration 67, loss = 0.02425415
Iteration 68, loss = 0.02390687
Iteration 69, loss = 0.02413344
Iteration 70, loss = 0.02396910
Iteration 71, loss = 0.02395949
Iteration 72, loss = 0.02383952
Iteration 73, loss = 0.02407597
Iteration 74, loss = 0.02400115
Iteration 75, loss = 0.02374866
Iteration 76, loss = 0.02341671
Iteration 77, loss = 0.02376919
Iteration 78, loss = 0.02407301
Iteration 79, loss = 0.02375408
Iteration 80, loss = 0.02409318
Iteration 81, loss = 0.02380423
Iteration 82, loss = 0.02335286
Iteration 83, loss = 0.02380039
Iteration 84, loss = 0.02357568
Iteration 85, loss = 0.02351289
Iteration 86, loss = 0.02362213
Iteration 87, loss = 0.02374975
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69483948
Iteration 2, loss = 0.62240907
Iteration 3, loss = 0.51539190
Iteration 4, loss = 0.39799333
Iteration 5, loss = 0.29776528
Iteration 6, loss = 0.22201385
Iteration 7, loss = 0.16838325
Iteration 8, loss = 0.13171756
Iteration 9, loss = 0.10648928
Iteration 10, loss = 0.08840056
Iteration 11, loss = 0.07533248
Iteration 12, loss = 0.06535050
Iteration 13, loss = 0.05814266
Iteration 14, loss = 0.05227830
Iteration 15, loss = 0.04779139
Iteration 16, loss = 0.04398385
Iteration 17, loss = 0.04094082
Iteration 18, loss = 0.03855418
Iteration 19, loss = 0.03623749
Iteration 20, loss = 0.03445122
Iteration 21, loss = 0.03302211
Iteration 22, loss = 0.03172463
Iteration 23, loss = 0.03049736
Iteration 24, loss = 0.02955855
Iteration 25, loss = 0.02869911
Iteration 26, loss = 0.02789813
Iteration 27, loss = 0.02725878
Iteration 28, loss = 0.02660631
Iteration 29, loss = 0.02596032
Iteration 30, loss = 0.02562029
Iteration 31, loss = 0.02505768
Iteration 32, loss = 0.02474234
Iteration 33, loss = 0.02432136
Iteration 34, loss = 0.02403903
Iteration 35, loss = 0.02387794
Iteration 36, loss = 0.02335014
Iteration 37, loss = 0.02326849
Iteration 38, loss = 0.02319859
Iteration 39, loss = 0.02314519
Iteration 40, loss = 0.02246358
Iteration 41, loss = 0.02232525
Iteration 42, loss = 0.02223634
Iteration 43, loss = 0.02208262
Iteration 44, loss = 0.02183876
Iteration 45, loss = 0.02156065
Iteration 46, loss = 0.02149405
Iteration 47, loss = 0.02155666
Iteration 48, loss = 0.02132070
Iteration 49, loss = 0.02106775
Iteration 50, loss = 0.02098053
Iteration 51, loss = 0.02119299
Iteration 52, loss = 0.02098390
Iteration 53, loss = 0.02113137
Iteration 54, loss = 0.02141765
Iteration 55, loss = 0.02058752
Iteration 56, loss = 0.02066985
Iteration 57, loss = 0.02077077
Iteration 58, loss = 0.02055532
Iteration 59, loss = 0.02050812
Iteration 60, loss = 0.02043083
Iteration 61, loss = 0.02024888
Iteration 62, loss = 0.02044355
Iteration 63, loss = 0.02032801
Iteration 64, loss = 0.02013226
Iteration 65, loss = 0.02001368
Iteration 66, loss = 0.02001017
Iteration 67, loss = 0.02017740
Iteration 68, loss = 0.02006744
Iteration 69, loss = 0.02014563
Iteration 70, loss = 0.01972503
Iteration 71, loss = 0.01993739
Iteration 72, loss = 0.01982156
Iteration 73, loss = 0.01988616
Iteration 74, loss = 0.02002609
Iteration 75, loss = 0.01972687
Iteration 76, loss = 0.01978816
Iteration 77, loss = 0.01980313
Iteration 78, loss = 0.01969813
Iteration 79, loss = 0.01966381
Iteration 80, loss = 0.01972240
Iteration 81, loss = 0.01954705
Iteration 82, loss = 0.01975299
Iteration 83, loss = 0.01958907
Iteration 84, loss = 0.01937088
Iteration 85, loss = 0.01973524
Iteration 86, loss = 0.01981192
Iteration 87, loss = 0.01955172
Iteration 88, loss = 0.01958399
Iteration 89, loss = 0.01926385
Iteration 90, loss = 0.01942294
Iteration 91, loss = 0.01970385
Iteration 92, loss = 0.01950412
Iteration 93, loss = 0.01972894
Iteration 94, loss = 0.01934063
Iteration 95, loss = 0.01955010
Iteration 96, loss = 0.01902208
Iteration 97, loss = 0.01940729
Iteration 98, loss = 0.01924512
Iteration 99, loss = 0.01940775
Iteration 100, loss = 0.01931948
Iteration 101, loss = 0.01925141
Iteration 102, loss = 0.01924246
Iteration 103, loss = 0.01924692
Iteration 104, loss = 0.01898799
Iteration 105, loss = 0.01925614
Iteration 106, loss = 0.01940245
Iteration 107, loss = 0.01917111
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67755054
Iteration 2, loss = 0.60298444
Iteration 3, loss = 0.49777532
Iteration 4, loss = 0.38761836
Iteration 5, loss = 0.29187356
Iteration 6, loss = 0.21902283
Iteration 7, loss = 0.16688062
Iteration 8, loss = 0.13071934
Iteration 9, loss = 0.10549192
Iteration 10, loss = 0.08754109
Iteration 11, loss = 0.07466339
Iteration 12, loss = 0.06467842
Iteration 13, loss = 0.05719986
Iteration 14, loss = 0.05149317
Iteration 15, loss = 0.04695790
Iteration 16, loss = 0.04325487
Iteration 17, loss = 0.04007665
Iteration 18, loss = 0.03779273
Iteration 19, loss = 0.03551761
Iteration 20, loss = 0.03381509
Iteration 21, loss = 0.03208058
Iteration 22, loss = 0.03071623
Iteration 23, loss = 0.02959367
Iteration 24, loss = 0.02862404
Iteration 25, loss = 0.02777974
Iteration 26, loss = 0.02750751
Iteration 27, loss = 0.02625424
Iteration 28, loss = 0.02590851
Iteration 29, loss = 0.02526004
Iteration 30, loss = 0.02478531
Iteration 31, loss = 0.02441406
Iteration 32, loss = 0.02378128
Iteration 33, loss = 0.02371494
Iteration 34, loss = 0.02315132
Iteration 35, loss = 0.02318094
Iteration 36, loss = 0.02281961
Iteration 37, loss = 0.02218928
Iteration 38, loss = 0.02214140
Iteration 39, loss = 0.02203302
Iteration 40, loss = 0.02180394
Iteration 41, loss = 0.02161688
Iteration 42, loss = 0.02150342
Iteration 43, loss = 0.02112150
Iteration 44, loss = 0.02135928
Iteration 45, loss = 0.02124352
Iteration 46, loss = 0.02106591
Iteration 47, loss = 0.02098401
Iteration 48, loss = 0.02062630
Iteration 49, loss = 0.02072736
Iteration 50, loss = 0.02066701
Iteration 51, loss = 0.02062884
Iteration 52, loss = 0.02045674
Iteration 53, loss = 0.02030903
Iteration 54, loss = 0.02051127
Iteration 55, loss = 0.02019947
Iteration 56, loss = 0.02002154
Iteration 57, loss = 0.01982331
Iteration 58, loss = 0.02013998
Iteration 59, loss = 0.02003104
Iteration 60, loss = 0.01961895
Iteration 61, loss = 0.01985022
Iteration 62, loss = 0.01974317
Iteration 63, loss = 0.01942335
Iteration 64, loss = 0.01962555
Iteration 65, loss = 0.01945589
Iteration 66, loss = 0.01963748
Iteration 67, loss = 0.01984591
Iteration 68, loss = 0.01957801
Iteration 69, loss = 0.01952483
Iteration 70, loss = 0.01949324
Iteration 71, loss = 0.01946136
Iteration 72, loss = 0.01941628
Iteration 73, loss = 0.01952036
Iteration 74, loss = 0.01926647
Iteration 75, loss = 0.01961518
Iteration 76, loss = 0.01939943
Iteration 77, loss = 0.01938108
Iteration 78, loss = 0.01936765
Iteration 79, loss = 0.01920743
Iteration 80, loss = 0.01907288
Iteration 81, loss = 0.01909226
Iteration 82, loss = 0.01893997
Iteration 83, loss = 0.01931897
Iteration 84, loss = 0.01911013
Iteration 85, loss = 0.01909602
Iteration 86, loss = 0.01903660
Iteration 87, loss = 0.01881807
Iteration 88, loss = 0.01884552
Iteration 89, loss = 0.01915860
Iteration 90, loss = 0.01868207
Iteration 91, loss = 0.01880099
Iteration 92, loss = 0.01908026
Iteration 93, loss = 0.01928897
Iteration 94, loss = 0.01898414
Iteration 95, loss = 0.01869500
Iteration 96, loss = 0.01879381
Iteration 97, loss = 0.01894425
Iteration 98, loss = 0.01870729
Iteration 99, loss = 0.01872318
Iteration 100, loss = 0.01868774
Iteration 101, loss = 0.01897199
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67319660
Iteration 2, loss = 0.59564420
Iteration 3, loss = 0.48180803
Iteration 4, loss = 0.36892058
Iteration 5, loss = 0.27577611
Iteration 6, loss = 0.20715585
Iteration 7, loss = 0.15926229
Iteration 8, loss = 0.12591340
Iteration 9, loss = 0.10281044
Iteration 10, loss = 0.08600182
Iteration 11, loss = 0.07399431
Iteration 12, loss = 0.06469839
Iteration 13, loss = 0.05765651
Iteration 14, loss = 0.05216236
Iteration 15, loss = 0.04774955
Iteration 16, loss = 0.04417657
Iteration 17, loss = 0.04118218
Iteration 18, loss = 0.03847814
Iteration 19, loss = 0.03626817
Iteration 20, loss = 0.03457743
Iteration 21, loss = 0.03299046
Iteration 22, loss = 0.03177394
Iteration 23, loss = 0.03083071
Iteration 24, loss = 0.02943608
Iteration 25, loss = 0.02878878
Iteration 26, loss = 0.02815924
Iteration 27, loss = 0.02726603
Iteration 28, loss = 0.02661631
Iteration 29, loss = 0.02613034
Iteration 30, loss = 0.02561222
Iteration 31, loss = 0.02514962
Iteration 32, loss = 0.02466135
Iteration 33, loss = 0.02441759
Iteration 34, loss = 0.02402017
Iteration 35, loss = 0.02375118
Iteration 36, loss = 0.02359021
Iteration 37, loss = 0.02360758
Iteration 38, loss = 0.02324569
Iteration 39, loss = 0.02294090
Iteration 40, loss = 0.02255550
Iteration 41, loss = 0.02237937
Iteration 42, loss = 0.02243609
Iteration 43, loss = 0.02186649
Iteration 44, loss = 0.02199022
Iteration 45, loss = 0.02193937
Iteration 46, loss = 0.02177698
Iteration 47, loss = 0.02160190
Iteration 48, loss = 0.02142471
Iteration 49, loss = 0.02151911
Iteration 50, loss = 0.02108967
Iteration 51, loss = 0.02129031
Iteration 52, loss = 0.02104652
Iteration 53, loss = 0.02121521
Iteration 54, loss = 0.02077528
Iteration 55, loss = 0.02057511
Iteration 56, loss = 0.02085786
Iteration 57, loss = 0.02087176
Iteration 58, loss = 0.02084279
Iteration 59, loss = 0.02055860
Iteration 60, loss = 0.02051224
Iteration 61, loss = 0.02032625
Iteration 62, loss = 0.02041066
Iteration 63, loss = 0.02028550
Iteration 64, loss = 0.02007991
Iteration 65, loss = 0.02028432
Iteration 66, loss = 0.02028510
Iteration 67, loss = 0.02019493
Iteration 68, loss = 0.02006374
Iteration 69, loss = 0.02020151
Iteration 70, loss = 0.02009908
Iteration 71, loss = 0.02007071
Iteration 72, loss = 0.02034503
Iteration 73, loss = 0.02021280
Iteration 74, loss = 0.02023921
Iteration 75, loss = 0.02003098
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69534727
Iteration 2, loss = 0.60698370
Iteration 3, loss = 0.47388107
Iteration 4, loss = 0.34354677
Iteration 5, loss = 0.24432034
Iteration 6, loss = 0.17756276
Iteration 7, loss = 0.13421504
Iteration 8, loss = 0.10592625
Iteration 9, loss = 0.08723930
Iteration 10, loss = 0.07386630
Iteration 11, loss = 0.06448421
Iteration 12, loss = 0.05705216
Iteration 13, loss = 0.05198893
Iteration 14, loss = 0.04744548
Iteration 15, loss = 0.04399078
Iteration 16, loss = 0.04122216
Iteration 17, loss = 0.03889583
Iteration 18, loss = 0.03711313
Iteration 19, loss = 0.03537520
Iteration 20, loss = 0.03428512
Iteration 21, loss = 0.03295296
Iteration 22, loss = 0.03211446
Iteration 23, loss = 0.03105787
Iteration 24, loss = 0.03031927
Iteration 25, loss = 0.02993703
Iteration 26, loss = 0.02902882
Iteration 27, loss = 0.02887690
Iteration 28, loss = 0.02836231
Iteration 29, loss = 0.02792538
Iteration 30, loss = 0.02757528
Iteration 31, loss = 0.02729554
Iteration 32, loss = 0.02692654
Iteration 33, loss = 0.02682798
Iteration 34, loss = 0.02652550
Iteration 35, loss = 0.02614239
Iteration 36, loss = 0.02591581
Iteration 37, loss = 0.02577967
Iteration 38, loss = 0.02571711
Iteration 39, loss = 0.02565944
Iteration 40, loss = 0.02536121
Iteration 41, loss = 0.02500136
Iteration 42, loss = 0.02505388
Iteration 43, loss = 0.02482443
Iteration 44, loss = 0.02485835
Iteration 45, loss = 0.02455175
Iteration 46, loss = 0.02461420
Iteration 47, loss = 0.02457502
Iteration 48, loss = 0.02444789
Iteration 49, loss = 0.02446313
Iteration 50, loss = 0.02425051
Iteration 51, loss = 0.02418597
Iteration 52, loss = 0.02414599
Iteration 53, loss = 0.02406889
Iteration 54, loss = 0.02413490
Iteration 55, loss = 0.02447779
Iteration 56, loss = 0.02416159
Iteration 57, loss = 0.02409219
Iteration 58, loss = 0.02398066
Iteration 59, loss = 0.02347170
Iteration 60, loss = 0.02382631
Iteration 61, loss = 0.02390190
Iteration 62, loss = 0.02402580
Iteration 63, loss = 0.02358529
Iteration 64, loss = 0.02369838
Iteration 65, loss = 0.02388266
Iteration 66, loss = 0.02339206
Iteration 67, loss = 0.02335374
Iteration 68, loss = 0.02364301
Iteration 69, loss = 0.02345296
Iteration 70, loss = 0.02374400
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on dropped_v10_lemma_stem_custom_stopwords...
Iteration 1, loss = 0.68325578
Iteration 2, loss = 0.66689082
Iteration 3, loss = 0.65381514
Iteration 4, loss = 0.64114165
Iteration 5, loss = 0.62827869
Iteration 6, loss = 0.61440007
Iteration 7, loss = 0.59919883
Iteration 8, loss = 0.58257195
Iteration 9, loss = 0.56446517
Iteration 10, loss = 0.54502299
Iteration 11, loss = 0.52432785
Iteration 12, loss = 0.50245316
Iteration 13, loss = 0.48009066
Iteration 14, loss = 0.45703501
Iteration 15, loss = 0.43371974
Iteration 16, loss = 0.41062426
Iteration 17, loss = 0.38786557
Iteration 18, loss = 0.36551788
Iteration 19, loss = 0.34393008
Iteration 20, loss = 0.32310104
Iteration 21, loss = 0.30334828
Iteration 22, loss = 0.28456615
Iteration 23, loss = 0.26670286
Iteration 24, loss = 0.25003242
Iteration 25, loss = 0.23433878
Iteration 26, loss = 0.21966498
Iteration 27, loss = 0.20598249
Iteration 28, loss = 0.19327111
Iteration 29, loss = 0.18147925
Iteration 30, loss = 0.17049453
Iteration 31, loss = 0.16036623
Iteration 32, loss = 0.15092652
Iteration 33, loss = 0.14228564
Iteration 34, loss = 0.13429202
Iteration 35, loss = 0.12681517
Iteration 36, loss = 0.12001894
Iteration 37, loss = 0.11366853
Iteration 38, loss = 0.10779877
Iteration 39, loss = 0.10229749
Iteration 40, loss = 0.09724187
Iteration 41, loss = 0.09253202
Iteration 42, loss = 0.08824219
Iteration 43, loss = 0.08417610
Iteration 44, loss = 0.08037620
Iteration 45, loss = 0.07683806
Iteration 46, loss = 0.07356380
Iteration 47, loss = 0.07047784
Iteration 48, loss = 0.06765290
Iteration 49, loss = 0.06491340
Iteration 50, loss = 0.06242674
Iteration 51, loss = 0.06002905
Iteration 52, loss = 0.05782233
Iteration 53, loss = 0.05576511
Iteration 54, loss = 0.05382756
Iteration 55, loss = 0.05198391
Iteration 56, loss = 0.05025515
Iteration 57, loss = 0.04863129
Iteration 58, loss = 0.04707732
Iteration 59, loss = 0.04561303
Iteration 60, loss = 0.04424873
Iteration 61, loss = 0.04295329
Iteration 62, loss = 0.04170658
Iteration 63, loss = 0.04053463
Iteration 64, loss = 0.03944495
Iteration 65, loss = 0.03837221
Iteration 66, loss = 0.03736140
Iteration 67, loss = 0.03643420
Iteration 68, loss = 0.03548194
Iteration 69, loss = 0.03462694
Iteration 70, loss = 0.03377811
Iteration 71, loss = 0.03297871
Iteration 72, loss = 0.03220307
Iteration 73, loss = 0.03147327
Iteration 74, loss = 0.03077089
Iteration 75, loss = 0.03009542
Iteration 76, loss = 0.02948154
Iteration 77, loss = 0.02885051
Iteration 78, loss = 0.02832024
Iteration 79, loss = 0.02773856
Iteration 80, loss = 0.02721719
Iteration 81, loss = 0.02673224
Iteration 82, loss = 0.02622837
Iteration 83, loss = 0.02570679
Iteration 84, loss = 0.02524321
Iteration 85, loss = 0.02480789
Iteration 86, loss = 0.02437009
Iteration 87, loss = 0.02389849
Iteration 88, loss = 0.02350800
Iteration 89, loss = 0.02310889
Iteration 90, loss = 0.02271413
Iteration 91, loss = 0.02235678
Iteration 92, loss = 0.02199033
Iteration 93, loss = 0.02166048
Iteration 94, loss = 0.02130756
Iteration 95, loss = 0.02099288
Iteration 96, loss = 0.02069989
Iteration 97, loss = 0.02040573
Iteration 98, loss = 0.02012710
Iteration 99, loss = 0.01982532
Iteration 100, loss = 0.01956177
Iteration 101, loss = 0.01929354
Iteration 102, loss = 0.01902558
Iteration 103, loss = 0.01880149
Iteration 104, loss = 0.01852986
Iteration 105, loss = 0.01830969
Iteration 106, loss = 0.01807182
Iteration 107, loss = 0.01786313
Iteration 108, loss = 0.01768341
Iteration 109, loss = 0.01745710
Iteration 110, loss = 0.01726258
Iteration 111, loss = 0.01708067
Iteration 112, loss = 0.01694402
Iteration 113, loss = 0.01672513
Iteration 114, loss = 0.01653558
Iteration 115, loss = 0.01636474
Iteration 116, loss = 0.01619450
Iteration 117, loss = 0.01602055
Iteration 118, loss = 0.01587639
Iteration 119, loss = 0.01570351
Iteration 120, loss = 0.01557533
Iteration 121, loss = 0.01539558
Iteration 122, loss = 0.01520647
Iteration 123, loss = 0.01507073
Iteration 124, loss = 0.01489794
Iteration 125, loss = 0.01477102
Iteration 126, loss = 0.01467270
Iteration 127, loss = 0.01458246
Iteration 128, loss = 0.01442026
Iteration 129, loss = 0.01430266
Iteration 130, loss = 0.01419937
Iteration 131, loss = 0.01412755
Iteration 132, loss = 0.01398253
Iteration 133, loss = 0.01386106
Iteration 134, loss = 0.01385193
Iteration 135, loss = 0.01369696
Iteration 136, loss = 0.01364647
Iteration 137, loss = 0.01352223
Iteration 138, loss = 0.01338147
Iteration 139, loss = 0.01327001
Iteration 140, loss = 0.01321970
Iteration 141, loss = 0.01304613
Iteration 142, loss = 0.01296048
Iteration 143, loss = 0.01284713
Iteration 144, loss = 0.01282310
Iteration 145, loss = 0.01276543
Iteration 146, loss = 0.01267007
Iteration 147, loss = 0.01257233
Iteration 148, loss = 0.01249930
Iteration 149, loss = 0.01240047
Iteration 150, loss = 0.01233301
Iteration 151, loss = 0.01226748
Iteration 152, loss = 0.01218691
Iteration 153, loss = 0.01210954
Iteration 154, loss = 0.01204383
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67564137
Iteration 2, loss = 0.66040695
Iteration 3, loss = 0.64627396
Iteration 4, loss = 0.63249367
Iteration 5, loss = 0.61807067
Iteration 6, loss = 0.60255091
Iteration 7, loss = 0.58571258
Iteration 8, loss = 0.56801540
Iteration 9, loss = 0.54899448
Iteration 10, loss = 0.52897594
Iteration 11, loss = 0.50803821
Iteration 12, loss = 0.48660723
Iteration 13, loss = 0.46462442
Iteration 14, loss = 0.44231251
Iteration 15, loss = 0.42007028
Iteration 16, loss = 0.39781995
Iteration 17, loss = 0.37584830
Iteration 18, loss = 0.35436873
Iteration 19, loss = 0.33375215
Iteration 20, loss = 0.31376056
Iteration 21, loss = 0.29471728
Iteration 22, loss = 0.27656660
Iteration 23, loss = 0.25936618
Iteration 24, loss = 0.24316310
Iteration 25, loss = 0.22772993
Iteration 26, loss = 0.21348148
Iteration 27, loss = 0.20009748
Iteration 28, loss = 0.18759733
Iteration 29, loss = 0.17598866
Iteration 30, loss = 0.16520470
Iteration 31, loss = 0.15503396
Iteration 32, loss = 0.14578251
Iteration 33, loss = 0.13717228
Iteration 34, loss = 0.12916621
Iteration 35, loss = 0.12175086
Iteration 36, loss = 0.11497693
Iteration 37, loss = 0.10853614
Iteration 38, loss = 0.10273762
Iteration 39, loss = 0.09729570
Iteration 40, loss = 0.09225730
Iteration 41, loss = 0.08756855
Iteration 42, loss = 0.08327299
Iteration 43, loss = 0.07923275
Iteration 44, loss = 0.07554425
Iteration 45, loss = 0.07207934
Iteration 46, loss = 0.06887179
Iteration 47, loss = 0.06584002
Iteration 48, loss = 0.06303082
Iteration 49, loss = 0.06045024
Iteration 50, loss = 0.05799269
Iteration 51, loss = 0.05569076
Iteration 52, loss = 0.05354304
Iteration 53, loss = 0.05155402
Iteration 54, loss = 0.04962790
Iteration 55, loss = 0.04787276
Iteration 56, loss = 0.04616634
Iteration 57, loss = 0.04455685
Iteration 58, loss = 0.04307123
Iteration 59, loss = 0.04162947
Iteration 60, loss = 0.04030403
Iteration 61, loss = 0.03900517
Iteration 62, loss = 0.03779364
Iteration 63, loss = 0.03663025
Iteration 64, loss = 0.03554547
Iteration 65, loss = 0.03451176
Iteration 66, loss = 0.03349039
Iteration 67, loss = 0.03254591
Iteration 68, loss = 0.03164153
Iteration 69, loss = 0.03082859
Iteration 70, loss = 0.02996832
Iteration 71, loss = 0.02918646
Iteration 72, loss = 0.02844647
Iteration 73, loss = 0.02772194
Iteration 74, loss = 0.02706896
Iteration 75, loss = 0.02639916
Iteration 76, loss = 0.02578331
Iteration 77, loss = 0.02514337
Iteration 78, loss = 0.02460770
Iteration 79, loss = 0.02403376
Iteration 80, loss = 0.02349190
Iteration 81, loss = 0.02299798
Iteration 82, loss = 0.02249144
Iteration 83, loss = 0.02202521
Iteration 84, loss = 0.02157389
Iteration 85, loss = 0.02113287
Iteration 86, loss = 0.02071465
Iteration 87, loss = 0.02032134
Iteration 88, loss = 0.01991638
Iteration 89, loss = 0.01955022
Iteration 90, loss = 0.01916951
Iteration 91, loss = 0.01881991
Iteration 92, loss = 0.01847713
Iteration 93, loss = 0.01814958
Iteration 94, loss = 0.01784908
Iteration 95, loss = 0.01754190
Iteration 96, loss = 0.01724013
Iteration 97, loss = 0.01697338
Iteration 98, loss = 0.01666973
Iteration 99, loss = 0.01639775
Iteration 100, loss = 0.01613352
Iteration 101, loss = 0.01593015
Iteration 102, loss = 0.01563507
Iteration 103, loss = 0.01539467
Iteration 104, loss = 0.01515865
Iteration 105, loss = 0.01494482
Iteration 106, loss = 0.01472800
Iteration 107, loss = 0.01451916
Iteration 108, loss = 0.01432316
Iteration 109, loss = 0.01412654
Iteration 110, loss = 0.01393248
Iteration 111, loss = 0.01373638
Iteration 112, loss = 0.01356705
Iteration 113, loss = 0.01338130
Iteration 114, loss = 0.01322896
Iteration 115, loss = 0.01303726
Iteration 116, loss = 0.01287408
Iteration 117, loss = 0.01274279
Iteration 118, loss = 0.01255622
Iteration 119, loss = 0.01240655
Iteration 120, loss = 0.01226229
Iteration 121, loss = 0.01213810
Iteration 122, loss = 0.01199625
Iteration 123, loss = 0.01184361
Iteration 124, loss = 0.01171484
Iteration 125, loss = 0.01158075
Iteration 126, loss = 0.01147250
Iteration 127, loss = 0.01134472
Iteration 128, loss = 0.01121043
Iteration 129, loss = 0.01110035
Iteration 130, loss = 0.01100718
Iteration 131, loss = 0.01089950
Iteration 132, loss = 0.01080883
Iteration 133, loss = 0.01069582
Iteration 134, loss = 0.01059794
Iteration 135, loss = 0.01049302
Iteration 136, loss = 0.01038293
Iteration 137, loss = 0.01027803
Iteration 138, loss = 0.01018313
Iteration 139, loss = 0.01008083
Iteration 140, loss = 0.00998767
Iteration 141, loss = 0.00989482
Iteration 142, loss = 0.00980392
Iteration 143, loss = 0.00970856
Iteration 144, loss = 0.00964561
Iteration 145, loss = 0.00953908
Iteration 146, loss = 0.00947298
Iteration 147, loss = 0.00937249
Iteration 148, loss = 0.00929608
Iteration 149, loss = 0.00921708
Iteration 150, loss = 0.00916128
Iteration 151, loss = 0.00906611
Iteration 152, loss = 0.00901093
Iteration 153, loss = 0.00893387
Iteration 154, loss = 0.00885975
Iteration 155, loss = 0.00881269
Iteration 156, loss = 0.00876958
Iteration 157, loss = 0.00867435
Iteration 158, loss = 0.00862668
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69158083
Iteration 2, loss = 0.66927513
Iteration 3, loss = 0.64356286
Iteration 4, loss = 0.61121736
Iteration 5, loss = 0.57027856
Iteration 6, loss = 0.52279878
Iteration 7, loss = 0.47326734
Iteration 8, loss = 0.42433679
Iteration 9, loss = 0.37744334
Iteration 10, loss = 0.33400433
Iteration 11, loss = 0.29415951
Iteration 12, loss = 0.25867219
Iteration 13, loss = 0.22756943
Iteration 14, loss = 0.20058925
Iteration 15, loss = 0.17735290
Iteration 16, loss = 0.15731965
Iteration 17, loss = 0.14036542
Iteration 18, loss = 0.12580840
Iteration 19, loss = 0.11331224
Iteration 20, loss = 0.10277940
Iteration 21, loss = 0.09368302
Iteration 22, loss = 0.08575352
Iteration 23, loss = 0.07891636
Iteration 24, loss = 0.07296837
Iteration 25, loss = 0.06767620
Iteration 26, loss = 0.06301892
Iteration 27, loss = 0.05895585
Iteration 28, loss = 0.05534009
Iteration 29, loss = 0.05205974
Iteration 30, loss = 0.04913667
Iteration 31, loss = 0.04659122
Iteration 32, loss = 0.04426209
Iteration 33, loss = 0.04228627
Iteration 34, loss = 0.04024633
Iteration 35, loss = 0.03848441
Iteration 36, loss = 0.03686087
Iteration 37, loss = 0.03541727
Iteration 38, loss = 0.03402248
Iteration 39, loss = 0.03275602
Iteration 40, loss = 0.03162631
Iteration 41, loss = 0.03055807
Iteration 42, loss = 0.02970079
Iteration 43, loss = 0.02892937
Iteration 44, loss = 0.02814436
Iteration 45, loss = 0.02732090
Iteration 46, loss = 0.02654326
Iteration 47, loss = 0.02574494
Iteration 48, loss = 0.02506029
Iteration 49, loss = 0.02441117
Iteration 50, loss = 0.02379924
Iteration 51, loss = 0.02349847
Iteration 52, loss = 0.02279588
Iteration 53, loss = 0.02259726
Iteration 54, loss = 0.02218619
Iteration 55, loss = 0.02171383
Iteration 56, loss = 0.02129638
Iteration 57, loss = 0.02079123
Iteration 58, loss = 0.02043991
Iteration 59, loss = 0.02004100
Iteration 60, loss = 0.01976979
Iteration 61, loss = 0.01936128
Iteration 62, loss = 0.01931816
Iteration 63, loss = 0.01896134
Iteration 64, loss = 0.01884930
Iteration 65, loss = 0.01860883
Iteration 66, loss = 0.01837000
Iteration 67, loss = 0.01817750
Iteration 68, loss = 0.01791648
Iteration 69, loss = 0.01767698
Iteration 70, loss = 0.01740523
Iteration 71, loss = 0.01710004
Iteration 72, loss = 0.01696390
Iteration 73, loss = 0.01678057
Iteration 74, loss = 0.01649398
Iteration 75, loss = 0.01638130
Iteration 76, loss = 0.01629572
Iteration 77, loss = 0.01608742
Iteration 78, loss = 0.01589773
Iteration 79, loss = 0.01583262
Iteration 80, loss = 0.01566561
Iteration 81, loss = 0.01557374
Iteration 82, loss = 0.01533690
Iteration 83, loss = 0.01528544
Iteration 84, loss = 0.01518263
Iteration 85, loss = 0.01507908
Iteration 86, loss = 0.01507992
Iteration 87, loss = 0.01494112
Iteration 88, loss = 0.01490035
Iteration 89, loss = 0.01495075
Iteration 90, loss = 0.01491981
Iteration 91, loss = 0.01489830
Iteration 92, loss = 0.01470375
Iteration 93, loss = 0.01465237
Iteration 94, loss = 0.01453794
Iteration 95, loss = 0.01439946
Iteration 96, loss = 0.01429784
Iteration 97, loss = 0.01419771
Iteration 98, loss = 0.01409097
Iteration 99, loss = 0.01412368
Iteration 100, loss = 0.01393856
Iteration 101, loss = 0.01393366
Iteration 102, loss = 0.01388182
Iteration 103, loss = 0.01375081
Iteration 104, loss = 0.01372914
Iteration 105, loss = 0.01368761
Iteration 106, loss = 0.01380610
Iteration 107, loss = 0.01383999
Iteration 108, loss = 0.01379712
Iteration 109, loss = 0.01345430
Iteration 110, loss = 0.01394542
Iteration 111, loss = 0.01386532
Iteration 112, loss = 0.01352964
Iteration 113, loss = 0.01355833
Iteration 114, loss = 0.01350292
Iteration 115, loss = 0.01337127
Iteration 116, loss = 0.01323184
Iteration 117, loss = 0.01313882
Iteration 118, loss = 0.01319558
Iteration 119, loss = 0.01305314
Iteration 120, loss = 0.01299372
Iteration 121, loss = 0.01298488
Iteration 122, loss = 0.01290114
Iteration 123, loss = 0.01291396
Iteration 124, loss = 0.01282285
Iteration 125, loss = 0.01289974
Iteration 126, loss = 0.01282242
Iteration 127, loss = 0.01281578
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67944936
Iteration 2, loss = 0.66009626
Iteration 3, loss = 0.63673834
Iteration 4, loss = 0.60488103
Iteration 5, loss = 0.56385451
Iteration 6, loss = 0.51727615
Iteration 7, loss = 0.46876124
Iteration 8, loss = 0.42055076
Iteration 9, loss = 0.37418391
Iteration 10, loss = 0.33117708
Iteration 11, loss = 0.29213718
Iteration 12, loss = 0.25718870
Iteration 13, loss = 0.22643200
Iteration 14, loss = 0.19976159
Iteration 15, loss = 0.17680856
Iteration 16, loss = 0.15710016
Iteration 17, loss = 0.14017656
Iteration 18, loss = 0.12560818
Iteration 19, loss = 0.11311178
Iteration 20, loss = 0.10242816
Iteration 21, loss = 0.09314432
Iteration 22, loss = 0.08514939
Iteration 23, loss = 0.07821020
Iteration 24, loss = 0.07209617
Iteration 25, loss = 0.06685121
Iteration 26, loss = 0.06213362
Iteration 27, loss = 0.05801987
Iteration 28, loss = 0.05421978
Iteration 29, loss = 0.05104802
Iteration 30, loss = 0.04806258
Iteration 31, loss = 0.04542202
Iteration 32, loss = 0.04297155
Iteration 33, loss = 0.04075907
Iteration 34, loss = 0.03881570
Iteration 35, loss = 0.03699392
Iteration 36, loss = 0.03547572
Iteration 37, loss = 0.03390770
Iteration 38, loss = 0.03253757
Iteration 39, loss = 0.03126631
Iteration 40, loss = 0.03006287
Iteration 41, loss = 0.02888410
Iteration 42, loss = 0.02784079
Iteration 43, loss = 0.02696643
Iteration 44, loss = 0.02611290
Iteration 45, loss = 0.02530996
Iteration 46, loss = 0.02450676
Iteration 47, loss = 0.02384030
Iteration 48, loss = 0.02321034
Iteration 49, loss = 0.02263318
Iteration 50, loss = 0.02218385
Iteration 51, loss = 0.02163384
Iteration 52, loss = 0.02120536
Iteration 53, loss = 0.02061319
Iteration 54, loss = 0.02013893
Iteration 55, loss = 0.01968601
Iteration 56, loss = 0.01925062
Iteration 57, loss = 0.01886085
Iteration 58, loss = 0.01851660
Iteration 59, loss = 0.01819943
Iteration 60, loss = 0.01783785
Iteration 61, loss = 0.01756757
Iteration 62, loss = 0.01729727
Iteration 63, loss = 0.01703385
Iteration 64, loss = 0.01679599
Iteration 65, loss = 0.01675714
Iteration 66, loss = 0.01647794
Iteration 67, loss = 0.01630858
Iteration 68, loss = 0.01603143
Iteration 69, loss = 0.01572082
Iteration 70, loss = 0.01549611
Iteration 71, loss = 0.01525185
Iteration 72, loss = 0.01504541
Iteration 73, loss = 0.01486257
Iteration 74, loss = 0.01466270
Iteration 75, loss = 0.01456407
Iteration 76, loss = 0.01435339
Iteration 77, loss = 0.01429334
Iteration 78, loss = 0.01413414
Iteration 79, loss = 0.01401132
Iteration 80, loss = 0.01393273
Iteration 81, loss = 0.01381966
Iteration 82, loss = 0.01379383
Iteration 83, loss = 0.01353510
Iteration 84, loss = 0.01351920
Iteration 85, loss = 0.01357561
Iteration 86, loss = 0.01339456
Iteration 87, loss = 0.01327296
Iteration 88, loss = 0.01310801
Iteration 89, loss = 0.01303485
Iteration 90, loss = 0.01289108
Iteration 91, loss = 0.01277416
Iteration 92, loss = 0.01273147
Iteration 93, loss = 0.01263741
Iteration 94, loss = 0.01252105
Iteration 95, loss = 0.01243546
Iteration 96, loss = 0.01234560
Iteration 97, loss = 0.01230546
Iteration 98, loss = 0.01225119
Iteration 99, loss = 0.01215831
Iteration 100, loss = 0.01207467
Iteration 101, loss = 0.01202584
Iteration 102, loss = 0.01202085
Iteration 103, loss = 0.01197398
Iteration 104, loss = 0.01190926
Iteration 105, loss = 0.01185351
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71566510
Iteration 2, loss = 0.68867023
Iteration 3, loss = 0.66332617
Iteration 4, loss = 0.63575515
Iteration 5, loss = 0.60546457
Iteration 6, loss = 0.57207000
Iteration 7, loss = 0.53755350
Iteration 8, loss = 0.50129686
Iteration 9, loss = 0.46459635
Iteration 10, loss = 0.42766879
Iteration 11, loss = 0.39169956
Iteration 12, loss = 0.35725127
Iteration 13, loss = 0.32516309
Iteration 14, loss = 0.29526830
Iteration 15, loss = 0.26802341
Iteration 16, loss = 0.24331341
Iteration 17, loss = 0.22102888
Iteration 18, loss = 0.20108681
Iteration 19, loss = 0.18327844
Iteration 20, loss = 0.16751594
Iteration 21, loss = 0.15327601
Iteration 22, loss = 0.14083842
Iteration 23, loss = 0.12963460
Iteration 24, loss = 0.11984715
Iteration 25, loss = 0.11098669
Iteration 26, loss = 0.10312695
Iteration 27, loss = 0.09604080
Iteration 28, loss = 0.08973743
Iteration 29, loss = 0.08404605
Iteration 30, loss = 0.07901807
Iteration 31, loss = 0.07438261
Iteration 32, loss = 0.07016470
Iteration 33, loss = 0.06638351
Iteration 34, loss = 0.06291524
Iteration 35, loss = 0.05976009
Iteration 36, loss = 0.05689543
Iteration 37, loss = 0.05423413
Iteration 38, loss = 0.05179356
Iteration 39, loss = 0.04953930
Iteration 40, loss = 0.04752574
Iteration 41, loss = 0.04561165
Iteration 42, loss = 0.04382161
Iteration 43, loss = 0.04218208
Iteration 44, loss = 0.04063605
Iteration 45, loss = 0.03923215
Iteration 46, loss = 0.03789269
Iteration 47, loss = 0.03665390
Iteration 48, loss = 0.03551803
Iteration 49, loss = 0.03448533
Iteration 50, loss = 0.03344420
Iteration 51, loss = 0.03246029
Iteration 52, loss = 0.03150316
Iteration 53, loss = 0.03072449
Iteration 54, loss = 0.02992102
Iteration 55, loss = 0.02916078
Iteration 56, loss = 0.02850598
Iteration 57, loss = 0.02781226
Iteration 58, loss = 0.02719558
Iteration 59, loss = 0.02653650
Iteration 60, loss = 0.02594420
Iteration 61, loss = 0.02549064
Iteration 62, loss = 0.02501310
Iteration 63, loss = 0.02439821
Iteration 64, loss = 0.02396775
Iteration 65, loss = 0.02354744
Iteration 66, loss = 0.02308545
Iteration 67, loss = 0.02269788
Iteration 68, loss = 0.02232822
Iteration 69, loss = 0.02194497
Iteration 70, loss = 0.02158276
Iteration 71, loss = 0.02122017
Iteration 72, loss = 0.02095460
Iteration 73, loss = 0.02065777
Iteration 74, loss = 0.02029509
Iteration 75, loss = 0.02001818
Iteration 76, loss = 0.01979609
Iteration 77, loss = 0.01949828
Iteration 78, loss = 0.01926702
Iteration 79, loss = 0.01902432
Iteration 80, loss = 0.01873970
Iteration 81, loss = 0.01856132
Iteration 82, loss = 0.01832174
Iteration 83, loss = 0.01815925
Iteration 84, loss = 0.01795272
Iteration 85, loss = 0.01776214
Iteration 86, loss = 0.01760519
Iteration 87, loss = 0.01741646
Iteration 88, loss = 0.01728532
Iteration 89, loss = 0.01708492
Iteration 90, loss = 0.01686687
Iteration 91, loss = 0.01674056
Iteration 92, loss = 0.01661267
Iteration 93, loss = 0.01648761
Iteration 94, loss = 0.01636072
Iteration 95, loss = 0.01624020
Iteration 96, loss = 0.01602033
Iteration 97, loss = 0.01596359
Iteration 98, loss = 0.01582627
Iteration 99, loss = 0.01565126
Iteration 100, loss = 0.01557962
Iteration 101, loss = 0.01543434
Iteration 102, loss = 0.01538608
Iteration 103, loss = 0.01526195
Iteration 104, loss = 0.01513388
Iteration 105, loss = 0.01504008
Iteration 106, loss = 0.01496216
Iteration 107, loss = 0.01485792
Iteration 108, loss = 0.01480375
Iteration 109, loss = 0.01465928
Iteration 110, loss = 0.01455932
Iteration 111, loss = 0.01456040
Iteration 112, loss = 0.01445347
Iteration 113, loss = 0.01433109
Iteration 114, loss = 0.01430489
Iteration 115, loss = 0.01421451
Iteration 116, loss = 0.01413762
Iteration 117, loss = 0.01404224
Iteration 118, loss = 0.01399367
Iteration 119, loss = 0.01390990
Iteration 120, loss = 0.01385042
Iteration 121, loss = 0.01383991
Iteration 122, loss = 0.01377356
Iteration 123, loss = 0.01369897
Iteration 124, loss = 0.01362637
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71052786
Iteration 2, loss = 0.67457397
Iteration 3, loss = 0.62982030
Iteration 4, loss = 0.57579930
Iteration 5, loss = 0.51455716
Iteration 6, loss = 0.45116810
Iteration 7, loss = 0.38930422
Iteration 8, loss = 0.33267991
Iteration 9, loss = 0.28286061
Iteration 10, loss = 0.24020252
Iteration 11, loss = 0.20446565
Iteration 12, loss = 0.17487382
Iteration 13, loss = 0.15056379
Iteration 14, loss = 0.13060509
Iteration 15, loss = 0.11424318
Iteration 16, loss = 0.10088007
Iteration 17, loss = 0.08972277
Iteration 18, loss = 0.08037852
Iteration 19, loss = 0.07260865
Iteration 20, loss = 0.06605036
Iteration 21, loss = 0.06030954
Iteration 22, loss = 0.05555971
Iteration 23, loss = 0.05129776
Iteration 24, loss = 0.04779236
Iteration 25, loss = 0.04443933
Iteration 26, loss = 0.04174219
Iteration 27, loss = 0.03918063
Iteration 28, loss = 0.03690217
Iteration 29, loss = 0.03497643
Iteration 30, loss = 0.03332812
Iteration 31, loss = 0.03176074
Iteration 32, loss = 0.03025169
Iteration 33, loss = 0.02897470
Iteration 34, loss = 0.02783906
Iteration 35, loss = 0.02677757
Iteration 36, loss = 0.02586498
Iteration 37, loss = 0.02496907
Iteration 38, loss = 0.02405793
Iteration 39, loss = 0.02341518
Iteration 40, loss = 0.02276079
Iteration 41, loss = 0.02202192
Iteration 42, loss = 0.02179385
Iteration 43, loss = 0.02105073
Iteration 44, loss = 0.02048926
Iteration 45, loss = 0.02002296
Iteration 46, loss = 0.01950102
Iteration 47, loss = 0.01913286
Iteration 48, loss = 0.01872790
Iteration 49, loss = 0.01850550
Iteration 50, loss = 0.01799894
Iteration 51, loss = 0.01768928
Iteration 52, loss = 0.01757002
Iteration 53, loss = 0.01713055
Iteration 54, loss = 0.01686445
Iteration 55, loss = 0.01664207
Iteration 56, loss = 0.01643246
Iteration 57, loss = 0.01618388
Iteration 58, loss = 0.01604132
Iteration 59, loss = 0.01579111
Iteration 60, loss = 0.01565592
Iteration 61, loss = 0.01537012
Iteration 62, loss = 0.01523497
Iteration 63, loss = 0.01513044
Iteration 64, loss = 0.01490677
Iteration 65, loss = 0.01487071
Iteration 66, loss = 0.01462700
Iteration 67, loss = 0.01446618
Iteration 68, loss = 0.01437006
Iteration 69, loss = 0.01429853
Iteration 70, loss = 0.01413971
Iteration 71, loss = 0.01400352
Iteration 72, loss = 0.01399739
Iteration 73, loss = 0.01381282
Iteration 74, loss = 0.01367393
Iteration 75, loss = 0.01365953
Iteration 76, loss = 0.01354942
Iteration 77, loss = 0.01341521
Iteration 78, loss = 0.01342273
Iteration 79, loss = 0.01332166
Iteration 80, loss = 0.01329539
Iteration 81, loss = 0.01324270
Iteration 82, loss = 0.01304774
Iteration 83, loss = 0.01305107
Iteration 84, loss = 0.01303909
Iteration 85, loss = 0.01286419
Iteration 86, loss = 0.01286905
Iteration 87, loss = 0.01274011
Iteration 88, loss = 0.01278041
Iteration 89, loss = 0.01279523
Iteration 90, loss = 0.01271363
Iteration 91, loss = 0.01254438
Iteration 92, loss = 0.01242769
Iteration 93, loss = 0.01243839
Iteration 94, loss = 0.01239177
Iteration 95, loss = 0.01241136
Iteration 96, loss = 0.01238131
Iteration 97, loss = 0.01233870
Iteration 98, loss = 0.01221909
Iteration 99, loss = 0.01226645
Iteration 100, loss = 0.01211309
Iteration 101, loss = 0.01218673
Iteration 102, loss = 0.01220094
Iteration 103, loss = 0.01212538
Iteration 104, loss = 0.01201627
Iteration 105, loss = 0.01206473
Iteration 106, loss = 0.01203086
Iteration 107, loss = 0.01190163
Iteration 108, loss = 0.01190860
Iteration 109, loss = 0.01201274
Iteration 110, loss = 0.01182757
Iteration 111, loss = 0.01186374
Iteration 112, loss = 0.01176705
Iteration 113, loss = 0.01177860
Iteration 114, loss = 0.01175481
Iteration 115, loss = 0.01173014
Iteration 116, loss = 0.01170336
Iteration 117, loss = 0.01170764
Iteration 118, loss = 0.01161616
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68663251
Iteration 2, loss = 0.65596615
Iteration 3, loss = 0.61818504
Iteration 4, loss = 0.56960665
Iteration 5, loss = 0.51281038
Iteration 6, loss = 0.45322282
Iteration 7, loss = 0.39399375
Iteration 8, loss = 0.33894957
Iteration 9, loss = 0.28978292
Iteration 10, loss = 0.24710702
Iteration 11, loss = 0.21093670
Iteration 12, loss = 0.18073908
Iteration 13, loss = 0.15581794
Iteration 14, loss = 0.13527564
Iteration 15, loss = 0.11810530
Iteration 16, loss = 0.10407091
Iteration 17, loss = 0.09241128
Iteration 18, loss = 0.08265576
Iteration 19, loss = 0.07444424
Iteration 20, loss = 0.06746169
Iteration 21, loss = 0.06152011
Iteration 22, loss = 0.05630606
Iteration 23, loss = 0.05203501
Iteration 24, loss = 0.04814875
Iteration 25, loss = 0.04479354
Iteration 26, loss = 0.04187144
Iteration 27, loss = 0.03925210
Iteration 28, loss = 0.03695270
Iteration 29, loss = 0.03485957
Iteration 30, loss = 0.03296841
Iteration 31, loss = 0.03131857
Iteration 32, loss = 0.02979601
Iteration 33, loss = 0.02853030
Iteration 34, loss = 0.02727271
Iteration 35, loss = 0.02612489
Iteration 36, loss = 0.02510398
Iteration 37, loss = 0.02413635
Iteration 38, loss = 0.02325701
Iteration 39, loss = 0.02256273
Iteration 40, loss = 0.02176109
Iteration 41, loss = 0.02113323
Iteration 42, loss = 0.02048008
Iteration 43, loss = 0.01990987
Iteration 44, loss = 0.01937802
Iteration 45, loss = 0.01885830
Iteration 46, loss = 0.01846261
Iteration 47, loss = 0.01798788
Iteration 48, loss = 0.01754837
Iteration 49, loss = 0.01719746
Iteration 50, loss = 0.01683317
Iteration 51, loss = 0.01655490
Iteration 52, loss = 0.01618210
Iteration 53, loss = 0.01582158
Iteration 54, loss = 0.01561081
Iteration 55, loss = 0.01536803
Iteration 56, loss = 0.01507198
Iteration 57, loss = 0.01481347
Iteration 58, loss = 0.01462421
Iteration 59, loss = 0.01445062
Iteration 60, loss = 0.01425911
Iteration 61, loss = 0.01403587
Iteration 62, loss = 0.01384364
Iteration 63, loss = 0.01371076
Iteration 64, loss = 0.01351300
Iteration 65, loss = 0.01336559
Iteration 66, loss = 0.01324930
Iteration 67, loss = 0.01308991
Iteration 68, loss = 0.01295842
Iteration 69, loss = 0.01286491
Iteration 70, loss = 0.01270173
Iteration 71, loss = 0.01261738
Iteration 72, loss = 0.01250782
Iteration 73, loss = 0.01240301
Iteration 74, loss = 0.01229251
Iteration 75, loss = 0.01225985
Iteration 76, loss = 0.01205508
Iteration 77, loss = 0.01196090
Iteration 78, loss = 0.01191188
Iteration 79, loss = 0.01179315
Iteration 80, loss = 0.01186109
Iteration 81, loss = 0.01165825
Iteration 82, loss = 0.01153241
Iteration 83, loss = 0.01153838
Iteration 84, loss = 0.01154982
Iteration 85, loss = 0.01134971
Iteration 86, loss = 0.01130745
Iteration 87, loss = 0.01125412
Iteration 88, loss = 0.01113204
Iteration 89, loss = 0.01105945
Iteration 90, loss = 0.01103873
Iteration 91, loss = 0.01096818
Iteration 92, loss = 0.01092764
Iteration 93, loss = 0.01098692
Iteration 94, loss = 0.01083862
Iteration 95, loss = 0.01079949
Iteration 96, loss = 0.01072746
Iteration 97, loss = 0.01067184
Iteration 98, loss = 0.01070001
Iteration 99, loss = 0.01056590
Iteration 100, loss = 0.01061269
Iteration 101, loss = 0.01059399
Iteration 102, loss = 0.01057593
Iteration 103, loss = 0.01050658
Iteration 104, loss = 0.01049302
Iteration 105, loss = 0.01041824
Iteration 106, loss = 0.01038592
Iteration 107, loss = 0.01035928
Iteration 108, loss = 0.01030615
Iteration 109, loss = 0.01033052
Iteration 110, loss = 0.01024201
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69849198
Iteration 2, loss = 0.66950335
Iteration 3, loss = 0.63657966
Iteration 4, loss = 0.59728030
Iteration 5, loss = 0.55249449
Iteration 6, loss = 0.50454359
Iteration 7, loss = 0.45552818
Iteration 8, loss = 0.40713954
Iteration 9, loss = 0.36078044
Iteration 10, loss = 0.31841103
Iteration 11, loss = 0.27997777
Iteration 12, loss = 0.24615597
Iteration 13, loss = 0.21639180
Iteration 14, loss = 0.19084086
Iteration 15, loss = 0.16876655
Iteration 16, loss = 0.14993951
Iteration 17, loss = 0.13403735
Iteration 18, loss = 0.12039692
Iteration 19, loss = 0.10886759
Iteration 20, loss = 0.09882853
Iteration 21, loss = 0.09040713
Iteration 22, loss = 0.08300564
Iteration 23, loss = 0.07655095
Iteration 24, loss = 0.07093304
Iteration 25, loss = 0.06596740
Iteration 26, loss = 0.06157582
Iteration 27, loss = 0.05783053
Iteration 28, loss = 0.05425347
Iteration 29, loss = 0.05121346
Iteration 30, loss = 0.04852504
Iteration 31, loss = 0.04607731
Iteration 32, loss = 0.04378952
Iteration 33, loss = 0.04184211
Iteration 34, loss = 0.04002731
Iteration 35, loss = 0.03834487
Iteration 36, loss = 0.03673543
Iteration 37, loss = 0.03543681
Iteration 38, loss = 0.03413628
Iteration 39, loss = 0.03293394
Iteration 40, loss = 0.03185616
Iteration 41, loss = 0.03079679
Iteration 42, loss = 0.02988861
Iteration 43, loss = 0.02907253
Iteration 44, loss = 0.02838343
Iteration 45, loss = 0.02740396
Iteration 46, loss = 0.02670032
Iteration 47, loss = 0.02605759
Iteration 48, loss = 0.02535621
Iteration 49, loss = 0.02502724
Iteration 50, loss = 0.02452506
Iteration 51, loss = 0.02398087
Iteration 52, loss = 0.02361743
Iteration 53, loss = 0.02321721
Iteration 54, loss = 0.02281946
Iteration 55, loss = 0.02232825
Iteration 56, loss = 0.02209720
Iteration 57, loss = 0.02218230
Iteration 58, loss = 0.02204044
Iteration 59, loss = 0.02146186
Iteration 60, loss = 0.02108129
Iteration 61, loss = 0.02052744
Iteration 62, loss = 0.02022624
Iteration 63, loss = 0.01967189
Iteration 64, loss = 0.01937266
Iteration 65, loss = 0.01915913
Iteration 66, loss = 0.01895071
Iteration 67, loss = 0.01885344
Iteration 68, loss = 0.01860445
Iteration 69, loss = 0.01830149
Iteration 70, loss = 0.01800954
Iteration 71, loss = 0.01786421
Iteration 72, loss = 0.01770755
Iteration 73, loss = 0.01747967
Iteration 74, loss = 0.01734392
Iteration 75, loss = 0.01704486
Iteration 76, loss = 0.01742785
Iteration 77, loss = 0.01725333
Iteration 78, loss = 0.01702227
Iteration 79, loss = 0.01680802
Iteration 80, loss = 0.01661403
Iteration 81, loss = 0.01649337
Iteration 82, loss = 0.01628485
Iteration 83, loss = 0.01614703
Iteration 84, loss = 0.01602753
Iteration 85, loss = 0.01592723
Iteration 86, loss = 0.01592483
Iteration 87, loss = 0.01579727
Iteration 88, loss = 0.01580439
Iteration 89, loss = 0.01565935
Iteration 90, loss = 0.01565712
Iteration 91, loss = 0.01557610
Iteration 92, loss = 0.01535034
Iteration 93, loss = 0.01526666
Iteration 94, loss = 0.01519342
Iteration 95, loss = 0.01502640
Iteration 96, loss = 0.01491540
Iteration 97, loss = 0.01490758
Iteration 98, loss = 0.01479689
Iteration 99, loss = 0.01482148
Iteration 100, loss = 0.01479960
Iteration 101, loss = 0.01467268
Iteration 102, loss = 0.01462185
Iteration 103, loss = 0.01450954
Iteration 104, loss = 0.01489136
Iteration 105, loss = 0.01483708
Iteration 106, loss = 0.01475007
Iteration 107, loss = 0.01456757
Iteration 108, loss = 0.01456225
Iteration 109, loss = 0.01451420
Iteration 110, loss = 0.01438058
Iteration 111, loss = 0.01424523
Iteration 112, loss = 0.01415701
Iteration 113, loss = 0.01421699
Iteration 114, loss = 0.01408184
Iteration 115, loss = 0.01414759
Iteration 116, loss = 0.01403984
Iteration 117, loss = 0.01398344
Iteration 118, loss = 0.01395534
Iteration 119, loss = 0.01399252
Iteration 120, loss = 0.01400806
Iteration 121, loss = 0.01405312
Iteration 122, loss = 0.01398722
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69236735
Iteration 2, loss = 0.66023876
Iteration 3, loss = 0.60594381
Iteration 4, loss = 0.52956251
Iteration 5, loss = 0.44861023
Iteration 6, loss = 0.37096821
Iteration 7, loss = 0.30267261
Iteration 8, loss = 0.24561661
Iteration 9, loss = 0.20013295
Iteration 10, loss = 0.16469111
Iteration 11, loss = 0.13729468
Iteration 12, loss = 0.11623423
Iteration 13, loss = 0.09977474
Iteration 14, loss = 0.08696226
Iteration 15, loss = 0.07660032
Iteration 16, loss = 0.06831068
Iteration 17, loss = 0.06147581
Iteration 18, loss = 0.05580700
Iteration 19, loss = 0.05124658
Iteration 20, loss = 0.04733114
Iteration 21, loss = 0.04387825
Iteration 22, loss = 0.04093826
Iteration 23, loss = 0.03844962
Iteration 24, loss = 0.03635610
Iteration 25, loss = 0.03445053
Iteration 26, loss = 0.03293918
Iteration 27, loss = 0.03142354
Iteration 28, loss = 0.02998786
Iteration 29, loss = 0.02889359
Iteration 30, loss = 0.02784683
Iteration 31, loss = 0.02689708
Iteration 32, loss = 0.02610298
Iteration 33, loss = 0.02546287
Iteration 34, loss = 0.02466816
Iteration 35, loss = 0.02393856
Iteration 36, loss = 0.02342111
Iteration 37, loss = 0.02278309
Iteration 38, loss = 0.02238981
Iteration 39, loss = 0.02188614
Iteration 40, loss = 0.02160819
Iteration 41, loss = 0.02108960
Iteration 42, loss = 0.02076045
Iteration 43, loss = 0.02054772
Iteration 44, loss = 0.02007662
Iteration 45, loss = 0.02002753
Iteration 46, loss = 0.02002528
Iteration 47, loss = 0.01944042
Iteration 48, loss = 0.01916289
Iteration 49, loss = 0.01888184
Iteration 50, loss = 0.01897707
Iteration 51, loss = 0.01860920
Iteration 52, loss = 0.01836054
Iteration 53, loss = 0.01822982
Iteration 54, loss = 0.01826438
Iteration 55, loss = 0.01787736
Iteration 56, loss = 0.01765740
Iteration 57, loss = 0.01770926
Iteration 58, loss = 0.01762335
Iteration 59, loss = 0.01740642
Iteration 60, loss = 0.01732259
Iteration 61, loss = 0.01733478
Iteration 62, loss = 0.01719175
Iteration 63, loss = 0.01700649
Iteration 64, loss = 0.01687289
Iteration 65, loss = 0.01703958
Iteration 66, loss = 0.01692907
Iteration 67, loss = 0.01671502
Iteration 68, loss = 0.01650038
Iteration 69, loss = 0.01663166
Iteration 70, loss = 0.01665890
Iteration 71, loss = 0.01658662
Iteration 72, loss = 0.01631033
Iteration 73, loss = 0.01621558
Iteration 74, loss = 0.01619312
Iteration 75, loss = 0.01612918
Iteration 76, loss = 0.01611488
Iteration 77, loss = 0.01606294
Iteration 78, loss = 0.01632359
Iteration 79, loss = 0.01601803
Iteration 80, loss = 0.01599619
Iteration 81, loss = 0.01577832
Iteration 82, loss = 0.01591907
Iteration 83, loss = 0.01585585
Iteration 84, loss = 0.01570401
Iteration 85, loss = 0.01574508
Iteration 86, loss = 0.01574019
Iteration 87, loss = 0.01577197
Iteration 88, loss = 0.01578544
Iteration 89, loss = 0.01546186
Iteration 90, loss = 0.01557847
Iteration 91, loss = 0.01550118
Iteration 92, loss = 0.01532729
Iteration 93, loss = 0.01535699
Iteration 94, loss = 0.01526335
Iteration 95, loss = 0.01541179
Iteration 96, loss = 0.01532654
Iteration 97, loss = 0.01540427
Iteration 98, loss = 0.01529246
Iteration 99, loss = 0.01518302
Iteration 100, loss = 0.01526774
Iteration 101, loss = 0.01535053
Iteration 102, loss = 0.01534644
Iteration 103, loss = 0.01525887
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67015059
Iteration 2, loss = 0.65313520
Iteration 3, loss = 0.63890906
Iteration 4, loss = 0.62473207
Iteration 5, loss = 0.61007442
Iteration 6, loss = 0.59419279
Iteration 7, loss = 0.57726511
Iteration 8, loss = 0.55902426
Iteration 9, loss = 0.53957916
Iteration 10, loss = 0.51939142
Iteration 11, loss = 0.49814924
Iteration 12, loss = 0.47625924
Iteration 13, loss = 0.45411816
Iteration 14, loss = 0.43180740
Iteration 15, loss = 0.40947568
Iteration 16, loss = 0.38733298
Iteration 17, loss = 0.36555805
Iteration 18, loss = 0.34449444
Iteration 19, loss = 0.32389226
Iteration 20, loss = 0.30427238
Iteration 21, loss = 0.28523005
Iteration 22, loss = 0.26726641
Iteration 23, loss = 0.25025212
Iteration 24, loss = 0.23421457
Iteration 25, loss = 0.21924439
Iteration 26, loss = 0.20518031
Iteration 27, loss = 0.19208399
Iteration 28, loss = 0.17998850
Iteration 29, loss = 0.16856412
Iteration 30, loss = 0.15808982
Iteration 31, loss = 0.14837198
Iteration 32, loss = 0.13936486
Iteration 33, loss = 0.13105177
Iteration 34, loss = 0.12341708
Iteration 35, loss = 0.11633873
Iteration 36, loss = 0.10981076
Iteration 37, loss = 0.10375665
Iteration 38, loss = 0.09816684
Iteration 39, loss = 0.09298787
Iteration 40, loss = 0.08817401
Iteration 41, loss = 0.08376310
Iteration 42, loss = 0.07962505
Iteration 43, loss = 0.07580512
Iteration 44, loss = 0.07223133
Iteration 45, loss = 0.06896970
Iteration 46, loss = 0.06583239
Iteration 47, loss = 0.06297661
Iteration 48, loss = 0.06027304
Iteration 49, loss = 0.05776248
Iteration 50, loss = 0.05540371
Iteration 51, loss = 0.05320104
Iteration 52, loss = 0.05115886
Iteration 53, loss = 0.04922499
Iteration 54, loss = 0.04736889
Iteration 55, loss = 0.04566232
Iteration 56, loss = 0.04404658
Iteration 57, loss = 0.04250339
Iteration 58, loss = 0.04105715
Iteration 59, loss = 0.03968635
Iteration 60, loss = 0.03838653
Iteration 61, loss = 0.03715530
Iteration 62, loss = 0.03600474
Iteration 63, loss = 0.03491246
Iteration 64, loss = 0.03385146
Iteration 65, loss = 0.03286709
Iteration 66, loss = 0.03191301
Iteration 67, loss = 0.03101267
Iteration 68, loss = 0.03014787
Iteration 69, loss = 0.02933850
Iteration 70, loss = 0.02854179
Iteration 71, loss = 0.02781481
Iteration 72, loss = 0.02709488
Iteration 73, loss = 0.02642020
Iteration 74, loss = 0.02577434
Iteration 75, loss = 0.02515242
Iteration 76, loss = 0.02456846
Iteration 77, loss = 0.02398405
Iteration 78, loss = 0.02344533
Iteration 79, loss = 0.02291002
Iteration 80, loss = 0.02242113
Iteration 81, loss = 0.02190335
Iteration 82, loss = 0.02143778
Iteration 83, loss = 0.02098835
Iteration 84, loss = 0.02057105
Iteration 85, loss = 0.02014709
Iteration 86, loss = 0.01974080
Iteration 87, loss = 0.01935330
Iteration 88, loss = 0.01899988
Iteration 89, loss = 0.01864181
Iteration 90, loss = 0.01828433
Iteration 91, loss = 0.01795861
Iteration 92, loss = 0.01764024
Iteration 93, loss = 0.01731593
Iteration 94, loss = 0.01703417
Iteration 95, loss = 0.01674046
Iteration 96, loss = 0.01645036
Iteration 97, loss = 0.01618940
Iteration 98, loss = 0.01590261
Iteration 99, loss = 0.01565011
Iteration 100, loss = 0.01543506
Iteration 101, loss = 0.01516019
Iteration 102, loss = 0.01492819
Iteration 103, loss = 0.01469732
Iteration 104, loss = 0.01450811
Iteration 105, loss = 0.01427129
Iteration 106, loss = 0.01406724
Iteration 107, loss = 0.01387794
Iteration 108, loss = 0.01367124
Iteration 109, loss = 0.01349003
Iteration 110, loss = 0.01330210
Iteration 111, loss = 0.01312211
Iteration 112, loss = 0.01294650
Iteration 113, loss = 0.01277776
Iteration 114, loss = 0.01262435
Iteration 115, loss = 0.01244509
Iteration 116, loss = 0.01228734
Iteration 117, loss = 0.01213454
Iteration 118, loss = 0.01199332
Iteration 119, loss = 0.01184113
Iteration 120, loss = 0.01170203
Iteration 121, loss = 0.01157631
Iteration 122, loss = 0.01143369
Iteration 123, loss = 0.01131932
Iteration 124, loss = 0.01117856
Iteration 125, loss = 0.01105668
Iteration 126, loss = 0.01093931
Iteration 127, loss = 0.01082546
Iteration 128, loss = 0.01071851
Iteration 129, loss = 0.01061063
Iteration 130, loss = 0.01053106
Iteration 131, loss = 0.01039878
Iteration 132, loss = 0.01028767
Iteration 133, loss = 0.01018595
Iteration 134, loss = 0.01010154
Iteration 135, loss = 0.00999925
Iteration 136, loss = 0.00989136
Iteration 137, loss = 0.00980178
Iteration 138, loss = 0.00971222
Iteration 139, loss = 0.00963539
Iteration 140, loss = 0.00953924
Iteration 141, loss = 0.00945528
Iteration 142, loss = 0.00937223
Iteration 143, loss = 0.00930283
Iteration 144, loss = 0.00922241
Iteration 145, loss = 0.00913554
Iteration 146, loss = 0.00906890
Iteration 147, loss = 0.00898560
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67619261
Iteration 2, loss = 0.65863880
Iteration 3, loss = 0.64131544
Iteration 4, loss = 0.62085280
Iteration 5, loss = 0.59641387
Iteration 6, loss = 0.56877460
Iteration 7, loss = 0.53834729
Iteration 8, loss = 0.50538512
Iteration 9, loss = 0.47172896
Iteration 10, loss = 0.43758169
Iteration 11, loss = 0.40410831
Iteration 12, loss = 0.37167959
Iteration 13, loss = 0.34084253
Iteration 14, loss = 0.31160764
Iteration 15, loss = 0.28420556
Iteration 16, loss = 0.25906937
Iteration 17, loss = 0.23607026
Iteration 18, loss = 0.21512985
Iteration 19, loss = 0.19610632
Iteration 20, loss = 0.17907679
Iteration 21, loss = 0.16385377
Iteration 22, loss = 0.15013255
Iteration 23, loss = 0.13797109
Iteration 24, loss = 0.12708147
Iteration 25, loss = 0.11734024
Iteration 26, loss = 0.10873957
Iteration 27, loss = 0.10099507
Iteration 28, loss = 0.09407425
Iteration 29, loss = 0.08787508
Iteration 30, loss = 0.08223568
Iteration 31, loss = 0.07720060
Iteration 32, loss = 0.07268418
Iteration 33, loss = 0.06855115
Iteration 34, loss = 0.06482542
Iteration 35, loss = 0.06141704
Iteration 36, loss = 0.05832421
Iteration 37, loss = 0.05546771
Iteration 38, loss = 0.05288580
Iteration 39, loss = 0.05051073
Iteration 40, loss = 0.04827957
Iteration 41, loss = 0.04625696
Iteration 42, loss = 0.04437185
Iteration 43, loss = 0.04266578
Iteration 44, loss = 0.04105767
Iteration 45, loss = 0.03957130
Iteration 46, loss = 0.03812811
Iteration 47, loss = 0.03687376
Iteration 48, loss = 0.03562467
Iteration 49, loss = 0.03451274
Iteration 50, loss = 0.03348717
Iteration 51, loss = 0.03247813
Iteration 52, loss = 0.03147644
Iteration 53, loss = 0.03059766
Iteration 54, loss = 0.02976127
Iteration 55, loss = 0.02898217
Iteration 56, loss = 0.02825181
Iteration 57, loss = 0.02754389
Iteration 58, loss = 0.02693296
Iteration 59, loss = 0.02624658
Iteration 60, loss = 0.02567793
Iteration 61, loss = 0.02512605
Iteration 62, loss = 0.02457596
Iteration 63, loss = 0.02406747
Iteration 64, loss = 0.02360211
Iteration 65, loss = 0.02313124
Iteration 66, loss = 0.02266702
Iteration 67, loss = 0.02229130
Iteration 68, loss = 0.02190956
Iteration 69, loss = 0.02152713
Iteration 70, loss = 0.02114102
Iteration 71, loss = 0.02080622
Iteration 72, loss = 0.02042366
Iteration 73, loss = 0.02013026
Iteration 74, loss = 0.01982542
Iteration 75, loss = 0.01957151
Iteration 76, loss = 0.01925821
Iteration 77, loss = 0.01898372
Iteration 78, loss = 0.01876583
Iteration 79, loss = 0.01850253
Iteration 80, loss = 0.01824728
Iteration 81, loss = 0.01801596
Iteration 82, loss = 0.01787956
Iteration 83, loss = 0.01758576
Iteration 84, loss = 0.01739975
Iteration 85, loss = 0.01721509
Iteration 86, loss = 0.01700099
Iteration 87, loss = 0.01685293
Iteration 88, loss = 0.01665863
Iteration 89, loss = 0.01650270
Iteration 90, loss = 0.01629842
Iteration 91, loss = 0.01616232
Iteration 92, loss = 0.01604384
Iteration 93, loss = 0.01586013
Iteration 94, loss = 0.01574148
Iteration 95, loss = 0.01562432
Iteration 96, loss = 0.01545104
Iteration 97, loss = 0.01534074
Iteration 98, loss = 0.01520287
Iteration 99, loss = 0.01506626
Iteration 100, loss = 0.01499056
Iteration 101, loss = 0.01487097
Iteration 102, loss = 0.01473434
Iteration 103, loss = 0.01466066
Iteration 104, loss = 0.01450298
Iteration 105, loss = 0.01443984
Iteration 106, loss = 0.01435429
Iteration 107, loss = 0.01424126
Iteration 108, loss = 0.01421010
Iteration 109, loss = 0.01407101
Iteration 110, loss = 0.01400246
Iteration 111, loss = 0.01391243
Iteration 112, loss = 0.01381817
Iteration 113, loss = 0.01372466
Iteration 114, loss = 0.01368231
Iteration 115, loss = 0.01361302
Iteration 116, loss = 0.01350157
Iteration 117, loss = 0.01347754
Iteration 118, loss = 0.01335231
Iteration 119, loss = 0.01331455
Iteration 120, loss = 0.01322677
Iteration 121, loss = 0.01319031
Iteration 122, loss = 0.01310894
Iteration 123, loss = 0.01304951
Iteration 124, loss = 0.01301536
Iteration 125, loss = 0.01293856
Iteration 126, loss = 0.01288767
Iteration 127, loss = 0.01284819
Iteration 128, loss = 0.01279358
Iteration 129, loss = 0.01272103
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67932924
Iteration 2, loss = 0.63645827
Iteration 3, loss = 0.56391181
Iteration 4, loss = 0.47242234
Iteration 5, loss = 0.38383636
Iteration 6, loss = 0.30726616
Iteration 7, loss = 0.24517843
Iteration 8, loss = 0.19648504
Iteration 9, loss = 0.15965597
Iteration 10, loss = 0.13198812
Iteration 11, loss = 0.11071008
Iteration 12, loss = 0.09486274
Iteration 13, loss = 0.08229412
Iteration 14, loss = 0.07255634
Iteration 15, loss = 0.06457692
Iteration 16, loss = 0.05816622
Iteration 17, loss = 0.05304342
Iteration 18, loss = 0.04858555
Iteration 19, loss = 0.04496199
Iteration 20, loss = 0.04192825
Iteration 21, loss = 0.03930227
Iteration 22, loss = 0.03701394
Iteration 23, loss = 0.03502870
Iteration 24, loss = 0.03347854
Iteration 25, loss = 0.03196373
Iteration 26, loss = 0.03070854
Iteration 27, loss = 0.02949437
Iteration 28, loss = 0.02845304
Iteration 29, loss = 0.02740785
Iteration 30, loss = 0.02675440
Iteration 31, loss = 0.02590994
Iteration 32, loss = 0.02540940
Iteration 33, loss = 0.02461031
Iteration 34, loss = 0.02409797
Iteration 35, loss = 0.02350544
Iteration 36, loss = 0.02319245
Iteration 37, loss = 0.02264939
Iteration 38, loss = 0.02248422
Iteration 39, loss = 0.02210306
Iteration 40, loss = 0.02162831
Iteration 41, loss = 0.02137852
Iteration 42, loss = 0.02113752
Iteration 43, loss = 0.02080657
Iteration 44, loss = 0.02067873
Iteration 45, loss = 0.02053326
Iteration 46, loss = 0.02015124
Iteration 47, loss = 0.02002510
Iteration 48, loss = 0.01973645
Iteration 49, loss = 0.01966578
Iteration 50, loss = 0.01956881
Iteration 51, loss = 0.01934063
Iteration 52, loss = 0.01917268
Iteration 53, loss = 0.01917077
Iteration 54, loss = 0.01890716
Iteration 55, loss = 0.01892416
Iteration 56, loss = 0.01876766
Iteration 57, loss = 0.01892471
Iteration 58, loss = 0.01855735
Iteration 59, loss = 0.01849577
Iteration 60, loss = 0.01833612
Iteration 61, loss = 0.01856077
Iteration 62, loss = 0.01822762
Iteration 63, loss = 0.01837200
Iteration 64, loss = 0.01818457
Iteration 65, loss = 0.01810799
Iteration 66, loss = 0.01803266
Iteration 67, loss = 0.01781529
Iteration 68, loss = 0.01786503
Iteration 69, loss = 0.01788093
Iteration 70, loss = 0.01772276
Iteration 71, loss = 0.01773047
Iteration 72, loss = 0.01760853
Iteration 73, loss = 0.01753257
Iteration 74, loss = 0.01736841
Iteration 75, loss = 0.01747735
Iteration 76, loss = 0.01758642
Iteration 77, loss = 0.01728415
Iteration 78, loss = 0.01734221
Iteration 79, loss = 0.01725438
Iteration 80, loss = 0.01718347
Iteration 81, loss = 0.01713933
Iteration 82, loss = 0.01719524
Iteration 83, loss = 0.01716713
Iteration 84, loss = 0.01706626
Iteration 85, loss = 0.01710978
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68629737
Iteration 2, loss = 0.63511808
Iteration 3, loss = 0.56086043
Iteration 4, loss = 0.46819610
Iteration 5, loss = 0.37920944
Iteration 6, loss = 0.30195930
Iteration 7, loss = 0.23902780
Iteration 8, loss = 0.19090389
Iteration 9, loss = 0.15492739
Iteration 10, loss = 0.12788271
Iteration 11, loss = 0.10782571
Iteration 12, loss = 0.09262019
Iteration 13, loss = 0.08098377
Iteration 14, loss = 0.07181910
Iteration 15, loss = 0.06456538
Iteration 16, loss = 0.05870237
Iteration 17, loss = 0.05379752
Iteration 18, loss = 0.04978975
Iteration 19, loss = 0.04656421
Iteration 20, loss = 0.04367681
Iteration 21, loss = 0.04126905
Iteration 22, loss = 0.03951363
Iteration 23, loss = 0.03775497
Iteration 24, loss = 0.03587770
Iteration 25, loss = 0.03482435
Iteration 26, loss = 0.03352592
Iteration 27, loss = 0.03250550
Iteration 28, loss = 0.03176615
Iteration 29, loss = 0.03081574
Iteration 30, loss = 0.03008068
Iteration 31, loss = 0.02943288
Iteration 32, loss = 0.02874684
Iteration 33, loss = 0.02830966
Iteration 34, loss = 0.02787885
Iteration 35, loss = 0.02730611
Iteration 36, loss = 0.02681056
Iteration 37, loss = 0.02629342
Iteration 38, loss = 0.02611257
Iteration 39, loss = 0.02590571
Iteration 40, loss = 0.02543817
Iteration 41, loss = 0.02520925
Iteration 42, loss = 0.02513310
Iteration 43, loss = 0.02483626
Iteration 44, loss = 0.02457208
Iteration 45, loss = 0.02444089
Iteration 46, loss = 0.02436511
Iteration 47, loss = 0.02401426
Iteration 48, loss = 0.02401844
Iteration 49, loss = 0.02372840
Iteration 50, loss = 0.02367822
Iteration 51, loss = 0.02336352
Iteration 52, loss = 0.02349756
Iteration 53, loss = 0.02329447
Iteration 54, loss = 0.02300215
Iteration 55, loss = 0.02322652
Iteration 56, loss = 0.02287281
Iteration 57, loss = 0.02277814
Iteration 58, loss = 0.02259718
Iteration 59, loss = 0.02248400
Iteration 60, loss = 0.02260869
Iteration 61, loss = 0.02241047
Iteration 62, loss = 0.02268206
Iteration 63, loss = 0.02235564
Iteration 64, loss = 0.02224207
Iteration 65, loss = 0.02211462
Iteration 66, loss = 0.02227810
Iteration 67, loss = 0.02234250
Iteration 68, loss = 0.02204330
Iteration 69, loss = 0.02190816
Iteration 70, loss = 0.02189313
Iteration 71, loss = 0.02212531
Iteration 72, loss = 0.02189474
Iteration 73, loss = 0.02189002
Iteration 74, loss = 0.02193497
Iteration 75, loss = 0.02189942
Iteration 76, loss = 0.02185856
Iteration 77, loss = 0.02155448
Iteration 78, loss = 0.02155261
Iteration 79, loss = 0.02158965
Iteration 80, loss = 0.02181288
Iteration 81, loss = 0.02147603
Iteration 82, loss = 0.02159330
Iteration 83, loss = 0.02150660
Iteration 84, loss = 0.02145051
Iteration 85, loss = 0.02138248
Iteration 86, loss = 0.02115609
Iteration 87, loss = 0.02135163
Iteration 88, loss = 0.02132294
Iteration 89, loss = 0.02150960
Iteration 90, loss = 0.02145576
Iteration 91, loss = 0.02137767
Iteration 92, loss = 0.02121283
Iteration 93, loss = 0.02121973
Iteration 94, loss = 0.02100546
Iteration 95, loss = 0.02128990
Iteration 96, loss = 0.02138136
Iteration 97, loss = 0.02126920
Iteration 98, loss = 0.02104026
Iteration 99, loss = 0.02109037
Iteration 100, loss = 0.02104216
Iteration 101, loss = 0.02124290
Iteration 102, loss = 0.02126221
Iteration 103, loss = 0.02083534
Iteration 104, loss = 0.02137135
Iteration 105, loss = 0.02091990
Iteration 106, loss = 0.02116278
Iteration 107, loss = 0.02111759
Iteration 108, loss = 0.02111364
Iteration 109, loss = 0.02080560
Iteration 110, loss = 0.02099136
Iteration 111, loss = 0.02101762
Iteration 112, loss = 0.02100464
Iteration 113, loss = 0.02071500
Iteration 114, loss = 0.02094267
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67477147
Iteration 2, loss = 0.60946666
Iteration 3, loss = 0.51723609
Iteration 4, loss = 0.41975452
Iteration 5, loss = 0.33215469
Iteration 6, loss = 0.26090607
Iteration 7, loss = 0.20634578
Iteration 8, loss = 0.16501250
Iteration 9, loss = 0.13469635
Iteration 10, loss = 0.11244322
Iteration 11, loss = 0.09594967
Iteration 12, loss = 0.08349705
Iteration 13, loss = 0.07363263
Iteration 14, loss = 0.06637659
Iteration 15, loss = 0.06035776
Iteration 16, loss = 0.05517998
Iteration 17, loss = 0.05142420
Iteration 18, loss = 0.04803468
Iteration 19, loss = 0.04502766
Iteration 20, loss = 0.04294539
Iteration 21, loss = 0.04076023
Iteration 22, loss = 0.03899865
Iteration 23, loss = 0.03722928
Iteration 24, loss = 0.03622446
Iteration 25, loss = 0.03500513
Iteration 26, loss = 0.03409804
Iteration 27, loss = 0.03314404
Iteration 28, loss = 0.03212738
Iteration 29, loss = 0.03167827
Iteration 30, loss = 0.03103676
Iteration 31, loss = 0.03025178
Iteration 32, loss = 0.02986851
Iteration 33, loss = 0.03001321
Iteration 34, loss = 0.02975904
Iteration 35, loss = 0.02917897
Iteration 36, loss = 0.03023070
Iteration 37, loss = 0.03002015
Iteration 38, loss = 0.02930310
Iteration 39, loss = 0.02881729
Iteration 40, loss = 0.02886129
Iteration 41, loss = 0.02899642
Iteration 42, loss = 0.02876532
Iteration 43, loss = 0.02803462
Iteration 44, loss = 0.02800780
Iteration 45, loss = 0.02771114
Iteration 46, loss = 0.02736292
Iteration 47, loss = 0.02703212
Iteration 48, loss = 0.02681704
Iteration 49, loss = 0.02646778
Iteration 50, loss = 0.02656806
Iteration 51, loss = 0.02629729
Iteration 52, loss = 0.02616129
Iteration 53, loss = 0.02594208
Iteration 54, loss = 0.02573091
Iteration 55, loss = 0.02569136
Iteration 56, loss = 0.02753552
Iteration 57, loss = 0.03015969
Iteration 58, loss = 0.02939260
Iteration 59, loss = 0.02845111
Iteration 60, loss = 0.02779687
Iteration 61, loss = 0.02711150
Iteration 62, loss = 0.02659089
Iteration 63, loss = 0.02618155
Iteration 64, loss = 0.02606371
Iteration 65, loss = 0.02549189
Iteration 66, loss = 0.02539213
Iteration 67, loss = 0.02503844
Iteration 68, loss = 0.02494252
Iteration 69, loss = 0.02459152
Iteration 70, loss = 0.02443508
Iteration 71, loss = 0.02443618
Iteration 72, loss = 0.02443095
Iteration 73, loss = 0.02407915
Iteration 74, loss = 0.02408847
Iteration 75, loss = 0.02403613
Iteration 76, loss = 0.02378252
Iteration 77, loss = 0.02400038
Iteration 78, loss = 0.02374028
Iteration 79, loss = 0.02366342
Iteration 80, loss = 0.02418500
Iteration 81, loss = 0.02755298
Iteration 82, loss = 0.02785339
Iteration 83, loss = 0.02701352
Iteration 84, loss = 0.02633510
Iteration 85, loss = 0.02585196
Iteration 86, loss = 0.02536839
Iteration 87, loss = 0.02597517
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71150717
Iteration 2, loss = 0.69375328
Iteration 3, loss = 0.67640258
Iteration 4, loss = 0.65592951
Iteration 5, loss = 0.63259361
Iteration 6, loss = 0.60529254
Iteration 7, loss = 0.57442184
Iteration 8, loss = 0.54067685
Iteration 9, loss = 0.50447255
Iteration 10, loss = 0.46710036
Iteration 11, loss = 0.42963146
Iteration 12, loss = 0.39313853
Iteration 13, loss = 0.35843172
Iteration 14, loss = 0.32585044
Iteration 15, loss = 0.29557788
Iteration 16, loss = 0.26795380
Iteration 17, loss = 0.24266300
Iteration 18, loss = 0.21996479
Iteration 19, loss = 0.19947569
Iteration 20, loss = 0.18123097
Iteration 21, loss = 0.16492888
Iteration 22, loss = 0.15055712
Iteration 23, loss = 0.13768902
Iteration 24, loss = 0.12636686
Iteration 25, loss = 0.11622451
Iteration 26, loss = 0.10726549
Iteration 27, loss = 0.09921326
Iteration 28, loss = 0.09214780
Iteration 29, loss = 0.08575680
Iteration 30, loss = 0.08007626
Iteration 31, loss = 0.07498301
Iteration 32, loss = 0.07035166
Iteration 33, loss = 0.06622041
Iteration 34, loss = 0.06245614
Iteration 35, loss = 0.05900454
Iteration 36, loss = 0.05592311
Iteration 37, loss = 0.05313790
Iteration 38, loss = 0.05056281
Iteration 39, loss = 0.04819858
Iteration 40, loss = 0.04600495
Iteration 41, loss = 0.04397027
Iteration 42, loss = 0.04214726
Iteration 43, loss = 0.04045784
Iteration 44, loss = 0.03886598
Iteration 45, loss = 0.03737761
Iteration 46, loss = 0.03603773
Iteration 47, loss = 0.03475745
Iteration 48, loss = 0.03359150
Iteration 49, loss = 0.03250935
Iteration 50, loss = 0.03151864
Iteration 51, loss = 0.03048798
Iteration 52, loss = 0.02959932
Iteration 53, loss = 0.02875372
Iteration 54, loss = 0.02794249
Iteration 55, loss = 0.02719620
Iteration 56, loss = 0.02649260
Iteration 57, loss = 0.02581658
Iteration 58, loss = 0.02522374
Iteration 59, loss = 0.02463277
Iteration 60, loss = 0.02405262
Iteration 61, loss = 0.02348409
Iteration 62, loss = 0.02302509
Iteration 63, loss = 0.02250812
Iteration 64, loss = 0.02207066
Iteration 65, loss = 0.02165460
Iteration 66, loss = 0.02122562
Iteration 67, loss = 0.02085807
Iteration 68, loss = 0.02052290
Iteration 69, loss = 0.02008972
Iteration 70, loss = 0.01975956
Iteration 71, loss = 0.01944014
Iteration 72, loss = 0.01908069
Iteration 73, loss = 0.01880423
Iteration 74, loss = 0.01849500
Iteration 75, loss = 0.01824251
Iteration 76, loss = 0.01795271
Iteration 77, loss = 0.01771668
Iteration 78, loss = 0.01748708
Iteration 79, loss = 0.01725134
Iteration 80, loss = 0.01705996
Iteration 81, loss = 0.01684766
Iteration 82, loss = 0.01658257
Iteration 83, loss = 0.01640680
Iteration 84, loss = 0.01622326
Iteration 85, loss = 0.01608673
Iteration 86, loss = 0.01585021
Iteration 87, loss = 0.01568294
Iteration 88, loss = 0.01552402
Iteration 89, loss = 0.01537194
Iteration 90, loss = 0.01525585
Iteration 91, loss = 0.01505165
Iteration 92, loss = 0.01496966
Iteration 93, loss = 0.01475833
Iteration 94, loss = 0.01466313
Iteration 95, loss = 0.01453974
Iteration 96, loss = 0.01441035
Iteration 97, loss = 0.01428369
Iteration 98, loss = 0.01412882
Iteration 99, loss = 0.01402201
Iteration 100, loss = 0.01390090
Iteration 101, loss = 0.01381681
Iteration 102, loss = 0.01368382
Iteration 103, loss = 0.01360494
Iteration 104, loss = 0.01359285
Iteration 105, loss = 0.01343933
Iteration 106, loss = 0.01331485
Iteration 107, loss = 0.01324919
Iteration 108, loss = 0.01316044
Iteration 109, loss = 0.01305246
Iteration 110, loss = 0.01298565
Iteration 111, loss = 0.01286267
Iteration 112, loss = 0.01284493
Iteration 113, loss = 0.01276932
Iteration 114, loss = 0.01267097
Iteration 115, loss = 0.01258769
Iteration 116, loss = 0.01254729
Iteration 117, loss = 0.01247089
Iteration 118, loss = 0.01239348
Iteration 119, loss = 0.01235406
Iteration 120, loss = 0.01226099
Iteration 121, loss = 0.01223659
Iteration 122, loss = 0.01216484
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68264073
Iteration 2, loss = 0.64144535
Iteration 3, loss = 0.58906947
Iteration 4, loss = 0.52468294
Iteration 5, loss = 0.45498388
Iteration 6, loss = 0.38661821
Iteration 7, loss = 0.32420959
Iteration 8, loss = 0.26999254
Iteration 9, loss = 0.22469053
Iteration 10, loss = 0.18791511
Iteration 11, loss = 0.15824529
Iteration 12, loss = 0.13469105
Iteration 13, loss = 0.11582240
Iteration 14, loss = 0.10076659
Iteration 15, loss = 0.08869069
Iteration 16, loss = 0.07868351
Iteration 17, loss = 0.07048465
Iteration 18, loss = 0.06376024
Iteration 19, loss = 0.05815694
Iteration 20, loss = 0.05325546
Iteration 21, loss = 0.04918754
Iteration 22, loss = 0.04554416
Iteration 23, loss = 0.04265448
Iteration 24, loss = 0.03998926
Iteration 25, loss = 0.03765531
Iteration 26, loss = 0.03554990
Iteration 27, loss = 0.03375916
Iteration 28, loss = 0.03217838
Iteration 29, loss = 0.03103675
Iteration 30, loss = 0.02947829
Iteration 31, loss = 0.02840144
Iteration 32, loss = 0.02741255
Iteration 33, loss = 0.02647363
Iteration 34, loss = 0.02557259
Iteration 35, loss = 0.02485095
Iteration 36, loss = 0.02402234
Iteration 37, loss = 0.02345707
Iteration 38, loss = 0.02283561
Iteration 39, loss = 0.02225247
Iteration 40, loss = 0.02172632
Iteration 41, loss = 0.02133107
Iteration 42, loss = 0.02095053
Iteration 43, loss = 0.02041677
Iteration 44, loss = 0.02004521
Iteration 45, loss = 0.01966888
Iteration 46, loss = 0.01945691
Iteration 47, loss = 0.01911569
Iteration 48, loss = 0.01897073
Iteration 49, loss = 0.01849768
Iteration 50, loss = 0.01834337
Iteration 51, loss = 0.01814998
Iteration 52, loss = 0.01783518
Iteration 53, loss = 0.01761085
Iteration 54, loss = 0.01752240
Iteration 55, loss = 0.01726391
Iteration 56, loss = 0.01711381
Iteration 57, loss = 0.01695504
Iteration 58, loss = 0.01679877
Iteration 59, loss = 0.01663435
Iteration 60, loss = 0.01652613
Iteration 61, loss = 0.01637627
Iteration 62, loss = 0.01628806
Iteration 63, loss = 0.01614461
Iteration 64, loss = 0.01598545
Iteration 65, loss = 0.01591591
Iteration 66, loss = 0.01581896
Iteration 67, loss = 0.01572660
Iteration 68, loss = 0.01568655
Iteration 69, loss = 0.01546353
Iteration 70, loss = 0.01534901
Iteration 71, loss = 0.01531365
Iteration 72, loss = 0.01518297
Iteration 73, loss = 0.01520690
Iteration 74, loss = 0.01516022
Iteration 75, loss = 0.01505236
Iteration 76, loss = 0.01502323
Iteration 77, loss = 0.01498604
Iteration 78, loss = 0.01489374
Iteration 79, loss = 0.01478079
Iteration 80, loss = 0.01473206
Iteration 81, loss = 0.01478338
Iteration 82, loss = 0.01453845
Iteration 83, loss = 0.01473411
Iteration 84, loss = 0.01447201
Iteration 85, loss = 0.01440675
Iteration 86, loss = 0.01433312
Iteration 87, loss = 0.01441231
Iteration 88, loss = 0.01444904
Iteration 89, loss = 0.01435522
Iteration 90, loss = 0.01434452
Iteration 91, loss = 0.01420069
Iteration 92, loss = 0.01403655
Iteration 93, loss = 0.01421009
Iteration 94, loss = 0.01416585
Iteration 95, loss = 0.01407447
Iteration 96, loss = 0.01400317
Iteration 97, loss = 0.01411721
Iteration 98, loss = 0.01400655
Iteration 99, loss = 0.01409906
Iteration 100, loss = 0.01402312
Iteration 101, loss = 0.01416594
Iteration 102, loss = 0.01379831
Iteration 103, loss = 0.01393046
Iteration 104, loss = 0.01397721
Iteration 105, loss = 0.01387867
Iteration 106, loss = 0.01372822
Iteration 107, loss = 0.01383423
Iteration 108, loss = 0.01375307
Iteration 109, loss = 0.01387177
Iteration 110, loss = 0.01368880
Iteration 111, loss = 0.01367742
Iteration 112, loss = 0.01357671
Iteration 113, loss = 0.01365509
Iteration 114, loss = 0.01370757
Iteration 115, loss = 0.01365772
Iteration 116, loss = 0.01355616
Iteration 117, loss = 0.01364401
Iteration 118, loss = 0.01362197
Iteration 119, loss = 0.01361609
Iteration 120, loss = 0.01352814
Iteration 121, loss = 0.01348991
Iteration 122, loss = 0.01357276
Iteration 123, loss = 0.01362765
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67513491
Iteration 2, loss = 0.61356790
Iteration 3, loss = 0.52209534
Iteration 4, loss = 0.41993990
Iteration 5, loss = 0.32569568
Iteration 6, loss = 0.24859085
Iteration 7, loss = 0.19156468
Iteration 8, loss = 0.15046554
Iteration 9, loss = 0.12153109
Iteration 10, loss = 0.10081576
Iteration 11, loss = 0.08567240
Iteration 12, loss = 0.07427418
Iteration 13, loss = 0.06557374
Iteration 14, loss = 0.05866366
Iteration 15, loss = 0.05330047
Iteration 16, loss = 0.04903479
Iteration 17, loss = 0.04561531
Iteration 18, loss = 0.04250169
Iteration 19, loss = 0.03998952
Iteration 20, loss = 0.03817051
Iteration 21, loss = 0.03629603
Iteration 22, loss = 0.03465671
Iteration 23, loss = 0.03356793
Iteration 24, loss = 0.03227810
Iteration 25, loss = 0.03125613
Iteration 26, loss = 0.03021262
Iteration 27, loss = 0.02943350
Iteration 28, loss = 0.02900482
Iteration 29, loss = 0.02832501
Iteration 30, loss = 0.02770117
Iteration 31, loss = 0.02731382
Iteration 32, loss = 0.02687338
Iteration 33, loss = 0.02644514
Iteration 34, loss = 0.02629313
Iteration 35, loss = 0.02568185
Iteration 36, loss = 0.02556010
Iteration 37, loss = 0.02516035
Iteration 38, loss = 0.02465689
Iteration 39, loss = 0.02434552
Iteration 40, loss = 0.02484132
Iteration 41, loss = 0.02441549
Iteration 42, loss = 0.02395789
Iteration 43, loss = 0.02377561
Iteration 44, loss = 0.02390022
Iteration 45, loss = 0.02352381
Iteration 46, loss = 0.02340380
Iteration 47, loss = 0.02294465
Iteration 48, loss = 0.02317470
Iteration 49, loss = 0.02293400
Iteration 50, loss = 0.02273529
Iteration 51, loss = 0.02278328
Iteration 52, loss = 0.02244821
Iteration 53, loss = 0.02278665
Iteration 54, loss = 0.02231113
Iteration 55, loss = 0.02236401
Iteration 56, loss = 0.02224588
Iteration 57, loss = 0.02229848
Iteration 58, loss = 0.02210292
Iteration 59, loss = 0.02196260
Iteration 60, loss = 0.02202720
Iteration 61, loss = 0.02169226
Iteration 62, loss = 0.02179844
Iteration 63, loss = 0.02168970
Iteration 64, loss = 0.02170500
Iteration 65, loss = 0.02151484
Iteration 66, loss = 0.02182330
Iteration 67, loss = 0.02171413
Iteration 68, loss = 0.02153732
Iteration 69, loss = 0.02132902
Iteration 70, loss = 0.02151274
Iteration 71, loss = 0.02157711
Iteration 72, loss = 0.02133690
Iteration 73, loss = 0.02142970
Iteration 74, loss = 0.02147675
Iteration 75, loss = 0.02124701
Iteration 76, loss = 0.02120001
Iteration 77, loss = 0.02132157
Iteration 78, loss = 0.02112910
Iteration 79, loss = 0.02125638
Iteration 80, loss = 0.02118427
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69477002
Iteration 2, loss = 0.65012016
Iteration 3, loss = 0.58319933
Iteration 4, loss = 0.50020408
Iteration 5, loss = 0.41650275
Iteration 6, loss = 0.33980017
Iteration 7, loss = 0.27507610
Iteration 8, loss = 0.22287482
Iteration 9, loss = 0.18188155
Iteration 10, loss = 0.15050594
Iteration 11, loss = 0.12641273
Iteration 12, loss = 0.10773861
Iteration 13, loss = 0.09339576
Iteration 14, loss = 0.08213154
Iteration 15, loss = 0.07308503
Iteration 16, loss = 0.06606457
Iteration 17, loss = 0.05988486
Iteration 18, loss = 0.05503646
Iteration 19, loss = 0.05083512
Iteration 20, loss = 0.04744643
Iteration 21, loss = 0.04450675
Iteration 22, loss = 0.04191098
Iteration 23, loss = 0.03988493
Iteration 24, loss = 0.03788374
Iteration 25, loss = 0.03611114
Iteration 26, loss = 0.03459784
Iteration 27, loss = 0.03336129
Iteration 28, loss = 0.03224035
Iteration 29, loss = 0.03123666
Iteration 30, loss = 0.03025184
Iteration 31, loss = 0.02956203
Iteration 32, loss = 0.02877213
Iteration 33, loss = 0.02783902
Iteration 34, loss = 0.02768342
Iteration 35, loss = 0.02691717
Iteration 36, loss = 0.02628147
Iteration 37, loss = 0.02583695
Iteration 38, loss = 0.02547487
Iteration 39, loss = 0.02508184
Iteration 40, loss = 0.02452012
Iteration 41, loss = 0.02425834
Iteration 42, loss = 0.02394745
Iteration 43, loss = 0.02360936
Iteration 44, loss = 0.02324272
Iteration 45, loss = 0.02319258
Iteration 46, loss = 0.02279063
Iteration 47, loss = 0.02262585
Iteration 48, loss = 0.02236515
Iteration 49, loss = 0.02227646
Iteration 50, loss = 0.02225613
Iteration 51, loss = 0.02185533
Iteration 52, loss = 0.02171646
Iteration 53, loss = 0.02145855
Iteration 54, loss = 0.02129928
Iteration 55, loss = 0.02138740
Iteration 56, loss = 0.02100558
Iteration 57, loss = 0.02101767
Iteration 58, loss = 0.02092058
Iteration 59, loss = 0.02092081
Iteration 60, loss = 0.02063530
Iteration 61, loss = 0.02067188
Iteration 62, loss = 0.02084422
Iteration 63, loss = 0.02052070
Iteration 64, loss = 0.02042557
Iteration 65, loss = 0.02042638
Iteration 66, loss = 0.02017136
Iteration 67, loss = 0.02022372
Iteration 68, loss = 0.02013162
Iteration 69, loss = 0.01981127
Iteration 70, loss = 0.01988342
Iteration 71, loss = 0.01987910
Iteration 72, loss = 0.01980858
Iteration 73, loss = 0.01958450
Iteration 74, loss = 0.01974128
Iteration 75, loss = 0.01956910
Iteration 76, loss = 0.01994190
Iteration 77, loss = 0.01964056
Iteration 78, loss = 0.01934628
Iteration 79, loss = 0.01952655
Iteration 80, loss = 0.01946704
Iteration 81, loss = 0.01919727
Iteration 82, loss = 0.01935736
Iteration 83, loss = 0.01932022
Iteration 84, loss = 0.01928337
Iteration 85, loss = 0.01913273
Iteration 86, loss = 0.01903131
Iteration 87, loss = 0.01900693
Iteration 88, loss = 0.01910486
Iteration 89, loss = 0.01914048
Iteration 90, loss = 0.01943896
Iteration 91, loss = 0.01929286
Iteration 92, loss = 0.01905498
Iteration 93, loss = 0.01914668
Iteration 94, loss = 0.01902262
Iteration 95, loss = 0.01881552
Iteration 96, loss = 0.01904226
Iteration 97, loss = 0.01891086
Iteration 98, loss = 0.01897657
Iteration 99, loss = 0.01879956
Iteration 100, loss = 0.01869205
Iteration 101, loss = 0.01858011
Iteration 102, loss = 0.01902641
Iteration 103, loss = 0.01903291
Iteration 104, loss = 0.01861505
Iteration 105, loss = 0.01872036
Iteration 106, loss = 0.01865308
Iteration 107, loss = 0.01875242
Iteration 108, loss = 0.01852122
Iteration 109, loss = 0.01857585
Iteration 110, loss = 0.01856277
Iteration 111, loss = 0.01851323
Iteration 112, loss = 0.01829518
Iteration 113, loss = 0.01852848
Iteration 114, loss = 0.01845213
Iteration 115, loss = 0.01844983
Iteration 116, loss = 0.01857480
Iteration 117, loss = 0.01846381
Iteration 118, loss = 0.01852250
Iteration 119, loss = 0.01843302
Iteration 120, loss = 0.01868028
Iteration 121, loss = 0.01839696
Iteration 122, loss = 0.01862011
Iteration 123, loss = 0.01844723
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70849116
Iteration 2, loss = 0.65451280
Iteration 3, loss = 0.57819027
Iteration 4, loss = 0.48019602
Iteration 5, loss = 0.38386218
Iteration 6, loss = 0.29779754
Iteration 7, loss = 0.23068237
Iteration 8, loss = 0.18117171
Iteration 9, loss = 0.14551220
Iteration 10, loss = 0.11960775
Iteration 11, loss = 0.10072827
Iteration 12, loss = 0.08688599
Iteration 13, loss = 0.07614153
Iteration 14, loss = 0.06742478
Iteration 15, loss = 0.06071145
Iteration 16, loss = 0.05543126
Iteration 17, loss = 0.05115344
Iteration 18, loss = 0.04759750
Iteration 19, loss = 0.04456584
Iteration 20, loss = 0.04198374
Iteration 21, loss = 0.03987638
Iteration 22, loss = 0.03815569
Iteration 23, loss = 0.03616997
Iteration 24, loss = 0.03505891
Iteration 25, loss = 0.03366929
Iteration 26, loss = 0.03344628
Iteration 27, loss = 0.03252048
Iteration 28, loss = 0.03161763
Iteration 29, loss = 0.03081913
Iteration 30, loss = 0.02990581
Iteration 31, loss = 0.02918493
Iteration 32, loss = 0.02861797
Iteration 33, loss = 0.02824100
Iteration 34, loss = 0.02761548
Iteration 35, loss = 0.02726460
Iteration 36, loss = 0.02684046
Iteration 37, loss = 0.02651379
Iteration 38, loss = 0.02607737
Iteration 39, loss = 0.02579374
Iteration 40, loss = 0.02561239
Iteration 41, loss = 0.02507924
Iteration 42, loss = 0.02506458
Iteration 43, loss = 0.02478520
Iteration 44, loss = 0.02511117
Iteration 45, loss = 0.02540422
Iteration 46, loss = 0.02478536
Iteration 47, loss = 0.02440527
Iteration 48, loss = 0.02420543
Iteration 49, loss = 0.02396999
Iteration 50, loss = 0.02395608
Iteration 51, loss = 0.02374072
Iteration 52, loss = 0.02359787
Iteration 53, loss = 0.02335480
Iteration 54, loss = 0.02334519
Iteration 55, loss = 0.02332009
Iteration 56, loss = 0.02316196
Iteration 57, loss = 0.02301431
Iteration 58, loss = 0.02282689
Iteration 59, loss = 0.02276184
Iteration 60, loss = 0.02283350
Iteration 61, loss = 0.02269158
Iteration 62, loss = 0.02244106
Iteration 63, loss = 0.02236119
Iteration 64, loss = 0.02231100
Iteration 65, loss = 0.02225662
Iteration 66, loss = 0.02218189
Iteration 67, loss = 0.02221111
Iteration 68, loss = 0.02395983
Iteration 69, loss = 0.02425754
Iteration 70, loss = 0.02375172
Iteration 71, loss = 0.02352164
Iteration 72, loss = 0.02339353
Iteration 73, loss = 0.02309594
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70385759
Iteration 2, loss = 0.68163480
Iteration 3, loss = 0.66002138
Iteration 4, loss = 0.63567535
Iteration 5, loss = 0.60849432
Iteration 6, loss = 0.57863476
Iteration 7, loss = 0.54687129
Iteration 8, loss = 0.51351450
Iteration 9, loss = 0.47968785
Iteration 10, loss = 0.44552447
Iteration 11, loss = 0.41199500
Iteration 12, loss = 0.37951806
Iteration 13, loss = 0.34857846
Iteration 14, loss = 0.31919061
Iteration 15, loss = 0.29174908
Iteration 16, loss = 0.26643300
Iteration 17, loss = 0.24310337
Iteration 18, loss = 0.22178277
Iteration 19, loss = 0.20249516
Iteration 20, loss = 0.18496695
Iteration 21, loss = 0.16921839
Iteration 22, loss = 0.15512044
Iteration 23, loss = 0.14248430
Iteration 24, loss = 0.13116853
Iteration 25, loss = 0.12104107
Iteration 26, loss = 0.11199262
Iteration 27, loss = 0.10389353
Iteration 28, loss = 0.09663066
Iteration 29, loss = 0.09007571
Iteration 30, loss = 0.08418063
Iteration 31, loss = 0.07891913
Iteration 32, loss = 0.07410413
Iteration 33, loss = 0.06980348
Iteration 34, loss = 0.06582542
Iteration 35, loss = 0.06225665
Iteration 36, loss = 0.05903862
Iteration 37, loss = 0.05603582
Iteration 38, loss = 0.05330841
Iteration 39, loss = 0.05077910
Iteration 40, loss = 0.04850380
Iteration 41, loss = 0.04636129
Iteration 42, loss = 0.04441918
Iteration 43, loss = 0.04263032
Iteration 44, loss = 0.04093856
Iteration 45, loss = 0.03939701
Iteration 46, loss = 0.03790129
Iteration 47, loss = 0.03655126
Iteration 48, loss = 0.03525776
Iteration 49, loss = 0.03408076
Iteration 50, loss = 0.03294898
Iteration 51, loss = 0.03193219
Iteration 52, loss = 0.03099003
Iteration 53, loss = 0.03001909
Iteration 54, loss = 0.02920105
Iteration 55, loss = 0.02844116
Iteration 56, loss = 0.02759388
Iteration 57, loss = 0.02686510
Iteration 58, loss = 0.02618113
Iteration 59, loss = 0.02553927
Iteration 60, loss = 0.02492065
Iteration 61, loss = 0.02444059
Iteration 62, loss = 0.02378910
Iteration 63, loss = 0.02327890
Iteration 64, loss = 0.02276046
Iteration 65, loss = 0.02229644
Iteration 66, loss = 0.02181877
Iteration 67, loss = 0.02141144
Iteration 68, loss = 0.02100740
Iteration 69, loss = 0.02061660
Iteration 70, loss = 0.02024769
Iteration 71, loss = 0.01984732
Iteration 72, loss = 0.01954347
Iteration 73, loss = 0.01919614
Iteration 74, loss = 0.01885155
Iteration 75, loss = 0.01859969
Iteration 76, loss = 0.01828395
Iteration 77, loss = 0.01803203
Iteration 78, loss = 0.01776459
Iteration 79, loss = 0.01750315
Iteration 80, loss = 0.01726518
Iteration 81, loss = 0.01703163
Iteration 82, loss = 0.01682268
Iteration 83, loss = 0.01660279
Iteration 84, loss = 0.01637373
Iteration 85, loss = 0.01617301
Iteration 86, loss = 0.01597852
Iteration 87, loss = 0.01581626
Iteration 88, loss = 0.01563170
Iteration 89, loss = 0.01543272
Iteration 90, loss = 0.01535066
Iteration 91, loss = 0.01513444
Iteration 92, loss = 0.01495674
Iteration 93, loss = 0.01481804
Iteration 94, loss = 0.01466332
Iteration 95, loss = 0.01453193
Iteration 96, loss = 0.01437871
Iteration 97, loss = 0.01425863
Iteration 98, loss = 0.01412307
Iteration 99, loss = 0.01399497
Iteration 100, loss = 0.01388751
Iteration 101, loss = 0.01377498
Iteration 102, loss = 0.01366494
Iteration 103, loss = 0.01352432
Iteration 104, loss = 0.01346737
Iteration 105, loss = 0.01330306
Iteration 106, loss = 0.01326467
Iteration 107, loss = 0.01309829
Iteration 108, loss = 0.01302313
Iteration 109, loss = 0.01297257
Iteration 110, loss = 0.01283545
Iteration 111, loss = 0.01284973
Iteration 112, loss = 0.01268215
Iteration 113, loss = 0.01260895
Iteration 114, loss = 0.01255064
Iteration 115, loss = 0.01248210
Iteration 116, loss = 0.01238477
Iteration 117, loss = 0.01233722
Iteration 118, loss = 0.01229052
Iteration 119, loss = 0.01215027
Iteration 120, loss = 0.01211193
Iteration 121, loss = 0.01208548
Iteration 122, loss = 0.01196762
Iteration 123, loss = 0.01196800
Iteration 124, loss = 0.01184327
Iteration 125, loss = 0.01179652
Iteration 126, loss = 0.01172985
Iteration 127, loss = 0.01176145
Iteration 128, loss = 0.01161731
Iteration 129, loss = 0.01161969
Iteration 130, loss = 0.01153469
Iteration 131, loss = 0.01154924
Iteration 132, loss = 0.01141804
Iteration 133, loss = 0.01137992
Iteration 134, loss = 0.01132244
Iteration 135, loss = 0.01126324
Iteration 136, loss = 0.01125712
Iteration 137, loss = 0.01117938
Iteration 138, loss = 0.01117144
Iteration 139, loss = 0.01112476
Iteration 140, loss = 0.01105899
Iteration 141, loss = 0.01101756
Iteration 142, loss = 0.01096593
Iteration 143, loss = 0.01094225
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71382128
Iteration 2, loss = 0.67984838
Iteration 3, loss = 0.63780741
Iteration 4, loss = 0.58700103
Iteration 5, loss = 0.52830869
Iteration 6, loss = 0.46624904
Iteration 7, loss = 0.40423334
Iteration 8, loss = 0.34664284
Iteration 9, loss = 0.29522321
Iteration 10, loss = 0.25052739
Iteration 11, loss = 0.21273531
Iteration 12, loss = 0.18134482
Iteration 13, loss = 0.15540966
Iteration 14, loss = 0.13417217
Iteration 15, loss = 0.11687992
Iteration 16, loss = 0.10261365
Iteration 17, loss = 0.09078062
Iteration 18, loss = 0.08101894
Iteration 19, loss = 0.07285038
Iteration 20, loss = 0.06588158
Iteration 21, loss = 0.06006840
Iteration 22, loss = 0.05501897
Iteration 23, loss = 0.05065597
Iteration 24, loss = 0.04691712
Iteration 25, loss = 0.04380252
Iteration 26, loss = 0.04080089
Iteration 27, loss = 0.03825552
Iteration 28, loss = 0.03600042
Iteration 29, loss = 0.03410341
Iteration 30, loss = 0.03244647
Iteration 31, loss = 0.03069082
Iteration 32, loss = 0.02929444
Iteration 33, loss = 0.02804147
Iteration 34, loss = 0.02682976
Iteration 35, loss = 0.02580007
Iteration 36, loss = 0.02489032
Iteration 37, loss = 0.02399316
Iteration 38, loss = 0.02309759
Iteration 39, loss = 0.02232727
Iteration 40, loss = 0.02169285
Iteration 41, loss = 0.02100872
Iteration 42, loss = 0.02049923
Iteration 43, loss = 0.01993874
Iteration 44, loss = 0.01950858
Iteration 45, loss = 0.01898660
Iteration 46, loss = 0.01843746
Iteration 47, loss = 0.01802294
Iteration 48, loss = 0.01774883
Iteration 49, loss = 0.01753290
Iteration 50, loss = 0.01705407
Iteration 51, loss = 0.01670720
Iteration 52, loss = 0.01651804
Iteration 53, loss = 0.01616012
Iteration 54, loss = 0.01582419
Iteration 55, loss = 0.01559697
Iteration 56, loss = 0.01536408
Iteration 57, loss = 0.01534380
Iteration 58, loss = 0.01498247
Iteration 59, loss = 0.01484481
Iteration 60, loss = 0.01463925
Iteration 61, loss = 0.01439049
Iteration 62, loss = 0.01421603
Iteration 63, loss = 0.01407892
Iteration 64, loss = 0.01391160
Iteration 65, loss = 0.01376708
Iteration 66, loss = 0.01364951
Iteration 67, loss = 0.01361161
Iteration 68, loss = 0.01338661
Iteration 69, loss = 0.01323433
Iteration 70, loss = 0.01326399
Iteration 71, loss = 0.01311304
Iteration 72, loss = 0.01295710
Iteration 73, loss = 0.01288615
Iteration 74, loss = 0.01278396
Iteration 75, loss = 0.01267596
Iteration 76, loss = 0.01252431
Iteration 77, loss = 0.01251062
Iteration 78, loss = 0.01244907
Iteration 79, loss = 0.01224751
Iteration 80, loss = 0.01242297
Iteration 81, loss = 0.01222619
Iteration 82, loss = 0.01225457
Iteration 83, loss = 0.01212885
Iteration 84, loss = 0.01196824
Iteration 85, loss = 0.01214897
Iteration 86, loss = 0.01185099
Iteration 87, loss = 0.01197831
Iteration 88, loss = 0.01166641
Iteration 89, loss = 0.01163709
Iteration 90, loss = 0.01168163
Iteration 91, loss = 0.01163013
Iteration 92, loss = 0.01141548
Iteration 93, loss = 0.01142002
Iteration 94, loss = 0.01147504
Iteration 95, loss = 0.01139741
Iteration 96, loss = 0.01134815
Iteration 97, loss = 0.01139197
Iteration 98, loss = 0.01118249
Iteration 99, loss = 0.01134050
Iteration 100, loss = 0.01109408
Iteration 101, loss = 0.01118197
Iteration 102, loss = 0.01116150
Iteration 103, loss = 0.01103560
Iteration 104, loss = 0.01105465
Iteration 105, loss = 0.01094153
Iteration 106, loss = 0.01104984
Iteration 107, loss = 0.01093842
Iteration 108, loss = 0.01097343
Iteration 109, loss = 0.01091915
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68872006
Iteration 2, loss = 0.65601345
Iteration 3, loss = 0.61181247
Iteration 4, loss = 0.55099772
Iteration 5, loss = 0.47853044
Iteration 6, loss = 0.40585930
Iteration 7, loss = 0.33953522
Iteration 8, loss = 0.28176941
Iteration 9, loss = 0.23375431
Iteration 10, loss = 0.19464127
Iteration 11, loss = 0.16328702
Iteration 12, loss = 0.13839868
Iteration 13, loss = 0.11875926
Iteration 14, loss = 0.10311296
Iteration 15, loss = 0.09048447
Iteration 16, loss = 0.08008487
Iteration 17, loss = 0.07162556
Iteration 18, loss = 0.06463035
Iteration 19, loss = 0.05880015
Iteration 20, loss = 0.05384721
Iteration 21, loss = 0.04965815
Iteration 22, loss = 0.04608610
Iteration 23, loss = 0.04299075
Iteration 24, loss = 0.04032470
Iteration 25, loss = 0.03801191
Iteration 26, loss = 0.03582816
Iteration 27, loss = 0.03395527
Iteration 28, loss = 0.03237888
Iteration 29, loss = 0.03097745
Iteration 30, loss = 0.02960511
Iteration 31, loss = 0.02838760
Iteration 32, loss = 0.02736464
Iteration 33, loss = 0.02643864
Iteration 34, loss = 0.02561905
Iteration 35, loss = 0.02478302
Iteration 36, loss = 0.02397865
Iteration 37, loss = 0.02342392
Iteration 38, loss = 0.02273861
Iteration 39, loss = 0.02225069
Iteration 40, loss = 0.02166135
Iteration 41, loss = 0.02123739
Iteration 42, loss = 0.02079813
Iteration 43, loss = 0.02028777
Iteration 44, loss = 0.02006122
Iteration 45, loss = 0.01965963
Iteration 46, loss = 0.01934911
Iteration 47, loss = 0.01895071
Iteration 48, loss = 0.01863024
Iteration 49, loss = 0.01837679
Iteration 50, loss = 0.01808396
Iteration 51, loss = 0.01782317
Iteration 52, loss = 0.01766024
Iteration 53, loss = 0.01744736
Iteration 54, loss = 0.01731421
Iteration 55, loss = 0.01712855
Iteration 56, loss = 0.01695103
Iteration 57, loss = 0.01671325
Iteration 58, loss = 0.01660037
Iteration 59, loss = 0.01636819
Iteration 60, loss = 0.01625966
Iteration 61, loss = 0.01612359
Iteration 62, loss = 0.01610929
Iteration 63, loss = 0.01581946
Iteration 64, loss = 0.01578074
Iteration 65, loss = 0.01563489
Iteration 66, loss = 0.01551755
Iteration 67, loss = 0.01533954
Iteration 68, loss = 0.01533565
Iteration 69, loss = 0.01526364
Iteration 70, loss = 0.01514493
Iteration 71, loss = 0.01509950
Iteration 72, loss = 0.01500124
Iteration 73, loss = 0.01495495
Iteration 74, loss = 0.01483723
Iteration 75, loss = 0.01497327
Iteration 76, loss = 0.01479091
Iteration 77, loss = 0.01461230
Iteration 78, loss = 0.01460453
Iteration 79, loss = 0.01463410
Iteration 80, loss = 0.01468039
Iteration 81, loss = 0.01441275
Iteration 82, loss = 0.01431043
Iteration 83, loss = 0.01430660
Iteration 84, loss = 0.01424235
Iteration 85, loss = 0.01416468
Iteration 86, loss = 0.01410261
Iteration 87, loss = 0.01422450
Iteration 88, loss = 0.01411638
Iteration 89, loss = 0.01393010
Iteration 90, loss = 0.01396738
Iteration 91, loss = 0.01399637
Iteration 92, loss = 0.01386498
Iteration 93, loss = 0.01389426
Iteration 94, loss = 0.01388243
Iteration 95, loss = 0.01388814
Iteration 96, loss = 0.01374895
Iteration 97, loss = 0.01376696
Iteration 98, loss = 0.01368811
Iteration 99, loss = 0.01361647
Iteration 100, loss = 0.01375532
Iteration 101, loss = 0.01364250
Iteration 102, loss = 0.01353643
Iteration 103, loss = 0.01372014
Iteration 104, loss = 0.01363984
Iteration 105, loss = 0.01361678
Iteration 106, loss = 0.01345958
Iteration 107, loss = 0.01354736
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72102076
Iteration 2, loss = 0.66993586
Iteration 3, loss = 0.60046125
Iteration 4, loss = 0.51711889
Iteration 5, loss = 0.43080736
Iteration 6, loss = 0.35196948
Iteration 7, loss = 0.28420042
Iteration 8, loss = 0.22989839
Iteration 9, loss = 0.18747992
Iteration 10, loss = 0.15510238
Iteration 11, loss = 0.13015970
Iteration 12, loss = 0.11082995
Iteration 13, loss = 0.09575699
Iteration 14, loss = 0.08392447
Iteration 15, loss = 0.07446548
Iteration 16, loss = 0.06702098
Iteration 17, loss = 0.06071222
Iteration 18, loss = 0.05545421
Iteration 19, loss = 0.05122808
Iteration 20, loss = 0.04780996
Iteration 21, loss = 0.04448205
Iteration 22, loss = 0.04174297
Iteration 23, loss = 0.03947428
Iteration 24, loss = 0.03750939
Iteration 25, loss = 0.03578640
Iteration 26, loss = 0.03408858
Iteration 27, loss = 0.03271408
Iteration 28, loss = 0.03147434
Iteration 29, loss = 0.03036434
Iteration 30, loss = 0.02957552
Iteration 31, loss = 0.02869325
Iteration 32, loss = 0.02784986
Iteration 33, loss = 0.02704253
Iteration 34, loss = 0.02641783
Iteration 35, loss = 0.02575688
Iteration 36, loss = 0.02532248
Iteration 37, loss = 0.02473733
Iteration 38, loss = 0.02426037
Iteration 39, loss = 0.02368969
Iteration 40, loss = 0.02342477
Iteration 41, loss = 0.02301757
Iteration 42, loss = 0.02272053
Iteration 43, loss = 0.02242654
Iteration 44, loss = 0.02201317
Iteration 45, loss = 0.02181442
Iteration 46, loss = 0.02158368
Iteration 47, loss = 0.02140980
Iteration 48, loss = 0.02130145
Iteration 49, loss = 0.02072665
Iteration 50, loss = 0.02068550
Iteration 51, loss = 0.02061175
Iteration 52, loss = 0.02032621
Iteration 53, loss = 0.02029338
Iteration 54, loss = 0.02014607
Iteration 55, loss = 0.01987522
Iteration 56, loss = 0.01966665
Iteration 57, loss = 0.01948681
Iteration 58, loss = 0.01940776
Iteration 59, loss = 0.01933710
Iteration 60, loss = 0.01931855
Iteration 61, loss = 0.01903352
Iteration 62, loss = 0.01911323
Iteration 63, loss = 0.01912179
Iteration 64, loss = 0.01893982
Iteration 65, loss = 0.01897167
Iteration 66, loss = 0.01863182
Iteration 67, loss = 0.01853170
Iteration 68, loss = 0.01841028
Iteration 69, loss = 0.01845916
Iteration 70, loss = 0.01849233
Iteration 71, loss = 0.01824103
Iteration 72, loss = 0.01807769
Iteration 73, loss = 0.01822154
Iteration 74, loss = 0.01807961
Iteration 75, loss = 0.01805254
Iteration 76, loss = 0.01808271
Iteration 77, loss = 0.01788136
Iteration 78, loss = 0.01809909
Iteration 79, loss = 0.01780665
Iteration 80, loss = 0.01774167
Iteration 81, loss = 0.01777605
Iteration 82, loss = 0.01769014
Iteration 83, loss = 0.01761798
Iteration 84, loss = 0.01748634
Iteration 85, loss = 0.01747487
Iteration 86, loss = 0.01749408
Iteration 87, loss = 0.01753212
Iteration 88, loss = 0.01771173
Iteration 89, loss = 0.01761990
Iteration 90, loss = 0.01732433
Iteration 91, loss = 0.01725745
Iteration 92, loss = 0.01720701
Iteration 93, loss = 0.01735396
Iteration 94, loss = 0.01737297
Iteration 95, loss = 0.01739220
Iteration 96, loss = 0.01710875
Iteration 97, loss = 0.01731170
Iteration 98, loss = 0.01717348
Iteration 99, loss = 0.01714630
Iteration 100, loss = 0.01747539
Iteration 101, loss = 0.01694117
Iteration 102, loss = 0.01716324
Iteration 103, loss = 0.01718957
Iteration 104, loss = 0.01703320
Iteration 105, loss = 0.01701629
Iteration 106, loss = 0.01691542
Iteration 107, loss = 0.01701828
Iteration 108, loss = 0.01721574
Iteration 109, loss = 0.01698596
Iteration 110, loss = 0.01671563
Iteration 111, loss = 0.01688696
Iteration 112, loss = 0.01687329
Iteration 113, loss = 0.01715115
Iteration 114, loss = 0.01684603
Iteration 115, loss = 0.01684909
Iteration 116, loss = 0.01684256
Iteration 117, loss = 0.01691851
Iteration 118, loss = 0.01685308
Iteration 119, loss = 0.01673795
Iteration 120, loss = 0.01677787
Iteration 121, loss = 0.01664549
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68067715
Iteration 2, loss = 0.63059696
Iteration 3, loss = 0.55823409
Iteration 4, loss = 0.47189907
Iteration 5, loss = 0.38655165
Iteration 6, loss = 0.30979607
Iteration 7, loss = 0.24623722
Iteration 8, loss = 0.19659565
Iteration 9, loss = 0.15889828
Iteration 10, loss = 0.13074188
Iteration 11, loss = 0.10924979
Iteration 12, loss = 0.09346719
Iteration 13, loss = 0.08112206
Iteration 14, loss = 0.07156350
Iteration 15, loss = 0.06376427
Iteration 16, loss = 0.05778907
Iteration 17, loss = 0.05249251
Iteration 18, loss = 0.04851489
Iteration 19, loss = 0.04504661
Iteration 20, loss = 0.04194351
Iteration 21, loss = 0.03964703
Iteration 22, loss = 0.03732532
Iteration 23, loss = 0.03556831
Iteration 24, loss = 0.03421166
Iteration 25, loss = 0.03254303
Iteration 26, loss = 0.03143994
Iteration 27, loss = 0.03021425
Iteration 28, loss = 0.02914868
Iteration 29, loss = 0.02838482
Iteration 30, loss = 0.02752856
Iteration 31, loss = 0.02682909
Iteration 32, loss = 0.02622511
Iteration 33, loss = 0.02568684
Iteration 34, loss = 0.02503351
Iteration 35, loss = 0.02457478
Iteration 36, loss = 0.02406857
Iteration 37, loss = 0.02370474
Iteration 38, loss = 0.02351328
Iteration 39, loss = 0.02312858
Iteration 40, loss = 0.02271594
Iteration 41, loss = 0.02268886
Iteration 42, loss = 0.02221625
Iteration 43, loss = 0.02187327
Iteration 44, loss = 0.02169996
Iteration 45, loss = 0.02153699
Iteration 46, loss = 0.02132825
Iteration 47, loss = 0.02102487
Iteration 48, loss = 0.02102210
Iteration 49, loss = 0.02082606
Iteration 50, loss = 0.02045909
Iteration 51, loss = 0.02030632
Iteration 52, loss = 0.02031572
Iteration 53, loss = 0.02001942
Iteration 54, loss = 0.02019578
Iteration 55, loss = 0.02002017
Iteration 56, loss = 0.01982679
Iteration 57, loss = 0.01973732
Iteration 58, loss = 0.01958200
Iteration 59, loss = 0.01927081
Iteration 60, loss = 0.01956748
Iteration 61, loss = 0.01933301
Iteration 62, loss = 0.01933048
Iteration 63, loss = 0.01907221
Iteration 64, loss = 0.01904828
Iteration 65, loss = 0.01904332
Iteration 66, loss = 0.01899168
Iteration 67, loss = 0.01909383
Iteration 68, loss = 0.01895024
Iteration 69, loss = 0.01882311
Iteration 70, loss = 0.01874247
Iteration 71, loss = 0.01859255
Iteration 72, loss = 0.01857696
Iteration 73, loss = 0.01874913
Iteration 74, loss = 0.01876582
Iteration 75, loss = 0.01842272
Iteration 76, loss = 0.01848418
Iteration 77, loss = 0.01849588
Iteration 78, loss = 0.01848466
Iteration 79, loss = 0.01816696
Iteration 80, loss = 0.01818106
Iteration 81, loss = 0.01826162
Iteration 82, loss = 0.01835400
Iteration 83, loss = 0.01825829
Iteration 84, loss = 0.01802366
Iteration 85, loss = 0.01814778
Iteration 86, loss = 0.01808950
Iteration 87, loss = 0.01798734
Iteration 88, loss = 0.01795891
Iteration 89, loss = 0.01814913
Iteration 90, loss = 0.01788746
Iteration 91, loss = 0.01791381
Iteration 92, loss = 0.01803556
Iteration 93, loss = 0.01784580
Iteration 94, loss = 0.01801049
Iteration 95, loss = 0.01795267
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67903798
Iteration 2, loss = 0.64368906
Iteration 3, loss = 0.59560717
Iteration 4, loss = 0.53132496
Iteration 5, loss = 0.46049769
Iteration 6, loss = 0.39249251
Iteration 7, loss = 0.32981835
Iteration 8, loss = 0.27543923
Iteration 9, loss = 0.22990822
Iteration 10, loss = 0.19263757
Iteration 11, loss = 0.16284512
Iteration 12, loss = 0.13895155
Iteration 13, loss = 0.11984539
Iteration 14, loss = 0.10459377
Iteration 15, loss = 0.09211280
Iteration 16, loss = 0.08212121
Iteration 17, loss = 0.07392210
Iteration 18, loss = 0.06702606
Iteration 19, loss = 0.06120742
Iteration 20, loss = 0.05643390
Iteration 21, loss = 0.05231000
Iteration 22, loss = 0.04877897
Iteration 23, loss = 0.04568756
Iteration 24, loss = 0.04299926
Iteration 25, loss = 0.04079665
Iteration 26, loss = 0.03874784
Iteration 27, loss = 0.03683593
Iteration 28, loss = 0.03540992
Iteration 29, loss = 0.03416639
Iteration 30, loss = 0.03260892
Iteration 31, loss = 0.03149072
Iteration 32, loss = 0.03042856
Iteration 33, loss = 0.02959918
Iteration 34, loss = 0.02866451
Iteration 35, loss = 0.02787420
Iteration 36, loss = 0.02728145
Iteration 37, loss = 0.02658847
Iteration 38, loss = 0.02593955
Iteration 39, loss = 0.02555580
Iteration 40, loss = 0.02500749
Iteration 41, loss = 0.02443514
Iteration 42, loss = 0.02404660
Iteration 43, loss = 0.02370824
Iteration 44, loss = 0.02333476
Iteration 45, loss = 0.02310503
Iteration 46, loss = 0.02273360
Iteration 47, loss = 0.02264065
Iteration 48, loss = 0.02222960
Iteration 49, loss = 0.02198170
Iteration 50, loss = 0.02166802
Iteration 51, loss = 0.02150238
Iteration 52, loss = 0.02121076
Iteration 53, loss = 0.02102226
Iteration 54, loss = 0.02081846
Iteration 55, loss = 0.02069308
Iteration 56, loss = 0.02053121
Iteration 57, loss = 0.02039549
Iteration 58, loss = 0.02018522
Iteration 59, loss = 0.01993823
Iteration 60, loss = 0.01984982
Iteration 61, loss = 0.01984445
Iteration 62, loss = 0.01961799
Iteration 63, loss = 0.01948634
Iteration 64, loss = 0.01935099
Iteration 65, loss = 0.01927401
Iteration 66, loss = 0.01931076
Iteration 67, loss = 0.01906102
Iteration 68, loss = 0.01899771
Iteration 69, loss = 0.01895514
Iteration 70, loss = 0.01885659
Iteration 71, loss = 0.01874073
Iteration 72, loss = 0.01874601
Iteration 73, loss = 0.01876669
Iteration 74, loss = 0.01855590
Iteration 75, loss = 0.01847736
Iteration 76, loss = 0.01839551
Iteration 77, loss = 0.01826662
Iteration 78, loss = 0.01828104
Iteration 79, loss = 0.01827457
Iteration 80, loss = 0.01808610
Iteration 81, loss = 0.01820643
Iteration 82, loss = 0.01811094
Iteration 83, loss = 0.01819335
Iteration 84, loss = 0.01812396
Iteration 85, loss = 0.01793300
Iteration 86, loss = 0.01792210
Iteration 87, loss = 0.01785690
Iteration 88, loss = 0.01786193
Iteration 89, loss = 0.01782600
Iteration 90, loss = 0.01773218
Iteration 91, loss = 0.01786456
Iteration 92, loss = 0.01783669
Iteration 93, loss = 0.01774087
Iteration 94, loss = 0.01779533
Iteration 95, loss = 0.01771814
Iteration 96, loss = 0.01749849
Iteration 97, loss = 0.01750637
Iteration 98, loss = 0.01751211
Iteration 99, loss = 0.01741808
Iteration 100, loss = 0.01736005
Iteration 101, loss = 0.01750249
Iteration 102, loss = 0.01741863
Iteration 103, loss = 0.01740711
Iteration 104, loss = 0.01733587
Iteration 105, loss = 0.01753284
Iteration 106, loss = 0.01731734
Iteration 107, loss = 0.01722986
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67771205
Iteration 2, loss = 0.64087582
Iteration 3, loss = 0.58161797
Iteration 4, loss = 0.50593428
Iteration 5, loss = 0.42479726
Iteration 6, loss = 0.34766319
Iteration 7, loss = 0.28074054
Iteration 8, loss = 0.22686834
Iteration 9, loss = 0.18451844
Iteration 10, loss = 0.15195605
Iteration 11, loss = 0.12733031
Iteration 12, loss = 0.10805264
Iteration 13, loss = 0.09328682
Iteration 14, loss = 0.08160998
Iteration 15, loss = 0.07227819
Iteration 16, loss = 0.06489624
Iteration 17, loss = 0.05884521
Iteration 18, loss = 0.05349566
Iteration 19, loss = 0.04923187
Iteration 20, loss = 0.04557669
Iteration 21, loss = 0.04250180
Iteration 22, loss = 0.03983034
Iteration 23, loss = 0.03764253
Iteration 24, loss = 0.03569767
Iteration 25, loss = 0.03404306
Iteration 26, loss = 0.03253989
Iteration 27, loss = 0.03107208
Iteration 28, loss = 0.03007170
Iteration 29, loss = 0.02883792
Iteration 30, loss = 0.02805294
Iteration 31, loss = 0.02699619
Iteration 32, loss = 0.02601973
Iteration 33, loss = 0.02563727
Iteration 34, loss = 0.02483972
Iteration 35, loss = 0.02435040
Iteration 36, loss = 0.02375074
Iteration 37, loss = 0.02322286
Iteration 38, loss = 0.02272857
Iteration 39, loss = 0.02234374
Iteration 40, loss = 0.02192535
Iteration 41, loss = 0.02159345
Iteration 42, loss = 0.02124391
Iteration 43, loss = 0.02096459
Iteration 44, loss = 0.02059833
Iteration 45, loss = 0.02035900
Iteration 46, loss = 0.02016451
Iteration 47, loss = 0.02004843
Iteration 48, loss = 0.01953088
Iteration 49, loss = 0.01945159
Iteration 50, loss = 0.01942578
Iteration 51, loss = 0.01913899
Iteration 52, loss = 0.01900846
Iteration 53, loss = 0.01870640
Iteration 54, loss = 0.01879111
Iteration 55, loss = 0.01855842
Iteration 56, loss = 0.01858179
Iteration 57, loss = 0.01825506
Iteration 58, loss = 0.01812921
Iteration 59, loss = 0.01802279
Iteration 60, loss = 0.01792704
Iteration 61, loss = 0.01773134
Iteration 62, loss = 0.01778301
Iteration 63, loss = 0.01739336
Iteration 64, loss = 0.01754192
Iteration 65, loss = 0.01759966
Iteration 66, loss = 0.01738364
Iteration 67, loss = 0.01706325
Iteration 68, loss = 0.01715003
Iteration 69, loss = 0.01711359
Iteration 70, loss = 0.01704813
Iteration 71, loss = 0.01707877
Iteration 72, loss = 0.01694699
Iteration 73, loss = 0.01679463
Iteration 74, loss = 0.01686459
Iteration 75, loss = 0.01705020
Iteration 76, loss = 0.01702110
Iteration 77, loss = 0.01680645
Iteration 78, loss = 0.01666265
Iteration 79, loss = 0.01683871
Iteration 80, loss = 0.01632418
Iteration 81, loss = 0.01636873
Iteration 82, loss = 0.01654316
Iteration 83, loss = 0.01633659
Iteration 84, loss = 0.01621451
Iteration 85, loss = 0.01615958
Iteration 86, loss = 0.01638790
Iteration 87, loss = 0.01606702
Iteration 88, loss = 0.01609532
Iteration 89, loss = 0.01618849
Iteration 90, loss = 0.01600516
Iteration 91, loss = 0.01620123
Iteration 92, loss = 0.01605985
Iteration 93, loss = 0.01607531
Iteration 94, loss = 0.01602011
Iteration 95, loss = 0.01601689
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68438662
Iteration 2, loss = 0.63012081
Iteration 3, loss = 0.54176285
Iteration 4, loss = 0.44013240
Iteration 5, loss = 0.34701414
Iteration 6, loss = 0.26751581
Iteration 7, loss = 0.20751490
Iteration 8, loss = 0.16311141
Iteration 9, loss = 0.13141842
Iteration 10, loss = 0.10848528
Iteration 11, loss = 0.09198758
Iteration 12, loss = 0.07913253
Iteration 13, loss = 0.06941215
Iteration 14, loss = 0.06180035
Iteration 15, loss = 0.05586350
Iteration 16, loss = 0.05094751
Iteration 17, loss = 0.04724362
Iteration 18, loss = 0.04389286
Iteration 19, loss = 0.04089025
Iteration 20, loss = 0.03871080
Iteration 21, loss = 0.03654992
Iteration 22, loss = 0.03474717
Iteration 23, loss = 0.03316941
Iteration 24, loss = 0.03224636
Iteration 25, loss = 0.03128856
Iteration 26, loss = 0.03009805
Iteration 27, loss = 0.02918739
Iteration 28, loss = 0.02819208
Iteration 29, loss = 0.02743795
Iteration 30, loss = 0.02716692
Iteration 31, loss = 0.02622813
Iteration 32, loss = 0.02559622
Iteration 33, loss = 0.02512070
Iteration 34, loss = 0.02455880
Iteration 35, loss = 0.02425369
Iteration 36, loss = 0.02384590
Iteration 37, loss = 0.02359251
Iteration 38, loss = 0.02323367
Iteration 39, loss = 0.02294133
Iteration 40, loss = 0.02294500
Iteration 41, loss = 0.02285940
Iteration 42, loss = 0.02258355
Iteration 43, loss = 0.02236290
Iteration 44, loss = 0.02205815
Iteration 45, loss = 0.02174278
Iteration 46, loss = 0.02158216
Iteration 47, loss = 0.02244131
Iteration 48, loss = 0.02414481
Iteration 49, loss = 0.02335152
Iteration 50, loss = 0.02295425
Iteration 51, loss = 0.02256098
Iteration 52, loss = 0.02215016
Iteration 53, loss = 0.02191564
Iteration 54, loss = 0.02148111
Iteration 55, loss = 0.02147078
Iteration 56, loss = 0.02123599
Iteration 57, loss = 0.02110735
Iteration 58, loss = 0.02090571
Iteration 59, loss = 0.02053309
Iteration 60, loss = 0.02070890
Iteration 61, loss = 0.02036289
Iteration 62, loss = 0.02021620
Iteration 63, loss = 0.02004978
Iteration 64, loss = 0.01991991
Iteration 65, loss = 0.01983832
Iteration 66, loss = 0.01997757
Iteration 67, loss = 0.02078409
Iteration 68, loss = 0.02197679
Iteration 69, loss = 0.02288242
Iteration 70, loss = 0.02329306
Iteration 71, loss = 0.02297153
Iteration 72, loss = 0.02274170
Iteration 73, loss = 0.02233094
Iteration 74, loss = 0.02194648
Iteration 75, loss = 0.02161900
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67045710
Iteration 2, loss = 0.65472152
Iteration 3, loss = 0.64118078
Iteration 4, loss = 0.62776364
Iteration 5, loss = 0.61355110
Iteration 6, loss = 0.59822988
Iteration 7, loss = 0.58157137
Iteration 8, loss = 0.56341519
Iteration 9, loss = 0.54376697
Iteration 10, loss = 0.52286566
Iteration 11, loss = 0.50087616
Iteration 12, loss = 0.47824944
Iteration 13, loss = 0.45523122
Iteration 14, loss = 0.43181332
Iteration 15, loss = 0.40854087
Iteration 16, loss = 0.38555047
Iteration 17, loss = 0.36307210
Iteration 18, loss = 0.34141398
Iteration 19, loss = 0.32036095
Iteration 20, loss = 0.30040330
Iteration 21, loss = 0.28131537
Iteration 22, loss = 0.26326016
Iteration 23, loss = 0.24626129
Iteration 24, loss = 0.23044813
Iteration 25, loss = 0.21548860
Iteration 26, loss = 0.20156566
Iteration 27, loss = 0.18858601
Iteration 28, loss = 0.17661794
Iteration 29, loss = 0.16545659
Iteration 30, loss = 0.15510446
Iteration 31, loss = 0.14549212
Iteration 32, loss = 0.13664901
Iteration 33, loss = 0.12851871
Iteration 34, loss = 0.12096826
Iteration 35, loss = 0.11400017
Iteration 36, loss = 0.10754670
Iteration 37, loss = 0.10158703
Iteration 38, loss = 0.09607117
Iteration 39, loss = 0.09096032
Iteration 40, loss = 0.08627582
Iteration 41, loss = 0.08192210
Iteration 42, loss = 0.07782115
Iteration 43, loss = 0.07401854
Iteration 44, loss = 0.07048817
Iteration 45, loss = 0.06719089
Iteration 46, loss = 0.06415102
Iteration 47, loss = 0.06128259
Iteration 48, loss = 0.05862489
Iteration 49, loss = 0.05612574
Iteration 50, loss = 0.05377321
Iteration 51, loss = 0.05156312
Iteration 52, loss = 0.04953327
Iteration 53, loss = 0.04757059
Iteration 54, loss = 0.04575033
Iteration 55, loss = 0.04402002
Iteration 56, loss = 0.04240314
Iteration 57, loss = 0.04085214
Iteration 58, loss = 0.03941425
Iteration 59, loss = 0.03802657
Iteration 60, loss = 0.03673007
Iteration 61, loss = 0.03548436
Iteration 62, loss = 0.03433538
Iteration 63, loss = 0.03322266
Iteration 64, loss = 0.03218499
Iteration 65, loss = 0.03116161
Iteration 66, loss = 0.03020884
Iteration 67, loss = 0.02929175
Iteration 68, loss = 0.02842497
Iteration 69, loss = 0.02759652
Iteration 70, loss = 0.02678276
Iteration 71, loss = 0.02603179
Iteration 72, loss = 0.02529708
Iteration 73, loss = 0.02457563
Iteration 74, loss = 0.02392389
Iteration 75, loss = 0.02328434
Iteration 76, loss = 0.02269448
Iteration 77, loss = 0.02210565
Iteration 78, loss = 0.02153034
Iteration 79, loss = 0.02098879
Iteration 80, loss = 0.02045932
Iteration 81, loss = 0.01996659
Iteration 82, loss = 0.01947999
Iteration 83, loss = 0.01901113
Iteration 84, loss = 0.01855990
Iteration 85, loss = 0.01813148
Iteration 86, loss = 0.01770232
Iteration 87, loss = 0.01731325
Iteration 88, loss = 0.01691936
Iteration 89, loss = 0.01654688
Iteration 90, loss = 0.01618100
Iteration 91, loss = 0.01583580
Iteration 92, loss = 0.01549652
Iteration 93, loss = 0.01518474
Iteration 94, loss = 0.01486847
Iteration 95, loss = 0.01456755
Iteration 96, loss = 0.01427029
Iteration 97, loss = 0.01399036
Iteration 98, loss = 0.01372311
Iteration 99, loss = 0.01345669
Iteration 100, loss = 0.01319361
Iteration 101, loss = 0.01293038
Iteration 102, loss = 0.01268953
Iteration 103, loss = 0.01245792
Iteration 104, loss = 0.01222011
Iteration 105, loss = 0.01199813
Iteration 106, loss = 0.01179040
Iteration 107, loss = 0.01157995
Iteration 108, loss = 0.01136966
Iteration 109, loss = 0.01117190
Iteration 110, loss = 0.01098203
Iteration 111, loss = 0.01080002
Iteration 112, loss = 0.01061615
Iteration 113, loss = 0.01043466
Iteration 114, loss = 0.01026165
Iteration 115, loss = 0.01009420
Iteration 116, loss = 0.00992695
Iteration 117, loss = 0.00976757
Iteration 118, loss = 0.00961246
Iteration 119, loss = 0.00946587
Iteration 120, loss = 0.00932053
Iteration 121, loss = 0.00917320
Iteration 122, loss = 0.00904069
Iteration 123, loss = 0.00890631
Iteration 124, loss = 0.00876679
Iteration 125, loss = 0.00864202
Iteration 126, loss = 0.00852092
Iteration 127, loss = 0.00838930
Iteration 128, loss = 0.00826838
Iteration 129, loss = 0.00814846
Iteration 130, loss = 0.00803346
Iteration 131, loss = 0.00792049
Iteration 132, loss = 0.00781159
Iteration 133, loss = 0.00770112
Iteration 134, loss = 0.00759860
Iteration 135, loss = 0.00749449
Iteration 136, loss = 0.00739349
Iteration 137, loss = 0.00729504
Iteration 138, loss = 0.00721001
Iteration 139, loss = 0.00711243
Iteration 140, loss = 0.00701763
Iteration 141, loss = 0.00692634
Iteration 142, loss = 0.00683727
Iteration 143, loss = 0.00675190
Iteration 144, loss = 0.00667022
Iteration 145, loss = 0.00658674
Iteration 146, loss = 0.00650748
Iteration 147, loss = 0.00642716
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68562020
Iteration 2, loss = 0.64654761
Iteration 3, loss = 0.59412832
Iteration 4, loss = 0.52896449
Iteration 5, loss = 0.46041381
Iteration 6, loss = 0.39437921
Iteration 7, loss = 0.33397742
Iteration 8, loss = 0.28091234
Iteration 9, loss = 0.23588015
Iteration 10, loss = 0.19859952
Iteration 11, loss = 0.16821668
Iteration 12, loss = 0.14365525
Iteration 13, loss = 0.12392163
Iteration 14, loss = 0.10793393
Iteration 15, loss = 0.09510275
Iteration 16, loss = 0.08450630
Iteration 17, loss = 0.07592132
Iteration 18, loss = 0.06877079
Iteration 19, loss = 0.06266758
Iteration 20, loss = 0.05755903
Iteration 21, loss = 0.05338116
Iteration 22, loss = 0.04958505
Iteration 23, loss = 0.04631161
Iteration 24, loss = 0.04372283
Iteration 25, loss = 0.04122645
Iteration 26, loss = 0.03904801
Iteration 27, loss = 0.03715350
Iteration 28, loss = 0.03552816
Iteration 29, loss = 0.03415388
Iteration 30, loss = 0.03270613
Iteration 31, loss = 0.03156090
Iteration 32, loss = 0.03053806
Iteration 33, loss = 0.02961080
Iteration 34, loss = 0.02863693
Iteration 35, loss = 0.02774556
Iteration 36, loss = 0.02717022
Iteration 37, loss = 0.02640173
Iteration 38, loss = 0.02596053
Iteration 39, loss = 0.02523837
Iteration 40, loss = 0.02475666
Iteration 41, loss = 0.02419867
Iteration 42, loss = 0.02377973
Iteration 43, loss = 0.02335929
Iteration 44, loss = 0.02299831
Iteration 45, loss = 0.02271736
Iteration 46, loss = 0.02228113
Iteration 47, loss = 0.02192760
Iteration 48, loss = 0.02171862
Iteration 49, loss = 0.02147197
Iteration 50, loss = 0.02122084
Iteration 51, loss = 0.02086814
Iteration 52, loss = 0.02069307
Iteration 53, loss = 0.02055622
Iteration 54, loss = 0.02030837
Iteration 55, loss = 0.02007075
Iteration 56, loss = 0.01994558
Iteration 57, loss = 0.01969319
Iteration 58, loss = 0.01964129
Iteration 59, loss = 0.01949606
Iteration 60, loss = 0.01940481
Iteration 61, loss = 0.01916237
Iteration 62, loss = 0.01904638
Iteration 63, loss = 0.01890384
Iteration 64, loss = 0.01882648
Iteration 65, loss = 0.01883413
Iteration 66, loss = 0.01852351
Iteration 67, loss = 0.01856921
Iteration 68, loss = 0.01826631
Iteration 69, loss = 0.01826861
Iteration 70, loss = 0.01813909
Iteration 71, loss = 0.01812588
Iteration 72, loss = 0.01794072
Iteration 73, loss = 0.01798886
Iteration 74, loss = 0.01791457
Iteration 75, loss = 0.01772765
Iteration 76, loss = 0.01782201
Iteration 77, loss = 0.01764656
Iteration 78, loss = 0.01768521
Iteration 79, loss = 0.01745215
Iteration 80, loss = 0.01756073
Iteration 81, loss = 0.01744165
Iteration 82, loss = 0.01727464
Iteration 83, loss = 0.01730539
Iteration 84, loss = 0.01724523
Iteration 85, loss = 0.01718666
Iteration 86, loss = 0.01721914
Iteration 87, loss = 0.01727973
Iteration 88, loss = 0.01735789
Iteration 89, loss = 0.01728582
Iteration 90, loss = 0.01700068
Iteration 91, loss = 0.01695278
Iteration 92, loss = 0.01694598
Iteration 93, loss = 0.01690797
Iteration 94, loss = 0.01686294
Iteration 95, loss = 0.01684024
Iteration 96, loss = 0.01676856
Iteration 97, loss = 0.01683681
Iteration 98, loss = 0.01675130
Iteration 99, loss = 0.01679633
Iteration 100, loss = 0.01663009
Iteration 101, loss = 0.01655586
Iteration 102, loss = 0.01664194
Iteration 103, loss = 0.01655318
Iteration 104, loss = 0.01664013
Iteration 105, loss = 0.01657450
Iteration 106, loss = 0.01656705
Iteration 107, loss = 0.01659193
Iteration 108, loss = 0.01647033
Iteration 109, loss = 0.01650605
Iteration 110, loss = 0.01633659
Iteration 111, loss = 0.01644324
Iteration 112, loss = 0.01639281
Iteration 113, loss = 0.01625774
Iteration 114, loss = 0.01638453
Iteration 115, loss = 0.01627735
Iteration 116, loss = 0.01628798
Iteration 117, loss = 0.01626426
Iteration 118, loss = 0.01620549
Iteration 119, loss = 0.01628925
Iteration 120, loss = 0.01626947
Iteration 121, loss = 0.01620922
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67310595
Iteration 2, loss = 0.65468703
Iteration 3, loss = 0.63920584
Iteration 4, loss = 0.62394228
Iteration 5, loss = 0.60791758
Iteration 6, loss = 0.59072349
Iteration 7, loss = 0.57206519
Iteration 8, loss = 0.55216257
Iteration 9, loss = 0.53100796
Iteration 10, loss = 0.50878497
Iteration 11, loss = 0.48569534
Iteration 12, loss = 0.46226198
Iteration 13, loss = 0.43860316
Iteration 14, loss = 0.41506887
Iteration 15, loss = 0.39205227
Iteration 16, loss = 0.36965631
Iteration 17, loss = 0.34809263
Iteration 18, loss = 0.32740390
Iteration 19, loss = 0.30778558
Iteration 20, loss = 0.28914192
Iteration 21, loss = 0.27142653
Iteration 22, loss = 0.25489303
Iteration 23, loss = 0.23926426
Iteration 24, loss = 0.22466383
Iteration 25, loss = 0.21091266
Iteration 26, loss = 0.19812173
Iteration 27, loss = 0.18611460
Iteration 28, loss = 0.17499095
Iteration 29, loss = 0.16464888
Iteration 30, loss = 0.15498374
Iteration 31, loss = 0.14601603
Iteration 32, loss = 0.13764169
Iteration 33, loss = 0.13002470
Iteration 34, loss = 0.12275811
Iteration 35, loss = 0.11612956
Iteration 36, loss = 0.10996923
Iteration 37, loss = 0.10425134
Iteration 38, loss = 0.09900909
Iteration 39, loss = 0.09404274
Iteration 40, loss = 0.08950592
Iteration 41, loss = 0.08517915
Iteration 42, loss = 0.08129401
Iteration 43, loss = 0.07761796
Iteration 44, loss = 0.07414071
Iteration 45, loss = 0.07097282
Iteration 46, loss = 0.06801511
Iteration 47, loss = 0.06520517
Iteration 48, loss = 0.06258343
Iteration 49, loss = 0.06014480
Iteration 50, loss = 0.05788016
Iteration 51, loss = 0.05566720
Iteration 52, loss = 0.05366078
Iteration 53, loss = 0.05174986
Iteration 54, loss = 0.04993268
Iteration 55, loss = 0.04821101
Iteration 56, loss = 0.04662426
Iteration 57, loss = 0.04508975
Iteration 58, loss = 0.04368307
Iteration 59, loss = 0.04230364
Iteration 60, loss = 0.04104243
Iteration 61, loss = 0.03982309
Iteration 62, loss = 0.03867553
Iteration 63, loss = 0.03759446
Iteration 64, loss = 0.03652693
Iteration 65, loss = 0.03558026
Iteration 66, loss = 0.03461948
Iteration 67, loss = 0.03371117
Iteration 68, loss = 0.03288315
Iteration 69, loss = 0.03207838
Iteration 70, loss = 0.03128303
Iteration 71, loss = 0.03056437
Iteration 72, loss = 0.02986416
Iteration 73, loss = 0.02920374
Iteration 74, loss = 0.02852996
Iteration 75, loss = 0.02790258
Iteration 76, loss = 0.02733167
Iteration 77, loss = 0.02677263
Iteration 78, loss = 0.02624339
Iteration 79, loss = 0.02573616
Iteration 80, loss = 0.02525191
Iteration 81, loss = 0.02475306
Iteration 82, loss = 0.02426851
Iteration 83, loss = 0.02378631
Iteration 84, loss = 0.02337266
Iteration 85, loss = 0.02294932
Iteration 86, loss = 0.02251847
Iteration 87, loss = 0.02215617
Iteration 88, loss = 0.02175835
Iteration 89, loss = 0.02138829
Iteration 90, loss = 0.02104121
Iteration 91, loss = 0.02072524
Iteration 92, loss = 0.02040376
Iteration 93, loss = 0.02011494
Iteration 94, loss = 0.01981585
Iteration 95, loss = 0.01954642
Iteration 96, loss = 0.01924108
Iteration 97, loss = 0.01896545
Iteration 98, loss = 0.01873400
Iteration 99, loss = 0.01845543
Iteration 100, loss = 0.01820729
Iteration 101, loss = 0.01796831
Iteration 102, loss = 0.01777413
Iteration 103, loss = 0.01753399
Iteration 104, loss = 0.01729281
Iteration 105, loss = 0.01709711
Iteration 106, loss = 0.01690896
Iteration 107, loss = 0.01672415
Iteration 108, loss = 0.01651237
Iteration 109, loss = 0.01631541
Iteration 110, loss = 0.01613066
Iteration 111, loss = 0.01595206
Iteration 112, loss = 0.01576400
Iteration 113, loss = 0.01559444
Iteration 114, loss = 0.01544675
Iteration 115, loss = 0.01526793
Iteration 116, loss = 0.01510478
Iteration 117, loss = 0.01498311
Iteration 118, loss = 0.01484345
Iteration 119, loss = 0.01477113
Iteration 120, loss = 0.01462647
Iteration 121, loss = 0.01451765
Iteration 122, loss = 0.01438373
Iteration 123, loss = 0.01427179
Iteration 124, loss = 0.01413912
Iteration 125, loss = 0.01403020
Iteration 126, loss = 0.01389951
Iteration 127, loss = 0.01379757
Iteration 128, loss = 0.01364132
Iteration 129, loss = 0.01350792
Iteration 130, loss = 0.01339263
Iteration 131, loss = 0.01323046
Iteration 132, loss = 0.01314150
Iteration 133, loss = 0.01308326
Iteration 134, loss = 0.01296753
Iteration 135, loss = 0.01282349
Iteration 136, loss = 0.01271248
Iteration 137, loss = 0.01265577
Iteration 138, loss = 0.01254048
Iteration 139, loss = 0.01244432
Iteration 140, loss = 0.01235491
Iteration 141, loss = 0.01228547
Iteration 142, loss = 0.01223637
Iteration 143, loss = 0.01211409
Iteration 144, loss = 0.01207510
Iteration 145, loss = 0.01195961
Iteration 146, loss = 0.01189286
Iteration 147, loss = 0.01183310
Iteration 148, loss = 0.01173698
Iteration 149, loss = 0.01168808
Iteration 150, loss = 0.01161693
Iteration 151, loss = 0.01156613
Iteration 152, loss = 0.01146951
Iteration 153, loss = 0.01142982
Iteration 154, loss = 0.01133965
Iteration 155, loss = 0.01127735
Iteration 156, loss = 0.01122899
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67608984
Iteration 2, loss = 0.62124068
Iteration 3, loss = 0.54386441
Iteration 4, loss = 0.45479634
Iteration 5, loss = 0.37052356
Iteration 6, loss = 0.29686714
Iteration 7, loss = 0.23776865
Iteration 8, loss = 0.19160237
Iteration 9, loss = 0.15650731
Iteration 10, loss = 0.13000049
Iteration 11, loss = 0.10996646
Iteration 12, loss = 0.09452784
Iteration 13, loss = 0.08241240
Iteration 14, loss = 0.07283298
Iteration 15, loss = 0.06511406
Iteration 16, loss = 0.05898953
Iteration 17, loss = 0.05383161
Iteration 18, loss = 0.04951636
Iteration 19, loss = 0.04615904
Iteration 20, loss = 0.04298735
Iteration 21, loss = 0.04030544
Iteration 22, loss = 0.03805936
Iteration 23, loss = 0.03624489
Iteration 24, loss = 0.03461622
Iteration 25, loss = 0.03292950
Iteration 26, loss = 0.03167447
Iteration 27, loss = 0.03043254
Iteration 28, loss = 0.02929854
Iteration 29, loss = 0.02850398
Iteration 30, loss = 0.02788447
Iteration 31, loss = 0.02677885
Iteration 32, loss = 0.02616365
Iteration 33, loss = 0.02566493
Iteration 34, loss = 0.02498057
Iteration 35, loss = 0.02447363
Iteration 36, loss = 0.02390337
Iteration 37, loss = 0.02368304
Iteration 38, loss = 0.02331122
Iteration 39, loss = 0.02299370
Iteration 40, loss = 0.02256483
Iteration 41, loss = 0.02219395
Iteration 42, loss = 0.02174481
Iteration 43, loss = 0.02161201
Iteration 44, loss = 0.02135833
Iteration 45, loss = 0.02106728
Iteration 46, loss = 0.02097758
Iteration 47, loss = 0.02086157
Iteration 48, loss = 0.02064064
Iteration 49, loss = 0.02047329
Iteration 50, loss = 0.02027040
Iteration 51, loss = 0.02013317
Iteration 52, loss = 0.02007994
Iteration 53, loss = 0.01968745
Iteration 54, loss = 0.01946594
Iteration 55, loss = 0.01950756
Iteration 56, loss = 0.01935748
Iteration 57, loss = 0.01918084
Iteration 58, loss = 0.01924776
Iteration 59, loss = 0.01940156
Iteration 60, loss = 0.01884752
Iteration 61, loss = 0.01884479
Iteration 62, loss = 0.01886302
Iteration 63, loss = 0.01861186
Iteration 64, loss = 0.01869173
Iteration 65, loss = 0.01874667
Iteration 66, loss = 0.01854102
Iteration 67, loss = 0.01858111
Iteration 68, loss = 0.01838714
Iteration 69, loss = 0.01840993
Iteration 70, loss = 0.01851734
Iteration 71, loss = 0.01815916
Iteration 72, loss = 0.01821740
Iteration 73, loss = 0.01823622
Iteration 74, loss = 0.01819454
Iteration 75, loss = 0.01796608
Iteration 76, loss = 0.01802849
Iteration 77, loss = 0.01795444
Iteration 78, loss = 0.01792940
Iteration 79, loss = 0.01794447
Iteration 80, loss = 0.01783382
Iteration 81, loss = 0.01805208
Iteration 82, loss = 0.01774299
Iteration 83, loss = 0.01768465
Iteration 84, loss = 0.01779322
Iteration 85, loss = 0.01763530
Iteration 86, loss = 0.01755012
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67718773
Iteration 2, loss = 0.60933251
Iteration 3, loss = 0.50220342
Iteration 4, loss = 0.38973322
Iteration 5, loss = 0.29167733
Iteration 6, loss = 0.21665341
Iteration 7, loss = 0.16447986
Iteration 8, loss = 0.12899464
Iteration 9, loss = 0.10467696
Iteration 10, loss = 0.08738636
Iteration 11, loss = 0.07494832
Iteration 12, loss = 0.06560487
Iteration 13, loss = 0.05833490
Iteration 14, loss = 0.05281996
Iteration 15, loss = 0.04830822
Iteration 16, loss = 0.04478329
Iteration 17, loss = 0.04199702
Iteration 18, loss = 0.03975816
Iteration 19, loss = 0.03738568
Iteration 20, loss = 0.03578036
Iteration 21, loss = 0.03415997
Iteration 22, loss = 0.03309872
Iteration 23, loss = 0.03185479
Iteration 24, loss = 0.03082168
Iteration 25, loss = 0.03009000
Iteration 26, loss = 0.02950694
Iteration 27, loss = 0.02877701
Iteration 28, loss = 0.02810999
Iteration 29, loss = 0.02765276
Iteration 30, loss = 0.02730501
Iteration 31, loss = 0.02667482
Iteration 32, loss = 0.02653023
Iteration 33, loss = 0.02602669
Iteration 34, loss = 0.02556016
Iteration 35, loss = 0.02546080
Iteration 36, loss = 0.02525349
Iteration 37, loss = 0.02499107
Iteration 38, loss = 0.02460075
Iteration 39, loss = 0.02452899
Iteration 40, loss = 0.02423825
Iteration 41, loss = 0.02421133
Iteration 42, loss = 0.02397124
Iteration 43, loss = 0.02388263
Iteration 44, loss = 0.02387920
Iteration 45, loss = 0.02351158
Iteration 46, loss = 0.02353120
Iteration 47, loss = 0.02339334
Iteration 48, loss = 0.02345047
Iteration 49, loss = 0.02328188
Iteration 50, loss = 0.02307707
Iteration 51, loss = 0.02290674
Iteration 52, loss = 0.02303256
Iteration 53, loss = 0.02286817
Iteration 54, loss = 0.02319546
Iteration 55, loss = 0.02262659
Iteration 56, loss = 0.02254098
Iteration 57, loss = 0.02233155
Iteration 58, loss = 0.02256167
Iteration 59, loss = 0.02247444
Iteration 60, loss = 0.02233188
Iteration 61, loss = 0.02254281
Iteration 62, loss = 0.02241986
Iteration 63, loss = 0.02254454
Iteration 64, loss = 0.02238337
Iteration 65, loss = 0.02232817
Iteration 66, loss = 0.02227168
Iteration 67, loss = 0.02235542
Iteration 68, loss = 0.02187870
Iteration 69, loss = 0.02220786
Iteration 70, loss = 0.02187072
Iteration 71, loss = 0.02181160
Iteration 72, loss = 0.02210190
Iteration 73, loss = 0.02170427
Iteration 74, loss = 0.02203638
Iteration 75, loss = 0.02195073
Iteration 76, loss = 0.02166144
Iteration 77, loss = 0.02188317
Iteration 78, loss = 0.02168901
Iteration 79, loss = 0.02206473
Iteration 80, loss = 0.02159683
Iteration 81, loss = 0.02160490
Iteration 82, loss = 0.02171449
Iteration 83, loss = 0.02166051
Iteration 84, loss = 0.02153351
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67625793
Iteration 2, loss = 0.59859405
Iteration 3, loss = 0.49089414
Iteration 4, loss = 0.38355580
Iteration 5, loss = 0.29056388
Iteration 6, loss = 0.21914865
Iteration 7, loss = 0.16809431
Iteration 8, loss = 0.13275010
Iteration 9, loss = 0.10796518
Iteration 10, loss = 0.09051699
Iteration 11, loss = 0.07752856
Iteration 12, loss = 0.06800839
Iteration 13, loss = 0.06090455
Iteration 14, loss = 0.05547587
Iteration 15, loss = 0.05042154
Iteration 16, loss = 0.04693811
Iteration 17, loss = 0.04378981
Iteration 18, loss = 0.04153665
Iteration 19, loss = 0.03944314
Iteration 20, loss = 0.03774783
Iteration 21, loss = 0.03618198
Iteration 22, loss = 0.03514829
Iteration 23, loss = 0.03378603
Iteration 24, loss = 0.03300322
Iteration 25, loss = 0.03195378
Iteration 26, loss = 0.03111472
Iteration 27, loss = 0.03083590
Iteration 28, loss = 0.03025384
Iteration 29, loss = 0.02968000
Iteration 30, loss = 0.02892834
Iteration 31, loss = 0.02860489
Iteration 32, loss = 0.02847880
Iteration 33, loss = 0.02792527
Iteration 34, loss = 0.02782836
Iteration 35, loss = 0.02739624
Iteration 36, loss = 0.02709766
Iteration 37, loss = 0.02679211
Iteration 38, loss = 0.02679357
Iteration 39, loss = 0.02658101
Iteration 40, loss = 0.02621511
Iteration 41, loss = 0.02607891
Iteration 42, loss = 0.02597887
Iteration 43, loss = 0.02577272
Iteration 44, loss = 0.02553093
Iteration 45, loss = 0.02562357
Iteration 46, loss = 0.02546257
Iteration 47, loss = 0.02507707
Iteration 48, loss = 0.02542933
Iteration 49, loss = 0.02501466
Iteration 50, loss = 0.02514427
Iteration 51, loss = 0.02512183
Iteration 52, loss = 0.02480069
Iteration 53, loss = 0.02457434
Iteration 54, loss = 0.02437648
Iteration 55, loss = 0.02472775
Iteration 56, loss = 0.02430345
Iteration 57, loss = 0.02458017
Iteration 58, loss = 0.02493035
Iteration 59, loss = 0.02426211
Iteration 60, loss = 0.02440781
Iteration 61, loss = 0.02469701
Iteration 62, loss = 0.02425969
Iteration 63, loss = 0.02445069
Iteration 64, loss = 0.02408182
Iteration 65, loss = 0.02404133
Iteration 66, loss = 0.02407081
Iteration 67, loss = 0.02425415
Iteration 68, loss = 0.02390687
Iteration 69, loss = 0.02413344
Iteration 70, loss = 0.02396910
Iteration 71, loss = 0.02395949
Iteration 72, loss = 0.02383952
Iteration 73, loss = 0.02407597
Iteration 74, loss = 0.02400115
Iteration 75, loss = 0.02374866
Iteration 76, loss = 0.02341671
Iteration 77, loss = 0.02376919
Iteration 78, loss = 0.02407301
Iteration 79, loss = 0.02375408
Iteration 80, loss = 0.02409318
Iteration 81, loss = 0.02380423
Iteration 82, loss = 0.02335286
Iteration 83, loss = 0.02380039
Iteration 84, loss = 0.02357568
Iteration 85, loss = 0.02351289
Iteration 86, loss = 0.02362213
Iteration 87, loss = 0.02374975
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68162736
Iteration 2, loss = 0.60626138
Iteration 3, loss = 0.50207957
Iteration 4, loss = 0.39584249
Iteration 5, loss = 0.30366005
Iteration 6, loss = 0.23118499
Iteration 7, loss = 0.17796617
Iteration 8, loss = 0.14056667
Iteration 9, loss = 0.11375345
Iteration 10, loss = 0.09436164
Iteration 11, loss = 0.08010359
Iteration 12, loss = 0.06967749
Iteration 13, loss = 0.06127357
Iteration 14, loss = 0.05500125
Iteration 15, loss = 0.04983558
Iteration 16, loss = 0.04544741
Iteration 17, loss = 0.04227306
Iteration 18, loss = 0.03936949
Iteration 19, loss = 0.03703566
Iteration 20, loss = 0.03493162
Iteration 21, loss = 0.03337108
Iteration 22, loss = 0.03165475
Iteration 23, loss = 0.03076691
Iteration 24, loss = 0.02934733
Iteration 25, loss = 0.02835647
Iteration 26, loss = 0.02741744
Iteration 27, loss = 0.02666141
Iteration 28, loss = 0.02612243
Iteration 29, loss = 0.02544996
Iteration 30, loss = 0.02482740
Iteration 31, loss = 0.02450397
Iteration 32, loss = 0.02380505
Iteration 33, loss = 0.02332545
Iteration 34, loss = 0.02312230
Iteration 35, loss = 0.02267436
Iteration 36, loss = 0.02227625
Iteration 37, loss = 0.02183328
Iteration 38, loss = 0.02198081
Iteration 39, loss = 0.02175171
Iteration 40, loss = 0.02138842
Iteration 41, loss = 0.02104456
Iteration 42, loss = 0.02093059
Iteration 43, loss = 0.02050982
Iteration 44, loss = 0.02061414
Iteration 45, loss = 0.02047454
Iteration 46, loss = 0.02026604
Iteration 47, loss = 0.02004635
Iteration 48, loss = 0.02010116
Iteration 49, loss = 0.01955675
Iteration 50, loss = 0.01965284
Iteration 51, loss = 0.01945761
Iteration 52, loss = 0.01952119
Iteration 53, loss = 0.01957709
Iteration 54, loss = 0.01943370
Iteration 55, loss = 0.01947092
Iteration 56, loss = 0.01901439
Iteration 57, loss = 0.01896264
Iteration 58, loss = 0.01912383
Iteration 59, loss = 0.01895341
Iteration 60, loss = 0.01902383
Iteration 61, loss = 0.01876504
Iteration 62, loss = 0.01872728
Iteration 63, loss = 0.01875496
Iteration 64, loss = 0.01877791
Iteration 65, loss = 0.01845384
Iteration 66, loss = 0.01871966
Iteration 67, loss = 0.01848345
Iteration 68, loss = 0.01837457
Iteration 69, loss = 0.01848031
Iteration 70, loss = 0.01869729
Iteration 71, loss = 0.01834312
Iteration 72, loss = 0.01827194
Iteration 73, loss = 0.01828844
Iteration 74, loss = 0.01811375
Iteration 75, loss = 0.01826153
Iteration 76, loss = 0.01822359
Iteration 77, loss = 0.01812523
Iteration 78, loss = 0.01802325
Iteration 79, loss = 0.01826457
Iteration 80, loss = 0.01795264
Iteration 81, loss = 0.01787808
Iteration 82, loss = 0.01778653
Iteration 83, loss = 0.01787471
Iteration 84, loss = 0.01791285
Iteration 85, loss = 0.01796208
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69010308
Iteration 2, loss = 0.66206893
Iteration 3, loss = 0.62998100
Iteration 4, loss = 0.59009233
Iteration 5, loss = 0.54355512
Iteration 6, loss = 0.49274576
Iteration 7, loss = 0.44115888
Iteration 8, loss = 0.39203808
Iteration 9, loss = 0.34647441
Iteration 10, loss = 0.30508239
Iteration 11, loss = 0.26835415
Iteration 12, loss = 0.23587682
Iteration 13, loss = 0.20755695
Iteration 14, loss = 0.18350675
Iteration 15, loss = 0.16274511
Iteration 16, loss = 0.14523336
Iteration 17, loss = 0.13027347
Iteration 18, loss = 0.11727530
Iteration 19, loss = 0.10641835
Iteration 20, loss = 0.09697824
Iteration 21, loss = 0.08882617
Iteration 22, loss = 0.08188841
Iteration 23, loss = 0.07580208
Iteration 24, loss = 0.07044933
Iteration 25, loss = 0.06582330
Iteration 26, loss = 0.06164160
Iteration 27, loss = 0.05790163
Iteration 28, loss = 0.05460040
Iteration 29, loss = 0.05168105
Iteration 30, loss = 0.04907743
Iteration 31, loss = 0.04683338
Iteration 32, loss = 0.04472131
Iteration 33, loss = 0.04265618
Iteration 34, loss = 0.04097926
Iteration 35, loss = 0.03934099
Iteration 36, loss = 0.03799130
Iteration 37, loss = 0.03641759
Iteration 38, loss = 0.03511396
Iteration 39, loss = 0.03391354
Iteration 40, loss = 0.03286034
Iteration 41, loss = 0.03197560
Iteration 42, loss = 0.03108613
Iteration 43, loss = 0.03023866
Iteration 44, loss = 0.02935704
Iteration 45, loss = 0.02860991
Iteration 46, loss = 0.02796694
Iteration 47, loss = 0.02725768
Iteration 48, loss = 0.02667353
Iteration 49, loss = 0.02636522
Iteration 50, loss = 0.02598340
Iteration 51, loss = 0.02568900
Iteration 52, loss = 0.02511410
Iteration 53, loss = 0.02465145
Iteration 54, loss = 0.02416915
Iteration 55, loss = 0.02372175
Iteration 56, loss = 0.02356449
Iteration 57, loss = 0.02329336
Iteration 58, loss = 0.02287647
Iteration 59, loss = 0.02237826
Iteration 60, loss = 0.02195937
Iteration 61, loss = 0.02163200
Iteration 62, loss = 0.02143891
Iteration 63, loss = 0.02104309
Iteration 64, loss = 0.02095205
Iteration 65, loss = 0.02063356
Iteration 66, loss = 0.02058137
Iteration 67, loss = 0.02030009
Iteration 68, loss = 0.02003286
Iteration 69, loss = 0.02000153
Iteration 70, loss = 0.01978283
Iteration 71, loss = 0.01965058
Iteration 72, loss = 0.01932200
Iteration 73, loss = 0.01912020
Iteration 74, loss = 0.01906617
Iteration 75, loss = 0.01893932
Iteration 76, loss = 0.01879629
Iteration 77, loss = 0.01865851
Iteration 78, loss = 0.01839358
Iteration 79, loss = 0.01830799
Iteration 80, loss = 0.01810411
Iteration 81, loss = 0.01800007
Iteration 82, loss = 0.01794535
Iteration 83, loss = 0.01774045
Iteration 84, loss = 0.01761381
Iteration 85, loss = 0.01755871
Iteration 86, loss = 0.01748605
Iteration 87, loss = 0.01742634
Iteration 88, loss = 0.01728962
Iteration 89, loss = 0.01718603
Iteration 90, loss = 0.01712771
Iteration 91, loss = 0.01702414
Iteration 92, loss = 0.01698568
Iteration 93, loss = 0.01693768
Iteration 94, loss = 0.01675404
Iteration 95, loss = 0.01708244
Iteration 96, loss = 0.01699408
Iteration 97, loss = 0.01730975
Iteration 98, loss = 0.01702000
Iteration 99, loss = 0.01698239
Iteration 100, loss = 0.01692007
Iteration 101, loss = 0.01668419
Iteration 102, loss = 0.01637987
Iteration 103, loss = 0.01626546
Iteration 104, loss = 0.01604749
Iteration 105, loss = 0.01676654
Iteration 106, loss = 0.01671107
Iteration 107, loss = 0.01663045
Iteration 108, loss = 0.01642377
Iteration 109, loss = 0.01625033
Iteration 110, loss = 0.01620431
Iteration 111, loss = 0.01589514
Iteration 112, loss = 0.01622777
Iteration 113, loss = 0.01604756
Iteration 114, loss = 0.01596791
Iteration 115, loss = 0.01582342
Iteration 116, loss = 0.01590788
Iteration 117, loss = 0.01608200
Iteration 118, loss = 0.01606689
Iteration 119, loss = 0.01602664
Iteration 120, loss = 0.01627402
Iteration 121, loss = 0.01631009
Iteration 122, loss = 0.01620839
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69612550
Iteration 2, loss = 0.63500558
Iteration 3, loss = 0.54274776
Iteration 4, loss = 0.43444117
Iteration 5, loss = 0.33533693
Iteration 6, loss = 0.25463417
Iteration 7, loss = 0.19425304
Iteration 8, loss = 0.15149876
Iteration 9, loss = 0.12141134
Iteration 10, loss = 0.09975572
Iteration 11, loss = 0.08425166
Iteration 12, loss = 0.07236042
Iteration 13, loss = 0.06344851
Iteration 14, loss = 0.05643832
Iteration 15, loss = 0.05096992
Iteration 16, loss = 0.04656573
Iteration 17, loss = 0.04284566
Iteration 18, loss = 0.03998734
Iteration 19, loss = 0.03758224
Iteration 20, loss = 0.03540885
Iteration 21, loss = 0.03353139
Iteration 22, loss = 0.03208342
Iteration 23, loss = 0.03084336
Iteration 24, loss = 0.02950838
Iteration 25, loss = 0.02846322
Iteration 26, loss = 0.02740843
Iteration 27, loss = 0.02678351
Iteration 28, loss = 0.02609922
Iteration 29, loss = 0.02531180
Iteration 30, loss = 0.02513530
Iteration 31, loss = 0.02439535
Iteration 32, loss = 0.02393586
Iteration 33, loss = 0.02353019
Iteration 34, loss = 0.02318156
Iteration 35, loss = 0.02262637
Iteration 36, loss = 0.02247614
Iteration 37, loss = 0.02211785
Iteration 38, loss = 0.02192110
Iteration 39, loss = 0.02182444
Iteration 40, loss = 0.02143048
Iteration 41, loss = 0.02107971
Iteration 42, loss = 0.02088872
Iteration 43, loss = 0.02070207
Iteration 44, loss = 0.02063077
Iteration 45, loss = 0.02043695
Iteration 46, loss = 0.02029686
Iteration 47, loss = 0.02011849
Iteration 48, loss = 0.02012877
Iteration 49, loss = 0.02008625
Iteration 50, loss = 0.01987869
Iteration 51, loss = 0.01976709
Iteration 52, loss = 0.01969844
Iteration 53, loss = 0.01914619
Iteration 54, loss = 0.01932515
Iteration 55, loss = 0.01939997
Iteration 56, loss = 0.01914564
Iteration 57, loss = 0.01912618
Iteration 58, loss = 0.01917261
Iteration 59, loss = 0.01903691
Iteration 60, loss = 0.01896488
Iteration 61, loss = 0.01885733
Iteration 62, loss = 0.01883120
Iteration 63, loss = 0.01877509
Iteration 64, loss = 0.01874124
Iteration 65, loss = 0.01886800
Iteration 66, loss = 0.01888396
Iteration 67, loss = 0.01853963
Iteration 68, loss = 0.01847446
Iteration 69, loss = 0.01843341
Iteration 70, loss = 0.01851641
Iteration 71, loss = 0.01840496
Iteration 72, loss = 0.01833635
Iteration 73, loss = 0.01862260
Iteration 74, loss = 0.01819051
Iteration 75, loss = 0.01827352
Iteration 76, loss = 0.01843513
Iteration 77, loss = 0.01829058
Iteration 78, loss = 0.01795562
Iteration 79, loss = 0.01824506
Iteration 80, loss = 0.01808181
Iteration 81, loss = 0.01821433
Iteration 82, loss = 0.01793525
Iteration 83, loss = 0.01817496
Iteration 84, loss = 0.01817281
Iteration 85, loss = 0.01803228
Iteration 86, loss = 0.01818599
Iteration 87, loss = 0.01817373
Iteration 88, loss = 0.01791106
Iteration 89, loss = 0.01803701
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68079145
Iteration 2, loss = 0.64558355
Iteration 3, loss = 0.60251339
Iteration 4, loss = 0.54870333
Iteration 5, loss = 0.48851934
Iteration 6, loss = 0.42766577
Iteration 7, loss = 0.37015791
Iteration 8, loss = 0.31730863
Iteration 9, loss = 0.27076117
Iteration 10, loss = 0.23098643
Iteration 11, loss = 0.19718017
Iteration 12, loss = 0.16933746
Iteration 13, loss = 0.14636482
Iteration 14, loss = 0.12744807
Iteration 15, loss = 0.11188795
Iteration 16, loss = 0.09922845
Iteration 17, loss = 0.08852313
Iteration 18, loss = 0.07965268
Iteration 19, loss = 0.07217454
Iteration 20, loss = 0.06594502
Iteration 21, loss = 0.06035279
Iteration 22, loss = 0.05577316
Iteration 23, loss = 0.05180762
Iteration 24, loss = 0.04842185
Iteration 25, loss = 0.04522307
Iteration 26, loss = 0.04255489
Iteration 27, loss = 0.04011083
Iteration 28, loss = 0.03804429
Iteration 29, loss = 0.03612272
Iteration 30, loss = 0.03447636
Iteration 31, loss = 0.03293622
Iteration 32, loss = 0.03150737
Iteration 33, loss = 0.03029520
Iteration 34, loss = 0.02920667
Iteration 35, loss = 0.02814254
Iteration 36, loss = 0.02727249
Iteration 37, loss = 0.02638466
Iteration 38, loss = 0.02562669
Iteration 39, loss = 0.02483250
Iteration 40, loss = 0.02421805
Iteration 41, loss = 0.02352089
Iteration 42, loss = 0.02312375
Iteration 43, loss = 0.02250288
Iteration 44, loss = 0.02192320
Iteration 45, loss = 0.02149703
Iteration 46, loss = 0.02111450
Iteration 47, loss = 0.02063788
Iteration 48, loss = 0.02028751
Iteration 49, loss = 0.01996703
Iteration 50, loss = 0.01963559
Iteration 51, loss = 0.01932479
Iteration 52, loss = 0.01899219
Iteration 53, loss = 0.01891435
Iteration 54, loss = 0.01863247
Iteration 55, loss = 0.01823490
Iteration 56, loss = 0.01805979
Iteration 57, loss = 0.01776452
Iteration 58, loss = 0.01767986
Iteration 59, loss = 0.01741372
Iteration 60, loss = 0.01725183
Iteration 61, loss = 0.01711366
Iteration 62, loss = 0.01698722
Iteration 63, loss = 0.01666711
Iteration 64, loss = 0.01655495
Iteration 65, loss = 0.01648374
Iteration 66, loss = 0.01628138
Iteration 67, loss = 0.01616998
Iteration 68, loss = 0.01601804
Iteration 69, loss = 0.01591994
Iteration 70, loss = 0.01573597
Iteration 71, loss = 0.01558961
Iteration 72, loss = 0.01558049
Iteration 73, loss = 0.01538904
Iteration 74, loss = 0.01531946
Iteration 75, loss = 0.01528106
Iteration 76, loss = 0.01518526
Iteration 77, loss = 0.01503542
Iteration 78, loss = 0.01506049
Iteration 79, loss = 0.01491802
Iteration 80, loss = 0.01481272
Iteration 81, loss = 0.01479140
Iteration 82, loss = 0.01467161
Iteration 83, loss = 0.01468668
Iteration 84, loss = 0.01461995
Iteration 85, loss = 0.01468890
Iteration 86, loss = 0.01457228
Iteration 87, loss = 0.01440902
Iteration 88, loss = 0.01435817
Iteration 89, loss = 0.01436922
Iteration 90, loss = 0.01422732
Iteration 91, loss = 0.01417867
Iteration 92, loss = 0.01417709
Iteration 93, loss = 0.01415011
Iteration 94, loss = 0.01409748
Iteration 95, loss = 0.01399789
Iteration 96, loss = 0.01391844
Iteration 97, loss = 0.01397483
Iteration 98, loss = 0.01387790
Iteration 99, loss = 0.01378015
Iteration 100, loss = 0.01383607
Iteration 101, loss = 0.01384404
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68512646
Iteration 2, loss = 0.63176483
Iteration 3, loss = 0.55021712
Iteration 4, loss = 0.45272416
Iteration 5, loss = 0.36026145
Iteration 6, loss = 0.28367645
Iteration 7, loss = 0.22274739
Iteration 8, loss = 0.17652528
Iteration 9, loss = 0.14250704
Iteration 10, loss = 0.11746906
Iteration 11, loss = 0.09898297
Iteration 12, loss = 0.08513929
Iteration 13, loss = 0.07418459
Iteration 14, loss = 0.06601443
Iteration 15, loss = 0.05932375
Iteration 16, loss = 0.05416874
Iteration 17, loss = 0.04970416
Iteration 18, loss = 0.04612019
Iteration 19, loss = 0.04303654
Iteration 20, loss = 0.04048200
Iteration 21, loss = 0.03837680
Iteration 22, loss = 0.03661062
Iteration 23, loss = 0.03501341
Iteration 24, loss = 0.03372758
Iteration 25, loss = 0.03238525
Iteration 26, loss = 0.03104262
Iteration 27, loss = 0.03013076
Iteration 28, loss = 0.02934963
Iteration 29, loss = 0.02855425
Iteration 30, loss = 0.02783710
Iteration 31, loss = 0.02720509
Iteration 32, loss = 0.02653919
Iteration 33, loss = 0.02612370
Iteration 34, loss = 0.02548918
Iteration 35, loss = 0.02508925
Iteration 36, loss = 0.02470445
Iteration 37, loss = 0.02430765
Iteration 38, loss = 0.02404572
Iteration 39, loss = 0.02372270
Iteration 40, loss = 0.02337654
Iteration 41, loss = 0.02312000
Iteration 42, loss = 0.02283363
Iteration 43, loss = 0.02266981
Iteration 44, loss = 0.02251905
Iteration 45, loss = 0.02220137
Iteration 46, loss = 0.02208146
Iteration 47, loss = 0.02182090
Iteration 48, loss = 0.02166020
Iteration 49, loss = 0.02160162
Iteration 50, loss = 0.02137778
Iteration 51, loss = 0.02122241
Iteration 52, loss = 0.02104298
Iteration 53, loss = 0.02104708
Iteration 54, loss = 0.02089614
Iteration 55, loss = 0.02079529
Iteration 56, loss = 0.02083349
Iteration 57, loss = 0.02056584
Iteration 58, loss = 0.02175789
Iteration 59, loss = 0.02237414
Iteration 60, loss = 0.02168957
Iteration 61, loss = 0.02145578
Iteration 62, loss = 0.02125643
Iteration 63, loss = 0.02097588
Iteration 64, loss = 0.02084096
Iteration 65, loss = 0.02057551
Iteration 66, loss = 0.02038401
Iteration 67, loss = 0.02033146
Iteration 68, loss = 0.02009229
Iteration 69, loss = 0.02021519
Iteration 70, loss = 0.02005025
Iteration 71, loss = 0.01990567
Iteration 72, loss = 0.01965562
Iteration 73, loss = 0.01965033
Iteration 74, loss = 0.01958982
Iteration 75, loss = 0.01953172
Iteration 76, loss = 0.01932401
Iteration 77, loss = 0.01936255
Iteration 78, loss = 0.01933792
Iteration 79, loss = 0.01927597
Iteration 80, loss = 0.01940007
Iteration 81, loss = 0.01928585
Iteration 82, loss = 0.01912155
Iteration 83, loss = 0.01962975
Iteration 84, loss = 0.01928025
Iteration 85, loss = 0.01922603
Iteration 86, loss = 0.01921784
Iteration 87, loss = 0.01893406
Iteration 88, loss = 0.01890350
Iteration 89, loss = 0.01880480
Iteration 90, loss = 0.01882921
Iteration 91, loss = 0.01887868
Iteration 92, loss = 0.01877216
Iteration 93, loss = 0.01891443
Iteration 94, loss = 0.01872438
Iteration 95, loss = 0.01875496
Iteration 96, loss = 0.01864808
Iteration 97, loss = 0.01861549
Iteration 98, loss = 0.01869331
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69526231
Iteration 2, loss = 0.62382654
Iteration 3, loss = 0.51646062
Iteration 4, loss = 0.39853656
Iteration 5, loss = 0.29756560
Iteration 6, loss = 0.22162973
Iteration 7, loss = 0.16795790
Iteration 8, loss = 0.13137337
Iteration 9, loss = 0.10598360
Iteration 10, loss = 0.08818411
Iteration 11, loss = 0.07507540
Iteration 12, loss = 0.06540905
Iteration 13, loss = 0.05808677
Iteration 14, loss = 0.05221574
Iteration 15, loss = 0.04773897
Iteration 16, loss = 0.04390553
Iteration 17, loss = 0.04087443
Iteration 18, loss = 0.03836060
Iteration 19, loss = 0.03614626
Iteration 20, loss = 0.03432169
Iteration 21, loss = 0.03278450
Iteration 22, loss = 0.03160518
Iteration 23, loss = 0.03040086
Iteration 24, loss = 0.02959471
Iteration 25, loss = 0.02869721
Iteration 26, loss = 0.02782964
Iteration 27, loss = 0.02704991
Iteration 28, loss = 0.02644721
Iteration 29, loss = 0.02619851
Iteration 30, loss = 0.02548775
Iteration 31, loss = 0.02507494
Iteration 32, loss = 0.02453900
Iteration 33, loss = 0.02407474
Iteration 34, loss = 0.02397037
Iteration 35, loss = 0.02357243
Iteration 36, loss = 0.02329561
Iteration 37, loss = 0.02302136
Iteration 38, loss = 0.02279617
Iteration 39, loss = 0.02271910
Iteration 40, loss = 0.02239532
Iteration 41, loss = 0.02226621
Iteration 42, loss = 0.02247109
Iteration 43, loss = 0.02227329
Iteration 44, loss = 0.02184343
Iteration 45, loss = 0.02165675
Iteration 46, loss = 0.02125828
Iteration 47, loss = 0.02130086
Iteration 48, loss = 0.02117362
Iteration 49, loss = 0.02114632
Iteration 50, loss = 0.02093406
Iteration 51, loss = 0.02109120
Iteration 52, loss = 0.02090627
Iteration 53, loss = 0.02092905
Iteration 54, loss = 0.02042698
Iteration 55, loss = 0.02061513
Iteration 56, loss = 0.02046428
Iteration 57, loss = 0.02035804
Iteration 58, loss = 0.02034183
Iteration 59, loss = 0.02030221
Iteration 60, loss = 0.02027397
Iteration 61, loss = 0.02055511
Iteration 62, loss = 0.02011304
Iteration 63, loss = 0.02036171
Iteration 64, loss = 0.02025001
Iteration 65, loss = 0.01986323
Iteration 66, loss = 0.02008467
Iteration 67, loss = 0.02017646
Iteration 68, loss = 0.01974713
Iteration 69, loss = 0.01991304
Iteration 70, loss = 0.02016956
Iteration 71, loss = 0.01993520
Iteration 72, loss = 0.01978652
Iteration 73, loss = 0.01986553
Iteration 74, loss = 0.01978395
Iteration 75, loss = 0.01962229
Iteration 76, loss = 0.01964529
Iteration 77, loss = 0.01962848
Iteration 78, loss = 0.01948909
Iteration 79, loss = 0.01962386
Iteration 80, loss = 0.01974817
Iteration 81, loss = 0.01985227
Iteration 82, loss = 0.01960408
Iteration 83, loss = 0.01957462
Iteration 84, loss = 0.01936231
Iteration 85, loss = 0.01933975
Iteration 86, loss = 0.01944615
Iteration 87, loss = 0.01939459
Iteration 88, loss = 0.01935228
Iteration 89, loss = 0.01918777
Iteration 90, loss = 0.01933463
Iteration 91, loss = 0.01929693
Iteration 92, loss = 0.01906248
Iteration 93, loss = 0.01937715
Iteration 94, loss = 0.01938089
Iteration 95, loss = 0.01928401
Iteration 96, loss = 0.01915715
Iteration 97, loss = 0.01961340
Iteration 98, loss = 0.01943338
Iteration 99, loss = 0.01910399
Iteration 100, loss = 0.01923968
Iteration 101, loss = 0.01916472
Iteration 102, loss = 0.01900429
Iteration 103, loss = 0.01913814
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69609170
Iteration 2, loss = 0.63638590
Iteration 3, loss = 0.54658871
Iteration 4, loss = 0.43619868
Iteration 5, loss = 0.33369752
Iteration 6, loss = 0.25198372
Iteration 7, loss = 0.19205204
Iteration 8, loss = 0.14991288
Iteration 9, loss = 0.12082352
Iteration 10, loss = 0.10025317
Iteration 11, loss = 0.08497840
Iteration 12, loss = 0.07393906
Iteration 13, loss = 0.06542502
Iteration 14, loss = 0.05899660
Iteration 15, loss = 0.05394811
Iteration 16, loss = 0.04948020
Iteration 17, loss = 0.04638373
Iteration 18, loss = 0.04323553
Iteration 19, loss = 0.04098009
Iteration 20, loss = 0.03901107
Iteration 21, loss = 0.03716940
Iteration 22, loss = 0.03574841
Iteration 23, loss = 0.03454367
Iteration 24, loss = 0.03341664
Iteration 25, loss = 0.03252001
Iteration 26, loss = 0.03174601
Iteration 27, loss = 0.03087760
Iteration 28, loss = 0.03000334
Iteration 29, loss = 0.02961916
Iteration 30, loss = 0.02919620
Iteration 31, loss = 0.02857818
Iteration 32, loss = 0.02828276
Iteration 33, loss = 0.02762437
Iteration 34, loss = 0.02742735
Iteration 35, loss = 0.02701679
Iteration 36, loss = 0.02687854
Iteration 37, loss = 0.02642226
Iteration 38, loss = 0.02680298
Iteration 39, loss = 0.02592804
Iteration 40, loss = 0.02575953
Iteration 41, loss = 0.02560553
Iteration 42, loss = 0.02543265
Iteration 43, loss = 0.02521605
Iteration 44, loss = 0.02499097
Iteration 45, loss = 0.02512176
Iteration 46, loss = 0.02493853
Iteration 47, loss = 0.02455457
Iteration 48, loss = 0.02481661
Iteration 49, loss = 0.02430627
Iteration 50, loss = 0.02439343
Iteration 51, loss = 0.02408450
Iteration 52, loss = 0.02410930
Iteration 53, loss = 0.02412935
Iteration 54, loss = 0.02410139
Iteration 55, loss = 0.02410468
Iteration 56, loss = 0.02406142
Iteration 57, loss = 0.02418088
Iteration 58, loss = 0.02381208
Iteration 59, loss = 0.02367756
Iteration 60, loss = 0.02370355
Iteration 61, loss = 0.02363198
Iteration 62, loss = 0.02350712
Iteration 63, loss = 0.02342515
Iteration 64, loss = 0.02347794
Iteration 65, loss = 0.02357477
Iteration 66, loss = 0.02386340
Iteration 67, loss = 0.02332193
Iteration 68, loss = 0.02368539
Iteration 69, loss = 0.02340998
Iteration 70, loss = 0.02323030
Iteration 71, loss = 0.02304324
Iteration 72, loss = 0.02359872
Iteration 73, loss = 0.02310932
Iteration 74, loss = 0.02310476
Iteration 75, loss = 0.02316252
Iteration 76, loss = 0.02298742
Iteration 77, loss = 0.02304897
Iteration 78, loss = 0.02325703
Iteration 79, loss = 0.02286061
Iteration 80, loss = 0.02283030
Iteration 81, loss = 0.02292610
Iteration 82, loss = 0.02268331
Iteration 83, loss = 0.02276273
Iteration 84, loss = 0.02312667
Iteration 85, loss = 0.02300422
Iteration 86, loss = 0.02274109
Iteration 87, loss = 0.02279617
Iteration 88, loss = 0.02273864
Iteration 89, loss = 0.02263570
Iteration 90, loss = 0.02275470
Iteration 91, loss = 0.02264984
Iteration 92, loss = 0.02271267
Iteration 93, loss = 0.02246877
Iteration 94, loss = 0.02267264
Iteration 95, loss = 0.02297220
Iteration 96, loss = 0.02282258
Iteration 97, loss = 0.02242813
Iteration 98, loss = 0.02283861
Iteration 99, loss = 0.02265895
Iteration 100, loss = 0.02255114
Iteration 101, loss = 0.02253913
Iteration 102, loss = 0.02273197
Iteration 103, loss = 0.02249299
Iteration 104, loss = 0.02259016
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68431817
Iteration 2, loss = 0.64494099
Iteration 3, loss = 0.59186784
Iteration 4, loss = 0.52565213
Iteration 5, loss = 0.45559792
Iteration 6, loss = 0.38830100
Iteration 7, loss = 0.32718367
Iteration 8, loss = 0.27376369
Iteration 9, loss = 0.22878303
Iteration 10, loss = 0.19182048
Iteration 11, loss = 0.16198581
Iteration 12, loss = 0.13802994
Iteration 13, loss = 0.11889678
Iteration 14, loss = 0.10341826
Iteration 15, loss = 0.09103540
Iteration 16, loss = 0.08074135
Iteration 17, loss = 0.07234869
Iteration 18, loss = 0.06538006
Iteration 19, loss = 0.05956968
Iteration 20, loss = 0.05466426
Iteration 21, loss = 0.05046452
Iteration 22, loss = 0.04694641
Iteration 23, loss = 0.04369711
Iteration 24, loss = 0.04118067
Iteration 25, loss = 0.03869861
Iteration 26, loss = 0.03669400
Iteration 27, loss = 0.03476399
Iteration 28, loss = 0.03330106
Iteration 29, loss = 0.03176503
Iteration 30, loss = 0.03052464
Iteration 31, loss = 0.02940324
Iteration 32, loss = 0.02824609
Iteration 33, loss = 0.02750361
Iteration 34, loss = 0.02653154
Iteration 35, loss = 0.02568684
Iteration 36, loss = 0.02508238
Iteration 37, loss = 0.02447112
Iteration 38, loss = 0.02364656
Iteration 39, loss = 0.02309260
Iteration 40, loss = 0.02263688
Iteration 41, loss = 0.02232800
Iteration 42, loss = 0.02176638
Iteration 43, loss = 0.02148128
Iteration 44, loss = 0.02111230
Iteration 45, loss = 0.02059843
Iteration 46, loss = 0.02030281
Iteration 47, loss = 0.01987988
Iteration 48, loss = 0.01968668
Iteration 49, loss = 0.01952432
Iteration 50, loss = 0.01931850
Iteration 51, loss = 0.01892766
Iteration 52, loss = 0.01880686
Iteration 53, loss = 0.01856900
Iteration 54, loss = 0.01829230
Iteration 55, loss = 0.01804904
Iteration 56, loss = 0.01799340
Iteration 57, loss = 0.01767864
Iteration 58, loss = 0.01766151
Iteration 59, loss = 0.01735462
Iteration 60, loss = 0.01721595
Iteration 61, loss = 0.01712329
Iteration 62, loss = 0.01705591
Iteration 63, loss = 0.01685078
Iteration 64, loss = 0.01677279
Iteration 65, loss = 0.01662960
Iteration 66, loss = 0.01662945
Iteration 67, loss = 0.01657780
Iteration 68, loss = 0.01645343
Iteration 69, loss = 0.01624692
Iteration 70, loss = 0.01618597
Iteration 71, loss = 0.01607121
Iteration 72, loss = 0.01609222
Iteration 73, loss = 0.01588860
Iteration 74, loss = 0.01620080
Iteration 75, loss = 0.01586858
Iteration 76, loss = 0.01572660
Iteration 77, loss = 0.01580492
Iteration 78, loss = 0.01568546
Iteration 79, loss = 0.01549343
Iteration 80, loss = 0.01546640
Iteration 81, loss = 0.01536220
Iteration 82, loss = 0.01537191
Iteration 83, loss = 0.01536513
Iteration 84, loss = 0.01524450
Iteration 85, loss = 0.01517096
Iteration 86, loss = 0.01518466
Iteration 87, loss = 0.01517630
Iteration 88, loss = 0.01499786
Iteration 89, loss = 0.01509381
Iteration 90, loss = 0.01501191
Iteration 91, loss = 0.01502587
Iteration 92, loss = 0.01495741
Iteration 93, loss = 0.01480723
Iteration 94, loss = 0.01480501
Iteration 95, loss = 0.01477393
Iteration 96, loss = 0.01481830
Iteration 97, loss = 0.01482460
Iteration 98, loss = 0.01465144
Iteration 99, loss = 0.01459684
Iteration 100, loss = 0.01461492
Iteration 101, loss = 0.01465922
Iteration 102, loss = 0.01477753
Iteration 103, loss = 0.01451884
Iteration 104, loss = 0.01441087
Iteration 105, loss = 0.01454763
Iteration 106, loss = 0.01452452
Iteration 107, loss = 0.01438660
Iteration 108, loss = 0.01434347
Iteration 109, loss = 0.01433351
Iteration 110, loss = 0.01424972
Iteration 111, loss = 0.01435850
Iteration 112, loss = 0.01426832
Iteration 113, loss = 0.01425078
Iteration 114, loss = 0.01438922
Iteration 115, loss = 0.01427292
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71363553
Iteration 2, loss = 0.65668855
Iteration 3, loss = 0.57983607
Iteration 4, loss = 0.48234792
Iteration 5, loss = 0.38411847
Iteration 6, loss = 0.30059998
Iteration 7, loss = 0.23475346
Iteration 8, loss = 0.18529203
Iteration 9, loss = 0.14889120
Iteration 10, loss = 0.12245239
Iteration 11, loss = 0.10316245
Iteration 12, loss = 0.08838770
Iteration 13, loss = 0.07731039
Iteration 14, loss = 0.06844635
Iteration 15, loss = 0.06162066
Iteration 16, loss = 0.05607089
Iteration 17, loss = 0.05158482
Iteration 18, loss = 0.04764527
Iteration 19, loss = 0.04446320
Iteration 20, loss = 0.04184402
Iteration 21, loss = 0.03939182
Iteration 22, loss = 0.03740851
Iteration 23, loss = 0.03594280
Iteration 24, loss = 0.03438083
Iteration 25, loss = 0.03298283
Iteration 26, loss = 0.03179694
Iteration 27, loss = 0.03086106
Iteration 28, loss = 0.02987966
Iteration 29, loss = 0.02906253
Iteration 30, loss = 0.02846448
Iteration 31, loss = 0.02765526
Iteration 32, loss = 0.02699850
Iteration 33, loss = 0.02641846
Iteration 34, loss = 0.02613273
Iteration 35, loss = 0.02555227
Iteration 36, loss = 0.02506675
Iteration 37, loss = 0.02466657
Iteration 38, loss = 0.02454491
Iteration 39, loss = 0.02395771
Iteration 40, loss = 0.02385349
Iteration 41, loss = 0.02355617
Iteration 42, loss = 0.02324355
Iteration 43, loss = 0.02299989
Iteration 44, loss = 0.02345217
Iteration 45, loss = 0.02310867
Iteration 46, loss = 0.02327377
Iteration 47, loss = 0.02311345
Iteration 48, loss = 0.02280141
Iteration 49, loss = 0.02266129
Iteration 50, loss = 0.02234815
Iteration 51, loss = 0.02230296
Iteration 52, loss = 0.02205813
Iteration 53, loss = 0.02190070
Iteration 54, loss = 0.02172286
Iteration 55, loss = 0.02146808
Iteration 56, loss = 0.02149828
Iteration 57, loss = 0.02143712
Iteration 58, loss = 0.02130279
Iteration 59, loss = 0.02099981
Iteration 60, loss = 0.02107625
Iteration 61, loss = 0.02094013
Iteration 62, loss = 0.02065527
Iteration 63, loss = 0.02079294
Iteration 64, loss = 0.02062661
Iteration 65, loss = 0.02071732
Iteration 66, loss = 0.02046599
Iteration 67, loss = 0.02041285
Iteration 68, loss = 0.02070611
Iteration 69, loss = 0.02023937
Iteration 70, loss = 0.02006357
Iteration 71, loss = 0.02028078
Iteration 72, loss = 0.02013433
Iteration 73, loss = 0.01996355
Iteration 74, loss = 0.02000719
Iteration 75, loss = 0.01973831
Iteration 76, loss = 0.01984877
Iteration 77, loss = 0.01981544
Iteration 78, loss = 0.01960085
Iteration 79, loss = 0.01963458
Iteration 80, loss = 0.02072827
Iteration 81, loss = 0.02076742
Iteration 82, loss = 0.02065665
Iteration 83, loss = 0.02034034
Iteration 84, loss = 0.02023552
Iteration 85, loss = 0.02002352
Iteration 86, loss = 0.02005088
Iteration 87, loss = 0.01951402
Iteration 88, loss = 0.01963609
Iteration 89, loss = 0.01938462
Iteration 90, loss = 0.01954332
Iteration 91, loss = 0.01902968
Iteration 92, loss = 0.01912304
Iteration 93, loss = 0.01917238
Iteration 94, loss = 0.01914336
Iteration 95, loss = 0.01892753
Iteration 96, loss = 0.01900604
Iteration 97, loss = 0.01871372
Iteration 98, loss = 0.01879571
Iteration 99, loss = 0.01898008
Iteration 100, loss = 0.01883225
Iteration 101, loss = 0.01874241
Iteration 102, loss = 0.01852953
Iteration 103, loss = 0.01883249
Iteration 104, loss = 0.01868380
Iteration 105, loss = 0.01859232
Iteration 106, loss = 0.01864828
Iteration 107, loss = 0.01845409
Iteration 108, loss = 0.01863769
Iteration 109, loss = 0.01846703
Iteration 110, loss = 0.01862684
Iteration 111, loss = 0.01851677
Iteration 112, loss = 0.01860880
Iteration 113, loss = 0.01857626
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71721386
Iteration 2, loss = 0.68548889
Iteration 3, loss = 0.64201908
Iteration 4, loss = 0.58854141
Iteration 5, loss = 0.52697505
Iteration 6, loss = 0.46375745
Iteration 7, loss = 0.40213602
Iteration 8, loss = 0.34465533
Iteration 9, loss = 0.29365017
Iteration 10, loss = 0.24947007
Iteration 11, loss = 0.21206623
Iteration 12, loss = 0.18106278
Iteration 13, loss = 0.15562166
Iteration 14, loss = 0.13462990
Iteration 15, loss = 0.11761663
Iteration 16, loss = 0.10359985
Iteration 17, loss = 0.09202921
Iteration 18, loss = 0.08238942
Iteration 19, loss = 0.07452262
Iteration 20, loss = 0.06766597
Iteration 21, loss = 0.06203480
Iteration 22, loss = 0.05713851
Iteration 23, loss = 0.05308316
Iteration 24, loss = 0.04935464
Iteration 25, loss = 0.04614265
Iteration 26, loss = 0.04351156
Iteration 27, loss = 0.04101804
Iteration 28, loss = 0.03894418
Iteration 29, loss = 0.03696297
Iteration 30, loss = 0.03518160
Iteration 31, loss = 0.03379443
Iteration 32, loss = 0.03231152
Iteration 33, loss = 0.03113501
Iteration 34, loss = 0.02994040
Iteration 35, loss = 0.02905847
Iteration 36, loss = 0.02811032
Iteration 37, loss = 0.02715584
Iteration 38, loss = 0.02635041
Iteration 39, loss = 0.02580023
Iteration 40, loss = 0.02514223
Iteration 41, loss = 0.02447848
Iteration 42, loss = 0.02385625
Iteration 43, loss = 0.02344360
Iteration 44, loss = 0.02303960
Iteration 45, loss = 0.02239888
Iteration 46, loss = 0.02202111
Iteration 47, loss = 0.02169475
Iteration 48, loss = 0.02134013
Iteration 49, loss = 0.02088714
Iteration 50, loss = 0.02068993
Iteration 51, loss = 0.02035237
Iteration 52, loss = 0.02015922
Iteration 53, loss = 0.01977217
Iteration 54, loss = 0.01961054
Iteration 55, loss = 0.01934548
Iteration 56, loss = 0.01908779
Iteration 57, loss = 0.01886765
Iteration 58, loss = 0.01875036
Iteration 59, loss = 0.01842136
Iteration 60, loss = 0.01833076
Iteration 61, loss = 0.01819777
Iteration 62, loss = 0.01798078
Iteration 63, loss = 0.01792864
Iteration 64, loss = 0.01777527
Iteration 65, loss = 0.01756115
Iteration 66, loss = 0.01753282
Iteration 67, loss = 0.01734048
Iteration 68, loss = 0.01713341
Iteration 69, loss = 0.01715768
Iteration 70, loss = 0.01698798
Iteration 71, loss = 0.01679862
Iteration 72, loss = 0.01687476
Iteration 73, loss = 0.01667278
Iteration 74, loss = 0.01674802
Iteration 75, loss = 0.01653497
Iteration 76, loss = 0.01643213
Iteration 77, loss = 0.01641788
Iteration 78, loss = 0.01633703
Iteration 79, loss = 0.01613518
Iteration 80, loss = 0.01615202
Iteration 81, loss = 0.01611760
Iteration 82, loss = 0.01603030
Iteration 83, loss = 0.01593098
Iteration 84, loss = 0.01590469
Iteration 85, loss = 0.01588736
Iteration 86, loss = 0.01577830
Iteration 87, loss = 0.01582158
Iteration 88, loss = 0.01573695
Iteration 89, loss = 0.01559128
Iteration 90, loss = 0.01559687
Iteration 91, loss = 0.01550432
Iteration 92, loss = 0.01551471
Iteration 93, loss = 0.01547117
Iteration 94, loss = 0.01536478
Iteration 95, loss = 0.01539277
Iteration 96, loss = 0.01550770
Iteration 97, loss = 0.01532176
Iteration 98, loss = 0.01515138
Iteration 99, loss = 0.01538930
Iteration 100, loss = 0.01526174
Iteration 101, loss = 0.01518945
Iteration 102, loss = 0.01519969
Iteration 103, loss = 0.01510690
Iteration 104, loss = 0.01503073
Iteration 105, loss = 0.01494086
Iteration 106, loss = 0.01507004
Iteration 107, loss = 0.01496042
Iteration 108, loss = 0.01511244
Iteration 109, loss = 0.01516637
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72020782
Iteration 2, loss = 0.69378350
Iteration 3, loss = 0.66892228
Iteration 4, loss = 0.64214359
Iteration 5, loss = 0.61330304
Iteration 6, loss = 0.58236537
Iteration 7, loss = 0.54927240
Iteration 8, loss = 0.51477431
Iteration 9, loss = 0.47956184
Iteration 10, loss = 0.44386893
Iteration 11, loss = 0.40914331
Iteration 12, loss = 0.37566988
Iteration 13, loss = 0.34390618
Iteration 14, loss = 0.31435644
Iteration 15, loss = 0.28694915
Iteration 16, loss = 0.26173085
Iteration 17, loss = 0.23874456
Iteration 18, loss = 0.21778465
Iteration 19, loss = 0.19893766
Iteration 20, loss = 0.18185829
Iteration 21, loss = 0.16660948
Iteration 22, loss = 0.15294637
Iteration 23, loss = 0.14065221
Iteration 24, loss = 0.12972004
Iteration 25, loss = 0.11983743
Iteration 26, loss = 0.11110061
Iteration 27, loss = 0.10319060
Iteration 28, loss = 0.09604039
Iteration 29, loss = 0.08963219
Iteration 30, loss = 0.08392228
Iteration 31, loss = 0.07877858
Iteration 32, loss = 0.07405585
Iteration 33, loss = 0.06982139
Iteration 34, loss = 0.06592951
Iteration 35, loss = 0.06240475
Iteration 36, loss = 0.05918823
Iteration 37, loss = 0.05628774
Iteration 38, loss = 0.05356857
Iteration 39, loss = 0.05115375
Iteration 40, loss = 0.04889234
Iteration 41, loss = 0.04679785
Iteration 42, loss = 0.04479155
Iteration 43, loss = 0.04303879
Iteration 44, loss = 0.04135274
Iteration 45, loss = 0.03983101
Iteration 46, loss = 0.03846154
Iteration 47, loss = 0.03710106
Iteration 48, loss = 0.03582002
Iteration 49, loss = 0.03468496
Iteration 50, loss = 0.03357672
Iteration 51, loss = 0.03254878
Iteration 52, loss = 0.03165656
Iteration 53, loss = 0.03070915
Iteration 54, loss = 0.02987169
Iteration 55, loss = 0.02907290
Iteration 56, loss = 0.02831185
Iteration 57, loss = 0.02762897
Iteration 58, loss = 0.02694590
Iteration 59, loss = 0.02630931
Iteration 60, loss = 0.02573149
Iteration 61, loss = 0.02512754
Iteration 62, loss = 0.02465178
Iteration 63, loss = 0.02412511
Iteration 64, loss = 0.02362782
Iteration 65, loss = 0.02313714
Iteration 66, loss = 0.02275072
Iteration 67, loss = 0.02230791
Iteration 68, loss = 0.02194099
Iteration 69, loss = 0.02154060
Iteration 70, loss = 0.02118988
Iteration 71, loss = 0.02084398
Iteration 72, loss = 0.02048145
Iteration 73, loss = 0.02019649
Iteration 74, loss = 0.01986501
Iteration 75, loss = 0.01959837
Iteration 76, loss = 0.01930886
Iteration 77, loss = 0.01901982
Iteration 78, loss = 0.01879778
Iteration 79, loss = 0.01852651
Iteration 80, loss = 0.01829988
Iteration 81, loss = 0.01808444
Iteration 82, loss = 0.01794358
Iteration 83, loss = 0.01763833
Iteration 84, loss = 0.01744736
Iteration 85, loss = 0.01727165
Iteration 86, loss = 0.01709542
Iteration 87, loss = 0.01691595
Iteration 88, loss = 0.01671519
Iteration 89, loss = 0.01656073
Iteration 90, loss = 0.01639130
Iteration 91, loss = 0.01623463
Iteration 92, loss = 0.01608778
Iteration 93, loss = 0.01593378
Iteration 94, loss = 0.01583843
Iteration 95, loss = 0.01568483
Iteration 96, loss = 0.01552910
Iteration 97, loss = 0.01540649
Iteration 98, loss = 0.01529682
Iteration 99, loss = 0.01517212
Iteration 100, loss = 0.01508262
Iteration 101, loss = 0.01492687
Iteration 102, loss = 0.01483182
Iteration 103, loss = 0.01474703
Iteration 104, loss = 0.01462087
Iteration 105, loss = 0.01451129
Iteration 106, loss = 0.01442766
Iteration 107, loss = 0.01434564
Iteration 108, loss = 0.01422223
Iteration 109, loss = 0.01418386
Iteration 110, loss = 0.01408738
Iteration 111, loss = 0.01398750
Iteration 112, loss = 0.01390922
Iteration 113, loss = 0.01387358
Iteration 114, loss = 0.01377508
Iteration 115, loss = 0.01369020
Iteration 116, loss = 0.01363437
Iteration 117, loss = 0.01357337
Iteration 118, loss = 0.01347375
Iteration 119, loss = 0.01341418
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67603814
Iteration 2, loss = 0.61533030
Iteration 3, loss = 0.51581225
Iteration 4, loss = 0.40697985
Iteration 5, loss = 0.30990568
Iteration 6, loss = 0.23455517
Iteration 7, loss = 0.17940130
Iteration 8, loss = 0.14108238
Iteration 9, loss = 0.11403451
Iteration 10, loss = 0.09469481
Iteration 11, loss = 0.08051932
Iteration 12, loss = 0.06968600
Iteration 13, loss = 0.06157980
Iteration 14, loss = 0.05510031
Iteration 15, loss = 0.05019557
Iteration 16, loss = 0.04601613
Iteration 17, loss = 0.04255599
Iteration 18, loss = 0.03961266
Iteration 19, loss = 0.03739594
Iteration 20, loss = 0.03542116
Iteration 21, loss = 0.03374119
Iteration 22, loss = 0.03204376
Iteration 23, loss = 0.03096014
Iteration 24, loss = 0.02957563
Iteration 25, loss = 0.02874523
Iteration 26, loss = 0.02784970
Iteration 27, loss = 0.02709897
Iteration 28, loss = 0.02634869
Iteration 29, loss = 0.02578176
Iteration 30, loss = 0.02515444
Iteration 31, loss = 0.02472782
Iteration 32, loss = 0.02441632
Iteration 33, loss = 0.02378543
Iteration 34, loss = 0.02344685
Iteration 35, loss = 0.02327634
Iteration 36, loss = 0.02279598
Iteration 37, loss = 0.02255737
Iteration 38, loss = 0.02234843
Iteration 39, loss = 0.02250780
Iteration 40, loss = 0.02215123
Iteration 41, loss = 0.02139887
Iteration 42, loss = 0.02152307
Iteration 43, loss = 0.02128432
Iteration 44, loss = 0.02102848
Iteration 45, loss = 0.02083388
Iteration 46, loss = 0.02085435
Iteration 47, loss = 0.02050675
Iteration 48, loss = 0.02065414
Iteration 49, loss = 0.02039265
Iteration 50, loss = 0.02034274
Iteration 51, loss = 0.02045718
Iteration 52, loss = 0.02007311
Iteration 53, loss = 0.02002343
Iteration 54, loss = 0.02001091
Iteration 55, loss = 0.01979484
Iteration 56, loss = 0.01957297
Iteration 57, loss = 0.01966360
Iteration 58, loss = 0.01968127
Iteration 59, loss = 0.01940980
Iteration 60, loss = 0.01962385
Iteration 61, loss = 0.01950158
Iteration 62, loss = 0.01945621
Iteration 63, loss = 0.01939117
Iteration 64, loss = 0.01935919
Iteration 65, loss = 0.01927230
Iteration 66, loss = 0.01919326
Iteration 67, loss = 0.01939699
Iteration 68, loss = 0.01921427
Iteration 69, loss = 0.01905007
Iteration 70, loss = 0.01902947
Iteration 71, loss = 0.01929102
Iteration 72, loss = 0.01886821
Iteration 73, loss = 0.01903330
Iteration 74, loss = 0.01882642
Iteration 75, loss = 0.01892493
Iteration 76, loss = 0.01867013
Iteration 77, loss = 0.01875916
Iteration 78, loss = 0.01856261
Iteration 79, loss = 0.01897890
Iteration 80, loss = 0.01870679
Iteration 81, loss = 0.01883191
Iteration 82, loss = 0.01871553
Iteration 83, loss = 0.01861829
Iteration 84, loss = 0.01860614
Iteration 85, loss = 0.01882576
Iteration 86, loss = 0.01857813
Iteration 87, loss = 0.01839466
Iteration 88, loss = 0.01825563
Iteration 89, loss = 0.01837890
Iteration 90, loss = 0.01844988
Iteration 91, loss = 0.01904532
Iteration 92, loss = 0.01852314
Iteration 93, loss = 0.01821190
Iteration 94, loss = 0.01850188
Iteration 95, loss = 0.01851279
Iteration 96, loss = 0.01817771
Iteration 97, loss = 0.01830231
Iteration 98, loss = 0.01844064
Iteration 99, loss = 0.01842674
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69669684
Iteration 2, loss = 0.66834933
Iteration 3, loss = 0.63495822
Iteration 4, loss = 0.59524258
Iteration 5, loss = 0.54996594
Iteration 6, loss = 0.50090432
Iteration 7, loss = 0.45020638
Iteration 8, loss = 0.40017044
Iteration 9, loss = 0.35286008
Iteration 10, loss = 0.30964279
Iteration 11, loss = 0.27103679
Iteration 12, loss = 0.23706582
Iteration 13, loss = 0.20776203
Iteration 14, loss = 0.18256148
Iteration 15, loss = 0.16113066
Iteration 16, loss = 0.14289670
Iteration 17, loss = 0.12737070
Iteration 18, loss = 0.11431039
Iteration 19, loss = 0.10304961
Iteration 20, loss = 0.09349993
Iteration 21, loss = 0.08527815
Iteration 22, loss = 0.07811542
Iteration 23, loss = 0.07192329
Iteration 24, loss = 0.06662719
Iteration 25, loss = 0.06174299
Iteration 26, loss = 0.05774764
Iteration 27, loss = 0.05411223
Iteration 28, loss = 0.05085323
Iteration 29, loss = 0.04787706
Iteration 30, loss = 0.04527771
Iteration 31, loss = 0.04284930
Iteration 32, loss = 0.04079459
Iteration 33, loss = 0.03888277
Iteration 34, loss = 0.03712761
Iteration 35, loss = 0.03568379
Iteration 36, loss = 0.03420836
Iteration 37, loss = 0.03305505
Iteration 38, loss = 0.03185365
Iteration 39, loss = 0.03077172
Iteration 40, loss = 0.02970034
Iteration 41, loss = 0.02867312
Iteration 42, loss = 0.02781790
Iteration 43, loss = 0.02695043
Iteration 44, loss = 0.02616720
Iteration 45, loss = 0.02550307
Iteration 46, loss = 0.02480070
Iteration 47, loss = 0.02427625
Iteration 48, loss = 0.02365040
Iteration 49, loss = 0.02311694
Iteration 50, loss = 0.02263051
Iteration 51, loss = 0.02215817
Iteration 52, loss = 0.02193380
Iteration 53, loss = 0.02150878
Iteration 54, loss = 0.02111706
Iteration 55, loss = 0.02072236
Iteration 56, loss = 0.02038461
Iteration 57, loss = 0.02000929
Iteration 58, loss = 0.01969125
Iteration 59, loss = 0.01953805
Iteration 60, loss = 0.01930150
Iteration 61, loss = 0.01897292
Iteration 62, loss = 0.01885452
Iteration 63, loss = 0.01849452
Iteration 64, loss = 0.01817532
Iteration 65, loss = 0.01834463
Iteration 66, loss = 0.01799141
Iteration 67, loss = 0.01774419
Iteration 68, loss = 0.01761646
Iteration 69, loss = 0.01739109
Iteration 70, loss = 0.01724241
Iteration 71, loss = 0.01697022
Iteration 72, loss = 0.01688810
Iteration 73, loss = 0.01672511
Iteration 74, loss = 0.01648702
Iteration 75, loss = 0.01626982
Iteration 76, loss = 0.01613119
Iteration 77, loss = 0.01604196
Iteration 78, loss = 0.01587784
Iteration 79, loss = 0.01596161
Iteration 80, loss = 0.01584500
Iteration 81, loss = 0.01569007
Iteration 82, loss = 0.01553248
Iteration 83, loss = 0.01541820
Iteration 84, loss = 0.01527169
Iteration 85, loss = 0.01524790
Iteration 86, loss = 0.01515227
Iteration 87, loss = 0.01507220
Iteration 88, loss = 0.01488110
Iteration 89, loss = 0.01504532
Iteration 90, loss = 0.01459011
Iteration 91, loss = 0.01479593
Iteration 92, loss = 0.01479502
Iteration 93, loss = 0.01480589
Iteration 94, loss = 0.01443927
Iteration 95, loss = 0.01456435
Iteration 96, loss = 0.01425030
Iteration 97, loss = 0.01431706
Iteration 98, loss = 0.01433072
Iteration 99, loss = 0.01431668
Iteration 100, loss = 0.01445538
Iteration 101, loss = 0.01448924
Iteration 102, loss = 0.01423751
Iteration 103, loss = 0.01399829
Iteration 104, loss = 0.01391393
Iteration 105, loss = 0.01377487
Iteration 106, loss = 0.01380833
Iteration 107, loss = 0.01375827
Iteration 108, loss = 0.01384933
Iteration 109, loss = 0.01382287
Iteration 110, loss = 0.01364801
Iteration 111, loss = 0.01354108
Iteration 112, loss = 0.01352809
Iteration 113, loss = 0.01348614
Iteration 114, loss = 0.01359890
Iteration 115, loss = 0.01363747
Iteration 116, loss = 0.01373151
Iteration 117, loss = 0.01374849
Iteration 118, loss = 0.01373718
Iteration 119, loss = 0.01362038
Iteration 120, loss = 0.01340278
Iteration 121, loss = 0.01336152
Iteration 122, loss = 0.01319293
Iteration 123, loss = 0.01310449
Iteration 124, loss = 0.01310648
Iteration 125, loss = 0.01322513
Iteration 126, loss = 0.01328255
Iteration 127, loss = 0.01336508
Iteration 128, loss = 0.01339768
Iteration 129, loss = 0.01320513
Iteration 130, loss = 0.01315836
Iteration 131, loss = 0.01308256
Iteration 132, loss = 0.01299257
Iteration 133, loss = 0.01289768
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67822688
Iteration 2, loss = 0.60510695
Iteration 3, loss = 0.50084933
Iteration 4, loss = 0.39068695
Iteration 5, loss = 0.29334334
Iteration 6, loss = 0.21943941
Iteration 7, loss = 0.16688601
Iteration 8, loss = 0.13026801
Iteration 9, loss = 0.10517969
Iteration 10, loss = 0.08727353
Iteration 11, loss = 0.07429111
Iteration 12, loss = 0.06450708
Iteration 13, loss = 0.05709448
Iteration 14, loss = 0.05142505
Iteration 15, loss = 0.04700895
Iteration 16, loss = 0.04316730
Iteration 17, loss = 0.04010348
Iteration 18, loss = 0.03751059
Iteration 19, loss = 0.03566411
Iteration 20, loss = 0.03376490
Iteration 21, loss = 0.03200331
Iteration 22, loss = 0.03077052
Iteration 23, loss = 0.02981954
Iteration 24, loss = 0.02875794
Iteration 25, loss = 0.02764232
Iteration 26, loss = 0.02710770
Iteration 27, loss = 0.02627120
Iteration 28, loss = 0.02573693
Iteration 29, loss = 0.02525362
Iteration 30, loss = 0.02464324
Iteration 31, loss = 0.02419669
Iteration 32, loss = 0.02376574
Iteration 33, loss = 0.02332338
Iteration 34, loss = 0.02322451
Iteration 35, loss = 0.02295789
Iteration 36, loss = 0.02261203
Iteration 37, loss = 0.02244858
Iteration 38, loss = 0.02212085
Iteration 39, loss = 0.02200137
Iteration 40, loss = 0.02196716
Iteration 41, loss = 0.02170330
Iteration 42, loss = 0.02120536
Iteration 43, loss = 0.02129296
Iteration 44, loss = 0.02113481
Iteration 45, loss = 0.02087189
Iteration 46, loss = 0.02104446
Iteration 47, loss = 0.02086214
Iteration 48, loss = 0.02065620
Iteration 49, loss = 0.02061221
Iteration 50, loss = 0.02052394
Iteration 51, loss = 0.02033735
Iteration 52, loss = 0.02052298
Iteration 53, loss = 0.02023415
Iteration 54, loss = 0.01989255
Iteration 55, loss = 0.02023275
Iteration 56, loss = 0.02004974
Iteration 57, loss = 0.01990740
Iteration 58, loss = 0.01996892
Iteration 59, loss = 0.01974316
Iteration 60, loss = 0.01971564
Iteration 61, loss = 0.01997717
Iteration 62, loss = 0.01976231
Iteration 63, loss = 0.01959613
Iteration 64, loss = 0.01964996
Iteration 65, loss = 0.01927761
Iteration 66, loss = 0.01941584
Iteration 67, loss = 0.01932446
Iteration 68, loss = 0.01947092
Iteration 69, loss = 0.01953930
Iteration 70, loss = 0.01953911
Iteration 71, loss = 0.01933688
Iteration 72, loss = 0.01910757
Iteration 73, loss = 0.01916765
Iteration 74, loss = 0.01945124
Iteration 75, loss = 0.01936810
Iteration 76, loss = 0.01922881
Iteration 77, loss = 0.01895284
Iteration 78, loss = 0.01907181
Iteration 79, loss = 0.01899861
Iteration 80, loss = 0.01894757
Iteration 81, loss = 0.01901154
Iteration 82, loss = 0.01917879
Iteration 83, loss = 0.01903359
Iteration 84, loss = 0.01907797
Iteration 85, loss = 0.01903009
Iteration 86, loss = 0.01897959
Iteration 87, loss = 0.01878548
Iteration 88, loss = 0.01882565
Iteration 89, loss = 0.01876368
Iteration 90, loss = 0.01884700
Iteration 91, loss = 0.01874459
Iteration 92, loss = 0.01884766
Iteration 93, loss = 0.01863564
Iteration 94, loss = 0.01895573
Iteration 95, loss = 0.01873695
Iteration 96, loss = 0.01858037
Iteration 97, loss = 0.01873015
Iteration 98, loss = 0.01879097
Iteration 99, loss = 0.01873853
Iteration 100, loss = 0.01867964
Iteration 101, loss = 0.01870500
Iteration 102, loss = 0.01868276
Iteration 103, loss = 0.01864110
Iteration 104, loss = 0.01876565
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69487159
Iteration 2, loss = 0.64417942
Iteration 3, loss = 0.57141049
Iteration 4, loss = 0.47785087
Iteration 5, loss = 0.38626433
Iteration 6, loss = 0.30445786
Iteration 7, loss = 0.23969244
Iteration 8, loss = 0.19042545
Iteration 9, loss = 0.15392086
Iteration 10, loss = 0.12692064
Iteration 11, loss = 0.10667620
Iteration 12, loss = 0.09152744
Iteration 13, loss = 0.07986626
Iteration 14, loss = 0.07052396
Iteration 15, loss = 0.06330208
Iteration 16, loss = 0.05738175
Iteration 17, loss = 0.05244776
Iteration 18, loss = 0.04878329
Iteration 19, loss = 0.04535365
Iteration 20, loss = 0.04229346
Iteration 21, loss = 0.03993278
Iteration 22, loss = 0.03778495
Iteration 23, loss = 0.03601759
Iteration 24, loss = 0.03464101
Iteration 25, loss = 0.03304688
Iteration 26, loss = 0.03179395
Iteration 27, loss = 0.03080737
Iteration 28, loss = 0.03001820
Iteration 29, loss = 0.02909106
Iteration 30, loss = 0.02824804
Iteration 31, loss = 0.02767624
Iteration 32, loss = 0.02695827
Iteration 33, loss = 0.02636935
Iteration 34, loss = 0.02582205
Iteration 35, loss = 0.02544363
Iteration 36, loss = 0.02488660
Iteration 37, loss = 0.02446859
Iteration 38, loss = 0.02418494
Iteration 39, loss = 0.02389388
Iteration 40, loss = 0.02367546
Iteration 41, loss = 0.02328170
Iteration 42, loss = 0.02320803
Iteration 43, loss = 0.02281079
Iteration 44, loss = 0.02253043
Iteration 45, loss = 0.02234687
Iteration 46, loss = 0.02204972
Iteration 47, loss = 0.02188849
Iteration 48, loss = 0.02203929
Iteration 49, loss = 0.02142933
Iteration 50, loss = 0.02137021
Iteration 51, loss = 0.02145291
Iteration 52, loss = 0.02152481
Iteration 53, loss = 0.02113141
Iteration 54, loss = 0.02085719
Iteration 55, loss = 0.02099137
Iteration 56, loss = 0.02094264
Iteration 57, loss = 0.02059467
Iteration 58, loss = 0.02069677
Iteration 59, loss = 0.02061538
Iteration 60, loss = 0.02033746
Iteration 61, loss = 0.02037978
Iteration 62, loss = 0.02019484
Iteration 63, loss = 0.02008151
Iteration 64, loss = 0.02016655
Iteration 65, loss = 0.01988169
Iteration 66, loss = 0.01981240
Iteration 67, loss = 0.01986376
Iteration 68, loss = 0.01980909
Iteration 69, loss = 0.01975225
Iteration 70, loss = 0.01977397
Iteration 71, loss = 0.01978269
Iteration 72, loss = 0.01968574
Iteration 73, loss = 0.01962831
Iteration 74, loss = 0.01960342
Iteration 75, loss = 0.01964869
Iteration 76, loss = 0.01936584
Iteration 77, loss = 0.01934117
Iteration 78, loss = 0.01940534
Iteration 79, loss = 0.01936102
Iteration 80, loss = 0.01934248
Iteration 81, loss = 0.01930143
Iteration 82, loss = 0.01925008
Iteration 83, loss = 0.01921475
Iteration 84, loss = 0.01898829
Iteration 85, loss = 0.01908204
Iteration 86, loss = 0.01932217
Iteration 87, loss = 0.01903284
Iteration 88, loss = 0.01906260
Iteration 89, loss = 0.01913111
Iteration 90, loss = 0.01905200
Iteration 91, loss = 0.01903123
Iteration 92, loss = 0.01897230
Iteration 93, loss = 0.01887902
Iteration 94, loss = 0.01879932
Iteration 95, loss = 0.01897224
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67349111
Iteration 2, loss = 0.59674010
Iteration 3, loss = 0.48303825
Iteration 4, loss = 0.37011786
Iteration 5, loss = 0.27685569
Iteration 6, loss = 0.20751091
Iteration 7, loss = 0.15931134
Iteration 8, loss = 0.12607850
Iteration 9, loss = 0.10283136
Iteration 10, loss = 0.08604048
Iteration 11, loss = 0.07395809
Iteration 12, loss = 0.06454026
Iteration 13, loss = 0.05757091
Iteration 14, loss = 0.05212307
Iteration 15, loss = 0.04750426
Iteration 16, loss = 0.04399643
Iteration 17, loss = 0.04074983
Iteration 18, loss = 0.03845046
Iteration 19, loss = 0.03630326
Iteration 20, loss = 0.03452738
Iteration 21, loss = 0.03296768
Iteration 22, loss = 0.03157893
Iteration 23, loss = 0.03058040
Iteration 24, loss = 0.02974714
Iteration 25, loss = 0.02871142
Iteration 26, loss = 0.02795891
Iteration 27, loss = 0.02739832
Iteration 28, loss = 0.02656457
Iteration 29, loss = 0.02622535
Iteration 30, loss = 0.02563353
Iteration 31, loss = 0.02512696
Iteration 32, loss = 0.02486610
Iteration 33, loss = 0.02477236
Iteration 34, loss = 0.02381105
Iteration 35, loss = 0.02376195
Iteration 36, loss = 0.02342877
Iteration 37, loss = 0.02315765
Iteration 38, loss = 0.02293887
Iteration 39, loss = 0.02291646
Iteration 40, loss = 0.02258544
Iteration 41, loss = 0.02225862
Iteration 42, loss = 0.02221282
Iteration 43, loss = 0.02219701
Iteration 44, loss = 0.02179716
Iteration 45, loss = 0.02166945
Iteration 46, loss = 0.02183259
Iteration 47, loss = 0.02143991
Iteration 48, loss = 0.02159052
Iteration 49, loss = 0.02135900
Iteration 50, loss = 0.02113612
Iteration 51, loss = 0.02114873
Iteration 52, loss = 0.02113608
Iteration 53, loss = 0.02105802
Iteration 54, loss = 0.02079537
Iteration 55, loss = 0.02073169
Iteration 56, loss = 0.02095377
Iteration 57, loss = 0.02058058
Iteration 58, loss = 0.02061974
Iteration 59, loss = 0.02052055
Iteration 60, loss = 0.02022044
Iteration 61, loss = 0.02044695
Iteration 62, loss = 0.02047376
Iteration 63, loss = 0.02072385
Iteration 64, loss = 0.02020894
Iteration 65, loss = 0.02023988
Iteration 66, loss = 0.02031757
Iteration 67, loss = 0.02038545
Iteration 68, loss = 0.02008918
Iteration 69, loss = 0.02049816
Iteration 70, loss = 0.01990852
Iteration 71, loss = 0.01999291
Iteration 72, loss = 0.01997792
Iteration 73, loss = 0.02007588
Iteration 74, loss = 0.02022252
Iteration 75, loss = 0.01999502
Iteration 76, loss = 0.02003382
Iteration 77, loss = 0.01985028
Iteration 78, loss = 0.01987127
Iteration 79, loss = 0.01986904
Iteration 80, loss = 0.01960600
Iteration 81, loss = 0.01974953
Iteration 82, loss = 0.01990561
Iteration 83, loss = 0.01948107
Iteration 84, loss = 0.01971148
Iteration 85, loss = 0.01954068
Iteration 86, loss = 0.01977193
Iteration 87, loss = 0.01947434
Iteration 88, loss = 0.01944182
Iteration 89, loss = 0.01960409
Iteration 90, loss = 0.01972671
Iteration 91, loss = 0.01972424
Iteration 92, loss = 0.01963887
Iteration 93, loss = 0.01960444
Iteration 94, loss = 0.01963328
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68211973
Iteration 2, loss = 0.64446209
Iteration 3, loss = 0.58553824
Iteration 4, loss = 0.50365133
Iteration 5, loss = 0.41841477
Iteration 6, loss = 0.34038732
Iteration 7, loss = 0.27470232
Iteration 8, loss = 0.22175108
Iteration 9, loss = 0.18046139
Iteration 10, loss = 0.14913436
Iteration 11, loss = 0.12538725
Iteration 12, loss = 0.10706023
Iteration 13, loss = 0.09273122
Iteration 14, loss = 0.08149868
Iteration 15, loss = 0.07272472
Iteration 16, loss = 0.06546099
Iteration 17, loss = 0.05965310
Iteration 18, loss = 0.05496659
Iteration 19, loss = 0.05075364
Iteration 20, loss = 0.04727414
Iteration 21, loss = 0.04443050
Iteration 22, loss = 0.04207697
Iteration 23, loss = 0.04030572
Iteration 24, loss = 0.03798639
Iteration 25, loss = 0.03652947
Iteration 26, loss = 0.03494046
Iteration 27, loss = 0.03372059
Iteration 28, loss = 0.03244205
Iteration 29, loss = 0.03173549
Iteration 30, loss = 0.03070948
Iteration 31, loss = 0.02999113
Iteration 32, loss = 0.02910414
Iteration 33, loss = 0.02846595
Iteration 34, loss = 0.02772422
Iteration 35, loss = 0.02714791
Iteration 36, loss = 0.02669110
Iteration 37, loss = 0.02631892
Iteration 38, loss = 0.02593175
Iteration 39, loss = 0.02541925
Iteration 40, loss = 0.02502445
Iteration 41, loss = 0.02474117
Iteration 42, loss = 0.02449184
Iteration 43, loss = 0.02419537
Iteration 44, loss = 0.02426559
Iteration 45, loss = 0.02383702
Iteration 46, loss = 0.02358094
Iteration 47, loss = 0.02335639
Iteration 48, loss = 0.02302022
Iteration 49, loss = 0.02288523
Iteration 50, loss = 0.02263429
Iteration 51, loss = 0.02247205
Iteration 52, loss = 0.02245756
Iteration 53, loss = 0.02223852
Iteration 54, loss = 0.02200164
Iteration 55, loss = 0.02210251
Iteration 56, loss = 0.02183951
Iteration 57, loss = 0.02184533
Iteration 58, loss = 0.02166201
Iteration 59, loss = 0.02179677
Iteration 60, loss = 0.02148996
Iteration 61, loss = 0.02145683
Iteration 62, loss = 0.02148183
Iteration 63, loss = 0.02155545
Iteration 64, loss = 0.02109656
Iteration 65, loss = 0.02099496
Iteration 66, loss = 0.02101189
Iteration 67, loss = 0.02109672
Iteration 68, loss = 0.02099997
Iteration 69, loss = 0.02078402
Iteration 70, loss = 0.02078419
Iteration 71, loss = 0.02099706
Iteration 72, loss = 0.02074421
Iteration 73, loss = 0.02058347
Iteration 74, loss = 0.02066303
Iteration 75, loss = 0.02077690
Iteration 76, loss = 0.02031876
Iteration 77, loss = 0.02016302
Iteration 78, loss = 0.02023729
Iteration 79, loss = 0.02043924
Iteration 80, loss = 0.02025561
Iteration 81, loss = 0.02007689
Iteration 82, loss = 0.02017813
Iteration 83, loss = 0.02013317
Iteration 84, loss = 0.02003058
Iteration 85, loss = 0.02011811
Iteration 86, loss = 0.02011057
Iteration 87, loss = 0.02013335
Iteration 88, loss = 0.01960782
Iteration 89, loss = 0.02018295
Iteration 90, loss = 0.02025761
Iteration 91, loss = 0.01967235
Iteration 92, loss = 0.01985305
Iteration 93, loss = 0.01979548
Iteration 94, loss = 0.01989278
Iteration 95, loss = 0.01949527
Iteration 96, loss = 0.02023969
Iteration 97, loss = 0.01971353
Iteration 98, loss = 0.01971305
Iteration 99, loss = 0.01976642
Iteration 100, loss = 0.01955615
Iteration 101, loss = 0.01965746
Iteration 102, loss = 0.01945822
Iteration 103, loss = 0.01961643
Iteration 104, loss = 0.01976779
Iteration 105, loss = 0.01983547
Iteration 106, loss = 0.01985652
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	85m28.070s
user	394m27.129s
sys	47m9.113s
Elapsed time: 5128 seconds
