Processing dataset: prepended_v9_minimal_processing

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: prepended_v9_minimal_processing (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on prepended_v9_minimal_processing...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on prepended_v9_minimal_processing...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on prepended_v9_minimal_processing...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on prepended_v9_minimal_processing...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on prepended_v9_minimal_processing...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.69843502
Iteration 2, loss = 0.61674635
Iteration 3, loss = 0.50380133
Iteration 4, loss = 0.38214339
Iteration 5, loss = 0.28017540
Iteration 6, loss = 0.20535361
Iteration 7, loss = 0.15338380
Iteration 8, loss = 0.11892754
Iteration 9, loss = 0.09536925
Iteration 10, loss = 0.07886651
Iteration 11, loss = 0.06697658
Iteration 12, loss = 0.05831668
Iteration 13, loss = 0.05175275
Iteration 14, loss = 0.04651043
Iteration 15, loss = 0.04241911
Iteration 16, loss = 0.03917813
Iteration 17, loss = 0.03657508
Iteration 18, loss = 0.03430213
Iteration 19, loss = 0.03250445
Iteration 20, loss = 0.03077664
Iteration 21, loss = 0.02954678
Iteration 22, loss = 0.02849191
Iteration 23, loss = 0.02733692
Iteration 24, loss = 0.02656254
Iteration 25, loss = 0.02581830
Iteration 26, loss = 0.02518860
Iteration 27, loss = 0.02475777
Iteration 28, loss = 0.02406435
Iteration 29, loss = 0.02345144
Iteration 30, loss = 0.02306927
Iteration 31, loss = 0.02269582
Iteration 32, loss = 0.02236538
Iteration 33, loss = 0.02214010
Iteration 34, loss = 0.02186131
Iteration 35, loss = 0.02167728
Iteration 36, loss = 0.02135426
Iteration 37, loss = 0.02112823
Iteration 38, loss = 0.02105126
Iteration 39, loss = 0.02081598
Iteration 40, loss = 0.02075208
Iteration 41, loss = 0.02047123
Iteration 42, loss = 0.02024670
Iteration 43, loss = 0.02006602
Iteration 44, loss = 0.02010068
Iteration 45, loss = 0.01994447
Iteration 46, loss = 0.01998012
Iteration 47, loss = 0.01981812
Iteration 48, loss = 0.01971156
Iteration 49, loss = 0.01979821
Iteration 50, loss = 0.01954498
Iteration 51, loss = 0.01968552
Iteration 52, loss = 0.01924640
Iteration 53, loss = 0.01924887
Iteration 54, loss = 0.01922290
Iteration 55, loss = 0.01919884
Iteration 56, loss = 0.01905987
Iteration 57, loss = 0.01908109
Iteration 58, loss = 0.01919870
Iteration 59, loss = 0.01902499
Iteration 60, loss = 0.01884617
Iteration 61, loss = 0.01887089
Iteration 62, loss = 0.01874459
Iteration 63, loss = 0.01861625
Iteration 64, loss = 0.01862184
Iteration 65, loss = 0.01847659
Iteration 66, loss = 0.01885271
Iteration 67, loss = 0.01856077
Iteration 68, loss = 0.01879376
Iteration 69, loss = 0.01835608
Iteration 70, loss = 0.01861735
Iteration 71, loss = 0.01845369
Iteration 72, loss = 0.01831318
Iteration 73, loss = 0.01861042
Iteration 74, loss = 0.01839400
Iteration 75, loss = 0.01838469
Iteration 76, loss = 0.01827190
Iteration 77, loss = 0.01844346
Iteration 78, loss = 0.01853445
Iteration 79, loss = 0.01820820
Iteration 80, loss = 0.01824242
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68586884
Iteration 2, loss = 0.61708165
Iteration 3, loss = 0.50878839
Iteration 4, loss = 0.38677804
Iteration 5, loss = 0.28349857
Iteration 6, loss = 0.20726511
Iteration 7, loss = 0.15491068
Iteration 8, loss = 0.11988670
Iteration 9, loss = 0.09618896
Iteration 10, loss = 0.07979286
Iteration 11, loss = 0.06793766
Iteration 12, loss = 0.05932205
Iteration 13, loss = 0.05247777
Iteration 14, loss = 0.04734871
Iteration 15, loss = 0.04347691
Iteration 16, loss = 0.03996729
Iteration 17, loss = 0.03736498
Iteration 18, loss = 0.03521221
Iteration 19, loss = 0.03329793
Iteration 20, loss = 0.03155302
Iteration 21, loss = 0.03037552
Iteration 22, loss = 0.02905553
Iteration 23, loss = 0.02818410
Iteration 24, loss = 0.02719013
Iteration 25, loss = 0.02651134
Iteration 26, loss = 0.02585337
Iteration 27, loss = 0.02514228
Iteration 28, loss = 0.02451892
Iteration 29, loss = 0.02408988
Iteration 30, loss = 0.02381495
Iteration 31, loss = 0.02336685
Iteration 32, loss = 0.02317783
Iteration 33, loss = 0.02293962
Iteration 34, loss = 0.02261500
Iteration 35, loss = 0.02204939
Iteration 36, loss = 0.02177058
Iteration 37, loss = 0.02164495
Iteration 38, loss = 0.02151611
Iteration 39, loss = 0.02135733
Iteration 40, loss = 0.02100218
Iteration 41, loss = 0.02099157
Iteration 42, loss = 0.02066419
Iteration 43, loss = 0.02061379
Iteration 44, loss = 0.02017781
Iteration 45, loss = 0.02026597
Iteration 46, loss = 0.02024546
Iteration 47, loss = 0.02032333
Iteration 48, loss = 0.02001724
Iteration 49, loss = 0.01997213
Iteration 50, loss = 0.01996216
Iteration 51, loss = 0.01958225
Iteration 52, loss = 0.01974484
Iteration 53, loss = 0.01977025
Iteration 54, loss = 0.01945578
Iteration 55, loss = 0.01940053
Iteration 56, loss = 0.01927625
Iteration 57, loss = 0.01949041
Iteration 58, loss = 0.01963495
Iteration 59, loss = 0.01903766
Iteration 60, loss = 0.01919829
Iteration 61, loss = 0.01903612
Iteration 62, loss = 0.01905566
Iteration 63, loss = 0.01904976
Iteration 64, loss = 0.01927330
Iteration 65, loss = 0.01895204
Iteration 66, loss = 0.01894599
Iteration 67, loss = 0.01896231
Iteration 68, loss = 0.01889640
Iteration 69, loss = 0.01860267
Iteration 70, loss = 0.01872216
Iteration 71, loss = 0.01871238
Iteration 72, loss = 0.01874149
Iteration 73, loss = 0.01868389
Iteration 74, loss = 0.01890476
Iteration 75, loss = 0.01859204
Iteration 76, loss = 0.01851606
Iteration 77, loss = 0.01857335
Iteration 78, loss = 0.01837879
Iteration 79, loss = 0.01861491
Iteration 80, loss = 0.01834971
Iteration 81, loss = 0.01849492
Iteration 82, loss = 0.01849661
Iteration 83, loss = 0.01820814
Iteration 84, loss = 0.01826368
Iteration 85, loss = 0.01846049
Iteration 86, loss = 0.01833764
Iteration 87, loss = 0.01850135
Iteration 88, loss = 0.01816318
Iteration 89, loss = 0.01821624
Iteration 90, loss = 0.01837457
Iteration 91, loss = 0.01819242
Iteration 92, loss = 0.01829567
Iteration 93, loss = 0.01833465
Iteration 94, loss = 0.01791371
Iteration 95, loss = 0.01805266
Iteration 96, loss = 0.01798723
Iteration 97, loss = 0.01790773
Iteration 98, loss = 0.01794755
Iteration 99, loss = 0.01817021
Iteration 100, loss = 0.01784386
Iteration 101, loss = 0.01778976
Iteration 102, loss = 0.01807829
Iteration 103, loss = 0.01789892
Iteration 104, loss = 0.01771869
Iteration 105, loss = 0.01805669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69061293
Iteration 2, loss = 0.60508372
Iteration 3, loss = 0.48271804
Iteration 4, loss = 0.35865238
Iteration 5, loss = 0.25898769
Iteration 6, loss = 0.18901901
Iteration 7, loss = 0.14173597
Iteration 8, loss = 0.11016797
Iteration 9, loss = 0.08871670
Iteration 10, loss = 0.07346617
Iteration 11, loss = 0.06275669
Iteration 12, loss = 0.05439209
Iteration 13, loss = 0.04820250
Iteration 14, loss = 0.04354569
Iteration 15, loss = 0.03949723
Iteration 16, loss = 0.03622426
Iteration 17, loss = 0.03394768
Iteration 18, loss = 0.03168069
Iteration 19, loss = 0.02984066
Iteration 20, loss = 0.02813004
Iteration 21, loss = 0.02696367
Iteration 22, loss = 0.02592105
Iteration 23, loss = 0.02496506
Iteration 24, loss = 0.02386024
Iteration 25, loss = 0.02316235
Iteration 26, loss = 0.02248792
Iteration 27, loss = 0.02210684
Iteration 28, loss = 0.02157199
Iteration 29, loss = 0.02112393
Iteration 30, loss = 0.02056638
Iteration 31, loss = 0.02007050
Iteration 32, loss = 0.01992747
Iteration 33, loss = 0.01956313
Iteration 34, loss = 0.01929444
Iteration 35, loss = 0.01902513
Iteration 36, loss = 0.01885054
Iteration 37, loss = 0.01864574
Iteration 38, loss = 0.01822490
Iteration 39, loss = 0.01813438
Iteration 40, loss = 0.01796552
Iteration 41, loss = 0.01806555
Iteration 42, loss = 0.01773548
Iteration 43, loss = 0.01770244
Iteration 44, loss = 0.01723337
Iteration 45, loss = 0.01726498
Iteration 46, loss = 0.01700259
Iteration 47, loss = 0.01687944
Iteration 48, loss = 0.01682158
Iteration 49, loss = 0.01689007
Iteration 50, loss = 0.01664138
Iteration 51, loss = 0.01675375
Iteration 52, loss = 0.01686645
Iteration 53, loss = 0.01647869
Iteration 54, loss = 0.01661035
Iteration 55, loss = 0.01653897
Iteration 56, loss = 0.01618676
Iteration 57, loss = 0.01637868
Iteration 58, loss = 0.01614448
Iteration 59, loss = 0.01614568
Iteration 60, loss = 0.01617221
Iteration 61, loss = 0.01602072
Iteration 62, loss = 0.01614131
Iteration 63, loss = 0.01598555
Iteration 64, loss = 0.01582152
Iteration 65, loss = 0.01584786
Iteration 66, loss = 0.01570750
Iteration 67, loss = 0.01571048
Iteration 68, loss = 0.01567739
Iteration 69, loss = 0.01583148
Iteration 70, loss = 0.01556840
Iteration 71, loss = 0.01563153
Iteration 72, loss = 0.01582581
Iteration 73, loss = 0.01550606
Iteration 74, loss = 0.01567448
Iteration 75, loss = 0.01546798
Iteration 76, loss = 0.01559958
Iteration 77, loss = 0.01546439
Iteration 78, loss = 0.01551422
Iteration 79, loss = 0.01564007
Iteration 80, loss = 0.01540950
Iteration 81, loss = 0.01516318
Iteration 82, loss = 0.01537815
Iteration 83, loss = 0.01534123
Iteration 84, loss = 0.01555822
Iteration 85, loss = 0.01531655
Iteration 86, loss = 0.01523690
Iteration 87, loss = 0.01520005
Iteration 88, loss = 0.01514972
Iteration 89, loss = 0.01521623
Iteration 90, loss = 0.01518859
Iteration 91, loss = 0.01526379
Iteration 92, loss = 0.01516345
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67210364
Iteration 2, loss = 0.58225859
Iteration 3, loss = 0.45677130
Iteration 4, loss = 0.33926421
Iteration 5, loss = 0.24691335
Iteration 6, loss = 0.18137193
Iteration 7, loss = 0.13702772
Iteration 8, loss = 0.10716026
Iteration 9, loss = 0.08647615
Iteration 10, loss = 0.07187976
Iteration 11, loss = 0.06128209
Iteration 12, loss = 0.05353205
Iteration 13, loss = 0.04764656
Iteration 14, loss = 0.04269503
Iteration 15, loss = 0.03879038
Iteration 16, loss = 0.03585695
Iteration 17, loss = 0.03338262
Iteration 18, loss = 0.03132523
Iteration 19, loss = 0.02951931
Iteration 20, loss = 0.02796998
Iteration 21, loss = 0.02672639
Iteration 22, loss = 0.02600111
Iteration 23, loss = 0.02456971
Iteration 24, loss = 0.02394509
Iteration 25, loss = 0.02316732
Iteration 26, loss = 0.02257239
Iteration 27, loss = 0.02188179
Iteration 28, loss = 0.02141080
Iteration 29, loss = 0.02084817
Iteration 30, loss = 0.02108900
Iteration 31, loss = 0.02047852
Iteration 32, loss = 0.01986566
Iteration 33, loss = 0.01924316
Iteration 34, loss = 0.01912137
Iteration 35, loss = 0.01883499
Iteration 36, loss = 0.01901660
Iteration 37, loss = 0.01844932
Iteration 38, loss = 0.01854643
Iteration 39, loss = 0.01799622
Iteration 40, loss = 0.01806762
Iteration 41, loss = 0.01776539
Iteration 42, loss = 0.01754559
Iteration 43, loss = 0.01731712
Iteration 44, loss = 0.01741757
Iteration 45, loss = 0.01731873
Iteration 46, loss = 0.01680743
Iteration 47, loss = 0.01678004
Iteration 48, loss = 0.01686991
Iteration 49, loss = 0.01674266
Iteration 50, loss = 0.01666383
Iteration 51, loss = 0.01660617
Iteration 52, loss = 0.01635638
Iteration 53, loss = 0.01680545
Iteration 54, loss = 0.01632976
Iteration 55, loss = 0.01612218
Iteration 56, loss = 0.01618316
Iteration 57, loss = 0.01628566
Iteration 58, loss = 0.01645936
Iteration 59, loss = 0.01608771
Iteration 60, loss = 0.01591930
Iteration 61, loss = 0.01576636
Iteration 62, loss = 0.01594081
Iteration 63, loss = 0.01587519
Iteration 64, loss = 0.01575794
Iteration 65, loss = 0.01551394
Iteration 66, loss = 0.01551088
Iteration 67, loss = 0.01559342
Iteration 68, loss = 0.01558772
Iteration 69, loss = 0.01566209
Iteration 70, loss = 0.01548075
Iteration 71, loss = 0.01558598
Iteration 72, loss = 0.01527309
Iteration 73, loss = 0.01548480
Iteration 74, loss = 0.01560457
Iteration 75, loss = 0.01554066
Iteration 76, loss = 0.01534099
Iteration 77, loss = 0.01538192
Iteration 78, loss = 0.01521182
Iteration 79, loss = 0.01517873
Iteration 80, loss = 0.01511241
Iteration 81, loss = 0.01539060
Iteration 82, loss = 0.01524591
Iteration 83, loss = 0.01511912
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70309805
Iteration 2, loss = 0.61584227
Iteration 3, loss = 0.49648962
Iteration 4, loss = 0.37359098
Iteration 5, loss = 0.27247084
Iteration 6, loss = 0.19898230
Iteration 7, loss = 0.14911006
Iteration 8, loss = 0.11557710
Iteration 9, loss = 0.09248200
Iteration 10, loss = 0.07660398
Iteration 11, loss = 0.06491294
Iteration 12, loss = 0.05634230
Iteration 13, loss = 0.04991927
Iteration 14, loss = 0.04456841
Iteration 15, loss = 0.04059435
Iteration 16, loss = 0.03711773
Iteration 17, loss = 0.03459243
Iteration 18, loss = 0.03250979
Iteration 19, loss = 0.03056948
Iteration 20, loss = 0.02894653
Iteration 21, loss = 0.02754823
Iteration 22, loss = 0.02621118
Iteration 23, loss = 0.02551636
Iteration 24, loss = 0.02447327
Iteration 25, loss = 0.02373687
Iteration 26, loss = 0.02304743
Iteration 27, loss = 0.02242603
Iteration 28, loss = 0.02194667
Iteration 29, loss = 0.02141875
Iteration 30, loss = 0.02090602
Iteration 31, loss = 0.02054680
Iteration 32, loss = 0.02021020
Iteration 33, loss = 0.01984517
Iteration 34, loss = 0.01979693
Iteration 35, loss = 0.01953806
Iteration 36, loss = 0.01901977
Iteration 37, loss = 0.01885928
Iteration 38, loss = 0.01890348
Iteration 39, loss = 0.01848595
Iteration 40, loss = 0.01823102
Iteration 41, loss = 0.01822296
Iteration 42, loss = 0.01797060
Iteration 43, loss = 0.01768880
Iteration 44, loss = 0.01771770
Iteration 45, loss = 0.01768185
Iteration 46, loss = 0.01754309
Iteration 47, loss = 0.01758841
Iteration 48, loss = 0.01707820
Iteration 49, loss = 0.01724170
Iteration 50, loss = 0.01696330
Iteration 51, loss = 0.01693326
Iteration 52, loss = 0.01673878
Iteration 53, loss = 0.01689309
Iteration 54, loss = 0.01683167
Iteration 55, loss = 0.01647135
Iteration 56, loss = 0.01659413
Iteration 57, loss = 0.01661929
Iteration 58, loss = 0.01666570
Iteration 59, loss = 0.01646642
Iteration 60, loss = 0.01643271
Iteration 61, loss = 0.01627836
Iteration 62, loss = 0.01635058
Iteration 63, loss = 0.01628904
Iteration 64, loss = 0.01628943
Iteration 65, loss = 0.01647884
Iteration 66, loss = 0.01634017
Iteration 67, loss = 0.01627085
Iteration 68, loss = 0.01603506
Iteration 69, loss = 0.01612642
Iteration 70, loss = 0.01614812
Iteration 71, loss = 0.01599850
Iteration 72, loss = 0.01595158
Iteration 73, loss = 0.01590412
Iteration 74, loss = 0.01598822
Iteration 75, loss = 0.01593470
Iteration 76, loss = 0.01571571
Iteration 77, loss = 0.01585849
Iteration 78, loss = 0.01567568
Iteration 79, loss = 0.01567685
Iteration 80, loss = 0.01583660
Iteration 81, loss = 0.01586223
Iteration 82, loss = 0.01577572
Iteration 83, loss = 0.01571862
Iteration 84, loss = 0.01561915
Iteration 85, loss = 0.01573986
Iteration 86, loss = 0.01554426
Iteration 87, loss = 0.01537862
Iteration 88, loss = 0.01559135
Iteration 89, loss = 0.01537047
Iteration 90, loss = 0.01577183
Iteration 91, loss = 0.01544604
Iteration 92, loss = 0.01547137
Iteration 93, loss = 0.01552091
Iteration 94, loss = 0.01564051
Iteration 95, loss = 0.01525879
Iteration 96, loss = 0.01533800
Iteration 97, loss = 0.01523121
Iteration 98, loss = 0.01544725
Iteration 99, loss = 0.01538050
Iteration 100, loss = 0.01539073
Iteration 101, loss = 0.01528045
Iteration 102, loss = 0.01513796
Iteration 103, loss = 0.01527047
Iteration 104, loss = 0.01534889
Iteration 105, loss = 0.01523573
Iteration 106, loss = 0.01539522
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69843502
Iteration 2, loss = 0.61674635
Iteration 3, loss = 0.50380133
Iteration 4, loss = 0.38214339
Iteration 5, loss = 0.28017540
Iteration 6, loss = 0.20535361
Iteration 7, loss = 0.15338380
Iteration 8, loss = 0.11892754
Iteration 9, loss = 0.09536925
Iteration 10, loss = 0.07886651
Iteration 11, loss = 0.06697658
Iteration 12, loss = 0.05831668
Iteration 13, loss = 0.05175275
Iteration 14, loss = 0.04651043
Iteration 15, loss = 0.04241911
Iteration 16, loss = 0.03917813
Iteration 17, loss = 0.03657508
Iteration 18, loss = 0.03430213
Iteration 19, loss = 0.03250445
Iteration 20, loss = 0.03077664
Iteration 21, loss = 0.02954678
Iteration 22, loss = 0.02849191
Iteration 23, loss = 0.02733692
Iteration 24, loss = 0.02656254
Iteration 25, loss = 0.02581830
Iteration 26, loss = 0.02518860
Iteration 27, loss = 0.02475777
Iteration 28, loss = 0.02406435
Iteration 29, loss = 0.02345144
Iteration 30, loss = 0.02306927
Iteration 31, loss = 0.02269582
Iteration 32, loss = 0.02236538
Iteration 33, loss = 0.02214010
Iteration 34, loss = 0.02186131
Iteration 35, loss = 0.02167728
Iteration 36, loss = 0.02135426
Iteration 37, loss = 0.02112823
Iteration 38, loss = 0.02105126
Iteration 39, loss = 0.02081598
Iteration 40, loss = 0.02075208
Iteration 41, loss = 0.02047123
Iteration 42, loss = 0.02024670
Iteration 43, loss = 0.02006602
Iteration 44, loss = 0.02010068
Iteration 45, loss = 0.01994447
Iteration 46, loss = 0.01998012
Iteration 47, loss = 0.01981812
Iteration 48, loss = 0.01971156
Iteration 49, loss = 0.01979821
Iteration 50, loss = 0.01954498
Iteration 51, loss = 0.01968552
Iteration 52, loss = 0.01924640
Iteration 53, loss = 0.01924887
Iteration 54, loss = 0.01922290
Iteration 55, loss = 0.01919884
Iteration 56, loss = 0.01905987
Iteration 57, loss = 0.01908109
Iteration 58, loss = 0.01919870
Iteration 59, loss = 0.01902499
Iteration 60, loss = 0.01884617
Iteration 61, loss = 0.01887089
Iteration 62, loss = 0.01874459
Iteration 63, loss = 0.01861625
Iteration 64, loss = 0.01862184
Iteration 65, loss = 0.01847659
Iteration 66, loss = 0.01885271
Iteration 67, loss = 0.01856077
Iteration 68, loss = 0.01879376
Iteration 69, loss = 0.01835608
Iteration 70, loss = 0.01861735
Iteration 71, loss = 0.01845369
Iteration 72, loss = 0.01831318
Iteration 73, loss = 0.01861042
Iteration 74, loss = 0.01839400
Iteration 75, loss = 0.01838469
Iteration 76, loss = 0.01827190
Iteration 77, loss = 0.01844346
Iteration 78, loss = 0.01853445
Iteration 79, loss = 0.01820820
Iteration 80, loss = 0.01824242
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68586884
Iteration 2, loss = 0.61708165
Iteration 3, loss = 0.50878839
Iteration 4, loss = 0.38677804
Iteration 5, loss = 0.28349857
Iteration 6, loss = 0.20726511
Iteration 7, loss = 0.15491068
Iteration 8, loss = 0.11988670
Iteration 9, loss = 0.09618896
Iteration 10, loss = 0.07979286
Iteration 11, loss = 0.06793766
Iteration 12, loss = 0.05932205
Iteration 13, loss = 0.05247777
Iteration 14, loss = 0.04734871
Iteration 15, loss = 0.04347691
Iteration 16, loss = 0.03996729
Iteration 17, loss = 0.03736498
Iteration 18, loss = 0.03521221
Iteration 19, loss = 0.03329793
Iteration 20, loss = 0.03155302
Iteration 21, loss = 0.03037552
Iteration 22, loss = 0.02905553
Iteration 23, loss = 0.02818410
Iteration 24, loss = 0.02719013
Iteration 25, loss = 0.02651134
Iteration 26, loss = 0.02585337
Iteration 27, loss = 0.02514228
Iteration 28, loss = 0.02451892
Iteration 29, loss = 0.02408988
Iteration 30, loss = 0.02381495
Iteration 31, loss = 0.02336685
Iteration 32, loss = 0.02317783
Iteration 33, loss = 0.02293962
Iteration 34, loss = 0.02261500
Iteration 35, loss = 0.02204939
Iteration 36, loss = 0.02177058
Iteration 37, loss = 0.02164495
Iteration 38, loss = 0.02151611
Iteration 39, loss = 0.02135733
Iteration 40, loss = 0.02100218
Iteration 41, loss = 0.02099157
Iteration 42, loss = 0.02066419
Iteration 43, loss = 0.02061379
Iteration 44, loss = 0.02017781
Iteration 45, loss = 0.02026597
Iteration 46, loss = 0.02024546
Iteration 47, loss = 0.02032333
Iteration 48, loss = 0.02001724
Iteration 49, loss = 0.01997213
Iteration 50, loss = 0.01996216
Iteration 51, loss = 0.01958225
Iteration 52, loss = 0.01974484
Iteration 53, loss = 0.01977025
Iteration 54, loss = 0.01945578
Iteration 55, loss = 0.01940053
Iteration 56, loss = 0.01927625
Iteration 57, loss = 0.01949041
Iteration 58, loss = 0.01963495
Iteration 59, loss = 0.01903766
Iteration 60, loss = 0.01919829
Iteration 61, loss = 0.01903612
Iteration 62, loss = 0.01905566
Iteration 63, loss = 0.01904976
Iteration 64, loss = 0.01927330
Iteration 65, loss = 0.01895204
Iteration 66, loss = 0.01894599
Iteration 67, loss = 0.01896231
Iteration 68, loss = 0.01889640
Iteration 69, loss = 0.01860267
Iteration 70, loss = 0.01872216
Iteration 71, loss = 0.01871238
Iteration 72, loss = 0.01874149
Iteration 73, loss = 0.01868389
Iteration 74, loss = 0.01890476
Iteration 75, loss = 0.01859204
Iteration 76, loss = 0.01851606
Iteration 77, loss = 0.01857335
Iteration 78, loss = 0.01837879
Iteration 79, loss = 0.01861491
Iteration 80, loss = 0.01834971
Iteration 81, loss = 0.01849492
Iteration 82, loss = 0.01849661
Iteration 83, loss = 0.01820814
Iteration 84, loss = 0.01826368
Iteration 85, loss = 0.01846049
Iteration 86, loss = 0.01833764
Iteration 87, loss = 0.01850135
Iteration 88, loss = 0.01816318
Iteration 89, loss = 0.01821624
Iteration 90, loss = 0.01837457
Iteration 91, loss = 0.01819242
Iteration 92, loss = 0.01829567
Iteration 93, loss = 0.01833465
Iteration 94, loss = 0.01791371
Iteration 95, loss = 0.01805266
Iteration 96, loss = 0.01798723
Iteration 97, loss = 0.01790773
Iteration 98, loss = 0.01794755
Iteration 99, loss = 0.01817021
Iteration 100, loss = 0.01784386
Iteration 101, loss = 0.01778976
Iteration 102, loss = 0.01807829
Iteration 103, loss = 0.01789892
Iteration 104, loss = 0.01771869
Iteration 105, loss = 0.01805669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69061293
Iteration 2, loss = 0.60508372
Iteration 3, loss = 0.48271804
Iteration 4, loss = 0.35865238
Iteration 5, loss = 0.25898769
Iteration 6, loss = 0.18901901
Iteration 7, loss = 0.14173597
Iteration 8, loss = 0.11016797
Iteration 9, loss = 0.08871670
Iteration 10, loss = 0.07346617
Iteration 11, loss = 0.06275669
Iteration 12, loss = 0.05439209
Iteration 13, loss = 0.04820250
Iteration 14, loss = 0.04354569
Iteration 15, loss = 0.03949723
Iteration 16, loss = 0.03622426
Iteration 17, loss = 0.03394768
Iteration 18, loss = 0.03168069
Iteration 19, loss = 0.02984066
Iteration 20, loss = 0.02813004
Iteration 21, loss = 0.02696367
Iteration 22, loss = 0.02592105
Iteration 23, loss = 0.02496506
Iteration 24, loss = 0.02386024
Iteration 25, loss = 0.02316235
Iteration 26, loss = 0.02248792
Iteration 27, loss = 0.02210684
Iteration 28, loss = 0.02157199
Iteration 29, loss = 0.02112393
Iteration 30, loss = 0.02056638
Iteration 31, loss = 0.02007050
Iteration 32, loss = 0.01992747
Iteration 33, loss = 0.01956313
Iteration 34, loss = 0.01929444
Iteration 35, loss = 0.01902513
Iteration 36, loss = 0.01885054
Iteration 37, loss = 0.01864574
Iteration 38, loss = 0.01822490
Iteration 39, loss = 0.01813438
Iteration 40, loss = 0.01796552
Iteration 41, loss = 0.01806555
Iteration 42, loss = 0.01773548
Iteration 43, loss = 0.01770244
Iteration 44, loss = 0.01723337
Iteration 45, loss = 0.01726498
Iteration 46, loss = 0.01700259
Iteration 47, loss = 0.01687944
Iteration 48, loss = 0.01682158
Iteration 49, loss = 0.01689007
Iteration 50, loss = 0.01664138
Iteration 51, loss = 0.01675375
Iteration 52, loss = 0.01686645
Iteration 53, loss = 0.01647869
Iteration 54, loss = 0.01661035
Iteration 55, loss = 0.01653897
Iteration 56, loss = 0.01618676
Iteration 57, loss = 0.01637868
Iteration 58, loss = 0.01614448
Iteration 59, loss = 0.01614568
Iteration 60, loss = 0.01617221
Iteration 61, loss = 0.01602072
Iteration 62, loss = 0.01614131
Iteration 63, loss = 0.01598555
Iteration 64, loss = 0.01582152
Iteration 65, loss = 0.01584786
Iteration 66, loss = 0.01570750
Iteration 67, loss = 0.01571048
Iteration 68, loss = 0.01567739
Iteration 69, loss = 0.01583148
Iteration 70, loss = 0.01556840
Iteration 71, loss = 0.01563153
Iteration 72, loss = 0.01582581
Iteration 73, loss = 0.01550606
Iteration 74, loss = 0.01567448
Iteration 75, loss = 0.01546798
Iteration 76, loss = 0.01559958
Iteration 77, loss = 0.01546439
Iteration 78, loss = 0.01551422
Iteration 79, loss = 0.01564007
Iteration 80, loss = 0.01540950
Iteration 81, loss = 0.01516318
Iteration 82, loss = 0.01537815
Iteration 83, loss = 0.01534123
Iteration 84, loss = 0.01555822
Iteration 85, loss = 0.01531655
Iteration 86, loss = 0.01523690
Iteration 87, loss = 0.01520005
Iteration 88, loss = 0.01514972
Iteration 89, loss = 0.01521623
Iteration 90, loss = 0.01518859
Iteration 91, loss = 0.01526379
Iteration 92, loss = 0.01516345
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67210364
Iteration 2, loss = 0.58225859
Iteration 3, loss = 0.45677130
Iteration 4, loss = 0.33926421
Iteration 5, loss = 0.24691335
Iteration 6, loss = 0.18137193
Iteration 7, loss = 0.13702772
Iteration 8, loss = 0.10716026
Iteration 9, loss = 0.08647615
Iteration 10, loss = 0.07187976
Iteration 11, loss = 0.06128209
Iteration 12, loss = 0.05353205
Iteration 13, loss = 0.04764656
Iteration 14, loss = 0.04269503
Iteration 15, loss = 0.03879038
Iteration 16, loss = 0.03585695
Iteration 17, loss = 0.03338262
Iteration 18, loss = 0.03132523
Iteration 19, loss = 0.02951931
Iteration 20, loss = 0.02796998
Iteration 21, loss = 0.02672639
Iteration 22, loss = 0.02600111
Iteration 23, loss = 0.02456971
Iteration 24, loss = 0.02394509
Iteration 25, loss = 0.02316732
Iteration 26, loss = 0.02257239
Iteration 27, loss = 0.02188179
Iteration 28, loss = 0.02141080
Iteration 29, loss = 0.02084817
Iteration 30, loss = 0.02108900
Iteration 31, loss = 0.02047852
Iteration 32, loss = 0.01986566
Iteration 33, loss = 0.01924316
Iteration 34, loss = 0.01912137
Iteration 35, loss = 0.01883499
Iteration 36, loss = 0.01901660
Iteration 37, loss = 0.01844932
Iteration 38, loss = 0.01854643
Iteration 39, loss = 0.01799622
Iteration 40, loss = 0.01806762
Iteration 41, loss = 0.01776539
Iteration 42, loss = 0.01754559
Iteration 43, loss = 0.01731712
Iteration 44, loss = 0.01741757
Iteration 45, loss = 0.01731873
Iteration 46, loss = 0.01680743
Iteration 47, loss = 0.01678004
Iteration 48, loss = 0.01686991
Iteration 49, loss = 0.01674266
Iteration 50, loss = 0.01666383
Iteration 51, loss = 0.01660617
Iteration 52, loss = 0.01635638
Iteration 53, loss = 0.01680545
Iteration 54, loss = 0.01632976
Iteration 55, loss = 0.01612218
Iteration 56, loss = 0.01618316
Iteration 57, loss = 0.01628566
Iteration 58, loss = 0.01645936
Iteration 59, loss = 0.01608771
Iteration 60, loss = 0.01591930
Iteration 61, loss = 0.01576636
Iteration 62, loss = 0.01594081
Iteration 63, loss = 0.01587519
Iteration 64, loss = 0.01575794
Iteration 65, loss = 0.01551394
Iteration 66, loss = 0.01551088
Iteration 67, loss = 0.01559342
Iteration 68, loss = 0.01558772
Iteration 69, loss = 0.01566209
Iteration 70, loss = 0.01548075
Iteration 71, loss = 0.01558598
Iteration 72, loss = 0.01527309
Iteration 73, loss = 0.01548480
Iteration 74, loss = 0.01560457
Iteration 75, loss = 0.01554066
Iteration 76, loss = 0.01534099
Iteration 77, loss = 0.01538192
Iteration 78, loss = 0.01521182
Iteration 79, loss = 0.01517873
Iteration 80, loss = 0.01511241
Iteration 81, loss = 0.01539060
Iteration 82, loss = 0.01524591
Iteration 83, loss = 0.01511912
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70309805
Iteration 2, loss = 0.61584227
Iteration 3, loss = 0.49648962
Iteration 4, loss = 0.37359098
Iteration 5, loss = 0.27247084
Iteration 6, loss = 0.19898230
Iteration 7, loss = 0.14911006
Iteration 8, loss = 0.11557710
Iteration 9, loss = 0.09248200
Iteration 10, loss = 0.07660398
Iteration 11, loss = 0.06491294
Iteration 12, loss = 0.05634230
Iteration 13, loss = 0.04991927
Iteration 14, loss = 0.04456841
Iteration 15, loss = 0.04059435
Iteration 16, loss = 0.03711773
Iteration 17, loss = 0.03459243
Iteration 18, loss = 0.03250979
Iteration 19, loss = 0.03056948
Iteration 20, loss = 0.02894653
Iteration 21, loss = 0.02754823
Iteration 22, loss = 0.02621118
Iteration 23, loss = 0.02551636
Iteration 24, loss = 0.02447327
Iteration 25, loss = 0.02373687
Iteration 26, loss = 0.02304743
Iteration 27, loss = 0.02242603
Iteration 28, loss = 0.02194667
Iteration 29, loss = 0.02141875
Iteration 30, loss = 0.02090602
Iteration 31, loss = 0.02054680
Iteration 32, loss = 0.02021020
Iteration 33, loss = 0.01984517
Iteration 34, loss = 0.01979693
Iteration 35, loss = 0.01953806
Iteration 36, loss = 0.01901977
Iteration 37, loss = 0.01885928
Iteration 38, loss = 0.01890348
Iteration 39, loss = 0.01848595
Iteration 40, loss = 0.01823102
Iteration 41, loss = 0.01822296
Iteration 42, loss = 0.01797060
Iteration 43, loss = 0.01768880
Iteration 44, loss = 0.01771770
Iteration 45, loss = 0.01768185
Iteration 46, loss = 0.01754309
Iteration 47, loss = 0.01758841
Iteration 48, loss = 0.01707820
Iteration 49, loss = 0.01724170
Iteration 50, loss = 0.01696330
Iteration 51, loss = 0.01693326
Iteration 52, loss = 0.01673878
Iteration 53, loss = 0.01689309
Iteration 54, loss = 0.01683167
Iteration 55, loss = 0.01647135
Iteration 56, loss = 0.01659413
Iteration 57, loss = 0.01661929
Iteration 58, loss = 0.01666570
Iteration 59, loss = 0.01646642
Iteration 60, loss = 0.01643271
Iteration 61, loss = 0.01627836
Iteration 62, loss = 0.01635058
Iteration 63, loss = 0.01628904
Iteration 64, loss = 0.01628943
Iteration 65, loss = 0.01647884
Iteration 66, loss = 0.01634017
Iteration 67, loss = 0.01627085
Iteration 68, loss = 0.01603506
Iteration 69, loss = 0.01612642
Iteration 70, loss = 0.01614812
Iteration 71, loss = 0.01599850
Iteration 72, loss = 0.01595158
Iteration 73, loss = 0.01590412
Iteration 74, loss = 0.01598822
Iteration 75, loss = 0.01593470
Iteration 76, loss = 0.01571571
Iteration 77, loss = 0.01585849
Iteration 78, loss = 0.01567568
Iteration 79, loss = 0.01567685
Iteration 80, loss = 0.01583660
Iteration 81, loss = 0.01586223
Iteration 82, loss = 0.01577572
Iteration 83, loss = 0.01571862
Iteration 84, loss = 0.01561915
Iteration 85, loss = 0.01573986
Iteration 86, loss = 0.01554426
Iteration 87, loss = 0.01537862
Iteration 88, loss = 0.01559135
Iteration 89, loss = 0.01537047
Iteration 90, loss = 0.01577183
Iteration 91, loss = 0.01544604
Iteration 92, loss = 0.01547137
Iteration 93, loss = 0.01552091
Iteration 94, loss = 0.01564051
Iteration 95, loss = 0.01525879
Iteration 96, loss = 0.01533800
Iteration 97, loss = 0.01523121
Iteration 98, loss = 0.01544725
Iteration 99, loss = 0.01538050
Iteration 100, loss = 0.01539073
Iteration 101, loss = 0.01528045
Iteration 102, loss = 0.01513796
Iteration 103, loss = 0.01527047
Iteration 104, loss = 0.01534889
Iteration 105, loss = 0.01523573
Iteration 106, loss = 0.01539522
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69843502
Iteration 2, loss = 0.61674635
Iteration 3, loss = 0.50380133
Iteration 4, loss = 0.38214339
Iteration 5, loss = 0.28017540
Iteration 6, loss = 0.20535361
Iteration 7, loss = 0.15338380
Iteration 8, loss = 0.11892754
Iteration 9, loss = 0.09536925
Iteration 10, loss = 0.07886651
Iteration 11, loss = 0.06697658
Iteration 12, loss = 0.05831668
Iteration 13, loss = 0.05175275
Iteration 14, loss = 0.04651043
Iteration 15, loss = 0.04241911
Iteration 16, loss = 0.03917813
Iteration 17, loss = 0.03657508
Iteration 18, loss = 0.03430213
Iteration 19, loss = 0.03250445
Iteration 20, loss = 0.03077664
Iteration 21, loss = 0.02954678
Iteration 22, loss = 0.02849191
Iteration 23, loss = 0.02733692
Iteration 24, loss = 0.02656254
Iteration 25, loss = 0.02581830
Iteration 26, loss = 0.02518860
Iteration 27, loss = 0.02475777
Iteration 28, loss = 0.02406435
Iteration 29, loss = 0.02345144
Iteration 30, loss = 0.02306927
Iteration 31, loss = 0.02269582
Iteration 32, loss = 0.02236538
Iteration 33, loss = 0.02214010
Iteration 34, loss = 0.02186131
Iteration 35, loss = 0.02167728
Iteration 36, loss = 0.02135426
Iteration 37, loss = 0.02112823
Iteration 38, loss = 0.02105126
Iteration 39, loss = 0.02081598
Iteration 40, loss = 0.02075208
Iteration 41, loss = 0.02047123
Iteration 42, loss = 0.02024670
Iteration 43, loss = 0.02006602
Iteration 44, loss = 0.02010068
Iteration 45, loss = 0.01994447
Iteration 46, loss = 0.01998012
Iteration 47, loss = 0.01981812
Iteration 48, loss = 0.01971156
Iteration 49, loss = 0.01979821
Iteration 50, loss = 0.01954498
Iteration 51, loss = 0.01968552
Iteration 52, loss = 0.01924640
Iteration 53, loss = 0.01924887
Iteration 54, loss = 0.01922290
Iteration 55, loss = 0.01919884
Iteration 56, loss = 0.01905987
Iteration 57, loss = 0.01908109
Iteration 58, loss = 0.01919870
Iteration 59, loss = 0.01902499
Iteration 60, loss = 0.01884617
Iteration 61, loss = 0.01887089
Iteration 62, loss = 0.01874459
Iteration 63, loss = 0.01861625
Iteration 64, loss = 0.01862184
Iteration 65, loss = 0.01847659
Iteration 66, loss = 0.01885271
Iteration 67, loss = 0.01856077
Iteration 68, loss = 0.01879376
Iteration 69, loss = 0.01835608
Iteration 70, loss = 0.01861735
Iteration 71, loss = 0.01845369
Iteration 72, loss = 0.01831318
Iteration 73, loss = 0.01861042
Iteration 74, loss = 0.01839400
Iteration 75, loss = 0.01838469
Iteration 76, loss = 0.01827190
Iteration 77, loss = 0.01844346
Iteration 78, loss = 0.01853445
Iteration 79, loss = 0.01820820
Iteration 80, loss = 0.01824242
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68586884
Iteration 2, loss = 0.61708165
Iteration 3, loss = 0.50878839
Iteration 4, loss = 0.38677804
Iteration 5, loss = 0.28349857
Iteration 6, loss = 0.20726511
Iteration 7, loss = 0.15491068
Iteration 8, loss = 0.11988670
Iteration 9, loss = 0.09618896
Iteration 10, loss = 0.07979286
Iteration 11, loss = 0.06793766
Iteration 12, loss = 0.05932205
Iteration 13, loss = 0.05247777
Iteration 14, loss = 0.04734871
Iteration 15, loss = 0.04347691
Iteration 16, loss = 0.03996729
Iteration 17, loss = 0.03736498
Iteration 18, loss = 0.03521221
Iteration 19, loss = 0.03329793
Iteration 20, loss = 0.03155302
Iteration 21, loss = 0.03037552
Iteration 22, loss = 0.02905553
Iteration 23, loss = 0.02818410
Iteration 24, loss = 0.02719013
Iteration 25, loss = 0.02651134
Iteration 26, loss = 0.02585337
Iteration 27, loss = 0.02514228
Iteration 28, loss = 0.02451892
Iteration 29, loss = 0.02408988
Iteration 30, loss = 0.02381495
Iteration 31, loss = 0.02336685
Iteration 32, loss = 0.02317783
Iteration 33, loss = 0.02293962
Iteration 34, loss = 0.02261500
Iteration 35, loss = 0.02204939
Iteration 36, loss = 0.02177058
Iteration 37, loss = 0.02164495
Iteration 38, loss = 0.02151611
Iteration 39, loss = 0.02135733
Iteration 40, loss = 0.02100218
Iteration 41, loss = 0.02099157
Iteration 42, loss = 0.02066419
Iteration 43, loss = 0.02061379
Iteration 44, loss = 0.02017781
Iteration 45, loss = 0.02026597
Iteration 46, loss = 0.02024546
Iteration 47, loss = 0.02032333
Iteration 48, loss = 0.02001724
Iteration 49, loss = 0.01997213
Iteration 50, loss = 0.01996216
Iteration 51, loss = 0.01958225
Iteration 52, loss = 0.01974484
Iteration 53, loss = 0.01977025
Iteration 54, loss = 0.01945578
Iteration 55, loss = 0.01940053
Iteration 56, loss = 0.01927625
Iteration 57, loss = 0.01949041
Iteration 58, loss = 0.01963495
Iteration 59, loss = 0.01903766
Iteration 60, loss = 0.01919829
Iteration 61, loss = 0.01903612
Iteration 62, loss = 0.01905566
Iteration 63, loss = 0.01904976
Iteration 64, loss = 0.01927330
Iteration 65, loss = 0.01895204
Iteration 66, loss = 0.01894599
Iteration 67, loss = 0.01896231
Iteration 68, loss = 0.01889640
Iteration 69, loss = 0.01860267
Iteration 70, loss = 0.01872216
Iteration 71, loss = 0.01871238
Iteration 72, loss = 0.01874149
Iteration 73, loss = 0.01868389
Iteration 74, loss = 0.01890476
Iteration 75, loss = 0.01859204
Iteration 76, loss = 0.01851606
Iteration 77, loss = 0.01857335
Iteration 78, loss = 0.01837879
Iteration 79, loss = 0.01861491
Iteration 80, loss = 0.01834971
Iteration 81, loss = 0.01849492
Iteration 82, loss = 0.01849661
Iteration 83, loss = 0.01820814
Iteration 84, loss = 0.01826368
Iteration 85, loss = 0.01846049
Iteration 86, loss = 0.01833764
Iteration 87, loss = 0.01850135
Iteration 88, loss = 0.01816318
Iteration 89, loss = 0.01821624
Iteration 90, loss = 0.01837457
Iteration 91, loss = 0.01819242
Iteration 92, loss = 0.01829567
Iteration 93, loss = 0.01833465
Iteration 94, loss = 0.01791371
Iteration 95, loss = 0.01805266
Iteration 96, loss = 0.01798723
Iteration 97, loss = 0.01790773
Iteration 98, loss = 0.01794755
Iteration 99, loss = 0.01817021
Iteration 100, loss = 0.01784386
Iteration 101, loss = 0.01778976
Iteration 102, loss = 0.01807829
Iteration 103, loss = 0.01789892
Iteration 104, loss = 0.01771869
Iteration 105, loss = 0.01805669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69061293
Iteration 2, loss = 0.60508372
Iteration 3, loss = 0.48271804
Iteration 4, loss = 0.35865238
Iteration 5, loss = 0.25898769
Iteration 6, loss = 0.18901901
Iteration 7, loss = 0.14173597
Iteration 8, loss = 0.11016797
Iteration 9, loss = 0.08871670
Iteration 10, loss = 0.07346617
Iteration 11, loss = 0.06275669
Iteration 12, loss = 0.05439209
Iteration 13, loss = 0.04820250
Iteration 14, loss = 0.04354569
Iteration 15, loss = 0.03949723
Iteration 16, loss = 0.03622426
Iteration 17, loss = 0.03394768
Iteration 18, loss = 0.03168069
Iteration 19, loss = 0.02984066
Iteration 20, loss = 0.02813004
Iteration 21, loss = 0.02696367
Iteration 22, loss = 0.02592105
Iteration 23, loss = 0.02496506
Iteration 24, loss = 0.02386024
Iteration 25, loss = 0.02316235
Iteration 26, loss = 0.02248792
Iteration 27, loss = 0.02210684
Iteration 28, loss = 0.02157199
Iteration 29, loss = 0.02112393
Iteration 30, loss = 0.02056638
Iteration 31, loss = 0.02007050
Iteration 32, loss = 0.01992747
Iteration 33, loss = 0.01956313
Iteration 34, loss = 0.01929444
Iteration 35, loss = 0.01902513
Iteration 36, loss = 0.01885054
Iteration 37, loss = 0.01864574
Iteration 38, loss = 0.01822490
Iteration 39, loss = 0.01813438
Iteration 40, loss = 0.01796552
Iteration 41, loss = 0.01806555
Iteration 42, loss = 0.01773548
Iteration 43, loss = 0.01770244
Iteration 44, loss = 0.01723337
Iteration 45, loss = 0.01726498
Iteration 46, loss = 0.01700259
Iteration 47, loss = 0.01687944
Iteration 48, loss = 0.01682158
Iteration 49, loss = 0.01689007
Iteration 50, loss = 0.01664138
Iteration 51, loss = 0.01675375
Iteration 52, loss = 0.01686645
Iteration 53, loss = 0.01647869
Iteration 54, loss = 0.01661035
Iteration 55, loss = 0.01653897
Iteration 56, loss = 0.01618676
Iteration 57, loss = 0.01637868
Iteration 58, loss = 0.01614448
Iteration 59, loss = 0.01614568
Iteration 60, loss = 0.01617221
Iteration 61, loss = 0.01602072
Iteration 62, loss = 0.01614131
Iteration 63, loss = 0.01598555
Iteration 64, loss = 0.01582152
Iteration 65, loss = 0.01584786
Iteration 66, loss = 0.01570750
Iteration 67, loss = 0.01571048
Iteration 68, loss = 0.01567739
Iteration 69, loss = 0.01583148
Iteration 70, loss = 0.01556840
Iteration 71, loss = 0.01563153
Iteration 72, loss = 0.01582581
Iteration 73, loss = 0.01550606
Iteration 74, loss = 0.01567448
Iteration 75, loss = 0.01546798
Iteration 76, loss = 0.01559958
Iteration 77, loss = 0.01546439
Iteration 78, loss = 0.01551422
Iteration 79, loss = 0.01564007
Iteration 80, loss = 0.01540950
Iteration 81, loss = 0.01516318
Iteration 82, loss = 0.01537815
Iteration 83, loss = 0.01534123
Iteration 84, loss = 0.01555822
Iteration 85, loss = 0.01531655
Iteration 86, loss = 0.01523690
Iteration 87, loss = 0.01520005
Iteration 88, loss = 0.01514972
Iteration 89, loss = 0.01521623
Iteration 90, loss = 0.01518859
Iteration 91, loss = 0.01526379
Iteration 92, loss = 0.01516345
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67210364
Iteration 2, loss = 0.58225859
Iteration 3, loss = 0.45677130
Iteration 4, loss = 0.33926421
Iteration 5, loss = 0.24691335
Iteration 6, loss = 0.18137193
Iteration 7, loss = 0.13702772
Iteration 8, loss = 0.10716026
Iteration 9, loss = 0.08647615
Iteration 10, loss = 0.07187976
Iteration 11, loss = 0.06128209
Iteration 12, loss = 0.05353205
Iteration 13, loss = 0.04764656
Iteration 14, loss = 0.04269503
Iteration 15, loss = 0.03879038
Iteration 16, loss = 0.03585695
Iteration 17, loss = 0.03338262
Iteration 18, loss = 0.03132523
Iteration 19, loss = 0.02951931
Iteration 20, loss = 0.02796998
Iteration 21, loss = 0.02672639
Iteration 22, loss = 0.02600111
Iteration 23, loss = 0.02456971
Iteration 24, loss = 0.02394509
Iteration 25, loss = 0.02316732
Iteration 26, loss = 0.02257239
Iteration 27, loss = 0.02188179
Iteration 28, loss = 0.02141080
Iteration 29, loss = 0.02084817
Iteration 30, loss = 0.02108900
Iteration 31, loss = 0.02047852
Iteration 32, loss = 0.01986566
Iteration 33, loss = 0.01924316
Iteration 34, loss = 0.01912137
Iteration 35, loss = 0.01883499
Iteration 36, loss = 0.01901660
Iteration 37, loss = 0.01844932
Iteration 38, loss = 0.01854643
Iteration 39, loss = 0.01799622
Iteration 40, loss = 0.01806762
Iteration 41, loss = 0.01776539
Iteration 42, loss = 0.01754559
Iteration 43, loss = 0.01731712
Iteration 44, loss = 0.01741757
Iteration 45, loss = 0.01731873
Iteration 46, loss = 0.01680743
Iteration 47, loss = 0.01678004
Iteration 48, loss = 0.01686991
Iteration 49, loss = 0.01674266
Iteration 50, loss = 0.01666383
Iteration 51, loss = 0.01660617
Iteration 52, loss = 0.01635638
Iteration 53, loss = 0.01680545
Iteration 54, loss = 0.01632976
Iteration 55, loss = 0.01612218
Iteration 56, loss = 0.01618316
Iteration 57, loss = 0.01628566
Iteration 58, loss = 0.01645936
Iteration 59, loss = 0.01608771
Iteration 60, loss = 0.01591930
Iteration 61, loss = 0.01576636
Iteration 62, loss = 0.01594081
Iteration 63, loss = 0.01587519
Iteration 64, loss = 0.01575794
Iteration 65, loss = 0.01551394
Iteration 66, loss = 0.01551088
Iteration 67, loss = 0.01559342
Iteration 68, loss = 0.01558772
Iteration 69, loss = 0.01566209
Iteration 70, loss = 0.01548075
Iteration 71, loss = 0.01558598
Iteration 72, loss = 0.01527309
Iteration 73, loss = 0.01548480
Iteration 74, loss = 0.01560457
Iteration 75, loss = 0.01554066
Iteration 76, loss = 0.01534099
Iteration 77, loss = 0.01538192
Iteration 78, loss = 0.01521182
Iteration 79, loss = 0.01517873
Iteration 80, loss = 0.01511241
Iteration 81, loss = 0.01539060
Iteration 82, loss = 0.01524591
Iteration 83, loss = 0.01511912
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70309805
Iteration 2, loss = 0.61584227
Iteration 3, loss = 0.49648962
Iteration 4, loss = 0.37359098
Iteration 5, loss = 0.27247084
Iteration 6, loss = 0.19898230
Iteration 7, loss = 0.14911006
Iteration 8, loss = 0.11557710
Iteration 9, loss = 0.09248200
Iteration 10, loss = 0.07660398
Iteration 11, loss = 0.06491294
Iteration 12, loss = 0.05634230
Iteration 13, loss = 0.04991927
Iteration 14, loss = 0.04456841
Iteration 15, loss = 0.04059435
Iteration 16, loss = 0.03711773
Iteration 17, loss = 0.03459243
Iteration 18, loss = 0.03250979
Iteration 19, loss = 0.03056948
Iteration 20, loss = 0.02894653
Iteration 21, loss = 0.02754823
Iteration 22, loss = 0.02621118
Iteration 23, loss = 0.02551636
Iteration 24, loss = 0.02447327
Iteration 25, loss = 0.02373687
Iteration 26, loss = 0.02304743
Iteration 27, loss = 0.02242603
Iteration 28, loss = 0.02194667
Iteration 29, loss = 0.02141875
Iteration 30, loss = 0.02090602
Iteration 31, loss = 0.02054680
Iteration 32, loss = 0.02021020
Iteration 33, loss = 0.01984517
Iteration 34, loss = 0.01979693
Iteration 35, loss = 0.01953806
Iteration 36, loss = 0.01901977
Iteration 37, loss = 0.01885928
Iteration 38, loss = 0.01890348
Iteration 39, loss = 0.01848595
Iteration 40, loss = 0.01823102
Iteration 41, loss = 0.01822296
Iteration 42, loss = 0.01797060
Iteration 43, loss = 0.01768880
Iteration 44, loss = 0.01771770
Iteration 45, loss = 0.01768185
Iteration 46, loss = 0.01754309
Iteration 47, loss = 0.01758841
Iteration 48, loss = 0.01707820
Iteration 49, loss = 0.01724170
Iteration 50, loss = 0.01696330
Iteration 51, loss = 0.01693326
Iteration 52, loss = 0.01673878
Iteration 53, loss = 0.01689309
Iteration 54, loss = 0.01683167
Iteration 55, loss = 0.01647135
Iteration 56, loss = 0.01659413
Iteration 57, loss = 0.01661929
Iteration 58, loss = 0.01666570
Iteration 59, loss = 0.01646642
Iteration 60, loss = 0.01643271
Iteration 61, loss = 0.01627836
Iteration 62, loss = 0.01635058
Iteration 63, loss = 0.01628904
Iteration 64, loss = 0.01628943
Iteration 65, loss = 0.01647884
Iteration 66, loss = 0.01634017
Iteration 67, loss = 0.01627085
Iteration 68, loss = 0.01603506
Iteration 69, loss = 0.01612642
Iteration 70, loss = 0.01614812
Iteration 71, loss = 0.01599850
Iteration 72, loss = 0.01595158
Iteration 73, loss = 0.01590412
Iteration 74, loss = 0.01598822
Iteration 75, loss = 0.01593470
Iteration 76, loss = 0.01571571
Iteration 77, loss = 0.01585849
Iteration 78, loss = 0.01567568
Iteration 79, loss = 0.01567685
Iteration 80, loss = 0.01583660
Iteration 81, loss = 0.01586223
Iteration 82, loss = 0.01577572
Iteration 83, loss = 0.01571862
Iteration 84, loss = 0.01561915
Iteration 85, loss = 0.01573986
Iteration 86, loss = 0.01554426
Iteration 87, loss = 0.01537862
Iteration 88, loss = 0.01559135
Iteration 89, loss = 0.01537047
Iteration 90, loss = 0.01577183
Iteration 91, loss = 0.01544604
Iteration 92, loss = 0.01547137
Iteration 93, loss = 0.01552091
Iteration 94, loss = 0.01564051
Iteration 95, loss = 0.01525879
Iteration 96, loss = 0.01533800
Iteration 97, loss = 0.01523121
Iteration 98, loss = 0.01544725
Iteration 99, loss = 0.01538050
Iteration 100, loss = 0.01539073
Iteration 101, loss = 0.01528045
Iteration 102, loss = 0.01513796
Iteration 103, loss = 0.01527047
Iteration 104, loss = 0.01534889
Iteration 105, loss = 0.01523573
Iteration 106, loss = 0.01539522
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69843502
Iteration 2, loss = 0.61674635
Iteration 3, loss = 0.50380133
Iteration 4, loss = 0.38214339
Iteration 5, loss = 0.28017540
Iteration 6, loss = 0.20535361
Iteration 7, loss = 0.15338380
Iteration 8, loss = 0.11892754
Iteration 9, loss = 0.09536925
Iteration 10, loss = 0.07886651
Iteration 11, loss = 0.06697658
Iteration 12, loss = 0.05831668
Iteration 13, loss = 0.05175275
Iteration 14, loss = 0.04651043
Iteration 15, loss = 0.04241911
Iteration 16, loss = 0.03917813
Iteration 17, loss = 0.03657508
Iteration 18, loss = 0.03430213
Iteration 19, loss = 0.03250445
Iteration 20, loss = 0.03077664
Iteration 21, loss = 0.02954678
Iteration 22, loss = 0.02849191
Iteration 23, loss = 0.02733692
Iteration 24, loss = 0.02656254
Iteration 25, loss = 0.02581830
Iteration 26, loss = 0.02518860
Iteration 27, loss = 0.02475777
Iteration 28, loss = 0.02406435
Iteration 29, loss = 0.02345144
Iteration 30, loss = 0.02306927
Iteration 31, loss = 0.02269582
Iteration 32, loss = 0.02236538
Iteration 33, loss = 0.02214010
Iteration 34, loss = 0.02186131
Iteration 35, loss = 0.02167728
Iteration 36, loss = 0.02135426
Iteration 37, loss = 0.02112823
Iteration 38, loss = 0.02105126
Iteration 39, loss = 0.02081598
Iteration 40, loss = 0.02075208
Iteration 41, loss = 0.02047123
Iteration 42, loss = 0.02024670
Iteration 43, loss = 0.02006602
Iteration 44, loss = 0.02010068
Iteration 45, loss = 0.01994447
Iteration 46, loss = 0.01998012
Iteration 47, loss = 0.01981812
Iteration 48, loss = 0.01971156
Iteration 49, loss = 0.01979821
Iteration 50, loss = 0.01954498
Iteration 51, loss = 0.01968552
Iteration 52, loss = 0.01924640
Iteration 53, loss = 0.01924887
Iteration 54, loss = 0.01922290
Iteration 55, loss = 0.01919884
Iteration 56, loss = 0.01905987
Iteration 57, loss = 0.01908109
Iteration 58, loss = 0.01919870
Iteration 59, loss = 0.01902499
Iteration 60, loss = 0.01884617
Iteration 61, loss = 0.01887089
Iteration 62, loss = 0.01874459
Iteration 63, loss = 0.01861625
Iteration 64, loss = 0.01862184
Iteration 65, loss = 0.01847659
Iteration 66, loss = 0.01885271
Iteration 67, loss = 0.01856077
Iteration 68, loss = 0.01879376
Iteration 69, loss = 0.01835608
Iteration 70, loss = 0.01861735
Iteration 71, loss = 0.01845369
Iteration 72, loss = 0.01831318
Iteration 73, loss = 0.01861042
Iteration 74, loss = 0.01839400
Iteration 75, loss = 0.01838469
Iteration 76, loss = 0.01827190
Iteration 77, loss = 0.01844346
Iteration 78, loss = 0.01853445
Iteration 79, loss = 0.01820820
Iteration 80, loss = 0.01824242
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68586884
Iteration 2, loss = 0.61708165
Iteration 3, loss = 0.50878839
Iteration 4, loss = 0.38677804
Iteration 5, loss = 0.28349857
Iteration 6, loss = 0.20726511
Iteration 7, loss = 0.15491068
Iteration 8, loss = 0.11988670
Iteration 9, loss = 0.09618896
Iteration 10, loss = 0.07979286
Iteration 11, loss = 0.06793766
Iteration 12, loss = 0.05932205
Iteration 13, loss = 0.05247777
Iteration 14, loss = 0.04734871
Iteration 15, loss = 0.04347691
Iteration 16, loss = 0.03996729
Iteration 17, loss = 0.03736498
Iteration 18, loss = 0.03521221
Iteration 19, loss = 0.03329793
Iteration 20, loss = 0.03155302
Iteration 21, loss = 0.03037552
Iteration 22, loss = 0.02905553
Iteration 23, loss = 0.02818410
Iteration 24, loss = 0.02719013
Iteration 25, loss = 0.02651134
Iteration 26, loss = 0.02585337
Iteration 27, loss = 0.02514228
Iteration 28, loss = 0.02451892
Iteration 29, loss = 0.02408988
Iteration 30, loss = 0.02381495
Iteration 31, loss = 0.02336685
Iteration 32, loss = 0.02317783
Iteration 33, loss = 0.02293962
Iteration 34, loss = 0.02261500
Iteration 35, loss = 0.02204939
Iteration 36, loss = 0.02177058
Iteration 37, loss = 0.02164495
Iteration 38, loss = 0.02151611
Iteration 39, loss = 0.02135733
Iteration 40, loss = 0.02100218
Iteration 41, loss = 0.02099157
Iteration 42, loss = 0.02066419
Iteration 43, loss = 0.02061379
Iteration 44, loss = 0.02017781
Iteration 45, loss = 0.02026597
Iteration 46, loss = 0.02024546
Iteration 47, loss = 0.02032333
Iteration 48, loss = 0.02001724
Iteration 49, loss = 0.01997213
Iteration 50, loss = 0.01996216
Iteration 51, loss = 0.01958225
Iteration 52, loss = 0.01974484
Iteration 53, loss = 0.01977025
Iteration 54, loss = 0.01945578
Iteration 55, loss = 0.01940053
Iteration 56, loss = 0.01927625
Iteration 57, loss = 0.01949041
Iteration 58, loss = 0.01963495
Iteration 59, loss = 0.01903766
Iteration 60, loss = 0.01919829
Iteration 61, loss = 0.01903612
Iteration 62, loss = 0.01905566
Iteration 63, loss = 0.01904976
Iteration 64, loss = 0.01927330
Iteration 65, loss = 0.01895204
Iteration 66, loss = 0.01894599
Iteration 67, loss = 0.01896231
Iteration 68, loss = 0.01889640
Iteration 69, loss = 0.01860267
Iteration 70, loss = 0.01872216
Iteration 71, loss = 0.01871238
Iteration 72, loss = 0.01874149
Iteration 73, loss = 0.01868389
Iteration 74, loss = 0.01890476
Iteration 75, loss = 0.01859204
Iteration 76, loss = 0.01851606
Iteration 77, loss = 0.01857335
Iteration 78, loss = 0.01837879
Iteration 79, loss = 0.01861491
Iteration 80, loss = 0.01834971
Iteration 81, loss = 0.01849492
Iteration 82, loss = 0.01849661
Iteration 83, loss = 0.01820814
Iteration 84, loss = 0.01826368
Iteration 85, loss = 0.01846049
Iteration 86, loss = 0.01833764
Iteration 87, loss = 0.01850135
Iteration 88, loss = 0.01816318
Iteration 89, loss = 0.01821624
Iteration 90, loss = 0.01837457
Iteration 91, loss = 0.01819242
Iteration 92, loss = 0.01829567
Iteration 93, loss = 0.01833465
Iteration 94, loss = 0.01791371
Iteration 95, loss = 0.01805266
Iteration 96, loss = 0.01798723
Iteration 97, loss = 0.01790773
Iteration 98, loss = 0.01794755
Iteration 99, loss = 0.01817021
Iteration 100, loss = 0.01784386
Iteration 101, loss = 0.01778976
Iteration 102, loss = 0.01807829
Iteration 103, loss = 0.01789892
Iteration 104, loss = 0.01771869
Iteration 105, loss = 0.01805669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69061293
Iteration 2, loss = 0.60508372
Iteration 3, loss = 0.48271804
Iteration 4, loss = 0.35865238
Iteration 5, loss = 0.25898769
Iteration 6, loss = 0.18901901
Iteration 7, loss = 0.14173597
Iteration 8, loss = 0.11016797
Iteration 9, loss = 0.08871670
Iteration 10, loss = 0.07346617
Iteration 11, loss = 0.06275669
Iteration 12, loss = 0.05439209
Iteration 13, loss = 0.04820250
Iteration 14, loss = 0.04354569
Iteration 15, loss = 0.03949723
Iteration 16, loss = 0.03622426
Iteration 17, loss = 0.03394768
Iteration 18, loss = 0.03168069
Iteration 19, loss = 0.02984066
Iteration 20, loss = 0.02813004
Iteration 21, loss = 0.02696367
Iteration 22, loss = 0.02592105
Iteration 23, loss = 0.02496506
Iteration 24, loss = 0.02386024
Iteration 25, loss = 0.02316235
Iteration 26, loss = 0.02248792
Iteration 27, loss = 0.02210684
Iteration 28, loss = 0.02157199
Iteration 29, loss = 0.02112393
Iteration 30, loss = 0.02056638
Iteration 31, loss = 0.02007050
Iteration 32, loss = 0.01992747
Iteration 33, loss = 0.01956313
Iteration 34, loss = 0.01929444
Iteration 35, loss = 0.01902513
Iteration 36, loss = 0.01885054
Iteration 37, loss = 0.01864574
Iteration 38, loss = 0.01822490
Iteration 39, loss = 0.01813438
Iteration 40, loss = 0.01796552
Iteration 41, loss = 0.01806555
Iteration 42, loss = 0.01773548
Iteration 43, loss = 0.01770244
Iteration 44, loss = 0.01723337
Iteration 45, loss = 0.01726498
Iteration 46, loss = 0.01700259
Iteration 47, loss = 0.01687944
Iteration 48, loss = 0.01682158
Iteration 49, loss = 0.01689007
Iteration 50, loss = 0.01664138
Iteration 51, loss = 0.01675375
Iteration 52, loss = 0.01686645
Iteration 53, loss = 0.01647869
Iteration 54, loss = 0.01661035
Iteration 55, loss = 0.01653897
Iteration 56, loss = 0.01618676
Iteration 57, loss = 0.01637868
Iteration 58, loss = 0.01614448
Iteration 59, loss = 0.01614568
Iteration 60, loss = 0.01617221
Iteration 61, loss = 0.01602072
Iteration 62, loss = 0.01614131
Iteration 63, loss = 0.01598555
Iteration 64, loss = 0.01582152
Iteration 65, loss = 0.01584786
Iteration 66, loss = 0.01570750
Iteration 67, loss = 0.01571048
Iteration 68, loss = 0.01567739
Iteration 69, loss = 0.01583148
Iteration 70, loss = 0.01556840
Iteration 71, loss = 0.01563153
Iteration 72, loss = 0.01582581
Iteration 73, loss = 0.01550606
Iteration 74, loss = 0.01567448
Iteration 75, loss = 0.01546798
Iteration 76, loss = 0.01559958
Iteration 77, loss = 0.01546439
Iteration 78, loss = 0.01551422
Iteration 79, loss = 0.01564007
Iteration 80, loss = 0.01540950
Iteration 81, loss = 0.01516318
Iteration 82, loss = 0.01537815
Iteration 83, loss = 0.01534123
Iteration 84, loss = 0.01555822
Iteration 85, loss = 0.01531655
Iteration 86, loss = 0.01523690
Iteration 87, loss = 0.01520005
Iteration 88, loss = 0.01514972
Iteration 89, loss = 0.01521623
Iteration 90, loss = 0.01518859
Iteration 91, loss = 0.01526379
Iteration 92, loss = 0.01516345
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67210364
Iteration 2, loss = 0.58225859
Iteration 3, loss = 0.45677130
Iteration 4, loss = 0.33926421
Iteration 5, loss = 0.24691335
Iteration 6, loss = 0.18137193
Iteration 7, loss = 0.13702772
Iteration 8, loss = 0.10716026
Iteration 9, loss = 0.08647615
Iteration 10, loss = 0.07187976
Iteration 11, loss = 0.06128209
Iteration 12, loss = 0.05353205
Iteration 13, loss = 0.04764656
Iteration 14, loss = 0.04269503
Iteration 15, loss = 0.03879038
Iteration 16, loss = 0.03585695
Iteration 17, loss = 0.03338262
Iteration 18, loss = 0.03132523
Iteration 19, loss = 0.02951931
Iteration 20, loss = 0.02796998
Iteration 21, loss = 0.02672639
Iteration 22, loss = 0.02600111
Iteration 23, loss = 0.02456971
Iteration 24, loss = 0.02394509
Iteration 25, loss = 0.02316732
Iteration 26, loss = 0.02257239
Iteration 27, loss = 0.02188179
Iteration 28, loss = 0.02141080
Iteration 29, loss = 0.02084817
Iteration 30, loss = 0.02108900
Iteration 31, loss = 0.02047852
Iteration 32, loss = 0.01986566
Iteration 33, loss = 0.01924316
Iteration 34, loss = 0.01912137
Iteration 35, loss = 0.01883499
Iteration 36, loss = 0.01901660
Iteration 37, loss = 0.01844932
Iteration 38, loss = 0.01854643
Iteration 39, loss = 0.01799622
Iteration 40, loss = 0.01806762
Iteration 41, loss = 0.01776539
Iteration 42, loss = 0.01754559
Iteration 43, loss = 0.01731712
Iteration 44, loss = 0.01741757
Iteration 45, loss = 0.01731873
Iteration 46, loss = 0.01680743
Iteration 47, loss = 0.01678004
Iteration 48, loss = 0.01686991
Iteration 49, loss = 0.01674266
Iteration 50, loss = 0.01666383
Iteration 51, loss = 0.01660617
Iteration 52, loss = 0.01635638
Iteration 53, loss = 0.01680545
Iteration 54, loss = 0.01632976
Iteration 55, loss = 0.01612218
Iteration 56, loss = 0.01618316
Iteration 57, loss = 0.01628566
Iteration 58, loss = 0.01645936
Iteration 59, loss = 0.01608771
Iteration 60, loss = 0.01591930
Iteration 61, loss = 0.01576636
Iteration 62, loss = 0.01594081
Iteration 63, loss = 0.01587519
Iteration 64, loss = 0.01575794
Iteration 65, loss = 0.01551394
Iteration 66, loss = 0.01551088
Iteration 67, loss = 0.01559342
Iteration 68, loss = 0.01558772
Iteration 69, loss = 0.01566209
Iteration 70, loss = 0.01548075
Iteration 71, loss = 0.01558598
Iteration 72, loss = 0.01527309
Iteration 73, loss = 0.01548480
Iteration 74, loss = 0.01560457
Iteration 75, loss = 0.01554066
Iteration 76, loss = 0.01534099
Iteration 77, loss = 0.01538192
Iteration 78, loss = 0.01521182
Iteration 79, loss = 0.01517873
Iteration 80, loss = 0.01511241
Iteration 81, loss = 0.01539060
Iteration 82, loss = 0.01524591
Iteration 83, loss = 0.01511912
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70309805
Iteration 2, loss = 0.61584227
Iteration 3, loss = 0.49648962
Iteration 4, loss = 0.37359098
Iteration 5, loss = 0.27247084
Iteration 6, loss = 0.19898230
Iteration 7, loss = 0.14911006
Iteration 8, loss = 0.11557710
Iteration 9, loss = 0.09248200
Iteration 10, loss = 0.07660398
Iteration 11, loss = 0.06491294
Iteration 12, loss = 0.05634230
Iteration 13, loss = 0.04991927
Iteration 14, loss = 0.04456841
Iteration 15, loss = 0.04059435
Iteration 16, loss = 0.03711773
Iteration 17, loss = 0.03459243
Iteration 18, loss = 0.03250979
Iteration 19, loss = 0.03056948
Iteration 20, loss = 0.02894653
Iteration 21, loss = 0.02754823
Iteration 22, loss = 0.02621118
Iteration 23, loss = 0.02551636
Iteration 24, loss = 0.02447327
Iteration 25, loss = 0.02373687
Iteration 26, loss = 0.02304743
Iteration 27, loss = 0.02242603
Iteration 28, loss = 0.02194667
Iteration 29, loss = 0.02141875
Iteration 30, loss = 0.02090602
Iteration 31, loss = 0.02054680
Iteration 32, loss = 0.02021020
Iteration 33, loss = 0.01984517
Iteration 34, loss = 0.01979693
Iteration 35, loss = 0.01953806
Iteration 36, loss = 0.01901977
Iteration 37, loss = 0.01885928
Iteration 38, loss = 0.01890348
Iteration 39, loss = 0.01848595
Iteration 40, loss = 0.01823102
Iteration 41, loss = 0.01822296
Iteration 42, loss = 0.01797060
Iteration 43, loss = 0.01768880
Iteration 44, loss = 0.01771770
Iteration 45, loss = 0.01768185
Iteration 46, loss = 0.01754309
Iteration 47, loss = 0.01758841
Iteration 48, loss = 0.01707820
Iteration 49, loss = 0.01724170
Iteration 50, loss = 0.01696330
Iteration 51, loss = 0.01693326
Iteration 52, loss = 0.01673878
Iteration 53, loss = 0.01689309
Iteration 54, loss = 0.01683167
Iteration 55, loss = 0.01647135
Iteration 56, loss = 0.01659413
Iteration 57, loss = 0.01661929
Iteration 58, loss = 0.01666570
Iteration 59, loss = 0.01646642
Iteration 60, loss = 0.01643271
Iteration 61, loss = 0.01627836
Iteration 62, loss = 0.01635058
Iteration 63, loss = 0.01628904
Iteration 64, loss = 0.01628943
Iteration 65, loss = 0.01647884
Iteration 66, loss = 0.01634017
Iteration 67, loss = 0.01627085
Iteration 68, loss = 0.01603506
Iteration 69, loss = 0.01612642
Iteration 70, loss = 0.01614812
Iteration 71, loss = 0.01599850
Iteration 72, loss = 0.01595158
Iteration 73, loss = 0.01590412
Iteration 74, loss = 0.01598822
Iteration 75, loss = 0.01593470
Iteration 76, loss = 0.01571571
Iteration 77, loss = 0.01585849
Iteration 78, loss = 0.01567568
Iteration 79, loss = 0.01567685
Iteration 80, loss = 0.01583660
Iteration 81, loss = 0.01586223
Iteration 82, loss = 0.01577572
Iteration 83, loss = 0.01571862
Iteration 84, loss = 0.01561915
Iteration 85, loss = 0.01573986
Iteration 86, loss = 0.01554426
Iteration 87, loss = 0.01537862
Iteration 88, loss = 0.01559135
Iteration 89, loss = 0.01537047
Iteration 90, loss = 0.01577183
Iteration 91, loss = 0.01544604
Iteration 92, loss = 0.01547137
Iteration 93, loss = 0.01552091
Iteration 94, loss = 0.01564051
Iteration 95, loss = 0.01525879
Iteration 96, loss = 0.01533800
Iteration 97, loss = 0.01523121
Iteration 98, loss = 0.01544725
Iteration 99, loss = 0.01538050
Iteration 100, loss = 0.01539073
Iteration 101, loss = 0.01528045
Iteration 102, loss = 0.01513796
Iteration 103, loss = 0.01527047
Iteration 104, loss = 0.01534889
Iteration 105, loss = 0.01523573
Iteration 106, loss = 0.01539522
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67366717
Iteration 2, loss = 0.58531566
Iteration 3, loss = 0.44941727
Iteration 4, loss = 0.32039566
Iteration 5, loss = 0.22321409
Iteration 6, loss = 0.15935848
Iteration 7, loss = 0.11907135
Iteration 8, loss = 0.09313851
Iteration 9, loss = 0.07606873
Iteration 10, loss = 0.06417357
Iteration 11, loss = 0.05539165
Iteration 12, loss = 0.04945024
Iteration 13, loss = 0.04437327
Iteration 14, loss = 0.04068427
Iteration 15, loss = 0.03764036
Iteration 16, loss = 0.03516706
Iteration 17, loss = 0.03329210
Iteration 18, loss = 0.03150430
Iteration 19, loss = 0.03022988
Iteration 20, loss = 0.02866244
Iteration 21, loss = 0.02794100
Iteration 22, loss = 0.02708086
Iteration 23, loss = 0.02620717
Iteration 24, loss = 0.02552645
Iteration 25, loss = 0.02513437
Iteration 26, loss = 0.02453987
Iteration 27, loss = 0.02434362
Iteration 28, loss = 0.02360803
Iteration 29, loss = 0.02337973
Iteration 30, loss = 0.02305847
Iteration 31, loss = 0.02302909
Iteration 32, loss = 0.02255433
Iteration 33, loss = 0.02190732
Iteration 34, loss = 0.02208337
Iteration 35, loss = 0.02178063
Iteration 36, loss = 0.02168136
Iteration 37, loss = 0.02137051
Iteration 38, loss = 0.02138091
Iteration 39, loss = 0.02135975
Iteration 40, loss = 0.02094555
Iteration 41, loss = 0.02103889
Iteration 42, loss = 0.02117983
Iteration 43, loss = 0.02083388
Iteration 44, loss = 0.02071022
Iteration 45, loss = 0.02034943
Iteration 46, loss = 0.02057295
Iteration 47, loss = 0.02029941
Iteration 48, loss = 0.02039380
Iteration 49, loss = 0.02029535
Iteration 50, loss = 0.02019878
Iteration 51, loss = 0.02004033
Iteration 52, loss = 0.01986781
Iteration 53, loss = 0.01998199
Iteration 54, loss = 0.01995233
Iteration 55, loss = 0.02011980
Iteration 56, loss = 0.01985106
Iteration 57, loss = 0.01974394
Iteration 58, loss = 0.01941991
Iteration 59, loss = 0.01954688
Iteration 60, loss = 0.01931078
Iteration 61, loss = 0.01942451
Iteration 62, loss = 0.01965538
Iteration 63, loss = 0.01949706
Iteration 64, loss = 0.01915628
Iteration 65, loss = 0.01929742
Iteration 66, loss = 0.01927459
Iteration 67, loss = 0.01941381
Iteration 68, loss = 0.01921361
Iteration 69, loss = 0.01918827
Iteration 70, loss = 0.01910722
Iteration 71, loss = 0.01931015
Iteration 72, loss = 0.01928934
Iteration 73, loss = 0.01952845
Iteration 74, loss = 0.01892350
Iteration 75, loss = 0.01911637
Iteration 76, loss = 0.01896656
Iteration 77, loss = 0.01923976
Iteration 78, loss = 0.01916062
Iteration 79, loss = 0.01907477
Iteration 80, loss = 0.01895646
Iteration 81, loss = 0.01885691
Iteration 82, loss = 0.01915278
Iteration 83, loss = 0.01885710
Iteration 84, loss = 0.01877057
Iteration 85, loss = 0.01908662
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on prepended_v9_minimal_processing...
Iteration 1, loss = 0.69159648
Iteration 2, loss = 0.63396322
Iteration 3, loss = 0.55886908
Iteration 4, loss = 0.47193705
Iteration 5, loss = 0.38723529
Iteration 6, loss = 0.31236292
Iteration 7, loss = 0.25026811
Iteration 8, loss = 0.20083948
Iteration 9, loss = 0.16275469
Iteration 10, loss = 0.13371619
Iteration 11, loss = 0.11174258
Iteration 12, loss = 0.09474627
Iteration 13, loss = 0.08191271
Iteration 14, loss = 0.07164647
Iteration 15, loss = 0.06381190
Iteration 16, loss = 0.05715039
Iteration 17, loss = 0.05203098
Iteration 18, loss = 0.04783552
Iteration 19, loss = 0.04407167
Iteration 20, loss = 0.04100004
Iteration 21, loss = 0.03843359
Iteration 22, loss = 0.03606065
Iteration 23, loss = 0.03424790
Iteration 24, loss = 0.03250700
Iteration 25, loss = 0.03116568
Iteration 26, loss = 0.02964700
Iteration 27, loss = 0.02847011
Iteration 28, loss = 0.02742799
Iteration 29, loss = 0.02662497
Iteration 30, loss = 0.02591668
Iteration 31, loss = 0.02515434
Iteration 32, loss = 0.02489086
Iteration 33, loss = 0.02413099
Iteration 34, loss = 0.02338173
Iteration 35, loss = 0.02296255
Iteration 36, loss = 0.02252385
Iteration 37, loss = 0.02211553
Iteration 38, loss = 0.02158035
Iteration 39, loss = 0.02122492
Iteration 40, loss = 0.02079454
Iteration 41, loss = 0.02062765
Iteration 42, loss = 0.02032702
Iteration 43, loss = 0.02038225
Iteration 44, loss = 0.01987715
Iteration 45, loss = 0.01963855
Iteration 46, loss = 0.01941325
Iteration 47, loss = 0.01938088
Iteration 48, loss = 0.01923412
Iteration 49, loss = 0.01911054
Iteration 50, loss = 0.01885791
Iteration 51, loss = 0.01844622
Iteration 52, loss = 0.01836106
Iteration 53, loss = 0.01800953
Iteration 54, loss = 0.01809649
Iteration 55, loss = 0.01787316
Iteration 56, loss = 0.01811821
Iteration 57, loss = 0.01784343
Iteration 58, loss = 0.01761979
Iteration 59, loss = 0.01747282
Iteration 60, loss = 0.01766679
Iteration 61, loss = 0.01743075
Iteration 62, loss = 0.01766201
Iteration 63, loss = 0.01748122
Iteration 64, loss = 0.01730566
Iteration 65, loss = 0.01710149
Iteration 66, loss = 0.01712300
Iteration 67, loss = 0.01698978
Iteration 68, loss = 0.01689182
Iteration 69, loss = 0.01669164
Iteration 70, loss = 0.01672029
Iteration 71, loss = 0.01660131
Iteration 72, loss = 0.01688540
Iteration 73, loss = 0.01676003
Iteration 74, loss = 0.01650795
Iteration 75, loss = 0.01639114
Iteration 76, loss = 0.01658326
Iteration 77, loss = 0.01635078
Iteration 78, loss = 0.01655411
Iteration 79, loss = 0.01637239
Iteration 80, loss = 0.01639234
Iteration 81, loss = 0.01636147
Iteration 82, loss = 0.01616576
Iteration 83, loss = 0.01653806
Iteration 84, loss = 0.01625557
Iteration 85, loss = 0.01614026
Iteration 86, loss = 0.01604642
Iteration 87, loss = 0.01604150
Iteration 88, loss = 0.01600649
Iteration 89, loss = 0.01594292
Iteration 90, loss = 0.01598978
Iteration 91, loss = 0.01614047
Iteration 92, loss = 0.01599979
Iteration 93, loss = 0.01605485
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71677665
Iteration 2, loss = 0.68603439
Iteration 3, loss = 0.64492307
Iteration 4, loss = 0.59393644
Iteration 5, loss = 0.53354013
Iteration 6, loss = 0.46899102
Iteration 7, loss = 0.40539733
Iteration 8, loss = 0.34581238
Iteration 9, loss = 0.29239151
Iteration 10, loss = 0.24607782
Iteration 11, loss = 0.20711390
Iteration 12, loss = 0.17486674
Iteration 13, loss = 0.14851469
Iteration 14, loss = 0.12721944
Iteration 15, loss = 0.10981457
Iteration 16, loss = 0.09583789
Iteration 17, loss = 0.08430317
Iteration 18, loss = 0.07482078
Iteration 19, loss = 0.06708815
Iteration 20, loss = 0.06037293
Iteration 21, loss = 0.05473049
Iteration 22, loss = 0.05005184
Iteration 23, loss = 0.04601615
Iteration 24, loss = 0.04252182
Iteration 25, loss = 0.03955525
Iteration 26, loss = 0.03690160
Iteration 27, loss = 0.03461391
Iteration 28, loss = 0.03254258
Iteration 29, loss = 0.03074375
Iteration 30, loss = 0.02909172
Iteration 31, loss = 0.02772517
Iteration 32, loss = 0.02637216
Iteration 33, loss = 0.02523059
Iteration 34, loss = 0.02422953
Iteration 35, loss = 0.02331877
Iteration 36, loss = 0.02250209
Iteration 37, loss = 0.02164485
Iteration 38, loss = 0.02087130
Iteration 39, loss = 0.02018509
Iteration 40, loss = 0.01957485
Iteration 41, loss = 0.01916296
Iteration 42, loss = 0.01856611
Iteration 43, loss = 0.01806235
Iteration 44, loss = 0.01768169
Iteration 45, loss = 0.01722991
Iteration 46, loss = 0.01690375
Iteration 47, loss = 0.01652801
Iteration 48, loss = 0.01621716
Iteration 49, loss = 0.01586135
Iteration 50, loss = 0.01557174
Iteration 51, loss = 0.01528394
Iteration 52, loss = 0.01506543
Iteration 53, loss = 0.01478693
Iteration 54, loss = 0.01453047
Iteration 55, loss = 0.01435792
Iteration 56, loss = 0.01421588
Iteration 57, loss = 0.01400560
Iteration 58, loss = 0.01376018
Iteration 59, loss = 0.01368615
Iteration 60, loss = 0.01351228
Iteration 61, loss = 0.01343250
Iteration 62, loss = 0.01312103
Iteration 63, loss = 0.01300725
Iteration 64, loss = 0.01285326
Iteration 65, loss = 0.01275734
Iteration 66, loss = 0.01271570
Iteration 67, loss = 0.01254945
Iteration 68, loss = 0.01237425
Iteration 69, loss = 0.01238019
Iteration 70, loss = 0.01226131
Iteration 71, loss = 0.01215133
Iteration 72, loss = 0.01206937
Iteration 73, loss = 0.01201000
Iteration 74, loss = 0.01186410
Iteration 75, loss = 0.01181501
Iteration 76, loss = 0.01171177
Iteration 77, loss = 0.01163544
Iteration 78, loss = 0.01157702
Iteration 79, loss = 0.01156141
Iteration 80, loss = 0.01143777
Iteration 81, loss = 0.01140753
Iteration 82, loss = 0.01133108
Iteration 83, loss = 0.01132722
Iteration 84, loss = 0.01130763
Iteration 85, loss = 0.01115950
Iteration 86, loss = 0.01107030
Iteration 87, loss = 0.01108755
Iteration 88, loss = 0.01100526
Iteration 89, loss = 0.01101955
Iteration 90, loss = 0.01093789
Iteration 91, loss = 0.01092658
Iteration 92, loss = 0.01081727
Iteration 93, loss = 0.01079448
Iteration 94, loss = 0.01076174
Iteration 95, loss = 0.01074963
Iteration 96, loss = 0.01071776
Iteration 97, loss = 0.01073734
Iteration 98, loss = 0.01062687
Iteration 99, loss = 0.01057302
Iteration 100, loss = 0.01052159
Iteration 101, loss = 0.01051280
Iteration 102, loss = 0.01059205
Iteration 103, loss = 0.01050558
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67699696
Iteration 2, loss = 0.65258035
Iteration 3, loss = 0.61835399
Iteration 4, loss = 0.57215366
Iteration 5, loss = 0.51740669
Iteration 6, loss = 0.45831548
Iteration 7, loss = 0.39863630
Iteration 8, loss = 0.34198394
Iteration 9, loss = 0.29053614
Iteration 10, loss = 0.24548372
Iteration 11, loss = 0.20716276
Iteration 12, loss = 0.17486492
Iteration 13, loss = 0.14862880
Iteration 14, loss = 0.12719350
Iteration 15, loss = 0.10961752
Iteration 16, loss = 0.09529281
Iteration 17, loss = 0.08367353
Iteration 18, loss = 0.07402140
Iteration 19, loss = 0.06596475
Iteration 20, loss = 0.05926778
Iteration 21, loss = 0.05363687
Iteration 22, loss = 0.04876401
Iteration 23, loss = 0.04464130
Iteration 24, loss = 0.04116637
Iteration 25, loss = 0.03807855
Iteration 26, loss = 0.03529567
Iteration 27, loss = 0.03296842
Iteration 28, loss = 0.03088550
Iteration 29, loss = 0.02898445
Iteration 30, loss = 0.02730080
Iteration 31, loss = 0.02582974
Iteration 32, loss = 0.02450155
Iteration 33, loss = 0.02326916
Iteration 34, loss = 0.02221963
Iteration 35, loss = 0.02120488
Iteration 36, loss = 0.02037396
Iteration 37, loss = 0.01952408
Iteration 38, loss = 0.01876994
Iteration 39, loss = 0.01811278
Iteration 40, loss = 0.01740261
Iteration 41, loss = 0.01677853
Iteration 42, loss = 0.01627134
Iteration 43, loss = 0.01581268
Iteration 44, loss = 0.01538564
Iteration 45, loss = 0.01489561
Iteration 46, loss = 0.01457354
Iteration 47, loss = 0.01415200
Iteration 48, loss = 0.01386726
Iteration 49, loss = 0.01346194
Iteration 50, loss = 0.01311719
Iteration 51, loss = 0.01287030
Iteration 52, loss = 0.01260412
Iteration 53, loss = 0.01233574
Iteration 54, loss = 0.01206684
Iteration 55, loss = 0.01182146
Iteration 56, loss = 0.01164158
Iteration 57, loss = 0.01142267
Iteration 58, loss = 0.01131954
Iteration 59, loss = 0.01105927
Iteration 60, loss = 0.01086864
Iteration 61, loss = 0.01078153
Iteration 62, loss = 0.01056917
Iteration 63, loss = 0.01046863
Iteration 64, loss = 0.01031379
Iteration 65, loss = 0.01014974
Iteration 66, loss = 0.01002960
Iteration 67, loss = 0.00992973
Iteration 68, loss = 0.00983350
Iteration 69, loss = 0.00969088
Iteration 70, loss = 0.00964501
Iteration 71, loss = 0.00947579
Iteration 72, loss = 0.00943377
Iteration 73, loss = 0.00928595
Iteration 74, loss = 0.00919674
Iteration 75, loss = 0.00913715
Iteration 76, loss = 0.00904510
Iteration 77, loss = 0.00895014
Iteration 78, loss = 0.00891026
Iteration 79, loss = 0.00883569
Iteration 80, loss = 0.00881226
Iteration 81, loss = 0.00867237
Iteration 82, loss = 0.00861558
Iteration 83, loss = 0.00855642
Iteration 84, loss = 0.00852519
Iteration 85, loss = 0.00844193
Iteration 86, loss = 0.00836517
Iteration 87, loss = 0.00835698
Iteration 88, loss = 0.00835554
Iteration 89, loss = 0.00830141
Iteration 90, loss = 0.00820713
Iteration 91, loss = 0.00814702
Iteration 92, loss = 0.00813360
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70004065
Iteration 2, loss = 0.68022978
Iteration 3, loss = 0.66060754
Iteration 4, loss = 0.63835041
Iteration 5, loss = 0.61193630
Iteration 6, loss = 0.58186512
Iteration 7, loss = 0.54815911
Iteration 8, loss = 0.51169260
Iteration 9, loss = 0.47380108
Iteration 10, loss = 0.43586296
Iteration 11, loss = 0.39874969
Iteration 12, loss = 0.36317366
Iteration 13, loss = 0.32950831
Iteration 14, loss = 0.29815554
Iteration 15, loss = 0.26927193
Iteration 16, loss = 0.24296764
Iteration 17, loss = 0.21907130
Iteration 18, loss = 0.19766126
Iteration 19, loss = 0.17852900
Iteration 20, loss = 0.16155544
Iteration 21, loss = 0.14649189
Iteration 22, loss = 0.13325107
Iteration 23, loss = 0.12147128
Iteration 24, loss = 0.11115672
Iteration 25, loss = 0.10204206
Iteration 26, loss = 0.09391000
Iteration 27, loss = 0.08673043
Iteration 28, loss = 0.08046418
Iteration 29, loss = 0.07474074
Iteration 30, loss = 0.06974599
Iteration 31, loss = 0.06522026
Iteration 32, loss = 0.06114163
Iteration 33, loss = 0.05750271
Iteration 34, loss = 0.05426457
Iteration 35, loss = 0.05125656
Iteration 36, loss = 0.04851877
Iteration 37, loss = 0.04609801
Iteration 38, loss = 0.04382625
Iteration 39, loss = 0.04183786
Iteration 40, loss = 0.03988914
Iteration 41, loss = 0.03820656
Iteration 42, loss = 0.03662736
Iteration 43, loss = 0.03515265
Iteration 44, loss = 0.03376416
Iteration 45, loss = 0.03250349
Iteration 46, loss = 0.03134260
Iteration 47, loss = 0.03023814
Iteration 48, loss = 0.02926128
Iteration 49, loss = 0.02829128
Iteration 50, loss = 0.02742796
Iteration 51, loss = 0.02667512
Iteration 52, loss = 0.02584210
Iteration 53, loss = 0.02509146
Iteration 54, loss = 0.02449705
Iteration 55, loss = 0.02381013
Iteration 56, loss = 0.02318902
Iteration 57, loss = 0.02260146
Iteration 58, loss = 0.02206599
Iteration 59, loss = 0.02155285
Iteration 60, loss = 0.02107313
Iteration 61, loss = 0.02061455
Iteration 62, loss = 0.02018595
Iteration 63, loss = 0.01978139
Iteration 64, loss = 0.01943231
Iteration 65, loss = 0.01900965
Iteration 66, loss = 0.01867663
Iteration 67, loss = 0.01838525
Iteration 68, loss = 0.01804479
Iteration 69, loss = 0.01773395
Iteration 70, loss = 0.01739991
Iteration 71, loss = 0.01711716
Iteration 72, loss = 0.01686987
Iteration 73, loss = 0.01658218
Iteration 74, loss = 0.01637847
Iteration 75, loss = 0.01612388
Iteration 76, loss = 0.01593217
Iteration 77, loss = 0.01569308
Iteration 78, loss = 0.01551446
Iteration 79, loss = 0.01528391
Iteration 80, loss = 0.01516236
Iteration 81, loss = 0.01491201
Iteration 82, loss = 0.01475918
Iteration 83, loss = 0.01453665
Iteration 84, loss = 0.01439275
Iteration 85, loss = 0.01424238
Iteration 86, loss = 0.01413044
Iteration 87, loss = 0.01392522
Iteration 88, loss = 0.01379930
Iteration 89, loss = 0.01364880
Iteration 90, loss = 0.01356552
Iteration 91, loss = 0.01342402
Iteration 92, loss = 0.01326646
Iteration 93, loss = 0.01316762
Iteration 94, loss = 0.01309599
Iteration 95, loss = 0.01293294
Iteration 96, loss = 0.01287147
Iteration 97, loss = 0.01278093
Iteration 98, loss = 0.01262601
Iteration 99, loss = 0.01253111
Iteration 100, loss = 0.01245187
Iteration 101, loss = 0.01235980
Iteration 102, loss = 0.01223669
Iteration 103, loss = 0.01221887
Iteration 104, loss = 0.01206754
Iteration 105, loss = 0.01201709
Iteration 106, loss = 0.01195058
Iteration 107, loss = 0.01182890
Iteration 108, loss = 0.01178974
Iteration 109, loss = 0.01169872
Iteration 110, loss = 0.01168401
Iteration 111, loss = 0.01157684
Iteration 112, loss = 0.01149623
Iteration 113, loss = 0.01142306
Iteration 114, loss = 0.01137693
Iteration 115, loss = 0.01134799
Iteration 116, loss = 0.01127655
Iteration 117, loss = 0.01122751
Iteration 118, loss = 0.01113600
Iteration 119, loss = 0.01109668
Iteration 120, loss = 0.01103533
Iteration 121, loss = 0.01101446
Iteration 122, loss = 0.01090671
Iteration 123, loss = 0.01091485
Iteration 124, loss = 0.01081706
Iteration 125, loss = 0.01080366
Iteration 126, loss = 0.01074352
Iteration 127, loss = 0.01068416
Iteration 128, loss = 0.01066984
Iteration 129, loss = 0.01063036
Iteration 130, loss = 0.01058903
Iteration 131, loss = 0.01051492
Iteration 132, loss = 0.01049442
Iteration 133, loss = 0.01043076
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70444623
Iteration 2, loss = 0.65561034
Iteration 3, loss = 0.59295619
Iteration 4, loss = 0.52064906
Iteration 5, loss = 0.44415455
Iteration 6, loss = 0.37276222
Iteration 7, loss = 0.30883763
Iteration 8, loss = 0.25443054
Iteration 9, loss = 0.20958106
Iteration 10, loss = 0.17361036
Iteration 11, loss = 0.14540313
Iteration 12, loss = 0.12276739
Iteration 13, loss = 0.10500822
Iteration 14, loss = 0.09105742
Iteration 15, loss = 0.07970841
Iteration 16, loss = 0.07068954
Iteration 17, loss = 0.06316729
Iteration 18, loss = 0.05707643
Iteration 19, loss = 0.05180787
Iteration 20, loss = 0.04756506
Iteration 21, loss = 0.04391147
Iteration 22, loss = 0.04062087
Iteration 23, loss = 0.03797808
Iteration 24, loss = 0.03567550
Iteration 25, loss = 0.03350252
Iteration 26, loss = 0.03187579
Iteration 27, loss = 0.03018974
Iteration 28, loss = 0.02878019
Iteration 29, loss = 0.02744675
Iteration 30, loss = 0.02629915
Iteration 31, loss = 0.02528494
Iteration 32, loss = 0.02439157
Iteration 33, loss = 0.02355610
Iteration 34, loss = 0.02281241
Iteration 35, loss = 0.02213015
Iteration 36, loss = 0.02161353
Iteration 37, loss = 0.02088912
Iteration 38, loss = 0.02033000
Iteration 39, loss = 0.01990956
Iteration 40, loss = 0.01947892
Iteration 41, loss = 0.01905983
Iteration 42, loss = 0.01856950
Iteration 43, loss = 0.01825218
Iteration 44, loss = 0.01794258
Iteration 45, loss = 0.01771261
Iteration 46, loss = 0.01743616
Iteration 47, loss = 0.01706463
Iteration 48, loss = 0.01685693
Iteration 49, loss = 0.01657617
Iteration 50, loss = 0.01642184
Iteration 51, loss = 0.01610229
Iteration 52, loss = 0.01595494
Iteration 53, loss = 0.01578485
Iteration 54, loss = 0.01561791
Iteration 55, loss = 0.01545141
Iteration 56, loss = 0.01536208
Iteration 57, loss = 0.01515925
Iteration 58, loss = 0.01502007
Iteration 59, loss = 0.01490596
Iteration 60, loss = 0.01481789
Iteration 61, loss = 0.01466871
Iteration 62, loss = 0.01460802
Iteration 63, loss = 0.01440719
Iteration 64, loss = 0.01435507
Iteration 65, loss = 0.01417341
Iteration 66, loss = 0.01402879
Iteration 67, loss = 0.01401994
Iteration 68, loss = 0.01388299
Iteration 69, loss = 0.01383889
Iteration 70, loss = 0.01388311
Iteration 71, loss = 0.01377734
Iteration 72, loss = 0.01365716
Iteration 73, loss = 0.01369633
Iteration 74, loss = 0.01351710
Iteration 75, loss = 0.01351366
Iteration 76, loss = 0.01349632
Iteration 77, loss = 0.01334421
Iteration 78, loss = 0.01325525
Iteration 79, loss = 0.01315628
Iteration 80, loss = 0.01322733
Iteration 81, loss = 0.01304661
Iteration 82, loss = 0.01304117
Iteration 83, loss = 0.01307106
Iteration 84, loss = 0.01307640
Iteration 85, loss = 0.01294776
Iteration 86, loss = 0.01283375
Iteration 87, loss = 0.01294440
Iteration 88, loss = 0.01276447
Iteration 89, loss = 0.01272244
Iteration 90, loss = 0.01273810
Iteration 91, loss = 0.01281196
Iteration 92, loss = 0.01268356
Iteration 93, loss = 0.01273349
Iteration 94, loss = 0.01255659
Iteration 95, loss = 0.01257949
Iteration 96, loss = 0.01246504
Iteration 97, loss = 0.01248055
Iteration 98, loss = 0.01254803
Iteration 99, loss = 0.01249100
Iteration 100, loss = 0.01255273
Iteration 101, loss = 0.01238703
Iteration 102, loss = 0.01247760
Iteration 103, loss = 0.01234934
Iteration 104, loss = 0.01228137
Iteration 105, loss = 0.01228780
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68896449
Iteration 2, loss = 0.67133412
Iteration 3, loss = 0.65697982
Iteration 4, loss = 0.64272566
Iteration 5, loss = 0.62785934
Iteration 6, loss = 0.61185858
Iteration 7, loss = 0.59477127
Iteration 8, loss = 0.57650070
Iteration 9, loss = 0.55709842
Iteration 10, loss = 0.53664089
Iteration 11, loss = 0.51512465
Iteration 12, loss = 0.49291949
Iteration 13, loss = 0.47013546
Iteration 14, loss = 0.44706670
Iteration 15, loss = 0.42391826
Iteration 16, loss = 0.40085617
Iteration 17, loss = 0.37815576
Iteration 18, loss = 0.35592924
Iteration 19, loss = 0.33434586
Iteration 20, loss = 0.31342556
Iteration 21, loss = 0.29353667
Iteration 22, loss = 0.27456527
Iteration 23, loss = 0.25654028
Iteration 24, loss = 0.23963728
Iteration 25, loss = 0.22373305
Iteration 26, loss = 0.20884833
Iteration 27, loss = 0.19508029
Iteration 28, loss = 0.18226290
Iteration 29, loss = 0.17033849
Iteration 30, loss = 0.15934833
Iteration 31, loss = 0.14917789
Iteration 32, loss = 0.13980879
Iteration 33, loss = 0.13116582
Iteration 34, loss = 0.12321224
Iteration 35, loss = 0.11585393
Iteration 36, loss = 0.10909433
Iteration 37, loss = 0.10279474
Iteration 38, loss = 0.09700279
Iteration 39, loss = 0.09166522
Iteration 40, loss = 0.08673776
Iteration 41, loss = 0.08216581
Iteration 42, loss = 0.07795199
Iteration 43, loss = 0.07401839
Iteration 44, loss = 0.07041305
Iteration 45, loss = 0.06703184
Iteration 46, loss = 0.06384234
Iteration 47, loss = 0.06092627
Iteration 48, loss = 0.05818513
Iteration 49, loss = 0.05564129
Iteration 50, loss = 0.05324458
Iteration 51, loss = 0.05099335
Iteration 52, loss = 0.04889337
Iteration 53, loss = 0.04690436
Iteration 54, loss = 0.04507766
Iteration 55, loss = 0.04333526
Iteration 56, loss = 0.04168512
Iteration 57, loss = 0.04016360
Iteration 58, loss = 0.03872966
Iteration 59, loss = 0.03734281
Iteration 60, loss = 0.03603312
Iteration 61, loss = 0.03482055
Iteration 62, loss = 0.03365556
Iteration 63, loss = 0.03253834
Iteration 64, loss = 0.03148245
Iteration 65, loss = 0.03049813
Iteration 66, loss = 0.02953198
Iteration 67, loss = 0.02864419
Iteration 68, loss = 0.02776365
Iteration 69, loss = 0.02693582
Iteration 70, loss = 0.02614991
Iteration 71, loss = 0.02541680
Iteration 72, loss = 0.02471007
Iteration 73, loss = 0.02399690
Iteration 74, loss = 0.02334146
Iteration 75, loss = 0.02270849
Iteration 76, loss = 0.02210641
Iteration 77, loss = 0.02153139
Iteration 78, loss = 0.02097060
Iteration 79, loss = 0.02045004
Iteration 80, loss = 0.01993386
Iteration 81, loss = 0.01944293
Iteration 82, loss = 0.01896991
Iteration 83, loss = 0.01852067
Iteration 84, loss = 0.01808768
Iteration 85, loss = 0.01766179
Iteration 86, loss = 0.01725668
Iteration 87, loss = 0.01686724
Iteration 88, loss = 0.01649995
Iteration 89, loss = 0.01613096
Iteration 90, loss = 0.01579058
Iteration 91, loss = 0.01544089
Iteration 92, loss = 0.01512030
Iteration 93, loss = 0.01479965
Iteration 94, loss = 0.01449657
Iteration 95, loss = 0.01419685
Iteration 96, loss = 0.01390701
Iteration 97, loss = 0.01362473
Iteration 98, loss = 0.01335786
Iteration 99, loss = 0.01310315
Iteration 100, loss = 0.01285054
Iteration 101, loss = 0.01261411
Iteration 102, loss = 0.01237426
Iteration 103, loss = 0.01214550
Iteration 104, loss = 0.01192374
Iteration 105, loss = 0.01170992
Iteration 106, loss = 0.01150381
Iteration 107, loss = 0.01130404
Iteration 108, loss = 0.01111423
Iteration 109, loss = 0.01092248
Iteration 110, loss = 0.01072493
Iteration 111, loss = 0.01055783
Iteration 112, loss = 0.01037479
Iteration 113, loss = 0.01021925
Iteration 114, loss = 0.01004264
Iteration 115, loss = 0.00987810
Iteration 116, loss = 0.00972696
Iteration 117, loss = 0.00955964
Iteration 118, loss = 0.00940752
Iteration 119, loss = 0.00925923
Iteration 120, loss = 0.00912954
Iteration 121, loss = 0.00898657
Iteration 122, loss = 0.00884915
Iteration 123, loss = 0.00871512
Iteration 124, loss = 0.00858535
Iteration 125, loss = 0.00846070
Iteration 126, loss = 0.00834408
Iteration 127, loss = 0.00822191
Iteration 128, loss = 0.00810683
Iteration 129, loss = 0.00799500
Iteration 130, loss = 0.00788561
Iteration 131, loss = 0.00778080
Iteration 132, loss = 0.00767622
Iteration 133, loss = 0.00757537
Iteration 134, loss = 0.00747540
Iteration 135, loss = 0.00738229
Iteration 136, loss = 0.00728436
Iteration 137, loss = 0.00719379
Iteration 138, loss = 0.00710278
Iteration 139, loss = 0.00701539
Iteration 140, loss = 0.00692979
Iteration 141, loss = 0.00684108
Iteration 142, loss = 0.00675793
Iteration 143, loss = 0.00667765
Iteration 144, loss = 0.00659632
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67704704
Iteration 2, loss = 0.62396915
Iteration 3, loss = 0.54703494
Iteration 4, loss = 0.45831536
Iteration 5, loss = 0.37209852
Iteration 6, loss = 0.29526244
Iteration 7, loss = 0.23270453
Iteration 8, loss = 0.18386328
Iteration 9, loss = 0.14750174
Iteration 10, loss = 0.12037560
Iteration 11, loss = 0.10037434
Iteration 12, loss = 0.08505024
Iteration 13, loss = 0.07348924
Iteration 14, loss = 0.06448735
Iteration 15, loss = 0.05728180
Iteration 16, loss = 0.05147696
Iteration 17, loss = 0.04701959
Iteration 18, loss = 0.04302378
Iteration 19, loss = 0.03972416
Iteration 20, loss = 0.03724658
Iteration 21, loss = 0.03493378
Iteration 22, loss = 0.03278861
Iteration 23, loss = 0.03118381
Iteration 24, loss = 0.02974314
Iteration 25, loss = 0.02839842
Iteration 26, loss = 0.02720286
Iteration 27, loss = 0.02633385
Iteration 28, loss = 0.02537135
Iteration 29, loss = 0.02459951
Iteration 30, loss = 0.02389966
Iteration 31, loss = 0.02325302
Iteration 32, loss = 0.02267863
Iteration 33, loss = 0.02210740
Iteration 34, loss = 0.02156461
Iteration 35, loss = 0.02120606
Iteration 36, loss = 0.02080919
Iteration 37, loss = 0.02042642
Iteration 38, loss = 0.02025310
Iteration 39, loss = 0.02001865
Iteration 40, loss = 0.01961707
Iteration 41, loss = 0.01928545
Iteration 42, loss = 0.01898453
Iteration 43, loss = 0.01880494
Iteration 44, loss = 0.01871084
Iteration 45, loss = 0.01845972
Iteration 46, loss = 0.01825500
Iteration 47, loss = 0.01800526
Iteration 48, loss = 0.01784990
Iteration 49, loss = 0.01773330
Iteration 50, loss = 0.01756447
Iteration 51, loss = 0.01761697
Iteration 52, loss = 0.01726293
Iteration 53, loss = 0.01736427
Iteration 54, loss = 0.01713435
Iteration 55, loss = 0.01709650
Iteration 56, loss = 0.01681854
Iteration 57, loss = 0.01692394
Iteration 58, loss = 0.01684915
Iteration 59, loss = 0.01671475
Iteration 60, loss = 0.01654347
Iteration 61, loss = 0.01644123
Iteration 62, loss = 0.01638626
Iteration 63, loss = 0.01638610
Iteration 64, loss = 0.01624023
Iteration 65, loss = 0.01668709
Iteration 66, loss = 0.01618469
Iteration 67, loss = 0.01608115
Iteration 68, loss = 0.01606899
Iteration 69, loss = 0.01609629
Iteration 70, loss = 0.01596334
Iteration 71, loss = 0.01594655
Iteration 72, loss = 0.01558152
Iteration 73, loss = 0.01579703
Iteration 74, loss = 0.01574528
Iteration 75, loss = 0.01587435
Iteration 76, loss = 0.01573112
Iteration 77, loss = 0.01565137
Iteration 78, loss = 0.01562352
Iteration 79, loss = 0.01566821
Iteration 80, loss = 0.01559086
Iteration 81, loss = 0.01549183
Iteration 82, loss = 0.01547733
Iteration 83, loss = 0.01550813
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69293726
Iteration 2, loss = 0.65020716
Iteration 3, loss = 0.58890006
Iteration 4, loss = 0.50760774
Iteration 5, loss = 0.42148252
Iteration 6, loss = 0.34055669
Iteration 7, loss = 0.27175200
Iteration 8, loss = 0.21711407
Iteration 9, loss = 0.17483164
Iteration 10, loss = 0.14281695
Iteration 11, loss = 0.11857959
Iteration 12, loss = 0.10036792
Iteration 13, loss = 0.08609924
Iteration 14, loss = 0.07518431
Iteration 15, loss = 0.06637423
Iteration 16, loss = 0.05915472
Iteration 17, loss = 0.05358898
Iteration 18, loss = 0.04901292
Iteration 19, loss = 0.04502903
Iteration 20, loss = 0.04196290
Iteration 21, loss = 0.03895897
Iteration 22, loss = 0.03642227
Iteration 23, loss = 0.03460572
Iteration 24, loss = 0.03273253
Iteration 25, loss = 0.03103208
Iteration 26, loss = 0.02995892
Iteration 27, loss = 0.02836337
Iteration 28, loss = 0.02741613
Iteration 29, loss = 0.02644420
Iteration 30, loss = 0.02579528
Iteration 31, loss = 0.02477335
Iteration 32, loss = 0.02413470
Iteration 33, loss = 0.02360139
Iteration 34, loss = 0.02273262
Iteration 35, loss = 0.02229480
Iteration 36, loss = 0.02183086
Iteration 37, loss = 0.02133291
Iteration 38, loss = 0.02091837
Iteration 39, loss = 0.02057781
Iteration 40, loss = 0.02034004
Iteration 41, loss = 0.02001007
Iteration 42, loss = 0.01966909
Iteration 43, loss = 0.01948606
Iteration 44, loss = 0.01925680
Iteration 45, loss = 0.01897423
Iteration 46, loss = 0.01876390
Iteration 47, loss = 0.01829579
Iteration 48, loss = 0.01831337
Iteration 49, loss = 0.01799491
Iteration 50, loss = 0.01791613
Iteration 51, loss = 0.01757959
Iteration 52, loss = 0.01754704
Iteration 53, loss = 0.01740459
Iteration 54, loss = 0.01730224
Iteration 55, loss = 0.01714496
Iteration 56, loss = 0.01705723
Iteration 57, loss = 0.01682860
Iteration 58, loss = 0.01679242
Iteration 59, loss = 0.01655758
Iteration 60, loss = 0.01646999
Iteration 61, loss = 0.01629988
Iteration 62, loss = 0.01621031
Iteration 63, loss = 0.01626611
Iteration 64, loss = 0.01622132
Iteration 65, loss = 0.01617641
Iteration 66, loss = 0.01608598
Iteration 67, loss = 0.01611479
Iteration 68, loss = 0.01581044
Iteration 69, loss = 0.01583112
Iteration 70, loss = 0.01587865
Iteration 71, loss = 0.01566547
Iteration 72, loss = 0.01565248
Iteration 73, loss = 0.01580308
Iteration 74, loss = 0.01551883
Iteration 75, loss = 0.01555749
Iteration 76, loss = 0.01532147
Iteration 77, loss = 0.01548699
Iteration 78, loss = 0.01539135
Iteration 79, loss = 0.01515017
Iteration 80, loss = 0.01520656
Iteration 81, loss = 0.01529408
Iteration 82, loss = 0.01521586
Iteration 83, loss = 0.01538847
Iteration 84, loss = 0.01538427
Iteration 85, loss = 0.01524446
Iteration 86, loss = 0.01501954
Iteration 87, loss = 0.01496732
Iteration 88, loss = 0.01492748
Iteration 89, loss = 0.01493117
Iteration 90, loss = 0.01505914
Iteration 91, loss = 0.01499663
Iteration 92, loss = 0.01483213
Iteration 93, loss = 0.01473038
Iteration 94, loss = 0.01476069
Iteration 95, loss = 0.01452565
Iteration 96, loss = 0.01473677
Iteration 97, loss = 0.01466485
Iteration 98, loss = 0.01452425
Iteration 99, loss = 0.01473352
Iteration 100, loss = 0.01456269
Iteration 101, loss = 0.01450883
Iteration 102, loss = 0.01448269
Iteration 103, loss = 0.01442383
Iteration 104, loss = 0.01456795
Iteration 105, loss = 0.01456027
Iteration 106, loss = 0.01437027
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67740402
Iteration 2, loss = 0.62543481
Iteration 3, loss = 0.55036605
Iteration 4, loss = 0.46281894
Iteration 5, loss = 0.37914202
Iteration 6, loss = 0.30516917
Iteration 7, loss = 0.24389431
Iteration 8, loss = 0.19515051
Iteration 9, loss = 0.15795694
Iteration 10, loss = 0.12972821
Iteration 11, loss = 0.10824847
Iteration 12, loss = 0.09192931
Iteration 13, loss = 0.07932182
Iteration 14, loss = 0.06912810
Iteration 15, loss = 0.06144766
Iteration 16, loss = 0.05504670
Iteration 17, loss = 0.04975771
Iteration 18, loss = 0.04552231
Iteration 19, loss = 0.04174049
Iteration 20, loss = 0.03874826
Iteration 21, loss = 0.03620928
Iteration 22, loss = 0.03389006
Iteration 23, loss = 0.03199013
Iteration 24, loss = 0.03028232
Iteration 25, loss = 0.02878197
Iteration 26, loss = 0.02758432
Iteration 27, loss = 0.02643920
Iteration 28, loss = 0.02551738
Iteration 29, loss = 0.02456901
Iteration 30, loss = 0.02365467
Iteration 31, loss = 0.02278393
Iteration 32, loss = 0.02224051
Iteration 33, loss = 0.02161194
Iteration 34, loss = 0.02099690
Iteration 35, loss = 0.02043669
Iteration 36, loss = 0.02014001
Iteration 37, loss = 0.01961664
Iteration 38, loss = 0.01930433
Iteration 39, loss = 0.01888122
Iteration 40, loss = 0.01856401
Iteration 41, loss = 0.01830566
Iteration 42, loss = 0.01780006
Iteration 43, loss = 0.01763897
Iteration 44, loss = 0.01722321
Iteration 45, loss = 0.01725646
Iteration 46, loss = 0.01711419
Iteration 47, loss = 0.01683715
Iteration 48, loss = 0.01679449
Iteration 49, loss = 0.01641901
Iteration 50, loss = 0.01624722
Iteration 51, loss = 0.01600149
Iteration 52, loss = 0.01575190
Iteration 53, loss = 0.01577397
Iteration 54, loss = 0.01577388
Iteration 55, loss = 0.01588164
Iteration 56, loss = 0.01552506
Iteration 57, loss = 0.01528914
Iteration 58, loss = 0.01510599
Iteration 59, loss = 0.01503869
Iteration 60, loss = 0.01494169
Iteration 61, loss = 0.01505290
Iteration 62, loss = 0.01487520
Iteration 63, loss = 0.01481708
Iteration 64, loss = 0.01489456
Iteration 65, loss = 0.01448915
Iteration 66, loss = 0.01457601
Iteration 67, loss = 0.01453921
Iteration 68, loss = 0.01435066
Iteration 69, loss = 0.01436101
Iteration 70, loss = 0.01421788
Iteration 71, loss = 0.01420210
Iteration 72, loss = 0.01405528
Iteration 73, loss = 0.01397617
Iteration 74, loss = 0.01411799
Iteration 75, loss = 0.01404897
Iteration 76, loss = 0.01405368
Iteration 77, loss = 0.01401675
Iteration 78, loss = 0.01419848
Iteration 79, loss = 0.01386940
Iteration 80, loss = 0.01365150
Iteration 81, loss = 0.01382073
Iteration 82, loss = 0.01376008
Iteration 83, loss = 0.01348882
Iteration 84, loss = 0.01382135
Iteration 85, loss = 0.01367486
Iteration 86, loss = 0.01368012
Iteration 87, loss = 0.01362038
Iteration 88, loss = 0.01369456
Iteration 89, loss = 0.01362162
Iteration 90, loss = 0.01368623
Iteration 91, loss = 0.01325486
Iteration 92, loss = 0.01324496
Iteration 93, loss = 0.01337065
Iteration 94, loss = 0.01326621
Iteration 95, loss = 0.01327751
Iteration 96, loss = 0.01330623
Iteration 97, loss = 0.01324710
Iteration 98, loss = 0.01351840
Iteration 99, loss = 0.01325151
Iteration 100, loss = 0.01318583
Iteration 101, loss = 0.01320147
Iteration 102, loss = 0.01335714
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67860103
Iteration 2, loss = 0.62072567
Iteration 3, loss = 0.53569167
Iteration 4, loss = 0.43891468
Iteration 5, loss = 0.34947822
Iteration 6, loss = 0.27405304
Iteration 7, loss = 0.21462976
Iteration 8, loss = 0.16983223
Iteration 9, loss = 0.13695865
Iteration 10, loss = 0.11266337
Iteration 11, loss = 0.09446355
Iteration 12, loss = 0.08086589
Iteration 13, loss = 0.07025123
Iteration 14, loss = 0.06217310
Iteration 15, loss = 0.05550124
Iteration 16, loss = 0.05042799
Iteration 17, loss = 0.04602283
Iteration 18, loss = 0.04249331
Iteration 19, loss = 0.03954869
Iteration 20, loss = 0.03711296
Iteration 21, loss = 0.03501180
Iteration 22, loss = 0.03317505
Iteration 23, loss = 0.03152235
Iteration 24, loss = 0.03011674
Iteration 25, loss = 0.02902579
Iteration 26, loss = 0.02809774
Iteration 27, loss = 0.02706998
Iteration 28, loss = 0.02612668
Iteration 29, loss = 0.02537025
Iteration 30, loss = 0.02483355
Iteration 31, loss = 0.02408357
Iteration 32, loss = 0.02358178
Iteration 33, loss = 0.02304054
Iteration 34, loss = 0.02268233
Iteration 35, loss = 0.02234993
Iteration 36, loss = 0.02209789
Iteration 37, loss = 0.02166537
Iteration 38, loss = 0.02137498
Iteration 39, loss = 0.02098999
Iteration 40, loss = 0.02079023
Iteration 41, loss = 0.02040792
Iteration 42, loss = 0.02021760
Iteration 43, loss = 0.02002355
Iteration 44, loss = 0.01972803
Iteration 45, loss = 0.01963797
Iteration 46, loss = 0.01932261
Iteration 47, loss = 0.01941601
Iteration 48, loss = 0.01918710
Iteration 49, loss = 0.01889392
Iteration 50, loss = 0.01881092
Iteration 51, loss = 0.01894403
Iteration 52, loss = 0.01880809
Iteration 53, loss = 0.01859275
Iteration 54, loss = 0.01842550
Iteration 55, loss = 0.01825037
Iteration 56, loss = 0.01815948
Iteration 57, loss = 0.01810172
Iteration 58, loss = 0.01786829
Iteration 59, loss = 0.01785944
Iteration 60, loss = 0.01778751
Iteration 61, loss = 0.01796211
Iteration 62, loss = 0.01784676
Iteration 63, loss = 0.01772109
Iteration 64, loss = 0.01743317
Iteration 65, loss = 0.01744431
Iteration 66, loss = 0.01752640
Iteration 67, loss = 0.01763459
Iteration 68, loss = 0.01739511
Iteration 69, loss = 0.01713593
Iteration 70, loss = 0.01725162
Iteration 71, loss = 0.01734198
Iteration 72, loss = 0.01716843
Iteration 73, loss = 0.01694840
Iteration 74, loss = 0.01701021
Iteration 75, loss = 0.01717079
Iteration 76, loss = 0.01695307
Iteration 77, loss = 0.01696398
Iteration 78, loss = 0.01702500
Iteration 79, loss = 0.01678509
Iteration 80, loss = 0.01702047
Iteration 81, loss = 0.01677342
Iteration 82, loss = 0.01674663
Iteration 83, loss = 0.01701721
Iteration 84, loss = 0.01693854
Iteration 85, loss = 0.01681649
Iteration 86, loss = 0.01680537
Iteration 87, loss = 0.01668103
Iteration 88, loss = 0.01663407
Iteration 89, loss = 0.01660536
Iteration 90, loss = 0.01684306
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69725629
Iteration 2, loss = 0.64652256
Iteration 3, loss = 0.57327164
Iteration 4, loss = 0.47814161
Iteration 5, loss = 0.38205059
Iteration 6, loss = 0.29758574
Iteration 7, loss = 0.22997489
Iteration 8, loss = 0.17935573
Iteration 9, loss = 0.14219166
Iteration 10, loss = 0.11533683
Iteration 11, loss = 0.09549376
Iteration 12, loss = 0.08083079
Iteration 13, loss = 0.06938985
Iteration 14, loss = 0.06073847
Iteration 15, loss = 0.05385392
Iteration 16, loss = 0.04824544
Iteration 17, loss = 0.04398424
Iteration 18, loss = 0.04018656
Iteration 19, loss = 0.03717120
Iteration 20, loss = 0.03450926
Iteration 21, loss = 0.03224954
Iteration 22, loss = 0.03046236
Iteration 23, loss = 0.02874600
Iteration 24, loss = 0.02734224
Iteration 25, loss = 0.02628454
Iteration 26, loss = 0.02504590
Iteration 27, loss = 0.02402331
Iteration 28, loss = 0.02321824
Iteration 29, loss = 0.02242744
Iteration 30, loss = 0.02180168
Iteration 31, loss = 0.02117495
Iteration 32, loss = 0.02052896
Iteration 33, loss = 0.02007057
Iteration 34, loss = 0.01961621
Iteration 35, loss = 0.01939979
Iteration 36, loss = 0.01896362
Iteration 37, loss = 0.01840679
Iteration 38, loss = 0.01829952
Iteration 39, loss = 0.01779905
Iteration 40, loss = 0.01745862
Iteration 41, loss = 0.01748390
Iteration 42, loss = 0.01720686
Iteration 43, loss = 0.01675519
Iteration 44, loss = 0.01667433
Iteration 45, loss = 0.01653757
Iteration 46, loss = 0.01618533
Iteration 47, loss = 0.01636601
Iteration 48, loss = 0.01617021
Iteration 49, loss = 0.01584070
Iteration 50, loss = 0.01566337
Iteration 51, loss = 0.01565551
Iteration 52, loss = 0.01535577
Iteration 53, loss = 0.01538794
Iteration 54, loss = 0.01516975
Iteration 55, loss = 0.01508219
Iteration 56, loss = 0.01502520
Iteration 57, loss = 0.01493967
Iteration 58, loss = 0.01482806
Iteration 59, loss = 0.01469726
Iteration 60, loss = 0.01456476
Iteration 61, loss = 0.01448734
Iteration 62, loss = 0.01452142
Iteration 63, loss = 0.01454870
Iteration 64, loss = 0.01461764
Iteration 65, loss = 0.01419021
Iteration 66, loss = 0.01444140
Iteration 67, loss = 0.01430954
Iteration 68, loss = 0.01415169
Iteration 69, loss = 0.01436337
Iteration 70, loss = 0.01408904
Iteration 71, loss = 0.01397784
Iteration 72, loss = 0.01387428
Iteration 73, loss = 0.01387322
Iteration 74, loss = 0.01395618
Iteration 75, loss = 0.01382234
Iteration 76, loss = 0.01384155
Iteration 77, loss = 0.01368677
Iteration 78, loss = 0.01368957
Iteration 79, loss = 0.01368035
Iteration 80, loss = 0.01361810
Iteration 81, loss = 0.01369863
Iteration 82, loss = 0.01380545
Iteration 83, loss = 0.01366096
Iteration 84, loss = 0.01366129
Iteration 85, loss = 0.01370145
Iteration 86, loss = 0.01354142
Iteration 87, loss = 0.01346354
Iteration 88, loss = 0.01343613
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71854830
Iteration 2, loss = 0.67619653
Iteration 3, loss = 0.62088465
Iteration 4, loss = 0.55458660
Iteration 5, loss = 0.48009762
Iteration 6, loss = 0.40502196
Iteration 7, loss = 0.33525703
Iteration 8, loss = 0.27475361
Iteration 9, loss = 0.22495878
Iteration 10, loss = 0.18519273
Iteration 11, loss = 0.15381503
Iteration 12, loss = 0.12935037
Iteration 13, loss = 0.11011544
Iteration 14, loss = 0.09490407
Iteration 15, loss = 0.08312018
Iteration 16, loss = 0.07346069
Iteration 17, loss = 0.06568680
Iteration 18, loss = 0.05929027
Iteration 19, loss = 0.05408654
Iteration 20, loss = 0.04951429
Iteration 21, loss = 0.04580345
Iteration 22, loss = 0.04262719
Iteration 23, loss = 0.03984409
Iteration 24, loss = 0.03750753
Iteration 25, loss = 0.03543630
Iteration 26, loss = 0.03357478
Iteration 27, loss = 0.03193254
Iteration 28, loss = 0.03058623
Iteration 29, loss = 0.02931358
Iteration 30, loss = 0.02813576
Iteration 31, loss = 0.02708959
Iteration 32, loss = 0.02640120
Iteration 33, loss = 0.02545396
Iteration 34, loss = 0.02482852
Iteration 35, loss = 0.02402711
Iteration 36, loss = 0.02348342
Iteration 37, loss = 0.02286965
Iteration 38, loss = 0.02230949
Iteration 39, loss = 0.02190300
Iteration 40, loss = 0.02136209
Iteration 41, loss = 0.02090755
Iteration 42, loss = 0.02068995
Iteration 43, loss = 0.02032923
Iteration 44, loss = 0.01999222
Iteration 45, loss = 0.01975924
Iteration 46, loss = 0.01930173
Iteration 47, loss = 0.01913024
Iteration 48, loss = 0.01887237
Iteration 49, loss = 0.01864810
Iteration 50, loss = 0.01843824
Iteration 51, loss = 0.01827751
Iteration 52, loss = 0.01808535
Iteration 53, loss = 0.01797688
Iteration 54, loss = 0.01789516
Iteration 55, loss = 0.01749414
Iteration 56, loss = 0.01732456
Iteration 57, loss = 0.01725575
Iteration 58, loss = 0.01705325
Iteration 59, loss = 0.01694087
Iteration 60, loss = 0.01695843
Iteration 61, loss = 0.01690355
Iteration 62, loss = 0.01677797
Iteration 63, loss = 0.01652720
Iteration 64, loss = 0.01638352
Iteration 65, loss = 0.01626921
Iteration 66, loss = 0.01615783
Iteration 67, loss = 0.01612703
Iteration 68, loss = 0.01611694
Iteration 69, loss = 0.01607653
Iteration 70, loss = 0.01601742
Iteration 71, loss = 0.01583713
Iteration 72, loss = 0.01583231
Iteration 73, loss = 0.01575392
Iteration 74, loss = 0.01572627
Iteration 75, loss = 0.01571680
Iteration 76, loss = 0.01552786
Iteration 77, loss = 0.01546902
Iteration 78, loss = 0.01537444
Iteration 79, loss = 0.01546180
Iteration 80, loss = 0.01528285
Iteration 81, loss = 0.01518927
Iteration 82, loss = 0.01530055
Iteration 83, loss = 0.01510584
Iteration 84, loss = 0.01517329
Iteration 85, loss = 0.01506470
Iteration 86, loss = 0.01500154
Iteration 87, loss = 0.01506843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69065230
Iteration 2, loss = 0.65240000
Iteration 3, loss = 0.59958871
Iteration 4, loss = 0.53188191
Iteration 5, loss = 0.45848037
Iteration 6, loss = 0.38780046
Iteration 7, loss = 0.32329237
Iteration 8, loss = 0.26725026
Iteration 9, loss = 0.22029849
Iteration 10, loss = 0.18230981
Iteration 11, loss = 0.15199843
Iteration 12, loss = 0.12800734
Iteration 13, loss = 0.10907360
Iteration 14, loss = 0.09398316
Iteration 15, loss = 0.08188494
Iteration 16, loss = 0.07202815
Iteration 17, loss = 0.06412197
Iteration 18, loss = 0.05751227
Iteration 19, loss = 0.05216418
Iteration 20, loss = 0.04740733
Iteration 21, loss = 0.04350617
Iteration 22, loss = 0.04021460
Iteration 23, loss = 0.03727874
Iteration 24, loss = 0.03477779
Iteration 25, loss = 0.03273968
Iteration 26, loss = 0.03077040
Iteration 27, loss = 0.02900953
Iteration 28, loss = 0.02761282
Iteration 29, loss = 0.02622673
Iteration 30, loss = 0.02503520
Iteration 31, loss = 0.02392859
Iteration 32, loss = 0.02304368
Iteration 33, loss = 0.02208772
Iteration 34, loss = 0.02137199
Iteration 35, loss = 0.02062483
Iteration 36, loss = 0.01988727
Iteration 37, loss = 0.01932862
Iteration 38, loss = 0.01871167
Iteration 39, loss = 0.01824055
Iteration 40, loss = 0.01781795
Iteration 41, loss = 0.01730673
Iteration 42, loss = 0.01690203
Iteration 43, loss = 0.01653248
Iteration 44, loss = 0.01625754
Iteration 45, loss = 0.01607333
Iteration 46, loss = 0.01556893
Iteration 47, loss = 0.01523803
Iteration 48, loss = 0.01500132
Iteration 49, loss = 0.01477062
Iteration 50, loss = 0.01452328
Iteration 51, loss = 0.01428545
Iteration 52, loss = 0.01416204
Iteration 53, loss = 0.01395632
Iteration 54, loss = 0.01367878
Iteration 55, loss = 0.01362054
Iteration 56, loss = 0.01333957
Iteration 57, loss = 0.01341792
Iteration 58, loss = 0.01317243
Iteration 59, loss = 0.01291956
Iteration 60, loss = 0.01285900
Iteration 61, loss = 0.01272994
Iteration 62, loss = 0.01255802
Iteration 63, loss = 0.01252291
Iteration 64, loss = 0.01238792
Iteration 65, loss = 0.01234446
Iteration 66, loss = 0.01220168
Iteration 67, loss = 0.01206460
Iteration 68, loss = 0.01201186
Iteration 69, loss = 0.01193634
Iteration 70, loss = 0.01194697
Iteration 71, loss = 0.01171300
Iteration 72, loss = 0.01162569
Iteration 73, loss = 0.01159295
Iteration 74, loss = 0.01151254
Iteration 75, loss = 0.01151617
Iteration 76, loss = 0.01143747
Iteration 77, loss = 0.01150897
Iteration 78, loss = 0.01126269
Iteration 79, loss = 0.01127048
Iteration 80, loss = 0.01114288
Iteration 81, loss = 0.01107880
Iteration 82, loss = 0.01105876
Iteration 83, loss = 0.01112228
Iteration 84, loss = 0.01098568
Iteration 85, loss = 0.01112348
Iteration 86, loss = 0.01095116
Iteration 87, loss = 0.01077725
Iteration 88, loss = 0.01081534
Iteration 89, loss = 0.01073482
Iteration 90, loss = 0.01077780
Iteration 91, loss = 0.01073457
Iteration 92, loss = 0.01073541
Iteration 93, loss = 0.01061451
Iteration 94, loss = 0.01063052
Iteration 95, loss = 0.01082636
Iteration 96, loss = 0.01054197
Iteration 97, loss = 0.01058223
Iteration 98, loss = 0.01054087
Iteration 99, loss = 0.01083219
Iteration 100, loss = 0.01077964
Iteration 101, loss = 0.01047169
Iteration 102, loss = 0.01040234
Iteration 103, loss = 0.01047016
Iteration 104, loss = 0.01036742
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72204197
Iteration 2, loss = 0.69267547
Iteration 3, loss = 0.65889470
Iteration 4, loss = 0.61627370
Iteration 5, loss = 0.56680435
Iteration 6, loss = 0.51322449
Iteration 7, loss = 0.45896922
Iteration 8, loss = 0.40608286
Iteration 9, loss = 0.35644066
Iteration 10, loss = 0.31124228
Iteration 11, loss = 0.27105601
Iteration 12, loss = 0.23607633
Iteration 13, loss = 0.20575006
Iteration 14, loss = 0.17992732
Iteration 15, loss = 0.15813849
Iteration 16, loss = 0.13961265
Iteration 17, loss = 0.12413972
Iteration 18, loss = 0.11092211
Iteration 19, loss = 0.09973520
Iteration 20, loss = 0.09017138
Iteration 21, loss = 0.08202219
Iteration 22, loss = 0.07512064
Iteration 23, loss = 0.06886154
Iteration 24, loss = 0.06373163
Iteration 25, loss = 0.05917156
Iteration 26, loss = 0.05524755
Iteration 27, loss = 0.05165246
Iteration 28, loss = 0.04853937
Iteration 29, loss = 0.04570335
Iteration 30, loss = 0.04312663
Iteration 31, loss = 0.04085015
Iteration 32, loss = 0.03866786
Iteration 33, loss = 0.03697893
Iteration 34, loss = 0.03526533
Iteration 35, loss = 0.03371795
Iteration 36, loss = 0.03236697
Iteration 37, loss = 0.03113349
Iteration 38, loss = 0.02987413
Iteration 39, loss = 0.02881912
Iteration 40, loss = 0.02787752
Iteration 41, loss = 0.02684581
Iteration 42, loss = 0.02599877
Iteration 43, loss = 0.02508221
Iteration 44, loss = 0.02432068
Iteration 45, loss = 0.02370326
Iteration 46, loss = 0.02314442
Iteration 47, loss = 0.02246006
Iteration 48, loss = 0.02193344
Iteration 49, loss = 0.02131706
Iteration 50, loss = 0.02083957
Iteration 51, loss = 0.02042360
Iteration 52, loss = 0.02003982
Iteration 53, loss = 0.01959213
Iteration 54, loss = 0.01916330
Iteration 55, loss = 0.01879027
Iteration 56, loss = 0.01848211
Iteration 57, loss = 0.01807610
Iteration 58, loss = 0.01776062
Iteration 59, loss = 0.01748239
Iteration 60, loss = 0.01722557
Iteration 61, loss = 0.01693781
Iteration 62, loss = 0.01669253
Iteration 63, loss = 0.01638665
Iteration 64, loss = 0.01617188
Iteration 65, loss = 0.01591776
Iteration 66, loss = 0.01569391
Iteration 67, loss = 0.01549674
Iteration 68, loss = 0.01527862
Iteration 69, loss = 0.01513277
Iteration 70, loss = 0.01489847
Iteration 71, loss = 0.01471574
Iteration 72, loss = 0.01456338
Iteration 73, loss = 0.01440942
Iteration 74, loss = 0.01428031
Iteration 75, loss = 0.01414341
Iteration 76, loss = 0.01395580
Iteration 77, loss = 0.01390830
Iteration 78, loss = 0.01389394
Iteration 79, loss = 0.01380925
Iteration 80, loss = 0.01377132
Iteration 81, loss = 0.01339723
Iteration 82, loss = 0.01333847
Iteration 83, loss = 0.01332960
Iteration 84, loss = 0.01322185
Iteration 85, loss = 0.01317769
Iteration 86, loss = 0.01309451
Iteration 87, loss = 0.01300545
Iteration 88, loss = 0.01285181
Iteration 89, loss = 0.01271722
Iteration 90, loss = 0.01271241
Iteration 91, loss = 0.01254052
Iteration 92, loss = 0.01247514
Iteration 93, loss = 0.01241151
Iteration 94, loss = 0.01229538
Iteration 95, loss = 0.01219352
Iteration 96, loss = 0.01211761
Iteration 97, loss = 0.01209696
Iteration 98, loss = 0.01201459
Iteration 99, loss = 0.01197754
Iteration 100, loss = 0.01181284
Iteration 101, loss = 0.01198273
Iteration 102, loss = 0.01203479
Iteration 103, loss = 0.01193042
Iteration 104, loss = 0.01182252
Iteration 105, loss = 0.01178030
Iteration 106, loss = 0.01164489
Iteration 107, loss = 0.01159953
Iteration 108, loss = 0.01152022
Iteration 109, loss = 0.01150473
Iteration 110, loss = 0.01137284
Iteration 111, loss = 0.01136271
Iteration 112, loss = 0.01133288
Iteration 113, loss = 0.01131092
Iteration 114, loss = 0.01124056
Iteration 115, loss = 0.01124817
Iteration 116, loss = 0.01119583
Iteration 117, loss = 0.01113656
Iteration 118, loss = 0.01098063
Iteration 119, loss = 0.01120163
Iteration 120, loss = 0.01108495
Iteration 121, loss = 0.01105622
Iteration 122, loss = 0.01096733
Iteration 123, loss = 0.01096969
Iteration 124, loss = 0.01096030
Iteration 125, loss = 0.01107841
Iteration 126, loss = 0.01098403
Iteration 127, loss = 0.01096862
Iteration 128, loss = 0.01086379
Iteration 129, loss = 0.01094526
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67682869
Iteration 2, loss = 0.62379551
Iteration 3, loss = 0.55240148
Iteration 4, loss = 0.47370528
Iteration 5, loss = 0.39656552
Iteration 6, loss = 0.32480281
Iteration 7, loss = 0.26218048
Iteration 8, loss = 0.21080906
Iteration 9, loss = 0.17065240
Iteration 10, loss = 0.13963022
Iteration 11, loss = 0.11625378
Iteration 12, loss = 0.09797261
Iteration 13, loss = 0.08385733
Iteration 14, loss = 0.07287103
Iteration 15, loss = 0.06407442
Iteration 16, loss = 0.05719260
Iteration 17, loss = 0.05135080
Iteration 18, loss = 0.04678719
Iteration 19, loss = 0.04288288
Iteration 20, loss = 0.03963298
Iteration 21, loss = 0.03671924
Iteration 22, loss = 0.03445825
Iteration 23, loss = 0.03230082
Iteration 24, loss = 0.03058534
Iteration 25, loss = 0.02913263
Iteration 26, loss = 0.02780508
Iteration 27, loss = 0.02688291
Iteration 28, loss = 0.02545019
Iteration 29, loss = 0.02442613
Iteration 30, loss = 0.02359804
Iteration 31, loss = 0.02265037
Iteration 32, loss = 0.02194941
Iteration 33, loss = 0.02137163
Iteration 34, loss = 0.02071729
Iteration 35, loss = 0.02047652
Iteration 36, loss = 0.01985456
Iteration 37, loss = 0.01934979
Iteration 38, loss = 0.01907491
Iteration 39, loss = 0.01876907
Iteration 40, loss = 0.01832484
Iteration 41, loss = 0.01815196
Iteration 42, loss = 0.01785611
Iteration 43, loss = 0.01765708
Iteration 44, loss = 0.01729056
Iteration 45, loss = 0.01698839
Iteration 46, loss = 0.01669909
Iteration 47, loss = 0.01645794
Iteration 48, loss = 0.01635716
Iteration 49, loss = 0.01615411
Iteration 50, loss = 0.01601472
Iteration 51, loss = 0.01570689
Iteration 52, loss = 0.01569915
Iteration 53, loss = 0.01554993
Iteration 54, loss = 0.01535386
Iteration 55, loss = 0.01533057
Iteration 56, loss = 0.01539254
Iteration 57, loss = 0.01520433
Iteration 58, loss = 0.01495465
Iteration 59, loss = 0.01494172
Iteration 60, loss = 0.01489150
Iteration 61, loss = 0.01474480
Iteration 62, loss = 0.01449512
Iteration 63, loss = 0.01475703
Iteration 64, loss = 0.01444550
Iteration 65, loss = 0.01434937
Iteration 66, loss = 0.01421185
Iteration 67, loss = 0.01404127
Iteration 68, loss = 0.01421361
Iteration 69, loss = 0.01414554
Iteration 70, loss = 0.01410354
Iteration 71, loss = 0.01392489
Iteration 72, loss = 0.01402074
Iteration 73, loss = 0.01383051
Iteration 74, loss = 0.01369643
Iteration 75, loss = 0.01369120
Iteration 76, loss = 0.01351991
Iteration 77, loss = 0.01363929
Iteration 78, loss = 0.01364488
Iteration 79, loss = 0.01370764
Iteration 80, loss = 0.01348061
Iteration 81, loss = 0.01341687
Iteration 82, loss = 0.01331408
Iteration 83, loss = 0.01341593
Iteration 84, loss = 0.01336718
Iteration 85, loss = 0.01330925
Iteration 86, loss = 0.01324726
Iteration 87, loss = 0.01330277
Iteration 88, loss = 0.01319473
Iteration 89, loss = 0.01321449
Iteration 90, loss = 0.01307291
Iteration 91, loss = 0.01315443
Iteration 92, loss = 0.01330175
Iteration 93, loss = 0.01320209
Iteration 94, loss = 0.01312955
Iteration 95, loss = 0.01295686
Iteration 96, loss = 0.01295251
Iteration 97, loss = 0.01299270
Iteration 98, loss = 0.01295958
Iteration 99, loss = 0.01281598
Iteration 100, loss = 0.01288763
Iteration 101, loss = 0.01293678
Iteration 102, loss = 0.01302744
Iteration 103, loss = 0.01306470
Iteration 104, loss = 0.01306304
Iteration 105, loss = 0.01299274
Iteration 106, loss = 0.01285478
Iteration 107, loss = 0.01280540
Iteration 108, loss = 0.01255749
Iteration 109, loss = 0.01284333
Iteration 110, loss = 0.01285256
Iteration 111, loss = 0.01276263
Iteration 112, loss = 0.01266538
Iteration 113, loss = 0.01267825
Iteration 114, loss = 0.01272131
Iteration 115, loss = 0.01266437
Iteration 116, loss = 0.01279170
Iteration 117, loss = 0.01251530
Iteration 118, loss = 0.01262219
Iteration 119, loss = 0.01252356
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67773116
Iteration 2, loss = 0.61391978
Iteration 3, loss = 0.52163824
Iteration 4, loss = 0.42487394
Iteration 5, loss = 0.33375062
Iteration 6, loss = 0.25543587
Iteration 7, loss = 0.19588332
Iteration 8, loss = 0.15275116
Iteration 9, loss = 0.12181391
Iteration 10, loss = 0.09969501
Iteration 11, loss = 0.08361935
Iteration 12, loss = 0.07133438
Iteration 13, loss = 0.06215185
Iteration 14, loss = 0.05514234
Iteration 15, loss = 0.04942725
Iteration 16, loss = 0.04479044
Iteration 17, loss = 0.04096256
Iteration 18, loss = 0.03782611
Iteration 19, loss = 0.03535085
Iteration 20, loss = 0.03307019
Iteration 21, loss = 0.03126285
Iteration 22, loss = 0.02963688
Iteration 23, loss = 0.02814107
Iteration 24, loss = 0.02704708
Iteration 25, loss = 0.02591663
Iteration 26, loss = 0.02503456
Iteration 27, loss = 0.02414242
Iteration 28, loss = 0.02335922
Iteration 29, loss = 0.02269919
Iteration 30, loss = 0.02211006
Iteration 31, loss = 0.02148132
Iteration 32, loss = 0.02118194
Iteration 33, loss = 0.02050682
Iteration 34, loss = 0.02013694
Iteration 35, loss = 0.01979972
Iteration 36, loss = 0.01936568
Iteration 37, loss = 0.01925028
Iteration 38, loss = 0.01889281
Iteration 39, loss = 0.01874925
Iteration 40, loss = 0.01829874
Iteration 41, loss = 0.01808085
Iteration 42, loss = 0.01790108
Iteration 43, loss = 0.01763900
Iteration 44, loss = 0.01770467
Iteration 45, loss = 0.01737948
Iteration 46, loss = 0.01722704
Iteration 47, loss = 0.01713616
Iteration 48, loss = 0.01684094
Iteration 49, loss = 0.01687319
Iteration 50, loss = 0.01654697
Iteration 51, loss = 0.01660514
Iteration 52, loss = 0.01638660
Iteration 53, loss = 0.01632272
Iteration 54, loss = 0.01609311
Iteration 55, loss = 0.01581175
Iteration 56, loss = 0.01586177
Iteration 57, loss = 0.01596456
Iteration 58, loss = 0.01557103
Iteration 59, loss = 0.01564200
Iteration 60, loss = 0.01538854
Iteration 61, loss = 0.01534244
Iteration 62, loss = 0.01531947
Iteration 63, loss = 0.01534961
Iteration 64, loss = 0.01521188
Iteration 65, loss = 0.01507068
Iteration 66, loss = 0.01519724
Iteration 67, loss = 0.01508128
Iteration 68, loss = 0.01492022
Iteration 69, loss = 0.01505475
Iteration 70, loss = 0.01532263
Iteration 71, loss = 0.01514942
Iteration 72, loss = 0.01480238
Iteration 73, loss = 0.01465594
Iteration 74, loss = 0.01479190
Iteration 75, loss = 0.01475035
Iteration 76, loss = 0.01450068
Iteration 77, loss = 0.01663160
Iteration 78, loss = 0.01703218
Iteration 79, loss = 0.01650370
Iteration 80, loss = 0.01620211
Iteration 81, loss = 0.01609082
Iteration 82, loss = 0.01621214
Iteration 83, loss = 0.01606663
Iteration 84, loss = 0.01565623
Iteration 85, loss = 0.01542822
Iteration 86, loss = 0.01534021
Iteration 87, loss = 0.01522641
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68944580
Iteration 2, loss = 0.65863058
Iteration 3, loss = 0.62241125
Iteration 4, loss = 0.57813990
Iteration 5, loss = 0.52668593
Iteration 6, loss = 0.47144148
Iteration 7, loss = 0.41709095
Iteration 8, loss = 0.36602981
Iteration 9, loss = 0.31971195
Iteration 10, loss = 0.27836504
Iteration 11, loss = 0.24242256
Iteration 12, loss = 0.21139094
Iteration 13, loss = 0.18483262
Iteration 14, loss = 0.16225739
Iteration 15, loss = 0.14329119
Iteration 16, loss = 0.12720308
Iteration 17, loss = 0.11346365
Iteration 18, loss = 0.10223418
Iteration 19, loss = 0.09263014
Iteration 20, loss = 0.08417549
Iteration 21, loss = 0.07706265
Iteration 22, loss = 0.07086265
Iteration 23, loss = 0.06546598
Iteration 24, loss = 0.06076816
Iteration 25, loss = 0.05670534
Iteration 26, loss = 0.05303911
Iteration 27, loss = 0.04985425
Iteration 28, loss = 0.04704020
Iteration 29, loss = 0.04444761
Iteration 30, loss = 0.04215618
Iteration 31, loss = 0.04010573
Iteration 32, loss = 0.03835775
Iteration 33, loss = 0.03658683
Iteration 34, loss = 0.03513055
Iteration 35, loss = 0.03383312
Iteration 36, loss = 0.03254867
Iteration 37, loss = 0.03153295
Iteration 38, loss = 0.03037399
Iteration 39, loss = 0.02931396
Iteration 40, loss = 0.02837652
Iteration 41, loss = 0.02775530
Iteration 42, loss = 0.02709197
Iteration 43, loss = 0.02641060
Iteration 44, loss = 0.02563018
Iteration 45, loss = 0.02486781
Iteration 46, loss = 0.02424963
Iteration 47, loss = 0.02358398
Iteration 48, loss = 0.02320277
Iteration 49, loss = 0.02318259
Iteration 50, loss = 0.02268035
Iteration 51, loss = 0.02228591
Iteration 52, loss = 0.02168164
Iteration 53, loss = 0.02125556
Iteration 54, loss = 0.02125119
Iteration 55, loss = 0.02083318
Iteration 56, loss = 0.02032379
Iteration 57, loss = 0.01988050
Iteration 58, loss = 0.01954596
Iteration 59, loss = 0.01930659
Iteration 60, loss = 0.01918412
Iteration 61, loss = 0.01890403
Iteration 62, loss = 0.01863950
Iteration 63, loss = 0.01833874
Iteration 64, loss = 0.01800607
Iteration 65, loss = 0.01785546
Iteration 66, loss = 0.01768707
Iteration 67, loss = 0.01745231
Iteration 68, loss = 0.01731137
Iteration 69, loss = 0.01704297
Iteration 70, loss = 0.01687414
Iteration 71, loss = 0.01677194
Iteration 72, loss = 0.01656413
Iteration 73, loss = 0.01638523
Iteration 74, loss = 0.01647113
Iteration 75, loss = 0.01645684
Iteration 76, loss = 0.01630980
Iteration 77, loss = 0.01614544
Iteration 78, loss = 0.01594832
Iteration 79, loss = 0.01574214
Iteration 80, loss = 0.01565720
Iteration 81, loss = 0.01559156
Iteration 82, loss = 0.01547263
Iteration 83, loss = 0.01534751
Iteration 84, loss = 0.01519162
Iteration 85, loss = 0.01512956
Iteration 86, loss = 0.01495523
Iteration 87, loss = 0.01490578
Iteration 88, loss = 0.01481340
Iteration 89, loss = 0.01474159
Iteration 90, loss = 0.01467333
Iteration 91, loss = 0.01477841
Iteration 92, loss = 0.01470199
Iteration 93, loss = 0.01465861
Iteration 94, loss = 0.01458896
Iteration 95, loss = 0.01449841
Iteration 96, loss = 0.01452422
Iteration 97, loss = 0.01503229
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71058288
Iteration 2, loss = 0.67957724
Iteration 3, loss = 0.62525134
Iteration 4, loss = 0.54385114
Iteration 5, loss = 0.45727399
Iteration 6, loss = 0.37479840
Iteration 7, loss = 0.30326424
Iteration 8, loss = 0.24385866
Iteration 9, loss = 0.19671368
Iteration 10, loss = 0.15998715
Iteration 11, loss = 0.13190977
Iteration 12, loss = 0.11029505
Iteration 13, loss = 0.09365430
Iteration 14, loss = 0.08089673
Iteration 15, loss = 0.07064260
Iteration 16, loss = 0.06256150
Iteration 17, loss = 0.05598309
Iteration 18, loss = 0.05057728
Iteration 19, loss = 0.04631019
Iteration 20, loss = 0.04258188
Iteration 21, loss = 0.03926432
Iteration 22, loss = 0.03668181
Iteration 23, loss = 0.03448290
Iteration 24, loss = 0.03236891
Iteration 25, loss = 0.03063458
Iteration 26, loss = 0.02901408
Iteration 27, loss = 0.02785535
Iteration 28, loss = 0.02650757
Iteration 29, loss = 0.02567314
Iteration 30, loss = 0.02467979
Iteration 31, loss = 0.02360078
Iteration 32, loss = 0.02297440
Iteration 33, loss = 0.02220285
Iteration 34, loss = 0.02172180
Iteration 35, loss = 0.02113033
Iteration 36, loss = 0.02064078
Iteration 37, loss = 0.02018577
Iteration 38, loss = 0.01957249
Iteration 39, loss = 0.01907786
Iteration 40, loss = 0.01892580
Iteration 41, loss = 0.01844283
Iteration 42, loss = 0.01825622
Iteration 43, loss = 0.01786311
Iteration 44, loss = 0.01776023
Iteration 45, loss = 0.01751812
Iteration 46, loss = 0.01715296
Iteration 47, loss = 0.01698233
Iteration 48, loss = 0.01673350
Iteration 49, loss = 0.01655340
Iteration 50, loss = 0.01643661
Iteration 51, loss = 0.01632325
Iteration 52, loss = 0.01616090
Iteration 53, loss = 0.01591620
Iteration 54, loss = 0.01584535
Iteration 55, loss = 0.01567698
Iteration 56, loss = 0.01540594
Iteration 57, loss = 0.01543468
Iteration 58, loss = 0.01538345
Iteration 59, loss = 0.01528028
Iteration 60, loss = 0.01521631
Iteration 61, loss = 0.01491882
Iteration 62, loss = 0.01494966
Iteration 63, loss = 0.01483726
Iteration 64, loss = 0.01475459
Iteration 65, loss = 0.01470520
Iteration 66, loss = 0.01458316
Iteration 67, loss = 0.01446886
Iteration 68, loss = 0.01445111
Iteration 69, loss = 0.01437721
Iteration 70, loss = 0.01422942
Iteration 71, loss = 0.01425628
Iteration 72, loss = 0.01418578
Iteration 73, loss = 0.01419706
Iteration 74, loss = 0.01400786
Iteration 75, loss = 0.01430368
Iteration 76, loss = 0.01396682
Iteration 77, loss = 0.01395809
Iteration 78, loss = 0.01385500
Iteration 79, loss = 0.01392972
Iteration 80, loss = 0.01380867
Iteration 81, loss = 0.01360592
Iteration 82, loss = 0.01367391
Iteration 83, loss = 0.01360962
Iteration 84, loss = 0.01357648
Iteration 85, loss = 0.01360810
Iteration 86, loss = 0.01349779
Iteration 87, loss = 0.01352652
Iteration 88, loss = 0.01353975
Iteration 89, loss = 0.01354438
Iteration 90, loss = 0.01353616
Iteration 91, loss = 0.01336663
Iteration 92, loss = 0.01338182
Iteration 93, loss = 0.01332083
Iteration 94, loss = 0.01357419
Iteration 95, loss = 0.01334203
Iteration 96, loss = 0.01332785
Iteration 97, loss = 0.01311070
Iteration 98, loss = 0.01331356
Iteration 99, loss = 0.01321064
Iteration 100, loss = 0.01341594
Iteration 101, loss = 0.01321684
Iteration 102, loss = 0.01312418
Iteration 103, loss = 0.01317424
Iteration 104, loss = 0.01309327
Iteration 105, loss = 0.01303757
Iteration 106, loss = 0.01304106
Iteration 107, loss = 0.01296400
Iteration 108, loss = 0.01309133
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.66801698
Iteration 2, loss = 0.65156103
Iteration 3, loss = 0.63794648
Iteration 4, loss = 0.62461835
Iteration 5, loss = 0.61064570
Iteration 6, loss = 0.59554044
Iteration 7, loss = 0.57916999
Iteration 8, loss = 0.56129388
Iteration 9, loss = 0.54202104
Iteration 10, loss = 0.52144833
Iteration 11, loss = 0.49975246
Iteration 12, loss = 0.47733154
Iteration 13, loss = 0.45465699
Iteration 14, loss = 0.43172294
Iteration 15, loss = 0.40861750
Iteration 16, loss = 0.38622148
Iteration 17, loss = 0.36408667
Iteration 18, loss = 0.34293899
Iteration 19, loss = 0.32198784
Iteration 20, loss = 0.30233333
Iteration 21, loss = 0.28357669
Iteration 22, loss = 0.26578848
Iteration 23, loss = 0.24908249
Iteration 24, loss = 0.23339206
Iteration 25, loss = 0.21866278
Iteration 26, loss = 0.20496887
Iteration 27, loss = 0.19224452
Iteration 28, loss = 0.18029125
Iteration 29, loss = 0.16934282
Iteration 30, loss = 0.15919941
Iteration 31, loss = 0.14979390
Iteration 32, loss = 0.14104712
Iteration 33, loss = 0.13298567
Iteration 34, loss = 0.12551752
Iteration 35, loss = 0.11860292
Iteration 36, loss = 0.11219886
Iteration 37, loss = 0.10629455
Iteration 38, loss = 0.10082694
Iteration 39, loss = 0.09579213
Iteration 40, loss = 0.09101548
Iteration 41, loss = 0.08669421
Iteration 42, loss = 0.08256835
Iteration 43, loss = 0.07877079
Iteration 44, loss = 0.07527884
Iteration 45, loss = 0.07197558
Iteration 46, loss = 0.06886369
Iteration 47, loss = 0.06598564
Iteration 48, loss = 0.06327227
Iteration 49, loss = 0.06073937
Iteration 50, loss = 0.05840372
Iteration 51, loss = 0.05614195
Iteration 52, loss = 0.05407169
Iteration 53, loss = 0.05209041
Iteration 54, loss = 0.05025910
Iteration 55, loss = 0.04849497
Iteration 56, loss = 0.04686182
Iteration 57, loss = 0.04530853
Iteration 58, loss = 0.04385652
Iteration 59, loss = 0.04248265
Iteration 60, loss = 0.04117000
Iteration 61, loss = 0.03996239
Iteration 62, loss = 0.03877381
Iteration 63, loss = 0.03762206
Iteration 64, loss = 0.03655511
Iteration 65, loss = 0.03553902
Iteration 66, loss = 0.03458122
Iteration 67, loss = 0.03365993
Iteration 68, loss = 0.03278335
Iteration 69, loss = 0.03192761
Iteration 70, loss = 0.03112577
Iteration 71, loss = 0.03033643
Iteration 72, loss = 0.02960645
Iteration 73, loss = 0.02888716
Iteration 74, loss = 0.02819276
Iteration 75, loss = 0.02753007
Iteration 76, loss = 0.02687864
Iteration 77, loss = 0.02631407
Iteration 78, loss = 0.02570057
Iteration 79, loss = 0.02516009
Iteration 80, loss = 0.02459966
Iteration 81, loss = 0.02409861
Iteration 82, loss = 0.02361079
Iteration 83, loss = 0.02314433
Iteration 84, loss = 0.02268926
Iteration 85, loss = 0.02226060
Iteration 86, loss = 0.02183853
Iteration 87, loss = 0.02143591
Iteration 88, loss = 0.02108091
Iteration 89, loss = 0.02066663
Iteration 90, loss = 0.02029422
Iteration 91, loss = 0.01994302
Iteration 92, loss = 0.01957430
Iteration 93, loss = 0.01924008
Iteration 94, loss = 0.01895042
Iteration 95, loss = 0.01859782
Iteration 96, loss = 0.01829260
Iteration 97, loss = 0.01800174
Iteration 98, loss = 0.01768426
Iteration 99, loss = 0.01739436
Iteration 100, loss = 0.01712403
Iteration 101, loss = 0.01687779
Iteration 102, loss = 0.01660512
Iteration 103, loss = 0.01636480
Iteration 104, loss = 0.01612010
Iteration 105, loss = 0.01590169
Iteration 106, loss = 0.01566886
Iteration 107, loss = 0.01544533
Iteration 108, loss = 0.01523722
Iteration 109, loss = 0.01501586
Iteration 110, loss = 0.01482310
Iteration 111, loss = 0.01462490
Iteration 112, loss = 0.01443186
Iteration 113, loss = 0.01425224
Iteration 114, loss = 0.01407359
Iteration 115, loss = 0.01388323
Iteration 116, loss = 0.01372402
Iteration 117, loss = 0.01355158
Iteration 118, loss = 0.01338776
Iteration 119, loss = 0.01321716
Iteration 120, loss = 0.01307625
Iteration 121, loss = 0.01291177
Iteration 122, loss = 0.01277394
Iteration 123, loss = 0.01263358
Iteration 124, loss = 0.01250068
Iteration 125, loss = 0.01236198
Iteration 126, loss = 0.01223743
Iteration 127, loss = 0.01209653
Iteration 128, loss = 0.01198495
Iteration 129, loss = 0.01186733
Iteration 130, loss = 0.01175764
Iteration 131, loss = 0.01165596
Iteration 132, loss = 0.01154499
Iteration 133, loss = 0.01141996
Iteration 134, loss = 0.01130437
Iteration 135, loss = 0.01120294
Iteration 136, loss = 0.01109211
Iteration 137, loss = 0.01095640
Iteration 138, loss = 0.01084392
Iteration 139, loss = 0.01075858
Iteration 140, loss = 0.01068816
Iteration 141, loss = 0.01057521
Iteration 142, loss = 0.01049299
Iteration 143, loss = 0.01037957
Iteration 144, loss = 0.01031693
Iteration 145, loss = 0.01020788
Iteration 146, loss = 0.01011815
Iteration 147, loss = 0.01004350
Iteration 148, loss = 0.00999985
Iteration 149, loss = 0.00989586
Iteration 150, loss = 0.00980266
Iteration 151, loss = 0.00971785
Iteration 152, loss = 0.00966465
Iteration 153, loss = 0.00956693
Iteration 154, loss = 0.00950700
Iteration 155, loss = 0.00942318
Iteration 156, loss = 0.00935502
Iteration 157, loss = 0.00928540
Iteration 158, loss = 0.00923827
Iteration 159, loss = 0.00915038
Iteration 160, loss = 0.00909299
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67965773
Iteration 2, loss = 0.66194137
Iteration 3, loss = 0.64660140
Iteration 4, loss = 0.63157227
Iteration 5, loss = 0.61591072
Iteration 6, loss = 0.59936968
Iteration 7, loss = 0.58172717
Iteration 8, loss = 0.56290174
Iteration 9, loss = 0.54293105
Iteration 10, loss = 0.52161175
Iteration 11, loss = 0.49939206
Iteration 12, loss = 0.47645638
Iteration 13, loss = 0.45307093
Iteration 14, loss = 0.42928094
Iteration 15, loss = 0.40553687
Iteration 16, loss = 0.38220191
Iteration 17, loss = 0.35945332
Iteration 18, loss = 0.33750958
Iteration 19, loss = 0.31644189
Iteration 20, loss = 0.29638631
Iteration 21, loss = 0.27738679
Iteration 22, loss = 0.25946931
Iteration 23, loss = 0.24269837
Iteration 24, loss = 0.22698840
Iteration 25, loss = 0.21228102
Iteration 26, loss = 0.19853069
Iteration 27, loss = 0.18585008
Iteration 28, loss = 0.17402032
Iteration 29, loss = 0.16307626
Iteration 30, loss = 0.15286684
Iteration 31, loss = 0.14352289
Iteration 32, loss = 0.13489904
Iteration 33, loss = 0.12688843
Iteration 34, loss = 0.11946338
Iteration 35, loss = 0.11258654
Iteration 36, loss = 0.10634026
Iteration 37, loss = 0.10056704
Iteration 38, loss = 0.09518609
Iteration 39, loss = 0.09016536
Iteration 40, loss = 0.08559401
Iteration 41, loss = 0.08133237
Iteration 42, loss = 0.07733446
Iteration 43, loss = 0.07364656
Iteration 44, loss = 0.07027351
Iteration 45, loss = 0.06702734
Iteration 46, loss = 0.06408479
Iteration 47, loss = 0.06130702
Iteration 48, loss = 0.05874349
Iteration 49, loss = 0.05631266
Iteration 50, loss = 0.05409698
Iteration 51, loss = 0.05195413
Iteration 52, loss = 0.04997364
Iteration 53, loss = 0.04816885
Iteration 54, loss = 0.04640355
Iteration 55, loss = 0.04473620
Iteration 56, loss = 0.04324262
Iteration 57, loss = 0.04171773
Iteration 58, loss = 0.04032427
Iteration 59, loss = 0.03900329
Iteration 60, loss = 0.03778325
Iteration 61, loss = 0.03659796
Iteration 62, loss = 0.03548838
Iteration 63, loss = 0.03443701
Iteration 64, loss = 0.03340799
Iteration 65, loss = 0.03244327
Iteration 66, loss = 0.03153267
Iteration 67, loss = 0.03067789
Iteration 68, loss = 0.02984382
Iteration 69, loss = 0.02909153
Iteration 70, loss = 0.02832346
Iteration 71, loss = 0.02756480
Iteration 72, loss = 0.02691679
Iteration 73, loss = 0.02623357
Iteration 74, loss = 0.02562573
Iteration 75, loss = 0.02501757
Iteration 76, loss = 0.02445114
Iteration 77, loss = 0.02387713
Iteration 78, loss = 0.02333031
Iteration 79, loss = 0.02282502
Iteration 80, loss = 0.02232315
Iteration 81, loss = 0.02186926
Iteration 82, loss = 0.02141354
Iteration 83, loss = 0.02096638
Iteration 84, loss = 0.02054865
Iteration 85, loss = 0.02015581
Iteration 86, loss = 0.01974656
Iteration 87, loss = 0.01937255
Iteration 88, loss = 0.01902855
Iteration 89, loss = 0.01866263
Iteration 90, loss = 0.01833122
Iteration 91, loss = 0.01799812
Iteration 92, loss = 0.01767545
Iteration 93, loss = 0.01741355
Iteration 94, loss = 0.01710143
Iteration 95, loss = 0.01682143
Iteration 96, loss = 0.01653783
Iteration 97, loss = 0.01628467
Iteration 98, loss = 0.01600722
Iteration 99, loss = 0.01575355
Iteration 100, loss = 0.01550831
Iteration 101, loss = 0.01527453
Iteration 102, loss = 0.01503503
Iteration 103, loss = 0.01482195
Iteration 104, loss = 0.01461066
Iteration 105, loss = 0.01441860
Iteration 106, loss = 0.01419766
Iteration 107, loss = 0.01398622
Iteration 108, loss = 0.01381387
Iteration 109, loss = 0.01366375
Iteration 110, loss = 0.01345368
Iteration 111, loss = 0.01327750
Iteration 112, loss = 0.01309177
Iteration 113, loss = 0.01293891
Iteration 114, loss = 0.01275307
Iteration 115, loss = 0.01259079
Iteration 116, loss = 0.01244663
Iteration 117, loss = 0.01228316
Iteration 118, loss = 0.01213698
Iteration 119, loss = 0.01198640
Iteration 120, loss = 0.01184580
Iteration 121, loss = 0.01172190
Iteration 122, loss = 0.01157809
Iteration 123, loss = 0.01145416
Iteration 124, loss = 0.01133914
Iteration 125, loss = 0.01121780
Iteration 126, loss = 0.01106593
Iteration 127, loss = 0.01103289
Iteration 128, loss = 0.01085166
Iteration 129, loss = 0.01074534
Iteration 130, loss = 0.01063089
Iteration 131, loss = 0.01054124
Iteration 132, loss = 0.01044094
Iteration 133, loss = 0.01031395
Iteration 134, loss = 0.01024829
Iteration 135, loss = 0.01012619
Iteration 136, loss = 0.01004253
Iteration 137, loss = 0.00995465
Iteration 138, loss = 0.00987619
Iteration 139, loss = 0.00975849
Iteration 140, loss = 0.00971569
Iteration 141, loss = 0.00962977
Iteration 142, loss = 0.00951528
Iteration 143, loss = 0.00945116
Iteration 144, loss = 0.00936017
Iteration 145, loss = 0.00929024
Iteration 146, loss = 0.00921427
Iteration 147, loss = 0.00915102
Iteration 148, loss = 0.00905089
Iteration 149, loss = 0.00897996
Iteration 150, loss = 0.00891011
Iteration 151, loss = 0.00884485
Iteration 152, loss = 0.00877574
Iteration 153, loss = 0.00872326
Iteration 154, loss = 0.00867578
Iteration 155, loss = 0.00860829
Iteration 156, loss = 0.00852884
Iteration 157, loss = 0.00847763
Iteration 158, loss = 0.00843227
Iteration 159, loss = 0.00835092
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68471207
Iteration 2, loss = 0.65411657
Iteration 3, loss = 0.62081159
Iteration 4, loss = 0.58038555
Iteration 5, loss = 0.53258372
Iteration 6, loss = 0.48087385
Iteration 7, loss = 0.42908391
Iteration 8, loss = 0.37938242
Iteration 9, loss = 0.33351214
Iteration 10, loss = 0.29163885
Iteration 11, loss = 0.25448074
Iteration 12, loss = 0.22176861
Iteration 13, loss = 0.19387174
Iteration 14, loss = 0.16988815
Iteration 15, loss = 0.14962244
Iteration 16, loss = 0.13248017
Iteration 17, loss = 0.11794533
Iteration 18, loss = 0.10563919
Iteration 19, loss = 0.09525061
Iteration 20, loss = 0.08624367
Iteration 21, loss = 0.07873047
Iteration 22, loss = 0.07209705
Iteration 23, loss = 0.06644065
Iteration 24, loss = 0.06126038
Iteration 25, loss = 0.05690527
Iteration 26, loss = 0.05308531
Iteration 27, loss = 0.04966347
Iteration 28, loss = 0.04657919
Iteration 29, loss = 0.04391358
Iteration 30, loss = 0.04157002
Iteration 31, loss = 0.03932541
Iteration 32, loss = 0.03729086
Iteration 33, loss = 0.03540429
Iteration 34, loss = 0.03411975
Iteration 35, loss = 0.03290284
Iteration 36, loss = 0.03159127
Iteration 37, loss = 0.03022896
Iteration 38, loss = 0.02893761
Iteration 39, loss = 0.02785867
Iteration 40, loss = 0.02685926
Iteration 41, loss = 0.02592982
Iteration 42, loss = 0.02513500
Iteration 43, loss = 0.02434823
Iteration 44, loss = 0.02366771
Iteration 45, loss = 0.02319039
Iteration 46, loss = 0.02244603
Iteration 47, loss = 0.02185018
Iteration 48, loss = 0.02121402
Iteration 49, loss = 0.02073490
Iteration 50, loss = 0.02018361
Iteration 51, loss = 0.01969917
Iteration 52, loss = 0.01925736
Iteration 53, loss = 0.01883213
Iteration 54, loss = 0.01845505
Iteration 55, loss = 0.01817176
Iteration 56, loss = 0.01776368
Iteration 57, loss = 0.01746004
Iteration 58, loss = 0.01713719
Iteration 59, loss = 0.01684591
Iteration 60, loss = 0.01661369
Iteration 61, loss = 0.01635587
Iteration 62, loss = 0.01616035
Iteration 63, loss = 0.01599005
Iteration 64, loss = 0.01566799
Iteration 65, loss = 0.01548691
Iteration 66, loss = 0.01529016
Iteration 67, loss = 0.01509696
Iteration 68, loss = 0.01479870
Iteration 69, loss = 0.01470584
Iteration 70, loss = 0.01451671
Iteration 71, loss = 0.01441625
Iteration 72, loss = 0.01426147
Iteration 73, loss = 0.01411689
Iteration 74, loss = 0.01398999
Iteration 75, loss = 0.01384747
Iteration 76, loss = 0.01375884
Iteration 77, loss = 0.01355153
Iteration 78, loss = 0.01347959
Iteration 79, loss = 0.01323979
Iteration 80, loss = 0.01337112
Iteration 81, loss = 0.01341833
Iteration 82, loss = 0.01313230
Iteration 83, loss = 0.01315579
Iteration 84, loss = 0.01308877
Iteration 85, loss = 0.01299252
Iteration 86, loss = 0.01282864
Iteration 87, loss = 0.01272038
Iteration 88, loss = 0.01258759
Iteration 89, loss = 0.01238562
Iteration 90, loss = 0.01227145
Iteration 91, loss = 0.01215241
Iteration 92, loss = 0.01219887
Iteration 93, loss = 0.01210192
Iteration 94, loss = 0.01204136
Iteration 95, loss = 0.01194247
Iteration 96, loss = 0.01183287
Iteration 97, loss = 0.01180022
Iteration 98, loss = 0.01173932
Iteration 99, loss = 0.01163831
Iteration 100, loss = 0.01153355
Iteration 101, loss = 0.01150136
Iteration 102, loss = 0.01143693
Iteration 103, loss = 0.01135928
Iteration 104, loss = 0.01130867
Iteration 105, loss = 0.01123678
Iteration 106, loss = 0.01119551
Iteration 107, loss = 0.01110702
Iteration 108, loss = 0.01121091
Iteration 109, loss = 0.01110419
Iteration 110, loss = 0.01105696
Iteration 111, loss = 0.01108638
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70457684
Iteration 2, loss = 0.68631455
Iteration 3, loss = 0.66889640
Iteration 4, loss = 0.64808365
Iteration 5, loss = 0.62252629
Iteration 6, loss = 0.59254604
Iteration 7, loss = 0.55856646
Iteration 8, loss = 0.52255441
Iteration 9, loss = 0.48517901
Iteration 10, loss = 0.44742633
Iteration 11, loss = 0.41044781
Iteration 12, loss = 0.37458685
Iteration 13, loss = 0.34060094
Iteration 14, loss = 0.30878495
Iteration 15, loss = 0.27933108
Iteration 16, loss = 0.25248060
Iteration 17, loss = 0.22809384
Iteration 18, loss = 0.20620416
Iteration 19, loss = 0.18658889
Iteration 20, loss = 0.16907322
Iteration 21, loss = 0.15364905
Iteration 22, loss = 0.13987865
Iteration 23, loss = 0.12776629
Iteration 24, loss = 0.11699996
Iteration 25, loss = 0.10750473
Iteration 26, loss = 0.09915661
Iteration 27, loss = 0.09156538
Iteration 28, loss = 0.08495961
Iteration 29, loss = 0.07902586
Iteration 30, loss = 0.07371610
Iteration 31, loss = 0.06892698
Iteration 32, loss = 0.06469746
Iteration 33, loss = 0.06079408
Iteration 34, loss = 0.05730134
Iteration 35, loss = 0.05415559
Iteration 36, loss = 0.05134222
Iteration 37, loss = 0.04861700
Iteration 38, loss = 0.04624866
Iteration 39, loss = 0.04406650
Iteration 40, loss = 0.04203382
Iteration 41, loss = 0.04018064
Iteration 42, loss = 0.03848499
Iteration 43, loss = 0.03688853
Iteration 44, loss = 0.03543049
Iteration 45, loss = 0.03408349
Iteration 46, loss = 0.03279942
Iteration 47, loss = 0.03164421
Iteration 48, loss = 0.03053649
Iteration 49, loss = 0.02952163
Iteration 50, loss = 0.02856601
Iteration 51, loss = 0.02765796
Iteration 52, loss = 0.02682958
Iteration 53, loss = 0.02603886
Iteration 54, loss = 0.02529790
Iteration 55, loss = 0.02457743
Iteration 56, loss = 0.02392703
Iteration 57, loss = 0.02330424
Iteration 58, loss = 0.02271296
Iteration 59, loss = 0.02219401
Iteration 60, loss = 0.02163456
Iteration 61, loss = 0.02113168
Iteration 62, loss = 0.02064234
Iteration 63, loss = 0.02020933
Iteration 64, loss = 0.01976849
Iteration 65, loss = 0.01938787
Iteration 66, loss = 0.01899289
Iteration 67, loss = 0.01861309
Iteration 68, loss = 0.01826869
Iteration 69, loss = 0.01790281
Iteration 70, loss = 0.01757689
Iteration 71, loss = 0.01731142
Iteration 72, loss = 0.01699608
Iteration 73, loss = 0.01673043
Iteration 74, loss = 0.01645825
Iteration 75, loss = 0.01615611
Iteration 76, loss = 0.01590141
Iteration 77, loss = 0.01566713
Iteration 78, loss = 0.01545390
Iteration 79, loss = 0.01523530
Iteration 80, loss = 0.01503267
Iteration 81, loss = 0.01483259
Iteration 82, loss = 0.01463217
Iteration 83, loss = 0.01442485
Iteration 84, loss = 0.01424701
Iteration 85, loss = 0.01407050
Iteration 86, loss = 0.01391829
Iteration 87, loss = 0.01373061
Iteration 88, loss = 0.01358909
Iteration 89, loss = 0.01345019
Iteration 90, loss = 0.01330161
Iteration 91, loss = 0.01316893
Iteration 92, loss = 0.01301474
Iteration 93, loss = 0.01288784
Iteration 94, loss = 0.01275940
Iteration 95, loss = 0.01262754
Iteration 96, loss = 0.01250296
Iteration 97, loss = 0.01238865
Iteration 98, loss = 0.01228406
Iteration 99, loss = 0.01216596
Iteration 100, loss = 0.01208000
Iteration 101, loss = 0.01197178
Iteration 102, loss = 0.01186833
Iteration 103, loss = 0.01174999
Iteration 104, loss = 0.01168087
Iteration 105, loss = 0.01157386
Iteration 106, loss = 0.01151406
Iteration 107, loss = 0.01140908
Iteration 108, loss = 0.01131433
Iteration 109, loss = 0.01126612
Iteration 110, loss = 0.01115219
Iteration 111, loss = 0.01109001
Iteration 112, loss = 0.01099022
Iteration 113, loss = 0.01096780
Iteration 114, loss = 0.01084624
Iteration 115, loss = 0.01079282
Iteration 116, loss = 0.01072967
Iteration 117, loss = 0.01066039
Iteration 118, loss = 0.01059000
Iteration 119, loss = 0.01055291
Iteration 120, loss = 0.01047570
Iteration 121, loss = 0.01045086
Iteration 122, loss = 0.01034772
Iteration 123, loss = 0.01035073
Iteration 124, loss = 0.01025668
Iteration 125, loss = 0.01019567
Iteration 126, loss = 0.01012464
Iteration 127, loss = 0.01007599
Iteration 128, loss = 0.01008179
Iteration 129, loss = 0.01002879
Iteration 130, loss = 0.00992346
Iteration 131, loss = 0.00988795
Iteration 132, loss = 0.00983530
Iteration 133, loss = 0.00981761
Iteration 134, loss = 0.00975299
Iteration 135, loss = 0.00971421
Iteration 136, loss = 0.00969765
Iteration 137, loss = 0.00966242
Iteration 138, loss = 0.00963626
Iteration 139, loss = 0.00957080
Iteration 140, loss = 0.00951490
Iteration 141, loss = 0.00947704
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68188384
Iteration 2, loss = 0.63686699
Iteration 3, loss = 0.57111042
Iteration 4, loss = 0.48552107
Iteration 5, loss = 0.39924563
Iteration 6, loss = 0.32145898
Iteration 7, loss = 0.25659578
Iteration 8, loss = 0.20455638
Iteration 9, loss = 0.16489502
Iteration 10, loss = 0.13490296
Iteration 11, loss = 0.11205760
Iteration 12, loss = 0.09487873
Iteration 13, loss = 0.08135752
Iteration 14, loss = 0.07091933
Iteration 15, loss = 0.06252649
Iteration 16, loss = 0.05591696
Iteration 17, loss = 0.05031322
Iteration 18, loss = 0.04569454
Iteration 19, loss = 0.04204408
Iteration 20, loss = 0.03886742
Iteration 21, loss = 0.03604308
Iteration 22, loss = 0.03364042
Iteration 23, loss = 0.03161888
Iteration 24, loss = 0.02985412
Iteration 25, loss = 0.02828046
Iteration 26, loss = 0.02680596
Iteration 27, loss = 0.02573018
Iteration 28, loss = 0.02464771
Iteration 29, loss = 0.02363717
Iteration 30, loss = 0.02268749
Iteration 31, loss = 0.02197282
Iteration 32, loss = 0.02133737
Iteration 33, loss = 0.02062988
Iteration 34, loss = 0.02007281
Iteration 35, loss = 0.01943646
Iteration 36, loss = 0.01920141
Iteration 37, loss = 0.01855163
Iteration 38, loss = 0.01824913
Iteration 39, loss = 0.01794570
Iteration 40, loss = 0.01735774
Iteration 41, loss = 0.01715183
Iteration 42, loss = 0.01697229
Iteration 43, loss = 0.01660117
Iteration 44, loss = 0.01628758
Iteration 45, loss = 0.01597062
Iteration 46, loss = 0.01579218
Iteration 47, loss = 0.01568014
Iteration 48, loss = 0.01536663
Iteration 49, loss = 0.01522588
Iteration 50, loss = 0.01516017
Iteration 51, loss = 0.01500599
Iteration 52, loss = 0.01479913
Iteration 53, loss = 0.01459832
Iteration 54, loss = 0.01447272
Iteration 55, loss = 0.01420939
Iteration 56, loss = 0.01419616
Iteration 57, loss = 0.01408458
Iteration 58, loss = 0.01397445
Iteration 59, loss = 0.01379576
Iteration 60, loss = 0.01372435
Iteration 61, loss = 0.01375681
Iteration 62, loss = 0.01359244
Iteration 63, loss = 0.01327424
Iteration 64, loss = 0.01343694
Iteration 65, loss = 0.01335687
Iteration 66, loss = 0.01324044
Iteration 67, loss = 0.01304390
Iteration 68, loss = 0.01295632
Iteration 69, loss = 0.01310881
Iteration 70, loss = 0.01292976
Iteration 71, loss = 0.01288676
Iteration 72, loss = 0.01286639
Iteration 73, loss = 0.01272456
Iteration 74, loss = 0.01269555
Iteration 75, loss = 0.01267417
Iteration 76, loss = 0.01269556
Iteration 77, loss = 0.01254346
Iteration 78, loss = 0.01258773
Iteration 79, loss = 0.01246789
Iteration 80, loss = 0.01262594
Iteration 81, loss = 0.01229772
Iteration 82, loss = 0.01228769
Iteration 83, loss = 0.01230813
Iteration 84, loss = 0.01222591
Iteration 85, loss = 0.01226760
Iteration 86, loss = 0.01213731
Iteration 87, loss = 0.01226633
Iteration 88, loss = 0.01213904
Iteration 89, loss = 0.01215033
Iteration 90, loss = 0.01219798
Iteration 91, loss = 0.01208890
Iteration 92, loss = 0.01197640
Iteration 93, loss = 0.01207911
Iteration 94, loss = 0.01199931
Iteration 95, loss = 0.01187702
Iteration 96, loss = 0.01200622
Iteration 97, loss = 0.01206425
Iteration 98, loss = 0.01185530
Iteration 99, loss = 0.01197891
Iteration 100, loss = 0.01184181
Iteration 101, loss = 0.01178804
Iteration 102, loss = 0.01188528
Iteration 103, loss = 0.01189537
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70261468
Iteration 2, loss = 0.61420116
Iteration 3, loss = 0.50917428
Iteration 4, loss = 0.39889917
Iteration 5, loss = 0.30273734
Iteration 6, loss = 0.22798767
Iteration 7, loss = 0.17330001
Iteration 8, loss = 0.13487557
Iteration 9, loss = 0.10786162
Iteration 10, loss = 0.08892085
Iteration 11, loss = 0.07480593
Iteration 12, loss = 0.06454840
Iteration 13, loss = 0.05660923
Iteration 14, loss = 0.05043551
Iteration 15, loss = 0.04566490
Iteration 16, loss = 0.04165169
Iteration 17, loss = 0.03863525
Iteration 18, loss = 0.03604911
Iteration 19, loss = 0.03380763
Iteration 20, loss = 0.03199081
Iteration 21, loss = 0.03039174
Iteration 22, loss = 0.02875349
Iteration 23, loss = 0.02761034
Iteration 24, loss = 0.02684124
Iteration 25, loss = 0.02598744
Iteration 26, loss = 0.02506256
Iteration 27, loss = 0.02435088
Iteration 28, loss = 0.02364740
Iteration 29, loss = 0.02318191
Iteration 30, loss = 0.02285096
Iteration 31, loss = 0.02230298
Iteration 32, loss = 0.02181759
Iteration 33, loss = 0.02156446
Iteration 34, loss = 0.02124343
Iteration 35, loss = 0.02075952
Iteration 36, loss = 0.02044598
Iteration 37, loss = 0.02029416
Iteration 38, loss = 0.01993619
Iteration 39, loss = 0.01990889
Iteration 40, loss = 0.01965175
Iteration 41, loss = 0.01942099
Iteration 42, loss = 0.01931170
Iteration 43, loss = 0.01907459
Iteration 44, loss = 0.01886706
Iteration 45, loss = 0.01872697
Iteration 46, loss = 0.01865148
Iteration 47, loss = 0.01855600
Iteration 48, loss = 0.01841859
Iteration 49, loss = 0.01834666
Iteration 50, loss = 0.01822870
Iteration 51, loss = 0.01811585
Iteration 52, loss = 0.01831130
Iteration 53, loss = 0.01811505
Iteration 54, loss = 0.01812459
Iteration 55, loss = 0.01775283
Iteration 56, loss = 0.01764609
Iteration 57, loss = 0.01776444
Iteration 58, loss = 0.01771199
Iteration 59, loss = 0.01753690
Iteration 60, loss = 0.01749424
Iteration 61, loss = 0.01750042
Iteration 62, loss = 0.01742970
Iteration 63, loss = 0.01733811
Iteration 64, loss = 0.01715377
Iteration 65, loss = 0.01712760
Iteration 66, loss = 0.01699639
Iteration 67, loss = 0.01721759
Iteration 68, loss = 0.01694510
Iteration 69, loss = 0.01716141
Iteration 70, loss = 0.01713541
Iteration 71, loss = 0.01693595
Iteration 72, loss = 0.01730242
Iteration 73, loss = 0.01690470
Iteration 74, loss = 0.01671304
Iteration 75, loss = 0.01700504
Iteration 76, loss = 0.01685083
Iteration 77, loss = 0.01672758
Iteration 78, loss = 0.01667439
Iteration 79, loss = 0.01666498
Iteration 80, loss = 0.01668310
Iteration 81, loss = 0.01662483
Iteration 82, loss = 0.01658091
Iteration 83, loss = 0.01660105
Iteration 84, loss = 0.01649663
Iteration 85, loss = 0.01650981
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67570797
Iteration 2, loss = 0.61106082
Iteration 3, loss = 0.50656377
Iteration 4, loss = 0.39446149
Iteration 5, loss = 0.29518536
Iteration 6, loss = 0.21906456
Iteration 7, loss = 0.16545858
Iteration 8, loss = 0.12831929
Iteration 9, loss = 0.10275852
Iteration 10, loss = 0.08473396
Iteration 11, loss = 0.07193488
Iteration 12, loss = 0.06206440
Iteration 13, loss = 0.05473253
Iteration 14, loss = 0.04927602
Iteration 15, loss = 0.04471941
Iteration 16, loss = 0.04101872
Iteration 17, loss = 0.03807125
Iteration 18, loss = 0.03550325
Iteration 19, loss = 0.03339565
Iteration 20, loss = 0.03167179
Iteration 21, loss = 0.03016124
Iteration 22, loss = 0.02879837
Iteration 23, loss = 0.02791729
Iteration 24, loss = 0.02705999
Iteration 25, loss = 0.02608188
Iteration 26, loss = 0.02519805
Iteration 27, loss = 0.02452752
Iteration 28, loss = 0.02408030
Iteration 29, loss = 0.02359136
Iteration 30, loss = 0.02285655
Iteration 31, loss = 0.02259717
Iteration 32, loss = 0.02210881
Iteration 33, loss = 0.02178952
Iteration 34, loss = 0.02148140
Iteration 35, loss = 0.02107406
Iteration 36, loss = 0.02098298
Iteration 37, loss = 0.02065115
Iteration 38, loss = 0.02064548
Iteration 39, loss = 0.02028371
Iteration 40, loss = 0.01988972
Iteration 41, loss = 0.02005709
Iteration 42, loss = 0.01959443
Iteration 43, loss = 0.01941039
Iteration 44, loss = 0.01936888
Iteration 45, loss = 0.01927892
Iteration 46, loss = 0.01910644
Iteration 47, loss = 0.01925035
Iteration 48, loss = 0.01872449
Iteration 49, loss = 0.01886509
Iteration 50, loss = 0.01858440
Iteration 51, loss = 0.01858360
Iteration 52, loss = 0.01854355
Iteration 53, loss = 0.01863155
Iteration 54, loss = 0.01839931
Iteration 55, loss = 0.01837480
Iteration 56, loss = 0.01799454
Iteration 57, loss = 0.01803654
Iteration 58, loss = 0.01798775
Iteration 59, loss = 0.01782332
Iteration 60, loss = 0.01780700
Iteration 61, loss = 0.01790835
Iteration 62, loss = 0.01771175
Iteration 63, loss = 0.01777580
Iteration 64, loss = 0.01773052
Iteration 65, loss = 0.01743743
Iteration 66, loss = 0.01732703
Iteration 67, loss = 0.01755633
Iteration 68, loss = 0.01758664
Iteration 69, loss = 0.01752727
Iteration 70, loss = 0.01747228
Iteration 71, loss = 0.01711591
Iteration 72, loss = 0.01715921
Iteration 73, loss = 0.01720659
Iteration 74, loss = 0.01721970
Iteration 75, loss = 0.01719052
Iteration 76, loss = 0.01705940
Iteration 77, loss = 0.01701481
Iteration 78, loss = 0.01724751
Iteration 79, loss = 0.01734863
Iteration 80, loss = 0.01722360
Iteration 81, loss = 0.01732950
Iteration 82, loss = 0.01699526
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69656164
Iteration 2, loss = 0.67822556
Iteration 3, loss = 0.66362426
Iteration 4, loss = 0.64934612
Iteration 5, loss = 0.63433475
Iteration 6, loss = 0.61797292
Iteration 7, loss = 0.60003398
Iteration 8, loss = 0.58045771
Iteration 9, loss = 0.55921536
Iteration 10, loss = 0.53668474
Iteration 11, loss = 0.51306625
Iteration 12, loss = 0.48878667
Iteration 13, loss = 0.46409390
Iteration 14, loss = 0.43934670
Iteration 15, loss = 0.41481241
Iteration 16, loss = 0.39063915
Iteration 17, loss = 0.36713395
Iteration 18, loss = 0.34423466
Iteration 19, loss = 0.32227939
Iteration 20, loss = 0.30134415
Iteration 21, loss = 0.28139313
Iteration 22, loss = 0.26253878
Iteration 23, loss = 0.24485880
Iteration 24, loss = 0.22835925
Iteration 25, loss = 0.21296986
Iteration 26, loss = 0.19867992
Iteration 27, loss = 0.18530813
Iteration 28, loss = 0.17307981
Iteration 29, loss = 0.16176402
Iteration 30, loss = 0.15130924
Iteration 31, loss = 0.14164178
Iteration 32, loss = 0.13280015
Iteration 33, loss = 0.12459223
Iteration 34, loss = 0.11710881
Iteration 35, loss = 0.11016501
Iteration 36, loss = 0.10383517
Iteration 37, loss = 0.09790439
Iteration 38, loss = 0.09250491
Iteration 39, loss = 0.08748642
Iteration 40, loss = 0.08284087
Iteration 41, loss = 0.07855962
Iteration 42, loss = 0.07455092
Iteration 43, loss = 0.07087516
Iteration 44, loss = 0.06743015
Iteration 45, loss = 0.06421190
Iteration 46, loss = 0.06124826
Iteration 47, loss = 0.05847099
Iteration 48, loss = 0.05586755
Iteration 49, loss = 0.05346263
Iteration 50, loss = 0.05118960
Iteration 51, loss = 0.04909386
Iteration 52, loss = 0.04710398
Iteration 53, loss = 0.04523710
Iteration 54, loss = 0.04348636
Iteration 55, loss = 0.04183190
Iteration 56, loss = 0.04026591
Iteration 57, loss = 0.03879977
Iteration 58, loss = 0.03743143
Iteration 59, loss = 0.03609197
Iteration 60, loss = 0.03484868
Iteration 61, loss = 0.03366781
Iteration 62, loss = 0.03256701
Iteration 63, loss = 0.03149325
Iteration 64, loss = 0.03049199
Iteration 65, loss = 0.02955356
Iteration 66, loss = 0.02864183
Iteration 67, loss = 0.02777886
Iteration 68, loss = 0.02695988
Iteration 69, loss = 0.02617077
Iteration 70, loss = 0.02543638
Iteration 71, loss = 0.02471696
Iteration 72, loss = 0.02402707
Iteration 73, loss = 0.02337274
Iteration 74, loss = 0.02275061
Iteration 75, loss = 0.02215528
Iteration 76, loss = 0.02157839
Iteration 77, loss = 0.02102015
Iteration 78, loss = 0.02050568
Iteration 79, loss = 0.01998432
Iteration 80, loss = 0.01949857
Iteration 81, loss = 0.01902509
Iteration 82, loss = 0.01856423
Iteration 83, loss = 0.01814046
Iteration 84, loss = 0.01771683
Iteration 85, loss = 0.01730690
Iteration 86, loss = 0.01692123
Iteration 87, loss = 0.01654593
Iteration 88, loss = 0.01618845
Iteration 89, loss = 0.01583477
Iteration 90, loss = 0.01549715
Iteration 91, loss = 0.01516724
Iteration 92, loss = 0.01485948
Iteration 93, loss = 0.01454955
Iteration 94, loss = 0.01425324
Iteration 95, loss = 0.01397623
Iteration 96, loss = 0.01370167
Iteration 97, loss = 0.01342268
Iteration 98, loss = 0.01316355
Iteration 99, loss = 0.01291376
Iteration 100, loss = 0.01265735
Iteration 101, loss = 0.01242859
Iteration 102, loss = 0.01219733
Iteration 103, loss = 0.01197845
Iteration 104, loss = 0.01176357
Iteration 105, loss = 0.01155452
Iteration 106, loss = 0.01135724
Iteration 107, loss = 0.01115782
Iteration 108, loss = 0.01096698
Iteration 109, loss = 0.01078093
Iteration 110, loss = 0.01059973
Iteration 111, loss = 0.01042198
Iteration 112, loss = 0.01025146
Iteration 113, loss = 0.01008556
Iteration 114, loss = 0.00992060
Iteration 115, loss = 0.00976536
Iteration 116, loss = 0.00961005
Iteration 117, loss = 0.00946331
Iteration 118, loss = 0.00931678
Iteration 119, loss = 0.00917588
Iteration 120, loss = 0.00903932
Iteration 121, loss = 0.00890420
Iteration 122, loss = 0.00877587
Iteration 123, loss = 0.00864963
Iteration 124, loss = 0.00852598
Iteration 125, loss = 0.00840231
Iteration 126, loss = 0.00829141
Iteration 127, loss = 0.00816881
Iteration 128, loss = 0.00805466
Iteration 129, loss = 0.00794075
Iteration 130, loss = 0.00783973
Iteration 131, loss = 0.00773395
Iteration 132, loss = 0.00762421
Iteration 133, loss = 0.00752222
Iteration 134, loss = 0.00742207
Iteration 135, loss = 0.00732552
Iteration 136, loss = 0.00723085
Iteration 137, loss = 0.00714140
Iteration 138, loss = 0.00705532
Iteration 139, loss = 0.00696666
Iteration 140, loss = 0.00687940
Iteration 141, loss = 0.00679709
Iteration 142, loss = 0.00671528
Iteration 143, loss = 0.00663177
Iteration 144, loss = 0.00655364
Iteration 145, loss = 0.00647361
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68362345
Iteration 2, loss = 0.66153334
Iteration 3, loss = 0.64092228
Iteration 4, loss = 0.61832192
Iteration 5, loss = 0.59208876
Iteration 6, loss = 0.56220200
Iteration 7, loss = 0.52915548
Iteration 8, loss = 0.49351675
Iteration 9, loss = 0.45685075
Iteration 10, loss = 0.41997749
Iteration 11, loss = 0.38433310
Iteration 12, loss = 0.35029610
Iteration 13, loss = 0.31828044
Iteration 14, loss = 0.28889031
Iteration 15, loss = 0.26190874
Iteration 16, loss = 0.23724091
Iteration 17, loss = 0.21509984
Iteration 18, loss = 0.19521390
Iteration 19, loss = 0.17740882
Iteration 20, loss = 0.16159328
Iteration 21, loss = 0.14751940
Iteration 22, loss = 0.13498921
Iteration 23, loss = 0.12393476
Iteration 24, loss = 0.11403565
Iteration 25, loss = 0.10523718
Iteration 26, loss = 0.09746524
Iteration 27, loss = 0.09054595
Iteration 28, loss = 0.08423538
Iteration 29, loss = 0.07870366
Iteration 30, loss = 0.07366573
Iteration 31, loss = 0.06919950
Iteration 32, loss = 0.06508792
Iteration 33, loss = 0.06141117
Iteration 34, loss = 0.05801299
Iteration 35, loss = 0.05500654
Iteration 36, loss = 0.05220679
Iteration 37, loss = 0.04964705
Iteration 38, loss = 0.04727127
Iteration 39, loss = 0.04516809
Iteration 40, loss = 0.04322288
Iteration 41, loss = 0.04136292
Iteration 42, loss = 0.03968714
Iteration 43, loss = 0.03815220
Iteration 44, loss = 0.03665872
Iteration 45, loss = 0.03532060
Iteration 46, loss = 0.03403520
Iteration 47, loss = 0.03285949
Iteration 48, loss = 0.03179229
Iteration 49, loss = 0.03073671
Iteration 50, loss = 0.02983469
Iteration 51, loss = 0.02889127
Iteration 52, loss = 0.02801002
Iteration 53, loss = 0.02717634
Iteration 54, loss = 0.02643762
Iteration 55, loss = 0.02575513
Iteration 56, loss = 0.02506164
Iteration 57, loss = 0.02441623
Iteration 58, loss = 0.02382433
Iteration 59, loss = 0.02320667
Iteration 60, loss = 0.02270412
Iteration 61, loss = 0.02215944
Iteration 62, loss = 0.02170860
Iteration 63, loss = 0.02122205
Iteration 64, loss = 0.02080358
Iteration 65, loss = 0.02036684
Iteration 66, loss = 0.01996140
Iteration 67, loss = 0.01963996
Iteration 68, loss = 0.01918043
Iteration 69, loss = 0.01883149
Iteration 70, loss = 0.01851956
Iteration 71, loss = 0.01818538
Iteration 72, loss = 0.01791307
Iteration 73, loss = 0.01755400
Iteration 74, loss = 0.01725340
Iteration 75, loss = 0.01701322
Iteration 76, loss = 0.01676068
Iteration 77, loss = 0.01652054
Iteration 78, loss = 0.01623637
Iteration 79, loss = 0.01599956
Iteration 80, loss = 0.01578212
Iteration 81, loss = 0.01556361
Iteration 82, loss = 0.01534412
Iteration 83, loss = 0.01512799
Iteration 84, loss = 0.01497612
Iteration 85, loss = 0.01475961
Iteration 86, loss = 0.01458991
Iteration 87, loss = 0.01442822
Iteration 88, loss = 0.01424655
Iteration 89, loss = 0.01411933
Iteration 90, loss = 0.01397413
Iteration 91, loss = 0.01381649
Iteration 92, loss = 0.01363277
Iteration 93, loss = 0.01348215
Iteration 94, loss = 0.01333764
Iteration 95, loss = 0.01319552
Iteration 96, loss = 0.01311017
Iteration 97, loss = 0.01293600
Iteration 98, loss = 0.01285504
Iteration 99, loss = 0.01275520
Iteration 100, loss = 0.01257885
Iteration 101, loss = 0.01247385
Iteration 102, loss = 0.01239073
Iteration 103, loss = 0.01224533
Iteration 104, loss = 0.01216782
Iteration 105, loss = 0.01204348
Iteration 106, loss = 0.01194786
Iteration 107, loss = 0.01185828
Iteration 108, loss = 0.01176912
Iteration 109, loss = 0.01172026
Iteration 110, loss = 0.01160130
Iteration 111, loss = 0.01153636
Iteration 112, loss = 0.01140894
Iteration 113, loss = 0.01134141
Iteration 114, loss = 0.01126800
Iteration 115, loss = 0.01119723
Iteration 116, loss = 0.01111151
Iteration 117, loss = 0.01102298
Iteration 118, loss = 0.01097149
Iteration 119, loss = 0.01093221
Iteration 120, loss = 0.01086069
Iteration 121, loss = 0.01081325
Iteration 122, loss = 0.01072766
Iteration 123, loss = 0.01063657
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68998424
Iteration 2, loss = 0.62963046
Iteration 3, loss = 0.54549090
Iteration 4, loss = 0.44360281
Iteration 5, loss = 0.34872543
Iteration 6, loss = 0.27006947
Iteration 7, loss = 0.20967216
Iteration 8, loss = 0.16426913
Iteration 9, loss = 0.13156961
Iteration 10, loss = 0.10767844
Iteration 11, loss = 0.08987476
Iteration 12, loss = 0.07709756
Iteration 13, loss = 0.06745889
Iteration 14, loss = 0.05940943
Iteration 15, loss = 0.05321811
Iteration 16, loss = 0.04821184
Iteration 17, loss = 0.04506017
Iteration 18, loss = 0.04147305
Iteration 19, loss = 0.03864096
Iteration 20, loss = 0.03609280
Iteration 21, loss = 0.03398992
Iteration 22, loss = 0.03216382
Iteration 23, loss = 0.03070149
Iteration 24, loss = 0.02924708
Iteration 25, loss = 0.02796883
Iteration 26, loss = 0.02683447
Iteration 27, loss = 0.02586607
Iteration 28, loss = 0.02505357
Iteration 29, loss = 0.02427693
Iteration 30, loss = 0.02353796
Iteration 31, loss = 0.02280843
Iteration 32, loss = 0.02240168
Iteration 33, loss = 0.02171858
Iteration 34, loss = 0.02128044
Iteration 35, loss = 0.02094261
Iteration 36, loss = 0.02039526
Iteration 37, loss = 0.02015426
Iteration 38, loss = 0.01982918
Iteration 39, loss = 0.01966263
Iteration 40, loss = 0.01914250
Iteration 41, loss = 0.01894570
Iteration 42, loss = 0.01864835
Iteration 43, loss = 0.01840107
Iteration 44, loss = 0.01817744
Iteration 45, loss = 0.01802632
Iteration 46, loss = 0.01773853
Iteration 47, loss = 0.01759781
Iteration 48, loss = 0.01743683
Iteration 49, loss = 0.01746569
Iteration 50, loss = 0.01722455
Iteration 51, loss = 0.01694789
Iteration 52, loss = 0.01685748
Iteration 53, loss = 0.01669974
Iteration 54, loss = 0.01664086
Iteration 55, loss = 0.01726689
Iteration 56, loss = 0.01797244
Iteration 57, loss = 0.01747650
Iteration 58, loss = 0.01717390
Iteration 59, loss = 0.01684397
Iteration 60, loss = 0.01682472
Iteration 61, loss = 0.01657292
Iteration 62, loss = 0.01649357
Iteration 63, loss = 0.01627473
Iteration 64, loss = 0.01633721
Iteration 65, loss = 0.01623239
Iteration 66, loss = 0.01597687
Iteration 67, loss = 0.01586548
Iteration 68, loss = 0.01583165
Iteration 69, loss = 0.01567657
Iteration 70, loss = 0.01564347
Iteration 71, loss = 0.01541163
Iteration 72, loss = 0.01541637
Iteration 73, loss = 0.01548066
Iteration 74, loss = 0.01522189
Iteration 75, loss = 0.01533795
Iteration 76, loss = 0.01528417
Iteration 77, loss = 0.01510411
Iteration 78, loss = 0.01512594
Iteration 79, loss = 0.01501214
Iteration 80, loss = 0.01492601
Iteration 81, loss = 0.01508375
Iteration 82, loss = 0.01485652
Iteration 83, loss = 0.01486839
Iteration 84, loss = 0.01463142
Iteration 85, loss = 0.01491893
Iteration 86, loss = 0.01461033
Iteration 87, loss = 0.01455530
Iteration 88, loss = 0.01523156
Iteration 89, loss = 0.01650215
Iteration 90, loss = 0.01600158
Iteration 91, loss = 0.01570112
Iteration 92, loss = 0.01548426
Iteration 93, loss = 0.01533644
Iteration 94, loss = 0.01546977
Iteration 95, loss = 0.01523651
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69843502
Iteration 2, loss = 0.61674635
Iteration 3, loss = 0.50380133
Iteration 4, loss = 0.38214339
Iteration 5, loss = 0.28017540
Iteration 6, loss = 0.20535361
Iteration 7, loss = 0.15338380
Iteration 8, loss = 0.11892754
Iteration 9, loss = 0.09536925
Iteration 10, loss = 0.07886651
Iteration 11, loss = 0.06697658
Iteration 12, loss = 0.05831668
Iteration 13, loss = 0.05175275
Iteration 14, loss = 0.04651043
Iteration 15, loss = 0.04241911
Iteration 16, loss = 0.03917813
Iteration 17, loss = 0.03657508
Iteration 18, loss = 0.03430213
Iteration 19, loss = 0.03250445
Iteration 20, loss = 0.03077664
Iteration 21, loss = 0.02954678
Iteration 22, loss = 0.02849191
Iteration 23, loss = 0.02733692
Iteration 24, loss = 0.02656254
Iteration 25, loss = 0.02581830
Iteration 26, loss = 0.02518860
Iteration 27, loss = 0.02475777
Iteration 28, loss = 0.02406435
Iteration 29, loss = 0.02345144
Iteration 30, loss = 0.02306927
Iteration 31, loss = 0.02269582
Iteration 32, loss = 0.02236538
Iteration 33, loss = 0.02214010
Iteration 34, loss = 0.02186131
Iteration 35, loss = 0.02167728
Iteration 36, loss = 0.02135426
Iteration 37, loss = 0.02112823
Iteration 38, loss = 0.02105126
Iteration 39, loss = 0.02081598
Iteration 40, loss = 0.02075208
Iteration 41, loss = 0.02047123
Iteration 42, loss = 0.02024670
Iteration 43, loss = 0.02006602
Iteration 44, loss = 0.02010068
Iteration 45, loss = 0.01994447
Iteration 46, loss = 0.01998012
Iteration 47, loss = 0.01981812
Iteration 48, loss = 0.01971156
Iteration 49, loss = 0.01979821
Iteration 50, loss = 0.01954498
Iteration 51, loss = 0.01968552
Iteration 52, loss = 0.01924640
Iteration 53, loss = 0.01924887
Iteration 54, loss = 0.01922290
Iteration 55, loss = 0.01919884
Iteration 56, loss = 0.01905987
Iteration 57, loss = 0.01908109
Iteration 58, loss = 0.01919870
Iteration 59, loss = 0.01902499
Iteration 60, loss = 0.01884617
Iteration 61, loss = 0.01887089
Iteration 62, loss = 0.01874459
Iteration 63, loss = 0.01861625
Iteration 64, loss = 0.01862184
Iteration 65, loss = 0.01847659
Iteration 66, loss = 0.01885271
Iteration 67, loss = 0.01856077
Iteration 68, loss = 0.01879376
Iteration 69, loss = 0.01835608
Iteration 70, loss = 0.01861735
Iteration 71, loss = 0.01845369
Iteration 72, loss = 0.01831318
Iteration 73, loss = 0.01861042
Iteration 74, loss = 0.01839400
Iteration 75, loss = 0.01838469
Iteration 76, loss = 0.01827190
Iteration 77, loss = 0.01844346
Iteration 78, loss = 0.01853445
Iteration 79, loss = 0.01820820
Iteration 80, loss = 0.01824242
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68124008
Iteration 2, loss = 0.63661355
Iteration 3, loss = 0.55520120
Iteration 4, loss = 0.45387927
Iteration 5, loss = 0.35482175
Iteration 6, loss = 0.27073005
Iteration 7, loss = 0.20606010
Iteration 8, loss = 0.15968356
Iteration 9, loss = 0.12688590
Iteration 10, loss = 0.10347432
Iteration 11, loss = 0.08662743
Iteration 12, loss = 0.07421329
Iteration 13, loss = 0.06522787
Iteration 14, loss = 0.05791492
Iteration 15, loss = 0.05231176
Iteration 16, loss = 0.04748209
Iteration 17, loss = 0.04398793
Iteration 18, loss = 0.04103727
Iteration 19, loss = 0.03823644
Iteration 20, loss = 0.03629333
Iteration 21, loss = 0.03438029
Iteration 22, loss = 0.03263100
Iteration 23, loss = 0.03130057
Iteration 24, loss = 0.03029644
Iteration 25, loss = 0.02905954
Iteration 26, loss = 0.02814868
Iteration 27, loss = 0.02743593
Iteration 28, loss = 0.02663188
Iteration 29, loss = 0.02598238
Iteration 30, loss = 0.02525420
Iteration 31, loss = 0.02480452
Iteration 32, loss = 0.02452584
Iteration 33, loss = 0.02412243
Iteration 34, loss = 0.02356360
Iteration 35, loss = 0.02329712
Iteration 36, loss = 0.02273550
Iteration 37, loss = 0.02259465
Iteration 38, loss = 0.02235474
Iteration 39, loss = 0.02210078
Iteration 40, loss = 0.02162060
Iteration 41, loss = 0.02149147
Iteration 42, loss = 0.02122808
Iteration 43, loss = 0.02096698
Iteration 44, loss = 0.02089445
Iteration 45, loss = 0.02066318
Iteration 46, loss = 0.02070891
Iteration 47, loss = 0.02145156
Iteration 48, loss = 0.02191998
Iteration 49, loss = 0.02142659
Iteration 50, loss = 0.02091480
Iteration 51, loss = 0.02063750
Iteration 52, loss = 0.02058900
Iteration 53, loss = 0.02028028
Iteration 54, loss = 0.02028366
Iteration 55, loss = 0.02088129
Iteration 56, loss = 0.02129904
Iteration 57, loss = 0.02081645
Iteration 58, loss = 0.02050362
Iteration 59, loss = 0.02028597
Iteration 60, loss = 0.02017131
Iteration 61, loss = 0.02005010
Iteration 62, loss = 0.01986897
Iteration 63, loss = 0.02003452
Iteration 64, loss = 0.01992579
Iteration 65, loss = 0.01964079
Iteration 66, loss = 0.01976965
Iteration 67, loss = 0.01939299
Iteration 68, loss = 0.01911308
Iteration 69, loss = 0.01908647
Iteration 70, loss = 0.01925212
Iteration 71, loss = 0.01896589
Iteration 72, loss = 0.01915237
Iteration 73, loss = 0.01870900
Iteration 74, loss = 0.01881880
Iteration 75, loss = 0.01857264
Iteration 76, loss = 0.01855435
Iteration 77, loss = 0.01838811
Iteration 78, loss = 0.01837727
Iteration 79, loss = 0.01838563
Iteration 80, loss = 0.01824538
Iteration 81, loss = 0.01820155
Iteration 82, loss = 0.01812545
Iteration 83, loss = 0.01791232
Iteration 84, loss = 0.01796861
Iteration 85, loss = 0.01793064
Iteration 86, loss = 0.01819558
Iteration 87, loss = 0.01765516
Iteration 88, loss = 0.01765210
Iteration 89, loss = 0.01764428
Iteration 90, loss = 0.01793199
Iteration 91, loss = 0.01771757
Iteration 92, loss = 0.01768874
Iteration 93, loss = 0.01756269
Iteration 94, loss = 0.01816619
Iteration 95, loss = 0.01809612
Iteration 96, loss = 0.01760453
Iteration 97, loss = 0.01753992
Iteration 98, loss = 0.01728473
Iteration 99, loss = 0.01766942
Iteration 100, loss = 0.01743471
Iteration 101, loss = 0.01733832
Iteration 102, loss = 0.01715478
Iteration 103, loss = 0.01717213
Iteration 104, loss = 0.01715962
Iteration 105, loss = 0.01709366
Iteration 106, loss = 0.01715390
Iteration 107, loss = 0.01714324
Iteration 108, loss = 0.01720708
Iteration 109, loss = 0.01701863
Iteration 110, loss = 0.01717055
Iteration 111, loss = 0.01709893
Iteration 112, loss = 0.01694755
Iteration 113, loss = 0.01711359
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67738786
Iteration 2, loss = 0.65769212
Iteration 3, loss = 0.63852204
Iteration 4, loss = 0.61626102
Iteration 5, loss = 0.58927057
Iteration 6, loss = 0.55785471
Iteration 7, loss = 0.52378941
Iteration 8, loss = 0.48823170
Iteration 9, loss = 0.45232574
Iteration 10, loss = 0.41651712
Iteration 11, loss = 0.38208051
Iteration 12, loss = 0.34880255
Iteration 13, loss = 0.31779033
Iteration 14, loss = 0.28876419
Iteration 15, loss = 0.26209188
Iteration 16, loss = 0.23773556
Iteration 17, loss = 0.21573835
Iteration 18, loss = 0.19586953
Iteration 19, loss = 0.17808592
Iteration 20, loss = 0.16223675
Iteration 21, loss = 0.14809690
Iteration 22, loss = 0.13557990
Iteration 23, loss = 0.12435825
Iteration 24, loss = 0.11448671
Iteration 25, loss = 0.10567274
Iteration 26, loss = 0.09780994
Iteration 27, loss = 0.09085372
Iteration 28, loss = 0.08460837
Iteration 29, loss = 0.07899552
Iteration 30, loss = 0.07399313
Iteration 31, loss = 0.06950120
Iteration 32, loss = 0.06536752
Iteration 33, loss = 0.06166483
Iteration 34, loss = 0.05834447
Iteration 35, loss = 0.05525634
Iteration 36, loss = 0.05246096
Iteration 37, loss = 0.05000677
Iteration 38, loss = 0.04762324
Iteration 39, loss = 0.04547328
Iteration 40, loss = 0.04352112
Iteration 41, loss = 0.04166018
Iteration 42, loss = 0.04003364
Iteration 43, loss = 0.03842557
Iteration 44, loss = 0.03696013
Iteration 45, loss = 0.03567502
Iteration 46, loss = 0.03441255
Iteration 47, loss = 0.03321752
Iteration 48, loss = 0.03215249
Iteration 49, loss = 0.03115259
Iteration 50, loss = 0.03021766
Iteration 51, loss = 0.02927385
Iteration 52, loss = 0.02842336
Iteration 53, loss = 0.02765316
Iteration 54, loss = 0.02688320
Iteration 55, loss = 0.02619784
Iteration 56, loss = 0.02552301
Iteration 57, loss = 0.02487448
Iteration 58, loss = 0.02425494
Iteration 59, loss = 0.02369492
Iteration 60, loss = 0.02317581
Iteration 61, loss = 0.02269289
Iteration 62, loss = 0.02218624
Iteration 63, loss = 0.02176494
Iteration 64, loss = 0.02133871
Iteration 65, loss = 0.02093723
Iteration 66, loss = 0.02046312
Iteration 67, loss = 0.02010998
Iteration 68, loss = 0.01972643
Iteration 69, loss = 0.01938443
Iteration 70, loss = 0.01905801
Iteration 71, loss = 0.01874484
Iteration 72, loss = 0.01845998
Iteration 73, loss = 0.01814926
Iteration 74, loss = 0.01789588
Iteration 75, loss = 0.01762323
Iteration 76, loss = 0.01734440
Iteration 77, loss = 0.01710972
Iteration 78, loss = 0.01690642
Iteration 79, loss = 0.01665253
Iteration 80, loss = 0.01642653
Iteration 81, loss = 0.01621920
Iteration 82, loss = 0.01604936
Iteration 83, loss = 0.01586833
Iteration 84, loss = 0.01566093
Iteration 85, loss = 0.01546254
Iteration 86, loss = 0.01526526
Iteration 87, loss = 0.01512595
Iteration 88, loss = 0.01494112
Iteration 89, loss = 0.01477083
Iteration 90, loss = 0.01466677
Iteration 91, loss = 0.01449379
Iteration 92, loss = 0.01439119
Iteration 93, loss = 0.01419746
Iteration 94, loss = 0.01405967
Iteration 95, loss = 0.01398088
Iteration 96, loss = 0.01381365
Iteration 97, loss = 0.01372396
Iteration 98, loss = 0.01360543
Iteration 99, loss = 0.01344815
Iteration 100, loss = 0.01337198
Iteration 101, loss = 0.01324523
Iteration 102, loss = 0.01315825
Iteration 103, loss = 0.01306466
Iteration 104, loss = 0.01291725
Iteration 105, loss = 0.01282319
Iteration 106, loss = 0.01277338
Iteration 107, loss = 0.01266163
Iteration 108, loss = 0.01254702
Iteration 109, loss = 0.01248894
Iteration 110, loss = 0.01240998
Iteration 111, loss = 0.01231061
Iteration 112, loss = 0.01225510
Iteration 113, loss = 0.01215292
Iteration 114, loss = 0.01210432
Iteration 115, loss = 0.01205836
Iteration 116, loss = 0.01195359
Iteration 117, loss = 0.01185467
Iteration 118, loss = 0.01180932
Iteration 119, loss = 0.01176729
Iteration 120, loss = 0.01172270
Iteration 121, loss = 0.01161411
Iteration 122, loss = 0.01159064
Iteration 123, loss = 0.01155478
Iteration 124, loss = 0.01143314
Iteration 125, loss = 0.01136661
Iteration 126, loss = 0.01131603
Iteration 127, loss = 0.01127810
Iteration 128, loss = 0.01123394
Iteration 129, loss = 0.01119721
Iteration 130, loss = 0.01109927
Iteration 131, loss = 0.01109735
Iteration 132, loss = 0.01102340
Iteration 133, loss = 0.01097791
Iteration 134, loss = 0.01091883
Iteration 135, loss = 0.01086369
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68485472
Iteration 2, loss = 0.64783029
Iteration 3, loss = 0.59948890
Iteration 4, loss = 0.53991680
Iteration 5, loss = 0.47599806
Iteration 6, loss = 0.41379857
Iteration 7, loss = 0.35593456
Iteration 8, loss = 0.30373550
Iteration 9, loss = 0.25766272
Iteration 10, loss = 0.21836036
Iteration 11, loss = 0.18530939
Iteration 12, loss = 0.15786825
Iteration 13, loss = 0.13544439
Iteration 14, loss = 0.11697818
Iteration 15, loss = 0.10185183
Iteration 16, loss = 0.08949554
Iteration 17, loss = 0.07930670
Iteration 18, loss = 0.07089962
Iteration 19, loss = 0.06378931
Iteration 20, loss = 0.05789791
Iteration 21, loss = 0.05275150
Iteration 22, loss = 0.04853685
Iteration 23, loss = 0.04489636
Iteration 24, loss = 0.04154101
Iteration 25, loss = 0.03872986
Iteration 26, loss = 0.03621187
Iteration 27, loss = 0.03409667
Iteration 28, loss = 0.03219528
Iteration 29, loss = 0.03038514
Iteration 30, loss = 0.02898748
Iteration 31, loss = 0.02750426
Iteration 32, loss = 0.02625080
Iteration 33, loss = 0.02521253
Iteration 34, loss = 0.02422479
Iteration 35, loss = 0.02330066
Iteration 36, loss = 0.02258257
Iteration 37, loss = 0.02167475
Iteration 38, loss = 0.02092259
Iteration 39, loss = 0.02032110
Iteration 40, loss = 0.01973784
Iteration 41, loss = 0.01911824
Iteration 42, loss = 0.01868509
Iteration 43, loss = 0.01814624
Iteration 44, loss = 0.01769278
Iteration 45, loss = 0.01728610
Iteration 46, loss = 0.01700846
Iteration 47, loss = 0.01656572
Iteration 48, loss = 0.01620133
Iteration 49, loss = 0.01593924
Iteration 50, loss = 0.01562054
Iteration 51, loss = 0.01533352
Iteration 52, loss = 0.01510027
Iteration 53, loss = 0.01482466
Iteration 54, loss = 0.01474491
Iteration 55, loss = 0.01443520
Iteration 56, loss = 0.01424018
Iteration 57, loss = 0.01402755
Iteration 58, loss = 0.01381982
Iteration 59, loss = 0.01364259
Iteration 60, loss = 0.01343389
Iteration 61, loss = 0.01333391
Iteration 62, loss = 0.01312908
Iteration 63, loss = 0.01296904
Iteration 64, loss = 0.01291647
Iteration 65, loss = 0.01275021
Iteration 66, loss = 0.01274779
Iteration 67, loss = 0.01252934
Iteration 68, loss = 0.01240895
Iteration 69, loss = 0.01232016
Iteration 70, loss = 0.01228291
Iteration 71, loss = 0.01213736
Iteration 72, loss = 0.01199879
Iteration 73, loss = 0.01191912
Iteration 74, loss = 0.01186582
Iteration 75, loss = 0.01171031
Iteration 76, loss = 0.01164683
Iteration 77, loss = 0.01157230
Iteration 78, loss = 0.01149745
Iteration 79, loss = 0.01151551
Iteration 80, loss = 0.01141042
Iteration 81, loss = 0.01132286
Iteration 82, loss = 0.01127735
Iteration 83, loss = 0.01121704
Iteration 84, loss = 0.01116894
Iteration 85, loss = 0.01112064
Iteration 86, loss = 0.01102040
Iteration 87, loss = 0.01102592
Iteration 88, loss = 0.01094678
Iteration 89, loss = 0.01083022
Iteration 90, loss = 0.01096410
Iteration 91, loss = 0.01078421
Iteration 92, loss = 0.01077511
Iteration 93, loss = 0.01071431
Iteration 94, loss = 0.01069723
Iteration 95, loss = 0.01063809
Iteration 96, loss = 0.01058910
Iteration 97, loss = 0.01055796
Iteration 98, loss = 0.01060547
Iteration 99, loss = 0.01070600
Iteration 100, loss = 0.01046121
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68077301
Iteration 2, loss = 0.63715723
Iteration 3, loss = 0.56582779
Iteration 4, loss = 0.47516323
Iteration 5, loss = 0.38502631
Iteration 6, loss = 0.30407269
Iteration 7, loss = 0.23873833
Iteration 8, loss = 0.18800285
Iteration 9, loss = 0.15034094
Iteration 10, loss = 0.12244952
Iteration 11, loss = 0.10175635
Iteration 12, loss = 0.08599534
Iteration 13, loss = 0.07410402
Iteration 14, loss = 0.06474532
Iteration 15, loss = 0.05741819
Iteration 16, loss = 0.05134910
Iteration 17, loss = 0.04659775
Iteration 18, loss = 0.04285021
Iteration 19, loss = 0.03920659
Iteration 20, loss = 0.03636464
Iteration 21, loss = 0.03401814
Iteration 22, loss = 0.03204583
Iteration 23, loss = 0.03020504
Iteration 24, loss = 0.02855840
Iteration 25, loss = 0.02727592
Iteration 26, loss = 0.02613735
Iteration 27, loss = 0.02523728
Iteration 28, loss = 0.02414235
Iteration 29, loss = 0.02316602
Iteration 30, loss = 0.02244819
Iteration 31, loss = 0.02177141
Iteration 32, loss = 0.02117752
Iteration 33, loss = 0.02066302
Iteration 34, loss = 0.02027151
Iteration 35, loss = 0.01984688
Iteration 36, loss = 0.01920034
Iteration 37, loss = 0.01883289
Iteration 38, loss = 0.01860043
Iteration 39, loss = 0.01814216
Iteration 40, loss = 0.01806947
Iteration 41, loss = 0.01769907
Iteration 42, loss = 0.01732278
Iteration 43, loss = 0.01710214
Iteration 44, loss = 0.01686736
Iteration 45, loss = 0.01670785
Iteration 46, loss = 0.01638917
Iteration 47, loss = 0.01620497
Iteration 48, loss = 0.01597117
Iteration 49, loss = 0.01598730
Iteration 50, loss = 0.01572402
Iteration 51, loss = 0.01552080
Iteration 52, loss = 0.01555544
Iteration 53, loss = 0.01552634
Iteration 54, loss = 0.01518790
Iteration 55, loss = 0.01505185
Iteration 56, loss = 0.01505285
Iteration 57, loss = 0.01492946
Iteration 58, loss = 0.01489466
Iteration 59, loss = 0.01463852
Iteration 60, loss = 0.01470909
Iteration 61, loss = 0.01468064
Iteration 62, loss = 0.01447249
Iteration 63, loss = 0.01446014
Iteration 64, loss = 0.01435864
Iteration 65, loss = 0.01420282
Iteration 66, loss = 0.01431341
Iteration 67, loss = 0.01410456
Iteration 68, loss = 0.01413697
Iteration 69, loss = 0.01397370
Iteration 70, loss = 0.01390744
Iteration 71, loss = 0.01372714
Iteration 72, loss = 0.01378516
Iteration 73, loss = 0.01377538
Iteration 74, loss = 0.01373373
Iteration 75, loss = 0.01376301
Iteration 76, loss = 0.01358376
Iteration 77, loss = 0.01372564
Iteration 78, loss = 0.01352574
Iteration 79, loss = 0.01351637
Iteration 80, loss = 0.01347638
Iteration 81, loss = 0.01347374
Iteration 82, loss = 0.01345771
Iteration 83, loss = 0.01321991
Iteration 84, loss = 0.01333519
Iteration 85, loss = 0.01337923
Iteration 86, loss = 0.01339737
Iteration 87, loss = 0.01332627
Iteration 88, loss = 0.01321911
Iteration 89, loss = 0.01311795
Iteration 90, loss = 0.01313953
Iteration 91, loss = 0.01314952
Iteration 92, loss = 0.01318954
Iteration 93, loss = 0.01324970
Iteration 94, loss = 0.01319136
Iteration 95, loss = 0.01296122
Iteration 96, loss = 0.01300693
Iteration 97, loss = 0.01309058
Iteration 98, loss = 0.01293335
Iteration 99, loss = 0.01302151
Iteration 100, loss = 0.01298993
Iteration 101, loss = 0.01288407
Iteration 102, loss = 0.01285642
Iteration 103, loss = 0.01282533
Iteration 104, loss = 0.01274854
Iteration 105, loss = 0.01283457
Iteration 106, loss = 0.01274263
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67705794
Iteration 2, loss = 0.61787906
Iteration 3, loss = 0.52818272
Iteration 4, loss = 0.42433033
Iteration 5, loss = 0.32959198
Iteration 6, loss = 0.25319334
Iteration 7, loss = 0.19574111
Iteration 8, loss = 0.15378429
Iteration 9, loss = 0.12362107
Iteration 10, loss = 0.10214933
Iteration 11, loss = 0.08603714
Iteration 12, loss = 0.07400011
Iteration 13, loss = 0.06495044
Iteration 14, loss = 0.05761281
Iteration 15, loss = 0.05201943
Iteration 16, loss = 0.04745470
Iteration 17, loss = 0.04366858
Iteration 18, loss = 0.04082575
Iteration 19, loss = 0.03799712
Iteration 20, loss = 0.03582474
Iteration 21, loss = 0.03375059
Iteration 22, loss = 0.03227682
Iteration 23, loss = 0.03076808
Iteration 24, loss = 0.02942451
Iteration 25, loss = 0.02858078
Iteration 26, loss = 0.02756197
Iteration 27, loss = 0.02695794
Iteration 28, loss = 0.02701616
Iteration 29, loss = 0.02619953
Iteration 30, loss = 0.02534010
Iteration 31, loss = 0.02479563
Iteration 32, loss = 0.02419687
Iteration 33, loss = 0.02355357
Iteration 34, loss = 0.02315229
Iteration 35, loss = 0.02275265
Iteration 36, loss = 0.02222947
Iteration 37, loss = 0.02176543
Iteration 38, loss = 0.02149218
Iteration 39, loss = 0.02119516
Iteration 40, loss = 0.02092052
Iteration 41, loss = 0.02051621
Iteration 42, loss = 0.02081469
Iteration 43, loss = 0.02245092
Iteration 44, loss = 0.02182616
Iteration 45, loss = 0.02095619
Iteration 46, loss = 0.02086869
Iteration 47, loss = 0.02049079
Iteration 48, loss = 0.02010879
Iteration 49, loss = 0.01982638
Iteration 50, loss = 0.01958416
Iteration 51, loss = 0.01931833
Iteration 52, loss = 0.01907398
Iteration 53, loss = 0.01904880
Iteration 54, loss = 0.01868759
Iteration 55, loss = 0.01883822
Iteration 56, loss = 0.01849452
Iteration 57, loss = 0.01840212
Iteration 58, loss = 0.01838005
Iteration 59, loss = 0.01810869
Iteration 60, loss = 0.01804597
Iteration 61, loss = 0.01792314
Iteration 62, loss = 0.01804863
Iteration 63, loss = 0.01829713
Iteration 64, loss = 0.01829335
Iteration 65, loss = 0.01830400
Iteration 66, loss = 0.02014845
Iteration 67, loss = 0.02034432
Iteration 68, loss = 0.01980374
Iteration 69, loss = 0.01946200
Iteration 70, loss = 0.01894271
Iteration 71, loss = 0.01873511
Iteration 72, loss = 0.01846706
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67776461
Iteration 2, loss = 0.63548433
Iteration 3, loss = 0.57917761
Iteration 4, loss = 0.50929408
Iteration 5, loss = 0.43490951
Iteration 6, loss = 0.36376368
Iteration 7, loss = 0.30039737
Iteration 8, loss = 0.24668688
Iteration 9, loss = 0.20264175
Iteration 10, loss = 0.16737576
Iteration 11, loss = 0.13974396
Iteration 12, loss = 0.11812087
Iteration 13, loss = 0.10094741
Iteration 14, loss = 0.08752796
Iteration 15, loss = 0.07686458
Iteration 16, loss = 0.06808615
Iteration 17, loss = 0.06092365
Iteration 18, loss = 0.05517805
Iteration 19, loss = 0.05009297
Iteration 20, loss = 0.04605614
Iteration 21, loss = 0.04255349
Iteration 22, loss = 0.03963204
Iteration 23, loss = 0.03701193
Iteration 24, loss = 0.03482298
Iteration 25, loss = 0.03279440
Iteration 26, loss = 0.03107921
Iteration 27, loss = 0.02978867
Iteration 28, loss = 0.02830199
Iteration 29, loss = 0.02714456
Iteration 30, loss = 0.02612299
Iteration 31, loss = 0.02505440
Iteration 32, loss = 0.02423256
Iteration 33, loss = 0.02344043
Iteration 34, loss = 0.02270766
Iteration 35, loss = 0.02210743
Iteration 36, loss = 0.02152234
Iteration 37, loss = 0.02098959
Iteration 38, loss = 0.02068601
Iteration 39, loss = 0.02006051
Iteration 40, loss = 0.01964184
Iteration 41, loss = 0.01922687
Iteration 42, loss = 0.01886456
Iteration 43, loss = 0.01846952
Iteration 44, loss = 0.01819918
Iteration 45, loss = 0.01789969
Iteration 46, loss = 0.01791509
Iteration 47, loss = 0.01737672
Iteration 48, loss = 0.01722425
Iteration 49, loss = 0.01691177
Iteration 50, loss = 0.01682522
Iteration 51, loss = 0.01651066
Iteration 52, loss = 0.01640332
Iteration 53, loss = 0.01629364
Iteration 54, loss = 0.01618529
Iteration 55, loss = 0.01595551
Iteration 56, loss = 0.01570966
Iteration 57, loss = 0.01556346
Iteration 58, loss = 0.01549914
Iteration 59, loss = 0.01526415
Iteration 60, loss = 0.01529626
Iteration 61, loss = 0.01512180
Iteration 62, loss = 0.01498890
Iteration 63, loss = 0.01497901
Iteration 64, loss = 0.01478172
Iteration 65, loss = 0.01475730
Iteration 66, loss = 0.01457300
Iteration 67, loss = 0.01466769
Iteration 68, loss = 0.01449423
Iteration 69, loss = 0.01437123
Iteration 70, loss = 0.01433421
Iteration 71, loss = 0.01418491
Iteration 72, loss = 0.01436320
Iteration 73, loss = 0.01409736
Iteration 74, loss = 0.01404927
Iteration 75, loss = 0.01404587
Iteration 76, loss = 0.01408278
Iteration 77, loss = 0.01388056
Iteration 78, loss = 0.01383155
Iteration 79, loss = 0.01382294
Iteration 80, loss = 0.01380896
Iteration 81, loss = 0.01368413
Iteration 82, loss = 0.01359932
Iteration 83, loss = 0.01359716
Iteration 84, loss = 0.01364202
Iteration 85, loss = 0.01357416
Iteration 86, loss = 0.01349243
Iteration 87, loss = 0.01343615
Iteration 88, loss = 0.01336299
Iteration 89, loss = 0.01357767
Iteration 90, loss = 0.01328493
Iteration 91, loss = 0.01341797
Iteration 92, loss = 0.01338454
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68629270
Iteration 2, loss = 0.62717890
Iteration 3, loss = 0.53037734
Iteration 4, loss = 0.41887354
Iteration 5, loss = 0.31828742
Iteration 6, loss = 0.23911147
Iteration 7, loss = 0.18185637
Iteration 8, loss = 0.14180703
Iteration 9, loss = 0.11354164
Iteration 10, loss = 0.09324840
Iteration 11, loss = 0.07841928
Iteration 12, loss = 0.06735339
Iteration 13, loss = 0.05892359
Iteration 14, loss = 0.05255944
Iteration 15, loss = 0.04715838
Iteration 16, loss = 0.04286083
Iteration 17, loss = 0.03935464
Iteration 18, loss = 0.03661081
Iteration 19, loss = 0.03412981
Iteration 20, loss = 0.03217542
Iteration 21, loss = 0.03063931
Iteration 22, loss = 0.02869351
Iteration 23, loss = 0.02752712
Iteration 24, loss = 0.02634839
Iteration 25, loss = 0.02545965
Iteration 26, loss = 0.02429224
Iteration 27, loss = 0.02369857
Iteration 28, loss = 0.02286006
Iteration 29, loss = 0.02247925
Iteration 30, loss = 0.02186402
Iteration 31, loss = 0.02116213
Iteration 32, loss = 0.02083128
Iteration 33, loss = 0.02053815
Iteration 34, loss = 0.02014415
Iteration 35, loss = 0.01980183
Iteration 36, loss = 0.01960713
Iteration 37, loss = 0.01924733
Iteration 38, loss = 0.01883598
Iteration 39, loss = 0.01823835
Iteration 40, loss = 0.01816474
Iteration 41, loss = 0.01774516
Iteration 42, loss = 0.01747381
Iteration 43, loss = 0.01728203
Iteration 44, loss = 0.01719720
Iteration 45, loss = 0.01705653
Iteration 46, loss = 0.01683513
Iteration 47, loss = 0.01667728
Iteration 48, loss = 0.01660765
Iteration 49, loss = 0.01646410
Iteration 50, loss = 0.01628509
Iteration 51, loss = 0.01625657
Iteration 52, loss = 0.01623919
Iteration 53, loss = 0.01588004
Iteration 54, loss = 0.01713429
Iteration 55, loss = 0.01708024
Iteration 56, loss = 0.01651097
Iteration 57, loss = 0.01632959
Iteration 58, loss = 0.01617383
Iteration 59, loss = 0.01619143
Iteration 60, loss = 0.01585884
Iteration 61, loss = 0.01571358
Iteration 62, loss = 0.01564189
Iteration 63, loss = 0.01560851
Iteration 64, loss = 0.01547448
Iteration 65, loss = 0.01548113
Iteration 66, loss = 0.01532451
Iteration 67, loss = 0.01533540
Iteration 68, loss = 0.01540881
Iteration 69, loss = 0.01508245
Iteration 70, loss = 0.01504700
Iteration 71, loss = 0.01497572
Iteration 72, loss = 0.01502501
Iteration 73, loss = 0.01499669
Iteration 74, loss = 0.01484614
Iteration 75, loss = 0.01485959
Iteration 76, loss = 0.01478365
Iteration 77, loss = 0.01538380
Iteration 78, loss = 0.01630174
Iteration 79, loss = 0.01605011
Iteration 80, loss = 0.01557798
Iteration 81, loss = 0.01564852
Iteration 82, loss = 0.01557112
Iteration 83, loss = 0.01560925
Iteration 84, loss = 0.01558462
Iteration 85, loss = 0.01527702
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71476383
Iteration 2, loss = 0.69235385
Iteration 3, loss = 0.67111826
Iteration 4, loss = 0.64651966
Iteration 5, loss = 0.61749805
Iteration 6, loss = 0.58472575
Iteration 7, loss = 0.54895137
Iteration 8, loss = 0.51072839
Iteration 9, loss = 0.47204068
Iteration 10, loss = 0.43363906
Iteration 11, loss = 0.39627918
Iteration 12, loss = 0.36039983
Iteration 13, loss = 0.32683398
Iteration 14, loss = 0.29567824
Iteration 15, loss = 0.26709684
Iteration 16, loss = 0.24113766
Iteration 17, loss = 0.21781617
Iteration 18, loss = 0.19684730
Iteration 19, loss = 0.17823589
Iteration 20, loss = 0.16170525
Iteration 21, loss = 0.14708720
Iteration 22, loss = 0.13412617
Iteration 23, loss = 0.12275740
Iteration 24, loss = 0.11264313
Iteration 25, loss = 0.10368007
Iteration 26, loss = 0.09569891
Iteration 27, loss = 0.08863397
Iteration 28, loss = 0.08241248
Iteration 29, loss = 0.07681394
Iteration 30, loss = 0.07178497
Iteration 31, loss = 0.06724176
Iteration 32, loss = 0.06321537
Iteration 33, loss = 0.05954338
Iteration 34, loss = 0.05615830
Iteration 35, loss = 0.05322125
Iteration 36, loss = 0.05044969
Iteration 37, loss = 0.04789057
Iteration 38, loss = 0.04561382
Iteration 39, loss = 0.04350441
Iteration 40, loss = 0.04152968
Iteration 41, loss = 0.03978092
Iteration 42, loss = 0.03809840
Iteration 43, loss = 0.03654557
Iteration 44, loss = 0.03513895
Iteration 45, loss = 0.03384527
Iteration 46, loss = 0.03258208
Iteration 47, loss = 0.03147533
Iteration 48, loss = 0.03038543
Iteration 49, loss = 0.02943183
Iteration 50, loss = 0.02847622
Iteration 51, loss = 0.02757060
Iteration 52, loss = 0.02675989
Iteration 53, loss = 0.02601541
Iteration 54, loss = 0.02525452
Iteration 55, loss = 0.02456339
Iteration 56, loss = 0.02393682
Iteration 57, loss = 0.02328766
Iteration 58, loss = 0.02274891
Iteration 59, loss = 0.02218396
Iteration 60, loss = 0.02165871
Iteration 61, loss = 0.02117217
Iteration 62, loss = 0.02069805
Iteration 63, loss = 0.02026975
Iteration 64, loss = 0.01983543
Iteration 65, loss = 0.01941575
Iteration 66, loss = 0.01905447
Iteration 67, loss = 0.01865903
Iteration 68, loss = 0.01834193
Iteration 69, loss = 0.01797326
Iteration 70, loss = 0.01764955
Iteration 71, loss = 0.01735109
Iteration 72, loss = 0.01703827
Iteration 73, loss = 0.01675465
Iteration 74, loss = 0.01650434
Iteration 75, loss = 0.01625206
Iteration 76, loss = 0.01600125
Iteration 77, loss = 0.01575116
Iteration 78, loss = 0.01555492
Iteration 79, loss = 0.01533840
Iteration 80, loss = 0.01514180
Iteration 81, loss = 0.01489686
Iteration 82, loss = 0.01469004
Iteration 83, loss = 0.01454994
Iteration 84, loss = 0.01435250
Iteration 85, loss = 0.01416435
Iteration 86, loss = 0.01398057
Iteration 87, loss = 0.01380209
Iteration 88, loss = 0.01367448
Iteration 89, loss = 0.01356268
Iteration 90, loss = 0.01334182
Iteration 91, loss = 0.01322976
Iteration 92, loss = 0.01312512
Iteration 93, loss = 0.01293694
Iteration 94, loss = 0.01284879
Iteration 95, loss = 0.01274641
Iteration 96, loss = 0.01264570
Iteration 97, loss = 0.01249869
Iteration 98, loss = 0.01234764
Iteration 99, loss = 0.01225857
Iteration 100, loss = 0.01214266
Iteration 101, loss = 0.01205295
Iteration 102, loss = 0.01196558
Iteration 103, loss = 0.01182709
Iteration 104, loss = 0.01173969
Iteration 105, loss = 0.01166914
Iteration 106, loss = 0.01157402
Iteration 107, loss = 0.01151205
Iteration 108, loss = 0.01141594
Iteration 109, loss = 0.01132150
Iteration 110, loss = 0.01124898
Iteration 111, loss = 0.01120456
Iteration 112, loss = 0.01106829
Iteration 113, loss = 0.01101458
Iteration 114, loss = 0.01098776
Iteration 115, loss = 0.01088888
Iteration 116, loss = 0.01085580
Iteration 117, loss = 0.01075257
Iteration 118, loss = 0.01068259
Iteration 119, loss = 0.01062007
Iteration 120, loss = 0.01055730
Iteration 121, loss = 0.01048942
Iteration 122, loss = 0.01044380
Iteration 123, loss = 0.01041961
Iteration 124, loss = 0.01034181
Iteration 125, loss = 0.01026913
Iteration 126, loss = 0.01022670
Iteration 127, loss = 0.01022846
Iteration 128, loss = 0.01014327
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68998703
Iteration 2, loss = 0.60265461
Iteration 3, loss = 0.48085438
Iteration 4, loss = 0.35988453
Iteration 5, loss = 0.26089713
Iteration 6, loss = 0.18943308
Iteration 7, loss = 0.14168729
Iteration 8, loss = 0.10990876
Iteration 9, loss = 0.08838363
Iteration 10, loss = 0.07309391
Iteration 11, loss = 0.06224486
Iteration 12, loss = 0.05406770
Iteration 13, loss = 0.04793414
Iteration 14, loss = 0.04297462
Iteration 15, loss = 0.03902014
Iteration 16, loss = 0.03580059
Iteration 17, loss = 0.03349812
Iteration 18, loss = 0.03095911
Iteration 19, loss = 0.02952775
Iteration 20, loss = 0.02808018
Iteration 21, loss = 0.02657127
Iteration 22, loss = 0.02545933
Iteration 23, loss = 0.02465229
Iteration 24, loss = 0.02383328
Iteration 25, loss = 0.02303834
Iteration 26, loss = 0.02213738
Iteration 27, loss = 0.02162074
Iteration 28, loss = 0.02112323
Iteration 29, loss = 0.02073741
Iteration 30, loss = 0.02039153
Iteration 31, loss = 0.01998929
Iteration 32, loss = 0.01975894
Iteration 33, loss = 0.01943798
Iteration 34, loss = 0.01895785
Iteration 35, loss = 0.01875470
Iteration 36, loss = 0.01830540
Iteration 37, loss = 0.01838293
Iteration 38, loss = 0.01825571
Iteration 39, loss = 0.01780337
Iteration 40, loss = 0.01758491
Iteration 41, loss = 0.01748132
Iteration 42, loss = 0.01745214
Iteration 43, loss = 0.01711165
Iteration 44, loss = 0.01701663
Iteration 45, loss = 0.01691804
Iteration 46, loss = 0.01677974
Iteration 47, loss = 0.01684261
Iteration 48, loss = 0.01677701
Iteration 49, loss = 0.01672987
Iteration 50, loss = 0.01647784
Iteration 51, loss = 0.01660386
Iteration 52, loss = 0.01653179
Iteration 53, loss = 0.01618771
Iteration 54, loss = 0.01618660
Iteration 55, loss = 0.01601397
Iteration 56, loss = 0.01606158
Iteration 57, loss = 0.01577928
Iteration 58, loss = 0.01605868
Iteration 59, loss = 0.01592516
Iteration 60, loss = 0.01588893
Iteration 61, loss = 0.01569996
Iteration 62, loss = 0.01563302
Iteration 63, loss = 0.01557220
Iteration 64, loss = 0.01560647
Iteration 65, loss = 0.01572371
Iteration 66, loss = 0.01572776
Iteration 67, loss = 0.01542598
Iteration 68, loss = 0.01575603
Iteration 69, loss = 0.01531493
Iteration 70, loss = 0.01541480
Iteration 71, loss = 0.01528386
Iteration 72, loss = 0.01515697
Iteration 73, loss = 0.01551525
Iteration 74, loss = 0.01520638
Iteration 75, loss = 0.01532745
Iteration 76, loss = 0.01531163
Iteration 77, loss = 0.01505692
Iteration 78, loss = 0.01524954
Iteration 79, loss = 0.01493211
Iteration 80, loss = 0.01537460
Iteration 81, loss = 0.01514078
Iteration 82, loss = 0.01527607
Iteration 83, loss = 0.01503068
Iteration 84, loss = 0.01509213
Iteration 85, loss = 0.01485370
Iteration 86, loss = 0.01497886
Iteration 87, loss = 0.01480236
Iteration 88, loss = 0.01479760
Iteration 89, loss = 0.01488742
Iteration 90, loss = 0.01481275
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69314395
Iteration 2, loss = 0.66108788
Iteration 3, loss = 0.62475968
Iteration 4, loss = 0.58188939
Iteration 5, loss = 0.53336382
Iteration 6, loss = 0.48192915
Iteration 7, loss = 0.43087484
Iteration 8, loss = 0.38129662
Iteration 9, loss = 0.33473757
Iteration 10, loss = 0.29211640
Iteration 11, loss = 0.25432021
Iteration 12, loss = 0.22112728
Iteration 13, loss = 0.19295054
Iteration 14, loss = 0.16854997
Iteration 15, loss = 0.14806166
Iteration 16, loss = 0.13073200
Iteration 17, loss = 0.11622531
Iteration 18, loss = 0.10398975
Iteration 19, loss = 0.09365547
Iteration 20, loss = 0.08478974
Iteration 21, loss = 0.07718399
Iteration 22, loss = 0.07059296
Iteration 23, loss = 0.06488319
Iteration 24, loss = 0.06001957
Iteration 25, loss = 0.05572399
Iteration 26, loss = 0.05206452
Iteration 27, loss = 0.04860070
Iteration 28, loss = 0.04573522
Iteration 29, loss = 0.04308980
Iteration 30, loss = 0.04082694
Iteration 31, loss = 0.03868299
Iteration 32, loss = 0.03678833
Iteration 33, loss = 0.03510792
Iteration 34, loss = 0.03360566
Iteration 35, loss = 0.03219093
Iteration 36, loss = 0.03091834
Iteration 37, loss = 0.02975618
Iteration 38, loss = 0.02861375
Iteration 39, loss = 0.02763044
Iteration 40, loss = 0.02678403
Iteration 41, loss = 0.02587354
Iteration 42, loss = 0.02517890
Iteration 43, loss = 0.02439774
Iteration 44, loss = 0.02382366
Iteration 45, loss = 0.02319544
Iteration 46, loss = 0.02261975
Iteration 47, loss = 0.02217981
Iteration 48, loss = 0.02157031
Iteration 49, loss = 0.02167047
Iteration 50, loss = 0.02124063
Iteration 51, loss = 0.02070102
Iteration 52, loss = 0.02013463
Iteration 53, loss = 0.02011298
Iteration 54, loss = 0.01964788
Iteration 55, loss = 0.01911576
Iteration 56, loss = 0.01857165
Iteration 57, loss = 0.01849874
Iteration 58, loss = 0.01814137
Iteration 59, loss = 0.01778039
Iteration 60, loss = 0.01754243
Iteration 61, loss = 0.01723182
Iteration 62, loss = 0.01701485
Iteration 63, loss = 0.01676291
Iteration 64, loss = 0.01657119
Iteration 65, loss = 0.01642553
Iteration 66, loss = 0.01633258
Iteration 67, loss = 0.01617407
Iteration 68, loss = 0.01596334
Iteration 69, loss = 0.01580205
Iteration 70, loss = 0.01575045
Iteration 71, loss = 0.01570844
Iteration 72, loss = 0.01554198
Iteration 73, loss = 0.01532098
Iteration 74, loss = 0.01502894
Iteration 75, loss = 0.01492845
Iteration 76, loss = 0.01478157
Iteration 77, loss = 0.01464769
Iteration 78, loss = 0.01454314
Iteration 79, loss = 0.01451250
Iteration 80, loss = 0.01438997
Iteration 81, loss = 0.01425204
Iteration 82, loss = 0.01417545
Iteration 83, loss = 0.01405304
Iteration 84, loss = 0.01391451
Iteration 85, loss = 0.01411115
Iteration 86, loss = 0.01395591
Iteration 87, loss = 0.01379583
Iteration 88, loss = 0.01367511
Iteration 89, loss = 0.01359036
Iteration 90, loss = 0.01353134
Iteration 91, loss = 0.01353546
Iteration 92, loss = 0.01343984
Iteration 93, loss = 0.01327214
Iteration 94, loss = 0.01329492
Iteration 95, loss = 0.01326306
Iteration 96, loss = 0.01316297
Iteration 97, loss = 0.01312110
Iteration 98, loss = 0.01307325
Iteration 99, loss = 0.01301510
Iteration 100, loss = 0.01302640
Iteration 101, loss = 0.01279525
Iteration 102, loss = 0.01318921
Iteration 103, loss = 0.01333918
Iteration 104, loss = 0.01327443
Iteration 105, loss = 0.01307606
Iteration 106, loss = 0.01294465
Iteration 107, loss = 0.01298071
Iteration 108, loss = 0.01280179
Iteration 109, loss = 0.01300364
Iteration 110, loss = 0.01298096
Iteration 111, loss = 0.01309431
Iteration 112, loss = 0.01296678
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70993823
Iteration 2, loss = 0.62188421
Iteration 3, loss = 0.51098003
Iteration 4, loss = 0.39207785
Iteration 5, loss = 0.29159606
Iteration 6, loss = 0.21604245
Iteration 7, loss = 0.16284176
Iteration 8, loss = 0.12604043
Iteration 9, loss = 0.10062414
Iteration 10, loss = 0.08257293
Iteration 11, loss = 0.06947826
Iteration 12, loss = 0.06007374
Iteration 13, loss = 0.05251530
Iteration 14, loss = 0.04677633
Iteration 15, loss = 0.04209433
Iteration 16, loss = 0.03865340
Iteration 17, loss = 0.03538049
Iteration 18, loss = 0.03294875
Iteration 19, loss = 0.03079627
Iteration 20, loss = 0.02906402
Iteration 21, loss = 0.02741799
Iteration 22, loss = 0.02594674
Iteration 23, loss = 0.02489165
Iteration 24, loss = 0.02403591
Iteration 25, loss = 0.02318923
Iteration 26, loss = 0.02233970
Iteration 27, loss = 0.02163120
Iteration 28, loss = 0.02094446
Iteration 29, loss = 0.02051578
Iteration 30, loss = 0.01986690
Iteration 31, loss = 0.01959453
Iteration 32, loss = 0.01904633
Iteration 33, loss = 0.01872995
Iteration 34, loss = 0.01843300
Iteration 35, loss = 0.01832941
Iteration 36, loss = 0.01790136
Iteration 37, loss = 0.01756224
Iteration 38, loss = 0.01728810
Iteration 39, loss = 0.01713359
Iteration 40, loss = 0.01704334
Iteration 41, loss = 0.01657815
Iteration 42, loss = 0.01674050
Iteration 43, loss = 0.01651780
Iteration 44, loss = 0.01621647
Iteration 45, loss = 0.01599789
Iteration 46, loss = 0.01595696
Iteration 47, loss = 0.01579694
Iteration 48, loss = 0.01566893
Iteration 49, loss = 0.01579132
Iteration 50, loss = 0.01516924
Iteration 51, loss = 0.01522124
Iteration 52, loss = 0.01525198
Iteration 53, loss = 0.01516492
Iteration 54, loss = 0.01523634
Iteration 55, loss = 0.01505740
Iteration 56, loss = 0.01486974
Iteration 57, loss = 0.01486813
Iteration 58, loss = 0.01474942
Iteration 59, loss = 0.01466337
Iteration 60, loss = 0.01463456
Iteration 61, loss = 0.01449469
Iteration 62, loss = 0.01440010
Iteration 63, loss = 0.01449421
Iteration 64, loss = 0.01444507
Iteration 65, loss = 0.01444748
Iteration 66, loss = 0.01422251
Iteration 67, loss = 0.01416425
Iteration 68, loss = 0.01440650
Iteration 69, loss = 0.01424602
Iteration 70, loss = 0.01414942
Iteration 71, loss = 0.01403960
Iteration 72, loss = 0.01418248
Iteration 73, loss = 0.01405996
Iteration 74, loss = 0.01415128
Iteration 75, loss = 0.01430965
Iteration 76, loss = 0.01400162
Iteration 77, loss = 0.01389935
Iteration 78, loss = 0.01409990
Iteration 79, loss = 0.01392282
Iteration 80, loss = 0.01386541
Iteration 81, loss = 0.01377135
Iteration 82, loss = 0.01385357
Iteration 83, loss = 0.01363327
Iteration 84, loss = 0.01381792
Iteration 85, loss = 0.01367846
Iteration 86, loss = 0.01360426
Iteration 87, loss = 0.01364168
Iteration 88, loss = 0.01362612
Iteration 89, loss = 0.01351734
Iteration 90, loss = 0.01361133
Iteration 91, loss = 0.01354157
Iteration 92, loss = 0.01360694
Iteration 93, loss = 0.01354364
Iteration 94, loss = 0.01347114
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67384242
Iteration 2, loss = 0.59831191
Iteration 3, loss = 0.48750295
Iteration 4, loss = 0.37526897
Iteration 5, loss = 0.28056793
Iteration 6, loss = 0.20840061
Iteration 7, loss = 0.15766694
Iteration 8, loss = 0.12228994
Iteration 9, loss = 0.09801728
Iteration 10, loss = 0.08051392
Iteration 11, loss = 0.06796446
Iteration 12, loss = 0.05843944
Iteration 13, loss = 0.05134175
Iteration 14, loss = 0.04561843
Iteration 15, loss = 0.04124003
Iteration 16, loss = 0.03756346
Iteration 17, loss = 0.03448282
Iteration 18, loss = 0.03195932
Iteration 19, loss = 0.02993118
Iteration 20, loss = 0.02809976
Iteration 21, loss = 0.02659224
Iteration 22, loss = 0.02531827
Iteration 23, loss = 0.02427190
Iteration 24, loss = 0.02307184
Iteration 25, loss = 0.02219815
Iteration 26, loss = 0.02155903
Iteration 27, loss = 0.02102490
Iteration 28, loss = 0.02058117
Iteration 29, loss = 0.02010124
Iteration 30, loss = 0.01934315
Iteration 31, loss = 0.01898662
Iteration 32, loss = 0.01864905
Iteration 33, loss = 0.01814417
Iteration 34, loss = 0.01784844
Iteration 35, loss = 0.01736959
Iteration 36, loss = 0.01709420
Iteration 37, loss = 0.01679076
Iteration 38, loss = 0.01667710
Iteration 39, loss = 0.01659363
Iteration 40, loss = 0.01617741
Iteration 41, loss = 0.01579197
Iteration 42, loss = 0.01585910
Iteration 43, loss = 0.01579914
Iteration 44, loss = 0.01566127
Iteration 45, loss = 0.01520038
Iteration 46, loss = 0.01530400
Iteration 47, loss = 0.01504938
Iteration 48, loss = 0.01494734
Iteration 49, loss = 0.01489658
Iteration 50, loss = 0.01492020
Iteration 51, loss = 0.01456137
Iteration 52, loss = 0.01493972
Iteration 53, loss = 0.01477560
Iteration 54, loss = 0.01428224
Iteration 55, loss = 0.01433620
Iteration 56, loss = 0.01430514
Iteration 57, loss = 0.01420139
Iteration 58, loss = 0.01414569
Iteration 59, loss = 0.01419596
Iteration 60, loss = 0.01399065
Iteration 61, loss = 0.01425109
Iteration 62, loss = 0.01401617
Iteration 63, loss = 0.01377400
Iteration 64, loss = 0.01377322
Iteration 65, loss = 0.01379977
Iteration 66, loss = 0.01394351
Iteration 67, loss = 0.01365363
Iteration 68, loss = 0.01356202
Iteration 69, loss = 0.01381261
Iteration 70, loss = 0.01365765
Iteration 71, loss = 0.01341679
Iteration 72, loss = 0.01355157
Iteration 73, loss = 0.01333536
Iteration 74, loss = 0.01338710
Iteration 75, loss = 0.01342028
Iteration 76, loss = 0.01361747
Iteration 77, loss = 0.01314745
Iteration 78, loss = 0.01323077
Iteration 79, loss = 0.01337767
Iteration 80, loss = 0.01338267
Iteration 81, loss = 0.01324583
Iteration 82, loss = 0.01323740
Iteration 83, loss = 0.01311170
Iteration 84, loss = 0.01314234
Iteration 85, loss = 0.01300490
Iteration 86, loss = 0.01311361
Iteration 87, loss = 0.01293364
Iteration 88, loss = 0.01292990
Iteration 89, loss = 0.01289805
Iteration 90, loss = 0.01298528
Iteration 91, loss = 0.01296635
Iteration 92, loss = 0.01278526
Iteration 93, loss = 0.01290000
Iteration 94, loss = 0.01291759
Iteration 95, loss = 0.01297072
Iteration 96, loss = 0.01287967
Iteration 97, loss = 0.01290884
Iteration 98, loss = 0.01284294
Iteration 99, loss = 0.01263230
Iteration 100, loss = 0.01286399
Iteration 101, loss = 0.01286569
Iteration 102, loss = 0.01278785
Iteration 103, loss = 0.01270668
Iteration 104, loss = 0.01284406
Iteration 105, loss = 0.01260122
Iteration 106, loss = 0.01276462
Iteration 107, loss = 0.01255415
Iteration 108, loss = 0.01269735
Iteration 109, loss = 0.01287532
Iteration 110, loss = 0.01262941
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70379842
Iteration 2, loss = 0.66929045
Iteration 3, loss = 0.61820297
Iteration 4, loss = 0.55390349
Iteration 5, loss = 0.47979009
Iteration 6, loss = 0.40581371
Iteration 7, loss = 0.33743624
Iteration 8, loss = 0.27787161
Iteration 9, loss = 0.22820412
Iteration 10, loss = 0.18819431
Iteration 11, loss = 0.15638622
Iteration 12, loss = 0.13138691
Iteration 13, loss = 0.11184566
Iteration 14, loss = 0.09641518
Iteration 15, loss = 0.08427823
Iteration 16, loss = 0.07421663
Iteration 17, loss = 0.06616012
Iteration 18, loss = 0.05965697
Iteration 19, loss = 0.05412201
Iteration 20, loss = 0.04949775
Iteration 21, loss = 0.04574273
Iteration 22, loss = 0.04224729
Iteration 23, loss = 0.03945576
Iteration 24, loss = 0.03702964
Iteration 25, loss = 0.03497235
Iteration 26, loss = 0.03304596
Iteration 27, loss = 0.03145279
Iteration 28, loss = 0.03005023
Iteration 29, loss = 0.02855801
Iteration 30, loss = 0.02745404
Iteration 31, loss = 0.02636904
Iteration 32, loss = 0.02543470
Iteration 33, loss = 0.02459825
Iteration 34, loss = 0.02384073
Iteration 35, loss = 0.02305037
Iteration 36, loss = 0.02243560
Iteration 37, loss = 0.02191469
Iteration 38, loss = 0.02139484
Iteration 39, loss = 0.02093428
Iteration 40, loss = 0.02039061
Iteration 41, loss = 0.02005329
Iteration 42, loss = 0.01974929
Iteration 43, loss = 0.01922369
Iteration 44, loss = 0.01889041
Iteration 45, loss = 0.01853300
Iteration 46, loss = 0.01832548
Iteration 47, loss = 0.01813424
Iteration 48, loss = 0.01780147
Iteration 49, loss = 0.01751319
Iteration 50, loss = 0.01729648
Iteration 51, loss = 0.01721664
Iteration 52, loss = 0.01687744
Iteration 53, loss = 0.01685853
Iteration 54, loss = 0.01653470
Iteration 55, loss = 0.01634922
Iteration 56, loss = 0.01633059
Iteration 57, loss = 0.01610700
Iteration 58, loss = 0.01616596
Iteration 59, loss = 0.01582157
Iteration 60, loss = 0.01583273
Iteration 61, loss = 0.01562877
Iteration 62, loss = 0.01565511
Iteration 63, loss = 0.01551850
Iteration 64, loss = 0.01545840
Iteration 65, loss = 0.01523170
Iteration 66, loss = 0.01515920
Iteration 67, loss = 0.01492080
Iteration 68, loss = 0.01486765
Iteration 69, loss = 0.01481135
Iteration 70, loss = 0.01472475
Iteration 71, loss = 0.01467410
Iteration 72, loss = 0.01457822
Iteration 73, loss = 0.01464066
Iteration 74, loss = 0.01441381
Iteration 75, loss = 0.01430752
Iteration 76, loss = 0.01434619
Iteration 77, loss = 0.01436900
Iteration 78, loss = 0.01428758
Iteration 79, loss = 0.01431837
Iteration 80, loss = 0.01418015
Iteration 81, loss = 0.01416990
Iteration 82, loss = 0.01406971
Iteration 83, loss = 0.01405713
Iteration 84, loss = 0.01408520
Iteration 85, loss = 0.01386481
Iteration 86, loss = 0.01407774
Iteration 87, loss = 0.01388668
Iteration 88, loss = 0.01379278
Iteration 89, loss = 0.01394200
Iteration 90, loss = 0.01384260
Iteration 91, loss = 0.01385399
Iteration 92, loss = 0.01358532
Iteration 93, loss = 0.01355003
Iteration 94, loss = 0.01365850
Iteration 95, loss = 0.01353578
Iteration 96, loss = 0.01354586
Iteration 97, loss = 0.01360768
Iteration 98, loss = 0.01350145
Iteration 99, loss = 0.01343338
Iteration 100, loss = 0.01332913
Iteration 101, loss = 0.01333429
Iteration 102, loss = 0.01339020
Iteration 103, loss = 0.01343324
Iteration 104, loss = 0.01335654
Iteration 105, loss = 0.01340225
Iteration 106, loss = 0.01339083
Iteration 107, loss = 0.01321027
Iteration 108, loss = 0.01333914
Iteration 109, loss = 0.01329538
Iteration 110, loss = 0.01328234
Iteration 111, loss = 0.01312147
Iteration 112, loss = 0.01317627
Iteration 113, loss = 0.01314171
Iteration 114, loss = 0.01325197
Iteration 115, loss = 0.01301720
Iteration 116, loss = 0.01322160
Iteration 117, loss = 0.01306067
Iteration 118, loss = 0.01309082
Iteration 119, loss = 0.01318245
Iteration 120, loss = 0.01305738
Iteration 121, loss = 0.01306898
Iteration 122, loss = 0.01311871
Iteration 123, loss = 0.01299508
Iteration 124, loss = 0.01304345
Iteration 125, loss = 0.01297129
Iteration 126, loss = 0.01303410
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70828596
Iteration 2, loss = 0.62970601
Iteration 3, loss = 0.52529652
Iteration 4, loss = 0.40891751
Iteration 5, loss = 0.30544792
Iteration 6, loss = 0.22579558
Iteration 7, loss = 0.16926954
Iteration 8, loss = 0.13033952
Iteration 9, loss = 0.10344693
Iteration 10, loss = 0.08439176
Iteration 11, loss = 0.07074365
Iteration 12, loss = 0.06072467
Iteration 13, loss = 0.05302422
Iteration 14, loss = 0.04710752
Iteration 15, loss = 0.04243310
Iteration 16, loss = 0.03875642
Iteration 17, loss = 0.03546952
Iteration 18, loss = 0.03304166
Iteration 19, loss = 0.03079898
Iteration 20, loss = 0.02891334
Iteration 21, loss = 0.02763238
Iteration 22, loss = 0.02633592
Iteration 23, loss = 0.02493395
Iteration 24, loss = 0.02401280
Iteration 25, loss = 0.02304102
Iteration 26, loss = 0.02225566
Iteration 27, loss = 0.02149228
Iteration 28, loss = 0.02090126
Iteration 29, loss = 0.02040023
Iteration 30, loss = 0.02000514
Iteration 31, loss = 0.01956633
Iteration 32, loss = 0.01910178
Iteration 33, loss = 0.01892706
Iteration 34, loss = 0.01842807
Iteration 35, loss = 0.01798813
Iteration 36, loss = 0.01776204
Iteration 37, loss = 0.01753823
Iteration 38, loss = 0.01734967
Iteration 39, loss = 0.01718426
Iteration 40, loss = 0.01706653
Iteration 41, loss = 0.01671024
Iteration 42, loss = 0.01652491
Iteration 43, loss = 0.01644873
Iteration 44, loss = 0.01645031
Iteration 45, loss = 0.01601058
Iteration 46, loss = 0.01597112
Iteration 47, loss = 0.01583896
Iteration 48, loss = 0.01563389
Iteration 49, loss = 0.01562363
Iteration 50, loss = 0.01561775
Iteration 51, loss = 0.01541937
Iteration 52, loss = 0.01548782
Iteration 53, loss = 0.01527353
Iteration 54, loss = 0.01511501
Iteration 55, loss = 0.01495380
Iteration 56, loss = 0.01523472
Iteration 57, loss = 0.01496916
Iteration 58, loss = 0.01493446
Iteration 59, loss = 0.01492428
Iteration 60, loss = 0.01489946
Iteration 61, loss = 0.01466666
Iteration 62, loss = 0.01462336
Iteration 63, loss = 0.01462948
Iteration 64, loss = 0.01455592
Iteration 65, loss = 0.01471121
Iteration 66, loss = 0.01438526
Iteration 67, loss = 0.01452045
Iteration 68, loss = 0.01470434
Iteration 69, loss = 0.01453792
Iteration 70, loss = 0.01429758
Iteration 71, loss = 0.01423598
Iteration 72, loss = 0.01417991
Iteration 73, loss = 0.01416239
Iteration 74, loss = 0.01426556
Iteration 75, loss = 0.01413507
Iteration 76, loss = 0.01409357
Iteration 77, loss = 0.01400854
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68586884
Iteration 2, loss = 0.61708165
Iteration 3, loss = 0.50878839
Iteration 4, loss = 0.38677804
Iteration 5, loss = 0.28349857
Iteration 6, loss = 0.20726511
Iteration 7, loss = 0.15491068
Iteration 8, loss = 0.11988670
Iteration 9, loss = 0.09618896
Iteration 10, loss = 0.07979286
Iteration 11, loss = 0.06793766
Iteration 12, loss = 0.05932205
Iteration 13, loss = 0.05247777
Iteration 14, loss = 0.04734871
Iteration 15, loss = 0.04347691
Iteration 16, loss = 0.03996729
Iteration 17, loss = 0.03736498
Iteration 18, loss = 0.03521221
Iteration 19, loss = 0.03329793
Iteration 20, loss = 0.03155302
Iteration 21, loss = 0.03037552
Iteration 22, loss = 0.02905553
Iteration 23, loss = 0.02818410
Iteration 24, loss = 0.02719013
Iteration 25, loss = 0.02651134
Iteration 26, loss = 0.02585337
Iteration 27, loss = 0.02514228
Iteration 28, loss = 0.02451892
Iteration 29, loss = 0.02408988
Iteration 30, loss = 0.02381495
Iteration 31, loss = 0.02336685
Iteration 32, loss = 0.02317783
Iteration 33, loss = 0.02293962
Iteration 34, loss = 0.02261500
Iteration 35, loss = 0.02204939
Iteration 36, loss = 0.02177058
Iteration 37, loss = 0.02164495
Iteration 38, loss = 0.02151611
Iteration 39, loss = 0.02135733
Iteration 40, loss = 0.02100218
Iteration 41, loss = 0.02099157
Iteration 42, loss = 0.02066419
Iteration 43, loss = 0.02061379
Iteration 44, loss = 0.02017781
Iteration 45, loss = 0.02026597
Iteration 46, loss = 0.02024546
Iteration 47, loss = 0.02032333
Iteration 48, loss = 0.02001724
Iteration 49, loss = 0.01997213
Iteration 50, loss = 0.01996216
Iteration 51, loss = 0.01958225
Iteration 52, loss = 0.01974484
Iteration 53, loss = 0.01977025
Iteration 54, loss = 0.01945578
Iteration 55, loss = 0.01940053
Iteration 56, loss = 0.01927625
Iteration 57, loss = 0.01949041
Iteration 58, loss = 0.01963495
Iteration 59, loss = 0.01903766
Iteration 60, loss = 0.01919829
Iteration 61, loss = 0.01903612
Iteration 62, loss = 0.01905566
Iteration 63, loss = 0.01904976
Iteration 64, loss = 0.01927330
Iteration 65, loss = 0.01895204
Iteration 66, loss = 0.01894599
Iteration 67, loss = 0.01896231
Iteration 68, loss = 0.01889640
Iteration 69, loss = 0.01860267
Iteration 70, loss = 0.01872216
Iteration 71, loss = 0.01871238
Iteration 72, loss = 0.01874149
Iteration 73, loss = 0.01868389
Iteration 74, loss = 0.01890476
Iteration 75, loss = 0.01859204
Iteration 76, loss = 0.01851606
Iteration 77, loss = 0.01857335
Iteration 78, loss = 0.01837879
Iteration 79, loss = 0.01861491
Iteration 80, loss = 0.01834971
Iteration 81, loss = 0.01849492
Iteration 82, loss = 0.01849661
Iteration 83, loss = 0.01820814
Iteration 84, loss = 0.01826368
Iteration 85, loss = 0.01846049
Iteration 86, loss = 0.01833764
Iteration 87, loss = 0.01850135
Iteration 88, loss = 0.01816318
Iteration 89, loss = 0.01821624
Iteration 90, loss = 0.01837457
Iteration 91, loss = 0.01819242
Iteration 92, loss = 0.01829567
Iteration 93, loss = 0.01833465
Iteration 94, loss = 0.01791371
Iteration 95, loss = 0.01805266
Iteration 96, loss = 0.01798723
Iteration 97, loss = 0.01790773
Iteration 98, loss = 0.01794755
Iteration 99, loss = 0.01817021
Iteration 100, loss = 0.01784386
Iteration 101, loss = 0.01778976
Iteration 102, loss = 0.01807829
Iteration 103, loss = 0.01789892
Iteration 104, loss = 0.01771869
Iteration 105, loss = 0.01805669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68822448
Iteration 2, loss = 0.65477244
Iteration 3, loss = 0.60867456
Iteration 4, loss = 0.55119843
Iteration 5, loss = 0.48731372
Iteration 6, loss = 0.42351042
Iteration 7, loss = 0.36400820
Iteration 8, loss = 0.31026430
Iteration 9, loss = 0.26280264
Iteration 10, loss = 0.22235864
Iteration 11, loss = 0.18850327
Iteration 12, loss = 0.16061670
Iteration 13, loss = 0.13764842
Iteration 14, loss = 0.11890805
Iteration 15, loss = 0.10362226
Iteration 16, loss = 0.09111613
Iteration 17, loss = 0.08092257
Iteration 18, loss = 0.07241987
Iteration 19, loss = 0.06532710
Iteration 20, loss = 0.05935949
Iteration 21, loss = 0.05429978
Iteration 22, loss = 0.05011057
Iteration 23, loss = 0.04635206
Iteration 24, loss = 0.04304692
Iteration 25, loss = 0.04026441
Iteration 26, loss = 0.03788062
Iteration 27, loss = 0.03571554
Iteration 28, loss = 0.03376664
Iteration 29, loss = 0.03211758
Iteration 30, loss = 0.03056051
Iteration 31, loss = 0.02923138
Iteration 32, loss = 0.02811394
Iteration 33, loss = 0.02695121
Iteration 34, loss = 0.02606873
Iteration 35, loss = 0.02509364
Iteration 36, loss = 0.02436630
Iteration 37, loss = 0.02355040
Iteration 38, loss = 0.02286711
Iteration 39, loss = 0.02216304
Iteration 40, loss = 0.02149975
Iteration 41, loss = 0.02102855
Iteration 42, loss = 0.02053683
Iteration 43, loss = 0.02015315
Iteration 44, loss = 0.01966503
Iteration 45, loss = 0.01920381
Iteration 46, loss = 0.01889418
Iteration 47, loss = 0.01857543
Iteration 48, loss = 0.01829883
Iteration 49, loss = 0.01796633
Iteration 50, loss = 0.01765959
Iteration 51, loss = 0.01735349
Iteration 52, loss = 0.01717098
Iteration 53, loss = 0.01699567
Iteration 54, loss = 0.01662511
Iteration 55, loss = 0.01650229
Iteration 56, loss = 0.01630291
Iteration 57, loss = 0.01616953
Iteration 58, loss = 0.01596497
Iteration 59, loss = 0.01575976
Iteration 60, loss = 0.01563268
Iteration 61, loss = 0.01552232
Iteration 62, loss = 0.01532907
Iteration 63, loss = 0.01518804
Iteration 64, loss = 0.01497195
Iteration 65, loss = 0.01488476
Iteration 66, loss = 0.01490516
Iteration 67, loss = 0.01477267
Iteration 68, loss = 0.01467791
Iteration 69, loss = 0.01457486
Iteration 70, loss = 0.01441097
Iteration 71, loss = 0.01432838
Iteration 72, loss = 0.01420513
Iteration 73, loss = 0.01404419
Iteration 74, loss = 0.01404498
Iteration 75, loss = 0.01395731
Iteration 76, loss = 0.01386380
Iteration 77, loss = 0.01376839
Iteration 78, loss = 0.01373461
Iteration 79, loss = 0.01377705
Iteration 80, loss = 0.01362317
Iteration 81, loss = 0.01350177
Iteration 82, loss = 0.01357507
Iteration 83, loss = 0.01358433
Iteration 84, loss = 0.01339959
Iteration 85, loss = 0.01347318
Iteration 86, loss = 0.01335092
Iteration 87, loss = 0.01333460
Iteration 88, loss = 0.01328087
Iteration 89, loss = 0.01311967
Iteration 90, loss = 0.01307834
Iteration 91, loss = 0.01305137
Iteration 92, loss = 0.01301407
Iteration 93, loss = 0.01294260
Iteration 94, loss = 0.01307623
Iteration 95, loss = 0.01290932
Iteration 96, loss = 0.01290573
Iteration 97, loss = 0.01291535
Iteration 98, loss = 0.01284390
Iteration 99, loss = 0.01290768
Iteration 100, loss = 0.01288237
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71772015
Iteration 2, loss = 0.69980640
Iteration 3, loss = 0.68601995
Iteration 4, loss = 0.67311142
Iteration 5, loss = 0.65950632
Iteration 6, loss = 0.64480988
Iteration 7, loss = 0.62862344
Iteration 8, loss = 0.61066594
Iteration 9, loss = 0.59084733
Iteration 10, loss = 0.56951130
Iteration 11, loss = 0.54649834
Iteration 12, loss = 0.52259776
Iteration 13, loss = 0.49785750
Iteration 14, loss = 0.47296735
Iteration 15, loss = 0.44779937
Iteration 16, loss = 0.42295464
Iteration 17, loss = 0.39863611
Iteration 18, loss = 0.37488151
Iteration 19, loss = 0.35196275
Iteration 20, loss = 0.32998672
Iteration 21, loss = 0.30898425
Iteration 22, loss = 0.28912736
Iteration 23, loss = 0.27038725
Iteration 24, loss = 0.25283977
Iteration 25, loss = 0.23639563
Iteration 26, loss = 0.22107974
Iteration 27, loss = 0.20678969
Iteration 28, loss = 0.19355902
Iteration 29, loss = 0.18129432
Iteration 30, loss = 0.16991883
Iteration 31, loss = 0.15939990
Iteration 32, loss = 0.14966028
Iteration 33, loss = 0.14074834
Iteration 34, loss = 0.13247144
Iteration 35, loss = 0.12479540
Iteration 36, loss = 0.11775667
Iteration 37, loss = 0.11128667
Iteration 38, loss = 0.10522670
Iteration 39, loss = 0.09971568
Iteration 40, loss = 0.09451852
Iteration 41, loss = 0.08981381
Iteration 42, loss = 0.08533200
Iteration 43, loss = 0.08121738
Iteration 44, loss = 0.07741915
Iteration 45, loss = 0.07385004
Iteration 46, loss = 0.07055825
Iteration 47, loss = 0.06745574
Iteration 48, loss = 0.06459245
Iteration 49, loss = 0.06193979
Iteration 50, loss = 0.05938790
Iteration 51, loss = 0.05701034
Iteration 52, loss = 0.05479778
Iteration 53, loss = 0.05274270
Iteration 54, loss = 0.05077184
Iteration 55, loss = 0.04892385
Iteration 56, loss = 0.04717237
Iteration 57, loss = 0.04557213
Iteration 58, loss = 0.04403793
Iteration 59, loss = 0.04259742
Iteration 60, loss = 0.04123220
Iteration 61, loss = 0.03990153
Iteration 62, loss = 0.03868322
Iteration 63, loss = 0.03750630
Iteration 64, loss = 0.03642186
Iteration 65, loss = 0.03533576
Iteration 66, loss = 0.03433689
Iteration 67, loss = 0.03337398
Iteration 68, loss = 0.03247676
Iteration 69, loss = 0.03161388
Iteration 70, loss = 0.03077356
Iteration 71, loss = 0.02996327
Iteration 72, loss = 0.02921516
Iteration 73, loss = 0.02847963
Iteration 74, loss = 0.02778174
Iteration 75, loss = 0.02712024
Iteration 76, loss = 0.02646626
Iteration 77, loss = 0.02587998
Iteration 78, loss = 0.02527807
Iteration 79, loss = 0.02470611
Iteration 80, loss = 0.02419275
Iteration 81, loss = 0.02364485
Iteration 82, loss = 0.02316934
Iteration 83, loss = 0.02266192
Iteration 84, loss = 0.02220874
Iteration 85, loss = 0.02176683
Iteration 86, loss = 0.02133661
Iteration 87, loss = 0.02093782
Iteration 88, loss = 0.02052282
Iteration 89, loss = 0.02015099
Iteration 90, loss = 0.01976830
Iteration 91, loss = 0.01942971
Iteration 92, loss = 0.01910138
Iteration 93, loss = 0.01876226
Iteration 94, loss = 0.01844171
Iteration 95, loss = 0.01815943
Iteration 96, loss = 0.01786819
Iteration 97, loss = 0.01756947
Iteration 98, loss = 0.01729930
Iteration 99, loss = 0.01701041
Iteration 100, loss = 0.01677420
Iteration 101, loss = 0.01647037
Iteration 102, loss = 0.01620464
Iteration 103, loss = 0.01595593
Iteration 104, loss = 0.01571086
Iteration 105, loss = 0.01550093
Iteration 106, loss = 0.01526621
Iteration 107, loss = 0.01503263
Iteration 108, loss = 0.01482114
Iteration 109, loss = 0.01463371
Iteration 110, loss = 0.01442219
Iteration 111, loss = 0.01423139
Iteration 112, loss = 0.01404512
Iteration 113, loss = 0.01386153
Iteration 114, loss = 0.01368826
Iteration 115, loss = 0.01351130
Iteration 116, loss = 0.01334006
Iteration 117, loss = 0.01319014
Iteration 118, loss = 0.01303264
Iteration 119, loss = 0.01285614
Iteration 120, loss = 0.01271044
Iteration 121, loss = 0.01256949
Iteration 122, loss = 0.01242170
Iteration 123, loss = 0.01230417
Iteration 124, loss = 0.01217264
Iteration 125, loss = 0.01201397
Iteration 126, loss = 0.01187717
Iteration 127, loss = 0.01172682
Iteration 128, loss = 0.01168270
Iteration 129, loss = 0.01150747
Iteration 130, loss = 0.01138406
Iteration 131, loss = 0.01127644
Iteration 132, loss = 0.01115866
Iteration 133, loss = 0.01105556
Iteration 134, loss = 0.01095008
Iteration 135, loss = 0.01083248
Iteration 136, loss = 0.01074270
Iteration 137, loss = 0.01065542
Iteration 138, loss = 0.01054313
Iteration 139, loss = 0.01045144
Iteration 140, loss = 0.01035626
Iteration 141, loss = 0.01027046
Iteration 142, loss = 0.01017420
Iteration 143, loss = 0.01009576
Iteration 144, loss = 0.01000054
Iteration 145, loss = 0.00992197
Iteration 146, loss = 0.00985186
Iteration 147, loss = 0.00974860
Iteration 148, loss = 0.00966563
Iteration 149, loss = 0.00958280
Iteration 150, loss = 0.00949224
Iteration 151, loss = 0.00938752
Iteration 152, loss = 0.00932998
Iteration 153, loss = 0.00936840
Iteration 154, loss = 0.00923523
Iteration 155, loss = 0.00917485
Iteration 156, loss = 0.00909861
Iteration 157, loss = 0.00902963
Iteration 158, loss = 0.00896355
Iteration 159, loss = 0.00889861
Iteration 160, loss = 0.00883697
Iteration 161, loss = 0.00876877
Iteration 162, loss = 0.00870482
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70985126
Iteration 2, loss = 0.67215515
Iteration 3, loss = 0.62923582
Iteration 4, loss = 0.58027193
Iteration 5, loss = 0.52714460
Iteration 6, loss = 0.47229920
Iteration 7, loss = 0.41882471
Iteration 8, loss = 0.36835318
Iteration 9, loss = 0.32234354
Iteration 10, loss = 0.28108840
Iteration 11, loss = 0.24490826
Iteration 12, loss = 0.21340042
Iteration 13, loss = 0.18650461
Iteration 14, loss = 0.16334433
Iteration 15, loss = 0.14377512
Iteration 16, loss = 0.12707722
Iteration 17, loss = 0.11301740
Iteration 18, loss = 0.10104669
Iteration 19, loss = 0.09096751
Iteration 20, loss = 0.08223511
Iteration 21, loss = 0.07467165
Iteration 22, loss = 0.06819375
Iteration 23, loss = 0.06263520
Iteration 24, loss = 0.05771903
Iteration 25, loss = 0.05363282
Iteration 26, loss = 0.04984662
Iteration 27, loss = 0.04655605
Iteration 28, loss = 0.04360305
Iteration 29, loss = 0.04094445
Iteration 30, loss = 0.03850163
Iteration 31, loss = 0.03636976
Iteration 32, loss = 0.03451943
Iteration 33, loss = 0.03270340
Iteration 34, loss = 0.03112417
Iteration 35, loss = 0.02969783
Iteration 36, loss = 0.02858705
Iteration 37, loss = 0.02739519
Iteration 38, loss = 0.02630015
Iteration 39, loss = 0.02528344
Iteration 40, loss = 0.02417473
Iteration 41, loss = 0.02329474
Iteration 42, loss = 0.02256314
Iteration 43, loss = 0.02177964
Iteration 44, loss = 0.02104268
Iteration 45, loss = 0.02035347
Iteration 46, loss = 0.01967681
Iteration 47, loss = 0.01913133
Iteration 48, loss = 0.01857309
Iteration 49, loss = 0.01804633
Iteration 50, loss = 0.01760944
Iteration 51, loss = 0.01727473
Iteration 52, loss = 0.01689528
Iteration 53, loss = 0.01661559
Iteration 54, loss = 0.01631959
Iteration 55, loss = 0.01596082
Iteration 56, loss = 0.01546083
Iteration 57, loss = 0.01510452
Iteration 58, loss = 0.01462109
Iteration 59, loss = 0.01451894
Iteration 60, loss = 0.01424077
Iteration 61, loss = 0.01399161
Iteration 62, loss = 0.01376906
Iteration 63, loss = 0.01349608
Iteration 64, loss = 0.01326387
Iteration 65, loss = 0.01305681
Iteration 66, loss = 0.01286819
Iteration 67, loss = 0.01269978
Iteration 68, loss = 0.01250743
Iteration 69, loss = 0.01230043
Iteration 70, loss = 0.01215440
Iteration 71, loss = 0.01196901
Iteration 72, loss = 0.01185376
Iteration 73, loss = 0.01172615
Iteration 74, loss = 0.01167890
Iteration 75, loss = 0.01158329
Iteration 76, loss = 0.01149725
Iteration 77, loss = 0.01140187
Iteration 78, loss = 0.01124471
Iteration 79, loss = 0.01107218
Iteration 80, loss = 0.01087672
Iteration 81, loss = 0.01079446
Iteration 82, loss = 0.01066743
Iteration 83, loss = 0.01056144
Iteration 84, loss = 0.01051956
Iteration 85, loss = 0.01037928
Iteration 86, loss = 0.01030722
Iteration 87, loss = 0.01024531
Iteration 88, loss = 0.01014439
Iteration 89, loss = 0.01003768
Iteration 90, loss = 0.01000281
Iteration 91, loss = 0.00989996
Iteration 92, loss = 0.00987457
Iteration 93, loss = 0.00977495
Iteration 94, loss = 0.00977908
Iteration 95, loss = 0.00965995
Iteration 96, loss = 0.00956127
Iteration 97, loss = 0.00952240
Iteration 98, loss = 0.00942099
Iteration 99, loss = 0.00938226
Iteration 100, loss = 0.00931645
Iteration 101, loss = 0.00924506
Iteration 102, loss = 0.00926878
Iteration 103, loss = 0.00935601
Iteration 104, loss = 0.00926094
Iteration 105, loss = 0.00919369
Iteration 106, loss = 0.00914795
Iteration 107, loss = 0.00904067
Iteration 108, loss = 0.00898647
Iteration 109, loss = 0.00889163
Iteration 110, loss = 0.00894681
Iteration 111, loss = 0.00887257
Iteration 112, loss = 0.00885825
Iteration 113, loss = 0.00883518
Iteration 114, loss = 0.00884546
Iteration 115, loss = 0.00871314
Iteration 116, loss = 0.00869967
Iteration 117, loss = 0.00870004
Iteration 118, loss = 0.00866900
Iteration 119, loss = 0.00858059
Iteration 120, loss = 0.00854281
Iteration 121, loss = 0.00848886
Iteration 122, loss = 0.00846742
Iteration 123, loss = 0.00840814
Iteration 124, loss = 0.00844332
Iteration 125, loss = 0.00846114
Iteration 126, loss = 0.00848185
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70283194
Iteration 2, loss = 0.61479385
Iteration 3, loss = 0.49510905
Iteration 4, loss = 0.37304058
Iteration 5, loss = 0.27231471
Iteration 6, loss = 0.19900384
Iteration 7, loss = 0.14905823
Iteration 8, loss = 0.11547628
Iteration 9, loss = 0.09268557
Iteration 10, loss = 0.07679443
Iteration 11, loss = 0.06477094
Iteration 12, loss = 0.05623128
Iteration 13, loss = 0.04962549
Iteration 14, loss = 0.04448174
Iteration 15, loss = 0.04043042
Iteration 16, loss = 0.03702865
Iteration 17, loss = 0.03459777
Iteration 18, loss = 0.03232130
Iteration 19, loss = 0.03035108
Iteration 20, loss = 0.02869664
Iteration 21, loss = 0.02763849
Iteration 22, loss = 0.02622430
Iteration 23, loss = 0.02521645
Iteration 24, loss = 0.02411974
Iteration 25, loss = 0.02352603
Iteration 26, loss = 0.02278740
Iteration 27, loss = 0.02238452
Iteration 28, loss = 0.02172915
Iteration 29, loss = 0.02121878
Iteration 30, loss = 0.02078927
Iteration 31, loss = 0.02034533
Iteration 32, loss = 0.02010229
Iteration 33, loss = 0.01961008
Iteration 34, loss = 0.01924679
Iteration 35, loss = 0.01904295
Iteration 36, loss = 0.01896881
Iteration 37, loss = 0.01864644
Iteration 38, loss = 0.01840247
Iteration 39, loss = 0.01824009
Iteration 40, loss = 0.01815765
Iteration 41, loss = 0.01792718
Iteration 42, loss = 0.01766856
Iteration 43, loss = 0.01746492
Iteration 44, loss = 0.01749156
Iteration 45, loss = 0.01722655
Iteration 46, loss = 0.01718668
Iteration 47, loss = 0.01701518
Iteration 48, loss = 0.01692672
Iteration 49, loss = 0.01691423
Iteration 50, loss = 0.01686175
Iteration 51, loss = 0.01660823
Iteration 52, loss = 0.01693339
Iteration 53, loss = 0.01661559
Iteration 54, loss = 0.01652526
Iteration 55, loss = 0.01631125
Iteration 56, loss = 0.01646966
Iteration 57, loss = 0.01624952
Iteration 58, loss = 0.01625314
Iteration 59, loss = 0.01615452
Iteration 60, loss = 0.01622628
Iteration 61, loss = 0.01602003
Iteration 62, loss = 0.01618582
Iteration 63, loss = 0.01610555
Iteration 64, loss = 0.01593115
Iteration 65, loss = 0.01586335
Iteration 66, loss = 0.01587471
Iteration 67, loss = 0.01566114
Iteration 68, loss = 0.01593126
Iteration 69, loss = 0.01569915
Iteration 70, loss = 0.01576575
Iteration 71, loss = 0.01560882
Iteration 72, loss = 0.01564115
Iteration 73, loss = 0.01563027
Iteration 74, loss = 0.01557164
Iteration 75, loss = 0.01568593
Iteration 76, loss = 0.01564797
Iteration 77, loss = 0.01523420
Iteration 78, loss = 0.01536512
Iteration 79, loss = 0.01545284
Iteration 80, loss = 0.01536517
Iteration 81, loss = 0.01543644
Iteration 82, loss = 0.01514649
Iteration 83, loss = 0.01541235
Iteration 84, loss = 0.01541333
Iteration 85, loss = 0.01503921
Iteration 86, loss = 0.01530153
Iteration 87, loss = 0.01517087
Iteration 88, loss = 0.01551813
Iteration 89, loss = 0.01535427
Iteration 90, loss = 0.01518479
Iteration 91, loss = 0.01540008
Iteration 92, loss = 0.01539058
Iteration 93, loss = 0.01514955
Iteration 94, loss = 0.01529045
Iteration 95, loss = 0.01525892
Iteration 96, loss = 0.01519952
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70252360
Iteration 2, loss = 0.66634201
Iteration 3, loss = 0.62084330
Iteration 4, loss = 0.56598032
Iteration 5, loss = 0.50346093
Iteration 6, loss = 0.43823709
Iteration 7, loss = 0.37495397
Iteration 8, loss = 0.31762241
Iteration 9, loss = 0.26750389
Iteration 10, loss = 0.22505084
Iteration 11, loss = 0.18971726
Iteration 12, loss = 0.16088250
Iteration 13, loss = 0.13745714
Iteration 14, loss = 0.11857646
Iteration 15, loss = 0.10322873
Iteration 16, loss = 0.09067752
Iteration 17, loss = 0.08038712
Iteration 18, loss = 0.07186463
Iteration 19, loss = 0.06463767
Iteration 20, loss = 0.05873598
Iteration 21, loss = 0.05361605
Iteration 22, loss = 0.04923902
Iteration 23, loss = 0.04556251
Iteration 24, loss = 0.04225752
Iteration 25, loss = 0.03952978
Iteration 26, loss = 0.03701841
Iteration 27, loss = 0.03481254
Iteration 28, loss = 0.03290574
Iteration 29, loss = 0.03113994
Iteration 30, loss = 0.02968020
Iteration 31, loss = 0.02830281
Iteration 32, loss = 0.02705906
Iteration 33, loss = 0.02594964
Iteration 34, loss = 0.02486825
Iteration 35, loss = 0.02401258
Iteration 36, loss = 0.02306154
Iteration 37, loss = 0.02238498
Iteration 38, loss = 0.02166565
Iteration 39, loss = 0.02095908
Iteration 40, loss = 0.02041986
Iteration 41, loss = 0.01981255
Iteration 42, loss = 0.01927046
Iteration 43, loss = 0.01891052
Iteration 44, loss = 0.01840241
Iteration 45, loss = 0.01794260
Iteration 46, loss = 0.01772453
Iteration 47, loss = 0.01726904
Iteration 48, loss = 0.01691892
Iteration 49, loss = 0.01666875
Iteration 50, loss = 0.01643067
Iteration 51, loss = 0.01600905
Iteration 52, loss = 0.01579554
Iteration 53, loss = 0.01557826
Iteration 54, loss = 0.01528305
Iteration 55, loss = 0.01518343
Iteration 56, loss = 0.01494326
Iteration 57, loss = 0.01479700
Iteration 58, loss = 0.01447105
Iteration 59, loss = 0.01434464
Iteration 60, loss = 0.01420491
Iteration 61, loss = 0.01411968
Iteration 62, loss = 0.01395284
Iteration 63, loss = 0.01380608
Iteration 64, loss = 0.01361849
Iteration 65, loss = 0.01347151
Iteration 66, loss = 0.01344464
Iteration 67, loss = 0.01325280
Iteration 68, loss = 0.01316353
Iteration 69, loss = 0.01304133
Iteration 70, loss = 0.01289670
Iteration 71, loss = 0.01282068
Iteration 72, loss = 0.01274689
Iteration 73, loss = 0.01267859
Iteration 74, loss = 0.01255270
Iteration 75, loss = 0.01258335
Iteration 76, loss = 0.01243926
Iteration 77, loss = 0.01236557
Iteration 78, loss = 0.01227315
Iteration 79, loss = 0.01223712
Iteration 80, loss = 0.01215694
Iteration 81, loss = 0.01215082
Iteration 82, loss = 0.01215675
Iteration 83, loss = 0.01202117
Iteration 84, loss = 0.01199309
Iteration 85, loss = 0.01184142
Iteration 86, loss = 0.01190485
Iteration 87, loss = 0.01176293
Iteration 88, loss = 0.01168246
Iteration 89, loss = 0.01168231
Iteration 90, loss = 0.01161282
Iteration 91, loss = 0.01158324
Iteration 92, loss = 0.01156094
Iteration 93, loss = 0.01151565
Iteration 94, loss = 0.01140723
Iteration 95, loss = 0.01144010
Iteration 96, loss = 0.01146546
Iteration 97, loss = 0.01144227
Iteration 98, loss = 0.01135649
Iteration 99, loss = 0.01136294
Iteration 100, loss = 0.01115883
Iteration 101, loss = 0.01128262
Iteration 102, loss = 0.01131347
Iteration 103, loss = 0.01115308
Iteration 104, loss = 0.01126998
Iteration 105, loss = 0.01107792
Iteration 106, loss = 0.01104969
Iteration 107, loss = 0.01108257
Iteration 108, loss = 0.01107363
Iteration 109, loss = 0.01103992
Iteration 110, loss = 0.01098566
Iteration 111, loss = 0.01101108
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67210574
Iteration 2, loss = 0.58242122
Iteration 3, loss = 0.45877808
Iteration 4, loss = 0.34049793
Iteration 5, loss = 0.24772132
Iteration 6, loss = 0.18163352
Iteration 7, loss = 0.13716239
Iteration 8, loss = 0.10694459
Iteration 9, loss = 0.08651074
Iteration 10, loss = 0.07177626
Iteration 11, loss = 0.06149440
Iteration 12, loss = 0.05341757
Iteration 13, loss = 0.04724983
Iteration 14, loss = 0.04255005
Iteration 15, loss = 0.03859670
Iteration 16, loss = 0.03581366
Iteration 17, loss = 0.03311782
Iteration 18, loss = 0.03124623
Iteration 19, loss = 0.02923158
Iteration 20, loss = 0.02774650
Iteration 21, loss = 0.02637964
Iteration 22, loss = 0.02545060
Iteration 23, loss = 0.02429348
Iteration 24, loss = 0.02357098
Iteration 25, loss = 0.02287786
Iteration 26, loss = 0.02220162
Iteration 27, loss = 0.02153411
Iteration 28, loss = 0.02098950
Iteration 29, loss = 0.02055648
Iteration 30, loss = 0.02007058
Iteration 31, loss = 0.01986836
Iteration 32, loss = 0.01950949
Iteration 33, loss = 0.01919271
Iteration 34, loss = 0.01876017
Iteration 35, loss = 0.01852782
Iteration 36, loss = 0.01829260
Iteration 37, loss = 0.01801087
Iteration 38, loss = 0.01788494
Iteration 39, loss = 0.01768518
Iteration 40, loss = 0.01758334
Iteration 41, loss = 0.01738984
Iteration 42, loss = 0.01733511
Iteration 43, loss = 0.01717825
Iteration 44, loss = 0.01696318
Iteration 45, loss = 0.01724032
Iteration 46, loss = 0.01665462
Iteration 47, loss = 0.01651796
Iteration 48, loss = 0.01672477
Iteration 49, loss = 0.01640376
Iteration 50, loss = 0.01631206
Iteration 51, loss = 0.01654620
Iteration 52, loss = 0.01630485
Iteration 53, loss = 0.01606856
Iteration 54, loss = 0.01610028
Iteration 55, loss = 0.01598672
Iteration 56, loss = 0.01587612
Iteration 57, loss = 0.01620547
Iteration 58, loss = 0.01582045
Iteration 59, loss = 0.01586284
Iteration 60, loss = 0.01596050
Iteration 61, loss = 0.01568207
Iteration 62, loss = 0.01550570
Iteration 63, loss = 0.01542779
Iteration 64, loss = 0.01549726
Iteration 65, loss = 0.01580075
Iteration 66, loss = 0.01533378
Iteration 67, loss = 0.01537021
Iteration 68, loss = 0.01555008
Iteration 69, loss = 0.01518526
Iteration 70, loss = 0.01545578
Iteration 71, loss = 0.01538408
Iteration 72, loss = 0.01531390
Iteration 73, loss = 0.01525102
Iteration 74, loss = 0.01535652
Iteration 75, loss = 0.01489623
Iteration 76, loss = 0.01532638
Iteration 77, loss = 0.01505051
Iteration 78, loss = 0.01509722
Iteration 79, loss = 0.01499458
Iteration 80, loss = 0.01497361
Iteration 81, loss = 0.01507862
Iteration 82, loss = 0.01488718
Iteration 83, loss = 0.01475382
Iteration 84, loss = 0.01491392
Iteration 85, loss = 0.01482414
Iteration 86, loss = 0.01486356
Iteration 87, loss = 0.01509990
Iteration 88, loss = 0.01457501
Iteration 89, loss = 0.01480815
Iteration 90, loss = 0.01466912
Iteration 91, loss = 0.01468666
Iteration 92, loss = 0.01481696
Iteration 93, loss = 0.01464642
Iteration 94, loss = 0.01471708
Iteration 95, loss = 0.01461251
Iteration 96, loss = 0.01454446
Iteration 97, loss = 0.01464864
Iteration 98, loss = 0.01487074
Iteration 99, loss = 0.01458544
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	101m16.696s
user	271m44.938s
sys	29m31.361s
Elapsed time: 6077 seconds
