Processing dataset: kept_v1_basic_clean

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: kept_v1_basic_clean (7443 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on kept_v1_basic_clean...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on kept_v1_basic_clean...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on kept_v1_basic_clean...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on kept_v1_basic_clean...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on kept_v1_basic_clean...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67301457
Iteration 2, loss = 0.59294869
Iteration 3, loss = 0.48380838
Iteration 4, loss = 0.37390509
Iteration 5, loss = 0.27964018
Iteration 6, loss = 0.20886867
Iteration 7, loss = 0.15949077
Iteration 8, loss = 0.12541550
Iteration 9, loss = 0.10243025
Iteration 10, loss = 0.08581465
Iteration 11, loss = 0.07393566
Iteration 12, loss = 0.06529597
Iteration 13, loss = 0.05846527
Iteration 14, loss = 0.05336301
Iteration 15, loss = 0.04917130
Iteration 16, loss = 0.04611188
Iteration 17, loss = 0.04336739
Iteration 18, loss = 0.04099216
Iteration 19, loss = 0.03931029
Iteration 20, loss = 0.03777705
Iteration 21, loss = 0.03638582
Iteration 22, loss = 0.03513823
Iteration 23, loss = 0.03411866
Iteration 24, loss = 0.03352209
Iteration 25, loss = 0.03260841
Iteration 26, loss = 0.03189861
Iteration 27, loss = 0.03130635
Iteration 28, loss = 0.03079600
Iteration 29, loss = 0.03041843
Iteration 30, loss = 0.03027502
Iteration 31, loss = 0.02933829
Iteration 32, loss = 0.02927217
Iteration 33, loss = 0.02896592
Iteration 34, loss = 0.02861857
Iteration 35, loss = 0.02849935
Iteration 36, loss = 0.02786009
Iteration 37, loss = 0.02817430
Iteration 38, loss = 0.02756029
Iteration 39, loss = 0.02731087
Iteration 40, loss = 0.02720277
Iteration 41, loss = 0.02759957
Iteration 42, loss = 0.02711892
Iteration 43, loss = 0.02683373
Iteration 44, loss = 0.02697866
Iteration 45, loss = 0.02687741
Iteration 46, loss = 0.02667018
Iteration 47, loss = 0.02681141
Iteration 48, loss = 0.02648713
Iteration 49, loss = 0.02637454
Iteration 50, loss = 0.02625416
Iteration 51, loss = 0.02638049
Iteration 52, loss = 0.02631891
Iteration 53, loss = 0.02597281
Iteration 54, loss = 0.02607211
Iteration 55, loss = 0.02594656
Iteration 56, loss = 0.02609063
Iteration 57, loss = 0.02568360
Iteration 58, loss = 0.02579283
Iteration 59, loss = 0.02570801
Iteration 60, loss = 0.02577764
Iteration 61, loss = 0.02548601
Iteration 62, loss = 0.02558817
Iteration 63, loss = 0.02558056
Iteration 64, loss = 0.02543105
Iteration 65, loss = 0.02511091
Iteration 66, loss = 0.02561025
Iteration 67, loss = 0.02535393
Iteration 68, loss = 0.02557394
Iteration 69, loss = 0.02538509
Iteration 70, loss = 0.02511628
Iteration 71, loss = 0.02538295
Iteration 72, loss = 0.02571976
Iteration 73, loss = 0.02528826
Iteration 74, loss = 0.02548334
Iteration 75, loss = 0.02503879
Iteration 76, loss = 0.02501401
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67540125
Iteration 2, loss = 0.60588733
Iteration 3, loss = 0.49804105
Iteration 4, loss = 0.38361889
Iteration 5, loss = 0.28529477
Iteration 6, loss = 0.21166704
Iteration 7, loss = 0.16031060
Iteration 8, loss = 0.12496822
Iteration 9, loss = 0.10122286
Iteration 10, loss = 0.08444646
Iteration 11, loss = 0.07232207
Iteration 12, loss = 0.06320474
Iteration 13, loss = 0.05616193
Iteration 14, loss = 0.05099272
Iteration 15, loss = 0.04680442
Iteration 16, loss = 0.04311685
Iteration 17, loss = 0.04051236
Iteration 18, loss = 0.03820940
Iteration 19, loss = 0.03619695
Iteration 20, loss = 0.03458297
Iteration 21, loss = 0.03328806
Iteration 22, loss = 0.03205111
Iteration 23, loss = 0.03112215
Iteration 24, loss = 0.03029634
Iteration 25, loss = 0.02928244
Iteration 26, loss = 0.02862104
Iteration 27, loss = 0.02788770
Iteration 28, loss = 0.02750188
Iteration 29, loss = 0.02706053
Iteration 30, loss = 0.02651511
Iteration 31, loss = 0.02627483
Iteration 32, loss = 0.02566734
Iteration 33, loss = 0.02549356
Iteration 34, loss = 0.02504535
Iteration 35, loss = 0.02500005
Iteration 36, loss = 0.02474063
Iteration 37, loss = 0.02423893
Iteration 38, loss = 0.02416622
Iteration 39, loss = 0.02408293
Iteration 40, loss = 0.02402309
Iteration 41, loss = 0.02386237
Iteration 42, loss = 0.02362903
Iteration 43, loss = 0.02363020
Iteration 44, loss = 0.02340314
Iteration 45, loss = 0.02334599
Iteration 46, loss = 0.02308337
Iteration 47, loss = 0.02312419
Iteration 48, loss = 0.02278380
Iteration 49, loss = 0.02276236
Iteration 50, loss = 0.02276741
Iteration 51, loss = 0.02278877
Iteration 52, loss = 0.02257513
Iteration 53, loss = 0.02262529
Iteration 54, loss = 0.02249726
Iteration 55, loss = 0.02223693
Iteration 56, loss = 0.02209918
Iteration 57, loss = 0.02216879
Iteration 58, loss = 0.02212315
Iteration 59, loss = 0.02227433
Iteration 60, loss = 0.02191901
Iteration 61, loss = 0.02231602
Iteration 62, loss = 0.02198305
Iteration 63, loss = 0.02182060
Iteration 64, loss = 0.02205471
Iteration 65, loss = 0.02176113
Iteration 66, loss = 0.02191508
Iteration 67, loss = 0.02188129
Iteration 68, loss = 0.02228660
Iteration 69, loss = 0.02167447
Iteration 70, loss = 0.02150622
Iteration 71, loss = 0.02156941
Iteration 72, loss = 0.02206715
Iteration 73, loss = 0.02143149
Iteration 74, loss = 0.02172549
Iteration 75, loss = 0.02147733
Iteration 76, loss = 0.02149080
Iteration 77, loss = 0.02155206
Iteration 78, loss = 0.02162030
Iteration 79, loss = 0.02155368
Iteration 80, loss = 0.02154874
Iteration 81, loss = 0.02151954
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68134263
Iteration 2, loss = 0.61496804
Iteration 3, loss = 0.51146277
Iteration 4, loss = 0.39566436
Iteration 5, loss = 0.29211371
Iteration 6, loss = 0.21417480
Iteration 7, loss = 0.16049566
Iteration 8, loss = 0.12428578
Iteration 9, loss = 0.09967792
Iteration 10, loss = 0.08275651
Iteration 11, loss = 0.07046838
Iteration 12, loss = 0.06142305
Iteration 13, loss = 0.05438715
Iteration 14, loss = 0.04910212
Iteration 15, loss = 0.04473097
Iteration 16, loss = 0.04142103
Iteration 17, loss = 0.03855631
Iteration 18, loss = 0.03640709
Iteration 19, loss = 0.03448024
Iteration 20, loss = 0.03285733
Iteration 21, loss = 0.03148785
Iteration 22, loss = 0.03034194
Iteration 23, loss = 0.02934722
Iteration 24, loss = 0.02832008
Iteration 25, loss = 0.02766128
Iteration 26, loss = 0.02685755
Iteration 27, loss = 0.02648322
Iteration 28, loss = 0.02614346
Iteration 29, loss = 0.02542989
Iteration 30, loss = 0.02494697
Iteration 31, loss = 0.02457840
Iteration 32, loss = 0.02436233
Iteration 33, loss = 0.02395478
Iteration 34, loss = 0.02359962
Iteration 35, loss = 0.02321073
Iteration 36, loss = 0.02321315
Iteration 37, loss = 0.02288885
Iteration 38, loss = 0.02263937
Iteration 39, loss = 0.02259299
Iteration 40, loss = 0.02236401
Iteration 41, loss = 0.02225987
Iteration 42, loss = 0.02207283
Iteration 43, loss = 0.02194123
Iteration 44, loss = 0.02220069
Iteration 45, loss = 0.02155928
Iteration 46, loss = 0.02146743
Iteration 47, loss = 0.02164056
Iteration 48, loss = 0.02143074
Iteration 49, loss = 0.02126004
Iteration 50, loss = 0.02145423
Iteration 51, loss = 0.02139320
Iteration 52, loss = 0.02141171
Iteration 53, loss = 0.02100361
Iteration 54, loss = 0.02093942
Iteration 55, loss = 0.02099644
Iteration 56, loss = 0.02080333
Iteration 57, loss = 0.02094593
Iteration 58, loss = 0.02069266
Iteration 59, loss = 0.02064287
Iteration 60, loss = 0.02084680
Iteration 61, loss = 0.02062517
Iteration 62, loss = 0.02063663
Iteration 63, loss = 0.02051546
Iteration 64, loss = 0.02067182
Iteration 65, loss = 0.02045651
Iteration 66, loss = 0.02042034
Iteration 67, loss = 0.02051760
Iteration 68, loss = 0.02058945
Iteration 69, loss = 0.02033777
Iteration 70, loss = 0.02020539
Iteration 71, loss = 0.02029938
Iteration 72, loss = 0.02039891
Iteration 73, loss = 0.02011663
Iteration 74, loss = 0.02011475
Iteration 75, loss = 0.02001308
Iteration 76, loss = 0.02006477
Iteration 77, loss = 0.02023980
Iteration 78, loss = 0.02026393
Iteration 79, loss = 0.02002016
Iteration 80, loss = 0.02019522
Iteration 81, loss = 0.02026172
Iteration 82, loss = 0.02008516
Iteration 83, loss = 0.02022474
Iteration 84, loss = 0.02022529
Iteration 85, loss = 0.01993271
Iteration 86, loss = 0.01992061
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67579529
Iteration 2, loss = 0.59333749
Iteration 3, loss = 0.47927595
Iteration 4, loss = 0.36666526
Iteration 5, loss = 0.27392344
Iteration 6, loss = 0.20464836
Iteration 7, loss = 0.15669945
Iteration 8, loss = 0.12363202
Iteration 9, loss = 0.10085375
Iteration 10, loss = 0.08469038
Iteration 11, loss = 0.07295615
Iteration 12, loss = 0.06426284
Iteration 13, loss = 0.05756000
Iteration 14, loss = 0.05253590
Iteration 15, loss = 0.04843280
Iteration 16, loss = 0.04517777
Iteration 17, loss = 0.04265157
Iteration 18, loss = 0.04049958
Iteration 19, loss = 0.03826698
Iteration 20, loss = 0.03672490
Iteration 21, loss = 0.03515097
Iteration 22, loss = 0.03409417
Iteration 23, loss = 0.03319547
Iteration 24, loss = 0.03236709
Iteration 25, loss = 0.03178798
Iteration 26, loss = 0.03075705
Iteration 27, loss = 0.03013902
Iteration 28, loss = 0.02961766
Iteration 29, loss = 0.02939476
Iteration 30, loss = 0.02884447
Iteration 31, loss = 0.02838264
Iteration 32, loss = 0.02803284
Iteration 33, loss = 0.02789461
Iteration 34, loss = 0.02744924
Iteration 35, loss = 0.02729072
Iteration 36, loss = 0.02705768
Iteration 37, loss = 0.02679047
Iteration 38, loss = 0.02653961
Iteration 39, loss = 0.02634715
Iteration 40, loss = 0.02602207
Iteration 41, loss = 0.02625071
Iteration 42, loss = 0.02574458
Iteration 43, loss = 0.02581033
Iteration 44, loss = 0.02546405
Iteration 45, loss = 0.02560433
Iteration 46, loss = 0.02547520
Iteration 47, loss = 0.02545966
Iteration 48, loss = 0.02496472
Iteration 49, loss = 0.02523686
Iteration 50, loss = 0.02524730
Iteration 51, loss = 0.02507563
Iteration 52, loss = 0.02482678
Iteration 53, loss = 0.02518086
Iteration 54, loss = 0.02475621
Iteration 55, loss = 0.02464585
Iteration 56, loss = 0.02460130
Iteration 57, loss = 0.02459577
Iteration 58, loss = 0.02457880
Iteration 59, loss = 0.02446815
Iteration 60, loss = 0.02446061
Iteration 61, loss = 0.02450362
Iteration 62, loss = 0.02440240
Iteration 63, loss = 0.02428898
Iteration 64, loss = 0.02432459
Iteration 65, loss = 0.02428118
Iteration 66, loss = 0.02419277
Iteration 67, loss = 0.02420254
Iteration 68, loss = 0.02406114
Iteration 69, loss = 0.02430474
Iteration 70, loss = 0.02435557
Iteration 71, loss = 0.02406029
Iteration 72, loss = 0.02394655
Iteration 73, loss = 0.02389255
Iteration 74, loss = 0.02380617
Iteration 75, loss = 0.02419326
Iteration 76, loss = 0.02458896
Iteration 77, loss = 0.02401509
Iteration 78, loss = 0.02384062
Iteration 79, loss = 0.02386371
Iteration 80, loss = 0.02376229
Iteration 81, loss = 0.02394163
Iteration 82, loss = 0.02411765
Iteration 83, loss = 0.02389962
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67451169
Iteration 2, loss = 0.60649750
Iteration 3, loss = 0.50658436
Iteration 4, loss = 0.39797833
Iteration 5, loss = 0.29908854
Iteration 6, loss = 0.22263775
Iteration 7, loss = 0.16833666
Iteration 8, loss = 0.13105323
Iteration 9, loss = 0.10545973
Iteration 10, loss = 0.08767779
Iteration 11, loss = 0.07454614
Iteration 12, loss = 0.06508619
Iteration 13, loss = 0.05812204
Iteration 14, loss = 0.05264141
Iteration 15, loss = 0.04850502
Iteration 16, loss = 0.04483546
Iteration 17, loss = 0.04203934
Iteration 18, loss = 0.03971431
Iteration 19, loss = 0.03767246
Iteration 20, loss = 0.03615102
Iteration 21, loss = 0.03470217
Iteration 22, loss = 0.03360963
Iteration 23, loss = 0.03267579
Iteration 24, loss = 0.03163007
Iteration 25, loss = 0.03098266
Iteration 26, loss = 0.03047148
Iteration 27, loss = 0.02949980
Iteration 28, loss = 0.02951890
Iteration 29, loss = 0.02857369
Iteration 30, loss = 0.02802328
Iteration 31, loss = 0.02786315
Iteration 32, loss = 0.02767163
Iteration 33, loss = 0.02744225
Iteration 34, loss = 0.02699856
Iteration 35, loss = 0.02663217
Iteration 36, loss = 0.02637564
Iteration 37, loss = 0.02620085
Iteration 38, loss = 0.02585831
Iteration 39, loss = 0.02573682
Iteration 40, loss = 0.02578650
Iteration 41, loss = 0.02569447
Iteration 42, loss = 0.02528988
Iteration 43, loss = 0.02539126
Iteration 44, loss = 0.02514072
Iteration 45, loss = 0.02535301
Iteration 46, loss = 0.02508810
Iteration 47, loss = 0.02476474
Iteration 48, loss = 0.02462787
Iteration 49, loss = 0.02460252
Iteration 50, loss = 0.02445715
Iteration 51, loss = 0.02433323
Iteration 52, loss = 0.02454587
Iteration 53, loss = 0.02446561
Iteration 54, loss = 0.02405467
Iteration 55, loss = 0.02413677
Iteration 56, loss = 0.02432013
Iteration 57, loss = 0.02404795
Iteration 58, loss = 0.02409611
Iteration 59, loss = 0.02421190
Iteration 60, loss = 0.02399439
Iteration 61, loss = 0.02373344
Iteration 62, loss = 0.02361419
Iteration 63, loss = 0.02401881
Iteration 64, loss = 0.02381413
Iteration 65, loss = 0.02393842
Iteration 66, loss = 0.02365276
Iteration 67, loss = 0.02359103
Iteration 68, loss = 0.02356156
Iteration 69, loss = 0.02348212
Iteration 70, loss = 0.02356454
Iteration 71, loss = 0.02377725
Iteration 72, loss = 0.02361686
Iteration 73, loss = 0.02374325
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67301457
Iteration 2, loss = 0.59294869
Iteration 3, loss = 0.48380838
Iteration 4, loss = 0.37390509
Iteration 5, loss = 0.27964018
Iteration 6, loss = 0.20886867
Iteration 7, loss = 0.15949077
Iteration 8, loss = 0.12541550
Iteration 9, loss = 0.10243025
Iteration 10, loss = 0.08581465
Iteration 11, loss = 0.07393566
Iteration 12, loss = 0.06529597
Iteration 13, loss = 0.05846527
Iteration 14, loss = 0.05336301
Iteration 15, loss = 0.04917130
Iteration 16, loss = 0.04611188
Iteration 17, loss = 0.04336739
Iteration 18, loss = 0.04099216
Iteration 19, loss = 0.03931029
Iteration 20, loss = 0.03777705
Iteration 21, loss = 0.03638582
Iteration 22, loss = 0.03513823
Iteration 23, loss = 0.03411866
Iteration 24, loss = 0.03352209
Iteration 25, loss = 0.03260841
Iteration 26, loss = 0.03189861
Iteration 27, loss = 0.03130635
Iteration 28, loss = 0.03079600
Iteration 29, loss = 0.03041843
Iteration 30, loss = 0.03027502
Iteration 31, loss = 0.02933829
Iteration 32, loss = 0.02927217
Iteration 33, loss = 0.02896592
Iteration 34, loss = 0.02861857
Iteration 35, loss = 0.02849935
Iteration 36, loss = 0.02786009
Iteration 37, loss = 0.02817430
Iteration 38, loss = 0.02756029
Iteration 39, loss = 0.02731087
Iteration 40, loss = 0.02720277
Iteration 41, loss = 0.02759957
Iteration 42, loss = 0.02711892
Iteration 43, loss = 0.02683373
Iteration 44, loss = 0.02697866
Iteration 45, loss = 0.02687741
Iteration 46, loss = 0.02667018
Iteration 47, loss = 0.02681141
Iteration 48, loss = 0.02648713
Iteration 49, loss = 0.02637454
Iteration 50, loss = 0.02625416
Iteration 51, loss = 0.02638049
Iteration 52, loss = 0.02631891
Iteration 53, loss = 0.02597281
Iteration 54, loss = 0.02607211
Iteration 55, loss = 0.02594656
Iteration 56, loss = 0.02609063
Iteration 57, loss = 0.02568360
Iteration 58, loss = 0.02579283
Iteration 59, loss = 0.02570801
Iteration 60, loss = 0.02577764
Iteration 61, loss = 0.02548601
Iteration 62, loss = 0.02558817
Iteration 63, loss = 0.02558056
Iteration 64, loss = 0.02543105
Iteration 65, loss = 0.02511091
Iteration 66, loss = 0.02561025
Iteration 67, loss = 0.02535393
Iteration 68, loss = 0.02557394
Iteration 69, loss = 0.02538509
Iteration 70, loss = 0.02511628
Iteration 71, loss = 0.02538295
Iteration 72, loss = 0.02571976
Iteration 73, loss = 0.02528826
Iteration 74, loss = 0.02548334
Iteration 75, loss = 0.02503879
Iteration 76, loss = 0.02501401
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67540125
Iteration 2, loss = 0.60588733
Iteration 3, loss = 0.49804105
Iteration 4, loss = 0.38361889
Iteration 5, loss = 0.28529477
Iteration 6, loss = 0.21166704
Iteration 7, loss = 0.16031060
Iteration 8, loss = 0.12496822
Iteration 9, loss = 0.10122286
Iteration 10, loss = 0.08444646
Iteration 11, loss = 0.07232207
Iteration 12, loss = 0.06320474
Iteration 13, loss = 0.05616193
Iteration 14, loss = 0.05099272
Iteration 15, loss = 0.04680442
Iteration 16, loss = 0.04311685
Iteration 17, loss = 0.04051236
Iteration 18, loss = 0.03820940
Iteration 19, loss = 0.03619695
Iteration 20, loss = 0.03458297
Iteration 21, loss = 0.03328806
Iteration 22, loss = 0.03205111
Iteration 23, loss = 0.03112215
Iteration 24, loss = 0.03029634
Iteration 25, loss = 0.02928244
Iteration 26, loss = 0.02862104
Iteration 27, loss = 0.02788770
Iteration 28, loss = 0.02750188
Iteration 29, loss = 0.02706053
Iteration 30, loss = 0.02651511
Iteration 31, loss = 0.02627483
Iteration 32, loss = 0.02566734
Iteration 33, loss = 0.02549356
Iteration 34, loss = 0.02504535
Iteration 35, loss = 0.02500005
Iteration 36, loss = 0.02474063
Iteration 37, loss = 0.02423893
Iteration 38, loss = 0.02416622
Iteration 39, loss = 0.02408293
Iteration 40, loss = 0.02402309
Iteration 41, loss = 0.02386237
Iteration 42, loss = 0.02362903
Iteration 43, loss = 0.02363020
Iteration 44, loss = 0.02340314
Iteration 45, loss = 0.02334599
Iteration 46, loss = 0.02308337
Iteration 47, loss = 0.02312419
Iteration 48, loss = 0.02278380
Iteration 49, loss = 0.02276236
Iteration 50, loss = 0.02276741
Iteration 51, loss = 0.02278877
Iteration 52, loss = 0.02257513
Iteration 53, loss = 0.02262529
Iteration 54, loss = 0.02249726
Iteration 55, loss = 0.02223693
Iteration 56, loss = 0.02209918
Iteration 57, loss = 0.02216879
Iteration 58, loss = 0.02212315
Iteration 59, loss = 0.02227433
Iteration 60, loss = 0.02191901
Iteration 61, loss = 0.02231602
Iteration 62, loss = 0.02198305
Iteration 63, loss = 0.02182060
Iteration 64, loss = 0.02205471
Iteration 65, loss = 0.02176113
Iteration 66, loss = 0.02191508
Iteration 67, loss = 0.02188129
Iteration 68, loss = 0.02228660
Iteration 69, loss = 0.02167447
Iteration 70, loss = 0.02150622
Iteration 71, loss = 0.02156941
Iteration 72, loss = 0.02206715
Iteration 73, loss = 0.02143149
Iteration 74, loss = 0.02172549
Iteration 75, loss = 0.02147733
Iteration 76, loss = 0.02149080
Iteration 77, loss = 0.02155206
Iteration 78, loss = 0.02162030
Iteration 79, loss = 0.02155368
Iteration 80, loss = 0.02154874
Iteration 81, loss = 0.02151954
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68134263
Iteration 2, loss = 0.61496804
Iteration 3, loss = 0.51146277
Iteration 4, loss = 0.39566436
Iteration 5, loss = 0.29211371
Iteration 6, loss = 0.21417480
Iteration 7, loss = 0.16049566
Iteration 8, loss = 0.12428578
Iteration 9, loss = 0.09967792
Iteration 10, loss = 0.08275651
Iteration 11, loss = 0.07046838
Iteration 12, loss = 0.06142305
Iteration 13, loss = 0.05438715
Iteration 14, loss = 0.04910212
Iteration 15, loss = 0.04473097
Iteration 16, loss = 0.04142103
Iteration 17, loss = 0.03855631
Iteration 18, loss = 0.03640709
Iteration 19, loss = 0.03448024
Iteration 20, loss = 0.03285733
Iteration 21, loss = 0.03148785
Iteration 22, loss = 0.03034194
Iteration 23, loss = 0.02934722
Iteration 24, loss = 0.02832008
Iteration 25, loss = 0.02766128
Iteration 26, loss = 0.02685755
Iteration 27, loss = 0.02648322
Iteration 28, loss = 0.02614346
Iteration 29, loss = 0.02542989
Iteration 30, loss = 0.02494697
Iteration 31, loss = 0.02457840
Iteration 32, loss = 0.02436233
Iteration 33, loss = 0.02395478
Iteration 34, loss = 0.02359962
Iteration 35, loss = 0.02321073
Iteration 36, loss = 0.02321315
Iteration 37, loss = 0.02288885
Iteration 38, loss = 0.02263937
Iteration 39, loss = 0.02259299
Iteration 40, loss = 0.02236401
Iteration 41, loss = 0.02225987
Iteration 42, loss = 0.02207283
Iteration 43, loss = 0.02194123
Iteration 44, loss = 0.02220069
Iteration 45, loss = 0.02155928
Iteration 46, loss = 0.02146743
Iteration 47, loss = 0.02164056
Iteration 48, loss = 0.02143074
Iteration 49, loss = 0.02126004
Iteration 50, loss = 0.02145423
Iteration 51, loss = 0.02139320
Iteration 52, loss = 0.02141171
Iteration 53, loss = 0.02100361
Iteration 54, loss = 0.02093942
Iteration 55, loss = 0.02099644
Iteration 56, loss = 0.02080333
Iteration 57, loss = 0.02094593
Iteration 58, loss = 0.02069266
Iteration 59, loss = 0.02064287
Iteration 60, loss = 0.02084680
Iteration 61, loss = 0.02062517
Iteration 62, loss = 0.02063663
Iteration 63, loss = 0.02051546
Iteration 64, loss = 0.02067182
Iteration 65, loss = 0.02045651
Iteration 66, loss = 0.02042034
Iteration 67, loss = 0.02051760
Iteration 68, loss = 0.02058945
Iteration 69, loss = 0.02033777
Iteration 70, loss = 0.02020539
Iteration 71, loss = 0.02029938
Iteration 72, loss = 0.02039891
Iteration 73, loss = 0.02011663
Iteration 74, loss = 0.02011475
Iteration 75, loss = 0.02001308
Iteration 76, loss = 0.02006477
Iteration 77, loss = 0.02023980
Iteration 78, loss = 0.02026393
Iteration 79, loss = 0.02002016
Iteration 80, loss = 0.02019522
Iteration 81, loss = 0.02026172
Iteration 82, loss = 0.02008516
Iteration 83, loss = 0.02022474
Iteration 84, loss = 0.02022529
Iteration 85, loss = 0.01993271
Iteration 86, loss = 0.01992061
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67579529
Iteration 2, loss = 0.59333749
Iteration 3, loss = 0.47927595
Iteration 4, loss = 0.36666526
Iteration 5, loss = 0.27392344
Iteration 6, loss = 0.20464836
Iteration 7, loss = 0.15669945
Iteration 8, loss = 0.12363202
Iteration 9, loss = 0.10085375
Iteration 10, loss = 0.08469038
Iteration 11, loss = 0.07295615
Iteration 12, loss = 0.06426284
Iteration 13, loss = 0.05756000
Iteration 14, loss = 0.05253590
Iteration 15, loss = 0.04843280
Iteration 16, loss = 0.04517777
Iteration 17, loss = 0.04265157
Iteration 18, loss = 0.04049958
Iteration 19, loss = 0.03826698
Iteration 20, loss = 0.03672490
Iteration 21, loss = 0.03515097
Iteration 22, loss = 0.03409417
Iteration 23, loss = 0.03319547
Iteration 24, loss = 0.03236709
Iteration 25, loss = 0.03178798
Iteration 26, loss = 0.03075705
Iteration 27, loss = 0.03013902
Iteration 28, loss = 0.02961766
Iteration 29, loss = 0.02939476
Iteration 30, loss = 0.02884447
Iteration 31, loss = 0.02838264
Iteration 32, loss = 0.02803284
Iteration 33, loss = 0.02789461
Iteration 34, loss = 0.02744924
Iteration 35, loss = 0.02729072
Iteration 36, loss = 0.02705768
Iteration 37, loss = 0.02679047
Iteration 38, loss = 0.02653961
Iteration 39, loss = 0.02634715
Iteration 40, loss = 0.02602207
Iteration 41, loss = 0.02625071
Iteration 42, loss = 0.02574458
Iteration 43, loss = 0.02581033
Iteration 44, loss = 0.02546405
Iteration 45, loss = 0.02560433
Iteration 46, loss = 0.02547520
Iteration 47, loss = 0.02545966
Iteration 48, loss = 0.02496472
Iteration 49, loss = 0.02523686
Iteration 50, loss = 0.02524730
Iteration 51, loss = 0.02507563
Iteration 52, loss = 0.02482678
Iteration 53, loss = 0.02518086
Iteration 54, loss = 0.02475621
Iteration 55, loss = 0.02464585
Iteration 56, loss = 0.02460130
Iteration 57, loss = 0.02459577
Iteration 58, loss = 0.02457880
Iteration 59, loss = 0.02446815
Iteration 60, loss = 0.02446061
Iteration 61, loss = 0.02450362
Iteration 62, loss = 0.02440240
Iteration 63, loss = 0.02428898
Iteration 64, loss = 0.02432459
Iteration 65, loss = 0.02428118
Iteration 66, loss = 0.02419277
Iteration 67, loss = 0.02420254
Iteration 68, loss = 0.02406114
Iteration 69, loss = 0.02430474
Iteration 70, loss = 0.02435557
Iteration 71, loss = 0.02406029
Iteration 72, loss = 0.02394655
Iteration 73, loss = 0.02389255
Iteration 74, loss = 0.02380617
Iteration 75, loss = 0.02419326
Iteration 76, loss = 0.02458896
Iteration 77, loss = 0.02401509
Iteration 78, loss = 0.02384062
Iteration 79, loss = 0.02386371
Iteration 80, loss = 0.02376229
Iteration 81, loss = 0.02394163
Iteration 82, loss = 0.02411765
Iteration 83, loss = 0.02389962
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67451169
Iteration 2, loss = 0.60649750
Iteration 3, loss = 0.50658436
Iteration 4, loss = 0.39797833
Iteration 5, loss = 0.29908854
Iteration 6, loss = 0.22263775
Iteration 7, loss = 0.16833666
Iteration 8, loss = 0.13105323
Iteration 9, loss = 0.10545973
Iteration 10, loss = 0.08767779
Iteration 11, loss = 0.07454614
Iteration 12, loss = 0.06508619
Iteration 13, loss = 0.05812204
Iteration 14, loss = 0.05264141
Iteration 15, loss = 0.04850502
Iteration 16, loss = 0.04483546
Iteration 17, loss = 0.04203934
Iteration 18, loss = 0.03971431
Iteration 19, loss = 0.03767246
Iteration 20, loss = 0.03615102
Iteration 21, loss = 0.03470217
Iteration 22, loss = 0.03360963
Iteration 23, loss = 0.03267579
Iteration 24, loss = 0.03163007
Iteration 25, loss = 0.03098266
Iteration 26, loss = 0.03047148
Iteration 27, loss = 0.02949980
Iteration 28, loss = 0.02951890
Iteration 29, loss = 0.02857369
Iteration 30, loss = 0.02802328
Iteration 31, loss = 0.02786315
Iteration 32, loss = 0.02767163
Iteration 33, loss = 0.02744225
Iteration 34, loss = 0.02699856
Iteration 35, loss = 0.02663217
Iteration 36, loss = 0.02637564
Iteration 37, loss = 0.02620085
Iteration 38, loss = 0.02585831
Iteration 39, loss = 0.02573682
Iteration 40, loss = 0.02578650
Iteration 41, loss = 0.02569447
Iteration 42, loss = 0.02528988
Iteration 43, loss = 0.02539126
Iteration 44, loss = 0.02514072
Iteration 45, loss = 0.02535301
Iteration 46, loss = 0.02508810
Iteration 47, loss = 0.02476474
Iteration 48, loss = 0.02462787
Iteration 49, loss = 0.02460252
Iteration 50, loss = 0.02445715
Iteration 51, loss = 0.02433323
Iteration 52, loss = 0.02454587
Iteration 53, loss = 0.02446561
Iteration 54, loss = 0.02405467
Iteration 55, loss = 0.02413677
Iteration 56, loss = 0.02432013
Iteration 57, loss = 0.02404795
Iteration 58, loss = 0.02409611
Iteration 59, loss = 0.02421190
Iteration 60, loss = 0.02399439
Iteration 61, loss = 0.02373344
Iteration 62, loss = 0.02361419
Iteration 63, loss = 0.02401881
Iteration 64, loss = 0.02381413
Iteration 65, loss = 0.02393842
Iteration 66, loss = 0.02365276
Iteration 67, loss = 0.02359103
Iteration 68, loss = 0.02356156
Iteration 69, loss = 0.02348212
Iteration 70, loss = 0.02356454
Iteration 71, loss = 0.02377725
Iteration 72, loss = 0.02361686
Iteration 73, loss = 0.02374325
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67301457
Iteration 2, loss = 0.59294869
Iteration 3, loss = 0.48380838
Iteration 4, loss = 0.37390509
Iteration 5, loss = 0.27964018
Iteration 6, loss = 0.20886867
Iteration 7, loss = 0.15949077
Iteration 8, loss = 0.12541550
Iteration 9, loss = 0.10243025
Iteration 10, loss = 0.08581465
Iteration 11, loss = 0.07393566
Iteration 12, loss = 0.06529597
Iteration 13, loss = 0.05846527
Iteration 14, loss = 0.05336301
Iteration 15, loss = 0.04917130
Iteration 16, loss = 0.04611188
Iteration 17, loss = 0.04336739
Iteration 18, loss = 0.04099216
Iteration 19, loss = 0.03931029
Iteration 20, loss = 0.03777705
Iteration 21, loss = 0.03638582
Iteration 22, loss = 0.03513823
Iteration 23, loss = 0.03411866
Iteration 24, loss = 0.03352209
Iteration 25, loss = 0.03260841
Iteration 26, loss = 0.03189861
Iteration 27, loss = 0.03130635
Iteration 28, loss = 0.03079600
Iteration 29, loss = 0.03041843
Iteration 30, loss = 0.03027502
Iteration 31, loss = 0.02933829
Iteration 32, loss = 0.02927217
Iteration 33, loss = 0.02896592
Iteration 34, loss = 0.02861857
Iteration 35, loss = 0.02849935
Iteration 36, loss = 0.02786009
Iteration 37, loss = 0.02817430
Iteration 38, loss = 0.02756029
Iteration 39, loss = 0.02731087
Iteration 40, loss = 0.02720277
Iteration 41, loss = 0.02759957
Iteration 42, loss = 0.02711892
Iteration 43, loss = 0.02683373
Iteration 44, loss = 0.02697866
Iteration 45, loss = 0.02687741
Iteration 46, loss = 0.02667018
Iteration 47, loss = 0.02681141
Iteration 48, loss = 0.02648713
Iteration 49, loss = 0.02637454
Iteration 50, loss = 0.02625416
Iteration 51, loss = 0.02638049
Iteration 52, loss = 0.02631891
Iteration 53, loss = 0.02597281
Iteration 54, loss = 0.02607211
Iteration 55, loss = 0.02594656
Iteration 56, loss = 0.02609063
Iteration 57, loss = 0.02568360
Iteration 58, loss = 0.02579283
Iteration 59, loss = 0.02570801
Iteration 60, loss = 0.02577764
Iteration 61, loss = 0.02548601
Iteration 62, loss = 0.02558817
Iteration 63, loss = 0.02558056
Iteration 64, loss = 0.02543105
Iteration 65, loss = 0.02511091
Iteration 66, loss = 0.02561025
Iteration 67, loss = 0.02535393
Iteration 68, loss = 0.02557394
Iteration 69, loss = 0.02538509
Iteration 70, loss = 0.02511628
Iteration 71, loss = 0.02538295
Iteration 72, loss = 0.02571976
Iteration 73, loss = 0.02528826
Iteration 74, loss = 0.02548334
Iteration 75, loss = 0.02503879
Iteration 76, loss = 0.02501401
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67540125
Iteration 2, loss = 0.60588733
Iteration 3, loss = 0.49804105
Iteration 4, loss = 0.38361889
Iteration 5, loss = 0.28529477
Iteration 6, loss = 0.21166704
Iteration 7, loss = 0.16031060
Iteration 8, loss = 0.12496822
Iteration 9, loss = 0.10122286
Iteration 10, loss = 0.08444646
Iteration 11, loss = 0.07232207
Iteration 12, loss = 0.06320474
Iteration 13, loss = 0.05616193
Iteration 14, loss = 0.05099272
Iteration 15, loss = 0.04680442
Iteration 16, loss = 0.04311685
Iteration 17, loss = 0.04051236
Iteration 18, loss = 0.03820940
Iteration 19, loss = 0.03619695
Iteration 20, loss = 0.03458297
Iteration 21, loss = 0.03328806
Iteration 22, loss = 0.03205111
Iteration 23, loss = 0.03112215
Iteration 24, loss = 0.03029634
Iteration 25, loss = 0.02928244
Iteration 26, loss = 0.02862104
Iteration 27, loss = 0.02788770
Iteration 28, loss = 0.02750188
Iteration 29, loss = 0.02706053
Iteration 30, loss = 0.02651511
Iteration 31, loss = 0.02627483
Iteration 32, loss = 0.02566734
Iteration 33, loss = 0.02549356
Iteration 34, loss = 0.02504535
Iteration 35, loss = 0.02500005
Iteration 36, loss = 0.02474063
Iteration 37, loss = 0.02423893
Iteration 38, loss = 0.02416622
Iteration 39, loss = 0.02408293
Iteration 40, loss = 0.02402309
Iteration 41, loss = 0.02386237
Iteration 42, loss = 0.02362903
Iteration 43, loss = 0.02363020
Iteration 44, loss = 0.02340314
Iteration 45, loss = 0.02334599
Iteration 46, loss = 0.02308337
Iteration 47, loss = 0.02312419
Iteration 48, loss = 0.02278380
Iteration 49, loss = 0.02276236
Iteration 50, loss = 0.02276741
Iteration 51, loss = 0.02278877
Iteration 52, loss = 0.02257513
Iteration 53, loss = 0.02262529
Iteration 54, loss = 0.02249726
Iteration 55, loss = 0.02223693
Iteration 56, loss = 0.02209918
Iteration 57, loss = 0.02216879
Iteration 58, loss = 0.02212315
Iteration 59, loss = 0.02227433
Iteration 60, loss = 0.02191901
Iteration 61, loss = 0.02231602
Iteration 62, loss = 0.02198305
Iteration 63, loss = 0.02182060
Iteration 64, loss = 0.02205471
Iteration 65, loss = 0.02176113
Iteration 66, loss = 0.02191508
Iteration 67, loss = 0.02188129
Iteration 68, loss = 0.02228660
Iteration 69, loss = 0.02167447
Iteration 70, loss = 0.02150622
Iteration 71, loss = 0.02156941
Iteration 72, loss = 0.02206715
Iteration 73, loss = 0.02143149
Iteration 74, loss = 0.02172549
Iteration 75, loss = 0.02147733
Iteration 76, loss = 0.02149080
Iteration 77, loss = 0.02155206
Iteration 78, loss = 0.02162030
Iteration 79, loss = 0.02155368
Iteration 80, loss = 0.02154874
Iteration 81, loss = 0.02151954
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68134263
Iteration 2, loss = 0.61496804
Iteration 3, loss = 0.51146277
Iteration 4, loss = 0.39566436
Iteration 5, loss = 0.29211371
Iteration 6, loss = 0.21417480
Iteration 7, loss = 0.16049566
Iteration 8, loss = 0.12428578
Iteration 9, loss = 0.09967792
Iteration 10, loss = 0.08275651
Iteration 11, loss = 0.07046838
Iteration 12, loss = 0.06142305
Iteration 13, loss = 0.05438715
Iteration 14, loss = 0.04910212
Iteration 15, loss = 0.04473097
Iteration 16, loss = 0.04142103
Iteration 17, loss = 0.03855631
Iteration 18, loss = 0.03640709
Iteration 19, loss = 0.03448024
Iteration 20, loss = 0.03285733
Iteration 21, loss = 0.03148785
Iteration 22, loss = 0.03034194
Iteration 23, loss = 0.02934722
Iteration 24, loss = 0.02832008
Iteration 25, loss = 0.02766128
Iteration 26, loss = 0.02685755
Iteration 27, loss = 0.02648322
Iteration 28, loss = 0.02614346
Iteration 29, loss = 0.02542989
Iteration 30, loss = 0.02494697
Iteration 31, loss = 0.02457840
Iteration 32, loss = 0.02436233
Iteration 33, loss = 0.02395478
Iteration 34, loss = 0.02359962
Iteration 35, loss = 0.02321073
Iteration 36, loss = 0.02321315
Iteration 37, loss = 0.02288885
Iteration 38, loss = 0.02263937
Iteration 39, loss = 0.02259299
Iteration 40, loss = 0.02236401
Iteration 41, loss = 0.02225987
Iteration 42, loss = 0.02207283
Iteration 43, loss = 0.02194123
Iteration 44, loss = 0.02220069
Iteration 45, loss = 0.02155928
Iteration 46, loss = 0.02146743
Iteration 47, loss = 0.02164056
Iteration 48, loss = 0.02143074
Iteration 49, loss = 0.02126004
Iteration 50, loss = 0.02145423
Iteration 51, loss = 0.02139320
Iteration 52, loss = 0.02141171
Iteration 53, loss = 0.02100361
Iteration 54, loss = 0.02093942
Iteration 55, loss = 0.02099644
Iteration 56, loss = 0.02080333
Iteration 57, loss = 0.02094593
Iteration 58, loss = 0.02069266
Iteration 59, loss = 0.02064287
Iteration 60, loss = 0.02084680
Iteration 61, loss = 0.02062517
Iteration 62, loss = 0.02063663
Iteration 63, loss = 0.02051546
Iteration 64, loss = 0.02067182
Iteration 65, loss = 0.02045651
Iteration 66, loss = 0.02042034
Iteration 67, loss = 0.02051760
Iteration 68, loss = 0.02058945
Iteration 69, loss = 0.02033777
Iteration 70, loss = 0.02020539
Iteration 71, loss = 0.02029938
Iteration 72, loss = 0.02039891
Iteration 73, loss = 0.02011663
Iteration 74, loss = 0.02011475
Iteration 75, loss = 0.02001308
Iteration 76, loss = 0.02006477
Iteration 77, loss = 0.02023980
Iteration 78, loss = 0.02026393
Iteration 79, loss = 0.02002016
Iteration 80, loss = 0.02019522
Iteration 81, loss = 0.02026172
Iteration 82, loss = 0.02008516
Iteration 83, loss = 0.02022474
Iteration 84, loss = 0.02022529
Iteration 85, loss = 0.01993271
Iteration 86, loss = 0.01992061
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67579529
Iteration 2, loss = 0.59333749
Iteration 3, loss = 0.47927595
Iteration 4, loss = 0.36666526
Iteration 5, loss = 0.27392344
Iteration 6, loss = 0.20464836
Iteration 7, loss = 0.15669945
Iteration 8, loss = 0.12363202
Iteration 9, loss = 0.10085375
Iteration 10, loss = 0.08469038
Iteration 11, loss = 0.07295615
Iteration 12, loss = 0.06426284
Iteration 13, loss = 0.05756000
Iteration 14, loss = 0.05253590
Iteration 15, loss = 0.04843280
Iteration 16, loss = 0.04517777
Iteration 17, loss = 0.04265157
Iteration 18, loss = 0.04049958
Iteration 19, loss = 0.03826698
Iteration 20, loss = 0.03672490
Iteration 21, loss = 0.03515097
Iteration 22, loss = 0.03409417
Iteration 23, loss = 0.03319547
Iteration 24, loss = 0.03236709
Iteration 25, loss = 0.03178798
Iteration 26, loss = 0.03075705
Iteration 27, loss = 0.03013902
Iteration 28, loss = 0.02961766
Iteration 29, loss = 0.02939476
Iteration 30, loss = 0.02884447
Iteration 31, loss = 0.02838264
Iteration 32, loss = 0.02803284
Iteration 33, loss = 0.02789461
Iteration 34, loss = 0.02744924
Iteration 35, loss = 0.02729072
Iteration 36, loss = 0.02705768
Iteration 37, loss = 0.02679047
Iteration 38, loss = 0.02653961
Iteration 39, loss = 0.02634715
Iteration 40, loss = 0.02602207
Iteration 41, loss = 0.02625071
Iteration 42, loss = 0.02574458
Iteration 43, loss = 0.02581033
Iteration 44, loss = 0.02546405
Iteration 45, loss = 0.02560433
Iteration 46, loss = 0.02547520
Iteration 47, loss = 0.02545966
Iteration 48, loss = 0.02496472
Iteration 49, loss = 0.02523686
Iteration 50, loss = 0.02524730
Iteration 51, loss = 0.02507563
Iteration 52, loss = 0.02482678
Iteration 53, loss = 0.02518086
Iteration 54, loss = 0.02475621
Iteration 55, loss = 0.02464585
Iteration 56, loss = 0.02460130
Iteration 57, loss = 0.02459577
Iteration 58, loss = 0.02457880
Iteration 59, loss = 0.02446815
Iteration 60, loss = 0.02446061
Iteration 61, loss = 0.02450362
Iteration 62, loss = 0.02440240
Iteration 63, loss = 0.02428898
Iteration 64, loss = 0.02432459
Iteration 65, loss = 0.02428118
Iteration 66, loss = 0.02419277
Iteration 67, loss = 0.02420254
Iteration 68, loss = 0.02406114
Iteration 69, loss = 0.02430474
Iteration 70, loss = 0.02435557
Iteration 71, loss = 0.02406029
Iteration 72, loss = 0.02394655
Iteration 73, loss = 0.02389255
Iteration 74, loss = 0.02380617
Iteration 75, loss = 0.02419326
Iteration 76, loss = 0.02458896
Iteration 77, loss = 0.02401509
Iteration 78, loss = 0.02384062
Iteration 79, loss = 0.02386371
Iteration 80, loss = 0.02376229
Iteration 81, loss = 0.02394163
Iteration 82, loss = 0.02411765
Iteration 83, loss = 0.02389962
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67451169
Iteration 2, loss = 0.60649750
Iteration 3, loss = 0.50658436
Iteration 4, loss = 0.39797833
Iteration 5, loss = 0.29908854
Iteration 6, loss = 0.22263775
Iteration 7, loss = 0.16833666
Iteration 8, loss = 0.13105323
Iteration 9, loss = 0.10545973
Iteration 10, loss = 0.08767779
Iteration 11, loss = 0.07454614
Iteration 12, loss = 0.06508619
Iteration 13, loss = 0.05812204
Iteration 14, loss = 0.05264141
Iteration 15, loss = 0.04850502
Iteration 16, loss = 0.04483546
Iteration 17, loss = 0.04203934
Iteration 18, loss = 0.03971431
Iteration 19, loss = 0.03767246
Iteration 20, loss = 0.03615102
Iteration 21, loss = 0.03470217
Iteration 22, loss = 0.03360963
Iteration 23, loss = 0.03267579
Iteration 24, loss = 0.03163007
Iteration 25, loss = 0.03098266
Iteration 26, loss = 0.03047148
Iteration 27, loss = 0.02949980
Iteration 28, loss = 0.02951890
Iteration 29, loss = 0.02857369
Iteration 30, loss = 0.02802328
Iteration 31, loss = 0.02786315
Iteration 32, loss = 0.02767163
Iteration 33, loss = 0.02744225
Iteration 34, loss = 0.02699856
Iteration 35, loss = 0.02663217
Iteration 36, loss = 0.02637564
Iteration 37, loss = 0.02620085
Iteration 38, loss = 0.02585831
Iteration 39, loss = 0.02573682
Iteration 40, loss = 0.02578650
Iteration 41, loss = 0.02569447
Iteration 42, loss = 0.02528988
Iteration 43, loss = 0.02539126
Iteration 44, loss = 0.02514072
Iteration 45, loss = 0.02535301
Iteration 46, loss = 0.02508810
Iteration 47, loss = 0.02476474
Iteration 48, loss = 0.02462787
Iteration 49, loss = 0.02460252
Iteration 50, loss = 0.02445715
Iteration 51, loss = 0.02433323
Iteration 52, loss = 0.02454587
Iteration 53, loss = 0.02446561
Iteration 54, loss = 0.02405467
Iteration 55, loss = 0.02413677
Iteration 56, loss = 0.02432013
Iteration 57, loss = 0.02404795
Iteration 58, loss = 0.02409611
Iteration 59, loss = 0.02421190
Iteration 60, loss = 0.02399439
Iteration 61, loss = 0.02373344
Iteration 62, loss = 0.02361419
Iteration 63, loss = 0.02401881
Iteration 64, loss = 0.02381413
Iteration 65, loss = 0.02393842
Iteration 66, loss = 0.02365276
Iteration 67, loss = 0.02359103
Iteration 68, loss = 0.02356156
Iteration 69, loss = 0.02348212
Iteration 70, loss = 0.02356454
Iteration 71, loss = 0.02377725
Iteration 72, loss = 0.02361686
Iteration 73, loss = 0.02374325
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67301457
Iteration 2, loss = 0.59294869
Iteration 3, loss = 0.48380838
Iteration 4, loss = 0.37390509
Iteration 5, loss = 0.27964018
Iteration 6, loss = 0.20886867
Iteration 7, loss = 0.15949077
Iteration 8, loss = 0.12541550
Iteration 9, loss = 0.10243025
Iteration 10, loss = 0.08581465
Iteration 11, loss = 0.07393566
Iteration 12, loss = 0.06529597
Iteration 13, loss = 0.05846527
Iteration 14, loss = 0.05336301
Iteration 15, loss = 0.04917130
Iteration 16, loss = 0.04611188
Iteration 17, loss = 0.04336739
Iteration 18, loss = 0.04099216
Iteration 19, loss = 0.03931029
Iteration 20, loss = 0.03777705
Iteration 21, loss = 0.03638582
Iteration 22, loss = 0.03513823
Iteration 23, loss = 0.03411866
Iteration 24, loss = 0.03352209
Iteration 25, loss = 0.03260841
Iteration 26, loss = 0.03189861
Iteration 27, loss = 0.03130635
Iteration 28, loss = 0.03079600
Iteration 29, loss = 0.03041843
Iteration 30, loss = 0.03027502
Iteration 31, loss = 0.02933829
Iteration 32, loss = 0.02927217
Iteration 33, loss = 0.02896592
Iteration 34, loss = 0.02861857
Iteration 35, loss = 0.02849935
Iteration 36, loss = 0.02786009
Iteration 37, loss = 0.02817430
Iteration 38, loss = 0.02756029
Iteration 39, loss = 0.02731087
Iteration 40, loss = 0.02720277
Iteration 41, loss = 0.02759957
Iteration 42, loss = 0.02711892
Iteration 43, loss = 0.02683373
Iteration 44, loss = 0.02697866
Iteration 45, loss = 0.02687741
Iteration 46, loss = 0.02667018
Iteration 47, loss = 0.02681141
Iteration 48, loss = 0.02648713
Iteration 49, loss = 0.02637454
Iteration 50, loss = 0.02625416
Iteration 51, loss = 0.02638049
Iteration 52, loss = 0.02631891
Iteration 53, loss = 0.02597281
Iteration 54, loss = 0.02607211
Iteration 55, loss = 0.02594656
Iteration 56, loss = 0.02609063
Iteration 57, loss = 0.02568360
Iteration 58, loss = 0.02579283
Iteration 59, loss = 0.02570801
Iteration 60, loss = 0.02577764
Iteration 61, loss = 0.02548601
Iteration 62, loss = 0.02558817
Iteration 63, loss = 0.02558056
Iteration 64, loss = 0.02543105
Iteration 65, loss = 0.02511091
Iteration 66, loss = 0.02561025
Iteration 67, loss = 0.02535393
Iteration 68, loss = 0.02557394
Iteration 69, loss = 0.02538509
Iteration 70, loss = 0.02511628
Iteration 71, loss = 0.02538295
Iteration 72, loss = 0.02571976
Iteration 73, loss = 0.02528826
Iteration 74, loss = 0.02548334
Iteration 75, loss = 0.02503879
Iteration 76, loss = 0.02501401
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67540125
Iteration 2, loss = 0.60588733
Iteration 3, loss = 0.49804105
Iteration 4, loss = 0.38361889
Iteration 5, loss = 0.28529477
Iteration 6, loss = 0.21166704
Iteration 7, loss = 0.16031060
Iteration 8, loss = 0.12496822
Iteration 9, loss = 0.10122286
Iteration 10, loss = 0.08444646
Iteration 11, loss = 0.07232207
Iteration 12, loss = 0.06320474
Iteration 13, loss = 0.05616193
Iteration 14, loss = 0.05099272
Iteration 15, loss = 0.04680442
Iteration 16, loss = 0.04311685
Iteration 17, loss = 0.04051236
Iteration 18, loss = 0.03820940
Iteration 19, loss = 0.03619695
Iteration 20, loss = 0.03458297
Iteration 21, loss = 0.03328806
Iteration 22, loss = 0.03205111
Iteration 23, loss = 0.03112215
Iteration 24, loss = 0.03029634
Iteration 25, loss = 0.02928244
Iteration 26, loss = 0.02862104
Iteration 27, loss = 0.02788770
Iteration 28, loss = 0.02750188
Iteration 29, loss = 0.02706053
Iteration 30, loss = 0.02651511
Iteration 31, loss = 0.02627483
Iteration 32, loss = 0.02566734
Iteration 33, loss = 0.02549356
Iteration 34, loss = 0.02504535
Iteration 35, loss = 0.02500005
Iteration 36, loss = 0.02474063
Iteration 37, loss = 0.02423893
Iteration 38, loss = 0.02416622
Iteration 39, loss = 0.02408293
Iteration 40, loss = 0.02402309
Iteration 41, loss = 0.02386237
Iteration 42, loss = 0.02362903
Iteration 43, loss = 0.02363020
Iteration 44, loss = 0.02340314
Iteration 45, loss = 0.02334599
Iteration 46, loss = 0.02308337
Iteration 47, loss = 0.02312419
Iteration 48, loss = 0.02278380
Iteration 49, loss = 0.02276236
Iteration 50, loss = 0.02276741
Iteration 51, loss = 0.02278877
Iteration 52, loss = 0.02257513
Iteration 53, loss = 0.02262529
Iteration 54, loss = 0.02249726
Iteration 55, loss = 0.02223693
Iteration 56, loss = 0.02209918
Iteration 57, loss = 0.02216879
Iteration 58, loss = 0.02212315
Iteration 59, loss = 0.02227433
Iteration 60, loss = 0.02191901
Iteration 61, loss = 0.02231602
Iteration 62, loss = 0.02198305
Iteration 63, loss = 0.02182060
Iteration 64, loss = 0.02205471
Iteration 65, loss = 0.02176113
Iteration 66, loss = 0.02191508
Iteration 67, loss = 0.02188129
Iteration 68, loss = 0.02228660
Iteration 69, loss = 0.02167447
Iteration 70, loss = 0.02150622
Iteration 71, loss = 0.02156941
Iteration 72, loss = 0.02206715
Iteration 73, loss = 0.02143149
Iteration 74, loss = 0.02172549
Iteration 75, loss = 0.02147733
Iteration 76, loss = 0.02149080
Iteration 77, loss = 0.02155206
Iteration 78, loss = 0.02162030
Iteration 79, loss = 0.02155368
Iteration 80, loss = 0.02154874
Iteration 81, loss = 0.02151954
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68134263
Iteration 2, loss = 0.61496804
Iteration 3, loss = 0.51146277
Iteration 4, loss = 0.39566436
Iteration 5, loss = 0.29211371
Iteration 6, loss = 0.21417480
Iteration 7, loss = 0.16049566
Iteration 8, loss = 0.12428578
Iteration 9, loss = 0.09967792
Iteration 10, loss = 0.08275651
Iteration 11, loss = 0.07046838
Iteration 12, loss = 0.06142305
Iteration 13, loss = 0.05438715
Iteration 14, loss = 0.04910212
Iteration 15, loss = 0.04473097
Iteration 16, loss = 0.04142103
Iteration 17, loss = 0.03855631
Iteration 18, loss = 0.03640709
Iteration 19, loss = 0.03448024
Iteration 20, loss = 0.03285733
Iteration 21, loss = 0.03148785
Iteration 22, loss = 0.03034194
Iteration 23, loss = 0.02934722
Iteration 24, loss = 0.02832008
Iteration 25, loss = 0.02766128
Iteration 26, loss = 0.02685755
Iteration 27, loss = 0.02648322
Iteration 28, loss = 0.02614346
Iteration 29, loss = 0.02542989
Iteration 30, loss = 0.02494697
Iteration 31, loss = 0.02457840
Iteration 32, loss = 0.02436233
Iteration 33, loss = 0.02395478
Iteration 34, loss = 0.02359962
Iteration 35, loss = 0.02321073
Iteration 36, loss = 0.02321315
Iteration 37, loss = 0.02288885
Iteration 38, loss = 0.02263937
Iteration 39, loss = 0.02259299
Iteration 40, loss = 0.02236401
Iteration 41, loss = 0.02225987
Iteration 42, loss = 0.02207283
Iteration 43, loss = 0.02194123
Iteration 44, loss = 0.02220069
Iteration 45, loss = 0.02155928
Iteration 46, loss = 0.02146743
Iteration 47, loss = 0.02164056
Iteration 48, loss = 0.02143074
Iteration 49, loss = 0.02126004
Iteration 50, loss = 0.02145423
Iteration 51, loss = 0.02139320
Iteration 52, loss = 0.02141171
Iteration 53, loss = 0.02100361
Iteration 54, loss = 0.02093942
Iteration 55, loss = 0.02099644
Iteration 56, loss = 0.02080333
Iteration 57, loss = 0.02094593
Iteration 58, loss = 0.02069266
Iteration 59, loss = 0.02064287
Iteration 60, loss = 0.02084680
Iteration 61, loss = 0.02062517
Iteration 62, loss = 0.02063663
Iteration 63, loss = 0.02051546
Iteration 64, loss = 0.02067182
Iteration 65, loss = 0.02045651
Iteration 66, loss = 0.02042034
Iteration 67, loss = 0.02051760
Iteration 68, loss = 0.02058945
Iteration 69, loss = 0.02033777
Iteration 70, loss = 0.02020539
Iteration 71, loss = 0.02029938
Iteration 72, loss = 0.02039891
Iteration 73, loss = 0.02011663
Iteration 74, loss = 0.02011475
Iteration 75, loss = 0.02001308
Iteration 76, loss = 0.02006477
Iteration 77, loss = 0.02023980
Iteration 78, loss = 0.02026393
Iteration 79, loss = 0.02002016
Iteration 80, loss = 0.02019522
Iteration 81, loss = 0.02026172
Iteration 82, loss = 0.02008516
Iteration 83, loss = 0.02022474
Iteration 84, loss = 0.02022529
Iteration 85, loss = 0.01993271
Iteration 86, loss = 0.01992061
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67579529
Iteration 2, loss = 0.59333749
Iteration 3, loss = 0.47927595
Iteration 4, loss = 0.36666526
Iteration 5, loss = 0.27392344
Iteration 6, loss = 0.20464836
Iteration 7, loss = 0.15669945
Iteration 8, loss = 0.12363202
Iteration 9, loss = 0.10085375
Iteration 10, loss = 0.08469038
Iteration 11, loss = 0.07295615
Iteration 12, loss = 0.06426284
Iteration 13, loss = 0.05756000
Iteration 14, loss = 0.05253590
Iteration 15, loss = 0.04843280
Iteration 16, loss = 0.04517777
Iteration 17, loss = 0.04265157
Iteration 18, loss = 0.04049958
Iteration 19, loss = 0.03826698
Iteration 20, loss = 0.03672490
Iteration 21, loss = 0.03515097
Iteration 22, loss = 0.03409417
Iteration 23, loss = 0.03319547
Iteration 24, loss = 0.03236709
Iteration 25, loss = 0.03178798
Iteration 26, loss = 0.03075705
Iteration 27, loss = 0.03013902
Iteration 28, loss = 0.02961766
Iteration 29, loss = 0.02939476
Iteration 30, loss = 0.02884447
Iteration 31, loss = 0.02838264
Iteration 32, loss = 0.02803284
Iteration 33, loss = 0.02789461
Iteration 34, loss = 0.02744924
Iteration 35, loss = 0.02729072
Iteration 36, loss = 0.02705768
Iteration 37, loss = 0.02679047
Iteration 38, loss = 0.02653961
Iteration 39, loss = 0.02634715
Iteration 40, loss = 0.02602207
Iteration 41, loss = 0.02625071
Iteration 42, loss = 0.02574458
Iteration 43, loss = 0.02581033
Iteration 44, loss = 0.02546405
Iteration 45, loss = 0.02560433
Iteration 46, loss = 0.02547520
Iteration 47, loss = 0.02545966
Iteration 48, loss = 0.02496472
Iteration 49, loss = 0.02523686
Iteration 50, loss = 0.02524730
Iteration 51, loss = 0.02507563
Iteration 52, loss = 0.02482678
Iteration 53, loss = 0.02518086
Iteration 54, loss = 0.02475621
Iteration 55, loss = 0.02464585
Iteration 56, loss = 0.02460130
Iteration 57, loss = 0.02459577
Iteration 58, loss = 0.02457880
Iteration 59, loss = 0.02446815
Iteration 60, loss = 0.02446061
Iteration 61, loss = 0.02450362
Iteration 62, loss = 0.02440240
Iteration 63, loss = 0.02428898
Iteration 64, loss = 0.02432459
Iteration 65, loss = 0.02428118
Iteration 66, loss = 0.02419277
Iteration 67, loss = 0.02420254
Iteration 68, loss = 0.02406114
Iteration 69, loss = 0.02430474
Iteration 70, loss = 0.02435557
Iteration 71, loss = 0.02406029
Iteration 72, loss = 0.02394655
Iteration 73, loss = 0.02389255
Iteration 74, loss = 0.02380617
Iteration 75, loss = 0.02419326
Iteration 76, loss = 0.02458896
Iteration 77, loss = 0.02401509
Iteration 78, loss = 0.02384062
Iteration 79, loss = 0.02386371
Iteration 80, loss = 0.02376229
Iteration 81, loss = 0.02394163
Iteration 82, loss = 0.02411765
Iteration 83, loss = 0.02389962
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67451169
Iteration 2, loss = 0.60649750
Iteration 3, loss = 0.50658436
Iteration 4, loss = 0.39797833
Iteration 5, loss = 0.29908854
Iteration 6, loss = 0.22263775
Iteration 7, loss = 0.16833666
Iteration 8, loss = 0.13105323
Iteration 9, loss = 0.10545973
Iteration 10, loss = 0.08767779
Iteration 11, loss = 0.07454614
Iteration 12, loss = 0.06508619
Iteration 13, loss = 0.05812204
Iteration 14, loss = 0.05264141
Iteration 15, loss = 0.04850502
Iteration 16, loss = 0.04483546
Iteration 17, loss = 0.04203934
Iteration 18, loss = 0.03971431
Iteration 19, loss = 0.03767246
Iteration 20, loss = 0.03615102
Iteration 21, loss = 0.03470217
Iteration 22, loss = 0.03360963
Iteration 23, loss = 0.03267579
Iteration 24, loss = 0.03163007
Iteration 25, loss = 0.03098266
Iteration 26, loss = 0.03047148
Iteration 27, loss = 0.02949980
Iteration 28, loss = 0.02951890
Iteration 29, loss = 0.02857369
Iteration 30, loss = 0.02802328
Iteration 31, loss = 0.02786315
Iteration 32, loss = 0.02767163
Iteration 33, loss = 0.02744225
Iteration 34, loss = 0.02699856
Iteration 35, loss = 0.02663217
Iteration 36, loss = 0.02637564
Iteration 37, loss = 0.02620085
Iteration 38, loss = 0.02585831
Iteration 39, loss = 0.02573682
Iteration 40, loss = 0.02578650
Iteration 41, loss = 0.02569447
Iteration 42, loss = 0.02528988
Iteration 43, loss = 0.02539126
Iteration 44, loss = 0.02514072
Iteration 45, loss = 0.02535301
Iteration 46, loss = 0.02508810
Iteration 47, loss = 0.02476474
Iteration 48, loss = 0.02462787
Iteration 49, loss = 0.02460252
Iteration 50, loss = 0.02445715
Iteration 51, loss = 0.02433323
Iteration 52, loss = 0.02454587
Iteration 53, loss = 0.02446561
Iteration 54, loss = 0.02405467
Iteration 55, loss = 0.02413677
Iteration 56, loss = 0.02432013
Iteration 57, loss = 0.02404795
Iteration 58, loss = 0.02409611
Iteration 59, loss = 0.02421190
Iteration 60, loss = 0.02399439
Iteration 61, loss = 0.02373344
Iteration 62, loss = 0.02361419
Iteration 63, loss = 0.02401881
Iteration 64, loss = 0.02381413
Iteration 65, loss = 0.02393842
Iteration 66, loss = 0.02365276
Iteration 67, loss = 0.02359103
Iteration 68, loss = 0.02356156
Iteration 69, loss = 0.02348212
Iteration 70, loss = 0.02356454
Iteration 71, loss = 0.02377725
Iteration 72, loss = 0.02361686
Iteration 73, loss = 0.02374325
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67605095
Iteration 2, loss = 0.57484001
Iteration 3, loss = 0.43370471
Iteration 4, loss = 0.30936918
Iteration 5, loss = 0.21867475
Iteration 6, loss = 0.15923561
Iteration 7, loss = 0.12165427
Iteration 8, loss = 0.09654840
Iteration 9, loss = 0.08048932
Iteration 10, loss = 0.06867483
Iteration 11, loss = 0.06033992
Iteration 12, loss = 0.05429959
Iteration 13, loss = 0.04979081
Iteration 14, loss = 0.04581671
Iteration 15, loss = 0.04307030
Iteration 16, loss = 0.04071088
Iteration 17, loss = 0.03855947
Iteration 18, loss = 0.03688278
Iteration 19, loss = 0.03567704
Iteration 20, loss = 0.03465372
Iteration 21, loss = 0.03341994
Iteration 22, loss = 0.03245024
Iteration 23, loss = 0.03232114
Iteration 24, loss = 0.03136393
Iteration 25, loss = 0.03091928
Iteration 26, loss = 0.03046806
Iteration 27, loss = 0.02991985
Iteration 28, loss = 0.02946076
Iteration 29, loss = 0.02921139
Iteration 30, loss = 0.02880225
Iteration 31, loss = 0.02902899
Iteration 32, loss = 0.02831802
Iteration 33, loss = 0.02835841
Iteration 34, loss = 0.02797497
Iteration 35, loss = 0.02777224
Iteration 36, loss = 0.02745545
Iteration 37, loss = 0.02740786
Iteration 38, loss = 0.02732386
Iteration 39, loss = 0.02715836
Iteration 40, loss = 0.02693769
Iteration 41, loss = 0.02700843
Iteration 42, loss = 0.02707675
Iteration 43, loss = 0.02696633
Iteration 44, loss = 0.02665445
Iteration 45, loss = 0.02672971
Iteration 46, loss = 0.02662951
Iteration 47, loss = 0.02631083
Iteration 48, loss = 0.02672079
Iteration 49, loss = 0.02656331
Iteration 50, loss = 0.02630701
Iteration 51, loss = 0.02601070
Iteration 52, loss = 0.02620138
Iteration 53, loss = 0.02626883
Iteration 54, loss = 0.02625253
Iteration 55, loss = 0.02590601
Iteration 56, loss = 0.02624740
Iteration 57, loss = 0.02574995
Iteration 58, loss = 0.02586160
Iteration 59, loss = 0.02613629
Iteration 60, loss = 0.02609224
Iteration 61, loss = 0.02602383
Iteration 62, loss = 0.02585979
Iteration 63, loss = 0.02587321
Iteration 64, loss = 0.02546662
Iteration 65, loss = 0.02561760
Iteration 66, loss = 0.02571450
Iteration 67, loss = 0.02545066
Iteration 68, loss = 0.02566344
Iteration 69, loss = 0.02578539
Iteration 70, loss = 0.02569367
Iteration 71, loss = 0.02600058
Iteration 72, loss = 0.02558975
Iteration 73, loss = 0.02543015
Iteration 74, loss = 0.02554460
Iteration 75, loss = 0.02554255
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on kept_v1_basic_clean...
Iteration 1, loss = 0.67793505
Iteration 2, loss = 0.64473662
Iteration 3, loss = 0.60426118
Iteration 4, loss = 0.55458901
Iteration 5, loss = 0.49936927
Iteration 6, loss = 0.44306442
Iteration 7, loss = 0.38821009
Iteration 8, loss = 0.33688981
Iteration 9, loss = 0.29016989
Iteration 10, loss = 0.24907866
Iteration 11, loss = 0.21347652
Iteration 12, loss = 0.18353202
Iteration 13, loss = 0.15823817
Iteration 14, loss = 0.13731100
Iteration 15, loss = 0.12023617
Iteration 16, loss = 0.10608695
Iteration 17, loss = 0.09417932
Iteration 18, loss = 0.08442763
Iteration 19, loss = 0.07620770
Iteration 20, loss = 0.06929779
Iteration 21, loss = 0.06343423
Iteration 22, loss = 0.05839375
Iteration 23, loss = 0.05410983
Iteration 24, loss = 0.05029299
Iteration 25, loss = 0.04713492
Iteration 26, loss = 0.04427546
Iteration 27, loss = 0.04193090
Iteration 28, loss = 0.03969327
Iteration 29, loss = 0.03757698
Iteration 30, loss = 0.03590901
Iteration 31, loss = 0.03426823
Iteration 32, loss = 0.03279679
Iteration 33, loss = 0.03154323
Iteration 34, loss = 0.03040408
Iteration 35, loss = 0.02929151
Iteration 36, loss = 0.02841378
Iteration 37, loss = 0.02742864
Iteration 38, loss = 0.02662987
Iteration 39, loss = 0.02583124
Iteration 40, loss = 0.02533697
Iteration 41, loss = 0.02455933
Iteration 42, loss = 0.02396272
Iteration 43, loss = 0.02354381
Iteration 44, loss = 0.02298617
Iteration 45, loss = 0.02252104
Iteration 46, loss = 0.02198520
Iteration 47, loss = 0.02159289
Iteration 48, loss = 0.02123910
Iteration 49, loss = 0.02093904
Iteration 50, loss = 0.02057204
Iteration 51, loss = 0.02020737
Iteration 52, loss = 0.02003041
Iteration 53, loss = 0.01979635
Iteration 54, loss = 0.01937924
Iteration 55, loss = 0.01926359
Iteration 56, loss = 0.01896594
Iteration 57, loss = 0.01873816
Iteration 58, loss = 0.01844284
Iteration 59, loss = 0.01829507
Iteration 60, loss = 0.01827162
Iteration 61, loss = 0.01803642
Iteration 62, loss = 0.01785949
Iteration 63, loss = 0.01759846
Iteration 64, loss = 0.01751101
Iteration 65, loss = 0.01732688
Iteration 66, loss = 0.01715056
Iteration 67, loss = 0.01699612
Iteration 68, loss = 0.01684119
Iteration 69, loss = 0.01689389
Iteration 70, loss = 0.01670592
Iteration 71, loss = 0.01663873
Iteration 72, loss = 0.01649130
Iteration 73, loss = 0.01645793
Iteration 74, loss = 0.01642640
Iteration 75, loss = 0.01620776
Iteration 76, loss = 0.01620318
Iteration 77, loss = 0.01599025
Iteration 78, loss = 0.01605727
Iteration 79, loss = 0.01584575
Iteration 80, loss = 0.01583879
Iteration 81, loss = 0.01575315
Iteration 82, loss = 0.01565994
Iteration 83, loss = 0.01550536
Iteration 84, loss = 0.01559188
Iteration 85, loss = 0.01551184
Iteration 86, loss = 0.01538316
Iteration 87, loss = 0.01529182
Iteration 88, loss = 0.01520150
Iteration 89, loss = 0.01521042
Iteration 90, loss = 0.01517671
Iteration 91, loss = 0.01517193
Iteration 92, loss = 0.01506404
Iteration 93, loss = 0.01499407
Iteration 94, loss = 0.01495263
Iteration 95, loss = 0.01500913
Iteration 96, loss = 0.01491479
Iteration 97, loss = 0.01495478
Iteration 98, loss = 0.01497492
Iteration 99, loss = 0.01499358
Iteration 100, loss = 0.01477908
Iteration 101, loss = 0.01484094
Iteration 102, loss = 0.01476499
Iteration 103, loss = 0.01466594
Iteration 104, loss = 0.01466332
Iteration 105, loss = 0.01469124
Iteration 106, loss = 0.01459104
Iteration 107, loss = 0.01460058
Iteration 108, loss = 0.01466368
Iteration 109, loss = 0.01452171
Iteration 110, loss = 0.01440510
Iteration 111, loss = 0.01442273
Iteration 112, loss = 0.01437037
Iteration 113, loss = 0.01451903
Iteration 114, loss = 0.01434375
Iteration 115, loss = 0.01430185
Iteration 116, loss = 0.01428869
Iteration 117, loss = 0.01425395
Iteration 118, loss = 0.01422970
Iteration 119, loss = 0.01428126
Iteration 120, loss = 0.01432026
Iteration 121, loss = 0.01424760
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72709157
Iteration 2, loss = 0.70110696
Iteration 3, loss = 0.66787738
Iteration 4, loss = 0.62779014
Iteration 5, loss = 0.58225191
Iteration 6, loss = 0.53273081
Iteration 7, loss = 0.48207763
Iteration 8, loss = 0.43235005
Iteration 9, loss = 0.38479682
Iteration 10, loss = 0.34002341
Iteration 11, loss = 0.29880468
Iteration 12, loss = 0.26230429
Iteration 13, loss = 0.23044933
Iteration 14, loss = 0.20285042
Iteration 15, loss = 0.17914007
Iteration 16, loss = 0.15893324
Iteration 17, loss = 0.14150669
Iteration 18, loss = 0.12671183
Iteration 19, loss = 0.11421365
Iteration 20, loss = 0.10360570
Iteration 21, loss = 0.09425012
Iteration 22, loss = 0.08602569
Iteration 23, loss = 0.07916847
Iteration 24, loss = 0.07311415
Iteration 25, loss = 0.06775871
Iteration 26, loss = 0.06292606
Iteration 27, loss = 0.05885033
Iteration 28, loss = 0.05511898
Iteration 29, loss = 0.05183607
Iteration 30, loss = 0.04906126
Iteration 31, loss = 0.04656152
Iteration 32, loss = 0.04401505
Iteration 33, loss = 0.04181873
Iteration 34, loss = 0.03978920
Iteration 35, loss = 0.03798997
Iteration 36, loss = 0.03641657
Iteration 37, loss = 0.03483249
Iteration 38, loss = 0.03364771
Iteration 39, loss = 0.03240843
Iteration 40, loss = 0.03131174
Iteration 41, loss = 0.03023630
Iteration 42, loss = 0.02925901
Iteration 43, loss = 0.02832163
Iteration 44, loss = 0.02759004
Iteration 45, loss = 0.02687425
Iteration 46, loss = 0.02612306
Iteration 47, loss = 0.02538398
Iteration 48, loss = 0.02471612
Iteration 49, loss = 0.02405388
Iteration 50, loss = 0.02346007
Iteration 51, loss = 0.02292774
Iteration 52, loss = 0.02244550
Iteration 53, loss = 0.02190060
Iteration 54, loss = 0.02137298
Iteration 55, loss = 0.02088888
Iteration 56, loss = 0.02076035
Iteration 57, loss = 0.02050361
Iteration 58, loss = 0.02001819
Iteration 59, loss = 0.01987330
Iteration 60, loss = 0.01939837
Iteration 61, loss = 0.01908073
Iteration 62, loss = 0.01885783
Iteration 63, loss = 0.01855331
Iteration 64, loss = 0.01813618
Iteration 65, loss = 0.01789496
Iteration 66, loss = 0.01762882
Iteration 67, loss = 0.01738692
Iteration 68, loss = 0.01718180
Iteration 69, loss = 0.01705811
Iteration 70, loss = 0.01680858
Iteration 71, loss = 0.01657845
Iteration 72, loss = 0.01659935
Iteration 73, loss = 0.01650933
Iteration 74, loss = 0.01630733
Iteration 75, loss = 0.01599070
Iteration 76, loss = 0.01585598
Iteration 77, loss = 0.01560385
Iteration 78, loss = 0.01545390
Iteration 79, loss = 0.01527633
Iteration 80, loss = 0.01517436
Iteration 81, loss = 0.01507911
Iteration 82, loss = 0.01492913
Iteration 83, loss = 0.01489050
Iteration 84, loss = 0.01507993
Iteration 85, loss = 0.01524721
Iteration 86, loss = 0.01486307
Iteration 87, loss = 0.01453906
Iteration 88, loss = 0.01433487
Iteration 89, loss = 0.01421919
Iteration 90, loss = 0.01400150
Iteration 91, loss = 0.01391780
Iteration 92, loss = 0.01374093
Iteration 93, loss = 0.01380092
Iteration 94, loss = 0.01380857
Iteration 95, loss = 0.01369184
Iteration 96, loss = 0.01353345
Iteration 97, loss = 0.01352324
Iteration 98, loss = 0.01341053
Iteration 99, loss = 0.01333622
Iteration 100, loss = 0.01325261
Iteration 101, loss = 0.01319021
Iteration 102, loss = 0.01312318
Iteration 103, loss = 0.01289607
Iteration 104, loss = 0.01310559
Iteration 105, loss = 0.01298922
Iteration 106, loss = 0.01293298
Iteration 107, loss = 0.01292468
Iteration 108, loss = 0.01264023
Iteration 109, loss = 0.01286391
Iteration 110, loss = 0.01308786
Iteration 111, loss = 0.01302894
Iteration 112, loss = 0.01286302
Iteration 113, loss = 0.01271384
Iteration 114, loss = 0.01270306
Iteration 115, loss = 0.01250320
Iteration 116, loss = 0.01236985
Iteration 117, loss = 0.01232050
Iteration 118, loss = 0.01232269
Iteration 119, loss = 0.01233713
Iteration 120, loss = 0.01232655
Iteration 121, loss = 0.01238514
Iteration 122, loss = 0.01230242
Iteration 123, loss = 0.01222916
Iteration 124, loss = 0.01228485
Iteration 125, loss = 0.01218352
Iteration 126, loss = 0.01221310
Iteration 127, loss = 0.01214934
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67765803
Iteration 2, loss = 0.62633598
Iteration 3, loss = 0.54694203
Iteration 4, loss = 0.45807118
Iteration 5, loss = 0.37243832
Iteration 6, loss = 0.29709605
Iteration 7, loss = 0.23557645
Iteration 8, loss = 0.18785942
Iteration 9, loss = 0.15209577
Iteration 10, loss = 0.12569653
Iteration 11, loss = 0.10578226
Iteration 12, loss = 0.09098645
Iteration 13, loss = 0.07949169
Iteration 14, loss = 0.07064005
Iteration 15, loss = 0.06355627
Iteration 16, loss = 0.05763435
Iteration 17, loss = 0.05317908
Iteration 18, loss = 0.04916656
Iteration 19, loss = 0.04607393
Iteration 20, loss = 0.04348409
Iteration 21, loss = 0.04093022
Iteration 22, loss = 0.03903384
Iteration 23, loss = 0.03760219
Iteration 24, loss = 0.03588944
Iteration 25, loss = 0.03454482
Iteration 26, loss = 0.03331044
Iteration 27, loss = 0.03242702
Iteration 28, loss = 0.03147997
Iteration 29, loss = 0.03078419
Iteration 30, loss = 0.03000783
Iteration 31, loss = 0.02943454
Iteration 32, loss = 0.02887045
Iteration 33, loss = 0.02830612
Iteration 34, loss = 0.02790915
Iteration 35, loss = 0.02777793
Iteration 36, loss = 0.02703508
Iteration 37, loss = 0.02657580
Iteration 38, loss = 0.02638134
Iteration 39, loss = 0.02592072
Iteration 40, loss = 0.02561281
Iteration 41, loss = 0.02545003
Iteration 42, loss = 0.02529564
Iteration 43, loss = 0.02508349
Iteration 44, loss = 0.02474700
Iteration 45, loss = 0.02466491
Iteration 46, loss = 0.02448501
Iteration 47, loss = 0.02423701
Iteration 48, loss = 0.02388595
Iteration 49, loss = 0.02400503
Iteration 50, loss = 0.02387819
Iteration 51, loss = 0.02388603
Iteration 52, loss = 0.02351328
Iteration 53, loss = 0.02337866
Iteration 54, loss = 0.02334693
Iteration 55, loss = 0.02309691
Iteration 56, loss = 0.02301324
Iteration 57, loss = 0.02298881
Iteration 58, loss = 0.02302731
Iteration 59, loss = 0.02295886
Iteration 60, loss = 0.02300286
Iteration 61, loss = 0.02250000
Iteration 62, loss = 0.02245844
Iteration 63, loss = 0.02227411
Iteration 64, loss = 0.02234245
Iteration 65, loss = 0.02251862
Iteration 66, loss = 0.02222055
Iteration 67, loss = 0.02211531
Iteration 68, loss = 0.02222739
Iteration 69, loss = 0.02247149
Iteration 70, loss = 0.02212243
Iteration 71, loss = 0.02220183
Iteration 72, loss = 0.02210455
Iteration 73, loss = 0.02201655
Iteration 74, loss = 0.02199506
Iteration 75, loss = 0.02191987
Iteration 76, loss = 0.02184129
Iteration 77, loss = 0.02191754
Iteration 78, loss = 0.02177635
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69627254
Iteration 2, loss = 0.64586925
Iteration 3, loss = 0.58076969
Iteration 4, loss = 0.49995061
Iteration 5, loss = 0.41712728
Iteration 6, loss = 0.34105206
Iteration 7, loss = 0.27675101
Iteration 8, loss = 0.22430762
Iteration 9, loss = 0.18343866
Iteration 10, loss = 0.15185813
Iteration 11, loss = 0.12794195
Iteration 12, loss = 0.10958760
Iteration 13, loss = 0.09528250
Iteration 14, loss = 0.08411692
Iteration 15, loss = 0.07558708
Iteration 16, loss = 0.06846047
Iteration 17, loss = 0.06254507
Iteration 18, loss = 0.05777470
Iteration 19, loss = 0.05359879
Iteration 20, loss = 0.05007135
Iteration 21, loss = 0.04733874
Iteration 22, loss = 0.04483732
Iteration 23, loss = 0.04277814
Iteration 24, loss = 0.04094643
Iteration 25, loss = 0.03940099
Iteration 26, loss = 0.03792005
Iteration 27, loss = 0.03664911
Iteration 28, loss = 0.03537929
Iteration 29, loss = 0.03453810
Iteration 30, loss = 0.03349239
Iteration 31, loss = 0.03282154
Iteration 32, loss = 0.03191182
Iteration 33, loss = 0.03142082
Iteration 34, loss = 0.03060669
Iteration 35, loss = 0.03023873
Iteration 36, loss = 0.02974001
Iteration 37, loss = 0.02930841
Iteration 38, loss = 0.02881912
Iteration 39, loss = 0.02841973
Iteration 40, loss = 0.02804507
Iteration 41, loss = 0.02758498
Iteration 42, loss = 0.02733837
Iteration 43, loss = 0.02697430
Iteration 44, loss = 0.02693464
Iteration 45, loss = 0.02648729
Iteration 46, loss = 0.02649459
Iteration 47, loss = 0.02627934
Iteration 48, loss = 0.02607368
Iteration 49, loss = 0.02557345
Iteration 50, loss = 0.02566046
Iteration 51, loss = 0.02559019
Iteration 52, loss = 0.02547886
Iteration 53, loss = 0.02521775
Iteration 54, loss = 0.02518133
Iteration 55, loss = 0.02491627
Iteration 56, loss = 0.02483091
Iteration 57, loss = 0.02486051
Iteration 58, loss = 0.02448910
Iteration 59, loss = 0.02442935
Iteration 60, loss = 0.02439327
Iteration 61, loss = 0.02419962
Iteration 62, loss = 0.02420031
Iteration 63, loss = 0.02421316
Iteration 64, loss = 0.02412984
Iteration 65, loss = 0.02394628
Iteration 66, loss = 0.02394948
Iteration 67, loss = 0.02379301
Iteration 68, loss = 0.02386907
Iteration 69, loss = 0.02358715
Iteration 70, loss = 0.02354540
Iteration 71, loss = 0.02335148
Iteration 72, loss = 0.02366322
Iteration 73, loss = 0.02373274
Iteration 74, loss = 0.02334088
Iteration 75, loss = 0.02333945
Iteration 76, loss = 0.02318626
Iteration 77, loss = 0.02307312
Iteration 78, loss = 0.02330328
Iteration 79, loss = 0.02320615
Iteration 80, loss = 0.02304749
Iteration 81, loss = 0.02293712
Iteration 82, loss = 0.02343746
Iteration 83, loss = 0.02317134
Iteration 84, loss = 0.02304890
Iteration 85, loss = 0.02285483
Iteration 86, loss = 0.02294149
Iteration 87, loss = 0.02290271
Iteration 88, loss = 0.02280893
Iteration 89, loss = 0.02300998
Iteration 90, loss = 0.02301158
Iteration 91, loss = 0.02270787
Iteration 92, loss = 0.02270729
Iteration 93, loss = 0.02283115
Iteration 94, loss = 0.02265079
Iteration 95, loss = 0.02253687
Iteration 96, loss = 0.02232682
Iteration 97, loss = 0.02253893
Iteration 98, loss = 0.02234578
Iteration 99, loss = 0.02276361
Iteration 100, loss = 0.02259597
Iteration 101, loss = 0.02224518
Iteration 102, loss = 0.02229309
Iteration 103, loss = 0.02254514
Iteration 104, loss = 0.02233181
Iteration 105, loss = 0.02236516
Iteration 106, loss = 0.02238039
Iteration 107, loss = 0.02261436
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67957383
Iteration 2, loss = 0.63752251
Iteration 3, loss = 0.57657398
Iteration 4, loss = 0.49594301
Iteration 5, loss = 0.41252795
Iteration 6, loss = 0.33568390
Iteration 7, loss = 0.27042039
Iteration 8, loss = 0.21802653
Iteration 9, loss = 0.17710686
Iteration 10, loss = 0.14610238
Iteration 11, loss = 0.12254109
Iteration 12, loss = 0.10479156
Iteration 13, loss = 0.09095022
Iteration 14, loss = 0.08016684
Iteration 15, loss = 0.07159136
Iteration 16, loss = 0.06461783
Iteration 17, loss = 0.05894648
Iteration 18, loss = 0.05428586
Iteration 19, loss = 0.05040290
Iteration 20, loss = 0.04725696
Iteration 21, loss = 0.04424532
Iteration 22, loss = 0.04199306
Iteration 23, loss = 0.03982508
Iteration 24, loss = 0.03794687
Iteration 25, loss = 0.03648137
Iteration 26, loss = 0.03521355
Iteration 27, loss = 0.03396319
Iteration 28, loss = 0.03276354
Iteration 29, loss = 0.03164821
Iteration 30, loss = 0.03077567
Iteration 31, loss = 0.03009878
Iteration 32, loss = 0.02934777
Iteration 33, loss = 0.02860943
Iteration 34, loss = 0.02816535
Iteration 35, loss = 0.02752329
Iteration 36, loss = 0.02670700
Iteration 37, loss = 0.02657678
Iteration 38, loss = 0.02623479
Iteration 39, loss = 0.02567811
Iteration 40, loss = 0.02546918
Iteration 41, loss = 0.02501522
Iteration 42, loss = 0.02472251
Iteration 43, loss = 0.02451977
Iteration 44, loss = 0.02413681
Iteration 45, loss = 0.02377055
Iteration 46, loss = 0.02355802
Iteration 47, loss = 0.02349859
Iteration 48, loss = 0.02311198
Iteration 49, loss = 0.02288421
Iteration 50, loss = 0.02280551
Iteration 51, loss = 0.02255000
Iteration 52, loss = 0.02254434
Iteration 53, loss = 0.02219071
Iteration 54, loss = 0.02235707
Iteration 55, loss = 0.02211523
Iteration 56, loss = 0.02177622
Iteration 57, loss = 0.02182398
Iteration 58, loss = 0.02146137
Iteration 59, loss = 0.02176870
Iteration 60, loss = 0.02150985
Iteration 61, loss = 0.02153648
Iteration 62, loss = 0.02131907
Iteration 63, loss = 0.02112953
Iteration 64, loss = 0.02104269
Iteration 65, loss = 0.02100473
Iteration 66, loss = 0.02087618
Iteration 67, loss = 0.02085792
Iteration 68, loss = 0.02077746
Iteration 69, loss = 0.02070640
Iteration 70, loss = 0.02063978
Iteration 71, loss = 0.02058317
Iteration 72, loss = 0.02057880
Iteration 73, loss = 0.02052386
Iteration 74, loss = 0.02050468
Iteration 75, loss = 0.02035322
Iteration 76, loss = 0.02041965
Iteration 77, loss = 0.02021161
Iteration 78, loss = 0.02022191
Iteration 79, loss = 0.02011170
Iteration 80, loss = 0.02032379
Iteration 81, loss = 0.02009561
Iteration 82, loss = 0.02011854
Iteration 83, loss = 0.01992036
Iteration 84, loss = 0.01995669
Iteration 85, loss = 0.01976556
Iteration 86, loss = 0.01998307
Iteration 87, loss = 0.02001283
Iteration 88, loss = 0.02006917
Iteration 89, loss = 0.01998830
Iteration 90, loss = 0.01987921
Iteration 91, loss = 0.01981114
Iteration 92, loss = 0.01976913
Iteration 93, loss = 0.01966481
Iteration 94, loss = 0.01981427
Iteration 95, loss = 0.01976090
Iteration 96, loss = 0.01985219
Iteration 97, loss = 0.01959831
Iteration 98, loss = 0.01960450
Iteration 99, loss = 0.01964986
Iteration 100, loss = 0.01976494
Iteration 101, loss = 0.01973882
Iteration 102, loss = 0.01954967
Iteration 103, loss = 0.01927890
Iteration 104, loss = 0.01950870
Iteration 105, loss = 0.01938392
Iteration 106, loss = 0.01920213
Iteration 107, loss = 0.01929553
Iteration 108, loss = 0.01985518
Iteration 109, loss = 0.01932360
Iteration 110, loss = 0.01922048
Iteration 111, loss = 0.01930732
Iteration 112, loss = 0.01942442
Iteration 113, loss = 0.01940102
Iteration 114, loss = 0.01934042
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69359427
Iteration 2, loss = 0.65195009
Iteration 3, loss = 0.59344309
Iteration 4, loss = 0.51659196
Iteration 5, loss = 0.43271651
Iteration 6, loss = 0.35268356
Iteration 7, loss = 0.28367793
Iteration 8, loss = 0.22762399
Iteration 9, loss = 0.18392797
Iteration 10, loss = 0.15057192
Iteration 11, loss = 0.12505688
Iteration 12, loss = 0.10579404
Iteration 13, loss = 0.09100168
Iteration 14, loss = 0.07929597
Iteration 15, loss = 0.07017686
Iteration 16, loss = 0.06272166
Iteration 17, loss = 0.05671505
Iteration 18, loss = 0.05165960
Iteration 19, loss = 0.04762739
Iteration 20, loss = 0.04398035
Iteration 21, loss = 0.04107480
Iteration 22, loss = 0.03847548
Iteration 23, loss = 0.03652246
Iteration 24, loss = 0.03450786
Iteration 25, loss = 0.03296019
Iteration 26, loss = 0.03153982
Iteration 27, loss = 0.03024376
Iteration 28, loss = 0.02902394
Iteration 29, loss = 0.02802189
Iteration 30, loss = 0.02711079
Iteration 31, loss = 0.02653075
Iteration 32, loss = 0.02564125
Iteration 33, loss = 0.02478967
Iteration 34, loss = 0.02433377
Iteration 35, loss = 0.02380891
Iteration 36, loss = 0.02321630
Iteration 37, loss = 0.02280569
Iteration 38, loss = 0.02236333
Iteration 39, loss = 0.02214445
Iteration 40, loss = 0.02155577
Iteration 41, loss = 0.02137598
Iteration 42, loss = 0.02097209
Iteration 43, loss = 0.02058910
Iteration 44, loss = 0.02038009
Iteration 45, loss = 0.02029639
Iteration 46, loss = 0.01984590
Iteration 47, loss = 0.01971592
Iteration 48, loss = 0.01956870
Iteration 49, loss = 0.01933580
Iteration 50, loss = 0.01904672
Iteration 51, loss = 0.01923294
Iteration 52, loss = 0.01869975
Iteration 53, loss = 0.01865473
Iteration 54, loss = 0.01854357
Iteration 55, loss = 0.01832721
Iteration 56, loss = 0.01822879
Iteration 57, loss = 0.01808089
Iteration 58, loss = 0.01784244
Iteration 59, loss = 0.01807078
Iteration 60, loss = 0.01792027
Iteration 61, loss = 0.01767921
Iteration 62, loss = 0.01804836
Iteration 63, loss = 0.01772605
Iteration 64, loss = 0.01740588
Iteration 65, loss = 0.01732991
Iteration 66, loss = 0.01746037
Iteration 67, loss = 0.01718346
Iteration 68, loss = 0.01717721
Iteration 69, loss = 0.01699098
Iteration 70, loss = 0.01733531
Iteration 71, loss = 0.01737582
Iteration 72, loss = 0.01686315
Iteration 73, loss = 0.01692543
Iteration 74, loss = 0.01678328
Iteration 75, loss = 0.01662640
Iteration 76, loss = 0.01653140
Iteration 77, loss = 0.01647451
Iteration 78, loss = 0.01654923
Iteration 79, loss = 0.01667577
Iteration 80, loss = 0.01657960
Iteration 81, loss = 0.01657136
Iteration 82, loss = 0.01651485
Iteration 83, loss = 0.01628902
Iteration 84, loss = 0.01628904
Iteration 85, loss = 0.01622318
Iteration 86, loss = 0.01646357
Iteration 87, loss = 0.01638976
Iteration 88, loss = 0.01602763
Iteration 89, loss = 0.01625168
Iteration 90, loss = 0.01615475
Iteration 91, loss = 0.01619901
Iteration 92, loss = 0.01604474
Iteration 93, loss = 0.01595522
Iteration 94, loss = 0.01607366
Iteration 95, loss = 0.01605179
Iteration 96, loss = 0.01612578
Iteration 97, loss = 0.01580256
Iteration 98, loss = 0.01597130
Iteration 99, loss = 0.01588107
Iteration 100, loss = 0.01596423
Iteration 101, loss = 0.01582498
Iteration 102, loss = 0.01596286
Iteration 103, loss = 0.01574769
Iteration 104, loss = 0.01592553
Iteration 105, loss = 0.01601509
Iteration 106, loss = 0.01621402
Iteration 107, loss = 0.01579917
Iteration 108, loss = 0.01585971
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67562909
Iteration 2, loss = 0.61924840
Iteration 3, loss = 0.53949526
Iteration 4, loss = 0.44564750
Iteration 5, loss = 0.35446874
Iteration 6, loss = 0.27762524
Iteration 7, loss = 0.21736596
Iteration 8, loss = 0.17210608
Iteration 9, loss = 0.13840478
Iteration 10, loss = 0.11398293
Iteration 11, loss = 0.09546723
Iteration 12, loss = 0.08164364
Iteration 13, loss = 0.07094254
Iteration 14, loss = 0.06245584
Iteration 15, loss = 0.05586892
Iteration 16, loss = 0.05046555
Iteration 17, loss = 0.04624782
Iteration 18, loss = 0.04254719
Iteration 19, loss = 0.03938617
Iteration 20, loss = 0.03707539
Iteration 21, loss = 0.03495084
Iteration 22, loss = 0.03287452
Iteration 23, loss = 0.03138771
Iteration 24, loss = 0.02999164
Iteration 25, loss = 0.02859400
Iteration 26, loss = 0.02774424
Iteration 27, loss = 0.02667161
Iteration 28, loss = 0.02566288
Iteration 29, loss = 0.02490299
Iteration 30, loss = 0.02422447
Iteration 31, loss = 0.02379185
Iteration 32, loss = 0.02333743
Iteration 33, loss = 0.02252270
Iteration 34, loss = 0.02212876
Iteration 35, loss = 0.02176644
Iteration 36, loss = 0.02132886
Iteration 37, loss = 0.02114481
Iteration 38, loss = 0.02080086
Iteration 39, loss = 0.02047604
Iteration 40, loss = 0.02026006
Iteration 41, loss = 0.01984203
Iteration 42, loss = 0.01971511
Iteration 43, loss = 0.01945384
Iteration 44, loss = 0.01902211
Iteration 45, loss = 0.01896902
Iteration 46, loss = 0.01899485
Iteration 47, loss = 0.01856907
Iteration 48, loss = 0.01862382
Iteration 49, loss = 0.01852408
Iteration 50, loss = 0.01819116
Iteration 51, loss = 0.01809836
Iteration 52, loss = 0.01791643
Iteration 53, loss = 0.01810664
Iteration 54, loss = 0.01777758
Iteration 55, loss = 0.01768381
Iteration 56, loss = 0.01752612
Iteration 57, loss = 0.01747131
Iteration 58, loss = 0.01737425
Iteration 59, loss = 0.01744651
Iteration 60, loss = 0.01737062
Iteration 61, loss = 0.01735749
Iteration 62, loss = 0.01716507
Iteration 63, loss = 0.01710648
Iteration 64, loss = 0.01721617
Iteration 65, loss = 0.01699365
Iteration 66, loss = 0.01682625
Iteration 67, loss = 0.01673166
Iteration 68, loss = 0.01665516
Iteration 69, loss = 0.01682594
Iteration 70, loss = 0.01665567
Iteration 71, loss = 0.01670187
Iteration 72, loss = 0.01652041
Iteration 73, loss = 0.01669595
Iteration 74, loss = 0.01668403
Iteration 75, loss = 0.01663496
Iteration 76, loss = 0.01644187
Iteration 77, loss = 0.01639061
Iteration 78, loss = 0.01646104
Iteration 79, loss = 0.01636818
Iteration 80, loss = 0.01636076
Iteration 81, loss = 0.01645154
Iteration 82, loss = 0.01654010
Iteration 83, loss = 0.01652781
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68051360
Iteration 2, loss = 0.64171353
Iteration 3, loss = 0.56957498
Iteration 4, loss = 0.47662816
Iteration 5, loss = 0.38545605
Iteration 6, loss = 0.30495910
Iteration 7, loss = 0.24015475
Iteration 8, loss = 0.19003333
Iteration 9, loss = 0.15310087
Iteration 10, loss = 0.12579775
Iteration 11, loss = 0.10529489
Iteration 12, loss = 0.09037063
Iteration 13, loss = 0.07871636
Iteration 14, loss = 0.06956704
Iteration 15, loss = 0.06229735
Iteration 16, loss = 0.05685653
Iteration 17, loss = 0.05218731
Iteration 18, loss = 0.04799639
Iteration 19, loss = 0.04484051
Iteration 20, loss = 0.04207507
Iteration 21, loss = 0.03966770
Iteration 22, loss = 0.03782593
Iteration 23, loss = 0.03590459
Iteration 24, loss = 0.03446598
Iteration 25, loss = 0.03307649
Iteration 26, loss = 0.03214755
Iteration 27, loss = 0.03096688
Iteration 28, loss = 0.03010012
Iteration 29, loss = 0.02930478
Iteration 30, loss = 0.02846988
Iteration 31, loss = 0.02790721
Iteration 32, loss = 0.02725594
Iteration 33, loss = 0.02697279
Iteration 34, loss = 0.02632001
Iteration 35, loss = 0.02592407
Iteration 36, loss = 0.02546838
Iteration 37, loss = 0.02496973
Iteration 38, loss = 0.02461904
Iteration 39, loss = 0.02480051
Iteration 40, loss = 0.02403924
Iteration 41, loss = 0.02383214
Iteration 42, loss = 0.02389730
Iteration 43, loss = 0.02332625
Iteration 44, loss = 0.02316274
Iteration 45, loss = 0.02290699
Iteration 46, loss = 0.02271658
Iteration 47, loss = 0.02286189
Iteration 48, loss = 0.02242893
Iteration 49, loss = 0.02237353
Iteration 50, loss = 0.02195979
Iteration 51, loss = 0.02209461
Iteration 52, loss = 0.02194716
Iteration 53, loss = 0.02185065
Iteration 54, loss = 0.02146897
Iteration 55, loss = 0.02178723
Iteration 56, loss = 0.02147191
Iteration 57, loss = 0.02142350
Iteration 58, loss = 0.02138008
Iteration 59, loss = 0.02100344
Iteration 60, loss = 0.02127239
Iteration 61, loss = 0.02101534
Iteration 62, loss = 0.02080052
Iteration 63, loss = 0.02082886
Iteration 64, loss = 0.02075278
Iteration 65, loss = 0.02107036
Iteration 66, loss = 0.02067439
Iteration 67, loss = 0.02058855
Iteration 68, loss = 0.02034808
Iteration 69, loss = 0.02047466
Iteration 70, loss = 0.02063086
Iteration 71, loss = 0.02055876
Iteration 72, loss = 0.02051302
Iteration 73, loss = 0.02024754
Iteration 74, loss = 0.02041125
Iteration 75, loss = 0.02030995
Iteration 76, loss = 0.02025534
Iteration 77, loss = 0.02004549
Iteration 78, loss = 0.02001891
Iteration 79, loss = 0.02029433
Iteration 80, loss = 0.02007714
Iteration 81, loss = 0.01987745
Iteration 82, loss = 0.02008560
Iteration 83, loss = 0.01984113
Iteration 84, loss = 0.01988138
Iteration 85, loss = 0.01998829
Iteration 86, loss = 0.01996837
Iteration 87, loss = 0.01991627
Iteration 88, loss = 0.01992601
Iteration 89, loss = 0.01993190
Iteration 90, loss = 0.02002244
Iteration 91, loss = 0.01978335
Iteration 92, loss = 0.01996116
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72202600
Iteration 2, loss = 0.69379624
Iteration 3, loss = 0.65681158
Iteration 4, loss = 0.61036657
Iteration 5, loss = 0.55464524
Iteration 6, loss = 0.49363061
Iteration 7, loss = 0.43096133
Iteration 8, loss = 0.37161531
Iteration 9, loss = 0.31736764
Iteration 10, loss = 0.26994345
Iteration 11, loss = 0.22937538
Iteration 12, loss = 0.19556569
Iteration 13, loss = 0.16785022
Iteration 14, loss = 0.14525590
Iteration 15, loss = 0.12668548
Iteration 16, loss = 0.11139478
Iteration 17, loss = 0.09894008
Iteration 18, loss = 0.08851992
Iteration 19, loss = 0.07985399
Iteration 20, loss = 0.07244168
Iteration 21, loss = 0.06630122
Iteration 22, loss = 0.06095031
Iteration 23, loss = 0.05647881
Iteration 24, loss = 0.05251201
Iteration 25, loss = 0.04901479
Iteration 26, loss = 0.04608425
Iteration 27, loss = 0.04348556
Iteration 28, loss = 0.04117231
Iteration 29, loss = 0.03904441
Iteration 30, loss = 0.03721577
Iteration 31, loss = 0.03544945
Iteration 32, loss = 0.03404982
Iteration 33, loss = 0.03251571
Iteration 34, loss = 0.03133328
Iteration 35, loss = 0.03018247
Iteration 36, loss = 0.02913736
Iteration 37, loss = 0.02818033
Iteration 38, loss = 0.02733730
Iteration 39, loss = 0.02664825
Iteration 40, loss = 0.02580861
Iteration 41, loss = 0.02533796
Iteration 42, loss = 0.02449018
Iteration 43, loss = 0.02388189
Iteration 44, loss = 0.02346423
Iteration 45, loss = 0.02296903
Iteration 46, loss = 0.02241301
Iteration 47, loss = 0.02196443
Iteration 48, loss = 0.02159816
Iteration 49, loss = 0.02115105
Iteration 50, loss = 0.02086591
Iteration 51, loss = 0.02044219
Iteration 52, loss = 0.02017914
Iteration 53, loss = 0.01987670
Iteration 54, loss = 0.01963253
Iteration 55, loss = 0.01928688
Iteration 56, loss = 0.01910370
Iteration 57, loss = 0.01892690
Iteration 58, loss = 0.01877456
Iteration 59, loss = 0.01842545
Iteration 60, loss = 0.01833797
Iteration 61, loss = 0.01813522
Iteration 62, loss = 0.01791815
Iteration 63, loss = 0.01779436
Iteration 64, loss = 0.01757917
Iteration 65, loss = 0.01738474
Iteration 66, loss = 0.01723364
Iteration 67, loss = 0.01706689
Iteration 68, loss = 0.01694190
Iteration 69, loss = 0.01685193
Iteration 70, loss = 0.01672046
Iteration 71, loss = 0.01659794
Iteration 72, loss = 0.01648676
Iteration 73, loss = 0.01638459
Iteration 74, loss = 0.01627878
Iteration 75, loss = 0.01606454
Iteration 76, loss = 0.01613141
Iteration 77, loss = 0.01604897
Iteration 78, loss = 0.01597544
Iteration 79, loss = 0.01579790
Iteration 80, loss = 0.01587279
Iteration 81, loss = 0.01564658
Iteration 82, loss = 0.01557788
Iteration 83, loss = 0.01554969
Iteration 84, loss = 0.01539412
Iteration 85, loss = 0.01536886
Iteration 86, loss = 0.01520176
Iteration 87, loss = 0.01517511
Iteration 88, loss = 0.01510667
Iteration 89, loss = 0.01505069
Iteration 90, loss = 0.01497315
Iteration 91, loss = 0.01503358
Iteration 92, loss = 0.01485386
Iteration 93, loss = 0.01489633
Iteration 94, loss = 0.01483603
Iteration 95, loss = 0.01476371
Iteration 96, loss = 0.01466338
Iteration 97, loss = 0.01466738
Iteration 98, loss = 0.01464549
Iteration 99, loss = 0.01466745
Iteration 100, loss = 0.01453380
Iteration 101, loss = 0.01447906
Iteration 102, loss = 0.01452631
Iteration 103, loss = 0.01444461
Iteration 104, loss = 0.01442307
Iteration 105, loss = 0.01440707
Iteration 106, loss = 0.01436309
Iteration 107, loss = 0.01434021
Iteration 108, loss = 0.01423224
Iteration 109, loss = 0.01435631
Iteration 110, loss = 0.01415480
Iteration 111, loss = 0.01423870
Iteration 112, loss = 0.01424618
Iteration 113, loss = 0.01418808
Iteration 114, loss = 0.01426142
Iteration 115, loss = 0.01410014
Iteration 116, loss = 0.01404916
Iteration 117, loss = 0.01391935
Iteration 118, loss = 0.01400318
Iteration 119, loss = 0.01398183
Iteration 120, loss = 0.01397705
Iteration 121, loss = 0.01399372
Iteration 122, loss = 0.01405842
Iteration 123, loss = 0.01378497
Iteration 124, loss = 0.01379531
Iteration 125, loss = 0.01390867
Iteration 126, loss = 0.01381210
Iteration 127, loss = 0.01380856
Iteration 128, loss = 0.01387351
Iteration 129, loss = 0.01388734
Iteration 130, loss = 0.01370580
Iteration 131, loss = 0.01376333
Iteration 132, loss = 0.01367651
Iteration 133, loss = 0.01371284
Iteration 134, loss = 0.01363286
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69048909
Iteration 2, loss = 0.65679766
Iteration 3, loss = 0.61269274
Iteration 4, loss = 0.55869064
Iteration 5, loss = 0.49841811
Iteration 6, loss = 0.43693572
Iteration 7, loss = 0.37891802
Iteration 8, loss = 0.32602252
Iteration 9, loss = 0.27915731
Iteration 10, loss = 0.23831470
Iteration 11, loss = 0.20411404
Iteration 12, loss = 0.17504598
Iteration 13, loss = 0.15124262
Iteration 14, loss = 0.13137939
Iteration 15, loss = 0.11510962
Iteration 16, loss = 0.10150325
Iteration 17, loss = 0.09043586
Iteration 18, loss = 0.08121257
Iteration 19, loss = 0.07340326
Iteration 20, loss = 0.06679243
Iteration 21, loss = 0.06122702
Iteration 22, loss = 0.05651818
Iteration 23, loss = 0.05235601
Iteration 24, loss = 0.04872768
Iteration 25, loss = 0.04560891
Iteration 26, loss = 0.04288821
Iteration 27, loss = 0.04038675
Iteration 28, loss = 0.03828423
Iteration 29, loss = 0.03640001
Iteration 30, loss = 0.03461844
Iteration 31, loss = 0.03304988
Iteration 32, loss = 0.03163934
Iteration 33, loss = 0.03057432
Iteration 34, loss = 0.02939402
Iteration 35, loss = 0.02824022
Iteration 36, loss = 0.02740676
Iteration 37, loss = 0.02657167
Iteration 38, loss = 0.02575241
Iteration 39, loss = 0.02503256
Iteration 40, loss = 0.02436302
Iteration 41, loss = 0.02378109
Iteration 42, loss = 0.02310194
Iteration 43, loss = 0.02255019
Iteration 44, loss = 0.02205091
Iteration 45, loss = 0.02160457
Iteration 46, loss = 0.02119177
Iteration 47, loss = 0.02072685
Iteration 48, loss = 0.02044052
Iteration 49, loss = 0.02006836
Iteration 50, loss = 0.01974267
Iteration 51, loss = 0.01957245
Iteration 52, loss = 0.01915087
Iteration 53, loss = 0.01890101
Iteration 54, loss = 0.01868693
Iteration 55, loss = 0.01856552
Iteration 56, loss = 0.01818404
Iteration 57, loss = 0.01793807
Iteration 58, loss = 0.01775333
Iteration 59, loss = 0.01757567
Iteration 60, loss = 0.01724852
Iteration 61, loss = 0.01718817
Iteration 62, loss = 0.01705684
Iteration 63, loss = 0.01707009
Iteration 64, loss = 0.01672357
Iteration 65, loss = 0.01656436
Iteration 66, loss = 0.01639868
Iteration 67, loss = 0.01631337
Iteration 68, loss = 0.01621810
Iteration 69, loss = 0.01597321
Iteration 70, loss = 0.01599688
Iteration 71, loss = 0.01587024
Iteration 72, loss = 0.01574247
Iteration 73, loss = 0.01558720
Iteration 74, loss = 0.01564699
Iteration 75, loss = 0.01541296
Iteration 76, loss = 0.01545404
Iteration 77, loss = 0.01531404
Iteration 78, loss = 0.01506922
Iteration 79, loss = 0.01517237
Iteration 80, loss = 0.01501167
Iteration 81, loss = 0.01503858
Iteration 82, loss = 0.01486350
Iteration 83, loss = 0.01489920
Iteration 84, loss = 0.01469408
Iteration 85, loss = 0.01471272
Iteration 86, loss = 0.01477917
Iteration 87, loss = 0.01454123
Iteration 88, loss = 0.01449069
Iteration 89, loss = 0.01446071
Iteration 90, loss = 0.01442864
Iteration 91, loss = 0.01449031
Iteration 92, loss = 0.01435803
Iteration 93, loss = 0.01422338
Iteration 94, loss = 0.01427709
Iteration 95, loss = 0.01418892
Iteration 96, loss = 0.01420306
Iteration 97, loss = 0.01427362
Iteration 98, loss = 0.01428338
Iteration 99, loss = 0.01410696
Iteration 100, loss = 0.01396373
Iteration 101, loss = 0.01394086
Iteration 102, loss = 0.01406090
Iteration 103, loss = 0.01394562
Iteration 104, loss = 0.01389921
Iteration 105, loss = 0.01390400
Iteration 106, loss = 0.01378346
Iteration 107, loss = 0.01373886
Iteration 108, loss = 0.01380439
Iteration 109, loss = 0.01376390
Iteration 110, loss = 0.01375127
Iteration 111, loss = 0.01375817
Iteration 112, loss = 0.01374408
Iteration 113, loss = 0.01364059
Iteration 114, loss = 0.01363987
Iteration 115, loss = 0.01354917
Iteration 116, loss = 0.01352292
Iteration 117, loss = 0.01356330
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67964368
Iteration 2, loss = 0.64188441
Iteration 3, loss = 0.59273852
Iteration 4, loss = 0.52777424
Iteration 5, loss = 0.45539866
Iteration 6, loss = 0.38406251
Iteration 7, loss = 0.32009877
Iteration 8, loss = 0.26458385
Iteration 9, loss = 0.21881591
Iteration 10, loss = 0.18214895
Iteration 11, loss = 0.15244476
Iteration 12, loss = 0.12928836
Iteration 13, loss = 0.11097286
Iteration 14, loss = 0.09642913
Iteration 15, loss = 0.08474057
Iteration 16, loss = 0.07517717
Iteration 17, loss = 0.06752544
Iteration 18, loss = 0.06103861
Iteration 19, loss = 0.05576082
Iteration 20, loss = 0.05120144
Iteration 21, loss = 0.04740937
Iteration 22, loss = 0.04422353
Iteration 23, loss = 0.04140329
Iteration 24, loss = 0.03886122
Iteration 25, loss = 0.03667087
Iteration 26, loss = 0.03484011
Iteration 27, loss = 0.03309975
Iteration 28, loss = 0.03176675
Iteration 29, loss = 0.03028213
Iteration 30, loss = 0.02906882
Iteration 31, loss = 0.02807743
Iteration 32, loss = 0.02704760
Iteration 33, loss = 0.02631823
Iteration 34, loss = 0.02543706
Iteration 35, loss = 0.02481262
Iteration 36, loss = 0.02396088
Iteration 37, loss = 0.02341470
Iteration 38, loss = 0.02292024
Iteration 39, loss = 0.02236207
Iteration 40, loss = 0.02197328
Iteration 41, loss = 0.02144098
Iteration 42, loss = 0.02108033
Iteration 43, loss = 0.02072859
Iteration 44, loss = 0.02027356
Iteration 45, loss = 0.01995880
Iteration 46, loss = 0.01974793
Iteration 47, loss = 0.01931721
Iteration 48, loss = 0.01916524
Iteration 49, loss = 0.01888113
Iteration 50, loss = 0.01867869
Iteration 51, loss = 0.01846251
Iteration 52, loss = 0.01825403
Iteration 53, loss = 0.01812285
Iteration 54, loss = 0.01801844
Iteration 55, loss = 0.01758908
Iteration 56, loss = 0.01752932
Iteration 57, loss = 0.01742461
Iteration 58, loss = 0.01726972
Iteration 59, loss = 0.01723055
Iteration 60, loss = 0.01707239
Iteration 61, loss = 0.01702409
Iteration 62, loss = 0.01688653
Iteration 63, loss = 0.01675972
Iteration 64, loss = 0.01666167
Iteration 65, loss = 0.01656697
Iteration 66, loss = 0.01636709
Iteration 67, loss = 0.01626141
Iteration 68, loss = 0.01614579
Iteration 69, loss = 0.01609166
Iteration 70, loss = 0.01607415
Iteration 71, loss = 0.01587704
Iteration 72, loss = 0.01572355
Iteration 73, loss = 0.01579243
Iteration 74, loss = 0.01578549
Iteration 75, loss = 0.01554700
Iteration 76, loss = 0.01569196
Iteration 77, loss = 0.01556622
Iteration 78, loss = 0.01546390
Iteration 79, loss = 0.01544324
Iteration 80, loss = 0.01546882
Iteration 81, loss = 0.01540713
Iteration 82, loss = 0.01520081
Iteration 83, loss = 0.01519083
Iteration 84, loss = 0.01518124
Iteration 85, loss = 0.01509738
Iteration 86, loss = 0.01513620
Iteration 87, loss = 0.01502445
Iteration 88, loss = 0.01501026
Iteration 89, loss = 0.01506560
Iteration 90, loss = 0.01496785
Iteration 91, loss = 0.01490087
Iteration 92, loss = 0.01484950
Iteration 93, loss = 0.01497556
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68574063
Iteration 2, loss = 0.64668384
Iteration 3, loss = 0.59283113
Iteration 4, loss = 0.52693549
Iteration 5, loss = 0.45526074
Iteration 6, loss = 0.38467010
Iteration 7, loss = 0.32039905
Iteration 8, loss = 0.26525628
Iteration 9, loss = 0.21977497
Iteration 10, loss = 0.18322283
Iteration 11, loss = 0.15386651
Iteration 12, loss = 0.13086667
Iteration 13, loss = 0.11256262
Iteration 14, loss = 0.09810866
Iteration 15, loss = 0.08665528
Iteration 16, loss = 0.07723947
Iteration 17, loss = 0.06947202
Iteration 18, loss = 0.06316419
Iteration 19, loss = 0.05783833
Iteration 20, loss = 0.05344153
Iteration 21, loss = 0.04958985
Iteration 22, loss = 0.04631062
Iteration 23, loss = 0.04361932
Iteration 24, loss = 0.04107955
Iteration 25, loss = 0.03886848
Iteration 26, loss = 0.03706984
Iteration 27, loss = 0.03546198
Iteration 28, loss = 0.03388929
Iteration 29, loss = 0.03256264
Iteration 30, loss = 0.03145503
Iteration 31, loss = 0.03032883
Iteration 32, loss = 0.02947522
Iteration 33, loss = 0.02861521
Iteration 34, loss = 0.02770798
Iteration 35, loss = 0.02701956
Iteration 36, loss = 0.02650781
Iteration 37, loss = 0.02571529
Iteration 38, loss = 0.02523576
Iteration 39, loss = 0.02490991
Iteration 40, loss = 0.02439001
Iteration 41, loss = 0.02387274
Iteration 42, loss = 0.02353533
Iteration 43, loss = 0.02306196
Iteration 44, loss = 0.02272407
Iteration 45, loss = 0.02245901
Iteration 46, loss = 0.02217145
Iteration 47, loss = 0.02186497
Iteration 48, loss = 0.02160906
Iteration 49, loss = 0.02133251
Iteration 50, loss = 0.02104542
Iteration 51, loss = 0.02086139
Iteration 52, loss = 0.02060757
Iteration 53, loss = 0.02051162
Iteration 54, loss = 0.02034367
Iteration 55, loss = 0.02017487
Iteration 56, loss = 0.01988490
Iteration 57, loss = 0.01988582
Iteration 58, loss = 0.01968092
Iteration 59, loss = 0.01954075
Iteration 60, loss = 0.01951163
Iteration 61, loss = 0.01930596
Iteration 62, loss = 0.01912884
Iteration 63, loss = 0.01915581
Iteration 64, loss = 0.01917917
Iteration 65, loss = 0.01885940
Iteration 66, loss = 0.01886017
Iteration 67, loss = 0.01873233
Iteration 68, loss = 0.01856321
Iteration 69, loss = 0.01856394
Iteration 70, loss = 0.01859202
Iteration 71, loss = 0.01841877
Iteration 72, loss = 0.01819322
Iteration 73, loss = 0.01823014
Iteration 74, loss = 0.01818157
Iteration 75, loss = 0.01817207
Iteration 76, loss = 0.01791246
Iteration 77, loss = 0.01791831
Iteration 78, loss = 0.01786382
Iteration 79, loss = 0.01777582
Iteration 80, loss = 0.01791354
Iteration 81, loss = 0.01779851
Iteration 82, loss = 0.01763069
Iteration 83, loss = 0.01771512
Iteration 84, loss = 0.01761175
Iteration 85, loss = 0.01759181
Iteration 86, loss = 0.01750437
Iteration 87, loss = 0.01745029
Iteration 88, loss = 0.01735978
Iteration 89, loss = 0.01746380
Iteration 90, loss = 0.01735640
Iteration 91, loss = 0.01741012
Iteration 92, loss = 0.01724591
Iteration 93, loss = 0.01723914
Iteration 94, loss = 0.01723762
Iteration 95, loss = 0.01714734
Iteration 96, loss = 0.01706253
Iteration 97, loss = 0.01711851
Iteration 98, loss = 0.01702645
Iteration 99, loss = 0.01726614
Iteration 100, loss = 0.01735276
Iteration 101, loss = 0.01705702
Iteration 102, loss = 0.01702771
Iteration 103, loss = 0.01716259
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71007740
Iteration 2, loss = 0.68547404
Iteration 3, loss = 0.65599839
Iteration 4, loss = 0.61805315
Iteration 5, loss = 0.57313684
Iteration 6, loss = 0.52343678
Iteration 7, loss = 0.47173294
Iteration 8, loss = 0.41999945
Iteration 9, loss = 0.37105208
Iteration 10, loss = 0.32596034
Iteration 11, loss = 0.28547008
Iteration 12, loss = 0.24969495
Iteration 13, loss = 0.21869387
Iteration 14, loss = 0.19227295
Iteration 15, loss = 0.16965068
Iteration 16, loss = 0.15043462
Iteration 17, loss = 0.13397958
Iteration 18, loss = 0.12026060
Iteration 19, loss = 0.10846180
Iteration 20, loss = 0.09839564
Iteration 21, loss = 0.08976487
Iteration 22, loss = 0.08241535
Iteration 23, loss = 0.07601860
Iteration 24, loss = 0.07040579
Iteration 25, loss = 0.06573258
Iteration 26, loss = 0.06132544
Iteration 27, loss = 0.05750737
Iteration 28, loss = 0.05409891
Iteration 29, loss = 0.05107921
Iteration 30, loss = 0.04835309
Iteration 31, loss = 0.04579440
Iteration 32, loss = 0.04375559
Iteration 33, loss = 0.04186827
Iteration 34, loss = 0.04006965
Iteration 35, loss = 0.03848447
Iteration 36, loss = 0.03701244
Iteration 37, loss = 0.03559913
Iteration 38, loss = 0.03430925
Iteration 39, loss = 0.03312736
Iteration 40, loss = 0.03226578
Iteration 41, loss = 0.03132031
Iteration 42, loss = 0.03034821
Iteration 43, loss = 0.02956022
Iteration 44, loss = 0.02870082
Iteration 45, loss = 0.02793973
Iteration 46, loss = 0.02717129
Iteration 47, loss = 0.02660388
Iteration 48, loss = 0.02594470
Iteration 49, loss = 0.02536760
Iteration 50, loss = 0.02487674
Iteration 51, loss = 0.02428327
Iteration 52, loss = 0.02395603
Iteration 53, loss = 0.02364649
Iteration 54, loss = 0.02325928
Iteration 55, loss = 0.02282144
Iteration 56, loss = 0.02246246
Iteration 57, loss = 0.02211319
Iteration 58, loss = 0.02200993
Iteration 59, loss = 0.02156288
Iteration 60, loss = 0.02124044
Iteration 61, loss = 0.02095699
Iteration 62, loss = 0.02059953
Iteration 63, loss = 0.02045806
Iteration 64, loss = 0.02024702
Iteration 65, loss = 0.01999344
Iteration 66, loss = 0.01991743
Iteration 67, loss = 0.01961774
Iteration 68, loss = 0.01938747
Iteration 69, loss = 0.01910888
Iteration 70, loss = 0.01888719
Iteration 71, loss = 0.01873097
Iteration 72, loss = 0.01850416
Iteration 73, loss = 0.01838407
Iteration 74, loss = 0.01818875
Iteration 75, loss = 0.01806363
Iteration 76, loss = 0.01789214
Iteration 77, loss = 0.01778482
Iteration 78, loss = 0.01752978
Iteration 79, loss = 0.01755883
Iteration 80, loss = 0.01743242
Iteration 81, loss = 0.01732548
Iteration 82, loss = 0.01716012
Iteration 83, loss = 0.01695494
Iteration 84, loss = 0.01714284
Iteration 85, loss = 0.01689151
Iteration 86, loss = 0.01697857
Iteration 87, loss = 0.01691454
Iteration 88, loss = 0.01679765
Iteration 89, loss = 0.01676966
Iteration 90, loss = 0.01663182
Iteration 91, loss = 0.01624464
Iteration 92, loss = 0.01617210
Iteration 93, loss = 0.01609998
Iteration 94, loss = 0.01598027
Iteration 95, loss = 0.01595536
Iteration 96, loss = 0.01580696
Iteration 97, loss = 0.01597679
Iteration 98, loss = 0.01591321
Iteration 99, loss = 0.01578786
Iteration 100, loss = 0.01601672
Iteration 101, loss = 0.01581735
Iteration 102, loss = 0.01570692
Iteration 103, loss = 0.01561497
Iteration 104, loss = 0.01540878
Iteration 105, loss = 0.01538157
Iteration 106, loss = 0.01520915
Iteration 107, loss = 0.01524840
Iteration 108, loss = 0.01518820
Iteration 109, loss = 0.01514259
Iteration 110, loss = 0.01515500
Iteration 111, loss = 0.01503498
Iteration 112, loss = 0.01488418
Iteration 113, loss = 0.01499898
Iteration 114, loss = 0.01494378
Iteration 115, loss = 0.01484632
Iteration 116, loss = 0.01483703
Iteration 117, loss = 0.01497307
Iteration 118, loss = 0.01487746
Iteration 119, loss = 0.01510028
Iteration 120, loss = 0.01506787
Iteration 121, loss = 0.01484819
Iteration 122, loss = 0.01473457
Iteration 123, loss = 0.01458242
Iteration 124, loss = 0.01465760
Iteration 125, loss = 0.01443857
Iteration 126, loss = 0.01449845
Iteration 127, loss = 0.01443322
Iteration 128, loss = 0.01445369
Iteration 129, loss = 0.01456941
Iteration 130, loss = 0.01473347
Iteration 131, loss = 0.01453209
Iteration 132, loss = 0.01464619
Iteration 133, loss = 0.01447748
Iteration 134, loss = 0.01443392
Iteration 135, loss = 0.01467726
Iteration 136, loss = 0.01462902
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67603000
Iteration 2, loss = 0.62981333
Iteration 3, loss = 0.56353129
Iteration 4, loss = 0.48263089
Iteration 5, loss = 0.40110289
Iteration 6, loss = 0.32596142
Iteration 7, loss = 0.26265013
Iteration 8, loss = 0.21182926
Iteration 9, loss = 0.17270115
Iteration 10, loss = 0.14284409
Iteration 11, loss = 0.12017178
Iteration 12, loss = 0.10289522
Iteration 13, loss = 0.08961231
Iteration 14, loss = 0.07927698
Iteration 15, loss = 0.07079418
Iteration 16, loss = 0.06405395
Iteration 17, loss = 0.05870491
Iteration 18, loss = 0.05407524
Iteration 19, loss = 0.05040758
Iteration 20, loss = 0.04716129
Iteration 21, loss = 0.04430997
Iteration 22, loss = 0.04193482
Iteration 23, loss = 0.03984455
Iteration 24, loss = 0.03815881
Iteration 25, loss = 0.03648063
Iteration 26, loss = 0.03516032
Iteration 27, loss = 0.03410337
Iteration 28, loss = 0.03292297
Iteration 29, loss = 0.03203175
Iteration 30, loss = 0.03117421
Iteration 31, loss = 0.03006637
Iteration 32, loss = 0.02948829
Iteration 33, loss = 0.02898774
Iteration 34, loss = 0.02836453
Iteration 35, loss = 0.02786627
Iteration 36, loss = 0.02740270
Iteration 37, loss = 0.02683343
Iteration 38, loss = 0.02687760
Iteration 39, loss = 0.02620955
Iteration 40, loss = 0.02572021
Iteration 41, loss = 0.02542727
Iteration 42, loss = 0.02524094
Iteration 43, loss = 0.02480991
Iteration 44, loss = 0.02442570
Iteration 45, loss = 0.02426518
Iteration 46, loss = 0.02404885
Iteration 47, loss = 0.02393653
Iteration 48, loss = 0.02389278
Iteration 49, loss = 0.02336930
Iteration 50, loss = 0.02335109
Iteration 51, loss = 0.02318827
Iteration 52, loss = 0.02304044
Iteration 53, loss = 0.02280518
Iteration 54, loss = 0.02275694
Iteration 55, loss = 0.02262368
Iteration 56, loss = 0.02254226
Iteration 57, loss = 0.02259615
Iteration 58, loss = 0.02225367
Iteration 59, loss = 0.02207370
Iteration 60, loss = 0.02207069
Iteration 61, loss = 0.02192753
Iteration 62, loss = 0.02201606
Iteration 63, loss = 0.02167444
Iteration 64, loss = 0.02182869
Iteration 65, loss = 0.02164652
Iteration 66, loss = 0.02181658
Iteration 67, loss = 0.02171202
Iteration 68, loss = 0.02157212
Iteration 69, loss = 0.02141144
Iteration 70, loss = 0.02148440
Iteration 71, loss = 0.02116862
Iteration 72, loss = 0.02122366
Iteration 73, loss = 0.02103991
Iteration 74, loss = 0.02116808
Iteration 75, loss = 0.02086408
Iteration 76, loss = 0.02087295
Iteration 77, loss = 0.02097215
Iteration 78, loss = 0.02087028
Iteration 79, loss = 0.02102201
Iteration 80, loss = 0.02075657
Iteration 81, loss = 0.02079461
Iteration 82, loss = 0.02078261
Iteration 83, loss = 0.02078374
Iteration 84, loss = 0.02058773
Iteration 85, loss = 0.02056928
Iteration 86, loss = 0.02069761
Iteration 87, loss = 0.02045485
Iteration 88, loss = 0.02057414
Iteration 89, loss = 0.02092078
Iteration 90, loss = 0.02082973
Iteration 91, loss = 0.02057356
Iteration 92, loss = 0.02044836
Iteration 93, loss = 0.02026703
Iteration 94, loss = 0.02043217
Iteration 95, loss = 0.02051935
Iteration 96, loss = 0.02013519
Iteration 97, loss = 0.02053388
Iteration 98, loss = 0.02027787
Iteration 99, loss = 0.02041292
Iteration 100, loss = 0.02032126
Iteration 101, loss = 0.02014944
Iteration 102, loss = 0.02005561
Iteration 103, loss = 0.02006748
Iteration 104, loss = 0.01999585
Iteration 105, loss = 0.01989253
Iteration 106, loss = 0.02028484
Iteration 107, loss = 0.02002351
Iteration 108, loss = 0.02040188
Iteration 109, loss = 0.02013542
Iteration 110, loss = 0.02009975
Iteration 111, loss = 0.01985577
Iteration 112, loss = 0.02019291
Iteration 113, loss = 0.02015592
Iteration 114, loss = 0.02002014
Iteration 115, loss = 0.02006222
Iteration 116, loss = 0.02007288
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67904317
Iteration 2, loss = 0.62459605
Iteration 3, loss = 0.53640533
Iteration 4, loss = 0.43099670
Iteration 5, loss = 0.33626669
Iteration 6, loss = 0.25871947
Iteration 7, loss = 0.20036343
Iteration 8, loss = 0.15788385
Iteration 9, loss = 0.12705386
Iteration 10, loss = 0.10495956
Iteration 11, loss = 0.08858810
Iteration 12, loss = 0.07641041
Iteration 13, loss = 0.06693085
Iteration 14, loss = 0.05965812
Iteration 15, loss = 0.05377751
Iteration 16, loss = 0.04922651
Iteration 17, loss = 0.04550538
Iteration 18, loss = 0.04224933
Iteration 19, loss = 0.03955399
Iteration 20, loss = 0.03737518
Iteration 21, loss = 0.03526616
Iteration 22, loss = 0.03369350
Iteration 23, loss = 0.03246463
Iteration 24, loss = 0.03121878
Iteration 25, loss = 0.03030114
Iteration 26, loss = 0.02920330
Iteration 27, loss = 0.02827739
Iteration 28, loss = 0.02774509
Iteration 29, loss = 0.02714425
Iteration 30, loss = 0.02629094
Iteration 31, loss = 0.02582090
Iteration 32, loss = 0.02514023
Iteration 33, loss = 0.02474733
Iteration 34, loss = 0.02410292
Iteration 35, loss = 0.02379733
Iteration 36, loss = 0.02336298
Iteration 37, loss = 0.02297010
Iteration 38, loss = 0.02308321
Iteration 39, loss = 0.02322241
Iteration 40, loss = 0.02301290
Iteration 41, loss = 0.02267352
Iteration 42, loss = 0.02299528
Iteration 43, loss = 0.02270123
Iteration 44, loss = 0.02264045
Iteration 45, loss = 0.02211209
Iteration 46, loss = 0.02175950
Iteration 47, loss = 0.02133718
Iteration 48, loss = 0.02100945
Iteration 49, loss = 0.02131771
Iteration 50, loss = 0.02111536
Iteration 51, loss = 0.02081054
Iteration 52, loss = 0.02043142
Iteration 53, loss = 0.02043899
Iteration 54, loss = 0.02026390
Iteration 55, loss = 0.01992578
Iteration 56, loss = 0.02015174
Iteration 57, loss = 0.02003721
Iteration 58, loss = 0.02001183
Iteration 59, loss = 0.02032121
Iteration 60, loss = 0.02063378
Iteration 61, loss = 0.01999267
Iteration 62, loss = 0.01988479
Iteration 63, loss = 0.01990834
Iteration 64, loss = 0.01977654
Iteration 65, loss = 0.01942455
Iteration 66, loss = 0.01946926
Iteration 67, loss = 0.01952225
Iteration 68, loss = 0.01937215
Iteration 69, loss = 0.01910519
Iteration 70, loss = 0.01909334
Iteration 71, loss = 0.01903722
Iteration 72, loss = 0.01907295
Iteration 73, loss = 0.01880749
Iteration 74, loss = 0.01878486
Iteration 75, loss = 0.01920138
Iteration 76, loss = 0.01924662
Iteration 77, loss = 0.01899309
Iteration 78, loss = 0.01917202
Iteration 79, loss = 0.01876150
Iteration 80, loss = 0.01867701
Iteration 81, loss = 0.01874685
Iteration 82, loss = 0.01886667
Iteration 83, loss = 0.01884701
Iteration 84, loss = 0.01875678
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68431789
Iteration 2, loss = 0.67027320
Iteration 3, loss = 0.65803011
Iteration 4, loss = 0.64561646
Iteration 5, loss = 0.63206970
Iteration 6, loss = 0.61720663
Iteration 7, loss = 0.60063587
Iteration 8, loss = 0.58240248
Iteration 9, loss = 0.56253944
Iteration 10, loss = 0.54093071
Iteration 11, loss = 0.51807536
Iteration 12, loss = 0.49427475
Iteration 13, loss = 0.46962642
Iteration 14, loss = 0.44435304
Iteration 15, loss = 0.41919462
Iteration 16, loss = 0.39439829
Iteration 17, loss = 0.36992138
Iteration 18, loss = 0.34649809
Iteration 19, loss = 0.32385561
Iteration 20, loss = 0.30266694
Iteration 21, loss = 0.28228659
Iteration 22, loss = 0.26329873
Iteration 23, loss = 0.24549601
Iteration 24, loss = 0.22895965
Iteration 25, loss = 0.21348058
Iteration 26, loss = 0.19913109
Iteration 27, loss = 0.18588199
Iteration 28, loss = 0.17353704
Iteration 29, loss = 0.16229557
Iteration 30, loss = 0.15183475
Iteration 31, loss = 0.14225051
Iteration 32, loss = 0.13329964
Iteration 33, loss = 0.12523745
Iteration 34, loss = 0.11770935
Iteration 35, loss = 0.11080873
Iteration 36, loss = 0.10455063
Iteration 37, loss = 0.09870029
Iteration 38, loss = 0.09326952
Iteration 39, loss = 0.08832958
Iteration 40, loss = 0.08373432
Iteration 41, loss = 0.07943685
Iteration 42, loss = 0.07552148
Iteration 43, loss = 0.07185227
Iteration 44, loss = 0.06846348
Iteration 45, loss = 0.06532806
Iteration 46, loss = 0.06239382
Iteration 47, loss = 0.05967697
Iteration 48, loss = 0.05712599
Iteration 49, loss = 0.05475853
Iteration 50, loss = 0.05253383
Iteration 51, loss = 0.05043705
Iteration 52, loss = 0.04850585
Iteration 53, loss = 0.04666692
Iteration 54, loss = 0.04494935
Iteration 55, loss = 0.04334774
Iteration 56, loss = 0.04184370
Iteration 57, loss = 0.04038731
Iteration 58, loss = 0.03900959
Iteration 59, loss = 0.03772492
Iteration 60, loss = 0.03653224
Iteration 61, loss = 0.03535740
Iteration 62, loss = 0.03424302
Iteration 63, loss = 0.03319003
Iteration 64, loss = 0.03221837
Iteration 65, loss = 0.03131297
Iteration 66, loss = 0.03042035
Iteration 67, loss = 0.02961211
Iteration 68, loss = 0.02873817
Iteration 69, loss = 0.02795912
Iteration 70, loss = 0.02720894
Iteration 71, loss = 0.02650155
Iteration 72, loss = 0.02584875
Iteration 73, loss = 0.02517680
Iteration 74, loss = 0.02455820
Iteration 75, loss = 0.02397510
Iteration 76, loss = 0.02340760
Iteration 77, loss = 0.02286691
Iteration 78, loss = 0.02237541
Iteration 79, loss = 0.02185001
Iteration 80, loss = 0.02139216
Iteration 81, loss = 0.02095267
Iteration 82, loss = 0.02051442
Iteration 83, loss = 0.02010326
Iteration 84, loss = 0.01971743
Iteration 85, loss = 0.01932270
Iteration 86, loss = 0.01893942
Iteration 87, loss = 0.01857393
Iteration 88, loss = 0.01818942
Iteration 89, loss = 0.01785293
Iteration 90, loss = 0.01751834
Iteration 91, loss = 0.01721679
Iteration 92, loss = 0.01689312
Iteration 93, loss = 0.01662121
Iteration 94, loss = 0.01632451
Iteration 95, loss = 0.01606341
Iteration 96, loss = 0.01580249
Iteration 97, loss = 0.01555162
Iteration 98, loss = 0.01527946
Iteration 99, loss = 0.01504736
Iteration 100, loss = 0.01481547
Iteration 101, loss = 0.01457492
Iteration 102, loss = 0.01438235
Iteration 103, loss = 0.01418731
Iteration 104, loss = 0.01400708
Iteration 105, loss = 0.01383478
Iteration 106, loss = 0.01365267
Iteration 107, loss = 0.01344551
Iteration 108, loss = 0.01325816
Iteration 109, loss = 0.01306986
Iteration 110, loss = 0.01288778
Iteration 111, loss = 0.01270644
Iteration 112, loss = 0.01252362
Iteration 113, loss = 0.01243858
Iteration 114, loss = 0.01216032
Iteration 115, loss = 0.01201604
Iteration 116, loss = 0.01186049
Iteration 117, loss = 0.01173642
Iteration 118, loss = 0.01156733
Iteration 119, loss = 0.01145561
Iteration 120, loss = 0.01129460
Iteration 121, loss = 0.01116898
Iteration 122, loss = 0.01106062
Iteration 123, loss = 0.01095511
Iteration 124, loss = 0.01080477
Iteration 125, loss = 0.01068962
Iteration 126, loss = 0.01058724
Iteration 127, loss = 0.01048081
Iteration 128, loss = 0.01036171
Iteration 129, loss = 0.01027167
Iteration 130, loss = 0.01015720
Iteration 131, loss = 0.01006495
Iteration 132, loss = 0.00999939
Iteration 133, loss = 0.00988489
Iteration 134, loss = 0.00980771
Iteration 135, loss = 0.00973889
Iteration 136, loss = 0.00958385
Iteration 137, loss = 0.00949719
Iteration 138, loss = 0.00944402
Iteration 139, loss = 0.00933053
Iteration 140, loss = 0.00925879
Iteration 141, loss = 0.00919406
Iteration 142, loss = 0.00911513
Iteration 143, loss = 0.00903802
Iteration 144, loss = 0.00896766
Iteration 145, loss = 0.00887457
Iteration 146, loss = 0.00880372
Iteration 147, loss = 0.00875399
Iteration 148, loss = 0.00866474
Iteration 149, loss = 0.00861183
Iteration 150, loss = 0.00855386
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68453050
Iteration 2, loss = 0.65836884
Iteration 3, loss = 0.63290135
Iteration 4, loss = 0.60476444
Iteration 5, loss = 0.57306451
Iteration 6, loss = 0.53836553
Iteration 7, loss = 0.50112993
Iteration 8, loss = 0.46288979
Iteration 9, loss = 0.42504308
Iteration 10, loss = 0.38825892
Iteration 11, loss = 0.35366951
Iteration 12, loss = 0.32099701
Iteration 13, loss = 0.29080765
Iteration 14, loss = 0.26328733
Iteration 15, loss = 0.23821774
Iteration 16, loss = 0.21551922
Iteration 17, loss = 0.19528150
Iteration 18, loss = 0.17718289
Iteration 19, loss = 0.16107196
Iteration 20, loss = 0.14680207
Iteration 21, loss = 0.13413668
Iteration 22, loss = 0.12287965
Iteration 23, loss = 0.11296391
Iteration 24, loss = 0.10417472
Iteration 25, loss = 0.09626209
Iteration 26, loss = 0.08932591
Iteration 27, loss = 0.08309658
Iteration 28, loss = 0.07746643
Iteration 29, loss = 0.07243923
Iteration 30, loss = 0.06791257
Iteration 31, loss = 0.06387868
Iteration 32, loss = 0.06018471
Iteration 33, loss = 0.05684118
Iteration 34, loss = 0.05379494
Iteration 35, loss = 0.05104254
Iteration 36, loss = 0.04847635
Iteration 37, loss = 0.04615602
Iteration 38, loss = 0.04407193
Iteration 39, loss = 0.04208526
Iteration 40, loss = 0.04026240
Iteration 41, loss = 0.03858400
Iteration 42, loss = 0.03705255
Iteration 43, loss = 0.03563039
Iteration 44, loss = 0.03430319
Iteration 45, loss = 0.03307510
Iteration 46, loss = 0.03193800
Iteration 47, loss = 0.03081649
Iteration 48, loss = 0.02984321
Iteration 49, loss = 0.02889246
Iteration 50, loss = 0.02796260
Iteration 51, loss = 0.02718005
Iteration 52, loss = 0.02635785
Iteration 53, loss = 0.02561595
Iteration 54, loss = 0.02498495
Iteration 55, loss = 0.02425627
Iteration 56, loss = 0.02365266
Iteration 57, loss = 0.02308349
Iteration 58, loss = 0.02247955
Iteration 59, loss = 0.02198806
Iteration 60, loss = 0.02148685
Iteration 61, loss = 0.02102342
Iteration 62, loss = 0.02057344
Iteration 63, loss = 0.02013432
Iteration 64, loss = 0.01974731
Iteration 65, loss = 0.01933921
Iteration 66, loss = 0.01899552
Iteration 67, loss = 0.01861303
Iteration 68, loss = 0.01830135
Iteration 69, loss = 0.01800459
Iteration 70, loss = 0.01769871
Iteration 71, loss = 0.01738709
Iteration 72, loss = 0.01710908
Iteration 73, loss = 0.01680985
Iteration 74, loss = 0.01655697
Iteration 75, loss = 0.01631860
Iteration 76, loss = 0.01610754
Iteration 77, loss = 0.01585426
Iteration 78, loss = 0.01559106
Iteration 79, loss = 0.01541925
Iteration 80, loss = 0.01520357
Iteration 81, loss = 0.01503124
Iteration 82, loss = 0.01484057
Iteration 83, loss = 0.01466340
Iteration 84, loss = 0.01445840
Iteration 85, loss = 0.01436481
Iteration 86, loss = 0.01415189
Iteration 87, loss = 0.01399303
Iteration 88, loss = 0.01385964
Iteration 89, loss = 0.01369573
Iteration 90, loss = 0.01351739
Iteration 91, loss = 0.01339906
Iteration 92, loss = 0.01327187
Iteration 93, loss = 0.01314001
Iteration 94, loss = 0.01305732
Iteration 95, loss = 0.01290805
Iteration 96, loss = 0.01284197
Iteration 97, loss = 0.01270452
Iteration 98, loss = 0.01255397
Iteration 99, loss = 0.01249539
Iteration 100, loss = 0.01240634
Iteration 101, loss = 0.01228416
Iteration 102, loss = 0.01222046
Iteration 103, loss = 0.01210275
Iteration 104, loss = 0.01198546
Iteration 105, loss = 0.01193658
Iteration 106, loss = 0.01182531
Iteration 107, loss = 0.01174757
Iteration 108, loss = 0.01166684
Iteration 109, loss = 0.01161004
Iteration 110, loss = 0.01153507
Iteration 111, loss = 0.01145623
Iteration 112, loss = 0.01139495
Iteration 113, loss = 0.01128576
Iteration 114, loss = 0.01122697
Iteration 115, loss = 0.01116755
Iteration 116, loss = 0.01113239
Iteration 117, loss = 0.01102499
Iteration 118, loss = 0.01100290
Iteration 119, loss = 0.01092286
Iteration 120, loss = 0.01087698
Iteration 121, loss = 0.01079271
Iteration 122, loss = 0.01078647
Iteration 123, loss = 0.01074830
Iteration 124, loss = 0.01066318
Iteration 125, loss = 0.01059606
Iteration 126, loss = 0.01054405
Iteration 127, loss = 0.01052039
Iteration 128, loss = 0.01049616
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68348195
Iteration 2, loss = 0.63930226
Iteration 3, loss = 0.57027926
Iteration 4, loss = 0.48592845
Iteration 5, loss = 0.39882259
Iteration 6, loss = 0.31947814
Iteration 7, loss = 0.25264045
Iteration 8, loss = 0.20057878
Iteration 9, loss = 0.16124867
Iteration 10, loss = 0.13216250
Iteration 11, loss = 0.11043936
Iteration 12, loss = 0.09412203
Iteration 13, loss = 0.08175459
Iteration 14, loss = 0.07200111
Iteration 15, loss = 0.06459137
Iteration 16, loss = 0.05833970
Iteration 17, loss = 0.05342533
Iteration 18, loss = 0.04946870
Iteration 19, loss = 0.04609625
Iteration 20, loss = 0.04319463
Iteration 21, loss = 0.04070187
Iteration 22, loss = 0.03863582
Iteration 23, loss = 0.03679436
Iteration 24, loss = 0.03531272
Iteration 25, loss = 0.03393245
Iteration 26, loss = 0.03277817
Iteration 27, loss = 0.03171441
Iteration 28, loss = 0.03078054
Iteration 29, loss = 0.02992572
Iteration 30, loss = 0.02938200
Iteration 31, loss = 0.02852587
Iteration 32, loss = 0.02791561
Iteration 33, loss = 0.02745313
Iteration 34, loss = 0.02678834
Iteration 35, loss = 0.02639733
Iteration 36, loss = 0.02604164
Iteration 37, loss = 0.02574392
Iteration 38, loss = 0.02525766
Iteration 39, loss = 0.02470398
Iteration 40, loss = 0.02471583
Iteration 41, loss = 0.02414120
Iteration 42, loss = 0.02399677
Iteration 43, loss = 0.02378364
Iteration 44, loss = 0.02358107
Iteration 45, loss = 0.02353705
Iteration 46, loss = 0.02305924
Iteration 47, loss = 0.02314280
Iteration 48, loss = 0.02293448
Iteration 49, loss = 0.02275171
Iteration 50, loss = 0.02251876
Iteration 51, loss = 0.02247270
Iteration 52, loss = 0.02251940
Iteration 53, loss = 0.02219804
Iteration 54, loss = 0.02196496
Iteration 55, loss = 0.02207792
Iteration 56, loss = 0.02179521
Iteration 57, loss = 0.02179653
Iteration 58, loss = 0.02163319
Iteration 59, loss = 0.02169441
Iteration 60, loss = 0.02156018
Iteration 61, loss = 0.02147080
Iteration 62, loss = 0.02172985
Iteration 63, loss = 0.02141719
Iteration 64, loss = 0.02140517
Iteration 65, loss = 0.02130931
Iteration 66, loss = 0.02110847
Iteration 67, loss = 0.02114854
Iteration 68, loss = 0.02114132
Iteration 69, loss = 0.02105447
Iteration 70, loss = 0.02104499
Iteration 71, loss = 0.02099945
Iteration 72, loss = 0.02072945
Iteration 73, loss = 0.02074461
Iteration 74, loss = 0.02073829
Iteration 75, loss = 0.02057588
Iteration 76, loss = 0.02071877
Iteration 77, loss = 0.02053077
Iteration 78, loss = 0.02056798
Iteration 79, loss = 0.02079831
Iteration 80, loss = 0.02058588
Iteration 81, loss = 0.02036797
Iteration 82, loss = 0.02034699
Iteration 83, loss = 0.02025322
Iteration 84, loss = 0.02031911
Iteration 85, loss = 0.02038535
Iteration 86, loss = 0.02050434
Iteration 87, loss = 0.02013429
Iteration 88, loss = 0.02035630
Iteration 89, loss = 0.02016507
Iteration 90, loss = 0.02021673
Iteration 91, loss = 0.02034750
Iteration 92, loss = 0.02019494
Iteration 93, loss = 0.02006829
Iteration 94, loss = 0.02021616
Iteration 95, loss = 0.02044729
Iteration 96, loss = 0.02036864
Iteration 97, loss = 0.02035035
Iteration 98, loss = 0.02008917
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67764651
Iteration 2, loss = 0.61604905
Iteration 3, loss = 0.50954124
Iteration 4, loss = 0.39469629
Iteration 5, loss = 0.29708566
Iteration 6, loss = 0.22243859
Iteration 7, loss = 0.16969810
Iteration 8, loss = 0.13290134
Iteration 9, loss = 0.10733983
Iteration 10, loss = 0.08948698
Iteration 11, loss = 0.07611750
Iteration 12, loss = 0.06642273
Iteration 13, loss = 0.05893061
Iteration 14, loss = 0.05298166
Iteration 15, loss = 0.04811385
Iteration 16, loss = 0.04464353
Iteration 17, loss = 0.04146139
Iteration 18, loss = 0.03909316
Iteration 19, loss = 0.03670137
Iteration 20, loss = 0.03487097
Iteration 21, loss = 0.03338365
Iteration 22, loss = 0.03186011
Iteration 23, loss = 0.03077272
Iteration 24, loss = 0.02980636
Iteration 25, loss = 0.02896019
Iteration 26, loss = 0.02829568
Iteration 27, loss = 0.02758157
Iteration 28, loss = 0.02689837
Iteration 29, loss = 0.02631274
Iteration 30, loss = 0.02571767
Iteration 31, loss = 0.02554820
Iteration 32, loss = 0.02510560
Iteration 33, loss = 0.02460412
Iteration 34, loss = 0.02420201
Iteration 35, loss = 0.02413746
Iteration 36, loss = 0.02376077
Iteration 37, loss = 0.02354146
Iteration 38, loss = 0.02331719
Iteration 39, loss = 0.02311363
Iteration 40, loss = 0.02293493
Iteration 41, loss = 0.02253507
Iteration 42, loss = 0.02292450
Iteration 43, loss = 0.02258145
Iteration 44, loss = 0.02231372
Iteration 45, loss = 0.02197688
Iteration 46, loss = 0.02185571
Iteration 47, loss = 0.02209488
Iteration 48, loss = 0.02165826
Iteration 49, loss = 0.02158875
Iteration 50, loss = 0.02149655
Iteration 51, loss = 0.02151227
Iteration 52, loss = 0.02140637
Iteration 53, loss = 0.02121200
Iteration 54, loss = 0.02133299
Iteration 55, loss = 0.02116088
Iteration 56, loss = 0.02131798
Iteration 57, loss = 0.02106583
Iteration 58, loss = 0.02087241
Iteration 59, loss = 0.02093040
Iteration 60, loss = 0.02071009
Iteration 61, loss = 0.02065439
Iteration 62, loss = 0.02075445
Iteration 63, loss = 0.02080523
Iteration 64, loss = 0.02056207
Iteration 65, loss = 0.02069112
Iteration 66, loss = 0.02069131
Iteration 67, loss = 0.02047775
Iteration 68, loss = 0.02044365
Iteration 69, loss = 0.02035509
Iteration 70, loss = 0.02043192
Iteration 71, loss = 0.02053160
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67922570
Iteration 2, loss = 0.61512147
Iteration 3, loss = 0.52508238
Iteration 4, loss = 0.42842958
Iteration 5, loss = 0.33994114
Iteration 6, loss = 0.26654040
Iteration 7, loss = 0.20919688
Iteration 8, loss = 0.16617738
Iteration 9, loss = 0.13473784
Iteration 10, loss = 0.11178362
Iteration 11, loss = 0.09441487
Iteration 12, loss = 0.08148573
Iteration 13, loss = 0.07132694
Iteration 14, loss = 0.06369339
Iteration 15, loss = 0.05755870
Iteration 16, loss = 0.05254088
Iteration 17, loss = 0.04861172
Iteration 18, loss = 0.04527501
Iteration 19, loss = 0.04245711
Iteration 20, loss = 0.04003193
Iteration 21, loss = 0.03813232
Iteration 22, loss = 0.03622103
Iteration 23, loss = 0.03479457
Iteration 24, loss = 0.03366295
Iteration 25, loss = 0.03234238
Iteration 26, loss = 0.03125145
Iteration 27, loss = 0.03049426
Iteration 28, loss = 0.02942896
Iteration 29, loss = 0.02924950
Iteration 30, loss = 0.02832369
Iteration 31, loss = 0.02808904
Iteration 32, loss = 0.02730040
Iteration 33, loss = 0.02722814
Iteration 34, loss = 0.02680139
Iteration 35, loss = 0.02643423
Iteration 36, loss = 0.02595521
Iteration 37, loss = 0.02590418
Iteration 38, loss = 0.02535990
Iteration 39, loss = 0.02495703
Iteration 40, loss = 0.02442070
Iteration 41, loss = 0.02421942
Iteration 42, loss = 0.02392333
Iteration 43, loss = 0.02381679
Iteration 44, loss = 0.02341724
Iteration 45, loss = 0.02351260
Iteration 46, loss = 0.02337344
Iteration 47, loss = 0.02332443
Iteration 48, loss = 0.02299146
Iteration 49, loss = 0.02257017
Iteration 50, loss = 0.02239556
Iteration 51, loss = 0.02303144
Iteration 52, loss = 0.02287519
Iteration 53, loss = 0.02276443
Iteration 54, loss = 0.02271735
Iteration 55, loss = 0.02227440
Iteration 56, loss = 0.02210110
Iteration 57, loss = 0.02192625
Iteration 58, loss = 0.02186930
Iteration 59, loss = 0.02230789
Iteration 60, loss = 0.02239241
Iteration 61, loss = 0.02230638
Iteration 62, loss = 0.02175179
Iteration 63, loss = 0.02152989
Iteration 64, loss = 0.02186337
Iteration 65, loss = 0.02171205
Iteration 66, loss = 0.02151846
Iteration 67, loss = 0.02140410
Iteration 68, loss = 0.02111138
Iteration 69, loss = 0.02084093
Iteration 70, loss = 0.02075112
Iteration 71, loss = 0.02093504
Iteration 72, loss = 0.02090562
Iteration 73, loss = 0.02156680
Iteration 74, loss = 0.02224242
Iteration 75, loss = 0.02193880
Iteration 76, loss = 0.02134468
Iteration 77, loss = 0.02098709
Iteration 78, loss = 0.02048678
Iteration 79, loss = 0.02086928
Iteration 80, loss = 0.02067108
Iteration 81, loss = 0.02063311
Iteration 82, loss = 0.02030889
Iteration 83, loss = 0.02053692
Iteration 84, loss = 0.02019700
Iteration 85, loss = 0.02076155
Iteration 86, loss = 0.02075496
Iteration 87, loss = 0.02078875
Iteration 88, loss = 0.02104577
Iteration 89, loss = 0.02121148
Iteration 90, loss = 0.02096883
Iteration 91, loss = 0.02080229
Iteration 92, loss = 0.02048276
Iteration 93, loss = 0.02033940
Iteration 94, loss = 0.02018040
Iteration 95, loss = 0.02010706
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72694965
Iteration 2, loss = 0.70626774
Iteration 3, loss = 0.68590102
Iteration 4, loss = 0.66217439
Iteration 5, loss = 0.63532521
Iteration 6, loss = 0.60525201
Iteration 7, loss = 0.57199707
Iteration 8, loss = 0.53680052
Iteration 9, loss = 0.50021262
Iteration 10, loss = 0.46268592
Iteration 11, loss = 0.42549835
Iteration 12, loss = 0.38950165
Iteration 13, loss = 0.35504981
Iteration 14, loss = 0.32295709
Iteration 15, loss = 0.29317160
Iteration 16, loss = 0.26555365
Iteration 17, loss = 0.24067472
Iteration 18, loss = 0.21824403
Iteration 19, loss = 0.19790472
Iteration 20, loss = 0.17996265
Iteration 21, loss = 0.16388988
Iteration 22, loss = 0.14957733
Iteration 23, loss = 0.13698854
Iteration 24, loss = 0.12583120
Iteration 25, loss = 0.11590173
Iteration 26, loss = 0.10705748
Iteration 27, loss = 0.09922061
Iteration 28, loss = 0.09224614
Iteration 29, loss = 0.08601013
Iteration 30, loss = 0.08048387
Iteration 31, loss = 0.07545576
Iteration 32, loss = 0.07098617
Iteration 33, loss = 0.06690457
Iteration 34, loss = 0.06326954
Iteration 35, loss = 0.05990157
Iteration 36, loss = 0.05685087
Iteration 37, loss = 0.05412782
Iteration 38, loss = 0.05156840
Iteration 39, loss = 0.04928614
Iteration 40, loss = 0.04712440
Iteration 41, loss = 0.04517527
Iteration 42, loss = 0.04335510
Iteration 43, loss = 0.04166417
Iteration 44, loss = 0.04009643
Iteration 45, loss = 0.03867508
Iteration 46, loss = 0.03736008
Iteration 47, loss = 0.03609117
Iteration 48, loss = 0.03491539
Iteration 49, loss = 0.03392150
Iteration 50, loss = 0.03282754
Iteration 51, loss = 0.03188638
Iteration 52, loss = 0.03099408
Iteration 53, loss = 0.03014650
Iteration 54, loss = 0.02940993
Iteration 55, loss = 0.02859458
Iteration 56, loss = 0.02794153
Iteration 57, loss = 0.02729149
Iteration 58, loss = 0.02663264
Iteration 59, loss = 0.02608052
Iteration 60, loss = 0.02547801
Iteration 61, loss = 0.02489939
Iteration 62, loss = 0.02442261
Iteration 63, loss = 0.02397652
Iteration 64, loss = 0.02351960
Iteration 65, loss = 0.02303673
Iteration 66, loss = 0.02264707
Iteration 67, loss = 0.02220809
Iteration 68, loss = 0.02181899
Iteration 69, loss = 0.02150737
Iteration 70, loss = 0.02115787
Iteration 71, loss = 0.02084734
Iteration 72, loss = 0.02053959
Iteration 73, loss = 0.02022106
Iteration 74, loss = 0.01999472
Iteration 75, loss = 0.01966758
Iteration 76, loss = 0.01940367
Iteration 77, loss = 0.01915144
Iteration 78, loss = 0.01892034
Iteration 79, loss = 0.01863924
Iteration 80, loss = 0.01844898
Iteration 81, loss = 0.01822332
Iteration 82, loss = 0.01796907
Iteration 83, loss = 0.01785086
Iteration 84, loss = 0.01760899
Iteration 85, loss = 0.01739754
Iteration 86, loss = 0.01716783
Iteration 87, loss = 0.01706574
Iteration 88, loss = 0.01694928
Iteration 89, loss = 0.01668764
Iteration 90, loss = 0.01657587
Iteration 91, loss = 0.01638075
Iteration 92, loss = 0.01627164
Iteration 93, loss = 0.01615576
Iteration 94, loss = 0.01598130
Iteration 95, loss = 0.01585711
Iteration 96, loss = 0.01573042
Iteration 97, loss = 0.01556452
Iteration 98, loss = 0.01548292
Iteration 99, loss = 0.01530325
Iteration 100, loss = 0.01522833
Iteration 101, loss = 0.01507473
Iteration 102, loss = 0.01502618
Iteration 103, loss = 0.01494923
Iteration 104, loss = 0.01483265
Iteration 105, loss = 0.01469488
Iteration 106, loss = 0.01473004
Iteration 107, loss = 0.01453914
Iteration 108, loss = 0.01446145
Iteration 109, loss = 0.01430765
Iteration 110, loss = 0.01423330
Iteration 111, loss = 0.01416765
Iteration 112, loss = 0.01408574
Iteration 113, loss = 0.01402428
Iteration 114, loss = 0.01397513
Iteration 115, loss = 0.01385648
Iteration 116, loss = 0.01381834
Iteration 117, loss = 0.01375845
Iteration 118, loss = 0.01367965
Iteration 119, loss = 0.01356377
Iteration 120, loss = 0.01354113
Iteration 121, loss = 0.01352776
Iteration 122, loss = 0.01346377
Iteration 123, loss = 0.01335302
Iteration 124, loss = 0.01330744
Iteration 125, loss = 0.01320519
Iteration 126, loss = 0.01316204
Iteration 127, loss = 0.01315003
Iteration 128, loss = 0.01304299
Iteration 129, loss = 0.01298191
Iteration 130, loss = 0.01300827
Iteration 131, loss = 0.01291125
Iteration 132, loss = 0.01284061
Iteration 133, loss = 0.01282519
Iteration 134, loss = 0.01274302
Iteration 135, loss = 0.01272599
Iteration 136, loss = 0.01264464
Iteration 137, loss = 0.01264117
Iteration 138, loss = 0.01260621
Iteration 139, loss = 0.01252923
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72691323
Iteration 2, loss = 0.69099762
Iteration 3, loss = 0.64463793
Iteration 4, loss = 0.58887671
Iteration 5, loss = 0.52666813
Iteration 6, loss = 0.46222641
Iteration 7, loss = 0.39918677
Iteration 8, loss = 0.34076185
Iteration 9, loss = 0.28931403
Iteration 10, loss = 0.24558508
Iteration 11, loss = 0.20872906
Iteration 12, loss = 0.17825362
Iteration 13, loss = 0.15328391
Iteration 14, loss = 0.13309746
Iteration 15, loss = 0.11642019
Iteration 16, loss = 0.10275217
Iteration 17, loss = 0.09152356
Iteration 18, loss = 0.08213007
Iteration 19, loss = 0.07427617
Iteration 20, loss = 0.06761282
Iteration 21, loss = 0.06205997
Iteration 22, loss = 0.05718971
Iteration 23, loss = 0.05294383
Iteration 24, loss = 0.04936226
Iteration 25, loss = 0.04608142
Iteration 26, loss = 0.04333972
Iteration 27, loss = 0.04086811
Iteration 28, loss = 0.03865532
Iteration 29, loss = 0.03664615
Iteration 30, loss = 0.03498239
Iteration 31, loss = 0.03344306
Iteration 32, loss = 0.03194715
Iteration 33, loss = 0.03073200
Iteration 34, loss = 0.02957505
Iteration 35, loss = 0.02847582
Iteration 36, loss = 0.02739584
Iteration 37, loss = 0.02654859
Iteration 38, loss = 0.02574993
Iteration 39, loss = 0.02495447
Iteration 40, loss = 0.02430314
Iteration 41, loss = 0.02368400
Iteration 42, loss = 0.02316157
Iteration 43, loss = 0.02258507
Iteration 44, loss = 0.02203210
Iteration 45, loss = 0.02155053
Iteration 46, loss = 0.02114603
Iteration 47, loss = 0.02068071
Iteration 48, loss = 0.02032810
Iteration 49, loss = 0.01994968
Iteration 50, loss = 0.01969256
Iteration 51, loss = 0.01934516
Iteration 52, loss = 0.01910879
Iteration 53, loss = 0.01880361
Iteration 54, loss = 0.01853586
Iteration 55, loss = 0.01827110
Iteration 56, loss = 0.01798879
Iteration 57, loss = 0.01764354
Iteration 58, loss = 0.01761177
Iteration 59, loss = 0.01734083
Iteration 60, loss = 0.01720761
Iteration 61, loss = 0.01692186
Iteration 62, loss = 0.01674044
Iteration 63, loss = 0.01666722
Iteration 64, loss = 0.01638037
Iteration 65, loss = 0.01636792
Iteration 66, loss = 0.01611637
Iteration 67, loss = 0.01597932
Iteration 68, loss = 0.01582755
Iteration 69, loss = 0.01575585
Iteration 70, loss = 0.01566976
Iteration 71, loss = 0.01545281
Iteration 72, loss = 0.01543249
Iteration 73, loss = 0.01522673
Iteration 74, loss = 0.01526928
Iteration 75, loss = 0.01506881
Iteration 76, loss = 0.01490812
Iteration 77, loss = 0.01491502
Iteration 78, loss = 0.01502979
Iteration 79, loss = 0.01475271
Iteration 80, loss = 0.01467653
Iteration 81, loss = 0.01457004
Iteration 82, loss = 0.01458578
Iteration 83, loss = 0.01452890
Iteration 84, loss = 0.01441813
Iteration 85, loss = 0.01437079
Iteration 86, loss = 0.01436016
Iteration 87, loss = 0.01425737
Iteration 88, loss = 0.01431464
Iteration 89, loss = 0.01415684
Iteration 90, loss = 0.01401757
Iteration 91, loss = 0.01409170
Iteration 92, loss = 0.01399535
Iteration 93, loss = 0.01389523
Iteration 94, loss = 0.01383514
Iteration 95, loss = 0.01383637
Iteration 96, loss = 0.01376081
Iteration 97, loss = 0.01372631
Iteration 98, loss = 0.01370980
Iteration 99, loss = 0.01369245
Iteration 100, loss = 0.01356397
Iteration 101, loss = 0.01351901
Iteration 102, loss = 0.01349933
Iteration 103, loss = 0.01364720
Iteration 104, loss = 0.01351052
Iteration 105, loss = 0.01345923
Iteration 106, loss = 0.01343406
Iteration 107, loss = 0.01336155
Iteration 108, loss = 0.01330918
Iteration 109, loss = 0.01341940
Iteration 110, loss = 0.01342572
Iteration 111, loss = 0.01325335
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71825437
Iteration 2, loss = 0.66396560
Iteration 3, loss = 0.59484026
Iteration 4, loss = 0.51289058
Iteration 5, loss = 0.42734391
Iteration 6, loss = 0.34717765
Iteration 7, loss = 0.27928603
Iteration 8, loss = 0.22464546
Iteration 9, loss = 0.18207118
Iteration 10, loss = 0.14975673
Iteration 11, loss = 0.12521414
Iteration 12, loss = 0.10640807
Iteration 13, loss = 0.09205869
Iteration 14, loss = 0.08091946
Iteration 15, loss = 0.07195290
Iteration 16, loss = 0.06499155
Iteration 17, loss = 0.05920461
Iteration 18, loss = 0.05451903
Iteration 19, loss = 0.05047651
Iteration 20, loss = 0.04705909
Iteration 21, loss = 0.04425299
Iteration 22, loss = 0.04178883
Iteration 23, loss = 0.03990535
Iteration 24, loss = 0.03803946
Iteration 25, loss = 0.03622156
Iteration 26, loss = 0.03472430
Iteration 27, loss = 0.03349582
Iteration 28, loss = 0.03234347
Iteration 29, loss = 0.03140733
Iteration 30, loss = 0.03053260
Iteration 31, loss = 0.02970463
Iteration 32, loss = 0.02920417
Iteration 33, loss = 0.02834724
Iteration 34, loss = 0.02788708
Iteration 35, loss = 0.02730831
Iteration 36, loss = 0.02696183
Iteration 37, loss = 0.02629942
Iteration 38, loss = 0.02605250
Iteration 39, loss = 0.02547421
Iteration 40, loss = 0.02522558
Iteration 41, loss = 0.02535019
Iteration 42, loss = 0.02475193
Iteration 43, loss = 0.02441303
Iteration 44, loss = 0.02405488
Iteration 45, loss = 0.02376837
Iteration 46, loss = 0.02383514
Iteration 47, loss = 0.02347873
Iteration 48, loss = 0.02322090
Iteration 49, loss = 0.02310587
Iteration 50, loss = 0.02316620
Iteration 51, loss = 0.02297957
Iteration 52, loss = 0.02261545
Iteration 53, loss = 0.02256081
Iteration 54, loss = 0.02227247
Iteration 55, loss = 0.02205055
Iteration 56, loss = 0.02184744
Iteration 57, loss = 0.02195277
Iteration 58, loss = 0.02193359
Iteration 59, loss = 0.02182587
Iteration 60, loss = 0.02167548
Iteration 61, loss = 0.02163945
Iteration 62, loss = 0.02170098
Iteration 63, loss = 0.02146742
Iteration 64, loss = 0.02121059
Iteration 65, loss = 0.02107264
Iteration 66, loss = 0.02104396
Iteration 67, loss = 0.02143974
Iteration 68, loss = 0.02110652
Iteration 69, loss = 0.02070849
Iteration 70, loss = 0.02072471
Iteration 71, loss = 0.02086046
Iteration 72, loss = 0.02102756
Iteration 73, loss = 0.02109198
Iteration 74, loss = 0.02060836
Iteration 75, loss = 0.02089665
Iteration 76, loss = 0.02074056
Iteration 77, loss = 0.02055666
Iteration 78, loss = 0.02090576
Iteration 79, loss = 0.02076045
Iteration 80, loss = 0.02048301
Iteration 81, loss = 0.02028170
Iteration 82, loss = 0.02013431
Iteration 83, loss = 0.02040232
Iteration 84, loss = 0.02050969
Iteration 85, loss = 0.02027247
Iteration 86, loss = 0.02012565
Iteration 87, loss = 0.02025326
Iteration 88, loss = 0.02006578
Iteration 89, loss = 0.02022542
Iteration 90, loss = 0.02008857
Iteration 91, loss = 0.02024019
Iteration 92, loss = 0.02004233
Iteration 93, loss = 0.01999801
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67788111
Iteration 2, loss = 0.61102412
Iteration 3, loss = 0.51316221
Iteration 4, loss = 0.40682867
Iteration 5, loss = 0.31087737
Iteration 6, loss = 0.23504309
Iteration 7, loss = 0.17941869
Iteration 8, loss = 0.14048333
Iteration 9, loss = 0.11352120
Iteration 10, loss = 0.09407258
Iteration 11, loss = 0.08003272
Iteration 12, loss = 0.06985960
Iteration 13, loss = 0.06189992
Iteration 14, loss = 0.05577228
Iteration 15, loss = 0.05090860
Iteration 16, loss = 0.04683128
Iteration 17, loss = 0.04367609
Iteration 18, loss = 0.04092126
Iteration 19, loss = 0.03872897
Iteration 20, loss = 0.03693400
Iteration 21, loss = 0.03521000
Iteration 22, loss = 0.03400112
Iteration 23, loss = 0.03289296
Iteration 24, loss = 0.03184448
Iteration 25, loss = 0.03085077
Iteration 26, loss = 0.03002043
Iteration 27, loss = 0.02916955
Iteration 28, loss = 0.02860577
Iteration 29, loss = 0.02814878
Iteration 30, loss = 0.02744702
Iteration 31, loss = 0.02745919
Iteration 32, loss = 0.02700911
Iteration 33, loss = 0.02646563
Iteration 34, loss = 0.02606790
Iteration 35, loss = 0.02557769
Iteration 36, loss = 0.02538557
Iteration 37, loss = 0.02534007
Iteration 38, loss = 0.02494664
Iteration 39, loss = 0.02479463
Iteration 40, loss = 0.02447682
Iteration 41, loss = 0.02427104
Iteration 42, loss = 0.02397281
Iteration 43, loss = 0.02385260
Iteration 44, loss = 0.02390781
Iteration 45, loss = 0.02354822
Iteration 46, loss = 0.02342144
Iteration 47, loss = 0.02340365
Iteration 48, loss = 0.02347590
Iteration 49, loss = 0.02306004
Iteration 50, loss = 0.02306087
Iteration 51, loss = 0.02285844
Iteration 52, loss = 0.02278534
Iteration 53, loss = 0.02303589
Iteration 54, loss = 0.02254503
Iteration 55, loss = 0.02268229
Iteration 56, loss = 0.02274415
Iteration 57, loss = 0.02226237
Iteration 58, loss = 0.02241250
Iteration 59, loss = 0.02204115
Iteration 60, loss = 0.02223146
Iteration 61, loss = 0.02225173
Iteration 62, loss = 0.02214123
Iteration 63, loss = 0.02222352
Iteration 64, loss = 0.02214650
Iteration 65, loss = 0.02195099
Iteration 66, loss = 0.02190655
Iteration 67, loss = 0.02206212
Iteration 68, loss = 0.02199965
Iteration 69, loss = 0.02204966
Iteration 70, loss = 0.02178417
Iteration 71, loss = 0.02194392
Iteration 72, loss = 0.02151244
Iteration 73, loss = 0.02188965
Iteration 74, loss = 0.02157432
Iteration 75, loss = 0.02141630
Iteration 76, loss = 0.02152269
Iteration 77, loss = 0.02141639
Iteration 78, loss = 0.02160587
Iteration 79, loss = 0.02138252
Iteration 80, loss = 0.02147267
Iteration 81, loss = 0.02156480
Iteration 82, loss = 0.02147995
Iteration 83, loss = 0.02160652
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67685908
Iteration 2, loss = 0.61919178
Iteration 3, loss = 0.52697528
Iteration 4, loss = 0.42412824
Iteration 5, loss = 0.33028084
Iteration 6, loss = 0.25481057
Iteration 7, loss = 0.19801715
Iteration 8, loss = 0.15694949
Iteration 9, loss = 0.12746230
Iteration 10, loss = 0.10619542
Iteration 11, loss = 0.09094125
Iteration 12, loss = 0.07905018
Iteration 13, loss = 0.07008739
Iteration 14, loss = 0.06308488
Iteration 15, loss = 0.05745002
Iteration 16, loss = 0.05304438
Iteration 17, loss = 0.04941302
Iteration 18, loss = 0.04631895
Iteration 19, loss = 0.04355756
Iteration 20, loss = 0.04180433
Iteration 21, loss = 0.03993429
Iteration 22, loss = 0.03860791
Iteration 23, loss = 0.03690388
Iteration 24, loss = 0.03623593
Iteration 25, loss = 0.03571715
Iteration 26, loss = 0.03437275
Iteration 27, loss = 0.03354993
Iteration 28, loss = 0.03273870
Iteration 29, loss = 0.03240481
Iteration 30, loss = 0.03163695
Iteration 31, loss = 0.03092271
Iteration 32, loss = 0.03032705
Iteration 33, loss = 0.02957221
Iteration 34, loss = 0.02937057
Iteration 35, loss = 0.02873467
Iteration 36, loss = 0.02842874
Iteration 37, loss = 0.02821175
Iteration 38, loss = 0.02797937
Iteration 39, loss = 0.02761819
Iteration 40, loss = 0.02742243
Iteration 41, loss = 0.02716873
Iteration 42, loss = 0.02681506
Iteration 43, loss = 0.02642546
Iteration 44, loss = 0.02633090
Iteration 45, loss = 0.02634354
Iteration 46, loss = 0.02607375
Iteration 47, loss = 0.02598900
Iteration 48, loss = 0.02585315
Iteration 49, loss = 0.02562569
Iteration 50, loss = 0.02532693
Iteration 51, loss = 0.02541201
Iteration 52, loss = 0.02569970
Iteration 53, loss = 0.02561391
Iteration 54, loss = 0.02519420
Iteration 55, loss = 0.02541313
Iteration 56, loss = 0.02519538
Iteration 57, loss = 0.02472588
Iteration 58, loss = 0.02465949
Iteration 59, loss = 0.02467434
Iteration 60, loss = 0.02458601
Iteration 61, loss = 0.02445666
Iteration 62, loss = 0.02444040
Iteration 63, loss = 0.02465419
Iteration 64, loss = 0.02410726
Iteration 65, loss = 0.02422163
Iteration 66, loss = 0.02409902
Iteration 67, loss = 0.02411133
Iteration 68, loss = 0.02443425
Iteration 69, loss = 0.02460226
Iteration 70, loss = 0.02447713
Iteration 71, loss = 0.02433378
Iteration 72, loss = 0.02396676
Iteration 73, loss = 0.02456124
Iteration 74, loss = 0.02424244
Iteration 75, loss = 0.02432201
Iteration 76, loss = 0.02430360
Iteration 77, loss = 0.02430670
Iteration 78, loss = 0.02517067
Iteration 79, loss = 0.02468643
Iteration 80, loss = 0.02419302
Iteration 81, loss = 0.02377729
Iteration 82, loss = 0.02431086
Iteration 83, loss = 0.02498222
Iteration 84, loss = 0.02458457
Iteration 85, loss = 0.02419398
Iteration 86, loss = 0.02420929
Iteration 87, loss = 0.02376318
Iteration 88, loss = 0.02364377
Iteration 89, loss = 0.02348542
Iteration 90, loss = 0.02330164
Iteration 91, loss = 0.02340682
Iteration 92, loss = 0.02335568
Iteration 93, loss = 0.02337503
Iteration 94, loss = 0.02314715
Iteration 95, loss = 0.02361563
Iteration 96, loss = 0.02450015
Iteration 97, loss = 0.02405027
Iteration 98, loss = 0.02367760
Iteration 99, loss = 0.02333895
Iteration 100, loss = 0.02314339
Iteration 101, loss = 0.02321621
Iteration 102, loss = 0.02321660
Iteration 103, loss = 0.02386066
Iteration 104, loss = 0.02366978
Iteration 105, loss = 0.02345220
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67301457
Iteration 2, loss = 0.59294869
Iteration 3, loss = 0.48380838
Iteration 4, loss = 0.37390509
Iteration 5, loss = 0.27964018
Iteration 6, loss = 0.20886867
Iteration 7, loss = 0.15949077
Iteration 8, loss = 0.12541550
Iteration 9, loss = 0.10243025
Iteration 10, loss = 0.08581465
Iteration 11, loss = 0.07393566
Iteration 12, loss = 0.06529597
Iteration 13, loss = 0.05846527
Iteration 14, loss = 0.05336301
Iteration 15, loss = 0.04917130
Iteration 16, loss = 0.04611188
Iteration 17, loss = 0.04336739
Iteration 18, loss = 0.04099216
Iteration 19, loss = 0.03931029
Iteration 20, loss = 0.03777705
Iteration 21, loss = 0.03638582
Iteration 22, loss = 0.03513823
Iteration 23, loss = 0.03411866
Iteration 24, loss = 0.03352209
Iteration 25, loss = 0.03260841
Iteration 26, loss = 0.03189861
Iteration 27, loss = 0.03130635
Iteration 28, loss = 0.03079600
Iteration 29, loss = 0.03041843
Iteration 30, loss = 0.03027502
Iteration 31, loss = 0.02933829
Iteration 32, loss = 0.02927217
Iteration 33, loss = 0.02896592
Iteration 34, loss = 0.02861857
Iteration 35, loss = 0.02849935
Iteration 36, loss = 0.02786009
Iteration 37, loss = 0.02817430
Iteration 38, loss = 0.02756029
Iteration 39, loss = 0.02731087
Iteration 40, loss = 0.02720277
Iteration 41, loss = 0.02759957
Iteration 42, loss = 0.02711892
Iteration 43, loss = 0.02683373
Iteration 44, loss = 0.02697866
Iteration 45, loss = 0.02687741
Iteration 46, loss = 0.02667018
Iteration 47, loss = 0.02681141
Iteration 48, loss = 0.02648713
Iteration 49, loss = 0.02637454
Iteration 50, loss = 0.02625416
Iteration 51, loss = 0.02638049
Iteration 52, loss = 0.02631891
Iteration 53, loss = 0.02597281
Iteration 54, loss = 0.02607211
Iteration 55, loss = 0.02594656
Iteration 56, loss = 0.02609063
Iteration 57, loss = 0.02568360
Iteration 58, loss = 0.02579283
Iteration 59, loss = 0.02570801
Iteration 60, loss = 0.02577764
Iteration 61, loss = 0.02548601
Iteration 62, loss = 0.02558817
Iteration 63, loss = 0.02558056
Iteration 64, loss = 0.02543105
Iteration 65, loss = 0.02511091
Iteration 66, loss = 0.02561025
Iteration 67, loss = 0.02535393
Iteration 68, loss = 0.02557394
Iteration 69, loss = 0.02538509
Iteration 70, loss = 0.02511628
Iteration 71, loss = 0.02538295
Iteration 72, loss = 0.02571976
Iteration 73, loss = 0.02528826
Iteration 74, loss = 0.02548334
Iteration 75, loss = 0.02503879
Iteration 76, loss = 0.02501401
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68417913
Iteration 2, loss = 0.66589583
Iteration 3, loss = 0.64812047
Iteration 4, loss = 0.62744532
Iteration 5, loss = 0.60324212
Iteration 6, loss = 0.57577881
Iteration 7, loss = 0.54527945
Iteration 8, loss = 0.51227982
Iteration 9, loss = 0.47787090
Iteration 10, loss = 0.44290629
Iteration 11, loss = 0.40832223
Iteration 12, loss = 0.37470150
Iteration 13, loss = 0.34290028
Iteration 14, loss = 0.31300885
Iteration 15, loss = 0.28501858
Iteration 16, loss = 0.25934861
Iteration 17, loss = 0.23589460
Iteration 18, loss = 0.21465794
Iteration 19, loss = 0.19523128
Iteration 20, loss = 0.17793396
Iteration 21, loss = 0.16243015
Iteration 22, loss = 0.14862506
Iteration 23, loss = 0.13612701
Iteration 24, loss = 0.12510729
Iteration 25, loss = 0.11524288
Iteration 26, loss = 0.10653355
Iteration 27, loss = 0.09871160
Iteration 28, loss = 0.09164662
Iteration 29, loss = 0.08540267
Iteration 30, loss = 0.07978163
Iteration 31, loss = 0.07471456
Iteration 32, loss = 0.07026642
Iteration 33, loss = 0.06608250
Iteration 34, loss = 0.06229990
Iteration 35, loss = 0.05896630
Iteration 36, loss = 0.05586144
Iteration 37, loss = 0.05309972
Iteration 38, loss = 0.05049084
Iteration 39, loss = 0.04813281
Iteration 40, loss = 0.04602318
Iteration 41, loss = 0.04400029
Iteration 42, loss = 0.04216252
Iteration 43, loss = 0.04038098
Iteration 44, loss = 0.03888962
Iteration 45, loss = 0.03740724
Iteration 46, loss = 0.03601286
Iteration 47, loss = 0.03479785
Iteration 48, loss = 0.03363192
Iteration 49, loss = 0.03248918
Iteration 50, loss = 0.03146889
Iteration 51, loss = 0.03045665
Iteration 52, loss = 0.02957414
Iteration 53, loss = 0.02873549
Iteration 54, loss = 0.02794437
Iteration 55, loss = 0.02718729
Iteration 56, loss = 0.02648849
Iteration 57, loss = 0.02579091
Iteration 58, loss = 0.02516417
Iteration 59, loss = 0.02454136
Iteration 60, loss = 0.02398176
Iteration 61, loss = 0.02346255
Iteration 62, loss = 0.02291195
Iteration 63, loss = 0.02242053
Iteration 64, loss = 0.02202040
Iteration 65, loss = 0.02153818
Iteration 66, loss = 0.02114508
Iteration 67, loss = 0.02072287
Iteration 68, loss = 0.02035894
Iteration 69, loss = 0.02003296
Iteration 70, loss = 0.01966932
Iteration 71, loss = 0.01928357
Iteration 72, loss = 0.01901093
Iteration 73, loss = 0.01871912
Iteration 74, loss = 0.01836963
Iteration 75, loss = 0.01814603
Iteration 76, loss = 0.01787652
Iteration 77, loss = 0.01760311
Iteration 78, loss = 0.01744110
Iteration 79, loss = 0.01715621
Iteration 80, loss = 0.01688737
Iteration 81, loss = 0.01666276
Iteration 82, loss = 0.01647311
Iteration 83, loss = 0.01624735
Iteration 84, loss = 0.01604471
Iteration 85, loss = 0.01591793
Iteration 86, loss = 0.01570343
Iteration 87, loss = 0.01549901
Iteration 88, loss = 0.01536748
Iteration 89, loss = 0.01518281
Iteration 90, loss = 0.01502821
Iteration 91, loss = 0.01488264
Iteration 92, loss = 0.01477808
Iteration 93, loss = 0.01464163
Iteration 94, loss = 0.01451361
Iteration 95, loss = 0.01432552
Iteration 96, loss = 0.01419370
Iteration 97, loss = 0.01409206
Iteration 98, loss = 0.01400175
Iteration 99, loss = 0.01389363
Iteration 100, loss = 0.01371090
Iteration 101, loss = 0.01370216
Iteration 102, loss = 0.01357505
Iteration 103, loss = 0.01346878
Iteration 104, loss = 0.01332022
Iteration 105, loss = 0.01325474
Iteration 106, loss = 0.01316573
Iteration 107, loss = 0.01304720
Iteration 108, loss = 0.01297041
Iteration 109, loss = 0.01285918
Iteration 110, loss = 0.01281664
Iteration 111, loss = 0.01267694
Iteration 112, loss = 0.01263847
Iteration 113, loss = 0.01253225
Iteration 114, loss = 0.01249615
Iteration 115, loss = 0.01235992
Iteration 116, loss = 0.01232745
Iteration 117, loss = 0.01223180
Iteration 118, loss = 0.01216767
Iteration 119, loss = 0.01213712
Iteration 120, loss = 0.01211502
Iteration 121, loss = 0.01193986
Iteration 122, loss = 0.01200316
Iteration 123, loss = 0.01189956
Iteration 124, loss = 0.01178759
Iteration 125, loss = 0.01173930
Iteration 126, loss = 0.01167424
Iteration 127, loss = 0.01162085
Iteration 128, loss = 0.01163641
Iteration 129, loss = 0.01151111
Iteration 130, loss = 0.01147615
Iteration 131, loss = 0.01146265
Iteration 132, loss = 0.01139467
Iteration 133, loss = 0.01132203
Iteration 134, loss = 0.01133659
Iteration 135, loss = 0.01127068
Iteration 136, loss = 0.01126309
Iteration 137, loss = 0.01121238
Iteration 138, loss = 0.01114471
Iteration 139, loss = 0.01106140
Iteration 140, loss = 0.01100721
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68779153
Iteration 2, loss = 0.67065777
Iteration 3, loss = 0.65594229
Iteration 4, loss = 0.64132781
Iteration 5, loss = 0.62600124
Iteration 6, loss = 0.60939583
Iteration 7, loss = 0.59138284
Iteration 8, loss = 0.57180743
Iteration 9, loss = 0.55090407
Iteration 10, loss = 0.52853866
Iteration 11, loss = 0.50509065
Iteration 12, loss = 0.48070509
Iteration 13, loss = 0.45593673
Iteration 14, loss = 0.43095858
Iteration 15, loss = 0.40616301
Iteration 16, loss = 0.38183592
Iteration 17, loss = 0.35814037
Iteration 18, loss = 0.33547940
Iteration 19, loss = 0.31381358
Iteration 20, loss = 0.29334219
Iteration 21, loss = 0.27394066
Iteration 22, loss = 0.25580652
Iteration 23, loss = 0.23881243
Iteration 24, loss = 0.22290408
Iteration 25, loss = 0.20817094
Iteration 26, loss = 0.19445834
Iteration 27, loss = 0.18169943
Iteration 28, loss = 0.16993631
Iteration 29, loss = 0.15903782
Iteration 30, loss = 0.14897835
Iteration 31, loss = 0.13969292
Iteration 32, loss = 0.13116700
Iteration 33, loss = 0.12331487
Iteration 34, loss = 0.11601171
Iteration 35, loss = 0.10930145
Iteration 36, loss = 0.10319264
Iteration 37, loss = 0.09746661
Iteration 38, loss = 0.09217872
Iteration 39, loss = 0.08735790
Iteration 40, loss = 0.08284530
Iteration 41, loss = 0.07870656
Iteration 42, loss = 0.07484571
Iteration 43, loss = 0.07127696
Iteration 44, loss = 0.06796423
Iteration 45, loss = 0.06485563
Iteration 46, loss = 0.06199524
Iteration 47, loss = 0.05932710
Iteration 48, loss = 0.05678097
Iteration 49, loss = 0.05444223
Iteration 50, loss = 0.05224837
Iteration 51, loss = 0.05020189
Iteration 52, loss = 0.04827966
Iteration 53, loss = 0.04646753
Iteration 54, loss = 0.04477087
Iteration 55, loss = 0.04314817
Iteration 56, loss = 0.04166370
Iteration 57, loss = 0.04022267
Iteration 58, loss = 0.03887568
Iteration 59, loss = 0.03763003
Iteration 60, loss = 0.03641648
Iteration 61, loss = 0.03526392
Iteration 62, loss = 0.03417282
Iteration 63, loss = 0.03314957
Iteration 64, loss = 0.03217875
Iteration 65, loss = 0.03124373
Iteration 66, loss = 0.03035511
Iteration 67, loss = 0.02949907
Iteration 68, loss = 0.02869346
Iteration 69, loss = 0.02792714
Iteration 70, loss = 0.02719182
Iteration 71, loss = 0.02649262
Iteration 72, loss = 0.02582947
Iteration 73, loss = 0.02518251
Iteration 74, loss = 0.02456819
Iteration 75, loss = 0.02398618
Iteration 76, loss = 0.02343513
Iteration 77, loss = 0.02288705
Iteration 78, loss = 0.02237568
Iteration 79, loss = 0.02188259
Iteration 80, loss = 0.02140778
Iteration 81, loss = 0.02094891
Iteration 82, loss = 0.02051266
Iteration 83, loss = 0.02009370
Iteration 84, loss = 0.01967980
Iteration 85, loss = 0.01929046
Iteration 86, loss = 0.01891040
Iteration 87, loss = 0.01855385
Iteration 88, loss = 0.01820305
Iteration 89, loss = 0.01785779
Iteration 90, loss = 0.01753070
Iteration 91, loss = 0.01721643
Iteration 92, loss = 0.01691144
Iteration 93, loss = 0.01661629
Iteration 94, loss = 0.01633886
Iteration 95, loss = 0.01606974
Iteration 96, loss = 0.01579674
Iteration 97, loss = 0.01553510
Iteration 98, loss = 0.01529322
Iteration 99, loss = 0.01505393
Iteration 100, loss = 0.01481993
Iteration 101, loss = 0.01459191
Iteration 102, loss = 0.01436341
Iteration 103, loss = 0.01415612
Iteration 104, loss = 0.01394513
Iteration 105, loss = 0.01374581
Iteration 106, loss = 0.01355759
Iteration 107, loss = 0.01335640
Iteration 108, loss = 0.01318057
Iteration 109, loss = 0.01300434
Iteration 110, loss = 0.01283330
Iteration 111, loss = 0.01267016
Iteration 112, loss = 0.01251055
Iteration 113, loss = 0.01234890
Iteration 114, loss = 0.01219481
Iteration 115, loss = 0.01203636
Iteration 116, loss = 0.01189335
Iteration 117, loss = 0.01174912
Iteration 118, loss = 0.01160848
Iteration 119, loss = 0.01147460
Iteration 120, loss = 0.01134121
Iteration 121, loss = 0.01121412
Iteration 122, loss = 0.01109279
Iteration 123, loss = 0.01096474
Iteration 124, loss = 0.01084843
Iteration 125, loss = 0.01073312
Iteration 126, loss = 0.01062191
Iteration 127, loss = 0.01051038
Iteration 128, loss = 0.01040708
Iteration 129, loss = 0.01029783
Iteration 130, loss = 0.01019692
Iteration 131, loss = 0.01010425
Iteration 132, loss = 0.00999875
Iteration 133, loss = 0.00990924
Iteration 134, loss = 0.00981404
Iteration 135, loss = 0.00971909
Iteration 136, loss = 0.00962714
Iteration 137, loss = 0.00954584
Iteration 138, loss = 0.00945500
Iteration 139, loss = 0.00937705
Iteration 140, loss = 0.00928784
Iteration 141, loss = 0.00920821
Iteration 142, loss = 0.00913054
Iteration 143, loss = 0.00906220
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72504353
Iteration 2, loss = 0.69301455
Iteration 3, loss = 0.65482002
Iteration 4, loss = 0.60665005
Iteration 5, loss = 0.55216580
Iteration 6, loss = 0.49620293
Iteration 7, loss = 0.44023595
Iteration 8, loss = 0.38715103
Iteration 9, loss = 0.33830840
Iteration 10, loss = 0.29491870
Iteration 11, loss = 0.25683831
Iteration 12, loss = 0.22395847
Iteration 13, loss = 0.19577567
Iteration 14, loss = 0.17181389
Iteration 15, loss = 0.15171022
Iteration 16, loss = 0.13448444
Iteration 17, loss = 0.12004603
Iteration 18, loss = 0.10758709
Iteration 19, loss = 0.09720108
Iteration 20, loss = 0.08816312
Iteration 21, loss = 0.08049167
Iteration 22, loss = 0.07371074
Iteration 23, loss = 0.06789219
Iteration 24, loss = 0.06284328
Iteration 25, loss = 0.05826660
Iteration 26, loss = 0.05443907
Iteration 27, loss = 0.05094616
Iteration 28, loss = 0.04785456
Iteration 29, loss = 0.04503406
Iteration 30, loss = 0.04260576
Iteration 31, loss = 0.04033701
Iteration 32, loss = 0.03843818
Iteration 33, loss = 0.03652411
Iteration 34, loss = 0.03492678
Iteration 35, loss = 0.03334837
Iteration 36, loss = 0.03200121
Iteration 37, loss = 0.03065851
Iteration 38, loss = 0.02945607
Iteration 39, loss = 0.02839862
Iteration 40, loss = 0.02740720
Iteration 41, loss = 0.02645298
Iteration 42, loss = 0.02555361
Iteration 43, loss = 0.02475839
Iteration 44, loss = 0.02398592
Iteration 45, loss = 0.02346044
Iteration 46, loss = 0.02276458
Iteration 47, loss = 0.02217017
Iteration 48, loss = 0.02161297
Iteration 49, loss = 0.02103777
Iteration 50, loss = 0.02057822
Iteration 51, loss = 0.02002177
Iteration 52, loss = 0.01955768
Iteration 53, loss = 0.01919145
Iteration 54, loss = 0.01889676
Iteration 55, loss = 0.01846204
Iteration 56, loss = 0.01807860
Iteration 57, loss = 0.01779297
Iteration 58, loss = 0.01740848
Iteration 59, loss = 0.01713786
Iteration 60, loss = 0.01681190
Iteration 61, loss = 0.01656991
Iteration 62, loss = 0.01621698
Iteration 63, loss = 0.01606420
Iteration 64, loss = 0.01579951
Iteration 65, loss = 0.01560693
Iteration 66, loss = 0.01535780
Iteration 67, loss = 0.01509190
Iteration 68, loss = 0.01491628
Iteration 69, loss = 0.01482160
Iteration 70, loss = 0.01458500
Iteration 71, loss = 0.01442288
Iteration 72, loss = 0.01426304
Iteration 73, loss = 0.01416221
Iteration 74, loss = 0.01391775
Iteration 75, loss = 0.01379981
Iteration 76, loss = 0.01378143
Iteration 77, loss = 0.01359007
Iteration 78, loss = 0.01343004
Iteration 79, loss = 0.01329836
Iteration 80, loss = 0.01322385
Iteration 81, loss = 0.01304311
Iteration 82, loss = 0.01295640
Iteration 83, loss = 0.01282487
Iteration 84, loss = 0.01268140
Iteration 85, loss = 0.01261487
Iteration 86, loss = 0.01253990
Iteration 87, loss = 0.01251089
Iteration 88, loss = 0.01235244
Iteration 89, loss = 0.01242240
Iteration 90, loss = 0.01258925
Iteration 91, loss = 0.01255420
Iteration 92, loss = 0.01233587
Iteration 93, loss = 0.01221847
Iteration 94, loss = 0.01205968
Iteration 95, loss = 0.01192444
Iteration 96, loss = 0.01196714
Iteration 97, loss = 0.01180138
Iteration 98, loss = 0.01173168
Iteration 99, loss = 0.01190454
Iteration 100, loss = 0.01193970
Iteration 101, loss = 0.01177579
Iteration 102, loss = 0.01165456
Iteration 103, loss = 0.01161313
Iteration 104, loss = 0.01148890
Iteration 105, loss = 0.01158088
Iteration 106, loss = 0.01140277
Iteration 107, loss = 0.01116220
Iteration 108, loss = 0.01129404
Iteration 109, loss = 0.01119148
Iteration 110, loss = 0.01109101
Iteration 111, loss = 0.01117655
Iteration 112, loss = 0.01111366
Iteration 113, loss = 0.01116750
Iteration 114, loss = 0.01101249
Iteration 115, loss = 0.01098630
Iteration 116, loss = 0.01089645
Iteration 117, loss = 0.01082535
Iteration 118, loss = 0.01081874
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71046699
Iteration 2, loss = 0.69455419
Iteration 3, loss = 0.68143917
Iteration 4, loss = 0.66906287
Iteration 5, loss = 0.65613022
Iteration 6, loss = 0.64225815
Iteration 7, loss = 0.62698475
Iteration 8, loss = 0.61031577
Iteration 9, loss = 0.59205366
Iteration 10, loss = 0.57251435
Iteration 11, loss = 0.55134970
Iteration 12, loss = 0.52914629
Iteration 13, loss = 0.50601467
Iteration 14, loss = 0.48230440
Iteration 15, loss = 0.45822724
Iteration 16, loss = 0.43439052
Iteration 17, loss = 0.41059200
Iteration 18, loss = 0.38744145
Iteration 19, loss = 0.36481833
Iteration 20, loss = 0.34289853
Iteration 21, loss = 0.32200559
Iteration 22, loss = 0.30190155
Iteration 23, loss = 0.28283793
Iteration 24, loss = 0.26479719
Iteration 25, loss = 0.24774585
Iteration 26, loss = 0.23169294
Iteration 27, loss = 0.21673246
Iteration 28, loss = 0.20268539
Iteration 29, loss = 0.18968975
Iteration 30, loss = 0.17745778
Iteration 31, loss = 0.16626629
Iteration 32, loss = 0.15581341
Iteration 33, loss = 0.14612403
Iteration 34, loss = 0.13722980
Iteration 35, loss = 0.12892695
Iteration 36, loss = 0.12128440
Iteration 37, loss = 0.11422977
Iteration 38, loss = 0.10768295
Iteration 39, loss = 0.10163699
Iteration 40, loss = 0.09605167
Iteration 41, loss = 0.09084763
Iteration 42, loss = 0.08608129
Iteration 43, loss = 0.08160282
Iteration 44, loss = 0.07747589
Iteration 45, loss = 0.07367383
Iteration 46, loss = 0.07008615
Iteration 47, loss = 0.06675616
Iteration 48, loss = 0.06367239
Iteration 49, loss = 0.06079782
Iteration 50, loss = 0.05809303
Iteration 51, loss = 0.05557811
Iteration 52, loss = 0.05319049
Iteration 53, loss = 0.05097458
Iteration 54, loss = 0.04890004
Iteration 55, loss = 0.04690955
Iteration 56, loss = 0.04507385
Iteration 57, loss = 0.04335089
Iteration 58, loss = 0.04171234
Iteration 59, loss = 0.04017872
Iteration 60, loss = 0.03870750
Iteration 61, loss = 0.03735107
Iteration 62, loss = 0.03602460
Iteration 63, loss = 0.03479665
Iteration 64, loss = 0.03361378
Iteration 65, loss = 0.03250895
Iteration 66, loss = 0.03144673
Iteration 67, loss = 0.03043565
Iteration 68, loss = 0.02948434
Iteration 69, loss = 0.02856793
Iteration 70, loss = 0.02769992
Iteration 71, loss = 0.02687715
Iteration 72, loss = 0.02608786
Iteration 73, loss = 0.02532544
Iteration 74, loss = 0.02460656
Iteration 75, loss = 0.02392010
Iteration 76, loss = 0.02325348
Iteration 77, loss = 0.02262607
Iteration 78, loss = 0.02203251
Iteration 79, loss = 0.02143860
Iteration 80, loss = 0.02089128
Iteration 81, loss = 0.02035731
Iteration 82, loss = 0.01984843
Iteration 83, loss = 0.01935374
Iteration 84, loss = 0.01888363
Iteration 85, loss = 0.01843277
Iteration 86, loss = 0.01799520
Iteration 87, loss = 0.01758458
Iteration 88, loss = 0.01717143
Iteration 89, loss = 0.01679112
Iteration 90, loss = 0.01641416
Iteration 91, loss = 0.01605602
Iteration 92, loss = 0.01570315
Iteration 93, loss = 0.01536441
Iteration 94, loss = 0.01503385
Iteration 95, loss = 0.01472569
Iteration 96, loss = 0.01441596
Iteration 97, loss = 0.01411955
Iteration 98, loss = 0.01382668
Iteration 99, loss = 0.01355708
Iteration 100, loss = 0.01328937
Iteration 101, loss = 0.01302900
Iteration 102, loss = 0.01277930
Iteration 103, loss = 0.01253313
Iteration 104, loss = 0.01230003
Iteration 105, loss = 0.01207526
Iteration 106, loss = 0.01185443
Iteration 107, loss = 0.01163761
Iteration 108, loss = 0.01143303
Iteration 109, loss = 0.01122820
Iteration 110, loss = 0.01103739
Iteration 111, loss = 0.01084252
Iteration 112, loss = 0.01065902
Iteration 113, loss = 0.01047624
Iteration 114, loss = 0.01029892
Iteration 115, loss = 0.01012684
Iteration 116, loss = 0.00996275
Iteration 117, loss = 0.00980006
Iteration 118, loss = 0.00964328
Iteration 119, loss = 0.00949048
Iteration 120, loss = 0.00933936
Iteration 121, loss = 0.00919602
Iteration 122, loss = 0.00905006
Iteration 123, loss = 0.00891331
Iteration 124, loss = 0.00878042
Iteration 125, loss = 0.00864789
Iteration 126, loss = 0.00851767
Iteration 127, loss = 0.00839247
Iteration 128, loss = 0.00827452
Iteration 129, loss = 0.00815438
Iteration 130, loss = 0.00804127
Iteration 131, loss = 0.00792794
Iteration 132, loss = 0.00781656
Iteration 133, loss = 0.00771118
Iteration 134, loss = 0.00760821
Iteration 135, loss = 0.00750508
Iteration 136, loss = 0.00740350
Iteration 137, loss = 0.00730577
Iteration 138, loss = 0.00721075
Iteration 139, loss = 0.00711932
Iteration 140, loss = 0.00702891
Iteration 141, loss = 0.00694048
Iteration 142, loss = 0.00685243
Iteration 143, loss = 0.00676507
Iteration 144, loss = 0.00668570
Iteration 145, loss = 0.00660113
Iteration 146, loss = 0.00652239
Iteration 147, loss = 0.00644225
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68736914
Iteration 2, loss = 0.66755891
Iteration 3, loss = 0.64803360
Iteration 4, loss = 0.62532056
Iteration 5, loss = 0.59859085
Iteration 6, loss = 0.56753206
Iteration 7, loss = 0.53362395
Iteration 8, loss = 0.49762127
Iteration 9, loss = 0.46096773
Iteration 10, loss = 0.42462624
Iteration 11, loss = 0.38934625
Iteration 12, loss = 0.35564832
Iteration 13, loss = 0.32387210
Iteration 14, loss = 0.29415898
Iteration 15, loss = 0.26682389
Iteration 16, loss = 0.24178301
Iteration 17, loss = 0.21900178
Iteration 18, loss = 0.19853386
Iteration 19, loss = 0.18024473
Iteration 20, loss = 0.16390166
Iteration 21, loss = 0.14932561
Iteration 22, loss = 0.13644170
Iteration 23, loss = 0.12494678
Iteration 24, loss = 0.11480545
Iteration 25, loss = 0.10568478
Iteration 26, loss = 0.09770055
Iteration 27, loss = 0.09057870
Iteration 28, loss = 0.08411784
Iteration 29, loss = 0.07848179
Iteration 30, loss = 0.07337550
Iteration 31, loss = 0.06872167
Iteration 32, loss = 0.06462657
Iteration 33, loss = 0.06090135
Iteration 34, loss = 0.05745475
Iteration 35, loss = 0.05436687
Iteration 36, loss = 0.05159190
Iteration 37, loss = 0.04905901
Iteration 38, loss = 0.04669873
Iteration 39, loss = 0.04456028
Iteration 40, loss = 0.04255320
Iteration 41, loss = 0.04069824
Iteration 42, loss = 0.03898955
Iteration 43, loss = 0.03739316
Iteration 44, loss = 0.03605917
Iteration 45, loss = 0.03466486
Iteration 46, loss = 0.03340126
Iteration 47, loss = 0.03222368
Iteration 48, loss = 0.03111052
Iteration 49, loss = 0.03015605
Iteration 50, loss = 0.02915214
Iteration 51, loss = 0.02825346
Iteration 52, loss = 0.02745201
Iteration 53, loss = 0.02661030
Iteration 54, loss = 0.02586635
Iteration 55, loss = 0.02521006
Iteration 56, loss = 0.02452106
Iteration 57, loss = 0.02387393
Iteration 58, loss = 0.02327075
Iteration 59, loss = 0.02272641
Iteration 60, loss = 0.02217159
Iteration 61, loss = 0.02172451
Iteration 62, loss = 0.02122881
Iteration 63, loss = 0.02076258
Iteration 64, loss = 0.02039009
Iteration 65, loss = 0.01996301
Iteration 66, loss = 0.01952348
Iteration 67, loss = 0.01917418
Iteration 68, loss = 0.01877851
Iteration 69, loss = 0.01847147
Iteration 70, loss = 0.01809465
Iteration 71, loss = 0.01778966
Iteration 72, loss = 0.01748015
Iteration 73, loss = 0.01723599
Iteration 74, loss = 0.01695197
Iteration 75, loss = 0.01669250
Iteration 76, loss = 0.01641724
Iteration 77, loss = 0.01616364
Iteration 78, loss = 0.01596249
Iteration 79, loss = 0.01573601
Iteration 80, loss = 0.01549383
Iteration 81, loss = 0.01529827
Iteration 82, loss = 0.01510377
Iteration 83, loss = 0.01490869
Iteration 84, loss = 0.01471956
Iteration 85, loss = 0.01450622
Iteration 86, loss = 0.01440509
Iteration 87, loss = 0.01421558
Iteration 88, loss = 0.01406017
Iteration 89, loss = 0.01387455
Iteration 90, loss = 0.01375320
Iteration 91, loss = 0.01360983
Iteration 92, loss = 0.01343033
Iteration 93, loss = 0.01335134
Iteration 94, loss = 0.01318982
Iteration 95, loss = 0.01308414
Iteration 96, loss = 0.01292786
Iteration 97, loss = 0.01280721
Iteration 98, loss = 0.01275074
Iteration 99, loss = 0.01257144
Iteration 100, loss = 0.01250744
Iteration 101, loss = 0.01238031
Iteration 102, loss = 0.01226204
Iteration 103, loss = 0.01218040
Iteration 104, loss = 0.01218780
Iteration 105, loss = 0.01197151
Iteration 106, loss = 0.01189131
Iteration 107, loss = 0.01181628
Iteration 108, loss = 0.01171472
Iteration 109, loss = 0.01162901
Iteration 110, loss = 0.01159842
Iteration 111, loss = 0.01146873
Iteration 112, loss = 0.01143060
Iteration 113, loss = 0.01131400
Iteration 114, loss = 0.01128440
Iteration 115, loss = 0.01120740
Iteration 116, loss = 0.01113298
Iteration 117, loss = 0.01104579
Iteration 118, loss = 0.01096230
Iteration 119, loss = 0.01090378
Iteration 120, loss = 0.01088453
Iteration 121, loss = 0.01083765
Iteration 122, loss = 0.01074254
Iteration 123, loss = 0.01064659
Iteration 124, loss = 0.01061369
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67854205
Iteration 2, loss = 0.62968547
Iteration 3, loss = 0.55366377
Iteration 4, loss = 0.46309205
Iteration 5, loss = 0.37523741
Iteration 6, loss = 0.29843609
Iteration 7, loss = 0.23633398
Iteration 8, loss = 0.18824101
Iteration 9, loss = 0.15191645
Iteration 10, loss = 0.12512163
Iteration 11, loss = 0.10508675
Iteration 12, loss = 0.08997452
Iteration 13, loss = 0.07840310
Iteration 14, loss = 0.06924190
Iteration 15, loss = 0.06198422
Iteration 16, loss = 0.05640261
Iteration 17, loss = 0.05180344
Iteration 18, loss = 0.04777989
Iteration 19, loss = 0.04447071
Iteration 20, loss = 0.04186135
Iteration 21, loss = 0.03948353
Iteration 22, loss = 0.03751681
Iteration 23, loss = 0.03585183
Iteration 24, loss = 0.03433353
Iteration 25, loss = 0.03304134
Iteration 26, loss = 0.03198237
Iteration 27, loss = 0.03099965
Iteration 28, loss = 0.02979128
Iteration 29, loss = 0.02918490
Iteration 30, loss = 0.02861499
Iteration 31, loss = 0.02780138
Iteration 32, loss = 0.02708171
Iteration 33, loss = 0.02669733
Iteration 34, loss = 0.02626958
Iteration 35, loss = 0.02573781
Iteration 36, loss = 0.02555506
Iteration 37, loss = 0.02486266
Iteration 38, loss = 0.02472827
Iteration 39, loss = 0.02443431
Iteration 40, loss = 0.02402632
Iteration 41, loss = 0.02390407
Iteration 42, loss = 0.02357025
Iteration 43, loss = 0.02339358
Iteration 44, loss = 0.02323065
Iteration 45, loss = 0.02291639
Iteration 46, loss = 0.02268066
Iteration 47, loss = 0.02273290
Iteration 48, loss = 0.02250069
Iteration 49, loss = 0.02242516
Iteration 50, loss = 0.02232447
Iteration 51, loss = 0.02221139
Iteration 52, loss = 0.02186614
Iteration 53, loss = 0.02177114
Iteration 54, loss = 0.02184189
Iteration 55, loss = 0.02158214
Iteration 56, loss = 0.02142357
Iteration 57, loss = 0.02139276
Iteration 58, loss = 0.02138018
Iteration 59, loss = 0.02144595
Iteration 60, loss = 0.02143471
Iteration 61, loss = 0.02109642
Iteration 62, loss = 0.02117633
Iteration 63, loss = 0.02073759
Iteration 64, loss = 0.02077884
Iteration 65, loss = 0.02086874
Iteration 66, loss = 0.02072271
Iteration 67, loss = 0.02081088
Iteration 68, loss = 0.02083269
Iteration 69, loss = 0.02063456
Iteration 70, loss = 0.02047634
Iteration 71, loss = 0.02064989
Iteration 72, loss = 0.02065188
Iteration 73, loss = 0.02030031
Iteration 74, loss = 0.02052449
Iteration 75, loss = 0.02029216
Iteration 76, loss = 0.02036067
Iteration 77, loss = 0.02039621
Iteration 78, loss = 0.02036865
Iteration 79, loss = 0.02014235
Iteration 80, loss = 0.02005649
Iteration 81, loss = 0.02010084
Iteration 82, loss = 0.02034223
Iteration 83, loss = 0.02036481
Iteration 84, loss = 0.02005739
Iteration 85, loss = 0.02023731
Iteration 86, loss = 0.02001760
Iteration 87, loss = 0.02022232
Iteration 88, loss = 0.02023654
Iteration 89, loss = 0.02018901
Iteration 90, loss = 0.01993146
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67540125
Iteration 2, loss = 0.60588733
Iteration 3, loss = 0.49804105
Iteration 4, loss = 0.38361889
Iteration 5, loss = 0.28529477
Iteration 6, loss = 0.21166704
Iteration 7, loss = 0.16031060
Iteration 8, loss = 0.12496822
Iteration 9, loss = 0.10122286
Iteration 10, loss = 0.08444646
Iteration 11, loss = 0.07232207
Iteration 12, loss = 0.06320474
Iteration 13, loss = 0.05616193
Iteration 14, loss = 0.05099272
Iteration 15, loss = 0.04680442
Iteration 16, loss = 0.04311685
Iteration 17, loss = 0.04051236
Iteration 18, loss = 0.03820940
Iteration 19, loss = 0.03619695
Iteration 20, loss = 0.03458297
Iteration 21, loss = 0.03328806
Iteration 22, loss = 0.03205111
Iteration 23, loss = 0.03112215
Iteration 24, loss = 0.03029634
Iteration 25, loss = 0.02928244
Iteration 26, loss = 0.02862104
Iteration 27, loss = 0.02788770
Iteration 28, loss = 0.02750188
Iteration 29, loss = 0.02706053
Iteration 30, loss = 0.02651511
Iteration 31, loss = 0.02627483
Iteration 32, loss = 0.02566734
Iteration 33, loss = 0.02549356
Iteration 34, loss = 0.02504535
Iteration 35, loss = 0.02500005
Iteration 36, loss = 0.02474063
Iteration 37, loss = 0.02423893
Iteration 38, loss = 0.02416622
Iteration 39, loss = 0.02408293
Iteration 40, loss = 0.02402309
Iteration 41, loss = 0.02386237
Iteration 42, loss = 0.02362903
Iteration 43, loss = 0.02363020
Iteration 44, loss = 0.02340314
Iteration 45, loss = 0.02334599
Iteration 46, loss = 0.02308337
Iteration 47, loss = 0.02312419
Iteration 48, loss = 0.02278380
Iteration 49, loss = 0.02276236
Iteration 50, loss = 0.02276741
Iteration 51, loss = 0.02278877
Iteration 52, loss = 0.02257513
Iteration 53, loss = 0.02262529
Iteration 54, loss = 0.02249726
Iteration 55, loss = 0.02223693
Iteration 56, loss = 0.02209918
Iteration 57, loss = 0.02216879
Iteration 58, loss = 0.02212315
Iteration 59, loss = 0.02227433
Iteration 60, loss = 0.02191901
Iteration 61, loss = 0.02231602
Iteration 62, loss = 0.02198305
Iteration 63, loss = 0.02182060
Iteration 64, loss = 0.02205471
Iteration 65, loss = 0.02176113
Iteration 66, loss = 0.02191508
Iteration 67, loss = 0.02188129
Iteration 68, loss = 0.02228660
Iteration 69, loss = 0.02167447
Iteration 70, loss = 0.02150622
Iteration 71, loss = 0.02156941
Iteration 72, loss = 0.02206715
Iteration 73, loss = 0.02143149
Iteration 74, loss = 0.02172549
Iteration 75, loss = 0.02147733
Iteration 76, loss = 0.02149080
Iteration 77, loss = 0.02155206
Iteration 78, loss = 0.02162030
Iteration 79, loss = 0.02155368
Iteration 80, loss = 0.02154874
Iteration 81, loss = 0.02151954
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67396275
Iteration 2, loss = 0.60076762
Iteration 3, loss = 0.49758293
Iteration 4, loss = 0.38968808
Iteration 5, loss = 0.29480808
Iteration 6, loss = 0.22240891
Iteration 7, loss = 0.17089218
Iteration 8, loss = 0.13515857
Iteration 9, loss = 0.11013407
Iteration 10, loss = 0.09245718
Iteration 11, loss = 0.07959778
Iteration 12, loss = 0.06967196
Iteration 13, loss = 0.06237533
Iteration 14, loss = 0.05663174
Iteration 15, loss = 0.05222859
Iteration 16, loss = 0.04831467
Iteration 17, loss = 0.04533461
Iteration 18, loss = 0.04290360
Iteration 19, loss = 0.04083320
Iteration 20, loss = 0.03886527
Iteration 21, loss = 0.03749472
Iteration 22, loss = 0.03614356
Iteration 23, loss = 0.03510119
Iteration 24, loss = 0.03409658
Iteration 25, loss = 0.03324291
Iteration 26, loss = 0.03266103
Iteration 27, loss = 0.03169601
Iteration 28, loss = 0.03126742
Iteration 29, loss = 0.03087122
Iteration 30, loss = 0.03027908
Iteration 31, loss = 0.02980519
Iteration 32, loss = 0.02966864
Iteration 33, loss = 0.02915488
Iteration 34, loss = 0.02888494
Iteration 35, loss = 0.02858472
Iteration 36, loss = 0.02839446
Iteration 37, loss = 0.02810416
Iteration 38, loss = 0.02772453
Iteration 39, loss = 0.02772940
Iteration 40, loss = 0.02734607
Iteration 41, loss = 0.02718028
Iteration 42, loss = 0.02703807
Iteration 43, loss = 0.02699045
Iteration 44, loss = 0.02686150
Iteration 45, loss = 0.02662129
Iteration 46, loss = 0.02651337
Iteration 47, loss = 0.02650309
Iteration 48, loss = 0.02613779
Iteration 49, loss = 0.02647733
Iteration 50, loss = 0.02601499
Iteration 51, loss = 0.02586935
Iteration 52, loss = 0.02579539
Iteration 53, loss = 0.02571330
Iteration 54, loss = 0.02572466
Iteration 55, loss = 0.02558231
Iteration 56, loss = 0.02543686
Iteration 57, loss = 0.02567613
Iteration 58, loss = 0.02550073
Iteration 59, loss = 0.02553616
Iteration 60, loss = 0.02531393
Iteration 61, loss = 0.02530015
Iteration 62, loss = 0.02506611
Iteration 63, loss = 0.02518810
Iteration 64, loss = 0.02565375
Iteration 65, loss = 0.02510453
Iteration 66, loss = 0.02521552
Iteration 67, loss = 0.02543829
Iteration 68, loss = 0.02488605
Iteration 69, loss = 0.02499407
Iteration 70, loss = 0.02483847
Iteration 71, loss = 0.02514132
Iteration 72, loss = 0.02477507
Iteration 73, loss = 0.02485097
Iteration 74, loss = 0.02481568
Iteration 75, loss = 0.02486925
Iteration 76, loss = 0.02499979
Iteration 77, loss = 0.02481243
Iteration 78, loss = 0.02455544
Iteration 79, loss = 0.02464794
Iteration 80, loss = 0.02458790
Iteration 81, loss = 0.02494784
Iteration 82, loss = 0.02453867
Iteration 83, loss = 0.02446953
Iteration 84, loss = 0.02471405
Iteration 85, loss = 0.02442440
Iteration 86, loss = 0.02451935
Iteration 87, loss = 0.02462131
Iteration 88, loss = 0.02425512
Iteration 89, loss = 0.02413820
Iteration 90, loss = 0.02423926
Iteration 91, loss = 0.02437090
Iteration 92, loss = 0.02426321
Iteration 93, loss = 0.02452738
Iteration 94, loss = 0.02434466
Iteration 95, loss = 0.02446473
Iteration 96, loss = 0.02446984
Iteration 97, loss = 0.02452107
Iteration 98, loss = 0.02441948
Iteration 99, loss = 0.02451553
Iteration 100, loss = 0.02442486
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69632799
Iteration 2, loss = 0.65904481
Iteration 3, loss = 0.61014617
Iteration 4, loss = 0.54661740
Iteration 5, loss = 0.47273620
Iteration 6, loss = 0.40100818
Iteration 7, loss = 0.33515036
Iteration 8, loss = 0.27822778
Iteration 9, loss = 0.23136852
Iteration 10, loss = 0.19318763
Iteration 11, loss = 0.16285648
Iteration 12, loss = 0.13889110
Iteration 13, loss = 0.11981766
Iteration 14, loss = 0.10469088
Iteration 15, loss = 0.09241791
Iteration 16, loss = 0.08257797
Iteration 17, loss = 0.07440585
Iteration 18, loss = 0.06769141
Iteration 19, loss = 0.06201876
Iteration 20, loss = 0.05742656
Iteration 21, loss = 0.05331833
Iteration 22, loss = 0.04996649
Iteration 23, loss = 0.04689410
Iteration 24, loss = 0.04426701
Iteration 25, loss = 0.04208848
Iteration 26, loss = 0.04004433
Iteration 27, loss = 0.03834529
Iteration 28, loss = 0.03664809
Iteration 29, loss = 0.03533237
Iteration 30, loss = 0.03400451
Iteration 31, loss = 0.03281748
Iteration 32, loss = 0.03184241
Iteration 33, loss = 0.03095410
Iteration 34, loss = 0.02996725
Iteration 35, loss = 0.02936143
Iteration 36, loss = 0.02861522
Iteration 37, loss = 0.02810701
Iteration 38, loss = 0.02741638
Iteration 39, loss = 0.02675920
Iteration 40, loss = 0.02628425
Iteration 41, loss = 0.02596919
Iteration 42, loss = 0.02542682
Iteration 43, loss = 0.02493693
Iteration 44, loss = 0.02458751
Iteration 45, loss = 0.02430106
Iteration 46, loss = 0.02391741
Iteration 47, loss = 0.02359587
Iteration 48, loss = 0.02340009
Iteration 49, loss = 0.02312908
Iteration 50, loss = 0.02277323
Iteration 51, loss = 0.02272417
Iteration 52, loss = 0.02240543
Iteration 53, loss = 0.02219489
Iteration 54, loss = 0.02204949
Iteration 55, loss = 0.02198882
Iteration 56, loss = 0.02165761
Iteration 57, loss = 0.02150204
Iteration 58, loss = 0.02152166
Iteration 59, loss = 0.02128229
Iteration 60, loss = 0.02116972
Iteration 61, loss = 0.02083911
Iteration 62, loss = 0.02081134
Iteration 63, loss = 0.02075665
Iteration 64, loss = 0.02058417
Iteration 65, loss = 0.02046536
Iteration 66, loss = 0.02038697
Iteration 67, loss = 0.02040281
Iteration 68, loss = 0.02028185
Iteration 69, loss = 0.02013699
Iteration 70, loss = 0.01999724
Iteration 71, loss = 0.01998868
Iteration 72, loss = 0.01980611
Iteration 73, loss = 0.01973357
Iteration 74, loss = 0.01964636
Iteration 75, loss = 0.01957992
Iteration 76, loss = 0.01963813
Iteration 77, loss = 0.01951758
Iteration 78, loss = 0.01947326
Iteration 79, loss = 0.01931358
Iteration 80, loss = 0.01923181
Iteration 81, loss = 0.01960098
Iteration 82, loss = 0.01921336
Iteration 83, loss = 0.01907979
Iteration 84, loss = 0.01899279
Iteration 85, loss = 0.01905935
Iteration 86, loss = 0.01900488
Iteration 87, loss = 0.01911794
Iteration 88, loss = 0.01885117
Iteration 89, loss = 0.01890862
Iteration 90, loss = 0.01887843
Iteration 91, loss = 0.01883738
Iteration 92, loss = 0.01877850
Iteration 93, loss = 0.01881407
Iteration 94, loss = 0.01864454
Iteration 95, loss = 0.01869948
Iteration 96, loss = 0.01868148
Iteration 97, loss = 0.01871527
Iteration 98, loss = 0.01847683
Iteration 99, loss = 0.01853101
Iteration 100, loss = 0.01866218
Iteration 101, loss = 0.01841775
Iteration 102, loss = 0.01859925
Iteration 103, loss = 0.01839661
Iteration 104, loss = 0.01853816
Iteration 105, loss = 0.01836592
Iteration 106, loss = 0.01841099
Iteration 107, loss = 0.01834711
Iteration 108, loss = 0.01846533
Iteration 109, loss = 0.01833705
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68900540
Iteration 2, loss = 0.63478700
Iteration 3, loss = 0.54790456
Iteration 4, loss = 0.44213209
Iteration 5, loss = 0.34543320
Iteration 6, loss = 0.26513590
Iteration 7, loss = 0.20543626
Iteration 8, loss = 0.16169919
Iteration 9, loss = 0.13061419
Iteration 10, loss = 0.10807977
Iteration 11, loss = 0.09161953
Iteration 12, loss = 0.07961017
Iteration 13, loss = 0.06992013
Iteration 14, loss = 0.06286222
Iteration 15, loss = 0.05673151
Iteration 16, loss = 0.05198870
Iteration 17, loss = 0.04817428
Iteration 18, loss = 0.04511963
Iteration 19, loss = 0.04259359
Iteration 20, loss = 0.04079154
Iteration 21, loss = 0.03862952
Iteration 22, loss = 0.03666274
Iteration 23, loss = 0.03525317
Iteration 24, loss = 0.03375886
Iteration 25, loss = 0.03319135
Iteration 26, loss = 0.03209255
Iteration 27, loss = 0.03158910
Iteration 28, loss = 0.03039020
Iteration 29, loss = 0.02987790
Iteration 30, loss = 0.02903141
Iteration 31, loss = 0.02850091
Iteration 32, loss = 0.02828723
Iteration 33, loss = 0.02794066
Iteration 34, loss = 0.02755189
Iteration 35, loss = 0.02704948
Iteration 36, loss = 0.02684196
Iteration 37, loss = 0.02640891
Iteration 38, loss = 0.02621430
Iteration 39, loss = 0.02586870
Iteration 40, loss = 0.02549339
Iteration 41, loss = 0.02541271
Iteration 42, loss = 0.02537697
Iteration 43, loss = 0.02526980
Iteration 44, loss = 0.02474519
Iteration 45, loss = 0.02461213
Iteration 46, loss = 0.02445640
Iteration 47, loss = 0.02424157
Iteration 48, loss = 0.02441874
Iteration 49, loss = 0.02472568
Iteration 50, loss = 0.02473201
Iteration 51, loss = 0.02433132
Iteration 52, loss = 0.02380871
Iteration 53, loss = 0.02369602
Iteration 54, loss = 0.02391124
Iteration 55, loss = 0.02355797
Iteration 56, loss = 0.02338776
Iteration 57, loss = 0.02308933
Iteration 58, loss = 0.02311247
Iteration 59, loss = 0.02295084
Iteration 60, loss = 0.02253720
Iteration 61, loss = 0.02250825
Iteration 62, loss = 0.02298735
Iteration 63, loss = 0.02283421
Iteration 64, loss = 0.02278194
Iteration 65, loss = 0.02256249
Iteration 66, loss = 0.02242257
Iteration 67, loss = 0.02214743
Iteration 68, loss = 0.02249421
Iteration 69, loss = 0.02229420
Iteration 70, loss = 0.02196371
Iteration 71, loss = 0.02193463
Iteration 72, loss = 0.02186972
Iteration 73, loss = 0.02183779
Iteration 74, loss = 0.02175455
Iteration 75, loss = 0.02199684
Iteration 76, loss = 0.02196649
Iteration 77, loss = 0.02176341
Iteration 78, loss = 0.02179125
Iteration 79, loss = 0.02174222
Iteration 80, loss = 0.02268132
Iteration 81, loss = 0.02270948
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71066958
Iteration 2, loss = 0.69280895
Iteration 3, loss = 0.67793208
Iteration 4, loss = 0.66362461
Iteration 5, loss = 0.64823688
Iteration 6, loss = 0.63155508
Iteration 7, loss = 0.61360979
Iteration 8, loss = 0.59365581
Iteration 9, loss = 0.57248906
Iteration 10, loss = 0.54971370
Iteration 11, loss = 0.52577173
Iteration 12, loss = 0.50086741
Iteration 13, loss = 0.47540552
Iteration 14, loss = 0.44974768
Iteration 15, loss = 0.42457886
Iteration 16, loss = 0.39976134
Iteration 17, loss = 0.37551194
Iteration 18, loss = 0.35231378
Iteration 19, loss = 0.32980672
Iteration 20, loss = 0.30834148
Iteration 21, loss = 0.28813064
Iteration 22, loss = 0.26898493
Iteration 23, loss = 0.25107430
Iteration 24, loss = 0.23432670
Iteration 25, loss = 0.21878980
Iteration 26, loss = 0.20434217
Iteration 27, loss = 0.19099666
Iteration 28, loss = 0.17860976
Iteration 29, loss = 0.16730597
Iteration 30, loss = 0.15668662
Iteration 31, loss = 0.14705556
Iteration 32, loss = 0.13812326
Iteration 33, loss = 0.12990410
Iteration 34, loss = 0.12233611
Iteration 35, loss = 0.11536091
Iteration 36, loss = 0.10891731
Iteration 37, loss = 0.10305602
Iteration 38, loss = 0.09751258
Iteration 39, loss = 0.09248071
Iteration 40, loss = 0.08778969
Iteration 41, loss = 0.08348624
Iteration 42, loss = 0.07947136
Iteration 43, loss = 0.07569383
Iteration 44, loss = 0.07224510
Iteration 45, loss = 0.06903429
Iteration 46, loss = 0.06602770
Iteration 47, loss = 0.06325549
Iteration 48, loss = 0.06064480
Iteration 49, loss = 0.05822839
Iteration 50, loss = 0.05600015
Iteration 51, loss = 0.05382381
Iteration 52, loss = 0.05181760
Iteration 53, loss = 0.04995653
Iteration 54, loss = 0.04820517
Iteration 55, loss = 0.04651181
Iteration 56, loss = 0.04497345
Iteration 57, loss = 0.04349423
Iteration 58, loss = 0.04211393
Iteration 59, loss = 0.04078437
Iteration 60, loss = 0.03955016
Iteration 61, loss = 0.03836830
Iteration 62, loss = 0.03727228
Iteration 63, loss = 0.03620850
Iteration 64, loss = 0.03519099
Iteration 65, loss = 0.03424909
Iteration 66, loss = 0.03334416
Iteration 67, loss = 0.03248457
Iteration 68, loss = 0.03164341
Iteration 69, loss = 0.03087687
Iteration 70, loss = 0.03012491
Iteration 71, loss = 0.02944295
Iteration 72, loss = 0.02877107
Iteration 73, loss = 0.02807710
Iteration 74, loss = 0.02747809
Iteration 75, loss = 0.02689270
Iteration 76, loss = 0.02635333
Iteration 77, loss = 0.02582170
Iteration 78, loss = 0.02535227
Iteration 79, loss = 0.02476045
Iteration 80, loss = 0.02428131
Iteration 81, loss = 0.02383325
Iteration 82, loss = 0.02341591
Iteration 83, loss = 0.02300816
Iteration 84, loss = 0.02253541
Iteration 85, loss = 0.02211509
Iteration 86, loss = 0.02176186
Iteration 87, loss = 0.02137989
Iteration 88, loss = 0.02103819
Iteration 89, loss = 0.02071305
Iteration 90, loss = 0.02038348
Iteration 91, loss = 0.02007364
Iteration 92, loss = 0.01980464
Iteration 93, loss = 0.01948170
Iteration 94, loss = 0.01920263
Iteration 95, loss = 0.01894544
Iteration 96, loss = 0.01869024
Iteration 97, loss = 0.01843387
Iteration 98, loss = 0.01819791
Iteration 99, loss = 0.01795427
Iteration 100, loss = 0.01771788
Iteration 101, loss = 0.01747818
Iteration 102, loss = 0.01729628
Iteration 103, loss = 0.01702593
Iteration 104, loss = 0.01682947
Iteration 105, loss = 0.01661710
Iteration 106, loss = 0.01643052
Iteration 107, loss = 0.01625366
Iteration 108, loss = 0.01602033
Iteration 109, loss = 0.01583152
Iteration 110, loss = 0.01569294
Iteration 111, loss = 0.01548660
Iteration 112, loss = 0.01537552
Iteration 113, loss = 0.01518865
Iteration 114, loss = 0.01508773
Iteration 115, loss = 0.01487796
Iteration 116, loss = 0.01478148
Iteration 117, loss = 0.01457559
Iteration 118, loss = 0.01445233
Iteration 119, loss = 0.01432417
Iteration 120, loss = 0.01418360
Iteration 121, loss = 0.01407406
Iteration 122, loss = 0.01394882
Iteration 123, loss = 0.01384252
Iteration 124, loss = 0.01372634
Iteration 125, loss = 0.01361793
Iteration 126, loss = 0.01348539
Iteration 127, loss = 0.01343255
Iteration 128, loss = 0.01327482
Iteration 129, loss = 0.01315356
Iteration 130, loss = 0.01304005
Iteration 131, loss = 0.01293755
Iteration 132, loss = 0.01283872
Iteration 133, loss = 0.01277044
Iteration 134, loss = 0.01265071
Iteration 135, loss = 0.01257423
Iteration 136, loss = 0.01245813
Iteration 137, loss = 0.01241049
Iteration 138, loss = 0.01231498
Iteration 139, loss = 0.01220616
Iteration 140, loss = 0.01217550
Iteration 141, loss = 0.01204914
Iteration 142, loss = 0.01205436
Iteration 143, loss = 0.01191090
Iteration 144, loss = 0.01183270
Iteration 145, loss = 0.01178508
Iteration 146, loss = 0.01172880
Iteration 147, loss = 0.01163674
Iteration 148, loss = 0.01156019
Iteration 149, loss = 0.01152268
Iteration 150, loss = 0.01142506
Iteration 151, loss = 0.01136469
Iteration 152, loss = 0.01130934
Iteration 153, loss = 0.01128520
Iteration 154, loss = 0.01117762
Iteration 155, loss = 0.01112117
Iteration 156, loss = 0.01107988
Iteration 157, loss = 0.01101618
Iteration 158, loss = 0.01096504
Iteration 159, loss = 0.01089580
Iteration 160, loss = 0.01083958
Iteration 161, loss = 0.01081922
Iteration 162, loss = 0.01074455
Iteration 163, loss = 0.01070846
Iteration 164, loss = 0.01066904
Iteration 165, loss = 0.01063556
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68134263
Iteration 2, loss = 0.61496804
Iteration 3, loss = 0.51146277
Iteration 4, loss = 0.39566436
Iteration 5, loss = 0.29211371
Iteration 6, loss = 0.21417480
Iteration 7, loss = 0.16049566
Iteration 8, loss = 0.12428578
Iteration 9, loss = 0.09967792
Iteration 10, loss = 0.08275651
Iteration 11, loss = 0.07046838
Iteration 12, loss = 0.06142305
Iteration 13, loss = 0.05438715
Iteration 14, loss = 0.04910212
Iteration 15, loss = 0.04473097
Iteration 16, loss = 0.04142103
Iteration 17, loss = 0.03855631
Iteration 18, loss = 0.03640709
Iteration 19, loss = 0.03448024
Iteration 20, loss = 0.03285733
Iteration 21, loss = 0.03148785
Iteration 22, loss = 0.03034194
Iteration 23, loss = 0.02934722
Iteration 24, loss = 0.02832008
Iteration 25, loss = 0.02766128
Iteration 26, loss = 0.02685755
Iteration 27, loss = 0.02648322
Iteration 28, loss = 0.02614346
Iteration 29, loss = 0.02542989
Iteration 30, loss = 0.02494697
Iteration 31, loss = 0.02457840
Iteration 32, loss = 0.02436233
Iteration 33, loss = 0.02395478
Iteration 34, loss = 0.02359962
Iteration 35, loss = 0.02321073
Iteration 36, loss = 0.02321315
Iteration 37, loss = 0.02288885
Iteration 38, loss = 0.02263937
Iteration 39, loss = 0.02259299
Iteration 40, loss = 0.02236401
Iteration 41, loss = 0.02225987
Iteration 42, loss = 0.02207283
Iteration 43, loss = 0.02194123
Iteration 44, loss = 0.02220069
Iteration 45, loss = 0.02155928
Iteration 46, loss = 0.02146743
Iteration 47, loss = 0.02164056
Iteration 48, loss = 0.02143074
Iteration 49, loss = 0.02126004
Iteration 50, loss = 0.02145423
Iteration 51, loss = 0.02139320
Iteration 52, loss = 0.02141171
Iteration 53, loss = 0.02100361
Iteration 54, loss = 0.02093942
Iteration 55, loss = 0.02099644
Iteration 56, loss = 0.02080333
Iteration 57, loss = 0.02094593
Iteration 58, loss = 0.02069266
Iteration 59, loss = 0.02064287
Iteration 60, loss = 0.02084680
Iteration 61, loss = 0.02062517
Iteration 62, loss = 0.02063663
Iteration 63, loss = 0.02051546
Iteration 64, loss = 0.02067182
Iteration 65, loss = 0.02045651
Iteration 66, loss = 0.02042034
Iteration 67, loss = 0.02051760
Iteration 68, loss = 0.02058945
Iteration 69, loss = 0.02033777
Iteration 70, loss = 0.02020539
Iteration 71, loss = 0.02029938
Iteration 72, loss = 0.02039891
Iteration 73, loss = 0.02011663
Iteration 74, loss = 0.02011475
Iteration 75, loss = 0.02001308
Iteration 76, loss = 0.02006477
Iteration 77, loss = 0.02023980
Iteration 78, loss = 0.02026393
Iteration 79, loss = 0.02002016
Iteration 80, loss = 0.02019522
Iteration 81, loss = 0.02026172
Iteration 82, loss = 0.02008516
Iteration 83, loss = 0.02022474
Iteration 84, loss = 0.02022529
Iteration 85, loss = 0.01993271
Iteration 86, loss = 0.01992061
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69392386
Iteration 2, loss = 0.66362720
Iteration 3, loss = 0.62863803
Iteration 4, loss = 0.58710388
Iteration 5, loss = 0.53969863
Iteration 6, loss = 0.48844145
Iteration 7, loss = 0.43600576
Iteration 8, loss = 0.38547312
Iteration 9, loss = 0.33864379
Iteration 10, loss = 0.29614843
Iteration 11, loss = 0.25870625
Iteration 12, loss = 0.22617183
Iteration 13, loss = 0.19810993
Iteration 14, loss = 0.17404038
Iteration 15, loss = 0.15384553
Iteration 16, loss = 0.13647956
Iteration 17, loss = 0.12192148
Iteration 18, loss = 0.10946727
Iteration 19, loss = 0.09897665
Iteration 20, loss = 0.08986432
Iteration 21, loss = 0.08213357
Iteration 22, loss = 0.07542639
Iteration 23, loss = 0.06965408
Iteration 24, loss = 0.06467614
Iteration 25, loss = 0.06017422
Iteration 26, loss = 0.05617860
Iteration 27, loss = 0.05265231
Iteration 28, loss = 0.04978514
Iteration 29, loss = 0.04695277
Iteration 30, loss = 0.04449873
Iteration 31, loss = 0.04229159
Iteration 32, loss = 0.04025548
Iteration 33, loss = 0.03852836
Iteration 34, loss = 0.03678397
Iteration 35, loss = 0.03527798
Iteration 36, loss = 0.03384808
Iteration 37, loss = 0.03258571
Iteration 38, loss = 0.03138130
Iteration 39, loss = 0.03030022
Iteration 40, loss = 0.02921092
Iteration 41, loss = 0.02824967
Iteration 42, loss = 0.02730638
Iteration 43, loss = 0.02668566
Iteration 44, loss = 0.02597339
Iteration 45, loss = 0.02524310
Iteration 46, loss = 0.02450613
Iteration 47, loss = 0.02391862
Iteration 48, loss = 0.02331800
Iteration 49, loss = 0.02273039
Iteration 50, loss = 0.02224382
Iteration 51, loss = 0.02174793
Iteration 52, loss = 0.02138622
Iteration 53, loss = 0.02094669
Iteration 54, loss = 0.02054847
Iteration 55, loss = 0.02012787
Iteration 56, loss = 0.01978228
Iteration 57, loss = 0.01952284
Iteration 58, loss = 0.01921152
Iteration 59, loss = 0.01909289
Iteration 60, loss = 0.01871333
Iteration 61, loss = 0.01863909
Iteration 62, loss = 0.01838073
Iteration 63, loss = 0.01816311
Iteration 64, loss = 0.01777144
Iteration 65, loss = 0.01742090
Iteration 66, loss = 0.01715005
Iteration 67, loss = 0.01688233
Iteration 68, loss = 0.01674321
Iteration 69, loss = 0.01657359
Iteration 70, loss = 0.01659939
Iteration 71, loss = 0.01653167
Iteration 72, loss = 0.01648919
Iteration 73, loss = 0.01615068
Iteration 74, loss = 0.01599925
Iteration 75, loss = 0.01599507
Iteration 76, loss = 0.01577647
Iteration 77, loss = 0.01552703
Iteration 78, loss = 0.01526339
Iteration 79, loss = 0.01507523
Iteration 80, loss = 0.01500614
Iteration 81, loss = 0.01493523
Iteration 82, loss = 0.01490110
Iteration 83, loss = 0.01483904
Iteration 84, loss = 0.01468701
Iteration 85, loss = 0.01447945
Iteration 86, loss = 0.01452794
Iteration 87, loss = 0.01436413
Iteration 88, loss = 0.01423910
Iteration 89, loss = 0.01417694
Iteration 90, loss = 0.01414438
Iteration 91, loss = 0.01400298
Iteration 92, loss = 0.01385658
Iteration 93, loss = 0.01371187
Iteration 94, loss = 0.01365513
Iteration 95, loss = 0.01354442
Iteration 96, loss = 0.01344527
Iteration 97, loss = 0.01338110
Iteration 98, loss = 0.01330490
Iteration 99, loss = 0.01328311
Iteration 100, loss = 0.01320730
Iteration 101, loss = 0.01310244
Iteration 102, loss = 0.01301882
Iteration 103, loss = 0.01297513
Iteration 104, loss = 0.01284250
Iteration 105, loss = 0.01300410
Iteration 106, loss = 0.01326111
Iteration 107, loss = 0.01308181
Iteration 108, loss = 0.01294823
Iteration 109, loss = 0.01301260
Iteration 110, loss = 0.01316169
Iteration 111, loss = 0.01319323
Iteration 112, loss = 0.01307251
Iteration 113, loss = 0.01290585
Iteration 114, loss = 0.01293794
Iteration 115, loss = 0.01276652
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68459612
Iteration 2, loss = 0.62118938
Iteration 3, loss = 0.52888505
Iteration 4, loss = 0.42001735
Iteration 5, loss = 0.32047807
Iteration 6, loss = 0.24152274
Iteration 7, loss = 0.18448928
Iteration 8, loss = 0.14486959
Iteration 9, loss = 0.11699073
Iteration 10, loss = 0.09748495
Iteration 11, loss = 0.08329121
Iteration 12, loss = 0.07276736
Iteration 13, loss = 0.06491051
Iteration 14, loss = 0.05851190
Iteration 15, loss = 0.05352594
Iteration 16, loss = 0.04963962
Iteration 17, loss = 0.04631748
Iteration 18, loss = 0.04350757
Iteration 19, loss = 0.04138031
Iteration 20, loss = 0.03951045
Iteration 21, loss = 0.03784045
Iteration 22, loss = 0.03641740
Iteration 23, loss = 0.03527052
Iteration 24, loss = 0.03418637
Iteration 25, loss = 0.03329213
Iteration 26, loss = 0.03271245
Iteration 27, loss = 0.03187437
Iteration 28, loss = 0.03089992
Iteration 29, loss = 0.03067526
Iteration 30, loss = 0.03013411
Iteration 31, loss = 0.02949561
Iteration 32, loss = 0.02906981
Iteration 33, loss = 0.02879250
Iteration 34, loss = 0.02873039
Iteration 35, loss = 0.02840417
Iteration 36, loss = 0.02798098
Iteration 37, loss = 0.02752732
Iteration 38, loss = 0.02757891
Iteration 39, loss = 0.02726626
Iteration 40, loss = 0.02687498
Iteration 41, loss = 0.02660094
Iteration 42, loss = 0.02671245
Iteration 43, loss = 0.02650473
Iteration 44, loss = 0.02644517
Iteration 45, loss = 0.02642042
Iteration 46, loss = 0.02617233
Iteration 47, loss = 0.02591124
Iteration 48, loss = 0.02593029
Iteration 49, loss = 0.02573685
Iteration 50, loss = 0.02586216
Iteration 51, loss = 0.02538204
Iteration 52, loss = 0.02553289
Iteration 53, loss = 0.02535560
Iteration 54, loss = 0.02545125
Iteration 55, loss = 0.02537488
Iteration 56, loss = 0.02532236
Iteration 57, loss = 0.02474971
Iteration 58, loss = 0.02506936
Iteration 59, loss = 0.02495242
Iteration 60, loss = 0.02498467
Iteration 61, loss = 0.02458817
Iteration 62, loss = 0.02457954
Iteration 63, loss = 0.02465672
Iteration 64, loss = 0.02458530
Iteration 65, loss = 0.02469510
Iteration 66, loss = 0.02444979
Iteration 67, loss = 0.02489600
Iteration 68, loss = 0.02478169
Iteration 69, loss = 0.02435203
Iteration 70, loss = 0.02447695
Iteration 71, loss = 0.02431991
Iteration 72, loss = 0.02436870
Iteration 73, loss = 0.02436261
Iteration 74, loss = 0.02443610
Iteration 75, loss = 0.02421659
Iteration 76, loss = 0.02440128
Iteration 77, loss = 0.02424249
Iteration 78, loss = 0.02425273
Iteration 79, loss = 0.02406254
Iteration 80, loss = 0.02437070
Iteration 81, loss = 0.02387725
Iteration 82, loss = 0.02392623
Iteration 83, loss = 0.02391555
Iteration 84, loss = 0.02388525
Iteration 85, loss = 0.02385431
Iteration 86, loss = 0.02390713
Iteration 87, loss = 0.02427667
Iteration 88, loss = 0.02444117
Iteration 89, loss = 0.02420502
Iteration 90, loss = 0.02401538
Iteration 91, loss = 0.02388769
Iteration 92, loss = 0.02384332
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69414101
Iteration 2, loss = 0.67604812
Iteration 3, loss = 0.66087260
Iteration 4, loss = 0.64565185
Iteration 5, loss = 0.62980504
Iteration 6, loss = 0.61258213
Iteration 7, loss = 0.59413917
Iteration 8, loss = 0.57412405
Iteration 9, loss = 0.55264159
Iteration 10, loss = 0.52995023
Iteration 11, loss = 0.50598254
Iteration 12, loss = 0.48142340
Iteration 13, loss = 0.45622471
Iteration 14, loss = 0.43111803
Iteration 15, loss = 0.40637054
Iteration 16, loss = 0.38199461
Iteration 17, loss = 0.35841962
Iteration 18, loss = 0.33577138
Iteration 19, loss = 0.31417337
Iteration 20, loss = 0.29375977
Iteration 21, loss = 0.27444168
Iteration 22, loss = 0.25633259
Iteration 23, loss = 0.23929513
Iteration 24, loss = 0.22349068
Iteration 25, loss = 0.20879039
Iteration 26, loss = 0.19516262
Iteration 27, loss = 0.18240095
Iteration 28, loss = 0.17075879
Iteration 29, loss = 0.15991270
Iteration 30, loss = 0.14994483
Iteration 31, loss = 0.14075021
Iteration 32, loss = 0.13227235
Iteration 33, loss = 0.12444949
Iteration 34, loss = 0.11726947
Iteration 35, loss = 0.11059939
Iteration 36, loss = 0.10450905
Iteration 37, loss = 0.09881341
Iteration 38, loss = 0.09364483
Iteration 39, loss = 0.08882363
Iteration 40, loss = 0.08438646
Iteration 41, loss = 0.08023219
Iteration 42, loss = 0.07644500
Iteration 43, loss = 0.07289283
Iteration 44, loss = 0.06961013
Iteration 45, loss = 0.06659211
Iteration 46, loss = 0.06376652
Iteration 47, loss = 0.06110273
Iteration 48, loss = 0.05859068
Iteration 49, loss = 0.05628498
Iteration 50, loss = 0.05406887
Iteration 51, loss = 0.05204563
Iteration 52, loss = 0.05012318
Iteration 53, loss = 0.04831370
Iteration 54, loss = 0.04666595
Iteration 55, loss = 0.04505770
Iteration 56, loss = 0.04354450
Iteration 57, loss = 0.04216580
Iteration 58, loss = 0.04082025
Iteration 59, loss = 0.03955909
Iteration 60, loss = 0.03836727
Iteration 61, loss = 0.03728989
Iteration 62, loss = 0.03622592
Iteration 63, loss = 0.03516294
Iteration 64, loss = 0.03422844
Iteration 65, loss = 0.03332597
Iteration 66, loss = 0.03242707
Iteration 67, loss = 0.03164145
Iteration 68, loss = 0.03081988
Iteration 69, loss = 0.03006396
Iteration 70, loss = 0.02939378
Iteration 71, loss = 0.02867702
Iteration 72, loss = 0.02803479
Iteration 73, loss = 0.02740240
Iteration 74, loss = 0.02682459
Iteration 75, loss = 0.02630843
Iteration 76, loss = 0.02568956
Iteration 77, loss = 0.02517506
Iteration 78, loss = 0.02467462
Iteration 79, loss = 0.02421356
Iteration 80, loss = 0.02375268
Iteration 81, loss = 0.02331337
Iteration 82, loss = 0.02286287
Iteration 83, loss = 0.02245338
Iteration 84, loss = 0.02204611
Iteration 85, loss = 0.02169141
Iteration 86, loss = 0.02137400
Iteration 87, loss = 0.02102761
Iteration 88, loss = 0.02065810
Iteration 89, loss = 0.02031016
Iteration 90, loss = 0.01997818
Iteration 91, loss = 0.01971436
Iteration 92, loss = 0.01936461
Iteration 93, loss = 0.01907513
Iteration 94, loss = 0.01881936
Iteration 95, loss = 0.01853666
Iteration 96, loss = 0.01829021
Iteration 97, loss = 0.01800926
Iteration 98, loss = 0.01780444
Iteration 99, loss = 0.01753867
Iteration 100, loss = 0.01735027
Iteration 101, loss = 0.01712978
Iteration 102, loss = 0.01687986
Iteration 103, loss = 0.01665228
Iteration 104, loss = 0.01645345
Iteration 105, loss = 0.01624482
Iteration 106, loss = 0.01609295
Iteration 107, loss = 0.01592221
Iteration 108, loss = 0.01574940
Iteration 109, loss = 0.01558888
Iteration 110, loss = 0.01545549
Iteration 111, loss = 0.01526171
Iteration 112, loss = 0.01513608
Iteration 113, loss = 0.01497946
Iteration 114, loss = 0.01482893
Iteration 115, loss = 0.01469197
Iteration 116, loss = 0.01455235
Iteration 117, loss = 0.01442561
Iteration 118, loss = 0.01428289
Iteration 119, loss = 0.01412929
Iteration 120, loss = 0.01401613
Iteration 121, loss = 0.01385627
Iteration 122, loss = 0.01373067
Iteration 123, loss = 0.01360621
Iteration 124, loss = 0.01348195
Iteration 125, loss = 0.01336990
Iteration 126, loss = 0.01327670
Iteration 127, loss = 0.01316527
Iteration 128, loss = 0.01306037
Iteration 129, loss = 0.01293918
Iteration 130, loss = 0.01283555
Iteration 131, loss = 0.01275492
Iteration 132, loss = 0.01266097
Iteration 133, loss = 0.01257447
Iteration 134, loss = 0.01247546
Iteration 135, loss = 0.01239504
Iteration 136, loss = 0.01229153
Iteration 137, loss = 0.01220959
Iteration 138, loss = 0.01213429
Iteration 139, loss = 0.01204837
Iteration 140, loss = 0.01197975
Iteration 141, loss = 0.01192217
Iteration 142, loss = 0.01184820
Iteration 143, loss = 0.01180540
Iteration 144, loss = 0.01173517
Iteration 145, loss = 0.01165407
Iteration 146, loss = 0.01160033
Iteration 147, loss = 0.01149458
Iteration 148, loss = 0.01139843
Iteration 149, loss = 0.01132877
Iteration 150, loss = 0.01130382
Iteration 151, loss = 0.01122342
Iteration 152, loss = 0.01115440
Iteration 153, loss = 0.01117268
Iteration 154, loss = 0.01107502
Iteration 155, loss = 0.01099697
Iteration 156, loss = 0.01090038
Iteration 157, loss = 0.01084529
Iteration 158, loss = 0.01079405
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68077491
Iteration 2, loss = 0.64470923
Iteration 3, loss = 0.60061252
Iteration 4, loss = 0.54699213
Iteration 5, loss = 0.48810212
Iteration 6, loss = 0.42862035
Iteration 7, loss = 0.37179876
Iteration 8, loss = 0.31966598
Iteration 9, loss = 0.27342789
Iteration 10, loss = 0.23314917
Iteration 11, loss = 0.19938567
Iteration 12, loss = 0.17113364
Iteration 13, loss = 0.14805022
Iteration 14, loss = 0.12899047
Iteration 15, loss = 0.11338579
Iteration 16, loss = 0.10054462
Iteration 17, loss = 0.08981363
Iteration 18, loss = 0.08091314
Iteration 19, loss = 0.07347237
Iteration 20, loss = 0.06716832
Iteration 21, loss = 0.06164229
Iteration 22, loss = 0.05707569
Iteration 23, loss = 0.05325627
Iteration 24, loss = 0.04971823
Iteration 25, loss = 0.04661393
Iteration 26, loss = 0.04411502
Iteration 27, loss = 0.04170813
Iteration 28, loss = 0.03960490
Iteration 29, loss = 0.03768723
Iteration 30, loss = 0.03598054
Iteration 31, loss = 0.03463446
Iteration 32, loss = 0.03331620
Iteration 33, loss = 0.03198534
Iteration 34, loss = 0.03083424
Iteration 35, loss = 0.02985382
Iteration 36, loss = 0.02897887
Iteration 37, loss = 0.02810477
Iteration 38, loss = 0.02725978
Iteration 39, loss = 0.02659973
Iteration 40, loss = 0.02594746
Iteration 41, loss = 0.02543179
Iteration 42, loss = 0.02480028
Iteration 43, loss = 0.02421493
Iteration 44, loss = 0.02390114
Iteration 45, loss = 0.02329725
Iteration 46, loss = 0.02288705
Iteration 47, loss = 0.02244300
Iteration 48, loss = 0.02210417
Iteration 49, loss = 0.02168959
Iteration 50, loss = 0.02134500
Iteration 51, loss = 0.02118644
Iteration 52, loss = 0.02068590
Iteration 53, loss = 0.02052401
Iteration 54, loss = 0.02025062
Iteration 55, loss = 0.01993850
Iteration 56, loss = 0.01987883
Iteration 57, loss = 0.01955487
Iteration 58, loss = 0.01933880
Iteration 59, loss = 0.01922548
Iteration 60, loss = 0.01908479
Iteration 61, loss = 0.01894663
Iteration 62, loss = 0.01859629
Iteration 63, loss = 0.01857447
Iteration 64, loss = 0.01830218
Iteration 65, loss = 0.01817383
Iteration 66, loss = 0.01808652
Iteration 67, loss = 0.01805939
Iteration 68, loss = 0.01785638
Iteration 69, loss = 0.01766971
Iteration 70, loss = 0.01765768
Iteration 71, loss = 0.01752982
Iteration 72, loss = 0.01738522
Iteration 73, loss = 0.01725788
Iteration 74, loss = 0.01722477
Iteration 75, loss = 0.01701101
Iteration 76, loss = 0.01717406
Iteration 77, loss = 0.01718522
Iteration 78, loss = 0.01687327
Iteration 79, loss = 0.01684906
Iteration 80, loss = 0.01666382
Iteration 81, loss = 0.01659356
Iteration 82, loss = 0.01660798
Iteration 83, loss = 0.01658286
Iteration 84, loss = 0.01646499
Iteration 85, loss = 0.01643743
Iteration 86, loss = 0.01631207
Iteration 87, loss = 0.01625098
Iteration 88, loss = 0.01619285
Iteration 89, loss = 0.01629673
Iteration 90, loss = 0.01604956
Iteration 91, loss = 0.01607636
Iteration 92, loss = 0.01614592
Iteration 93, loss = 0.01592032
Iteration 94, loss = 0.01587988
Iteration 95, loss = 0.01587993
Iteration 96, loss = 0.01587033
Iteration 97, loss = 0.01584822
Iteration 98, loss = 0.01575839
Iteration 99, loss = 0.01569025
Iteration 100, loss = 0.01576110
Iteration 101, loss = 0.01584422
Iteration 102, loss = 0.01579515
Iteration 103, loss = 0.01561482
Iteration 104, loss = 0.01557444
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68333710
Iteration 2, loss = 0.62192672
Iteration 3, loss = 0.52862404
Iteration 4, loss = 0.42290656
Iteration 5, loss = 0.32887166
Iteration 6, loss = 0.25349578
Iteration 7, loss = 0.19664877
Iteration 8, loss = 0.15589498
Iteration 9, loss = 0.12634360
Iteration 10, loss = 0.10538389
Iteration 11, loss = 0.08989992
Iteration 12, loss = 0.07817990
Iteration 13, loss = 0.06950684
Iteration 14, loss = 0.06254589
Iteration 15, loss = 0.05686934
Iteration 16, loss = 0.05255104
Iteration 17, loss = 0.04887679
Iteration 18, loss = 0.04582173
Iteration 19, loss = 0.04332566
Iteration 20, loss = 0.04100024
Iteration 21, loss = 0.03914892
Iteration 22, loss = 0.03756228
Iteration 23, loss = 0.03610069
Iteration 24, loss = 0.03503818
Iteration 25, loss = 0.03457726
Iteration 26, loss = 0.03359210
Iteration 27, loss = 0.03255575
Iteration 28, loss = 0.03183811
Iteration 29, loss = 0.03092268
Iteration 30, loss = 0.02997908
Iteration 31, loss = 0.02950949
Iteration 32, loss = 0.02915177
Iteration 33, loss = 0.02844636
Iteration 34, loss = 0.02823735
Iteration 35, loss = 0.02788922
Iteration 36, loss = 0.02736228
Iteration 37, loss = 0.02690052
Iteration 38, loss = 0.02662044
Iteration 39, loss = 0.02636171
Iteration 40, loss = 0.02631475
Iteration 41, loss = 0.02582504
Iteration 42, loss = 0.02558854
Iteration 43, loss = 0.02539601
Iteration 44, loss = 0.02585176
Iteration 45, loss = 0.02600556
Iteration 46, loss = 0.02569548
Iteration 47, loss = 0.02522661
Iteration 48, loss = 0.02492667
Iteration 49, loss = 0.02504939
Iteration 50, loss = 0.02467740
Iteration 51, loss = 0.02450007
Iteration 52, loss = 0.02428454
Iteration 53, loss = 0.02391406
Iteration 54, loss = 0.02400036
Iteration 55, loss = 0.02401573
Iteration 56, loss = 0.02354029
Iteration 57, loss = 0.02364049
Iteration 58, loss = 0.02354870
Iteration 59, loss = 0.02343292
Iteration 60, loss = 0.02349215
Iteration 61, loss = 0.02348767
Iteration 62, loss = 0.02324982
Iteration 63, loss = 0.02324129
Iteration 64, loss = 0.02302102
Iteration 65, loss = 0.02310715
Iteration 66, loss = 0.02295418
Iteration 67, loss = 0.02295610
Iteration 68, loss = 0.02277252
Iteration 69, loss = 0.02285442
Iteration 70, loss = 0.02274754
Iteration 71, loss = 0.02281332
Iteration 72, loss = 0.02274183
Iteration 73, loss = 0.02280929
Iteration 74, loss = 0.02280818
Iteration 75, loss = 0.02263950
Iteration 76, loss = 0.02295312
Iteration 77, loss = 0.02366430
Iteration 78, loss = 0.02325256
Iteration 79, loss = 0.02282440
Iteration 80, loss = 0.02266275
Iteration 81, loss = 0.02244996
Iteration 82, loss = 0.02289189
Iteration 83, loss = 0.02294767
Iteration 84, loss = 0.02317535
Iteration 85, loss = 0.02366828
Iteration 86, loss = 0.02335610
Iteration 87, loss = 0.02301378
Iteration 88, loss = 0.02296547
Iteration 89, loss = 0.02303869
Iteration 90, loss = 0.02274381
Iteration 91, loss = 0.02234065
Iteration 92, loss = 0.02244444
Iteration 93, loss = 0.02218490
Iteration 94, loss = 0.02225283
Iteration 95, loss = 0.02222264
Iteration 96, loss = 0.02250575
Iteration 97, loss = 0.02242335
Iteration 98, loss = 0.02226986
Iteration 99, loss = 0.02230705
Iteration 100, loss = 0.02222177
Iteration 101, loss = 0.02191004
Iteration 102, loss = 0.02197350
Iteration 103, loss = 0.02221866
Iteration 104, loss = 0.02241756
Iteration 105, loss = 0.02248875
Iteration 106, loss = 0.02221188
Iteration 107, loss = 0.02222249
Iteration 108, loss = 0.02224280
Iteration 109, loss = 0.02224247
Iteration 110, loss = 0.02162421
Iteration 111, loss = 0.02188007
Iteration 112, loss = 0.02172701
Iteration 113, loss = 0.02178193
Iteration 114, loss = 0.02173880
Iteration 115, loss = 0.02242231
Iteration 116, loss = 0.02267228
Iteration 117, loss = 0.02217956
Iteration 118, loss = 0.02293695
Iteration 119, loss = 0.02354435
Iteration 120, loss = 0.02350808
Iteration 121, loss = 0.02295692
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69767745
Iteration 2, loss = 0.66268820
Iteration 3, loss = 0.62592866
Iteration 4, loss = 0.58257463
Iteration 5, loss = 0.53255714
Iteration 6, loss = 0.47882374
Iteration 7, loss = 0.42590811
Iteration 8, loss = 0.37555158
Iteration 9, loss = 0.32952226
Iteration 10, loss = 0.28859146
Iteration 11, loss = 0.25249302
Iteration 12, loss = 0.22108031
Iteration 13, loss = 0.19401356
Iteration 14, loss = 0.17092407
Iteration 15, loss = 0.15118649
Iteration 16, loss = 0.13437021
Iteration 17, loss = 0.12039146
Iteration 18, loss = 0.10824454
Iteration 19, loss = 0.09784783
Iteration 20, loss = 0.08890862
Iteration 21, loss = 0.08119019
Iteration 22, loss = 0.07459371
Iteration 23, loss = 0.06875357
Iteration 24, loss = 0.06366609
Iteration 25, loss = 0.05913663
Iteration 26, loss = 0.05525413
Iteration 27, loss = 0.05176439
Iteration 28, loss = 0.04868318
Iteration 29, loss = 0.04599874
Iteration 30, loss = 0.04342562
Iteration 31, loss = 0.04143610
Iteration 32, loss = 0.03937416
Iteration 33, loss = 0.03746398
Iteration 34, loss = 0.03576068
Iteration 35, loss = 0.03411196
Iteration 36, loss = 0.03263295
Iteration 37, loss = 0.03120825
Iteration 38, loss = 0.03007331
Iteration 39, loss = 0.02904184
Iteration 40, loss = 0.02792371
Iteration 41, loss = 0.02708389
Iteration 42, loss = 0.02613171
Iteration 43, loss = 0.02535676
Iteration 44, loss = 0.02462756
Iteration 45, loss = 0.02391540
Iteration 46, loss = 0.02321529
Iteration 47, loss = 0.02260237
Iteration 48, loss = 0.02207265
Iteration 49, loss = 0.02162370
Iteration 50, loss = 0.02120447
Iteration 51, loss = 0.02067449
Iteration 52, loss = 0.02025450
Iteration 53, loss = 0.01982944
Iteration 54, loss = 0.01956702
Iteration 55, loss = 0.01915302
Iteration 56, loss = 0.01883564
Iteration 57, loss = 0.01858696
Iteration 58, loss = 0.01824602
Iteration 59, loss = 0.01785321
Iteration 60, loss = 0.01752998
Iteration 61, loss = 0.01724145
Iteration 62, loss = 0.01660355
Iteration 63, loss = 0.01673956
Iteration 64, loss = 0.01660133
Iteration 65, loss = 0.01637782
Iteration 66, loss = 0.01619818
Iteration 67, loss = 0.01594563
Iteration 68, loss = 0.01569245
Iteration 69, loss = 0.01542514
Iteration 70, loss = 0.01517755
Iteration 71, loss = 0.01502028
Iteration 72, loss = 0.01486306
Iteration 73, loss = 0.01483470
Iteration 74, loss = 0.01472908
Iteration 75, loss = 0.01455549
Iteration 76, loss = 0.01438213
Iteration 77, loss = 0.01416140
Iteration 78, loss = 0.01417093
Iteration 79, loss = 0.01400022
Iteration 80, loss = 0.01385859
Iteration 81, loss = 0.01372914
Iteration 82, loss = 0.01357193
Iteration 83, loss = 0.01347224
Iteration 84, loss = 0.01334658
Iteration 85, loss = 0.01326823
Iteration 86, loss = 0.01317071
Iteration 87, loss = 0.01315383
Iteration 88, loss = 0.01306600
Iteration 89, loss = 0.01297362
Iteration 90, loss = 0.01284368
Iteration 91, loss = 0.01292598
Iteration 92, loss = 0.01285811
Iteration 93, loss = 0.01272822
Iteration 94, loss = 0.01270704
Iteration 95, loss = 0.01248504
Iteration 96, loss = 0.01222782
Iteration 97, loss = 0.01258464
Iteration 98, loss = 0.01262802
Iteration 99, loss = 0.01230495
Iteration 100, loss = 0.01226008
Iteration 101, loss = 0.01241515
Iteration 102, loss = 0.01209027
Iteration 103, loss = 0.01217634
Iteration 104, loss = 0.01188419
Iteration 105, loss = 0.01214363
Iteration 106, loss = 0.01212883
Iteration 107, loss = 0.01202529
Iteration 108, loss = 0.01196843
Iteration 109, loss = 0.01172062
Iteration 110, loss = 0.01162115
Iteration 111, loss = 0.01161009
Iteration 112, loss = 0.01149022
Iteration 113, loss = 0.01152384
Iteration 114, loss = 0.01148519
Iteration 115, loss = 0.01144504
Iteration 116, loss = 0.01136120
Iteration 117, loss = 0.01132665
Iteration 118, loss = 0.01135088
Iteration 119, loss = 0.01125590
Iteration 120, loss = 0.01116838
Iteration 121, loss = 0.01116231
Iteration 122, loss = 0.01109515
Iteration 123, loss = 0.01105703
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67579529
Iteration 2, loss = 0.59333749
Iteration 3, loss = 0.47927595
Iteration 4, loss = 0.36666526
Iteration 5, loss = 0.27392344
Iteration 6, loss = 0.20464836
Iteration 7, loss = 0.15669945
Iteration 8, loss = 0.12363202
Iteration 9, loss = 0.10085375
Iteration 10, loss = 0.08469038
Iteration 11, loss = 0.07295615
Iteration 12, loss = 0.06426284
Iteration 13, loss = 0.05756000
Iteration 14, loss = 0.05253590
Iteration 15, loss = 0.04843280
Iteration 16, loss = 0.04517777
Iteration 17, loss = 0.04265157
Iteration 18, loss = 0.04049958
Iteration 19, loss = 0.03826698
Iteration 20, loss = 0.03672490
Iteration 21, loss = 0.03515097
Iteration 22, loss = 0.03409417
Iteration 23, loss = 0.03319547
Iteration 24, loss = 0.03236709
Iteration 25, loss = 0.03178798
Iteration 26, loss = 0.03075705
Iteration 27, loss = 0.03013902
Iteration 28, loss = 0.02961766
Iteration 29, loss = 0.02939476
Iteration 30, loss = 0.02884447
Iteration 31, loss = 0.02838264
Iteration 32, loss = 0.02803284
Iteration 33, loss = 0.02789461
Iteration 34, loss = 0.02744924
Iteration 35, loss = 0.02729072
Iteration 36, loss = 0.02705768
Iteration 37, loss = 0.02679047
Iteration 38, loss = 0.02653961
Iteration 39, loss = 0.02634715
Iteration 40, loss = 0.02602207
Iteration 41, loss = 0.02625071
Iteration 42, loss = 0.02574458
Iteration 43, loss = 0.02581033
Iteration 44, loss = 0.02546405
Iteration 45, loss = 0.02560433
Iteration 46, loss = 0.02547520
Iteration 47, loss = 0.02545966
Iteration 48, loss = 0.02496472
Iteration 49, loss = 0.02523686
Iteration 50, loss = 0.02524730
Iteration 51, loss = 0.02507563
Iteration 52, loss = 0.02482678
Iteration 53, loss = 0.02518086
Iteration 54, loss = 0.02475621
Iteration 55, loss = 0.02464585
Iteration 56, loss = 0.02460130
Iteration 57, loss = 0.02459577
Iteration 58, loss = 0.02457880
Iteration 59, loss = 0.02446815
Iteration 60, loss = 0.02446061
Iteration 61, loss = 0.02450362
Iteration 62, loss = 0.02440240
Iteration 63, loss = 0.02428898
Iteration 64, loss = 0.02432459
Iteration 65, loss = 0.02428118
Iteration 66, loss = 0.02419277
Iteration 67, loss = 0.02420254
Iteration 68, loss = 0.02406114
Iteration 69, loss = 0.02430474
Iteration 70, loss = 0.02435557
Iteration 71, loss = 0.02406029
Iteration 72, loss = 0.02394655
Iteration 73, loss = 0.02389255
Iteration 74, loss = 0.02380617
Iteration 75, loss = 0.02419326
Iteration 76, loss = 0.02458896
Iteration 77, loss = 0.02401509
Iteration 78, loss = 0.02384062
Iteration 79, loss = 0.02386371
Iteration 80, loss = 0.02376229
Iteration 81, loss = 0.02394163
Iteration 82, loss = 0.02411765
Iteration 83, loss = 0.02389962
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68001602
Iteration 2, loss = 0.64416851
Iteration 3, loss = 0.59202110
Iteration 4, loss = 0.52574782
Iteration 5, loss = 0.45553814
Iteration 6, loss = 0.38748513
Iteration 7, loss = 0.32522041
Iteration 8, loss = 0.27074569
Iteration 9, loss = 0.22509690
Iteration 10, loss = 0.18785071
Iteration 11, loss = 0.15833304
Iteration 12, loss = 0.13462168
Iteration 13, loss = 0.11609945
Iteration 14, loss = 0.10112704
Iteration 15, loss = 0.08922212
Iteration 16, loss = 0.07962923
Iteration 17, loss = 0.07175381
Iteration 18, loss = 0.06513853
Iteration 19, loss = 0.05982285
Iteration 20, loss = 0.05520325
Iteration 21, loss = 0.05131640
Iteration 22, loss = 0.04803301
Iteration 23, loss = 0.04501496
Iteration 24, loss = 0.04265896
Iteration 25, loss = 0.04041697
Iteration 26, loss = 0.03860769
Iteration 27, loss = 0.03680719
Iteration 28, loss = 0.03521576
Iteration 29, loss = 0.03387723
Iteration 30, loss = 0.03259617
Iteration 31, loss = 0.03153803
Iteration 32, loss = 0.03054661
Iteration 33, loss = 0.02970177
Iteration 34, loss = 0.02898180
Iteration 35, loss = 0.02811769
Iteration 36, loss = 0.02753692
Iteration 37, loss = 0.02689174
Iteration 38, loss = 0.02627840
Iteration 39, loss = 0.02566691
Iteration 40, loss = 0.02529129
Iteration 41, loss = 0.02484879
Iteration 42, loss = 0.02441151
Iteration 43, loss = 0.02403451
Iteration 44, loss = 0.02356243
Iteration 45, loss = 0.02341709
Iteration 46, loss = 0.02300050
Iteration 47, loss = 0.02266995
Iteration 48, loss = 0.02240228
Iteration 49, loss = 0.02207715
Iteration 50, loss = 0.02194970
Iteration 51, loss = 0.02164453
Iteration 52, loss = 0.02151109
Iteration 53, loss = 0.02127011
Iteration 54, loss = 0.02098142
Iteration 55, loss = 0.02085883
Iteration 56, loss = 0.02061831
Iteration 57, loss = 0.02062028
Iteration 58, loss = 0.02036709
Iteration 59, loss = 0.02034732
Iteration 60, loss = 0.02019012
Iteration 61, loss = 0.02008707
Iteration 62, loss = 0.01984668
Iteration 63, loss = 0.01983152
Iteration 64, loss = 0.01968950
Iteration 65, loss = 0.01972635
Iteration 66, loss = 0.01944391
Iteration 67, loss = 0.01934667
Iteration 68, loss = 0.01924316
Iteration 69, loss = 0.01924467
Iteration 70, loss = 0.01900687
Iteration 71, loss = 0.01919078
Iteration 72, loss = 0.01893762
Iteration 73, loss = 0.01887447
Iteration 74, loss = 0.01889012
Iteration 75, loss = 0.01867085
Iteration 76, loss = 0.01874173
Iteration 77, loss = 0.01859135
Iteration 78, loss = 0.01864178
Iteration 79, loss = 0.01845781
Iteration 80, loss = 0.01843908
Iteration 81, loss = 0.01841822
Iteration 82, loss = 0.01835703
Iteration 83, loss = 0.01828234
Iteration 84, loss = 0.01817989
Iteration 85, loss = 0.01816457
Iteration 86, loss = 0.01824701
Iteration 87, loss = 0.01802279
Iteration 88, loss = 0.01816122
Iteration 89, loss = 0.01798147
Iteration 90, loss = 0.01799590
Iteration 91, loss = 0.01785959
Iteration 92, loss = 0.01797424
Iteration 93, loss = 0.01780563
Iteration 94, loss = 0.01795691
Iteration 95, loss = 0.01804473
Iteration 96, loss = 0.01783610
Iteration 97, loss = 0.01768496
Iteration 98, loss = 0.01772680
Iteration 99, loss = 0.01763158
Iteration 100, loss = 0.01775081
Iteration 101, loss = 0.01765530
Iteration 102, loss = 0.01752798
Iteration 103, loss = 0.01768601
Iteration 104, loss = 0.01755742
Iteration 105, loss = 0.01747848
Iteration 106, loss = 0.01750979
Iteration 107, loss = 0.01748548
Iteration 108, loss = 0.01742896
Iteration 109, loss = 0.01732485
Iteration 110, loss = 0.01737843
Iteration 111, loss = 0.01741609
Iteration 112, loss = 0.01745859
Iteration 113, loss = 0.01736612
Iteration 114, loss = 0.01728117
Iteration 115, loss = 0.01742221
Iteration 116, loss = 0.01726843
Iteration 117, loss = 0.01733164
Iteration 118, loss = 0.01721642
Iteration 119, loss = 0.01722403
Iteration 120, loss = 0.01710703
Iteration 121, loss = 0.01713787
Iteration 122, loss = 0.01736702
Iteration 123, loss = 0.01712786
Iteration 124, loss = 0.01715117
Iteration 125, loss = 0.01720192
Iteration 126, loss = 0.01707883
Iteration 127, loss = 0.01700981
Iteration 128, loss = 0.01699078
Iteration 129, loss = 0.01700050
Iteration 130, loss = 0.01708180
Iteration 131, loss = 0.01699690
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68592650
Iteration 2, loss = 0.62191444
Iteration 3, loss = 0.52198002
Iteration 4, loss = 0.41036977
Iteration 5, loss = 0.31086520
Iteration 6, loss = 0.23396815
Iteration 7, loss = 0.17872650
Iteration 8, loss = 0.14031282
Iteration 9, loss = 0.11302111
Iteration 10, loss = 0.09403668
Iteration 11, loss = 0.08022972
Iteration 12, loss = 0.07006194
Iteration 13, loss = 0.06223711
Iteration 14, loss = 0.05602536
Iteration 15, loss = 0.05136506
Iteration 16, loss = 0.04779140
Iteration 17, loss = 0.04486001
Iteration 18, loss = 0.04205878
Iteration 19, loss = 0.03948875
Iteration 20, loss = 0.03773207
Iteration 21, loss = 0.03612123
Iteration 22, loss = 0.03481729
Iteration 23, loss = 0.03387437
Iteration 24, loss = 0.03265999
Iteration 25, loss = 0.03173805
Iteration 26, loss = 0.03101014
Iteration 27, loss = 0.03035019
Iteration 28, loss = 0.02967823
Iteration 29, loss = 0.02950957
Iteration 30, loss = 0.02857089
Iteration 31, loss = 0.02838930
Iteration 32, loss = 0.02796278
Iteration 33, loss = 0.02756808
Iteration 34, loss = 0.02726053
Iteration 35, loss = 0.02675084
Iteration 36, loss = 0.02651752
Iteration 37, loss = 0.02659002
Iteration 38, loss = 0.02608322
Iteration 39, loss = 0.02595321
Iteration 40, loss = 0.02586987
Iteration 41, loss = 0.02571229
Iteration 42, loss = 0.02536270
Iteration 43, loss = 0.02518796
Iteration 44, loss = 0.02516226
Iteration 45, loss = 0.02490664
Iteration 46, loss = 0.02518737
Iteration 47, loss = 0.02491792
Iteration 48, loss = 0.02450384
Iteration 49, loss = 0.02459536
Iteration 50, loss = 0.02475732
Iteration 51, loss = 0.02474546
Iteration 52, loss = 0.02444601
Iteration 53, loss = 0.02425169
Iteration 54, loss = 0.02433924
Iteration 55, loss = 0.02409663
Iteration 56, loss = 0.02403947
Iteration 57, loss = 0.02403785
Iteration 58, loss = 0.02397598
Iteration 59, loss = 0.02409180
Iteration 60, loss = 0.02369568
Iteration 61, loss = 0.02410397
Iteration 62, loss = 0.02389538
Iteration 63, loss = 0.02359441
Iteration 64, loss = 0.02372758
Iteration 65, loss = 0.02364624
Iteration 66, loss = 0.02365859
Iteration 67, loss = 0.02337549
Iteration 68, loss = 0.02361816
Iteration 69, loss = 0.02316678
Iteration 70, loss = 0.02353165
Iteration 71, loss = 0.02307601
Iteration 72, loss = 0.02306082
Iteration 73, loss = 0.02331095
Iteration 74, loss = 0.02317407
Iteration 75, loss = 0.02345931
Iteration 76, loss = 0.02329958
Iteration 77, loss = 0.02299874
Iteration 78, loss = 0.02302329
Iteration 79, loss = 0.02311346
Iteration 80, loss = 0.02280744
Iteration 81, loss = 0.02285733
Iteration 82, loss = 0.02310156
Iteration 83, loss = 0.02316403
Iteration 84, loss = 0.02324368
Iteration 85, loss = 0.02284310
Iteration 86, loss = 0.02282217
Iteration 87, loss = 0.02291322
Iteration 88, loss = 0.02315750
Iteration 89, loss = 0.02291248
Iteration 90, loss = 0.02296605
Iteration 91, loss = 0.02286147
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68180221
Iteration 2, loss = 0.66009476
Iteration 3, loss = 0.63798994
Iteration 4, loss = 0.61249875
Iteration 5, loss = 0.58179443
Iteration 6, loss = 0.54709961
Iteration 7, loss = 0.50987179
Iteration 8, loss = 0.47136265
Iteration 9, loss = 0.43306041
Iteration 10, loss = 0.39555971
Iteration 11, loss = 0.35988522
Iteration 12, loss = 0.32624101
Iteration 13, loss = 0.29503802
Iteration 14, loss = 0.26663306
Iteration 15, loss = 0.24069966
Iteration 16, loss = 0.21742060
Iteration 17, loss = 0.19651488
Iteration 18, loss = 0.17788908
Iteration 19, loss = 0.16133749
Iteration 20, loss = 0.14669868
Iteration 21, loss = 0.13371743
Iteration 22, loss = 0.12223446
Iteration 23, loss = 0.11211549
Iteration 24, loss = 0.10301753
Iteration 25, loss = 0.09510434
Iteration 26, loss = 0.08793512
Iteration 27, loss = 0.08160148
Iteration 28, loss = 0.07589902
Iteration 29, loss = 0.07085425
Iteration 30, loss = 0.06620349
Iteration 31, loss = 0.06207967
Iteration 32, loss = 0.05833717
Iteration 33, loss = 0.05500271
Iteration 34, loss = 0.05185718
Iteration 35, loss = 0.04907075
Iteration 36, loss = 0.04653172
Iteration 37, loss = 0.04414823
Iteration 38, loss = 0.04201140
Iteration 39, loss = 0.04007543
Iteration 40, loss = 0.03818520
Iteration 41, loss = 0.03655066
Iteration 42, loss = 0.03494816
Iteration 43, loss = 0.03353991
Iteration 44, loss = 0.03212125
Iteration 45, loss = 0.03089932
Iteration 46, loss = 0.02975429
Iteration 47, loss = 0.02869102
Iteration 48, loss = 0.02766968
Iteration 49, loss = 0.02672328
Iteration 50, loss = 0.02578819
Iteration 51, loss = 0.02497512
Iteration 52, loss = 0.02417384
Iteration 53, loss = 0.02344243
Iteration 54, loss = 0.02275572
Iteration 55, loss = 0.02207683
Iteration 56, loss = 0.02150442
Iteration 57, loss = 0.02085164
Iteration 58, loss = 0.02038079
Iteration 59, loss = 0.01978645
Iteration 60, loss = 0.01931547
Iteration 61, loss = 0.01885042
Iteration 62, loss = 0.01842621
Iteration 63, loss = 0.01794901
Iteration 64, loss = 0.01755611
Iteration 65, loss = 0.01712835
Iteration 66, loss = 0.01681225
Iteration 67, loss = 0.01644508
Iteration 68, loss = 0.01611768
Iteration 69, loss = 0.01577130
Iteration 70, loss = 0.01548071
Iteration 71, loss = 0.01518519
Iteration 72, loss = 0.01488514
Iteration 73, loss = 0.01463666
Iteration 74, loss = 0.01446481
Iteration 75, loss = 0.01417696
Iteration 76, loss = 0.01388973
Iteration 77, loss = 0.01367318
Iteration 78, loss = 0.01343994
Iteration 79, loss = 0.01325353
Iteration 80, loss = 0.01305105
Iteration 81, loss = 0.01282308
Iteration 82, loss = 0.01267647
Iteration 83, loss = 0.01250041
Iteration 84, loss = 0.01229017
Iteration 85, loss = 0.01218906
Iteration 86, loss = 0.01197971
Iteration 87, loss = 0.01185252
Iteration 88, loss = 0.01167465
Iteration 89, loss = 0.01154585
Iteration 90, loss = 0.01140363
Iteration 91, loss = 0.01129269
Iteration 92, loss = 0.01113349
Iteration 93, loss = 0.01099673
Iteration 94, loss = 0.01090716
Iteration 95, loss = 0.01083771
Iteration 96, loss = 0.01062857
Iteration 97, loss = 0.01058380
Iteration 98, loss = 0.01048810
Iteration 99, loss = 0.01036081
Iteration 100, loss = 0.01027216
Iteration 101, loss = 0.01015825
Iteration 102, loss = 0.01005713
Iteration 103, loss = 0.00995225
Iteration 104, loss = 0.00989645
Iteration 105, loss = 0.00978766
Iteration 106, loss = 0.00972480
Iteration 107, loss = 0.00960937
Iteration 108, loss = 0.00953561
Iteration 109, loss = 0.00948992
Iteration 110, loss = 0.00941490
Iteration 111, loss = 0.00937333
Iteration 112, loss = 0.00926799
Iteration 113, loss = 0.00922691
Iteration 114, loss = 0.00910725
Iteration 115, loss = 0.00906348
Iteration 116, loss = 0.00898047
Iteration 117, loss = 0.00894182
Iteration 118, loss = 0.00893120
Iteration 119, loss = 0.00883619
Iteration 120, loss = 0.00874725
Iteration 121, loss = 0.00876158
Iteration 122, loss = 0.00868563
Iteration 123, loss = 0.00866355
Iteration 124, loss = 0.00856761
Iteration 125, loss = 0.00851199
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68429504
Iteration 2, loss = 0.65572352
Iteration 3, loss = 0.61454497
Iteration 4, loss = 0.55694199
Iteration 5, loss = 0.48844730
Iteration 6, loss = 0.41758645
Iteration 7, loss = 0.35074067
Iteration 8, loss = 0.29139668
Iteration 9, loss = 0.24108829
Iteration 10, loss = 0.20001146
Iteration 11, loss = 0.16721282
Iteration 12, loss = 0.14104959
Iteration 13, loss = 0.12070758
Iteration 14, loss = 0.10444135
Iteration 15, loss = 0.09165743
Iteration 16, loss = 0.08133742
Iteration 17, loss = 0.07275855
Iteration 18, loss = 0.06595589
Iteration 19, loss = 0.06022589
Iteration 20, loss = 0.05534598
Iteration 21, loss = 0.05113497
Iteration 22, loss = 0.04775428
Iteration 23, loss = 0.04476240
Iteration 24, loss = 0.04223735
Iteration 25, loss = 0.03985490
Iteration 26, loss = 0.03791492
Iteration 27, loss = 0.03596246
Iteration 28, loss = 0.03454388
Iteration 29, loss = 0.03317711
Iteration 30, loss = 0.03198204
Iteration 31, loss = 0.03081081
Iteration 32, loss = 0.02979874
Iteration 33, loss = 0.02898824
Iteration 34, loss = 0.02810827
Iteration 35, loss = 0.02737981
Iteration 36, loss = 0.02668131
Iteration 37, loss = 0.02613684
Iteration 38, loss = 0.02558996
Iteration 39, loss = 0.02510207
Iteration 40, loss = 0.02449759
Iteration 41, loss = 0.02405485
Iteration 42, loss = 0.02364675
Iteration 43, loss = 0.02328083
Iteration 44, loss = 0.02289625
Iteration 45, loss = 0.02253278
Iteration 46, loss = 0.02228378
Iteration 47, loss = 0.02191676
Iteration 48, loss = 0.02166250
Iteration 49, loss = 0.02139046
Iteration 50, loss = 0.02115702
Iteration 51, loss = 0.02091164
Iteration 52, loss = 0.02081793
Iteration 53, loss = 0.02062339
Iteration 54, loss = 0.02040410
Iteration 55, loss = 0.02022002
Iteration 56, loss = 0.02006677
Iteration 57, loss = 0.01983808
Iteration 58, loss = 0.01963847
Iteration 59, loss = 0.01971605
Iteration 60, loss = 0.01938781
Iteration 61, loss = 0.01933976
Iteration 62, loss = 0.01914209
Iteration 63, loss = 0.01906208
Iteration 64, loss = 0.01905438
Iteration 65, loss = 0.01887248
Iteration 66, loss = 0.01874595
Iteration 67, loss = 0.01869200
Iteration 68, loss = 0.01860560
Iteration 69, loss = 0.01852380
Iteration 70, loss = 0.01845629
Iteration 71, loss = 0.01837953
Iteration 72, loss = 0.01819428
Iteration 73, loss = 0.01809557
Iteration 74, loss = 0.01795963
Iteration 75, loss = 0.01805857
Iteration 76, loss = 0.01807105
Iteration 77, loss = 0.01791693
Iteration 78, loss = 0.01784516
Iteration 79, loss = 0.01780379
Iteration 80, loss = 0.01780720
Iteration 81, loss = 0.01757061
Iteration 82, loss = 0.01781336
Iteration 83, loss = 0.01759401
Iteration 84, loss = 0.01750377
Iteration 85, loss = 0.01747636
Iteration 86, loss = 0.01745162
Iteration 87, loss = 0.01753103
Iteration 88, loss = 0.01734832
Iteration 89, loss = 0.01742590
Iteration 90, loss = 0.01749224
Iteration 91, loss = 0.01733522
Iteration 92, loss = 0.01721873
Iteration 93, loss = 0.01731292
Iteration 94, loss = 0.01710837
Iteration 95, loss = 0.01712211
Iteration 96, loss = 0.01704524
Iteration 97, loss = 0.01719062
Iteration 98, loss = 0.01713105
Iteration 99, loss = 0.01717824
Iteration 100, loss = 0.01695730
Iteration 101, loss = 0.01691945
Iteration 102, loss = 0.01687936
Iteration 103, loss = 0.01690558
Iteration 104, loss = 0.01688717
Iteration 105, loss = 0.01696265
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68548653
Iteration 2, loss = 0.60387311
Iteration 3, loss = 0.49487517
Iteration 4, loss = 0.38378502
Iteration 5, loss = 0.28913342
Iteration 6, loss = 0.21641984
Iteration 7, loss = 0.16514068
Iteration 8, loss = 0.12970940
Iteration 9, loss = 0.10469400
Iteration 10, loss = 0.08712367
Iteration 11, loss = 0.07480049
Iteration 12, loss = 0.06534833
Iteration 13, loss = 0.05828847
Iteration 14, loss = 0.05286558
Iteration 15, loss = 0.04857977
Iteration 16, loss = 0.04505647
Iteration 17, loss = 0.04233642
Iteration 18, loss = 0.04004106
Iteration 19, loss = 0.03796938
Iteration 20, loss = 0.03631916
Iteration 21, loss = 0.03527690
Iteration 22, loss = 0.03368535
Iteration 23, loss = 0.03273390
Iteration 24, loss = 0.03182193
Iteration 25, loss = 0.03113318
Iteration 26, loss = 0.03061290
Iteration 27, loss = 0.02983873
Iteration 28, loss = 0.02931299
Iteration 29, loss = 0.02889592
Iteration 30, loss = 0.02808371
Iteration 31, loss = 0.02791113
Iteration 32, loss = 0.02775857
Iteration 33, loss = 0.02716963
Iteration 34, loss = 0.02704862
Iteration 35, loss = 0.02677885
Iteration 36, loss = 0.02660689
Iteration 37, loss = 0.02635079
Iteration 38, loss = 0.02616265
Iteration 39, loss = 0.02616165
Iteration 40, loss = 0.02582952
Iteration 41, loss = 0.02560101
Iteration 42, loss = 0.02548252
Iteration 43, loss = 0.02561144
Iteration 44, loss = 0.02528543
Iteration 45, loss = 0.02534083
Iteration 46, loss = 0.02517714
Iteration 47, loss = 0.02492537
Iteration 48, loss = 0.02493626
Iteration 49, loss = 0.02485747
Iteration 50, loss = 0.02483028
Iteration 51, loss = 0.02461596
Iteration 52, loss = 0.02444818
Iteration 53, loss = 0.02446714
Iteration 54, loss = 0.02429231
Iteration 55, loss = 0.02413981
Iteration 56, loss = 0.02450798
Iteration 57, loss = 0.02411292
Iteration 58, loss = 0.02402408
Iteration 59, loss = 0.02407746
Iteration 60, loss = 0.02421762
Iteration 61, loss = 0.02394045
Iteration 62, loss = 0.02404505
Iteration 63, loss = 0.02409565
Iteration 64, loss = 0.02377730
Iteration 65, loss = 0.02417960
Iteration 66, loss = 0.02361790
Iteration 67, loss = 0.02387640
Iteration 68, loss = 0.02384504
Iteration 69, loss = 0.02386421
Iteration 70, loss = 0.02353583
Iteration 71, loss = 0.02354227
Iteration 72, loss = 0.02365564
Iteration 73, loss = 0.02374498
Iteration 74, loss = 0.02345553
Iteration 75, loss = 0.02333034
Iteration 76, loss = 0.02366075
Iteration 77, loss = 0.02356376
Iteration 78, loss = 0.02340959
Iteration 79, loss = 0.02330419
Iteration 80, loss = 0.02345664
Iteration 81, loss = 0.02347309
Iteration 82, loss = 0.02323178
Iteration 83, loss = 0.02327337
Iteration 84, loss = 0.02314087
Iteration 85, loss = 0.02342591
Iteration 86, loss = 0.02312540
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	82m52.299s
user	370m36.660s
sys	37m5.910s
Elapsed time: 4972 seconds
