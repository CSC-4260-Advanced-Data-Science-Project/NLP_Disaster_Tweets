Processing dataset: dropped_v7_lowercase_words_only

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: dropped_v7_lowercase_words_only (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on dropped_v7_lowercase_words_only...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on dropped_v7_lowercase_words_only...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on dropped_v7_lowercase_words_only...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on dropped_v7_lowercase_words_only...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on dropped_v7_lowercase_words_only...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.68413076
Iteration 2, loss = 0.61808516
Iteration 3, loss = 0.51384550
Iteration 4, loss = 0.39953174
Iteration 5, loss = 0.29958660
Iteration 6, loss = 0.22443616
Iteration 7, loss = 0.17135803
Iteration 8, loss = 0.13480374
Iteration 9, loss = 0.10963786
Iteration 10, loss = 0.09205319
Iteration 11, loss = 0.07907471
Iteration 12, loss = 0.06932105
Iteration 13, loss = 0.06221011
Iteration 14, loss = 0.05642601
Iteration 15, loss = 0.05171795
Iteration 16, loss = 0.04771086
Iteration 17, loss = 0.04492750
Iteration 18, loss = 0.04235122
Iteration 19, loss = 0.04022050
Iteration 20, loss = 0.03866780
Iteration 21, loss = 0.03688821
Iteration 22, loss = 0.03553907
Iteration 23, loss = 0.03455811
Iteration 24, loss = 0.03317658
Iteration 25, loss = 0.03267333
Iteration 26, loss = 0.03147139
Iteration 27, loss = 0.03093038
Iteration 28, loss = 0.03021259
Iteration 29, loss = 0.02984408
Iteration 30, loss = 0.02936033
Iteration 31, loss = 0.02893804
Iteration 32, loss = 0.02899197
Iteration 33, loss = 0.02840530
Iteration 34, loss = 0.02798354
Iteration 35, loss = 0.02775464
Iteration 36, loss = 0.02736206
Iteration 37, loss = 0.02700085
Iteration 38, loss = 0.02659983
Iteration 39, loss = 0.02676792
Iteration 40, loss = 0.02627096
Iteration 41, loss = 0.02627902
Iteration 42, loss = 0.02614471
Iteration 43, loss = 0.02586519
Iteration 44, loss = 0.02582374
Iteration 45, loss = 0.02601019
Iteration 46, loss = 0.02536131
Iteration 47, loss = 0.02541268
Iteration 48, loss = 0.02566429
Iteration 49, loss = 0.02539103
Iteration 50, loss = 0.02502808
Iteration 51, loss = 0.02482679
Iteration 52, loss = 0.02476377
Iteration 53, loss = 0.02516314
Iteration 54, loss = 0.02477302
Iteration 55, loss = 0.02481308
Iteration 56, loss = 0.02509425
Iteration 57, loss = 0.02462482
Iteration 58, loss = 0.02467788
Iteration 59, loss = 0.02425647
Iteration 60, loss = 0.02426164
Iteration 61, loss = 0.02430673
Iteration 62, loss = 0.02420488
Iteration 63, loss = 0.02403265
Iteration 64, loss = 0.02443738
Iteration 65, loss = 0.02397600
Iteration 66, loss = 0.02443546
Iteration 67, loss = 0.02396654
Iteration 68, loss = 0.02389890
Iteration 69, loss = 0.02385129
Iteration 70, loss = 0.02383891
Iteration 71, loss = 0.02400848
Iteration 72, loss = 0.02368868
Iteration 73, loss = 0.02392049
Iteration 74, loss = 0.02390102
Iteration 75, loss = 0.02385124
Iteration 76, loss = 0.02382285
Iteration 77, loss = 0.02364604
Iteration 78, loss = 0.02394046
Iteration 79, loss = 0.02363261
Iteration 80, loss = 0.02394544
Iteration 81, loss = 0.02385290
Iteration 82, loss = 0.02344547
Iteration 83, loss = 0.02370467
Iteration 84, loss = 0.02347568
Iteration 85, loss = 0.02347882
Iteration 86, loss = 0.02341423
Iteration 87, loss = 0.02330067
Iteration 88, loss = 0.02332169
Iteration 89, loss = 0.02337406
Iteration 90, loss = 0.02313402
Iteration 91, loss = 0.02335541
Iteration 92, loss = 0.02345969
Iteration 93, loss = 0.02348148
Iteration 94, loss = 0.02319795
Iteration 95, loss = 0.02333483
Iteration 96, loss = 0.02332289
Iteration 97, loss = 0.02321844
Iteration 98, loss = 0.02328673
Iteration 99, loss = 0.02346247
Iteration 100, loss = 0.02335005
Iteration 101, loss = 0.02341258
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67550330
Iteration 2, loss = 0.60170797
Iteration 3, loss = 0.49313701
Iteration 4, loss = 0.37983918
Iteration 5, loss = 0.28443651
Iteration 6, loss = 0.21324415
Iteration 7, loss = 0.16372981
Iteration 8, loss = 0.13008520
Iteration 9, loss = 0.10660477
Iteration 10, loss = 0.08993104
Iteration 11, loss = 0.07783842
Iteration 12, loss = 0.06857303
Iteration 13, loss = 0.06162709
Iteration 14, loss = 0.05590142
Iteration 15, loss = 0.05175399
Iteration 16, loss = 0.04795879
Iteration 17, loss = 0.04508994
Iteration 18, loss = 0.04270718
Iteration 19, loss = 0.04077581
Iteration 20, loss = 0.03894332
Iteration 21, loss = 0.03747031
Iteration 22, loss = 0.03611345
Iteration 23, loss = 0.03492300
Iteration 24, loss = 0.03419141
Iteration 25, loss = 0.03327292
Iteration 26, loss = 0.03230723
Iteration 27, loss = 0.03153871
Iteration 28, loss = 0.03140444
Iteration 29, loss = 0.03085512
Iteration 30, loss = 0.03000770
Iteration 31, loss = 0.02969647
Iteration 32, loss = 0.02923129
Iteration 33, loss = 0.02916218
Iteration 34, loss = 0.02850342
Iteration 35, loss = 0.02823477
Iteration 36, loss = 0.02795034
Iteration 37, loss = 0.02782791
Iteration 38, loss = 0.02781304
Iteration 39, loss = 0.02734600
Iteration 40, loss = 0.02727316
Iteration 41, loss = 0.02705194
Iteration 42, loss = 0.02711177
Iteration 43, loss = 0.02650332
Iteration 44, loss = 0.02659962
Iteration 45, loss = 0.02639333
Iteration 46, loss = 0.02607664
Iteration 47, loss = 0.02624885
Iteration 48, loss = 0.02616865
Iteration 49, loss = 0.02571048
Iteration 50, loss = 0.02599937
Iteration 51, loss = 0.02570195
Iteration 52, loss = 0.02575608
Iteration 53, loss = 0.02569258
Iteration 54, loss = 0.02542766
Iteration 55, loss = 0.02522660
Iteration 56, loss = 0.02535863
Iteration 57, loss = 0.02505995
Iteration 58, loss = 0.02522607
Iteration 59, loss = 0.02535154
Iteration 60, loss = 0.02538213
Iteration 61, loss = 0.02499563
Iteration 62, loss = 0.02524340
Iteration 63, loss = 0.02497605
Iteration 64, loss = 0.02507365
Iteration 65, loss = 0.02511162
Iteration 66, loss = 0.02503434
Iteration 67, loss = 0.02520440
Iteration 68, loss = 0.02496843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69260109
Iteration 2, loss = 0.61790605
Iteration 3, loss = 0.50777993
Iteration 4, loss = 0.39010546
Iteration 5, loss = 0.29171583
Iteration 6, loss = 0.21776443
Iteration 7, loss = 0.16620487
Iteration 8, loss = 0.13092776
Iteration 9, loss = 0.10629097
Iteration 10, loss = 0.08890654
Iteration 11, loss = 0.07624650
Iteration 12, loss = 0.06680871
Iteration 13, loss = 0.05956012
Iteration 14, loss = 0.05362984
Iteration 15, loss = 0.04927363
Iteration 16, loss = 0.04529133
Iteration 17, loss = 0.04245956
Iteration 18, loss = 0.03983526
Iteration 19, loss = 0.03785160
Iteration 20, loss = 0.03631280
Iteration 21, loss = 0.03447118
Iteration 22, loss = 0.03342926
Iteration 23, loss = 0.03185982
Iteration 24, loss = 0.03102607
Iteration 25, loss = 0.03020590
Iteration 26, loss = 0.02944975
Iteration 27, loss = 0.02865123
Iteration 28, loss = 0.02806396
Iteration 29, loss = 0.02743727
Iteration 30, loss = 0.02716236
Iteration 31, loss = 0.02668105
Iteration 32, loss = 0.02635972
Iteration 33, loss = 0.02575510
Iteration 34, loss = 0.02542044
Iteration 35, loss = 0.02509207
Iteration 36, loss = 0.02488736
Iteration 37, loss = 0.02445617
Iteration 38, loss = 0.02439967
Iteration 39, loss = 0.02425283
Iteration 40, loss = 0.02393187
Iteration 41, loss = 0.02382984
Iteration 42, loss = 0.02370361
Iteration 43, loss = 0.02356742
Iteration 44, loss = 0.02332874
Iteration 45, loss = 0.02337160
Iteration 46, loss = 0.02299506
Iteration 47, loss = 0.02294622
Iteration 48, loss = 0.02297295
Iteration 49, loss = 0.02280642
Iteration 50, loss = 0.02254478
Iteration 51, loss = 0.02248339
Iteration 52, loss = 0.02244918
Iteration 53, loss = 0.02218288
Iteration 54, loss = 0.02212184
Iteration 55, loss = 0.02238548
Iteration 56, loss = 0.02233965
Iteration 57, loss = 0.02208707
Iteration 58, loss = 0.02208124
Iteration 59, loss = 0.02183687
Iteration 60, loss = 0.02204975
Iteration 61, loss = 0.02205745
Iteration 62, loss = 0.02179028
Iteration 63, loss = 0.02166570
Iteration 64, loss = 0.02177508
Iteration 65, loss = 0.02166955
Iteration 66, loss = 0.02171338
Iteration 67, loss = 0.02179062
Iteration 68, loss = 0.02147092
Iteration 69, loss = 0.02165957
Iteration 70, loss = 0.02144857
Iteration 71, loss = 0.02138122
Iteration 72, loss = 0.02157071
Iteration 73, loss = 0.02139394
Iteration 74, loss = 0.02126997
Iteration 75, loss = 0.02116893
Iteration 76, loss = 0.02129133
Iteration 77, loss = 0.02136148
Iteration 78, loss = 0.02110665
Iteration 79, loss = 0.02123970
Iteration 80, loss = 0.02125779
Iteration 81, loss = 0.02120034
Iteration 82, loss = 0.02104704
Iteration 83, loss = 0.02125497
Iteration 84, loss = 0.02096448
Iteration 85, loss = 0.02103696
Iteration 86, loss = 0.02119981
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68424005
Iteration 2, loss = 0.60406149
Iteration 3, loss = 0.49670596
Iteration 4, loss = 0.38451450
Iteration 5, loss = 0.28838567
Iteration 6, loss = 0.21602944
Iteration 7, loss = 0.16477694
Iteration 8, loss = 0.12980239
Iteration 9, loss = 0.10550603
Iteration 10, loss = 0.08840839
Iteration 11, loss = 0.07576046
Iteration 12, loss = 0.06647728
Iteration 13, loss = 0.05907842
Iteration 14, loss = 0.05343674
Iteration 15, loss = 0.04894668
Iteration 16, loss = 0.04520320
Iteration 17, loss = 0.04256285
Iteration 18, loss = 0.04006382
Iteration 19, loss = 0.03777499
Iteration 20, loss = 0.03607854
Iteration 21, loss = 0.03445402
Iteration 22, loss = 0.03354707
Iteration 23, loss = 0.03194946
Iteration 24, loss = 0.03110975
Iteration 25, loss = 0.03028158
Iteration 26, loss = 0.02939236
Iteration 27, loss = 0.02869222
Iteration 28, loss = 0.02809266
Iteration 29, loss = 0.02752520
Iteration 30, loss = 0.02723376
Iteration 31, loss = 0.02656610
Iteration 32, loss = 0.02635351
Iteration 33, loss = 0.02605514
Iteration 34, loss = 0.02560784
Iteration 35, loss = 0.02518664
Iteration 36, loss = 0.02497153
Iteration 37, loss = 0.02468598
Iteration 38, loss = 0.02460633
Iteration 39, loss = 0.02427669
Iteration 40, loss = 0.02401199
Iteration 41, loss = 0.02399491
Iteration 42, loss = 0.02380727
Iteration 43, loss = 0.02358210
Iteration 44, loss = 0.02344259
Iteration 45, loss = 0.02351392
Iteration 46, loss = 0.02329049
Iteration 47, loss = 0.02306195
Iteration 48, loss = 0.02319992
Iteration 49, loss = 0.02267445
Iteration 50, loss = 0.02269533
Iteration 51, loss = 0.02255048
Iteration 52, loss = 0.02251574
Iteration 53, loss = 0.02252164
Iteration 54, loss = 0.02262414
Iteration 55, loss = 0.02227799
Iteration 56, loss = 0.02249743
Iteration 57, loss = 0.02243017
Iteration 58, loss = 0.02220723
Iteration 59, loss = 0.02229847
Iteration 60, loss = 0.02213198
Iteration 61, loss = 0.02216993
Iteration 62, loss = 0.02196446
Iteration 63, loss = 0.02190466
Iteration 64, loss = 0.02203342
Iteration 65, loss = 0.02193854
Iteration 66, loss = 0.02177320
Iteration 67, loss = 0.02207178
Iteration 68, loss = 0.02171118
Iteration 69, loss = 0.02192084
Iteration 70, loss = 0.02157800
Iteration 71, loss = 0.02167495
Iteration 72, loss = 0.02174262
Iteration 73, loss = 0.02163539
Iteration 74, loss = 0.02157980
Iteration 75, loss = 0.02147791
Iteration 76, loss = 0.02148970
Iteration 77, loss = 0.02172093
Iteration 78, loss = 0.02139659
Iteration 79, loss = 0.02142087
Iteration 80, loss = 0.02140139
Iteration 81, loss = 0.02141742
Iteration 82, loss = 0.02121863
Iteration 83, loss = 0.02133361
Iteration 84, loss = 0.02157135
Iteration 85, loss = 0.02129171
Iteration 86, loss = 0.02118559
Iteration 87, loss = 0.02136830
Iteration 88, loss = 0.02157312
Iteration 89, loss = 0.02105237
Iteration 90, loss = 0.02145224
Iteration 91, loss = 0.02139344
Iteration 92, loss = 0.02113279
Iteration 93, loss = 0.02112261
Iteration 94, loss = 0.02109799
Iteration 95, loss = 0.02154563
Iteration 96, loss = 0.02135130
Iteration 97, loss = 0.02113732
Iteration 98, loss = 0.02096416
Iteration 99, loss = 0.02093224
Iteration 100, loss = 0.02121938
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71833177
Iteration 2, loss = 0.65580292
Iteration 3, loss = 0.55646164
Iteration 4, loss = 0.43861290
Iteration 5, loss = 0.33251760
Iteration 6, loss = 0.24994684
Iteration 7, loss = 0.18985323
Iteration 8, loss = 0.14788196
Iteration 9, loss = 0.11896982
Iteration 10, loss = 0.09839127
Iteration 11, loss = 0.08389783
Iteration 12, loss = 0.07271211
Iteration 13, loss = 0.06438933
Iteration 14, loss = 0.05814150
Iteration 15, loss = 0.05250691
Iteration 16, loss = 0.04878223
Iteration 17, loss = 0.04509925
Iteration 18, loss = 0.04214450
Iteration 19, loss = 0.03985235
Iteration 20, loss = 0.03780419
Iteration 21, loss = 0.03612292
Iteration 22, loss = 0.03453342
Iteration 23, loss = 0.03338868
Iteration 24, loss = 0.03237810
Iteration 25, loss = 0.03128482
Iteration 26, loss = 0.03039589
Iteration 27, loss = 0.02988055
Iteration 28, loss = 0.02888475
Iteration 29, loss = 0.02847669
Iteration 30, loss = 0.02772662
Iteration 31, loss = 0.02728270
Iteration 32, loss = 0.02681188
Iteration 33, loss = 0.02662336
Iteration 34, loss = 0.02625174
Iteration 35, loss = 0.02550222
Iteration 36, loss = 0.02545399
Iteration 37, loss = 0.02525044
Iteration 38, loss = 0.02483540
Iteration 39, loss = 0.02463672
Iteration 40, loss = 0.02460153
Iteration 41, loss = 0.02422775
Iteration 42, loss = 0.02398078
Iteration 43, loss = 0.02390449
Iteration 44, loss = 0.02373593
Iteration 45, loss = 0.02347569
Iteration 46, loss = 0.02356136
Iteration 47, loss = 0.02341875
Iteration 48, loss = 0.02327137
Iteration 49, loss = 0.02316549
Iteration 50, loss = 0.02280447
Iteration 51, loss = 0.02283596
Iteration 52, loss = 0.02290367
Iteration 53, loss = 0.02261693
Iteration 54, loss = 0.02259324
Iteration 55, loss = 0.02242722
Iteration 56, loss = 0.02246740
Iteration 57, loss = 0.02240077
Iteration 58, loss = 0.02234744
Iteration 59, loss = 0.02211607
Iteration 60, loss = 0.02215475
Iteration 61, loss = 0.02205561
Iteration 62, loss = 0.02229426
Iteration 63, loss = 0.02186034
Iteration 64, loss = 0.02180090
Iteration 65, loss = 0.02202792
Iteration 66, loss = 0.02168157
Iteration 67, loss = 0.02187154
Iteration 68, loss = 0.02171831
Iteration 69, loss = 0.02169409
Iteration 70, loss = 0.02170534
Iteration 71, loss = 0.02158997
Iteration 72, loss = 0.02168940
Iteration 73, loss = 0.02174117
Iteration 74, loss = 0.02167538
Iteration 75, loss = 0.02163673
Iteration 76, loss = 0.02150253
Iteration 77, loss = 0.02142653
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68413076
Iteration 2, loss = 0.61808516
Iteration 3, loss = 0.51384550
Iteration 4, loss = 0.39953174
Iteration 5, loss = 0.29958660
Iteration 6, loss = 0.22443616
Iteration 7, loss = 0.17135803
Iteration 8, loss = 0.13480374
Iteration 9, loss = 0.10963786
Iteration 10, loss = 0.09205319
Iteration 11, loss = 0.07907471
Iteration 12, loss = 0.06932105
Iteration 13, loss = 0.06221011
Iteration 14, loss = 0.05642601
Iteration 15, loss = 0.05171795
Iteration 16, loss = 0.04771086
Iteration 17, loss = 0.04492750
Iteration 18, loss = 0.04235122
Iteration 19, loss = 0.04022050
Iteration 20, loss = 0.03866780
Iteration 21, loss = 0.03688821
Iteration 22, loss = 0.03553907
Iteration 23, loss = 0.03455811
Iteration 24, loss = 0.03317658
Iteration 25, loss = 0.03267333
Iteration 26, loss = 0.03147139
Iteration 27, loss = 0.03093038
Iteration 28, loss = 0.03021259
Iteration 29, loss = 0.02984408
Iteration 30, loss = 0.02936033
Iteration 31, loss = 0.02893804
Iteration 32, loss = 0.02899197
Iteration 33, loss = 0.02840530
Iteration 34, loss = 0.02798354
Iteration 35, loss = 0.02775464
Iteration 36, loss = 0.02736206
Iteration 37, loss = 0.02700085
Iteration 38, loss = 0.02659983
Iteration 39, loss = 0.02676792
Iteration 40, loss = 0.02627096
Iteration 41, loss = 0.02627902
Iteration 42, loss = 0.02614471
Iteration 43, loss = 0.02586519
Iteration 44, loss = 0.02582374
Iteration 45, loss = 0.02601019
Iteration 46, loss = 0.02536131
Iteration 47, loss = 0.02541268
Iteration 48, loss = 0.02566429
Iteration 49, loss = 0.02539103
Iteration 50, loss = 0.02502808
Iteration 51, loss = 0.02482679
Iteration 52, loss = 0.02476377
Iteration 53, loss = 0.02516314
Iteration 54, loss = 0.02477302
Iteration 55, loss = 0.02481308
Iteration 56, loss = 0.02509425
Iteration 57, loss = 0.02462482
Iteration 58, loss = 0.02467788
Iteration 59, loss = 0.02425647
Iteration 60, loss = 0.02426164
Iteration 61, loss = 0.02430673
Iteration 62, loss = 0.02420488
Iteration 63, loss = 0.02403265
Iteration 64, loss = 0.02443738
Iteration 65, loss = 0.02397600
Iteration 66, loss = 0.02443546
Iteration 67, loss = 0.02396654
Iteration 68, loss = 0.02389890
Iteration 69, loss = 0.02385129
Iteration 70, loss = 0.02383891
Iteration 71, loss = 0.02400848
Iteration 72, loss = 0.02368868
Iteration 73, loss = 0.02392049
Iteration 74, loss = 0.02390102
Iteration 75, loss = 0.02385124
Iteration 76, loss = 0.02382285
Iteration 77, loss = 0.02364604
Iteration 78, loss = 0.02394046
Iteration 79, loss = 0.02363261
Iteration 80, loss = 0.02394544
Iteration 81, loss = 0.02385290
Iteration 82, loss = 0.02344547
Iteration 83, loss = 0.02370467
Iteration 84, loss = 0.02347568
Iteration 85, loss = 0.02347882
Iteration 86, loss = 0.02341423
Iteration 87, loss = 0.02330067
Iteration 88, loss = 0.02332169
Iteration 89, loss = 0.02337406
Iteration 90, loss = 0.02313402
Iteration 91, loss = 0.02335541
Iteration 92, loss = 0.02345969
Iteration 93, loss = 0.02348148
Iteration 94, loss = 0.02319795
Iteration 95, loss = 0.02333483
Iteration 96, loss = 0.02332289
Iteration 97, loss = 0.02321844
Iteration 98, loss = 0.02328673
Iteration 99, loss = 0.02346247
Iteration 100, loss = 0.02335005
Iteration 101, loss = 0.02341258
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67550330
Iteration 2, loss = 0.60170797
Iteration 3, loss = 0.49313701
Iteration 4, loss = 0.37983918
Iteration 5, loss = 0.28443651
Iteration 6, loss = 0.21324415
Iteration 7, loss = 0.16372981
Iteration 8, loss = 0.13008520
Iteration 9, loss = 0.10660477
Iteration 10, loss = 0.08993104
Iteration 11, loss = 0.07783842
Iteration 12, loss = 0.06857303
Iteration 13, loss = 0.06162709
Iteration 14, loss = 0.05590142
Iteration 15, loss = 0.05175399
Iteration 16, loss = 0.04795879
Iteration 17, loss = 0.04508994
Iteration 18, loss = 0.04270718
Iteration 19, loss = 0.04077581
Iteration 20, loss = 0.03894332
Iteration 21, loss = 0.03747031
Iteration 22, loss = 0.03611345
Iteration 23, loss = 0.03492300
Iteration 24, loss = 0.03419141
Iteration 25, loss = 0.03327292
Iteration 26, loss = 0.03230723
Iteration 27, loss = 0.03153871
Iteration 28, loss = 0.03140444
Iteration 29, loss = 0.03085512
Iteration 30, loss = 0.03000770
Iteration 31, loss = 0.02969647
Iteration 32, loss = 0.02923129
Iteration 33, loss = 0.02916218
Iteration 34, loss = 0.02850342
Iteration 35, loss = 0.02823477
Iteration 36, loss = 0.02795034
Iteration 37, loss = 0.02782791
Iteration 38, loss = 0.02781304
Iteration 39, loss = 0.02734600
Iteration 40, loss = 0.02727316
Iteration 41, loss = 0.02705194
Iteration 42, loss = 0.02711177
Iteration 43, loss = 0.02650332
Iteration 44, loss = 0.02659962
Iteration 45, loss = 0.02639333
Iteration 46, loss = 0.02607664
Iteration 47, loss = 0.02624885
Iteration 48, loss = 0.02616865
Iteration 49, loss = 0.02571048
Iteration 50, loss = 0.02599937
Iteration 51, loss = 0.02570195
Iteration 52, loss = 0.02575608
Iteration 53, loss = 0.02569258
Iteration 54, loss = 0.02542766
Iteration 55, loss = 0.02522660
Iteration 56, loss = 0.02535863
Iteration 57, loss = 0.02505995
Iteration 58, loss = 0.02522607
Iteration 59, loss = 0.02535154
Iteration 60, loss = 0.02538213
Iteration 61, loss = 0.02499563
Iteration 62, loss = 0.02524340
Iteration 63, loss = 0.02497605
Iteration 64, loss = 0.02507365
Iteration 65, loss = 0.02511162
Iteration 66, loss = 0.02503434
Iteration 67, loss = 0.02520440
Iteration 68, loss = 0.02496843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69260109
Iteration 2, loss = 0.61790605
Iteration 3, loss = 0.50777993
Iteration 4, loss = 0.39010546
Iteration 5, loss = 0.29171583
Iteration 6, loss = 0.21776443
Iteration 7, loss = 0.16620487
Iteration 8, loss = 0.13092776
Iteration 9, loss = 0.10629097
Iteration 10, loss = 0.08890654
Iteration 11, loss = 0.07624650
Iteration 12, loss = 0.06680871
Iteration 13, loss = 0.05956012
Iteration 14, loss = 0.05362984
Iteration 15, loss = 0.04927363
Iteration 16, loss = 0.04529133
Iteration 17, loss = 0.04245956
Iteration 18, loss = 0.03983526
Iteration 19, loss = 0.03785160
Iteration 20, loss = 0.03631280
Iteration 21, loss = 0.03447118
Iteration 22, loss = 0.03342926
Iteration 23, loss = 0.03185982
Iteration 24, loss = 0.03102607
Iteration 25, loss = 0.03020590
Iteration 26, loss = 0.02944975
Iteration 27, loss = 0.02865123
Iteration 28, loss = 0.02806396
Iteration 29, loss = 0.02743727
Iteration 30, loss = 0.02716236
Iteration 31, loss = 0.02668105
Iteration 32, loss = 0.02635972
Iteration 33, loss = 0.02575510
Iteration 34, loss = 0.02542044
Iteration 35, loss = 0.02509207
Iteration 36, loss = 0.02488736
Iteration 37, loss = 0.02445617
Iteration 38, loss = 0.02439967
Iteration 39, loss = 0.02425283
Iteration 40, loss = 0.02393187
Iteration 41, loss = 0.02382984
Iteration 42, loss = 0.02370361
Iteration 43, loss = 0.02356742
Iteration 44, loss = 0.02332874
Iteration 45, loss = 0.02337160
Iteration 46, loss = 0.02299506
Iteration 47, loss = 0.02294622
Iteration 48, loss = 0.02297295
Iteration 49, loss = 0.02280642
Iteration 50, loss = 0.02254478
Iteration 51, loss = 0.02248339
Iteration 52, loss = 0.02244918
Iteration 53, loss = 0.02218288
Iteration 54, loss = 0.02212184
Iteration 55, loss = 0.02238548
Iteration 56, loss = 0.02233965
Iteration 57, loss = 0.02208707
Iteration 58, loss = 0.02208124
Iteration 59, loss = 0.02183687
Iteration 60, loss = 0.02204975
Iteration 61, loss = 0.02205745
Iteration 62, loss = 0.02179028
Iteration 63, loss = 0.02166570
Iteration 64, loss = 0.02177508
Iteration 65, loss = 0.02166955
Iteration 66, loss = 0.02171338
Iteration 67, loss = 0.02179062
Iteration 68, loss = 0.02147092
Iteration 69, loss = 0.02165957
Iteration 70, loss = 0.02144857
Iteration 71, loss = 0.02138122
Iteration 72, loss = 0.02157071
Iteration 73, loss = 0.02139394
Iteration 74, loss = 0.02126997
Iteration 75, loss = 0.02116893
Iteration 76, loss = 0.02129133
Iteration 77, loss = 0.02136148
Iteration 78, loss = 0.02110665
Iteration 79, loss = 0.02123970
Iteration 80, loss = 0.02125779
Iteration 81, loss = 0.02120034
Iteration 82, loss = 0.02104704
Iteration 83, loss = 0.02125497
Iteration 84, loss = 0.02096448
Iteration 85, loss = 0.02103696
Iteration 86, loss = 0.02119981
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68424005
Iteration 2, loss = 0.60406149
Iteration 3, loss = 0.49670596
Iteration 4, loss = 0.38451450
Iteration 5, loss = 0.28838567
Iteration 6, loss = 0.21602944
Iteration 7, loss = 0.16477694
Iteration 8, loss = 0.12980239
Iteration 9, loss = 0.10550603
Iteration 10, loss = 0.08840839
Iteration 11, loss = 0.07576046
Iteration 12, loss = 0.06647728
Iteration 13, loss = 0.05907842
Iteration 14, loss = 0.05343674
Iteration 15, loss = 0.04894668
Iteration 16, loss = 0.04520320
Iteration 17, loss = 0.04256285
Iteration 18, loss = 0.04006382
Iteration 19, loss = 0.03777499
Iteration 20, loss = 0.03607854
Iteration 21, loss = 0.03445402
Iteration 22, loss = 0.03354707
Iteration 23, loss = 0.03194946
Iteration 24, loss = 0.03110975
Iteration 25, loss = 0.03028158
Iteration 26, loss = 0.02939236
Iteration 27, loss = 0.02869222
Iteration 28, loss = 0.02809266
Iteration 29, loss = 0.02752520
Iteration 30, loss = 0.02723376
Iteration 31, loss = 0.02656610
Iteration 32, loss = 0.02635351
Iteration 33, loss = 0.02605514
Iteration 34, loss = 0.02560784
Iteration 35, loss = 0.02518664
Iteration 36, loss = 0.02497153
Iteration 37, loss = 0.02468598
Iteration 38, loss = 0.02460633
Iteration 39, loss = 0.02427669
Iteration 40, loss = 0.02401199
Iteration 41, loss = 0.02399491
Iteration 42, loss = 0.02380727
Iteration 43, loss = 0.02358210
Iteration 44, loss = 0.02344259
Iteration 45, loss = 0.02351392
Iteration 46, loss = 0.02329049
Iteration 47, loss = 0.02306195
Iteration 48, loss = 0.02319992
Iteration 49, loss = 0.02267445
Iteration 50, loss = 0.02269533
Iteration 51, loss = 0.02255048
Iteration 52, loss = 0.02251574
Iteration 53, loss = 0.02252164
Iteration 54, loss = 0.02262414
Iteration 55, loss = 0.02227799
Iteration 56, loss = 0.02249743
Iteration 57, loss = 0.02243017
Iteration 58, loss = 0.02220723
Iteration 59, loss = 0.02229847
Iteration 60, loss = 0.02213198
Iteration 61, loss = 0.02216993
Iteration 62, loss = 0.02196446
Iteration 63, loss = 0.02190466
Iteration 64, loss = 0.02203342
Iteration 65, loss = 0.02193854
Iteration 66, loss = 0.02177320
Iteration 67, loss = 0.02207178
Iteration 68, loss = 0.02171118
Iteration 69, loss = 0.02192084
Iteration 70, loss = 0.02157800
Iteration 71, loss = 0.02167495
Iteration 72, loss = 0.02174262
Iteration 73, loss = 0.02163539
Iteration 74, loss = 0.02157980
Iteration 75, loss = 0.02147791
Iteration 76, loss = 0.02148970
Iteration 77, loss = 0.02172093
Iteration 78, loss = 0.02139659
Iteration 79, loss = 0.02142087
Iteration 80, loss = 0.02140139
Iteration 81, loss = 0.02141742
Iteration 82, loss = 0.02121863
Iteration 83, loss = 0.02133361
Iteration 84, loss = 0.02157135
Iteration 85, loss = 0.02129171
Iteration 86, loss = 0.02118559
Iteration 87, loss = 0.02136830
Iteration 88, loss = 0.02157312
Iteration 89, loss = 0.02105237
Iteration 90, loss = 0.02145224
Iteration 91, loss = 0.02139344
Iteration 92, loss = 0.02113279
Iteration 93, loss = 0.02112261
Iteration 94, loss = 0.02109799
Iteration 95, loss = 0.02154563
Iteration 96, loss = 0.02135130
Iteration 97, loss = 0.02113732
Iteration 98, loss = 0.02096416
Iteration 99, loss = 0.02093224
Iteration 100, loss = 0.02121938
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71833177
Iteration 2, loss = 0.65580292
Iteration 3, loss = 0.55646164
Iteration 4, loss = 0.43861290
Iteration 5, loss = 0.33251760
Iteration 6, loss = 0.24994684
Iteration 7, loss = 0.18985323
Iteration 8, loss = 0.14788196
Iteration 9, loss = 0.11896982
Iteration 10, loss = 0.09839127
Iteration 11, loss = 0.08389783
Iteration 12, loss = 0.07271211
Iteration 13, loss = 0.06438933
Iteration 14, loss = 0.05814150
Iteration 15, loss = 0.05250691
Iteration 16, loss = 0.04878223
Iteration 17, loss = 0.04509925
Iteration 18, loss = 0.04214450
Iteration 19, loss = 0.03985235
Iteration 20, loss = 0.03780419
Iteration 21, loss = 0.03612292
Iteration 22, loss = 0.03453342
Iteration 23, loss = 0.03338868
Iteration 24, loss = 0.03237810
Iteration 25, loss = 0.03128482
Iteration 26, loss = 0.03039589
Iteration 27, loss = 0.02988055
Iteration 28, loss = 0.02888475
Iteration 29, loss = 0.02847669
Iteration 30, loss = 0.02772662
Iteration 31, loss = 0.02728270
Iteration 32, loss = 0.02681188
Iteration 33, loss = 0.02662336
Iteration 34, loss = 0.02625174
Iteration 35, loss = 0.02550222
Iteration 36, loss = 0.02545399
Iteration 37, loss = 0.02525044
Iteration 38, loss = 0.02483540
Iteration 39, loss = 0.02463672
Iteration 40, loss = 0.02460153
Iteration 41, loss = 0.02422775
Iteration 42, loss = 0.02398078
Iteration 43, loss = 0.02390449
Iteration 44, loss = 0.02373593
Iteration 45, loss = 0.02347569
Iteration 46, loss = 0.02356136
Iteration 47, loss = 0.02341875
Iteration 48, loss = 0.02327137
Iteration 49, loss = 0.02316549
Iteration 50, loss = 0.02280447
Iteration 51, loss = 0.02283596
Iteration 52, loss = 0.02290367
Iteration 53, loss = 0.02261693
Iteration 54, loss = 0.02259324
Iteration 55, loss = 0.02242722
Iteration 56, loss = 0.02246740
Iteration 57, loss = 0.02240077
Iteration 58, loss = 0.02234744
Iteration 59, loss = 0.02211607
Iteration 60, loss = 0.02215475
Iteration 61, loss = 0.02205561
Iteration 62, loss = 0.02229426
Iteration 63, loss = 0.02186034
Iteration 64, loss = 0.02180090
Iteration 65, loss = 0.02202792
Iteration 66, loss = 0.02168157
Iteration 67, loss = 0.02187154
Iteration 68, loss = 0.02171831
Iteration 69, loss = 0.02169409
Iteration 70, loss = 0.02170534
Iteration 71, loss = 0.02158997
Iteration 72, loss = 0.02168940
Iteration 73, loss = 0.02174117
Iteration 74, loss = 0.02167538
Iteration 75, loss = 0.02163673
Iteration 76, loss = 0.02150253
Iteration 77, loss = 0.02142653
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68413076
Iteration 2, loss = 0.61808516
Iteration 3, loss = 0.51384550
Iteration 4, loss = 0.39953174
Iteration 5, loss = 0.29958660
Iteration 6, loss = 0.22443616
Iteration 7, loss = 0.17135803
Iteration 8, loss = 0.13480374
Iteration 9, loss = 0.10963786
Iteration 10, loss = 0.09205319
Iteration 11, loss = 0.07907471
Iteration 12, loss = 0.06932105
Iteration 13, loss = 0.06221011
Iteration 14, loss = 0.05642601
Iteration 15, loss = 0.05171795
Iteration 16, loss = 0.04771086
Iteration 17, loss = 0.04492750
Iteration 18, loss = 0.04235122
Iteration 19, loss = 0.04022050
Iteration 20, loss = 0.03866780
Iteration 21, loss = 0.03688821
Iteration 22, loss = 0.03553907
Iteration 23, loss = 0.03455811
Iteration 24, loss = 0.03317658
Iteration 25, loss = 0.03267333
Iteration 26, loss = 0.03147139
Iteration 27, loss = 0.03093038
Iteration 28, loss = 0.03021259
Iteration 29, loss = 0.02984408
Iteration 30, loss = 0.02936033
Iteration 31, loss = 0.02893804
Iteration 32, loss = 0.02899197
Iteration 33, loss = 0.02840530
Iteration 34, loss = 0.02798354
Iteration 35, loss = 0.02775464
Iteration 36, loss = 0.02736206
Iteration 37, loss = 0.02700085
Iteration 38, loss = 0.02659983
Iteration 39, loss = 0.02676792
Iteration 40, loss = 0.02627096
Iteration 41, loss = 0.02627902
Iteration 42, loss = 0.02614471
Iteration 43, loss = 0.02586519
Iteration 44, loss = 0.02582374
Iteration 45, loss = 0.02601019
Iteration 46, loss = 0.02536131
Iteration 47, loss = 0.02541268
Iteration 48, loss = 0.02566429
Iteration 49, loss = 0.02539103
Iteration 50, loss = 0.02502808
Iteration 51, loss = 0.02482679
Iteration 52, loss = 0.02476377
Iteration 53, loss = 0.02516314
Iteration 54, loss = 0.02477302
Iteration 55, loss = 0.02481308
Iteration 56, loss = 0.02509425
Iteration 57, loss = 0.02462482
Iteration 58, loss = 0.02467788
Iteration 59, loss = 0.02425647
Iteration 60, loss = 0.02426164
Iteration 61, loss = 0.02430673
Iteration 62, loss = 0.02420488
Iteration 63, loss = 0.02403265
Iteration 64, loss = 0.02443738
Iteration 65, loss = 0.02397600
Iteration 66, loss = 0.02443546
Iteration 67, loss = 0.02396654
Iteration 68, loss = 0.02389890
Iteration 69, loss = 0.02385129
Iteration 70, loss = 0.02383891
Iteration 71, loss = 0.02400848
Iteration 72, loss = 0.02368868
Iteration 73, loss = 0.02392049
Iteration 74, loss = 0.02390102
Iteration 75, loss = 0.02385124
Iteration 76, loss = 0.02382285
Iteration 77, loss = 0.02364604
Iteration 78, loss = 0.02394046
Iteration 79, loss = 0.02363261
Iteration 80, loss = 0.02394544
Iteration 81, loss = 0.02385290
Iteration 82, loss = 0.02344547
Iteration 83, loss = 0.02370467
Iteration 84, loss = 0.02347568
Iteration 85, loss = 0.02347882
Iteration 86, loss = 0.02341423
Iteration 87, loss = 0.02330067
Iteration 88, loss = 0.02332169
Iteration 89, loss = 0.02337406
Iteration 90, loss = 0.02313402
Iteration 91, loss = 0.02335541
Iteration 92, loss = 0.02345969
Iteration 93, loss = 0.02348148
Iteration 94, loss = 0.02319795
Iteration 95, loss = 0.02333483
Iteration 96, loss = 0.02332289
Iteration 97, loss = 0.02321844
Iteration 98, loss = 0.02328673
Iteration 99, loss = 0.02346247
Iteration 100, loss = 0.02335005
Iteration 101, loss = 0.02341258
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67550330
Iteration 2, loss = 0.60170797
Iteration 3, loss = 0.49313701
Iteration 4, loss = 0.37983918
Iteration 5, loss = 0.28443651
Iteration 6, loss = 0.21324415
Iteration 7, loss = 0.16372981
Iteration 8, loss = 0.13008520
Iteration 9, loss = 0.10660477
Iteration 10, loss = 0.08993104
Iteration 11, loss = 0.07783842
Iteration 12, loss = 0.06857303
Iteration 13, loss = 0.06162709
Iteration 14, loss = 0.05590142
Iteration 15, loss = 0.05175399
Iteration 16, loss = 0.04795879
Iteration 17, loss = 0.04508994
Iteration 18, loss = 0.04270718
Iteration 19, loss = 0.04077581
Iteration 20, loss = 0.03894332
Iteration 21, loss = 0.03747031
Iteration 22, loss = 0.03611345
Iteration 23, loss = 0.03492300
Iteration 24, loss = 0.03419141
Iteration 25, loss = 0.03327292
Iteration 26, loss = 0.03230723
Iteration 27, loss = 0.03153871
Iteration 28, loss = 0.03140444
Iteration 29, loss = 0.03085512
Iteration 30, loss = 0.03000770
Iteration 31, loss = 0.02969647
Iteration 32, loss = 0.02923129
Iteration 33, loss = 0.02916218
Iteration 34, loss = 0.02850342
Iteration 35, loss = 0.02823477
Iteration 36, loss = 0.02795034
Iteration 37, loss = 0.02782791
Iteration 38, loss = 0.02781304
Iteration 39, loss = 0.02734600
Iteration 40, loss = 0.02727316
Iteration 41, loss = 0.02705194
Iteration 42, loss = 0.02711177
Iteration 43, loss = 0.02650332
Iteration 44, loss = 0.02659962
Iteration 45, loss = 0.02639333
Iteration 46, loss = 0.02607664
Iteration 47, loss = 0.02624885
Iteration 48, loss = 0.02616865
Iteration 49, loss = 0.02571048
Iteration 50, loss = 0.02599937
Iteration 51, loss = 0.02570195
Iteration 52, loss = 0.02575608
Iteration 53, loss = 0.02569258
Iteration 54, loss = 0.02542766
Iteration 55, loss = 0.02522660
Iteration 56, loss = 0.02535863
Iteration 57, loss = 0.02505995
Iteration 58, loss = 0.02522607
Iteration 59, loss = 0.02535154
Iteration 60, loss = 0.02538213
Iteration 61, loss = 0.02499563
Iteration 62, loss = 0.02524340
Iteration 63, loss = 0.02497605
Iteration 64, loss = 0.02507365
Iteration 65, loss = 0.02511162
Iteration 66, loss = 0.02503434
Iteration 67, loss = 0.02520440
Iteration 68, loss = 0.02496843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69260109
Iteration 2, loss = 0.61790605
Iteration 3, loss = 0.50777993
Iteration 4, loss = 0.39010546
Iteration 5, loss = 0.29171583
Iteration 6, loss = 0.21776443
Iteration 7, loss = 0.16620487
Iteration 8, loss = 0.13092776
Iteration 9, loss = 0.10629097
Iteration 10, loss = 0.08890654
Iteration 11, loss = 0.07624650
Iteration 12, loss = 0.06680871
Iteration 13, loss = 0.05956012
Iteration 14, loss = 0.05362984
Iteration 15, loss = 0.04927363
Iteration 16, loss = 0.04529133
Iteration 17, loss = 0.04245956
Iteration 18, loss = 0.03983526
Iteration 19, loss = 0.03785160
Iteration 20, loss = 0.03631280
Iteration 21, loss = 0.03447118
Iteration 22, loss = 0.03342926
Iteration 23, loss = 0.03185982
Iteration 24, loss = 0.03102607
Iteration 25, loss = 0.03020590
Iteration 26, loss = 0.02944975
Iteration 27, loss = 0.02865123
Iteration 28, loss = 0.02806396
Iteration 29, loss = 0.02743727
Iteration 30, loss = 0.02716236
Iteration 31, loss = 0.02668105
Iteration 32, loss = 0.02635972
Iteration 33, loss = 0.02575510
Iteration 34, loss = 0.02542044
Iteration 35, loss = 0.02509207
Iteration 36, loss = 0.02488736
Iteration 37, loss = 0.02445617
Iteration 38, loss = 0.02439967
Iteration 39, loss = 0.02425283
Iteration 40, loss = 0.02393187
Iteration 41, loss = 0.02382984
Iteration 42, loss = 0.02370361
Iteration 43, loss = 0.02356742
Iteration 44, loss = 0.02332874
Iteration 45, loss = 0.02337160
Iteration 46, loss = 0.02299506
Iteration 47, loss = 0.02294622
Iteration 48, loss = 0.02297295
Iteration 49, loss = 0.02280642
Iteration 50, loss = 0.02254478
Iteration 51, loss = 0.02248339
Iteration 52, loss = 0.02244918
Iteration 53, loss = 0.02218288
Iteration 54, loss = 0.02212184
Iteration 55, loss = 0.02238548
Iteration 56, loss = 0.02233965
Iteration 57, loss = 0.02208707
Iteration 58, loss = 0.02208124
Iteration 59, loss = 0.02183687
Iteration 60, loss = 0.02204975
Iteration 61, loss = 0.02205745
Iteration 62, loss = 0.02179028
Iteration 63, loss = 0.02166570
Iteration 64, loss = 0.02177508
Iteration 65, loss = 0.02166955
Iteration 66, loss = 0.02171338
Iteration 67, loss = 0.02179062
Iteration 68, loss = 0.02147092
Iteration 69, loss = 0.02165957
Iteration 70, loss = 0.02144857
Iteration 71, loss = 0.02138122
Iteration 72, loss = 0.02157071
Iteration 73, loss = 0.02139394
Iteration 74, loss = 0.02126997
Iteration 75, loss = 0.02116893
Iteration 76, loss = 0.02129133
Iteration 77, loss = 0.02136148
Iteration 78, loss = 0.02110665
Iteration 79, loss = 0.02123970
Iteration 80, loss = 0.02125779
Iteration 81, loss = 0.02120034
Iteration 82, loss = 0.02104704
Iteration 83, loss = 0.02125497
Iteration 84, loss = 0.02096448
Iteration 85, loss = 0.02103696
Iteration 86, loss = 0.02119981
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68424005
Iteration 2, loss = 0.60406149
Iteration 3, loss = 0.49670596
Iteration 4, loss = 0.38451450
Iteration 5, loss = 0.28838567
Iteration 6, loss = 0.21602944
Iteration 7, loss = 0.16477694
Iteration 8, loss = 0.12980239
Iteration 9, loss = 0.10550603
Iteration 10, loss = 0.08840839
Iteration 11, loss = 0.07576046
Iteration 12, loss = 0.06647728
Iteration 13, loss = 0.05907842
Iteration 14, loss = 0.05343674
Iteration 15, loss = 0.04894668
Iteration 16, loss = 0.04520320
Iteration 17, loss = 0.04256285
Iteration 18, loss = 0.04006382
Iteration 19, loss = 0.03777499
Iteration 20, loss = 0.03607854
Iteration 21, loss = 0.03445402
Iteration 22, loss = 0.03354707
Iteration 23, loss = 0.03194946
Iteration 24, loss = 0.03110975
Iteration 25, loss = 0.03028158
Iteration 26, loss = 0.02939236
Iteration 27, loss = 0.02869222
Iteration 28, loss = 0.02809266
Iteration 29, loss = 0.02752520
Iteration 30, loss = 0.02723376
Iteration 31, loss = 0.02656610
Iteration 32, loss = 0.02635351
Iteration 33, loss = 0.02605514
Iteration 34, loss = 0.02560784
Iteration 35, loss = 0.02518664
Iteration 36, loss = 0.02497153
Iteration 37, loss = 0.02468598
Iteration 38, loss = 0.02460633
Iteration 39, loss = 0.02427669
Iteration 40, loss = 0.02401199
Iteration 41, loss = 0.02399491
Iteration 42, loss = 0.02380727
Iteration 43, loss = 0.02358210
Iteration 44, loss = 0.02344259
Iteration 45, loss = 0.02351392
Iteration 46, loss = 0.02329049
Iteration 47, loss = 0.02306195
Iteration 48, loss = 0.02319992
Iteration 49, loss = 0.02267445
Iteration 50, loss = 0.02269533
Iteration 51, loss = 0.02255048
Iteration 52, loss = 0.02251574
Iteration 53, loss = 0.02252164
Iteration 54, loss = 0.02262414
Iteration 55, loss = 0.02227799
Iteration 56, loss = 0.02249743
Iteration 57, loss = 0.02243017
Iteration 58, loss = 0.02220723
Iteration 59, loss = 0.02229847
Iteration 60, loss = 0.02213198
Iteration 61, loss = 0.02216993
Iteration 62, loss = 0.02196446
Iteration 63, loss = 0.02190466
Iteration 64, loss = 0.02203342
Iteration 65, loss = 0.02193854
Iteration 66, loss = 0.02177320
Iteration 67, loss = 0.02207178
Iteration 68, loss = 0.02171118
Iteration 69, loss = 0.02192084
Iteration 70, loss = 0.02157800
Iteration 71, loss = 0.02167495
Iteration 72, loss = 0.02174262
Iteration 73, loss = 0.02163539
Iteration 74, loss = 0.02157980
Iteration 75, loss = 0.02147791
Iteration 76, loss = 0.02148970
Iteration 77, loss = 0.02172093
Iteration 78, loss = 0.02139659
Iteration 79, loss = 0.02142087
Iteration 80, loss = 0.02140139
Iteration 81, loss = 0.02141742
Iteration 82, loss = 0.02121863
Iteration 83, loss = 0.02133361
Iteration 84, loss = 0.02157135
Iteration 85, loss = 0.02129171
Iteration 86, loss = 0.02118559
Iteration 87, loss = 0.02136830
Iteration 88, loss = 0.02157312
Iteration 89, loss = 0.02105237
Iteration 90, loss = 0.02145224
Iteration 91, loss = 0.02139344
Iteration 92, loss = 0.02113279
Iteration 93, loss = 0.02112261
Iteration 94, loss = 0.02109799
Iteration 95, loss = 0.02154563
Iteration 96, loss = 0.02135130
Iteration 97, loss = 0.02113732
Iteration 98, loss = 0.02096416
Iteration 99, loss = 0.02093224
Iteration 100, loss = 0.02121938
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71833177
Iteration 2, loss = 0.65580292
Iteration 3, loss = 0.55646164
Iteration 4, loss = 0.43861290
Iteration 5, loss = 0.33251760
Iteration 6, loss = 0.24994684
Iteration 7, loss = 0.18985323
Iteration 8, loss = 0.14788196
Iteration 9, loss = 0.11896982
Iteration 10, loss = 0.09839127
Iteration 11, loss = 0.08389783
Iteration 12, loss = 0.07271211
Iteration 13, loss = 0.06438933
Iteration 14, loss = 0.05814150
Iteration 15, loss = 0.05250691
Iteration 16, loss = 0.04878223
Iteration 17, loss = 0.04509925
Iteration 18, loss = 0.04214450
Iteration 19, loss = 0.03985235
Iteration 20, loss = 0.03780419
Iteration 21, loss = 0.03612292
Iteration 22, loss = 0.03453342
Iteration 23, loss = 0.03338868
Iteration 24, loss = 0.03237810
Iteration 25, loss = 0.03128482
Iteration 26, loss = 0.03039589
Iteration 27, loss = 0.02988055
Iteration 28, loss = 0.02888475
Iteration 29, loss = 0.02847669
Iteration 30, loss = 0.02772662
Iteration 31, loss = 0.02728270
Iteration 32, loss = 0.02681188
Iteration 33, loss = 0.02662336
Iteration 34, loss = 0.02625174
Iteration 35, loss = 0.02550222
Iteration 36, loss = 0.02545399
Iteration 37, loss = 0.02525044
Iteration 38, loss = 0.02483540
Iteration 39, loss = 0.02463672
Iteration 40, loss = 0.02460153
Iteration 41, loss = 0.02422775
Iteration 42, loss = 0.02398078
Iteration 43, loss = 0.02390449
Iteration 44, loss = 0.02373593
Iteration 45, loss = 0.02347569
Iteration 46, loss = 0.02356136
Iteration 47, loss = 0.02341875
Iteration 48, loss = 0.02327137
Iteration 49, loss = 0.02316549
Iteration 50, loss = 0.02280447
Iteration 51, loss = 0.02283596
Iteration 52, loss = 0.02290367
Iteration 53, loss = 0.02261693
Iteration 54, loss = 0.02259324
Iteration 55, loss = 0.02242722
Iteration 56, loss = 0.02246740
Iteration 57, loss = 0.02240077
Iteration 58, loss = 0.02234744
Iteration 59, loss = 0.02211607
Iteration 60, loss = 0.02215475
Iteration 61, loss = 0.02205561
Iteration 62, loss = 0.02229426
Iteration 63, loss = 0.02186034
Iteration 64, loss = 0.02180090
Iteration 65, loss = 0.02202792
Iteration 66, loss = 0.02168157
Iteration 67, loss = 0.02187154
Iteration 68, loss = 0.02171831
Iteration 69, loss = 0.02169409
Iteration 70, loss = 0.02170534
Iteration 71, loss = 0.02158997
Iteration 72, loss = 0.02168940
Iteration 73, loss = 0.02174117
Iteration 74, loss = 0.02167538
Iteration 75, loss = 0.02163673
Iteration 76, loss = 0.02150253
Iteration 77, loss = 0.02142653
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68413076
Iteration 2, loss = 0.61808516
Iteration 3, loss = 0.51384550
Iteration 4, loss = 0.39953174
Iteration 5, loss = 0.29958660
Iteration 6, loss = 0.22443616
Iteration 7, loss = 0.17135803
Iteration 8, loss = 0.13480374
Iteration 9, loss = 0.10963786
Iteration 10, loss = 0.09205319
Iteration 11, loss = 0.07907471
Iteration 12, loss = 0.06932105
Iteration 13, loss = 0.06221011
Iteration 14, loss = 0.05642601
Iteration 15, loss = 0.05171795
Iteration 16, loss = 0.04771086
Iteration 17, loss = 0.04492750
Iteration 18, loss = 0.04235122
Iteration 19, loss = 0.04022050
Iteration 20, loss = 0.03866780
Iteration 21, loss = 0.03688821
Iteration 22, loss = 0.03553907
Iteration 23, loss = 0.03455811
Iteration 24, loss = 0.03317658
Iteration 25, loss = 0.03267333
Iteration 26, loss = 0.03147139
Iteration 27, loss = 0.03093038
Iteration 28, loss = 0.03021259
Iteration 29, loss = 0.02984408
Iteration 30, loss = 0.02936033
Iteration 31, loss = 0.02893804
Iteration 32, loss = 0.02899197
Iteration 33, loss = 0.02840530
Iteration 34, loss = 0.02798354
Iteration 35, loss = 0.02775464
Iteration 36, loss = 0.02736206
Iteration 37, loss = 0.02700085
Iteration 38, loss = 0.02659983
Iteration 39, loss = 0.02676792
Iteration 40, loss = 0.02627096
Iteration 41, loss = 0.02627902
Iteration 42, loss = 0.02614471
Iteration 43, loss = 0.02586519
Iteration 44, loss = 0.02582374
Iteration 45, loss = 0.02601019
Iteration 46, loss = 0.02536131
Iteration 47, loss = 0.02541268
Iteration 48, loss = 0.02566429
Iteration 49, loss = 0.02539103
Iteration 50, loss = 0.02502808
Iteration 51, loss = 0.02482679
Iteration 52, loss = 0.02476377
Iteration 53, loss = 0.02516314
Iteration 54, loss = 0.02477302
Iteration 55, loss = 0.02481308
Iteration 56, loss = 0.02509425
Iteration 57, loss = 0.02462482
Iteration 58, loss = 0.02467788
Iteration 59, loss = 0.02425647
Iteration 60, loss = 0.02426164
Iteration 61, loss = 0.02430673
Iteration 62, loss = 0.02420488
Iteration 63, loss = 0.02403265
Iteration 64, loss = 0.02443738
Iteration 65, loss = 0.02397600
Iteration 66, loss = 0.02443546
Iteration 67, loss = 0.02396654
Iteration 68, loss = 0.02389890
Iteration 69, loss = 0.02385129
Iteration 70, loss = 0.02383891
Iteration 71, loss = 0.02400848
Iteration 72, loss = 0.02368868
Iteration 73, loss = 0.02392049
Iteration 74, loss = 0.02390102
Iteration 75, loss = 0.02385124
Iteration 76, loss = 0.02382285
Iteration 77, loss = 0.02364604
Iteration 78, loss = 0.02394046
Iteration 79, loss = 0.02363261
Iteration 80, loss = 0.02394544
Iteration 81, loss = 0.02385290
Iteration 82, loss = 0.02344547
Iteration 83, loss = 0.02370467
Iteration 84, loss = 0.02347568
Iteration 85, loss = 0.02347882
Iteration 86, loss = 0.02341423
Iteration 87, loss = 0.02330067
Iteration 88, loss = 0.02332169
Iteration 89, loss = 0.02337406
Iteration 90, loss = 0.02313402
Iteration 91, loss = 0.02335541
Iteration 92, loss = 0.02345969
Iteration 93, loss = 0.02348148
Iteration 94, loss = 0.02319795
Iteration 95, loss = 0.02333483
Iteration 96, loss = 0.02332289
Iteration 97, loss = 0.02321844
Iteration 98, loss = 0.02328673
Iteration 99, loss = 0.02346247
Iteration 100, loss = 0.02335005
Iteration 101, loss = 0.02341258
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67550330
Iteration 2, loss = 0.60170797
Iteration 3, loss = 0.49313701
Iteration 4, loss = 0.37983918
Iteration 5, loss = 0.28443651
Iteration 6, loss = 0.21324415
Iteration 7, loss = 0.16372981
Iteration 8, loss = 0.13008520
Iteration 9, loss = 0.10660477
Iteration 10, loss = 0.08993104
Iteration 11, loss = 0.07783842
Iteration 12, loss = 0.06857303
Iteration 13, loss = 0.06162709
Iteration 14, loss = 0.05590142
Iteration 15, loss = 0.05175399
Iteration 16, loss = 0.04795879
Iteration 17, loss = 0.04508994
Iteration 18, loss = 0.04270718
Iteration 19, loss = 0.04077581
Iteration 20, loss = 0.03894332
Iteration 21, loss = 0.03747031
Iteration 22, loss = 0.03611345
Iteration 23, loss = 0.03492300
Iteration 24, loss = 0.03419141
Iteration 25, loss = 0.03327292
Iteration 26, loss = 0.03230723
Iteration 27, loss = 0.03153871
Iteration 28, loss = 0.03140444
Iteration 29, loss = 0.03085512
Iteration 30, loss = 0.03000770
Iteration 31, loss = 0.02969647
Iteration 32, loss = 0.02923129
Iteration 33, loss = 0.02916218
Iteration 34, loss = 0.02850342
Iteration 35, loss = 0.02823477
Iteration 36, loss = 0.02795034
Iteration 37, loss = 0.02782791
Iteration 38, loss = 0.02781304
Iteration 39, loss = 0.02734600
Iteration 40, loss = 0.02727316
Iteration 41, loss = 0.02705194
Iteration 42, loss = 0.02711177
Iteration 43, loss = 0.02650332
Iteration 44, loss = 0.02659962
Iteration 45, loss = 0.02639333
Iteration 46, loss = 0.02607664
Iteration 47, loss = 0.02624885
Iteration 48, loss = 0.02616865
Iteration 49, loss = 0.02571048
Iteration 50, loss = 0.02599937
Iteration 51, loss = 0.02570195
Iteration 52, loss = 0.02575608
Iteration 53, loss = 0.02569258
Iteration 54, loss = 0.02542766
Iteration 55, loss = 0.02522660
Iteration 56, loss = 0.02535863
Iteration 57, loss = 0.02505995
Iteration 58, loss = 0.02522607
Iteration 59, loss = 0.02535154
Iteration 60, loss = 0.02538213
Iteration 61, loss = 0.02499563
Iteration 62, loss = 0.02524340
Iteration 63, loss = 0.02497605
Iteration 64, loss = 0.02507365
Iteration 65, loss = 0.02511162
Iteration 66, loss = 0.02503434
Iteration 67, loss = 0.02520440
Iteration 68, loss = 0.02496843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69260109
Iteration 2, loss = 0.61790605
Iteration 3, loss = 0.50777993
Iteration 4, loss = 0.39010546
Iteration 5, loss = 0.29171583
Iteration 6, loss = 0.21776443
Iteration 7, loss = 0.16620487
Iteration 8, loss = 0.13092776
Iteration 9, loss = 0.10629097
Iteration 10, loss = 0.08890654
Iteration 11, loss = 0.07624650
Iteration 12, loss = 0.06680871
Iteration 13, loss = 0.05956012
Iteration 14, loss = 0.05362984
Iteration 15, loss = 0.04927363
Iteration 16, loss = 0.04529133
Iteration 17, loss = 0.04245956
Iteration 18, loss = 0.03983526
Iteration 19, loss = 0.03785160
Iteration 20, loss = 0.03631280
Iteration 21, loss = 0.03447118
Iteration 22, loss = 0.03342926
Iteration 23, loss = 0.03185982
Iteration 24, loss = 0.03102607
Iteration 25, loss = 0.03020590
Iteration 26, loss = 0.02944975
Iteration 27, loss = 0.02865123
Iteration 28, loss = 0.02806396
Iteration 29, loss = 0.02743727
Iteration 30, loss = 0.02716236
Iteration 31, loss = 0.02668105
Iteration 32, loss = 0.02635972
Iteration 33, loss = 0.02575510
Iteration 34, loss = 0.02542044
Iteration 35, loss = 0.02509207
Iteration 36, loss = 0.02488736
Iteration 37, loss = 0.02445617
Iteration 38, loss = 0.02439967
Iteration 39, loss = 0.02425283
Iteration 40, loss = 0.02393187
Iteration 41, loss = 0.02382984
Iteration 42, loss = 0.02370361
Iteration 43, loss = 0.02356742
Iteration 44, loss = 0.02332874
Iteration 45, loss = 0.02337160
Iteration 46, loss = 0.02299506
Iteration 47, loss = 0.02294622
Iteration 48, loss = 0.02297295
Iteration 49, loss = 0.02280642
Iteration 50, loss = 0.02254478
Iteration 51, loss = 0.02248339
Iteration 52, loss = 0.02244918
Iteration 53, loss = 0.02218288
Iteration 54, loss = 0.02212184
Iteration 55, loss = 0.02238548
Iteration 56, loss = 0.02233965
Iteration 57, loss = 0.02208707
Iteration 58, loss = 0.02208124
Iteration 59, loss = 0.02183687
Iteration 60, loss = 0.02204975
Iteration 61, loss = 0.02205745
Iteration 62, loss = 0.02179028
Iteration 63, loss = 0.02166570
Iteration 64, loss = 0.02177508
Iteration 65, loss = 0.02166955
Iteration 66, loss = 0.02171338
Iteration 67, loss = 0.02179062
Iteration 68, loss = 0.02147092
Iteration 69, loss = 0.02165957
Iteration 70, loss = 0.02144857
Iteration 71, loss = 0.02138122
Iteration 72, loss = 0.02157071
Iteration 73, loss = 0.02139394
Iteration 74, loss = 0.02126997
Iteration 75, loss = 0.02116893
Iteration 76, loss = 0.02129133
Iteration 77, loss = 0.02136148
Iteration 78, loss = 0.02110665
Iteration 79, loss = 0.02123970
Iteration 80, loss = 0.02125779
Iteration 81, loss = 0.02120034
Iteration 82, loss = 0.02104704
Iteration 83, loss = 0.02125497
Iteration 84, loss = 0.02096448
Iteration 85, loss = 0.02103696
Iteration 86, loss = 0.02119981
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68424005
Iteration 2, loss = 0.60406149
Iteration 3, loss = 0.49670596
Iteration 4, loss = 0.38451450
Iteration 5, loss = 0.28838567
Iteration 6, loss = 0.21602944
Iteration 7, loss = 0.16477694
Iteration 8, loss = 0.12980239
Iteration 9, loss = 0.10550603
Iteration 10, loss = 0.08840839
Iteration 11, loss = 0.07576046
Iteration 12, loss = 0.06647728
Iteration 13, loss = 0.05907842
Iteration 14, loss = 0.05343674
Iteration 15, loss = 0.04894668
Iteration 16, loss = 0.04520320
Iteration 17, loss = 0.04256285
Iteration 18, loss = 0.04006382
Iteration 19, loss = 0.03777499
Iteration 20, loss = 0.03607854
Iteration 21, loss = 0.03445402
Iteration 22, loss = 0.03354707
Iteration 23, loss = 0.03194946
Iteration 24, loss = 0.03110975
Iteration 25, loss = 0.03028158
Iteration 26, loss = 0.02939236
Iteration 27, loss = 0.02869222
Iteration 28, loss = 0.02809266
Iteration 29, loss = 0.02752520
Iteration 30, loss = 0.02723376
Iteration 31, loss = 0.02656610
Iteration 32, loss = 0.02635351
Iteration 33, loss = 0.02605514
Iteration 34, loss = 0.02560784
Iteration 35, loss = 0.02518664
Iteration 36, loss = 0.02497153
Iteration 37, loss = 0.02468598
Iteration 38, loss = 0.02460633
Iteration 39, loss = 0.02427669
Iteration 40, loss = 0.02401199
Iteration 41, loss = 0.02399491
Iteration 42, loss = 0.02380727
Iteration 43, loss = 0.02358210
Iteration 44, loss = 0.02344259
Iteration 45, loss = 0.02351392
Iteration 46, loss = 0.02329049
Iteration 47, loss = 0.02306195
Iteration 48, loss = 0.02319992
Iteration 49, loss = 0.02267445
Iteration 50, loss = 0.02269533
Iteration 51, loss = 0.02255048
Iteration 52, loss = 0.02251574
Iteration 53, loss = 0.02252164
Iteration 54, loss = 0.02262414
Iteration 55, loss = 0.02227799
Iteration 56, loss = 0.02249743
Iteration 57, loss = 0.02243017
Iteration 58, loss = 0.02220723
Iteration 59, loss = 0.02229847
Iteration 60, loss = 0.02213198
Iteration 61, loss = 0.02216993
Iteration 62, loss = 0.02196446
Iteration 63, loss = 0.02190466
Iteration 64, loss = 0.02203342
Iteration 65, loss = 0.02193854
Iteration 66, loss = 0.02177320
Iteration 67, loss = 0.02207178
Iteration 68, loss = 0.02171118
Iteration 69, loss = 0.02192084
Iteration 70, loss = 0.02157800
Iteration 71, loss = 0.02167495
Iteration 72, loss = 0.02174262
Iteration 73, loss = 0.02163539
Iteration 74, loss = 0.02157980
Iteration 75, loss = 0.02147791
Iteration 76, loss = 0.02148970
Iteration 77, loss = 0.02172093
Iteration 78, loss = 0.02139659
Iteration 79, loss = 0.02142087
Iteration 80, loss = 0.02140139
Iteration 81, loss = 0.02141742
Iteration 82, loss = 0.02121863
Iteration 83, loss = 0.02133361
Iteration 84, loss = 0.02157135
Iteration 85, loss = 0.02129171
Iteration 86, loss = 0.02118559
Iteration 87, loss = 0.02136830
Iteration 88, loss = 0.02157312
Iteration 89, loss = 0.02105237
Iteration 90, loss = 0.02145224
Iteration 91, loss = 0.02139344
Iteration 92, loss = 0.02113279
Iteration 93, loss = 0.02112261
Iteration 94, loss = 0.02109799
Iteration 95, loss = 0.02154563
Iteration 96, loss = 0.02135130
Iteration 97, loss = 0.02113732
Iteration 98, loss = 0.02096416
Iteration 99, loss = 0.02093224
Iteration 100, loss = 0.02121938
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71833177
Iteration 2, loss = 0.65580292
Iteration 3, loss = 0.55646164
Iteration 4, loss = 0.43861290
Iteration 5, loss = 0.33251760
Iteration 6, loss = 0.24994684
Iteration 7, loss = 0.18985323
Iteration 8, loss = 0.14788196
Iteration 9, loss = 0.11896982
Iteration 10, loss = 0.09839127
Iteration 11, loss = 0.08389783
Iteration 12, loss = 0.07271211
Iteration 13, loss = 0.06438933
Iteration 14, loss = 0.05814150
Iteration 15, loss = 0.05250691
Iteration 16, loss = 0.04878223
Iteration 17, loss = 0.04509925
Iteration 18, loss = 0.04214450
Iteration 19, loss = 0.03985235
Iteration 20, loss = 0.03780419
Iteration 21, loss = 0.03612292
Iteration 22, loss = 0.03453342
Iteration 23, loss = 0.03338868
Iteration 24, loss = 0.03237810
Iteration 25, loss = 0.03128482
Iteration 26, loss = 0.03039589
Iteration 27, loss = 0.02988055
Iteration 28, loss = 0.02888475
Iteration 29, loss = 0.02847669
Iteration 30, loss = 0.02772662
Iteration 31, loss = 0.02728270
Iteration 32, loss = 0.02681188
Iteration 33, loss = 0.02662336
Iteration 34, loss = 0.02625174
Iteration 35, loss = 0.02550222
Iteration 36, loss = 0.02545399
Iteration 37, loss = 0.02525044
Iteration 38, loss = 0.02483540
Iteration 39, loss = 0.02463672
Iteration 40, loss = 0.02460153
Iteration 41, loss = 0.02422775
Iteration 42, loss = 0.02398078
Iteration 43, loss = 0.02390449
Iteration 44, loss = 0.02373593
Iteration 45, loss = 0.02347569
Iteration 46, loss = 0.02356136
Iteration 47, loss = 0.02341875
Iteration 48, loss = 0.02327137
Iteration 49, loss = 0.02316549
Iteration 50, loss = 0.02280447
Iteration 51, loss = 0.02283596
Iteration 52, loss = 0.02290367
Iteration 53, loss = 0.02261693
Iteration 54, loss = 0.02259324
Iteration 55, loss = 0.02242722
Iteration 56, loss = 0.02246740
Iteration 57, loss = 0.02240077
Iteration 58, loss = 0.02234744
Iteration 59, loss = 0.02211607
Iteration 60, loss = 0.02215475
Iteration 61, loss = 0.02205561
Iteration 62, loss = 0.02229426
Iteration 63, loss = 0.02186034
Iteration 64, loss = 0.02180090
Iteration 65, loss = 0.02202792
Iteration 66, loss = 0.02168157
Iteration 67, loss = 0.02187154
Iteration 68, loss = 0.02171831
Iteration 69, loss = 0.02169409
Iteration 70, loss = 0.02170534
Iteration 71, loss = 0.02158997
Iteration 72, loss = 0.02168940
Iteration 73, loss = 0.02174117
Iteration 74, loss = 0.02167538
Iteration 75, loss = 0.02163673
Iteration 76, loss = 0.02150253
Iteration 77, loss = 0.02142653
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.66941657
Iteration 2, loss = 0.57390903
Iteration 3, loss = 0.44668142
Iteration 4, loss = 0.32735578
Iteration 5, loss = 0.23579234
Iteration 6, loss = 0.17326203
Iteration 7, loss = 0.13274158
Iteration 8, loss = 0.10565128
Iteration 9, loss = 0.08757901
Iteration 10, loss = 0.07481066
Iteration 11, loss = 0.06572083
Iteration 12, loss = 0.05859737
Iteration 13, loss = 0.05344293
Iteration 14, loss = 0.04909004
Iteration 15, loss = 0.04559924
Iteration 16, loss = 0.04290220
Iteration 17, loss = 0.04063729
Iteration 18, loss = 0.03885739
Iteration 19, loss = 0.03723264
Iteration 20, loss = 0.03581345
Iteration 21, loss = 0.03499264
Iteration 22, loss = 0.03408682
Iteration 23, loss = 0.03318972
Iteration 24, loss = 0.03233166
Iteration 25, loss = 0.03171551
Iteration 26, loss = 0.03086042
Iteration 27, loss = 0.03042291
Iteration 28, loss = 0.03019164
Iteration 29, loss = 0.02951262
Iteration 30, loss = 0.02927541
Iteration 31, loss = 0.02893983
Iteration 32, loss = 0.02864029
Iteration 33, loss = 0.02871129
Iteration 34, loss = 0.02831770
Iteration 35, loss = 0.02819715
Iteration 36, loss = 0.02807468
Iteration 37, loss = 0.02765296
Iteration 38, loss = 0.02744859
Iteration 39, loss = 0.02755651
Iteration 40, loss = 0.02731442
Iteration 41, loss = 0.02699671
Iteration 42, loss = 0.02705856
Iteration 43, loss = 0.02673372
Iteration 44, loss = 0.02715392
Iteration 45, loss = 0.02710232
Iteration 46, loss = 0.02651551
Iteration 47, loss = 0.02674064
Iteration 48, loss = 0.02656283
Iteration 49, loss = 0.02616584
Iteration 50, loss = 0.02634238
Iteration 51, loss = 0.02580601
Iteration 52, loss = 0.02623520
Iteration 53, loss = 0.02613418
Iteration 54, loss = 0.02589963
Iteration 55, loss = 0.02599057
Iteration 56, loss = 0.02591495
Iteration 57, loss = 0.02576188
Iteration 58, loss = 0.02589868
Iteration 59, loss = 0.02595286
Iteration 60, loss = 0.02579806
Iteration 61, loss = 0.02537811
Iteration 62, loss = 0.02545895
Iteration 63, loss = 0.02564362
Iteration 64, loss = 0.02580679
Iteration 65, loss = 0.02559405
Iteration 66, loss = 0.02553130
Iteration 67, loss = 0.02564067
Iteration 68, loss = 0.02556925
Iteration 69, loss = 0.02491947
Iteration 70, loss = 0.02526592
Iteration 71, loss = 0.02537378
Iteration 72, loss = 0.02518694
Iteration 73, loss = 0.02523307
Iteration 74, loss = 0.02540003
Iteration 75, loss = 0.02513227
Iteration 76, loss = 0.02532295
Iteration 77, loss = 0.02512355
Iteration 78, loss = 0.02512620
Iteration 79, loss = 0.02515339
Iteration 80, loss = 0.02538383
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on dropped_v7_lowercase_words_only...
Iteration 1, loss = 0.68043459
Iteration 2, loss = 0.66047432
Iteration 3, loss = 0.64054925
Iteration 4, loss = 0.61751390
Iteration 5, loss = 0.59075952
Iteration 6, loss = 0.56027481
Iteration 7, loss = 0.52713205
Iteration 8, loss = 0.49200893
Iteration 9, loss = 0.45657448
Iteration 10, loss = 0.42127190
Iteration 11, loss = 0.38705143
Iteration 12, loss = 0.35415919
Iteration 13, loss = 0.32314076
Iteration 14, loss = 0.29412878
Iteration 15, loss = 0.26755271
Iteration 16, loss = 0.24297225
Iteration 17, loss = 0.22094832
Iteration 18, loss = 0.20082828
Iteration 19, loss = 0.18289001
Iteration 20, loss = 0.16669873
Iteration 21, loss = 0.15230977
Iteration 22, loss = 0.13954698
Iteration 23, loss = 0.12814070
Iteration 24, loss = 0.11804738
Iteration 25, loss = 0.10906597
Iteration 26, loss = 0.10102866
Iteration 27, loss = 0.09385555
Iteration 28, loss = 0.08743234
Iteration 29, loss = 0.08166326
Iteration 30, loss = 0.07649511
Iteration 31, loss = 0.07187057
Iteration 32, loss = 0.06770418
Iteration 33, loss = 0.06384611
Iteration 34, loss = 0.06039540
Iteration 35, loss = 0.05727132
Iteration 36, loss = 0.05440614
Iteration 37, loss = 0.05177186
Iteration 38, loss = 0.04940527
Iteration 39, loss = 0.04718824
Iteration 40, loss = 0.04516685
Iteration 41, loss = 0.04334305
Iteration 42, loss = 0.04157450
Iteration 43, loss = 0.04002369
Iteration 44, loss = 0.03852190
Iteration 45, loss = 0.03712410
Iteration 46, loss = 0.03588685
Iteration 47, loss = 0.03474417
Iteration 48, loss = 0.03357288
Iteration 49, loss = 0.03252918
Iteration 50, loss = 0.03154281
Iteration 51, loss = 0.03064214
Iteration 52, loss = 0.02981027
Iteration 53, loss = 0.02900022
Iteration 54, loss = 0.02826013
Iteration 55, loss = 0.02752495
Iteration 56, loss = 0.02687927
Iteration 57, loss = 0.02625192
Iteration 58, loss = 0.02564382
Iteration 59, loss = 0.02510413
Iteration 60, loss = 0.02451341
Iteration 61, loss = 0.02399352
Iteration 62, loss = 0.02354905
Iteration 63, loss = 0.02307250
Iteration 64, loss = 0.02273573
Iteration 65, loss = 0.02227853
Iteration 66, loss = 0.02189669
Iteration 67, loss = 0.02149950
Iteration 68, loss = 0.02107469
Iteration 69, loss = 0.02075471
Iteration 70, loss = 0.02042112
Iteration 71, loss = 0.02015915
Iteration 72, loss = 0.01979730
Iteration 73, loss = 0.01951800
Iteration 74, loss = 0.01929058
Iteration 75, loss = 0.01896934
Iteration 76, loss = 0.01877280
Iteration 77, loss = 0.01851584
Iteration 78, loss = 0.01828864
Iteration 79, loss = 0.01805686
Iteration 80, loss = 0.01782275
Iteration 81, loss = 0.01766734
Iteration 82, loss = 0.01751060
Iteration 83, loss = 0.01726127
Iteration 84, loss = 0.01707419
Iteration 85, loss = 0.01689133
Iteration 86, loss = 0.01674312
Iteration 87, loss = 0.01662581
Iteration 88, loss = 0.01640006
Iteration 89, loss = 0.01625772
Iteration 90, loss = 0.01614442
Iteration 91, loss = 0.01600950
Iteration 92, loss = 0.01586212
Iteration 93, loss = 0.01570611
Iteration 94, loss = 0.01557240
Iteration 95, loss = 0.01549071
Iteration 96, loss = 0.01537222
Iteration 97, loss = 0.01522061
Iteration 98, loss = 0.01512108
Iteration 99, loss = 0.01498551
Iteration 100, loss = 0.01491272
Iteration 101, loss = 0.01478298
Iteration 102, loss = 0.01473897
Iteration 103, loss = 0.01463015
Iteration 104, loss = 0.01460806
Iteration 105, loss = 0.01441082
Iteration 106, loss = 0.01432973
Iteration 107, loss = 0.01423872
Iteration 108, loss = 0.01417857
Iteration 109, loss = 0.01410924
Iteration 110, loss = 0.01401273
Iteration 111, loss = 0.01392470
Iteration 112, loss = 0.01386777
Iteration 113, loss = 0.01380268
Iteration 114, loss = 0.01373944
Iteration 115, loss = 0.01367303
Iteration 116, loss = 0.01354698
Iteration 117, loss = 0.01354598
Iteration 118, loss = 0.01344056
Iteration 119, loss = 0.01341346
Iteration 120, loss = 0.01332773
Iteration 121, loss = 0.01326591
Iteration 122, loss = 0.01323036
Iteration 123, loss = 0.01322134
Iteration 124, loss = 0.01311883
Iteration 125, loss = 0.01309832
Iteration 126, loss = 0.01304666
Iteration 127, loss = 0.01294828
Iteration 128, loss = 0.01295910
Iteration 129, loss = 0.01288852
Iteration 130, loss = 0.01293752
Iteration 131, loss = 0.01283135
Iteration 132, loss = 0.01277093
Iteration 133, loss = 0.01274470
Iteration 134, loss = 0.01269957
Iteration 135, loss = 0.01262224
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67773439
Iteration 2, loss = 0.63936998
Iteration 3, loss = 0.58744074
Iteration 4, loss = 0.52277160
Iteration 5, loss = 0.45405099
Iteration 6, loss = 0.38661010
Iteration 7, loss = 0.32572578
Iteration 8, loss = 0.27243209
Iteration 9, loss = 0.22814478
Iteration 10, loss = 0.19173907
Iteration 11, loss = 0.16217940
Iteration 12, loss = 0.13880615
Iteration 13, loss = 0.11989569
Iteration 14, loss = 0.10470658
Iteration 15, loss = 0.09229298
Iteration 16, loss = 0.08218533
Iteration 17, loss = 0.07389036
Iteration 18, loss = 0.06686343
Iteration 19, loss = 0.06112993
Iteration 20, loss = 0.05621940
Iteration 21, loss = 0.05187689
Iteration 22, loss = 0.04835356
Iteration 23, loss = 0.04515179
Iteration 24, loss = 0.04239841
Iteration 25, loss = 0.04010857
Iteration 26, loss = 0.03794662
Iteration 27, loss = 0.03610452
Iteration 28, loss = 0.03428226
Iteration 29, loss = 0.03288913
Iteration 30, loss = 0.03156717
Iteration 31, loss = 0.03027089
Iteration 32, loss = 0.02928333
Iteration 33, loss = 0.02820088
Iteration 34, loss = 0.02768448
Iteration 35, loss = 0.02667853
Iteration 36, loss = 0.02582864
Iteration 37, loss = 0.02521033
Iteration 38, loss = 0.02461207
Iteration 39, loss = 0.02403500
Iteration 40, loss = 0.02360588
Iteration 41, loss = 0.02306727
Iteration 42, loss = 0.02259376
Iteration 43, loss = 0.02228225
Iteration 44, loss = 0.02178829
Iteration 45, loss = 0.02137799
Iteration 46, loss = 0.02109374
Iteration 47, loss = 0.02071498
Iteration 48, loss = 0.02058718
Iteration 49, loss = 0.02012711
Iteration 50, loss = 0.02004445
Iteration 51, loss = 0.01972845
Iteration 52, loss = 0.01950492
Iteration 53, loss = 0.01932026
Iteration 54, loss = 0.01906768
Iteration 55, loss = 0.01900619
Iteration 56, loss = 0.01868961
Iteration 57, loss = 0.01869828
Iteration 58, loss = 0.01838151
Iteration 59, loss = 0.01819993
Iteration 60, loss = 0.01815042
Iteration 61, loss = 0.01785745
Iteration 62, loss = 0.01785355
Iteration 63, loss = 0.01774891
Iteration 64, loss = 0.01766070
Iteration 65, loss = 0.01741848
Iteration 66, loss = 0.01741180
Iteration 67, loss = 0.01714456
Iteration 68, loss = 0.01721705
Iteration 69, loss = 0.01706359
Iteration 70, loss = 0.01701940
Iteration 71, loss = 0.01675785
Iteration 72, loss = 0.01687837
Iteration 73, loss = 0.01678339
Iteration 74, loss = 0.01671401
Iteration 75, loss = 0.01657040
Iteration 76, loss = 0.01639360
Iteration 77, loss = 0.01651179
Iteration 78, loss = 0.01628901
Iteration 79, loss = 0.01635794
Iteration 80, loss = 0.01630908
Iteration 81, loss = 0.01622876
Iteration 82, loss = 0.01609876
Iteration 83, loss = 0.01645783
Iteration 84, loss = 0.01607457
Iteration 85, loss = 0.01612906
Iteration 86, loss = 0.01588628
Iteration 87, loss = 0.01592901
Iteration 88, loss = 0.01590238
Iteration 89, loss = 0.01583611
Iteration 90, loss = 0.01582853
Iteration 91, loss = 0.01585651
Iteration 92, loss = 0.01560528
Iteration 93, loss = 0.01561434
Iteration 94, loss = 0.01570279
Iteration 95, loss = 0.01552643
Iteration 96, loss = 0.01566129
Iteration 97, loss = 0.01559465
Iteration 98, loss = 0.01557716
Iteration 99, loss = 0.01546195
Iteration 100, loss = 0.01545203
Iteration 101, loss = 0.01553346
Iteration 102, loss = 0.01542703
Iteration 103, loss = 0.01545370
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70679531
Iteration 2, loss = 0.68900594
Iteration 3, loss = 0.67498466
Iteration 4, loss = 0.66191401
Iteration 5, loss = 0.64875117
Iteration 6, loss = 0.63482748
Iteration 7, loss = 0.61967858
Iteration 8, loss = 0.60352246
Iteration 9, loss = 0.58603088
Iteration 10, loss = 0.56741518
Iteration 11, loss = 0.54791443
Iteration 12, loss = 0.52738029
Iteration 13, loss = 0.50601019
Iteration 14, loss = 0.48413459
Iteration 15, loss = 0.46206374
Iteration 16, loss = 0.43996088
Iteration 17, loss = 0.41792423
Iteration 18, loss = 0.39619627
Iteration 19, loss = 0.37502878
Iteration 20, loss = 0.35450691
Iteration 21, loss = 0.33474036
Iteration 22, loss = 0.31572177
Iteration 23, loss = 0.29751124
Iteration 24, loss = 0.28009929
Iteration 25, loss = 0.26363468
Iteration 26, loss = 0.24812539
Iteration 27, loss = 0.23349701
Iteration 28, loss = 0.21967733
Iteration 29, loss = 0.20679246
Iteration 30, loss = 0.19473181
Iteration 31, loss = 0.18339525
Iteration 32, loss = 0.17289581
Iteration 33, loss = 0.16311043
Iteration 34, loss = 0.15403810
Iteration 35, loss = 0.14554234
Iteration 36, loss = 0.13772087
Iteration 37, loss = 0.13036703
Iteration 38, loss = 0.12360352
Iteration 39, loss = 0.11724488
Iteration 40, loss = 0.11144664
Iteration 41, loss = 0.10595492
Iteration 42, loss = 0.10093627
Iteration 43, loss = 0.09617113
Iteration 44, loss = 0.09177992
Iteration 45, loss = 0.08772403
Iteration 46, loss = 0.08393506
Iteration 47, loss = 0.08033037
Iteration 48, loss = 0.07698447
Iteration 49, loss = 0.07387216
Iteration 50, loss = 0.07092302
Iteration 51, loss = 0.06819367
Iteration 52, loss = 0.06561900
Iteration 53, loss = 0.06321063
Iteration 54, loss = 0.06088975
Iteration 55, loss = 0.05876679
Iteration 56, loss = 0.05675089
Iteration 57, loss = 0.05483146
Iteration 58, loss = 0.05301797
Iteration 59, loss = 0.05130501
Iteration 60, loss = 0.04971570
Iteration 61, loss = 0.04816587
Iteration 62, loss = 0.04673728
Iteration 63, loss = 0.04534826
Iteration 64, loss = 0.04407005
Iteration 65, loss = 0.04282460
Iteration 66, loss = 0.04164929
Iteration 67, loss = 0.04049835
Iteration 68, loss = 0.03945585
Iteration 69, loss = 0.03841628
Iteration 70, loss = 0.03746189
Iteration 71, loss = 0.03652066
Iteration 72, loss = 0.03565130
Iteration 73, loss = 0.03475566
Iteration 74, loss = 0.03394788
Iteration 75, loss = 0.03315733
Iteration 76, loss = 0.03241999
Iteration 77, loss = 0.03173090
Iteration 78, loss = 0.03108284
Iteration 79, loss = 0.03042979
Iteration 80, loss = 0.02981027
Iteration 81, loss = 0.02919215
Iteration 82, loss = 0.02860734
Iteration 83, loss = 0.02804806
Iteration 84, loss = 0.02748236
Iteration 85, loss = 0.02696516
Iteration 86, loss = 0.02649317
Iteration 87, loss = 0.02594135
Iteration 88, loss = 0.02555980
Iteration 89, loss = 0.02506543
Iteration 90, loss = 0.02462607
Iteration 91, loss = 0.02420491
Iteration 92, loss = 0.02382252
Iteration 93, loss = 0.02341563
Iteration 94, loss = 0.02306451
Iteration 95, loss = 0.02270441
Iteration 96, loss = 0.02234817
Iteration 97, loss = 0.02201535
Iteration 98, loss = 0.02167655
Iteration 99, loss = 0.02136763
Iteration 100, loss = 0.02104936
Iteration 101, loss = 0.02078161
Iteration 102, loss = 0.02045293
Iteration 103, loss = 0.02015718
Iteration 104, loss = 0.01991220
Iteration 105, loss = 0.01961643
Iteration 106, loss = 0.01938523
Iteration 107, loss = 0.01912898
Iteration 108, loss = 0.01887013
Iteration 109, loss = 0.01865728
Iteration 110, loss = 0.01843654
Iteration 111, loss = 0.01818291
Iteration 112, loss = 0.01799608
Iteration 113, loss = 0.01777458
Iteration 114, loss = 0.01759544
Iteration 115, loss = 0.01739055
Iteration 116, loss = 0.01719983
Iteration 117, loss = 0.01706023
Iteration 118, loss = 0.01686697
Iteration 119, loss = 0.01669326
Iteration 120, loss = 0.01652261
Iteration 121, loss = 0.01634580
Iteration 122, loss = 0.01618524
Iteration 123, loss = 0.01601962
Iteration 124, loss = 0.01586819
Iteration 125, loss = 0.01571731
Iteration 126, loss = 0.01555221
Iteration 127, loss = 0.01546927
Iteration 128, loss = 0.01527179
Iteration 129, loss = 0.01515165
Iteration 130, loss = 0.01501355
Iteration 131, loss = 0.01489339
Iteration 132, loss = 0.01476951
Iteration 133, loss = 0.01464352
Iteration 134, loss = 0.01450849
Iteration 135, loss = 0.01438435
Iteration 136, loss = 0.01427820
Iteration 137, loss = 0.01416431
Iteration 138, loss = 0.01405791
Iteration 139, loss = 0.01395110
Iteration 140, loss = 0.01384056
Iteration 141, loss = 0.01376779
Iteration 142, loss = 0.01365792
Iteration 143, loss = 0.01354148
Iteration 144, loss = 0.01344847
Iteration 145, loss = 0.01337665
Iteration 146, loss = 0.01328896
Iteration 147, loss = 0.01318626
Iteration 148, loss = 0.01309272
Iteration 149, loss = 0.01299198
Iteration 150, loss = 0.01294028
Iteration 151, loss = 0.01282195
Iteration 152, loss = 0.01273596
Iteration 153, loss = 0.01268296
Iteration 154, loss = 0.01262135
Iteration 155, loss = 0.01252989
Iteration 156, loss = 0.01244030
Iteration 157, loss = 0.01234956
Iteration 158, loss = 0.01229030
Iteration 159, loss = 0.01223745
Iteration 160, loss = 0.01216113
Iteration 161, loss = 0.01218065
Iteration 162, loss = 0.01203359
Iteration 163, loss = 0.01194477
Iteration 164, loss = 0.01190661
Iteration 165, loss = 0.01182759
Iteration 166, loss = 0.01175463
Iteration 167, loss = 0.01169863
Iteration 168, loss = 0.01166883
Iteration 169, loss = 0.01162168
Iteration 170, loss = 0.01152463
Iteration 171, loss = 0.01146759
Iteration 172, loss = 0.01142710
Iteration 173, loss = 0.01137555
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.66382509
Iteration 2, loss = 0.64744207
Iteration 3, loss = 0.63323274
Iteration 4, loss = 0.61897398
Iteration 5, loss = 0.60397101
Iteration 6, loss = 0.58812120
Iteration 7, loss = 0.57081836
Iteration 8, loss = 0.55243841
Iteration 9, loss = 0.53296096
Iteration 10, loss = 0.51231239
Iteration 11, loss = 0.49098094
Iteration 12, loss = 0.46890831
Iteration 13, loss = 0.44651626
Iteration 14, loss = 0.42386928
Iteration 15, loss = 0.40150761
Iteration 16, loss = 0.37912180
Iteration 17, loss = 0.35727602
Iteration 18, loss = 0.33591791
Iteration 19, loss = 0.31563802
Iteration 20, loss = 0.29608682
Iteration 21, loss = 0.27762593
Iteration 22, loss = 0.26013222
Iteration 23, loss = 0.24360223
Iteration 24, loss = 0.22820347
Iteration 25, loss = 0.21379342
Iteration 26, loss = 0.20029282
Iteration 27, loss = 0.18777246
Iteration 28, loss = 0.17615789
Iteration 29, loss = 0.16539337
Iteration 30, loss = 0.15544083
Iteration 31, loss = 0.14617379
Iteration 32, loss = 0.13771671
Iteration 33, loss = 0.12981867
Iteration 34, loss = 0.12246044
Iteration 35, loss = 0.11573600
Iteration 36, loss = 0.10944651
Iteration 37, loss = 0.10365250
Iteration 38, loss = 0.09830861
Iteration 39, loss = 0.09329368
Iteration 40, loss = 0.08871921
Iteration 41, loss = 0.08443723
Iteration 42, loss = 0.08049371
Iteration 43, loss = 0.07680361
Iteration 44, loss = 0.07332720
Iteration 45, loss = 0.07015698
Iteration 46, loss = 0.06711485
Iteration 47, loss = 0.06432659
Iteration 48, loss = 0.06170098
Iteration 49, loss = 0.05926342
Iteration 50, loss = 0.05696809
Iteration 51, loss = 0.05476819
Iteration 52, loss = 0.05273284
Iteration 53, loss = 0.05079341
Iteration 54, loss = 0.04898145
Iteration 55, loss = 0.04729878
Iteration 56, loss = 0.04564955
Iteration 57, loss = 0.04414263
Iteration 58, loss = 0.04271759
Iteration 59, loss = 0.04133187
Iteration 60, loss = 0.04004504
Iteration 61, loss = 0.03883209
Iteration 62, loss = 0.03765630
Iteration 63, loss = 0.03655108
Iteration 64, loss = 0.03549260
Iteration 65, loss = 0.03449127
Iteration 66, loss = 0.03350899
Iteration 67, loss = 0.03257978
Iteration 68, loss = 0.03173264
Iteration 69, loss = 0.03092559
Iteration 70, loss = 0.03013935
Iteration 71, loss = 0.02937751
Iteration 72, loss = 0.02865882
Iteration 73, loss = 0.02796231
Iteration 74, loss = 0.02730340
Iteration 75, loss = 0.02665031
Iteration 76, loss = 0.02602231
Iteration 77, loss = 0.02543833
Iteration 78, loss = 0.02486405
Iteration 79, loss = 0.02430435
Iteration 80, loss = 0.02377537
Iteration 81, loss = 0.02324984
Iteration 82, loss = 0.02275696
Iteration 83, loss = 0.02228285
Iteration 84, loss = 0.02183968
Iteration 85, loss = 0.02138894
Iteration 86, loss = 0.02096217
Iteration 87, loss = 0.02055467
Iteration 88, loss = 0.02016495
Iteration 89, loss = 0.01978897
Iteration 90, loss = 0.01944315
Iteration 91, loss = 0.01906331
Iteration 92, loss = 0.01871312
Iteration 93, loss = 0.01839286
Iteration 94, loss = 0.01807502
Iteration 95, loss = 0.01776485
Iteration 96, loss = 0.01746166
Iteration 97, loss = 0.01717969
Iteration 98, loss = 0.01690054
Iteration 99, loss = 0.01661610
Iteration 100, loss = 0.01633880
Iteration 101, loss = 0.01609765
Iteration 102, loss = 0.01583469
Iteration 103, loss = 0.01558713
Iteration 104, loss = 0.01534920
Iteration 105, loss = 0.01511038
Iteration 106, loss = 0.01491707
Iteration 107, loss = 0.01470181
Iteration 108, loss = 0.01447208
Iteration 109, loss = 0.01427659
Iteration 110, loss = 0.01407793
Iteration 111, loss = 0.01387387
Iteration 112, loss = 0.01370120
Iteration 113, loss = 0.01351135
Iteration 114, loss = 0.01334213
Iteration 115, loss = 0.01317981
Iteration 116, loss = 0.01301213
Iteration 117, loss = 0.01285019
Iteration 118, loss = 0.01273628
Iteration 119, loss = 0.01256895
Iteration 120, loss = 0.01241966
Iteration 121, loss = 0.01228894
Iteration 122, loss = 0.01213527
Iteration 123, loss = 0.01199665
Iteration 124, loss = 0.01188188
Iteration 125, loss = 0.01172622
Iteration 126, loss = 0.01159936
Iteration 127, loss = 0.01148601
Iteration 128, loss = 0.01135141
Iteration 129, loss = 0.01124928
Iteration 130, loss = 0.01111534
Iteration 131, loss = 0.01097899
Iteration 132, loss = 0.01083534
Iteration 133, loss = 0.01068808
Iteration 134, loss = 0.01062149
Iteration 135, loss = 0.01055477
Iteration 136, loss = 0.01042092
Iteration 137, loss = 0.01032343
Iteration 138, loss = 0.01022825
Iteration 139, loss = 0.01013502
Iteration 140, loss = 0.01005932
Iteration 141, loss = 0.00994373
Iteration 142, loss = 0.00985469
Iteration 143, loss = 0.00976897
Iteration 144, loss = 0.00967958
Iteration 145, loss = 0.00960035
Iteration 146, loss = 0.00951717
Iteration 147, loss = 0.00944372
Iteration 148, loss = 0.00934956
Iteration 149, loss = 0.00927293
Iteration 150, loss = 0.00920299
Iteration 151, loss = 0.00911775
Iteration 152, loss = 0.00905585
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67677513
Iteration 2, loss = 0.64677363
Iteration 3, loss = 0.61295224
Iteration 4, loss = 0.57118447
Iteration 5, loss = 0.52328492
Iteration 6, loss = 0.47309016
Iteration 7, loss = 0.42332648
Iteration 8, loss = 0.37594489
Iteration 9, loss = 0.33204475
Iteration 10, loss = 0.29248363
Iteration 11, loss = 0.25740770
Iteration 12, loss = 0.22673241
Iteration 13, loss = 0.20017918
Iteration 14, loss = 0.17728973
Iteration 15, loss = 0.15790624
Iteration 16, loss = 0.14119792
Iteration 17, loss = 0.12698108
Iteration 18, loss = 0.11500337
Iteration 19, loss = 0.10440291
Iteration 20, loss = 0.09553422
Iteration 21, loss = 0.08776201
Iteration 22, loss = 0.08091489
Iteration 23, loss = 0.07494658
Iteration 24, loss = 0.06973520
Iteration 25, loss = 0.06503634
Iteration 26, loss = 0.06100232
Iteration 27, loss = 0.05743284
Iteration 28, loss = 0.05420150
Iteration 29, loss = 0.05131240
Iteration 30, loss = 0.04871996
Iteration 31, loss = 0.04635950
Iteration 32, loss = 0.04428713
Iteration 33, loss = 0.04241416
Iteration 34, loss = 0.04076999
Iteration 35, loss = 0.03910740
Iteration 36, loss = 0.03747319
Iteration 37, loss = 0.03610216
Iteration 38, loss = 0.03494951
Iteration 39, loss = 0.03382112
Iteration 40, loss = 0.03281516
Iteration 41, loss = 0.03179269
Iteration 42, loss = 0.03080513
Iteration 43, loss = 0.03003078
Iteration 44, loss = 0.02927722
Iteration 45, loss = 0.02848704
Iteration 46, loss = 0.02778487
Iteration 47, loss = 0.02716489
Iteration 48, loss = 0.02649157
Iteration 49, loss = 0.02591060
Iteration 50, loss = 0.02557648
Iteration 51, loss = 0.02510021
Iteration 52, loss = 0.02458288
Iteration 53, loss = 0.02407712
Iteration 54, loss = 0.02376391
Iteration 55, loss = 0.02327686
Iteration 56, loss = 0.02279829
Iteration 57, loss = 0.02280569
Iteration 58, loss = 0.02256972
Iteration 59, loss = 0.02223452
Iteration 60, loss = 0.02183218
Iteration 61, loss = 0.02145823
Iteration 62, loss = 0.02095724
Iteration 63, loss = 0.02082515
Iteration 64, loss = 0.02056011
Iteration 65, loss = 0.02028398
Iteration 66, loss = 0.02004878
Iteration 67, loss = 0.01983004
Iteration 68, loss = 0.01966310
Iteration 69, loss = 0.01945103
Iteration 70, loss = 0.01928985
Iteration 71, loss = 0.01907106
Iteration 72, loss = 0.01893992
Iteration 73, loss = 0.01867521
Iteration 74, loss = 0.01867421
Iteration 75, loss = 0.01881241
Iteration 76, loss = 0.01858403
Iteration 77, loss = 0.01833708
Iteration 78, loss = 0.01812365
Iteration 79, loss = 0.01792746
Iteration 80, loss = 0.01775922
Iteration 81, loss = 0.01766253
Iteration 82, loss = 0.01752478
Iteration 83, loss = 0.01739466
Iteration 84, loss = 0.01729521
Iteration 85, loss = 0.01717041
Iteration 86, loss = 0.01711397
Iteration 87, loss = 0.01698537
Iteration 88, loss = 0.01686904
Iteration 89, loss = 0.01675602
Iteration 90, loss = 0.01676520
Iteration 91, loss = 0.01673651
Iteration 92, loss = 0.01669932
Iteration 93, loss = 0.01665469
Iteration 94, loss = 0.01655070
Iteration 95, loss = 0.01654793
Iteration 96, loss = 0.01648724
Iteration 97, loss = 0.01620538
Iteration 98, loss = 0.01636144
Iteration 99, loss = 0.01678148
Iteration 100, loss = 0.01678342
Iteration 101, loss = 0.01658051
Iteration 102, loss = 0.01635267
Iteration 103, loss = 0.01612072
Iteration 104, loss = 0.01608668
Iteration 105, loss = 0.01593037
Iteration 106, loss = 0.01603032
Iteration 107, loss = 0.01596489
Iteration 108, loss = 0.01585654
Iteration 109, loss = 0.01601058
Iteration 110, loss = 0.01613496
Iteration 111, loss = 0.01586131
Iteration 112, loss = 0.01571687
Iteration 113, loss = 0.01559390
Iteration 114, loss = 0.01547174
Iteration 115, loss = 0.01547753
Iteration 116, loss = 0.01533196
Iteration 117, loss = 0.01527640
Iteration 118, loss = 0.01526320
Iteration 119, loss = 0.01520025
Iteration 120, loss = 0.01515328
Iteration 121, loss = 0.01511134
Iteration 122, loss = 0.01502828
Iteration 123, loss = 0.01509417
Iteration 124, loss = 0.01500729
Iteration 125, loss = 0.01525969
Iteration 126, loss = 0.01560953
Iteration 127, loss = 0.01588562
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67635212
Iteration 2, loss = 0.66184908
Iteration 3, loss = 0.64985704
Iteration 4, loss = 0.63822371
Iteration 5, loss = 0.62621089
Iteration 6, loss = 0.61313454
Iteration 7, loss = 0.59889721
Iteration 8, loss = 0.58331420
Iteration 9, loss = 0.56626478
Iteration 10, loss = 0.54784627
Iteration 11, loss = 0.52783143
Iteration 12, loss = 0.50661810
Iteration 13, loss = 0.48423584
Iteration 14, loss = 0.46116805
Iteration 15, loss = 0.43769990
Iteration 16, loss = 0.41417643
Iteration 17, loss = 0.39074161
Iteration 18, loss = 0.36806073
Iteration 19, loss = 0.34597721
Iteration 20, loss = 0.32479350
Iteration 21, loss = 0.30448446
Iteration 22, loss = 0.28536591
Iteration 23, loss = 0.26720580
Iteration 24, loss = 0.25020164
Iteration 25, loss = 0.23415281
Iteration 26, loss = 0.21932862
Iteration 27, loss = 0.20535684
Iteration 28, loss = 0.19245893
Iteration 29, loss = 0.18044253
Iteration 30, loss = 0.16927281
Iteration 31, loss = 0.15908139
Iteration 32, loss = 0.14950889
Iteration 33, loss = 0.14072177
Iteration 34, loss = 0.13263559
Iteration 35, loss = 0.12516531
Iteration 36, loss = 0.11824574
Iteration 37, loss = 0.11185660
Iteration 38, loss = 0.10595973
Iteration 39, loss = 0.10051485
Iteration 40, loss = 0.09547962
Iteration 41, loss = 0.09078317
Iteration 42, loss = 0.08644738
Iteration 43, loss = 0.08240603
Iteration 44, loss = 0.07864539
Iteration 45, loss = 0.07516572
Iteration 46, loss = 0.07191092
Iteration 47, loss = 0.06888909
Iteration 48, loss = 0.06606127
Iteration 49, loss = 0.06340152
Iteration 50, loss = 0.06093481
Iteration 51, loss = 0.05857794
Iteration 52, loss = 0.05644428
Iteration 53, loss = 0.05438162
Iteration 54, loss = 0.05243615
Iteration 55, loss = 0.05063893
Iteration 56, loss = 0.04888376
Iteration 57, loss = 0.04729554
Iteration 58, loss = 0.04574573
Iteration 59, loss = 0.04431521
Iteration 60, loss = 0.04295675
Iteration 61, loss = 0.04167582
Iteration 62, loss = 0.04045776
Iteration 63, loss = 0.03927180
Iteration 64, loss = 0.03819119
Iteration 65, loss = 0.03713280
Iteration 66, loss = 0.03610197
Iteration 67, loss = 0.03516761
Iteration 68, loss = 0.03427270
Iteration 69, loss = 0.03343766
Iteration 70, loss = 0.03262911
Iteration 71, loss = 0.03181711
Iteration 72, loss = 0.03107053
Iteration 73, loss = 0.03035792
Iteration 74, loss = 0.02965132
Iteration 75, loss = 0.02899163
Iteration 76, loss = 0.02834343
Iteration 77, loss = 0.02777348
Iteration 78, loss = 0.02722531
Iteration 79, loss = 0.02662023
Iteration 80, loss = 0.02612009
Iteration 81, loss = 0.02560855
Iteration 82, loss = 0.02513336
Iteration 83, loss = 0.02465854
Iteration 84, loss = 0.02422706
Iteration 85, loss = 0.02375793
Iteration 86, loss = 0.02333581
Iteration 87, loss = 0.02291922
Iteration 88, loss = 0.02253325
Iteration 89, loss = 0.02215325
Iteration 90, loss = 0.02180304
Iteration 91, loss = 0.02144478
Iteration 92, loss = 0.02113139
Iteration 93, loss = 0.02078045
Iteration 94, loss = 0.02046305
Iteration 95, loss = 0.02014724
Iteration 96, loss = 0.01985629
Iteration 97, loss = 0.01957661
Iteration 98, loss = 0.01929820
Iteration 99, loss = 0.01903849
Iteration 100, loss = 0.01876950
Iteration 101, loss = 0.01853968
Iteration 102, loss = 0.01827320
Iteration 103, loss = 0.01806873
Iteration 104, loss = 0.01782400
Iteration 105, loss = 0.01759726
Iteration 106, loss = 0.01740674
Iteration 107, loss = 0.01720207
Iteration 108, loss = 0.01696375
Iteration 109, loss = 0.01676826
Iteration 110, loss = 0.01659091
Iteration 111, loss = 0.01639552
Iteration 112, loss = 0.01620746
Iteration 113, loss = 0.01611480
Iteration 114, loss = 0.01587547
Iteration 115, loss = 0.01570766
Iteration 116, loss = 0.01555638
Iteration 117, loss = 0.01543376
Iteration 118, loss = 0.01528646
Iteration 119, loss = 0.01509445
Iteration 120, loss = 0.01493077
Iteration 121, loss = 0.01476207
Iteration 122, loss = 0.01469543
Iteration 123, loss = 0.01450368
Iteration 124, loss = 0.01437240
Iteration 125, loss = 0.01425224
Iteration 126, loss = 0.01412138
Iteration 127, loss = 0.01402932
Iteration 128, loss = 0.01388262
Iteration 129, loss = 0.01377312
Iteration 130, loss = 0.01366715
Iteration 131, loss = 0.01357424
Iteration 132, loss = 0.01350235
Iteration 133, loss = 0.01335727
Iteration 134, loss = 0.01324305
Iteration 135, loss = 0.01314040
Iteration 136, loss = 0.01305296
Iteration 137, loss = 0.01293817
Iteration 138, loss = 0.01286120
Iteration 139, loss = 0.01275723
Iteration 140, loss = 0.01270102
Iteration 141, loss = 0.01258401
Iteration 142, loss = 0.01248310
Iteration 143, loss = 0.01240723
Iteration 144, loss = 0.01232670
Iteration 145, loss = 0.01223985
Iteration 146, loss = 0.01218105
Iteration 147, loss = 0.01212493
Iteration 148, loss = 0.01208685
Iteration 149, loss = 0.01198018
Iteration 150, loss = 0.01189985
Iteration 151, loss = 0.01190339
Iteration 152, loss = 0.01181155
Iteration 153, loss = 0.01176628
Iteration 154, loss = 0.01169173
Iteration 155, loss = 0.01164387
Iteration 156, loss = 0.01156029
Iteration 157, loss = 0.01151425
Iteration 158, loss = 0.01145035
Iteration 159, loss = 0.01137193
Iteration 160, loss = 0.01130338
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68038378
Iteration 2, loss = 0.65746109
Iteration 3, loss = 0.63124097
Iteration 4, loss = 0.59808434
Iteration 5, loss = 0.55731481
Iteration 6, loss = 0.51185520
Iteration 7, loss = 0.46419632
Iteration 8, loss = 0.41702971
Iteration 9, loss = 0.37081881
Iteration 10, loss = 0.32753764
Iteration 11, loss = 0.28835738
Iteration 12, loss = 0.25326771
Iteration 13, loss = 0.22272031
Iteration 14, loss = 0.19606913
Iteration 15, loss = 0.17327936
Iteration 16, loss = 0.15375391
Iteration 17, loss = 0.13704810
Iteration 18, loss = 0.12300284
Iteration 19, loss = 0.11087515
Iteration 20, loss = 0.10051138
Iteration 21, loss = 0.09160390
Iteration 22, loss = 0.08391545
Iteration 23, loss = 0.07717651
Iteration 24, loss = 0.07142380
Iteration 25, loss = 0.06634093
Iteration 26, loss = 0.06189468
Iteration 27, loss = 0.05787247
Iteration 28, loss = 0.05434000
Iteration 29, loss = 0.05122010
Iteration 30, loss = 0.04846884
Iteration 31, loss = 0.04595840
Iteration 32, loss = 0.04382271
Iteration 33, loss = 0.04157602
Iteration 34, loss = 0.03973928
Iteration 35, loss = 0.03802175
Iteration 36, loss = 0.03641975
Iteration 37, loss = 0.03506866
Iteration 38, loss = 0.03368752
Iteration 39, loss = 0.03250319
Iteration 40, loss = 0.03135285
Iteration 41, loss = 0.03030568
Iteration 42, loss = 0.02935292
Iteration 43, loss = 0.02846736
Iteration 44, loss = 0.02763648
Iteration 45, loss = 0.02690098
Iteration 46, loss = 0.02614372
Iteration 47, loss = 0.02549503
Iteration 48, loss = 0.02486585
Iteration 49, loss = 0.02431184
Iteration 50, loss = 0.02378788
Iteration 51, loss = 0.02327029
Iteration 52, loss = 0.02283863
Iteration 53, loss = 0.02238825
Iteration 54, loss = 0.02196356
Iteration 55, loss = 0.02146558
Iteration 56, loss = 0.02107958
Iteration 57, loss = 0.02074866
Iteration 58, loss = 0.02035527
Iteration 59, loss = 0.02009439
Iteration 60, loss = 0.01970773
Iteration 61, loss = 0.01951440
Iteration 62, loss = 0.01924688
Iteration 63, loss = 0.01904767
Iteration 64, loss = 0.01873754
Iteration 65, loss = 0.01849779
Iteration 66, loss = 0.01824134
Iteration 67, loss = 0.01803578
Iteration 68, loss = 0.01789293
Iteration 69, loss = 0.01785010
Iteration 70, loss = 0.01769484
Iteration 71, loss = 0.01751389
Iteration 72, loss = 0.01732612
Iteration 73, loss = 0.01711164
Iteration 74, loss = 0.01696549
Iteration 75, loss = 0.01682030
Iteration 76, loss = 0.01668673
Iteration 77, loss = 0.01662559
Iteration 78, loss = 0.01655194
Iteration 79, loss = 0.01642478
Iteration 80, loss = 0.01621880
Iteration 81, loss = 0.01628513
Iteration 82, loss = 0.01613197
Iteration 83, loss = 0.01585938
Iteration 84, loss = 0.01558887
Iteration 85, loss = 0.01542508
Iteration 86, loss = 0.01533002
Iteration 87, loss = 0.01535157
Iteration 88, loss = 0.01513136
Iteration 89, loss = 0.01531328
Iteration 90, loss = 0.01537818
Iteration 91, loss = 0.01533617
Iteration 92, loss = 0.01520152
Iteration 93, loss = 0.01501772
Iteration 94, loss = 0.01481528
Iteration 95, loss = 0.01472752
Iteration 96, loss = 0.01458656
Iteration 97, loss = 0.01454338
Iteration 98, loss = 0.01443068
Iteration 99, loss = 0.01440878
Iteration 100, loss = 0.01426773
Iteration 101, loss = 0.01414058
Iteration 102, loss = 0.01417887
Iteration 103, loss = 0.01441979
Iteration 104, loss = 0.01417153
Iteration 105, loss = 0.01422381
Iteration 106, loss = 0.01411110
Iteration 107, loss = 0.01405852
Iteration 108, loss = 0.01390263
Iteration 109, loss = 0.01388489
Iteration 110, loss = 0.01383603
Iteration 111, loss = 0.01377877
Iteration 112, loss = 0.01367478
Iteration 113, loss = 0.01364873
Iteration 114, loss = 0.01357007
Iteration 115, loss = 0.01355123
Iteration 116, loss = 0.01348930
Iteration 117, loss = 0.01358604
Iteration 118, loss = 0.01352557
Iteration 119, loss = 0.01360213
Iteration 120, loss = 0.01353574
Iteration 121, loss = 0.01343707
Iteration 122, loss = 0.01334456
Iteration 123, loss = 0.01330699
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68184174
Iteration 2, loss = 0.64729727
Iteration 3, loss = 0.59075886
Iteration 4, loss = 0.51427316
Iteration 5, loss = 0.43430235
Iteration 6, loss = 0.35834675
Iteration 7, loss = 0.29107274
Iteration 8, loss = 0.23583525
Iteration 9, loss = 0.19193936
Iteration 10, loss = 0.15807190
Iteration 11, loss = 0.13237678
Iteration 12, loss = 0.11271921
Iteration 13, loss = 0.09743931
Iteration 14, loss = 0.08543240
Iteration 15, loss = 0.07592427
Iteration 16, loss = 0.06835233
Iteration 17, loss = 0.06201401
Iteration 18, loss = 0.05683415
Iteration 19, loss = 0.05249426
Iteration 20, loss = 0.04886894
Iteration 21, loss = 0.04572050
Iteration 22, loss = 0.04307538
Iteration 23, loss = 0.04079077
Iteration 24, loss = 0.03878397
Iteration 25, loss = 0.03739042
Iteration 26, loss = 0.03570390
Iteration 27, loss = 0.03413443
Iteration 28, loss = 0.03276091
Iteration 29, loss = 0.03173297
Iteration 30, loss = 0.03063945
Iteration 31, loss = 0.02981358
Iteration 32, loss = 0.02911678
Iteration 33, loss = 0.02827328
Iteration 34, loss = 0.02785522
Iteration 35, loss = 0.02732608
Iteration 36, loss = 0.02663786
Iteration 37, loss = 0.02604712
Iteration 38, loss = 0.02561163
Iteration 39, loss = 0.02519420
Iteration 40, loss = 0.02474462
Iteration 41, loss = 0.02435329
Iteration 42, loss = 0.02406146
Iteration 43, loss = 0.02369614
Iteration 44, loss = 0.02344668
Iteration 45, loss = 0.02331434
Iteration 46, loss = 0.02308146
Iteration 47, loss = 0.02274233
Iteration 48, loss = 0.02274137
Iteration 49, loss = 0.02228447
Iteration 50, loss = 0.02222355
Iteration 51, loss = 0.02202197
Iteration 52, loss = 0.02177447
Iteration 53, loss = 0.02188365
Iteration 54, loss = 0.02175268
Iteration 55, loss = 0.02139465
Iteration 56, loss = 0.02110651
Iteration 57, loss = 0.02106329
Iteration 58, loss = 0.02096800
Iteration 59, loss = 0.02063954
Iteration 60, loss = 0.02063774
Iteration 61, loss = 0.02101965
Iteration 62, loss = 0.02079376
Iteration 63, loss = 0.02059812
Iteration 64, loss = 0.02035033
Iteration 65, loss = 0.02045594
Iteration 66, loss = 0.02025678
Iteration 67, loss = 0.02004017
Iteration 68, loss = 0.01983709
Iteration 69, loss = 0.02021986
Iteration 70, loss = 0.01979917
Iteration 71, loss = 0.01971042
Iteration 72, loss = 0.01954300
Iteration 73, loss = 0.01937618
Iteration 74, loss = 0.01955039
Iteration 75, loss = 0.01933926
Iteration 76, loss = 0.01951372
Iteration 77, loss = 0.01938905
Iteration 78, loss = 0.01923565
Iteration 79, loss = 0.01937170
Iteration 80, loss = 0.01910320
Iteration 81, loss = 0.01908971
Iteration 82, loss = 0.01906759
Iteration 83, loss = 0.01903484
Iteration 84, loss = 0.01910835
Iteration 85, loss = 0.01900333
Iteration 86, loss = 0.01906369
Iteration 87, loss = 0.01871316
Iteration 88, loss = 0.01913086
Iteration 89, loss = 0.01894547
Iteration 90, loss = 0.01869944
Iteration 91, loss = 0.01914553
Iteration 92, loss = 0.01875637
Iteration 93, loss = 0.01887697
Iteration 94, loss = 0.01889453
Iteration 95, loss = 0.01863375
Iteration 96, loss = 0.01868951
Iteration 97, loss = 0.01853984
Iteration 98, loss = 0.01871912
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68891313
Iteration 2, loss = 0.64902668
Iteration 3, loss = 0.58597493
Iteration 4, loss = 0.49994109
Iteration 5, loss = 0.41345070
Iteration 6, loss = 0.33515096
Iteration 7, loss = 0.26968839
Iteration 8, loss = 0.21727053
Iteration 9, loss = 0.17714565
Iteration 10, loss = 0.14627417
Iteration 11, loss = 0.12286297
Iteration 12, loss = 0.10505067
Iteration 13, loss = 0.09123655
Iteration 14, loss = 0.08062662
Iteration 15, loss = 0.07193288
Iteration 16, loss = 0.06508964
Iteration 17, loss = 0.05966416
Iteration 18, loss = 0.05504701
Iteration 19, loss = 0.05118115
Iteration 20, loss = 0.04790438
Iteration 21, loss = 0.04505186
Iteration 22, loss = 0.04254931
Iteration 23, loss = 0.04059782
Iteration 24, loss = 0.03891718
Iteration 25, loss = 0.03729632
Iteration 26, loss = 0.03606906
Iteration 27, loss = 0.03488818
Iteration 28, loss = 0.03355085
Iteration 29, loss = 0.03287744
Iteration 30, loss = 0.03203347
Iteration 31, loss = 0.03105510
Iteration 32, loss = 0.03032836
Iteration 33, loss = 0.02966394
Iteration 34, loss = 0.02906598
Iteration 35, loss = 0.02865897
Iteration 36, loss = 0.02805289
Iteration 37, loss = 0.02771480
Iteration 38, loss = 0.02709911
Iteration 39, loss = 0.02693712
Iteration 40, loss = 0.02656816
Iteration 41, loss = 0.02611151
Iteration 42, loss = 0.02601092
Iteration 43, loss = 0.02552859
Iteration 44, loss = 0.02532245
Iteration 45, loss = 0.02525863
Iteration 46, loss = 0.02480327
Iteration 47, loss = 0.02467712
Iteration 48, loss = 0.02455722
Iteration 49, loss = 0.02449591
Iteration 50, loss = 0.02414209
Iteration 51, loss = 0.02386491
Iteration 52, loss = 0.02383716
Iteration 53, loss = 0.02373143
Iteration 54, loss = 0.02340072
Iteration 55, loss = 0.02337785
Iteration 56, loss = 0.02338682
Iteration 57, loss = 0.02322814
Iteration 58, loss = 0.02328498
Iteration 59, loss = 0.02277096
Iteration 60, loss = 0.02279213
Iteration 61, loss = 0.02267788
Iteration 62, loss = 0.02292345
Iteration 63, loss = 0.02259753
Iteration 64, loss = 0.02254852
Iteration 65, loss = 0.02238408
Iteration 66, loss = 0.02247465
Iteration 67, loss = 0.02235655
Iteration 68, loss = 0.02229326
Iteration 69, loss = 0.02206825
Iteration 70, loss = 0.02210636
Iteration 71, loss = 0.02204052
Iteration 72, loss = 0.02213508
Iteration 73, loss = 0.02177362
Iteration 74, loss = 0.02202148
Iteration 75, loss = 0.02170522
Iteration 76, loss = 0.02178257
Iteration 77, loss = 0.02175059
Iteration 78, loss = 0.02192080
Iteration 79, loss = 0.02171701
Iteration 80, loss = 0.02162341
Iteration 81, loss = 0.02175441
Iteration 82, loss = 0.02176322
Iteration 83, loss = 0.02170892
Iteration 84, loss = 0.02160837
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68399034
Iteration 2, loss = 0.64411667
Iteration 3, loss = 0.58325839
Iteration 4, loss = 0.50430139
Iteration 5, loss = 0.42319266
Iteration 6, loss = 0.34751398
Iteration 7, loss = 0.28239259
Iteration 8, loss = 0.22933717
Iteration 9, loss = 0.18764739
Iteration 10, loss = 0.15549539
Iteration 11, loss = 0.13086263
Iteration 12, loss = 0.11208401
Iteration 13, loss = 0.09736549
Iteration 14, loss = 0.08587826
Iteration 15, loss = 0.07655066
Iteration 16, loss = 0.06926045
Iteration 17, loss = 0.06311288
Iteration 18, loss = 0.05792437
Iteration 19, loss = 0.05383506
Iteration 20, loss = 0.05024516
Iteration 21, loss = 0.04714346
Iteration 22, loss = 0.04462521
Iteration 23, loss = 0.04244951
Iteration 24, loss = 0.04025615
Iteration 25, loss = 0.03871234
Iteration 26, loss = 0.03726077
Iteration 27, loss = 0.03585746
Iteration 28, loss = 0.03458614
Iteration 29, loss = 0.03347966
Iteration 30, loss = 0.03265263
Iteration 31, loss = 0.03186428
Iteration 32, loss = 0.03087761
Iteration 33, loss = 0.03042327
Iteration 34, loss = 0.02973067
Iteration 35, loss = 0.02893592
Iteration 36, loss = 0.02855363
Iteration 37, loss = 0.02804389
Iteration 38, loss = 0.02754746
Iteration 39, loss = 0.02705934
Iteration 40, loss = 0.02686033
Iteration 41, loss = 0.02646126
Iteration 42, loss = 0.02612765
Iteration 43, loss = 0.02612780
Iteration 44, loss = 0.02573688
Iteration 45, loss = 0.02539502
Iteration 46, loss = 0.02504762
Iteration 47, loss = 0.02459732
Iteration 48, loss = 0.02455662
Iteration 49, loss = 0.02438597
Iteration 50, loss = 0.02410098
Iteration 51, loss = 0.02402418
Iteration 52, loss = 0.02393634
Iteration 53, loss = 0.02376378
Iteration 54, loss = 0.02371429
Iteration 55, loss = 0.02338279
Iteration 56, loss = 0.02326245
Iteration 57, loss = 0.02318816
Iteration 58, loss = 0.02282042
Iteration 59, loss = 0.02295381
Iteration 60, loss = 0.02273644
Iteration 61, loss = 0.02272400
Iteration 62, loss = 0.02253270
Iteration 63, loss = 0.02260110
Iteration 64, loss = 0.02248708
Iteration 65, loss = 0.02242748
Iteration 66, loss = 0.02222186
Iteration 67, loss = 0.02226330
Iteration 68, loss = 0.02225085
Iteration 69, loss = 0.02194503
Iteration 70, loss = 0.02189956
Iteration 71, loss = 0.02201445
Iteration 72, loss = 0.02227223
Iteration 73, loss = 0.02224732
Iteration 74, loss = 0.02185257
Iteration 75, loss = 0.02162629
Iteration 76, loss = 0.02175151
Iteration 77, loss = 0.02167296
Iteration 78, loss = 0.02164626
Iteration 79, loss = 0.02137077
Iteration 80, loss = 0.02146318
Iteration 81, loss = 0.02161218
Iteration 82, loss = 0.02175286
Iteration 83, loss = 0.02153566
Iteration 84, loss = 0.02117464
Iteration 85, loss = 0.02120417
Iteration 86, loss = 0.02136739
Iteration 87, loss = 0.02122304
Iteration 88, loss = 0.02107454
Iteration 89, loss = 0.02124527
Iteration 90, loss = 0.02105508
Iteration 91, loss = 0.02130389
Iteration 92, loss = 0.02098166
Iteration 93, loss = 0.02123271
Iteration 94, loss = 0.02126389
Iteration 95, loss = 0.02127882
Iteration 96, loss = 0.02094537
Iteration 97, loss = 0.02089442
Iteration 98, loss = 0.02076629
Iteration 99, loss = 0.02114395
Iteration 100, loss = 0.02108069
Iteration 101, loss = 0.02135796
Iteration 102, loss = 0.02083188
Iteration 103, loss = 0.02088667
Iteration 104, loss = 0.02111672
Iteration 105, loss = 0.02113154
Iteration 106, loss = 0.02122292
Iteration 107, loss = 0.02090695
Iteration 108, loss = 0.02054769
Iteration 109, loss = 0.02073928
Iteration 110, loss = 0.02071672
Iteration 111, loss = 0.02062065
Iteration 112, loss = 0.02068169
Iteration 113, loss = 0.02086023
Iteration 114, loss = 0.02073230
Iteration 115, loss = 0.02066420
Iteration 116, loss = 0.02044825
Iteration 117, loss = 0.02049231
Iteration 118, loss = 0.02063301
Iteration 119, loss = 0.02068976
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67838225
Iteration 2, loss = 0.64722184
Iteration 3, loss = 0.60762354
Iteration 4, loss = 0.55413372
Iteration 5, loss = 0.49381973
Iteration 6, loss = 0.43251002
Iteration 7, loss = 0.37508396
Iteration 8, loss = 0.32199482
Iteration 9, loss = 0.27504085
Iteration 10, loss = 0.23471623
Iteration 11, loss = 0.20068112
Iteration 12, loss = 0.17248189
Iteration 13, loss = 0.14903157
Iteration 14, loss = 0.12991194
Iteration 15, loss = 0.11423092
Iteration 16, loss = 0.10116233
Iteration 17, loss = 0.09053533
Iteration 18, loss = 0.08143525
Iteration 19, loss = 0.07385316
Iteration 20, loss = 0.06742393
Iteration 21, loss = 0.06193013
Iteration 22, loss = 0.05717768
Iteration 23, loss = 0.05327571
Iteration 24, loss = 0.04962890
Iteration 25, loss = 0.04657951
Iteration 26, loss = 0.04385770
Iteration 27, loss = 0.04146082
Iteration 28, loss = 0.03941791
Iteration 29, loss = 0.03742074
Iteration 30, loss = 0.03578193
Iteration 31, loss = 0.03415840
Iteration 32, loss = 0.03279522
Iteration 33, loss = 0.03155317
Iteration 34, loss = 0.03046596
Iteration 35, loss = 0.02938015
Iteration 36, loss = 0.02861602
Iteration 37, loss = 0.02764524
Iteration 38, loss = 0.02684528
Iteration 39, loss = 0.02622449
Iteration 40, loss = 0.02544809
Iteration 41, loss = 0.02482490
Iteration 42, loss = 0.02418177
Iteration 43, loss = 0.02366342
Iteration 44, loss = 0.02317072
Iteration 45, loss = 0.02274304
Iteration 46, loss = 0.02232462
Iteration 47, loss = 0.02197888
Iteration 48, loss = 0.02154905
Iteration 49, loss = 0.02125086
Iteration 50, loss = 0.02083190
Iteration 51, loss = 0.02060327
Iteration 52, loss = 0.02030047
Iteration 53, loss = 0.02006247
Iteration 54, loss = 0.01981260
Iteration 55, loss = 0.01952547
Iteration 56, loss = 0.01932887
Iteration 57, loss = 0.01905049
Iteration 58, loss = 0.01883549
Iteration 59, loss = 0.01864288
Iteration 60, loss = 0.01846572
Iteration 61, loss = 0.01828780
Iteration 62, loss = 0.01821035
Iteration 63, loss = 0.01793411
Iteration 64, loss = 0.01784577
Iteration 65, loss = 0.01763020
Iteration 66, loss = 0.01743470
Iteration 67, loss = 0.01734906
Iteration 68, loss = 0.01733706
Iteration 69, loss = 0.01720914
Iteration 70, loss = 0.01704043
Iteration 71, loss = 0.01704429
Iteration 72, loss = 0.01678145
Iteration 73, loss = 0.01674056
Iteration 74, loss = 0.01652675
Iteration 75, loss = 0.01656804
Iteration 76, loss = 0.01647891
Iteration 77, loss = 0.01634387
Iteration 78, loss = 0.01630658
Iteration 79, loss = 0.01637032
Iteration 80, loss = 0.01610125
Iteration 81, loss = 0.01609658
Iteration 82, loss = 0.01604684
Iteration 83, loss = 0.01592648
Iteration 84, loss = 0.01598795
Iteration 85, loss = 0.01581834
Iteration 86, loss = 0.01579691
Iteration 87, loss = 0.01575981
Iteration 88, loss = 0.01555204
Iteration 89, loss = 0.01568159
Iteration 90, loss = 0.01565959
Iteration 91, loss = 0.01560009
Iteration 92, loss = 0.01537843
Iteration 93, loss = 0.01540717
Iteration 94, loss = 0.01535027
Iteration 95, loss = 0.01527636
Iteration 96, loss = 0.01528262
Iteration 97, loss = 0.01521841
Iteration 98, loss = 0.01509020
Iteration 99, loss = 0.01517221
Iteration 100, loss = 0.01507398
Iteration 101, loss = 0.01499732
Iteration 102, loss = 0.01503828
Iteration 103, loss = 0.01505687
Iteration 104, loss = 0.01492330
Iteration 105, loss = 0.01500640
Iteration 106, loss = 0.01491436
Iteration 107, loss = 0.01489666
Iteration 108, loss = 0.01489340
Iteration 109, loss = 0.01475878
Iteration 110, loss = 0.01482284
Iteration 111, loss = 0.01480854
Iteration 112, loss = 0.01478069
Iteration 113, loss = 0.01472101
Iteration 114, loss = 0.01476492
Iteration 115, loss = 0.01460444
Iteration 116, loss = 0.01467775
Iteration 117, loss = 0.01458859
Iteration 118, loss = 0.01462203
Iteration 119, loss = 0.01454505
Iteration 120, loss = 0.01459147
Iteration 121, loss = 0.01459951
Iteration 122, loss = 0.01451833
Iteration 123, loss = 0.01448166
Iteration 124, loss = 0.01462045
Iteration 125, loss = 0.01435461
Iteration 126, loss = 0.01456227
Iteration 127, loss = 0.01442113
Iteration 128, loss = 0.01447954
Iteration 129, loss = 0.01425259
Iteration 130, loss = 0.01431456
Iteration 131, loss = 0.01433967
Iteration 132, loss = 0.01439425
Iteration 133, loss = 0.01433856
Iteration 134, loss = 0.01420853
Iteration 135, loss = 0.01421460
Iteration 136, loss = 0.01422406
Iteration 137, loss = 0.01438251
Iteration 138, loss = 0.01414465
Iteration 139, loss = 0.01430556
Iteration 140, loss = 0.01418006
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70823412
Iteration 2, loss = 0.68168669
Iteration 3, loss = 0.64581437
Iteration 4, loss = 0.59976903
Iteration 5, loss = 0.54337535
Iteration 6, loss = 0.48184351
Iteration 7, loss = 0.41997523
Iteration 8, loss = 0.36187168
Iteration 9, loss = 0.30981337
Iteration 10, loss = 0.26433419
Iteration 11, loss = 0.22582250
Iteration 12, loss = 0.19349573
Iteration 13, loss = 0.16689023
Iteration 14, loss = 0.14475922
Iteration 15, loss = 0.12660664
Iteration 16, loss = 0.11147799
Iteration 17, loss = 0.09908806
Iteration 18, loss = 0.08857923
Iteration 19, loss = 0.07981487
Iteration 20, loss = 0.07231380
Iteration 21, loss = 0.06605258
Iteration 22, loss = 0.06056037
Iteration 23, loss = 0.05585365
Iteration 24, loss = 0.05169872
Iteration 25, loss = 0.04818634
Iteration 26, loss = 0.04500862
Iteration 27, loss = 0.04229911
Iteration 28, loss = 0.03988316
Iteration 29, loss = 0.03780120
Iteration 30, loss = 0.03587984
Iteration 31, loss = 0.03400297
Iteration 32, loss = 0.03235354
Iteration 33, loss = 0.03096255
Iteration 34, loss = 0.02975735
Iteration 35, loss = 0.02856128
Iteration 36, loss = 0.02740315
Iteration 37, loss = 0.02646469
Iteration 38, loss = 0.02551799
Iteration 39, loss = 0.02476756
Iteration 40, loss = 0.02395810
Iteration 41, loss = 0.02327214
Iteration 42, loss = 0.02267107
Iteration 43, loss = 0.02205574
Iteration 44, loss = 0.02147600
Iteration 45, loss = 0.02095567
Iteration 46, loss = 0.02049748
Iteration 47, loss = 0.02015957
Iteration 48, loss = 0.01962431
Iteration 49, loss = 0.01916561
Iteration 50, loss = 0.01881943
Iteration 51, loss = 0.01848439
Iteration 52, loss = 0.01806304
Iteration 53, loss = 0.01780470
Iteration 54, loss = 0.01751691
Iteration 55, loss = 0.01728518
Iteration 56, loss = 0.01702943
Iteration 57, loss = 0.01672210
Iteration 58, loss = 0.01646973
Iteration 59, loss = 0.01625448
Iteration 60, loss = 0.01606024
Iteration 61, loss = 0.01588014
Iteration 62, loss = 0.01570940
Iteration 63, loss = 0.01562717
Iteration 64, loss = 0.01537043
Iteration 65, loss = 0.01515909
Iteration 66, loss = 0.01515065
Iteration 67, loss = 0.01483857
Iteration 68, loss = 0.01470610
Iteration 69, loss = 0.01460938
Iteration 70, loss = 0.01447046
Iteration 71, loss = 0.01432984
Iteration 72, loss = 0.01422636
Iteration 73, loss = 0.01417264
Iteration 74, loss = 0.01401573
Iteration 75, loss = 0.01394153
Iteration 76, loss = 0.01384290
Iteration 77, loss = 0.01379274
Iteration 78, loss = 0.01360393
Iteration 79, loss = 0.01356995
Iteration 80, loss = 0.01340893
Iteration 81, loss = 0.01338660
Iteration 82, loss = 0.01330401
Iteration 83, loss = 0.01322989
Iteration 84, loss = 0.01315539
Iteration 85, loss = 0.01309600
Iteration 86, loss = 0.01303067
Iteration 87, loss = 0.01294144
Iteration 88, loss = 0.01290601
Iteration 89, loss = 0.01276435
Iteration 90, loss = 0.01272677
Iteration 91, loss = 0.01271669
Iteration 92, loss = 0.01261452
Iteration 93, loss = 0.01269545
Iteration 94, loss = 0.01246502
Iteration 95, loss = 0.01246652
Iteration 96, loss = 0.01244538
Iteration 97, loss = 0.01236218
Iteration 98, loss = 0.01242369
Iteration 99, loss = 0.01242190
Iteration 100, loss = 0.01245548
Iteration 101, loss = 0.01225307
Iteration 102, loss = 0.01219846
Iteration 103, loss = 0.01210133
Iteration 104, loss = 0.01216550
Iteration 105, loss = 0.01202047
Iteration 106, loss = 0.01201872
Iteration 107, loss = 0.01201157
Iteration 108, loss = 0.01198285
Iteration 109, loss = 0.01188282
Iteration 110, loss = 0.01192869
Iteration 111, loss = 0.01196163
Iteration 112, loss = 0.01192688
Iteration 113, loss = 0.01181561
Iteration 114, loss = 0.01172064
Iteration 115, loss = 0.01180170
Iteration 116, loss = 0.01177737
Iteration 117, loss = 0.01173571
Iteration 118, loss = 0.01177077
Iteration 119, loss = 0.01171394
Iteration 120, loss = 0.01160491
Iteration 121, loss = 0.01160796
Iteration 122, loss = 0.01155012
Iteration 123, loss = 0.01153306
Iteration 124, loss = 0.01155976
Iteration 125, loss = 0.01146911
Iteration 126, loss = 0.01160870
Iteration 127, loss = 0.01160693
Iteration 128, loss = 0.01148533
Iteration 129, loss = 0.01147268
Iteration 130, loss = 0.01139848
Iteration 131, loss = 0.01144140
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72085826
Iteration 2, loss = 0.70206980
Iteration 3, loss = 0.68578979
Iteration 4, loss = 0.66959308
Iteration 5, loss = 0.65271265
Iteration 6, loss = 0.63436415
Iteration 7, loss = 0.61545398
Iteration 8, loss = 0.59484136
Iteration 9, loss = 0.57313945
Iteration 10, loss = 0.55048160
Iteration 11, loss = 0.52665243
Iteration 12, loss = 0.50210041
Iteration 13, loss = 0.47708898
Iteration 14, loss = 0.45184314
Iteration 15, loss = 0.42661843
Iteration 16, loss = 0.40194678
Iteration 17, loss = 0.37778687
Iteration 18, loss = 0.35448144
Iteration 19, loss = 0.33211582
Iteration 20, loss = 0.31084038
Iteration 21, loss = 0.29073371
Iteration 22, loss = 0.27171594
Iteration 23, loss = 0.25392087
Iteration 24, loss = 0.23733228
Iteration 25, loss = 0.22182741
Iteration 26, loss = 0.20743155
Iteration 27, loss = 0.19403597
Iteration 28, loss = 0.18165250
Iteration 29, loss = 0.17017140
Iteration 30, loss = 0.15952032
Iteration 31, loss = 0.14975731
Iteration 32, loss = 0.14072102
Iteration 33, loss = 0.13239860
Iteration 34, loss = 0.12473362
Iteration 35, loss = 0.11764479
Iteration 36, loss = 0.11106858
Iteration 37, loss = 0.10502187
Iteration 38, loss = 0.09945423
Iteration 39, loss = 0.09425894
Iteration 40, loss = 0.08945787
Iteration 41, loss = 0.08497378
Iteration 42, loss = 0.08085684
Iteration 43, loss = 0.07703367
Iteration 44, loss = 0.07339726
Iteration 45, loss = 0.07008109
Iteration 46, loss = 0.06694264
Iteration 47, loss = 0.06408147
Iteration 48, loss = 0.06135579
Iteration 49, loss = 0.05882455
Iteration 50, loss = 0.05645822
Iteration 51, loss = 0.05421951
Iteration 52, loss = 0.05212620
Iteration 53, loss = 0.05016039
Iteration 54, loss = 0.04831501
Iteration 55, loss = 0.04656584
Iteration 56, loss = 0.04491269
Iteration 57, loss = 0.04335745
Iteration 58, loss = 0.04189507
Iteration 59, loss = 0.04050178
Iteration 60, loss = 0.03920455
Iteration 61, loss = 0.03795567
Iteration 62, loss = 0.03677073
Iteration 63, loss = 0.03565161
Iteration 64, loss = 0.03459484
Iteration 65, loss = 0.03357839
Iteration 66, loss = 0.03259919
Iteration 67, loss = 0.03168349
Iteration 68, loss = 0.03079934
Iteration 69, loss = 0.02997166
Iteration 70, loss = 0.02916632
Iteration 71, loss = 0.02842222
Iteration 72, loss = 0.02767824
Iteration 73, loss = 0.02699107
Iteration 74, loss = 0.02635330
Iteration 75, loss = 0.02569049
Iteration 76, loss = 0.02512647
Iteration 77, loss = 0.02451356
Iteration 78, loss = 0.02396497
Iteration 79, loss = 0.02342294
Iteration 80, loss = 0.02288818
Iteration 81, loss = 0.02240825
Iteration 82, loss = 0.02193491
Iteration 83, loss = 0.02145713
Iteration 84, loss = 0.02101518
Iteration 85, loss = 0.02060336
Iteration 86, loss = 0.02020146
Iteration 87, loss = 0.01982308
Iteration 88, loss = 0.01943712
Iteration 89, loss = 0.01907991
Iteration 90, loss = 0.01872712
Iteration 91, loss = 0.01838363
Iteration 92, loss = 0.01804323
Iteration 93, loss = 0.01773891
Iteration 94, loss = 0.01742518
Iteration 95, loss = 0.01712199
Iteration 96, loss = 0.01682124
Iteration 97, loss = 0.01655681
Iteration 98, loss = 0.01626538
Iteration 99, loss = 0.01602810
Iteration 100, loss = 0.01576776
Iteration 101, loss = 0.01550105
Iteration 102, loss = 0.01526045
Iteration 103, loss = 0.01502841
Iteration 104, loss = 0.01482043
Iteration 105, loss = 0.01457774
Iteration 106, loss = 0.01435866
Iteration 107, loss = 0.01417116
Iteration 108, loss = 0.01403939
Iteration 109, loss = 0.01379466
Iteration 110, loss = 0.01360865
Iteration 111, loss = 0.01343180
Iteration 112, loss = 0.01324254
Iteration 113, loss = 0.01312726
Iteration 114, loss = 0.01291369
Iteration 115, loss = 0.01276012
Iteration 116, loss = 0.01259036
Iteration 117, loss = 0.01244757
Iteration 118, loss = 0.01228633
Iteration 119, loss = 0.01211525
Iteration 120, loss = 0.01196883
Iteration 121, loss = 0.01183200
Iteration 122, loss = 0.01169337
Iteration 123, loss = 0.01158671
Iteration 124, loss = 0.01143214
Iteration 125, loss = 0.01131004
Iteration 126, loss = 0.01119066
Iteration 127, loss = 0.01108474
Iteration 128, loss = 0.01096557
Iteration 129, loss = 0.01085108
Iteration 130, loss = 0.01073942
Iteration 131, loss = 0.01062482
Iteration 132, loss = 0.01051761
Iteration 133, loss = 0.01043424
Iteration 134, loss = 0.01031211
Iteration 135, loss = 0.01024929
Iteration 136, loss = 0.01017520
Iteration 137, loss = 0.01004301
Iteration 138, loss = 0.00996049
Iteration 139, loss = 0.00985335
Iteration 140, loss = 0.00977112
Iteration 141, loss = 0.00968020
Iteration 142, loss = 0.00959464
Iteration 143, loss = 0.00952273
Iteration 144, loss = 0.00944583
Iteration 145, loss = 0.00936338
Iteration 146, loss = 0.00927461
Iteration 147, loss = 0.00919707
Iteration 148, loss = 0.00912865
Iteration 149, loss = 0.00904672
Iteration 150, loss = 0.00898409
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68261997
Iteration 2, loss = 0.66296387
Iteration 3, loss = 0.64398267
Iteration 4, loss = 0.62221595
Iteration 5, loss = 0.59719590
Iteration 6, loss = 0.56825179
Iteration 7, loss = 0.53595319
Iteration 8, loss = 0.50104503
Iteration 9, loss = 0.46533264
Iteration 10, loss = 0.42956327
Iteration 11, loss = 0.39480010
Iteration 12, loss = 0.36182612
Iteration 13, loss = 0.33060217
Iteration 14, loss = 0.30161964
Iteration 15, loss = 0.27455965
Iteration 16, loss = 0.25027367
Iteration 17, loss = 0.22786506
Iteration 18, loss = 0.20773144
Iteration 19, loss = 0.18952737
Iteration 20, loss = 0.17333031
Iteration 21, loss = 0.15886667
Iteration 22, loss = 0.14592357
Iteration 23, loss = 0.13442803
Iteration 24, loss = 0.12418201
Iteration 25, loss = 0.11500225
Iteration 26, loss = 0.10686176
Iteration 27, loss = 0.09952880
Iteration 28, loss = 0.09299963
Iteration 29, loss = 0.08711101
Iteration 30, loss = 0.08178351
Iteration 31, loss = 0.07697999
Iteration 32, loss = 0.07264866
Iteration 33, loss = 0.06873107
Iteration 34, loss = 0.06514614
Iteration 35, loss = 0.06185451
Iteration 36, loss = 0.05882416
Iteration 37, loss = 0.05612156
Iteration 38, loss = 0.05359553
Iteration 39, loss = 0.05127826
Iteration 40, loss = 0.04915465
Iteration 41, loss = 0.04718771
Iteration 42, loss = 0.04530403
Iteration 43, loss = 0.04364280
Iteration 44, loss = 0.04207082
Iteration 45, loss = 0.04057108
Iteration 46, loss = 0.03922913
Iteration 47, loss = 0.03794524
Iteration 48, loss = 0.03678078
Iteration 49, loss = 0.03559470
Iteration 50, loss = 0.03459057
Iteration 51, loss = 0.03358298
Iteration 52, loss = 0.03267117
Iteration 53, loss = 0.03183561
Iteration 54, loss = 0.03095630
Iteration 55, loss = 0.03021217
Iteration 56, loss = 0.02944912
Iteration 57, loss = 0.02875632
Iteration 58, loss = 0.02806253
Iteration 59, loss = 0.02745181
Iteration 60, loss = 0.02688225
Iteration 61, loss = 0.02630107
Iteration 62, loss = 0.02576748
Iteration 63, loss = 0.02529011
Iteration 64, loss = 0.02479053
Iteration 65, loss = 0.02434239
Iteration 66, loss = 0.02391330
Iteration 67, loss = 0.02347463
Iteration 68, loss = 0.02304194
Iteration 69, loss = 0.02271561
Iteration 70, loss = 0.02232179
Iteration 71, loss = 0.02202180
Iteration 72, loss = 0.02168822
Iteration 73, loss = 0.02136727
Iteration 74, loss = 0.02107246
Iteration 75, loss = 0.02073209
Iteration 76, loss = 0.02049134
Iteration 77, loss = 0.02021273
Iteration 78, loss = 0.01992580
Iteration 79, loss = 0.01967159
Iteration 80, loss = 0.01945378
Iteration 81, loss = 0.01919257
Iteration 82, loss = 0.01898797
Iteration 83, loss = 0.01874403
Iteration 84, loss = 0.01861459
Iteration 85, loss = 0.01838948
Iteration 86, loss = 0.01815700
Iteration 87, loss = 0.01801308
Iteration 88, loss = 0.01778896
Iteration 89, loss = 0.01762537
Iteration 90, loss = 0.01753101
Iteration 91, loss = 0.01734650
Iteration 92, loss = 0.01715861
Iteration 93, loss = 0.01703317
Iteration 94, loss = 0.01690181
Iteration 95, loss = 0.01673435
Iteration 96, loss = 0.01661058
Iteration 97, loss = 0.01649832
Iteration 98, loss = 0.01633708
Iteration 99, loss = 0.01624488
Iteration 100, loss = 0.01616684
Iteration 101, loss = 0.01600014
Iteration 102, loss = 0.01586360
Iteration 103, loss = 0.01581899
Iteration 104, loss = 0.01567820
Iteration 105, loss = 0.01555952
Iteration 106, loss = 0.01548754
Iteration 107, loss = 0.01541200
Iteration 108, loss = 0.01529255
Iteration 109, loss = 0.01518291
Iteration 110, loss = 0.01515962
Iteration 111, loss = 0.01503089
Iteration 112, loss = 0.01495442
Iteration 113, loss = 0.01486683
Iteration 114, loss = 0.01478122
Iteration 115, loss = 0.01468183
Iteration 116, loss = 0.01461040
Iteration 117, loss = 0.01450646
Iteration 118, loss = 0.01448649
Iteration 119, loss = 0.01441144
Iteration 120, loss = 0.01432535
Iteration 121, loss = 0.01427448
Iteration 122, loss = 0.01419074
Iteration 123, loss = 0.01413318
Iteration 124, loss = 0.01406132
Iteration 125, loss = 0.01400957
Iteration 126, loss = 0.01395665
Iteration 127, loss = 0.01391810
Iteration 128, loss = 0.01387177
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68846694
Iteration 2, loss = 0.65182260
Iteration 3, loss = 0.60047887
Iteration 4, loss = 0.52813235
Iteration 5, loss = 0.44561124
Iteration 6, loss = 0.36616308
Iteration 7, loss = 0.29660816
Iteration 8, loss = 0.23977395
Iteration 9, loss = 0.19490916
Iteration 10, loss = 0.16063198
Iteration 11, loss = 0.13447148
Iteration 12, loss = 0.11427229
Iteration 13, loss = 0.09873662
Iteration 14, loss = 0.08637972
Iteration 15, loss = 0.07648357
Iteration 16, loss = 0.06870031
Iteration 17, loss = 0.06221939
Iteration 18, loss = 0.05674437
Iteration 19, loss = 0.05224375
Iteration 20, loss = 0.04858603
Iteration 21, loss = 0.04520178
Iteration 22, loss = 0.04254638
Iteration 23, loss = 0.04008047
Iteration 24, loss = 0.03786306
Iteration 25, loss = 0.03623191
Iteration 26, loss = 0.03450430
Iteration 27, loss = 0.03341102
Iteration 28, loss = 0.03224945
Iteration 29, loss = 0.03088520
Iteration 30, loss = 0.02989113
Iteration 31, loss = 0.02879621
Iteration 32, loss = 0.02801405
Iteration 33, loss = 0.02729965
Iteration 34, loss = 0.02658388
Iteration 35, loss = 0.02595592
Iteration 36, loss = 0.02544910
Iteration 37, loss = 0.02483517
Iteration 38, loss = 0.02447745
Iteration 39, loss = 0.02390551
Iteration 40, loss = 0.02359448
Iteration 41, loss = 0.02354461
Iteration 42, loss = 0.02297526
Iteration 43, loss = 0.02271438
Iteration 44, loss = 0.02213083
Iteration 45, loss = 0.02212323
Iteration 46, loss = 0.02174800
Iteration 47, loss = 0.02121778
Iteration 48, loss = 0.02114691
Iteration 49, loss = 0.02106117
Iteration 50, loss = 0.02076976
Iteration 51, loss = 0.02058812
Iteration 52, loss = 0.02060890
Iteration 53, loss = 0.02046772
Iteration 54, loss = 0.02022688
Iteration 55, loss = 0.02000912
Iteration 56, loss = 0.01976174
Iteration 57, loss = 0.01975881
Iteration 58, loss = 0.01944566
Iteration 59, loss = 0.01930265
Iteration 60, loss = 0.01932314
Iteration 61, loss = 0.01933853
Iteration 62, loss = 0.01911967
Iteration 63, loss = 0.01889136
Iteration 64, loss = 0.01898952
Iteration 65, loss = 0.01904237
Iteration 66, loss = 0.01886136
Iteration 67, loss = 0.01875841
Iteration 68, loss = 0.01878720
Iteration 69, loss = 0.01875628
Iteration 70, loss = 0.01857009
Iteration 71, loss = 0.01834241
Iteration 72, loss = 0.01831490
Iteration 73, loss = 0.01814685
Iteration 74, loss = 0.01823182
Iteration 75, loss = 0.01823653
Iteration 76, loss = 0.01820105
Iteration 77, loss = 0.01797978
Iteration 78, loss = 0.01792617
Iteration 79, loss = 0.01795541
Iteration 80, loss = 0.01786730
Iteration 81, loss = 0.01801149
Iteration 82, loss = 0.01796468
Iteration 83, loss = 0.01781886
Iteration 84, loss = 0.01804840
Iteration 85, loss = 0.01796274
Iteration 86, loss = 0.01789427
Iteration 87, loss = 0.01768901
Iteration 88, loss = 0.01762874
Iteration 89, loss = 0.01770980
Iteration 90, loss = 0.01765689
Iteration 91, loss = 0.01747339
Iteration 92, loss = 0.01764938
Iteration 93, loss = 0.01787140
Iteration 94, loss = 0.01756980
Iteration 95, loss = 0.01756022
Iteration 96, loss = 0.01748180
Iteration 97, loss = 0.01763608
Iteration 98, loss = 0.01741868
Iteration 99, loss = 0.01751405
Iteration 100, loss = 0.01733608
Iteration 101, loss = 0.01737120
Iteration 102, loss = 0.01743993
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67235025
Iteration 2, loss = 0.65474783
Iteration 3, loss = 0.63974914
Iteration 4, loss = 0.62491625
Iteration 5, loss = 0.60961164
Iteration 6, loss = 0.59347939
Iteration 7, loss = 0.57634771
Iteration 8, loss = 0.55811402
Iteration 9, loss = 0.53866602
Iteration 10, loss = 0.51789172
Iteration 11, loss = 0.49605697
Iteration 12, loss = 0.47355728
Iteration 13, loss = 0.45032665
Iteration 14, loss = 0.42679421
Iteration 15, loss = 0.40353810
Iteration 16, loss = 0.38060236
Iteration 17, loss = 0.35839601
Iteration 18, loss = 0.33707107
Iteration 19, loss = 0.31643998
Iteration 20, loss = 0.29693255
Iteration 21, loss = 0.27821906
Iteration 22, loss = 0.26066089
Iteration 23, loss = 0.24406836
Iteration 24, loss = 0.22859289
Iteration 25, loss = 0.21402825
Iteration 26, loss = 0.20049217
Iteration 27, loss = 0.18774342
Iteration 28, loss = 0.17602833
Iteration 29, loss = 0.16511772
Iteration 30, loss = 0.15491495
Iteration 31, loss = 0.14556485
Iteration 32, loss = 0.13688600
Iteration 33, loss = 0.12881934
Iteration 34, loss = 0.12141241
Iteration 35, loss = 0.11456401
Iteration 36, loss = 0.10815372
Iteration 37, loss = 0.10226197
Iteration 38, loss = 0.09676550
Iteration 39, loss = 0.09174409
Iteration 40, loss = 0.08703764
Iteration 41, loss = 0.08264733
Iteration 42, loss = 0.07866632
Iteration 43, loss = 0.07483468
Iteration 44, loss = 0.07131029
Iteration 45, loss = 0.06808836
Iteration 46, loss = 0.06500808
Iteration 47, loss = 0.06217965
Iteration 48, loss = 0.05949423
Iteration 49, loss = 0.05698703
Iteration 50, loss = 0.05460255
Iteration 51, loss = 0.05240302
Iteration 52, loss = 0.05033596
Iteration 53, loss = 0.04831085
Iteration 54, loss = 0.04648827
Iteration 55, loss = 0.04471819
Iteration 56, loss = 0.04303930
Iteration 57, loss = 0.04145584
Iteration 58, loss = 0.03996922
Iteration 59, loss = 0.03857979
Iteration 60, loss = 0.03725082
Iteration 61, loss = 0.03598887
Iteration 62, loss = 0.03480399
Iteration 63, loss = 0.03367187
Iteration 64, loss = 0.03259282
Iteration 65, loss = 0.03155067
Iteration 66, loss = 0.03056850
Iteration 67, loss = 0.02966594
Iteration 68, loss = 0.02875688
Iteration 69, loss = 0.02791681
Iteration 70, loss = 0.02711405
Iteration 71, loss = 0.02635883
Iteration 72, loss = 0.02559860
Iteration 73, loss = 0.02489046
Iteration 74, loss = 0.02423876
Iteration 75, loss = 0.02357525
Iteration 76, loss = 0.02294434
Iteration 77, loss = 0.02234076
Iteration 78, loss = 0.02177459
Iteration 79, loss = 0.02120801
Iteration 80, loss = 0.02067674
Iteration 81, loss = 0.02017087
Iteration 82, loss = 0.01967326
Iteration 83, loss = 0.01918647
Iteration 84, loss = 0.01873687
Iteration 85, loss = 0.01828570
Iteration 86, loss = 0.01786104
Iteration 87, loss = 0.01745381
Iteration 88, loss = 0.01705938
Iteration 89, loss = 0.01668117
Iteration 90, loss = 0.01631287
Iteration 91, loss = 0.01595915
Iteration 92, loss = 0.01562835
Iteration 93, loss = 0.01529767
Iteration 94, loss = 0.01497793
Iteration 95, loss = 0.01467402
Iteration 96, loss = 0.01437534
Iteration 97, loss = 0.01409524
Iteration 98, loss = 0.01382096
Iteration 99, loss = 0.01353275
Iteration 100, loss = 0.01326732
Iteration 101, loss = 0.01300961
Iteration 102, loss = 0.01275888
Iteration 103, loss = 0.01252209
Iteration 104, loss = 0.01229145
Iteration 105, loss = 0.01206804
Iteration 106, loss = 0.01184614
Iteration 107, loss = 0.01162839
Iteration 108, loss = 0.01142497
Iteration 109, loss = 0.01121883
Iteration 110, loss = 0.01102789
Iteration 111, loss = 0.01084101
Iteration 112, loss = 0.01065431
Iteration 113, loss = 0.01048165
Iteration 114, loss = 0.01030457
Iteration 115, loss = 0.01013937
Iteration 116, loss = 0.00997436
Iteration 117, loss = 0.00981505
Iteration 118, loss = 0.00966100
Iteration 119, loss = 0.00950893
Iteration 120, loss = 0.00936536
Iteration 121, loss = 0.00921550
Iteration 122, loss = 0.00907011
Iteration 123, loss = 0.00893178
Iteration 124, loss = 0.00879673
Iteration 125, loss = 0.00866093
Iteration 126, loss = 0.00853278
Iteration 127, loss = 0.00841698
Iteration 128, loss = 0.00829022
Iteration 129, loss = 0.00816864
Iteration 130, loss = 0.00805332
Iteration 131, loss = 0.00793863
Iteration 132, loss = 0.00782790
Iteration 133, loss = 0.00771772
Iteration 134, loss = 0.00761374
Iteration 135, loss = 0.00750780
Iteration 136, loss = 0.00740485
Iteration 137, loss = 0.00731046
Iteration 138, loss = 0.00720837
Iteration 139, loss = 0.00711840
Iteration 140, loss = 0.00703037
Iteration 141, loss = 0.00694105
Iteration 142, loss = 0.00685409
Iteration 143, loss = 0.00677067
Iteration 144, loss = 0.00668881
Iteration 145, loss = 0.00660269
Iteration 146, loss = 0.00652017
Iteration 147, loss = 0.00644385
Iteration 148, loss = 0.00635998
Iteration 149, loss = 0.00628589
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69712639
Iteration 2, loss = 0.67490251
Iteration 3, loss = 0.65460809
Iteration 4, loss = 0.63223641
Iteration 5, loss = 0.60691562
Iteration 6, loss = 0.57824940
Iteration 7, loss = 0.54648051
Iteration 8, loss = 0.51241338
Iteration 9, loss = 0.47713878
Iteration 10, loss = 0.44185748
Iteration 11, loss = 0.40711193
Iteration 12, loss = 0.37384930
Iteration 13, loss = 0.34247708
Iteration 14, loss = 0.31284795
Iteration 15, loss = 0.28542641
Iteration 16, loss = 0.26026143
Iteration 17, loss = 0.23724155
Iteration 18, loss = 0.21639040
Iteration 19, loss = 0.19746722
Iteration 20, loss = 0.18064520
Iteration 21, loss = 0.16540408
Iteration 22, loss = 0.15187026
Iteration 23, loss = 0.13970840
Iteration 24, loss = 0.12890801
Iteration 25, loss = 0.11921338
Iteration 26, loss = 0.11062115
Iteration 27, loss = 0.10286129
Iteration 28, loss = 0.09594186
Iteration 29, loss = 0.08964650
Iteration 30, loss = 0.08410195
Iteration 31, loss = 0.07897528
Iteration 32, loss = 0.07438382
Iteration 33, loss = 0.07020946
Iteration 34, loss = 0.06644918
Iteration 35, loss = 0.06301126
Iteration 36, loss = 0.05985240
Iteration 37, loss = 0.05693998
Iteration 38, loss = 0.05428780
Iteration 39, loss = 0.05184176
Iteration 40, loss = 0.04960118
Iteration 41, loss = 0.04757460
Iteration 42, loss = 0.04561994
Iteration 43, loss = 0.04385251
Iteration 44, loss = 0.04217845
Iteration 45, loss = 0.04071735
Iteration 46, loss = 0.03925101
Iteration 47, loss = 0.03791511
Iteration 48, loss = 0.03663934
Iteration 49, loss = 0.03551378
Iteration 50, loss = 0.03440875
Iteration 51, loss = 0.03338396
Iteration 52, loss = 0.03247984
Iteration 53, loss = 0.03152071
Iteration 54, loss = 0.03068277
Iteration 55, loss = 0.02985631
Iteration 56, loss = 0.02910327
Iteration 57, loss = 0.02842688
Iteration 58, loss = 0.02773821
Iteration 59, loss = 0.02704779
Iteration 60, loss = 0.02645086
Iteration 61, loss = 0.02581880
Iteration 62, loss = 0.02530287
Iteration 63, loss = 0.02476349
Iteration 64, loss = 0.02426122
Iteration 65, loss = 0.02377701
Iteration 66, loss = 0.02336121
Iteration 67, loss = 0.02295096
Iteration 68, loss = 0.02251207
Iteration 69, loss = 0.02222533
Iteration 70, loss = 0.02174586
Iteration 71, loss = 0.02136978
Iteration 72, loss = 0.02102640
Iteration 73, loss = 0.02072235
Iteration 74, loss = 0.02039277
Iteration 75, loss = 0.02009633
Iteration 76, loss = 0.01982126
Iteration 77, loss = 0.01952844
Iteration 78, loss = 0.01928401
Iteration 79, loss = 0.01901927
Iteration 80, loss = 0.01874919
Iteration 81, loss = 0.01855461
Iteration 82, loss = 0.01834030
Iteration 83, loss = 0.01806640
Iteration 84, loss = 0.01787964
Iteration 85, loss = 0.01765076
Iteration 86, loss = 0.01747764
Iteration 87, loss = 0.01728434
Iteration 88, loss = 0.01708691
Iteration 89, loss = 0.01692976
Iteration 90, loss = 0.01677147
Iteration 91, loss = 0.01656251
Iteration 92, loss = 0.01643661
Iteration 93, loss = 0.01631701
Iteration 94, loss = 0.01613645
Iteration 95, loss = 0.01600401
Iteration 96, loss = 0.01584849
Iteration 97, loss = 0.01579372
Iteration 98, loss = 0.01566182
Iteration 99, loss = 0.01549003
Iteration 100, loss = 0.01538645
Iteration 101, loss = 0.01524150
Iteration 102, loss = 0.01513258
Iteration 103, loss = 0.01501771
Iteration 104, loss = 0.01492332
Iteration 105, loss = 0.01476809
Iteration 106, loss = 0.01468711
Iteration 107, loss = 0.01458823
Iteration 108, loss = 0.01451371
Iteration 109, loss = 0.01442516
Iteration 110, loss = 0.01429464
Iteration 111, loss = 0.01425386
Iteration 112, loss = 0.01414322
Iteration 113, loss = 0.01406873
Iteration 114, loss = 0.01396086
Iteration 115, loss = 0.01387552
Iteration 116, loss = 0.01389149
Iteration 117, loss = 0.01374228
Iteration 118, loss = 0.01367752
Iteration 119, loss = 0.01361135
Iteration 120, loss = 0.01367095
Iteration 121, loss = 0.01347371
Iteration 122, loss = 0.01342095
Iteration 123, loss = 0.01341271
Iteration 124, loss = 0.01329578
Iteration 125, loss = 0.01321885
Iteration 126, loss = 0.01319506
Iteration 127, loss = 0.01315947
Iteration 128, loss = 0.01305461
Iteration 129, loss = 0.01299072
Iteration 130, loss = 0.01295190
Iteration 131, loss = 0.01289057
Iteration 132, loss = 0.01287599
Iteration 133, loss = 0.01278976
Iteration 134, loss = 0.01277832
Iteration 135, loss = 0.01272782
Iteration 136, loss = 0.01270580
Iteration 137, loss = 0.01263704
Iteration 138, loss = 0.01255685
Iteration 139, loss = 0.01255290
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71116952
Iteration 2, loss = 0.65510709
Iteration 3, loss = 0.57966082
Iteration 4, loss = 0.48791735
Iteration 5, loss = 0.39494459
Iteration 6, loss = 0.31218118
Iteration 7, loss = 0.24588367
Iteration 8, loss = 0.19543309
Iteration 9, loss = 0.15793690
Iteration 10, loss = 0.13039173
Iteration 11, loss = 0.10950432
Iteration 12, loss = 0.09383174
Iteration 13, loss = 0.08191763
Iteration 14, loss = 0.07247606
Iteration 15, loss = 0.06503465
Iteration 16, loss = 0.05903369
Iteration 17, loss = 0.05399512
Iteration 18, loss = 0.04997833
Iteration 19, loss = 0.04633623
Iteration 20, loss = 0.04345310
Iteration 21, loss = 0.04085563
Iteration 22, loss = 0.03893874
Iteration 23, loss = 0.03727988
Iteration 24, loss = 0.03547281
Iteration 25, loss = 0.03426192
Iteration 26, loss = 0.03274859
Iteration 27, loss = 0.03157883
Iteration 28, loss = 0.03065633
Iteration 29, loss = 0.02979592
Iteration 30, loss = 0.02880199
Iteration 31, loss = 0.02815587
Iteration 32, loss = 0.02763726
Iteration 33, loss = 0.02694520
Iteration 34, loss = 0.02643241
Iteration 35, loss = 0.02575149
Iteration 36, loss = 0.02537806
Iteration 37, loss = 0.02507506
Iteration 38, loss = 0.02468781
Iteration 39, loss = 0.02437732
Iteration 40, loss = 0.02388371
Iteration 41, loss = 0.02382741
Iteration 42, loss = 0.02349542
Iteration 43, loss = 0.02321530
Iteration 44, loss = 0.02300450
Iteration 45, loss = 0.02280180
Iteration 46, loss = 0.02257857
Iteration 47, loss = 0.02265389
Iteration 48, loss = 0.02207509
Iteration 49, loss = 0.02206431
Iteration 50, loss = 0.02182092
Iteration 51, loss = 0.02164840
Iteration 52, loss = 0.02164329
Iteration 53, loss = 0.02141210
Iteration 54, loss = 0.02125753
Iteration 55, loss = 0.02101285
Iteration 56, loss = 0.02091828
Iteration 57, loss = 0.02092586
Iteration 58, loss = 0.02078148
Iteration 59, loss = 0.02063400
Iteration 60, loss = 0.02055713
Iteration 61, loss = 0.02039982
Iteration 62, loss = 0.02027410
Iteration 63, loss = 0.02035259
Iteration 64, loss = 0.02038314
Iteration 65, loss = 0.02034498
Iteration 66, loss = 0.02010952
Iteration 67, loss = 0.01998130
Iteration 68, loss = 0.02010258
Iteration 69, loss = 0.01995248
Iteration 70, loss = 0.01985730
Iteration 71, loss = 0.01985637
Iteration 72, loss = 0.01961542
Iteration 73, loss = 0.01970843
Iteration 74, loss = 0.01936000
Iteration 75, loss = 0.01958574
Iteration 76, loss = 0.01949885
Iteration 77, loss = 0.01946204
Iteration 78, loss = 0.01923982
Iteration 79, loss = 0.01938662
Iteration 80, loss = 0.01947626
Iteration 81, loss = 0.01959152
Iteration 82, loss = 0.01923402
Iteration 83, loss = 0.01916924
Iteration 84, loss = 0.01919381
Iteration 85, loss = 0.01922304
Iteration 86, loss = 0.01942263
Iteration 87, loss = 0.01912026
Iteration 88, loss = 0.01899698
Iteration 89, loss = 0.01939244
Iteration 90, loss = 0.01914743
Iteration 91, loss = 0.01899993
Iteration 92, loss = 0.01905843
Iteration 93, loss = 0.01884311
Iteration 94, loss = 0.01903746
Iteration 95, loss = 0.01911765
Iteration 96, loss = 0.01896917
Iteration 97, loss = 0.01891412
Iteration 98, loss = 0.01889727
Iteration 99, loss = 0.01887138
Iteration 100, loss = 0.01896797
Iteration 101, loss = 0.01893924
Iteration 102, loss = 0.01886741
Iteration 103, loss = 0.01889296
Iteration 104, loss = 0.01875727
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68779153
Iteration 2, loss = 0.64452588
Iteration 3, loss = 0.57663846
Iteration 4, loss = 0.48815692
Iteration 5, loss = 0.39708126
Iteration 6, loss = 0.31597463
Iteration 7, loss = 0.24985049
Iteration 8, loss = 0.19933234
Iteration 9, loss = 0.16099415
Iteration 10, loss = 0.13278912
Iteration 11, loss = 0.11176717
Iteration 12, loss = 0.09593300
Iteration 13, loss = 0.08426046
Iteration 14, loss = 0.07430532
Iteration 15, loss = 0.06676091
Iteration 16, loss = 0.06086707
Iteration 17, loss = 0.05570624
Iteration 18, loss = 0.05175602
Iteration 19, loss = 0.04823101
Iteration 20, loss = 0.04535414
Iteration 21, loss = 0.04288338
Iteration 22, loss = 0.04087152
Iteration 23, loss = 0.03891338
Iteration 24, loss = 0.03741451
Iteration 25, loss = 0.03597258
Iteration 26, loss = 0.03479412
Iteration 27, loss = 0.03360698
Iteration 28, loss = 0.03288380
Iteration 29, loss = 0.03200963
Iteration 30, loss = 0.03094124
Iteration 31, loss = 0.03032698
Iteration 32, loss = 0.02963645
Iteration 33, loss = 0.02918130
Iteration 34, loss = 0.02864875
Iteration 35, loss = 0.02842920
Iteration 36, loss = 0.02773015
Iteration 37, loss = 0.02722334
Iteration 38, loss = 0.02701673
Iteration 39, loss = 0.02645487
Iteration 40, loss = 0.02635101
Iteration 41, loss = 0.02595142
Iteration 42, loss = 0.02571983
Iteration 43, loss = 0.02555574
Iteration 44, loss = 0.02526031
Iteration 45, loss = 0.02505651
Iteration 46, loss = 0.02475376
Iteration 47, loss = 0.02459313
Iteration 48, loss = 0.02476775
Iteration 49, loss = 0.02404111
Iteration 50, loss = 0.02411704
Iteration 51, loss = 0.02416550
Iteration 52, loss = 0.02397113
Iteration 53, loss = 0.02390471
Iteration 54, loss = 0.02351269
Iteration 55, loss = 0.02369873
Iteration 56, loss = 0.02363659
Iteration 57, loss = 0.02325502
Iteration 58, loss = 0.02328260
Iteration 59, loss = 0.02315616
Iteration 60, loss = 0.02309017
Iteration 61, loss = 0.02298596
Iteration 62, loss = 0.02295635
Iteration 63, loss = 0.02285921
Iteration 64, loss = 0.02267644
Iteration 65, loss = 0.02282147
Iteration 66, loss = 0.02293040
Iteration 67, loss = 0.02258492
Iteration 68, loss = 0.02252660
Iteration 69, loss = 0.02249137
Iteration 70, loss = 0.02237790
Iteration 71, loss = 0.02248963
Iteration 72, loss = 0.02234448
Iteration 73, loss = 0.02250055
Iteration 74, loss = 0.02232167
Iteration 75, loss = 0.02214607
Iteration 76, loss = 0.02238538
Iteration 77, loss = 0.02209584
Iteration 78, loss = 0.02197843
Iteration 79, loss = 0.02223186
Iteration 80, loss = 0.02190152
Iteration 81, loss = 0.02202811
Iteration 82, loss = 0.02189520
Iteration 83, loss = 0.02207018
Iteration 84, loss = 0.02171061
Iteration 85, loss = 0.02176023
Iteration 86, loss = 0.02194635
Iteration 87, loss = 0.02178241
Iteration 88, loss = 0.02174464
Iteration 89, loss = 0.02204343
Iteration 90, loss = 0.02191403
Iteration 91, loss = 0.02173035
Iteration 92, loss = 0.02206972
Iteration 93, loss = 0.02194126
Iteration 94, loss = 0.02137968
Iteration 95, loss = 0.02172415
Iteration 96, loss = 0.02170225
Iteration 97, loss = 0.02159555
Iteration 98, loss = 0.02160519
Iteration 99, loss = 0.02163720
Iteration 100, loss = 0.02166008
Iteration 101, loss = 0.02159855
Iteration 102, loss = 0.02151137
Iteration 103, loss = 0.02152272
Iteration 104, loss = 0.02152443
Iteration 105, loss = 0.02159719
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72587602
Iteration 2, loss = 0.70260902
Iteration 3, loss = 0.68232442
Iteration 4, loss = 0.66059074
Iteration 5, loss = 0.63615984
Iteration 6, loss = 0.60859641
Iteration 7, loss = 0.57859889
Iteration 8, loss = 0.54658413
Iteration 9, loss = 0.51312459
Iteration 10, loss = 0.47882450
Iteration 11, loss = 0.44482965
Iteration 12, loss = 0.41157908
Iteration 13, loss = 0.37960828
Iteration 14, loss = 0.34933122
Iteration 15, loss = 0.32104372
Iteration 16, loss = 0.29473742
Iteration 17, loss = 0.27048514
Iteration 18, loss = 0.24823557
Iteration 19, loss = 0.22802909
Iteration 20, loss = 0.20968485
Iteration 21, loss = 0.19312867
Iteration 22, loss = 0.17805882
Iteration 23, loss = 0.16471150
Iteration 24, loss = 0.15247643
Iteration 25, loss = 0.14155601
Iteration 26, loss = 0.13153434
Iteration 27, loss = 0.12264025
Iteration 28, loss = 0.11458769
Iteration 29, loss = 0.10725979
Iteration 30, loss = 0.10064426
Iteration 31, loss = 0.09477627
Iteration 32, loss = 0.08919555
Iteration 33, loss = 0.08421950
Iteration 34, loss = 0.07969087
Iteration 35, loss = 0.07549543
Iteration 36, loss = 0.07171294
Iteration 37, loss = 0.06824759
Iteration 38, loss = 0.06504443
Iteration 39, loss = 0.06209739
Iteration 40, loss = 0.05935123
Iteration 41, loss = 0.05678791
Iteration 42, loss = 0.05450954
Iteration 43, loss = 0.05232739
Iteration 44, loss = 0.05028411
Iteration 45, loss = 0.04843313
Iteration 46, loss = 0.04662812
Iteration 47, loss = 0.04500165
Iteration 48, loss = 0.04346617
Iteration 49, loss = 0.04214138
Iteration 50, loss = 0.04072513
Iteration 51, loss = 0.03946204
Iteration 52, loss = 0.03829390
Iteration 53, loss = 0.03712374
Iteration 54, loss = 0.03606303
Iteration 55, loss = 0.03507096
Iteration 56, loss = 0.03410300
Iteration 57, loss = 0.03322110
Iteration 58, loss = 0.03247568
Iteration 59, loss = 0.03165078
Iteration 60, loss = 0.03087592
Iteration 61, loss = 0.03020914
Iteration 62, loss = 0.02950825
Iteration 63, loss = 0.02889752
Iteration 64, loss = 0.02821159
Iteration 65, loss = 0.02766790
Iteration 66, loss = 0.02710552
Iteration 67, loss = 0.02659227
Iteration 68, loss = 0.02609023
Iteration 69, loss = 0.02559269
Iteration 70, loss = 0.02512608
Iteration 71, loss = 0.02468154
Iteration 72, loss = 0.02427215
Iteration 73, loss = 0.02388343
Iteration 74, loss = 0.02349465
Iteration 75, loss = 0.02309303
Iteration 76, loss = 0.02274523
Iteration 77, loss = 0.02237976
Iteration 78, loss = 0.02211341
Iteration 79, loss = 0.02179363
Iteration 80, loss = 0.02152002
Iteration 81, loss = 0.02123004
Iteration 82, loss = 0.02091375
Iteration 83, loss = 0.02067177
Iteration 84, loss = 0.02038400
Iteration 85, loss = 0.02015288
Iteration 86, loss = 0.01989130
Iteration 87, loss = 0.01965595
Iteration 88, loss = 0.01944596
Iteration 89, loss = 0.01925142
Iteration 90, loss = 0.01900383
Iteration 91, loss = 0.01883959
Iteration 92, loss = 0.01862182
Iteration 93, loss = 0.01842126
Iteration 94, loss = 0.01824672
Iteration 95, loss = 0.01806985
Iteration 96, loss = 0.01786446
Iteration 97, loss = 0.01772865
Iteration 98, loss = 0.01754329
Iteration 99, loss = 0.01746095
Iteration 100, loss = 0.01731403
Iteration 101, loss = 0.01712139
Iteration 102, loss = 0.01705689
Iteration 103, loss = 0.01686148
Iteration 104, loss = 0.01672541
Iteration 105, loss = 0.01668170
Iteration 106, loss = 0.01648485
Iteration 107, loss = 0.01637334
Iteration 108, loss = 0.01622673
Iteration 109, loss = 0.01611316
Iteration 110, loss = 0.01602149
Iteration 111, loss = 0.01590302
Iteration 112, loss = 0.01580090
Iteration 113, loss = 0.01569382
Iteration 114, loss = 0.01559320
Iteration 115, loss = 0.01548291
Iteration 116, loss = 0.01543162
Iteration 117, loss = 0.01528181
Iteration 118, loss = 0.01522366
Iteration 119, loss = 0.01513343
Iteration 120, loss = 0.01502556
Iteration 121, loss = 0.01507552
Iteration 122, loss = 0.01490583
Iteration 123, loss = 0.01476668
Iteration 124, loss = 0.01472295
Iteration 125, loss = 0.01469613
Iteration 126, loss = 0.01458869
Iteration 127, loss = 0.01456888
Iteration 128, loss = 0.01441782
Iteration 129, loss = 0.01442764
Iteration 130, loss = 0.01430540
Iteration 131, loss = 0.01433400
Iteration 132, loss = 0.01419431
Iteration 133, loss = 0.01409494
Iteration 134, loss = 0.01409074
Iteration 135, loss = 0.01404491
Iteration 136, loss = 0.01396474
Iteration 137, loss = 0.01390152
Iteration 138, loss = 0.01386378
Iteration 139, loss = 0.01386296
Iteration 140, loss = 0.01379518
Iteration 141, loss = 0.01376675
Iteration 142, loss = 0.01365312
Iteration 143, loss = 0.01361769
Iteration 144, loss = 0.01352855
Iteration 145, loss = 0.01347872
Iteration 146, loss = 0.01346406
Iteration 147, loss = 0.01344220
Iteration 148, loss = 0.01336984
Iteration 149, loss = 0.01333328
Iteration 150, loss = 0.01332077
Iteration 151, loss = 0.01321670
Iteration 152, loss = 0.01316162
Iteration 153, loss = 0.01318366
Iteration 154, loss = 0.01312272
Iteration 155, loss = 0.01312234
Iteration 156, loss = 0.01306513
Iteration 157, loss = 0.01300745
Iteration 158, loss = 0.01299268
Iteration 159, loss = 0.01299435
Iteration 160, loss = 0.01286489
Iteration 161, loss = 0.01283402
Iteration 162, loss = 0.01283449
Iteration 163, loss = 0.01278477
Iteration 164, loss = 0.01280476
Iteration 165, loss = 0.01269805
Iteration 166, loss = 0.01266529
Iteration 167, loss = 0.01265602
Iteration 168, loss = 0.01262433
Iteration 169, loss = 0.01258056
Iteration 170, loss = 0.01258162
Iteration 171, loss = 0.01256261
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70921484
Iteration 2, loss = 0.68056379
Iteration 3, loss = 0.64353078
Iteration 4, loss = 0.59775197
Iteration 5, loss = 0.54468990
Iteration 6, loss = 0.48645417
Iteration 7, loss = 0.42575443
Iteration 8, loss = 0.36641380
Iteration 9, loss = 0.31273412
Iteration 10, loss = 0.26593815
Iteration 11, loss = 0.22650193
Iteration 12, loss = 0.19353842
Iteration 13, loss = 0.16650280
Iteration 14, loss = 0.14445636
Iteration 15, loss = 0.12614351
Iteration 16, loss = 0.11103134
Iteration 17, loss = 0.09876430
Iteration 18, loss = 0.08831511
Iteration 19, loss = 0.07976384
Iteration 20, loss = 0.07228068
Iteration 21, loss = 0.06606156
Iteration 22, loss = 0.06065665
Iteration 23, loss = 0.05613083
Iteration 24, loss = 0.05211931
Iteration 25, loss = 0.04859410
Iteration 26, loss = 0.04559324
Iteration 27, loss = 0.04303035
Iteration 28, loss = 0.04048353
Iteration 29, loss = 0.03829408
Iteration 30, loss = 0.03655000
Iteration 31, loss = 0.03490397
Iteration 32, loss = 0.03319204
Iteration 33, loss = 0.03183058
Iteration 34, loss = 0.03061825
Iteration 35, loss = 0.02939373
Iteration 36, loss = 0.02837605
Iteration 37, loss = 0.02744367
Iteration 38, loss = 0.02659437
Iteration 39, loss = 0.02593919
Iteration 40, loss = 0.02505734
Iteration 41, loss = 0.02442347
Iteration 42, loss = 0.02378566
Iteration 43, loss = 0.02313928
Iteration 44, loss = 0.02268915
Iteration 45, loss = 0.02214641
Iteration 46, loss = 0.02173890
Iteration 47, loss = 0.02121735
Iteration 48, loss = 0.02081360
Iteration 49, loss = 0.02043181
Iteration 50, loss = 0.02018568
Iteration 51, loss = 0.01975030
Iteration 52, loss = 0.01948315
Iteration 53, loss = 0.01918310
Iteration 54, loss = 0.01892793
Iteration 55, loss = 0.01858935
Iteration 56, loss = 0.01836538
Iteration 57, loss = 0.01812219
Iteration 58, loss = 0.01795329
Iteration 59, loss = 0.01774525
Iteration 60, loss = 0.01750188
Iteration 61, loss = 0.01730941
Iteration 62, loss = 0.01722466
Iteration 63, loss = 0.01694616
Iteration 64, loss = 0.01683599
Iteration 65, loss = 0.01665637
Iteration 66, loss = 0.01653612
Iteration 67, loss = 0.01653361
Iteration 68, loss = 0.01633874
Iteration 69, loss = 0.01626705
Iteration 70, loss = 0.01595407
Iteration 71, loss = 0.01599831
Iteration 72, loss = 0.01586215
Iteration 73, loss = 0.01575720
Iteration 74, loss = 0.01559241
Iteration 75, loss = 0.01542589
Iteration 76, loss = 0.01535614
Iteration 77, loss = 0.01534074
Iteration 78, loss = 0.01521650
Iteration 79, loss = 0.01510407
Iteration 80, loss = 0.01502004
Iteration 81, loss = 0.01499507
Iteration 82, loss = 0.01488391
Iteration 83, loss = 0.01485775
Iteration 84, loss = 0.01473334
Iteration 85, loss = 0.01482088
Iteration 86, loss = 0.01469553
Iteration 87, loss = 0.01460566
Iteration 88, loss = 0.01463707
Iteration 89, loss = 0.01443751
Iteration 90, loss = 0.01449278
Iteration 91, loss = 0.01435753
Iteration 92, loss = 0.01447840
Iteration 93, loss = 0.01446426
Iteration 94, loss = 0.01429500
Iteration 95, loss = 0.01428357
Iteration 96, loss = 0.01418180
Iteration 97, loss = 0.01411090
Iteration 98, loss = 0.01407211
Iteration 99, loss = 0.01399155
Iteration 100, loss = 0.01397557
Iteration 101, loss = 0.01393780
Iteration 102, loss = 0.01393682
Iteration 103, loss = 0.01387452
Iteration 104, loss = 0.01384588
Iteration 105, loss = 0.01394072
Iteration 106, loss = 0.01382546
Iteration 107, loss = 0.01387139
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70120502
Iteration 2, loss = 0.66733423
Iteration 3, loss = 0.61499691
Iteration 4, loss = 0.54632748
Iteration 5, loss = 0.47004353
Iteration 6, loss = 0.39595913
Iteration 7, loss = 0.32977927
Iteration 8, loss = 0.27307611
Iteration 9, loss = 0.22571730
Iteration 10, loss = 0.18785981
Iteration 11, loss = 0.15779537
Iteration 12, loss = 0.13424987
Iteration 13, loss = 0.11549328
Iteration 14, loss = 0.10070120
Iteration 15, loss = 0.08857833
Iteration 16, loss = 0.07872860
Iteration 17, loss = 0.07080830
Iteration 18, loss = 0.06414040
Iteration 19, loss = 0.05862144
Iteration 20, loss = 0.05385494
Iteration 21, loss = 0.04983708
Iteration 22, loss = 0.04639969
Iteration 23, loss = 0.04327364
Iteration 24, loss = 0.04083597
Iteration 25, loss = 0.03842183
Iteration 26, loss = 0.03647177
Iteration 27, loss = 0.03466643
Iteration 28, loss = 0.03310307
Iteration 29, loss = 0.03160257
Iteration 30, loss = 0.03047264
Iteration 31, loss = 0.02922732
Iteration 32, loss = 0.02824251
Iteration 33, loss = 0.02741028
Iteration 34, loss = 0.02647230
Iteration 35, loss = 0.02565109
Iteration 36, loss = 0.02502758
Iteration 37, loss = 0.02429964
Iteration 38, loss = 0.02370820
Iteration 39, loss = 0.02331245
Iteration 40, loss = 0.02261653
Iteration 41, loss = 0.02216604
Iteration 42, loss = 0.02168327
Iteration 43, loss = 0.02155445
Iteration 44, loss = 0.02111716
Iteration 45, loss = 0.02068549
Iteration 46, loss = 0.02030826
Iteration 47, loss = 0.02006359
Iteration 48, loss = 0.01971994
Iteration 49, loss = 0.01954068
Iteration 50, loss = 0.01918638
Iteration 51, loss = 0.01896347
Iteration 52, loss = 0.01874173
Iteration 53, loss = 0.01866013
Iteration 54, loss = 0.01836073
Iteration 55, loss = 0.01813571
Iteration 56, loss = 0.01801220
Iteration 57, loss = 0.01780994
Iteration 58, loss = 0.01770485
Iteration 59, loss = 0.01754253
Iteration 60, loss = 0.01750176
Iteration 61, loss = 0.01727927
Iteration 62, loss = 0.01728939
Iteration 63, loss = 0.01692540
Iteration 64, loss = 0.01677121
Iteration 65, loss = 0.01673852
Iteration 66, loss = 0.01668433
Iteration 67, loss = 0.01655494
Iteration 68, loss = 0.01661123
Iteration 69, loss = 0.01644961
Iteration 70, loss = 0.01631140
Iteration 71, loss = 0.01638446
Iteration 72, loss = 0.01612494
Iteration 73, loss = 0.01608062
Iteration 74, loss = 0.01602386
Iteration 75, loss = 0.01590400
Iteration 76, loss = 0.01599117
Iteration 77, loss = 0.01573759
Iteration 78, loss = 0.01573012
Iteration 79, loss = 0.01557443
Iteration 80, loss = 0.01551153
Iteration 81, loss = 0.01548785
Iteration 82, loss = 0.01556989
Iteration 83, loss = 0.01545676
Iteration 84, loss = 0.01537404
Iteration 85, loss = 0.01546438
Iteration 86, loss = 0.01531390
Iteration 87, loss = 0.01530692
Iteration 88, loss = 0.01519458
Iteration 89, loss = 0.01513815
Iteration 90, loss = 0.01511343
Iteration 91, loss = 0.01527213
Iteration 92, loss = 0.01504440
Iteration 93, loss = 0.01518525
Iteration 94, loss = 0.01507984
Iteration 95, loss = 0.01496479
Iteration 96, loss = 0.01489191
Iteration 97, loss = 0.01495438
Iteration 98, loss = 0.01487845
Iteration 99, loss = 0.01486243
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67576982
Iteration 2, loss = 0.60906755
Iteration 3, loss = 0.50550041
Iteration 4, loss = 0.39784741
Iteration 5, loss = 0.30496653
Iteration 6, loss = 0.23256084
Iteration 7, loss = 0.18025867
Iteration 8, loss = 0.14306300
Iteration 9, loss = 0.11685594
Iteration 10, loss = 0.09815942
Iteration 11, loss = 0.08420005
Iteration 12, loss = 0.07360383
Iteration 13, loss = 0.06558935
Iteration 14, loss = 0.05922106
Iteration 15, loss = 0.05432376
Iteration 16, loss = 0.05019966
Iteration 17, loss = 0.04679370
Iteration 18, loss = 0.04366746
Iteration 19, loss = 0.04147110
Iteration 20, loss = 0.03967555
Iteration 21, loss = 0.03791164
Iteration 22, loss = 0.03637313
Iteration 23, loss = 0.03499693
Iteration 24, loss = 0.03411445
Iteration 25, loss = 0.03307460
Iteration 26, loss = 0.03213778
Iteration 27, loss = 0.03117677
Iteration 28, loss = 0.03076773
Iteration 29, loss = 0.02995494
Iteration 30, loss = 0.02961165
Iteration 31, loss = 0.02895500
Iteration 32, loss = 0.02838090
Iteration 33, loss = 0.02821093
Iteration 34, loss = 0.02768195
Iteration 35, loss = 0.02752478
Iteration 36, loss = 0.02730819
Iteration 37, loss = 0.02689123
Iteration 38, loss = 0.02672004
Iteration 39, loss = 0.02610344
Iteration 40, loss = 0.02595111
Iteration 41, loss = 0.02614848
Iteration 42, loss = 0.02570147
Iteration 43, loss = 0.02552125
Iteration 44, loss = 0.02516893
Iteration 45, loss = 0.02544901
Iteration 46, loss = 0.02495798
Iteration 47, loss = 0.02475748
Iteration 48, loss = 0.02492079
Iteration 49, loss = 0.02452103
Iteration 50, loss = 0.02445775
Iteration 51, loss = 0.02436385
Iteration 52, loss = 0.02430756
Iteration 53, loss = 0.02441968
Iteration 54, loss = 0.02409543
Iteration 55, loss = 0.02412954
Iteration 56, loss = 0.02424415
Iteration 57, loss = 0.02393404
Iteration 58, loss = 0.02380953
Iteration 59, loss = 0.02394506
Iteration 60, loss = 0.02365268
Iteration 61, loss = 0.02373931
Iteration 62, loss = 0.02358915
Iteration 63, loss = 0.02333946
Iteration 64, loss = 0.02334249
Iteration 65, loss = 0.02350641
Iteration 66, loss = 0.02349233
Iteration 67, loss = 0.02351706
Iteration 68, loss = 0.02325391
Iteration 69, loss = 0.02335346
Iteration 70, loss = 0.02359954
Iteration 71, loss = 0.02328377
Iteration 72, loss = 0.02317097
Iteration 73, loss = 0.02311906
Iteration 74, loss = 0.02306164
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68752075
Iteration 2, loss = 0.63422682
Iteration 3, loss = 0.55634248
Iteration 4, loss = 0.46096245
Iteration 5, loss = 0.36919473
Iteration 6, loss = 0.28943468
Iteration 7, loss = 0.22692726
Iteration 8, loss = 0.18048717
Iteration 9, loss = 0.14617103
Iteration 10, loss = 0.12111166
Iteration 11, loss = 0.10289538
Iteration 12, loss = 0.08924193
Iteration 13, loss = 0.07861457
Iteration 14, loss = 0.07040363
Iteration 15, loss = 0.06388607
Iteration 16, loss = 0.05857136
Iteration 17, loss = 0.05448500
Iteration 18, loss = 0.05084056
Iteration 19, loss = 0.04774420
Iteration 20, loss = 0.04550117
Iteration 21, loss = 0.04309000
Iteration 22, loss = 0.04137080
Iteration 23, loss = 0.03965962
Iteration 24, loss = 0.03835256
Iteration 25, loss = 0.03713481
Iteration 26, loss = 0.03633357
Iteration 27, loss = 0.03513979
Iteration 28, loss = 0.03403363
Iteration 29, loss = 0.03346950
Iteration 30, loss = 0.03270110
Iteration 31, loss = 0.03233041
Iteration 32, loss = 0.03157122
Iteration 33, loss = 0.03112245
Iteration 34, loss = 0.03073435
Iteration 35, loss = 0.03031373
Iteration 36, loss = 0.03008763
Iteration 37, loss = 0.02936388
Iteration 38, loss = 0.02911369
Iteration 39, loss = 0.02889880
Iteration 40, loss = 0.02847616
Iteration 41, loss = 0.02909152
Iteration 42, loss = 0.02861879
Iteration 43, loss = 0.02810719
Iteration 44, loss = 0.02783748
Iteration 45, loss = 0.02760228
Iteration 46, loss = 0.02742518
Iteration 47, loss = 0.02702398
Iteration 48, loss = 0.02682510
Iteration 49, loss = 0.02692471
Iteration 50, loss = 0.02656182
Iteration 51, loss = 0.02674306
Iteration 52, loss = 0.02646716
Iteration 53, loss = 0.02636710
Iteration 54, loss = 0.02624872
Iteration 55, loss = 0.02631396
Iteration 56, loss = 0.02594025
Iteration 57, loss = 0.02585693
Iteration 58, loss = 0.02603368
Iteration 59, loss = 0.02572435
Iteration 60, loss = 0.02580220
Iteration 61, loss = 0.02525771
Iteration 62, loss = 0.02569875
Iteration 63, loss = 0.02557740
Iteration 64, loss = 0.02549790
Iteration 65, loss = 0.02559639
Iteration 66, loss = 0.02509382
Iteration 67, loss = 0.02518622
Iteration 68, loss = 0.02515241
Iteration 69, loss = 0.02479851
Iteration 70, loss = 0.02525587
Iteration 71, loss = 0.02525913
Iteration 72, loss = 0.02499342
Iteration 73, loss = 0.02505229
Iteration 74, loss = 0.02489220
Iteration 75, loss = 0.02478452
Iteration 76, loss = 0.02449875
Iteration 77, loss = 0.02489697
Iteration 78, loss = 0.02447575
Iteration 79, loss = 0.02445015
Iteration 80, loss = 0.02430766
Iteration 81, loss = 0.02464442
Iteration 82, loss = 0.02440562
Iteration 83, loss = 0.02488622
Iteration 84, loss = 0.02462304
Iteration 85, loss = 0.02458603
Iteration 86, loss = 0.02450168
Iteration 87, loss = 0.02472416
Iteration 88, loss = 0.02456225
Iteration 89, loss = 0.02434533
Iteration 90, loss = 0.02428016
Iteration 91, loss = 0.02417118
Iteration 92, loss = 0.02418598
Iteration 93, loss = 0.02444212
Iteration 94, loss = 0.02425782
Iteration 95, loss = 0.02434024
Iteration 96, loss = 0.02786027
Iteration 97, loss = 0.02965072
Iteration 98, loss = 0.02838102
Iteration 99, loss = 0.02787749
Iteration 100, loss = 0.02715179
Iteration 101, loss = 0.02663938
Iteration 102, loss = 0.02626437
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67550330
Iteration 2, loss = 0.60170797
Iteration 3, loss = 0.49313701
Iteration 4, loss = 0.37983918
Iteration 5, loss = 0.28443651
Iteration 6, loss = 0.21324415
Iteration 7, loss = 0.16372981
Iteration 8, loss = 0.13008520
Iteration 9, loss = 0.10660477
Iteration 10, loss = 0.08993104
Iteration 11, loss = 0.07783842
Iteration 12, loss = 0.06857303
Iteration 13, loss = 0.06162709
Iteration 14, loss = 0.05590142
Iteration 15, loss = 0.05175399
Iteration 16, loss = 0.04795879
Iteration 17, loss = 0.04508994
Iteration 18, loss = 0.04270718
Iteration 19, loss = 0.04077581
Iteration 20, loss = 0.03894332
Iteration 21, loss = 0.03747031
Iteration 22, loss = 0.03611345
Iteration 23, loss = 0.03492300
Iteration 24, loss = 0.03419141
Iteration 25, loss = 0.03327292
Iteration 26, loss = 0.03230723
Iteration 27, loss = 0.03153871
Iteration 28, loss = 0.03140444
Iteration 29, loss = 0.03085512
Iteration 30, loss = 0.03000770
Iteration 31, loss = 0.02969647
Iteration 32, loss = 0.02923129
Iteration 33, loss = 0.02916218
Iteration 34, loss = 0.02850342
Iteration 35, loss = 0.02823477
Iteration 36, loss = 0.02795034
Iteration 37, loss = 0.02782791
Iteration 38, loss = 0.02781304
Iteration 39, loss = 0.02734600
Iteration 40, loss = 0.02727316
Iteration 41, loss = 0.02705194
Iteration 42, loss = 0.02711177
Iteration 43, loss = 0.02650332
Iteration 44, loss = 0.02659962
Iteration 45, loss = 0.02639333
Iteration 46, loss = 0.02607664
Iteration 47, loss = 0.02624885
Iteration 48, loss = 0.02616865
Iteration 49, loss = 0.02571048
Iteration 50, loss = 0.02599937
Iteration 51, loss = 0.02570195
Iteration 52, loss = 0.02575608
Iteration 53, loss = 0.02569258
Iteration 54, loss = 0.02542766
Iteration 55, loss = 0.02522660
Iteration 56, loss = 0.02535863
Iteration 57, loss = 0.02505995
Iteration 58, loss = 0.02522607
Iteration 59, loss = 0.02535154
Iteration 60, loss = 0.02538213
Iteration 61, loss = 0.02499563
Iteration 62, loss = 0.02524340
Iteration 63, loss = 0.02497605
Iteration 64, loss = 0.02507365
Iteration 65, loss = 0.02511162
Iteration 66, loss = 0.02503434
Iteration 67, loss = 0.02520440
Iteration 68, loss = 0.02496843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71168579
Iteration 2, loss = 0.65055561
Iteration 3, loss = 0.56601184
Iteration 4, loss = 0.46618088
Iteration 5, loss = 0.36941900
Iteration 6, loss = 0.28659077
Iteration 7, loss = 0.22302110
Iteration 8, loss = 0.17602620
Iteration 9, loss = 0.14186042
Iteration 10, loss = 0.11719119
Iteration 11, loss = 0.09871981
Iteration 12, loss = 0.08515550
Iteration 13, loss = 0.07472025
Iteration 14, loss = 0.06650589
Iteration 15, loss = 0.06023321
Iteration 16, loss = 0.05493597
Iteration 17, loss = 0.05070634
Iteration 18, loss = 0.04730859
Iteration 19, loss = 0.04410726
Iteration 20, loss = 0.04183542
Iteration 21, loss = 0.03969590
Iteration 22, loss = 0.03782372
Iteration 23, loss = 0.03619443
Iteration 24, loss = 0.03486164
Iteration 25, loss = 0.03359735
Iteration 26, loss = 0.03392963
Iteration 27, loss = 0.03265374
Iteration 28, loss = 0.03122185
Iteration 29, loss = 0.03066658
Iteration 30, loss = 0.02989461
Iteration 31, loss = 0.03010301
Iteration 32, loss = 0.02969466
Iteration 33, loss = 0.02847270
Iteration 34, loss = 0.02814154
Iteration 35, loss = 0.02763821
Iteration 36, loss = 0.02700506
Iteration 37, loss = 0.02704974
Iteration 38, loss = 0.02639472
Iteration 39, loss = 0.02609423
Iteration 40, loss = 0.02582221
Iteration 41, loss = 0.02531660
Iteration 42, loss = 0.02503845
Iteration 43, loss = 0.02469089
Iteration 44, loss = 0.02561276
Iteration 45, loss = 0.02577973
Iteration 46, loss = 0.02525029
Iteration 47, loss = 0.02465326
Iteration 48, loss = 0.02453312
Iteration 49, loss = 0.02425620
Iteration 50, loss = 0.02394200
Iteration 51, loss = 0.02350585
Iteration 52, loss = 0.02332802
Iteration 53, loss = 0.02310968
Iteration 54, loss = 0.02299381
Iteration 55, loss = 0.02319049
Iteration 56, loss = 0.02348325
Iteration 57, loss = 0.02352754
Iteration 58, loss = 0.02296326
Iteration 59, loss = 0.02285628
Iteration 60, loss = 0.02273145
Iteration 61, loss = 0.02261621
Iteration 62, loss = 0.02246416
Iteration 63, loss = 0.02224482
Iteration 64, loss = 0.02210021
Iteration 65, loss = 0.02185171
Iteration 66, loss = 0.02178783
Iteration 67, loss = 0.02167142
Iteration 68, loss = 0.02142669
Iteration 69, loss = 0.02138463
Iteration 70, loss = 0.02132534
Iteration 71, loss = 0.02124943
Iteration 72, loss = 0.02137714
Iteration 73, loss = 0.02115901
Iteration 74, loss = 0.02109778
Iteration 75, loss = 0.02113230
Iteration 76, loss = 0.02099545
Iteration 77, loss = 0.02082515
Iteration 78, loss = 0.02072040
Iteration 79, loss = 0.02064792
Iteration 80, loss = 0.02074989
Iteration 81, loss = 0.02065699
Iteration 82, loss = 0.02054024
Iteration 83, loss = 0.02063293
Iteration 84, loss = 0.02074104
Iteration 85, loss = 0.02036887
Iteration 86, loss = 0.02058696
Iteration 87, loss = 0.02039238
Iteration 88, loss = 0.02005388
Iteration 89, loss = 0.02026314
Iteration 90, loss = 0.02031630
Iteration 91, loss = 0.02033230
Iteration 92, loss = 0.02047463
Iteration 93, loss = 0.02042573
Iteration 94, loss = 0.02015519
Iteration 95, loss = 0.02057558
Iteration 96, loss = 0.02004251
Iteration 97, loss = 0.02020946
Iteration 98, loss = 0.01999780
Iteration 99, loss = 0.02011357
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69897329
Iteration 2, loss = 0.62754470
Iteration 3, loss = 0.53358245
Iteration 4, loss = 0.42012562
Iteration 5, loss = 0.31905373
Iteration 6, loss = 0.24026923
Iteration 7, loss = 0.18393723
Iteration 8, loss = 0.14426724
Iteration 9, loss = 0.11653336
Iteration 10, loss = 0.09658511
Iteration 11, loss = 0.08235199
Iteration 12, loss = 0.07133011
Iteration 13, loss = 0.06286196
Iteration 14, loss = 0.05643400
Iteration 15, loss = 0.05129728
Iteration 16, loss = 0.04732485
Iteration 17, loss = 0.04359158
Iteration 18, loss = 0.04062688
Iteration 19, loss = 0.03839551
Iteration 20, loss = 0.03630311
Iteration 21, loss = 0.03475367
Iteration 22, loss = 0.03293368
Iteration 23, loss = 0.03221141
Iteration 24, loss = 0.03066148
Iteration 25, loss = 0.02970906
Iteration 26, loss = 0.02865553
Iteration 27, loss = 0.02809572
Iteration 28, loss = 0.02750981
Iteration 29, loss = 0.02658806
Iteration 30, loss = 0.02598254
Iteration 31, loss = 0.02570435
Iteration 32, loss = 0.02531948
Iteration 33, loss = 0.02464763
Iteration 34, loss = 0.02445482
Iteration 35, loss = 0.02390855
Iteration 36, loss = 0.02359003
Iteration 37, loss = 0.02317379
Iteration 38, loss = 0.02320102
Iteration 39, loss = 0.02274893
Iteration 40, loss = 0.02270108
Iteration 41, loss = 0.02240760
Iteration 42, loss = 0.02203772
Iteration 43, loss = 0.02192683
Iteration 44, loss = 0.02188779
Iteration 45, loss = 0.02157264
Iteration 46, loss = 0.02136214
Iteration 47, loss = 0.02139873
Iteration 48, loss = 0.02130241
Iteration 49, loss = 0.02116317
Iteration 50, loss = 0.02149599
Iteration 51, loss = 0.02088378
Iteration 52, loss = 0.02084780
Iteration 53, loss = 0.02067630
Iteration 54, loss = 0.02067803
Iteration 55, loss = 0.02054708
Iteration 56, loss = 0.02051187
Iteration 57, loss = 0.02060943
Iteration 58, loss = 0.02024527
Iteration 59, loss = 0.02036847
Iteration 60, loss = 0.02025368
Iteration 61, loss = 0.02014588
Iteration 62, loss = 0.02020735
Iteration 63, loss = 0.01992631
Iteration 64, loss = 0.01975795
Iteration 65, loss = 0.01986856
Iteration 66, loss = 0.01993221
Iteration 67, loss = 0.02000341
Iteration 68, loss = 0.01982140
Iteration 69, loss = 0.01977561
Iteration 70, loss = 0.01951352
Iteration 71, loss = 0.01966111
Iteration 72, loss = 0.01958296
Iteration 73, loss = 0.01943873
Iteration 74, loss = 0.01955772
Iteration 75, loss = 0.01943444
Iteration 76, loss = 0.01937932
Iteration 77, loss = 0.01942262
Iteration 78, loss = 0.01971088
Iteration 79, loss = 0.01961441
Iteration 80, loss = 0.01938466
Iteration 81, loss = 0.01979649
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67704996
Iteration 2, loss = 0.63380163
Iteration 3, loss = 0.57659704
Iteration 4, loss = 0.50758299
Iteration 5, loss = 0.43594626
Iteration 6, loss = 0.36836512
Iteration 7, loss = 0.30818716
Iteration 8, loss = 0.25661923
Iteration 9, loss = 0.21397475
Iteration 10, loss = 0.17948913
Iteration 11, loss = 0.15199407
Iteration 12, loss = 0.12984538
Iteration 13, loss = 0.11230609
Iteration 14, loss = 0.09843674
Iteration 15, loss = 0.08689365
Iteration 16, loss = 0.07752572
Iteration 17, loss = 0.06977424
Iteration 18, loss = 0.06337579
Iteration 19, loss = 0.05809041
Iteration 20, loss = 0.05347816
Iteration 21, loss = 0.04956501
Iteration 22, loss = 0.04630210
Iteration 23, loss = 0.04343836
Iteration 24, loss = 0.04080024
Iteration 25, loss = 0.03853650
Iteration 26, loss = 0.03672459
Iteration 27, loss = 0.03497409
Iteration 28, loss = 0.03349836
Iteration 29, loss = 0.03201613
Iteration 30, loss = 0.03073257
Iteration 31, loss = 0.02966539
Iteration 32, loss = 0.02874404
Iteration 33, loss = 0.02770184
Iteration 34, loss = 0.02693768
Iteration 35, loss = 0.02622927
Iteration 36, loss = 0.02557184
Iteration 37, loss = 0.02484317
Iteration 38, loss = 0.02431216
Iteration 39, loss = 0.02377522
Iteration 40, loss = 0.02333861
Iteration 41, loss = 0.02283924
Iteration 42, loss = 0.02237143
Iteration 43, loss = 0.02192840
Iteration 44, loss = 0.02155994
Iteration 45, loss = 0.02131069
Iteration 46, loss = 0.02094000
Iteration 47, loss = 0.02066563
Iteration 48, loss = 0.02030263
Iteration 49, loss = 0.02001704
Iteration 50, loss = 0.01988078
Iteration 51, loss = 0.01962214
Iteration 52, loss = 0.01956941
Iteration 53, loss = 0.01931709
Iteration 54, loss = 0.01894824
Iteration 55, loss = 0.01876412
Iteration 56, loss = 0.01867505
Iteration 57, loss = 0.01859734
Iteration 58, loss = 0.01834769
Iteration 59, loss = 0.01831885
Iteration 60, loss = 0.01812905
Iteration 61, loss = 0.01782732
Iteration 62, loss = 0.01781824
Iteration 63, loss = 0.01765627
Iteration 64, loss = 0.01760173
Iteration 65, loss = 0.01737335
Iteration 66, loss = 0.01734034
Iteration 67, loss = 0.01728588
Iteration 68, loss = 0.01713516
Iteration 69, loss = 0.01713238
Iteration 70, loss = 0.01689519
Iteration 71, loss = 0.01689054
Iteration 72, loss = 0.01681024
Iteration 73, loss = 0.01670201
Iteration 74, loss = 0.01668490
Iteration 75, loss = 0.01668710
Iteration 76, loss = 0.01645798
Iteration 77, loss = 0.01655224
Iteration 78, loss = 0.01633308
Iteration 79, loss = 0.01634975
Iteration 80, loss = 0.01620394
Iteration 81, loss = 0.01630645
Iteration 82, loss = 0.01639106
Iteration 83, loss = 0.01605516
Iteration 84, loss = 0.01603126
Iteration 85, loss = 0.01604560
Iteration 86, loss = 0.01597603
Iteration 87, loss = 0.01581843
Iteration 88, loss = 0.01582886
Iteration 89, loss = 0.01570046
Iteration 90, loss = 0.01595844
Iteration 91, loss = 0.01577725
Iteration 92, loss = 0.01567037
Iteration 93, loss = 0.01571108
Iteration 94, loss = 0.01572641
Iteration 95, loss = 0.01557348
Iteration 96, loss = 0.01560277
Iteration 97, loss = 0.01550067
Iteration 98, loss = 0.01552987
Iteration 99, loss = 0.01555848
Iteration 100, loss = 0.01544325
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69802355
Iteration 2, loss = 0.66116809
Iteration 3, loss = 0.60344876
Iteration 4, loss = 0.52496576
Iteration 5, loss = 0.44002683
Iteration 6, loss = 0.35898226
Iteration 7, loss = 0.29021596
Iteration 8, loss = 0.23435203
Iteration 9, loss = 0.19091530
Iteration 10, loss = 0.15751998
Iteration 11, loss = 0.13223199
Iteration 12, loss = 0.11254374
Iteration 13, loss = 0.09751862
Iteration 14, loss = 0.08542958
Iteration 15, loss = 0.07581062
Iteration 16, loss = 0.06823399
Iteration 17, loss = 0.06174099
Iteration 18, loss = 0.05672676
Iteration 19, loss = 0.05222836
Iteration 20, loss = 0.04847352
Iteration 21, loss = 0.04523389
Iteration 22, loss = 0.04245065
Iteration 23, loss = 0.04018951
Iteration 24, loss = 0.03814939
Iteration 25, loss = 0.03628009
Iteration 26, loss = 0.03466263
Iteration 27, loss = 0.03334399
Iteration 28, loss = 0.03236694
Iteration 29, loss = 0.03111318
Iteration 30, loss = 0.03010936
Iteration 31, loss = 0.02947190
Iteration 32, loss = 0.02837098
Iteration 33, loss = 0.02757845
Iteration 34, loss = 0.02678633
Iteration 35, loss = 0.02622128
Iteration 36, loss = 0.02568263
Iteration 37, loss = 0.02531579
Iteration 38, loss = 0.02467422
Iteration 39, loss = 0.02432689
Iteration 40, loss = 0.02367043
Iteration 41, loss = 0.02339572
Iteration 42, loss = 0.02312600
Iteration 43, loss = 0.02283854
Iteration 44, loss = 0.02240111
Iteration 45, loss = 0.02217044
Iteration 46, loss = 0.02216703
Iteration 47, loss = 0.02178906
Iteration 48, loss = 0.02155526
Iteration 49, loss = 0.02114085
Iteration 50, loss = 0.02094949
Iteration 51, loss = 0.02109491
Iteration 52, loss = 0.02078875
Iteration 53, loss = 0.02060698
Iteration 54, loss = 0.02041938
Iteration 55, loss = 0.02037317
Iteration 56, loss = 0.02042102
Iteration 57, loss = 0.02002436
Iteration 58, loss = 0.01978374
Iteration 59, loss = 0.01968814
Iteration 60, loss = 0.01992749
Iteration 61, loss = 0.01949041
Iteration 62, loss = 0.01953930
Iteration 63, loss = 0.01919418
Iteration 64, loss = 0.01910056
Iteration 65, loss = 0.01942578
Iteration 66, loss = 0.01918368
Iteration 67, loss = 0.01898254
Iteration 68, loss = 0.01900786
Iteration 69, loss = 0.01881688
Iteration 70, loss = 0.01891704
Iteration 71, loss = 0.01889275
Iteration 72, loss = 0.01878244
Iteration 73, loss = 0.01856816
Iteration 74, loss = 0.01856875
Iteration 75, loss = 0.01838346
Iteration 76, loss = 0.01847844
Iteration 77, loss = 0.01852922
Iteration 78, loss = 0.01842272
Iteration 79, loss = 0.01820365
Iteration 80, loss = 0.01815616
Iteration 81, loss = 0.01817416
Iteration 82, loss = 0.01799576
Iteration 83, loss = 0.01803613
Iteration 84, loss = 0.01803199
Iteration 85, loss = 0.01811374
Iteration 86, loss = 0.01796927
Iteration 87, loss = 0.01809218
Iteration 88, loss = 0.01773050
Iteration 89, loss = 0.01790902
Iteration 90, loss = 0.01784338
Iteration 91, loss = 0.01786824
Iteration 92, loss = 0.01783659
Iteration 93, loss = 0.01759416
Iteration 94, loss = 0.01774870
Iteration 95, loss = 0.01753560
Iteration 96, loss = 0.01787431
Iteration 97, loss = 0.01772705
Iteration 98, loss = 0.01777077
Iteration 99, loss = 0.01764523
Iteration 100, loss = 0.01750023
Iteration 101, loss = 0.01751103
Iteration 102, loss = 0.01746517
Iteration 103, loss = 0.01767982
Iteration 104, loss = 0.01795977
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70191694
Iteration 2, loss = 0.67667264
Iteration 3, loss = 0.64794109
Iteration 4, loss = 0.61173607
Iteration 5, loss = 0.56924721
Iteration 6, loss = 0.52173718
Iteration 7, loss = 0.47179080
Iteration 8, loss = 0.42226677
Iteration 9, loss = 0.37528076
Iteration 10, loss = 0.33151723
Iteration 11, loss = 0.29192050
Iteration 12, loss = 0.25660209
Iteration 13, loss = 0.22585578
Iteration 14, loss = 0.19916340
Iteration 15, loss = 0.17628654
Iteration 16, loss = 0.15662157
Iteration 17, loss = 0.14001098
Iteration 18, loss = 0.12574285
Iteration 19, loss = 0.11362572
Iteration 20, loss = 0.10338584
Iteration 21, loss = 0.09431749
Iteration 22, loss = 0.08655855
Iteration 23, loss = 0.07978383
Iteration 24, loss = 0.07405081
Iteration 25, loss = 0.06895874
Iteration 26, loss = 0.06445288
Iteration 27, loss = 0.06041582
Iteration 28, loss = 0.05685900
Iteration 29, loss = 0.05372699
Iteration 30, loss = 0.05072783
Iteration 31, loss = 0.04838121
Iteration 32, loss = 0.04604267
Iteration 33, loss = 0.04383509
Iteration 34, loss = 0.04205110
Iteration 35, loss = 0.04063432
Iteration 36, loss = 0.03916080
Iteration 37, loss = 0.03795268
Iteration 38, loss = 0.03665405
Iteration 39, loss = 0.03527299
Iteration 40, loss = 0.03393532
Iteration 41, loss = 0.03295337
Iteration 42, loss = 0.03206071
Iteration 43, loss = 0.03104993
Iteration 44, loss = 0.03022656
Iteration 45, loss = 0.02936051
Iteration 46, loss = 0.02866617
Iteration 47, loss = 0.02799744
Iteration 48, loss = 0.02725722
Iteration 49, loss = 0.02663561
Iteration 50, loss = 0.02599990
Iteration 51, loss = 0.02559478
Iteration 52, loss = 0.02530524
Iteration 53, loss = 0.02497483
Iteration 54, loss = 0.02450517
Iteration 55, loss = 0.02428881
Iteration 56, loss = 0.02357740
Iteration 57, loss = 0.02339045
Iteration 58, loss = 0.02298279
Iteration 59, loss = 0.02256792
Iteration 60, loss = 0.02231377
Iteration 61, loss = 0.02179579
Iteration 62, loss = 0.02162253
Iteration 63, loss = 0.02136202
Iteration 64, loss = 0.02105257
Iteration 65, loss = 0.02083978
Iteration 66, loss = 0.02066337
Iteration 67, loss = 0.02050799
Iteration 68, loss = 0.02025420
Iteration 69, loss = 0.02000211
Iteration 70, loss = 0.01970974
Iteration 71, loss = 0.01946788
Iteration 72, loss = 0.01931308
Iteration 73, loss = 0.01913622
Iteration 74, loss = 0.01895232
Iteration 75, loss = 0.01885370
Iteration 76, loss = 0.01871338
Iteration 77, loss = 0.01858116
Iteration 78, loss = 0.01844743
Iteration 79, loss = 0.01835749
Iteration 80, loss = 0.01802547
Iteration 81, loss = 0.01811745
Iteration 82, loss = 0.01798642
Iteration 83, loss = 0.01782000
Iteration 84, loss = 0.01773497
Iteration 85, loss = 0.01762645
Iteration 86, loss = 0.01746891
Iteration 87, loss = 0.01743982
Iteration 88, loss = 0.01733683
Iteration 89, loss = 0.01724674
Iteration 90, loss = 0.01725424
Iteration 91, loss = 0.01702637
Iteration 92, loss = 0.01716494
Iteration 93, loss = 0.01709448
Iteration 94, loss = 0.01691345
Iteration 95, loss = 0.01680137
Iteration 96, loss = 0.01668698
Iteration 97, loss = 0.01666743
Iteration 98, loss = 0.01665115
Iteration 99, loss = 0.01671930
Iteration 100, loss = 0.01667040
Iteration 101, loss = 0.01650629
Iteration 102, loss = 0.01641029
Iteration 103, loss = 0.01628312
Iteration 104, loss = 0.01618847
Iteration 105, loss = 0.01604215
Iteration 106, loss = 0.01592869
Iteration 107, loss = 0.01607631
Iteration 108, loss = 0.01603147
Iteration 109, loss = 0.01597140
Iteration 110, loss = 0.01595000
Iteration 111, loss = 0.01592975
Iteration 112, loss = 0.01585361
Iteration 113, loss = 0.01572180
Iteration 114, loss = 0.01566776
Iteration 115, loss = 0.01561539
Iteration 116, loss = 0.01560712
Iteration 117, loss = 0.01581032
Iteration 118, loss = 0.01560354
Iteration 119, loss = 0.01561562
Iteration 120, loss = 0.01551279
Iteration 121, loss = 0.01544523
Iteration 122, loss = 0.01531390
Iteration 123, loss = 0.01529348
Iteration 124, loss = 0.01521034
Iteration 125, loss = 0.01520662
Iteration 126, loss = 0.01521639
Iteration 127, loss = 0.01522304
Iteration 128, loss = 0.01527994
Iteration 129, loss = 0.01509032
Iteration 130, loss = 0.01518553
Iteration 131, loss = 0.01514444
Iteration 132, loss = 0.01509879
Iteration 133, loss = 0.01512535
Iteration 134, loss = 0.01501341
Iteration 135, loss = 0.01505981
Iteration 136, loss = 0.01502250
Iteration 137, loss = 0.01504176
Iteration 138, loss = 0.01501080
Iteration 139, loss = 0.01518079
Iteration 140, loss = 0.01517162
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71104451
Iteration 2, loss = 0.66836542
Iteration 3, loss = 0.60484581
Iteration 4, loss = 0.51905444
Iteration 5, loss = 0.42852616
Iteration 6, loss = 0.34479631
Iteration 7, loss = 0.27479185
Iteration 8, loss = 0.21911497
Iteration 9, loss = 0.17707564
Iteration 10, loss = 0.14553477
Iteration 11, loss = 0.12155849
Iteration 12, loss = 0.10375178
Iteration 13, loss = 0.08991885
Iteration 14, loss = 0.07910284
Iteration 15, loss = 0.07054695
Iteration 16, loss = 0.06390771
Iteration 17, loss = 0.05815358
Iteration 18, loss = 0.05333523
Iteration 19, loss = 0.04959423
Iteration 20, loss = 0.04628320
Iteration 21, loss = 0.04352009
Iteration 22, loss = 0.04109098
Iteration 23, loss = 0.03895717
Iteration 24, loss = 0.03738613
Iteration 25, loss = 0.03556870
Iteration 26, loss = 0.03446827
Iteration 27, loss = 0.03300899
Iteration 28, loss = 0.03195728
Iteration 29, loss = 0.03089385
Iteration 30, loss = 0.03019655
Iteration 31, loss = 0.02937233
Iteration 32, loss = 0.02867280
Iteration 33, loss = 0.02801788
Iteration 34, loss = 0.02745317
Iteration 35, loss = 0.02681122
Iteration 36, loss = 0.02638203
Iteration 37, loss = 0.02592707
Iteration 38, loss = 0.02551400
Iteration 39, loss = 0.02523366
Iteration 40, loss = 0.02483695
Iteration 41, loss = 0.02444373
Iteration 42, loss = 0.02409226
Iteration 43, loss = 0.02405817
Iteration 44, loss = 0.02354724
Iteration 45, loss = 0.02350026
Iteration 46, loss = 0.02315281
Iteration 47, loss = 0.02311187
Iteration 48, loss = 0.02278829
Iteration 49, loss = 0.02260366
Iteration 50, loss = 0.02244860
Iteration 51, loss = 0.02232836
Iteration 52, loss = 0.02211092
Iteration 53, loss = 0.02211476
Iteration 54, loss = 0.02179331
Iteration 55, loss = 0.02182919
Iteration 56, loss = 0.02160774
Iteration 57, loss = 0.02135931
Iteration 58, loss = 0.02138394
Iteration 59, loss = 0.02131682
Iteration 60, loss = 0.02127537
Iteration 61, loss = 0.02137835
Iteration 62, loss = 0.02103746
Iteration 63, loss = 0.02095646
Iteration 64, loss = 0.02092962
Iteration 65, loss = 0.02082597
Iteration 66, loss = 0.02086481
Iteration 67, loss = 0.02049682
Iteration 68, loss = 0.02066367
Iteration 69, loss = 0.02049209
Iteration 70, loss = 0.02052470
Iteration 71, loss = 0.02033840
Iteration 72, loss = 0.02040874
Iteration 73, loss = 0.02031439
Iteration 74, loss = 0.02006459
Iteration 75, loss = 0.02024969
Iteration 76, loss = 0.01998919
Iteration 77, loss = 0.02009733
Iteration 78, loss = 0.02007572
Iteration 79, loss = 0.02002060
Iteration 80, loss = 0.01990250
Iteration 81, loss = 0.01981888
Iteration 82, loss = 0.01980235
Iteration 83, loss = 0.01980383
Iteration 84, loss = 0.01986375
Iteration 85, loss = 0.01961182
Iteration 86, loss = 0.01972899
Iteration 87, loss = 0.01987354
Iteration 88, loss = 0.01992241
Iteration 89, loss = 0.01961798
Iteration 90, loss = 0.01969751
Iteration 91, loss = 0.01957939
Iteration 92, loss = 0.01939766
Iteration 93, loss = 0.01941890
Iteration 94, loss = 0.01944615
Iteration 95, loss = 0.01937032
Iteration 96, loss = 0.01949758
Iteration 97, loss = 0.01931738
Iteration 98, loss = 0.01933835
Iteration 99, loss = 0.01926015
Iteration 100, loss = 0.01915621
Iteration 101, loss = 0.01929794
Iteration 102, loss = 0.01920785
Iteration 103, loss = 0.01934922
Iteration 104, loss = 0.01934418
Iteration 105, loss = 0.01929436
Iteration 106, loss = 0.01923716
Iteration 107, loss = 0.01932462
Iteration 108, loss = 0.01910922
Iteration 109, loss = 0.01918624
Iteration 110, loss = 0.01936416
Iteration 111, loss = 0.01910698
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67845955
Iteration 2, loss = 0.62800665
Iteration 3, loss = 0.54703744
Iteration 4, loss = 0.45223787
Iteration 5, loss = 0.35981398
Iteration 6, loss = 0.28072916
Iteration 7, loss = 0.21915269
Iteration 8, loss = 0.17374070
Iteration 9, loss = 0.14065019
Iteration 10, loss = 0.11696261
Iteration 11, loss = 0.09930769
Iteration 12, loss = 0.08641951
Iteration 13, loss = 0.07656842
Iteration 14, loss = 0.06878300
Iteration 15, loss = 0.06241308
Iteration 16, loss = 0.05728459
Iteration 17, loss = 0.05318303
Iteration 18, loss = 0.04962052
Iteration 19, loss = 0.04669835
Iteration 20, loss = 0.04432449
Iteration 21, loss = 0.04230631
Iteration 22, loss = 0.04046343
Iteration 23, loss = 0.03885649
Iteration 24, loss = 0.03756423
Iteration 25, loss = 0.03627723
Iteration 26, loss = 0.03524885
Iteration 27, loss = 0.03414639
Iteration 28, loss = 0.03347245
Iteration 29, loss = 0.03249546
Iteration 30, loss = 0.03256737
Iteration 31, loss = 0.03203571
Iteration 32, loss = 0.03105576
Iteration 33, loss = 0.03054342
Iteration 34, loss = 0.02999786
Iteration 35, loss = 0.02964109
Iteration 36, loss = 0.02906122
Iteration 37, loss = 0.02919056
Iteration 38, loss = 0.02853509
Iteration 39, loss = 0.02827829
Iteration 40, loss = 0.02817522
Iteration 41, loss = 0.02769198
Iteration 42, loss = 0.02746062
Iteration 43, loss = 0.02724746
Iteration 44, loss = 0.02694803
Iteration 45, loss = 0.02683926
Iteration 46, loss = 0.02653485
Iteration 47, loss = 0.02616770
Iteration 48, loss = 0.02613448
Iteration 49, loss = 0.02640894
Iteration 50, loss = 0.02647103
Iteration 51, loss = 0.02596038
Iteration 52, loss = 0.02548342
Iteration 53, loss = 0.02580318
Iteration 54, loss = 0.02637449
Iteration 55, loss = 0.02639682
Iteration 56, loss = 0.02615935
Iteration 57, loss = 0.02588448
Iteration 58, loss = 0.02550139
Iteration 59, loss = 0.02563664
Iteration 60, loss = 0.02533045
Iteration 61, loss = 0.02512105
Iteration 62, loss = 0.02500088
Iteration 63, loss = 0.02533174
Iteration 64, loss = 0.02501830
Iteration 65, loss = 0.02480432
Iteration 66, loss = 0.02457338
Iteration 67, loss = 0.02456343
Iteration 68, loss = 0.02460656
Iteration 69, loss = 0.02443413
Iteration 70, loss = 0.02423316
Iteration 71, loss = 0.02413688
Iteration 72, loss = 0.02510316
Iteration 73, loss = 0.02491663
Iteration 74, loss = 0.02471243
Iteration 75, loss = 0.02465352
Iteration 76, loss = 0.02439497
Iteration 77, loss = 0.02427249
Iteration 78, loss = 0.02435627
Iteration 79, loss = 0.02455712
Iteration 80, loss = 0.02386565
Iteration 81, loss = 0.02389142
Iteration 82, loss = 0.02379094
Iteration 83, loss = 0.02362894
Iteration 84, loss = 0.02366944
Iteration 85, loss = 0.02348031
Iteration 86, loss = 0.02344041
Iteration 87, loss = 0.02367038
Iteration 88, loss = 0.02349426
Iteration 89, loss = 0.02334955
Iteration 90, loss = 0.02334962
Iteration 91, loss = 0.02331546
Iteration 92, loss = 0.02323718
Iteration 93, loss = 0.02335829
Iteration 94, loss = 0.02312641
Iteration 95, loss = 0.02307723
Iteration 96, loss = 0.02279855
Iteration 97, loss = 0.02300958
Iteration 98, loss = 0.02302226
Iteration 99, loss = 0.02295760
Iteration 100, loss = 0.02291409
Iteration 101, loss = 0.02287635
Iteration 102, loss = 0.02287246
Iteration 103, loss = 0.02251821
Iteration 104, loss = 0.02306846
Iteration 105, loss = 0.02300485
Iteration 106, loss = 0.02268612
Iteration 107, loss = 0.02256641
Iteration 108, loss = 0.02254408
Iteration 109, loss = 0.02247752
Iteration 110, loss = 0.02256360
Iteration 111, loss = 0.02262238
Iteration 112, loss = 0.02252961
Iteration 113, loss = 0.02264564
Iteration 114, loss = 0.02297274
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71400450
Iteration 2, loss = 0.67328997
Iteration 3, loss = 0.61913436
Iteration 4, loss = 0.55370157
Iteration 5, loss = 0.48174051
Iteration 6, loss = 0.41093418
Iteration 7, loss = 0.34573968
Iteration 8, loss = 0.28862335
Iteration 9, loss = 0.24061761
Iteration 10, loss = 0.20142737
Iteration 11, loss = 0.16993886
Iteration 12, loss = 0.14479790
Iteration 13, loss = 0.12482294
Iteration 14, loss = 0.10875570
Iteration 15, loss = 0.09583170
Iteration 16, loss = 0.08524242
Iteration 17, loss = 0.07667488
Iteration 18, loss = 0.06943496
Iteration 19, loss = 0.06350841
Iteration 20, loss = 0.05835776
Iteration 21, loss = 0.05411218
Iteration 22, loss = 0.05039354
Iteration 23, loss = 0.04740590
Iteration 24, loss = 0.04443432
Iteration 25, loss = 0.04210408
Iteration 26, loss = 0.04006526
Iteration 27, loss = 0.03804367
Iteration 28, loss = 0.03644039
Iteration 29, loss = 0.03509173
Iteration 30, loss = 0.03355909
Iteration 31, loss = 0.03267942
Iteration 32, loss = 0.03135913
Iteration 33, loss = 0.03037607
Iteration 34, loss = 0.02958071
Iteration 35, loss = 0.02874095
Iteration 36, loss = 0.02808122
Iteration 37, loss = 0.02734226
Iteration 38, loss = 0.02664639
Iteration 39, loss = 0.02627066
Iteration 40, loss = 0.02590498
Iteration 41, loss = 0.02520415
Iteration 42, loss = 0.02466879
Iteration 43, loss = 0.02432476
Iteration 44, loss = 0.02400734
Iteration 45, loss = 0.02350021
Iteration 46, loss = 0.02333646
Iteration 47, loss = 0.02308284
Iteration 48, loss = 0.02285209
Iteration 49, loss = 0.02248951
Iteration 50, loss = 0.02235235
Iteration 51, loss = 0.02200630
Iteration 52, loss = 0.02158646
Iteration 53, loss = 0.02137745
Iteration 54, loss = 0.02140362
Iteration 55, loss = 0.02121715
Iteration 56, loss = 0.02105255
Iteration 57, loss = 0.02076013
Iteration 58, loss = 0.02078587
Iteration 59, loss = 0.02044116
Iteration 60, loss = 0.02049120
Iteration 61, loss = 0.02012110
Iteration 62, loss = 0.02016272
Iteration 63, loss = 0.01988111
Iteration 64, loss = 0.01996376
Iteration 65, loss = 0.01969422
Iteration 66, loss = 0.01981631
Iteration 67, loss = 0.01951643
Iteration 68, loss = 0.01934376
Iteration 69, loss = 0.01930609
Iteration 70, loss = 0.01926289
Iteration 71, loss = 0.01915711
Iteration 72, loss = 0.01909475
Iteration 73, loss = 0.01891004
Iteration 74, loss = 0.01897174
Iteration 75, loss = 0.01896758
Iteration 76, loss = 0.01883984
Iteration 77, loss = 0.01871633
Iteration 78, loss = 0.01897092
Iteration 79, loss = 0.01850735
Iteration 80, loss = 0.01864960
Iteration 81, loss = 0.01841252
Iteration 82, loss = 0.01838863
Iteration 83, loss = 0.01838285
Iteration 84, loss = 0.01828223
Iteration 85, loss = 0.01833148
Iteration 86, loss = 0.01830552
Iteration 87, loss = 0.01827895
Iteration 88, loss = 0.01814586
Iteration 89, loss = 0.01819058
Iteration 90, loss = 0.01818864
Iteration 91, loss = 0.01795150
Iteration 92, loss = 0.01795936
Iteration 93, loss = 0.01796673
Iteration 94, loss = 0.01779662
Iteration 95, loss = 0.01784090
Iteration 96, loss = 0.01789461
Iteration 97, loss = 0.01783125
Iteration 98, loss = 0.01782749
Iteration 99, loss = 0.01793181
Iteration 100, loss = 0.01794475
Iteration 101, loss = 0.01778489
Iteration 102, loss = 0.01771272
Iteration 103, loss = 0.01773920
Iteration 104, loss = 0.01760523
Iteration 105, loss = 0.01765186
Iteration 106, loss = 0.01785962
Iteration 107, loss = 0.01746167
Iteration 108, loss = 0.01768743
Iteration 109, loss = 0.01755637
Iteration 110, loss = 0.01742980
Iteration 111, loss = 0.01738371
Iteration 112, loss = 0.01753818
Iteration 113, loss = 0.01744039
Iteration 114, loss = 0.01743022
Iteration 115, loss = 0.01756775
Iteration 116, loss = 0.01749434
Iteration 117, loss = 0.01735298
Iteration 118, loss = 0.01729638
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68770526
Iteration 2, loss = 0.63538426
Iteration 3, loss = 0.54975232
Iteration 4, loss = 0.44984316
Iteration 5, loss = 0.35636817
Iteration 6, loss = 0.27798788
Iteration 7, loss = 0.21694974
Iteration 8, loss = 0.17213747
Iteration 9, loss = 0.13929137
Iteration 10, loss = 0.11565439
Iteration 11, loss = 0.09815057
Iteration 12, loss = 0.08503419
Iteration 13, loss = 0.07505271
Iteration 14, loss = 0.06689750
Iteration 15, loss = 0.06062335
Iteration 16, loss = 0.05667683
Iteration 17, loss = 0.05256974
Iteration 18, loss = 0.04932072
Iteration 19, loss = 0.04610221
Iteration 20, loss = 0.04349283
Iteration 21, loss = 0.04111161
Iteration 22, loss = 0.03933628
Iteration 23, loss = 0.03744260
Iteration 24, loss = 0.03604331
Iteration 25, loss = 0.03489345
Iteration 26, loss = 0.03370218
Iteration 27, loss = 0.03281270
Iteration 28, loss = 0.03167108
Iteration 29, loss = 0.03072878
Iteration 30, loss = 0.02992422
Iteration 31, loss = 0.02944394
Iteration 32, loss = 0.02876415
Iteration 33, loss = 0.02823564
Iteration 34, loss = 0.02769707
Iteration 35, loss = 0.02724032
Iteration 36, loss = 0.02684060
Iteration 37, loss = 0.02652123
Iteration 38, loss = 0.02602128
Iteration 39, loss = 0.02578459
Iteration 40, loss = 0.02537858
Iteration 41, loss = 0.02522000
Iteration 42, loss = 0.02496948
Iteration 43, loss = 0.02463328
Iteration 44, loss = 0.02452831
Iteration 45, loss = 0.02496373
Iteration 46, loss = 0.02462897
Iteration 47, loss = 0.02396928
Iteration 48, loss = 0.02388166
Iteration 49, loss = 0.02368003
Iteration 50, loss = 0.02360553
Iteration 51, loss = 0.02346574
Iteration 52, loss = 0.02319283
Iteration 53, loss = 0.02300257
Iteration 54, loss = 0.02285514
Iteration 55, loss = 0.02287611
Iteration 56, loss = 0.02267832
Iteration 57, loss = 0.02238038
Iteration 58, loss = 0.02253207
Iteration 59, loss = 0.02233752
Iteration 60, loss = 0.02231268
Iteration 61, loss = 0.02223098
Iteration 62, loss = 0.02195219
Iteration 63, loss = 0.02207916
Iteration 64, loss = 0.02192468
Iteration 65, loss = 0.02180824
Iteration 66, loss = 0.02182257
Iteration 67, loss = 0.02161450
Iteration 68, loss = 0.02143777
Iteration 69, loss = 0.02183544
Iteration 70, loss = 0.02143849
Iteration 71, loss = 0.02158250
Iteration 72, loss = 0.02271074
Iteration 73, loss = 0.02272040
Iteration 74, loss = 0.02229739
Iteration 75, loss = 0.02265544
Iteration 76, loss = 0.02179983
Iteration 77, loss = 0.02179597
Iteration 78, loss = 0.02177710
Iteration 79, loss = 0.02154914
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69886490
Iteration 2, loss = 0.64867296
Iteration 3, loss = 0.55966763
Iteration 4, loss = 0.44670397
Iteration 5, loss = 0.34255914
Iteration 6, loss = 0.25853888
Iteration 7, loss = 0.19771838
Iteration 8, loss = 0.15455490
Iteration 9, loss = 0.12489462
Iteration 10, loss = 0.10369516
Iteration 11, loss = 0.08862401
Iteration 12, loss = 0.07714344
Iteration 13, loss = 0.06850236
Iteration 14, loss = 0.06158627
Iteration 15, loss = 0.05628945
Iteration 16, loss = 0.05186828
Iteration 17, loss = 0.04813945
Iteration 18, loss = 0.04517740
Iteration 19, loss = 0.04291480
Iteration 20, loss = 0.04084683
Iteration 21, loss = 0.03892158
Iteration 22, loss = 0.03722243
Iteration 23, loss = 0.03591280
Iteration 24, loss = 0.03514154
Iteration 25, loss = 0.03385620
Iteration 26, loss = 0.03307538
Iteration 27, loss = 0.03222539
Iteration 28, loss = 0.03159020
Iteration 29, loss = 0.03104791
Iteration 30, loss = 0.03045876
Iteration 31, loss = 0.02989096
Iteration 32, loss = 0.02942867
Iteration 33, loss = 0.02889419
Iteration 34, loss = 0.02878259
Iteration 35, loss = 0.02816406
Iteration 36, loss = 0.02812791
Iteration 37, loss = 0.02741579
Iteration 38, loss = 0.02771395
Iteration 39, loss = 0.02702233
Iteration 40, loss = 0.02702257
Iteration 41, loss = 0.02662219
Iteration 42, loss = 0.02666335
Iteration 43, loss = 0.02617520
Iteration 44, loss = 0.02635574
Iteration 45, loss = 0.02604033
Iteration 46, loss = 0.02588260
Iteration 47, loss = 0.02565356
Iteration 48, loss = 0.02544509
Iteration 49, loss = 0.02548906
Iteration 50, loss = 0.02574944
Iteration 51, loss = 0.02534756
Iteration 52, loss = 0.02523729
Iteration 53, loss = 0.02502710
Iteration 54, loss = 0.02522121
Iteration 55, loss = 0.02492387
Iteration 56, loss = 0.02461421
Iteration 57, loss = 0.02476609
Iteration 58, loss = 0.02482595
Iteration 59, loss = 0.02484524
Iteration 60, loss = 0.02477689
Iteration 61, loss = 0.02493198
Iteration 62, loss = 0.02437166
Iteration 63, loss = 0.02432721
Iteration 64, loss = 0.02435851
Iteration 65, loss = 0.02422762
Iteration 66, loss = 0.02416735
Iteration 67, loss = 0.02411188
Iteration 68, loss = 0.02420453
Iteration 69, loss = 0.02396072
Iteration 70, loss = 0.02406028
Iteration 71, loss = 0.02395347
Iteration 72, loss = 0.02392176
Iteration 73, loss = 0.02411405
Iteration 74, loss = 0.02380586
Iteration 75, loss = 0.02378007
Iteration 76, loss = 0.02384781
Iteration 77, loss = 0.02372891
Iteration 78, loss = 0.02390670
Iteration 79, loss = 0.02376423
Iteration 80, loss = 0.02364194
Iteration 81, loss = 0.02379076
Iteration 82, loss = 0.02419275
Iteration 83, loss = 0.02350997
Iteration 84, loss = 0.02361706
Iteration 85, loss = 0.02360336
Iteration 86, loss = 0.02374557
Iteration 87, loss = 0.02366402
Iteration 88, loss = 0.02359440
Iteration 89, loss = 0.02352852
Iteration 90, loss = 0.02343166
Iteration 91, loss = 0.02328060
Iteration 92, loss = 0.02354257
Iteration 93, loss = 0.02363072
Iteration 94, loss = 0.02339718
Iteration 95, loss = 0.02341042
Iteration 96, loss = 0.02340940
Iteration 97, loss = 0.02338495
Iteration 98, loss = 0.02366761
Iteration 99, loss = 0.02324268
Iteration 100, loss = 0.02378154
Iteration 101, loss = 0.02340909
Iteration 102, loss = 0.02333817
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69860753
Iteration 2, loss = 0.67565214
Iteration 3, loss = 0.64789202
Iteration 4, loss = 0.61038616
Iteration 5, loss = 0.56357270
Iteration 6, loss = 0.51233413
Iteration 7, loss = 0.45971683
Iteration 8, loss = 0.40865774
Iteration 9, loss = 0.36091903
Iteration 10, loss = 0.31702069
Iteration 11, loss = 0.27787074
Iteration 12, loss = 0.24354227
Iteration 13, loss = 0.21355083
Iteration 14, loss = 0.18778577
Iteration 15, loss = 0.16573719
Iteration 16, loss = 0.14707257
Iteration 17, loss = 0.13129137
Iteration 18, loss = 0.11784453
Iteration 19, loss = 0.10633576
Iteration 20, loss = 0.09663005
Iteration 21, loss = 0.08830512
Iteration 22, loss = 0.08100264
Iteration 23, loss = 0.07479831
Iteration 24, loss = 0.06931208
Iteration 25, loss = 0.06462208
Iteration 26, loss = 0.06043012
Iteration 27, loss = 0.05648881
Iteration 28, loss = 0.05318389
Iteration 29, loss = 0.05017273
Iteration 30, loss = 0.04759472
Iteration 31, loss = 0.04500875
Iteration 32, loss = 0.04309084
Iteration 33, loss = 0.04113581
Iteration 34, loss = 0.03937235
Iteration 35, loss = 0.03783549
Iteration 36, loss = 0.03638512
Iteration 37, loss = 0.03514048
Iteration 38, loss = 0.03395464
Iteration 39, loss = 0.03281563
Iteration 40, loss = 0.03167961
Iteration 41, loss = 0.03057011
Iteration 42, loss = 0.02970220
Iteration 43, loss = 0.02874094
Iteration 44, loss = 0.02811796
Iteration 45, loss = 0.02744380
Iteration 46, loss = 0.02702298
Iteration 47, loss = 0.02628551
Iteration 48, loss = 0.02561892
Iteration 49, loss = 0.02495072
Iteration 50, loss = 0.02438610
Iteration 51, loss = 0.02384209
Iteration 52, loss = 0.02342555
Iteration 53, loss = 0.02301454
Iteration 54, loss = 0.02269864
Iteration 55, loss = 0.02223280
Iteration 56, loss = 0.02194732
Iteration 57, loss = 0.02158886
Iteration 58, loss = 0.02117064
Iteration 59, loss = 0.02091637
Iteration 60, loss = 0.02065136
Iteration 61, loss = 0.02028569
Iteration 62, loss = 0.02004584
Iteration 63, loss = 0.01978883
Iteration 64, loss = 0.01963006
Iteration 65, loss = 0.01951767
Iteration 66, loss = 0.01939496
Iteration 67, loss = 0.01928583
Iteration 68, loss = 0.01905348
Iteration 69, loss = 0.01865846
Iteration 70, loss = 0.01865715
Iteration 71, loss = 0.01861415
Iteration 72, loss = 0.01844603
Iteration 73, loss = 0.01818456
Iteration 74, loss = 0.01801512
Iteration 75, loss = 0.01780759
Iteration 76, loss = 0.01763752
Iteration 77, loss = 0.01742832
Iteration 78, loss = 0.01747076
Iteration 79, loss = 0.01732972
Iteration 80, loss = 0.01720224
Iteration 81, loss = 0.01716086
Iteration 82, loss = 0.01690004
Iteration 83, loss = 0.01678577
Iteration 84, loss = 0.01676373
Iteration 85, loss = 0.01660306
Iteration 86, loss = 0.01653867
Iteration 87, loss = 0.01661188
Iteration 88, loss = 0.01662709
Iteration 89, loss = 0.01648112
Iteration 90, loss = 0.01634690
Iteration 91, loss = 0.01612746
Iteration 92, loss = 0.01597075
Iteration 93, loss = 0.01616203
Iteration 94, loss = 0.01606996
Iteration 95, loss = 0.01598707
Iteration 96, loss = 0.01590825
Iteration 97, loss = 0.01595453
Iteration 98, loss = 0.01578779
Iteration 99, loss = 0.01565527
Iteration 100, loss = 0.01553658
Iteration 101, loss = 0.01541253
Iteration 102, loss = 0.01526485
Iteration 103, loss = 0.01529905
Iteration 104, loss = 0.01536598
Iteration 105, loss = 0.01519676
Iteration 106, loss = 0.01528899
Iteration 107, loss = 0.01525219
Iteration 108, loss = 0.01518125
Iteration 109, loss = 0.01502575
Iteration 110, loss = 0.01479928
Iteration 111, loss = 0.01502826
Iteration 112, loss = 0.01498184
Iteration 113, loss = 0.01491612
Iteration 114, loss = 0.01489405
Iteration 115, loss = 0.01482998
Iteration 116, loss = 0.01478616
Iteration 117, loss = 0.01504814
Iteration 118, loss = 0.01501539
Iteration 119, loss = 0.01493593
Iteration 120, loss = 0.01479350
Iteration 121, loss = 0.01475780
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67726267
Iteration 2, loss = 0.61954403
Iteration 3, loss = 0.51803254
Iteration 4, loss = 0.40908095
Iteration 5, loss = 0.31284937
Iteration 6, loss = 0.23750483
Iteration 7, loss = 0.18270739
Iteration 8, loss = 0.14399845
Iteration 9, loss = 0.11692761
Iteration 10, loss = 0.09747766
Iteration 11, loss = 0.08303382
Iteration 12, loss = 0.07226409
Iteration 13, loss = 0.06387093
Iteration 14, loss = 0.05745617
Iteration 15, loss = 0.05241474
Iteration 16, loss = 0.04812902
Iteration 17, loss = 0.04478284
Iteration 18, loss = 0.04183572
Iteration 19, loss = 0.03931733
Iteration 20, loss = 0.03755211
Iteration 21, loss = 0.03548234
Iteration 22, loss = 0.03445102
Iteration 23, loss = 0.03283566
Iteration 24, loss = 0.03179112
Iteration 25, loss = 0.03057641
Iteration 26, loss = 0.02991365
Iteration 27, loss = 0.02913288
Iteration 28, loss = 0.02816811
Iteration 29, loss = 0.02773600
Iteration 30, loss = 0.02724226
Iteration 31, loss = 0.02663217
Iteration 32, loss = 0.02640459
Iteration 33, loss = 0.02577616
Iteration 34, loss = 0.02554872
Iteration 35, loss = 0.02498667
Iteration 36, loss = 0.02457522
Iteration 37, loss = 0.02429040
Iteration 38, loss = 0.02416692
Iteration 39, loss = 0.02398412
Iteration 40, loss = 0.02375724
Iteration 41, loss = 0.02348389
Iteration 42, loss = 0.02327673
Iteration 43, loss = 0.02300009
Iteration 44, loss = 0.02288729
Iteration 45, loss = 0.02282285
Iteration 46, loss = 0.02251196
Iteration 47, loss = 0.02236408
Iteration 48, loss = 0.02239811
Iteration 49, loss = 0.02253749
Iteration 50, loss = 0.02218083
Iteration 51, loss = 0.02201372
Iteration 52, loss = 0.02166977
Iteration 53, loss = 0.02180726
Iteration 54, loss = 0.02182226
Iteration 55, loss = 0.02158716
Iteration 56, loss = 0.02174115
Iteration 57, loss = 0.02157183
Iteration 58, loss = 0.02155992
Iteration 59, loss = 0.02117304
Iteration 60, loss = 0.02146056
Iteration 61, loss = 0.02098660
Iteration 62, loss = 0.02102837
Iteration 63, loss = 0.02132134
Iteration 64, loss = 0.02112326
Iteration 65, loss = 0.02117099
Iteration 66, loss = 0.02075090
Iteration 67, loss = 0.02116408
Iteration 68, loss = 0.02092340
Iteration 69, loss = 0.02081701
Iteration 70, loss = 0.02060894
Iteration 71, loss = 0.02100205
Iteration 72, loss = 0.02064669
Iteration 73, loss = 0.02065226
Iteration 74, loss = 0.02073749
Iteration 75, loss = 0.02086495
Iteration 76, loss = 0.02066712
Iteration 77, loss = 0.02030669
Iteration 78, loss = 0.02050385
Iteration 79, loss = 0.02032294
Iteration 80, loss = 0.02064628
Iteration 81, loss = 0.02040691
Iteration 82, loss = 0.02102084
Iteration 83, loss = 0.02084408
Iteration 84, loss = 0.02035955
Iteration 85, loss = 0.02052876
Iteration 86, loss = 0.02046032
Iteration 87, loss = 0.02056573
Iteration 88, loss = 0.02047771
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70796011
Iteration 2, loss = 0.67661612
Iteration 3, loss = 0.63747689
Iteration 4, loss = 0.58866320
Iteration 5, loss = 0.53179185
Iteration 6, loss = 0.47230356
Iteration 7, loss = 0.41326936
Iteration 8, loss = 0.35702858
Iteration 9, loss = 0.30585658
Iteration 10, loss = 0.26026356
Iteration 11, loss = 0.22139700
Iteration 12, loss = 0.18900479
Iteration 13, loss = 0.16226468
Iteration 14, loss = 0.14018373
Iteration 15, loss = 0.12220966
Iteration 16, loss = 0.10735869
Iteration 17, loss = 0.09507037
Iteration 18, loss = 0.08492613
Iteration 19, loss = 0.07648248
Iteration 20, loss = 0.06922304
Iteration 21, loss = 0.06304807
Iteration 22, loss = 0.05784329
Iteration 23, loss = 0.05331973
Iteration 24, loss = 0.04941759
Iteration 25, loss = 0.04594165
Iteration 26, loss = 0.04295745
Iteration 27, loss = 0.04037305
Iteration 28, loss = 0.03810143
Iteration 29, loss = 0.03596848
Iteration 30, loss = 0.03419132
Iteration 31, loss = 0.03251229
Iteration 32, loss = 0.03096166
Iteration 33, loss = 0.02966175
Iteration 34, loss = 0.02835929
Iteration 35, loss = 0.02725167
Iteration 36, loss = 0.02634109
Iteration 37, loss = 0.02538987
Iteration 38, loss = 0.02461479
Iteration 39, loss = 0.02377057
Iteration 40, loss = 0.02307894
Iteration 41, loss = 0.02245442
Iteration 42, loss = 0.02192460
Iteration 43, loss = 0.02122457
Iteration 44, loss = 0.02070336
Iteration 45, loss = 0.02021732
Iteration 46, loss = 0.01979682
Iteration 47, loss = 0.01929276
Iteration 48, loss = 0.01903447
Iteration 49, loss = 0.01868827
Iteration 50, loss = 0.01821566
Iteration 51, loss = 0.01796485
Iteration 52, loss = 0.01759568
Iteration 53, loss = 0.01731864
Iteration 54, loss = 0.01702330
Iteration 55, loss = 0.01682316
Iteration 56, loss = 0.01651620
Iteration 57, loss = 0.01642641
Iteration 58, loss = 0.01603309
Iteration 59, loss = 0.01592522
Iteration 60, loss = 0.01580388
Iteration 61, loss = 0.01547458
Iteration 62, loss = 0.01536333
Iteration 63, loss = 0.01527793
Iteration 64, loss = 0.01514845
Iteration 65, loss = 0.01487279
Iteration 66, loss = 0.01480085
Iteration 67, loss = 0.01459815
Iteration 68, loss = 0.01446846
Iteration 69, loss = 0.01438989
Iteration 70, loss = 0.01422194
Iteration 71, loss = 0.01424417
Iteration 72, loss = 0.01411750
Iteration 73, loss = 0.01389640
Iteration 74, loss = 0.01387823
Iteration 75, loss = 0.01372702
Iteration 76, loss = 0.01369450
Iteration 77, loss = 0.01357222
Iteration 78, loss = 0.01352094
Iteration 79, loss = 0.01337935
Iteration 80, loss = 0.01345758
Iteration 81, loss = 0.01329954
Iteration 82, loss = 0.01319418
Iteration 83, loss = 0.01320476
Iteration 84, loss = 0.01302913
Iteration 85, loss = 0.01300539
Iteration 86, loss = 0.01295176
Iteration 87, loss = 0.01289255
Iteration 88, loss = 0.01284341
Iteration 89, loss = 0.01272497
Iteration 90, loss = 0.01277643
Iteration 91, loss = 0.01259104
Iteration 92, loss = 0.01251333
Iteration 93, loss = 0.01252944
Iteration 94, loss = 0.01246204
Iteration 95, loss = 0.01249863
Iteration 96, loss = 0.01246526
Iteration 97, loss = 0.01242021
Iteration 98, loss = 0.01227138
Iteration 99, loss = 0.01231933
Iteration 100, loss = 0.01227469
Iteration 101, loss = 0.01221537
Iteration 102, loss = 0.01216215
Iteration 103, loss = 0.01211352
Iteration 104, loss = 0.01219357
Iteration 105, loss = 0.01207463
Iteration 106, loss = 0.01205964
Iteration 107, loss = 0.01202215
Iteration 108, loss = 0.01196947
Iteration 109, loss = 0.01199583
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70590376
Iteration 2, loss = 0.68452372
Iteration 3, loss = 0.66562337
Iteration 4, loss = 0.64498691
Iteration 5, loss = 0.62152634
Iteration 6, loss = 0.59503338
Iteration 7, loss = 0.56573144
Iteration 8, loss = 0.53395786
Iteration 9, loss = 0.50104820
Iteration 10, loss = 0.46737725
Iteration 11, loss = 0.43375232
Iteration 12, loss = 0.40076388
Iteration 13, loss = 0.36887165
Iteration 14, loss = 0.33860726
Iteration 15, loss = 0.30999579
Iteration 16, loss = 0.28346596
Iteration 17, loss = 0.25884156
Iteration 18, loss = 0.23639982
Iteration 19, loss = 0.21604045
Iteration 20, loss = 0.19749709
Iteration 21, loss = 0.18088506
Iteration 22, loss = 0.16592220
Iteration 23, loss = 0.15253263
Iteration 24, loss = 0.14057142
Iteration 25, loss = 0.12982205
Iteration 26, loss = 0.12021994
Iteration 27, loss = 0.11159579
Iteration 28, loss = 0.10389924
Iteration 29, loss = 0.09695711
Iteration 30, loss = 0.09075127
Iteration 31, loss = 0.08512833
Iteration 32, loss = 0.08002434
Iteration 33, loss = 0.07543642
Iteration 34, loss = 0.07127084
Iteration 35, loss = 0.06747349
Iteration 36, loss = 0.06400062
Iteration 37, loss = 0.06088801
Iteration 38, loss = 0.05792807
Iteration 39, loss = 0.05529278
Iteration 40, loss = 0.05281380
Iteration 41, loss = 0.05056238
Iteration 42, loss = 0.04847932
Iteration 43, loss = 0.04661706
Iteration 44, loss = 0.04473704
Iteration 45, loss = 0.04309294
Iteration 46, loss = 0.04157430
Iteration 47, loss = 0.04013171
Iteration 48, loss = 0.03880843
Iteration 49, loss = 0.03755654
Iteration 50, loss = 0.03634814
Iteration 51, loss = 0.03525567
Iteration 52, loss = 0.03422536
Iteration 53, loss = 0.03328347
Iteration 54, loss = 0.03236193
Iteration 55, loss = 0.03150329
Iteration 56, loss = 0.03068157
Iteration 57, loss = 0.02993848
Iteration 58, loss = 0.02923525
Iteration 59, loss = 0.02854524
Iteration 60, loss = 0.02789178
Iteration 61, loss = 0.02730709
Iteration 62, loss = 0.02671636
Iteration 63, loss = 0.02615387
Iteration 64, loss = 0.02564534
Iteration 65, loss = 0.02517440
Iteration 66, loss = 0.02467332
Iteration 67, loss = 0.02425808
Iteration 68, loss = 0.02382910
Iteration 69, loss = 0.02337671
Iteration 70, loss = 0.02299484
Iteration 71, loss = 0.02262943
Iteration 72, loss = 0.02226062
Iteration 73, loss = 0.02194074
Iteration 74, loss = 0.02161708
Iteration 75, loss = 0.02127575
Iteration 76, loss = 0.02098000
Iteration 77, loss = 0.02074114
Iteration 78, loss = 0.02041319
Iteration 79, loss = 0.02018820
Iteration 80, loss = 0.01996869
Iteration 81, loss = 0.01967729
Iteration 82, loss = 0.01946242
Iteration 83, loss = 0.01921892
Iteration 84, loss = 0.01902606
Iteration 85, loss = 0.01884575
Iteration 86, loss = 0.01858604
Iteration 87, loss = 0.01842772
Iteration 88, loss = 0.01827275
Iteration 89, loss = 0.01803489
Iteration 90, loss = 0.01784600
Iteration 91, loss = 0.01775277
Iteration 92, loss = 0.01756252
Iteration 93, loss = 0.01739566
Iteration 94, loss = 0.01720980
Iteration 95, loss = 0.01706045
Iteration 96, loss = 0.01693120
Iteration 97, loss = 0.01681167
Iteration 98, loss = 0.01670463
Iteration 99, loss = 0.01651865
Iteration 100, loss = 0.01645196
Iteration 101, loss = 0.01630617
Iteration 102, loss = 0.01616712
Iteration 103, loss = 0.01605713
Iteration 104, loss = 0.01601802
Iteration 105, loss = 0.01583188
Iteration 106, loss = 0.01580263
Iteration 107, loss = 0.01561955
Iteration 108, loss = 0.01553319
Iteration 109, loss = 0.01542557
Iteration 110, loss = 0.01537283
Iteration 111, loss = 0.01525833
Iteration 112, loss = 0.01516887
Iteration 113, loss = 0.01514319
Iteration 114, loss = 0.01502727
Iteration 115, loss = 0.01492470
Iteration 116, loss = 0.01485789
Iteration 117, loss = 0.01483950
Iteration 118, loss = 0.01468396
Iteration 119, loss = 0.01464046
Iteration 120, loss = 0.01455884
Iteration 121, loss = 0.01452422
Iteration 122, loss = 0.01447042
Iteration 123, loss = 0.01442174
Iteration 124, loss = 0.01432806
Iteration 125, loss = 0.01423394
Iteration 126, loss = 0.01422907
Iteration 127, loss = 0.01411023
Iteration 128, loss = 0.01405156
Iteration 129, loss = 0.01399017
Iteration 130, loss = 0.01396894
Iteration 131, loss = 0.01394130
Iteration 132, loss = 0.01381959
Iteration 133, loss = 0.01378579
Iteration 134, loss = 0.01377436
Iteration 135, loss = 0.01369485
Iteration 136, loss = 0.01370199
Iteration 137, loss = 0.01365605
Iteration 138, loss = 0.01357729
Iteration 139, loss = 0.01350909
Iteration 140, loss = 0.01347765
Iteration 141, loss = 0.01344799
Iteration 142, loss = 0.01339396
Iteration 143, loss = 0.01334688
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71386228
Iteration 2, loss = 0.66820965
Iteration 3, loss = 0.60512995
Iteration 4, loss = 0.52232088
Iteration 5, loss = 0.43470687
Iteration 6, loss = 0.35176645
Iteration 7, loss = 0.28090333
Iteration 8, loss = 0.22408410
Iteration 9, loss = 0.18061611
Iteration 10, loss = 0.14802777
Iteration 11, loss = 0.12336118
Iteration 12, loss = 0.10495662
Iteration 13, loss = 0.09065131
Iteration 14, loss = 0.07958203
Iteration 15, loss = 0.07093599
Iteration 16, loss = 0.06388670
Iteration 17, loss = 0.05823947
Iteration 18, loss = 0.05346973
Iteration 19, loss = 0.04962939
Iteration 20, loss = 0.04623958
Iteration 21, loss = 0.04339109
Iteration 22, loss = 0.04092888
Iteration 23, loss = 0.03888944
Iteration 24, loss = 0.03710036
Iteration 25, loss = 0.03537219
Iteration 26, loss = 0.03400512
Iteration 27, loss = 0.03293073
Iteration 28, loss = 0.03175465
Iteration 29, loss = 0.03075597
Iteration 30, loss = 0.02986864
Iteration 31, loss = 0.02915572
Iteration 32, loss = 0.02841669
Iteration 33, loss = 0.02756665
Iteration 34, loss = 0.02699757
Iteration 35, loss = 0.02662609
Iteration 36, loss = 0.02607134
Iteration 37, loss = 0.02550295
Iteration 38, loss = 0.02510032
Iteration 39, loss = 0.02486420
Iteration 40, loss = 0.02443238
Iteration 41, loss = 0.02421139
Iteration 42, loss = 0.02382300
Iteration 43, loss = 0.02362062
Iteration 44, loss = 0.02337002
Iteration 45, loss = 0.02314149
Iteration 46, loss = 0.02283587
Iteration 47, loss = 0.02272107
Iteration 48, loss = 0.02225151
Iteration 49, loss = 0.02223678
Iteration 50, loss = 0.02201436
Iteration 51, loss = 0.02178985
Iteration 52, loss = 0.02173402
Iteration 53, loss = 0.02157963
Iteration 54, loss = 0.02145648
Iteration 55, loss = 0.02131056
Iteration 56, loss = 0.02131552
Iteration 57, loss = 0.02099147
Iteration 58, loss = 0.02107854
Iteration 59, loss = 0.02088758
Iteration 60, loss = 0.02064035
Iteration 61, loss = 0.02092069
Iteration 62, loss = 0.02064183
Iteration 63, loss = 0.02053999
Iteration 64, loss = 0.02044415
Iteration 65, loss = 0.02034580
Iteration 66, loss = 0.02034651
Iteration 67, loss = 0.02050453
Iteration 68, loss = 0.02020358
Iteration 69, loss = 0.02016427
Iteration 70, loss = 0.02000630
Iteration 71, loss = 0.02001130
Iteration 72, loss = 0.01998408
Iteration 73, loss = 0.01996874
Iteration 74, loss = 0.01981579
Iteration 75, loss = 0.01996778
Iteration 76, loss = 0.02000454
Iteration 77, loss = 0.01982446
Iteration 78, loss = 0.01965549
Iteration 79, loss = 0.01960363
Iteration 80, loss = 0.01960586
Iteration 81, loss = 0.01978226
Iteration 82, loss = 0.01971057
Iteration 83, loss = 0.01941976
Iteration 84, loss = 0.01941720
Iteration 85, loss = 0.01935155
Iteration 86, loss = 0.01935510
Iteration 87, loss = 0.01928663
Iteration 88, loss = 0.01919441
Iteration 89, loss = 0.01926756
Iteration 90, loss = 0.01948135
Iteration 91, loss = 0.01946540
Iteration 92, loss = 0.01908266
Iteration 93, loss = 0.01921236
Iteration 94, loss = 0.01915611
Iteration 95, loss = 0.01926046
Iteration 96, loss = 0.01913176
Iteration 97, loss = 0.01906491
Iteration 98, loss = 0.01893155
Iteration 99, loss = 0.01927554
Iteration 100, loss = 0.01913432
Iteration 101, loss = 0.01918515
Iteration 102, loss = 0.01914032
Iteration 103, loss = 0.01923567
Iteration 104, loss = 0.01892464
Iteration 105, loss = 0.01890534
Iteration 106, loss = 0.01899037
Iteration 107, loss = 0.01875239
Iteration 108, loss = 0.01883089
Iteration 109, loss = 0.01879951
Iteration 110, loss = 0.01872084
Iteration 111, loss = 0.01893725
Iteration 112, loss = 0.01917236
Iteration 113, loss = 0.01876884
Iteration 114, loss = 0.01869854
Iteration 115, loss = 0.01881545
Iteration 116, loss = 0.01879582
Iteration 117, loss = 0.01878330
Iteration 118, loss = 0.01882480
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73094573
Iteration 2, loss = 0.70732936
Iteration 3, loss = 0.68118839
Iteration 4, loss = 0.64658793
Iteration 5, loss = 0.60551222
Iteration 6, loss = 0.55867397
Iteration 7, loss = 0.50815202
Iteration 8, loss = 0.45760113
Iteration 9, loss = 0.40772851
Iteration 10, loss = 0.36069012
Iteration 11, loss = 0.31815887
Iteration 12, loss = 0.27996841
Iteration 13, loss = 0.24619472
Iteration 14, loss = 0.21699509
Iteration 15, loss = 0.19187845
Iteration 16, loss = 0.17035382
Iteration 17, loss = 0.15210546
Iteration 18, loss = 0.13655244
Iteration 19, loss = 0.12334652
Iteration 20, loss = 0.11208921
Iteration 21, loss = 0.10237891
Iteration 22, loss = 0.09376092
Iteration 23, loss = 0.08650031
Iteration 24, loss = 0.08003358
Iteration 25, loss = 0.07433160
Iteration 26, loss = 0.06937689
Iteration 27, loss = 0.06490910
Iteration 28, loss = 0.06097694
Iteration 29, loss = 0.05754545
Iteration 30, loss = 0.05448137
Iteration 31, loss = 0.05175922
Iteration 32, loss = 0.04916714
Iteration 33, loss = 0.04691051
Iteration 34, loss = 0.04489785
Iteration 35, loss = 0.04315481
Iteration 36, loss = 0.04167370
Iteration 37, loss = 0.04008337
Iteration 38, loss = 0.03870160
Iteration 39, loss = 0.03731710
Iteration 40, loss = 0.03612701
Iteration 41, loss = 0.03516756
Iteration 42, loss = 0.03401962
Iteration 43, loss = 0.03311326
Iteration 44, loss = 0.03208708
Iteration 45, loss = 0.03125384
Iteration 46, loss = 0.03052437
Iteration 47, loss = 0.02985561
Iteration 48, loss = 0.02917356
Iteration 49, loss = 0.02847592
Iteration 50, loss = 0.02789770
Iteration 51, loss = 0.02733747
Iteration 52, loss = 0.02684130
Iteration 53, loss = 0.02638964
Iteration 54, loss = 0.02599346
Iteration 55, loss = 0.02551282
Iteration 56, loss = 0.02500705
Iteration 57, loss = 0.02458729
Iteration 58, loss = 0.02421081
Iteration 59, loss = 0.02391992
Iteration 60, loss = 0.02370322
Iteration 61, loss = 0.02331213
Iteration 62, loss = 0.02296647
Iteration 63, loss = 0.02263476
Iteration 64, loss = 0.02242955
Iteration 65, loss = 0.02207114
Iteration 66, loss = 0.02221175
Iteration 67, loss = 0.02196216
Iteration 68, loss = 0.02168693
Iteration 69, loss = 0.02159073
Iteration 70, loss = 0.02136191
Iteration 71, loss = 0.02111223
Iteration 72, loss = 0.02079653
Iteration 73, loss = 0.02056970
Iteration 74, loss = 0.02062496
Iteration 75, loss = 0.02041953
Iteration 76, loss = 0.02025616
Iteration 77, loss = 0.02010939
Iteration 78, loss = 0.01987611
Iteration 79, loss = 0.01979095
Iteration 80, loss = 0.01975479
Iteration 81, loss = 0.01978399
Iteration 82, loss = 0.01959148
Iteration 83, loss = 0.01953421
Iteration 84, loss = 0.01935887
Iteration 85, loss = 0.01909629
Iteration 86, loss = 0.01916144
Iteration 87, loss = 0.01893577
Iteration 88, loss = 0.01882084
Iteration 89, loss = 0.01860656
Iteration 90, loss = 0.01851341
Iteration 91, loss = 0.01854001
Iteration 92, loss = 0.01837514
Iteration 93, loss = 0.01816318
Iteration 94, loss = 0.01820874
Iteration 95, loss = 0.01790670
Iteration 96, loss = 0.01789022
Iteration 97, loss = 0.01811741
Iteration 98, loss = 0.01813889
Iteration 99, loss = 0.01801284
Iteration 100, loss = 0.01754687
Iteration 101, loss = 0.01748262
Iteration 102, loss = 0.01735439
Iteration 103, loss = 0.01744440
Iteration 104, loss = 0.01736258
Iteration 105, loss = 0.01734856
Iteration 106, loss = 0.01725447
Iteration 107, loss = 0.01720033
Iteration 108, loss = 0.01719635
Iteration 109, loss = 0.01705495
Iteration 110, loss = 0.01698687
Iteration 111, loss = 0.01695408
Iteration 112, loss = 0.01682539
Iteration 113, loss = 0.01688327
Iteration 114, loss = 0.01699796
Iteration 115, loss = 0.01700166
Iteration 116, loss = 0.01691859
Iteration 117, loss = 0.01672894
Iteration 118, loss = 0.01662423
Iteration 119, loss = 0.01696784
Iteration 120, loss = 0.01667922
Iteration 121, loss = 0.01655507
Iteration 122, loss = 0.01643190
Iteration 123, loss = 0.01654745
Iteration 124, loss = 0.01651579
Iteration 125, loss = 0.01644772
Iteration 126, loss = 0.01700654
Iteration 127, loss = 0.01717571
Iteration 128, loss = 0.01696376
Iteration 129, loss = 0.01689081
Iteration 130, loss = 0.01652690
Iteration 131, loss = 0.01641597
Iteration 132, loss = 0.01647663
Iteration 133, loss = 0.01655522
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68897454
Iteration 2, loss = 0.62044230
Iteration 3, loss = 0.53055384
Iteration 4, loss = 0.43342076
Iteration 5, loss = 0.34472219
Iteration 6, loss = 0.27050313
Iteration 7, loss = 0.21202101
Iteration 8, loss = 0.16925777
Iteration 9, loss = 0.13799218
Iteration 10, loss = 0.11485546
Iteration 11, loss = 0.09787654
Iteration 12, loss = 0.08507957
Iteration 13, loss = 0.07490454
Iteration 14, loss = 0.06702975
Iteration 15, loss = 0.06084087
Iteration 16, loss = 0.05579881
Iteration 17, loss = 0.05173944
Iteration 18, loss = 0.04806824
Iteration 19, loss = 0.04514658
Iteration 20, loss = 0.04281921
Iteration 21, loss = 0.04066979
Iteration 22, loss = 0.03899294
Iteration 23, loss = 0.03723403
Iteration 24, loss = 0.03597422
Iteration 25, loss = 0.03431542
Iteration 26, loss = 0.03402340
Iteration 27, loss = 0.03306206
Iteration 28, loss = 0.03215865
Iteration 29, loss = 0.03131895
Iteration 30, loss = 0.03064280
Iteration 31, loss = 0.02971365
Iteration 32, loss = 0.02941094
Iteration 33, loss = 0.02903882
Iteration 34, loss = 0.02822073
Iteration 35, loss = 0.02777650
Iteration 36, loss = 0.02724018
Iteration 37, loss = 0.02688059
Iteration 38, loss = 0.02674917
Iteration 39, loss = 0.02613957
Iteration 40, loss = 0.02604558
Iteration 41, loss = 0.02561851
Iteration 42, loss = 0.02539399
Iteration 43, loss = 0.02501379
Iteration 44, loss = 0.02473738
Iteration 45, loss = 0.02472237
Iteration 46, loss = 0.02459283
Iteration 47, loss = 0.02490233
Iteration 48, loss = 0.02560447
Iteration 49, loss = 0.02485282
Iteration 50, loss = 0.02422801
Iteration 51, loss = 0.02403850
Iteration 52, loss = 0.02395736
Iteration 53, loss = 0.02394371
Iteration 54, loss = 0.02354820
Iteration 55, loss = 0.02360790
Iteration 56, loss = 0.02339594
Iteration 57, loss = 0.02357828
Iteration 58, loss = 0.02315139
Iteration 59, loss = 0.02344327
Iteration 60, loss = 0.02313924
Iteration 61, loss = 0.02294702
Iteration 62, loss = 0.02284084
Iteration 63, loss = 0.02281137
Iteration 64, loss = 0.02268455
Iteration 65, loss = 0.02253212
Iteration 66, loss = 0.02243534
Iteration 67, loss = 0.02239900
Iteration 68, loss = 0.02250264
Iteration 69, loss = 0.02232207
Iteration 70, loss = 0.02229294
Iteration 71, loss = 0.02228074
Iteration 72, loss = 0.02203700
Iteration 73, loss = 0.02195308
Iteration 74, loss = 0.02222640
Iteration 75, loss = 0.02183488
Iteration 76, loss = 0.02209017
Iteration 77, loss = 0.02181894
Iteration 78, loss = 0.02197784
Iteration 79, loss = 0.02160638
Iteration 80, loss = 0.02158175
Iteration 81, loss = 0.02159251
Iteration 82, loss = 0.02158349
Iteration 83, loss = 0.02156923
Iteration 84, loss = 0.02136811
Iteration 85, loss = 0.02142999
Iteration 86, loss = 0.02119937
Iteration 87, loss = 0.02140602
Iteration 88, loss = 0.02143324
Iteration 89, loss = 0.02128039
Iteration 90, loss = 0.02135881
Iteration 91, loss = 0.02129854
Iteration 92, loss = 0.02108220
Iteration 93, loss = 0.02124632
Iteration 94, loss = 0.02115427
Iteration 95, loss = 0.02123191
Iteration 96, loss = 0.02142961
Iteration 97, loss = 0.02102210
Iteration 98, loss = 0.02118392
Iteration 99, loss = 0.02105071
Iteration 100, loss = 0.02096936
Iteration 101, loss = 0.02082689
Iteration 102, loss = 0.02110283
Iteration 103, loss = 0.02104794
Iteration 104, loss = 0.02102308
Iteration 105, loss = 0.02106202
Iteration 106, loss = 0.02104012
Iteration 107, loss = 0.02122311
Iteration 108, loss = 0.02078293
Iteration 109, loss = 0.02093109
Iteration 110, loss = 0.02076305
Iteration 111, loss = 0.02091634
Iteration 112, loss = 0.02086814
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69148110
Iteration 2, loss = 0.65829206
Iteration 3, loss = 0.61625625
Iteration 4, loss = 0.56255980
Iteration 5, loss = 0.50054025
Iteration 6, loss = 0.43781966
Iteration 7, loss = 0.37819806
Iteration 8, loss = 0.32402446
Iteration 9, loss = 0.27645819
Iteration 10, loss = 0.23559775
Iteration 11, loss = 0.20124537
Iteration 12, loss = 0.17271591
Iteration 13, loss = 0.14929432
Iteration 14, loss = 0.12999642
Iteration 15, loss = 0.11414143
Iteration 16, loss = 0.10096586
Iteration 17, loss = 0.09019543
Iteration 18, loss = 0.08118909
Iteration 19, loss = 0.07373784
Iteration 20, loss = 0.06720622
Iteration 21, loss = 0.06175632
Iteration 22, loss = 0.05711578
Iteration 23, loss = 0.05301929
Iteration 24, loss = 0.04950545
Iteration 25, loss = 0.04649335
Iteration 26, loss = 0.04388097
Iteration 27, loss = 0.04151538
Iteration 28, loss = 0.03944627
Iteration 29, loss = 0.03741328
Iteration 30, loss = 0.03573918
Iteration 31, loss = 0.03431567
Iteration 32, loss = 0.03295854
Iteration 33, loss = 0.03170721
Iteration 34, loss = 0.03071353
Iteration 35, loss = 0.02961879
Iteration 36, loss = 0.02869044
Iteration 37, loss = 0.02781478
Iteration 38, loss = 0.02721202
Iteration 39, loss = 0.02636607
Iteration 40, loss = 0.02581396
Iteration 41, loss = 0.02513494
Iteration 42, loss = 0.02451955
Iteration 43, loss = 0.02406903
Iteration 44, loss = 0.02359235
Iteration 45, loss = 0.02319857
Iteration 46, loss = 0.02278815
Iteration 47, loss = 0.02236015
Iteration 48, loss = 0.02190497
Iteration 49, loss = 0.02159354
Iteration 50, loss = 0.02128259
Iteration 51, loss = 0.02108696
Iteration 52, loss = 0.02069303
Iteration 53, loss = 0.02053577
Iteration 54, loss = 0.02024521
Iteration 55, loss = 0.02014951
Iteration 56, loss = 0.01975001
Iteration 57, loss = 0.01961358
Iteration 58, loss = 0.01953134
Iteration 59, loss = 0.01920690
Iteration 60, loss = 0.01894134
Iteration 61, loss = 0.01891368
Iteration 62, loss = 0.01873306
Iteration 63, loss = 0.01859072
Iteration 64, loss = 0.01839365
Iteration 65, loss = 0.01830261
Iteration 66, loss = 0.01808826
Iteration 67, loss = 0.01800785
Iteration 68, loss = 0.01792681
Iteration 69, loss = 0.01778401
Iteration 70, loss = 0.01763230
Iteration 71, loss = 0.01766498
Iteration 72, loss = 0.01737334
Iteration 73, loss = 0.01738637
Iteration 74, loss = 0.01732244
Iteration 75, loss = 0.01712570
Iteration 76, loss = 0.01715327
Iteration 77, loss = 0.01708511
Iteration 78, loss = 0.01695238
Iteration 79, loss = 0.01693723
Iteration 80, loss = 0.01685126
Iteration 81, loss = 0.01677455
Iteration 82, loss = 0.01674608
Iteration 83, loss = 0.01662476
Iteration 84, loss = 0.01674716
Iteration 85, loss = 0.01652842
Iteration 86, loss = 0.01643238
Iteration 87, loss = 0.01645919
Iteration 88, loss = 0.01635385
Iteration 89, loss = 0.01624165
Iteration 90, loss = 0.01627308
Iteration 91, loss = 0.01618866
Iteration 92, loss = 0.01611068
Iteration 93, loss = 0.01606113
Iteration 94, loss = 0.01605949
Iteration 95, loss = 0.01616651
Iteration 96, loss = 0.01600518
Iteration 97, loss = 0.01596120
Iteration 98, loss = 0.01589531
Iteration 99, loss = 0.01585929
Iteration 100, loss = 0.01603126
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68423278
Iteration 2, loss = 0.60339561
Iteration 3, loss = 0.49455563
Iteration 4, loss = 0.38286885
Iteration 5, loss = 0.28789624
Iteration 6, loss = 0.21609460
Iteration 7, loss = 0.16533874
Iteration 8, loss = 0.13017697
Iteration 9, loss = 0.10610654
Iteration 10, loss = 0.08895411
Iteration 11, loss = 0.07603759
Iteration 12, loss = 0.06663398
Iteration 13, loss = 0.05937668
Iteration 14, loss = 0.05360341
Iteration 15, loss = 0.04909456
Iteration 16, loss = 0.04554077
Iteration 17, loss = 0.04245730
Iteration 18, loss = 0.03983795
Iteration 19, loss = 0.03777135
Iteration 20, loss = 0.03590620
Iteration 21, loss = 0.03449436
Iteration 22, loss = 0.03301605
Iteration 23, loss = 0.03218530
Iteration 24, loss = 0.03110111
Iteration 25, loss = 0.03023253
Iteration 26, loss = 0.02951430
Iteration 27, loss = 0.02864543
Iteration 28, loss = 0.02808839
Iteration 29, loss = 0.02770940
Iteration 30, loss = 0.02716522
Iteration 31, loss = 0.02657179
Iteration 32, loss = 0.02614276
Iteration 33, loss = 0.02579938
Iteration 34, loss = 0.02546887
Iteration 35, loss = 0.02532776
Iteration 36, loss = 0.02509198
Iteration 37, loss = 0.02468825
Iteration 38, loss = 0.02448828
Iteration 39, loss = 0.02428317
Iteration 40, loss = 0.02408798
Iteration 41, loss = 0.02372630
Iteration 42, loss = 0.02367573
Iteration 43, loss = 0.02377865
Iteration 44, loss = 0.02338101
Iteration 45, loss = 0.02322144
Iteration 46, loss = 0.02300068
Iteration 47, loss = 0.02303751
Iteration 48, loss = 0.02277489
Iteration 49, loss = 0.02272808
Iteration 50, loss = 0.02277142
Iteration 51, loss = 0.02282925
Iteration 52, loss = 0.02262898
Iteration 53, loss = 0.02232183
Iteration 54, loss = 0.02228179
Iteration 55, loss = 0.02239753
Iteration 56, loss = 0.02219830
Iteration 57, loss = 0.02241563
Iteration 58, loss = 0.02201070
Iteration 59, loss = 0.02209310
Iteration 60, loss = 0.02202785
Iteration 61, loss = 0.02196328
Iteration 62, loss = 0.02197166
Iteration 63, loss = 0.02192458
Iteration 64, loss = 0.02169045
Iteration 65, loss = 0.02187009
Iteration 66, loss = 0.02189618
Iteration 67, loss = 0.02178446
Iteration 68, loss = 0.02170392
Iteration 69, loss = 0.02161385
Iteration 70, loss = 0.02190589
Iteration 71, loss = 0.02142137
Iteration 72, loss = 0.02157369
Iteration 73, loss = 0.02168711
Iteration 74, loss = 0.02147373
Iteration 75, loss = 0.02145369
Iteration 76, loss = 0.02137164
Iteration 77, loss = 0.02146983
Iteration 78, loss = 0.02128788
Iteration 79, loss = 0.02148180
Iteration 80, loss = 0.02131348
Iteration 81, loss = 0.02124791
Iteration 82, loss = 0.02127030
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68413076
Iteration 2, loss = 0.61808516
Iteration 3, loss = 0.51384550
Iteration 4, loss = 0.39953174
Iteration 5, loss = 0.29958660
Iteration 6, loss = 0.22443616
Iteration 7, loss = 0.17135803
Iteration 8, loss = 0.13480374
Iteration 9, loss = 0.10963786
Iteration 10, loss = 0.09205319
Iteration 11, loss = 0.07907471
Iteration 12, loss = 0.06932105
Iteration 13, loss = 0.06221011
Iteration 14, loss = 0.05642601
Iteration 15, loss = 0.05171795
Iteration 16, loss = 0.04771086
Iteration 17, loss = 0.04492750
Iteration 18, loss = 0.04235122
Iteration 19, loss = 0.04022050
Iteration 20, loss = 0.03866780
Iteration 21, loss = 0.03688821
Iteration 22, loss = 0.03553907
Iteration 23, loss = 0.03455811
Iteration 24, loss = 0.03317658
Iteration 25, loss = 0.03267333
Iteration 26, loss = 0.03147139
Iteration 27, loss = 0.03093038
Iteration 28, loss = 0.03021259
Iteration 29, loss = 0.02984408
Iteration 30, loss = 0.02936033
Iteration 31, loss = 0.02893804
Iteration 32, loss = 0.02899197
Iteration 33, loss = 0.02840530
Iteration 34, loss = 0.02798354
Iteration 35, loss = 0.02775464
Iteration 36, loss = 0.02736206
Iteration 37, loss = 0.02700085
Iteration 38, loss = 0.02659983
Iteration 39, loss = 0.02676792
Iteration 40, loss = 0.02627096
Iteration 41, loss = 0.02627902
Iteration 42, loss = 0.02614471
Iteration 43, loss = 0.02586519
Iteration 44, loss = 0.02582374
Iteration 45, loss = 0.02601019
Iteration 46, loss = 0.02536131
Iteration 47, loss = 0.02541268
Iteration 48, loss = 0.02566429
Iteration 49, loss = 0.02539103
Iteration 50, loss = 0.02502808
Iteration 51, loss = 0.02482679
Iteration 52, loss = 0.02476377
Iteration 53, loss = 0.02516314
Iteration 54, loss = 0.02477302
Iteration 55, loss = 0.02481308
Iteration 56, loss = 0.02509425
Iteration 57, loss = 0.02462482
Iteration 58, loss = 0.02467788
Iteration 59, loss = 0.02425647
Iteration 60, loss = 0.02426164
Iteration 61, loss = 0.02430673
Iteration 62, loss = 0.02420488
Iteration 63, loss = 0.02403265
Iteration 64, loss = 0.02443738
Iteration 65, loss = 0.02397600
Iteration 66, loss = 0.02443546
Iteration 67, loss = 0.02396654
Iteration 68, loss = 0.02389890
Iteration 69, loss = 0.02385129
Iteration 70, loss = 0.02383891
Iteration 71, loss = 0.02400848
Iteration 72, loss = 0.02368868
Iteration 73, loss = 0.02392049
Iteration 74, loss = 0.02390102
Iteration 75, loss = 0.02385124
Iteration 76, loss = 0.02382285
Iteration 77, loss = 0.02364604
Iteration 78, loss = 0.02394046
Iteration 79, loss = 0.02363261
Iteration 80, loss = 0.02394544
Iteration 81, loss = 0.02385290
Iteration 82, loss = 0.02344547
Iteration 83, loss = 0.02370467
Iteration 84, loss = 0.02347568
Iteration 85, loss = 0.02347882
Iteration 86, loss = 0.02341423
Iteration 87, loss = 0.02330067
Iteration 88, loss = 0.02332169
Iteration 89, loss = 0.02337406
Iteration 90, loss = 0.02313402
Iteration 91, loss = 0.02335541
Iteration 92, loss = 0.02345969
Iteration 93, loss = 0.02348148
Iteration 94, loss = 0.02319795
Iteration 95, loss = 0.02333483
Iteration 96, loss = 0.02332289
Iteration 97, loss = 0.02321844
Iteration 98, loss = 0.02328673
Iteration 99, loss = 0.02346247
Iteration 100, loss = 0.02335005
Iteration 101, loss = 0.02341258
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69257110
Iteration 2, loss = 0.61799601
Iteration 3, loss = 0.50727367
Iteration 4, loss = 0.38981504
Iteration 5, loss = 0.29134478
Iteration 6, loss = 0.21768295
Iteration 7, loss = 0.16625200
Iteration 8, loss = 0.13089335
Iteration 9, loss = 0.10632508
Iteration 10, loss = 0.08894651
Iteration 11, loss = 0.07614541
Iteration 12, loss = 0.06670017
Iteration 13, loss = 0.05932359
Iteration 14, loss = 0.05355722
Iteration 15, loss = 0.04918860
Iteration 16, loss = 0.04544244
Iteration 17, loss = 0.04226977
Iteration 18, loss = 0.03988580
Iteration 19, loss = 0.03797434
Iteration 20, loss = 0.03587197
Iteration 21, loss = 0.03422294
Iteration 22, loss = 0.03284045
Iteration 23, loss = 0.03175823
Iteration 24, loss = 0.03087076
Iteration 25, loss = 0.02992813
Iteration 26, loss = 0.02923842
Iteration 27, loss = 0.02862602
Iteration 28, loss = 0.02794646
Iteration 29, loss = 0.02742926
Iteration 30, loss = 0.02676752
Iteration 31, loss = 0.02629530
Iteration 32, loss = 0.02617404
Iteration 33, loss = 0.02556442
Iteration 34, loss = 0.02545123
Iteration 35, loss = 0.02517163
Iteration 36, loss = 0.02448476
Iteration 37, loss = 0.02450309
Iteration 38, loss = 0.02428509
Iteration 39, loss = 0.02423932
Iteration 40, loss = 0.02375358
Iteration 41, loss = 0.02367513
Iteration 42, loss = 0.02377857
Iteration 43, loss = 0.02317311
Iteration 44, loss = 0.02335021
Iteration 45, loss = 0.02294159
Iteration 46, loss = 0.02280162
Iteration 47, loss = 0.02278491
Iteration 48, loss = 0.02276730
Iteration 49, loss = 0.02264596
Iteration 50, loss = 0.02260109
Iteration 51, loss = 0.02220798
Iteration 52, loss = 0.02252805
Iteration 53, loss = 0.02236745
Iteration 54, loss = 0.02204998
Iteration 55, loss = 0.02235866
Iteration 56, loss = 0.02175601
Iteration 57, loss = 0.02203345
Iteration 58, loss = 0.02185203
Iteration 59, loss = 0.02218934
Iteration 60, loss = 0.02175825
Iteration 61, loss = 0.02209935
Iteration 62, loss = 0.02163910
Iteration 63, loss = 0.02188358
Iteration 64, loss = 0.02159991
Iteration 65, loss = 0.02146257
Iteration 66, loss = 0.02160775
Iteration 67, loss = 0.02156731
Iteration 68, loss = 0.02127287
Iteration 69, loss = 0.02151922
Iteration 70, loss = 0.02143308
Iteration 71, loss = 0.02109255
Iteration 72, loss = 0.02108468
Iteration 73, loss = 0.02137461
Iteration 74, loss = 0.02138984
Iteration 75, loss = 0.02117911
Iteration 76, loss = 0.02114765
Iteration 77, loss = 0.02138612
Iteration 78, loss = 0.02120793
Iteration 79, loss = 0.02093791
Iteration 80, loss = 0.02112812
Iteration 81, loss = 0.02108251
Iteration 82, loss = 0.02094552
Iteration 83, loss = 0.02138675
Iteration 84, loss = 0.02073162
Iteration 85, loss = 0.02113813
Iteration 86, loss = 0.02088077
Iteration 87, loss = 0.02122435
Iteration 88, loss = 0.02073580
Iteration 89, loss = 0.02108176
Iteration 90, loss = 0.02088545
Iteration 91, loss = 0.02102482
Iteration 92, loss = 0.02061507
Iteration 93, loss = 0.02096556
Iteration 94, loss = 0.02070951
Iteration 95, loss = 0.02075420
Iteration 96, loss = 0.02079797
Iteration 97, loss = 0.02088086
Iteration 98, loss = 0.02084802
Iteration 99, loss = 0.02092663
Iteration 100, loss = 0.02068179
Iteration 101, loss = 0.02068148
Iteration 102, loss = 0.02074489
Iteration 103, loss = 0.02073930
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69015919
Iteration 2, loss = 0.65786144
Iteration 3, loss = 0.60841516
Iteration 4, loss = 0.54262266
Iteration 5, loss = 0.46956298
Iteration 6, loss = 0.39780344
Iteration 7, loss = 0.33336967
Iteration 8, loss = 0.27748865
Iteration 9, loss = 0.23111357
Iteration 10, loss = 0.19372116
Iteration 11, loss = 0.16374926
Iteration 12, loss = 0.13976615
Iteration 13, loss = 0.12078676
Iteration 14, loss = 0.10563299
Iteration 15, loss = 0.09337426
Iteration 16, loss = 0.08323169
Iteration 17, loss = 0.07497726
Iteration 18, loss = 0.06812535
Iteration 19, loss = 0.06243276
Iteration 20, loss = 0.05751823
Iteration 21, loss = 0.05342603
Iteration 22, loss = 0.04974286
Iteration 23, loss = 0.04675713
Iteration 24, loss = 0.04412484
Iteration 25, loss = 0.04188170
Iteration 26, loss = 0.03965700
Iteration 27, loss = 0.03781641
Iteration 28, loss = 0.03618770
Iteration 29, loss = 0.03474531
Iteration 30, loss = 0.03346231
Iteration 31, loss = 0.03231057
Iteration 32, loss = 0.03123631
Iteration 33, loss = 0.03041722
Iteration 34, loss = 0.02954563
Iteration 35, loss = 0.02869557
Iteration 36, loss = 0.02783241
Iteration 37, loss = 0.02729058
Iteration 38, loss = 0.02677629
Iteration 39, loss = 0.02630294
Iteration 40, loss = 0.02565350
Iteration 41, loss = 0.02524960
Iteration 42, loss = 0.02480976
Iteration 43, loss = 0.02443806
Iteration 44, loss = 0.02400912
Iteration 45, loss = 0.02377741
Iteration 46, loss = 0.02353698
Iteration 47, loss = 0.02301275
Iteration 48, loss = 0.02256994
Iteration 49, loss = 0.02246156
Iteration 50, loss = 0.02214830
Iteration 51, loss = 0.02200946
Iteration 52, loss = 0.02188460
Iteration 53, loss = 0.02175176
Iteration 54, loss = 0.02140837
Iteration 55, loss = 0.02127892
Iteration 56, loss = 0.02114067
Iteration 57, loss = 0.02101712
Iteration 58, loss = 0.02063614
Iteration 59, loss = 0.02048180
Iteration 60, loss = 0.02039154
Iteration 61, loss = 0.02021415
Iteration 62, loss = 0.02009039
Iteration 63, loss = 0.02001778
Iteration 64, loss = 0.01993920
Iteration 65, loss = 0.01978915
Iteration 66, loss = 0.01973607
Iteration 67, loss = 0.01974766
Iteration 68, loss = 0.01947981
Iteration 69, loss = 0.01948941
Iteration 70, loss = 0.01932845
Iteration 71, loss = 0.01933092
Iteration 72, loss = 0.01927055
Iteration 73, loss = 0.01907531
Iteration 74, loss = 0.01918893
Iteration 75, loss = 0.01900155
Iteration 76, loss = 0.01897049
Iteration 77, loss = 0.01886137
Iteration 78, loss = 0.01884523
Iteration 79, loss = 0.01872264
Iteration 80, loss = 0.01862265
Iteration 81, loss = 0.01873308
Iteration 82, loss = 0.01854728
Iteration 83, loss = 0.01859934
Iteration 84, loss = 0.01841104
Iteration 85, loss = 0.01851894
Iteration 86, loss = 0.01839973
Iteration 87, loss = 0.01833249
Iteration 88, loss = 0.01827986
Iteration 89, loss = 0.01815908
Iteration 90, loss = 0.01853906
Iteration 91, loss = 0.01813066
Iteration 92, loss = 0.01810055
Iteration 93, loss = 0.01812249
Iteration 94, loss = 0.01812244
Iteration 95, loss = 0.01795906
Iteration 96, loss = 0.01798919
Iteration 97, loss = 0.01801993
Iteration 98, loss = 0.01796906
Iteration 99, loss = 0.01793744
Iteration 100, loss = 0.01791749
Iteration 101, loss = 0.01783299
Iteration 102, loss = 0.01781392
Iteration 103, loss = 0.01778386
Iteration 104, loss = 0.01782808
Iteration 105, loss = 0.01774980
Iteration 106, loss = 0.01758110
Iteration 107, loss = 0.01758681
Iteration 108, loss = 0.01776293
Iteration 109, loss = 0.01769075
Iteration 110, loss = 0.01760328
Iteration 111, loss = 0.01753291
Iteration 112, loss = 0.01769925
Iteration 113, loss = 0.01775139
Iteration 114, loss = 0.01752412
Iteration 115, loss = 0.01755998
Iteration 116, loss = 0.01753589
Iteration 117, loss = 0.01749637
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67449853
Iteration 2, loss = 0.60067448
Iteration 3, loss = 0.49864090
Iteration 4, loss = 0.39292542
Iteration 5, loss = 0.30178651
Iteration 6, loss = 0.23007837
Iteration 7, loss = 0.17807605
Iteration 8, loss = 0.14099123
Iteration 9, loss = 0.11470488
Iteration 10, loss = 0.09577906
Iteration 11, loss = 0.08193558
Iteration 12, loss = 0.07138687
Iteration 13, loss = 0.06309529
Iteration 14, loss = 0.05689312
Iteration 15, loss = 0.05202784
Iteration 16, loss = 0.04775738
Iteration 17, loss = 0.04417341
Iteration 18, loss = 0.04147773
Iteration 19, loss = 0.03928015
Iteration 20, loss = 0.03725046
Iteration 21, loss = 0.03567452
Iteration 22, loss = 0.03403984
Iteration 23, loss = 0.03262505
Iteration 24, loss = 0.03141145
Iteration 25, loss = 0.03039433
Iteration 26, loss = 0.02970477
Iteration 27, loss = 0.02892423
Iteration 28, loss = 0.02799022
Iteration 29, loss = 0.02752733
Iteration 30, loss = 0.02690077
Iteration 31, loss = 0.02632449
Iteration 32, loss = 0.02608001
Iteration 33, loss = 0.02572259
Iteration 34, loss = 0.02518363
Iteration 35, loss = 0.02469701
Iteration 36, loss = 0.02436941
Iteration 37, loss = 0.02428407
Iteration 38, loss = 0.02374807
Iteration 39, loss = 0.02353283
Iteration 40, loss = 0.02354164
Iteration 41, loss = 0.02355547
Iteration 42, loss = 0.02346128
Iteration 43, loss = 0.02310948
Iteration 44, loss = 0.02254159
Iteration 45, loss = 0.02258219
Iteration 46, loss = 0.02239844
Iteration 47, loss = 0.02256779
Iteration 48, loss = 0.02225927
Iteration 49, loss = 0.02184368
Iteration 50, loss = 0.02226591
Iteration 51, loss = 0.02189420
Iteration 52, loss = 0.02179593
Iteration 53, loss = 0.02144785
Iteration 54, loss = 0.02167871
Iteration 55, loss = 0.02141353
Iteration 56, loss = 0.02131784
Iteration 57, loss = 0.02120966
Iteration 58, loss = 0.02155269
Iteration 59, loss = 0.02155566
Iteration 60, loss = 0.02114307
Iteration 61, loss = 0.02118577
Iteration 62, loss = 0.02105474
Iteration 63, loss = 0.02099462
Iteration 64, loss = 0.02095354
Iteration 65, loss = 0.02097793
Iteration 66, loss = 0.02078452
Iteration 67, loss = 0.02095289
Iteration 68, loss = 0.02088981
Iteration 69, loss = 0.02057872
Iteration 70, loss = 0.02071039
Iteration 71, loss = 0.02059042
Iteration 72, loss = 0.02054549
Iteration 73, loss = 0.02037669
Iteration 74, loss = 0.02052645
Iteration 75, loss = 0.02021371
Iteration 76, loss = 0.02038780
Iteration 77, loss = 0.02045507
Iteration 78, loss = 0.02074212
Iteration 79, loss = 0.02073515
Iteration 80, loss = 0.02021147
Iteration 81, loss = 0.02033399
Iteration 82, loss = 0.02048638
Iteration 83, loss = 0.02034883
Iteration 84, loss = 0.02017918
Iteration 85, loss = 0.02003202
Iteration 86, loss = 0.02009249
Iteration 87, loss = 0.02000984
Iteration 88, loss = 0.02015670
Iteration 89, loss = 0.02013317
Iteration 90, loss = 0.02000387
Iteration 91, loss = 0.01993064
Iteration 92, loss = 0.02022070
Iteration 93, loss = 0.02014814
Iteration 94, loss = 0.02016093
Iteration 95, loss = 0.02040164
Iteration 96, loss = 0.02013232
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67941984
Iteration 2, loss = 0.63683105
Iteration 3, loss = 0.57312940
Iteration 4, loss = 0.49282994
Iteration 5, loss = 0.40984908
Iteration 6, loss = 0.33506781
Iteration 7, loss = 0.27160248
Iteration 8, loss = 0.22067685
Iteration 9, loss = 0.18090723
Iteration 10, loss = 0.15025293
Iteration 11, loss = 0.12685738
Iteration 12, loss = 0.10885114
Iteration 13, loss = 0.09476484
Iteration 14, loss = 0.08384059
Iteration 15, loss = 0.07490476
Iteration 16, loss = 0.06765591
Iteration 17, loss = 0.06184572
Iteration 18, loss = 0.05716845
Iteration 19, loss = 0.05276146
Iteration 20, loss = 0.04927295
Iteration 21, loss = 0.04650872
Iteration 22, loss = 0.04375895
Iteration 23, loss = 0.04162509
Iteration 24, loss = 0.03972981
Iteration 25, loss = 0.03807995
Iteration 26, loss = 0.03670096
Iteration 27, loss = 0.03537746
Iteration 28, loss = 0.03451503
Iteration 29, loss = 0.03320482
Iteration 30, loss = 0.03212712
Iteration 31, loss = 0.03149925
Iteration 32, loss = 0.03073838
Iteration 33, loss = 0.03001970
Iteration 34, loss = 0.02946657
Iteration 35, loss = 0.02870432
Iteration 36, loss = 0.02819164
Iteration 37, loss = 0.02781138
Iteration 38, loss = 0.02729919
Iteration 39, loss = 0.02720755
Iteration 40, loss = 0.02669657
Iteration 41, loss = 0.02623914
Iteration 42, loss = 0.02565514
Iteration 43, loss = 0.02554303
Iteration 44, loss = 0.02530208
Iteration 45, loss = 0.02512495
Iteration 46, loss = 0.02503371
Iteration 47, loss = 0.02470133
Iteration 48, loss = 0.02450717
Iteration 49, loss = 0.02418143
Iteration 50, loss = 0.02402198
Iteration 51, loss = 0.02400785
Iteration 52, loss = 0.02369346
Iteration 53, loss = 0.02346762
Iteration 54, loss = 0.02318820
Iteration 55, loss = 0.02319071
Iteration 56, loss = 0.02314049
Iteration 57, loss = 0.02309209
Iteration 58, loss = 0.02283382
Iteration 59, loss = 0.02263889
Iteration 60, loss = 0.02249856
Iteration 61, loss = 0.02250823
Iteration 62, loss = 0.02252265
Iteration 63, loss = 0.02235216
Iteration 64, loss = 0.02223583
Iteration 65, loss = 0.02216398
Iteration 66, loss = 0.02206544
Iteration 67, loss = 0.02189506
Iteration 68, loss = 0.02195206
Iteration 69, loss = 0.02192787
Iteration 70, loss = 0.02162569
Iteration 71, loss = 0.02190777
Iteration 72, loss = 0.02157929
Iteration 73, loss = 0.02144417
Iteration 74, loss = 0.02197584
Iteration 75, loss = 0.02156356
Iteration 76, loss = 0.02157996
Iteration 77, loss = 0.02152822
Iteration 78, loss = 0.02162590
Iteration 79, loss = 0.02144979
Iteration 80, loss = 0.02153108
Iteration 81, loss = 0.02130622
Iteration 82, loss = 0.02121696
Iteration 83, loss = 0.02148544
Iteration 84, loss = 0.02176024
Iteration 85, loss = 0.02150470
Iteration 86, loss = 0.02123448
Iteration 87, loss = 0.02124528
Iteration 88, loss = 0.02114262
Iteration 89, loss = 0.02106973
Iteration 90, loss = 0.02110262
Iteration 91, loss = 0.02107424
Iteration 92, loss = 0.02115508
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71780902
Iteration 2, loss = 0.65405499
Iteration 3, loss = 0.55501245
Iteration 4, loss = 0.43647032
Iteration 5, loss = 0.33020575
Iteration 6, loss = 0.24756337
Iteration 7, loss = 0.18872467
Iteration 8, loss = 0.14712426
Iteration 9, loss = 0.11867122
Iteration 10, loss = 0.09857839
Iteration 11, loss = 0.08348408
Iteration 12, loss = 0.07274642
Iteration 13, loss = 0.06430208
Iteration 14, loss = 0.05763743
Iteration 15, loss = 0.05247186
Iteration 16, loss = 0.04847275
Iteration 17, loss = 0.04499963
Iteration 18, loss = 0.04221055
Iteration 19, loss = 0.03956546
Iteration 20, loss = 0.03771623
Iteration 21, loss = 0.03592789
Iteration 22, loss = 0.03480429
Iteration 23, loss = 0.03355821
Iteration 24, loss = 0.03253444
Iteration 25, loss = 0.03104712
Iteration 26, loss = 0.03037157
Iteration 27, loss = 0.02945887
Iteration 28, loss = 0.02881863
Iteration 29, loss = 0.02845812
Iteration 30, loss = 0.02744011
Iteration 31, loss = 0.02728847
Iteration 32, loss = 0.02704334
Iteration 33, loss = 0.02631935
Iteration 34, loss = 0.02602032
Iteration 35, loss = 0.02573442
Iteration 36, loss = 0.02541694
Iteration 37, loss = 0.02509671
Iteration 38, loss = 0.02507459
Iteration 39, loss = 0.02471459
Iteration 40, loss = 0.02421242
Iteration 41, loss = 0.02399149
Iteration 42, loss = 0.02422420
Iteration 43, loss = 0.02415866
Iteration 44, loss = 0.02378072
Iteration 45, loss = 0.02356818
Iteration 46, loss = 0.02330859
Iteration 47, loss = 0.02317583
Iteration 48, loss = 0.02309041
Iteration 49, loss = 0.02279626
Iteration 50, loss = 0.02308143
Iteration 51, loss = 0.02288098
Iteration 52, loss = 0.02272389
Iteration 53, loss = 0.02260272
Iteration 54, loss = 0.02256595
Iteration 55, loss = 0.02258013
Iteration 56, loss = 0.02235280
Iteration 57, loss = 0.02250013
Iteration 58, loss = 0.02220796
Iteration 59, loss = 0.02229314
Iteration 60, loss = 0.02208776
Iteration 61, loss = 0.02204784
Iteration 62, loss = 0.02209504
Iteration 63, loss = 0.02232235
Iteration 64, loss = 0.02216185
Iteration 65, loss = 0.02182774
Iteration 66, loss = 0.02165119
Iteration 67, loss = 0.02169411
Iteration 68, loss = 0.02181233
Iteration 69, loss = 0.02198519
Iteration 70, loss = 0.02192536
Iteration 71, loss = 0.02189960
Iteration 72, loss = 0.02204667
Iteration 73, loss = 0.02127514
Iteration 74, loss = 0.02152440
Iteration 75, loss = 0.02149474
Iteration 76, loss = 0.02137227
Iteration 77, loss = 0.02170763
Iteration 78, loss = 0.02157651
Iteration 79, loss = 0.02133709
Iteration 80, loss = 0.02141639
Iteration 81, loss = 0.02107038
Iteration 82, loss = 0.02138953
Iteration 83, loss = 0.02102701
Iteration 84, loss = 0.02149848
Iteration 85, loss = 0.02126186
Iteration 86, loss = 0.02114723
Iteration 87, loss = 0.02127563
Iteration 88, loss = 0.02118181
Iteration 89, loss = 0.02128838
Iteration 90, loss = 0.02128405
Iteration 91, loss = 0.02135403
Iteration 92, loss = 0.02135310
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	82m15.774s
user	357m34.627s
sys	36m30.432s
Elapsed time: 4936 seconds
