Processing dataset: kept_v6_custom_stopwords

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: kept_v6_custom_stopwords (7439 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on kept_v6_custom_stopwords...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on kept_v6_custom_stopwords...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on kept_v6_custom_stopwords...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on kept_v6_custom_stopwords...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on kept_v6_custom_stopwords...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67365024
Iteration 2, loss = 0.59990255
Iteration 3, loss = 0.49199980
Iteration 4, loss = 0.38205819
Iteration 5, loss = 0.28841101
Iteration 6, loss = 0.21765874
Iteration 7, loss = 0.16791579
Iteration 8, loss = 0.13313768
Iteration 9, loss = 0.10898573
Iteration 10, loss = 0.09144246
Iteration 11, loss = 0.07872061
Iteration 12, loss = 0.06919082
Iteration 13, loss = 0.06182795
Iteration 14, loss = 0.05598552
Iteration 15, loss = 0.05149724
Iteration 16, loss = 0.04789926
Iteration 17, loss = 0.04457133
Iteration 18, loss = 0.04220645
Iteration 19, loss = 0.04005639
Iteration 20, loss = 0.03821465
Iteration 21, loss = 0.03641288
Iteration 22, loss = 0.03507827
Iteration 23, loss = 0.03410452
Iteration 24, loss = 0.03291806
Iteration 25, loss = 0.03202874
Iteration 26, loss = 0.03115280
Iteration 27, loss = 0.03057729
Iteration 28, loss = 0.02975953
Iteration 29, loss = 0.02938798
Iteration 30, loss = 0.02905495
Iteration 31, loss = 0.02833664
Iteration 32, loss = 0.02786947
Iteration 33, loss = 0.02760450
Iteration 34, loss = 0.02751344
Iteration 35, loss = 0.02672244
Iteration 36, loss = 0.02657955
Iteration 37, loss = 0.02641586
Iteration 38, loss = 0.02610922
Iteration 39, loss = 0.02603443
Iteration 40, loss = 0.02577796
Iteration 41, loss = 0.02585449
Iteration 42, loss = 0.02536320
Iteration 43, loss = 0.02525305
Iteration 44, loss = 0.02506438
Iteration 45, loss = 0.02520973
Iteration 46, loss = 0.02469776
Iteration 47, loss = 0.02464457
Iteration 48, loss = 0.02459150
Iteration 49, loss = 0.02434409
Iteration 50, loss = 0.02449750
Iteration 51, loss = 0.02443067
Iteration 52, loss = 0.02471988
Iteration 53, loss = 0.02404979
Iteration 54, loss = 0.02420145
Iteration 55, loss = 0.02403625
Iteration 56, loss = 0.02406362
Iteration 57, loss = 0.02377528
Iteration 58, loss = 0.02368943
Iteration 59, loss = 0.02372388
Iteration 60, loss = 0.02375806
Iteration 61, loss = 0.02352728
Iteration 62, loss = 0.02370359
Iteration 63, loss = 0.02344515
Iteration 64, loss = 0.02359075
Iteration 65, loss = 0.02343590
Iteration 66, loss = 0.02353259
Iteration 67, loss = 0.02309859
Iteration 68, loss = 0.02321855
Iteration 69, loss = 0.02332428
Iteration 70, loss = 0.02300099
Iteration 71, loss = 0.02322713
Iteration 72, loss = 0.02339231
Iteration 73, loss = 0.02316973
Iteration 74, loss = 0.02308809
Iteration 75, loss = 0.02311747
Iteration 76, loss = 0.02280633
Iteration 77, loss = 0.02312473
Iteration 78, loss = 0.02277606
Iteration 79, loss = 0.02292376
Iteration 80, loss = 0.02287892
Iteration 81, loss = 0.02301607
Iteration 82, loss = 0.02279723
Iteration 83, loss = 0.02306280
Iteration 84, loss = 0.02272052
Iteration 85, loss = 0.02264217
Iteration 86, loss = 0.02306691
Iteration 87, loss = 0.02286191
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67580184
Iteration 2, loss = 0.60619182
Iteration 3, loss = 0.49624507
Iteration 4, loss = 0.38400566
Iteration 5, loss = 0.28695982
Iteration 6, loss = 0.21472718
Iteration 7, loss = 0.16434656
Iteration 8, loss = 0.13045095
Iteration 9, loss = 0.10663661
Iteration 10, loss = 0.08998819
Iteration 11, loss = 0.07746315
Iteration 12, loss = 0.06797314
Iteration 13, loss = 0.06081623
Iteration 14, loss = 0.05549487
Iteration 15, loss = 0.05089720
Iteration 16, loss = 0.04721528
Iteration 17, loss = 0.04428554
Iteration 18, loss = 0.04170708
Iteration 19, loss = 0.03969933
Iteration 20, loss = 0.03778599
Iteration 21, loss = 0.03625493
Iteration 22, loss = 0.03462598
Iteration 23, loss = 0.03367368
Iteration 24, loss = 0.03281658
Iteration 25, loss = 0.03178577
Iteration 26, loss = 0.03109160
Iteration 27, loss = 0.03048553
Iteration 28, loss = 0.02974370
Iteration 29, loss = 0.02941036
Iteration 30, loss = 0.02835349
Iteration 31, loss = 0.02817581
Iteration 32, loss = 0.02769208
Iteration 33, loss = 0.02737383
Iteration 34, loss = 0.02723817
Iteration 35, loss = 0.02685937
Iteration 36, loss = 0.02655364
Iteration 37, loss = 0.02609743
Iteration 38, loss = 0.02605834
Iteration 39, loss = 0.02576466
Iteration 40, loss = 0.02551642
Iteration 41, loss = 0.02546886
Iteration 42, loss = 0.02496836
Iteration 43, loss = 0.02511235
Iteration 44, loss = 0.02502158
Iteration 45, loss = 0.02476761
Iteration 46, loss = 0.02437988
Iteration 47, loss = 0.02450066
Iteration 48, loss = 0.02443353
Iteration 49, loss = 0.02432496
Iteration 50, loss = 0.02392915
Iteration 51, loss = 0.02415951
Iteration 52, loss = 0.02413209
Iteration 53, loss = 0.02385375
Iteration 54, loss = 0.02405052
Iteration 55, loss = 0.02384501
Iteration 56, loss = 0.02379366
Iteration 57, loss = 0.02377612
Iteration 58, loss = 0.02351498
Iteration 59, loss = 0.02336399
Iteration 60, loss = 0.02347713
Iteration 61, loss = 0.02334925
Iteration 62, loss = 0.02347166
Iteration 63, loss = 0.02323538
Iteration 64, loss = 0.02334075
Iteration 65, loss = 0.02318895
Iteration 66, loss = 0.02319017
Iteration 67, loss = 0.02286825
Iteration 68, loss = 0.02297413
Iteration 69, loss = 0.02300215
Iteration 70, loss = 0.02315709
Iteration 71, loss = 0.02266768
Iteration 72, loss = 0.02302611
Iteration 73, loss = 0.02287704
Iteration 74, loss = 0.02270902
Iteration 75, loss = 0.02313721
Iteration 76, loss = 0.02277688
Iteration 77, loss = 0.02290482
Iteration 78, loss = 0.02300377
Iteration 79, loss = 0.02272191
Iteration 80, loss = 0.02298141
Iteration 81, loss = 0.02266350
Iteration 82, loss = 0.02274019
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69563314
Iteration 2, loss = 0.62856076
Iteration 3, loss = 0.53136544
Iteration 4, loss = 0.41765940
Iteration 5, loss = 0.31572223
Iteration 6, loss = 0.23702839
Iteration 7, loss = 0.18051519
Iteration 8, loss = 0.14174777
Iteration 9, loss = 0.11486428
Iteration 10, loss = 0.09560317
Iteration 11, loss = 0.08179181
Iteration 12, loss = 0.07157631
Iteration 13, loss = 0.06357636
Iteration 14, loss = 0.05751536
Iteration 15, loss = 0.05249250
Iteration 16, loss = 0.04846407
Iteration 17, loss = 0.04530653
Iteration 18, loss = 0.04236552
Iteration 19, loss = 0.04014594
Iteration 20, loss = 0.03848306
Iteration 21, loss = 0.03679240
Iteration 22, loss = 0.03524557
Iteration 23, loss = 0.03415916
Iteration 24, loss = 0.03304402
Iteration 25, loss = 0.03209571
Iteration 26, loss = 0.03134436
Iteration 27, loss = 0.03099686
Iteration 28, loss = 0.03020830
Iteration 29, loss = 0.02952934
Iteration 30, loss = 0.02908326
Iteration 31, loss = 0.02896663
Iteration 32, loss = 0.02797576
Iteration 33, loss = 0.02768014
Iteration 34, loss = 0.02741348
Iteration 35, loss = 0.02715874
Iteration 36, loss = 0.02666437
Iteration 37, loss = 0.02639403
Iteration 38, loss = 0.02615221
Iteration 39, loss = 0.02591644
Iteration 40, loss = 0.02605896
Iteration 41, loss = 0.02577966
Iteration 42, loss = 0.02555538
Iteration 43, loss = 0.02535023
Iteration 44, loss = 0.02504804
Iteration 45, loss = 0.02515665
Iteration 46, loss = 0.02501545
Iteration 47, loss = 0.02464354
Iteration 48, loss = 0.02457044
Iteration 49, loss = 0.02460456
Iteration 50, loss = 0.02445194
Iteration 51, loss = 0.02469135
Iteration 52, loss = 0.02427289
Iteration 53, loss = 0.02434779
Iteration 54, loss = 0.02427284
Iteration 55, loss = 0.02412955
Iteration 56, loss = 0.02421596
Iteration 57, loss = 0.02404759
Iteration 58, loss = 0.02410333
Iteration 59, loss = 0.02380039
Iteration 60, loss = 0.02427984
Iteration 61, loss = 0.02376782
Iteration 62, loss = 0.02355904
Iteration 63, loss = 0.02351356
Iteration 64, loss = 0.02367178
Iteration 65, loss = 0.02374643
Iteration 66, loss = 0.02327902
Iteration 67, loss = 0.02350087
Iteration 68, loss = 0.02370862
Iteration 69, loss = 0.02337938
Iteration 70, loss = 0.02370522
Iteration 71, loss = 0.02345929
Iteration 72, loss = 0.02361781
Iteration 73, loss = 0.02330414
Iteration 74, loss = 0.02337234
Iteration 75, loss = 0.02321135
Iteration 76, loss = 0.02312435
Iteration 77, loss = 0.02304482
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67308215
Iteration 2, loss = 0.59661885
Iteration 3, loss = 0.48781529
Iteration 4, loss = 0.37915795
Iteration 5, loss = 0.28668749
Iteration 6, loss = 0.21648420
Iteration 7, loss = 0.16686423
Iteration 8, loss = 0.13270902
Iteration 9, loss = 0.10888796
Iteration 10, loss = 0.09197524
Iteration 11, loss = 0.07922514
Iteration 12, loss = 0.07000198
Iteration 13, loss = 0.06283441
Iteration 14, loss = 0.05711691
Iteration 15, loss = 0.05239958
Iteration 16, loss = 0.04891116
Iteration 17, loss = 0.04595908
Iteration 18, loss = 0.04337115
Iteration 19, loss = 0.04125761
Iteration 20, loss = 0.03957280
Iteration 21, loss = 0.03816921
Iteration 22, loss = 0.03668210
Iteration 23, loss = 0.03550281
Iteration 24, loss = 0.03457885
Iteration 25, loss = 0.03355321
Iteration 26, loss = 0.03290884
Iteration 27, loss = 0.03234316
Iteration 28, loss = 0.03159589
Iteration 29, loss = 0.03130047
Iteration 30, loss = 0.03064851
Iteration 31, loss = 0.03033053
Iteration 32, loss = 0.02964386
Iteration 33, loss = 0.02932399
Iteration 34, loss = 0.02904651
Iteration 35, loss = 0.02882672
Iteration 36, loss = 0.02839627
Iteration 37, loss = 0.02822798
Iteration 38, loss = 0.02808664
Iteration 39, loss = 0.02768208
Iteration 40, loss = 0.02745837
Iteration 41, loss = 0.02746109
Iteration 42, loss = 0.02725362
Iteration 43, loss = 0.02704505
Iteration 44, loss = 0.02683057
Iteration 45, loss = 0.02673902
Iteration 46, loss = 0.02662508
Iteration 47, loss = 0.02650236
Iteration 48, loss = 0.02649717
Iteration 49, loss = 0.02618739
Iteration 50, loss = 0.02631201
Iteration 51, loss = 0.02626461
Iteration 52, loss = 0.02587691
Iteration 53, loss = 0.02619407
Iteration 54, loss = 0.02610511
Iteration 55, loss = 0.02567792
Iteration 56, loss = 0.02575410
Iteration 57, loss = 0.02558740
Iteration 58, loss = 0.02578319
Iteration 59, loss = 0.02564616
Iteration 60, loss = 0.02534401
Iteration 61, loss = 0.02563520
Iteration 62, loss = 0.02540883
Iteration 63, loss = 0.02537402
Iteration 64, loss = 0.02542712
Iteration 65, loss = 0.02513786
Iteration 66, loss = 0.02572983
Iteration 67, loss = 0.02547889
Iteration 68, loss = 0.02530941
Iteration 69, loss = 0.02534718
Iteration 70, loss = 0.02523994
Iteration 71, loss = 0.02535469
Iteration 72, loss = 0.02482969
Iteration 73, loss = 0.02510641
Iteration 74, loss = 0.02511557
Iteration 75, loss = 0.02494471
Iteration 76, loss = 0.02505043
Iteration 77, loss = 0.02501555
Iteration 78, loss = 0.02540326
Iteration 79, loss = 0.02482830
Iteration 80, loss = 0.02530610
Iteration 81, loss = 0.02507091
Iteration 82, loss = 0.02490830
Iteration 83, loss = 0.02466274
Iteration 84, loss = 0.02511904
Iteration 85, loss = 0.02467252
Iteration 86, loss = 0.02489135
Iteration 87, loss = 0.02492847
Iteration 88, loss = 0.02479323
Iteration 89, loss = 0.02519255
Iteration 90, loss = 0.02483059
Iteration 91, loss = 0.02469224
Iteration 92, loss = 0.02448404
Iteration 93, loss = 0.02498155
Iteration 94, loss = 0.02443785
Iteration 95, loss = 0.02484305
Iteration 96, loss = 0.02491473
Iteration 97, loss = 0.02478496
Iteration 98, loss = 0.02456720
Iteration 99, loss = 0.02456319
Iteration 100, loss = 0.02451380
Iteration 101, loss = 0.02452059
Iteration 102, loss = 0.02474999
Iteration 103, loss = 0.02464522
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68808552
Iteration 2, loss = 0.61761625
Iteration 3, loss = 0.50925323
Iteration 4, loss = 0.39377220
Iteration 5, loss = 0.29648849
Iteration 6, loss = 0.22403361
Iteration 7, loss = 0.17249683
Iteration 8, loss = 0.13721542
Iteration 9, loss = 0.11256156
Iteration 10, loss = 0.09501476
Iteration 11, loss = 0.08203661
Iteration 12, loss = 0.07234805
Iteration 13, loss = 0.06500566
Iteration 14, loss = 0.05894125
Iteration 15, loss = 0.05436477
Iteration 16, loss = 0.05049896
Iteration 17, loss = 0.04752727
Iteration 18, loss = 0.04478444
Iteration 19, loss = 0.04293641
Iteration 20, loss = 0.04113422
Iteration 21, loss = 0.03941838
Iteration 22, loss = 0.03799005
Iteration 23, loss = 0.03681701
Iteration 24, loss = 0.03587427
Iteration 25, loss = 0.03504153
Iteration 26, loss = 0.03416748
Iteration 27, loss = 0.03376722
Iteration 28, loss = 0.03288025
Iteration 29, loss = 0.03233141
Iteration 30, loss = 0.03191925
Iteration 31, loss = 0.03145064
Iteration 32, loss = 0.03075597
Iteration 33, loss = 0.03043922
Iteration 34, loss = 0.03024033
Iteration 35, loss = 0.02988007
Iteration 36, loss = 0.02984744
Iteration 37, loss = 0.02939472
Iteration 38, loss = 0.02918146
Iteration 39, loss = 0.02895532
Iteration 40, loss = 0.02878112
Iteration 41, loss = 0.02854801
Iteration 42, loss = 0.02869775
Iteration 43, loss = 0.02820066
Iteration 44, loss = 0.02793313
Iteration 45, loss = 0.02791547
Iteration 46, loss = 0.02776993
Iteration 47, loss = 0.02758601
Iteration 48, loss = 0.02750300
Iteration 49, loss = 0.02756924
Iteration 50, loss = 0.02747593
Iteration 51, loss = 0.02723715
Iteration 52, loss = 0.02726546
Iteration 53, loss = 0.02711224
Iteration 54, loss = 0.02737658
Iteration 55, loss = 0.02710708
Iteration 56, loss = 0.02722077
Iteration 57, loss = 0.02681753
Iteration 58, loss = 0.02679832
Iteration 59, loss = 0.02684005
Iteration 60, loss = 0.02659850
Iteration 61, loss = 0.02651633
Iteration 62, loss = 0.02677442
Iteration 63, loss = 0.02637375
Iteration 64, loss = 0.02639313
Iteration 65, loss = 0.02634108
Iteration 66, loss = 0.02648311
Iteration 67, loss = 0.02621979
Iteration 68, loss = 0.02641291
Iteration 69, loss = 0.02642552
Iteration 70, loss = 0.02604422
Iteration 71, loss = 0.02630883
Iteration 72, loss = 0.02652846
Iteration 73, loss = 0.02617636
Iteration 74, loss = 0.02610725
Iteration 75, loss = 0.02627016
Iteration 76, loss = 0.02624673
Iteration 77, loss = 0.02609201
Iteration 78, loss = 0.02620254
Iteration 79, loss = 0.02621694
Iteration 80, loss = 0.02623426
Iteration 81, loss = 0.02564267
Iteration 82, loss = 0.02613239
Iteration 83, loss = 0.02592378
Iteration 84, loss = 0.02594531
Iteration 85, loss = 0.02594705
Iteration 86, loss = 0.02564628
Iteration 87, loss = 0.02590831
Iteration 88, loss = 0.02561980
Iteration 89, loss = 0.02571932
Iteration 90, loss = 0.02587378
Iteration 91, loss = 0.02586583
Iteration 92, loss = 0.02591250
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67365024
Iteration 2, loss = 0.59990255
Iteration 3, loss = 0.49199980
Iteration 4, loss = 0.38205819
Iteration 5, loss = 0.28841101
Iteration 6, loss = 0.21765874
Iteration 7, loss = 0.16791579
Iteration 8, loss = 0.13313768
Iteration 9, loss = 0.10898573
Iteration 10, loss = 0.09144246
Iteration 11, loss = 0.07872061
Iteration 12, loss = 0.06919082
Iteration 13, loss = 0.06182795
Iteration 14, loss = 0.05598552
Iteration 15, loss = 0.05149724
Iteration 16, loss = 0.04789926
Iteration 17, loss = 0.04457133
Iteration 18, loss = 0.04220645
Iteration 19, loss = 0.04005639
Iteration 20, loss = 0.03821465
Iteration 21, loss = 0.03641288
Iteration 22, loss = 0.03507827
Iteration 23, loss = 0.03410452
Iteration 24, loss = 0.03291806
Iteration 25, loss = 0.03202874
Iteration 26, loss = 0.03115280
Iteration 27, loss = 0.03057729
Iteration 28, loss = 0.02975953
Iteration 29, loss = 0.02938798
Iteration 30, loss = 0.02905495
Iteration 31, loss = 0.02833664
Iteration 32, loss = 0.02786947
Iteration 33, loss = 0.02760450
Iteration 34, loss = 0.02751344
Iteration 35, loss = 0.02672244
Iteration 36, loss = 0.02657955
Iteration 37, loss = 0.02641586
Iteration 38, loss = 0.02610922
Iteration 39, loss = 0.02603443
Iteration 40, loss = 0.02577796
Iteration 41, loss = 0.02585449
Iteration 42, loss = 0.02536320
Iteration 43, loss = 0.02525305
Iteration 44, loss = 0.02506438
Iteration 45, loss = 0.02520973
Iteration 46, loss = 0.02469776
Iteration 47, loss = 0.02464457
Iteration 48, loss = 0.02459150
Iteration 49, loss = 0.02434409
Iteration 50, loss = 0.02449750
Iteration 51, loss = 0.02443067
Iteration 52, loss = 0.02471988
Iteration 53, loss = 0.02404979
Iteration 54, loss = 0.02420145
Iteration 55, loss = 0.02403625
Iteration 56, loss = 0.02406362
Iteration 57, loss = 0.02377528
Iteration 58, loss = 0.02368943
Iteration 59, loss = 0.02372388
Iteration 60, loss = 0.02375806
Iteration 61, loss = 0.02352728
Iteration 62, loss = 0.02370359
Iteration 63, loss = 0.02344515
Iteration 64, loss = 0.02359075
Iteration 65, loss = 0.02343590
Iteration 66, loss = 0.02353259
Iteration 67, loss = 0.02309859
Iteration 68, loss = 0.02321855
Iteration 69, loss = 0.02332428
Iteration 70, loss = 0.02300099
Iteration 71, loss = 0.02322713
Iteration 72, loss = 0.02339231
Iteration 73, loss = 0.02316973
Iteration 74, loss = 0.02308809
Iteration 75, loss = 0.02311747
Iteration 76, loss = 0.02280633
Iteration 77, loss = 0.02312473
Iteration 78, loss = 0.02277606
Iteration 79, loss = 0.02292376
Iteration 80, loss = 0.02287892
Iteration 81, loss = 0.02301607
Iteration 82, loss = 0.02279723
Iteration 83, loss = 0.02306280
Iteration 84, loss = 0.02272052
Iteration 85, loss = 0.02264217
Iteration 86, loss = 0.02306691
Iteration 87, loss = 0.02286191
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67580184
Iteration 2, loss = 0.60619182
Iteration 3, loss = 0.49624507
Iteration 4, loss = 0.38400566
Iteration 5, loss = 0.28695982
Iteration 6, loss = 0.21472718
Iteration 7, loss = 0.16434656
Iteration 8, loss = 0.13045095
Iteration 9, loss = 0.10663661
Iteration 10, loss = 0.08998819
Iteration 11, loss = 0.07746315
Iteration 12, loss = 0.06797314
Iteration 13, loss = 0.06081623
Iteration 14, loss = 0.05549487
Iteration 15, loss = 0.05089720
Iteration 16, loss = 0.04721528
Iteration 17, loss = 0.04428554
Iteration 18, loss = 0.04170708
Iteration 19, loss = 0.03969933
Iteration 20, loss = 0.03778599
Iteration 21, loss = 0.03625493
Iteration 22, loss = 0.03462598
Iteration 23, loss = 0.03367368
Iteration 24, loss = 0.03281658
Iteration 25, loss = 0.03178577
Iteration 26, loss = 0.03109160
Iteration 27, loss = 0.03048553
Iteration 28, loss = 0.02974370
Iteration 29, loss = 0.02941036
Iteration 30, loss = 0.02835349
Iteration 31, loss = 0.02817581
Iteration 32, loss = 0.02769208
Iteration 33, loss = 0.02737383
Iteration 34, loss = 0.02723817
Iteration 35, loss = 0.02685937
Iteration 36, loss = 0.02655364
Iteration 37, loss = 0.02609743
Iteration 38, loss = 0.02605834
Iteration 39, loss = 0.02576466
Iteration 40, loss = 0.02551642
Iteration 41, loss = 0.02546886
Iteration 42, loss = 0.02496836
Iteration 43, loss = 0.02511235
Iteration 44, loss = 0.02502158
Iteration 45, loss = 0.02476761
Iteration 46, loss = 0.02437988
Iteration 47, loss = 0.02450066
Iteration 48, loss = 0.02443353
Iteration 49, loss = 0.02432496
Iteration 50, loss = 0.02392915
Iteration 51, loss = 0.02415951
Iteration 52, loss = 0.02413209
Iteration 53, loss = 0.02385375
Iteration 54, loss = 0.02405052
Iteration 55, loss = 0.02384501
Iteration 56, loss = 0.02379366
Iteration 57, loss = 0.02377612
Iteration 58, loss = 0.02351498
Iteration 59, loss = 0.02336399
Iteration 60, loss = 0.02347713
Iteration 61, loss = 0.02334925
Iteration 62, loss = 0.02347166
Iteration 63, loss = 0.02323538
Iteration 64, loss = 0.02334075
Iteration 65, loss = 0.02318895
Iteration 66, loss = 0.02319017
Iteration 67, loss = 0.02286825
Iteration 68, loss = 0.02297413
Iteration 69, loss = 0.02300215
Iteration 70, loss = 0.02315709
Iteration 71, loss = 0.02266768
Iteration 72, loss = 0.02302611
Iteration 73, loss = 0.02287704
Iteration 74, loss = 0.02270902
Iteration 75, loss = 0.02313721
Iteration 76, loss = 0.02277688
Iteration 77, loss = 0.02290482
Iteration 78, loss = 0.02300377
Iteration 79, loss = 0.02272191
Iteration 80, loss = 0.02298141
Iteration 81, loss = 0.02266350
Iteration 82, loss = 0.02274019
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69563314
Iteration 2, loss = 0.62856076
Iteration 3, loss = 0.53136544
Iteration 4, loss = 0.41765940
Iteration 5, loss = 0.31572223
Iteration 6, loss = 0.23702839
Iteration 7, loss = 0.18051519
Iteration 8, loss = 0.14174777
Iteration 9, loss = 0.11486428
Iteration 10, loss = 0.09560317
Iteration 11, loss = 0.08179181
Iteration 12, loss = 0.07157631
Iteration 13, loss = 0.06357636
Iteration 14, loss = 0.05751536
Iteration 15, loss = 0.05249250
Iteration 16, loss = 0.04846407
Iteration 17, loss = 0.04530653
Iteration 18, loss = 0.04236552
Iteration 19, loss = 0.04014594
Iteration 20, loss = 0.03848306
Iteration 21, loss = 0.03679240
Iteration 22, loss = 0.03524557
Iteration 23, loss = 0.03415916
Iteration 24, loss = 0.03304402
Iteration 25, loss = 0.03209571
Iteration 26, loss = 0.03134436
Iteration 27, loss = 0.03099686
Iteration 28, loss = 0.03020830
Iteration 29, loss = 0.02952934
Iteration 30, loss = 0.02908326
Iteration 31, loss = 0.02896663
Iteration 32, loss = 0.02797576
Iteration 33, loss = 0.02768014
Iteration 34, loss = 0.02741348
Iteration 35, loss = 0.02715874
Iteration 36, loss = 0.02666437
Iteration 37, loss = 0.02639403
Iteration 38, loss = 0.02615221
Iteration 39, loss = 0.02591644
Iteration 40, loss = 0.02605896
Iteration 41, loss = 0.02577966
Iteration 42, loss = 0.02555538
Iteration 43, loss = 0.02535023
Iteration 44, loss = 0.02504804
Iteration 45, loss = 0.02515665
Iteration 46, loss = 0.02501545
Iteration 47, loss = 0.02464354
Iteration 48, loss = 0.02457044
Iteration 49, loss = 0.02460456
Iteration 50, loss = 0.02445194
Iteration 51, loss = 0.02469135
Iteration 52, loss = 0.02427289
Iteration 53, loss = 0.02434779
Iteration 54, loss = 0.02427284
Iteration 55, loss = 0.02412955
Iteration 56, loss = 0.02421596
Iteration 57, loss = 0.02404759
Iteration 58, loss = 0.02410333
Iteration 59, loss = 0.02380039
Iteration 60, loss = 0.02427984
Iteration 61, loss = 0.02376782
Iteration 62, loss = 0.02355904
Iteration 63, loss = 0.02351356
Iteration 64, loss = 0.02367178
Iteration 65, loss = 0.02374643
Iteration 66, loss = 0.02327902
Iteration 67, loss = 0.02350087
Iteration 68, loss = 0.02370862
Iteration 69, loss = 0.02337938
Iteration 70, loss = 0.02370522
Iteration 71, loss = 0.02345929
Iteration 72, loss = 0.02361781
Iteration 73, loss = 0.02330414
Iteration 74, loss = 0.02337234
Iteration 75, loss = 0.02321135
Iteration 76, loss = 0.02312435
Iteration 77, loss = 0.02304482
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67308215
Iteration 2, loss = 0.59661885
Iteration 3, loss = 0.48781529
Iteration 4, loss = 0.37915795
Iteration 5, loss = 0.28668749
Iteration 6, loss = 0.21648420
Iteration 7, loss = 0.16686423
Iteration 8, loss = 0.13270902
Iteration 9, loss = 0.10888796
Iteration 10, loss = 0.09197524
Iteration 11, loss = 0.07922514
Iteration 12, loss = 0.07000198
Iteration 13, loss = 0.06283441
Iteration 14, loss = 0.05711691
Iteration 15, loss = 0.05239958
Iteration 16, loss = 0.04891116
Iteration 17, loss = 0.04595908
Iteration 18, loss = 0.04337115
Iteration 19, loss = 0.04125761
Iteration 20, loss = 0.03957280
Iteration 21, loss = 0.03816921
Iteration 22, loss = 0.03668210
Iteration 23, loss = 0.03550281
Iteration 24, loss = 0.03457885
Iteration 25, loss = 0.03355321
Iteration 26, loss = 0.03290884
Iteration 27, loss = 0.03234316
Iteration 28, loss = 0.03159589
Iteration 29, loss = 0.03130047
Iteration 30, loss = 0.03064851
Iteration 31, loss = 0.03033053
Iteration 32, loss = 0.02964386
Iteration 33, loss = 0.02932399
Iteration 34, loss = 0.02904651
Iteration 35, loss = 0.02882672
Iteration 36, loss = 0.02839627
Iteration 37, loss = 0.02822798
Iteration 38, loss = 0.02808664
Iteration 39, loss = 0.02768208
Iteration 40, loss = 0.02745837
Iteration 41, loss = 0.02746109
Iteration 42, loss = 0.02725362
Iteration 43, loss = 0.02704505
Iteration 44, loss = 0.02683057
Iteration 45, loss = 0.02673902
Iteration 46, loss = 0.02662508
Iteration 47, loss = 0.02650236
Iteration 48, loss = 0.02649717
Iteration 49, loss = 0.02618739
Iteration 50, loss = 0.02631201
Iteration 51, loss = 0.02626461
Iteration 52, loss = 0.02587691
Iteration 53, loss = 0.02619407
Iteration 54, loss = 0.02610511
Iteration 55, loss = 0.02567792
Iteration 56, loss = 0.02575410
Iteration 57, loss = 0.02558740
Iteration 58, loss = 0.02578319
Iteration 59, loss = 0.02564616
Iteration 60, loss = 0.02534401
Iteration 61, loss = 0.02563520
Iteration 62, loss = 0.02540883
Iteration 63, loss = 0.02537402
Iteration 64, loss = 0.02542712
Iteration 65, loss = 0.02513786
Iteration 66, loss = 0.02572983
Iteration 67, loss = 0.02547889
Iteration 68, loss = 0.02530941
Iteration 69, loss = 0.02534718
Iteration 70, loss = 0.02523994
Iteration 71, loss = 0.02535469
Iteration 72, loss = 0.02482969
Iteration 73, loss = 0.02510641
Iteration 74, loss = 0.02511557
Iteration 75, loss = 0.02494471
Iteration 76, loss = 0.02505043
Iteration 77, loss = 0.02501555
Iteration 78, loss = 0.02540326
Iteration 79, loss = 0.02482830
Iteration 80, loss = 0.02530610
Iteration 81, loss = 0.02507091
Iteration 82, loss = 0.02490830
Iteration 83, loss = 0.02466274
Iteration 84, loss = 0.02511904
Iteration 85, loss = 0.02467252
Iteration 86, loss = 0.02489135
Iteration 87, loss = 0.02492847
Iteration 88, loss = 0.02479323
Iteration 89, loss = 0.02519255
Iteration 90, loss = 0.02483059
Iteration 91, loss = 0.02469224
Iteration 92, loss = 0.02448404
Iteration 93, loss = 0.02498155
Iteration 94, loss = 0.02443785
Iteration 95, loss = 0.02484305
Iteration 96, loss = 0.02491473
Iteration 97, loss = 0.02478496
Iteration 98, loss = 0.02456720
Iteration 99, loss = 0.02456319
Iteration 100, loss = 0.02451380
Iteration 101, loss = 0.02452059
Iteration 102, loss = 0.02474999
Iteration 103, loss = 0.02464522
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68808552
Iteration 2, loss = 0.61761625
Iteration 3, loss = 0.50925323
Iteration 4, loss = 0.39377220
Iteration 5, loss = 0.29648849
Iteration 6, loss = 0.22403361
Iteration 7, loss = 0.17249683
Iteration 8, loss = 0.13721542
Iteration 9, loss = 0.11256156
Iteration 10, loss = 0.09501476
Iteration 11, loss = 0.08203661
Iteration 12, loss = 0.07234805
Iteration 13, loss = 0.06500566
Iteration 14, loss = 0.05894125
Iteration 15, loss = 0.05436477
Iteration 16, loss = 0.05049896
Iteration 17, loss = 0.04752727
Iteration 18, loss = 0.04478444
Iteration 19, loss = 0.04293641
Iteration 20, loss = 0.04113422
Iteration 21, loss = 0.03941838
Iteration 22, loss = 0.03799005
Iteration 23, loss = 0.03681701
Iteration 24, loss = 0.03587427
Iteration 25, loss = 0.03504153
Iteration 26, loss = 0.03416748
Iteration 27, loss = 0.03376722
Iteration 28, loss = 0.03288025
Iteration 29, loss = 0.03233141
Iteration 30, loss = 0.03191925
Iteration 31, loss = 0.03145064
Iteration 32, loss = 0.03075597
Iteration 33, loss = 0.03043922
Iteration 34, loss = 0.03024033
Iteration 35, loss = 0.02988007
Iteration 36, loss = 0.02984744
Iteration 37, loss = 0.02939472
Iteration 38, loss = 0.02918146
Iteration 39, loss = 0.02895532
Iteration 40, loss = 0.02878112
Iteration 41, loss = 0.02854801
Iteration 42, loss = 0.02869775
Iteration 43, loss = 0.02820066
Iteration 44, loss = 0.02793313
Iteration 45, loss = 0.02791547
Iteration 46, loss = 0.02776993
Iteration 47, loss = 0.02758601
Iteration 48, loss = 0.02750300
Iteration 49, loss = 0.02756924
Iteration 50, loss = 0.02747593
Iteration 51, loss = 0.02723715
Iteration 52, loss = 0.02726546
Iteration 53, loss = 0.02711224
Iteration 54, loss = 0.02737658
Iteration 55, loss = 0.02710708
Iteration 56, loss = 0.02722077
Iteration 57, loss = 0.02681753
Iteration 58, loss = 0.02679832
Iteration 59, loss = 0.02684005
Iteration 60, loss = 0.02659850
Iteration 61, loss = 0.02651633
Iteration 62, loss = 0.02677442
Iteration 63, loss = 0.02637375
Iteration 64, loss = 0.02639313
Iteration 65, loss = 0.02634108
Iteration 66, loss = 0.02648311
Iteration 67, loss = 0.02621979
Iteration 68, loss = 0.02641291
Iteration 69, loss = 0.02642552
Iteration 70, loss = 0.02604422
Iteration 71, loss = 0.02630883
Iteration 72, loss = 0.02652846
Iteration 73, loss = 0.02617636
Iteration 74, loss = 0.02610725
Iteration 75, loss = 0.02627016
Iteration 76, loss = 0.02624673
Iteration 77, loss = 0.02609201
Iteration 78, loss = 0.02620254
Iteration 79, loss = 0.02621694
Iteration 80, loss = 0.02623426
Iteration 81, loss = 0.02564267
Iteration 82, loss = 0.02613239
Iteration 83, loss = 0.02592378
Iteration 84, loss = 0.02594531
Iteration 85, loss = 0.02594705
Iteration 86, loss = 0.02564628
Iteration 87, loss = 0.02590831
Iteration 88, loss = 0.02561980
Iteration 89, loss = 0.02571932
Iteration 90, loss = 0.02587378
Iteration 91, loss = 0.02586583
Iteration 92, loss = 0.02591250
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67365024
Iteration 2, loss = 0.59990255
Iteration 3, loss = 0.49199980
Iteration 4, loss = 0.38205819
Iteration 5, loss = 0.28841101
Iteration 6, loss = 0.21765874
Iteration 7, loss = 0.16791579
Iteration 8, loss = 0.13313768
Iteration 9, loss = 0.10898573
Iteration 10, loss = 0.09144246
Iteration 11, loss = 0.07872061
Iteration 12, loss = 0.06919082
Iteration 13, loss = 0.06182795
Iteration 14, loss = 0.05598552
Iteration 15, loss = 0.05149724
Iteration 16, loss = 0.04789926
Iteration 17, loss = 0.04457133
Iteration 18, loss = 0.04220645
Iteration 19, loss = 0.04005639
Iteration 20, loss = 0.03821465
Iteration 21, loss = 0.03641288
Iteration 22, loss = 0.03507827
Iteration 23, loss = 0.03410452
Iteration 24, loss = 0.03291806
Iteration 25, loss = 0.03202874
Iteration 26, loss = 0.03115280
Iteration 27, loss = 0.03057729
Iteration 28, loss = 0.02975953
Iteration 29, loss = 0.02938798
Iteration 30, loss = 0.02905495
Iteration 31, loss = 0.02833664
Iteration 32, loss = 0.02786947
Iteration 33, loss = 0.02760450
Iteration 34, loss = 0.02751344
Iteration 35, loss = 0.02672244
Iteration 36, loss = 0.02657955
Iteration 37, loss = 0.02641586
Iteration 38, loss = 0.02610922
Iteration 39, loss = 0.02603443
Iteration 40, loss = 0.02577796
Iteration 41, loss = 0.02585449
Iteration 42, loss = 0.02536320
Iteration 43, loss = 0.02525305
Iteration 44, loss = 0.02506438
Iteration 45, loss = 0.02520973
Iteration 46, loss = 0.02469776
Iteration 47, loss = 0.02464457
Iteration 48, loss = 0.02459150
Iteration 49, loss = 0.02434409
Iteration 50, loss = 0.02449750
Iteration 51, loss = 0.02443067
Iteration 52, loss = 0.02471988
Iteration 53, loss = 0.02404979
Iteration 54, loss = 0.02420145
Iteration 55, loss = 0.02403625
Iteration 56, loss = 0.02406362
Iteration 57, loss = 0.02377528
Iteration 58, loss = 0.02368943
Iteration 59, loss = 0.02372388
Iteration 60, loss = 0.02375806
Iteration 61, loss = 0.02352728
Iteration 62, loss = 0.02370359
Iteration 63, loss = 0.02344515
Iteration 64, loss = 0.02359075
Iteration 65, loss = 0.02343590
Iteration 66, loss = 0.02353259
Iteration 67, loss = 0.02309859
Iteration 68, loss = 0.02321855
Iteration 69, loss = 0.02332428
Iteration 70, loss = 0.02300099
Iteration 71, loss = 0.02322713
Iteration 72, loss = 0.02339231
Iteration 73, loss = 0.02316973
Iteration 74, loss = 0.02308809
Iteration 75, loss = 0.02311747
Iteration 76, loss = 0.02280633
Iteration 77, loss = 0.02312473
Iteration 78, loss = 0.02277606
Iteration 79, loss = 0.02292376
Iteration 80, loss = 0.02287892
Iteration 81, loss = 0.02301607
Iteration 82, loss = 0.02279723
Iteration 83, loss = 0.02306280
Iteration 84, loss = 0.02272052
Iteration 85, loss = 0.02264217
Iteration 86, loss = 0.02306691
Iteration 87, loss = 0.02286191
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67580184
Iteration 2, loss = 0.60619182
Iteration 3, loss = 0.49624507
Iteration 4, loss = 0.38400566
Iteration 5, loss = 0.28695982
Iteration 6, loss = 0.21472718
Iteration 7, loss = 0.16434656
Iteration 8, loss = 0.13045095
Iteration 9, loss = 0.10663661
Iteration 10, loss = 0.08998819
Iteration 11, loss = 0.07746315
Iteration 12, loss = 0.06797314
Iteration 13, loss = 0.06081623
Iteration 14, loss = 0.05549487
Iteration 15, loss = 0.05089720
Iteration 16, loss = 0.04721528
Iteration 17, loss = 0.04428554
Iteration 18, loss = 0.04170708
Iteration 19, loss = 0.03969933
Iteration 20, loss = 0.03778599
Iteration 21, loss = 0.03625493
Iteration 22, loss = 0.03462598
Iteration 23, loss = 0.03367368
Iteration 24, loss = 0.03281658
Iteration 25, loss = 0.03178577
Iteration 26, loss = 0.03109160
Iteration 27, loss = 0.03048553
Iteration 28, loss = 0.02974370
Iteration 29, loss = 0.02941036
Iteration 30, loss = 0.02835349
Iteration 31, loss = 0.02817581
Iteration 32, loss = 0.02769208
Iteration 33, loss = 0.02737383
Iteration 34, loss = 0.02723817
Iteration 35, loss = 0.02685937
Iteration 36, loss = 0.02655364
Iteration 37, loss = 0.02609743
Iteration 38, loss = 0.02605834
Iteration 39, loss = 0.02576466
Iteration 40, loss = 0.02551642
Iteration 41, loss = 0.02546886
Iteration 42, loss = 0.02496836
Iteration 43, loss = 0.02511235
Iteration 44, loss = 0.02502158
Iteration 45, loss = 0.02476761
Iteration 46, loss = 0.02437988
Iteration 47, loss = 0.02450066
Iteration 48, loss = 0.02443353
Iteration 49, loss = 0.02432496
Iteration 50, loss = 0.02392915
Iteration 51, loss = 0.02415951
Iteration 52, loss = 0.02413209
Iteration 53, loss = 0.02385375
Iteration 54, loss = 0.02405052
Iteration 55, loss = 0.02384501
Iteration 56, loss = 0.02379366
Iteration 57, loss = 0.02377612
Iteration 58, loss = 0.02351498
Iteration 59, loss = 0.02336399
Iteration 60, loss = 0.02347713
Iteration 61, loss = 0.02334925
Iteration 62, loss = 0.02347166
Iteration 63, loss = 0.02323538
Iteration 64, loss = 0.02334075
Iteration 65, loss = 0.02318895
Iteration 66, loss = 0.02319017
Iteration 67, loss = 0.02286825
Iteration 68, loss = 0.02297413
Iteration 69, loss = 0.02300215
Iteration 70, loss = 0.02315709
Iteration 71, loss = 0.02266768
Iteration 72, loss = 0.02302611
Iteration 73, loss = 0.02287704
Iteration 74, loss = 0.02270902
Iteration 75, loss = 0.02313721
Iteration 76, loss = 0.02277688
Iteration 77, loss = 0.02290482
Iteration 78, loss = 0.02300377
Iteration 79, loss = 0.02272191
Iteration 80, loss = 0.02298141
Iteration 81, loss = 0.02266350
Iteration 82, loss = 0.02274019
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69563314
Iteration 2, loss = 0.62856076
Iteration 3, loss = 0.53136544
Iteration 4, loss = 0.41765940
Iteration 5, loss = 0.31572223
Iteration 6, loss = 0.23702839
Iteration 7, loss = 0.18051519
Iteration 8, loss = 0.14174777
Iteration 9, loss = 0.11486428
Iteration 10, loss = 0.09560317
Iteration 11, loss = 0.08179181
Iteration 12, loss = 0.07157631
Iteration 13, loss = 0.06357636
Iteration 14, loss = 0.05751536
Iteration 15, loss = 0.05249250
Iteration 16, loss = 0.04846407
Iteration 17, loss = 0.04530653
Iteration 18, loss = 0.04236552
Iteration 19, loss = 0.04014594
Iteration 20, loss = 0.03848306
Iteration 21, loss = 0.03679240
Iteration 22, loss = 0.03524557
Iteration 23, loss = 0.03415916
Iteration 24, loss = 0.03304402
Iteration 25, loss = 0.03209571
Iteration 26, loss = 0.03134436
Iteration 27, loss = 0.03099686
Iteration 28, loss = 0.03020830
Iteration 29, loss = 0.02952934
Iteration 30, loss = 0.02908326
Iteration 31, loss = 0.02896663
Iteration 32, loss = 0.02797576
Iteration 33, loss = 0.02768014
Iteration 34, loss = 0.02741348
Iteration 35, loss = 0.02715874
Iteration 36, loss = 0.02666437
Iteration 37, loss = 0.02639403
Iteration 38, loss = 0.02615221
Iteration 39, loss = 0.02591644
Iteration 40, loss = 0.02605896
Iteration 41, loss = 0.02577966
Iteration 42, loss = 0.02555538
Iteration 43, loss = 0.02535023
Iteration 44, loss = 0.02504804
Iteration 45, loss = 0.02515665
Iteration 46, loss = 0.02501545
Iteration 47, loss = 0.02464354
Iteration 48, loss = 0.02457044
Iteration 49, loss = 0.02460456
Iteration 50, loss = 0.02445194
Iteration 51, loss = 0.02469135
Iteration 52, loss = 0.02427289
Iteration 53, loss = 0.02434779
Iteration 54, loss = 0.02427284
Iteration 55, loss = 0.02412955
Iteration 56, loss = 0.02421596
Iteration 57, loss = 0.02404759
Iteration 58, loss = 0.02410333
Iteration 59, loss = 0.02380039
Iteration 60, loss = 0.02427984
Iteration 61, loss = 0.02376782
Iteration 62, loss = 0.02355904
Iteration 63, loss = 0.02351356
Iteration 64, loss = 0.02367178
Iteration 65, loss = 0.02374643
Iteration 66, loss = 0.02327902
Iteration 67, loss = 0.02350087
Iteration 68, loss = 0.02370862
Iteration 69, loss = 0.02337938
Iteration 70, loss = 0.02370522
Iteration 71, loss = 0.02345929
Iteration 72, loss = 0.02361781
Iteration 73, loss = 0.02330414
Iteration 74, loss = 0.02337234
Iteration 75, loss = 0.02321135
Iteration 76, loss = 0.02312435
Iteration 77, loss = 0.02304482
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67308215
Iteration 2, loss = 0.59661885
Iteration 3, loss = 0.48781529
Iteration 4, loss = 0.37915795
Iteration 5, loss = 0.28668749
Iteration 6, loss = 0.21648420
Iteration 7, loss = 0.16686423
Iteration 8, loss = 0.13270902
Iteration 9, loss = 0.10888796
Iteration 10, loss = 0.09197524
Iteration 11, loss = 0.07922514
Iteration 12, loss = 0.07000198
Iteration 13, loss = 0.06283441
Iteration 14, loss = 0.05711691
Iteration 15, loss = 0.05239958
Iteration 16, loss = 0.04891116
Iteration 17, loss = 0.04595908
Iteration 18, loss = 0.04337115
Iteration 19, loss = 0.04125761
Iteration 20, loss = 0.03957280
Iteration 21, loss = 0.03816921
Iteration 22, loss = 0.03668210
Iteration 23, loss = 0.03550281
Iteration 24, loss = 0.03457885
Iteration 25, loss = 0.03355321
Iteration 26, loss = 0.03290884
Iteration 27, loss = 0.03234316
Iteration 28, loss = 0.03159589
Iteration 29, loss = 0.03130047
Iteration 30, loss = 0.03064851
Iteration 31, loss = 0.03033053
Iteration 32, loss = 0.02964386
Iteration 33, loss = 0.02932399
Iteration 34, loss = 0.02904651
Iteration 35, loss = 0.02882672
Iteration 36, loss = 0.02839627
Iteration 37, loss = 0.02822798
Iteration 38, loss = 0.02808664
Iteration 39, loss = 0.02768208
Iteration 40, loss = 0.02745837
Iteration 41, loss = 0.02746109
Iteration 42, loss = 0.02725362
Iteration 43, loss = 0.02704505
Iteration 44, loss = 0.02683057
Iteration 45, loss = 0.02673902
Iteration 46, loss = 0.02662508
Iteration 47, loss = 0.02650236
Iteration 48, loss = 0.02649717
Iteration 49, loss = 0.02618739
Iteration 50, loss = 0.02631201
Iteration 51, loss = 0.02626461
Iteration 52, loss = 0.02587691
Iteration 53, loss = 0.02619407
Iteration 54, loss = 0.02610511
Iteration 55, loss = 0.02567792
Iteration 56, loss = 0.02575410
Iteration 57, loss = 0.02558740
Iteration 58, loss = 0.02578319
Iteration 59, loss = 0.02564616
Iteration 60, loss = 0.02534401
Iteration 61, loss = 0.02563520
Iteration 62, loss = 0.02540883
Iteration 63, loss = 0.02537402
Iteration 64, loss = 0.02542712
Iteration 65, loss = 0.02513786
Iteration 66, loss = 0.02572983
Iteration 67, loss = 0.02547889
Iteration 68, loss = 0.02530941
Iteration 69, loss = 0.02534718
Iteration 70, loss = 0.02523994
Iteration 71, loss = 0.02535469
Iteration 72, loss = 0.02482969
Iteration 73, loss = 0.02510641
Iteration 74, loss = 0.02511557
Iteration 75, loss = 0.02494471
Iteration 76, loss = 0.02505043
Iteration 77, loss = 0.02501555
Iteration 78, loss = 0.02540326
Iteration 79, loss = 0.02482830
Iteration 80, loss = 0.02530610
Iteration 81, loss = 0.02507091
Iteration 82, loss = 0.02490830
Iteration 83, loss = 0.02466274
Iteration 84, loss = 0.02511904
Iteration 85, loss = 0.02467252
Iteration 86, loss = 0.02489135
Iteration 87, loss = 0.02492847
Iteration 88, loss = 0.02479323
Iteration 89, loss = 0.02519255
Iteration 90, loss = 0.02483059
Iteration 91, loss = 0.02469224
Iteration 92, loss = 0.02448404
Iteration 93, loss = 0.02498155
Iteration 94, loss = 0.02443785
Iteration 95, loss = 0.02484305
Iteration 96, loss = 0.02491473
Iteration 97, loss = 0.02478496
Iteration 98, loss = 0.02456720
Iteration 99, loss = 0.02456319
Iteration 100, loss = 0.02451380
Iteration 101, loss = 0.02452059
Iteration 102, loss = 0.02474999
Iteration 103, loss = 0.02464522
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68808552
Iteration 2, loss = 0.61761625
Iteration 3, loss = 0.50925323
Iteration 4, loss = 0.39377220
Iteration 5, loss = 0.29648849
Iteration 6, loss = 0.22403361
Iteration 7, loss = 0.17249683
Iteration 8, loss = 0.13721542
Iteration 9, loss = 0.11256156
Iteration 10, loss = 0.09501476
Iteration 11, loss = 0.08203661
Iteration 12, loss = 0.07234805
Iteration 13, loss = 0.06500566
Iteration 14, loss = 0.05894125
Iteration 15, loss = 0.05436477
Iteration 16, loss = 0.05049896
Iteration 17, loss = 0.04752727
Iteration 18, loss = 0.04478444
Iteration 19, loss = 0.04293641
Iteration 20, loss = 0.04113422
Iteration 21, loss = 0.03941838
Iteration 22, loss = 0.03799005
Iteration 23, loss = 0.03681701
Iteration 24, loss = 0.03587427
Iteration 25, loss = 0.03504153
Iteration 26, loss = 0.03416748
Iteration 27, loss = 0.03376722
Iteration 28, loss = 0.03288025
Iteration 29, loss = 0.03233141
Iteration 30, loss = 0.03191925
Iteration 31, loss = 0.03145064
Iteration 32, loss = 0.03075597
Iteration 33, loss = 0.03043922
Iteration 34, loss = 0.03024033
Iteration 35, loss = 0.02988007
Iteration 36, loss = 0.02984744
Iteration 37, loss = 0.02939472
Iteration 38, loss = 0.02918146
Iteration 39, loss = 0.02895532
Iteration 40, loss = 0.02878112
Iteration 41, loss = 0.02854801
Iteration 42, loss = 0.02869775
Iteration 43, loss = 0.02820066
Iteration 44, loss = 0.02793313
Iteration 45, loss = 0.02791547
Iteration 46, loss = 0.02776993
Iteration 47, loss = 0.02758601
Iteration 48, loss = 0.02750300
Iteration 49, loss = 0.02756924
Iteration 50, loss = 0.02747593
Iteration 51, loss = 0.02723715
Iteration 52, loss = 0.02726546
Iteration 53, loss = 0.02711224
Iteration 54, loss = 0.02737658
Iteration 55, loss = 0.02710708
Iteration 56, loss = 0.02722077
Iteration 57, loss = 0.02681753
Iteration 58, loss = 0.02679832
Iteration 59, loss = 0.02684005
Iteration 60, loss = 0.02659850
Iteration 61, loss = 0.02651633
Iteration 62, loss = 0.02677442
Iteration 63, loss = 0.02637375
Iteration 64, loss = 0.02639313
Iteration 65, loss = 0.02634108
Iteration 66, loss = 0.02648311
Iteration 67, loss = 0.02621979
Iteration 68, loss = 0.02641291
Iteration 69, loss = 0.02642552
Iteration 70, loss = 0.02604422
Iteration 71, loss = 0.02630883
Iteration 72, loss = 0.02652846
Iteration 73, loss = 0.02617636
Iteration 74, loss = 0.02610725
Iteration 75, loss = 0.02627016
Iteration 76, loss = 0.02624673
Iteration 77, loss = 0.02609201
Iteration 78, loss = 0.02620254
Iteration 79, loss = 0.02621694
Iteration 80, loss = 0.02623426
Iteration 81, loss = 0.02564267
Iteration 82, loss = 0.02613239
Iteration 83, loss = 0.02592378
Iteration 84, loss = 0.02594531
Iteration 85, loss = 0.02594705
Iteration 86, loss = 0.02564628
Iteration 87, loss = 0.02590831
Iteration 88, loss = 0.02561980
Iteration 89, loss = 0.02571932
Iteration 90, loss = 0.02587378
Iteration 91, loss = 0.02586583
Iteration 92, loss = 0.02591250
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67365024
Iteration 2, loss = 0.59990255
Iteration 3, loss = 0.49199980
Iteration 4, loss = 0.38205819
Iteration 5, loss = 0.28841101
Iteration 6, loss = 0.21765874
Iteration 7, loss = 0.16791579
Iteration 8, loss = 0.13313768
Iteration 9, loss = 0.10898573
Iteration 10, loss = 0.09144246
Iteration 11, loss = 0.07872061
Iteration 12, loss = 0.06919082
Iteration 13, loss = 0.06182795
Iteration 14, loss = 0.05598552
Iteration 15, loss = 0.05149724
Iteration 16, loss = 0.04789926
Iteration 17, loss = 0.04457133
Iteration 18, loss = 0.04220645
Iteration 19, loss = 0.04005639
Iteration 20, loss = 0.03821465
Iteration 21, loss = 0.03641288
Iteration 22, loss = 0.03507827
Iteration 23, loss = 0.03410452
Iteration 24, loss = 0.03291806
Iteration 25, loss = 0.03202874
Iteration 26, loss = 0.03115280
Iteration 27, loss = 0.03057729
Iteration 28, loss = 0.02975953
Iteration 29, loss = 0.02938798
Iteration 30, loss = 0.02905495
Iteration 31, loss = 0.02833664
Iteration 32, loss = 0.02786947
Iteration 33, loss = 0.02760450
Iteration 34, loss = 0.02751344
Iteration 35, loss = 0.02672244
Iteration 36, loss = 0.02657955
Iteration 37, loss = 0.02641586
Iteration 38, loss = 0.02610922
Iteration 39, loss = 0.02603443
Iteration 40, loss = 0.02577796
Iteration 41, loss = 0.02585449
Iteration 42, loss = 0.02536320
Iteration 43, loss = 0.02525305
Iteration 44, loss = 0.02506438
Iteration 45, loss = 0.02520973
Iteration 46, loss = 0.02469776
Iteration 47, loss = 0.02464457
Iteration 48, loss = 0.02459150
Iteration 49, loss = 0.02434409
Iteration 50, loss = 0.02449750
Iteration 51, loss = 0.02443067
Iteration 52, loss = 0.02471988
Iteration 53, loss = 0.02404979
Iteration 54, loss = 0.02420145
Iteration 55, loss = 0.02403625
Iteration 56, loss = 0.02406362
Iteration 57, loss = 0.02377528
Iteration 58, loss = 0.02368943
Iteration 59, loss = 0.02372388
Iteration 60, loss = 0.02375806
Iteration 61, loss = 0.02352728
Iteration 62, loss = 0.02370359
Iteration 63, loss = 0.02344515
Iteration 64, loss = 0.02359075
Iteration 65, loss = 0.02343590
Iteration 66, loss = 0.02353259
Iteration 67, loss = 0.02309859
Iteration 68, loss = 0.02321855
Iteration 69, loss = 0.02332428
Iteration 70, loss = 0.02300099
Iteration 71, loss = 0.02322713
Iteration 72, loss = 0.02339231
Iteration 73, loss = 0.02316973
Iteration 74, loss = 0.02308809
Iteration 75, loss = 0.02311747
Iteration 76, loss = 0.02280633
Iteration 77, loss = 0.02312473
Iteration 78, loss = 0.02277606
Iteration 79, loss = 0.02292376
Iteration 80, loss = 0.02287892
Iteration 81, loss = 0.02301607
Iteration 82, loss = 0.02279723
Iteration 83, loss = 0.02306280
Iteration 84, loss = 0.02272052
Iteration 85, loss = 0.02264217
Iteration 86, loss = 0.02306691
Iteration 87, loss = 0.02286191
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67580184
Iteration 2, loss = 0.60619182
Iteration 3, loss = 0.49624507
Iteration 4, loss = 0.38400566
Iteration 5, loss = 0.28695982
Iteration 6, loss = 0.21472718
Iteration 7, loss = 0.16434656
Iteration 8, loss = 0.13045095
Iteration 9, loss = 0.10663661
Iteration 10, loss = 0.08998819
Iteration 11, loss = 0.07746315
Iteration 12, loss = 0.06797314
Iteration 13, loss = 0.06081623
Iteration 14, loss = 0.05549487
Iteration 15, loss = 0.05089720
Iteration 16, loss = 0.04721528
Iteration 17, loss = 0.04428554
Iteration 18, loss = 0.04170708
Iteration 19, loss = 0.03969933
Iteration 20, loss = 0.03778599
Iteration 21, loss = 0.03625493
Iteration 22, loss = 0.03462598
Iteration 23, loss = 0.03367368
Iteration 24, loss = 0.03281658
Iteration 25, loss = 0.03178577
Iteration 26, loss = 0.03109160
Iteration 27, loss = 0.03048553
Iteration 28, loss = 0.02974370
Iteration 29, loss = 0.02941036
Iteration 30, loss = 0.02835349
Iteration 31, loss = 0.02817581
Iteration 32, loss = 0.02769208
Iteration 33, loss = 0.02737383
Iteration 34, loss = 0.02723817
Iteration 35, loss = 0.02685937
Iteration 36, loss = 0.02655364
Iteration 37, loss = 0.02609743
Iteration 38, loss = 0.02605834
Iteration 39, loss = 0.02576466
Iteration 40, loss = 0.02551642
Iteration 41, loss = 0.02546886
Iteration 42, loss = 0.02496836
Iteration 43, loss = 0.02511235
Iteration 44, loss = 0.02502158
Iteration 45, loss = 0.02476761
Iteration 46, loss = 0.02437988
Iteration 47, loss = 0.02450066
Iteration 48, loss = 0.02443353
Iteration 49, loss = 0.02432496
Iteration 50, loss = 0.02392915
Iteration 51, loss = 0.02415951
Iteration 52, loss = 0.02413209
Iteration 53, loss = 0.02385375
Iteration 54, loss = 0.02405052
Iteration 55, loss = 0.02384501
Iteration 56, loss = 0.02379366
Iteration 57, loss = 0.02377612
Iteration 58, loss = 0.02351498
Iteration 59, loss = 0.02336399
Iteration 60, loss = 0.02347713
Iteration 61, loss = 0.02334925
Iteration 62, loss = 0.02347166
Iteration 63, loss = 0.02323538
Iteration 64, loss = 0.02334075
Iteration 65, loss = 0.02318895
Iteration 66, loss = 0.02319017
Iteration 67, loss = 0.02286825
Iteration 68, loss = 0.02297413
Iteration 69, loss = 0.02300215
Iteration 70, loss = 0.02315709
Iteration 71, loss = 0.02266768
Iteration 72, loss = 0.02302611
Iteration 73, loss = 0.02287704
Iteration 74, loss = 0.02270902
Iteration 75, loss = 0.02313721
Iteration 76, loss = 0.02277688
Iteration 77, loss = 0.02290482
Iteration 78, loss = 0.02300377
Iteration 79, loss = 0.02272191
Iteration 80, loss = 0.02298141
Iteration 81, loss = 0.02266350
Iteration 82, loss = 0.02274019
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69563314
Iteration 2, loss = 0.62856076
Iteration 3, loss = 0.53136544
Iteration 4, loss = 0.41765940
Iteration 5, loss = 0.31572223
Iteration 6, loss = 0.23702839
Iteration 7, loss = 0.18051519
Iteration 8, loss = 0.14174777
Iteration 9, loss = 0.11486428
Iteration 10, loss = 0.09560317
Iteration 11, loss = 0.08179181
Iteration 12, loss = 0.07157631
Iteration 13, loss = 0.06357636
Iteration 14, loss = 0.05751536
Iteration 15, loss = 0.05249250
Iteration 16, loss = 0.04846407
Iteration 17, loss = 0.04530653
Iteration 18, loss = 0.04236552
Iteration 19, loss = 0.04014594
Iteration 20, loss = 0.03848306
Iteration 21, loss = 0.03679240
Iteration 22, loss = 0.03524557
Iteration 23, loss = 0.03415916
Iteration 24, loss = 0.03304402
Iteration 25, loss = 0.03209571
Iteration 26, loss = 0.03134436
Iteration 27, loss = 0.03099686
Iteration 28, loss = 0.03020830
Iteration 29, loss = 0.02952934
Iteration 30, loss = 0.02908326
Iteration 31, loss = 0.02896663
Iteration 32, loss = 0.02797576
Iteration 33, loss = 0.02768014
Iteration 34, loss = 0.02741348
Iteration 35, loss = 0.02715874
Iteration 36, loss = 0.02666437
Iteration 37, loss = 0.02639403
Iteration 38, loss = 0.02615221
Iteration 39, loss = 0.02591644
Iteration 40, loss = 0.02605896
Iteration 41, loss = 0.02577966
Iteration 42, loss = 0.02555538
Iteration 43, loss = 0.02535023
Iteration 44, loss = 0.02504804
Iteration 45, loss = 0.02515665
Iteration 46, loss = 0.02501545
Iteration 47, loss = 0.02464354
Iteration 48, loss = 0.02457044
Iteration 49, loss = 0.02460456
Iteration 50, loss = 0.02445194
Iteration 51, loss = 0.02469135
Iteration 52, loss = 0.02427289
Iteration 53, loss = 0.02434779
Iteration 54, loss = 0.02427284
Iteration 55, loss = 0.02412955
Iteration 56, loss = 0.02421596
Iteration 57, loss = 0.02404759
Iteration 58, loss = 0.02410333
Iteration 59, loss = 0.02380039
Iteration 60, loss = 0.02427984
Iteration 61, loss = 0.02376782
Iteration 62, loss = 0.02355904
Iteration 63, loss = 0.02351356
Iteration 64, loss = 0.02367178
Iteration 65, loss = 0.02374643
Iteration 66, loss = 0.02327902
Iteration 67, loss = 0.02350087
Iteration 68, loss = 0.02370862
Iteration 69, loss = 0.02337938
Iteration 70, loss = 0.02370522
Iteration 71, loss = 0.02345929
Iteration 72, loss = 0.02361781
Iteration 73, loss = 0.02330414
Iteration 74, loss = 0.02337234
Iteration 75, loss = 0.02321135
Iteration 76, loss = 0.02312435
Iteration 77, loss = 0.02304482
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67308215
Iteration 2, loss = 0.59661885
Iteration 3, loss = 0.48781529
Iteration 4, loss = 0.37915795
Iteration 5, loss = 0.28668749
Iteration 6, loss = 0.21648420
Iteration 7, loss = 0.16686423
Iteration 8, loss = 0.13270902
Iteration 9, loss = 0.10888796
Iteration 10, loss = 0.09197524
Iteration 11, loss = 0.07922514
Iteration 12, loss = 0.07000198
Iteration 13, loss = 0.06283441
Iteration 14, loss = 0.05711691
Iteration 15, loss = 0.05239958
Iteration 16, loss = 0.04891116
Iteration 17, loss = 0.04595908
Iteration 18, loss = 0.04337115
Iteration 19, loss = 0.04125761
Iteration 20, loss = 0.03957280
Iteration 21, loss = 0.03816921
Iteration 22, loss = 0.03668210
Iteration 23, loss = 0.03550281
Iteration 24, loss = 0.03457885
Iteration 25, loss = 0.03355321
Iteration 26, loss = 0.03290884
Iteration 27, loss = 0.03234316
Iteration 28, loss = 0.03159589
Iteration 29, loss = 0.03130047
Iteration 30, loss = 0.03064851
Iteration 31, loss = 0.03033053
Iteration 32, loss = 0.02964386
Iteration 33, loss = 0.02932399
Iteration 34, loss = 0.02904651
Iteration 35, loss = 0.02882672
Iteration 36, loss = 0.02839627
Iteration 37, loss = 0.02822798
Iteration 38, loss = 0.02808664
Iteration 39, loss = 0.02768208
Iteration 40, loss = 0.02745837
Iteration 41, loss = 0.02746109
Iteration 42, loss = 0.02725362
Iteration 43, loss = 0.02704505
Iteration 44, loss = 0.02683057
Iteration 45, loss = 0.02673902
Iteration 46, loss = 0.02662508
Iteration 47, loss = 0.02650236
Iteration 48, loss = 0.02649717
Iteration 49, loss = 0.02618739
Iteration 50, loss = 0.02631201
Iteration 51, loss = 0.02626461
Iteration 52, loss = 0.02587691
Iteration 53, loss = 0.02619407
Iteration 54, loss = 0.02610511
Iteration 55, loss = 0.02567792
Iteration 56, loss = 0.02575410
Iteration 57, loss = 0.02558740
Iteration 58, loss = 0.02578319
Iteration 59, loss = 0.02564616
Iteration 60, loss = 0.02534401
Iteration 61, loss = 0.02563520
Iteration 62, loss = 0.02540883
Iteration 63, loss = 0.02537402
Iteration 64, loss = 0.02542712
Iteration 65, loss = 0.02513786
Iteration 66, loss = 0.02572983
Iteration 67, loss = 0.02547889
Iteration 68, loss = 0.02530941
Iteration 69, loss = 0.02534718
Iteration 70, loss = 0.02523994
Iteration 71, loss = 0.02535469
Iteration 72, loss = 0.02482969
Iteration 73, loss = 0.02510641
Iteration 74, loss = 0.02511557
Iteration 75, loss = 0.02494471
Iteration 76, loss = 0.02505043
Iteration 77, loss = 0.02501555
Iteration 78, loss = 0.02540326
Iteration 79, loss = 0.02482830
Iteration 80, loss = 0.02530610
Iteration 81, loss = 0.02507091
Iteration 82, loss = 0.02490830
Iteration 83, loss = 0.02466274
Iteration 84, loss = 0.02511904
Iteration 85, loss = 0.02467252
Iteration 86, loss = 0.02489135
Iteration 87, loss = 0.02492847
Iteration 88, loss = 0.02479323
Iteration 89, loss = 0.02519255
Iteration 90, loss = 0.02483059
Iteration 91, loss = 0.02469224
Iteration 92, loss = 0.02448404
Iteration 93, loss = 0.02498155
Iteration 94, loss = 0.02443785
Iteration 95, loss = 0.02484305
Iteration 96, loss = 0.02491473
Iteration 97, loss = 0.02478496
Iteration 98, loss = 0.02456720
Iteration 99, loss = 0.02456319
Iteration 100, loss = 0.02451380
Iteration 101, loss = 0.02452059
Iteration 102, loss = 0.02474999
Iteration 103, loss = 0.02464522
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68808552
Iteration 2, loss = 0.61761625
Iteration 3, loss = 0.50925323
Iteration 4, loss = 0.39377220
Iteration 5, loss = 0.29648849
Iteration 6, loss = 0.22403361
Iteration 7, loss = 0.17249683
Iteration 8, loss = 0.13721542
Iteration 9, loss = 0.11256156
Iteration 10, loss = 0.09501476
Iteration 11, loss = 0.08203661
Iteration 12, loss = 0.07234805
Iteration 13, loss = 0.06500566
Iteration 14, loss = 0.05894125
Iteration 15, loss = 0.05436477
Iteration 16, loss = 0.05049896
Iteration 17, loss = 0.04752727
Iteration 18, loss = 0.04478444
Iteration 19, loss = 0.04293641
Iteration 20, loss = 0.04113422
Iteration 21, loss = 0.03941838
Iteration 22, loss = 0.03799005
Iteration 23, loss = 0.03681701
Iteration 24, loss = 0.03587427
Iteration 25, loss = 0.03504153
Iteration 26, loss = 0.03416748
Iteration 27, loss = 0.03376722
Iteration 28, loss = 0.03288025
Iteration 29, loss = 0.03233141
Iteration 30, loss = 0.03191925
Iteration 31, loss = 0.03145064
Iteration 32, loss = 0.03075597
Iteration 33, loss = 0.03043922
Iteration 34, loss = 0.03024033
Iteration 35, loss = 0.02988007
Iteration 36, loss = 0.02984744
Iteration 37, loss = 0.02939472
Iteration 38, loss = 0.02918146
Iteration 39, loss = 0.02895532
Iteration 40, loss = 0.02878112
Iteration 41, loss = 0.02854801
Iteration 42, loss = 0.02869775
Iteration 43, loss = 0.02820066
Iteration 44, loss = 0.02793313
Iteration 45, loss = 0.02791547
Iteration 46, loss = 0.02776993
Iteration 47, loss = 0.02758601
Iteration 48, loss = 0.02750300
Iteration 49, loss = 0.02756924
Iteration 50, loss = 0.02747593
Iteration 51, loss = 0.02723715
Iteration 52, loss = 0.02726546
Iteration 53, loss = 0.02711224
Iteration 54, loss = 0.02737658
Iteration 55, loss = 0.02710708
Iteration 56, loss = 0.02722077
Iteration 57, loss = 0.02681753
Iteration 58, loss = 0.02679832
Iteration 59, loss = 0.02684005
Iteration 60, loss = 0.02659850
Iteration 61, loss = 0.02651633
Iteration 62, loss = 0.02677442
Iteration 63, loss = 0.02637375
Iteration 64, loss = 0.02639313
Iteration 65, loss = 0.02634108
Iteration 66, loss = 0.02648311
Iteration 67, loss = 0.02621979
Iteration 68, loss = 0.02641291
Iteration 69, loss = 0.02642552
Iteration 70, loss = 0.02604422
Iteration 71, loss = 0.02630883
Iteration 72, loss = 0.02652846
Iteration 73, loss = 0.02617636
Iteration 74, loss = 0.02610725
Iteration 75, loss = 0.02627016
Iteration 76, loss = 0.02624673
Iteration 77, loss = 0.02609201
Iteration 78, loss = 0.02620254
Iteration 79, loss = 0.02621694
Iteration 80, loss = 0.02623426
Iteration 81, loss = 0.02564267
Iteration 82, loss = 0.02613239
Iteration 83, loss = 0.02592378
Iteration 84, loss = 0.02594531
Iteration 85, loss = 0.02594705
Iteration 86, loss = 0.02564628
Iteration 87, loss = 0.02590831
Iteration 88, loss = 0.02561980
Iteration 89, loss = 0.02571932
Iteration 90, loss = 0.02587378
Iteration 91, loss = 0.02586583
Iteration 92, loss = 0.02591250
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67231395
Iteration 2, loss = 0.59049072
Iteration 3, loss = 0.46494118
Iteration 4, loss = 0.34065885
Iteration 5, loss = 0.24471821
Iteration 6, loss = 0.17943179
Iteration 7, loss = 0.13713657
Iteration 8, loss = 0.10965843
Iteration 9, loss = 0.09078350
Iteration 10, loss = 0.07775117
Iteration 11, loss = 0.06805946
Iteration 12, loss = 0.06064815
Iteration 13, loss = 0.05557407
Iteration 14, loss = 0.05104569
Iteration 15, loss = 0.04763582
Iteration 16, loss = 0.04492025
Iteration 17, loss = 0.04274812
Iteration 18, loss = 0.04073051
Iteration 19, loss = 0.03923154
Iteration 20, loss = 0.03788368
Iteration 21, loss = 0.03672516
Iteration 22, loss = 0.03550660
Iteration 23, loss = 0.03502882
Iteration 24, loss = 0.03392706
Iteration 25, loss = 0.03339196
Iteration 26, loss = 0.03280750
Iteration 27, loss = 0.03234491
Iteration 28, loss = 0.03217363
Iteration 29, loss = 0.03161242
Iteration 30, loss = 0.03107153
Iteration 31, loss = 0.03062186
Iteration 32, loss = 0.03040048
Iteration 33, loss = 0.03034340
Iteration 34, loss = 0.03009200
Iteration 35, loss = 0.02999759
Iteration 36, loss = 0.02996185
Iteration 37, loss = 0.02973051
Iteration 38, loss = 0.02952311
Iteration 39, loss = 0.02890003
Iteration 40, loss = 0.02886422
Iteration 41, loss = 0.02877548
Iteration 42, loss = 0.02865812
Iteration 43, loss = 0.02859310
Iteration 44, loss = 0.02847281
Iteration 45, loss = 0.02844333
Iteration 46, loss = 0.02800955
Iteration 47, loss = 0.02850691
Iteration 48, loss = 0.02830240
Iteration 49, loss = 0.02774058
Iteration 50, loss = 0.02801429
Iteration 51, loss = 0.02793206
Iteration 52, loss = 0.02808831
Iteration 53, loss = 0.02781713
Iteration 54, loss = 0.02756389
Iteration 55, loss = 0.02769047
Iteration 56, loss = 0.02787320
Iteration 57, loss = 0.02740398
Iteration 58, loss = 0.02747334
Iteration 59, loss = 0.02764397
Iteration 60, loss = 0.02743339
Iteration 61, loss = 0.02750072
Iteration 62, loss = 0.02708955
Iteration 63, loss = 0.02739972
Iteration 64, loss = 0.02735817
Iteration 65, loss = 0.02758733
Iteration 66, loss = 0.02745524
Iteration 67, loss = 0.02696938
Iteration 68, loss = 0.02738886
Iteration 69, loss = 0.02710756
Iteration 70, loss = 0.02703374
Iteration 71, loss = 0.02691066
Iteration 72, loss = 0.02695774
Iteration 73, loss = 0.02737485
Iteration 74, loss = 0.02702292
Iteration 75, loss = 0.02696648
Iteration 76, loss = 0.02691803
Iteration 77, loss = 0.02705250
Iteration 78, loss = 0.02684456
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on kept_v6_custom_stopwords...
Iteration 1, loss = 0.72257596
Iteration 2, loss = 0.70492281
Iteration 3, loss = 0.69083921
Iteration 4, loss = 0.67749092
Iteration 5, loss = 0.66400354
Iteration 6, loss = 0.64969843
Iteration 7, loss = 0.63440708
Iteration 8, loss = 0.61802610
Iteration 9, loss = 0.60063255
Iteration 10, loss = 0.58168783
Iteration 11, loss = 0.56165529
Iteration 12, loss = 0.54069856
Iteration 13, loss = 0.51886948
Iteration 14, loss = 0.49656499
Iteration 15, loss = 0.47378977
Iteration 16, loss = 0.45096604
Iteration 17, loss = 0.42809710
Iteration 18, loss = 0.40569042
Iteration 19, loss = 0.38361701
Iteration 20, loss = 0.36216085
Iteration 21, loss = 0.34157040
Iteration 22, loss = 0.32164430
Iteration 23, loss = 0.30278924
Iteration 24, loss = 0.28476898
Iteration 25, loss = 0.26788712
Iteration 26, loss = 0.25186657
Iteration 27, loss = 0.23685665
Iteration 28, loss = 0.22287871
Iteration 29, loss = 0.20976022
Iteration 30, loss = 0.19757111
Iteration 31, loss = 0.18613733
Iteration 32, loss = 0.17554546
Iteration 33, loss = 0.16565220
Iteration 34, loss = 0.15652144
Iteration 35, loss = 0.14792725
Iteration 36, loss = 0.14009319
Iteration 37, loss = 0.13272665
Iteration 38, loss = 0.12588926
Iteration 39, loss = 0.11957390
Iteration 40, loss = 0.11369721
Iteration 41, loss = 0.10826720
Iteration 42, loss = 0.10315546
Iteration 43, loss = 0.09840493
Iteration 44, loss = 0.09394841
Iteration 45, loss = 0.08976116
Iteration 46, loss = 0.08591025
Iteration 47, loss = 0.08234194
Iteration 48, loss = 0.07895719
Iteration 49, loss = 0.07587144
Iteration 50, loss = 0.07286515
Iteration 51, loss = 0.07013056
Iteration 52, loss = 0.06753506
Iteration 53, loss = 0.06510690
Iteration 54, loss = 0.06279562
Iteration 55, loss = 0.06068103
Iteration 56, loss = 0.05859408
Iteration 57, loss = 0.05665052
Iteration 58, loss = 0.05483574
Iteration 59, loss = 0.05311271
Iteration 60, loss = 0.05146935
Iteration 61, loss = 0.04987734
Iteration 62, loss = 0.04842202
Iteration 63, loss = 0.04701889
Iteration 64, loss = 0.04570081
Iteration 65, loss = 0.04452570
Iteration 66, loss = 0.04331562
Iteration 67, loss = 0.04207000
Iteration 68, loss = 0.04109467
Iteration 69, loss = 0.03995959
Iteration 70, loss = 0.03904248
Iteration 71, loss = 0.03812367
Iteration 72, loss = 0.03724048
Iteration 73, loss = 0.03632980
Iteration 74, loss = 0.03549616
Iteration 75, loss = 0.03474441
Iteration 76, loss = 0.03397204
Iteration 77, loss = 0.03325667
Iteration 78, loss = 0.03257085
Iteration 79, loss = 0.03194351
Iteration 80, loss = 0.03127328
Iteration 81, loss = 0.03066979
Iteration 82, loss = 0.03010823
Iteration 83, loss = 0.02949832
Iteration 84, loss = 0.02896581
Iteration 85, loss = 0.02848905
Iteration 86, loss = 0.02795604
Iteration 87, loss = 0.02748762
Iteration 88, loss = 0.02715898
Iteration 89, loss = 0.02665595
Iteration 90, loss = 0.02623628
Iteration 91, loss = 0.02579240
Iteration 92, loss = 0.02540025
Iteration 93, loss = 0.02501689
Iteration 94, loss = 0.02463914
Iteration 95, loss = 0.02423928
Iteration 96, loss = 0.02391992
Iteration 97, loss = 0.02357799
Iteration 98, loss = 0.02321438
Iteration 99, loss = 0.02287687
Iteration 100, loss = 0.02258863
Iteration 101, loss = 0.02228502
Iteration 102, loss = 0.02201285
Iteration 103, loss = 0.02173667
Iteration 104, loss = 0.02148355
Iteration 105, loss = 0.02125746
Iteration 106, loss = 0.02101598
Iteration 107, loss = 0.02074336
Iteration 108, loss = 0.02049916
Iteration 109, loss = 0.02022424
Iteration 110, loss = 0.01996789
Iteration 111, loss = 0.01979567
Iteration 112, loss = 0.01952807
Iteration 113, loss = 0.01934109
Iteration 114, loss = 0.01912687
Iteration 115, loss = 0.01896431
Iteration 116, loss = 0.01878875
Iteration 117, loss = 0.01861564
Iteration 118, loss = 0.01841327
Iteration 119, loss = 0.01825753
Iteration 120, loss = 0.01806396
Iteration 121, loss = 0.01785407
Iteration 122, loss = 0.01768605
Iteration 123, loss = 0.01752707
Iteration 124, loss = 0.01736901
Iteration 125, loss = 0.01725422
Iteration 126, loss = 0.01708251
Iteration 127, loss = 0.01696606
Iteration 128, loss = 0.01682269
Iteration 129, loss = 0.01668561
Iteration 130, loss = 0.01655169
Iteration 131, loss = 0.01640762
Iteration 132, loss = 0.01630135
Iteration 133, loss = 0.01614541
Iteration 134, loss = 0.01601344
Iteration 135, loss = 0.01591370
Iteration 136, loss = 0.01581021
Iteration 137, loss = 0.01567507
Iteration 138, loss = 0.01556144
Iteration 139, loss = 0.01551531
Iteration 140, loss = 0.01541144
Iteration 141, loss = 0.01530581
Iteration 142, loss = 0.01517533
Iteration 143, loss = 0.01507914
Iteration 144, loss = 0.01501289
Iteration 145, loss = 0.01489578
Iteration 146, loss = 0.01482309
Iteration 147, loss = 0.01472221
Iteration 148, loss = 0.01469191
Iteration 149, loss = 0.01459448
Iteration 150, loss = 0.01449597
Iteration 151, loss = 0.01442922
Iteration 152, loss = 0.01436914
Iteration 153, loss = 0.01425619
Iteration 154, loss = 0.01422464
Iteration 155, loss = 0.01414428
Iteration 156, loss = 0.01408011
Iteration 157, loss = 0.01398221
Iteration 158, loss = 0.01390398
Iteration 159, loss = 0.01381392
Iteration 160, loss = 0.01373314
Iteration 161, loss = 0.01363053
Iteration 162, loss = 0.01354158
Iteration 163, loss = 0.01344439
Iteration 164, loss = 0.01343349
Iteration 165, loss = 0.01332894
Iteration 166, loss = 0.01325163
Iteration 167, loss = 0.01320190
Iteration 168, loss = 0.01314543
Iteration 169, loss = 0.01309084
Iteration 170, loss = 0.01307922
Iteration 171, loss = 0.01296514
Iteration 172, loss = 0.01292850
Iteration 173, loss = 0.01288484
Iteration 174, loss = 0.01279542
Iteration 175, loss = 0.01274647
Iteration 176, loss = 0.01270987
Iteration 177, loss = 0.01273946
Iteration 178, loss = 0.01270154
Iteration 179, loss = 0.01255624
Iteration 180, loss = 0.01247360
Iteration 181, loss = 0.01243916
Iteration 182, loss = 0.01243673
Iteration 183, loss = 0.01240974
Iteration 184, loss = 0.01237705
Iteration 185, loss = 0.01239117
Iteration 186, loss = 0.01230377
Iteration 187, loss = 0.01231598
Iteration 188, loss = 0.01229673
Iteration 189, loss = 0.01232492
Iteration 190, loss = 0.01224169
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68200955
Iteration 2, loss = 0.65972753
Iteration 3, loss = 0.63362015
Iteration 4, loss = 0.59770905
Iteration 5, loss = 0.55282017
Iteration 6, loss = 0.50304297
Iteration 7, loss = 0.45221810
Iteration 8, loss = 0.40334927
Iteration 9, loss = 0.35728395
Iteration 10, loss = 0.31518268
Iteration 11, loss = 0.27731699
Iteration 12, loss = 0.24377588
Iteration 13, loss = 0.21482876
Iteration 14, loss = 0.18976842
Iteration 15, loss = 0.16832800
Iteration 16, loss = 0.14999363
Iteration 17, loss = 0.13439654
Iteration 18, loss = 0.12105221
Iteration 19, loss = 0.10967439
Iteration 20, loss = 0.09981421
Iteration 21, loss = 0.09135898
Iteration 22, loss = 0.08402287
Iteration 23, loss = 0.07774800
Iteration 24, loss = 0.07223199
Iteration 25, loss = 0.06713955
Iteration 26, loss = 0.06290762
Iteration 27, loss = 0.05909144
Iteration 28, loss = 0.05562974
Iteration 29, loss = 0.05257088
Iteration 30, loss = 0.04965158
Iteration 31, loss = 0.04739790
Iteration 32, loss = 0.04508870
Iteration 33, loss = 0.04318003
Iteration 34, loss = 0.04132646
Iteration 35, loss = 0.03957657
Iteration 36, loss = 0.03799099
Iteration 37, loss = 0.03657876
Iteration 38, loss = 0.03530501
Iteration 39, loss = 0.03408980
Iteration 40, loss = 0.03294704
Iteration 41, loss = 0.03183448
Iteration 42, loss = 0.03093904
Iteration 43, loss = 0.03004197
Iteration 44, loss = 0.02917683
Iteration 45, loss = 0.02842348
Iteration 46, loss = 0.02795371
Iteration 47, loss = 0.02755057
Iteration 48, loss = 0.02695699
Iteration 49, loss = 0.02620420
Iteration 50, loss = 0.02548776
Iteration 51, loss = 0.02489773
Iteration 52, loss = 0.02458004
Iteration 53, loss = 0.02414917
Iteration 54, loss = 0.02346286
Iteration 55, loss = 0.02316718
Iteration 56, loss = 0.02290508
Iteration 57, loss = 0.02277804
Iteration 58, loss = 0.02232124
Iteration 59, loss = 0.02182672
Iteration 60, loss = 0.02150701
Iteration 61, loss = 0.02104596
Iteration 62, loss = 0.02076699
Iteration 63, loss = 0.02058250
Iteration 64, loss = 0.02035624
Iteration 65, loss = 0.02036874
Iteration 66, loss = 0.01999557
Iteration 67, loss = 0.01982408
Iteration 68, loss = 0.01964043
Iteration 69, loss = 0.01943051
Iteration 70, loss = 0.01912873
Iteration 71, loss = 0.01891777
Iteration 72, loss = 0.01898543
Iteration 73, loss = 0.01870962
Iteration 74, loss = 0.01853321
Iteration 75, loss = 0.01833912
Iteration 76, loss = 0.01811621
Iteration 77, loss = 0.01811855
Iteration 78, loss = 0.01794240
Iteration 79, loss = 0.01770789
Iteration 80, loss = 0.01766591
Iteration 81, loss = 0.01755249
Iteration 82, loss = 0.01740243
Iteration 83, loss = 0.01720999
Iteration 84, loss = 0.01703527
Iteration 85, loss = 0.01688935
Iteration 86, loss = 0.01697959
Iteration 87, loss = 0.01682400
Iteration 88, loss = 0.01665809
Iteration 89, loss = 0.01665170
Iteration 90, loss = 0.01646125
Iteration 91, loss = 0.01645583
Iteration 92, loss = 0.01639075
Iteration 93, loss = 0.01666018
Iteration 94, loss = 0.01678222
Iteration 95, loss = 0.01659260
Iteration 96, loss = 0.01642005
Iteration 97, loss = 0.01628035
Iteration 98, loss = 0.01603362
Iteration 99, loss = 0.01592659
Iteration 100, loss = 0.01595235
Iteration 101, loss = 0.01584959
Iteration 102, loss = 0.01574008
Iteration 103, loss = 0.01575876
Iteration 104, loss = 0.01560602
Iteration 105, loss = 0.01551076
Iteration 106, loss = 0.01543864
Iteration 107, loss = 0.01518474
Iteration 108, loss = 0.01533671
Iteration 109, loss = 0.01547614
Iteration 110, loss = 0.01544116
Iteration 111, loss = 0.01538320
Iteration 112, loss = 0.01526671
Iteration 113, loss = 0.01525288
Iteration 114, loss = 0.01519338
Iteration 115, loss = 0.01503125
Iteration 116, loss = 0.01502890
Iteration 117, loss = 0.01497806
Iteration 118, loss = 0.01483962
Iteration 119, loss = 0.01512455
Iteration 120, loss = 0.01504678
Iteration 121, loss = 0.01497050
Iteration 122, loss = 0.01475289
Iteration 123, loss = 0.01471954
Iteration 124, loss = 0.01455463
Iteration 125, loss = 0.01470305
Iteration 126, loss = 0.01482062
Iteration 127, loss = 0.01476360
Iteration 128, loss = 0.01475108
Iteration 129, loss = 0.01488770
Iteration 130, loss = 0.01496913
Iteration 131, loss = 0.01508192
Iteration 132, loss = 0.01528358
Iteration 133, loss = 0.01513060
Iteration 134, loss = 0.01492152
Iteration 135, loss = 0.01480855
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67951020
Iteration 2, loss = 0.65474501
Iteration 3, loss = 0.61434625
Iteration 4, loss = 0.55356443
Iteration 5, loss = 0.48335639
Iteration 6, loss = 0.41371634
Iteration 7, loss = 0.34946302
Iteration 8, loss = 0.29296296
Iteration 9, loss = 0.24527948
Iteration 10, loss = 0.20591512
Iteration 11, loss = 0.17431185
Iteration 12, loss = 0.14913974
Iteration 13, loss = 0.12902060
Iteration 14, loss = 0.11290186
Iteration 15, loss = 0.09995339
Iteration 16, loss = 0.08939646
Iteration 17, loss = 0.08083488
Iteration 18, loss = 0.07367389
Iteration 19, loss = 0.06780223
Iteration 20, loss = 0.06266741
Iteration 21, loss = 0.05821526
Iteration 22, loss = 0.05474051
Iteration 23, loss = 0.05170594
Iteration 24, loss = 0.04875432
Iteration 25, loss = 0.04623567
Iteration 26, loss = 0.04417026
Iteration 27, loss = 0.04222257
Iteration 28, loss = 0.04062995
Iteration 29, loss = 0.03922165
Iteration 30, loss = 0.03790091
Iteration 31, loss = 0.03670436
Iteration 32, loss = 0.03561402
Iteration 33, loss = 0.03472011
Iteration 34, loss = 0.03372052
Iteration 35, loss = 0.03284931
Iteration 36, loss = 0.03224643
Iteration 37, loss = 0.03145201
Iteration 38, loss = 0.03094351
Iteration 39, loss = 0.03049107
Iteration 40, loss = 0.02962669
Iteration 41, loss = 0.02934012
Iteration 42, loss = 0.02887468
Iteration 43, loss = 0.02848368
Iteration 44, loss = 0.02804661
Iteration 45, loss = 0.02775253
Iteration 46, loss = 0.02729056
Iteration 47, loss = 0.02718827
Iteration 48, loss = 0.02678923
Iteration 49, loss = 0.02671606
Iteration 50, loss = 0.02628272
Iteration 51, loss = 0.02608734
Iteration 52, loss = 0.02588773
Iteration 53, loss = 0.02566263
Iteration 54, loss = 0.02550598
Iteration 55, loss = 0.02520302
Iteration 56, loss = 0.02518988
Iteration 57, loss = 0.02483353
Iteration 58, loss = 0.02484864
Iteration 59, loss = 0.02452307
Iteration 60, loss = 0.02426758
Iteration 61, loss = 0.02434391
Iteration 62, loss = 0.02430614
Iteration 63, loss = 0.02418194
Iteration 64, loss = 0.02397743
Iteration 65, loss = 0.02372305
Iteration 66, loss = 0.02375151
Iteration 67, loss = 0.02356209
Iteration 68, loss = 0.02370514
Iteration 69, loss = 0.02341343
Iteration 70, loss = 0.02342144
Iteration 71, loss = 0.02324393
Iteration 72, loss = 0.02324157
Iteration 73, loss = 0.02297654
Iteration 74, loss = 0.02308975
Iteration 75, loss = 0.02303423
Iteration 76, loss = 0.02303744
Iteration 77, loss = 0.02299621
Iteration 78, loss = 0.02270992
Iteration 79, loss = 0.02271789
Iteration 80, loss = 0.02277558
Iteration 81, loss = 0.02262733
Iteration 82, loss = 0.02251073
Iteration 83, loss = 0.02245589
Iteration 84, loss = 0.02235830
Iteration 85, loss = 0.02234225
Iteration 86, loss = 0.02241317
Iteration 87, loss = 0.02226831
Iteration 88, loss = 0.02233463
Iteration 89, loss = 0.02211481
Iteration 90, loss = 0.02228389
Iteration 91, loss = 0.02226258
Iteration 92, loss = 0.02226045
Iteration 93, loss = 0.02207947
Iteration 94, loss = 0.02196593
Iteration 95, loss = 0.02205940
Iteration 96, loss = 0.02218153
Iteration 97, loss = 0.02195088
Iteration 98, loss = 0.02200782
Iteration 99, loss = 0.02192981
Iteration 100, loss = 0.02196221
Iteration 101, loss = 0.02185109
Iteration 102, loss = 0.02188778
Iteration 103, loss = 0.02172519
Iteration 104, loss = 0.02177639
Iteration 105, loss = 0.02168326
Iteration 106, loss = 0.02152378
Iteration 107, loss = 0.02152432
Iteration 108, loss = 0.02177162
Iteration 109, loss = 0.02167532
Iteration 110, loss = 0.02163151
Iteration 111, loss = 0.02164621
Iteration 112, loss = 0.02161095
Iteration 113, loss = 0.02170399
Iteration 114, loss = 0.02156849
Iteration 115, loss = 0.02150947
Iteration 116, loss = 0.02156580
Iteration 117, loss = 0.02157060
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69155623
Iteration 2, loss = 0.65602098
Iteration 3, loss = 0.60862031
Iteration 4, loss = 0.54855459
Iteration 5, loss = 0.48038818
Iteration 6, loss = 0.41251581
Iteration 7, loss = 0.34885453
Iteration 8, loss = 0.29298235
Iteration 9, loss = 0.24517138
Iteration 10, loss = 0.20631033
Iteration 11, loss = 0.17460211
Iteration 12, loss = 0.14940828
Iteration 13, loss = 0.12939851
Iteration 14, loss = 0.11310408
Iteration 15, loss = 0.10021168
Iteration 16, loss = 0.08972567
Iteration 17, loss = 0.08095454
Iteration 18, loss = 0.07385821
Iteration 19, loss = 0.06780909
Iteration 20, loss = 0.06274648
Iteration 21, loss = 0.05847165
Iteration 22, loss = 0.05470902
Iteration 23, loss = 0.05157041
Iteration 24, loss = 0.04889497
Iteration 25, loss = 0.04653988
Iteration 26, loss = 0.04430406
Iteration 27, loss = 0.04240006
Iteration 28, loss = 0.04068589
Iteration 29, loss = 0.03917390
Iteration 30, loss = 0.03806734
Iteration 31, loss = 0.03660119
Iteration 32, loss = 0.03567264
Iteration 33, loss = 0.03466565
Iteration 34, loss = 0.03398505
Iteration 35, loss = 0.03295151
Iteration 36, loss = 0.03221841
Iteration 37, loss = 0.03142470
Iteration 38, loss = 0.03074973
Iteration 39, loss = 0.03022109
Iteration 40, loss = 0.02979151
Iteration 41, loss = 0.02945339
Iteration 42, loss = 0.02890764
Iteration 43, loss = 0.02846721
Iteration 44, loss = 0.02811471
Iteration 45, loss = 0.02769851
Iteration 46, loss = 0.02734453
Iteration 47, loss = 0.02706981
Iteration 48, loss = 0.02680439
Iteration 49, loss = 0.02674662
Iteration 50, loss = 0.02619692
Iteration 51, loss = 0.02598605
Iteration 52, loss = 0.02585447
Iteration 53, loss = 0.02552058
Iteration 54, loss = 0.02532729
Iteration 55, loss = 0.02509808
Iteration 56, loss = 0.02501554
Iteration 57, loss = 0.02481479
Iteration 58, loss = 0.02457751
Iteration 59, loss = 0.02447129
Iteration 60, loss = 0.02438327
Iteration 61, loss = 0.02411924
Iteration 62, loss = 0.02408963
Iteration 63, loss = 0.02395887
Iteration 64, loss = 0.02398143
Iteration 65, loss = 0.02362642
Iteration 66, loss = 0.02368172
Iteration 67, loss = 0.02341120
Iteration 68, loss = 0.02357304
Iteration 69, loss = 0.02340460
Iteration 70, loss = 0.02339013
Iteration 71, loss = 0.02307604
Iteration 72, loss = 0.02304665
Iteration 73, loss = 0.02309502
Iteration 74, loss = 0.02300931
Iteration 75, loss = 0.02295113
Iteration 76, loss = 0.02277601
Iteration 77, loss = 0.02262110
Iteration 78, loss = 0.02265775
Iteration 79, loss = 0.02262854
Iteration 80, loss = 0.02259406
Iteration 81, loss = 0.02258808
Iteration 82, loss = 0.02262524
Iteration 83, loss = 0.02235889
Iteration 84, loss = 0.02250656
Iteration 85, loss = 0.02238838
Iteration 86, loss = 0.02229381
Iteration 87, loss = 0.02230149
Iteration 88, loss = 0.02221089
Iteration 89, loss = 0.02213222
Iteration 90, loss = 0.02194415
Iteration 91, loss = 0.02194414
Iteration 92, loss = 0.02213586
Iteration 93, loss = 0.02196975
Iteration 94, loss = 0.02194470
Iteration 95, loss = 0.02196723
Iteration 96, loss = 0.02182369
Iteration 97, loss = 0.02192478
Iteration 98, loss = 0.02175871
Iteration 99, loss = 0.02181035
Iteration 100, loss = 0.02158246
Iteration 101, loss = 0.02170477
Iteration 102, loss = 0.02171862
Iteration 103, loss = 0.02166263
Iteration 104, loss = 0.02156935
Iteration 105, loss = 0.02159426
Iteration 106, loss = 0.02159165
Iteration 107, loss = 0.02149854
Iteration 108, loss = 0.02152568
Iteration 109, loss = 0.02166752
Iteration 110, loss = 0.02186817
Iteration 111, loss = 0.02151417
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69444562
Iteration 2, loss = 0.67733561
Iteration 3, loss = 0.66044206
Iteration 4, loss = 0.64080618
Iteration 5, loss = 0.61753356
Iteration 6, loss = 0.58978723
Iteration 7, loss = 0.55798045
Iteration 8, loss = 0.52309544
Iteration 9, loss = 0.48622045
Iteration 10, loss = 0.44848960
Iteration 11, loss = 0.41132578
Iteration 12, loss = 0.37569454
Iteration 13, loss = 0.34222608
Iteration 14, loss = 0.31098670
Iteration 15, loss = 0.28212489
Iteration 16, loss = 0.25584839
Iteration 17, loss = 0.23197749
Iteration 18, loss = 0.21050817
Iteration 19, loss = 0.19139392
Iteration 20, loss = 0.17429495
Iteration 21, loss = 0.15892258
Iteration 22, loss = 0.14540223
Iteration 23, loss = 0.13336217
Iteration 24, loss = 0.12266966
Iteration 25, loss = 0.11312104
Iteration 26, loss = 0.10460787
Iteration 27, loss = 0.09704732
Iteration 28, loss = 0.09021087
Iteration 29, loss = 0.08415150
Iteration 30, loss = 0.07865880
Iteration 31, loss = 0.07374996
Iteration 32, loss = 0.06924746
Iteration 33, loss = 0.06523951
Iteration 34, loss = 0.06153951
Iteration 35, loss = 0.05823377
Iteration 36, loss = 0.05516467
Iteration 37, loss = 0.05234793
Iteration 38, loss = 0.04978293
Iteration 39, loss = 0.04750919
Iteration 40, loss = 0.04532039
Iteration 41, loss = 0.04328305
Iteration 42, loss = 0.04149118
Iteration 43, loss = 0.03974041
Iteration 44, loss = 0.03813855
Iteration 45, loss = 0.03666265
Iteration 46, loss = 0.03528991
Iteration 47, loss = 0.03402066
Iteration 48, loss = 0.03280850
Iteration 49, loss = 0.03169179
Iteration 50, loss = 0.03062438
Iteration 51, loss = 0.02964243
Iteration 52, loss = 0.02872761
Iteration 53, loss = 0.02787032
Iteration 54, loss = 0.02702404
Iteration 55, loss = 0.02625817
Iteration 56, loss = 0.02553927
Iteration 57, loss = 0.02483997
Iteration 58, loss = 0.02417598
Iteration 59, loss = 0.02357445
Iteration 60, loss = 0.02294085
Iteration 61, loss = 0.02243173
Iteration 62, loss = 0.02187903
Iteration 63, loss = 0.02141831
Iteration 64, loss = 0.02093438
Iteration 65, loss = 0.02047642
Iteration 66, loss = 0.02002124
Iteration 67, loss = 0.01959728
Iteration 68, loss = 0.01923633
Iteration 69, loss = 0.01885617
Iteration 70, loss = 0.01852620
Iteration 71, loss = 0.01817622
Iteration 72, loss = 0.01781697
Iteration 73, loss = 0.01753651
Iteration 74, loss = 0.01722694
Iteration 75, loss = 0.01691155
Iteration 76, loss = 0.01668861
Iteration 77, loss = 0.01639822
Iteration 78, loss = 0.01613558
Iteration 79, loss = 0.01589011
Iteration 80, loss = 0.01567219
Iteration 81, loss = 0.01542087
Iteration 82, loss = 0.01523726
Iteration 83, loss = 0.01499319
Iteration 84, loss = 0.01482801
Iteration 85, loss = 0.01467345
Iteration 86, loss = 0.01443782
Iteration 87, loss = 0.01428053
Iteration 88, loss = 0.01408777
Iteration 89, loss = 0.01391650
Iteration 90, loss = 0.01373546
Iteration 91, loss = 0.01362641
Iteration 92, loss = 0.01344549
Iteration 93, loss = 0.01332633
Iteration 94, loss = 0.01320812
Iteration 95, loss = 0.01304849
Iteration 96, loss = 0.01289329
Iteration 97, loss = 0.01278144
Iteration 98, loss = 0.01265107
Iteration 99, loss = 0.01251383
Iteration 100, loss = 0.01242058
Iteration 101, loss = 0.01232493
Iteration 102, loss = 0.01218936
Iteration 103, loss = 0.01208213
Iteration 104, loss = 0.01199385
Iteration 105, loss = 0.01187751
Iteration 106, loss = 0.01180118
Iteration 107, loss = 0.01166595
Iteration 108, loss = 0.01168432
Iteration 109, loss = 0.01153313
Iteration 110, loss = 0.01143181
Iteration 111, loss = 0.01134511
Iteration 112, loss = 0.01131902
Iteration 113, loss = 0.01118819
Iteration 114, loss = 0.01113715
Iteration 115, loss = 0.01104680
Iteration 116, loss = 0.01095291
Iteration 117, loss = 0.01088594
Iteration 118, loss = 0.01084305
Iteration 119, loss = 0.01079649
Iteration 120, loss = 0.01068927
Iteration 121, loss = 0.01062476
Iteration 122, loss = 0.01057199
Iteration 123, loss = 0.01052118
Iteration 124, loss = 0.01046186
Iteration 125, loss = 0.01038501
Iteration 126, loss = 0.01033715
Iteration 127, loss = 0.01026814
Iteration 128, loss = 0.01021787
Iteration 129, loss = 0.01016615
Iteration 130, loss = 0.01016602
Iteration 131, loss = 0.01008876
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67951786
Iteration 2, loss = 0.64002647
Iteration 3, loss = 0.57984325
Iteration 4, loss = 0.50224086
Iteration 5, loss = 0.42222760
Iteration 6, loss = 0.34873782
Iteration 7, loss = 0.28470062
Iteration 8, loss = 0.23251868
Iteration 9, loss = 0.19126820
Iteration 10, loss = 0.15932077
Iteration 11, loss = 0.13464491
Iteration 12, loss = 0.11548833
Iteration 13, loss = 0.10080278
Iteration 14, loss = 0.08894043
Iteration 15, loss = 0.07947750
Iteration 16, loss = 0.07151082
Iteration 17, loss = 0.06539449
Iteration 18, loss = 0.06003039
Iteration 19, loss = 0.05572084
Iteration 20, loss = 0.05212422
Iteration 21, loss = 0.04900144
Iteration 22, loss = 0.04648643
Iteration 23, loss = 0.04391905
Iteration 24, loss = 0.04204037
Iteration 25, loss = 0.04029164
Iteration 26, loss = 0.03869538
Iteration 27, loss = 0.03765825
Iteration 28, loss = 0.03631858
Iteration 29, loss = 0.03510507
Iteration 30, loss = 0.03437039
Iteration 31, loss = 0.03341463
Iteration 32, loss = 0.03238887
Iteration 33, loss = 0.03159786
Iteration 34, loss = 0.03135470
Iteration 35, loss = 0.03078968
Iteration 36, loss = 0.02997763
Iteration 37, loss = 0.02937482
Iteration 38, loss = 0.02897436
Iteration 39, loss = 0.02869588
Iteration 40, loss = 0.02815231
Iteration 41, loss = 0.02790741
Iteration 42, loss = 0.02747526
Iteration 43, loss = 0.02730311
Iteration 44, loss = 0.02705061
Iteration 45, loss = 0.02672728
Iteration 46, loss = 0.02651979
Iteration 47, loss = 0.02616749
Iteration 48, loss = 0.02597807
Iteration 49, loss = 0.02565681
Iteration 50, loss = 0.02568162
Iteration 51, loss = 0.02567362
Iteration 52, loss = 0.02532414
Iteration 53, loss = 0.02515733
Iteration 54, loss = 0.02520345
Iteration 55, loss = 0.02529394
Iteration 56, loss = 0.02494731
Iteration 57, loss = 0.02453170
Iteration 58, loss = 0.02447037
Iteration 59, loss = 0.02429645
Iteration 60, loss = 0.02410561
Iteration 61, loss = 0.02427955
Iteration 62, loss = 0.02409886
Iteration 63, loss = 0.02401986
Iteration 64, loss = 0.02428705
Iteration 65, loss = 0.02408139
Iteration 66, loss = 0.02378639
Iteration 67, loss = 0.02429497
Iteration 68, loss = 0.02385946
Iteration 69, loss = 0.02357695
Iteration 70, loss = 0.02353474
Iteration 71, loss = 0.02343912
Iteration 72, loss = 0.02329699
Iteration 73, loss = 0.02310995
Iteration 74, loss = 0.02319454
Iteration 75, loss = 0.02321208
Iteration 76, loss = 0.02314853
Iteration 77, loss = 0.02322051
Iteration 78, loss = 0.02334997
Iteration 79, loss = 0.02302576
Iteration 80, loss = 0.02307693
Iteration 81, loss = 0.02325468
Iteration 82, loss = 0.02309514
Iteration 83, loss = 0.02293952
Iteration 84, loss = 0.02329727
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68479916
Iteration 2, loss = 0.67007434
Iteration 3, loss = 0.65731184
Iteration 4, loss = 0.64447342
Iteration 5, loss = 0.63119805
Iteration 6, loss = 0.61679318
Iteration 7, loss = 0.60123461
Iteration 8, loss = 0.58440622
Iteration 9, loss = 0.56605408
Iteration 10, loss = 0.54648622
Iteration 11, loss = 0.52560459
Iteration 12, loss = 0.50339458
Iteration 13, loss = 0.48032607
Iteration 14, loss = 0.45637713
Iteration 15, loss = 0.43188504
Iteration 16, loss = 0.40740613
Iteration 17, loss = 0.38309456
Iteration 18, loss = 0.35940574
Iteration 19, loss = 0.33662673
Iteration 20, loss = 0.31468065
Iteration 21, loss = 0.29399894
Iteration 22, loss = 0.27470208
Iteration 23, loss = 0.25636494
Iteration 24, loss = 0.23937882
Iteration 25, loss = 0.22355349
Iteration 26, loss = 0.20882466
Iteration 27, loss = 0.19524545
Iteration 28, loss = 0.18263179
Iteration 29, loss = 0.17106038
Iteration 30, loss = 0.16034312
Iteration 31, loss = 0.15047849
Iteration 32, loss = 0.14141980
Iteration 33, loss = 0.13303954
Iteration 34, loss = 0.12536195
Iteration 35, loss = 0.11822382
Iteration 36, loss = 0.11164566
Iteration 37, loss = 0.10563274
Iteration 38, loss = 0.10001307
Iteration 39, loss = 0.09485814
Iteration 40, loss = 0.09004499
Iteration 41, loss = 0.08557988
Iteration 42, loss = 0.08143417
Iteration 43, loss = 0.07763576
Iteration 44, loss = 0.07404970
Iteration 45, loss = 0.07070079
Iteration 46, loss = 0.06763176
Iteration 47, loss = 0.06473902
Iteration 48, loss = 0.06200683
Iteration 49, loss = 0.05949301
Iteration 50, loss = 0.05711382
Iteration 51, loss = 0.05492945
Iteration 52, loss = 0.05282989
Iteration 53, loss = 0.05089420
Iteration 54, loss = 0.04903265
Iteration 55, loss = 0.04734267
Iteration 56, loss = 0.04567808
Iteration 57, loss = 0.04411721
Iteration 58, loss = 0.04265865
Iteration 59, loss = 0.04127127
Iteration 60, loss = 0.03997574
Iteration 61, loss = 0.03875310
Iteration 62, loss = 0.03755982
Iteration 63, loss = 0.03643352
Iteration 64, loss = 0.03537167
Iteration 65, loss = 0.03439880
Iteration 66, loss = 0.03347703
Iteration 67, loss = 0.03252749
Iteration 68, loss = 0.03166298
Iteration 69, loss = 0.03084630
Iteration 70, loss = 0.03006345
Iteration 71, loss = 0.02929783
Iteration 72, loss = 0.02858224
Iteration 73, loss = 0.02789558
Iteration 74, loss = 0.02723736
Iteration 75, loss = 0.02662473
Iteration 76, loss = 0.02603065
Iteration 77, loss = 0.02542421
Iteration 78, loss = 0.02485781
Iteration 79, loss = 0.02432407
Iteration 80, loss = 0.02380518
Iteration 81, loss = 0.02333496
Iteration 82, loss = 0.02283004
Iteration 83, loss = 0.02239809
Iteration 84, loss = 0.02196181
Iteration 85, loss = 0.02158343
Iteration 86, loss = 0.02111175
Iteration 87, loss = 0.02072250
Iteration 88, loss = 0.02036385
Iteration 89, loss = 0.01998399
Iteration 90, loss = 0.01963934
Iteration 91, loss = 0.01927503
Iteration 92, loss = 0.01894125
Iteration 93, loss = 0.01862998
Iteration 94, loss = 0.01836497
Iteration 95, loss = 0.01807771
Iteration 96, loss = 0.01777815
Iteration 97, loss = 0.01753975
Iteration 98, loss = 0.01726931
Iteration 99, loss = 0.01703626
Iteration 100, loss = 0.01678538
Iteration 101, loss = 0.01653830
Iteration 102, loss = 0.01630029
Iteration 103, loss = 0.01605062
Iteration 104, loss = 0.01582386
Iteration 105, loss = 0.01560670
Iteration 106, loss = 0.01536978
Iteration 107, loss = 0.01515170
Iteration 108, loss = 0.01494486
Iteration 109, loss = 0.01474612
Iteration 110, loss = 0.01450646
Iteration 111, loss = 0.01433968
Iteration 112, loss = 0.01416950
Iteration 113, loss = 0.01407539
Iteration 114, loss = 0.01392566
Iteration 115, loss = 0.01381905
Iteration 116, loss = 0.01360978
Iteration 117, loss = 0.01348682
Iteration 118, loss = 0.01328521
Iteration 119, loss = 0.01314450
Iteration 120, loss = 0.01297347
Iteration 121, loss = 0.01282001
Iteration 122, loss = 0.01267090
Iteration 123, loss = 0.01251982
Iteration 124, loss = 0.01239185
Iteration 125, loss = 0.01228104
Iteration 126, loss = 0.01217138
Iteration 127, loss = 0.01203911
Iteration 128, loss = 0.01196360
Iteration 129, loss = 0.01182799
Iteration 130, loss = 0.01174604
Iteration 131, loss = 0.01162809
Iteration 132, loss = 0.01153183
Iteration 133, loss = 0.01143177
Iteration 134, loss = 0.01133022
Iteration 135, loss = 0.01126222
Iteration 136, loss = 0.01111251
Iteration 137, loss = 0.01103292
Iteration 138, loss = 0.01090623
Iteration 139, loss = 0.01080040
Iteration 140, loss = 0.01077675
Iteration 141, loss = 0.01064636
Iteration 142, loss = 0.01054935
Iteration 143, loss = 0.01048130
Iteration 144, loss = 0.01038887
Iteration 145, loss = 0.01033599
Iteration 146, loss = 0.01027738
Iteration 147, loss = 0.01021303
Iteration 148, loss = 0.01015616
Iteration 149, loss = 0.01005443
Iteration 150, loss = 0.00998499
Iteration 151, loss = 0.00988392
Iteration 152, loss = 0.00980301
Iteration 153, loss = 0.00972962
Iteration 154, loss = 0.00965791
Iteration 155, loss = 0.00960377
Iteration 156, loss = 0.00954243
Iteration 157, loss = 0.00946547
Iteration 158, loss = 0.00942452
Iteration 159, loss = 0.00935504
Iteration 160, loss = 0.00931667
Iteration 161, loss = 0.00923934
Iteration 162, loss = 0.00918948
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67457234
Iteration 2, loss = 0.62706379
Iteration 3, loss = 0.55843694
Iteration 4, loss = 0.47535104
Iteration 5, loss = 0.39581627
Iteration 6, loss = 0.32468401
Iteration 7, loss = 0.26473890
Iteration 8, loss = 0.21594763
Iteration 9, loss = 0.17792224
Iteration 10, loss = 0.14843565
Iteration 11, loss = 0.12572871
Iteration 12, loss = 0.10796841
Iteration 13, loss = 0.09435898
Iteration 14, loss = 0.08308363
Iteration 15, loss = 0.07464817
Iteration 16, loss = 0.06761222
Iteration 17, loss = 0.06132198
Iteration 18, loss = 0.05643309
Iteration 19, loss = 0.05238211
Iteration 20, loss = 0.04900367
Iteration 21, loss = 0.04599274
Iteration 22, loss = 0.04353523
Iteration 23, loss = 0.04126944
Iteration 24, loss = 0.03945917
Iteration 25, loss = 0.03771125
Iteration 26, loss = 0.03648646
Iteration 27, loss = 0.03512785
Iteration 28, loss = 0.03371428
Iteration 29, loss = 0.03283792
Iteration 30, loss = 0.03196749
Iteration 31, loss = 0.03084792
Iteration 32, loss = 0.03007985
Iteration 33, loss = 0.02938793
Iteration 34, loss = 0.02876035
Iteration 35, loss = 0.02826840
Iteration 36, loss = 0.02777658
Iteration 37, loss = 0.02711978
Iteration 38, loss = 0.02670877
Iteration 39, loss = 0.02662890
Iteration 40, loss = 0.02615092
Iteration 41, loss = 0.02584455
Iteration 42, loss = 0.02547715
Iteration 43, loss = 0.02516428
Iteration 44, loss = 0.02509402
Iteration 45, loss = 0.02475823
Iteration 46, loss = 0.02439412
Iteration 47, loss = 0.02418216
Iteration 48, loss = 0.02393305
Iteration 49, loss = 0.02384095
Iteration 50, loss = 0.02362929
Iteration 51, loss = 0.02337631
Iteration 52, loss = 0.02340012
Iteration 53, loss = 0.02318821
Iteration 54, loss = 0.02313672
Iteration 55, loss = 0.02270674
Iteration 56, loss = 0.02249250
Iteration 57, loss = 0.02277413
Iteration 58, loss = 0.02261774
Iteration 59, loss = 0.02239029
Iteration 60, loss = 0.02216976
Iteration 61, loss = 0.02226181
Iteration 62, loss = 0.02210313
Iteration 63, loss = 0.02175432
Iteration 64, loss = 0.02220206
Iteration 65, loss = 0.02198213
Iteration 66, loss = 0.02201319
Iteration 67, loss = 0.02174849
Iteration 68, loss = 0.02178953
Iteration 69, loss = 0.02152621
Iteration 70, loss = 0.02144261
Iteration 71, loss = 0.02144683
Iteration 72, loss = 0.02128829
Iteration 73, loss = 0.02122223
Iteration 74, loss = 0.02099945
Iteration 75, loss = 0.02111996
Iteration 76, loss = 0.02109586
Iteration 77, loss = 0.02120073
Iteration 78, loss = 0.02081204
Iteration 79, loss = 0.02099615
Iteration 80, loss = 0.02088471
Iteration 81, loss = 0.02099501
Iteration 82, loss = 0.02103743
Iteration 83, loss = 0.02076383
Iteration 84, loss = 0.02077049
Iteration 85, loss = 0.02093986
Iteration 86, loss = 0.02100558
Iteration 87, loss = 0.02077142
Iteration 88, loss = 0.02082833
Iteration 89, loss = 0.02055332
Iteration 90, loss = 0.02040701
Iteration 91, loss = 0.02073599
Iteration 92, loss = 0.02081497
Iteration 93, loss = 0.02068484
Iteration 94, loss = 0.02056656
Iteration 95, loss = 0.02037133
Iteration 96, loss = 0.02060749
Iteration 97, loss = 0.02037267
Iteration 98, loss = 0.02030072
Iteration 99, loss = 0.02023316
Iteration 100, loss = 0.02013864
Iteration 101, loss = 0.02022918
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68147031
Iteration 2, loss = 0.65851161
Iteration 3, loss = 0.62519588
Iteration 4, loss = 0.57614074
Iteration 5, loss = 0.51692943
Iteration 6, loss = 0.45633446
Iteration 7, loss = 0.39772418
Iteration 8, loss = 0.34335530
Iteration 9, loss = 0.29463023
Iteration 10, loss = 0.25199061
Iteration 11, loss = 0.21565648
Iteration 12, loss = 0.18537884
Iteration 13, loss = 0.16016469
Iteration 14, loss = 0.13960115
Iteration 15, loss = 0.12243125
Iteration 16, loss = 0.10844744
Iteration 17, loss = 0.09672268
Iteration 18, loss = 0.08702637
Iteration 19, loss = 0.07890897
Iteration 20, loss = 0.07192760
Iteration 21, loss = 0.06612674
Iteration 22, loss = 0.06103665
Iteration 23, loss = 0.05668955
Iteration 24, loss = 0.05302220
Iteration 25, loss = 0.04959334
Iteration 26, loss = 0.04676104
Iteration 27, loss = 0.04406446
Iteration 28, loss = 0.04179495
Iteration 29, loss = 0.03985791
Iteration 30, loss = 0.03798389
Iteration 31, loss = 0.03634629
Iteration 32, loss = 0.03489865
Iteration 33, loss = 0.03356326
Iteration 34, loss = 0.03237765
Iteration 35, loss = 0.03123581
Iteration 36, loss = 0.03029658
Iteration 37, loss = 0.02931349
Iteration 38, loss = 0.02854269
Iteration 39, loss = 0.02768642
Iteration 40, loss = 0.02698711
Iteration 41, loss = 0.02634972
Iteration 42, loss = 0.02569128
Iteration 43, loss = 0.02518555
Iteration 44, loss = 0.02469969
Iteration 45, loss = 0.02419963
Iteration 46, loss = 0.02386818
Iteration 47, loss = 0.02329043
Iteration 48, loss = 0.02306076
Iteration 49, loss = 0.02251256
Iteration 50, loss = 0.02212249
Iteration 51, loss = 0.02189765
Iteration 52, loss = 0.02154218
Iteration 53, loss = 0.02135495
Iteration 54, loss = 0.02113619
Iteration 55, loss = 0.02074294
Iteration 56, loss = 0.02057036
Iteration 57, loss = 0.02024594
Iteration 58, loss = 0.02008972
Iteration 59, loss = 0.01985238
Iteration 60, loss = 0.01962908
Iteration 61, loss = 0.01956949
Iteration 62, loss = 0.01930736
Iteration 63, loss = 0.01924707
Iteration 64, loss = 0.01898147
Iteration 65, loss = 0.01885615
Iteration 66, loss = 0.01875838
Iteration 67, loss = 0.01854966
Iteration 68, loss = 0.01839983
Iteration 69, loss = 0.01827167
Iteration 70, loss = 0.01815927
Iteration 71, loss = 0.01802697
Iteration 72, loss = 0.01810123
Iteration 73, loss = 0.01791595
Iteration 74, loss = 0.01777155
Iteration 75, loss = 0.01774565
Iteration 76, loss = 0.01750526
Iteration 77, loss = 0.01753374
Iteration 78, loss = 0.01733641
Iteration 79, loss = 0.01740926
Iteration 80, loss = 0.01721979
Iteration 81, loss = 0.01713840
Iteration 82, loss = 0.01711076
Iteration 83, loss = 0.01696942
Iteration 84, loss = 0.01720629
Iteration 85, loss = 0.01689458
Iteration 86, loss = 0.01686767
Iteration 87, loss = 0.01667772
Iteration 88, loss = 0.01676080
Iteration 89, loss = 0.01663047
Iteration 90, loss = 0.01656283
Iteration 91, loss = 0.01644907
Iteration 92, loss = 0.01644034
Iteration 93, loss = 0.01639184
Iteration 94, loss = 0.01638189
Iteration 95, loss = 0.01629746
Iteration 96, loss = 0.01640017
Iteration 97, loss = 0.01619604
Iteration 98, loss = 0.01620125
Iteration 99, loss = 0.01608461
Iteration 100, loss = 0.01612895
Iteration 101, loss = 0.01605978
Iteration 102, loss = 0.01601873
Iteration 103, loss = 0.01604073
Iteration 104, loss = 0.01602855
Iteration 105, loss = 0.01595091
Iteration 106, loss = 0.01591126
Iteration 107, loss = 0.01588485
Iteration 108, loss = 0.01596119
Iteration 109, loss = 0.01584610
Iteration 110, loss = 0.01575868
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70309574
Iteration 2, loss = 0.67604567
Iteration 3, loss = 0.64519387
Iteration 4, loss = 0.60686767
Iteration 5, loss = 0.56229520
Iteration 6, loss = 0.51349062
Iteration 7, loss = 0.46311373
Iteration 8, loss = 0.41339826
Iteration 9, loss = 0.36618867
Iteration 10, loss = 0.32272675
Iteration 11, loss = 0.28384670
Iteration 12, loss = 0.24970187
Iteration 13, loss = 0.21996346
Iteration 14, loss = 0.19429379
Iteration 15, loss = 0.17248528
Iteration 16, loss = 0.15365232
Iteration 17, loss = 0.13781530
Iteration 18, loss = 0.12417122
Iteration 19, loss = 0.11248307
Iteration 20, loss = 0.10239756
Iteration 21, loss = 0.09392185
Iteration 22, loss = 0.08637134
Iteration 23, loss = 0.07977598
Iteration 24, loss = 0.07404762
Iteration 25, loss = 0.06903345
Iteration 26, loss = 0.06459975
Iteration 27, loss = 0.06062594
Iteration 28, loss = 0.05715131
Iteration 29, loss = 0.05390720
Iteration 30, loss = 0.05116860
Iteration 31, loss = 0.04868671
Iteration 32, loss = 0.04650714
Iteration 33, loss = 0.04435480
Iteration 34, loss = 0.04263719
Iteration 35, loss = 0.04093947
Iteration 36, loss = 0.03937096
Iteration 37, loss = 0.03781764
Iteration 38, loss = 0.03656518
Iteration 39, loss = 0.03534703
Iteration 40, loss = 0.03421080
Iteration 41, loss = 0.03321498
Iteration 42, loss = 0.03222864
Iteration 43, loss = 0.03133730
Iteration 44, loss = 0.03057603
Iteration 45, loss = 0.02982135
Iteration 46, loss = 0.02902677
Iteration 47, loss = 0.02825026
Iteration 48, loss = 0.02746884
Iteration 49, loss = 0.02711388
Iteration 50, loss = 0.02673950
Iteration 51, loss = 0.02625775
Iteration 52, loss = 0.02577293
Iteration 53, loss = 0.02526487
Iteration 54, loss = 0.02498244
Iteration 55, loss = 0.02448056
Iteration 56, loss = 0.02416304
Iteration 57, loss = 0.02372234
Iteration 58, loss = 0.02325560
Iteration 59, loss = 0.02299898
Iteration 60, loss = 0.02284239
Iteration 61, loss = 0.02251930
Iteration 62, loss = 0.02225958
Iteration 63, loss = 0.02203278
Iteration 64, loss = 0.02197389
Iteration 65, loss = 0.02173880
Iteration 66, loss = 0.02156455
Iteration 67, loss = 0.02132834
Iteration 68, loss = 0.02089208
Iteration 69, loss = 0.02055727
Iteration 70, loss = 0.02028379
Iteration 71, loss = 0.02012806
Iteration 72, loss = 0.01994079
Iteration 73, loss = 0.01965839
Iteration 74, loss = 0.01949526
Iteration 75, loss = 0.01937610
Iteration 76, loss = 0.01921757
Iteration 77, loss = 0.01910696
Iteration 78, loss = 0.01894463
Iteration 79, loss = 0.01912774
Iteration 80, loss = 0.01914729
Iteration 81, loss = 0.01910468
Iteration 82, loss = 0.01905272
Iteration 83, loss = 0.01889452
Iteration 84, loss = 0.01871676
Iteration 85, loss = 0.01847540
Iteration 86, loss = 0.01851266
Iteration 87, loss = 0.01805231
Iteration 88, loss = 0.01823485
Iteration 89, loss = 0.01801283
Iteration 90, loss = 0.01795618
Iteration 91, loss = 0.01777708
Iteration 92, loss = 0.01755768
Iteration 93, loss = 0.01775737
Iteration 94, loss = 0.01759868
Iteration 95, loss = 0.01748455
Iteration 96, loss = 0.01732936
Iteration 97, loss = 0.01711475
Iteration 98, loss = 0.01707502
Iteration 99, loss = 0.01700995
Iteration 100, loss = 0.01691043
Iteration 101, loss = 0.01684858
Iteration 102, loss = 0.01677172
Iteration 103, loss = 0.01657351
Iteration 104, loss = 0.01673291
Iteration 105, loss = 0.01670823
Iteration 106, loss = 0.01656064
Iteration 107, loss = 0.01648766
Iteration 108, loss = 0.01647868
Iteration 109, loss = 0.01654927
Iteration 110, loss = 0.01633331
Iteration 111, loss = 0.01632414
Iteration 112, loss = 0.01619586
Iteration 113, loss = 0.01641929
Iteration 114, loss = 0.01639697
Iteration 115, loss = 0.01628327
Iteration 116, loss = 0.01623105
Iteration 117, loss = 0.01613451
Iteration 118, loss = 0.01618359
Iteration 119, loss = 0.01632758
Iteration 120, loss = 0.01626732
Iteration 121, loss = 0.01609068
Iteration 122, loss = 0.01596635
Iteration 123, loss = 0.01606033
Iteration 124, loss = 0.01600656
Iteration 125, loss = 0.01594792
Iteration 126, loss = 0.01585622
Iteration 127, loss = 0.01573150
Iteration 128, loss = 0.01568690
Iteration 129, loss = 0.01561058
Iteration 130, loss = 0.01562616
Iteration 131, loss = 0.01569185
Iteration 132, loss = 0.01586082
Iteration 133, loss = 0.01593378
Iteration 134, loss = 0.01585740
Iteration 135, loss = 0.01578664
Iteration 136, loss = 0.01563097
Iteration 137, loss = 0.01558052
Iteration 138, loss = 0.01561271
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69110213
Iteration 2, loss = 0.67077647
Iteration 3, loss = 0.65199863
Iteration 4, loss = 0.63043774
Iteration 5, loss = 0.60559607
Iteration 6, loss = 0.57685523
Iteration 7, loss = 0.54480525
Iteration 8, loss = 0.51074804
Iteration 9, loss = 0.47604562
Iteration 10, loss = 0.44104490
Iteration 11, loss = 0.40728988
Iteration 12, loss = 0.37468649
Iteration 13, loss = 0.34404583
Iteration 14, loss = 0.31505612
Iteration 15, loss = 0.28817559
Iteration 16, loss = 0.26351354
Iteration 17, loss = 0.24088109
Iteration 18, loss = 0.22028187
Iteration 19, loss = 0.20170751
Iteration 20, loss = 0.18491839
Iteration 21, loss = 0.16980465
Iteration 22, loss = 0.15617175
Iteration 23, loss = 0.14403167
Iteration 24, loss = 0.13315559
Iteration 25, loss = 0.12326933
Iteration 26, loss = 0.11449774
Iteration 27, loss = 0.10657438
Iteration 28, loss = 0.09952470
Iteration 29, loss = 0.09301769
Iteration 30, loss = 0.08720510
Iteration 31, loss = 0.08195026
Iteration 32, loss = 0.07726257
Iteration 33, loss = 0.07284355
Iteration 34, loss = 0.06895862
Iteration 35, loss = 0.06538185
Iteration 36, loss = 0.06205443
Iteration 37, loss = 0.05901167
Iteration 38, loss = 0.05624749
Iteration 39, loss = 0.05372763
Iteration 40, loss = 0.05137715
Iteration 41, loss = 0.04925096
Iteration 42, loss = 0.04720870
Iteration 43, loss = 0.04539290
Iteration 44, loss = 0.04365043
Iteration 45, loss = 0.04204193
Iteration 46, loss = 0.04054392
Iteration 47, loss = 0.03914270
Iteration 48, loss = 0.03786135
Iteration 49, loss = 0.03667960
Iteration 50, loss = 0.03553048
Iteration 51, loss = 0.03442236
Iteration 52, loss = 0.03340380
Iteration 53, loss = 0.03246987
Iteration 54, loss = 0.03160787
Iteration 55, loss = 0.03073082
Iteration 56, loss = 0.02994289
Iteration 57, loss = 0.02922196
Iteration 58, loss = 0.02851202
Iteration 59, loss = 0.02789876
Iteration 60, loss = 0.02719255
Iteration 61, loss = 0.02663924
Iteration 62, loss = 0.02607767
Iteration 63, loss = 0.02546737
Iteration 64, loss = 0.02501428
Iteration 65, loss = 0.02455950
Iteration 66, loss = 0.02400550
Iteration 67, loss = 0.02360194
Iteration 68, loss = 0.02314983
Iteration 69, loss = 0.02278979
Iteration 70, loss = 0.02241087
Iteration 71, loss = 0.02201331
Iteration 72, loss = 0.02166498
Iteration 73, loss = 0.02135523
Iteration 74, loss = 0.02101713
Iteration 75, loss = 0.02076651
Iteration 76, loss = 0.02043322
Iteration 77, loss = 0.02018106
Iteration 78, loss = 0.01985486
Iteration 79, loss = 0.01963175
Iteration 80, loss = 0.01938165
Iteration 81, loss = 0.01914311
Iteration 82, loss = 0.01888479
Iteration 83, loss = 0.01864156
Iteration 84, loss = 0.01840673
Iteration 85, loss = 0.01826014
Iteration 86, loss = 0.01804994
Iteration 87, loss = 0.01782033
Iteration 88, loss = 0.01764616
Iteration 89, loss = 0.01748471
Iteration 90, loss = 0.01731229
Iteration 91, loss = 0.01715955
Iteration 92, loss = 0.01694215
Iteration 93, loss = 0.01683566
Iteration 94, loss = 0.01667552
Iteration 95, loss = 0.01651211
Iteration 96, loss = 0.01636099
Iteration 97, loss = 0.01630380
Iteration 98, loss = 0.01608042
Iteration 99, loss = 0.01597744
Iteration 100, loss = 0.01590038
Iteration 101, loss = 0.01576574
Iteration 102, loss = 0.01564524
Iteration 103, loss = 0.01551664
Iteration 104, loss = 0.01542118
Iteration 105, loss = 0.01534171
Iteration 106, loss = 0.01519516
Iteration 107, loss = 0.01512712
Iteration 108, loss = 0.01503997
Iteration 109, loss = 0.01489096
Iteration 110, loss = 0.01476953
Iteration 111, loss = 0.01475710
Iteration 112, loss = 0.01461603
Iteration 113, loss = 0.01456450
Iteration 114, loss = 0.01444482
Iteration 115, loss = 0.01438605
Iteration 116, loss = 0.01435535
Iteration 117, loss = 0.01430335
Iteration 118, loss = 0.01420032
Iteration 119, loss = 0.01410430
Iteration 120, loss = 0.01402654
Iteration 121, loss = 0.01399161
Iteration 122, loss = 0.01395103
Iteration 123, loss = 0.01382160
Iteration 124, loss = 0.01377107
Iteration 125, loss = 0.01369881
Iteration 126, loss = 0.01368076
Iteration 127, loss = 0.01356830
Iteration 128, loss = 0.01355397
Iteration 129, loss = 0.01346822
Iteration 130, loss = 0.01345451
Iteration 131, loss = 0.01339241
Iteration 132, loss = 0.01334998
Iteration 133, loss = 0.01326406
Iteration 134, loss = 0.01324171
Iteration 135, loss = 0.01316832
Iteration 136, loss = 0.01318884
Iteration 137, loss = 0.01308936
Iteration 138, loss = 0.01309555
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68027204
Iteration 2, loss = 0.63150785
Iteration 3, loss = 0.55218092
Iteration 4, loss = 0.45925229
Iteration 5, loss = 0.36995769
Iteration 6, loss = 0.29366134
Iteration 7, loss = 0.23291712
Iteration 8, loss = 0.18633472
Iteration 9, loss = 0.15208226
Iteration 10, loss = 0.12635437
Iteration 11, loss = 0.10691242
Iteration 12, loss = 0.09212412
Iteration 13, loss = 0.08059698
Iteration 14, loss = 0.07151066
Iteration 15, loss = 0.06429828
Iteration 16, loss = 0.05859077
Iteration 17, loss = 0.05368843
Iteration 18, loss = 0.04970931
Iteration 19, loss = 0.04638604
Iteration 20, loss = 0.04343466
Iteration 21, loss = 0.04108156
Iteration 22, loss = 0.03894548
Iteration 23, loss = 0.03701625
Iteration 24, loss = 0.03537741
Iteration 25, loss = 0.03406572
Iteration 26, loss = 0.03288336
Iteration 27, loss = 0.03167476
Iteration 28, loss = 0.03082929
Iteration 29, loss = 0.02989477
Iteration 30, loss = 0.02923149
Iteration 31, loss = 0.02885127
Iteration 32, loss = 0.02787259
Iteration 33, loss = 0.02719553
Iteration 34, loss = 0.02686371
Iteration 35, loss = 0.02618295
Iteration 36, loss = 0.02571652
Iteration 37, loss = 0.02537038
Iteration 38, loss = 0.02495548
Iteration 39, loss = 0.02472540
Iteration 40, loss = 0.02435084
Iteration 41, loss = 0.02388254
Iteration 42, loss = 0.02377596
Iteration 43, loss = 0.02337045
Iteration 44, loss = 0.02309951
Iteration 45, loss = 0.02288033
Iteration 46, loss = 0.02271270
Iteration 47, loss = 0.02260513
Iteration 48, loss = 0.02241630
Iteration 49, loss = 0.02247042
Iteration 50, loss = 0.02194577
Iteration 51, loss = 0.02190401
Iteration 52, loss = 0.02188225
Iteration 53, loss = 0.02179519
Iteration 54, loss = 0.02141541
Iteration 55, loss = 0.02155235
Iteration 56, loss = 0.02148577
Iteration 57, loss = 0.02106441
Iteration 58, loss = 0.02126962
Iteration 59, loss = 0.02116229
Iteration 60, loss = 0.02098410
Iteration 61, loss = 0.02077793
Iteration 62, loss = 0.02084643
Iteration 63, loss = 0.02075842
Iteration 64, loss = 0.02039224
Iteration 65, loss = 0.02048433
Iteration 66, loss = 0.02050605
Iteration 67, loss = 0.02055297
Iteration 68, loss = 0.02058042
Iteration 69, loss = 0.02017624
Iteration 70, loss = 0.02048131
Iteration 71, loss = 0.02030012
Iteration 72, loss = 0.02016515
Iteration 73, loss = 0.02009961
Iteration 74, loss = 0.02025950
Iteration 75, loss = 0.01995000
Iteration 76, loss = 0.01996379
Iteration 77, loss = 0.01986300
Iteration 78, loss = 0.01997390
Iteration 79, loss = 0.01989027
Iteration 80, loss = 0.01973594
Iteration 81, loss = 0.01980073
Iteration 82, loss = 0.01975808
Iteration 83, loss = 0.01986936
Iteration 84, loss = 0.01985303
Iteration 85, loss = 0.01993564
Iteration 86, loss = 0.01993959
Iteration 87, loss = 0.01974312
Iteration 88, loss = 0.01955574
Iteration 89, loss = 0.01955657
Iteration 90, loss = 0.01960315
Iteration 91, loss = 0.01965644
Iteration 92, loss = 0.01964890
Iteration 93, loss = 0.01930994
Iteration 94, loss = 0.01948232
Iteration 95, loss = 0.01953015
Iteration 96, loss = 0.01939031
Iteration 97, loss = 0.01927218
Iteration 98, loss = 0.01959290
Iteration 99, loss = 0.01937208
Iteration 100, loss = 0.01948513
Iteration 101, loss = 0.01953443
Iteration 102, loss = 0.01944641
Iteration 103, loss = 0.01952120
Iteration 104, loss = 0.01938978
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69463099
Iteration 2, loss = 0.63651227
Iteration 3, loss = 0.55550380
Iteration 4, loss = 0.45634223
Iteration 5, loss = 0.35971936
Iteration 6, loss = 0.28013080
Iteration 7, loss = 0.21907072
Iteration 8, loss = 0.17385568
Iteration 9, loss = 0.14069088
Iteration 10, loss = 0.11691224
Iteration 11, loss = 0.09948103
Iteration 12, loss = 0.08637369
Iteration 13, loss = 0.07617539
Iteration 14, loss = 0.06839176
Iteration 15, loss = 0.06191940
Iteration 16, loss = 0.05678980
Iteration 17, loss = 0.05293664
Iteration 18, loss = 0.04931180
Iteration 19, loss = 0.04604917
Iteration 20, loss = 0.04362005
Iteration 21, loss = 0.04168319
Iteration 22, loss = 0.03957671
Iteration 23, loss = 0.03821373
Iteration 24, loss = 0.03628855
Iteration 25, loss = 0.03591790
Iteration 26, loss = 0.03471959
Iteration 27, loss = 0.03382856
Iteration 28, loss = 0.03246991
Iteration 29, loss = 0.03201867
Iteration 30, loss = 0.03121674
Iteration 31, loss = 0.03044629
Iteration 32, loss = 0.02974988
Iteration 33, loss = 0.02972026
Iteration 34, loss = 0.02896776
Iteration 35, loss = 0.02844444
Iteration 36, loss = 0.02789235
Iteration 37, loss = 0.02780472
Iteration 38, loss = 0.02731612
Iteration 39, loss = 0.02758367
Iteration 40, loss = 0.02711792
Iteration 41, loss = 0.02654909
Iteration 42, loss = 0.02636214
Iteration 43, loss = 0.02595050
Iteration 44, loss = 0.02585698
Iteration 45, loss = 0.02551194
Iteration 46, loss = 0.02523278
Iteration 47, loss = 0.02504569
Iteration 48, loss = 0.02474370
Iteration 49, loss = 0.02448269
Iteration 50, loss = 0.02447461
Iteration 51, loss = 0.02428677
Iteration 52, loss = 0.02426912
Iteration 53, loss = 0.02432586
Iteration 54, loss = 0.02426052
Iteration 55, loss = 0.02390932
Iteration 56, loss = 0.02475329
Iteration 57, loss = 0.02450713
Iteration 58, loss = 0.02462606
Iteration 59, loss = 0.02431383
Iteration 60, loss = 0.02414903
Iteration 61, loss = 0.02413923
Iteration 62, loss = 0.02434950
Iteration 63, loss = 0.02396378
Iteration 64, loss = 0.02351613
Iteration 65, loss = 0.02327356
Iteration 66, loss = 0.02317821
Iteration 67, loss = 0.02298494
Iteration 68, loss = 0.02299766
Iteration 69, loss = 0.02273578
Iteration 70, loss = 0.02281322
Iteration 71, loss = 0.02294187
Iteration 72, loss = 0.02377278
Iteration 73, loss = 0.02397416
Iteration 74, loss = 0.02404499
Iteration 75, loss = 0.02391008
Iteration 76, loss = 0.02357577
Iteration 77, loss = 0.02306506
Iteration 78, loss = 0.02283176
Iteration 79, loss = 0.02338168
Iteration 80, loss = 0.02306117
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68767924
Iteration 2, loss = 0.66299499
Iteration 3, loss = 0.63479598
Iteration 4, loss = 0.59809352
Iteration 5, loss = 0.55206655
Iteration 6, loss = 0.49975742
Iteration 7, loss = 0.44641485
Iteration 8, loss = 0.39569829
Iteration 9, loss = 0.34879020
Iteration 10, loss = 0.30648326
Iteration 11, loss = 0.26902936
Iteration 12, loss = 0.23644232
Iteration 13, loss = 0.20798473
Iteration 14, loss = 0.18390621
Iteration 15, loss = 0.16326197
Iteration 16, loss = 0.14573296
Iteration 17, loss = 0.13074077
Iteration 18, loss = 0.11799104
Iteration 19, loss = 0.10713084
Iteration 20, loss = 0.09775325
Iteration 21, loss = 0.08962905
Iteration 22, loss = 0.08280288
Iteration 23, loss = 0.07672968
Iteration 24, loss = 0.07143803
Iteration 25, loss = 0.06682951
Iteration 26, loss = 0.06259735
Iteration 27, loss = 0.05889943
Iteration 28, loss = 0.05561298
Iteration 29, loss = 0.05265304
Iteration 30, loss = 0.05009671
Iteration 31, loss = 0.04768423
Iteration 32, loss = 0.04559174
Iteration 33, loss = 0.04372762
Iteration 34, loss = 0.04192922
Iteration 35, loss = 0.04031015
Iteration 36, loss = 0.03888704
Iteration 37, loss = 0.03747715
Iteration 38, loss = 0.03636935
Iteration 39, loss = 0.03503895
Iteration 40, loss = 0.03415419
Iteration 41, loss = 0.03306193
Iteration 42, loss = 0.03210287
Iteration 43, loss = 0.03119413
Iteration 44, loss = 0.03071895
Iteration 45, loss = 0.02985257
Iteration 46, loss = 0.02924023
Iteration 47, loss = 0.02859741
Iteration 48, loss = 0.02797104
Iteration 49, loss = 0.02736982
Iteration 50, loss = 0.02686457
Iteration 51, loss = 0.02631960
Iteration 52, loss = 0.02571932
Iteration 53, loss = 0.02531092
Iteration 54, loss = 0.02484618
Iteration 55, loss = 0.02449641
Iteration 56, loss = 0.02411884
Iteration 57, loss = 0.02374817
Iteration 58, loss = 0.02356410
Iteration 59, loss = 0.02348918
Iteration 60, loss = 0.02308140
Iteration 61, loss = 0.02291110
Iteration 62, loss = 0.02255055
Iteration 63, loss = 0.02219350
Iteration 64, loss = 0.02190718
Iteration 65, loss = 0.02178227
Iteration 66, loss = 0.02151801
Iteration 67, loss = 0.02125509
Iteration 68, loss = 0.02106209
Iteration 69, loss = 0.02081113
Iteration 70, loss = 0.02055926
Iteration 71, loss = 0.02048100
Iteration 72, loss = 0.02031358
Iteration 73, loss = 0.02008233
Iteration 74, loss = 0.01997268
Iteration 75, loss = 0.01987558
Iteration 76, loss = 0.01961354
Iteration 77, loss = 0.01947141
Iteration 78, loss = 0.01933152
Iteration 79, loss = 0.01917324
Iteration 80, loss = 0.01903781
Iteration 81, loss = 0.01914244
Iteration 82, loss = 0.01887332
Iteration 83, loss = 0.01883746
Iteration 84, loss = 0.01913847
Iteration 85, loss = 0.01906716
Iteration 86, loss = 0.01893590
Iteration 87, loss = 0.01866643
Iteration 88, loss = 0.01853515
Iteration 89, loss = 0.01837740
Iteration 90, loss = 0.01817379
Iteration 91, loss = 0.01805651
Iteration 92, loss = 0.01795746
Iteration 93, loss = 0.01789892
Iteration 94, loss = 0.01788426
Iteration 95, loss = 0.01795181
Iteration 96, loss = 0.01804935
Iteration 97, loss = 0.01790947
Iteration 98, loss = 0.01781912
Iteration 99, loss = 0.01772621
Iteration 100, loss = 0.01772414
Iteration 101, loss = 0.01757513
Iteration 102, loss = 0.01764715
Iteration 103, loss = 0.01758309
Iteration 104, loss = 0.01767061
Iteration 105, loss = 0.01754574
Iteration 106, loss = 0.01775886
Iteration 107, loss = 0.01791302
Iteration 108, loss = 0.01758539
Iteration 109, loss = 0.01721853
Iteration 110, loss = 0.01712578
Iteration 111, loss = 0.01687758
Iteration 112, loss = 0.01684392
Iteration 113, loss = 0.01701258
Iteration 114, loss = 0.01722572
Iteration 115, loss = 0.01720444
Iteration 116, loss = 0.01707815
Iteration 117, loss = 0.01691730
Iteration 118, loss = 0.01666068
Iteration 119, loss = 0.01664061
Iteration 120, loss = 0.01703159
Iteration 121, loss = 0.01714316
Iteration 122, loss = 0.01709689
Iteration 123, loss = 0.01680753
Iteration 124, loss = 0.01653705
Iteration 125, loss = 0.01648314
Iteration 126, loss = 0.01635294
Iteration 127, loss = 0.01642101
Iteration 128, loss = 0.01638817
Iteration 129, loss = 0.01630538
Iteration 130, loss = 0.01629068
Iteration 131, loss = 0.01583488
Iteration 132, loss = 0.01658002
Iteration 133, loss = 0.01698479
Iteration 134, loss = 0.01686234
Iteration 135, loss = 0.01657603
Iteration 136, loss = 0.01628553
Iteration 137, loss = 0.01615489
Iteration 138, loss = 0.01619659
Iteration 139, loss = 0.01619226
Iteration 140, loss = 0.01604447
Iteration 141, loss = 0.01598052
Iteration 142, loss = 0.01590153
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68590011
Iteration 2, loss = 0.65614951
Iteration 3, loss = 0.61692528
Iteration 4, loss = 0.56365371
Iteration 5, loss = 0.50224528
Iteration 6, loss = 0.44086571
Iteration 7, loss = 0.38218079
Iteration 8, loss = 0.32900870
Iteration 9, loss = 0.28185496
Iteration 10, loss = 0.24142314
Iteration 11, loss = 0.20725219
Iteration 12, loss = 0.17888421
Iteration 13, loss = 0.15555693
Iteration 14, loss = 0.13647841
Iteration 15, loss = 0.12041697
Iteration 16, loss = 0.10743525
Iteration 17, loss = 0.09655783
Iteration 18, loss = 0.08743240
Iteration 19, loss = 0.07982058
Iteration 20, loss = 0.07326326
Iteration 21, loss = 0.06772817
Iteration 22, loss = 0.06301932
Iteration 23, loss = 0.05894463
Iteration 24, loss = 0.05525300
Iteration 25, loss = 0.05205830
Iteration 26, loss = 0.04926042
Iteration 27, loss = 0.04678781
Iteration 28, loss = 0.04464485
Iteration 29, loss = 0.04271179
Iteration 30, loss = 0.04089473
Iteration 31, loss = 0.03937305
Iteration 32, loss = 0.03801976
Iteration 33, loss = 0.03674902
Iteration 34, loss = 0.03556643
Iteration 35, loss = 0.03454300
Iteration 36, loss = 0.03348485
Iteration 37, loss = 0.03282302
Iteration 38, loss = 0.03185167
Iteration 39, loss = 0.03107689
Iteration 40, loss = 0.03048846
Iteration 41, loss = 0.02984693
Iteration 42, loss = 0.02932585
Iteration 43, loss = 0.02865464
Iteration 44, loss = 0.02822423
Iteration 45, loss = 0.02779285
Iteration 46, loss = 0.02733404
Iteration 47, loss = 0.02693115
Iteration 48, loss = 0.02652086
Iteration 49, loss = 0.02612033
Iteration 50, loss = 0.02568679
Iteration 51, loss = 0.02546432
Iteration 52, loss = 0.02511229
Iteration 53, loss = 0.02482895
Iteration 54, loss = 0.02461808
Iteration 55, loss = 0.02424958
Iteration 56, loss = 0.02413608
Iteration 57, loss = 0.02397857
Iteration 58, loss = 0.02369147
Iteration 59, loss = 0.02341926
Iteration 60, loss = 0.02333526
Iteration 61, loss = 0.02321378
Iteration 62, loss = 0.02290754
Iteration 63, loss = 0.02287910
Iteration 64, loss = 0.02265593
Iteration 65, loss = 0.02244509
Iteration 66, loss = 0.02225473
Iteration 67, loss = 0.02218923
Iteration 68, loss = 0.02219365
Iteration 69, loss = 0.02209270
Iteration 70, loss = 0.02207512
Iteration 71, loss = 0.02184401
Iteration 72, loss = 0.02170204
Iteration 73, loss = 0.02170843
Iteration 74, loss = 0.02136583
Iteration 75, loss = 0.02130920
Iteration 76, loss = 0.02132710
Iteration 77, loss = 0.02125446
Iteration 78, loss = 0.02108409
Iteration 79, loss = 0.02103863
Iteration 80, loss = 0.02093480
Iteration 81, loss = 0.02107877
Iteration 82, loss = 0.02074345
Iteration 83, loss = 0.02072202
Iteration 84, loss = 0.02064332
Iteration 85, loss = 0.02051198
Iteration 86, loss = 0.02054399
Iteration 87, loss = 0.02068645
Iteration 88, loss = 0.02039992
Iteration 89, loss = 0.02033320
Iteration 90, loss = 0.02034642
Iteration 91, loss = 0.02031279
Iteration 92, loss = 0.02021654
Iteration 93, loss = 0.02029144
Iteration 94, loss = 0.02008465
Iteration 95, loss = 0.02006498
Iteration 96, loss = 0.02009981
Iteration 97, loss = 0.02010544
Iteration 98, loss = 0.01985967
Iteration 99, loss = 0.02013359
Iteration 100, loss = 0.01994907
Iteration 101, loss = 0.01990932
Iteration 102, loss = 0.01976706
Iteration 103, loss = 0.01996774
Iteration 104, loss = 0.01981166
Iteration 105, loss = 0.01966073
Iteration 106, loss = 0.01968220
Iteration 107, loss = 0.01951899
Iteration 108, loss = 0.01959554
Iteration 109, loss = 0.01953492
Iteration 110, loss = 0.01949448
Iteration 111, loss = 0.01949093
Iteration 112, loss = 0.01947208
Iteration 113, loss = 0.01943286
Iteration 114, loss = 0.01934888
Iteration 115, loss = 0.01942680
Iteration 116, loss = 0.01940514
Iteration 117, loss = 0.01944022
Iteration 118, loss = 0.01939347
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67707673
Iteration 2, loss = 0.66454452
Iteration 3, loss = 0.65350325
Iteration 4, loss = 0.64218772
Iteration 5, loss = 0.63019844
Iteration 6, loss = 0.61719877
Iteration 7, loss = 0.60294781
Iteration 8, loss = 0.58725923
Iteration 9, loss = 0.57027197
Iteration 10, loss = 0.55194355
Iteration 11, loss = 0.53228948
Iteration 12, loss = 0.51168544
Iteration 13, loss = 0.49011925
Iteration 14, loss = 0.46803341
Iteration 15, loss = 0.44567004
Iteration 16, loss = 0.42311601
Iteration 17, loss = 0.40081424
Iteration 18, loss = 0.37897031
Iteration 19, loss = 0.35788649
Iteration 20, loss = 0.33735809
Iteration 21, loss = 0.31752698
Iteration 22, loss = 0.29888925
Iteration 23, loss = 0.28097961
Iteration 24, loss = 0.26403400
Iteration 25, loss = 0.24802056
Iteration 26, loss = 0.23293370
Iteration 27, loss = 0.21886448
Iteration 28, loss = 0.20555459
Iteration 29, loss = 0.19319823
Iteration 30, loss = 0.18166845
Iteration 31, loss = 0.17089049
Iteration 32, loss = 0.16087250
Iteration 33, loss = 0.15163287
Iteration 34, loss = 0.14303641
Iteration 35, loss = 0.13488705
Iteration 36, loss = 0.12757184
Iteration 37, loss = 0.12076318
Iteration 38, loss = 0.11429198
Iteration 39, loss = 0.10843768
Iteration 40, loss = 0.10293286
Iteration 41, loss = 0.09784727
Iteration 42, loss = 0.09312942
Iteration 43, loss = 0.08874266
Iteration 44, loss = 0.08461481
Iteration 45, loss = 0.08092777
Iteration 46, loss = 0.07732828
Iteration 47, loss = 0.07401458
Iteration 48, loss = 0.07096190
Iteration 49, loss = 0.06806632
Iteration 50, loss = 0.06541311
Iteration 51, loss = 0.06283712
Iteration 52, loss = 0.06048933
Iteration 53, loss = 0.05826263
Iteration 54, loss = 0.05617458
Iteration 55, loss = 0.05417631
Iteration 56, loss = 0.05231634
Iteration 57, loss = 0.05055192
Iteration 58, loss = 0.04886819
Iteration 59, loss = 0.04730189
Iteration 60, loss = 0.04584863
Iteration 61, loss = 0.04440521
Iteration 62, loss = 0.04303947
Iteration 63, loss = 0.04176952
Iteration 64, loss = 0.04055693
Iteration 65, loss = 0.03941074
Iteration 66, loss = 0.03834883
Iteration 67, loss = 0.03735731
Iteration 68, loss = 0.03633557
Iteration 69, loss = 0.03538012
Iteration 70, loss = 0.03449092
Iteration 71, loss = 0.03364626
Iteration 72, loss = 0.03283367
Iteration 73, loss = 0.03205725
Iteration 74, loss = 0.03131002
Iteration 75, loss = 0.03060885
Iteration 76, loss = 0.02990377
Iteration 77, loss = 0.02925920
Iteration 78, loss = 0.02861244
Iteration 79, loss = 0.02800328
Iteration 80, loss = 0.02743151
Iteration 81, loss = 0.02693649
Iteration 82, loss = 0.02634892
Iteration 83, loss = 0.02582116
Iteration 84, loss = 0.02529237
Iteration 85, loss = 0.02483497
Iteration 86, loss = 0.02437642
Iteration 87, loss = 0.02396230
Iteration 88, loss = 0.02355731
Iteration 89, loss = 0.02310819
Iteration 90, loss = 0.02275690
Iteration 91, loss = 0.02233754
Iteration 92, loss = 0.02194882
Iteration 93, loss = 0.02158006
Iteration 94, loss = 0.02124045
Iteration 95, loss = 0.02090871
Iteration 96, loss = 0.02059152
Iteration 97, loss = 0.02026918
Iteration 98, loss = 0.01997987
Iteration 99, loss = 0.01968290
Iteration 100, loss = 0.01939595
Iteration 101, loss = 0.01912303
Iteration 102, loss = 0.01886951
Iteration 103, loss = 0.01861266
Iteration 104, loss = 0.01835106
Iteration 105, loss = 0.01810293
Iteration 106, loss = 0.01785819
Iteration 107, loss = 0.01763201
Iteration 108, loss = 0.01740220
Iteration 109, loss = 0.01721335
Iteration 110, loss = 0.01696998
Iteration 111, loss = 0.01677094
Iteration 112, loss = 0.01662649
Iteration 113, loss = 0.01638802
Iteration 114, loss = 0.01618712
Iteration 115, loss = 0.01604763
Iteration 116, loss = 0.01587076
Iteration 117, loss = 0.01565447
Iteration 118, loss = 0.01547985
Iteration 119, loss = 0.01533655
Iteration 120, loss = 0.01515643
Iteration 121, loss = 0.01500331
Iteration 122, loss = 0.01484575
Iteration 123, loss = 0.01471236
Iteration 124, loss = 0.01454670
Iteration 125, loss = 0.01440441
Iteration 126, loss = 0.01431480
Iteration 127, loss = 0.01415028
Iteration 128, loss = 0.01405039
Iteration 129, loss = 0.01392265
Iteration 130, loss = 0.01374743
Iteration 131, loss = 0.01363913
Iteration 132, loss = 0.01349655
Iteration 133, loss = 0.01338173
Iteration 134, loss = 0.01327076
Iteration 135, loss = 0.01315481
Iteration 136, loss = 0.01305526
Iteration 137, loss = 0.01294725
Iteration 138, loss = 0.01285418
Iteration 139, loss = 0.01275075
Iteration 140, loss = 0.01263962
Iteration 141, loss = 0.01255234
Iteration 142, loss = 0.01246067
Iteration 143, loss = 0.01235671
Iteration 144, loss = 0.01224701
Iteration 145, loss = 0.01216154
Iteration 146, loss = 0.01206192
Iteration 147, loss = 0.01197562
Iteration 148, loss = 0.01188844
Iteration 149, loss = 0.01181889
Iteration 150, loss = 0.01172184
Iteration 151, loss = 0.01163574
Iteration 152, loss = 0.01155629
Iteration 153, loss = 0.01148768
Iteration 154, loss = 0.01140764
Iteration 155, loss = 0.01135194
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70753672
Iteration 2, loss = 0.68782318
Iteration 3, loss = 0.66911367
Iteration 4, loss = 0.64727198
Iteration 5, loss = 0.62132424
Iteration 6, loss = 0.59129021
Iteration 7, loss = 0.55755633
Iteration 8, loss = 0.52177301
Iteration 9, loss = 0.48455039
Iteration 10, loss = 0.44739358
Iteration 11, loss = 0.41091576
Iteration 12, loss = 0.37615665
Iteration 13, loss = 0.34320144
Iteration 14, loss = 0.31278234
Iteration 15, loss = 0.28455959
Iteration 16, loss = 0.25872736
Iteration 17, loss = 0.23537105
Iteration 18, loss = 0.21430055
Iteration 19, loss = 0.19546609
Iteration 20, loss = 0.17852841
Iteration 21, loss = 0.16341662
Iteration 22, loss = 0.14998114
Iteration 23, loss = 0.13794970
Iteration 24, loss = 0.12723728
Iteration 25, loss = 0.11778761
Iteration 26, loss = 0.10930453
Iteration 27, loss = 0.10165996
Iteration 28, loss = 0.09490516
Iteration 29, loss = 0.08870593
Iteration 30, loss = 0.08322224
Iteration 31, loss = 0.07827764
Iteration 32, loss = 0.07375265
Iteration 33, loss = 0.06966459
Iteration 34, loss = 0.06594810
Iteration 35, loss = 0.06254445
Iteration 36, loss = 0.05947021
Iteration 37, loss = 0.05661027
Iteration 38, loss = 0.05401235
Iteration 39, loss = 0.05165299
Iteration 40, loss = 0.04952267
Iteration 41, loss = 0.04748260
Iteration 42, loss = 0.04566405
Iteration 43, loss = 0.04394005
Iteration 44, loss = 0.04218841
Iteration 45, loss = 0.04066489
Iteration 46, loss = 0.03929131
Iteration 47, loss = 0.03794307
Iteration 48, loss = 0.03672114
Iteration 49, loss = 0.03560874
Iteration 50, loss = 0.03453328
Iteration 51, loss = 0.03347686
Iteration 52, loss = 0.03254328
Iteration 53, loss = 0.03167884
Iteration 54, loss = 0.03079788
Iteration 55, loss = 0.03001135
Iteration 56, loss = 0.02932159
Iteration 57, loss = 0.02861666
Iteration 58, loss = 0.02791208
Iteration 59, loss = 0.02728802
Iteration 60, loss = 0.02667020
Iteration 61, loss = 0.02609964
Iteration 62, loss = 0.02556134
Iteration 63, loss = 0.02506196
Iteration 64, loss = 0.02460957
Iteration 65, loss = 0.02413427
Iteration 66, loss = 0.02364428
Iteration 67, loss = 0.02325568
Iteration 68, loss = 0.02284141
Iteration 69, loss = 0.02249179
Iteration 70, loss = 0.02216587
Iteration 71, loss = 0.02172077
Iteration 72, loss = 0.02142708
Iteration 73, loss = 0.02109008
Iteration 74, loss = 0.02077696
Iteration 75, loss = 0.02047466
Iteration 76, loss = 0.02016827
Iteration 77, loss = 0.01991800
Iteration 78, loss = 0.01968888
Iteration 79, loss = 0.01939551
Iteration 80, loss = 0.01917054
Iteration 81, loss = 0.01895463
Iteration 82, loss = 0.01873542
Iteration 83, loss = 0.01844086
Iteration 84, loss = 0.01825608
Iteration 85, loss = 0.01812058
Iteration 86, loss = 0.01784180
Iteration 87, loss = 0.01766421
Iteration 88, loss = 0.01759841
Iteration 89, loss = 0.01741977
Iteration 90, loss = 0.01719155
Iteration 91, loss = 0.01699499
Iteration 92, loss = 0.01687145
Iteration 93, loss = 0.01668391
Iteration 94, loss = 0.01657196
Iteration 95, loss = 0.01640514
Iteration 96, loss = 0.01625491
Iteration 97, loss = 0.01621552
Iteration 98, loss = 0.01601413
Iteration 99, loss = 0.01589177
Iteration 100, loss = 0.01577010
Iteration 101, loss = 0.01574279
Iteration 102, loss = 0.01557402
Iteration 103, loss = 0.01546426
Iteration 104, loss = 0.01529514
Iteration 105, loss = 0.01523405
Iteration 106, loss = 0.01515349
Iteration 107, loss = 0.01503248
Iteration 108, loss = 0.01491334
Iteration 109, loss = 0.01479874
Iteration 110, loss = 0.01474112
Iteration 111, loss = 0.01462722
Iteration 112, loss = 0.01466938
Iteration 113, loss = 0.01449537
Iteration 114, loss = 0.01439984
Iteration 115, loss = 0.01441761
Iteration 116, loss = 0.01428585
Iteration 117, loss = 0.01414746
Iteration 118, loss = 0.01406871
Iteration 119, loss = 0.01406104
Iteration 120, loss = 0.01393968
Iteration 121, loss = 0.01389165
Iteration 122, loss = 0.01389370
Iteration 123, loss = 0.01373152
Iteration 124, loss = 0.01366376
Iteration 125, loss = 0.01360171
Iteration 126, loss = 0.01364233
Iteration 127, loss = 0.01354089
Iteration 128, loss = 0.01344255
Iteration 129, loss = 0.01337897
Iteration 130, loss = 0.01335743
Iteration 131, loss = 0.01330809
Iteration 132, loss = 0.01325271
Iteration 133, loss = 0.01322375
Iteration 134, loss = 0.01312780
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71955993
Iteration 2, loss = 0.66338822
Iteration 3, loss = 0.58826127
Iteration 4, loss = 0.49686494
Iteration 5, loss = 0.40519047
Iteration 6, loss = 0.32347384
Iteration 7, loss = 0.25742586
Iteration 8, loss = 0.20580783
Iteration 9, loss = 0.16754042
Iteration 10, loss = 0.13894778
Iteration 11, loss = 0.11734226
Iteration 12, loss = 0.10099130
Iteration 13, loss = 0.08854456
Iteration 14, loss = 0.07889713
Iteration 15, loss = 0.07077061
Iteration 16, loss = 0.06478465
Iteration 17, loss = 0.05953231
Iteration 18, loss = 0.05543576
Iteration 19, loss = 0.05166525
Iteration 20, loss = 0.04880035
Iteration 21, loss = 0.04632253
Iteration 22, loss = 0.04391357
Iteration 23, loss = 0.04210809
Iteration 24, loss = 0.04050167
Iteration 25, loss = 0.03911937
Iteration 26, loss = 0.03782446
Iteration 27, loss = 0.03671121
Iteration 28, loss = 0.03565819
Iteration 29, loss = 0.03497798
Iteration 30, loss = 0.03406665
Iteration 31, loss = 0.03319725
Iteration 32, loss = 0.03279955
Iteration 33, loss = 0.03212224
Iteration 34, loss = 0.03156972
Iteration 35, loss = 0.03096408
Iteration 36, loss = 0.03081132
Iteration 37, loss = 0.03027126
Iteration 38, loss = 0.02975767
Iteration 39, loss = 0.02927241
Iteration 40, loss = 0.02923889
Iteration 41, loss = 0.02897952
Iteration 42, loss = 0.02885810
Iteration 43, loss = 0.02822939
Iteration 44, loss = 0.02804329
Iteration 45, loss = 0.02806036
Iteration 46, loss = 0.02749016
Iteration 47, loss = 0.02775655
Iteration 48, loss = 0.02748077
Iteration 49, loss = 0.02734532
Iteration 50, loss = 0.02744583
Iteration 51, loss = 0.02694114
Iteration 52, loss = 0.02678158
Iteration 53, loss = 0.02647026
Iteration 54, loss = 0.02643592
Iteration 55, loss = 0.02632542
Iteration 56, loss = 0.02614360
Iteration 57, loss = 0.02621822
Iteration 58, loss = 0.02606834
Iteration 59, loss = 0.02582009
Iteration 60, loss = 0.02589654
Iteration 61, loss = 0.02582262
Iteration 62, loss = 0.02586733
Iteration 63, loss = 0.02540149
Iteration 64, loss = 0.02578062
Iteration 65, loss = 0.02559264
Iteration 66, loss = 0.02538859
Iteration 67, loss = 0.02526532
Iteration 68, loss = 0.02524944
Iteration 69, loss = 0.02529248
Iteration 70, loss = 0.02537249
Iteration 71, loss = 0.02500235
Iteration 72, loss = 0.02499231
Iteration 73, loss = 0.02514364
Iteration 74, loss = 0.02513508
Iteration 75, loss = 0.02489891
Iteration 76, loss = 0.02500185
Iteration 77, loss = 0.02503691
Iteration 78, loss = 0.02470030
Iteration 79, loss = 0.02450334
Iteration 80, loss = 0.02467399
Iteration 81, loss = 0.02458241
Iteration 82, loss = 0.02462174
Iteration 83, loss = 0.02445625
Iteration 84, loss = 0.02454356
Iteration 85, loss = 0.02453163
Iteration 86, loss = 0.02467169
Iteration 87, loss = 0.02444085
Iteration 88, loss = 0.02443519
Iteration 89, loss = 0.02468266
Iteration 90, loss = 0.02452079
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71845656
Iteration 2, loss = 0.67063555
Iteration 3, loss = 0.61383559
Iteration 4, loss = 0.55054368
Iteration 5, loss = 0.48208273
Iteration 6, loss = 0.41325367
Iteration 7, loss = 0.34936044
Iteration 8, loss = 0.29302144
Iteration 9, loss = 0.24575574
Iteration 10, loss = 0.20679179
Iteration 11, loss = 0.17515334
Iteration 12, loss = 0.14979520
Iteration 13, loss = 0.12955820
Iteration 14, loss = 0.11327811
Iteration 15, loss = 0.10014759
Iteration 16, loss = 0.08937470
Iteration 17, loss = 0.08062123
Iteration 18, loss = 0.07326842
Iteration 19, loss = 0.06718100
Iteration 20, loss = 0.06189606
Iteration 21, loss = 0.05750632
Iteration 22, loss = 0.05374652
Iteration 23, loss = 0.05062235
Iteration 24, loss = 0.04756464
Iteration 25, loss = 0.04508496
Iteration 26, loss = 0.04297407
Iteration 27, loss = 0.04099014
Iteration 28, loss = 0.03933298
Iteration 29, loss = 0.03769908
Iteration 30, loss = 0.03641390
Iteration 31, loss = 0.03521424
Iteration 32, loss = 0.03402128
Iteration 33, loss = 0.03297400
Iteration 34, loss = 0.03196825
Iteration 35, loss = 0.03109424
Iteration 36, loss = 0.03050743
Iteration 37, loss = 0.02962069
Iteration 38, loss = 0.02913176
Iteration 39, loss = 0.02852061
Iteration 40, loss = 0.02807817
Iteration 41, loss = 0.02744277
Iteration 42, loss = 0.02685049
Iteration 43, loss = 0.02655375
Iteration 44, loss = 0.02604947
Iteration 45, loss = 0.02557496
Iteration 46, loss = 0.02519437
Iteration 47, loss = 0.02492476
Iteration 48, loss = 0.02450506
Iteration 49, loss = 0.02433443
Iteration 50, loss = 0.02429301
Iteration 51, loss = 0.02382479
Iteration 52, loss = 0.02352974
Iteration 53, loss = 0.02340613
Iteration 54, loss = 0.02314404
Iteration 55, loss = 0.02304849
Iteration 56, loss = 0.02277010
Iteration 57, loss = 0.02257490
Iteration 58, loss = 0.02242307
Iteration 59, loss = 0.02225942
Iteration 60, loss = 0.02194162
Iteration 61, loss = 0.02198362
Iteration 62, loss = 0.02182973
Iteration 63, loss = 0.02167447
Iteration 64, loss = 0.02149032
Iteration 65, loss = 0.02143011
Iteration 66, loss = 0.02129854
Iteration 67, loss = 0.02133945
Iteration 68, loss = 0.02114971
Iteration 69, loss = 0.02100010
Iteration 70, loss = 0.02081637
Iteration 71, loss = 0.02094239
Iteration 72, loss = 0.02074201
Iteration 73, loss = 0.02075072
Iteration 74, loss = 0.02055068
Iteration 75, loss = 0.02037574
Iteration 76, loss = 0.02047042
Iteration 77, loss = 0.02035963
Iteration 78, loss = 0.02014927
Iteration 79, loss = 0.02013937
Iteration 80, loss = 0.02029311
Iteration 81, loss = 0.02001404
Iteration 82, loss = 0.02005938
Iteration 83, loss = 0.01987846
Iteration 84, loss = 0.01980439
Iteration 85, loss = 0.02002456
Iteration 86, loss = 0.01987537
Iteration 87, loss = 0.01969762
Iteration 88, loss = 0.01975047
Iteration 89, loss = 0.01955962
Iteration 90, loss = 0.01963752
Iteration 91, loss = 0.01954794
Iteration 92, loss = 0.01950088
Iteration 93, loss = 0.01943066
Iteration 94, loss = 0.01950094
Iteration 95, loss = 0.01936965
Iteration 96, loss = 0.01932861
Iteration 97, loss = 0.01930285
Iteration 98, loss = 0.01945404
Iteration 99, loss = 0.01921022
Iteration 100, loss = 0.01926060
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72257097
Iteration 2, loss = 0.70667773
Iteration 3, loss = 0.69379778
Iteration 4, loss = 0.68113213
Iteration 5, loss = 0.66819395
Iteration 6, loss = 0.65447167
Iteration 7, loss = 0.63947713
Iteration 8, loss = 0.62365571
Iteration 9, loss = 0.60588644
Iteration 10, loss = 0.58715546
Iteration 11, loss = 0.56705243
Iteration 12, loss = 0.54564563
Iteration 13, loss = 0.52354321
Iteration 14, loss = 0.50039644
Iteration 15, loss = 0.47697937
Iteration 16, loss = 0.45338608
Iteration 17, loss = 0.42966667
Iteration 18, loss = 0.40630909
Iteration 19, loss = 0.38357914
Iteration 20, loss = 0.36149878
Iteration 21, loss = 0.34010020
Iteration 22, loss = 0.31976477
Iteration 23, loss = 0.30037364
Iteration 24, loss = 0.28207179
Iteration 25, loss = 0.26479745
Iteration 26, loss = 0.24852598
Iteration 27, loss = 0.23338468
Iteration 28, loss = 0.21911185
Iteration 29, loss = 0.20586940
Iteration 30, loss = 0.19353485
Iteration 31, loss = 0.18214032
Iteration 32, loss = 0.17148535
Iteration 33, loss = 0.16163448
Iteration 34, loss = 0.15255716
Iteration 35, loss = 0.14417123
Iteration 36, loss = 0.13636985
Iteration 37, loss = 0.12920270
Iteration 38, loss = 0.12252032
Iteration 39, loss = 0.11631757
Iteration 40, loss = 0.11062302
Iteration 41, loss = 0.10526593
Iteration 42, loss = 0.10029739
Iteration 43, loss = 0.09573467
Iteration 44, loss = 0.09136131
Iteration 45, loss = 0.08743128
Iteration 46, loss = 0.08368631
Iteration 47, loss = 0.08018997
Iteration 48, loss = 0.07690022
Iteration 49, loss = 0.07391162
Iteration 50, loss = 0.07100447
Iteration 51, loss = 0.06832028
Iteration 52, loss = 0.06586608
Iteration 53, loss = 0.06347608
Iteration 54, loss = 0.06127941
Iteration 55, loss = 0.05916275
Iteration 56, loss = 0.05725017
Iteration 57, loss = 0.05536875
Iteration 58, loss = 0.05365933
Iteration 59, loss = 0.05200263
Iteration 60, loss = 0.05040334
Iteration 61, loss = 0.04897016
Iteration 62, loss = 0.04754774
Iteration 63, loss = 0.04622610
Iteration 64, loss = 0.04497020
Iteration 65, loss = 0.04377709
Iteration 66, loss = 0.04266643
Iteration 67, loss = 0.04156052
Iteration 68, loss = 0.04057246
Iteration 69, loss = 0.03958483
Iteration 70, loss = 0.03861837
Iteration 71, loss = 0.03770597
Iteration 72, loss = 0.03680272
Iteration 73, loss = 0.03599536
Iteration 74, loss = 0.03519232
Iteration 75, loss = 0.03443401
Iteration 76, loss = 0.03372566
Iteration 77, loss = 0.03300581
Iteration 78, loss = 0.03232632
Iteration 79, loss = 0.03170079
Iteration 80, loss = 0.03106664
Iteration 81, loss = 0.03050648
Iteration 82, loss = 0.03002173
Iteration 83, loss = 0.02945505
Iteration 84, loss = 0.02896028
Iteration 85, loss = 0.02840838
Iteration 86, loss = 0.02791300
Iteration 87, loss = 0.02746017
Iteration 88, loss = 0.02703447
Iteration 89, loss = 0.02664654
Iteration 90, loss = 0.02621624
Iteration 91, loss = 0.02581462
Iteration 92, loss = 0.02542678
Iteration 93, loss = 0.02503228
Iteration 94, loss = 0.02467788
Iteration 95, loss = 0.02433725
Iteration 96, loss = 0.02399613
Iteration 97, loss = 0.02368392
Iteration 98, loss = 0.02336896
Iteration 99, loss = 0.02308340
Iteration 100, loss = 0.02280204
Iteration 101, loss = 0.02248348
Iteration 102, loss = 0.02219141
Iteration 103, loss = 0.02194083
Iteration 104, loss = 0.02163386
Iteration 105, loss = 0.02139722
Iteration 106, loss = 0.02112627
Iteration 107, loss = 0.02090896
Iteration 108, loss = 0.02068584
Iteration 109, loss = 0.02048514
Iteration 110, loss = 0.02023731
Iteration 111, loss = 0.02002113
Iteration 112, loss = 0.01981934
Iteration 113, loss = 0.01962345
Iteration 114, loss = 0.01944815
Iteration 115, loss = 0.01928041
Iteration 116, loss = 0.01910867
Iteration 117, loss = 0.01888173
Iteration 118, loss = 0.01874706
Iteration 119, loss = 0.01856746
Iteration 120, loss = 0.01829690
Iteration 121, loss = 0.01813831
Iteration 122, loss = 0.01797682
Iteration 123, loss = 0.01783318
Iteration 124, loss = 0.01771710
Iteration 125, loss = 0.01756211
Iteration 126, loss = 0.01748506
Iteration 127, loss = 0.01725630
Iteration 128, loss = 0.01710346
Iteration 129, loss = 0.01698128
Iteration 130, loss = 0.01685808
Iteration 131, loss = 0.01674493
Iteration 132, loss = 0.01659907
Iteration 133, loss = 0.01651129
Iteration 134, loss = 0.01639236
Iteration 135, loss = 0.01626694
Iteration 136, loss = 0.01628368
Iteration 137, loss = 0.01609937
Iteration 138, loss = 0.01599094
Iteration 139, loss = 0.01587555
Iteration 140, loss = 0.01575055
Iteration 141, loss = 0.01567101
Iteration 142, loss = 0.01556594
Iteration 143, loss = 0.01550314
Iteration 144, loss = 0.01536410
Iteration 145, loss = 0.01528390
Iteration 146, loss = 0.01526284
Iteration 147, loss = 0.01510670
Iteration 148, loss = 0.01500055
Iteration 149, loss = 0.01492072
Iteration 150, loss = 0.01479080
Iteration 151, loss = 0.01468526
Iteration 152, loss = 0.01464345
Iteration 153, loss = 0.01451306
Iteration 154, loss = 0.01453393
Iteration 155, loss = 0.01438171
Iteration 156, loss = 0.01433527
Iteration 157, loss = 0.01423274
Iteration 158, loss = 0.01415891
Iteration 159, loss = 0.01410994
Iteration 160, loss = 0.01409278
Iteration 161, loss = 0.01394358
Iteration 162, loss = 0.01388639
Iteration 163, loss = 0.01380219
Iteration 164, loss = 0.01372667
Iteration 165, loss = 0.01368548
Iteration 166, loss = 0.01360897
Iteration 167, loss = 0.01357920
Iteration 168, loss = 0.01353058
Iteration 169, loss = 0.01345497
Iteration 170, loss = 0.01341735
Iteration 171, loss = 0.01332554
Iteration 172, loss = 0.01330020
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69353301
Iteration 2, loss = 0.67181804
Iteration 3, loss = 0.65028356
Iteration 4, loss = 0.62541755
Iteration 5, loss = 0.59789283
Iteration 6, loss = 0.56699213
Iteration 7, loss = 0.53363875
Iteration 8, loss = 0.49920769
Iteration 9, loss = 0.46434957
Iteration 10, loss = 0.42995632
Iteration 11, loss = 0.39664797
Iteration 12, loss = 0.36467860
Iteration 13, loss = 0.33444774
Iteration 14, loss = 0.30602046
Iteration 15, loss = 0.27962877
Iteration 16, loss = 0.25524751
Iteration 17, loss = 0.23305275
Iteration 18, loss = 0.21276695
Iteration 19, loss = 0.19454984
Iteration 20, loss = 0.17810239
Iteration 21, loss = 0.16322740
Iteration 22, loss = 0.15006792
Iteration 23, loss = 0.13831269
Iteration 24, loss = 0.12775543
Iteration 25, loss = 0.11825620
Iteration 26, loss = 0.10987361
Iteration 27, loss = 0.10230979
Iteration 28, loss = 0.09547280
Iteration 29, loss = 0.08934748
Iteration 30, loss = 0.08391700
Iteration 31, loss = 0.07885901
Iteration 32, loss = 0.07439230
Iteration 33, loss = 0.07032956
Iteration 34, loss = 0.06659903
Iteration 35, loss = 0.06317995
Iteration 36, loss = 0.06007839
Iteration 37, loss = 0.05729628
Iteration 38, loss = 0.05467848
Iteration 39, loss = 0.05227672
Iteration 40, loss = 0.05009025
Iteration 41, loss = 0.04800233
Iteration 42, loss = 0.04614887
Iteration 43, loss = 0.04443251
Iteration 44, loss = 0.04277341
Iteration 45, loss = 0.04128452
Iteration 46, loss = 0.03987932
Iteration 47, loss = 0.03860799
Iteration 48, loss = 0.03738332
Iteration 49, loss = 0.03618566
Iteration 50, loss = 0.03507785
Iteration 51, loss = 0.03408905
Iteration 52, loss = 0.03317167
Iteration 53, loss = 0.03228453
Iteration 54, loss = 0.03140507
Iteration 55, loss = 0.03061736
Iteration 56, loss = 0.02987890
Iteration 57, loss = 0.02922467
Iteration 58, loss = 0.02847586
Iteration 59, loss = 0.02791602
Iteration 60, loss = 0.02724405
Iteration 61, loss = 0.02672469
Iteration 62, loss = 0.02618827
Iteration 63, loss = 0.02565420
Iteration 64, loss = 0.02519244
Iteration 65, loss = 0.02467800
Iteration 66, loss = 0.02425908
Iteration 67, loss = 0.02383578
Iteration 68, loss = 0.02350015
Iteration 69, loss = 0.02303356
Iteration 70, loss = 0.02266523
Iteration 71, loss = 0.02234880
Iteration 72, loss = 0.02196313
Iteration 73, loss = 0.02165530
Iteration 74, loss = 0.02137517
Iteration 75, loss = 0.02107092
Iteration 76, loss = 0.02083004
Iteration 77, loss = 0.02048761
Iteration 78, loss = 0.02025200
Iteration 79, loss = 0.02002172
Iteration 80, loss = 0.01974264
Iteration 81, loss = 0.01949631
Iteration 82, loss = 0.01930382
Iteration 83, loss = 0.01903995
Iteration 84, loss = 0.01891451
Iteration 85, loss = 0.01867665
Iteration 86, loss = 0.01846677
Iteration 87, loss = 0.01825984
Iteration 88, loss = 0.01807576
Iteration 89, loss = 0.01797816
Iteration 90, loss = 0.01774395
Iteration 91, loss = 0.01765951
Iteration 92, loss = 0.01750627
Iteration 93, loss = 0.01729007
Iteration 94, loss = 0.01713412
Iteration 95, loss = 0.01704222
Iteration 96, loss = 0.01687043
Iteration 97, loss = 0.01681816
Iteration 98, loss = 0.01658090
Iteration 99, loss = 0.01650529
Iteration 100, loss = 0.01634303
Iteration 101, loss = 0.01626223
Iteration 102, loss = 0.01612457
Iteration 103, loss = 0.01603554
Iteration 104, loss = 0.01594437
Iteration 105, loss = 0.01581644
Iteration 106, loss = 0.01569744
Iteration 107, loss = 0.01564114
Iteration 108, loss = 0.01553143
Iteration 109, loss = 0.01541441
Iteration 110, loss = 0.01540779
Iteration 111, loss = 0.01525886
Iteration 112, loss = 0.01520258
Iteration 113, loss = 0.01507711
Iteration 114, loss = 0.01502588
Iteration 115, loss = 0.01492634
Iteration 116, loss = 0.01488695
Iteration 117, loss = 0.01473109
Iteration 118, loss = 0.01469513
Iteration 119, loss = 0.01464528
Iteration 120, loss = 0.01461047
Iteration 121, loss = 0.01452362
Iteration 122, loss = 0.01444238
Iteration 123, loss = 0.01437264
Iteration 124, loss = 0.01445993
Iteration 125, loss = 0.01424924
Iteration 126, loss = 0.01417389
Iteration 127, loss = 0.01412931
Iteration 128, loss = 0.01415171
Iteration 129, loss = 0.01403479
Iteration 130, loss = 0.01403942
Iteration 131, loss = 0.01388710
Iteration 132, loss = 0.01383644
Iteration 133, loss = 0.01378184
Iteration 134, loss = 0.01387051
Iteration 135, loss = 0.01378678
Iteration 136, loss = 0.01367718
Iteration 137, loss = 0.01367014
Iteration 138, loss = 0.01363393
Iteration 139, loss = 0.01353322
Iteration 140, loss = 0.01349669
Iteration 141, loss = 0.01343313
Iteration 142, loss = 0.01350127
Iteration 143, loss = 0.01338570
Iteration 144, loss = 0.01336997
Iteration 145, loss = 0.01331171
Iteration 146, loss = 0.01331070
Iteration 147, loss = 0.01330931
Iteration 148, loss = 0.01319818
Iteration 149, loss = 0.01317352
Iteration 150, loss = 0.01308911
Iteration 151, loss = 0.01309829
Iteration 152, loss = 0.01304545
Iteration 153, loss = 0.01304158
Iteration 154, loss = 0.01301191
Iteration 155, loss = 0.01300655
Iteration 156, loss = 0.01293031
Iteration 157, loss = 0.01291267
Iteration 158, loss = 0.01287651
Iteration 159, loss = 0.01282118
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71902350
Iteration 2, loss = 0.68467599
Iteration 3, loss = 0.63881961
Iteration 4, loss = 0.58189471
Iteration 5, loss = 0.51494179
Iteration 6, loss = 0.44457087
Iteration 7, loss = 0.37684700
Iteration 8, loss = 0.31684272
Iteration 9, loss = 0.26597074
Iteration 10, loss = 0.22388285
Iteration 11, loss = 0.18981551
Iteration 12, loss = 0.16251203
Iteration 13, loss = 0.14067701
Iteration 14, loss = 0.12321875
Iteration 15, loss = 0.10890418
Iteration 16, loss = 0.09732191
Iteration 17, loss = 0.08774936
Iteration 18, loss = 0.08006252
Iteration 19, loss = 0.07340421
Iteration 20, loss = 0.06791391
Iteration 21, loss = 0.06332409
Iteration 22, loss = 0.05904719
Iteration 23, loss = 0.05578998
Iteration 24, loss = 0.05243353
Iteration 25, loss = 0.04988285
Iteration 26, loss = 0.04762877
Iteration 27, loss = 0.04528908
Iteration 28, loss = 0.04362279
Iteration 29, loss = 0.04198167
Iteration 30, loss = 0.04040575
Iteration 31, loss = 0.03918253
Iteration 32, loss = 0.03789263
Iteration 33, loss = 0.03684657
Iteration 34, loss = 0.03588536
Iteration 35, loss = 0.03503834
Iteration 36, loss = 0.03411995
Iteration 37, loss = 0.03354594
Iteration 38, loss = 0.03273881
Iteration 39, loss = 0.03201953
Iteration 40, loss = 0.03164758
Iteration 41, loss = 0.03104977
Iteration 42, loss = 0.03062127
Iteration 43, loss = 0.03004367
Iteration 44, loss = 0.02975719
Iteration 45, loss = 0.02932889
Iteration 46, loss = 0.02888607
Iteration 47, loss = 0.02846406
Iteration 48, loss = 0.02804345
Iteration 49, loss = 0.02792042
Iteration 50, loss = 0.02742812
Iteration 51, loss = 0.02725088
Iteration 52, loss = 0.02701173
Iteration 53, loss = 0.02680393
Iteration 54, loss = 0.02661855
Iteration 55, loss = 0.02625268
Iteration 56, loss = 0.02611339
Iteration 57, loss = 0.02582877
Iteration 58, loss = 0.02581622
Iteration 59, loss = 0.02550676
Iteration 60, loss = 0.02546781
Iteration 61, loss = 0.02531177
Iteration 62, loss = 0.02500769
Iteration 63, loss = 0.02490521
Iteration 64, loss = 0.02481296
Iteration 65, loss = 0.02486106
Iteration 66, loss = 0.02461030
Iteration 67, loss = 0.02440505
Iteration 68, loss = 0.02440421
Iteration 69, loss = 0.02420639
Iteration 70, loss = 0.02407544
Iteration 71, loss = 0.02409651
Iteration 72, loss = 0.02384856
Iteration 73, loss = 0.02368299
Iteration 74, loss = 0.02379933
Iteration 75, loss = 0.02391864
Iteration 76, loss = 0.02364467
Iteration 77, loss = 0.02378692
Iteration 78, loss = 0.02326465
Iteration 79, loss = 0.02348591
Iteration 80, loss = 0.02330844
Iteration 81, loss = 0.02322039
Iteration 82, loss = 0.02311964
Iteration 83, loss = 0.02303675
Iteration 84, loss = 0.02301972
Iteration 85, loss = 0.02299750
Iteration 86, loss = 0.02273575
Iteration 87, loss = 0.02312201
Iteration 88, loss = 0.02284313
Iteration 89, loss = 0.02282028
Iteration 90, loss = 0.02272076
Iteration 91, loss = 0.02264084
Iteration 92, loss = 0.02269219
Iteration 93, loss = 0.02254257
Iteration 94, loss = 0.02260464
Iteration 95, loss = 0.02269792
Iteration 96, loss = 0.02257938
Iteration 97, loss = 0.02262363
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67652662
Iteration 2, loss = 0.62181178
Iteration 3, loss = 0.53033645
Iteration 4, loss = 0.42664774
Iteration 5, loss = 0.33060125
Iteration 6, loss = 0.25287335
Iteration 7, loss = 0.19536232
Iteration 8, loss = 0.15420411
Iteration 9, loss = 0.12494353
Iteration 10, loss = 0.10403482
Iteration 11, loss = 0.08885581
Iteration 12, loss = 0.07731415
Iteration 13, loss = 0.06851644
Iteration 14, loss = 0.06157979
Iteration 15, loss = 0.05657652
Iteration 16, loss = 0.05186128
Iteration 17, loss = 0.04800086
Iteration 18, loss = 0.04513969
Iteration 19, loss = 0.04277760
Iteration 20, loss = 0.04061699
Iteration 21, loss = 0.03862567
Iteration 22, loss = 0.03713617
Iteration 23, loss = 0.03561303
Iteration 24, loss = 0.03461089
Iteration 25, loss = 0.03363706
Iteration 26, loss = 0.03255270
Iteration 27, loss = 0.03176649
Iteration 28, loss = 0.03098834
Iteration 29, loss = 0.03049909
Iteration 30, loss = 0.02979384
Iteration 31, loss = 0.02924820
Iteration 32, loss = 0.02879910
Iteration 33, loss = 0.02856690
Iteration 34, loss = 0.02819574
Iteration 35, loss = 0.02777713
Iteration 36, loss = 0.02732429
Iteration 37, loss = 0.02696378
Iteration 38, loss = 0.02720888
Iteration 39, loss = 0.02670834
Iteration 40, loss = 0.02639982
Iteration 41, loss = 0.02584092
Iteration 42, loss = 0.02566915
Iteration 43, loss = 0.02568084
Iteration 44, loss = 0.02542708
Iteration 45, loss = 0.02539800
Iteration 46, loss = 0.02521180
Iteration 47, loss = 0.02518014
Iteration 48, loss = 0.02497103
Iteration 49, loss = 0.02497967
Iteration 50, loss = 0.02475907
Iteration 51, loss = 0.02480897
Iteration 52, loss = 0.02442351
Iteration 53, loss = 0.02465958
Iteration 54, loss = 0.02451116
Iteration 55, loss = 0.02456861
Iteration 56, loss = 0.02438765
Iteration 57, loss = 0.02431629
Iteration 58, loss = 0.02406748
Iteration 59, loss = 0.02384068
Iteration 60, loss = 0.02400305
Iteration 61, loss = 0.02380783
Iteration 62, loss = 0.02396613
Iteration 63, loss = 0.02361551
Iteration 64, loss = 0.02349260
Iteration 65, loss = 0.02346301
Iteration 66, loss = 0.02351614
Iteration 67, loss = 0.02361053
Iteration 68, loss = 0.02337462
Iteration 69, loss = 0.02338937
Iteration 70, loss = 0.02348046
Iteration 71, loss = 0.02342483
Iteration 72, loss = 0.02321619
Iteration 73, loss = 0.02337208
Iteration 74, loss = 0.02331616
Iteration 75, loss = 0.02320585
Iteration 76, loss = 0.02316043
Iteration 77, loss = 0.02318831
Iteration 78, loss = 0.02335562
Iteration 79, loss = 0.02317330
Iteration 80, loss = 0.02329169
Iteration 81, loss = 0.02308345
Iteration 82, loss = 0.02311561
Iteration 83, loss = 0.02305827
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69794019
Iteration 2, loss = 0.64515114
Iteration 3, loss = 0.57381704
Iteration 4, loss = 0.48556963
Iteration 5, loss = 0.39612120
Iteration 6, loss = 0.31689538
Iteration 7, loss = 0.25168035
Iteration 8, loss = 0.20126165
Iteration 9, loss = 0.16366865
Iteration 10, loss = 0.13539093
Iteration 11, loss = 0.11418520
Iteration 12, loss = 0.09801374
Iteration 13, loss = 0.08558642
Iteration 14, loss = 0.07591760
Iteration 15, loss = 0.06818827
Iteration 16, loss = 0.06192248
Iteration 17, loss = 0.05668171
Iteration 18, loss = 0.05244087
Iteration 19, loss = 0.04892864
Iteration 20, loss = 0.04597676
Iteration 21, loss = 0.04327995
Iteration 22, loss = 0.04107551
Iteration 23, loss = 0.03900680
Iteration 24, loss = 0.03738580
Iteration 25, loss = 0.03610424
Iteration 26, loss = 0.03466816
Iteration 27, loss = 0.03344802
Iteration 28, loss = 0.03247545
Iteration 29, loss = 0.03159616
Iteration 30, loss = 0.03067918
Iteration 31, loss = 0.02994000
Iteration 32, loss = 0.02951672
Iteration 33, loss = 0.02873920
Iteration 34, loss = 0.02800662
Iteration 35, loss = 0.02770834
Iteration 36, loss = 0.02716488
Iteration 37, loss = 0.02668497
Iteration 38, loss = 0.02625822
Iteration 39, loss = 0.02597956
Iteration 40, loss = 0.02578311
Iteration 41, loss = 0.02532451
Iteration 42, loss = 0.02483089
Iteration 43, loss = 0.02459348
Iteration 44, loss = 0.02440496
Iteration 45, loss = 0.02434398
Iteration 46, loss = 0.02406574
Iteration 47, loss = 0.02398981
Iteration 48, loss = 0.02369221
Iteration 49, loss = 0.02340521
Iteration 50, loss = 0.02322882
Iteration 51, loss = 0.02317577
Iteration 52, loss = 0.02299529
Iteration 53, loss = 0.02281037
Iteration 54, loss = 0.02270832
Iteration 55, loss = 0.02264774
Iteration 56, loss = 0.02240723
Iteration 57, loss = 0.02227665
Iteration 58, loss = 0.02222283
Iteration 59, loss = 0.02227694
Iteration 60, loss = 0.02207199
Iteration 61, loss = 0.02202490
Iteration 62, loss = 0.02195853
Iteration 63, loss = 0.02199637
Iteration 64, loss = 0.02162314
Iteration 65, loss = 0.02167655
Iteration 66, loss = 0.02151806
Iteration 67, loss = 0.02142258
Iteration 68, loss = 0.02170486
Iteration 69, loss = 0.02153916
Iteration 70, loss = 0.02152047
Iteration 71, loss = 0.02131420
Iteration 72, loss = 0.02123968
Iteration 73, loss = 0.02127282
Iteration 74, loss = 0.02141138
Iteration 75, loss = 0.02116345
Iteration 76, loss = 0.02128049
Iteration 77, loss = 0.02094619
Iteration 78, loss = 0.02097582
Iteration 79, loss = 0.02090507
Iteration 80, loss = 0.02078415
Iteration 81, loss = 0.02096441
Iteration 82, loss = 0.02081080
Iteration 83, loss = 0.02109144
Iteration 84, loss = 0.02082477
Iteration 85, loss = 0.02088326
Iteration 86, loss = 0.02056778
Iteration 87, loss = 0.02066139
Iteration 88, loss = 0.02065169
Iteration 89, loss = 0.02062362
Iteration 90, loss = 0.02086412
Iteration 91, loss = 0.02066841
Iteration 92, loss = 0.02076215
Iteration 93, loss = 0.02060608
Iteration 94, loss = 0.02070781
Iteration 95, loss = 0.02038097
Iteration 96, loss = 0.02059385
Iteration 97, loss = 0.02043364
Iteration 98, loss = 0.02045168
Iteration 99, loss = 0.02055074
Iteration 100, loss = 0.02035197
Iteration 101, loss = 0.02066335
Iteration 102, loss = 0.02017166
Iteration 103, loss = 0.02048587
Iteration 104, loss = 0.02035968
Iteration 105, loss = 0.02027637
Iteration 106, loss = 0.02017924
Iteration 107, loss = 0.02033187
Iteration 108, loss = 0.02012497
Iteration 109, loss = 0.02021803
Iteration 110, loss = 0.02021925
Iteration 111, loss = 0.02043415
Iteration 112, loss = 0.02017551
Iteration 113, loss = 0.02012701
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67825804
Iteration 2, loss = 0.62893388
Iteration 3, loss = 0.54416696
Iteration 4, loss = 0.44459334
Iteration 5, loss = 0.35034651
Iteration 6, loss = 0.27043790
Iteration 7, loss = 0.20999279
Iteration 8, loss = 0.16627152
Iteration 9, loss = 0.13485889
Iteration 10, loss = 0.11246731
Iteration 11, loss = 0.09557249
Iteration 12, loss = 0.08288420
Iteration 13, loss = 0.07347114
Iteration 14, loss = 0.06600972
Iteration 15, loss = 0.05993638
Iteration 16, loss = 0.05519288
Iteration 17, loss = 0.05101140
Iteration 18, loss = 0.04783586
Iteration 19, loss = 0.04492292
Iteration 20, loss = 0.04254278
Iteration 21, loss = 0.04052520
Iteration 22, loss = 0.03888983
Iteration 23, loss = 0.03726819
Iteration 24, loss = 0.03610395
Iteration 25, loss = 0.03493709
Iteration 26, loss = 0.03395267
Iteration 27, loss = 0.03332540
Iteration 28, loss = 0.03243391
Iteration 29, loss = 0.03177756
Iteration 30, loss = 0.03097740
Iteration 31, loss = 0.03067471
Iteration 32, loss = 0.03019348
Iteration 33, loss = 0.02942284
Iteration 34, loss = 0.02875267
Iteration 35, loss = 0.02822651
Iteration 36, loss = 0.02776613
Iteration 37, loss = 0.02736722
Iteration 38, loss = 0.02707076
Iteration 39, loss = 0.02676304
Iteration 40, loss = 0.02732351
Iteration 41, loss = 0.02712451
Iteration 42, loss = 0.02661754
Iteration 43, loss = 0.02607527
Iteration 44, loss = 0.02567964
Iteration 45, loss = 0.02522763
Iteration 46, loss = 0.02518930
Iteration 47, loss = 0.02517260
Iteration 48, loss = 0.02467393
Iteration 49, loss = 0.02506998
Iteration 50, loss = 0.02476086
Iteration 51, loss = 0.02516980
Iteration 52, loss = 0.02479329
Iteration 53, loss = 0.02447928
Iteration 54, loss = 0.02452200
Iteration 55, loss = 0.02417096
Iteration 56, loss = 0.02405580
Iteration 57, loss = 0.02401034
Iteration 58, loss = 0.02437766
Iteration 59, loss = 0.02411653
Iteration 60, loss = 0.02395523
Iteration 61, loss = 0.02372584
Iteration 62, loss = 0.02389200
Iteration 63, loss = 0.02385672
Iteration 64, loss = 0.02377683
Iteration 65, loss = 0.02546023
Iteration 66, loss = 0.02489736
Iteration 67, loss = 0.02487229
Iteration 68, loss = 0.02412307
Iteration 69, loss = 0.02379414
Iteration 70, loss = 0.02371008
Iteration 71, loss = 0.02327667
Iteration 72, loss = 0.02356400
Iteration 73, loss = 0.02452544
Iteration 74, loss = 0.02404129
Iteration 75, loss = 0.02466065
Iteration 76, loss = 0.02423067
Iteration 77, loss = 0.02383340
Iteration 78, loss = 0.02377986
Iteration 79, loss = 0.02353235
Iteration 80, loss = 0.02315916
Iteration 81, loss = 0.02309740
Iteration 82, loss = 0.02288115
Iteration 83, loss = 0.02252322
Iteration 84, loss = 0.02330336
Iteration 85, loss = 0.02323635
Iteration 86, loss = 0.02287561
Iteration 87, loss = 0.02249459
Iteration 88, loss = 0.02227029
Iteration 89, loss = 0.02217586
Iteration 90, loss = 0.02381736
Iteration 91, loss = 0.02350018
Iteration 92, loss = 0.02296628
Iteration 93, loss = 0.02257981
Iteration 94, loss = 0.02246528
Iteration 95, loss = 0.02299438
Iteration 96, loss = 0.02260386
Iteration 97, loss = 0.02273267
Iteration 98, loss = 0.02232300
Iteration 99, loss = 0.02265353
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68952425
Iteration 2, loss = 0.65748656
Iteration 3, loss = 0.61527703
Iteration 4, loss = 0.56260901
Iteration 5, loss = 0.50168851
Iteration 6, loss = 0.43862787
Iteration 7, loss = 0.37825117
Iteration 8, loss = 0.32383406
Iteration 9, loss = 0.27599829
Iteration 10, loss = 0.23550127
Iteration 11, loss = 0.20148942
Iteration 12, loss = 0.17348609
Iteration 13, loss = 0.15043181
Iteration 14, loss = 0.13150552
Iteration 15, loss = 0.11604729
Iteration 16, loss = 0.10335735
Iteration 17, loss = 0.09279667
Iteration 18, loss = 0.08393933
Iteration 19, loss = 0.07647271
Iteration 20, loss = 0.07002058
Iteration 21, loss = 0.06474152
Iteration 22, loss = 0.05994897
Iteration 23, loss = 0.05605950
Iteration 24, loss = 0.05259410
Iteration 25, loss = 0.04949035
Iteration 26, loss = 0.04667561
Iteration 27, loss = 0.04441032
Iteration 28, loss = 0.04236880
Iteration 29, loss = 0.04047810
Iteration 30, loss = 0.03872527
Iteration 31, loss = 0.03716695
Iteration 32, loss = 0.03586596
Iteration 33, loss = 0.03455164
Iteration 34, loss = 0.03343006
Iteration 35, loss = 0.03261282
Iteration 36, loss = 0.03153156
Iteration 37, loss = 0.03059918
Iteration 38, loss = 0.02990395
Iteration 39, loss = 0.02906585
Iteration 40, loss = 0.02846874
Iteration 41, loss = 0.02788191
Iteration 42, loss = 0.02722639
Iteration 43, loss = 0.02664929
Iteration 44, loss = 0.02622707
Iteration 45, loss = 0.02577463
Iteration 46, loss = 0.02539931
Iteration 47, loss = 0.02489782
Iteration 48, loss = 0.02455484
Iteration 49, loss = 0.02427916
Iteration 50, loss = 0.02384889
Iteration 51, loss = 0.02364321
Iteration 52, loss = 0.02310631
Iteration 53, loss = 0.02306607
Iteration 54, loss = 0.02264716
Iteration 55, loss = 0.02244007
Iteration 56, loss = 0.02222074
Iteration 57, loss = 0.02211837
Iteration 58, loss = 0.02171167
Iteration 59, loss = 0.02169057
Iteration 60, loss = 0.02136844
Iteration 61, loss = 0.02134035
Iteration 62, loss = 0.02123730
Iteration 63, loss = 0.02095741
Iteration 64, loss = 0.02079962
Iteration 65, loss = 0.02053094
Iteration 66, loss = 0.02043761
Iteration 67, loss = 0.02041577
Iteration 68, loss = 0.02012722
Iteration 69, loss = 0.02001427
Iteration 70, loss = 0.01998400
Iteration 71, loss = 0.02002019
Iteration 72, loss = 0.01987790
Iteration 73, loss = 0.01970380
Iteration 74, loss = 0.01961828
Iteration 75, loss = 0.01957935
Iteration 76, loss = 0.01948576
Iteration 77, loss = 0.01926802
Iteration 78, loss = 0.01930330
Iteration 79, loss = 0.01926732
Iteration 80, loss = 0.01921446
Iteration 81, loss = 0.01908721
Iteration 82, loss = 0.01890627
Iteration 83, loss = 0.01898366
Iteration 84, loss = 0.01883389
Iteration 85, loss = 0.01880872
Iteration 86, loss = 0.01877567
Iteration 87, loss = 0.01867004
Iteration 88, loss = 0.01862893
Iteration 89, loss = 0.01879878
Iteration 90, loss = 0.01844778
Iteration 91, loss = 0.01848555
Iteration 92, loss = 0.01844461
Iteration 93, loss = 0.01823939
Iteration 94, loss = 0.01826582
Iteration 95, loss = 0.01824762
Iteration 96, loss = 0.01816450
Iteration 97, loss = 0.01822208
Iteration 98, loss = 0.01803174
Iteration 99, loss = 0.01802527
Iteration 100, loss = 0.01794646
Iteration 101, loss = 0.01822745
Iteration 102, loss = 0.01810418
Iteration 103, loss = 0.01796216
Iteration 104, loss = 0.01821527
Iteration 105, loss = 0.01789305
Iteration 106, loss = 0.01802984
Iteration 107, loss = 0.01778150
Iteration 108, loss = 0.01763434
Iteration 109, loss = 0.01772870
Iteration 110, loss = 0.01775089
Iteration 111, loss = 0.01781654
Iteration 112, loss = 0.01765876
Iteration 113, loss = 0.01768356
Iteration 114, loss = 0.01764745
Iteration 115, loss = 0.01789702
Iteration 116, loss = 0.01763158
Iteration 117, loss = 0.01760114
Iteration 118, loss = 0.01764372
Iteration 119, loss = 0.01752284
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67741721
Iteration 2, loss = 0.62821875
Iteration 3, loss = 0.56354882
Iteration 4, loss = 0.48862983
Iteration 5, loss = 0.41382848
Iteration 6, loss = 0.34406345
Iteration 7, loss = 0.28334312
Iteration 8, loss = 0.23308090
Iteration 9, loss = 0.19253549
Iteration 10, loss = 0.16072880
Iteration 11, loss = 0.13603942
Iteration 12, loss = 0.11692680
Iteration 13, loss = 0.10180613
Iteration 14, loss = 0.08982774
Iteration 15, loss = 0.08045929
Iteration 16, loss = 0.07274854
Iteration 17, loss = 0.06621366
Iteration 18, loss = 0.06099453
Iteration 19, loss = 0.05652785
Iteration 20, loss = 0.05286327
Iteration 21, loss = 0.04957772
Iteration 22, loss = 0.04698054
Iteration 23, loss = 0.04461621
Iteration 24, loss = 0.04251284
Iteration 25, loss = 0.04066553
Iteration 26, loss = 0.03909823
Iteration 27, loss = 0.03763790
Iteration 28, loss = 0.03639797
Iteration 29, loss = 0.03528762
Iteration 30, loss = 0.03424680
Iteration 31, loss = 0.03341245
Iteration 32, loss = 0.03267663
Iteration 33, loss = 0.03185723
Iteration 34, loss = 0.03113857
Iteration 35, loss = 0.03049267
Iteration 36, loss = 0.03014948
Iteration 37, loss = 0.02956079
Iteration 38, loss = 0.02919164
Iteration 39, loss = 0.02873786
Iteration 40, loss = 0.02835844
Iteration 41, loss = 0.02790155
Iteration 42, loss = 0.02741673
Iteration 43, loss = 0.02771306
Iteration 44, loss = 0.02741666
Iteration 45, loss = 0.02643548
Iteration 46, loss = 0.02643597
Iteration 47, loss = 0.02597793
Iteration 48, loss = 0.02591410
Iteration 49, loss = 0.02555288
Iteration 50, loss = 0.02557397
Iteration 51, loss = 0.02516913
Iteration 52, loss = 0.02519649
Iteration 53, loss = 0.02509321
Iteration 54, loss = 0.02480304
Iteration 55, loss = 0.02446312
Iteration 56, loss = 0.02455949
Iteration 57, loss = 0.02444875
Iteration 58, loss = 0.02419320
Iteration 59, loss = 0.02431588
Iteration 60, loss = 0.02441963
Iteration 61, loss = 0.02400826
Iteration 62, loss = 0.02399063
Iteration 63, loss = 0.02360612
Iteration 64, loss = 0.02378976
Iteration 65, loss = 0.02356679
Iteration 66, loss = 0.02353304
Iteration 67, loss = 0.02342641
Iteration 68, loss = 0.02357997
Iteration 69, loss = 0.02319891
Iteration 70, loss = 0.02325512
Iteration 71, loss = 0.02305344
Iteration 72, loss = 0.02298625
Iteration 73, loss = 0.02282852
Iteration 74, loss = 0.02316663
Iteration 75, loss = 0.02304057
Iteration 76, loss = 0.02278821
Iteration 77, loss = 0.02281721
Iteration 78, loss = 0.02286452
Iteration 79, loss = 0.02272866
Iteration 80, loss = 0.02276384
Iteration 81, loss = 0.02263735
Iteration 82, loss = 0.02261290
Iteration 83, loss = 0.02258078
Iteration 84, loss = 0.02252250
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71596294
Iteration 2, loss = 0.66200509
Iteration 3, loss = 0.59044595
Iteration 4, loss = 0.49990006
Iteration 5, loss = 0.40712673
Iteration 6, loss = 0.32415898
Iteration 7, loss = 0.25685534
Iteration 8, loss = 0.20470124
Iteration 9, loss = 0.16560763
Iteration 10, loss = 0.13657917
Iteration 11, loss = 0.11466103
Iteration 12, loss = 0.09807542
Iteration 13, loss = 0.08553045
Iteration 14, loss = 0.07544346
Iteration 15, loss = 0.06760294
Iteration 16, loss = 0.06125491
Iteration 17, loss = 0.05592420
Iteration 18, loss = 0.05176476
Iteration 19, loss = 0.04808637
Iteration 20, loss = 0.04498242
Iteration 21, loss = 0.04235196
Iteration 22, loss = 0.04010062
Iteration 23, loss = 0.03827596
Iteration 24, loss = 0.03656190
Iteration 25, loss = 0.03515277
Iteration 26, loss = 0.03410479
Iteration 27, loss = 0.03263227
Iteration 28, loss = 0.03171630
Iteration 29, loss = 0.03081806
Iteration 30, loss = 0.03008426
Iteration 31, loss = 0.02926746
Iteration 32, loss = 0.02856152
Iteration 33, loss = 0.02803080
Iteration 34, loss = 0.02772317
Iteration 35, loss = 0.02704927
Iteration 36, loss = 0.02648886
Iteration 37, loss = 0.02632105
Iteration 38, loss = 0.02576605
Iteration 39, loss = 0.02553574
Iteration 40, loss = 0.02514645
Iteration 41, loss = 0.02483426
Iteration 42, loss = 0.02458708
Iteration 43, loss = 0.02432976
Iteration 44, loss = 0.02415506
Iteration 45, loss = 0.02374745
Iteration 46, loss = 0.02362665
Iteration 47, loss = 0.02339376
Iteration 48, loss = 0.02324637
Iteration 49, loss = 0.02315873
Iteration 50, loss = 0.02310494
Iteration 51, loss = 0.02282706
Iteration 52, loss = 0.02281994
Iteration 53, loss = 0.02254814
Iteration 54, loss = 0.02248042
Iteration 55, loss = 0.02233186
Iteration 56, loss = 0.02244822
Iteration 57, loss = 0.02232631
Iteration 58, loss = 0.02204392
Iteration 59, loss = 0.02191181
Iteration 60, loss = 0.02183353
Iteration 61, loss = 0.02176756
Iteration 62, loss = 0.02154814
Iteration 63, loss = 0.02152946
Iteration 64, loss = 0.02142789
Iteration 65, loss = 0.02152668
Iteration 66, loss = 0.02143524
Iteration 67, loss = 0.02126794
Iteration 68, loss = 0.02161123
Iteration 69, loss = 0.02139714
Iteration 70, loss = 0.02120878
Iteration 71, loss = 0.02102439
Iteration 72, loss = 0.02129368
Iteration 73, loss = 0.02075251
Iteration 74, loss = 0.02088605
Iteration 75, loss = 0.02087752
Iteration 76, loss = 0.02073991
Iteration 77, loss = 0.02077555
Iteration 78, loss = 0.02091448
Iteration 79, loss = 0.02071414
Iteration 80, loss = 0.02076248
Iteration 81, loss = 0.02070022
Iteration 82, loss = 0.02056156
Iteration 83, loss = 0.02070715
Iteration 84, loss = 0.02090639
Iteration 85, loss = 0.02068146
Iteration 86, loss = 0.02042525
Iteration 87, loss = 0.02076371
Iteration 88, loss = 0.02075737
Iteration 89, loss = 0.02044067
Iteration 90, loss = 0.02028606
Iteration 91, loss = 0.02042609
Iteration 92, loss = 0.02054110
Iteration 93, loss = 0.02027203
Iteration 94, loss = 0.02025648
Iteration 95, loss = 0.02030155
Iteration 96, loss = 0.02033673
Iteration 97, loss = 0.02018745
Iteration 98, loss = 0.02024859
Iteration 99, loss = 0.02041222
Iteration 100, loss = 0.02039209
Iteration 101, loss = 0.02035698
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69494495
Iteration 2, loss = 0.62277429
Iteration 3, loss = 0.52866582
Iteration 4, loss = 0.42238040
Iteration 5, loss = 0.33017330
Iteration 6, loss = 0.25603098
Iteration 7, loss = 0.20029158
Iteration 8, loss = 0.15952833
Iteration 9, loss = 0.12993297
Iteration 10, loss = 0.10824596
Iteration 11, loss = 0.09238589
Iteration 12, loss = 0.08016957
Iteration 13, loss = 0.07047977
Iteration 14, loss = 0.06318030
Iteration 15, loss = 0.05750687
Iteration 16, loss = 0.05261002
Iteration 17, loss = 0.04856928
Iteration 18, loss = 0.04565687
Iteration 19, loss = 0.04284869
Iteration 20, loss = 0.04060399
Iteration 21, loss = 0.03848579
Iteration 22, loss = 0.03664503
Iteration 23, loss = 0.03508567
Iteration 24, loss = 0.03361076
Iteration 25, loss = 0.03252772
Iteration 26, loss = 0.03140200
Iteration 27, loss = 0.03052091
Iteration 28, loss = 0.02968882
Iteration 29, loss = 0.02985441
Iteration 30, loss = 0.02933253
Iteration 31, loss = 0.02853811
Iteration 32, loss = 0.02792715
Iteration 33, loss = 0.02723002
Iteration 34, loss = 0.02685982
Iteration 35, loss = 0.02618628
Iteration 36, loss = 0.02548486
Iteration 37, loss = 0.02514261
Iteration 38, loss = 0.02499270
Iteration 39, loss = 0.02478325
Iteration 40, loss = 0.02426270
Iteration 41, loss = 0.02492912
Iteration 42, loss = 0.02458725
Iteration 43, loss = 0.02421041
Iteration 44, loss = 0.02376176
Iteration 45, loss = 0.02360601
Iteration 46, loss = 0.02340354
Iteration 47, loss = 0.02303817
Iteration 48, loss = 0.02288847
Iteration 49, loss = 0.02277386
Iteration 50, loss = 0.02273524
Iteration 51, loss = 0.02242837
Iteration 52, loss = 0.02218652
Iteration 53, loss = 0.02208916
Iteration 54, loss = 0.02249191
Iteration 55, loss = 0.02204239
Iteration 56, loss = 0.02208518
Iteration 57, loss = 0.02189766
Iteration 58, loss = 0.02193358
Iteration 59, loss = 0.02162112
Iteration 60, loss = 0.02151066
Iteration 61, loss = 0.02140505
Iteration 62, loss = 0.02113619
Iteration 63, loss = 0.02112453
Iteration 64, loss = 0.02106327
Iteration 65, loss = 0.02103959
Iteration 66, loss = 0.02089740
Iteration 67, loss = 0.02106436
Iteration 68, loss = 0.02091757
Iteration 69, loss = 0.02102072
Iteration 70, loss = 0.02079479
Iteration 71, loss = 0.02103892
Iteration 72, loss = 0.02086039
Iteration 73, loss = 0.02153722
Iteration 74, loss = 0.02180800
Iteration 75, loss = 0.02137437
Iteration 76, loss = 0.02127691
Iteration 77, loss = 0.02083033
Iteration 78, loss = 0.02057694
Iteration 79, loss = 0.02049382
Iteration 80, loss = 0.02062802
Iteration 81, loss = 0.02091328
Iteration 82, loss = 0.02211733
Iteration 83, loss = 0.02241777
Iteration 84, loss = 0.02187424
Iteration 85, loss = 0.02169813
Iteration 86, loss = 0.02106002
Iteration 87, loss = 0.02093461
Iteration 88, loss = 0.02057932
Iteration 89, loss = 0.02075595
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70641155
Iteration 2, loss = 0.69022293
Iteration 3, loss = 0.67599548
Iteration 4, loss = 0.66232093
Iteration 5, loss = 0.64799834
Iteration 6, loss = 0.63244089
Iteration 7, loss = 0.61613884
Iteration 8, loss = 0.59828630
Iteration 9, loss = 0.57911474
Iteration 10, loss = 0.55872605
Iteration 11, loss = 0.53705547
Iteration 12, loss = 0.51444748
Iteration 13, loss = 0.49094274
Iteration 14, loss = 0.46722576
Iteration 15, loss = 0.44302251
Iteration 16, loss = 0.41928143
Iteration 17, loss = 0.39570201
Iteration 18, loss = 0.37282790
Iteration 19, loss = 0.35083107
Iteration 20, loss = 0.32974720
Iteration 21, loss = 0.30970377
Iteration 22, loss = 0.29084561
Iteration 23, loss = 0.27294630
Iteration 24, loss = 0.25620278
Iteration 25, loss = 0.24050782
Iteration 26, loss = 0.22574580
Iteration 27, loss = 0.21211239
Iteration 28, loss = 0.19939563
Iteration 29, loss = 0.18752934
Iteration 30, loss = 0.17649335
Iteration 31, loss = 0.16638826
Iteration 32, loss = 0.15691465
Iteration 33, loss = 0.14813504
Iteration 34, loss = 0.14008806
Iteration 35, loss = 0.13259861
Iteration 36, loss = 0.12564442
Iteration 37, loss = 0.11926304
Iteration 38, loss = 0.11325754
Iteration 39, loss = 0.10776680
Iteration 40, loss = 0.10263073
Iteration 41, loss = 0.09785682
Iteration 42, loss = 0.09343766
Iteration 43, loss = 0.08933950
Iteration 44, loss = 0.08548446
Iteration 45, loss = 0.08195501
Iteration 46, loss = 0.07859416
Iteration 47, loss = 0.07546833
Iteration 48, loss = 0.07248867
Iteration 49, loss = 0.06977962
Iteration 50, loss = 0.06722151
Iteration 51, loss = 0.06482641
Iteration 52, loss = 0.06251985
Iteration 53, loss = 0.06039709
Iteration 54, loss = 0.05841105
Iteration 55, loss = 0.05653006
Iteration 56, loss = 0.05477170
Iteration 57, loss = 0.05310241
Iteration 58, loss = 0.05153511
Iteration 59, loss = 0.05005438
Iteration 60, loss = 0.04863342
Iteration 61, loss = 0.04728808
Iteration 62, loss = 0.04602810
Iteration 63, loss = 0.04480866
Iteration 64, loss = 0.04363640
Iteration 65, loss = 0.04251835
Iteration 66, loss = 0.04148146
Iteration 67, loss = 0.04049465
Iteration 68, loss = 0.03959004
Iteration 69, loss = 0.03862205
Iteration 70, loss = 0.03772933
Iteration 71, loss = 0.03689987
Iteration 72, loss = 0.03614006
Iteration 73, loss = 0.03531086
Iteration 74, loss = 0.03456039
Iteration 75, loss = 0.03387932
Iteration 76, loss = 0.03325803
Iteration 77, loss = 0.03258943
Iteration 78, loss = 0.03203397
Iteration 79, loss = 0.03138612
Iteration 80, loss = 0.03081260
Iteration 81, loss = 0.03025300
Iteration 82, loss = 0.02973567
Iteration 83, loss = 0.02923204
Iteration 84, loss = 0.02880068
Iteration 85, loss = 0.02827780
Iteration 86, loss = 0.02784099
Iteration 87, loss = 0.02740507
Iteration 88, loss = 0.02697840
Iteration 89, loss = 0.02655509
Iteration 90, loss = 0.02618643
Iteration 91, loss = 0.02586773
Iteration 92, loss = 0.02552841
Iteration 93, loss = 0.02519738
Iteration 94, loss = 0.02482241
Iteration 95, loss = 0.02450585
Iteration 96, loss = 0.02426328
Iteration 97, loss = 0.02396025
Iteration 98, loss = 0.02360476
Iteration 99, loss = 0.02328794
Iteration 100, loss = 0.02303443
Iteration 101, loss = 0.02271849
Iteration 102, loss = 0.02246185
Iteration 103, loss = 0.02220011
Iteration 104, loss = 0.02193775
Iteration 105, loss = 0.02166998
Iteration 106, loss = 0.02150094
Iteration 107, loss = 0.02124301
Iteration 108, loss = 0.02111209
Iteration 109, loss = 0.02080980
Iteration 110, loss = 0.02060069
Iteration 111, loss = 0.02038937
Iteration 112, loss = 0.02018386
Iteration 113, loss = 0.01997481
Iteration 114, loss = 0.01987475
Iteration 115, loss = 0.01964708
Iteration 116, loss = 0.01950030
Iteration 117, loss = 0.01937880
Iteration 118, loss = 0.01924327
Iteration 119, loss = 0.01912433
Iteration 120, loss = 0.01895301
Iteration 121, loss = 0.01884645
Iteration 122, loss = 0.01863469
Iteration 123, loss = 0.01845391
Iteration 124, loss = 0.01828793
Iteration 125, loss = 0.01821984
Iteration 126, loss = 0.01796039
Iteration 127, loss = 0.01780392
Iteration 128, loss = 0.01766972
Iteration 129, loss = 0.01751881
Iteration 130, loss = 0.01739930
Iteration 131, loss = 0.01725922
Iteration 132, loss = 0.01716793
Iteration 133, loss = 0.01703514
Iteration 134, loss = 0.01693140
Iteration 135, loss = 0.01684295
Iteration 136, loss = 0.01673968
Iteration 137, loss = 0.01665181
Iteration 138, loss = 0.01654905
Iteration 139, loss = 0.01646138
Iteration 140, loss = 0.01637393
Iteration 141, loss = 0.01630053
Iteration 142, loss = 0.01623892
Iteration 143, loss = 0.01613348
Iteration 144, loss = 0.01605900
Iteration 145, loss = 0.01590759
Iteration 146, loss = 0.01579614
Iteration 147, loss = 0.01572406
Iteration 148, loss = 0.01560286
Iteration 149, loss = 0.01548248
Iteration 150, loss = 0.01547673
Iteration 151, loss = 0.01542987
Iteration 152, loss = 0.01528505
Iteration 153, loss = 0.01529175
Iteration 154, loss = 0.01517044
Iteration 155, loss = 0.01512493
Iteration 156, loss = 0.01502206
Iteration 157, loss = 0.01490565
Iteration 158, loss = 0.01488777
Iteration 159, loss = 0.01475670
Iteration 160, loss = 0.01466357
Iteration 161, loss = 0.01462991
Iteration 162, loss = 0.01455743
Iteration 163, loss = 0.01449929
Iteration 164, loss = 0.01446196
Iteration 165, loss = 0.01444555
Iteration 166, loss = 0.01438796
Iteration 167, loss = 0.01432309
Iteration 168, loss = 0.01422413
Iteration 169, loss = 0.01416078
Iteration 170, loss = 0.01414568
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67401337
Iteration 2, loss = 0.60034436
Iteration 3, loss = 0.49678611
Iteration 4, loss = 0.38897084
Iteration 5, loss = 0.29507042
Iteration 6, loss = 0.22327801
Iteration 7, loss = 0.17210417
Iteration 8, loss = 0.13647357
Iteration 9, loss = 0.11150595
Iteration 10, loss = 0.09350905
Iteration 11, loss = 0.08036782
Iteration 12, loss = 0.07044149
Iteration 13, loss = 0.06302284
Iteration 14, loss = 0.05694468
Iteration 15, loss = 0.05228214
Iteration 16, loss = 0.04845768
Iteration 17, loss = 0.04529123
Iteration 18, loss = 0.04247930
Iteration 19, loss = 0.04044174
Iteration 20, loss = 0.03867087
Iteration 21, loss = 0.03710767
Iteration 22, loss = 0.03555496
Iteration 23, loss = 0.03435689
Iteration 24, loss = 0.03325931
Iteration 25, loss = 0.03230200
Iteration 26, loss = 0.03144854
Iteration 27, loss = 0.03080062
Iteration 28, loss = 0.03008568
Iteration 29, loss = 0.02959941
Iteration 30, loss = 0.02897528
Iteration 31, loss = 0.02866608
Iteration 32, loss = 0.02795219
Iteration 33, loss = 0.02777964
Iteration 34, loss = 0.02730893
Iteration 35, loss = 0.02704935
Iteration 36, loss = 0.02674972
Iteration 37, loss = 0.02661401
Iteration 38, loss = 0.02637276
Iteration 39, loss = 0.02614463
Iteration 40, loss = 0.02576150
Iteration 41, loss = 0.02574996
Iteration 42, loss = 0.02555833
Iteration 43, loss = 0.02533615
Iteration 44, loss = 0.02519456
Iteration 45, loss = 0.02491329
Iteration 46, loss = 0.02473136
Iteration 47, loss = 0.02491201
Iteration 48, loss = 0.02448025
Iteration 49, loss = 0.02445730
Iteration 50, loss = 0.02468348
Iteration 51, loss = 0.02446009
Iteration 52, loss = 0.02431218
Iteration 53, loss = 0.02431125
Iteration 54, loss = 0.02392492
Iteration 55, loss = 0.02419338
Iteration 56, loss = 0.02377084
Iteration 57, loss = 0.02381279
Iteration 58, loss = 0.02381276
Iteration 59, loss = 0.02365204
Iteration 60, loss = 0.02381723
Iteration 61, loss = 0.02354167
Iteration 62, loss = 0.02410448
Iteration 63, loss = 0.02382327
Iteration 64, loss = 0.02339775
Iteration 65, loss = 0.02355236
Iteration 66, loss = 0.02345556
Iteration 67, loss = 0.02359266
Iteration 68, loss = 0.02324094
Iteration 69, loss = 0.02321608
Iteration 70, loss = 0.02345304
Iteration 71, loss = 0.02352500
Iteration 72, loss = 0.02306545
Iteration 73, loss = 0.02336112
Iteration 74, loss = 0.02314976
Iteration 75, loss = 0.02289955
Iteration 76, loss = 0.02301773
Iteration 77, loss = 0.02283245
Iteration 78, loss = 0.02290128
Iteration 79, loss = 0.02297469
Iteration 80, loss = 0.02302233
Iteration 81, loss = 0.02292567
Iteration 82, loss = 0.02291945
Iteration 83, loss = 0.02313181
Iteration 84, loss = 0.02294566
Iteration 85, loss = 0.02292702
Iteration 86, loss = 0.02284943
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67365024
Iteration 2, loss = 0.59990255
Iteration 3, loss = 0.49199980
Iteration 4, loss = 0.38205819
Iteration 5, loss = 0.28841101
Iteration 6, loss = 0.21765874
Iteration 7, loss = 0.16791579
Iteration 8, loss = 0.13313768
Iteration 9, loss = 0.10898573
Iteration 10, loss = 0.09144246
Iteration 11, loss = 0.07872061
Iteration 12, loss = 0.06919082
Iteration 13, loss = 0.06182795
Iteration 14, loss = 0.05598552
Iteration 15, loss = 0.05149724
Iteration 16, loss = 0.04789926
Iteration 17, loss = 0.04457133
Iteration 18, loss = 0.04220645
Iteration 19, loss = 0.04005639
Iteration 20, loss = 0.03821465
Iteration 21, loss = 0.03641288
Iteration 22, loss = 0.03507827
Iteration 23, loss = 0.03410452
Iteration 24, loss = 0.03291806
Iteration 25, loss = 0.03202874
Iteration 26, loss = 0.03115280
Iteration 27, loss = 0.03057729
Iteration 28, loss = 0.02975953
Iteration 29, loss = 0.02938798
Iteration 30, loss = 0.02905495
Iteration 31, loss = 0.02833664
Iteration 32, loss = 0.02786947
Iteration 33, loss = 0.02760450
Iteration 34, loss = 0.02751344
Iteration 35, loss = 0.02672244
Iteration 36, loss = 0.02657955
Iteration 37, loss = 0.02641586
Iteration 38, loss = 0.02610922
Iteration 39, loss = 0.02603443
Iteration 40, loss = 0.02577796
Iteration 41, loss = 0.02585449
Iteration 42, loss = 0.02536320
Iteration 43, loss = 0.02525305
Iteration 44, loss = 0.02506438
Iteration 45, loss = 0.02520973
Iteration 46, loss = 0.02469776
Iteration 47, loss = 0.02464457
Iteration 48, loss = 0.02459150
Iteration 49, loss = 0.02434409
Iteration 50, loss = 0.02449750
Iteration 51, loss = 0.02443067
Iteration 52, loss = 0.02471988
Iteration 53, loss = 0.02404979
Iteration 54, loss = 0.02420145
Iteration 55, loss = 0.02403625
Iteration 56, loss = 0.02406362
Iteration 57, loss = 0.02377528
Iteration 58, loss = 0.02368943
Iteration 59, loss = 0.02372388
Iteration 60, loss = 0.02375806
Iteration 61, loss = 0.02352728
Iteration 62, loss = 0.02370359
Iteration 63, loss = 0.02344515
Iteration 64, loss = 0.02359075
Iteration 65, loss = 0.02343590
Iteration 66, loss = 0.02353259
Iteration 67, loss = 0.02309859
Iteration 68, loss = 0.02321855
Iteration 69, loss = 0.02332428
Iteration 70, loss = 0.02300099
Iteration 71, loss = 0.02322713
Iteration 72, loss = 0.02339231
Iteration 73, loss = 0.02316973
Iteration 74, loss = 0.02308809
Iteration 75, loss = 0.02311747
Iteration 76, loss = 0.02280633
Iteration 77, loss = 0.02312473
Iteration 78, loss = 0.02277606
Iteration 79, loss = 0.02292376
Iteration 80, loss = 0.02287892
Iteration 81, loss = 0.02301607
Iteration 82, loss = 0.02279723
Iteration 83, loss = 0.02306280
Iteration 84, loss = 0.02272052
Iteration 85, loss = 0.02264217
Iteration 86, loss = 0.02306691
Iteration 87, loss = 0.02286191
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68574134
Iteration 2, loss = 0.65803892
Iteration 3, loss = 0.62669108
Iteration 4, loss = 0.58838196
Iteration 5, loss = 0.54373668
Iteration 6, loss = 0.49559035
Iteration 7, loss = 0.44680627
Iteration 8, loss = 0.39969259
Iteration 9, loss = 0.35563543
Iteration 10, loss = 0.31476852
Iteration 11, loss = 0.27802780
Iteration 12, loss = 0.24521432
Iteration 13, loss = 0.21665257
Iteration 14, loss = 0.19189552
Iteration 15, loss = 0.17052925
Iteration 16, loss = 0.15240765
Iteration 17, loss = 0.13662381
Iteration 18, loss = 0.12313435
Iteration 19, loss = 0.11139324
Iteration 20, loss = 0.10132270
Iteration 21, loss = 0.09255557
Iteration 22, loss = 0.08493540
Iteration 23, loss = 0.07831919
Iteration 24, loss = 0.07252489
Iteration 25, loss = 0.06747142
Iteration 26, loss = 0.06295662
Iteration 27, loss = 0.05892753
Iteration 28, loss = 0.05541372
Iteration 29, loss = 0.05223752
Iteration 30, loss = 0.04934630
Iteration 31, loss = 0.04672902
Iteration 32, loss = 0.04440155
Iteration 33, loss = 0.04224989
Iteration 34, loss = 0.04029292
Iteration 35, loss = 0.03869592
Iteration 36, loss = 0.03711840
Iteration 37, loss = 0.03555712
Iteration 38, loss = 0.03430796
Iteration 39, loss = 0.03303381
Iteration 40, loss = 0.03191194
Iteration 41, loss = 0.03092501
Iteration 42, loss = 0.02987758
Iteration 43, loss = 0.02893616
Iteration 44, loss = 0.02814484
Iteration 45, loss = 0.02731624
Iteration 46, loss = 0.02657282
Iteration 47, loss = 0.02591485
Iteration 48, loss = 0.02538923
Iteration 49, loss = 0.02480785
Iteration 50, loss = 0.02411612
Iteration 51, loss = 0.02366033
Iteration 52, loss = 0.02315276
Iteration 53, loss = 0.02254396
Iteration 54, loss = 0.02214364
Iteration 55, loss = 0.02173316
Iteration 56, loss = 0.02142685
Iteration 57, loss = 0.02099926
Iteration 58, loss = 0.02074149
Iteration 59, loss = 0.02042771
Iteration 60, loss = 0.01992867
Iteration 61, loss = 0.01990968
Iteration 62, loss = 0.01965455
Iteration 63, loss = 0.01936489
Iteration 64, loss = 0.01905067
Iteration 65, loss = 0.01880859
Iteration 66, loss = 0.01854734
Iteration 67, loss = 0.01834711
Iteration 68, loss = 0.01816247
Iteration 69, loss = 0.01796306
Iteration 70, loss = 0.01769212
Iteration 71, loss = 0.01746981
Iteration 72, loss = 0.01705693
Iteration 73, loss = 0.01711311
Iteration 74, loss = 0.01700325
Iteration 75, loss = 0.01695370
Iteration 76, loss = 0.01673542
Iteration 77, loss = 0.01655844
Iteration 78, loss = 0.01638975
Iteration 79, loss = 0.01618957
Iteration 80, loss = 0.01614505
Iteration 81, loss = 0.01601360
Iteration 82, loss = 0.01574590
Iteration 83, loss = 0.01570070
Iteration 84, loss = 0.01593322
Iteration 85, loss = 0.01586580
Iteration 86, loss = 0.01575485
Iteration 87, loss = 0.01549592
Iteration 88, loss = 0.01526735
Iteration 89, loss = 0.01507923
Iteration 90, loss = 0.01503099
Iteration 91, loss = 0.01489354
Iteration 92, loss = 0.01483155
Iteration 93, loss = 0.01469680
Iteration 94, loss = 0.01465498
Iteration 95, loss = 0.01454829
Iteration 96, loss = 0.01445993
Iteration 97, loss = 0.01441925
Iteration 98, loss = 0.01435973
Iteration 99, loss = 0.01426256
Iteration 100, loss = 0.01424381
Iteration 101, loss = 0.01418862
Iteration 102, loss = 0.01404872
Iteration 103, loss = 0.01402439
Iteration 104, loss = 0.01392807
Iteration 105, loss = 0.01386864
Iteration 106, loss = 0.01381024
Iteration 107, loss = 0.01376621
Iteration 108, loss = 0.01410271
Iteration 109, loss = 0.01403878
Iteration 110, loss = 0.01421260
Iteration 111, loss = 0.01424054
Iteration 112, loss = 0.01407420
Iteration 113, loss = 0.01408879
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67676039
Iteration 2, loss = 0.61750969
Iteration 3, loss = 0.52680336
Iteration 4, loss = 0.43037263
Iteration 5, loss = 0.34206176
Iteration 6, loss = 0.26776501
Iteration 7, loss = 0.21054051
Iteration 8, loss = 0.16869587
Iteration 9, loss = 0.13775822
Iteration 10, loss = 0.11543566
Iteration 11, loss = 0.09889207
Iteration 12, loss = 0.08624735
Iteration 13, loss = 0.07679332
Iteration 14, loss = 0.06917533
Iteration 15, loss = 0.06295914
Iteration 16, loss = 0.05857244
Iteration 17, loss = 0.05498915
Iteration 18, loss = 0.05149914
Iteration 19, loss = 0.04808402
Iteration 20, loss = 0.04575154
Iteration 21, loss = 0.04358004
Iteration 22, loss = 0.04172699
Iteration 23, loss = 0.04019431
Iteration 24, loss = 0.03880439
Iteration 25, loss = 0.03786680
Iteration 26, loss = 0.03679539
Iteration 27, loss = 0.03617413
Iteration 28, loss = 0.03507582
Iteration 29, loss = 0.03446934
Iteration 30, loss = 0.03379245
Iteration 31, loss = 0.03342617
Iteration 32, loss = 0.03293439
Iteration 33, loss = 0.03310668
Iteration 34, loss = 0.03246462
Iteration 35, loss = 0.03151497
Iteration 36, loss = 0.03111821
Iteration 37, loss = 0.03055043
Iteration 38, loss = 0.03057226
Iteration 39, loss = 0.02992996
Iteration 40, loss = 0.02968186
Iteration 41, loss = 0.02939897
Iteration 42, loss = 0.02946186
Iteration 43, loss = 0.02903943
Iteration 44, loss = 0.02894177
Iteration 45, loss = 0.02937061
Iteration 46, loss = 0.02938672
Iteration 47, loss = 0.02848577
Iteration 48, loss = 0.02820975
Iteration 49, loss = 0.02780121
Iteration 50, loss = 0.02746783
Iteration 51, loss = 0.02781143
Iteration 52, loss = 0.02756646
Iteration 53, loss = 0.02745477
Iteration 54, loss = 0.02719297
Iteration 55, loss = 0.02720696
Iteration 56, loss = 0.02698949
Iteration 57, loss = 0.02689144
Iteration 58, loss = 0.02671787
Iteration 59, loss = 0.02719080
Iteration 60, loss = 0.02698233
Iteration 61, loss = 0.02699630
Iteration 62, loss = 0.02698093
Iteration 63, loss = 0.02665441
Iteration 64, loss = 0.02657163
Iteration 65, loss = 0.02644526
Iteration 66, loss = 0.02643289
Iteration 67, loss = 0.02609186
Iteration 68, loss = 0.02623169
Iteration 69, loss = 0.02618589
Iteration 70, loss = 0.02625471
Iteration 71, loss = 0.02626319
Iteration 72, loss = 0.02605518
Iteration 73, loss = 0.02591910
Iteration 74, loss = 0.02595954
Iteration 75, loss = 0.02575837
Iteration 76, loss = 0.02590774
Iteration 77, loss = 0.02592874
Iteration 78, loss = 0.02588281
Iteration 79, loss = 0.02602957
Iteration 80, loss = 0.02565401
Iteration 81, loss = 0.02577311
Iteration 82, loss = 0.02562236
Iteration 83, loss = 0.02549010
Iteration 84, loss = 0.02549039
Iteration 85, loss = 0.02607003
Iteration 86, loss = 0.02572754
Iteration 87, loss = 0.02586314
Iteration 88, loss = 0.02550460
Iteration 89, loss = 0.02555661
Iteration 90, loss = 0.02566137
Iteration 91, loss = 0.02530256
Iteration 92, loss = 0.02557388
Iteration 93, loss = 0.02558097
Iteration 94, loss = 0.02578604
Iteration 95, loss = 0.02569584
Iteration 96, loss = 0.02549139
Iteration 97, loss = 0.02540524
Iteration 98, loss = 0.02562421
Iteration 99, loss = 0.02567879
Iteration 100, loss = 0.02552449
Iteration 101, loss = 0.02579701
Iteration 102, loss = 0.02534526
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68624443
Iteration 2, loss = 0.61603903
Iteration 3, loss = 0.51950303
Iteration 4, loss = 0.41730103
Iteration 5, loss = 0.32438514
Iteration 6, loss = 0.24964867
Iteration 7, loss = 0.19363130
Iteration 8, loss = 0.15378745
Iteration 9, loss = 0.12497029
Iteration 10, loss = 0.10407997
Iteration 11, loss = 0.08875767
Iteration 12, loss = 0.07736171
Iteration 13, loss = 0.06834246
Iteration 14, loss = 0.06118734
Iteration 15, loss = 0.05559412
Iteration 16, loss = 0.05128308
Iteration 17, loss = 0.04749859
Iteration 18, loss = 0.04449411
Iteration 19, loss = 0.04186285
Iteration 20, loss = 0.03973398
Iteration 21, loss = 0.03792283
Iteration 22, loss = 0.03620601
Iteration 23, loss = 0.03484837
Iteration 24, loss = 0.03372854
Iteration 25, loss = 0.03237283
Iteration 26, loss = 0.03192290
Iteration 27, loss = 0.03106721
Iteration 28, loss = 0.03002391
Iteration 29, loss = 0.02945493
Iteration 30, loss = 0.02917154
Iteration 31, loss = 0.02846025
Iteration 32, loss = 0.02802302
Iteration 33, loss = 0.02762271
Iteration 34, loss = 0.02729304
Iteration 35, loss = 0.02729345
Iteration 36, loss = 0.02659257
Iteration 37, loss = 0.02610512
Iteration 38, loss = 0.02605088
Iteration 39, loss = 0.02531367
Iteration 40, loss = 0.02524254
Iteration 41, loss = 0.02521827
Iteration 42, loss = 0.02480725
Iteration 43, loss = 0.02479967
Iteration 44, loss = 0.02445355
Iteration 45, loss = 0.02444455
Iteration 46, loss = 0.02438139
Iteration 47, loss = 0.02455827
Iteration 48, loss = 0.02432558
Iteration 49, loss = 0.02399300
Iteration 50, loss = 0.02391558
Iteration 51, loss = 0.02381297
Iteration 52, loss = 0.02379195
Iteration 53, loss = 0.02339909
Iteration 54, loss = 0.02373383
Iteration 55, loss = 0.02328262
Iteration 56, loss = 0.02317131
Iteration 57, loss = 0.02314489
Iteration 58, loss = 0.02331867
Iteration 59, loss = 0.02325432
Iteration 60, loss = 0.02293244
Iteration 61, loss = 0.02340837
Iteration 62, loss = 0.02296223
Iteration 63, loss = 0.02300750
Iteration 64, loss = 0.02297018
Iteration 65, loss = 0.02266980
Iteration 66, loss = 0.02319575
Iteration 67, loss = 0.02271900
Iteration 68, loss = 0.02277113
Iteration 69, loss = 0.02262696
Iteration 70, loss = 0.02258387
Iteration 71, loss = 0.02304900
Iteration 72, loss = 0.02269011
Iteration 73, loss = 0.02245549
Iteration 74, loss = 0.02249338
Iteration 75, loss = 0.02214676
Iteration 76, loss = 0.02233291
Iteration 77, loss = 0.02232829
Iteration 78, loss = 0.02215284
Iteration 79, loss = 0.02220804
Iteration 80, loss = 0.02231640
Iteration 81, loss = 0.02230378
Iteration 82, loss = 0.02241105
Iteration 83, loss = 0.02196287
Iteration 84, loss = 0.02200856
Iteration 85, loss = 0.02246605
Iteration 86, loss = 0.02218931
Iteration 87, loss = 0.02211070
Iteration 88, loss = 0.02200782
Iteration 89, loss = 0.02220845
Iteration 90, loss = 0.02207152
Iteration 91, loss = 0.02185639
Iteration 92, loss = 0.02212183
Iteration 93, loss = 0.02195957
Iteration 94, loss = 0.02171330
Iteration 95, loss = 0.02191437
Iteration 96, loss = 0.02180334
Iteration 97, loss = 0.02200694
Iteration 98, loss = 0.02220136
Iteration 99, loss = 0.02185373
Iteration 100, loss = 0.02176740
Iteration 101, loss = 0.02179678
Iteration 102, loss = 0.02178732
Iteration 103, loss = 0.02197338
Iteration 104, loss = 0.02187100
Iteration 105, loss = 0.02169616
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68927967
Iteration 2, loss = 0.64984582
Iteration 3, loss = 0.59624883
Iteration 4, loss = 0.52769324
Iteration 5, loss = 0.45215084
Iteration 6, loss = 0.38116763
Iteration 7, loss = 0.31815993
Iteration 8, loss = 0.26509636
Iteration 9, loss = 0.22150546
Iteration 10, loss = 0.18618032
Iteration 11, loss = 0.15847329
Iteration 12, loss = 0.13626154
Iteration 13, loss = 0.11844296
Iteration 14, loss = 0.10407495
Iteration 15, loss = 0.09260873
Iteration 16, loss = 0.08314220
Iteration 17, loss = 0.07545149
Iteration 18, loss = 0.06873699
Iteration 19, loss = 0.06333575
Iteration 20, loss = 0.05864170
Iteration 21, loss = 0.05460456
Iteration 22, loss = 0.05104199
Iteration 23, loss = 0.04812357
Iteration 24, loss = 0.04560913
Iteration 25, loss = 0.04329296
Iteration 26, loss = 0.04133165
Iteration 27, loss = 0.03930858
Iteration 28, loss = 0.03766722
Iteration 29, loss = 0.03621306
Iteration 30, loss = 0.03510517
Iteration 31, loss = 0.03377000
Iteration 32, loss = 0.03265266
Iteration 33, loss = 0.03197866
Iteration 34, loss = 0.03089400
Iteration 35, loss = 0.03008621
Iteration 36, loss = 0.02936696
Iteration 37, loss = 0.02861017
Iteration 38, loss = 0.02798615
Iteration 39, loss = 0.02742200
Iteration 40, loss = 0.02694341
Iteration 41, loss = 0.02627710
Iteration 42, loss = 0.02583861
Iteration 43, loss = 0.02555219
Iteration 44, loss = 0.02506976
Iteration 45, loss = 0.02467988
Iteration 46, loss = 0.02430205
Iteration 47, loss = 0.02419142
Iteration 48, loss = 0.02367399
Iteration 49, loss = 0.02344519
Iteration 50, loss = 0.02320037
Iteration 51, loss = 0.02282458
Iteration 52, loss = 0.02279456
Iteration 53, loss = 0.02249353
Iteration 54, loss = 0.02228209
Iteration 55, loss = 0.02197391
Iteration 56, loss = 0.02190988
Iteration 57, loss = 0.02166071
Iteration 58, loss = 0.02156885
Iteration 59, loss = 0.02151658
Iteration 60, loss = 0.02116258
Iteration 61, loss = 0.02106221
Iteration 62, loss = 0.02106070
Iteration 63, loss = 0.02081442
Iteration 64, loss = 0.02063294
Iteration 65, loss = 0.02072246
Iteration 66, loss = 0.02048861
Iteration 67, loss = 0.02048688
Iteration 68, loss = 0.02010919
Iteration 69, loss = 0.02019726
Iteration 70, loss = 0.01998941
Iteration 71, loss = 0.01980736
Iteration 72, loss = 0.01993772
Iteration 73, loss = 0.01980085
Iteration 74, loss = 0.01982079
Iteration 75, loss = 0.01958590
Iteration 76, loss = 0.01953352
Iteration 77, loss = 0.01943800
Iteration 78, loss = 0.01946853
Iteration 79, loss = 0.01926908
Iteration 80, loss = 0.01919049
Iteration 81, loss = 0.01935521
Iteration 82, loss = 0.01925342
Iteration 83, loss = 0.01923518
Iteration 84, loss = 0.01903770
Iteration 85, loss = 0.01903184
Iteration 86, loss = 0.01897691
Iteration 87, loss = 0.01894822
Iteration 88, loss = 0.01887454
Iteration 89, loss = 0.01907534
Iteration 90, loss = 0.01883199
Iteration 91, loss = 0.01881287
Iteration 92, loss = 0.01852955
Iteration 93, loss = 0.01854619
Iteration 94, loss = 0.01872469
Iteration 95, loss = 0.01850973
Iteration 96, loss = 0.01859126
Iteration 97, loss = 0.01860587
Iteration 98, loss = 0.01841907
Iteration 99, loss = 0.01846257
Iteration 100, loss = 0.01848551
Iteration 101, loss = 0.01842966
Iteration 102, loss = 0.01849518
Iteration 103, loss = 0.01846117
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67806534
Iteration 2, loss = 0.62863225
Iteration 3, loss = 0.55513597
Iteration 4, loss = 0.47032003
Iteration 5, loss = 0.38663617
Iteration 6, loss = 0.31140715
Iteration 7, loss = 0.24944382
Iteration 8, loss = 0.20088638
Iteration 9, loss = 0.16450597
Iteration 10, loss = 0.13674921
Iteration 11, loss = 0.11613114
Iteration 12, loss = 0.10027677
Iteration 13, loss = 0.08805816
Iteration 14, loss = 0.07832945
Iteration 15, loss = 0.07086701
Iteration 16, loss = 0.06464180
Iteration 17, loss = 0.05964710
Iteration 18, loss = 0.05532932
Iteration 19, loss = 0.05179166
Iteration 20, loss = 0.04904132
Iteration 21, loss = 0.04651227
Iteration 22, loss = 0.04437809
Iteration 23, loss = 0.04232685
Iteration 24, loss = 0.04079913
Iteration 25, loss = 0.03923928
Iteration 26, loss = 0.03801148
Iteration 27, loss = 0.03692741
Iteration 28, loss = 0.03599418
Iteration 29, loss = 0.03508421
Iteration 30, loss = 0.03424138
Iteration 31, loss = 0.03369985
Iteration 32, loss = 0.03308576
Iteration 33, loss = 0.03233471
Iteration 34, loss = 0.03212716
Iteration 35, loss = 0.03139882
Iteration 36, loss = 0.03074740
Iteration 37, loss = 0.03036708
Iteration 38, loss = 0.03012062
Iteration 39, loss = 0.02980321
Iteration 40, loss = 0.02946005
Iteration 41, loss = 0.02932424
Iteration 42, loss = 0.02883615
Iteration 43, loss = 0.02858407
Iteration 44, loss = 0.02858858
Iteration 45, loss = 0.02831353
Iteration 46, loss = 0.02791867
Iteration 47, loss = 0.02788198
Iteration 48, loss = 0.02748497
Iteration 49, loss = 0.02771902
Iteration 50, loss = 0.02742675
Iteration 51, loss = 0.02738120
Iteration 52, loss = 0.02696741
Iteration 53, loss = 0.02728142
Iteration 54, loss = 0.02681315
Iteration 55, loss = 0.02661936
Iteration 56, loss = 0.02674340
Iteration 57, loss = 0.02644619
Iteration 58, loss = 0.02639600
Iteration 59, loss = 0.02643851
Iteration 60, loss = 0.02627745
Iteration 61, loss = 0.02631008
Iteration 62, loss = 0.02613142
Iteration 63, loss = 0.02613465
Iteration 64, loss = 0.02613394
Iteration 65, loss = 0.02584560
Iteration 66, loss = 0.02572866
Iteration 67, loss = 0.02567981
Iteration 68, loss = 0.02593355
Iteration 69, loss = 0.02582974
Iteration 70, loss = 0.02573646
Iteration 71, loss = 0.02562513
Iteration 72, loss = 0.02534462
Iteration 73, loss = 0.02555606
Iteration 74, loss = 0.02544145
Iteration 75, loss = 0.02534589
Iteration 76, loss = 0.02528448
Iteration 77, loss = 0.02536480
Iteration 78, loss = 0.02534276
Iteration 79, loss = 0.02515401
Iteration 80, loss = 0.02538946
Iteration 81, loss = 0.02513371
Iteration 82, loss = 0.02516176
Iteration 83, loss = 0.02506620
Iteration 84, loss = 0.02493705
Iteration 85, loss = 0.02510463
Iteration 86, loss = 0.02524838
Iteration 87, loss = 0.02495795
Iteration 88, loss = 0.02513317
Iteration 89, loss = 0.02489470
Iteration 90, loss = 0.02484148
Iteration 91, loss = 0.02492820
Iteration 92, loss = 0.02517212
Iteration 93, loss = 0.02491829
Iteration 94, loss = 0.02480993
Iteration 95, loss = 0.02470228
Iteration 96, loss = 0.02471310
Iteration 97, loss = 0.02490722
Iteration 98, loss = 0.02487744
Iteration 99, loss = 0.02475010
Iteration 100, loss = 0.02503217
Iteration 101, loss = 0.02485252
Iteration 102, loss = 0.02474114
Iteration 103, loss = 0.02463300
Iteration 104, loss = 0.02476745
Iteration 105, loss = 0.02476098
Iteration 106, loss = 0.02468126
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69942944
Iteration 2, loss = 0.64148405
Iteration 3, loss = 0.55376843
Iteration 4, loss = 0.44948325
Iteration 5, loss = 0.34977647
Iteration 6, loss = 0.26649896
Iteration 7, loss = 0.20405224
Iteration 8, loss = 0.15981249
Iteration 9, loss = 0.12873215
Iteration 10, loss = 0.10656053
Iteration 11, loss = 0.09043982
Iteration 12, loss = 0.07852928
Iteration 13, loss = 0.06970542
Iteration 14, loss = 0.06246007
Iteration 15, loss = 0.05694737
Iteration 16, loss = 0.05243030
Iteration 17, loss = 0.04863344
Iteration 18, loss = 0.04578305
Iteration 19, loss = 0.04314863
Iteration 20, loss = 0.04090210
Iteration 21, loss = 0.03905203
Iteration 22, loss = 0.03740227
Iteration 23, loss = 0.03606081
Iteration 24, loss = 0.03496205
Iteration 25, loss = 0.03393174
Iteration 26, loss = 0.03287667
Iteration 27, loss = 0.03199459
Iteration 28, loss = 0.03132331
Iteration 29, loss = 0.03053048
Iteration 30, loss = 0.03014221
Iteration 31, loss = 0.02970368
Iteration 32, loss = 0.02917533
Iteration 33, loss = 0.02880365
Iteration 34, loss = 0.02862656
Iteration 35, loss = 0.02797042
Iteration 36, loss = 0.02744766
Iteration 37, loss = 0.02716830
Iteration 38, loss = 0.02703915
Iteration 39, loss = 0.02703991
Iteration 40, loss = 0.02669092
Iteration 41, loss = 0.02631030
Iteration 42, loss = 0.02614638
Iteration 43, loss = 0.02591535
Iteration 44, loss = 0.02588940
Iteration 45, loss = 0.02598825
Iteration 46, loss = 0.02555788
Iteration 47, loss = 0.02540543
Iteration 48, loss = 0.02519972
Iteration 49, loss = 0.02547832
Iteration 50, loss = 0.02476298
Iteration 51, loss = 0.02501656
Iteration 52, loss = 0.02475528
Iteration 53, loss = 0.02463521
Iteration 54, loss = 0.02466024
Iteration 55, loss = 0.02459899
Iteration 56, loss = 0.02442384
Iteration 57, loss = 0.02487058
Iteration 58, loss = 0.02456415
Iteration 59, loss = 0.02406263
Iteration 60, loss = 0.02409990
Iteration 61, loss = 0.02390842
Iteration 62, loss = 0.02411527
Iteration 63, loss = 0.02431082
Iteration 64, loss = 0.02389526
Iteration 65, loss = 0.02400570
Iteration 66, loss = 0.02388735
Iteration 67, loss = 0.02385401
Iteration 68, loss = 0.02383367
Iteration 69, loss = 0.02381545
Iteration 70, loss = 0.02380710
Iteration 71, loss = 0.02361691
Iteration 72, loss = 0.02356772
Iteration 73, loss = 0.02339605
Iteration 74, loss = 0.02370834
Iteration 75, loss = 0.02363824
Iteration 76, loss = 0.02366427
Iteration 77, loss = 0.02351474
Iteration 78, loss = 0.02328771
Iteration 79, loss = 0.02351325
Iteration 80, loss = 0.02354000
Iteration 81, loss = 0.02347479
Iteration 82, loss = 0.02318261
Iteration 83, loss = 0.02326441
Iteration 84, loss = 0.02325686
Iteration 85, loss = 0.02333895
Iteration 86, loss = 0.02321824
Iteration 87, loss = 0.02345198
Iteration 88, loss = 0.02292597
Iteration 89, loss = 0.02306829
Iteration 90, loss = 0.02335638
Iteration 91, loss = 0.02303557
Iteration 92, loss = 0.02332269
Iteration 93, loss = 0.02336592
Iteration 94, loss = 0.02303283
Iteration 95, loss = 0.02320006
Iteration 96, loss = 0.02300265
Iteration 97, loss = 0.02305884
Iteration 98, loss = 0.02300904
Iteration 99, loss = 0.02316076
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69561131
Iteration 2, loss = 0.62822706
Iteration 3, loss = 0.53116532
Iteration 4, loss = 0.41827220
Iteration 5, loss = 0.31602158
Iteration 6, loss = 0.23724270
Iteration 7, loss = 0.18060983
Iteration 8, loss = 0.14150893
Iteration 9, loss = 0.11460395
Iteration 10, loss = 0.09574413
Iteration 11, loss = 0.08207945
Iteration 12, loss = 0.07136198
Iteration 13, loss = 0.06339527
Iteration 14, loss = 0.05715135
Iteration 15, loss = 0.05265493
Iteration 16, loss = 0.04874115
Iteration 17, loss = 0.04502405
Iteration 18, loss = 0.04221770
Iteration 19, loss = 0.04016355
Iteration 20, loss = 0.03823098
Iteration 21, loss = 0.03666656
Iteration 22, loss = 0.03543103
Iteration 23, loss = 0.03433694
Iteration 24, loss = 0.03310255
Iteration 25, loss = 0.03216689
Iteration 26, loss = 0.03126118
Iteration 27, loss = 0.03068122
Iteration 28, loss = 0.03002140
Iteration 29, loss = 0.02967940
Iteration 30, loss = 0.02897469
Iteration 31, loss = 0.02859804
Iteration 32, loss = 0.02814568
Iteration 33, loss = 0.02780854
Iteration 34, loss = 0.02765808
Iteration 35, loss = 0.02711144
Iteration 36, loss = 0.02682842
Iteration 37, loss = 0.02680674
Iteration 38, loss = 0.02639466
Iteration 39, loss = 0.02627190
Iteration 40, loss = 0.02603948
Iteration 41, loss = 0.02588366
Iteration 42, loss = 0.02564605
Iteration 43, loss = 0.02532188
Iteration 44, loss = 0.02542703
Iteration 45, loss = 0.02522839
Iteration 46, loss = 0.02497596
Iteration 47, loss = 0.02478612
Iteration 48, loss = 0.02477362
Iteration 49, loss = 0.02460812
Iteration 50, loss = 0.02465430
Iteration 51, loss = 0.02451223
Iteration 52, loss = 0.02441087
Iteration 53, loss = 0.02434588
Iteration 54, loss = 0.02418859
Iteration 55, loss = 0.02422638
Iteration 56, loss = 0.02428635
Iteration 57, loss = 0.02409186
Iteration 58, loss = 0.02390244
Iteration 59, loss = 0.02368492
Iteration 60, loss = 0.02380135
Iteration 61, loss = 0.02398470
Iteration 62, loss = 0.02390761
Iteration 63, loss = 0.02375723
Iteration 64, loss = 0.02374967
Iteration 65, loss = 0.02357468
Iteration 66, loss = 0.02350798
Iteration 67, loss = 0.02368830
Iteration 68, loss = 0.02368381
Iteration 69, loss = 0.02348786
Iteration 70, loss = 0.02335863
Iteration 71, loss = 0.02355844
Iteration 72, loss = 0.02325812
Iteration 73, loss = 0.02333747
Iteration 74, loss = 0.02358417
Iteration 75, loss = 0.02349841
Iteration 76, loss = 0.02321059
Iteration 77, loss = 0.02304624
Iteration 78, loss = 0.02333163
Iteration 79, loss = 0.02328673
Iteration 80, loss = 0.02323462
Iteration 81, loss = 0.02321999
Iteration 82, loss = 0.02326334
Iteration 83, loss = 0.02326408
Iteration 84, loss = 0.02307736
Iteration 85, loss = 0.02285878
Iteration 86, loss = 0.02339359
Iteration 87, loss = 0.02289755
Iteration 88, loss = 0.02286889
Iteration 89, loss = 0.02301680
Iteration 90, loss = 0.02308737
Iteration 91, loss = 0.02291223
Iteration 92, loss = 0.02306115
Iteration 93, loss = 0.02291832
Iteration 94, loss = 0.02284472
Iteration 95, loss = 0.02306445
Iteration 96, loss = 0.02284279
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67882397
Iteration 2, loss = 0.64612653
Iteration 3, loss = 0.60590277
Iteration 4, loss = 0.55327318
Iteration 5, loss = 0.49340066
Iteration 6, loss = 0.43286359
Iteration 7, loss = 0.37584871
Iteration 8, loss = 0.32404601
Iteration 9, loss = 0.27850990
Iteration 10, loss = 0.23956880
Iteration 11, loss = 0.20652683
Iteration 12, loss = 0.17894497
Iteration 13, loss = 0.15614656
Iteration 14, loss = 0.13726558
Iteration 15, loss = 0.12144731
Iteration 16, loss = 0.10846876
Iteration 17, loss = 0.09746007
Iteration 18, loss = 0.08823891
Iteration 19, loss = 0.08035692
Iteration 20, loss = 0.07381283
Iteration 21, loss = 0.06807684
Iteration 22, loss = 0.06325840
Iteration 23, loss = 0.05893808
Iteration 24, loss = 0.05536056
Iteration 25, loss = 0.05205643
Iteration 26, loss = 0.04905842
Iteration 27, loss = 0.04648208
Iteration 28, loss = 0.04419654
Iteration 29, loss = 0.04214416
Iteration 30, loss = 0.04037228
Iteration 31, loss = 0.03877326
Iteration 32, loss = 0.03726875
Iteration 33, loss = 0.03590851
Iteration 34, loss = 0.03472066
Iteration 35, loss = 0.03354118
Iteration 36, loss = 0.03260040
Iteration 37, loss = 0.03162282
Iteration 38, loss = 0.03083279
Iteration 39, loss = 0.03010215
Iteration 40, loss = 0.02932088
Iteration 41, loss = 0.02873472
Iteration 42, loss = 0.02811468
Iteration 43, loss = 0.02741139
Iteration 44, loss = 0.02709821
Iteration 45, loss = 0.02654010
Iteration 46, loss = 0.02610752
Iteration 47, loss = 0.02570594
Iteration 48, loss = 0.02516904
Iteration 49, loss = 0.02489931
Iteration 50, loss = 0.02447110
Iteration 51, loss = 0.02413693
Iteration 52, loss = 0.02375725
Iteration 53, loss = 0.02347355
Iteration 54, loss = 0.02323842
Iteration 55, loss = 0.02305491
Iteration 56, loss = 0.02278122
Iteration 57, loss = 0.02245370
Iteration 58, loss = 0.02225762
Iteration 59, loss = 0.02206087
Iteration 60, loss = 0.02188935
Iteration 61, loss = 0.02167571
Iteration 62, loss = 0.02150040
Iteration 63, loss = 0.02140224
Iteration 64, loss = 0.02111875
Iteration 65, loss = 0.02098107
Iteration 66, loss = 0.02098122
Iteration 67, loss = 0.02073706
Iteration 68, loss = 0.02060472
Iteration 69, loss = 0.02052214
Iteration 70, loss = 0.02037610
Iteration 71, loss = 0.02029299
Iteration 72, loss = 0.02016868
Iteration 73, loss = 0.02014755
Iteration 74, loss = 0.01997817
Iteration 75, loss = 0.01999966
Iteration 76, loss = 0.01977972
Iteration 77, loss = 0.01965394
Iteration 78, loss = 0.01952818
Iteration 79, loss = 0.01952910
Iteration 80, loss = 0.01944457
Iteration 81, loss = 0.01929252
Iteration 82, loss = 0.01928682
Iteration 83, loss = 0.01917759
Iteration 84, loss = 0.01895998
Iteration 85, loss = 0.01904206
Iteration 86, loss = 0.01909274
Iteration 87, loss = 0.01895973
Iteration 88, loss = 0.01892075
Iteration 89, loss = 0.01888227
Iteration 90, loss = 0.01880033
Iteration 91, loss = 0.01873341
Iteration 92, loss = 0.01867526
Iteration 93, loss = 0.01860035
Iteration 94, loss = 0.01868464
Iteration 95, loss = 0.01849828
Iteration 96, loss = 0.01852315
Iteration 97, loss = 0.01842477
Iteration 98, loss = 0.01841228
Iteration 99, loss = 0.01836361
Iteration 100, loss = 0.01831812
Iteration 101, loss = 0.01823175
Iteration 102, loss = 0.01828849
Iteration 103, loss = 0.01825986
Iteration 104, loss = 0.01810615
Iteration 105, loss = 0.01802167
Iteration 106, loss = 0.01816445
Iteration 107, loss = 0.01800259
Iteration 108, loss = 0.01794204
Iteration 109, loss = 0.01806587
Iteration 110, loss = 0.01793286
Iteration 111, loss = 0.01796465
Iteration 112, loss = 0.01808715
Iteration 113, loss = 0.01790045
Iteration 114, loss = 0.01780016
Iteration 115, loss = 0.01785103
Iteration 116, loss = 0.01785440
Iteration 117, loss = 0.01788902
Iteration 118, loss = 0.01777578
Iteration 119, loss = 0.01766991
Iteration 120, loss = 0.01769740
Iteration 121, loss = 0.01757545
Iteration 122, loss = 0.01765004
Iteration 123, loss = 0.01761783
Iteration 124, loss = 0.01768509
Iteration 125, loss = 0.01752076
Iteration 126, loss = 0.01763916
Iteration 127, loss = 0.01770946
Iteration 128, loss = 0.01784983
Iteration 129, loss = 0.01765966
Iteration 130, loss = 0.01758813
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68558039
Iteration 2, loss = 0.61916016
Iteration 3, loss = 0.52264632
Iteration 4, loss = 0.41388261
Iteration 5, loss = 0.31782352
Iteration 6, loss = 0.24301327
Iteration 7, loss = 0.18853161
Iteration 8, loss = 0.15028097
Iteration 9, loss = 0.12280992
Iteration 10, loss = 0.10348974
Iteration 11, loss = 0.08922612
Iteration 12, loss = 0.07811277
Iteration 13, loss = 0.06983221
Iteration 14, loss = 0.06323233
Iteration 15, loss = 0.05820175
Iteration 16, loss = 0.05360207
Iteration 17, loss = 0.05012903
Iteration 18, loss = 0.04731114
Iteration 19, loss = 0.04487933
Iteration 20, loss = 0.04263177
Iteration 21, loss = 0.04109242
Iteration 22, loss = 0.03968491
Iteration 23, loss = 0.03808909
Iteration 24, loss = 0.03729840
Iteration 25, loss = 0.03623597
Iteration 26, loss = 0.03506594
Iteration 27, loss = 0.03405339
Iteration 28, loss = 0.03358222
Iteration 29, loss = 0.03283930
Iteration 30, loss = 0.03247261
Iteration 31, loss = 0.03195453
Iteration 32, loss = 0.03149282
Iteration 33, loss = 0.03106337
Iteration 34, loss = 0.03048146
Iteration 35, loss = 0.03037591
Iteration 36, loss = 0.03003123
Iteration 37, loss = 0.02978774
Iteration 38, loss = 0.02948834
Iteration 39, loss = 0.02894936
Iteration 40, loss = 0.02887788
Iteration 41, loss = 0.02850330
Iteration 42, loss = 0.02860800
Iteration 43, loss = 0.02824599
Iteration 44, loss = 0.02814298
Iteration 45, loss = 0.02797146
Iteration 46, loss = 0.02780784
Iteration 47, loss = 0.02793263
Iteration 48, loss = 0.02753390
Iteration 49, loss = 0.02761614
Iteration 50, loss = 0.02717894
Iteration 51, loss = 0.02726459
Iteration 52, loss = 0.02727547
Iteration 53, loss = 0.02700646
Iteration 54, loss = 0.02690838
Iteration 55, loss = 0.02724398
Iteration 56, loss = 0.02668837
Iteration 57, loss = 0.02643297
Iteration 58, loss = 0.02659496
Iteration 59, loss = 0.02653307
Iteration 60, loss = 0.02641445
Iteration 61, loss = 0.02625440
Iteration 62, loss = 0.02643138
Iteration 63, loss = 0.02625062
Iteration 64, loss = 0.02614156
Iteration 65, loss = 0.02617929
Iteration 66, loss = 0.02614923
Iteration 67, loss = 0.02619771
Iteration 68, loss = 0.02615995
Iteration 69, loss = 0.02601835
Iteration 70, loss = 0.02631686
Iteration 71, loss = 0.02618883
Iteration 72, loss = 0.02607683
Iteration 73, loss = 0.02619525
Iteration 74, loss = 0.02568261
Iteration 75, loss = 0.02593435
Iteration 76, loss = 0.02588613
Iteration 77, loss = 0.02582625
Iteration 78, loss = 0.02575356
Iteration 79, loss = 0.02560627
Iteration 80, loss = 0.02583947
Iteration 81, loss = 0.02554312
Iteration 82, loss = 0.02577497
Iteration 83, loss = 0.02539234
Iteration 84, loss = 0.02571272
Iteration 85, loss = 0.02583396
Iteration 86, loss = 0.02561496
Iteration 87, loss = 0.02544585
Iteration 88, loss = 0.02541259
Iteration 89, loss = 0.02548432
Iteration 90, loss = 0.02555918
Iteration 91, loss = 0.02582093
Iteration 92, loss = 0.02576634
Iteration 93, loss = 0.02517712
Iteration 94, loss = 0.02537328
Iteration 95, loss = 0.02540547
Iteration 96, loss = 0.02550439
Iteration 97, loss = 0.02571030
Iteration 98, loss = 0.02545321
Iteration 99, loss = 0.02529063
Iteration 100, loss = 0.02525385
Iteration 101, loss = 0.02531693
Iteration 102, loss = 0.02551289
Iteration 103, loss = 0.02570922
Iteration 104, loss = 0.02526939
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69273724
Iteration 2, loss = 0.66424629
Iteration 3, loss = 0.63289169
Iteration 4, loss = 0.59460487
Iteration 5, loss = 0.54971747
Iteration 6, loss = 0.50099039
Iteration 7, loss = 0.45153843
Iteration 8, loss = 0.40340729
Iteration 9, loss = 0.35838644
Iteration 10, loss = 0.31690022
Iteration 11, loss = 0.27948636
Iteration 12, loss = 0.24644408
Iteration 13, loss = 0.21768002
Iteration 14, loss = 0.19266622
Iteration 15, loss = 0.17126449
Iteration 16, loss = 0.15276929
Iteration 17, loss = 0.13702961
Iteration 18, loss = 0.12348988
Iteration 19, loss = 0.11193186
Iteration 20, loss = 0.10200681
Iteration 21, loss = 0.09334776
Iteration 22, loss = 0.08595263
Iteration 23, loss = 0.07970614
Iteration 24, loss = 0.07399934
Iteration 25, loss = 0.06899908
Iteration 26, loss = 0.06460049
Iteration 27, loss = 0.06061945
Iteration 28, loss = 0.05742990
Iteration 29, loss = 0.05437134
Iteration 30, loss = 0.05160030
Iteration 31, loss = 0.04908547
Iteration 32, loss = 0.04677883
Iteration 33, loss = 0.04462854
Iteration 34, loss = 0.04277401
Iteration 35, loss = 0.04116314
Iteration 36, loss = 0.03952963
Iteration 37, loss = 0.03812435
Iteration 38, loss = 0.03682822
Iteration 39, loss = 0.03558835
Iteration 40, loss = 0.03447084
Iteration 41, loss = 0.03356415
Iteration 42, loss = 0.03248868
Iteration 43, loss = 0.03184068
Iteration 44, loss = 0.03113323
Iteration 45, loss = 0.03041627
Iteration 46, loss = 0.02984109
Iteration 47, loss = 0.02909538
Iteration 48, loss = 0.02831125
Iteration 49, loss = 0.02774728
Iteration 50, loss = 0.02715333
Iteration 51, loss = 0.02661923
Iteration 52, loss = 0.02618487
Iteration 53, loss = 0.02585249
Iteration 54, loss = 0.02547637
Iteration 55, loss = 0.02482451
Iteration 56, loss = 0.02454425
Iteration 57, loss = 0.02444908
Iteration 58, loss = 0.02429629
Iteration 59, loss = 0.02374870
Iteration 60, loss = 0.02341391
Iteration 61, loss = 0.02311635
Iteration 62, loss = 0.02264438
Iteration 63, loss = 0.02232844
Iteration 64, loss = 0.02241772
Iteration 65, loss = 0.02221031
Iteration 66, loss = 0.02205487
Iteration 67, loss = 0.02174151
Iteration 68, loss = 0.02146014
Iteration 69, loss = 0.02103705
Iteration 70, loss = 0.02093383
Iteration 71, loss = 0.02081634
Iteration 72, loss = 0.02083700
Iteration 73, loss = 0.02069237
Iteration 74, loss = 0.02048878
Iteration 75, loss = 0.02032824
Iteration 76, loss = 0.02009821
Iteration 77, loss = 0.01994868
Iteration 78, loss = 0.01982898
Iteration 79, loss = 0.01975971
Iteration 80, loss = 0.01956801
Iteration 81, loss = 0.01950985
Iteration 82, loss = 0.01941113
Iteration 83, loss = 0.01929349
Iteration 84, loss = 0.01908205
Iteration 85, loss = 0.01889553
Iteration 86, loss = 0.01881990
Iteration 87, loss = 0.01892228
Iteration 88, loss = 0.01880813
Iteration 89, loss = 0.01878662
Iteration 90, loss = 0.01880725
Iteration 91, loss = 0.01875796
Iteration 92, loss = 0.01860287
Iteration 93, loss = 0.01843638
Iteration 94, loss = 0.01821527
Iteration 95, loss = 0.01820940
Iteration 96, loss = 0.01813180
Iteration 97, loss = 0.01799016
Iteration 98, loss = 0.01799919
Iteration 99, loss = 0.01805143
Iteration 100, loss = 0.01790941
Iteration 101, loss = 0.01772138
Iteration 102, loss = 0.01772303
Iteration 103, loss = 0.01754051
Iteration 104, loss = 0.01752937
Iteration 105, loss = 0.01750282
Iteration 106, loss = 0.01750894
Iteration 107, loss = 0.01747359
Iteration 108, loss = 0.01739099
Iteration 109, loss = 0.01737608
Iteration 110, loss = 0.01724511
Iteration 111, loss = 0.01723796
Iteration 112, loss = 0.01712648
Iteration 113, loss = 0.01712570
Iteration 114, loss = 0.01771507
Iteration 115, loss = 0.01759778
Iteration 116, loss = 0.01740205
Iteration 117, loss = 0.01710465
Iteration 118, loss = 0.01702787
Iteration 119, loss = 0.01708224
Iteration 120, loss = 0.01712196
Iteration 121, loss = 0.01698259
Iteration 122, loss = 0.01702870
Iteration 123, loss = 0.01719763
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67343973
Iteration 2, loss = 0.59973187
Iteration 3, loss = 0.49212425
Iteration 4, loss = 0.38151752
Iteration 5, loss = 0.28713079
Iteration 6, loss = 0.21701866
Iteration 7, loss = 0.16759754
Iteration 8, loss = 0.13318935
Iteration 9, loss = 0.10928579
Iteration 10, loss = 0.09236918
Iteration 11, loss = 0.07973317
Iteration 12, loss = 0.07026502
Iteration 13, loss = 0.06301966
Iteration 14, loss = 0.05734651
Iteration 15, loss = 0.05270720
Iteration 16, loss = 0.04895671
Iteration 17, loss = 0.04611854
Iteration 18, loss = 0.04370328
Iteration 19, loss = 0.04141846
Iteration 20, loss = 0.03969941
Iteration 21, loss = 0.03821306
Iteration 22, loss = 0.03684456
Iteration 23, loss = 0.03554343
Iteration 24, loss = 0.03465251
Iteration 25, loss = 0.03388660
Iteration 26, loss = 0.03286049
Iteration 27, loss = 0.03226245
Iteration 28, loss = 0.03177786
Iteration 29, loss = 0.03093419
Iteration 30, loss = 0.03044542
Iteration 31, loss = 0.02996867
Iteration 32, loss = 0.02994061
Iteration 33, loss = 0.02925522
Iteration 34, loss = 0.02903170
Iteration 35, loss = 0.02885135
Iteration 36, loss = 0.02875623
Iteration 37, loss = 0.02813685
Iteration 38, loss = 0.02800596
Iteration 39, loss = 0.02779693
Iteration 40, loss = 0.02741799
Iteration 41, loss = 0.02758508
Iteration 42, loss = 0.02702151
Iteration 43, loss = 0.02700225
Iteration 44, loss = 0.02707189
Iteration 45, loss = 0.02678310
Iteration 46, loss = 0.02686849
Iteration 47, loss = 0.02676458
Iteration 48, loss = 0.02658521
Iteration 49, loss = 0.02639066
Iteration 50, loss = 0.02608814
Iteration 51, loss = 0.02643089
Iteration 52, loss = 0.02607023
Iteration 53, loss = 0.02604975
Iteration 54, loss = 0.02599515
Iteration 55, loss = 0.02584750
Iteration 56, loss = 0.02578738
Iteration 57, loss = 0.02582791
Iteration 58, loss = 0.02553616
Iteration 59, loss = 0.02554624
Iteration 60, loss = 0.02531714
Iteration 61, loss = 0.02554142
Iteration 62, loss = 0.02540234
Iteration 63, loss = 0.02553256
Iteration 64, loss = 0.02546203
Iteration 65, loss = 0.02535716
Iteration 66, loss = 0.02520348
Iteration 67, loss = 0.02519441
Iteration 68, loss = 0.02525845
Iteration 69, loss = 0.02494274
Iteration 70, loss = 0.02514379
Iteration 71, loss = 0.02503030
Iteration 72, loss = 0.02501947
Iteration 73, loss = 0.02527218
Iteration 74, loss = 0.02485234
Iteration 75, loss = 0.02520136
Iteration 76, loss = 0.02488460
Iteration 77, loss = 0.02503400
Iteration 78, loss = 0.02491613
Iteration 79, loss = 0.02505440
Iteration 80, loss = 0.02509087
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68463682
Iteration 2, loss = 0.66832448
Iteration 3, loss = 0.65199890
Iteration 4, loss = 0.63229658
Iteration 5, loss = 0.60834400
Iteration 6, loss = 0.58022754
Iteration 7, loss = 0.54880343
Iteration 8, loss = 0.51463599
Iteration 9, loss = 0.47933904
Iteration 10, loss = 0.44419081
Iteration 11, loss = 0.40980891
Iteration 12, loss = 0.37678258
Iteration 13, loss = 0.34557864
Iteration 14, loss = 0.31623288
Iteration 15, loss = 0.28878907
Iteration 16, loss = 0.26382748
Iteration 17, loss = 0.24083320
Iteration 18, loss = 0.21982711
Iteration 19, loss = 0.20085128
Iteration 20, loss = 0.18372983
Iteration 21, loss = 0.16838482
Iteration 22, loss = 0.15458707
Iteration 23, loss = 0.14234831
Iteration 24, loss = 0.13108005
Iteration 25, loss = 0.12123630
Iteration 26, loss = 0.11229039
Iteration 27, loss = 0.10432170
Iteration 28, loss = 0.09721988
Iteration 29, loss = 0.09077070
Iteration 30, loss = 0.08499791
Iteration 31, loss = 0.07982130
Iteration 32, loss = 0.07503485
Iteration 33, loss = 0.07078647
Iteration 34, loss = 0.06695152
Iteration 35, loss = 0.06337119
Iteration 36, loss = 0.06018199
Iteration 37, loss = 0.05721766
Iteration 38, loss = 0.05452285
Iteration 39, loss = 0.05203908
Iteration 40, loss = 0.04980330
Iteration 41, loss = 0.04765434
Iteration 42, loss = 0.04574995
Iteration 43, loss = 0.04387938
Iteration 44, loss = 0.04222194
Iteration 45, loss = 0.04071348
Iteration 46, loss = 0.03920158
Iteration 47, loss = 0.03786550
Iteration 48, loss = 0.03662995
Iteration 49, loss = 0.03546639
Iteration 50, loss = 0.03437895
Iteration 51, loss = 0.03330691
Iteration 52, loss = 0.03228558
Iteration 53, loss = 0.03146428
Iteration 54, loss = 0.03056903
Iteration 55, loss = 0.02976040
Iteration 56, loss = 0.02897841
Iteration 57, loss = 0.02825687
Iteration 58, loss = 0.02760124
Iteration 59, loss = 0.02692504
Iteration 60, loss = 0.02634929
Iteration 61, loss = 0.02574068
Iteration 62, loss = 0.02518771
Iteration 63, loss = 0.02465058
Iteration 64, loss = 0.02421313
Iteration 65, loss = 0.02371039
Iteration 66, loss = 0.02328179
Iteration 67, loss = 0.02282133
Iteration 68, loss = 0.02242058
Iteration 69, loss = 0.02204671
Iteration 70, loss = 0.02161405
Iteration 71, loss = 0.02127591
Iteration 72, loss = 0.02093407
Iteration 73, loss = 0.02065101
Iteration 74, loss = 0.02032913
Iteration 75, loss = 0.02000596
Iteration 76, loss = 0.01974787
Iteration 77, loss = 0.01951775
Iteration 78, loss = 0.01919868
Iteration 79, loss = 0.01893695
Iteration 80, loss = 0.01867804
Iteration 81, loss = 0.01843119
Iteration 82, loss = 0.01821021
Iteration 83, loss = 0.01798002
Iteration 84, loss = 0.01779919
Iteration 85, loss = 0.01761065
Iteration 86, loss = 0.01742145
Iteration 87, loss = 0.01723136
Iteration 88, loss = 0.01704127
Iteration 89, loss = 0.01686513
Iteration 90, loss = 0.01668050
Iteration 91, loss = 0.01663358
Iteration 92, loss = 0.01636603
Iteration 93, loss = 0.01620398
Iteration 94, loss = 0.01607790
Iteration 95, loss = 0.01594624
Iteration 96, loss = 0.01583548
Iteration 97, loss = 0.01563345
Iteration 98, loss = 0.01558498
Iteration 99, loss = 0.01541889
Iteration 100, loss = 0.01529956
Iteration 101, loss = 0.01519144
Iteration 102, loss = 0.01507040
Iteration 103, loss = 0.01502047
Iteration 104, loss = 0.01485024
Iteration 105, loss = 0.01477860
Iteration 106, loss = 0.01461418
Iteration 107, loss = 0.01455818
Iteration 108, loss = 0.01444240
Iteration 109, loss = 0.01435130
Iteration 110, loss = 0.01429267
Iteration 111, loss = 0.01420216
Iteration 112, loss = 0.01407127
Iteration 113, loss = 0.01408731
Iteration 114, loss = 0.01399237
Iteration 115, loss = 0.01386991
Iteration 116, loss = 0.01382630
Iteration 117, loss = 0.01367991
Iteration 118, loss = 0.01364478
Iteration 119, loss = 0.01355332
Iteration 120, loss = 0.01349695
Iteration 121, loss = 0.01339781
Iteration 122, loss = 0.01341006
Iteration 123, loss = 0.01336105
Iteration 124, loss = 0.01323639
Iteration 125, loss = 0.01315944
Iteration 126, loss = 0.01309071
Iteration 127, loss = 0.01308537
Iteration 128, loss = 0.01296174
Iteration 129, loss = 0.01294255
Iteration 130, loss = 0.01290765
Iteration 131, loss = 0.01283620
Iteration 132, loss = 0.01286569
Iteration 133, loss = 0.01270889
Iteration 134, loss = 0.01271830
Iteration 135, loss = 0.01261187
Iteration 136, loss = 0.01265917
Iteration 137, loss = 0.01259855
Iteration 138, loss = 0.01253770
Iteration 139, loss = 0.01245624
Iteration 140, loss = 0.01240671
Iteration 141, loss = 0.01243427
Iteration 142, loss = 0.01242369
Iteration 143, loss = 0.01231842
Iteration 144, loss = 0.01222354
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67966355
Iteration 2, loss = 0.65092989
Iteration 3, loss = 0.61389923
Iteration 4, loss = 0.56243265
Iteration 5, loss = 0.50083379
Iteration 6, loss = 0.43741508
Iteration 7, loss = 0.37767232
Iteration 8, loss = 0.32377040
Iteration 9, loss = 0.27646730
Iteration 10, loss = 0.23604029
Iteration 11, loss = 0.20210476
Iteration 12, loss = 0.17396301
Iteration 13, loss = 0.15104035
Iteration 14, loss = 0.13218250
Iteration 15, loss = 0.11676604
Iteration 16, loss = 0.10381390
Iteration 17, loss = 0.09310115
Iteration 18, loss = 0.08417227
Iteration 19, loss = 0.07667900
Iteration 20, loss = 0.07041391
Iteration 21, loss = 0.06495566
Iteration 22, loss = 0.06025212
Iteration 23, loss = 0.05619988
Iteration 24, loss = 0.05271944
Iteration 25, loss = 0.04971306
Iteration 26, loss = 0.04683559
Iteration 27, loss = 0.04447539
Iteration 28, loss = 0.04230193
Iteration 29, loss = 0.04041657
Iteration 30, loss = 0.03874904
Iteration 31, loss = 0.03723885
Iteration 32, loss = 0.03582304
Iteration 33, loss = 0.03467864
Iteration 34, loss = 0.03342290
Iteration 35, loss = 0.03254640
Iteration 36, loss = 0.03136267
Iteration 37, loss = 0.03062624
Iteration 38, loss = 0.02990328
Iteration 39, loss = 0.02902782
Iteration 40, loss = 0.02843488
Iteration 41, loss = 0.02789023
Iteration 42, loss = 0.02725781
Iteration 43, loss = 0.02680323
Iteration 44, loss = 0.02622192
Iteration 45, loss = 0.02569197
Iteration 46, loss = 0.02529467
Iteration 47, loss = 0.02492166
Iteration 48, loss = 0.02451838
Iteration 49, loss = 0.02432084
Iteration 50, loss = 0.02386788
Iteration 51, loss = 0.02357207
Iteration 52, loss = 0.02321649
Iteration 53, loss = 0.02302324
Iteration 54, loss = 0.02286648
Iteration 55, loss = 0.02245624
Iteration 56, loss = 0.02243874
Iteration 57, loss = 0.02210934
Iteration 58, loss = 0.02210562
Iteration 59, loss = 0.02173511
Iteration 60, loss = 0.02149651
Iteration 61, loss = 0.02138796
Iteration 62, loss = 0.02116177
Iteration 63, loss = 0.02109443
Iteration 64, loss = 0.02093380
Iteration 65, loss = 0.02065969
Iteration 66, loss = 0.02065318
Iteration 67, loss = 0.02036738
Iteration 68, loss = 0.02033060
Iteration 69, loss = 0.02012539
Iteration 70, loss = 0.02015848
Iteration 71, loss = 0.01999361
Iteration 72, loss = 0.01983002
Iteration 73, loss = 0.02012961
Iteration 74, loss = 0.01978022
Iteration 75, loss = 0.01984846
Iteration 76, loss = 0.01956508
Iteration 77, loss = 0.01944132
Iteration 78, loss = 0.01948503
Iteration 79, loss = 0.01942248
Iteration 80, loss = 0.01932432
Iteration 81, loss = 0.01908375
Iteration 82, loss = 0.01911003
Iteration 83, loss = 0.01910568
Iteration 84, loss = 0.01921850
Iteration 85, loss = 0.01906803
Iteration 86, loss = 0.01891200
Iteration 87, loss = 0.01894979
Iteration 88, loss = 0.01888215
Iteration 89, loss = 0.01867609
Iteration 90, loss = 0.01863079
Iteration 91, loss = 0.01853032
Iteration 92, loss = 0.01857680
Iteration 93, loss = 0.01868168
Iteration 94, loss = 0.01862936
Iteration 95, loss = 0.01843990
Iteration 96, loss = 0.01842087
Iteration 97, loss = 0.01831154
Iteration 98, loss = 0.01825528
Iteration 99, loss = 0.01815347
Iteration 100, loss = 0.01819836
Iteration 101, loss = 0.01828590
Iteration 102, loss = 0.01833540
Iteration 103, loss = 0.01808760
Iteration 104, loss = 0.01818294
Iteration 105, loss = 0.01817953
Iteration 106, loss = 0.01796681
Iteration 107, loss = 0.01794743
Iteration 108, loss = 0.01805264
Iteration 109, loss = 0.01779996
Iteration 110, loss = 0.01794531
Iteration 111, loss = 0.01796603
Iteration 112, loss = 0.01776249
Iteration 113, loss = 0.01784723
Iteration 114, loss = 0.01774034
Iteration 115, loss = 0.01777440
Iteration 116, loss = 0.01779405
Iteration 117, loss = 0.01784825
Iteration 118, loss = 0.01772890
Iteration 119, loss = 0.01773956
Iteration 120, loss = 0.01786140
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69051225
Iteration 2, loss = 0.64130686
Iteration 3, loss = 0.56595882
Iteration 4, loss = 0.47081738
Iteration 5, loss = 0.37457123
Iteration 6, loss = 0.29264763
Iteration 7, loss = 0.22892504
Iteration 8, loss = 0.18226540
Iteration 9, loss = 0.14827621
Iteration 10, loss = 0.12371328
Iteration 11, loss = 0.10514556
Iteration 12, loss = 0.09146205
Iteration 13, loss = 0.08084967
Iteration 14, loss = 0.07238970
Iteration 15, loss = 0.06600599
Iteration 16, loss = 0.06071834
Iteration 17, loss = 0.05637723
Iteration 18, loss = 0.05255226
Iteration 19, loss = 0.04938249
Iteration 20, loss = 0.04662531
Iteration 21, loss = 0.04502040
Iteration 22, loss = 0.04334396
Iteration 23, loss = 0.04154676
Iteration 24, loss = 0.04029909
Iteration 25, loss = 0.03884113
Iteration 26, loss = 0.03796672
Iteration 27, loss = 0.03658775
Iteration 28, loss = 0.03585358
Iteration 29, loss = 0.03571638
Iteration 30, loss = 0.03487330
Iteration 31, loss = 0.03402607
Iteration 32, loss = 0.03424373
Iteration 33, loss = 0.03423365
Iteration 34, loss = 0.03293639
Iteration 35, loss = 0.03220166
Iteration 36, loss = 0.03157752
Iteration 37, loss = 0.03121984
Iteration 38, loss = 0.03154773
Iteration 39, loss = 0.03136706
Iteration 40, loss = 0.03113241
Iteration 41, loss = 0.03060526
Iteration 42, loss = 0.03035410
Iteration 43, loss = 0.02964560
Iteration 44, loss = 0.02969062
Iteration 45, loss = 0.02933581
Iteration 46, loss = 0.02929217
Iteration 47, loss = 0.02970586
Iteration 48, loss = 0.02898029
Iteration 49, loss = 0.02980772
Iteration 50, loss = 0.02930085
Iteration 51, loss = 0.02879407
Iteration 52, loss = 0.02925963
Iteration 53, loss = 0.03048469
Iteration 54, loss = 0.02979168
Iteration 55, loss = 0.02916011
Iteration 56, loss = 0.02872712
Iteration 57, loss = 0.02854057
Iteration 58, loss = 0.02827816
Iteration 59, loss = 0.02791867
Iteration 60, loss = 0.02785809
Iteration 61, loss = 0.02836619
Iteration 62, loss = 0.02806167
Iteration 63, loss = 0.02760136
Iteration 64, loss = 0.02722195
Iteration 65, loss = 0.02687803
Iteration 66, loss = 0.02669664
Iteration 67, loss = 0.02655825
Iteration 68, loss = 0.02714578
Iteration 69, loss = 0.02711391
Iteration 70, loss = 0.02677738
Iteration 71, loss = 0.02717272
Iteration 72, loss = 0.02761715
Iteration 73, loss = 0.02744031
Iteration 74, loss = 0.02692635
Iteration 75, loss = 0.02659584
Iteration 76, loss = 0.02627101
Iteration 77, loss = 0.02638574
Iteration 78, loss = 0.02611615
Iteration 79, loss = 0.02653259
Iteration 80, loss = 0.02638678
Iteration 81, loss = 0.02655151
Iteration 82, loss = 0.02691594
Iteration 83, loss = 0.02662138
Iteration 84, loss = 0.02634164
Iteration 85, loss = 0.02604455
Iteration 86, loss = 0.02588719
Iteration 87, loss = 0.02572284
Iteration 88, loss = 0.02561471
Iteration 89, loss = 0.02548473
Iteration 90, loss = 0.02542697
Iteration 91, loss = 0.02555242
Iteration 92, loss = 0.02545035
Iteration 93, loss = 0.02544899
Iteration 94, loss = 0.02544902
Iteration 95, loss = 0.02534660
Iteration 96, loss = 0.02507103
Iteration 97, loss = 0.02519433
Iteration 98, loss = 0.02534040
Iteration 99, loss = 0.02549189
Iteration 100, loss = 0.02548468
Iteration 101, loss = 0.02539990
Iteration 102, loss = 0.02512621
Iteration 103, loss = 0.02505831
Iteration 104, loss = 0.02584978
Iteration 105, loss = 0.02567118
Iteration 106, loss = 0.02558484
Iteration 107, loss = 0.02548799
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70201069
Iteration 2, loss = 0.66848960
Iteration 3, loss = 0.61644521
Iteration 4, loss = 0.54563146
Iteration 5, loss = 0.46699260
Iteration 6, loss = 0.38950254
Iteration 7, loss = 0.31983064
Iteration 8, loss = 0.26037944
Iteration 9, loss = 0.21300617
Iteration 10, loss = 0.17576898
Iteration 11, loss = 0.14693067
Iteration 12, loss = 0.12468416
Iteration 13, loss = 0.10724154
Iteration 14, loss = 0.09400051
Iteration 15, loss = 0.08297853
Iteration 16, loss = 0.07435047
Iteration 17, loss = 0.06724905
Iteration 18, loss = 0.06134087
Iteration 19, loss = 0.05644851
Iteration 20, loss = 0.05230404
Iteration 21, loss = 0.04894513
Iteration 22, loss = 0.04577566
Iteration 23, loss = 0.04315495
Iteration 24, loss = 0.04090260
Iteration 25, loss = 0.03919586
Iteration 26, loss = 0.03732771
Iteration 27, loss = 0.03563212
Iteration 28, loss = 0.03441163
Iteration 29, loss = 0.03320413
Iteration 30, loss = 0.03209712
Iteration 31, loss = 0.03097849
Iteration 32, loss = 0.03012514
Iteration 33, loss = 0.02933301
Iteration 34, loss = 0.02872543
Iteration 35, loss = 0.02797588
Iteration 36, loss = 0.02752683
Iteration 37, loss = 0.02676619
Iteration 38, loss = 0.02647685
Iteration 39, loss = 0.02599868
Iteration 40, loss = 0.02545730
Iteration 41, loss = 0.02511567
Iteration 42, loss = 0.02484230
Iteration 43, loss = 0.02433853
Iteration 44, loss = 0.02410934
Iteration 45, loss = 0.02373212
Iteration 46, loss = 0.02361018
Iteration 47, loss = 0.02315387
Iteration 48, loss = 0.02313248
Iteration 49, loss = 0.02272201
Iteration 50, loss = 0.02253321
Iteration 51, loss = 0.02245212
Iteration 52, loss = 0.02216969
Iteration 53, loss = 0.02199794
Iteration 54, loss = 0.02204727
Iteration 55, loss = 0.02171192
Iteration 56, loss = 0.02152541
Iteration 57, loss = 0.02163035
Iteration 58, loss = 0.02142560
Iteration 59, loss = 0.02118870
Iteration 60, loss = 0.02089815
Iteration 61, loss = 0.02086087
Iteration 62, loss = 0.02102559
Iteration 63, loss = 0.02069684
Iteration 64, loss = 0.02056427
Iteration 65, loss = 0.02044908
Iteration 66, loss = 0.02040374
Iteration 67, loss = 0.02036366
Iteration 68, loss = 0.02032731
Iteration 69, loss = 0.02030015
Iteration 70, loss = 0.02024059
Iteration 71, loss = 0.02033528
Iteration 72, loss = 0.01998200
Iteration 73, loss = 0.01991432
Iteration 74, loss = 0.01976373
Iteration 75, loss = 0.01984966
Iteration 76, loss = 0.01974121
Iteration 77, loss = 0.01970556
Iteration 78, loss = 0.01958100
Iteration 79, loss = 0.01954477
Iteration 80, loss = 0.01969853
Iteration 81, loss = 0.01947484
Iteration 82, loss = 0.01951515
Iteration 83, loss = 0.01932359
Iteration 84, loss = 0.01938687
Iteration 85, loss = 0.01928791
Iteration 86, loss = 0.01939286
Iteration 87, loss = 0.01930055
Iteration 88, loss = 0.01914823
Iteration 89, loss = 0.01925434
Iteration 90, loss = 0.01918097
Iteration 91, loss = 0.01924097
Iteration 92, loss = 0.01908779
Iteration 93, loss = 0.01898400
Iteration 94, loss = 0.01903414
Iteration 95, loss = 0.01907635
Iteration 96, loss = 0.01914587
Iteration 97, loss = 0.01889879
Iteration 98, loss = 0.01899929
Iteration 99, loss = 0.01884818
Iteration 100, loss = 0.01889199
Iteration 101, loss = 0.01893349
Iteration 102, loss = 0.01886836
Iteration 103, loss = 0.01882174
Iteration 104, loss = 0.01893900
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68829271
Iteration 2, loss = 0.63023325
Iteration 3, loss = 0.54207291
Iteration 4, loss = 0.43556830
Iteration 5, loss = 0.33662556
Iteration 6, loss = 0.25586930
Iteration 7, loss = 0.19717502
Iteration 8, loss = 0.15562572
Iteration 9, loss = 0.12630280
Iteration 10, loss = 0.10576496
Iteration 11, loss = 0.09053030
Iteration 12, loss = 0.07939300
Iteration 13, loss = 0.07067367
Iteration 14, loss = 0.06382864
Iteration 15, loss = 0.05852199
Iteration 16, loss = 0.05422425
Iteration 17, loss = 0.05064477
Iteration 18, loss = 0.04778460
Iteration 19, loss = 0.04552803
Iteration 20, loss = 0.04356525
Iteration 21, loss = 0.04140589
Iteration 22, loss = 0.04002720
Iteration 23, loss = 0.03869208
Iteration 24, loss = 0.03744146
Iteration 25, loss = 0.03650922
Iteration 26, loss = 0.03570073
Iteration 27, loss = 0.03464667
Iteration 28, loss = 0.03402685
Iteration 29, loss = 0.03333227
Iteration 30, loss = 0.03313814
Iteration 31, loss = 0.03244568
Iteration 32, loss = 0.03211750
Iteration 33, loss = 0.03154392
Iteration 34, loss = 0.03108473
Iteration 35, loss = 0.03108687
Iteration 36, loss = 0.03055760
Iteration 37, loss = 0.03010508
Iteration 38, loss = 0.03017594
Iteration 39, loss = 0.02963983
Iteration 40, loss = 0.02955538
Iteration 41, loss = 0.02937334
Iteration 42, loss = 0.02935745
Iteration 43, loss = 0.02890279
Iteration 44, loss = 0.02899840
Iteration 45, loss = 0.02908239
Iteration 46, loss = 0.02855431
Iteration 47, loss = 0.02835136
Iteration 48, loss = 0.02846915
Iteration 49, loss = 0.02859235
Iteration 50, loss = 0.02801994
Iteration 51, loss = 0.02779434
Iteration 52, loss = 0.02777355
Iteration 53, loss = 0.02763250
Iteration 54, loss = 0.02761906
Iteration 55, loss = 0.02736137
Iteration 56, loss = 0.02711777
Iteration 57, loss = 0.02714276
Iteration 58, loss = 0.02729977
Iteration 59, loss = 0.02713403
Iteration 60, loss = 0.02702912
Iteration 61, loss = 0.02687220
Iteration 62, loss = 0.02711510
Iteration 63, loss = 0.02695301
Iteration 64, loss = 0.02717358
Iteration 65, loss = 0.02712499
Iteration 66, loss = 0.02686710
Iteration 67, loss = 0.02669985
Iteration 68, loss = 0.02660788
Iteration 69, loss = 0.02685970
Iteration 70, loss = 0.02659766
Iteration 71, loss = 0.02665348
Iteration 72, loss = 0.02632583
Iteration 73, loss = 0.02674014
Iteration 74, loss = 0.02656898
Iteration 75, loss = 0.02661785
Iteration 76, loss = 0.02630824
Iteration 77, loss = 0.02647040
Iteration 78, loss = 0.02618350
Iteration 79, loss = 0.02623037
Iteration 80, loss = 0.02631095
Iteration 81, loss = 0.02646292
Iteration 82, loss = 0.02618861
Iteration 83, loss = 0.02649197
Iteration 84, loss = 0.02639799
Iteration 85, loss = 0.02618720
Iteration 86, loss = 0.02617039
Iteration 87, loss = 0.02594192
Iteration 88, loss = 0.02616371
Iteration 89, loss = 0.02615213
Iteration 90, loss = 0.02583027
Iteration 91, loss = 0.02614510
Iteration 92, loss = 0.02627120
Iteration 93, loss = 0.02601713
Iteration 94, loss = 0.02614508
Iteration 95, loss = 0.02643032
Iteration 96, loss = 0.02635697
Iteration 97, loss = 0.02607170
Iteration 98, loss = 0.02564516
Iteration 99, loss = 0.02593832
Iteration 100, loss = 0.02596668
Iteration 101, loss = 0.02620480
Iteration 102, loss = 0.02570367
Iteration 103, loss = 0.02594813
Iteration 104, loss = 0.02597998
Iteration 105, loss = 0.02616784
Iteration 106, loss = 0.02603807
Iteration 107, loss = 0.02594446
Iteration 108, loss = 0.02563754
Iteration 109, loss = 0.02587488
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71933479
Iteration 2, loss = 0.67771791
Iteration 3, loss = 0.61760948
Iteration 4, loss = 0.54155133
Iteration 5, loss = 0.45730243
Iteration 6, loss = 0.37538171
Iteration 7, loss = 0.30446300
Iteration 8, loss = 0.24632702
Iteration 9, loss = 0.20052063
Iteration 10, loss = 0.16502630
Iteration 11, loss = 0.13825857
Iteration 12, loss = 0.11707212
Iteration 13, loss = 0.10107546
Iteration 14, loss = 0.08834087
Iteration 15, loss = 0.07837632
Iteration 16, loss = 0.07031394
Iteration 17, loss = 0.06353772
Iteration 18, loss = 0.05831495
Iteration 19, loss = 0.05373009
Iteration 20, loss = 0.04973705
Iteration 21, loss = 0.04633917
Iteration 22, loss = 0.04361069
Iteration 23, loss = 0.04137549
Iteration 24, loss = 0.03927910
Iteration 25, loss = 0.03743922
Iteration 26, loss = 0.03564576
Iteration 27, loss = 0.03419405
Iteration 28, loss = 0.03300320
Iteration 29, loss = 0.03180230
Iteration 30, loss = 0.03072979
Iteration 31, loss = 0.02987405
Iteration 32, loss = 0.02885716
Iteration 33, loss = 0.02824357
Iteration 34, loss = 0.02764681
Iteration 35, loss = 0.02694592
Iteration 36, loss = 0.02642057
Iteration 37, loss = 0.02597636
Iteration 38, loss = 0.02533157
Iteration 39, loss = 0.02478567
Iteration 40, loss = 0.02440659
Iteration 41, loss = 0.02408997
Iteration 42, loss = 0.02368788
Iteration 43, loss = 0.02340974
Iteration 44, loss = 0.02345033
Iteration 45, loss = 0.02288068
Iteration 46, loss = 0.02260331
Iteration 47, loss = 0.02241461
Iteration 48, loss = 0.02204487
Iteration 49, loss = 0.02183699
Iteration 50, loss = 0.02163910
Iteration 51, loss = 0.02147846
Iteration 52, loss = 0.02118327
Iteration 53, loss = 0.02111676
Iteration 54, loss = 0.02099058
Iteration 55, loss = 0.02088994
Iteration 56, loss = 0.02075395
Iteration 57, loss = 0.02056269
Iteration 58, loss = 0.02039751
Iteration 59, loss = 0.02019545
Iteration 60, loss = 0.02013157
Iteration 61, loss = 0.02012721
Iteration 62, loss = 0.02003949
Iteration 63, loss = 0.02014660
Iteration 64, loss = 0.01986324
Iteration 65, loss = 0.01977645
Iteration 66, loss = 0.01955963
Iteration 67, loss = 0.01944191
Iteration 68, loss = 0.01961641
Iteration 69, loss = 0.01934128
Iteration 70, loss = 0.01926866
Iteration 71, loss = 0.01932531
Iteration 72, loss = 0.01933312
Iteration 73, loss = 0.01917788
Iteration 74, loss = 0.01909331
Iteration 75, loss = 0.01913705
Iteration 76, loss = 0.01885404
Iteration 77, loss = 0.01900672
Iteration 78, loss = 0.01879751
Iteration 79, loss = 0.01904748
Iteration 80, loss = 0.01877909
Iteration 81, loss = 0.01869892
Iteration 82, loss = 0.01923131
Iteration 83, loss = 0.01891067
Iteration 84, loss = 0.01904883
Iteration 85, loss = 0.01888563
Iteration 86, loss = 0.01866158
Iteration 87, loss = 0.01857785
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68786935
Iteration 2, loss = 0.61558330
Iteration 3, loss = 0.50666432
Iteration 4, loss = 0.39199473
Iteration 5, loss = 0.29562214
Iteration 6, loss = 0.22334933
Iteration 7, loss = 0.17242412
Iteration 8, loss = 0.13715242
Iteration 9, loss = 0.11258257
Iteration 10, loss = 0.09496014
Iteration 11, loss = 0.08197557
Iteration 12, loss = 0.07215694
Iteration 13, loss = 0.06511927
Iteration 14, loss = 0.05910110
Iteration 15, loss = 0.05432333
Iteration 16, loss = 0.05066495
Iteration 17, loss = 0.04747823
Iteration 18, loss = 0.04491294
Iteration 19, loss = 0.04282525
Iteration 20, loss = 0.04087388
Iteration 21, loss = 0.03957244
Iteration 22, loss = 0.03811938
Iteration 23, loss = 0.03685149
Iteration 24, loss = 0.03592131
Iteration 25, loss = 0.03483768
Iteration 26, loss = 0.03399342
Iteration 27, loss = 0.03357635
Iteration 28, loss = 0.03299290
Iteration 29, loss = 0.03228249
Iteration 30, loss = 0.03195330
Iteration 31, loss = 0.03153015
Iteration 32, loss = 0.03114512
Iteration 33, loss = 0.03086297
Iteration 34, loss = 0.03018483
Iteration 35, loss = 0.03006131
Iteration 36, loss = 0.02966960
Iteration 37, loss = 0.02938973
Iteration 38, loss = 0.02923099
Iteration 39, loss = 0.02900848
Iteration 40, loss = 0.02883786
Iteration 41, loss = 0.02855201
Iteration 42, loss = 0.02817498
Iteration 43, loss = 0.02807741
Iteration 44, loss = 0.02800977
Iteration 45, loss = 0.02826933
Iteration 46, loss = 0.02810776
Iteration 47, loss = 0.02774637
Iteration 48, loss = 0.02763719
Iteration 49, loss = 0.02748258
Iteration 50, loss = 0.02741490
Iteration 51, loss = 0.02750604
Iteration 52, loss = 0.02747034
Iteration 53, loss = 0.02711693
Iteration 54, loss = 0.02709229
Iteration 55, loss = 0.02702675
Iteration 56, loss = 0.02710407
Iteration 57, loss = 0.02680487
Iteration 58, loss = 0.02654954
Iteration 59, loss = 0.02690109
Iteration 60, loss = 0.02657880
Iteration 61, loss = 0.02671032
Iteration 62, loss = 0.02628790
Iteration 63, loss = 0.02664177
Iteration 64, loss = 0.02631769
Iteration 65, loss = 0.02661562
Iteration 66, loss = 0.02621736
Iteration 67, loss = 0.02622929
Iteration 68, loss = 0.02621277
Iteration 69, loss = 0.02655751
Iteration 70, loss = 0.02667413
Iteration 71, loss = 0.02628274
Iteration 72, loss = 0.02616271
Iteration 73, loss = 0.02632722
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	91m2.565s
user	522m33.359s
sys	67m47.464s
Elapsed time: 5463 seconds
