Processing dataset: prepended_v7_lowercase_words_only

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: prepended_v7_lowercase_words_only (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on prepended_v7_lowercase_words_only...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on prepended_v7_lowercase_words_only...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on prepended_v7_lowercase_words_only...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on prepended_v7_lowercase_words_only...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on prepended_v7_lowercase_words_only...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.70124995
Iteration 2, loss = 0.63246906
Iteration 3, loss = 0.52429991
Iteration 4, loss = 0.40384510
Iteration 5, loss = 0.29884172
Iteration 6, loss = 0.22100070
Iteration 7, loss = 0.16692008
Iteration 8, loss = 0.13044416
Iteration 9, loss = 0.10556590
Iteration 10, loss = 0.08825863
Iteration 11, loss = 0.07575094
Iteration 12, loss = 0.06633823
Iteration 13, loss = 0.05929684
Iteration 14, loss = 0.05361734
Iteration 15, loss = 0.04930117
Iteration 16, loss = 0.04570118
Iteration 17, loss = 0.04293929
Iteration 18, loss = 0.04030081
Iteration 19, loss = 0.03855468
Iteration 20, loss = 0.03688146
Iteration 21, loss = 0.03531430
Iteration 22, loss = 0.03414973
Iteration 23, loss = 0.03283925
Iteration 24, loss = 0.03223582
Iteration 25, loss = 0.03136790
Iteration 26, loss = 0.03051685
Iteration 27, loss = 0.02995128
Iteration 28, loss = 0.02936031
Iteration 29, loss = 0.02885093
Iteration 30, loss = 0.02831862
Iteration 31, loss = 0.02792950
Iteration 32, loss = 0.02752579
Iteration 33, loss = 0.02709489
Iteration 34, loss = 0.02689552
Iteration 35, loss = 0.02682008
Iteration 36, loss = 0.02638622
Iteration 37, loss = 0.02589328
Iteration 38, loss = 0.02589694
Iteration 39, loss = 0.02583947
Iteration 40, loss = 0.02570369
Iteration 41, loss = 0.02551263
Iteration 42, loss = 0.02521156
Iteration 43, loss = 0.02514160
Iteration 44, loss = 0.02474680
Iteration 45, loss = 0.02500844
Iteration 46, loss = 0.02509690
Iteration 47, loss = 0.02479879
Iteration 48, loss = 0.02432195
Iteration 49, loss = 0.02442186
Iteration 50, loss = 0.02416722
Iteration 51, loss = 0.02414524
Iteration 52, loss = 0.02394194
Iteration 53, loss = 0.02422956
Iteration 54, loss = 0.02408947
Iteration 55, loss = 0.02396611
Iteration 56, loss = 0.02393857
Iteration 57, loss = 0.02389019
Iteration 58, loss = 0.02378828
Iteration 59, loss = 0.02382877
Iteration 60, loss = 0.02349021
Iteration 61, loss = 0.02367843
Iteration 62, loss = 0.02363281
Iteration 63, loss = 0.02342623
Iteration 64, loss = 0.02366035
Iteration 65, loss = 0.02331388
Iteration 66, loss = 0.02331264
Iteration 67, loss = 0.02337510
Iteration 68, loss = 0.02349996
Iteration 69, loss = 0.02359903
Iteration 70, loss = 0.02337285
Iteration 71, loss = 0.02345579
Iteration 72, loss = 0.02337051
Iteration 73, loss = 0.02331100
Iteration 74, loss = 0.02321086
Iteration 75, loss = 0.02309372
Iteration 76, loss = 0.02322476
Iteration 77, loss = 0.02329469
Iteration 78, loss = 0.02335113
Iteration 79, loss = 0.02341553
Iteration 80, loss = 0.02302939
Iteration 81, loss = 0.02305848
Iteration 82, loss = 0.02311065
Iteration 83, loss = 0.02334604
Iteration 84, loss = 0.02311674
Iteration 85, loss = 0.02310237
Iteration 86, loss = 0.02314848
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67476343
Iteration 2, loss = 0.59204383
Iteration 3, loss = 0.48082219
Iteration 4, loss = 0.36913139
Iteration 5, loss = 0.27616169
Iteration 6, loss = 0.20727802
Iteration 7, loss = 0.15895827
Iteration 8, loss = 0.12570669
Iteration 9, loss = 0.10251193
Iteration 10, loss = 0.08652639
Iteration 11, loss = 0.07463401
Iteration 12, loss = 0.06585138
Iteration 13, loss = 0.05883418
Iteration 14, loss = 0.05383341
Iteration 15, loss = 0.04934938
Iteration 16, loss = 0.04602568
Iteration 17, loss = 0.04311607
Iteration 18, loss = 0.04094983
Iteration 19, loss = 0.03886092
Iteration 20, loss = 0.03721366
Iteration 21, loss = 0.03609706
Iteration 22, loss = 0.03438312
Iteration 23, loss = 0.03371968
Iteration 24, loss = 0.03279909
Iteration 25, loss = 0.03199183
Iteration 26, loss = 0.03113603
Iteration 27, loss = 0.03043298
Iteration 28, loss = 0.03005473
Iteration 29, loss = 0.02951772
Iteration 30, loss = 0.02903605
Iteration 31, loss = 0.02893951
Iteration 32, loss = 0.02817556
Iteration 33, loss = 0.02795878
Iteration 34, loss = 0.02791539
Iteration 35, loss = 0.02735267
Iteration 36, loss = 0.02705244
Iteration 37, loss = 0.02690026
Iteration 38, loss = 0.02688814
Iteration 39, loss = 0.02646764
Iteration 40, loss = 0.02648666
Iteration 41, loss = 0.02579045
Iteration 42, loss = 0.02587865
Iteration 43, loss = 0.02590975
Iteration 44, loss = 0.02557641
Iteration 45, loss = 0.02559273
Iteration 46, loss = 0.02574417
Iteration 47, loss = 0.02548870
Iteration 48, loss = 0.02538160
Iteration 49, loss = 0.02526010
Iteration 50, loss = 0.02500088
Iteration 51, loss = 0.02488729
Iteration 52, loss = 0.02492859
Iteration 53, loss = 0.02488793
Iteration 54, loss = 0.02472482
Iteration 55, loss = 0.02492020
Iteration 56, loss = 0.02475988
Iteration 57, loss = 0.02469182
Iteration 58, loss = 0.02458531
Iteration 59, loss = 0.02478050
Iteration 60, loss = 0.02444398
Iteration 61, loss = 0.02408857
Iteration 62, loss = 0.02440718
Iteration 63, loss = 0.02447574
Iteration 64, loss = 0.02433023
Iteration 65, loss = 0.02436224
Iteration 66, loss = 0.02425611
Iteration 67, loss = 0.02440663
Iteration 68, loss = 0.02418007
Iteration 69, loss = 0.02373169
Iteration 70, loss = 0.02404983
Iteration 71, loss = 0.02405792
Iteration 72, loss = 0.02400330
Iteration 73, loss = 0.02430276
Iteration 74, loss = 0.02416731
Iteration 75, loss = 0.02385474
Iteration 76, loss = 0.02407374
Iteration 77, loss = 0.02366730
Iteration 78, loss = 0.02366848
Iteration 79, loss = 0.02378420
Iteration 80, loss = 0.02379881
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67294411
Iteration 2, loss = 0.59605809
Iteration 3, loss = 0.47896505
Iteration 4, loss = 0.36203930
Iteration 5, loss = 0.26663361
Iteration 6, loss = 0.19752756
Iteration 7, loss = 0.15037078
Iteration 8, loss = 0.11866269
Iteration 9, loss = 0.09674701
Iteration 10, loss = 0.08073066
Iteration 11, loss = 0.06944770
Iteration 12, loss = 0.06105140
Iteration 13, loss = 0.05440961
Iteration 14, loss = 0.04950245
Iteration 15, loss = 0.04559891
Iteration 16, loss = 0.04196948
Iteration 17, loss = 0.03930204
Iteration 18, loss = 0.03713807
Iteration 19, loss = 0.03511238
Iteration 20, loss = 0.03368363
Iteration 21, loss = 0.03230231
Iteration 22, loss = 0.03113507
Iteration 23, loss = 0.03022532
Iteration 24, loss = 0.02905322
Iteration 25, loss = 0.02857148
Iteration 26, loss = 0.02752486
Iteration 27, loss = 0.02736545
Iteration 28, loss = 0.02668986
Iteration 29, loss = 0.02596192
Iteration 30, loss = 0.02571503
Iteration 31, loss = 0.02534547
Iteration 32, loss = 0.02500539
Iteration 33, loss = 0.02454918
Iteration 34, loss = 0.02432813
Iteration 35, loss = 0.02417582
Iteration 36, loss = 0.02419979
Iteration 37, loss = 0.02369350
Iteration 38, loss = 0.02345098
Iteration 39, loss = 0.02340272
Iteration 40, loss = 0.02351585
Iteration 41, loss = 0.02279412
Iteration 42, loss = 0.02282971
Iteration 43, loss = 0.02271586
Iteration 44, loss = 0.02240786
Iteration 45, loss = 0.02234749
Iteration 46, loss = 0.02239825
Iteration 47, loss = 0.02243678
Iteration 48, loss = 0.02216253
Iteration 49, loss = 0.02202844
Iteration 50, loss = 0.02211981
Iteration 51, loss = 0.02189186
Iteration 52, loss = 0.02193111
Iteration 53, loss = 0.02192093
Iteration 54, loss = 0.02164581
Iteration 55, loss = 0.02161834
Iteration 56, loss = 0.02139742
Iteration 57, loss = 0.02186936
Iteration 58, loss = 0.02134263
Iteration 59, loss = 0.02128562
Iteration 60, loss = 0.02150239
Iteration 61, loss = 0.02146413
Iteration 62, loss = 0.02136798
Iteration 63, loss = 0.02117515
Iteration 64, loss = 0.02133798
Iteration 65, loss = 0.02124001
Iteration 66, loss = 0.02092827
Iteration 67, loss = 0.02114587
Iteration 68, loss = 0.02141820
Iteration 69, loss = 0.02148424
Iteration 70, loss = 0.02116747
Iteration 71, loss = 0.02082060
Iteration 72, loss = 0.02107395
Iteration 73, loss = 0.02115180
Iteration 74, loss = 0.02115780
Iteration 75, loss = 0.02090089
Iteration 76, loss = 0.02105373
Iteration 77, loss = 0.02090681
Iteration 78, loss = 0.02110319
Iteration 79, loss = 0.02063388
Iteration 80, loss = 0.02049821
Iteration 81, loss = 0.02085709
Iteration 82, loss = 0.02108455
Iteration 83, loss = 0.02073631
Iteration 84, loss = 0.02070124
Iteration 85, loss = 0.02065404
Iteration 86, loss = 0.02071339
Iteration 87, loss = 0.02078783
Iteration 88, loss = 0.02062979
Iteration 89, loss = 0.02057533
Iteration 90, loss = 0.02083742
Iteration 91, loss = 0.02059504
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67762825
Iteration 2, loss = 0.60637055
Iteration 3, loss = 0.49599407
Iteration 4, loss = 0.38099863
Iteration 5, loss = 0.28307508
Iteration 6, loss = 0.21003986
Iteration 7, loss = 0.15934412
Iteration 8, loss = 0.12510647
Iteration 9, loss = 0.10115864
Iteration 10, loss = 0.08442698
Iteration 11, loss = 0.07226451
Iteration 12, loss = 0.06345276
Iteration 13, loss = 0.05637887
Iteration 14, loss = 0.05092827
Iteration 15, loss = 0.04656864
Iteration 16, loss = 0.04337669
Iteration 17, loss = 0.04049068
Iteration 18, loss = 0.03803775
Iteration 19, loss = 0.03605008
Iteration 20, loss = 0.03427993
Iteration 21, loss = 0.03297505
Iteration 22, loss = 0.03174883
Iteration 23, loss = 0.03077922
Iteration 24, loss = 0.02979752
Iteration 25, loss = 0.02910688
Iteration 26, loss = 0.02858526
Iteration 27, loss = 0.02758740
Iteration 28, loss = 0.02704830
Iteration 29, loss = 0.02639600
Iteration 30, loss = 0.02617882
Iteration 31, loss = 0.02574712
Iteration 32, loss = 0.02560881
Iteration 33, loss = 0.02512701
Iteration 34, loss = 0.02477447
Iteration 35, loss = 0.02443737
Iteration 36, loss = 0.02397830
Iteration 37, loss = 0.02388671
Iteration 38, loss = 0.02377894
Iteration 39, loss = 0.02364832
Iteration 40, loss = 0.02388417
Iteration 41, loss = 0.02326469
Iteration 42, loss = 0.02319243
Iteration 43, loss = 0.02295794
Iteration 44, loss = 0.02279768
Iteration 45, loss = 0.02305154
Iteration 46, loss = 0.02274893
Iteration 47, loss = 0.02250100
Iteration 48, loss = 0.02232587
Iteration 49, loss = 0.02231993
Iteration 50, loss = 0.02211205
Iteration 51, loss = 0.02176765
Iteration 52, loss = 0.02203280
Iteration 53, loss = 0.02220624
Iteration 54, loss = 0.02191552
Iteration 55, loss = 0.02183611
Iteration 56, loss = 0.02183622
Iteration 57, loss = 0.02189378
Iteration 58, loss = 0.02157441
Iteration 59, loss = 0.02151389
Iteration 60, loss = 0.02156626
Iteration 61, loss = 0.02170974
Iteration 62, loss = 0.02141659
Iteration 63, loss = 0.02151468
Iteration 64, loss = 0.02116806
Iteration 65, loss = 0.02150005
Iteration 66, loss = 0.02127013
Iteration 67, loss = 0.02154862
Iteration 68, loss = 0.02139173
Iteration 69, loss = 0.02127356
Iteration 70, loss = 0.02094291
Iteration 71, loss = 0.02125051
Iteration 72, loss = 0.02105441
Iteration 73, loss = 0.02118415
Iteration 74, loss = 0.02101854
Iteration 75, loss = 0.02088906
Iteration 76, loss = 0.02125105
Iteration 77, loss = 0.02118529
Iteration 78, loss = 0.02112681
Iteration 79, loss = 0.02080437
Iteration 80, loss = 0.02114424
Iteration 81, loss = 0.02105349
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68652250
Iteration 2, loss = 0.61614324
Iteration 3, loss = 0.50370581
Iteration 4, loss = 0.38537687
Iteration 5, loss = 0.28691027
Iteration 6, loss = 0.21332386
Iteration 7, loss = 0.16283789
Iteration 8, loss = 0.12779820
Iteration 9, loss = 0.10375520
Iteration 10, loss = 0.08683173
Iteration 11, loss = 0.07401553
Iteration 12, loss = 0.06454751
Iteration 13, loss = 0.05747734
Iteration 14, loss = 0.05193360
Iteration 15, loss = 0.04743452
Iteration 16, loss = 0.04402826
Iteration 17, loss = 0.04070157
Iteration 18, loss = 0.03864444
Iteration 19, loss = 0.03643831
Iteration 20, loss = 0.03472029
Iteration 21, loss = 0.03324913
Iteration 22, loss = 0.03201079
Iteration 23, loss = 0.03067800
Iteration 24, loss = 0.02972183
Iteration 25, loss = 0.02887865
Iteration 26, loss = 0.02817226
Iteration 27, loss = 0.02749262
Iteration 28, loss = 0.02700424
Iteration 29, loss = 0.02629091
Iteration 30, loss = 0.02606084
Iteration 31, loss = 0.02566677
Iteration 32, loss = 0.02515032
Iteration 33, loss = 0.02514046
Iteration 34, loss = 0.02466377
Iteration 35, loss = 0.02410315
Iteration 36, loss = 0.02399740
Iteration 37, loss = 0.02382711
Iteration 38, loss = 0.02355104
Iteration 39, loss = 0.02349334
Iteration 40, loss = 0.02308312
Iteration 41, loss = 0.02331173
Iteration 42, loss = 0.02281559
Iteration 43, loss = 0.02267483
Iteration 44, loss = 0.02249447
Iteration 45, loss = 0.02232408
Iteration 46, loss = 0.02220772
Iteration 47, loss = 0.02207589
Iteration 48, loss = 0.02218908
Iteration 49, loss = 0.02210390
Iteration 50, loss = 0.02214309
Iteration 51, loss = 0.02171538
Iteration 52, loss = 0.02202788
Iteration 53, loss = 0.02195204
Iteration 54, loss = 0.02174057
Iteration 55, loss = 0.02147871
Iteration 56, loss = 0.02155652
Iteration 57, loss = 0.02139955
Iteration 58, loss = 0.02122695
Iteration 59, loss = 0.02126337
Iteration 60, loss = 0.02139525
Iteration 61, loss = 0.02130787
Iteration 62, loss = 0.02140551
Iteration 63, loss = 0.02118073
Iteration 64, loss = 0.02106061
Iteration 65, loss = 0.02109565
Iteration 66, loss = 0.02104475
Iteration 67, loss = 0.02085049
Iteration 68, loss = 0.02114415
Iteration 69, loss = 0.02075636
Iteration 70, loss = 0.02088073
Iteration 71, loss = 0.02089050
Iteration 72, loss = 0.02125376
Iteration 73, loss = 0.02088708
Iteration 74, loss = 0.02073345
Iteration 75, loss = 0.02082300
Iteration 76, loss = 0.02083094
Iteration 77, loss = 0.02073432
Iteration 78, loss = 0.02094170
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70124995
Iteration 2, loss = 0.63246906
Iteration 3, loss = 0.52429991
Iteration 4, loss = 0.40384510
Iteration 5, loss = 0.29884172
Iteration 6, loss = 0.22100070
Iteration 7, loss = 0.16692008
Iteration 8, loss = 0.13044416
Iteration 9, loss = 0.10556590
Iteration 10, loss = 0.08825863
Iteration 11, loss = 0.07575094
Iteration 12, loss = 0.06633823
Iteration 13, loss = 0.05929684
Iteration 14, loss = 0.05361734
Iteration 15, loss = 0.04930117
Iteration 16, loss = 0.04570118
Iteration 17, loss = 0.04293929
Iteration 18, loss = 0.04030081
Iteration 19, loss = 0.03855468
Iteration 20, loss = 0.03688146
Iteration 21, loss = 0.03531430
Iteration 22, loss = 0.03414973
Iteration 23, loss = 0.03283925
Iteration 24, loss = 0.03223582
Iteration 25, loss = 0.03136790
Iteration 26, loss = 0.03051685
Iteration 27, loss = 0.02995128
Iteration 28, loss = 0.02936031
Iteration 29, loss = 0.02885093
Iteration 30, loss = 0.02831862
Iteration 31, loss = 0.02792950
Iteration 32, loss = 0.02752579
Iteration 33, loss = 0.02709489
Iteration 34, loss = 0.02689552
Iteration 35, loss = 0.02682008
Iteration 36, loss = 0.02638622
Iteration 37, loss = 0.02589328
Iteration 38, loss = 0.02589694
Iteration 39, loss = 0.02583947
Iteration 40, loss = 0.02570369
Iteration 41, loss = 0.02551263
Iteration 42, loss = 0.02521156
Iteration 43, loss = 0.02514160
Iteration 44, loss = 0.02474680
Iteration 45, loss = 0.02500844
Iteration 46, loss = 0.02509690
Iteration 47, loss = 0.02479879
Iteration 48, loss = 0.02432195
Iteration 49, loss = 0.02442186
Iteration 50, loss = 0.02416722
Iteration 51, loss = 0.02414524
Iteration 52, loss = 0.02394194
Iteration 53, loss = 0.02422956
Iteration 54, loss = 0.02408947
Iteration 55, loss = 0.02396611
Iteration 56, loss = 0.02393857
Iteration 57, loss = 0.02389019
Iteration 58, loss = 0.02378828
Iteration 59, loss = 0.02382877
Iteration 60, loss = 0.02349021
Iteration 61, loss = 0.02367843
Iteration 62, loss = 0.02363281
Iteration 63, loss = 0.02342623
Iteration 64, loss = 0.02366035
Iteration 65, loss = 0.02331388
Iteration 66, loss = 0.02331264
Iteration 67, loss = 0.02337510
Iteration 68, loss = 0.02349996
Iteration 69, loss = 0.02359903
Iteration 70, loss = 0.02337285
Iteration 71, loss = 0.02345579
Iteration 72, loss = 0.02337051
Iteration 73, loss = 0.02331100
Iteration 74, loss = 0.02321086
Iteration 75, loss = 0.02309372
Iteration 76, loss = 0.02322476
Iteration 77, loss = 0.02329469
Iteration 78, loss = 0.02335113
Iteration 79, loss = 0.02341553
Iteration 80, loss = 0.02302939
Iteration 81, loss = 0.02305848
Iteration 82, loss = 0.02311065
Iteration 83, loss = 0.02334604
Iteration 84, loss = 0.02311674
Iteration 85, loss = 0.02310237
Iteration 86, loss = 0.02314848
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67476343
Iteration 2, loss = 0.59204383
Iteration 3, loss = 0.48082219
Iteration 4, loss = 0.36913139
Iteration 5, loss = 0.27616169
Iteration 6, loss = 0.20727802
Iteration 7, loss = 0.15895827
Iteration 8, loss = 0.12570669
Iteration 9, loss = 0.10251193
Iteration 10, loss = 0.08652639
Iteration 11, loss = 0.07463401
Iteration 12, loss = 0.06585138
Iteration 13, loss = 0.05883418
Iteration 14, loss = 0.05383341
Iteration 15, loss = 0.04934938
Iteration 16, loss = 0.04602568
Iteration 17, loss = 0.04311607
Iteration 18, loss = 0.04094983
Iteration 19, loss = 0.03886092
Iteration 20, loss = 0.03721366
Iteration 21, loss = 0.03609706
Iteration 22, loss = 0.03438312
Iteration 23, loss = 0.03371968
Iteration 24, loss = 0.03279909
Iteration 25, loss = 0.03199183
Iteration 26, loss = 0.03113603
Iteration 27, loss = 0.03043298
Iteration 28, loss = 0.03005473
Iteration 29, loss = 0.02951772
Iteration 30, loss = 0.02903605
Iteration 31, loss = 0.02893951
Iteration 32, loss = 0.02817556
Iteration 33, loss = 0.02795878
Iteration 34, loss = 0.02791539
Iteration 35, loss = 0.02735267
Iteration 36, loss = 0.02705244
Iteration 37, loss = 0.02690026
Iteration 38, loss = 0.02688814
Iteration 39, loss = 0.02646764
Iteration 40, loss = 0.02648666
Iteration 41, loss = 0.02579045
Iteration 42, loss = 0.02587865
Iteration 43, loss = 0.02590975
Iteration 44, loss = 0.02557641
Iteration 45, loss = 0.02559273
Iteration 46, loss = 0.02574417
Iteration 47, loss = 0.02548870
Iteration 48, loss = 0.02538160
Iteration 49, loss = 0.02526010
Iteration 50, loss = 0.02500088
Iteration 51, loss = 0.02488729
Iteration 52, loss = 0.02492859
Iteration 53, loss = 0.02488793
Iteration 54, loss = 0.02472482
Iteration 55, loss = 0.02492020
Iteration 56, loss = 0.02475988
Iteration 57, loss = 0.02469182
Iteration 58, loss = 0.02458531
Iteration 59, loss = 0.02478050
Iteration 60, loss = 0.02444398
Iteration 61, loss = 0.02408857
Iteration 62, loss = 0.02440718
Iteration 63, loss = 0.02447574
Iteration 64, loss = 0.02433023
Iteration 65, loss = 0.02436224
Iteration 66, loss = 0.02425611
Iteration 67, loss = 0.02440663
Iteration 68, loss = 0.02418007
Iteration 69, loss = 0.02373169
Iteration 70, loss = 0.02404983
Iteration 71, loss = 0.02405792
Iteration 72, loss = 0.02400330
Iteration 73, loss = 0.02430276
Iteration 74, loss = 0.02416731
Iteration 75, loss = 0.02385474
Iteration 76, loss = 0.02407374
Iteration 77, loss = 0.02366730
Iteration 78, loss = 0.02366848
Iteration 79, loss = 0.02378420
Iteration 80, loss = 0.02379881
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67294411
Iteration 2, loss = 0.59605809
Iteration 3, loss = 0.47896505
Iteration 4, loss = 0.36203930
Iteration 5, loss = 0.26663361
Iteration 6, loss = 0.19752756
Iteration 7, loss = 0.15037078
Iteration 8, loss = 0.11866269
Iteration 9, loss = 0.09674701
Iteration 10, loss = 0.08073066
Iteration 11, loss = 0.06944770
Iteration 12, loss = 0.06105140
Iteration 13, loss = 0.05440961
Iteration 14, loss = 0.04950245
Iteration 15, loss = 0.04559891
Iteration 16, loss = 0.04196948
Iteration 17, loss = 0.03930204
Iteration 18, loss = 0.03713807
Iteration 19, loss = 0.03511238
Iteration 20, loss = 0.03368363
Iteration 21, loss = 0.03230231
Iteration 22, loss = 0.03113507
Iteration 23, loss = 0.03022532
Iteration 24, loss = 0.02905322
Iteration 25, loss = 0.02857148
Iteration 26, loss = 0.02752486
Iteration 27, loss = 0.02736545
Iteration 28, loss = 0.02668986
Iteration 29, loss = 0.02596192
Iteration 30, loss = 0.02571503
Iteration 31, loss = 0.02534547
Iteration 32, loss = 0.02500539
Iteration 33, loss = 0.02454918
Iteration 34, loss = 0.02432813
Iteration 35, loss = 0.02417582
Iteration 36, loss = 0.02419979
Iteration 37, loss = 0.02369350
Iteration 38, loss = 0.02345098
Iteration 39, loss = 0.02340272
Iteration 40, loss = 0.02351585
Iteration 41, loss = 0.02279412
Iteration 42, loss = 0.02282971
Iteration 43, loss = 0.02271586
Iteration 44, loss = 0.02240786
Iteration 45, loss = 0.02234749
Iteration 46, loss = 0.02239825
Iteration 47, loss = 0.02243678
Iteration 48, loss = 0.02216253
Iteration 49, loss = 0.02202844
Iteration 50, loss = 0.02211981
Iteration 51, loss = 0.02189186
Iteration 52, loss = 0.02193111
Iteration 53, loss = 0.02192093
Iteration 54, loss = 0.02164581
Iteration 55, loss = 0.02161834
Iteration 56, loss = 0.02139742
Iteration 57, loss = 0.02186936
Iteration 58, loss = 0.02134263
Iteration 59, loss = 0.02128562
Iteration 60, loss = 0.02150239
Iteration 61, loss = 0.02146413
Iteration 62, loss = 0.02136798
Iteration 63, loss = 0.02117515
Iteration 64, loss = 0.02133798
Iteration 65, loss = 0.02124001
Iteration 66, loss = 0.02092827
Iteration 67, loss = 0.02114587
Iteration 68, loss = 0.02141820
Iteration 69, loss = 0.02148424
Iteration 70, loss = 0.02116747
Iteration 71, loss = 0.02082060
Iteration 72, loss = 0.02107395
Iteration 73, loss = 0.02115180
Iteration 74, loss = 0.02115780
Iteration 75, loss = 0.02090089
Iteration 76, loss = 0.02105373
Iteration 77, loss = 0.02090681
Iteration 78, loss = 0.02110319
Iteration 79, loss = 0.02063388
Iteration 80, loss = 0.02049821
Iteration 81, loss = 0.02085709
Iteration 82, loss = 0.02108455
Iteration 83, loss = 0.02073631
Iteration 84, loss = 0.02070124
Iteration 85, loss = 0.02065404
Iteration 86, loss = 0.02071339
Iteration 87, loss = 0.02078783
Iteration 88, loss = 0.02062979
Iteration 89, loss = 0.02057533
Iteration 90, loss = 0.02083742
Iteration 91, loss = 0.02059504
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67762825
Iteration 2, loss = 0.60637055
Iteration 3, loss = 0.49599407
Iteration 4, loss = 0.38099863
Iteration 5, loss = 0.28307508
Iteration 6, loss = 0.21003986
Iteration 7, loss = 0.15934412
Iteration 8, loss = 0.12510647
Iteration 9, loss = 0.10115864
Iteration 10, loss = 0.08442698
Iteration 11, loss = 0.07226451
Iteration 12, loss = 0.06345276
Iteration 13, loss = 0.05637887
Iteration 14, loss = 0.05092827
Iteration 15, loss = 0.04656864
Iteration 16, loss = 0.04337669
Iteration 17, loss = 0.04049068
Iteration 18, loss = 0.03803775
Iteration 19, loss = 0.03605008
Iteration 20, loss = 0.03427993
Iteration 21, loss = 0.03297505
Iteration 22, loss = 0.03174883
Iteration 23, loss = 0.03077922
Iteration 24, loss = 0.02979752
Iteration 25, loss = 0.02910688
Iteration 26, loss = 0.02858526
Iteration 27, loss = 0.02758740
Iteration 28, loss = 0.02704830
Iteration 29, loss = 0.02639600
Iteration 30, loss = 0.02617882
Iteration 31, loss = 0.02574712
Iteration 32, loss = 0.02560881
Iteration 33, loss = 0.02512701
Iteration 34, loss = 0.02477447
Iteration 35, loss = 0.02443737
Iteration 36, loss = 0.02397830
Iteration 37, loss = 0.02388671
Iteration 38, loss = 0.02377894
Iteration 39, loss = 0.02364832
Iteration 40, loss = 0.02388417
Iteration 41, loss = 0.02326469
Iteration 42, loss = 0.02319243
Iteration 43, loss = 0.02295794
Iteration 44, loss = 0.02279768
Iteration 45, loss = 0.02305154
Iteration 46, loss = 0.02274893
Iteration 47, loss = 0.02250100
Iteration 48, loss = 0.02232587
Iteration 49, loss = 0.02231993
Iteration 50, loss = 0.02211205
Iteration 51, loss = 0.02176765
Iteration 52, loss = 0.02203280
Iteration 53, loss = 0.02220624
Iteration 54, loss = 0.02191552
Iteration 55, loss = 0.02183611
Iteration 56, loss = 0.02183622
Iteration 57, loss = 0.02189378
Iteration 58, loss = 0.02157441
Iteration 59, loss = 0.02151389
Iteration 60, loss = 0.02156626
Iteration 61, loss = 0.02170974
Iteration 62, loss = 0.02141659
Iteration 63, loss = 0.02151468
Iteration 64, loss = 0.02116806
Iteration 65, loss = 0.02150005
Iteration 66, loss = 0.02127013
Iteration 67, loss = 0.02154862
Iteration 68, loss = 0.02139173
Iteration 69, loss = 0.02127356
Iteration 70, loss = 0.02094291
Iteration 71, loss = 0.02125051
Iteration 72, loss = 0.02105441
Iteration 73, loss = 0.02118415
Iteration 74, loss = 0.02101854
Iteration 75, loss = 0.02088906
Iteration 76, loss = 0.02125105
Iteration 77, loss = 0.02118529
Iteration 78, loss = 0.02112681
Iteration 79, loss = 0.02080437
Iteration 80, loss = 0.02114424
Iteration 81, loss = 0.02105349
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68652250
Iteration 2, loss = 0.61614324
Iteration 3, loss = 0.50370581
Iteration 4, loss = 0.38537687
Iteration 5, loss = 0.28691027
Iteration 6, loss = 0.21332386
Iteration 7, loss = 0.16283789
Iteration 8, loss = 0.12779820
Iteration 9, loss = 0.10375520
Iteration 10, loss = 0.08683173
Iteration 11, loss = 0.07401553
Iteration 12, loss = 0.06454751
Iteration 13, loss = 0.05747734
Iteration 14, loss = 0.05193360
Iteration 15, loss = 0.04743452
Iteration 16, loss = 0.04402826
Iteration 17, loss = 0.04070157
Iteration 18, loss = 0.03864444
Iteration 19, loss = 0.03643831
Iteration 20, loss = 0.03472029
Iteration 21, loss = 0.03324913
Iteration 22, loss = 0.03201079
Iteration 23, loss = 0.03067800
Iteration 24, loss = 0.02972183
Iteration 25, loss = 0.02887865
Iteration 26, loss = 0.02817226
Iteration 27, loss = 0.02749262
Iteration 28, loss = 0.02700424
Iteration 29, loss = 0.02629091
Iteration 30, loss = 0.02606084
Iteration 31, loss = 0.02566677
Iteration 32, loss = 0.02515032
Iteration 33, loss = 0.02514046
Iteration 34, loss = 0.02466377
Iteration 35, loss = 0.02410315
Iteration 36, loss = 0.02399740
Iteration 37, loss = 0.02382711
Iteration 38, loss = 0.02355104
Iteration 39, loss = 0.02349334
Iteration 40, loss = 0.02308312
Iteration 41, loss = 0.02331173
Iteration 42, loss = 0.02281559
Iteration 43, loss = 0.02267483
Iteration 44, loss = 0.02249447
Iteration 45, loss = 0.02232408
Iteration 46, loss = 0.02220772
Iteration 47, loss = 0.02207589
Iteration 48, loss = 0.02218908
Iteration 49, loss = 0.02210390
Iteration 50, loss = 0.02214309
Iteration 51, loss = 0.02171538
Iteration 52, loss = 0.02202788
Iteration 53, loss = 0.02195204
Iteration 54, loss = 0.02174057
Iteration 55, loss = 0.02147871
Iteration 56, loss = 0.02155652
Iteration 57, loss = 0.02139955
Iteration 58, loss = 0.02122695
Iteration 59, loss = 0.02126337
Iteration 60, loss = 0.02139525
Iteration 61, loss = 0.02130787
Iteration 62, loss = 0.02140551
Iteration 63, loss = 0.02118073
Iteration 64, loss = 0.02106061
Iteration 65, loss = 0.02109565
Iteration 66, loss = 0.02104475
Iteration 67, loss = 0.02085049
Iteration 68, loss = 0.02114415
Iteration 69, loss = 0.02075636
Iteration 70, loss = 0.02088073
Iteration 71, loss = 0.02089050
Iteration 72, loss = 0.02125376
Iteration 73, loss = 0.02088708
Iteration 74, loss = 0.02073345
Iteration 75, loss = 0.02082300
Iteration 76, loss = 0.02083094
Iteration 77, loss = 0.02073432
Iteration 78, loss = 0.02094170
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70124995
Iteration 2, loss = 0.63246906
Iteration 3, loss = 0.52429991
Iteration 4, loss = 0.40384510
Iteration 5, loss = 0.29884172
Iteration 6, loss = 0.22100070
Iteration 7, loss = 0.16692008
Iteration 8, loss = 0.13044416
Iteration 9, loss = 0.10556590
Iteration 10, loss = 0.08825863
Iteration 11, loss = 0.07575094
Iteration 12, loss = 0.06633823
Iteration 13, loss = 0.05929684
Iteration 14, loss = 0.05361734
Iteration 15, loss = 0.04930117
Iteration 16, loss = 0.04570118
Iteration 17, loss = 0.04293929
Iteration 18, loss = 0.04030081
Iteration 19, loss = 0.03855468
Iteration 20, loss = 0.03688146
Iteration 21, loss = 0.03531430
Iteration 22, loss = 0.03414973
Iteration 23, loss = 0.03283925
Iteration 24, loss = 0.03223582
Iteration 25, loss = 0.03136790
Iteration 26, loss = 0.03051685
Iteration 27, loss = 0.02995128
Iteration 28, loss = 0.02936031
Iteration 29, loss = 0.02885093
Iteration 30, loss = 0.02831862
Iteration 31, loss = 0.02792950
Iteration 32, loss = 0.02752579
Iteration 33, loss = 0.02709489
Iteration 34, loss = 0.02689552
Iteration 35, loss = 0.02682008
Iteration 36, loss = 0.02638622
Iteration 37, loss = 0.02589328
Iteration 38, loss = 0.02589694
Iteration 39, loss = 0.02583947
Iteration 40, loss = 0.02570369
Iteration 41, loss = 0.02551263
Iteration 42, loss = 0.02521156
Iteration 43, loss = 0.02514160
Iteration 44, loss = 0.02474680
Iteration 45, loss = 0.02500844
Iteration 46, loss = 0.02509690
Iteration 47, loss = 0.02479879
Iteration 48, loss = 0.02432195
Iteration 49, loss = 0.02442186
Iteration 50, loss = 0.02416722
Iteration 51, loss = 0.02414524
Iteration 52, loss = 0.02394194
Iteration 53, loss = 0.02422956
Iteration 54, loss = 0.02408947
Iteration 55, loss = 0.02396611
Iteration 56, loss = 0.02393857
Iteration 57, loss = 0.02389019
Iteration 58, loss = 0.02378828
Iteration 59, loss = 0.02382877
Iteration 60, loss = 0.02349021
Iteration 61, loss = 0.02367843
Iteration 62, loss = 0.02363281
Iteration 63, loss = 0.02342623
Iteration 64, loss = 0.02366035
Iteration 65, loss = 0.02331388
Iteration 66, loss = 0.02331264
Iteration 67, loss = 0.02337510
Iteration 68, loss = 0.02349996
Iteration 69, loss = 0.02359903
Iteration 70, loss = 0.02337285
Iteration 71, loss = 0.02345579
Iteration 72, loss = 0.02337051
Iteration 73, loss = 0.02331100
Iteration 74, loss = 0.02321086
Iteration 75, loss = 0.02309372
Iteration 76, loss = 0.02322476
Iteration 77, loss = 0.02329469
Iteration 78, loss = 0.02335113
Iteration 79, loss = 0.02341553
Iteration 80, loss = 0.02302939
Iteration 81, loss = 0.02305848
Iteration 82, loss = 0.02311065
Iteration 83, loss = 0.02334604
Iteration 84, loss = 0.02311674
Iteration 85, loss = 0.02310237
Iteration 86, loss = 0.02314848
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67476343
Iteration 2, loss = 0.59204383
Iteration 3, loss = 0.48082219
Iteration 4, loss = 0.36913139
Iteration 5, loss = 0.27616169
Iteration 6, loss = 0.20727802
Iteration 7, loss = 0.15895827
Iteration 8, loss = 0.12570669
Iteration 9, loss = 0.10251193
Iteration 10, loss = 0.08652639
Iteration 11, loss = 0.07463401
Iteration 12, loss = 0.06585138
Iteration 13, loss = 0.05883418
Iteration 14, loss = 0.05383341
Iteration 15, loss = 0.04934938
Iteration 16, loss = 0.04602568
Iteration 17, loss = 0.04311607
Iteration 18, loss = 0.04094983
Iteration 19, loss = 0.03886092
Iteration 20, loss = 0.03721366
Iteration 21, loss = 0.03609706
Iteration 22, loss = 0.03438312
Iteration 23, loss = 0.03371968
Iteration 24, loss = 0.03279909
Iteration 25, loss = 0.03199183
Iteration 26, loss = 0.03113603
Iteration 27, loss = 0.03043298
Iteration 28, loss = 0.03005473
Iteration 29, loss = 0.02951772
Iteration 30, loss = 0.02903605
Iteration 31, loss = 0.02893951
Iteration 32, loss = 0.02817556
Iteration 33, loss = 0.02795878
Iteration 34, loss = 0.02791539
Iteration 35, loss = 0.02735267
Iteration 36, loss = 0.02705244
Iteration 37, loss = 0.02690026
Iteration 38, loss = 0.02688814
Iteration 39, loss = 0.02646764
Iteration 40, loss = 0.02648666
Iteration 41, loss = 0.02579045
Iteration 42, loss = 0.02587865
Iteration 43, loss = 0.02590975
Iteration 44, loss = 0.02557641
Iteration 45, loss = 0.02559273
Iteration 46, loss = 0.02574417
Iteration 47, loss = 0.02548870
Iteration 48, loss = 0.02538160
Iteration 49, loss = 0.02526010
Iteration 50, loss = 0.02500088
Iteration 51, loss = 0.02488729
Iteration 52, loss = 0.02492859
Iteration 53, loss = 0.02488793
Iteration 54, loss = 0.02472482
Iteration 55, loss = 0.02492020
Iteration 56, loss = 0.02475988
Iteration 57, loss = 0.02469182
Iteration 58, loss = 0.02458531
Iteration 59, loss = 0.02478050
Iteration 60, loss = 0.02444398
Iteration 61, loss = 0.02408857
Iteration 62, loss = 0.02440718
Iteration 63, loss = 0.02447574
Iteration 64, loss = 0.02433023
Iteration 65, loss = 0.02436224
Iteration 66, loss = 0.02425611
Iteration 67, loss = 0.02440663
Iteration 68, loss = 0.02418007
Iteration 69, loss = 0.02373169
Iteration 70, loss = 0.02404983
Iteration 71, loss = 0.02405792
Iteration 72, loss = 0.02400330
Iteration 73, loss = 0.02430276
Iteration 74, loss = 0.02416731
Iteration 75, loss = 0.02385474
Iteration 76, loss = 0.02407374
Iteration 77, loss = 0.02366730
Iteration 78, loss = 0.02366848
Iteration 79, loss = 0.02378420
Iteration 80, loss = 0.02379881
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67294411
Iteration 2, loss = 0.59605809
Iteration 3, loss = 0.47896505
Iteration 4, loss = 0.36203930
Iteration 5, loss = 0.26663361
Iteration 6, loss = 0.19752756
Iteration 7, loss = 0.15037078
Iteration 8, loss = 0.11866269
Iteration 9, loss = 0.09674701
Iteration 10, loss = 0.08073066
Iteration 11, loss = 0.06944770
Iteration 12, loss = 0.06105140
Iteration 13, loss = 0.05440961
Iteration 14, loss = 0.04950245
Iteration 15, loss = 0.04559891
Iteration 16, loss = 0.04196948
Iteration 17, loss = 0.03930204
Iteration 18, loss = 0.03713807
Iteration 19, loss = 0.03511238
Iteration 20, loss = 0.03368363
Iteration 21, loss = 0.03230231
Iteration 22, loss = 0.03113507
Iteration 23, loss = 0.03022532
Iteration 24, loss = 0.02905322
Iteration 25, loss = 0.02857148
Iteration 26, loss = 0.02752486
Iteration 27, loss = 0.02736545
Iteration 28, loss = 0.02668986
Iteration 29, loss = 0.02596192
Iteration 30, loss = 0.02571503
Iteration 31, loss = 0.02534547
Iteration 32, loss = 0.02500539
Iteration 33, loss = 0.02454918
Iteration 34, loss = 0.02432813
Iteration 35, loss = 0.02417582
Iteration 36, loss = 0.02419979
Iteration 37, loss = 0.02369350
Iteration 38, loss = 0.02345098
Iteration 39, loss = 0.02340272
Iteration 40, loss = 0.02351585
Iteration 41, loss = 0.02279412
Iteration 42, loss = 0.02282971
Iteration 43, loss = 0.02271586
Iteration 44, loss = 0.02240786
Iteration 45, loss = 0.02234749
Iteration 46, loss = 0.02239825
Iteration 47, loss = 0.02243678
Iteration 48, loss = 0.02216253
Iteration 49, loss = 0.02202844
Iteration 50, loss = 0.02211981
Iteration 51, loss = 0.02189186
Iteration 52, loss = 0.02193111
Iteration 53, loss = 0.02192093
Iteration 54, loss = 0.02164581
Iteration 55, loss = 0.02161834
Iteration 56, loss = 0.02139742
Iteration 57, loss = 0.02186936
Iteration 58, loss = 0.02134263
Iteration 59, loss = 0.02128562
Iteration 60, loss = 0.02150239
Iteration 61, loss = 0.02146413
Iteration 62, loss = 0.02136798
Iteration 63, loss = 0.02117515
Iteration 64, loss = 0.02133798
Iteration 65, loss = 0.02124001
Iteration 66, loss = 0.02092827
Iteration 67, loss = 0.02114587
Iteration 68, loss = 0.02141820
Iteration 69, loss = 0.02148424
Iteration 70, loss = 0.02116747
Iteration 71, loss = 0.02082060
Iteration 72, loss = 0.02107395
Iteration 73, loss = 0.02115180
Iteration 74, loss = 0.02115780
Iteration 75, loss = 0.02090089
Iteration 76, loss = 0.02105373
Iteration 77, loss = 0.02090681
Iteration 78, loss = 0.02110319
Iteration 79, loss = 0.02063388
Iteration 80, loss = 0.02049821
Iteration 81, loss = 0.02085709
Iteration 82, loss = 0.02108455
Iteration 83, loss = 0.02073631
Iteration 84, loss = 0.02070124
Iteration 85, loss = 0.02065404
Iteration 86, loss = 0.02071339
Iteration 87, loss = 0.02078783
Iteration 88, loss = 0.02062979
Iteration 89, loss = 0.02057533
Iteration 90, loss = 0.02083742
Iteration 91, loss = 0.02059504
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67762825
Iteration 2, loss = 0.60637055
Iteration 3, loss = 0.49599407
Iteration 4, loss = 0.38099863
Iteration 5, loss = 0.28307508
Iteration 6, loss = 0.21003986
Iteration 7, loss = 0.15934412
Iteration 8, loss = 0.12510647
Iteration 9, loss = 0.10115864
Iteration 10, loss = 0.08442698
Iteration 11, loss = 0.07226451
Iteration 12, loss = 0.06345276
Iteration 13, loss = 0.05637887
Iteration 14, loss = 0.05092827
Iteration 15, loss = 0.04656864
Iteration 16, loss = 0.04337669
Iteration 17, loss = 0.04049068
Iteration 18, loss = 0.03803775
Iteration 19, loss = 0.03605008
Iteration 20, loss = 0.03427993
Iteration 21, loss = 0.03297505
Iteration 22, loss = 0.03174883
Iteration 23, loss = 0.03077922
Iteration 24, loss = 0.02979752
Iteration 25, loss = 0.02910688
Iteration 26, loss = 0.02858526
Iteration 27, loss = 0.02758740
Iteration 28, loss = 0.02704830
Iteration 29, loss = 0.02639600
Iteration 30, loss = 0.02617882
Iteration 31, loss = 0.02574712
Iteration 32, loss = 0.02560881
Iteration 33, loss = 0.02512701
Iteration 34, loss = 0.02477447
Iteration 35, loss = 0.02443737
Iteration 36, loss = 0.02397830
Iteration 37, loss = 0.02388671
Iteration 38, loss = 0.02377894
Iteration 39, loss = 0.02364832
Iteration 40, loss = 0.02388417
Iteration 41, loss = 0.02326469
Iteration 42, loss = 0.02319243
Iteration 43, loss = 0.02295794
Iteration 44, loss = 0.02279768
Iteration 45, loss = 0.02305154
Iteration 46, loss = 0.02274893
Iteration 47, loss = 0.02250100
Iteration 48, loss = 0.02232587
Iteration 49, loss = 0.02231993
Iteration 50, loss = 0.02211205
Iteration 51, loss = 0.02176765
Iteration 52, loss = 0.02203280
Iteration 53, loss = 0.02220624
Iteration 54, loss = 0.02191552
Iteration 55, loss = 0.02183611
Iteration 56, loss = 0.02183622
Iteration 57, loss = 0.02189378
Iteration 58, loss = 0.02157441
Iteration 59, loss = 0.02151389
Iteration 60, loss = 0.02156626
Iteration 61, loss = 0.02170974
Iteration 62, loss = 0.02141659
Iteration 63, loss = 0.02151468
Iteration 64, loss = 0.02116806
Iteration 65, loss = 0.02150005
Iteration 66, loss = 0.02127013
Iteration 67, loss = 0.02154862
Iteration 68, loss = 0.02139173
Iteration 69, loss = 0.02127356
Iteration 70, loss = 0.02094291
Iteration 71, loss = 0.02125051
Iteration 72, loss = 0.02105441
Iteration 73, loss = 0.02118415
Iteration 74, loss = 0.02101854
Iteration 75, loss = 0.02088906
Iteration 76, loss = 0.02125105
Iteration 77, loss = 0.02118529
Iteration 78, loss = 0.02112681
Iteration 79, loss = 0.02080437
Iteration 80, loss = 0.02114424
Iteration 81, loss = 0.02105349
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68652250
Iteration 2, loss = 0.61614324
Iteration 3, loss = 0.50370581
Iteration 4, loss = 0.38537687
Iteration 5, loss = 0.28691027
Iteration 6, loss = 0.21332386
Iteration 7, loss = 0.16283789
Iteration 8, loss = 0.12779820
Iteration 9, loss = 0.10375520
Iteration 10, loss = 0.08683173
Iteration 11, loss = 0.07401553
Iteration 12, loss = 0.06454751
Iteration 13, loss = 0.05747734
Iteration 14, loss = 0.05193360
Iteration 15, loss = 0.04743452
Iteration 16, loss = 0.04402826
Iteration 17, loss = 0.04070157
Iteration 18, loss = 0.03864444
Iteration 19, loss = 0.03643831
Iteration 20, loss = 0.03472029
Iteration 21, loss = 0.03324913
Iteration 22, loss = 0.03201079
Iteration 23, loss = 0.03067800
Iteration 24, loss = 0.02972183
Iteration 25, loss = 0.02887865
Iteration 26, loss = 0.02817226
Iteration 27, loss = 0.02749262
Iteration 28, loss = 0.02700424
Iteration 29, loss = 0.02629091
Iteration 30, loss = 0.02606084
Iteration 31, loss = 0.02566677
Iteration 32, loss = 0.02515032
Iteration 33, loss = 0.02514046
Iteration 34, loss = 0.02466377
Iteration 35, loss = 0.02410315
Iteration 36, loss = 0.02399740
Iteration 37, loss = 0.02382711
Iteration 38, loss = 0.02355104
Iteration 39, loss = 0.02349334
Iteration 40, loss = 0.02308312
Iteration 41, loss = 0.02331173
Iteration 42, loss = 0.02281559
Iteration 43, loss = 0.02267483
Iteration 44, loss = 0.02249447
Iteration 45, loss = 0.02232408
Iteration 46, loss = 0.02220772
Iteration 47, loss = 0.02207589
Iteration 48, loss = 0.02218908
Iteration 49, loss = 0.02210390
Iteration 50, loss = 0.02214309
Iteration 51, loss = 0.02171538
Iteration 52, loss = 0.02202788
Iteration 53, loss = 0.02195204
Iteration 54, loss = 0.02174057
Iteration 55, loss = 0.02147871
Iteration 56, loss = 0.02155652
Iteration 57, loss = 0.02139955
Iteration 58, loss = 0.02122695
Iteration 59, loss = 0.02126337
Iteration 60, loss = 0.02139525
Iteration 61, loss = 0.02130787
Iteration 62, loss = 0.02140551
Iteration 63, loss = 0.02118073
Iteration 64, loss = 0.02106061
Iteration 65, loss = 0.02109565
Iteration 66, loss = 0.02104475
Iteration 67, loss = 0.02085049
Iteration 68, loss = 0.02114415
Iteration 69, loss = 0.02075636
Iteration 70, loss = 0.02088073
Iteration 71, loss = 0.02089050
Iteration 72, loss = 0.02125376
Iteration 73, loss = 0.02088708
Iteration 74, loss = 0.02073345
Iteration 75, loss = 0.02082300
Iteration 76, loss = 0.02083094
Iteration 77, loss = 0.02073432
Iteration 78, loss = 0.02094170
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70124995
Iteration 2, loss = 0.63246906
Iteration 3, loss = 0.52429991
Iteration 4, loss = 0.40384510
Iteration 5, loss = 0.29884172
Iteration 6, loss = 0.22100070
Iteration 7, loss = 0.16692008
Iteration 8, loss = 0.13044416
Iteration 9, loss = 0.10556590
Iteration 10, loss = 0.08825863
Iteration 11, loss = 0.07575094
Iteration 12, loss = 0.06633823
Iteration 13, loss = 0.05929684
Iteration 14, loss = 0.05361734
Iteration 15, loss = 0.04930117
Iteration 16, loss = 0.04570118
Iteration 17, loss = 0.04293929
Iteration 18, loss = 0.04030081
Iteration 19, loss = 0.03855468
Iteration 20, loss = 0.03688146
Iteration 21, loss = 0.03531430
Iteration 22, loss = 0.03414973
Iteration 23, loss = 0.03283925
Iteration 24, loss = 0.03223582
Iteration 25, loss = 0.03136790
Iteration 26, loss = 0.03051685
Iteration 27, loss = 0.02995128
Iteration 28, loss = 0.02936031
Iteration 29, loss = 0.02885093
Iteration 30, loss = 0.02831862
Iteration 31, loss = 0.02792950
Iteration 32, loss = 0.02752579
Iteration 33, loss = 0.02709489
Iteration 34, loss = 0.02689552
Iteration 35, loss = 0.02682008
Iteration 36, loss = 0.02638622
Iteration 37, loss = 0.02589328
Iteration 38, loss = 0.02589694
Iteration 39, loss = 0.02583947
Iteration 40, loss = 0.02570369
Iteration 41, loss = 0.02551263
Iteration 42, loss = 0.02521156
Iteration 43, loss = 0.02514160
Iteration 44, loss = 0.02474680
Iteration 45, loss = 0.02500844
Iteration 46, loss = 0.02509690
Iteration 47, loss = 0.02479879
Iteration 48, loss = 0.02432195
Iteration 49, loss = 0.02442186
Iteration 50, loss = 0.02416722
Iteration 51, loss = 0.02414524
Iteration 52, loss = 0.02394194
Iteration 53, loss = 0.02422956
Iteration 54, loss = 0.02408947
Iteration 55, loss = 0.02396611
Iteration 56, loss = 0.02393857
Iteration 57, loss = 0.02389019
Iteration 58, loss = 0.02378828
Iteration 59, loss = 0.02382877
Iteration 60, loss = 0.02349021
Iteration 61, loss = 0.02367843
Iteration 62, loss = 0.02363281
Iteration 63, loss = 0.02342623
Iteration 64, loss = 0.02366035
Iteration 65, loss = 0.02331388
Iteration 66, loss = 0.02331264
Iteration 67, loss = 0.02337510
Iteration 68, loss = 0.02349996
Iteration 69, loss = 0.02359903
Iteration 70, loss = 0.02337285
Iteration 71, loss = 0.02345579
Iteration 72, loss = 0.02337051
Iteration 73, loss = 0.02331100
Iteration 74, loss = 0.02321086
Iteration 75, loss = 0.02309372
Iteration 76, loss = 0.02322476
Iteration 77, loss = 0.02329469
Iteration 78, loss = 0.02335113
Iteration 79, loss = 0.02341553
Iteration 80, loss = 0.02302939
Iteration 81, loss = 0.02305848
Iteration 82, loss = 0.02311065
Iteration 83, loss = 0.02334604
Iteration 84, loss = 0.02311674
Iteration 85, loss = 0.02310237
Iteration 86, loss = 0.02314848
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67476343
Iteration 2, loss = 0.59204383
Iteration 3, loss = 0.48082219
Iteration 4, loss = 0.36913139
Iteration 5, loss = 0.27616169
Iteration 6, loss = 0.20727802
Iteration 7, loss = 0.15895827
Iteration 8, loss = 0.12570669
Iteration 9, loss = 0.10251193
Iteration 10, loss = 0.08652639
Iteration 11, loss = 0.07463401
Iteration 12, loss = 0.06585138
Iteration 13, loss = 0.05883418
Iteration 14, loss = 0.05383341
Iteration 15, loss = 0.04934938
Iteration 16, loss = 0.04602568
Iteration 17, loss = 0.04311607
Iteration 18, loss = 0.04094983
Iteration 19, loss = 0.03886092
Iteration 20, loss = 0.03721366
Iteration 21, loss = 0.03609706
Iteration 22, loss = 0.03438312
Iteration 23, loss = 0.03371968
Iteration 24, loss = 0.03279909
Iteration 25, loss = 0.03199183
Iteration 26, loss = 0.03113603
Iteration 27, loss = 0.03043298
Iteration 28, loss = 0.03005473
Iteration 29, loss = 0.02951772
Iteration 30, loss = 0.02903605
Iteration 31, loss = 0.02893951
Iteration 32, loss = 0.02817556
Iteration 33, loss = 0.02795878
Iteration 34, loss = 0.02791539
Iteration 35, loss = 0.02735267
Iteration 36, loss = 0.02705244
Iteration 37, loss = 0.02690026
Iteration 38, loss = 0.02688814
Iteration 39, loss = 0.02646764
Iteration 40, loss = 0.02648666
Iteration 41, loss = 0.02579045
Iteration 42, loss = 0.02587865
Iteration 43, loss = 0.02590975
Iteration 44, loss = 0.02557641
Iteration 45, loss = 0.02559273
Iteration 46, loss = 0.02574417
Iteration 47, loss = 0.02548870
Iteration 48, loss = 0.02538160
Iteration 49, loss = 0.02526010
Iteration 50, loss = 0.02500088
Iteration 51, loss = 0.02488729
Iteration 52, loss = 0.02492859
Iteration 53, loss = 0.02488793
Iteration 54, loss = 0.02472482
Iteration 55, loss = 0.02492020
Iteration 56, loss = 0.02475988
Iteration 57, loss = 0.02469182
Iteration 58, loss = 0.02458531
Iteration 59, loss = 0.02478050
Iteration 60, loss = 0.02444398
Iteration 61, loss = 0.02408857
Iteration 62, loss = 0.02440718
Iteration 63, loss = 0.02447574
Iteration 64, loss = 0.02433023
Iteration 65, loss = 0.02436224
Iteration 66, loss = 0.02425611
Iteration 67, loss = 0.02440663
Iteration 68, loss = 0.02418007
Iteration 69, loss = 0.02373169
Iteration 70, loss = 0.02404983
Iteration 71, loss = 0.02405792
Iteration 72, loss = 0.02400330
Iteration 73, loss = 0.02430276
Iteration 74, loss = 0.02416731
Iteration 75, loss = 0.02385474
Iteration 76, loss = 0.02407374
Iteration 77, loss = 0.02366730
Iteration 78, loss = 0.02366848
Iteration 79, loss = 0.02378420
Iteration 80, loss = 0.02379881
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67294411
Iteration 2, loss = 0.59605809
Iteration 3, loss = 0.47896505
Iteration 4, loss = 0.36203930
Iteration 5, loss = 0.26663361
Iteration 6, loss = 0.19752756
Iteration 7, loss = 0.15037078
Iteration 8, loss = 0.11866269
Iteration 9, loss = 0.09674701
Iteration 10, loss = 0.08073066
Iteration 11, loss = 0.06944770
Iteration 12, loss = 0.06105140
Iteration 13, loss = 0.05440961
Iteration 14, loss = 0.04950245
Iteration 15, loss = 0.04559891
Iteration 16, loss = 0.04196948
Iteration 17, loss = 0.03930204
Iteration 18, loss = 0.03713807
Iteration 19, loss = 0.03511238
Iteration 20, loss = 0.03368363
Iteration 21, loss = 0.03230231
Iteration 22, loss = 0.03113507
Iteration 23, loss = 0.03022532
Iteration 24, loss = 0.02905322
Iteration 25, loss = 0.02857148
Iteration 26, loss = 0.02752486
Iteration 27, loss = 0.02736545
Iteration 28, loss = 0.02668986
Iteration 29, loss = 0.02596192
Iteration 30, loss = 0.02571503
Iteration 31, loss = 0.02534547
Iteration 32, loss = 0.02500539
Iteration 33, loss = 0.02454918
Iteration 34, loss = 0.02432813
Iteration 35, loss = 0.02417582
Iteration 36, loss = 0.02419979
Iteration 37, loss = 0.02369350
Iteration 38, loss = 0.02345098
Iteration 39, loss = 0.02340272
Iteration 40, loss = 0.02351585
Iteration 41, loss = 0.02279412
Iteration 42, loss = 0.02282971
Iteration 43, loss = 0.02271586
Iteration 44, loss = 0.02240786
Iteration 45, loss = 0.02234749
Iteration 46, loss = 0.02239825
Iteration 47, loss = 0.02243678
Iteration 48, loss = 0.02216253
Iteration 49, loss = 0.02202844
Iteration 50, loss = 0.02211981
Iteration 51, loss = 0.02189186
Iteration 52, loss = 0.02193111
Iteration 53, loss = 0.02192093
Iteration 54, loss = 0.02164581
Iteration 55, loss = 0.02161834
Iteration 56, loss = 0.02139742
Iteration 57, loss = 0.02186936
Iteration 58, loss = 0.02134263
Iteration 59, loss = 0.02128562
Iteration 60, loss = 0.02150239
Iteration 61, loss = 0.02146413
Iteration 62, loss = 0.02136798
Iteration 63, loss = 0.02117515
Iteration 64, loss = 0.02133798
Iteration 65, loss = 0.02124001
Iteration 66, loss = 0.02092827
Iteration 67, loss = 0.02114587
Iteration 68, loss = 0.02141820
Iteration 69, loss = 0.02148424
Iteration 70, loss = 0.02116747
Iteration 71, loss = 0.02082060
Iteration 72, loss = 0.02107395
Iteration 73, loss = 0.02115180
Iteration 74, loss = 0.02115780
Iteration 75, loss = 0.02090089
Iteration 76, loss = 0.02105373
Iteration 77, loss = 0.02090681
Iteration 78, loss = 0.02110319
Iteration 79, loss = 0.02063388
Iteration 80, loss = 0.02049821
Iteration 81, loss = 0.02085709
Iteration 82, loss = 0.02108455
Iteration 83, loss = 0.02073631
Iteration 84, loss = 0.02070124
Iteration 85, loss = 0.02065404
Iteration 86, loss = 0.02071339
Iteration 87, loss = 0.02078783
Iteration 88, loss = 0.02062979
Iteration 89, loss = 0.02057533
Iteration 90, loss = 0.02083742
Iteration 91, loss = 0.02059504
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67762825
Iteration 2, loss = 0.60637055
Iteration 3, loss = 0.49599407
Iteration 4, loss = 0.38099863
Iteration 5, loss = 0.28307508
Iteration 6, loss = 0.21003986
Iteration 7, loss = 0.15934412
Iteration 8, loss = 0.12510647
Iteration 9, loss = 0.10115864
Iteration 10, loss = 0.08442698
Iteration 11, loss = 0.07226451
Iteration 12, loss = 0.06345276
Iteration 13, loss = 0.05637887
Iteration 14, loss = 0.05092827
Iteration 15, loss = 0.04656864
Iteration 16, loss = 0.04337669
Iteration 17, loss = 0.04049068
Iteration 18, loss = 0.03803775
Iteration 19, loss = 0.03605008
Iteration 20, loss = 0.03427993
Iteration 21, loss = 0.03297505
Iteration 22, loss = 0.03174883
Iteration 23, loss = 0.03077922
Iteration 24, loss = 0.02979752
Iteration 25, loss = 0.02910688
Iteration 26, loss = 0.02858526
Iteration 27, loss = 0.02758740
Iteration 28, loss = 0.02704830
Iteration 29, loss = 0.02639600
Iteration 30, loss = 0.02617882
Iteration 31, loss = 0.02574712
Iteration 32, loss = 0.02560881
Iteration 33, loss = 0.02512701
Iteration 34, loss = 0.02477447
Iteration 35, loss = 0.02443737
Iteration 36, loss = 0.02397830
Iteration 37, loss = 0.02388671
Iteration 38, loss = 0.02377894
Iteration 39, loss = 0.02364832
Iteration 40, loss = 0.02388417
Iteration 41, loss = 0.02326469
Iteration 42, loss = 0.02319243
Iteration 43, loss = 0.02295794
Iteration 44, loss = 0.02279768
Iteration 45, loss = 0.02305154
Iteration 46, loss = 0.02274893
Iteration 47, loss = 0.02250100
Iteration 48, loss = 0.02232587
Iteration 49, loss = 0.02231993
Iteration 50, loss = 0.02211205
Iteration 51, loss = 0.02176765
Iteration 52, loss = 0.02203280
Iteration 53, loss = 0.02220624
Iteration 54, loss = 0.02191552
Iteration 55, loss = 0.02183611
Iteration 56, loss = 0.02183622
Iteration 57, loss = 0.02189378
Iteration 58, loss = 0.02157441
Iteration 59, loss = 0.02151389
Iteration 60, loss = 0.02156626
Iteration 61, loss = 0.02170974
Iteration 62, loss = 0.02141659
Iteration 63, loss = 0.02151468
Iteration 64, loss = 0.02116806
Iteration 65, loss = 0.02150005
Iteration 66, loss = 0.02127013
Iteration 67, loss = 0.02154862
Iteration 68, loss = 0.02139173
Iteration 69, loss = 0.02127356
Iteration 70, loss = 0.02094291
Iteration 71, loss = 0.02125051
Iteration 72, loss = 0.02105441
Iteration 73, loss = 0.02118415
Iteration 74, loss = 0.02101854
Iteration 75, loss = 0.02088906
Iteration 76, loss = 0.02125105
Iteration 77, loss = 0.02118529
Iteration 78, loss = 0.02112681
Iteration 79, loss = 0.02080437
Iteration 80, loss = 0.02114424
Iteration 81, loss = 0.02105349
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68652250
Iteration 2, loss = 0.61614324
Iteration 3, loss = 0.50370581
Iteration 4, loss = 0.38537687
Iteration 5, loss = 0.28691027
Iteration 6, loss = 0.21332386
Iteration 7, loss = 0.16283789
Iteration 8, loss = 0.12779820
Iteration 9, loss = 0.10375520
Iteration 10, loss = 0.08683173
Iteration 11, loss = 0.07401553
Iteration 12, loss = 0.06454751
Iteration 13, loss = 0.05747734
Iteration 14, loss = 0.05193360
Iteration 15, loss = 0.04743452
Iteration 16, loss = 0.04402826
Iteration 17, loss = 0.04070157
Iteration 18, loss = 0.03864444
Iteration 19, loss = 0.03643831
Iteration 20, loss = 0.03472029
Iteration 21, loss = 0.03324913
Iteration 22, loss = 0.03201079
Iteration 23, loss = 0.03067800
Iteration 24, loss = 0.02972183
Iteration 25, loss = 0.02887865
Iteration 26, loss = 0.02817226
Iteration 27, loss = 0.02749262
Iteration 28, loss = 0.02700424
Iteration 29, loss = 0.02629091
Iteration 30, loss = 0.02606084
Iteration 31, loss = 0.02566677
Iteration 32, loss = 0.02515032
Iteration 33, loss = 0.02514046
Iteration 34, loss = 0.02466377
Iteration 35, loss = 0.02410315
Iteration 36, loss = 0.02399740
Iteration 37, loss = 0.02382711
Iteration 38, loss = 0.02355104
Iteration 39, loss = 0.02349334
Iteration 40, loss = 0.02308312
Iteration 41, loss = 0.02331173
Iteration 42, loss = 0.02281559
Iteration 43, loss = 0.02267483
Iteration 44, loss = 0.02249447
Iteration 45, loss = 0.02232408
Iteration 46, loss = 0.02220772
Iteration 47, loss = 0.02207589
Iteration 48, loss = 0.02218908
Iteration 49, loss = 0.02210390
Iteration 50, loss = 0.02214309
Iteration 51, loss = 0.02171538
Iteration 52, loss = 0.02202788
Iteration 53, loss = 0.02195204
Iteration 54, loss = 0.02174057
Iteration 55, loss = 0.02147871
Iteration 56, loss = 0.02155652
Iteration 57, loss = 0.02139955
Iteration 58, loss = 0.02122695
Iteration 59, loss = 0.02126337
Iteration 60, loss = 0.02139525
Iteration 61, loss = 0.02130787
Iteration 62, loss = 0.02140551
Iteration 63, loss = 0.02118073
Iteration 64, loss = 0.02106061
Iteration 65, loss = 0.02109565
Iteration 66, loss = 0.02104475
Iteration 67, loss = 0.02085049
Iteration 68, loss = 0.02114415
Iteration 69, loss = 0.02075636
Iteration 70, loss = 0.02088073
Iteration 71, loss = 0.02089050
Iteration 72, loss = 0.02125376
Iteration 73, loss = 0.02088708
Iteration 74, loss = 0.02073345
Iteration 75, loss = 0.02082300
Iteration 76, loss = 0.02083094
Iteration 77, loss = 0.02073432
Iteration 78, loss = 0.02094170
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68507242
Iteration 2, loss = 0.59044645
Iteration 3, loss = 0.44812826
Iteration 4, loss = 0.31923416
Iteration 5, loss = 0.22499390
Iteration 6, loss = 0.16364699
Iteration 7, loss = 0.12458235
Iteration 8, loss = 0.09924342
Iteration 9, loss = 0.08222530
Iteration 10, loss = 0.06994409
Iteration 11, loss = 0.06155091
Iteration 12, loss = 0.05495607
Iteration 13, loss = 0.05013033
Iteration 14, loss = 0.04638191
Iteration 15, loss = 0.04311350
Iteration 16, loss = 0.04040438
Iteration 17, loss = 0.03842979
Iteration 18, loss = 0.03689602
Iteration 19, loss = 0.03553687
Iteration 20, loss = 0.03409551
Iteration 21, loss = 0.03276754
Iteration 22, loss = 0.03225803
Iteration 23, loss = 0.03159600
Iteration 24, loss = 0.03099493
Iteration 25, loss = 0.03020026
Iteration 26, loss = 0.02967924
Iteration 27, loss = 0.02919412
Iteration 28, loss = 0.02888082
Iteration 29, loss = 0.02831040
Iteration 30, loss = 0.02836508
Iteration 31, loss = 0.02813207
Iteration 32, loss = 0.02785477
Iteration 33, loss = 0.02721405
Iteration 34, loss = 0.02703019
Iteration 35, loss = 0.02701224
Iteration 36, loss = 0.02699603
Iteration 37, loss = 0.02668619
Iteration 38, loss = 0.02655134
Iteration 39, loss = 0.02643852
Iteration 40, loss = 0.02624193
Iteration 41, loss = 0.02617018
Iteration 42, loss = 0.02593815
Iteration 43, loss = 0.02632991
Iteration 44, loss = 0.02573425
Iteration 45, loss = 0.02567556
Iteration 46, loss = 0.02581677
Iteration 47, loss = 0.02539658
Iteration 48, loss = 0.02528085
Iteration 49, loss = 0.02534885
Iteration 50, loss = 0.02530614
Iteration 51, loss = 0.02518896
Iteration 52, loss = 0.02519458
Iteration 53, loss = 0.02536750
Iteration 54, loss = 0.02489784
Iteration 55, loss = 0.02503762
Iteration 56, loss = 0.02538629
Iteration 57, loss = 0.02514187
Iteration 58, loss = 0.02506841
Iteration 59, loss = 0.02487180
Iteration 60, loss = 0.02503047
Iteration 61, loss = 0.02496876
Iteration 62, loss = 0.02472994
Iteration 63, loss = 0.02457149
Iteration 64, loss = 0.02472948
Iteration 65, loss = 0.02462023
Iteration 66, loss = 0.02460699
Iteration 67, loss = 0.02467716
Iteration 68, loss = 0.02485185
Iteration 69, loss = 0.02473351
Iteration 70, loss = 0.02444698
Iteration 71, loss = 0.02469930
Iteration 72, loss = 0.02469564
Iteration 73, loss = 0.02432047
Iteration 74, loss = 0.02435601
Iteration 75, loss = 0.02447980
Iteration 76, loss = 0.02442645
Iteration 77, loss = 0.02442789
Iteration 78, loss = 0.02436988
Iteration 79, loss = 0.02449825
Iteration 80, loss = 0.02435916
Iteration 81, loss = 0.02417852
Iteration 82, loss = 0.02462141
Iteration 83, loss = 0.02423107
Iteration 84, loss = 0.02423406
Iteration 85, loss = 0.02399921
Iteration 86, loss = 0.02395035
Iteration 87, loss = 0.02404208
Iteration 88, loss = 0.02414580
Iteration 89, loss = 0.02432096
Iteration 90, loss = 0.02407150
Iteration 91, loss = 0.02405516
Iteration 92, loss = 0.02433239
Iteration 93, loss = 0.02387504
Iteration 94, loss = 0.02408480
Iteration 95, loss = 0.02400557
Iteration 96, loss = 0.02391340
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on prepended_v7_lowercase_words_only...
Iteration 1, loss = 0.69464776
Iteration 2, loss = 0.64715175
Iteration 3, loss = 0.58193932
Iteration 4, loss = 0.50265464
Iteration 5, loss = 0.42059768
Iteration 6, loss = 0.34482134
Iteration 7, loss = 0.27896936
Iteration 8, loss = 0.22506219
Iteration 9, loss = 0.18278176
Iteration 10, loss = 0.15009524
Iteration 11, loss = 0.12538400
Iteration 12, loss = 0.10642434
Iteration 13, loss = 0.09175729
Iteration 14, loss = 0.08030779
Iteration 15, loss = 0.07127809
Iteration 16, loss = 0.06408401
Iteration 17, loss = 0.05804565
Iteration 18, loss = 0.05276440
Iteration 19, loss = 0.04927157
Iteration 20, loss = 0.04555423
Iteration 21, loss = 0.04268307
Iteration 22, loss = 0.04016627
Iteration 23, loss = 0.03785583
Iteration 24, loss = 0.03598810
Iteration 25, loss = 0.03451140
Iteration 26, loss = 0.03297310
Iteration 27, loss = 0.03149619
Iteration 28, loss = 0.03058177
Iteration 29, loss = 0.02945739
Iteration 30, loss = 0.02853860
Iteration 31, loss = 0.02771688
Iteration 32, loss = 0.02705729
Iteration 33, loss = 0.02661715
Iteration 34, loss = 0.02573204
Iteration 35, loss = 0.02521727
Iteration 36, loss = 0.02502504
Iteration 37, loss = 0.02430638
Iteration 38, loss = 0.02384856
Iteration 39, loss = 0.02357540
Iteration 40, loss = 0.02313088
Iteration 41, loss = 0.02282562
Iteration 42, loss = 0.02256064
Iteration 43, loss = 0.02213049
Iteration 44, loss = 0.02185910
Iteration 45, loss = 0.02162816
Iteration 46, loss = 0.02149074
Iteration 47, loss = 0.02145193
Iteration 48, loss = 0.02093281
Iteration 49, loss = 0.02094264
Iteration 50, loss = 0.02076666
Iteration 51, loss = 0.02064504
Iteration 52, loss = 0.02036059
Iteration 53, loss = 0.02026173
Iteration 54, loss = 0.01999601
Iteration 55, loss = 0.01987339
Iteration 56, loss = 0.01973907
Iteration 57, loss = 0.01982094
Iteration 58, loss = 0.01965938
Iteration 59, loss = 0.01947219
Iteration 60, loss = 0.01930070
Iteration 61, loss = 0.01934128
Iteration 62, loss = 0.01920806
Iteration 63, loss = 0.01901864
Iteration 64, loss = 0.01900386
Iteration 65, loss = 0.01906279
Iteration 66, loss = 0.01877321
Iteration 67, loss = 0.01883044
Iteration 68, loss = 0.01887624
Iteration 69, loss = 0.01845379
Iteration 70, loss = 0.01857924
Iteration 71, loss = 0.01846349
Iteration 72, loss = 0.01853348
Iteration 73, loss = 0.01848490
Iteration 74, loss = 0.01868452
Iteration 75, loss = 0.01833816
Iteration 76, loss = 0.01830380
Iteration 77, loss = 0.01814877
Iteration 78, loss = 0.01812724
Iteration 79, loss = 0.01803277
Iteration 80, loss = 0.01804557
Iteration 81, loss = 0.01811835
Iteration 82, loss = 0.01796001
Iteration 83, loss = 0.01792971
Iteration 84, loss = 0.01791485
Iteration 85, loss = 0.01782961
Iteration 86, loss = 0.01787765
Iteration 87, loss = 0.01809623
Iteration 88, loss = 0.01780989
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72220947
Iteration 2, loss = 0.68306955
Iteration 3, loss = 0.62599636
Iteration 4, loss = 0.54860116
Iteration 5, loss = 0.46322137
Iteration 6, loss = 0.37924224
Iteration 7, loss = 0.30621718
Iteration 8, loss = 0.24629035
Iteration 9, loss = 0.19932290
Iteration 10, loss = 0.16337281
Iteration 11, loss = 0.13575478
Iteration 12, loss = 0.11510502
Iteration 13, loss = 0.09916794
Iteration 14, loss = 0.08655964
Iteration 15, loss = 0.07702126
Iteration 16, loss = 0.06901559
Iteration 17, loss = 0.06265828
Iteration 18, loss = 0.05739353
Iteration 19, loss = 0.05309653
Iteration 20, loss = 0.04960541
Iteration 21, loss = 0.04630657
Iteration 22, loss = 0.04375343
Iteration 23, loss = 0.04133836
Iteration 24, loss = 0.03940434
Iteration 25, loss = 0.03755332
Iteration 26, loss = 0.03607058
Iteration 27, loss = 0.03496338
Iteration 28, loss = 0.03348448
Iteration 29, loss = 0.03225922
Iteration 30, loss = 0.03135927
Iteration 31, loss = 0.03046516
Iteration 32, loss = 0.02985157
Iteration 33, loss = 0.02914663
Iteration 34, loss = 0.02849610
Iteration 35, loss = 0.02774899
Iteration 36, loss = 0.02731442
Iteration 37, loss = 0.02703725
Iteration 38, loss = 0.02644368
Iteration 39, loss = 0.02600081
Iteration 40, loss = 0.02571465
Iteration 41, loss = 0.02512695
Iteration 42, loss = 0.02505689
Iteration 43, loss = 0.02481231
Iteration 44, loss = 0.02452563
Iteration 45, loss = 0.02414239
Iteration 46, loss = 0.02398246
Iteration 47, loss = 0.02357552
Iteration 48, loss = 0.02338411
Iteration 49, loss = 0.02328987
Iteration 50, loss = 0.02300691
Iteration 51, loss = 0.02309072
Iteration 52, loss = 0.02269020
Iteration 53, loss = 0.02251096
Iteration 54, loss = 0.02242014
Iteration 55, loss = 0.02239700
Iteration 56, loss = 0.02230247
Iteration 57, loss = 0.02199153
Iteration 58, loss = 0.02174799
Iteration 59, loss = 0.02187101
Iteration 60, loss = 0.02174235
Iteration 61, loss = 0.02170366
Iteration 62, loss = 0.02150945
Iteration 63, loss = 0.02156964
Iteration 64, loss = 0.02135127
Iteration 65, loss = 0.02129996
Iteration 66, loss = 0.02122720
Iteration 67, loss = 0.02108363
Iteration 68, loss = 0.02078486
Iteration 69, loss = 0.02086851
Iteration 70, loss = 0.02080966
Iteration 71, loss = 0.02065264
Iteration 72, loss = 0.02090584
Iteration 73, loss = 0.02078779
Iteration 74, loss = 0.02071412
Iteration 75, loss = 0.02091291
Iteration 76, loss = 0.02071620
Iteration 77, loss = 0.02048908
Iteration 78, loss = 0.02052086
Iteration 79, loss = 0.02042643
Iteration 80, loss = 0.02063101
Iteration 81, loss = 0.02062056
Iteration 82, loss = 0.02041248
Iteration 83, loss = 0.02029231
Iteration 84, loss = 0.02035936
Iteration 85, loss = 0.02031117
Iteration 86, loss = 0.02024251
Iteration 87, loss = 0.02023976
Iteration 88, loss = 0.02009748
Iteration 89, loss = 0.02019766
Iteration 90, loss = 0.01988822
Iteration 91, loss = 0.02015694
Iteration 92, loss = 0.02005385
Iteration 93, loss = 0.02000828
Iteration 94, loss = 0.01982750
Iteration 95, loss = 0.02006681
Iteration 96, loss = 0.02016916
Iteration 97, loss = 0.01994116
Iteration 98, loss = 0.01964064
Iteration 99, loss = 0.01986476
Iteration 100, loss = 0.01973688
Iteration 101, loss = 0.01956440
Iteration 102, loss = 0.01992780
Iteration 103, loss = 0.01962664
Iteration 104, loss = 0.01966451
Iteration 105, loss = 0.01957872
Iteration 106, loss = 0.01963906
Iteration 107, loss = 0.01969865
Iteration 108, loss = 0.01962882
Iteration 109, loss = 0.01961871
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69413213
Iteration 2, loss = 0.66940880
Iteration 3, loss = 0.64586552
Iteration 4, loss = 0.61879887
Iteration 5, loss = 0.58825228
Iteration 6, loss = 0.55471993
Iteration 7, loss = 0.51913306
Iteration 8, loss = 0.48250431
Iteration 9, loss = 0.44579704
Iteration 10, loss = 0.40944428
Iteration 11, loss = 0.37447464
Iteration 12, loss = 0.34103328
Iteration 13, loss = 0.30973821
Iteration 14, loss = 0.28090138
Iteration 15, loss = 0.25444155
Iteration 16, loss = 0.23055647
Iteration 17, loss = 0.20905156
Iteration 18, loss = 0.18985089
Iteration 19, loss = 0.17273233
Iteration 20, loss = 0.15738788
Iteration 21, loss = 0.14391399
Iteration 22, loss = 0.13195068
Iteration 23, loss = 0.12129697
Iteration 24, loss = 0.11189159
Iteration 25, loss = 0.10345673
Iteration 26, loss = 0.09606337
Iteration 27, loss = 0.08930724
Iteration 28, loss = 0.08336652
Iteration 29, loss = 0.07802410
Iteration 30, loss = 0.07320450
Iteration 31, loss = 0.06884669
Iteration 32, loss = 0.06497673
Iteration 33, loss = 0.06146611
Iteration 34, loss = 0.05820851
Iteration 35, loss = 0.05526126
Iteration 36, loss = 0.05261631
Iteration 37, loss = 0.05014451
Iteration 38, loss = 0.04786583
Iteration 39, loss = 0.04576462
Iteration 40, loss = 0.04385179
Iteration 41, loss = 0.04210119
Iteration 42, loss = 0.04046490
Iteration 43, loss = 0.03893228
Iteration 44, loss = 0.03760103
Iteration 45, loss = 0.03628311
Iteration 46, loss = 0.03503616
Iteration 47, loss = 0.03393785
Iteration 48, loss = 0.03279473
Iteration 49, loss = 0.03183154
Iteration 50, loss = 0.03091162
Iteration 51, loss = 0.03000400
Iteration 52, loss = 0.02916666
Iteration 53, loss = 0.02842218
Iteration 54, loss = 0.02771730
Iteration 55, loss = 0.02700856
Iteration 56, loss = 0.02636510
Iteration 57, loss = 0.02570450
Iteration 58, loss = 0.02513450
Iteration 59, loss = 0.02464059
Iteration 60, loss = 0.02405841
Iteration 61, loss = 0.02355831
Iteration 62, loss = 0.02315819
Iteration 63, loss = 0.02265832
Iteration 64, loss = 0.02230288
Iteration 65, loss = 0.02183245
Iteration 66, loss = 0.02142342
Iteration 67, loss = 0.02112559
Iteration 68, loss = 0.02073036
Iteration 69, loss = 0.02043926
Iteration 70, loss = 0.02009170
Iteration 71, loss = 0.01973760
Iteration 72, loss = 0.01946772
Iteration 73, loss = 0.01921037
Iteration 74, loss = 0.01898231
Iteration 75, loss = 0.01875272
Iteration 76, loss = 0.01843315
Iteration 77, loss = 0.01818453
Iteration 78, loss = 0.01798519
Iteration 79, loss = 0.01779994
Iteration 80, loss = 0.01752674
Iteration 81, loss = 0.01739849
Iteration 82, loss = 0.01716067
Iteration 83, loss = 0.01697011
Iteration 84, loss = 0.01676829
Iteration 85, loss = 0.01669599
Iteration 86, loss = 0.01643009
Iteration 87, loss = 0.01629696
Iteration 88, loss = 0.01616517
Iteration 89, loss = 0.01595182
Iteration 90, loss = 0.01579825
Iteration 91, loss = 0.01571717
Iteration 92, loss = 0.01552179
Iteration 93, loss = 0.01539313
Iteration 94, loss = 0.01528924
Iteration 95, loss = 0.01518780
Iteration 96, loss = 0.01507176
Iteration 97, loss = 0.01494478
Iteration 98, loss = 0.01488809
Iteration 99, loss = 0.01469706
Iteration 100, loss = 0.01466998
Iteration 101, loss = 0.01449460
Iteration 102, loss = 0.01437747
Iteration 103, loss = 0.01428650
Iteration 104, loss = 0.01419470
Iteration 105, loss = 0.01412468
Iteration 106, loss = 0.01404092
Iteration 107, loss = 0.01391643
Iteration 108, loss = 0.01388242
Iteration 109, loss = 0.01376858
Iteration 110, loss = 0.01372188
Iteration 111, loss = 0.01368014
Iteration 112, loss = 0.01358932
Iteration 113, loss = 0.01346906
Iteration 114, loss = 0.01343309
Iteration 115, loss = 0.01336043
Iteration 116, loss = 0.01328747
Iteration 117, loss = 0.01320902
Iteration 118, loss = 0.01314393
Iteration 119, loss = 0.01313014
Iteration 120, loss = 0.01303375
Iteration 121, loss = 0.01302109
Iteration 122, loss = 0.01291918
Iteration 123, loss = 0.01297636
Iteration 124, loss = 0.01279418
Iteration 125, loss = 0.01274470
Iteration 126, loss = 0.01272791
Iteration 127, loss = 0.01270239
Iteration 128, loss = 0.01264695
Iteration 129, loss = 0.01264226
Iteration 130, loss = 0.01254536
Iteration 131, loss = 0.01249202
Iteration 132, loss = 0.01250732
Iteration 133, loss = 0.01241963
Iteration 134, loss = 0.01238796
Iteration 135, loss = 0.01234665
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73897538
Iteration 2, loss = 0.71789380
Iteration 3, loss = 0.70021286
Iteration 4, loss = 0.68380683
Iteration 5, loss = 0.66704373
Iteration 6, loss = 0.64950736
Iteration 7, loss = 0.63060061
Iteration 8, loss = 0.61120776
Iteration 9, loss = 0.59037535
Iteration 10, loss = 0.56870252
Iteration 11, loss = 0.54610789
Iteration 12, loss = 0.52277795
Iteration 13, loss = 0.49884704
Iteration 14, loss = 0.47479115
Iteration 15, loss = 0.45075241
Iteration 16, loss = 0.42668963
Iteration 17, loss = 0.40293691
Iteration 18, loss = 0.37968709
Iteration 19, loss = 0.35719568
Iteration 20, loss = 0.33552225
Iteration 21, loss = 0.31476366
Iteration 22, loss = 0.29500398
Iteration 23, loss = 0.27641585
Iteration 24, loss = 0.25886406
Iteration 25, loss = 0.24236978
Iteration 26, loss = 0.22693997
Iteration 27, loss = 0.21255358
Iteration 28, loss = 0.19929057
Iteration 29, loss = 0.18680287
Iteration 30, loss = 0.17531021
Iteration 31, loss = 0.16470990
Iteration 32, loss = 0.15480411
Iteration 33, loss = 0.14573889
Iteration 34, loss = 0.13731329
Iteration 35, loss = 0.12948784
Iteration 36, loss = 0.12233284
Iteration 37, loss = 0.11569851
Iteration 38, loss = 0.10954891
Iteration 39, loss = 0.10389193
Iteration 40, loss = 0.09862049
Iteration 41, loss = 0.09375723
Iteration 42, loss = 0.08925231
Iteration 43, loss = 0.08512975
Iteration 44, loss = 0.08118299
Iteration 45, loss = 0.07758714
Iteration 46, loss = 0.07428379
Iteration 47, loss = 0.07111626
Iteration 48, loss = 0.06819396
Iteration 49, loss = 0.06542648
Iteration 50, loss = 0.06287429
Iteration 51, loss = 0.06049264
Iteration 52, loss = 0.05822730
Iteration 53, loss = 0.05614219
Iteration 54, loss = 0.05409930
Iteration 55, loss = 0.05222253
Iteration 56, loss = 0.05049234
Iteration 57, loss = 0.04880909
Iteration 58, loss = 0.04725395
Iteration 59, loss = 0.04575288
Iteration 60, loss = 0.04432701
Iteration 61, loss = 0.04299510
Iteration 62, loss = 0.04174655
Iteration 63, loss = 0.04056617
Iteration 64, loss = 0.03943803
Iteration 65, loss = 0.03840983
Iteration 66, loss = 0.03734089
Iteration 67, loss = 0.03638161
Iteration 68, loss = 0.03542180
Iteration 69, loss = 0.03458233
Iteration 70, loss = 0.03370674
Iteration 71, loss = 0.03289953
Iteration 72, loss = 0.03220062
Iteration 73, loss = 0.03142957
Iteration 74, loss = 0.03074314
Iteration 75, loss = 0.03005618
Iteration 76, loss = 0.02938031
Iteration 77, loss = 0.02876475
Iteration 78, loss = 0.02815402
Iteration 79, loss = 0.02759719
Iteration 80, loss = 0.02707110
Iteration 81, loss = 0.02653651
Iteration 82, loss = 0.02606670
Iteration 83, loss = 0.02556041
Iteration 84, loss = 0.02506857
Iteration 85, loss = 0.02467713
Iteration 86, loss = 0.02423568
Iteration 87, loss = 0.02382498
Iteration 88, loss = 0.02340425
Iteration 89, loss = 0.02304176
Iteration 90, loss = 0.02266235
Iteration 91, loss = 0.02229563
Iteration 92, loss = 0.02193964
Iteration 93, loss = 0.02160993
Iteration 94, loss = 0.02128536
Iteration 95, loss = 0.02095516
Iteration 96, loss = 0.02064090
Iteration 97, loss = 0.02034903
Iteration 98, loss = 0.02005262
Iteration 99, loss = 0.01980925
Iteration 100, loss = 0.01953581
Iteration 101, loss = 0.01927172
Iteration 102, loss = 0.01904554
Iteration 103, loss = 0.01883859
Iteration 104, loss = 0.01859293
Iteration 105, loss = 0.01837941
Iteration 106, loss = 0.01816806
Iteration 107, loss = 0.01796221
Iteration 108, loss = 0.01773106
Iteration 109, loss = 0.01750904
Iteration 110, loss = 0.01728649
Iteration 111, loss = 0.01708631
Iteration 112, loss = 0.01686737
Iteration 113, loss = 0.01668227
Iteration 114, loss = 0.01650911
Iteration 115, loss = 0.01629263
Iteration 116, loss = 0.01614000
Iteration 117, loss = 0.01597523
Iteration 118, loss = 0.01580451
Iteration 119, loss = 0.01567116
Iteration 120, loss = 0.01550152
Iteration 121, loss = 0.01535085
Iteration 122, loss = 0.01520066
Iteration 123, loss = 0.01507056
Iteration 124, loss = 0.01494711
Iteration 125, loss = 0.01482468
Iteration 126, loss = 0.01470407
Iteration 127, loss = 0.01453736
Iteration 128, loss = 0.01440693
Iteration 129, loss = 0.01428192
Iteration 130, loss = 0.01416359
Iteration 131, loss = 0.01403549
Iteration 132, loss = 0.01390332
Iteration 133, loss = 0.01384234
Iteration 134, loss = 0.01370258
Iteration 135, loss = 0.01361715
Iteration 136, loss = 0.01352307
Iteration 137, loss = 0.01343078
Iteration 138, loss = 0.01334773
Iteration 139, loss = 0.01325635
Iteration 140, loss = 0.01314861
Iteration 141, loss = 0.01306796
Iteration 142, loss = 0.01298873
Iteration 143, loss = 0.01289266
Iteration 144, loss = 0.01278876
Iteration 145, loss = 0.01269173
Iteration 146, loss = 0.01262904
Iteration 147, loss = 0.01253207
Iteration 148, loss = 0.01247544
Iteration 149, loss = 0.01238148
Iteration 150, loss = 0.01229154
Iteration 151, loss = 0.01223378
Iteration 152, loss = 0.01216048
Iteration 153, loss = 0.01213973
Iteration 154, loss = 0.01203585
Iteration 155, loss = 0.01195438
Iteration 156, loss = 0.01188272
Iteration 157, loss = 0.01181958
Iteration 158, loss = 0.01174291
Iteration 159, loss = 0.01168645
Iteration 160, loss = 0.01165087
Iteration 161, loss = 0.01158204
Iteration 162, loss = 0.01151893
Iteration 163, loss = 0.01147377
Iteration 164, loss = 0.01138858
Iteration 165, loss = 0.01134926
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67695386
Iteration 2, loss = 0.63402371
Iteration 3, loss = 0.56686092
Iteration 4, loss = 0.48536356
Iteration 5, loss = 0.40454351
Iteration 6, loss = 0.33083636
Iteration 7, loss = 0.26808619
Iteration 8, loss = 0.21722814
Iteration 9, loss = 0.17712034
Iteration 10, loss = 0.14636135
Iteration 11, loss = 0.12264369
Iteration 12, loss = 0.10473609
Iteration 13, loss = 0.09071209
Iteration 14, loss = 0.07991645
Iteration 15, loss = 0.07137472
Iteration 16, loss = 0.06440405
Iteration 17, loss = 0.05879558
Iteration 18, loss = 0.05402408
Iteration 19, loss = 0.05002410
Iteration 20, loss = 0.04670749
Iteration 21, loss = 0.04404200
Iteration 22, loss = 0.04147845
Iteration 23, loss = 0.03947275
Iteration 24, loss = 0.03774969
Iteration 25, loss = 0.03662671
Iteration 26, loss = 0.03481268
Iteration 27, loss = 0.03356632
Iteration 28, loss = 0.03257641
Iteration 29, loss = 0.03156763
Iteration 30, loss = 0.03060951
Iteration 31, loss = 0.02994437
Iteration 32, loss = 0.02940581
Iteration 33, loss = 0.02878722
Iteration 34, loss = 0.02811558
Iteration 35, loss = 0.02751354
Iteration 36, loss = 0.02713173
Iteration 37, loss = 0.02684457
Iteration 38, loss = 0.02618101
Iteration 39, loss = 0.02596167
Iteration 40, loss = 0.02561283
Iteration 41, loss = 0.02525308
Iteration 42, loss = 0.02490569
Iteration 43, loss = 0.02466644
Iteration 44, loss = 0.02433097
Iteration 45, loss = 0.02430349
Iteration 46, loss = 0.02392612
Iteration 47, loss = 0.02381405
Iteration 48, loss = 0.02383039
Iteration 49, loss = 0.02354915
Iteration 50, loss = 0.02370617
Iteration 51, loss = 0.02308073
Iteration 52, loss = 0.02294088
Iteration 53, loss = 0.02284453
Iteration 54, loss = 0.02277530
Iteration 55, loss = 0.02294606
Iteration 56, loss = 0.02264073
Iteration 57, loss = 0.02288414
Iteration 58, loss = 0.02249048
Iteration 59, loss = 0.02233951
Iteration 60, loss = 0.02225178
Iteration 61, loss = 0.02192453
Iteration 62, loss = 0.02231547
Iteration 63, loss = 0.02184395
Iteration 64, loss = 0.02189485
Iteration 65, loss = 0.02172968
Iteration 66, loss = 0.02179139
Iteration 67, loss = 0.02146240
Iteration 68, loss = 0.02168096
Iteration 69, loss = 0.02147336
Iteration 70, loss = 0.02145646
Iteration 71, loss = 0.02129067
Iteration 72, loss = 0.02124262
Iteration 73, loss = 0.02144851
Iteration 74, loss = 0.02132754
Iteration 75, loss = 0.02135753
Iteration 76, loss = 0.02122424
Iteration 77, loss = 0.02151003
Iteration 78, loss = 0.02144400
Iteration 79, loss = 0.02110903
Iteration 80, loss = 0.02124616
Iteration 81, loss = 0.02114394
Iteration 82, loss = 0.02104347
Iteration 83, loss = 0.02112703
Iteration 84, loss = 0.02094412
Iteration 85, loss = 0.02090298
Iteration 86, loss = 0.02122831
Iteration 87, loss = 0.02093368
Iteration 88, loss = 0.02092428
Iteration 89, loss = 0.02073637
Iteration 90, loss = 0.02070674
Iteration 91, loss = 0.02097935
Iteration 92, loss = 0.02067178
Iteration 93, loss = 0.02061605
Iteration 94, loss = 0.02054324
Iteration 95, loss = 0.02054951
Iteration 96, loss = 0.02066623
Iteration 97, loss = 0.02048873
Iteration 98, loss = 0.02074263
Iteration 99, loss = 0.02043896
Iteration 100, loss = 0.02058456
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71262781
Iteration 2, loss = 0.66969621
Iteration 3, loss = 0.62030755
Iteration 4, loss = 0.56359098
Iteration 5, loss = 0.50117864
Iteration 6, loss = 0.43818019
Iteration 7, loss = 0.37792073
Iteration 8, loss = 0.32286757
Iteration 9, loss = 0.27409541
Iteration 10, loss = 0.23239901
Iteration 11, loss = 0.19726664
Iteration 12, loss = 0.16812602
Iteration 13, loss = 0.14427947
Iteration 14, loss = 0.12475630
Iteration 15, loss = 0.10878990
Iteration 16, loss = 0.09560530
Iteration 17, loss = 0.08478984
Iteration 18, loss = 0.07585236
Iteration 19, loss = 0.06827913
Iteration 20, loss = 0.06194094
Iteration 21, loss = 0.05656173
Iteration 22, loss = 0.05192758
Iteration 23, loss = 0.04788700
Iteration 24, loss = 0.04442845
Iteration 25, loss = 0.04141518
Iteration 26, loss = 0.03874767
Iteration 27, loss = 0.03645186
Iteration 28, loss = 0.03443142
Iteration 29, loss = 0.03257003
Iteration 30, loss = 0.03088576
Iteration 31, loss = 0.02951151
Iteration 32, loss = 0.02818654
Iteration 33, loss = 0.02700696
Iteration 34, loss = 0.02586775
Iteration 35, loss = 0.02492357
Iteration 36, loss = 0.02391674
Iteration 37, loss = 0.02313807
Iteration 38, loss = 0.02236377
Iteration 39, loss = 0.02175472
Iteration 40, loss = 0.02110555
Iteration 41, loss = 0.02056925
Iteration 42, loss = 0.02000810
Iteration 43, loss = 0.01944718
Iteration 44, loss = 0.01900413
Iteration 45, loss = 0.01870940
Iteration 46, loss = 0.01813322
Iteration 47, loss = 0.01782515
Iteration 48, loss = 0.01742303
Iteration 49, loss = 0.01716816
Iteration 50, loss = 0.01684089
Iteration 51, loss = 0.01650417
Iteration 52, loss = 0.01631942
Iteration 53, loss = 0.01599329
Iteration 54, loss = 0.01587489
Iteration 55, loss = 0.01554977
Iteration 56, loss = 0.01539247
Iteration 57, loss = 0.01508545
Iteration 58, loss = 0.01490822
Iteration 59, loss = 0.01481485
Iteration 60, loss = 0.01470563
Iteration 61, loss = 0.01451039
Iteration 62, loss = 0.01423321
Iteration 63, loss = 0.01421975
Iteration 64, loss = 0.01406035
Iteration 65, loss = 0.01393429
Iteration 66, loss = 0.01380204
Iteration 67, loss = 0.01360438
Iteration 68, loss = 0.01359188
Iteration 69, loss = 0.01348713
Iteration 70, loss = 0.01329021
Iteration 71, loss = 0.01335102
Iteration 72, loss = 0.01326987
Iteration 73, loss = 0.01310637
Iteration 74, loss = 0.01297029
Iteration 75, loss = 0.01288490
Iteration 76, loss = 0.01282926
Iteration 77, loss = 0.01270970
Iteration 78, loss = 0.01274311
Iteration 79, loss = 0.01255331
Iteration 80, loss = 0.01255190
Iteration 81, loss = 0.01238274
Iteration 82, loss = 0.01236547
Iteration 83, loss = 0.01230189
Iteration 84, loss = 0.01231870
Iteration 85, loss = 0.01218767
Iteration 86, loss = 0.01221538
Iteration 87, loss = 0.01218059
Iteration 88, loss = 0.01202721
Iteration 89, loss = 0.01197597
Iteration 90, loss = 0.01197842
Iteration 91, loss = 0.01200592
Iteration 92, loss = 0.01183070
Iteration 93, loss = 0.01182636
Iteration 94, loss = 0.01187082
Iteration 95, loss = 0.01171346
Iteration 96, loss = 0.01180033
Iteration 97, loss = 0.01172561
Iteration 98, loss = 0.01166397
Iteration 99, loss = 0.01166484
Iteration 100, loss = 0.01163238
Iteration 101, loss = 0.01154250
Iteration 102, loss = 0.01147375
Iteration 103, loss = 0.01143546
Iteration 104, loss = 0.01152051
Iteration 105, loss = 0.01150059
Iteration 106, loss = 0.01140706
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68704131
Iteration 2, loss = 0.65788046
Iteration 3, loss = 0.61849104
Iteration 4, loss = 0.56568637
Iteration 5, loss = 0.50176152
Iteration 6, loss = 0.43683676
Iteration 7, loss = 0.37555570
Iteration 8, loss = 0.32014095
Iteration 9, loss = 0.27198572
Iteration 10, loss = 0.23062792
Iteration 11, loss = 0.19608219
Iteration 12, loss = 0.16761574
Iteration 13, loss = 0.14405990
Iteration 14, loss = 0.12483456
Iteration 15, loss = 0.10909441
Iteration 16, loss = 0.09615679
Iteration 17, loss = 0.08544864
Iteration 18, loss = 0.07646485
Iteration 19, loss = 0.06894143
Iteration 20, loss = 0.06265572
Iteration 21, loss = 0.05714460
Iteration 22, loss = 0.05258264
Iteration 23, loss = 0.04857230
Iteration 24, loss = 0.04503478
Iteration 25, loss = 0.04200203
Iteration 26, loss = 0.03930915
Iteration 27, loss = 0.03702577
Iteration 28, loss = 0.03481904
Iteration 29, loss = 0.03305583
Iteration 30, loss = 0.03132817
Iteration 31, loss = 0.02986003
Iteration 32, loss = 0.02844027
Iteration 33, loss = 0.02735649
Iteration 34, loss = 0.02627441
Iteration 35, loss = 0.02521239
Iteration 36, loss = 0.02423428
Iteration 37, loss = 0.02341190
Iteration 38, loss = 0.02270911
Iteration 39, loss = 0.02203135
Iteration 40, loss = 0.02125567
Iteration 41, loss = 0.02072321
Iteration 42, loss = 0.02015638
Iteration 43, loss = 0.01951004
Iteration 44, loss = 0.01926470
Iteration 45, loss = 0.01867981
Iteration 46, loss = 0.01839168
Iteration 47, loss = 0.01792357
Iteration 48, loss = 0.01749770
Iteration 49, loss = 0.01725860
Iteration 50, loss = 0.01688241
Iteration 51, loss = 0.01663196
Iteration 52, loss = 0.01635507
Iteration 53, loss = 0.01616675
Iteration 54, loss = 0.01591721
Iteration 55, loss = 0.01563385
Iteration 56, loss = 0.01542795
Iteration 57, loss = 0.01516812
Iteration 58, loss = 0.01496860
Iteration 59, loss = 0.01473946
Iteration 60, loss = 0.01469400
Iteration 61, loss = 0.01448311
Iteration 62, loss = 0.01431307
Iteration 63, loss = 0.01412708
Iteration 64, loss = 0.01397231
Iteration 65, loss = 0.01385232
Iteration 66, loss = 0.01372571
Iteration 67, loss = 0.01361157
Iteration 68, loss = 0.01347345
Iteration 69, loss = 0.01344470
Iteration 70, loss = 0.01323271
Iteration 71, loss = 0.01314300
Iteration 72, loss = 0.01305736
Iteration 73, loss = 0.01299407
Iteration 74, loss = 0.01286132
Iteration 75, loss = 0.01280928
Iteration 76, loss = 0.01278420
Iteration 77, loss = 0.01264967
Iteration 78, loss = 0.01264551
Iteration 79, loss = 0.01248280
Iteration 80, loss = 0.01243190
Iteration 81, loss = 0.01235806
Iteration 82, loss = 0.01231243
Iteration 83, loss = 0.01235076
Iteration 84, loss = 0.01241657
Iteration 85, loss = 0.01213660
Iteration 86, loss = 0.01200753
Iteration 87, loss = 0.01200081
Iteration 88, loss = 0.01196734
Iteration 89, loss = 0.01194795
Iteration 90, loss = 0.01199746
Iteration 91, loss = 0.01183608
Iteration 92, loss = 0.01178980
Iteration 93, loss = 0.01171532
Iteration 94, loss = 0.01163272
Iteration 95, loss = 0.01154891
Iteration 96, loss = 0.01156270
Iteration 97, loss = 0.01160245
Iteration 98, loss = 0.01154531
Iteration 99, loss = 0.01148929
Iteration 100, loss = 0.01144140
Iteration 101, loss = 0.01147101
Iteration 102, loss = 0.01135010
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68340189
Iteration 2, loss = 0.65593400
Iteration 3, loss = 0.61969145
Iteration 4, loss = 0.56741780
Iteration 5, loss = 0.50474587
Iteration 6, loss = 0.44063615
Iteration 7, loss = 0.37969342
Iteration 8, loss = 0.32409967
Iteration 9, loss = 0.27498151
Iteration 10, loss = 0.23290411
Iteration 11, loss = 0.19788621
Iteration 12, loss = 0.16904688
Iteration 13, loss = 0.14540132
Iteration 14, loss = 0.12610289
Iteration 15, loss = 0.11050047
Iteration 16, loss = 0.09758831
Iteration 17, loss = 0.08695621
Iteration 18, loss = 0.07808972
Iteration 19, loss = 0.07067844
Iteration 20, loss = 0.06449039
Iteration 21, loss = 0.05921351
Iteration 22, loss = 0.05462689
Iteration 23, loss = 0.05077459
Iteration 24, loss = 0.04736967
Iteration 25, loss = 0.04448767
Iteration 26, loss = 0.04178962
Iteration 27, loss = 0.03951246
Iteration 28, loss = 0.03758953
Iteration 29, loss = 0.03578288
Iteration 30, loss = 0.03402264
Iteration 31, loss = 0.03260850
Iteration 32, loss = 0.03134619
Iteration 33, loss = 0.03015387
Iteration 34, loss = 0.02909526
Iteration 35, loss = 0.02810716
Iteration 36, loss = 0.02731629
Iteration 37, loss = 0.02650033
Iteration 38, loss = 0.02581034
Iteration 39, loss = 0.02494546
Iteration 40, loss = 0.02440668
Iteration 41, loss = 0.02382724
Iteration 42, loss = 0.02324765
Iteration 43, loss = 0.02279011
Iteration 44, loss = 0.02229568
Iteration 45, loss = 0.02191781
Iteration 46, loss = 0.02155200
Iteration 47, loss = 0.02117999
Iteration 48, loss = 0.02090371
Iteration 49, loss = 0.02063749
Iteration 50, loss = 0.02019623
Iteration 51, loss = 0.01999775
Iteration 52, loss = 0.01969263
Iteration 53, loss = 0.01949247
Iteration 54, loss = 0.01913940
Iteration 55, loss = 0.01893775
Iteration 56, loss = 0.01875622
Iteration 57, loss = 0.01857623
Iteration 58, loss = 0.01834886
Iteration 59, loss = 0.01810339
Iteration 60, loss = 0.01814346
Iteration 61, loss = 0.01796897
Iteration 62, loss = 0.01765759
Iteration 63, loss = 0.01775644
Iteration 64, loss = 0.01751367
Iteration 65, loss = 0.01742261
Iteration 66, loss = 0.01726833
Iteration 67, loss = 0.01716076
Iteration 68, loss = 0.01692907
Iteration 69, loss = 0.01695230
Iteration 70, loss = 0.01676540
Iteration 71, loss = 0.01674567
Iteration 72, loss = 0.01648521
Iteration 73, loss = 0.01654947
Iteration 74, loss = 0.01637332
Iteration 75, loss = 0.01648015
Iteration 76, loss = 0.01624009
Iteration 77, loss = 0.01622517
Iteration 78, loss = 0.01606175
Iteration 79, loss = 0.01621336
Iteration 80, loss = 0.01585530
Iteration 81, loss = 0.01594107
Iteration 82, loss = 0.01578201
Iteration 83, loss = 0.01576526
Iteration 84, loss = 0.01575444
Iteration 85, loss = 0.01577666
Iteration 86, loss = 0.01561536
Iteration 87, loss = 0.01552930
Iteration 88, loss = 0.01552677
Iteration 89, loss = 0.01548739
Iteration 90, loss = 0.01534803
Iteration 91, loss = 0.01539903
Iteration 92, loss = 0.01541485
Iteration 93, loss = 0.01530880
Iteration 94, loss = 0.01517215
Iteration 95, loss = 0.01532865
Iteration 96, loss = 0.01506819
Iteration 97, loss = 0.01540923
Iteration 98, loss = 0.01510708
Iteration 99, loss = 0.01523219
Iteration 100, loss = 0.01513553
Iteration 101, loss = 0.01498605
Iteration 102, loss = 0.01507772
Iteration 103, loss = 0.01502864
Iteration 104, loss = 0.01492167
Iteration 105, loss = 0.01488858
Iteration 106, loss = 0.01482229
Iteration 107, loss = 0.01477817
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72025109
Iteration 2, loss = 0.67602409
Iteration 3, loss = 0.61991041
Iteration 4, loss = 0.55292286
Iteration 5, loss = 0.47842988
Iteration 6, loss = 0.40352725
Iteration 7, loss = 0.33552598
Iteration 8, loss = 0.27701020
Iteration 9, loss = 0.22858293
Iteration 10, loss = 0.18954080
Iteration 11, loss = 0.15845039
Iteration 12, loss = 0.13410499
Iteration 13, loss = 0.11481097
Iteration 14, loss = 0.09945340
Iteration 15, loss = 0.08726121
Iteration 16, loss = 0.07729699
Iteration 17, loss = 0.06906896
Iteration 18, loss = 0.06230767
Iteration 19, loss = 0.05687538
Iteration 20, loss = 0.05206698
Iteration 21, loss = 0.04822198
Iteration 22, loss = 0.04478589
Iteration 23, loss = 0.04167401
Iteration 24, loss = 0.03915379
Iteration 25, loss = 0.03696053
Iteration 26, loss = 0.03505543
Iteration 27, loss = 0.03329814
Iteration 28, loss = 0.03196400
Iteration 29, loss = 0.03045914
Iteration 30, loss = 0.02931129
Iteration 31, loss = 0.02827727
Iteration 32, loss = 0.02718493
Iteration 33, loss = 0.02632339
Iteration 34, loss = 0.02553495
Iteration 35, loss = 0.02476510
Iteration 36, loss = 0.02404686
Iteration 37, loss = 0.02355843
Iteration 38, loss = 0.02305351
Iteration 39, loss = 0.02247165
Iteration 40, loss = 0.02212235
Iteration 41, loss = 0.02158920
Iteration 42, loss = 0.02119988
Iteration 43, loss = 0.02088505
Iteration 44, loss = 0.02058857
Iteration 45, loss = 0.02029779
Iteration 46, loss = 0.01977618
Iteration 47, loss = 0.01957547
Iteration 48, loss = 0.01933788
Iteration 49, loss = 0.01900917
Iteration 50, loss = 0.01897844
Iteration 51, loss = 0.01864018
Iteration 52, loss = 0.01846092
Iteration 53, loss = 0.01846398
Iteration 54, loss = 0.01820178
Iteration 55, loss = 0.01789413
Iteration 56, loss = 0.01791040
Iteration 57, loss = 0.01766616
Iteration 58, loss = 0.01745956
Iteration 59, loss = 0.01731629
Iteration 60, loss = 0.01730676
Iteration 61, loss = 0.01709110
Iteration 62, loss = 0.01693053
Iteration 63, loss = 0.01692507
Iteration 64, loss = 0.01686544
Iteration 65, loss = 0.01672478
Iteration 66, loss = 0.01671980
Iteration 67, loss = 0.01644993
Iteration 68, loss = 0.01641721
Iteration 69, loss = 0.01633944
Iteration 70, loss = 0.01626193
Iteration 71, loss = 0.01632011
Iteration 72, loss = 0.01635489
Iteration 73, loss = 0.01621090
Iteration 74, loss = 0.01599914
Iteration 75, loss = 0.01594725
Iteration 76, loss = 0.01608705
Iteration 77, loss = 0.01584116
Iteration 78, loss = 0.01576554
Iteration 79, loss = 0.01584929
Iteration 80, loss = 0.01585045
Iteration 81, loss = 0.01572425
Iteration 82, loss = 0.01557752
Iteration 83, loss = 0.01553992
Iteration 84, loss = 0.01549251
Iteration 85, loss = 0.01555420
Iteration 86, loss = 0.01541256
Iteration 87, loss = 0.01542036
Iteration 88, loss = 0.01532981
Iteration 89, loss = 0.01530479
Iteration 90, loss = 0.01540226
Iteration 91, loss = 0.01525321
Iteration 92, loss = 0.01511671
Iteration 93, loss = 0.01531339
Iteration 94, loss = 0.01520596
Iteration 95, loss = 0.01532019
Iteration 96, loss = 0.01515623
Iteration 97, loss = 0.01503364
Iteration 98, loss = 0.01512410
Iteration 99, loss = 0.01502316
Iteration 100, loss = 0.01489309
Iteration 101, loss = 0.01504786
Iteration 102, loss = 0.01496354
Iteration 103, loss = 0.01495429
Iteration 104, loss = 0.01488299
Iteration 105, loss = 0.01495611
Iteration 106, loss = 0.01503590
Iteration 107, loss = 0.01488880
Iteration 108, loss = 0.01484623
Iteration 109, loss = 0.01493698
Iteration 110, loss = 0.01475873
Iteration 111, loss = 0.01470819
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67831913
Iteration 2, loss = 0.62847173
Iteration 3, loss = 0.55325993
Iteration 4, loss = 0.46127956
Iteration 5, loss = 0.37144176
Iteration 6, loss = 0.29406034
Iteration 7, loss = 0.23152670
Iteration 8, loss = 0.18409469
Iteration 9, loss = 0.14887408
Iteration 10, loss = 0.12274557
Iteration 11, loss = 0.10330459
Iteration 12, loss = 0.08866831
Iteration 13, loss = 0.07747199
Iteration 14, loss = 0.06873620
Iteration 15, loss = 0.06173530
Iteration 16, loss = 0.05620363
Iteration 17, loss = 0.05155977
Iteration 18, loss = 0.04766777
Iteration 19, loss = 0.04473449
Iteration 20, loss = 0.04196195
Iteration 21, loss = 0.03973243
Iteration 22, loss = 0.03778918
Iteration 23, loss = 0.03611072
Iteration 24, loss = 0.03465044
Iteration 25, loss = 0.03342846
Iteration 26, loss = 0.03241369
Iteration 27, loss = 0.03108670
Iteration 28, loss = 0.03034229
Iteration 29, loss = 0.02985693
Iteration 30, loss = 0.02890694
Iteration 31, loss = 0.02827879
Iteration 32, loss = 0.02761955
Iteration 33, loss = 0.02710721
Iteration 34, loss = 0.02657740
Iteration 35, loss = 0.02606220
Iteration 36, loss = 0.02570942
Iteration 37, loss = 0.02541396
Iteration 38, loss = 0.02520824
Iteration 39, loss = 0.02482506
Iteration 40, loss = 0.02453712
Iteration 41, loss = 0.02441206
Iteration 42, loss = 0.02417378
Iteration 43, loss = 0.02390789
Iteration 44, loss = 0.02340616
Iteration 45, loss = 0.02345117
Iteration 46, loss = 0.02333820
Iteration 47, loss = 0.02301222
Iteration 48, loss = 0.02292121
Iteration 49, loss = 0.02266973
Iteration 50, loss = 0.02255530
Iteration 51, loss = 0.02254757
Iteration 52, loss = 0.02221950
Iteration 53, loss = 0.02218481
Iteration 54, loss = 0.02207625
Iteration 55, loss = 0.02211585
Iteration 56, loss = 0.02200564
Iteration 57, loss = 0.02200102
Iteration 58, loss = 0.02197310
Iteration 59, loss = 0.02156629
Iteration 60, loss = 0.02173772
Iteration 61, loss = 0.02144508
Iteration 62, loss = 0.02129753
Iteration 63, loss = 0.02130504
Iteration 64, loss = 0.02117585
Iteration 65, loss = 0.02109077
Iteration 66, loss = 0.02131385
Iteration 67, loss = 0.02102838
Iteration 68, loss = 0.02144258
Iteration 69, loss = 0.02113129
Iteration 70, loss = 0.02124529
Iteration 71, loss = 0.02098104
Iteration 72, loss = 0.02082754
Iteration 73, loss = 0.02079735
Iteration 74, loss = 0.02068513
Iteration 75, loss = 0.02090556
Iteration 76, loss = 0.02078076
Iteration 77, loss = 0.02053598
Iteration 78, loss = 0.02060425
Iteration 79, loss = 0.02075025
Iteration 80, loss = 0.02060247
Iteration 81, loss = 0.02035098
Iteration 82, loss = 0.02032107
Iteration 83, loss = 0.02056299
Iteration 84, loss = 0.02032604
Iteration 85, loss = 0.02041583
Iteration 86, loss = 0.02043598
Iteration 87, loss = 0.02039007
Iteration 88, loss = 0.02033703
Iteration 89, loss = 0.02035930
Iteration 90, loss = 0.02034291
Iteration 91, loss = 0.02043386
Iteration 92, loss = 0.02022516
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71430344
Iteration 2, loss = 0.66816014
Iteration 3, loss = 0.60536443
Iteration 4, loss = 0.52157745
Iteration 5, loss = 0.43136215
Iteration 6, loss = 0.34553691
Iteration 7, loss = 0.27389867
Iteration 8, loss = 0.21748727
Iteration 9, loss = 0.17479804
Iteration 10, loss = 0.14293122
Iteration 11, loss = 0.11936906
Iteration 12, loss = 0.10156652
Iteration 13, loss = 0.08785746
Iteration 14, loss = 0.07724620
Iteration 15, loss = 0.06892280
Iteration 16, loss = 0.06244422
Iteration 17, loss = 0.05693009
Iteration 18, loss = 0.05255605
Iteration 19, loss = 0.04915144
Iteration 20, loss = 0.04587636
Iteration 21, loss = 0.04344658
Iteration 22, loss = 0.04137858
Iteration 23, loss = 0.03922992
Iteration 24, loss = 0.03745273
Iteration 25, loss = 0.03618929
Iteration 26, loss = 0.03477492
Iteration 27, loss = 0.03368017
Iteration 28, loss = 0.03280355
Iteration 29, loss = 0.03196711
Iteration 30, loss = 0.03117819
Iteration 31, loss = 0.03056044
Iteration 32, loss = 0.02975225
Iteration 33, loss = 0.02915860
Iteration 34, loss = 0.02859812
Iteration 35, loss = 0.02844434
Iteration 36, loss = 0.02793668
Iteration 37, loss = 0.02746854
Iteration 38, loss = 0.02709959
Iteration 39, loss = 0.02686878
Iteration 40, loss = 0.02639602
Iteration 41, loss = 0.02631336
Iteration 42, loss = 0.02602624
Iteration 43, loss = 0.02581373
Iteration 44, loss = 0.02532470
Iteration 45, loss = 0.02507504
Iteration 46, loss = 0.02496085
Iteration 47, loss = 0.02504324
Iteration 48, loss = 0.02472382
Iteration 49, loss = 0.02438530
Iteration 50, loss = 0.02438347
Iteration 51, loss = 0.02424850
Iteration 52, loss = 0.02403232
Iteration 53, loss = 0.02391014
Iteration 54, loss = 0.02388516
Iteration 55, loss = 0.02406256
Iteration 56, loss = 0.02366439
Iteration 57, loss = 0.02367930
Iteration 58, loss = 0.02331189
Iteration 59, loss = 0.02327044
Iteration 60, loss = 0.02312222
Iteration 61, loss = 0.02326713
Iteration 62, loss = 0.02322610
Iteration 63, loss = 0.02295160
Iteration 64, loss = 0.02263490
Iteration 65, loss = 0.02281782
Iteration 66, loss = 0.02284231
Iteration 67, loss = 0.02261439
Iteration 68, loss = 0.02253207
Iteration 69, loss = 0.02235778
Iteration 70, loss = 0.02237793
Iteration 71, loss = 0.02256723
Iteration 72, loss = 0.02247118
Iteration 73, loss = 0.02219072
Iteration 74, loss = 0.02256307
Iteration 75, loss = 0.02197978
Iteration 76, loss = 0.02243471
Iteration 77, loss = 0.02214243
Iteration 78, loss = 0.02225388
Iteration 79, loss = 0.02219777
Iteration 80, loss = 0.02200337
Iteration 81, loss = 0.02193573
Iteration 82, loss = 0.02188830
Iteration 83, loss = 0.02207568
Iteration 84, loss = 0.02174670
Iteration 85, loss = 0.02195251
Iteration 86, loss = 0.02187229
Iteration 87, loss = 0.02206220
Iteration 88, loss = 0.02177459
Iteration 89, loss = 0.02176910
Iteration 90, loss = 0.02169895
Iteration 91, loss = 0.02169919
Iteration 92, loss = 0.02181654
Iteration 93, loss = 0.02174877
Iteration 94, loss = 0.02164191
Iteration 95, loss = 0.02174456
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68284773
Iteration 2, loss = 0.66169270
Iteration 3, loss = 0.63992817
Iteration 4, loss = 0.61450522
Iteration 5, loss = 0.58412626
Iteration 6, loss = 0.54854607
Iteration 7, loss = 0.50982709
Iteration 8, loss = 0.46973138
Iteration 9, loss = 0.43017230
Iteration 10, loss = 0.39174799
Iteration 11, loss = 0.35576781
Iteration 12, loss = 0.32199797
Iteration 13, loss = 0.29121704
Iteration 14, loss = 0.26306016
Iteration 15, loss = 0.23785252
Iteration 16, loss = 0.21498389
Iteration 17, loss = 0.19475387
Iteration 18, loss = 0.17677921
Iteration 19, loss = 0.16077388
Iteration 20, loss = 0.14673073
Iteration 21, loss = 0.13415607
Iteration 22, loss = 0.12308511
Iteration 23, loss = 0.11333558
Iteration 24, loss = 0.10467277
Iteration 25, loss = 0.09699010
Iteration 26, loss = 0.09018427
Iteration 27, loss = 0.08402903
Iteration 28, loss = 0.07859109
Iteration 29, loss = 0.07362726
Iteration 30, loss = 0.06924405
Iteration 31, loss = 0.06530151
Iteration 32, loss = 0.06174156
Iteration 33, loss = 0.05841614
Iteration 34, loss = 0.05553726
Iteration 35, loss = 0.05281911
Iteration 36, loss = 0.05033458
Iteration 37, loss = 0.04805821
Iteration 38, loss = 0.04599657
Iteration 39, loss = 0.04414498
Iteration 40, loss = 0.04236594
Iteration 41, loss = 0.04066776
Iteration 42, loss = 0.03920368
Iteration 43, loss = 0.03786099
Iteration 44, loss = 0.03647819
Iteration 45, loss = 0.03522473
Iteration 46, loss = 0.03408541
Iteration 47, loss = 0.03307588
Iteration 48, loss = 0.03212194
Iteration 49, loss = 0.03118571
Iteration 50, loss = 0.03032532
Iteration 51, loss = 0.02951134
Iteration 52, loss = 0.02868835
Iteration 53, loss = 0.02798595
Iteration 54, loss = 0.02727483
Iteration 55, loss = 0.02666845
Iteration 56, loss = 0.02604825
Iteration 57, loss = 0.02549378
Iteration 58, loss = 0.02500000
Iteration 59, loss = 0.02449403
Iteration 60, loss = 0.02391683
Iteration 61, loss = 0.02347901
Iteration 62, loss = 0.02303507
Iteration 63, loss = 0.02258471
Iteration 64, loss = 0.02220995
Iteration 65, loss = 0.02182346
Iteration 66, loss = 0.02146725
Iteration 67, loss = 0.02115586
Iteration 68, loss = 0.02088107
Iteration 69, loss = 0.02047231
Iteration 70, loss = 0.02017362
Iteration 71, loss = 0.01988642
Iteration 72, loss = 0.01966561
Iteration 73, loss = 0.01936099
Iteration 74, loss = 0.01908471
Iteration 75, loss = 0.01887772
Iteration 76, loss = 0.01860639
Iteration 77, loss = 0.01845263
Iteration 78, loss = 0.01823180
Iteration 79, loss = 0.01796326
Iteration 80, loss = 0.01775282
Iteration 81, loss = 0.01759943
Iteration 82, loss = 0.01741561
Iteration 83, loss = 0.01719120
Iteration 84, loss = 0.01701040
Iteration 85, loss = 0.01686495
Iteration 86, loss = 0.01672710
Iteration 87, loss = 0.01655434
Iteration 88, loss = 0.01643476
Iteration 89, loss = 0.01629713
Iteration 90, loss = 0.01616587
Iteration 91, loss = 0.01603004
Iteration 92, loss = 0.01586706
Iteration 93, loss = 0.01575140
Iteration 94, loss = 0.01560746
Iteration 95, loss = 0.01556056
Iteration 96, loss = 0.01543543
Iteration 97, loss = 0.01529641
Iteration 98, loss = 0.01515669
Iteration 99, loss = 0.01513114
Iteration 100, loss = 0.01497356
Iteration 101, loss = 0.01494259
Iteration 102, loss = 0.01485887
Iteration 103, loss = 0.01468652
Iteration 104, loss = 0.01461716
Iteration 105, loss = 0.01451989
Iteration 106, loss = 0.01452265
Iteration 107, loss = 0.01435263
Iteration 108, loss = 0.01427975
Iteration 109, loss = 0.01420463
Iteration 110, loss = 0.01419634
Iteration 111, loss = 0.01404818
Iteration 112, loss = 0.01401989
Iteration 113, loss = 0.01390844
Iteration 114, loss = 0.01390379
Iteration 115, loss = 0.01378509
Iteration 116, loss = 0.01379437
Iteration 117, loss = 0.01374272
Iteration 118, loss = 0.01365791
Iteration 119, loss = 0.01355367
Iteration 120, loss = 0.01346692
Iteration 121, loss = 0.01340953
Iteration 122, loss = 0.01350274
Iteration 123, loss = 0.01337671
Iteration 124, loss = 0.01327770
Iteration 125, loss = 0.01320067
Iteration 126, loss = 0.01320414
Iteration 127, loss = 0.01315841
Iteration 128, loss = 0.01316956
Iteration 129, loss = 0.01305255
Iteration 130, loss = 0.01303335
Iteration 131, loss = 0.01295204
Iteration 132, loss = 0.01292047
Iteration 133, loss = 0.01291602
Iteration 134, loss = 0.01282426
Iteration 135, loss = 0.01282501
Iteration 136, loss = 0.01281290
Iteration 137, loss = 0.01270645
Iteration 138, loss = 0.01264828
Iteration 139, loss = 0.01267602
Iteration 140, loss = 0.01260674
Iteration 141, loss = 0.01261053
Iteration 142, loss = 0.01254508
Iteration 143, loss = 0.01252693
Iteration 144, loss = 0.01249009
Iteration 145, loss = 0.01249124
Iteration 146, loss = 0.01243854
Iteration 147, loss = 0.01238149
Iteration 148, loss = 0.01234233
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67648971
Iteration 2, loss = 0.65814874
Iteration 3, loss = 0.64030525
Iteration 4, loss = 0.61907971
Iteration 5, loss = 0.59384937
Iteration 6, loss = 0.56482007
Iteration 7, loss = 0.53244952
Iteration 8, loss = 0.49822661
Iteration 9, loss = 0.46294615
Iteration 10, loss = 0.42800528
Iteration 11, loss = 0.39380421
Iteration 12, loss = 0.36115521
Iteration 13, loss = 0.33012289
Iteration 14, loss = 0.30108885
Iteration 15, loss = 0.27422401
Iteration 16, loss = 0.24961601
Iteration 17, loss = 0.22697425
Iteration 18, loss = 0.20662708
Iteration 19, loss = 0.18828769
Iteration 20, loss = 0.17161177
Iteration 21, loss = 0.15695445
Iteration 22, loss = 0.14380480
Iteration 23, loss = 0.13199091
Iteration 24, loss = 0.12158037
Iteration 25, loss = 0.11232966
Iteration 26, loss = 0.10402393
Iteration 27, loss = 0.09663741
Iteration 28, loss = 0.08997700
Iteration 29, loss = 0.08404622
Iteration 30, loss = 0.07874301
Iteration 31, loss = 0.07396027
Iteration 32, loss = 0.06966645
Iteration 33, loss = 0.06574214
Iteration 34, loss = 0.06224015
Iteration 35, loss = 0.05897830
Iteration 36, loss = 0.05609846
Iteration 37, loss = 0.05331219
Iteration 38, loss = 0.05086640
Iteration 39, loss = 0.04862336
Iteration 40, loss = 0.04660114
Iteration 41, loss = 0.04467889
Iteration 42, loss = 0.04294573
Iteration 43, loss = 0.04130472
Iteration 44, loss = 0.03978080
Iteration 45, loss = 0.03841245
Iteration 46, loss = 0.03709151
Iteration 47, loss = 0.03591018
Iteration 48, loss = 0.03476784
Iteration 49, loss = 0.03370037
Iteration 50, loss = 0.03271952
Iteration 51, loss = 0.03181414
Iteration 52, loss = 0.03091532
Iteration 53, loss = 0.03010226
Iteration 54, loss = 0.02933637
Iteration 55, loss = 0.02859400
Iteration 56, loss = 0.02794551
Iteration 57, loss = 0.02729236
Iteration 58, loss = 0.02667174
Iteration 59, loss = 0.02610455
Iteration 60, loss = 0.02553002
Iteration 61, loss = 0.02501074
Iteration 62, loss = 0.02453687
Iteration 63, loss = 0.02405353
Iteration 64, loss = 0.02365865
Iteration 65, loss = 0.02317536
Iteration 66, loss = 0.02283401
Iteration 67, loss = 0.02242930
Iteration 68, loss = 0.02207868
Iteration 69, loss = 0.02173633
Iteration 70, loss = 0.02135517
Iteration 71, loss = 0.02103404
Iteration 72, loss = 0.02072065
Iteration 73, loss = 0.02043494
Iteration 74, loss = 0.02019654
Iteration 75, loss = 0.01993435
Iteration 76, loss = 0.01966044
Iteration 77, loss = 0.01943333
Iteration 78, loss = 0.01916979
Iteration 79, loss = 0.01893473
Iteration 80, loss = 0.01873354
Iteration 81, loss = 0.01853634
Iteration 82, loss = 0.01834049
Iteration 83, loss = 0.01814163
Iteration 84, loss = 0.01800103
Iteration 85, loss = 0.01776646
Iteration 86, loss = 0.01758837
Iteration 87, loss = 0.01740965
Iteration 88, loss = 0.01732075
Iteration 89, loss = 0.01712147
Iteration 90, loss = 0.01696123
Iteration 91, loss = 0.01686338
Iteration 92, loss = 0.01671517
Iteration 93, loss = 0.01660204
Iteration 94, loss = 0.01650175
Iteration 95, loss = 0.01630521
Iteration 96, loss = 0.01619923
Iteration 97, loss = 0.01610766
Iteration 98, loss = 0.01595156
Iteration 99, loss = 0.01587520
Iteration 100, loss = 0.01571365
Iteration 101, loss = 0.01560333
Iteration 102, loss = 0.01557590
Iteration 103, loss = 0.01541102
Iteration 104, loss = 0.01538601
Iteration 105, loss = 0.01522640
Iteration 106, loss = 0.01510838
Iteration 107, loss = 0.01507354
Iteration 108, loss = 0.01495981
Iteration 109, loss = 0.01489848
Iteration 110, loss = 0.01486377
Iteration 111, loss = 0.01476016
Iteration 112, loss = 0.01466208
Iteration 113, loss = 0.01458569
Iteration 114, loss = 0.01454794
Iteration 115, loss = 0.01449685
Iteration 116, loss = 0.01442010
Iteration 117, loss = 0.01430468
Iteration 118, loss = 0.01425692
Iteration 119, loss = 0.01419473
Iteration 120, loss = 0.01411626
Iteration 121, loss = 0.01414278
Iteration 122, loss = 0.01402105
Iteration 123, loss = 0.01397909
Iteration 124, loss = 0.01393955
Iteration 125, loss = 0.01387711
Iteration 126, loss = 0.01381381
Iteration 127, loss = 0.01372335
Iteration 128, loss = 0.01381536
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71925393
Iteration 2, loss = 0.66779361
Iteration 3, loss = 0.60194182
Iteration 4, loss = 0.52157878
Iteration 5, loss = 0.43483836
Iteration 6, loss = 0.35426598
Iteration 7, loss = 0.28534352
Iteration 8, loss = 0.22986537
Iteration 9, loss = 0.18636400
Iteration 10, loss = 0.15298776
Iteration 11, loss = 0.12734040
Iteration 12, loss = 0.10785911
Iteration 13, loss = 0.09249766
Iteration 14, loss = 0.08088782
Iteration 15, loss = 0.07145690
Iteration 16, loss = 0.06387675
Iteration 17, loss = 0.05777260
Iteration 18, loss = 0.05267825
Iteration 19, loss = 0.04850975
Iteration 20, loss = 0.04492601
Iteration 21, loss = 0.04180366
Iteration 22, loss = 0.03921315
Iteration 23, loss = 0.03705212
Iteration 24, loss = 0.03492995
Iteration 25, loss = 0.03339778
Iteration 26, loss = 0.03183102
Iteration 27, loss = 0.03069498
Iteration 28, loss = 0.02948916
Iteration 29, loss = 0.02847778
Iteration 30, loss = 0.02774924
Iteration 31, loss = 0.02664745
Iteration 32, loss = 0.02603425
Iteration 33, loss = 0.02531464
Iteration 34, loss = 0.02464277
Iteration 35, loss = 0.02407649
Iteration 36, loss = 0.02339726
Iteration 37, loss = 0.02321236
Iteration 38, loss = 0.02267479
Iteration 39, loss = 0.02222620
Iteration 40, loss = 0.02196003
Iteration 41, loss = 0.02188237
Iteration 42, loss = 0.02171859
Iteration 43, loss = 0.02113098
Iteration 44, loss = 0.02096784
Iteration 45, loss = 0.02054755
Iteration 46, loss = 0.02036568
Iteration 47, loss = 0.02037547
Iteration 48, loss = 0.02012179
Iteration 49, loss = 0.01974624
Iteration 50, loss = 0.01966691
Iteration 51, loss = 0.01942877
Iteration 52, loss = 0.01921432
Iteration 53, loss = 0.01922704
Iteration 54, loss = 0.01907914
Iteration 55, loss = 0.01887341
Iteration 56, loss = 0.01855289
Iteration 57, loss = 0.01860399
Iteration 58, loss = 0.01853659
Iteration 59, loss = 0.01824937
Iteration 60, loss = 0.01825052
Iteration 61, loss = 0.01816649
Iteration 62, loss = 0.01838882
Iteration 63, loss = 0.01799981
Iteration 64, loss = 0.01792068
Iteration 65, loss = 0.01785870
Iteration 66, loss = 0.01761995
Iteration 67, loss = 0.01768960
Iteration 68, loss = 0.01748644
Iteration 69, loss = 0.01753796
Iteration 70, loss = 0.01755907
Iteration 71, loss = 0.01739007
Iteration 72, loss = 0.01756296
Iteration 73, loss = 0.01754454
Iteration 74, loss = 0.01726859
Iteration 75, loss = 0.01737392
Iteration 76, loss = 0.01740215
Iteration 77, loss = 0.01740879
Iteration 78, loss = 0.01730736
Iteration 79, loss = 0.01725842
Iteration 80, loss = 0.01709866
Iteration 81, loss = 0.01706739
Iteration 82, loss = 0.01722825
Iteration 83, loss = 0.01699971
Iteration 84, loss = 0.01685580
Iteration 85, loss = 0.01695773
Iteration 86, loss = 0.01682249
Iteration 87, loss = 0.01677741
Iteration 88, loss = 0.01697131
Iteration 89, loss = 0.01691812
Iteration 90, loss = 0.01684829
Iteration 91, loss = 0.01677069
Iteration 92, loss = 0.01670302
Iteration 93, loss = 0.01682838
Iteration 94, loss = 0.01675257
Iteration 95, loss = 0.01660812
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71581987
Iteration 2, loss = 0.65339040
Iteration 3, loss = 0.56882858
Iteration 4, loss = 0.46536991
Iteration 5, loss = 0.36467436
Iteration 6, loss = 0.28020713
Iteration 7, loss = 0.21613667
Iteration 8, loss = 0.16875662
Iteration 9, loss = 0.13502438
Iteration 10, loss = 0.11080587
Iteration 11, loss = 0.09317259
Iteration 12, loss = 0.08013162
Iteration 13, loss = 0.07025539
Iteration 14, loss = 0.06256366
Iteration 15, loss = 0.05626311
Iteration 16, loss = 0.05149567
Iteration 17, loss = 0.04747484
Iteration 18, loss = 0.04448459
Iteration 19, loss = 0.04170764
Iteration 20, loss = 0.03942957
Iteration 21, loss = 0.03736598
Iteration 22, loss = 0.03573152
Iteration 23, loss = 0.03413939
Iteration 24, loss = 0.03270555
Iteration 25, loss = 0.03169206
Iteration 26, loss = 0.03054364
Iteration 27, loss = 0.02978582
Iteration 28, loss = 0.02894425
Iteration 29, loss = 0.02797500
Iteration 30, loss = 0.02750464
Iteration 31, loss = 0.02675566
Iteration 32, loss = 0.02636913
Iteration 33, loss = 0.02605274
Iteration 34, loss = 0.02552209
Iteration 35, loss = 0.02541007
Iteration 36, loss = 0.02477115
Iteration 37, loss = 0.02442537
Iteration 38, loss = 0.02418275
Iteration 39, loss = 0.02374617
Iteration 40, loss = 0.02343887
Iteration 41, loss = 0.02327503
Iteration 42, loss = 0.02297569
Iteration 43, loss = 0.02279922
Iteration 44, loss = 0.02261769
Iteration 45, loss = 0.02243643
Iteration 46, loss = 0.02227549
Iteration 47, loss = 0.02228124
Iteration 48, loss = 0.02217153
Iteration 49, loss = 0.02168965
Iteration 50, loss = 0.02169880
Iteration 51, loss = 0.02148509
Iteration 52, loss = 0.02159480
Iteration 53, loss = 0.02159131
Iteration 54, loss = 0.02181370
Iteration 55, loss = 0.02262935
Iteration 56, loss = 0.02199314
Iteration 57, loss = 0.02173300
Iteration 58, loss = 0.02161739
Iteration 59, loss = 0.02133185
Iteration 60, loss = 0.02121321
Iteration 61, loss = 0.02199850
Iteration 62, loss = 0.02208645
Iteration 63, loss = 0.02320123
Iteration 64, loss = 0.02364528
Iteration 65, loss = 0.02338495
Iteration 66, loss = 0.02269995
Iteration 67, loss = 0.02270079
Iteration 68, loss = 0.02234564
Iteration 69, loss = 0.02216675
Iteration 70, loss = 0.02220029
Iteration 71, loss = 0.02197091
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68030573
Iteration 2, loss = 0.62472953
Iteration 3, loss = 0.53284906
Iteration 4, loss = 0.42418507
Iteration 5, loss = 0.32845464
Iteration 6, loss = 0.25224725
Iteration 7, loss = 0.19573070
Iteration 8, loss = 0.15546387
Iteration 9, loss = 0.12743566
Iteration 10, loss = 0.10654492
Iteration 11, loss = 0.09101092
Iteration 12, loss = 0.07942697
Iteration 13, loss = 0.07047551
Iteration 14, loss = 0.06344324
Iteration 15, loss = 0.05809043
Iteration 16, loss = 0.05341496
Iteration 17, loss = 0.04980476
Iteration 18, loss = 0.04678264
Iteration 19, loss = 0.04395195
Iteration 20, loss = 0.04203182
Iteration 21, loss = 0.04010795
Iteration 22, loss = 0.03859111
Iteration 23, loss = 0.03738104
Iteration 24, loss = 0.03602405
Iteration 25, loss = 0.03492744
Iteration 26, loss = 0.03409768
Iteration 27, loss = 0.03369727
Iteration 28, loss = 0.03292853
Iteration 29, loss = 0.03197839
Iteration 30, loss = 0.03149797
Iteration 31, loss = 0.03063506
Iteration 32, loss = 0.03030629
Iteration 33, loss = 0.02982049
Iteration 34, loss = 0.02945773
Iteration 35, loss = 0.02893233
Iteration 36, loss = 0.02893423
Iteration 37, loss = 0.02829928
Iteration 38, loss = 0.02810909
Iteration 39, loss = 0.02775916
Iteration 40, loss = 0.02758286
Iteration 41, loss = 0.02746880
Iteration 42, loss = 0.02736709
Iteration 43, loss = 0.02691339
Iteration 44, loss = 0.02658250
Iteration 45, loss = 0.02633183
Iteration 46, loss = 0.02648777
Iteration 47, loss = 0.02634483
Iteration 48, loss = 0.02602785
Iteration 49, loss = 0.02614623
Iteration 50, loss = 0.02561984
Iteration 51, loss = 0.02578782
Iteration 52, loss = 0.02570638
Iteration 53, loss = 0.02522473
Iteration 54, loss = 0.02527880
Iteration 55, loss = 0.02530680
Iteration 56, loss = 0.02499824
Iteration 57, loss = 0.02501218
Iteration 58, loss = 0.02482896
Iteration 59, loss = 0.02488264
Iteration 60, loss = 0.02478930
Iteration 61, loss = 0.02461080
Iteration 62, loss = 0.02478480
Iteration 63, loss = 0.02443659
Iteration 64, loss = 0.02444726
Iteration 65, loss = 0.02457186
Iteration 66, loss = 0.02433403
Iteration 67, loss = 0.02438493
Iteration 68, loss = 0.02431015
Iteration 69, loss = 0.02526507
Iteration 70, loss = 0.02557468
Iteration 71, loss = 0.02503910
Iteration 72, loss = 0.02494514
Iteration 73, loss = 0.02490585
Iteration 74, loss = 0.02477572
Iteration 75, loss = 0.02462105
Iteration 76, loss = 0.02469107
Iteration 77, loss = 0.02440680
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68325384
Iteration 2, loss = 0.66293921
Iteration 3, loss = 0.64274067
Iteration 4, loss = 0.61939497
Iteration 5, loss = 0.59134658
Iteration 6, loss = 0.55901095
Iteration 7, loss = 0.52339818
Iteration 8, loss = 0.48578468
Iteration 9, loss = 0.44844334
Iteration 10, loss = 0.41182601
Iteration 11, loss = 0.37662043
Iteration 12, loss = 0.34316972
Iteration 13, loss = 0.31199878
Iteration 14, loss = 0.28309445
Iteration 15, loss = 0.25673489
Iteration 16, loss = 0.23259878
Iteration 17, loss = 0.21090981
Iteration 18, loss = 0.19125683
Iteration 19, loss = 0.17379858
Iteration 20, loss = 0.15830471
Iteration 21, loss = 0.14433839
Iteration 22, loss = 0.13206073
Iteration 23, loss = 0.12110936
Iteration 24, loss = 0.11142847
Iteration 25, loss = 0.10280411
Iteration 26, loss = 0.09519037
Iteration 27, loss = 0.08828791
Iteration 28, loss = 0.08222369
Iteration 29, loss = 0.07670750
Iteration 30, loss = 0.07185373
Iteration 31, loss = 0.06742353
Iteration 32, loss = 0.06344904
Iteration 33, loss = 0.05984867
Iteration 34, loss = 0.05669833
Iteration 35, loss = 0.05369889
Iteration 36, loss = 0.05099132
Iteration 37, loss = 0.04853130
Iteration 38, loss = 0.04631153
Iteration 39, loss = 0.04423389
Iteration 40, loss = 0.04234471
Iteration 41, loss = 0.04059772
Iteration 42, loss = 0.03903188
Iteration 43, loss = 0.03753013
Iteration 44, loss = 0.03616635
Iteration 45, loss = 0.03491533
Iteration 46, loss = 0.03371402
Iteration 47, loss = 0.03258050
Iteration 48, loss = 0.03153236
Iteration 49, loss = 0.03058953
Iteration 50, loss = 0.02976452
Iteration 51, loss = 0.02888002
Iteration 52, loss = 0.02808454
Iteration 53, loss = 0.02738514
Iteration 54, loss = 0.02665420
Iteration 55, loss = 0.02600784
Iteration 56, loss = 0.02541028
Iteration 57, loss = 0.02480472
Iteration 58, loss = 0.02428207
Iteration 59, loss = 0.02371841
Iteration 60, loss = 0.02323379
Iteration 61, loss = 0.02278713
Iteration 62, loss = 0.02231385
Iteration 63, loss = 0.02195325
Iteration 64, loss = 0.02155427
Iteration 65, loss = 0.02115634
Iteration 66, loss = 0.02079061
Iteration 67, loss = 0.02044852
Iteration 68, loss = 0.02022354
Iteration 69, loss = 0.01985076
Iteration 70, loss = 0.01956119
Iteration 71, loss = 0.01922133
Iteration 72, loss = 0.01899023
Iteration 73, loss = 0.01867189
Iteration 74, loss = 0.01843878
Iteration 75, loss = 0.01820585
Iteration 76, loss = 0.01809127
Iteration 77, loss = 0.01779394
Iteration 78, loss = 0.01758889
Iteration 79, loss = 0.01736263
Iteration 80, loss = 0.01716435
Iteration 81, loss = 0.01701937
Iteration 82, loss = 0.01682578
Iteration 83, loss = 0.01664061
Iteration 84, loss = 0.01646895
Iteration 85, loss = 0.01635428
Iteration 86, loss = 0.01615565
Iteration 87, loss = 0.01604266
Iteration 88, loss = 0.01591885
Iteration 89, loss = 0.01574674
Iteration 90, loss = 0.01561257
Iteration 91, loss = 0.01549981
Iteration 92, loss = 0.01533766
Iteration 93, loss = 0.01524537
Iteration 94, loss = 0.01518347
Iteration 95, loss = 0.01505167
Iteration 96, loss = 0.01490175
Iteration 97, loss = 0.01485684
Iteration 98, loss = 0.01469450
Iteration 99, loss = 0.01459233
Iteration 100, loss = 0.01453267
Iteration 101, loss = 0.01445385
Iteration 102, loss = 0.01432737
Iteration 103, loss = 0.01423560
Iteration 104, loss = 0.01417232
Iteration 105, loss = 0.01407429
Iteration 106, loss = 0.01404672
Iteration 107, loss = 0.01391442
Iteration 108, loss = 0.01386980
Iteration 109, loss = 0.01381333
Iteration 110, loss = 0.01373934
Iteration 111, loss = 0.01368181
Iteration 112, loss = 0.01359888
Iteration 113, loss = 0.01351576
Iteration 114, loss = 0.01349034
Iteration 115, loss = 0.01337049
Iteration 116, loss = 0.01331141
Iteration 117, loss = 0.01335909
Iteration 118, loss = 0.01322740
Iteration 119, loss = 0.01314407
Iteration 120, loss = 0.01310732
Iteration 121, loss = 0.01307204
Iteration 122, loss = 0.01304449
Iteration 123, loss = 0.01302257
Iteration 124, loss = 0.01291295
Iteration 125, loss = 0.01286537
Iteration 126, loss = 0.01287458
Iteration 127, loss = 0.01276526
Iteration 128, loss = 0.01272570
Iteration 129, loss = 0.01275004
Iteration 130, loss = 0.01275454
Iteration 131, loss = 0.01265204
Iteration 132, loss = 0.01259337
Iteration 133, loss = 0.01254967
Iteration 134, loss = 0.01259234
Iteration 135, loss = 0.01248161
Iteration 136, loss = 0.01247217
Iteration 137, loss = 0.01241040
Iteration 138, loss = 0.01239073
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72208861
Iteration 2, loss = 0.69304753
Iteration 3, loss = 0.65985017
Iteration 4, loss = 0.61859821
Iteration 5, loss = 0.57135298
Iteration 6, loss = 0.52050658
Iteration 7, loss = 0.46706195
Iteration 8, loss = 0.41364681
Iteration 9, loss = 0.36328514
Iteration 10, loss = 0.31760979
Iteration 11, loss = 0.27684628
Iteration 12, loss = 0.24142456
Iteration 13, loss = 0.21098314
Iteration 14, loss = 0.18497354
Iteration 15, loss = 0.16308468
Iteration 16, loss = 0.14448593
Iteration 17, loss = 0.12883629
Iteration 18, loss = 0.11546220
Iteration 19, loss = 0.10413044
Iteration 20, loss = 0.09457472
Iteration 21, loss = 0.08626153
Iteration 22, loss = 0.07918754
Iteration 23, loss = 0.07294851
Iteration 24, loss = 0.06771014
Iteration 25, loss = 0.06313020
Iteration 26, loss = 0.05892461
Iteration 27, loss = 0.05535276
Iteration 28, loss = 0.05207017
Iteration 29, loss = 0.04925405
Iteration 30, loss = 0.04672188
Iteration 31, loss = 0.04429055
Iteration 32, loss = 0.04234845
Iteration 33, loss = 0.04049938
Iteration 34, loss = 0.03885853
Iteration 35, loss = 0.03732229
Iteration 36, loss = 0.03568012
Iteration 37, loss = 0.03427212
Iteration 38, loss = 0.03305003
Iteration 39, loss = 0.03187958
Iteration 40, loss = 0.03077645
Iteration 41, loss = 0.02998140
Iteration 42, loss = 0.02921304
Iteration 43, loss = 0.02830101
Iteration 44, loss = 0.02762485
Iteration 45, loss = 0.02696832
Iteration 46, loss = 0.02636462
Iteration 47, loss = 0.02568827
Iteration 48, loss = 0.02507530
Iteration 49, loss = 0.02448389
Iteration 50, loss = 0.02400018
Iteration 51, loss = 0.02344201
Iteration 52, loss = 0.02303112
Iteration 53, loss = 0.02268544
Iteration 54, loss = 0.02235194
Iteration 55, loss = 0.02199924
Iteration 56, loss = 0.02169643
Iteration 57, loss = 0.02120938
Iteration 58, loss = 0.02095793
Iteration 59, loss = 0.02066122
Iteration 60, loss = 0.02030722
Iteration 61, loss = 0.02001604
Iteration 62, loss = 0.01982462
Iteration 63, loss = 0.01960956
Iteration 64, loss = 0.01956994
Iteration 65, loss = 0.01936569
Iteration 66, loss = 0.01906535
Iteration 67, loss = 0.01884909
Iteration 68, loss = 0.01900851
Iteration 69, loss = 0.01865846
Iteration 70, loss = 0.01845734
Iteration 71, loss = 0.01846202
Iteration 72, loss = 0.01814331
Iteration 73, loss = 0.01798804
Iteration 74, loss = 0.01782920
Iteration 75, loss = 0.01752222
Iteration 76, loss = 0.01739319
Iteration 77, loss = 0.01739811
Iteration 78, loss = 0.01728013
Iteration 79, loss = 0.01709703
Iteration 80, loss = 0.01692777
Iteration 81, loss = 0.01684280
Iteration 82, loss = 0.01664871
Iteration 83, loss = 0.01660820
Iteration 84, loss = 0.01650073
Iteration 85, loss = 0.01668133
Iteration 86, loss = 0.01593887
Iteration 87, loss = 0.01619256
Iteration 88, loss = 0.01617502
Iteration 89, loss = 0.01620448
Iteration 90, loss = 0.01602335
Iteration 91, loss = 0.01594201
Iteration 92, loss = 0.01590664
Iteration 93, loss = 0.01566356
Iteration 94, loss = 0.01562584
Iteration 95, loss = 0.01556743
Iteration 96, loss = 0.01543819
Iteration 97, loss = 0.01548769
Iteration 98, loss = 0.01532938
Iteration 99, loss = 0.01529875
Iteration 100, loss = 0.01532041
Iteration 101, loss = 0.01517226
Iteration 102, loss = 0.01541691
Iteration 103, loss = 0.01529729
Iteration 104, loss = 0.01528031
Iteration 105, loss = 0.01516455
Iteration 106, loss = 0.01510375
Iteration 107, loss = 0.01481014
Iteration 108, loss = 0.01496927
Iteration 109, loss = 0.01493917
Iteration 110, loss = 0.01505062
Iteration 111, loss = 0.01507072
Iteration 112, loss = 0.01518324
Iteration 113, loss = 0.01518066
Iteration 114, loss = 0.01502156
Iteration 115, loss = 0.01485738
Iteration 116, loss = 0.01473800
Iteration 117, loss = 0.01477685
Iteration 118, loss = 0.01469607
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68874454
Iteration 2, loss = 0.66091070
Iteration 3, loss = 0.61525752
Iteration 4, loss = 0.55325826
Iteration 5, loss = 0.48356663
Iteration 6, loss = 0.41307805
Iteration 7, loss = 0.34674388
Iteration 8, loss = 0.28817642
Iteration 9, loss = 0.23839697
Iteration 10, loss = 0.19759633
Iteration 11, loss = 0.16485115
Iteration 12, loss = 0.13907494
Iteration 13, loss = 0.11864460
Iteration 14, loss = 0.10245042
Iteration 15, loss = 0.08943295
Iteration 16, loss = 0.07911870
Iteration 17, loss = 0.07062000
Iteration 18, loss = 0.06364892
Iteration 19, loss = 0.05780485
Iteration 20, loss = 0.05294640
Iteration 21, loss = 0.04884030
Iteration 22, loss = 0.04519372
Iteration 23, loss = 0.04221362
Iteration 24, loss = 0.03955201
Iteration 25, loss = 0.03724000
Iteration 26, loss = 0.03524966
Iteration 27, loss = 0.03353458
Iteration 28, loss = 0.03190388
Iteration 29, loss = 0.03057224
Iteration 30, loss = 0.02931915
Iteration 31, loss = 0.02814256
Iteration 32, loss = 0.02721372
Iteration 33, loss = 0.02619265
Iteration 34, loss = 0.02541361
Iteration 35, loss = 0.02469932
Iteration 36, loss = 0.02400337
Iteration 37, loss = 0.02332486
Iteration 38, loss = 0.02278567
Iteration 39, loss = 0.02229094
Iteration 40, loss = 0.02188697
Iteration 41, loss = 0.02144341
Iteration 42, loss = 0.02089670
Iteration 43, loss = 0.02050951
Iteration 44, loss = 0.02042690
Iteration 45, loss = 0.01989693
Iteration 46, loss = 0.01960241
Iteration 47, loss = 0.01948823
Iteration 48, loss = 0.01912773
Iteration 49, loss = 0.01877404
Iteration 50, loss = 0.01854428
Iteration 51, loss = 0.01831951
Iteration 52, loss = 0.01809420
Iteration 53, loss = 0.01795623
Iteration 54, loss = 0.01776278
Iteration 55, loss = 0.01755124
Iteration 56, loss = 0.01751994
Iteration 57, loss = 0.01739112
Iteration 58, loss = 0.01723581
Iteration 59, loss = 0.01703564
Iteration 60, loss = 0.01690431
Iteration 61, loss = 0.01694194
Iteration 62, loss = 0.01680633
Iteration 63, loss = 0.01645229
Iteration 64, loss = 0.01646007
Iteration 65, loss = 0.01643421
Iteration 66, loss = 0.01637229
Iteration 67, loss = 0.01619608
Iteration 68, loss = 0.01608899
Iteration 69, loss = 0.01604935
Iteration 70, loss = 0.01587696
Iteration 71, loss = 0.01579665
Iteration 72, loss = 0.01585375
Iteration 73, loss = 0.01571592
Iteration 74, loss = 0.01560067
Iteration 75, loss = 0.01560488
Iteration 76, loss = 0.01556507
Iteration 77, loss = 0.01552708
Iteration 78, loss = 0.01545659
Iteration 79, loss = 0.01540216
Iteration 80, loss = 0.01531914
Iteration 81, loss = 0.01530560
Iteration 82, loss = 0.01525517
Iteration 83, loss = 0.01513460
Iteration 84, loss = 0.01534997
Iteration 85, loss = 0.01502259
Iteration 86, loss = 0.01506814
Iteration 87, loss = 0.01499327
Iteration 88, loss = 0.01489038
Iteration 89, loss = 0.01484063
Iteration 90, loss = 0.01494085
Iteration 91, loss = 0.01487678
Iteration 92, loss = 0.01491193
Iteration 93, loss = 0.01477158
Iteration 94, loss = 0.01464724
Iteration 95, loss = 0.01470876
Iteration 96, loss = 0.01496490
Iteration 97, loss = 0.01469628
Iteration 98, loss = 0.01461463
Iteration 99, loss = 0.01460437
Iteration 100, loss = 0.01451884
Iteration 101, loss = 0.01466382
Iteration 102, loss = 0.01467232
Iteration 103, loss = 0.01457539
Iteration 104, loss = 0.01455577
Iteration 105, loss = 0.01457498
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69302644
Iteration 2, loss = 0.63170636
Iteration 3, loss = 0.53322008
Iteration 4, loss = 0.42130434
Iteration 5, loss = 0.32072455
Iteration 6, loss = 0.24256429
Iteration 7, loss = 0.18548787
Iteration 8, loss = 0.14582456
Iteration 9, loss = 0.11789929
Iteration 10, loss = 0.09820498
Iteration 11, loss = 0.08367418
Iteration 12, loss = 0.07286005
Iteration 13, loss = 0.06457674
Iteration 14, loss = 0.05810277
Iteration 15, loss = 0.05332205
Iteration 16, loss = 0.04895636
Iteration 17, loss = 0.04569915
Iteration 18, loss = 0.04267651
Iteration 19, loss = 0.04076133
Iteration 20, loss = 0.03844720
Iteration 21, loss = 0.03694048
Iteration 22, loss = 0.03511215
Iteration 23, loss = 0.03420271
Iteration 24, loss = 0.03308612
Iteration 25, loss = 0.03201094
Iteration 26, loss = 0.03108191
Iteration 27, loss = 0.03016645
Iteration 28, loss = 0.02967369
Iteration 29, loss = 0.02894767
Iteration 30, loss = 0.02840660
Iteration 31, loss = 0.02824938
Iteration 32, loss = 0.02773415
Iteration 33, loss = 0.02733766
Iteration 34, loss = 0.02687004
Iteration 35, loss = 0.02646660
Iteration 36, loss = 0.02634304
Iteration 37, loss = 0.02585494
Iteration 38, loss = 0.02562603
Iteration 39, loss = 0.02573134
Iteration 40, loss = 0.02514692
Iteration 41, loss = 0.02541213
Iteration 42, loss = 0.02479274
Iteration 43, loss = 0.02474136
Iteration 44, loss = 0.02458117
Iteration 45, loss = 0.02442969
Iteration 46, loss = 0.02421846
Iteration 47, loss = 0.02388754
Iteration 48, loss = 0.02390512
Iteration 49, loss = 0.02378524
Iteration 50, loss = 0.02381740
Iteration 51, loss = 0.02385392
Iteration 52, loss = 0.02336950
Iteration 53, loss = 0.02350942
Iteration 54, loss = 0.02337847
Iteration 55, loss = 0.02334342
Iteration 56, loss = 0.02311358
Iteration 57, loss = 0.02317974
Iteration 58, loss = 0.02310379
Iteration 59, loss = 0.02317914
Iteration 60, loss = 0.02301148
Iteration 61, loss = 0.02289637
Iteration 62, loss = 0.02291412
Iteration 63, loss = 0.02294113
Iteration 64, loss = 0.02248776
Iteration 65, loss = 0.02274312
Iteration 66, loss = 0.02255490
Iteration 67, loss = 0.02251311
Iteration 68, loss = 0.02259091
Iteration 69, loss = 0.02236810
Iteration 70, loss = 0.02260112
Iteration 71, loss = 0.02236103
Iteration 72, loss = 0.02229958
Iteration 73, loss = 0.02243437
Iteration 74, loss = 0.02238207
Iteration 75, loss = 0.02246169
Iteration 76, loss = 0.02229883
Iteration 77, loss = 0.02249743
Iteration 78, loss = 0.02221682
Iteration 79, loss = 0.02216713
Iteration 80, loss = 0.02216474
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67291476
Iteration 2, loss = 0.59731587
Iteration 3, loss = 0.49181091
Iteration 4, loss = 0.38443335
Iteration 5, loss = 0.29102267
Iteration 6, loss = 0.21986195
Iteration 7, loss = 0.16862089
Iteration 8, loss = 0.13317884
Iteration 9, loss = 0.10844113
Iteration 10, loss = 0.09069779
Iteration 11, loss = 0.07763000
Iteration 12, loss = 0.06821006
Iteration 13, loss = 0.06083067
Iteration 14, loss = 0.05485762
Iteration 15, loss = 0.05055114
Iteration 16, loss = 0.04699288
Iteration 17, loss = 0.04377055
Iteration 18, loss = 0.04141683
Iteration 19, loss = 0.03942379
Iteration 20, loss = 0.03745489
Iteration 21, loss = 0.03577987
Iteration 22, loss = 0.03448196
Iteration 23, loss = 0.03328761
Iteration 24, loss = 0.03253900
Iteration 25, loss = 0.03155202
Iteration 26, loss = 0.03084797
Iteration 27, loss = 0.03030683
Iteration 28, loss = 0.02966412
Iteration 29, loss = 0.02901420
Iteration 30, loss = 0.02862954
Iteration 31, loss = 0.02810101
Iteration 32, loss = 0.02752092
Iteration 33, loss = 0.02752693
Iteration 34, loss = 0.02751806
Iteration 35, loss = 0.02730054
Iteration 36, loss = 0.02672006
Iteration 37, loss = 0.02646664
Iteration 38, loss = 0.02593250
Iteration 39, loss = 0.02570337
Iteration 40, loss = 0.02559991
Iteration 41, loss = 0.02571059
Iteration 42, loss = 0.02543008
Iteration 43, loss = 0.02519671
Iteration 44, loss = 0.02501014
Iteration 45, loss = 0.02508770
Iteration 46, loss = 0.02465095
Iteration 47, loss = 0.02493236
Iteration 48, loss = 0.02446294
Iteration 49, loss = 0.02449749
Iteration 50, loss = 0.02433516
Iteration 51, loss = 0.02403300
Iteration 52, loss = 0.02432940
Iteration 53, loss = 0.02439006
Iteration 54, loss = 0.02434011
Iteration 55, loss = 0.02397368
Iteration 56, loss = 0.02392164
Iteration 57, loss = 0.02389013
Iteration 58, loss = 0.02381088
Iteration 59, loss = 0.02363040
Iteration 60, loss = 0.02360419
Iteration 61, loss = 0.02384478
Iteration 62, loss = 0.02344312
Iteration 63, loss = 0.02365862
Iteration 64, loss = 0.02367287
Iteration 65, loss = 0.02293389
Iteration 66, loss = 0.02350646
Iteration 67, loss = 0.02312008
Iteration 68, loss = 0.02314699
Iteration 69, loss = 0.02320207
Iteration 70, loss = 0.02345153
Iteration 71, loss = 0.02348168
Iteration 72, loss = 0.02311718
Iteration 73, loss = 0.02325858
Iteration 74, loss = 0.02317291
Iteration 75, loss = 0.02283583
Iteration 76, loss = 0.02322741
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67743471
Iteration 2, loss = 0.61763949
Iteration 3, loss = 0.52979201
Iteration 4, loss = 0.43626673
Iteration 5, loss = 0.35146406
Iteration 6, loss = 0.27925870
Iteration 7, loss = 0.22179751
Iteration 8, loss = 0.17770133
Iteration 9, loss = 0.14460644
Iteration 10, loss = 0.11996421
Iteration 11, loss = 0.10113166
Iteration 12, loss = 0.08678392
Iteration 13, loss = 0.07583838
Iteration 14, loss = 0.06695474
Iteration 15, loss = 0.06038781
Iteration 16, loss = 0.05474935
Iteration 17, loss = 0.04997445
Iteration 18, loss = 0.04628302
Iteration 19, loss = 0.04305921
Iteration 20, loss = 0.04041083
Iteration 21, loss = 0.03787043
Iteration 22, loss = 0.03609991
Iteration 23, loss = 0.03458535
Iteration 24, loss = 0.03297432
Iteration 25, loss = 0.03156041
Iteration 26, loss = 0.03052459
Iteration 27, loss = 0.02952226
Iteration 28, loss = 0.02841530
Iteration 29, loss = 0.02784443
Iteration 30, loss = 0.02709899
Iteration 31, loss = 0.02648292
Iteration 32, loss = 0.02563854
Iteration 33, loss = 0.02524675
Iteration 34, loss = 0.02472449
Iteration 35, loss = 0.02468212
Iteration 36, loss = 0.02412726
Iteration 37, loss = 0.02373696
Iteration 38, loss = 0.02319344
Iteration 39, loss = 0.02299677
Iteration 40, loss = 0.02253891
Iteration 41, loss = 0.02251593
Iteration 42, loss = 0.02218468
Iteration 43, loss = 0.02195539
Iteration 44, loss = 0.02176273
Iteration 45, loss = 0.02158807
Iteration 46, loss = 0.02136680
Iteration 47, loss = 0.02132500
Iteration 48, loss = 0.02117380
Iteration 49, loss = 0.02114868
Iteration 50, loss = 0.02091396
Iteration 51, loss = 0.02081217
Iteration 52, loss = 0.02051157
Iteration 53, loss = 0.02058904
Iteration 54, loss = 0.02035470
Iteration 55, loss = 0.02020952
Iteration 56, loss = 0.02019546
Iteration 57, loss = 0.02004353
Iteration 58, loss = 0.01998654
Iteration 59, loss = 0.02005683
Iteration 60, loss = 0.01975459
Iteration 61, loss = 0.01990917
Iteration 62, loss = 0.01965362
Iteration 63, loss = 0.01969822
Iteration 64, loss = 0.01964466
Iteration 65, loss = 0.01952855
Iteration 66, loss = 0.01956107
Iteration 67, loss = 0.01944842
Iteration 68, loss = 0.01954725
Iteration 69, loss = 0.01949444
Iteration 70, loss = 0.01928151
Iteration 71, loss = 0.01919315
Iteration 72, loss = 0.01921717
Iteration 73, loss = 0.01910168
Iteration 74, loss = 0.01918675
Iteration 75, loss = 0.01912986
Iteration 76, loss = 0.01912975
Iteration 77, loss = 0.01924709
Iteration 78, loss = 0.01900723
Iteration 79, loss = 0.01896938
Iteration 80, loss = 0.01884460
Iteration 81, loss = 0.01910233
Iteration 82, loss = 0.01913026
Iteration 83, loss = 0.01868769
Iteration 84, loss = 0.01912097
Iteration 85, loss = 0.01862102
Iteration 86, loss = 0.01866765
Iteration 87, loss = 0.01886355
Iteration 88, loss = 0.01864897
Iteration 89, loss = 0.01881268
Iteration 90, loss = 0.01863776
Iteration 91, loss = 0.01857593
Iteration 92, loss = 0.01891451
Iteration 93, loss = 0.01852356
Iteration 94, loss = 0.01901815
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69366019
Iteration 2, loss = 0.64523774
Iteration 3, loss = 0.57574446
Iteration 4, loss = 0.48863976
Iteration 5, loss = 0.39850047
Iteration 6, loss = 0.31641650
Iteration 7, loss = 0.24873040
Iteration 8, loss = 0.19576702
Iteration 9, loss = 0.15665643
Iteration 10, loss = 0.12746006
Iteration 11, loss = 0.10609994
Iteration 12, loss = 0.09012361
Iteration 13, loss = 0.07788447
Iteration 14, loss = 0.06875877
Iteration 15, loss = 0.06128378
Iteration 16, loss = 0.05535542
Iteration 17, loss = 0.05058463
Iteration 18, loss = 0.04648277
Iteration 19, loss = 0.04314482
Iteration 20, loss = 0.04042227
Iteration 21, loss = 0.03833746
Iteration 22, loss = 0.03619610
Iteration 23, loss = 0.03439335
Iteration 24, loss = 0.03281997
Iteration 25, loss = 0.03159084
Iteration 26, loss = 0.03068282
Iteration 27, loss = 0.02943697
Iteration 28, loss = 0.02843079
Iteration 29, loss = 0.02763064
Iteration 30, loss = 0.02693452
Iteration 31, loss = 0.02637671
Iteration 32, loss = 0.02565701
Iteration 33, loss = 0.02529827
Iteration 34, loss = 0.02494175
Iteration 35, loss = 0.02433904
Iteration 36, loss = 0.02387659
Iteration 37, loss = 0.02345069
Iteration 38, loss = 0.02314673
Iteration 39, loss = 0.02316806
Iteration 40, loss = 0.02250765
Iteration 41, loss = 0.02229186
Iteration 42, loss = 0.02203759
Iteration 43, loss = 0.02189333
Iteration 44, loss = 0.02172297
Iteration 45, loss = 0.02143585
Iteration 46, loss = 0.02141141
Iteration 47, loss = 0.02102151
Iteration 48, loss = 0.02103350
Iteration 49, loss = 0.02072324
Iteration 50, loss = 0.02064853
Iteration 51, loss = 0.02067206
Iteration 52, loss = 0.02042230
Iteration 53, loss = 0.02021299
Iteration 54, loss = 0.02012345
Iteration 55, loss = 0.02013740
Iteration 56, loss = 0.02007574
Iteration 57, loss = 0.01992956
Iteration 58, loss = 0.01971613
Iteration 59, loss = 0.01969966
Iteration 60, loss = 0.01968206
Iteration 61, loss = 0.01965434
Iteration 62, loss = 0.01957970
Iteration 63, loss = 0.01964021
Iteration 64, loss = 0.01938072
Iteration 65, loss = 0.01912235
Iteration 66, loss = 0.01919171
Iteration 67, loss = 0.01922693
Iteration 68, loss = 0.01910624
Iteration 69, loss = 0.01895064
Iteration 70, loss = 0.01906072
Iteration 71, loss = 0.01893158
Iteration 72, loss = 0.01889862
Iteration 73, loss = 0.01906319
Iteration 74, loss = 0.01885852
Iteration 75, loss = 0.01893900
Iteration 76, loss = 0.01883290
Iteration 77, loss = 0.01887103
Iteration 78, loss = 0.01872710
Iteration 79, loss = 0.01894637
Iteration 80, loss = 0.01880751
Iteration 81, loss = 0.01880015
Iteration 82, loss = 0.01889513
Iteration 83, loss = 0.01873295
Iteration 84, loss = 0.01847629
Iteration 85, loss = 0.01858228
Iteration 86, loss = 0.01830892
Iteration 87, loss = 0.01839028
Iteration 88, loss = 0.01849300
Iteration 89, loss = 0.01841424
Iteration 90, loss = 0.01829544
Iteration 91, loss = 0.01847199
Iteration 92, loss = 0.01843604
Iteration 93, loss = 0.01819474
Iteration 94, loss = 0.01842233
Iteration 95, loss = 0.01829880
Iteration 96, loss = 0.01828736
Iteration 97, loss = 0.01824918
Iteration 98, loss = 0.01819245
Iteration 99, loss = 0.01810237
Iteration 100, loss = 0.01828306
Iteration 101, loss = 0.01841320
Iteration 102, loss = 0.01835241
Iteration 103, loss = 0.01806863
Iteration 104, loss = 0.01788871
Iteration 105, loss = 0.01814216
Iteration 106, loss = 0.01817521
Iteration 107, loss = 0.01807153
Iteration 108, loss = 0.01796699
Iteration 109, loss = 0.01808439
Iteration 110, loss = 0.01794775
Iteration 111, loss = 0.01790085
Iteration 112, loss = 0.01799539
Iteration 113, loss = 0.01813699
Iteration 114, loss = 0.01790325
Iteration 115, loss = 0.01809627
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67511401
Iteration 2, loss = 0.60836469
Iteration 3, loss = 0.51514032
Iteration 4, loss = 0.42026215
Iteration 5, loss = 0.33792477
Iteration 6, loss = 0.27005404
Iteration 7, loss = 0.21671810
Iteration 8, loss = 0.17515689
Iteration 9, loss = 0.14431721
Iteration 10, loss = 0.12112942
Iteration 11, loss = 0.10335683
Iteration 12, loss = 0.08948352
Iteration 13, loss = 0.07900089
Iteration 14, loss = 0.07073543
Iteration 15, loss = 0.06453245
Iteration 16, loss = 0.05927012
Iteration 17, loss = 0.05478168
Iteration 18, loss = 0.05084733
Iteration 19, loss = 0.04761656
Iteration 20, loss = 0.04514342
Iteration 21, loss = 0.04290357
Iteration 22, loss = 0.04083321
Iteration 23, loss = 0.03915559
Iteration 24, loss = 0.03790565
Iteration 25, loss = 0.03648816
Iteration 26, loss = 0.03534213
Iteration 27, loss = 0.03451537
Iteration 28, loss = 0.03350680
Iteration 29, loss = 0.03365599
Iteration 30, loss = 0.03362689
Iteration 31, loss = 0.03230034
Iteration 32, loss = 0.03153296
Iteration 33, loss = 0.03066709
Iteration 34, loss = 0.03015415
Iteration 35, loss = 0.02952587
Iteration 36, loss = 0.02892853
Iteration 37, loss = 0.02851295
Iteration 38, loss = 0.02824332
Iteration 39, loss = 0.02777090
Iteration 40, loss = 0.02743916
Iteration 41, loss = 0.02682529
Iteration 42, loss = 0.02725833
Iteration 43, loss = 0.02700986
Iteration 44, loss = 0.02651176
Iteration 45, loss = 0.02608259
Iteration 46, loss = 0.02576738
Iteration 47, loss = 0.02556152
Iteration 48, loss = 0.02546249
Iteration 49, loss = 0.02547103
Iteration 50, loss = 0.02514646
Iteration 51, loss = 0.02527647
Iteration 52, loss = 0.02540628
Iteration 53, loss = 0.02520217
Iteration 54, loss = 0.02491030
Iteration 55, loss = 0.02479034
Iteration 56, loss = 0.02479686
Iteration 57, loss = 0.02434514
Iteration 58, loss = 0.02449475
Iteration 59, loss = 0.02426946
Iteration 60, loss = 0.02402811
Iteration 61, loss = 0.02409298
Iteration 62, loss = 0.02372741
Iteration 63, loss = 0.02376340
Iteration 64, loss = 0.02366390
Iteration 65, loss = 0.02358194
Iteration 66, loss = 0.02350933
Iteration 67, loss = 0.02363981
Iteration 68, loss = 0.02329766
Iteration 69, loss = 0.02315540
Iteration 70, loss = 0.02334984
Iteration 71, loss = 0.02428216
Iteration 72, loss = 0.02446431
Iteration 73, loss = 0.02404107
Iteration 74, loss = 0.02370557
Iteration 75, loss = 0.02357530
Iteration 76, loss = 0.02344672
Iteration 77, loss = 0.02340893
Iteration 78, loss = 0.02315456
Iteration 79, loss = 0.02310609
Iteration 80, loss = 0.02291861
Iteration 81, loss = 0.02276504
Iteration 82, loss = 0.02272478
Iteration 83, loss = 0.02336473
Iteration 84, loss = 0.02347704
Iteration 85, loss = 0.02343229
Iteration 86, loss = 0.02293186
Iteration 87, loss = 0.02293188
Iteration 88, loss = 0.02260038
Iteration 89, loss = 0.02268966
Iteration 90, loss = 0.02245262
Iteration 91, loss = 0.02260373
Iteration 92, loss = 0.02224906
Iteration 93, loss = 0.02224308
Iteration 94, loss = 0.02219588
Iteration 95, loss = 0.02199935
Iteration 96, loss = 0.02210279
Iteration 97, loss = 0.02223439
Iteration 98, loss = 0.02191035
Iteration 99, loss = 0.02175344
Iteration 100, loss = 0.02166524
Iteration 101, loss = 0.02169491
Iteration 102, loss = 0.02157972
Iteration 103, loss = 0.02184013
Iteration 104, loss = 0.02161099
Iteration 105, loss = 0.02165135
Iteration 106, loss = 0.02167017
Iteration 107, loss = 0.02182155
Iteration 108, loss = 0.02207294
Iteration 109, loss = 0.02335193
Iteration 110, loss = 0.02424275
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67827008
Iteration 2, loss = 0.62817150
Iteration 3, loss = 0.54429914
Iteration 4, loss = 0.44706999
Iteration 5, loss = 0.35488711
Iteration 6, loss = 0.27512657
Iteration 7, loss = 0.21245251
Iteration 8, loss = 0.16668583
Iteration 9, loss = 0.13385030
Iteration 10, loss = 0.11023604
Iteration 11, loss = 0.09325550
Iteration 12, loss = 0.08067010
Iteration 13, loss = 0.07057292
Iteration 14, loss = 0.06316766
Iteration 15, loss = 0.05713996
Iteration 16, loss = 0.05232077
Iteration 17, loss = 0.04836717
Iteration 18, loss = 0.04501998
Iteration 19, loss = 0.04240406
Iteration 20, loss = 0.04108344
Iteration 21, loss = 0.03892384
Iteration 22, loss = 0.03725021
Iteration 23, loss = 0.03538867
Iteration 24, loss = 0.03425956
Iteration 25, loss = 0.03290594
Iteration 26, loss = 0.03197494
Iteration 27, loss = 0.03125183
Iteration 28, loss = 0.02994009
Iteration 29, loss = 0.02946069
Iteration 30, loss = 0.02886571
Iteration 31, loss = 0.02825446
Iteration 32, loss = 0.02772524
Iteration 33, loss = 0.02736226
Iteration 34, loss = 0.02674523
Iteration 35, loss = 0.02649968
Iteration 36, loss = 0.02611867
Iteration 37, loss = 0.02586390
Iteration 38, loss = 0.02542569
Iteration 39, loss = 0.02527676
Iteration 40, loss = 0.02500399
Iteration 41, loss = 0.02463642
Iteration 42, loss = 0.02453525
Iteration 43, loss = 0.02432925
Iteration 44, loss = 0.02394432
Iteration 45, loss = 0.02382307
Iteration 46, loss = 0.02372946
Iteration 47, loss = 0.02337180
Iteration 48, loss = 0.02322002
Iteration 49, loss = 0.02351875
Iteration 50, loss = 0.02306048
Iteration 51, loss = 0.02298598
Iteration 52, loss = 0.02301735
Iteration 53, loss = 0.02268060
Iteration 54, loss = 0.02257864
Iteration 55, loss = 0.02267227
Iteration 56, loss = 0.02232706
Iteration 57, loss = 0.02229626
Iteration 58, loss = 0.02231345
Iteration 59, loss = 0.02237378
Iteration 60, loss = 0.02253133
Iteration 61, loss = 0.02244014
Iteration 62, loss = 0.02234550
Iteration 63, loss = 0.02228027
Iteration 64, loss = 0.02243024
Iteration 65, loss = 0.02216128
Iteration 66, loss = 0.02204047
Iteration 67, loss = 0.02189324
Iteration 68, loss = 0.02196980
Iteration 69, loss = 0.02192352
Iteration 70, loss = 0.02157818
Iteration 71, loss = 0.02175151
Iteration 72, loss = 0.02146085
Iteration 73, loss = 0.02130812
Iteration 74, loss = 0.02159709
Iteration 75, loss = 0.02166629
Iteration 76, loss = 0.02198523
Iteration 77, loss = 0.02294960
Iteration 78, loss = 0.02249702
Iteration 79, loss = 0.02223358
Iteration 80, loss = 0.02186651
Iteration 81, loss = 0.02168439
Iteration 82, loss = 0.02161130
Iteration 83, loss = 0.02136931
Iteration 84, loss = 0.02135287
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72849624
Iteration 2, loss = 0.69446923
Iteration 3, loss = 0.65309102
Iteration 4, loss = 0.60395273
Iteration 5, loss = 0.55091705
Iteration 6, loss = 0.49568745
Iteration 7, loss = 0.44154705
Iteration 8, loss = 0.39066585
Iteration 9, loss = 0.34366180
Iteration 10, loss = 0.30127310
Iteration 11, loss = 0.26380698
Iteration 12, loss = 0.23124986
Iteration 13, loss = 0.20320062
Iteration 14, loss = 0.17911883
Iteration 15, loss = 0.15875850
Iteration 16, loss = 0.14136647
Iteration 17, loss = 0.12656206
Iteration 18, loss = 0.11404803
Iteration 19, loss = 0.10324225
Iteration 20, loss = 0.09408202
Iteration 21, loss = 0.08615690
Iteration 22, loss = 0.07933654
Iteration 23, loss = 0.07327850
Iteration 24, loss = 0.06815009
Iteration 25, loss = 0.06365141
Iteration 26, loss = 0.05950358
Iteration 27, loss = 0.05592213
Iteration 28, loss = 0.05264007
Iteration 29, loss = 0.04971952
Iteration 30, loss = 0.04734098
Iteration 31, loss = 0.04511395
Iteration 32, loss = 0.04283734
Iteration 33, loss = 0.04110534
Iteration 34, loss = 0.03936122
Iteration 35, loss = 0.03783988
Iteration 36, loss = 0.03640007
Iteration 37, loss = 0.03509240
Iteration 38, loss = 0.03392653
Iteration 39, loss = 0.03286014
Iteration 40, loss = 0.03178954
Iteration 41, loss = 0.03085047
Iteration 42, loss = 0.03002361
Iteration 43, loss = 0.02925933
Iteration 44, loss = 0.02850210
Iteration 45, loss = 0.02781997
Iteration 46, loss = 0.02720112
Iteration 47, loss = 0.02667003
Iteration 48, loss = 0.02612100
Iteration 49, loss = 0.02543524
Iteration 50, loss = 0.02522457
Iteration 51, loss = 0.02465644
Iteration 52, loss = 0.02418149
Iteration 53, loss = 0.02396181
Iteration 54, loss = 0.02360496
Iteration 55, loss = 0.02332388
Iteration 56, loss = 0.02307243
Iteration 57, loss = 0.02255248
Iteration 58, loss = 0.02221252
Iteration 59, loss = 0.02175262
Iteration 60, loss = 0.02156023
Iteration 61, loss = 0.02130875
Iteration 62, loss = 0.02098113
Iteration 63, loss = 0.02066746
Iteration 64, loss = 0.02039661
Iteration 65, loss = 0.02035026
Iteration 66, loss = 0.01998930
Iteration 67, loss = 0.01981249
Iteration 68, loss = 0.01960609
Iteration 69, loss = 0.01943861
Iteration 70, loss = 0.01929693
Iteration 71, loss = 0.01922870
Iteration 72, loss = 0.01924621
Iteration 73, loss = 0.01909642
Iteration 74, loss = 0.01911196
Iteration 75, loss = 0.01896179
Iteration 76, loss = 0.01874698
Iteration 77, loss = 0.01847302
Iteration 78, loss = 0.01828726
Iteration 79, loss = 0.01807473
Iteration 80, loss = 0.01781629
Iteration 81, loss = 0.01788164
Iteration 82, loss = 0.01783889
Iteration 83, loss = 0.01784098
Iteration 84, loss = 0.01772945
Iteration 85, loss = 0.01762779
Iteration 86, loss = 0.01740546
Iteration 87, loss = 0.01740582
Iteration 88, loss = 0.01730308
Iteration 89, loss = 0.01729647
Iteration 90, loss = 0.01704503
Iteration 91, loss = 0.01691778
Iteration 92, loss = 0.01684687
Iteration 93, loss = 0.01680452
Iteration 94, loss = 0.01668449
Iteration 95, loss = 0.01665911
Iteration 96, loss = 0.01650145
Iteration 97, loss = 0.01643956
Iteration 98, loss = 0.01639226
Iteration 99, loss = 0.01635108
Iteration 100, loss = 0.01621205
Iteration 101, loss = 0.01649363
Iteration 102, loss = 0.01649310
Iteration 103, loss = 0.01625547
Iteration 104, loss = 0.01602040
Iteration 105, loss = 0.01636315
Iteration 106, loss = 0.01658914
Iteration 107, loss = 0.01638380
Iteration 108, loss = 0.01601271
Iteration 109, loss = 0.01588176
Iteration 110, loss = 0.01618379
Iteration 111, loss = 0.01620039
Iteration 112, loss = 0.01611885
Iteration 113, loss = 0.01594623
Iteration 114, loss = 0.01588915
Iteration 115, loss = 0.01577713
Iteration 116, loss = 0.01581982
Iteration 117, loss = 0.01575400
Iteration 118, loss = 0.01570523
Iteration 119, loss = 0.01580181
Iteration 120, loss = 0.01570574
Iteration 121, loss = 0.01561823
Iteration 122, loss = 0.01546552
Iteration 123, loss = 0.01539270
Iteration 124, loss = 0.01544434
Iteration 125, loss = 0.01541220
Iteration 126, loss = 0.01530166
Iteration 127, loss = 0.01533526
Iteration 128, loss = 0.01529730
Iteration 129, loss = 0.01504869
Iteration 130, loss = 0.01560408
Iteration 131, loss = 0.01585301
Iteration 132, loss = 0.01566907
Iteration 133, loss = 0.01564812
Iteration 134, loss = 0.01545873
Iteration 135, loss = 0.01532919
Iteration 136, loss = 0.01516380
Iteration 137, loss = 0.01504476
Iteration 138, loss = 0.01505226
Iteration 139, loss = 0.01503147
Iteration 140, loss = 0.01491902
Iteration 141, loss = 0.01522192
Iteration 142, loss = 0.01526763
Iteration 143, loss = 0.01540164
Iteration 144, loss = 0.01532583
Iteration 145, loss = 0.01532000
Iteration 146, loss = 0.01509864
Iteration 147, loss = 0.01496600
Iteration 148, loss = 0.01485129
Iteration 149, loss = 0.01493706
Iteration 150, loss = 0.01475056
Iteration 151, loss = 0.01479830
Iteration 152, loss = 0.01473621
Iteration 153, loss = 0.01480411
Iteration 154, loss = 0.01473153
Iteration 155, loss = 0.01469882
Iteration 156, loss = 0.01468940
Iteration 157, loss = 0.01514462
Iteration 158, loss = 0.01512385
Iteration 159, loss = 0.01501793
Iteration 160, loss = 0.01507135
Iteration 161, loss = 0.01501620
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69564020
Iteration 2, loss = 0.68024693
Iteration 3, loss = 0.66860448
Iteration 4, loss = 0.65788696
Iteration 5, loss = 0.64664735
Iteration 6, loss = 0.63441683
Iteration 7, loss = 0.62114784
Iteration 8, loss = 0.60635816
Iteration 9, loss = 0.59049736
Iteration 10, loss = 0.57303012
Iteration 11, loss = 0.55433019
Iteration 12, loss = 0.53466066
Iteration 13, loss = 0.51399053
Iteration 14, loss = 0.49275087
Iteration 15, loss = 0.47117257
Iteration 16, loss = 0.44935008
Iteration 17, loss = 0.42743680
Iteration 18, loss = 0.40614189
Iteration 19, loss = 0.38481890
Iteration 20, loss = 0.36439598
Iteration 21, loss = 0.34451011
Iteration 22, loss = 0.32547450
Iteration 23, loss = 0.30711146
Iteration 24, loss = 0.28975887
Iteration 25, loss = 0.27328244
Iteration 26, loss = 0.25770539
Iteration 27, loss = 0.24314995
Iteration 28, loss = 0.22935093
Iteration 29, loss = 0.21640666
Iteration 30, loss = 0.20429230
Iteration 31, loss = 0.19299462
Iteration 32, loss = 0.18234512
Iteration 33, loss = 0.17238090
Iteration 34, loss = 0.16307388
Iteration 35, loss = 0.15438153
Iteration 36, loss = 0.14624475
Iteration 37, loss = 0.13869884
Iteration 38, loss = 0.13156923
Iteration 39, loss = 0.12500656
Iteration 40, loss = 0.11881000
Iteration 41, loss = 0.11305787
Iteration 42, loss = 0.10767462
Iteration 43, loss = 0.10267083
Iteration 44, loss = 0.09790659
Iteration 45, loss = 0.09353663
Iteration 46, loss = 0.08941510
Iteration 47, loss = 0.08554344
Iteration 48, loss = 0.08192218
Iteration 49, loss = 0.07859375
Iteration 50, loss = 0.07537457
Iteration 51, loss = 0.07236643
Iteration 52, loss = 0.06958672
Iteration 53, loss = 0.06695001
Iteration 54, loss = 0.06444632
Iteration 55, loss = 0.06209434
Iteration 56, loss = 0.05991549
Iteration 57, loss = 0.05781983
Iteration 58, loss = 0.05587669
Iteration 59, loss = 0.05398290
Iteration 60, loss = 0.05221900
Iteration 61, loss = 0.05053866
Iteration 62, loss = 0.04898289
Iteration 63, loss = 0.04749723
Iteration 64, loss = 0.04606577
Iteration 65, loss = 0.04470340
Iteration 66, loss = 0.04340927
Iteration 67, loss = 0.04221065
Iteration 68, loss = 0.04101954
Iteration 69, loss = 0.03991031
Iteration 70, loss = 0.03887862
Iteration 71, loss = 0.03788592
Iteration 72, loss = 0.03689751
Iteration 73, loss = 0.03596657
Iteration 74, loss = 0.03509523
Iteration 75, loss = 0.03424029
Iteration 76, loss = 0.03343680
Iteration 77, loss = 0.03267611
Iteration 78, loss = 0.03196681
Iteration 79, loss = 0.03124275
Iteration 80, loss = 0.03056789
Iteration 81, loss = 0.02991779
Iteration 82, loss = 0.02930834
Iteration 83, loss = 0.02869405
Iteration 84, loss = 0.02816902
Iteration 85, loss = 0.02757736
Iteration 86, loss = 0.02703513
Iteration 87, loss = 0.02653092
Iteration 88, loss = 0.02607733
Iteration 89, loss = 0.02560732
Iteration 90, loss = 0.02517695
Iteration 91, loss = 0.02475167
Iteration 92, loss = 0.02432335
Iteration 93, loss = 0.02387118
Iteration 94, loss = 0.02350892
Iteration 95, loss = 0.02311095
Iteration 96, loss = 0.02275326
Iteration 97, loss = 0.02242526
Iteration 98, loss = 0.02204295
Iteration 99, loss = 0.02173756
Iteration 100, loss = 0.02140665
Iteration 101, loss = 0.02110173
Iteration 102, loss = 0.02078400
Iteration 103, loss = 0.02050440
Iteration 104, loss = 0.02023081
Iteration 105, loss = 0.01997450
Iteration 106, loss = 0.01969156
Iteration 107, loss = 0.01944435
Iteration 108, loss = 0.01917634
Iteration 109, loss = 0.01894071
Iteration 110, loss = 0.01869057
Iteration 111, loss = 0.01846251
Iteration 112, loss = 0.01824772
Iteration 113, loss = 0.01802336
Iteration 114, loss = 0.01781582
Iteration 115, loss = 0.01762334
Iteration 116, loss = 0.01739508
Iteration 117, loss = 0.01721064
Iteration 118, loss = 0.01703240
Iteration 119, loss = 0.01684327
Iteration 120, loss = 0.01666708
Iteration 121, loss = 0.01647994
Iteration 122, loss = 0.01631783
Iteration 123, loss = 0.01613336
Iteration 124, loss = 0.01597166
Iteration 125, loss = 0.01583702
Iteration 126, loss = 0.01568091
Iteration 127, loss = 0.01553631
Iteration 128, loss = 0.01545199
Iteration 129, loss = 0.01529448
Iteration 130, loss = 0.01513185
Iteration 131, loss = 0.01500069
Iteration 132, loss = 0.01486559
Iteration 133, loss = 0.01477770
Iteration 134, loss = 0.01464093
Iteration 135, loss = 0.01453805
Iteration 136, loss = 0.01441659
Iteration 137, loss = 0.01428030
Iteration 138, loss = 0.01418886
Iteration 139, loss = 0.01408474
Iteration 140, loss = 0.01400886
Iteration 141, loss = 0.01391173
Iteration 142, loss = 0.01381234
Iteration 143, loss = 0.01369463
Iteration 144, loss = 0.01358289
Iteration 145, loss = 0.01347244
Iteration 146, loss = 0.01337288
Iteration 147, loss = 0.01327437
Iteration 148, loss = 0.01315213
Iteration 149, loss = 0.01305004
Iteration 150, loss = 0.01296935
Iteration 151, loss = 0.01290248
Iteration 152, loss = 0.01283321
Iteration 153, loss = 0.01272573
Iteration 154, loss = 0.01262655
Iteration 155, loss = 0.01257004
Iteration 156, loss = 0.01249070
Iteration 157, loss = 0.01239765
Iteration 158, loss = 0.01237874
Iteration 159, loss = 0.01226299
Iteration 160, loss = 0.01216477
Iteration 161, loss = 0.01210995
Iteration 162, loss = 0.01205815
Iteration 163, loss = 0.01198511
Iteration 164, loss = 0.01192819
Iteration 165, loss = 0.01187999
Iteration 166, loss = 0.01182551
Iteration 167, loss = 0.01176280
Iteration 168, loss = 0.01170455
Iteration 169, loss = 0.01163405
Iteration 170, loss = 0.01158060
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70704151
Iteration 2, loss = 0.68333824
Iteration 3, loss = 0.66167442
Iteration 4, loss = 0.63765099
Iteration 5, loss = 0.61005881
Iteration 6, loss = 0.57932977
Iteration 7, loss = 0.54575850
Iteration 8, loss = 0.50981827
Iteration 9, loss = 0.47300929
Iteration 10, loss = 0.43598865
Iteration 11, loss = 0.39955511
Iteration 12, loss = 0.36477111
Iteration 13, loss = 0.33207675
Iteration 14, loss = 0.30155276
Iteration 15, loss = 0.27351678
Iteration 16, loss = 0.24803656
Iteration 17, loss = 0.22476890
Iteration 18, loss = 0.20413464
Iteration 19, loss = 0.18547692
Iteration 20, loss = 0.16896092
Iteration 21, loss = 0.15418585
Iteration 22, loss = 0.14114638
Iteration 23, loss = 0.12958224
Iteration 24, loss = 0.11923461
Iteration 25, loss = 0.11012999
Iteration 26, loss = 0.10195304
Iteration 27, loss = 0.09471245
Iteration 28, loss = 0.08825061
Iteration 29, loss = 0.08248645
Iteration 30, loss = 0.07724026
Iteration 31, loss = 0.07256911
Iteration 32, loss = 0.06836424
Iteration 33, loss = 0.06452374
Iteration 34, loss = 0.06105773
Iteration 35, loss = 0.05794833
Iteration 36, loss = 0.05502530
Iteration 37, loss = 0.05242763
Iteration 38, loss = 0.05009550
Iteration 39, loss = 0.04785539
Iteration 40, loss = 0.04585528
Iteration 41, loss = 0.04395758
Iteration 42, loss = 0.04222462
Iteration 43, loss = 0.04064507
Iteration 44, loss = 0.03918146
Iteration 45, loss = 0.03781303
Iteration 46, loss = 0.03653425
Iteration 47, loss = 0.03530929
Iteration 48, loss = 0.03419333
Iteration 49, loss = 0.03317545
Iteration 50, loss = 0.03218947
Iteration 51, loss = 0.03129421
Iteration 52, loss = 0.03048456
Iteration 53, loss = 0.02970756
Iteration 54, loss = 0.02892356
Iteration 55, loss = 0.02814955
Iteration 56, loss = 0.02752609
Iteration 57, loss = 0.02688043
Iteration 58, loss = 0.02624388
Iteration 59, loss = 0.02575344
Iteration 60, loss = 0.02514577
Iteration 61, loss = 0.02467552
Iteration 62, loss = 0.02424587
Iteration 63, loss = 0.02376001
Iteration 64, loss = 0.02329412
Iteration 65, loss = 0.02289800
Iteration 66, loss = 0.02249902
Iteration 67, loss = 0.02211797
Iteration 68, loss = 0.02176556
Iteration 69, loss = 0.02142908
Iteration 70, loss = 0.02109621
Iteration 71, loss = 0.02075981
Iteration 72, loss = 0.02051210
Iteration 73, loss = 0.02021732
Iteration 74, loss = 0.01991625
Iteration 75, loss = 0.01969165
Iteration 76, loss = 0.01944514
Iteration 77, loss = 0.01920157
Iteration 78, loss = 0.01893259
Iteration 79, loss = 0.01875770
Iteration 80, loss = 0.01856565
Iteration 81, loss = 0.01832303
Iteration 82, loss = 0.01812247
Iteration 83, loss = 0.01794848
Iteration 84, loss = 0.01775534
Iteration 85, loss = 0.01758036
Iteration 86, loss = 0.01737894
Iteration 87, loss = 0.01721691
Iteration 88, loss = 0.01715196
Iteration 89, loss = 0.01693735
Iteration 90, loss = 0.01679241
Iteration 91, loss = 0.01664644
Iteration 92, loss = 0.01654226
Iteration 93, loss = 0.01635000
Iteration 94, loss = 0.01625676
Iteration 95, loss = 0.01615879
Iteration 96, loss = 0.01598388
Iteration 97, loss = 0.01592241
Iteration 98, loss = 0.01574589
Iteration 99, loss = 0.01565587
Iteration 100, loss = 0.01555106
Iteration 101, loss = 0.01551866
Iteration 102, loss = 0.01536904
Iteration 103, loss = 0.01523367
Iteration 104, loss = 0.01518439
Iteration 105, loss = 0.01507043
Iteration 106, loss = 0.01501798
Iteration 107, loss = 0.01488267
Iteration 108, loss = 0.01483752
Iteration 109, loss = 0.01474072
Iteration 110, loss = 0.01468572
Iteration 111, loss = 0.01459100
Iteration 112, loss = 0.01452478
Iteration 113, loss = 0.01447360
Iteration 114, loss = 0.01439060
Iteration 115, loss = 0.01431430
Iteration 116, loss = 0.01424185
Iteration 117, loss = 0.01419205
Iteration 118, loss = 0.01410455
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68246342
Iteration 2, loss = 0.63921678
Iteration 3, loss = 0.57313069
Iteration 4, loss = 0.49123536
Iteration 5, loss = 0.40822139
Iteration 6, loss = 0.33154519
Iteration 7, loss = 0.26693057
Iteration 8, loss = 0.21520287
Iteration 9, loss = 0.17496132
Iteration 10, loss = 0.14430972
Iteration 11, loss = 0.12094382
Iteration 12, loss = 0.10288069
Iteration 13, loss = 0.08891001
Iteration 14, loss = 0.07783402
Iteration 15, loss = 0.06925793
Iteration 16, loss = 0.06199688
Iteration 17, loss = 0.05617675
Iteration 18, loss = 0.05129213
Iteration 19, loss = 0.04738434
Iteration 20, loss = 0.04410170
Iteration 21, loss = 0.04122530
Iteration 22, loss = 0.03863153
Iteration 23, loss = 0.03657356
Iteration 24, loss = 0.03474789
Iteration 25, loss = 0.03310101
Iteration 26, loss = 0.03157866
Iteration 27, loss = 0.03013629
Iteration 28, loss = 0.02906146
Iteration 29, loss = 0.02813029
Iteration 30, loss = 0.02712122
Iteration 31, loss = 0.02638073
Iteration 32, loss = 0.02550486
Iteration 33, loss = 0.02485515
Iteration 34, loss = 0.02436543
Iteration 35, loss = 0.02382917
Iteration 36, loss = 0.02334160
Iteration 37, loss = 0.02302704
Iteration 38, loss = 0.02240929
Iteration 39, loss = 0.02201494
Iteration 40, loss = 0.02156836
Iteration 41, loss = 0.02133037
Iteration 42, loss = 0.02094714
Iteration 43, loss = 0.02080623
Iteration 44, loss = 0.02047895
Iteration 45, loss = 0.02027585
Iteration 46, loss = 0.01997996
Iteration 47, loss = 0.01973735
Iteration 48, loss = 0.01941360
Iteration 49, loss = 0.01936910
Iteration 50, loss = 0.01917001
Iteration 51, loss = 0.01915949
Iteration 52, loss = 0.01891991
Iteration 53, loss = 0.01876159
Iteration 54, loss = 0.01862471
Iteration 55, loss = 0.01850981
Iteration 56, loss = 0.01840544
Iteration 57, loss = 0.01812104
Iteration 58, loss = 0.01805066
Iteration 59, loss = 0.01795552
Iteration 60, loss = 0.01787803
Iteration 61, loss = 0.01787094
Iteration 62, loss = 0.01769069
Iteration 63, loss = 0.01755554
Iteration 64, loss = 0.01742358
Iteration 65, loss = 0.01731845
Iteration 66, loss = 0.01743570
Iteration 67, loss = 0.01728496
Iteration 68, loss = 0.01726495
Iteration 69, loss = 0.01719721
Iteration 70, loss = 0.01712624
Iteration 71, loss = 0.01711267
Iteration 72, loss = 0.01710115
Iteration 73, loss = 0.01703710
Iteration 74, loss = 0.01686927
Iteration 75, loss = 0.01681820
Iteration 76, loss = 0.01679842
Iteration 77, loss = 0.01670252
Iteration 78, loss = 0.01682094
Iteration 79, loss = 0.01676434
Iteration 80, loss = 0.01666863
Iteration 81, loss = 0.01657366
Iteration 82, loss = 0.01666947
Iteration 83, loss = 0.01639471
Iteration 84, loss = 0.01651901
Iteration 85, loss = 0.01644750
Iteration 86, loss = 0.01650870
Iteration 87, loss = 0.01640765
Iteration 88, loss = 0.01635640
Iteration 89, loss = 0.01671900
Iteration 90, loss = 0.01662914
Iteration 91, loss = 0.01648534
Iteration 92, loss = 0.01639970
Iteration 93, loss = 0.01624606
Iteration 94, loss = 0.01614070
Iteration 95, loss = 0.01607980
Iteration 96, loss = 0.01613367
Iteration 97, loss = 0.01610232
Iteration 98, loss = 0.01623872
Iteration 99, loss = 0.01617090
Iteration 100, loss = 0.01639074
Iteration 101, loss = 0.01607862
Iteration 102, loss = 0.01590843
Iteration 103, loss = 0.01585256
Iteration 104, loss = 0.01607767
Iteration 105, loss = 0.01580032
Iteration 106, loss = 0.01600661
Iteration 107, loss = 0.01588252
Iteration 108, loss = 0.01579636
Iteration 109, loss = 0.01595144
Iteration 110, loss = 0.01618526
Iteration 111, loss = 0.01593183
Iteration 112, loss = 0.01596668
Iteration 113, loss = 0.01575251
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69000240
Iteration 2, loss = 0.65372660
Iteration 3, loss = 0.60310771
Iteration 4, loss = 0.53697021
Iteration 5, loss = 0.46319141
Iteration 6, loss = 0.39120330
Iteration 7, loss = 0.32596104
Iteration 8, loss = 0.27001661
Iteration 9, loss = 0.22374014
Iteration 10, loss = 0.18621648
Iteration 11, loss = 0.15653424
Iteration 12, loss = 0.13315153
Iteration 13, loss = 0.11449185
Iteration 14, loss = 0.09969915
Iteration 15, loss = 0.08783970
Iteration 16, loss = 0.07816460
Iteration 17, loss = 0.07040323
Iteration 18, loss = 0.06384362
Iteration 19, loss = 0.05843686
Iteration 20, loss = 0.05385816
Iteration 21, loss = 0.05008086
Iteration 22, loss = 0.04662791
Iteration 23, loss = 0.04380908
Iteration 24, loss = 0.04127665
Iteration 25, loss = 0.03936583
Iteration 26, loss = 0.03729006
Iteration 27, loss = 0.03561684
Iteration 28, loss = 0.03404413
Iteration 29, loss = 0.03278270
Iteration 30, loss = 0.03160547
Iteration 31, loss = 0.03055874
Iteration 32, loss = 0.02959592
Iteration 33, loss = 0.02867837
Iteration 34, loss = 0.02790489
Iteration 35, loss = 0.02733941
Iteration 36, loss = 0.02662619
Iteration 37, loss = 0.02594018
Iteration 38, loss = 0.02557249
Iteration 39, loss = 0.02493468
Iteration 40, loss = 0.02451123
Iteration 41, loss = 0.02414078
Iteration 42, loss = 0.02363849
Iteration 43, loss = 0.02324465
Iteration 44, loss = 0.02301403
Iteration 45, loss = 0.02258502
Iteration 46, loss = 0.02233182
Iteration 47, loss = 0.02199852
Iteration 48, loss = 0.02187332
Iteration 49, loss = 0.02160845
Iteration 50, loss = 0.02142572
Iteration 51, loss = 0.02128860
Iteration 52, loss = 0.02095141
Iteration 53, loss = 0.02069424
Iteration 54, loss = 0.02062908
Iteration 55, loss = 0.02056049
Iteration 56, loss = 0.02039584
Iteration 57, loss = 0.02008708
Iteration 58, loss = 0.02013207
Iteration 59, loss = 0.01993028
Iteration 60, loss = 0.01974251
Iteration 61, loss = 0.01965356
Iteration 62, loss = 0.01974017
Iteration 63, loss = 0.01934373
Iteration 64, loss = 0.01936648
Iteration 65, loss = 0.01916616
Iteration 66, loss = 0.01921892
Iteration 67, loss = 0.01911444
Iteration 68, loss = 0.01893623
Iteration 69, loss = 0.01906572
Iteration 70, loss = 0.01883822
Iteration 71, loss = 0.01875193
Iteration 72, loss = 0.01862319
Iteration 73, loss = 0.01858960
Iteration 74, loss = 0.01847361
Iteration 75, loss = 0.01845293
Iteration 76, loss = 0.01838114
Iteration 77, loss = 0.01834196
Iteration 78, loss = 0.01826641
Iteration 79, loss = 0.01829089
Iteration 80, loss = 0.01814534
Iteration 81, loss = 0.01806314
Iteration 82, loss = 0.01805632
Iteration 83, loss = 0.01821792
Iteration 84, loss = 0.01808392
Iteration 85, loss = 0.01798292
Iteration 86, loss = 0.01798332
Iteration 87, loss = 0.01801912
Iteration 88, loss = 0.01791281
Iteration 89, loss = 0.01783892
Iteration 90, loss = 0.01779794
Iteration 91, loss = 0.01765865
Iteration 92, loss = 0.01776770
Iteration 93, loss = 0.01762398
Iteration 94, loss = 0.01761617
Iteration 95, loss = 0.01757428
Iteration 96, loss = 0.01762071
Iteration 97, loss = 0.01761921
Iteration 98, loss = 0.01754543
Iteration 99, loss = 0.01737561
Iteration 100, loss = 0.01761258
Iteration 101, loss = 0.01741446
Iteration 102, loss = 0.01754264
Iteration 103, loss = 0.01742944
Iteration 104, loss = 0.01744385
Iteration 105, loss = 0.01743682
Iteration 106, loss = 0.01737792
Iteration 107, loss = 0.01731404
Iteration 108, loss = 0.01719962
Iteration 109, loss = 0.01711492
Iteration 110, loss = 0.01715757
Iteration 111, loss = 0.01725286
Iteration 112, loss = 0.01722346
Iteration 113, loss = 0.01728147
Iteration 114, loss = 0.01751451
Iteration 115, loss = 0.01703021
Iteration 116, loss = 0.01727151
Iteration 117, loss = 0.01707435
Iteration 118, loss = 0.01717986
Iteration 119, loss = 0.01721003
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71956190
Iteration 2, loss = 0.66493083
Iteration 3, loss = 0.59406978
Iteration 4, loss = 0.50209294
Iteration 5, loss = 0.40702217
Iteration 6, loss = 0.32070835
Iteration 7, loss = 0.25099031
Iteration 8, loss = 0.19746260
Iteration 9, loss = 0.15824777
Iteration 10, loss = 0.12910446
Iteration 11, loss = 0.10772270
Iteration 12, loss = 0.09157357
Iteration 13, loss = 0.07918583
Iteration 14, loss = 0.06953120
Iteration 15, loss = 0.06176469
Iteration 16, loss = 0.05590889
Iteration 17, loss = 0.05094074
Iteration 18, loss = 0.04704815
Iteration 19, loss = 0.04371393
Iteration 20, loss = 0.04046449
Iteration 21, loss = 0.03813690
Iteration 22, loss = 0.03603849
Iteration 23, loss = 0.03431222
Iteration 24, loss = 0.03270907
Iteration 25, loss = 0.03124719
Iteration 26, loss = 0.03021751
Iteration 27, loss = 0.02908045
Iteration 28, loss = 0.02813157
Iteration 29, loss = 0.02730307
Iteration 30, loss = 0.02672528
Iteration 31, loss = 0.02587078
Iteration 32, loss = 0.02532405
Iteration 33, loss = 0.02467977
Iteration 34, loss = 0.02437800
Iteration 35, loss = 0.02358413
Iteration 36, loss = 0.02360446
Iteration 37, loss = 0.02299278
Iteration 38, loss = 0.02270449
Iteration 39, loss = 0.02226636
Iteration 40, loss = 0.02208579
Iteration 41, loss = 0.02163807
Iteration 42, loss = 0.02162854
Iteration 43, loss = 0.02128738
Iteration 44, loss = 0.02109888
Iteration 45, loss = 0.02080856
Iteration 46, loss = 0.02048825
Iteration 47, loss = 0.02062944
Iteration 48, loss = 0.02035238
Iteration 49, loss = 0.02000431
Iteration 50, loss = 0.02006156
Iteration 51, loss = 0.01985714
Iteration 52, loss = 0.01975510
Iteration 53, loss = 0.01967058
Iteration 54, loss = 0.01949590
Iteration 55, loss = 0.01923475
Iteration 56, loss = 0.01940317
Iteration 57, loss = 0.01928454
Iteration 58, loss = 0.01920683
Iteration 59, loss = 0.01897819
Iteration 60, loss = 0.01913910
Iteration 61, loss = 0.01884595
Iteration 62, loss = 0.01901414
Iteration 63, loss = 0.01878691
Iteration 64, loss = 0.01892136
Iteration 65, loss = 0.01869079
Iteration 66, loss = 0.01860079
Iteration 67, loss = 0.01845967
Iteration 68, loss = 0.01841185
Iteration 69, loss = 0.01845160
Iteration 70, loss = 0.01868380
Iteration 71, loss = 0.01818833
Iteration 72, loss = 0.01826954
Iteration 73, loss = 0.01829626
Iteration 74, loss = 0.01810018
Iteration 75, loss = 0.01813856
Iteration 76, loss = 0.01831629
Iteration 77, loss = 0.01812625
Iteration 78, loss = 0.01807845
Iteration 79, loss = 0.01803378
Iteration 80, loss = 0.01808818
Iteration 81, loss = 0.01794970
Iteration 82, loss = 0.01816934
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67321483
Iteration 2, loss = 0.59739358
Iteration 3, loss = 0.47947882
Iteration 4, loss = 0.36090769
Iteration 5, loss = 0.26551461
Iteration 6, loss = 0.19663347
Iteration 7, loss = 0.14967091
Iteration 8, loss = 0.11801482
Iteration 9, loss = 0.09606781
Iteration 10, loss = 0.08058445
Iteration 11, loss = 0.06925465
Iteration 12, loss = 0.06070295
Iteration 13, loss = 0.05428059
Iteration 14, loss = 0.04900033
Iteration 15, loss = 0.04515381
Iteration 16, loss = 0.04183785
Iteration 17, loss = 0.03904201
Iteration 18, loss = 0.03680630
Iteration 19, loss = 0.03512430
Iteration 20, loss = 0.03357138
Iteration 21, loss = 0.03221877
Iteration 22, loss = 0.03092868
Iteration 23, loss = 0.02992510
Iteration 24, loss = 0.02905832
Iteration 25, loss = 0.02809128
Iteration 26, loss = 0.02774254
Iteration 27, loss = 0.02699782
Iteration 28, loss = 0.02646870
Iteration 29, loss = 0.02595486
Iteration 30, loss = 0.02550760
Iteration 31, loss = 0.02517852
Iteration 32, loss = 0.02475314
Iteration 33, loss = 0.02481465
Iteration 34, loss = 0.02396923
Iteration 35, loss = 0.02420042
Iteration 36, loss = 0.02393571
Iteration 37, loss = 0.02370721
Iteration 38, loss = 0.02333564
Iteration 39, loss = 0.02335477
Iteration 40, loss = 0.02339158
Iteration 41, loss = 0.02270295
Iteration 42, loss = 0.02283667
Iteration 43, loss = 0.02248070
Iteration 44, loss = 0.02228921
Iteration 45, loss = 0.02240052
Iteration 46, loss = 0.02244701
Iteration 47, loss = 0.02207304
Iteration 48, loss = 0.02196471
Iteration 49, loss = 0.02194430
Iteration 50, loss = 0.02182879
Iteration 51, loss = 0.02176656
Iteration 52, loss = 0.02161417
Iteration 53, loss = 0.02176980
Iteration 54, loss = 0.02175092
Iteration 55, loss = 0.02133302
Iteration 56, loss = 0.02144452
Iteration 57, loss = 0.02107854
Iteration 58, loss = 0.02134165
Iteration 59, loss = 0.02117666
Iteration 60, loss = 0.02133821
Iteration 61, loss = 0.02097309
Iteration 62, loss = 0.02097943
Iteration 63, loss = 0.02105271
Iteration 64, loss = 0.02099003
Iteration 65, loss = 0.02102350
Iteration 66, loss = 0.02105605
Iteration 67, loss = 0.02088617
Iteration 68, loss = 0.02093617
Iteration 69, loss = 0.02082095
Iteration 70, loss = 0.02084110
Iteration 71, loss = 0.02095561
Iteration 72, loss = 0.02105452
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.66993014
Iteration 2, loss = 0.65654513
Iteration 3, loss = 0.64502782
Iteration 4, loss = 0.63345197
Iteration 5, loss = 0.62089835
Iteration 6, loss = 0.60706893
Iteration 7, loss = 0.59180005
Iteration 8, loss = 0.57500717
Iteration 9, loss = 0.55654684
Iteration 10, loss = 0.53687052
Iteration 11, loss = 0.51610987
Iteration 12, loss = 0.49445117
Iteration 13, loss = 0.47213856
Iteration 14, loss = 0.44952937
Iteration 15, loss = 0.42686473
Iteration 16, loss = 0.40423309
Iteration 17, loss = 0.38192837
Iteration 18, loss = 0.35994632
Iteration 19, loss = 0.33844097
Iteration 20, loss = 0.31815970
Iteration 21, loss = 0.29829461
Iteration 22, loss = 0.27947026
Iteration 23, loss = 0.26176516
Iteration 24, loss = 0.24502765
Iteration 25, loss = 0.22909313
Iteration 26, loss = 0.21418182
Iteration 27, loss = 0.20047023
Iteration 28, loss = 0.18755232
Iteration 29, loss = 0.17550091
Iteration 30, loss = 0.16436906
Iteration 31, loss = 0.15397415
Iteration 32, loss = 0.14444807
Iteration 33, loss = 0.13557749
Iteration 34, loss = 0.12746050
Iteration 35, loss = 0.11985226
Iteration 36, loss = 0.11291174
Iteration 37, loss = 0.10644237
Iteration 38, loss = 0.10047498
Iteration 39, loss = 0.09499155
Iteration 40, loss = 0.08989560
Iteration 41, loss = 0.08519082
Iteration 42, loss = 0.08079808
Iteration 43, loss = 0.07675069
Iteration 44, loss = 0.07300716
Iteration 45, loss = 0.06949411
Iteration 46, loss = 0.06626080
Iteration 47, loss = 0.06325722
Iteration 48, loss = 0.06041636
Iteration 49, loss = 0.05779664
Iteration 50, loss = 0.05534370
Iteration 51, loss = 0.05305415
Iteration 52, loss = 0.05092459
Iteration 53, loss = 0.04893597
Iteration 54, loss = 0.04702838
Iteration 55, loss = 0.04527364
Iteration 56, loss = 0.04359922
Iteration 57, loss = 0.04202678
Iteration 58, loss = 0.04055132
Iteration 59, loss = 0.03916118
Iteration 60, loss = 0.03785413
Iteration 61, loss = 0.03660575
Iteration 62, loss = 0.03544398
Iteration 63, loss = 0.03434949
Iteration 64, loss = 0.03328986
Iteration 65, loss = 0.03229973
Iteration 66, loss = 0.03133965
Iteration 67, loss = 0.03042907
Iteration 68, loss = 0.02958983
Iteration 69, loss = 0.02874346
Iteration 70, loss = 0.02797786
Iteration 71, loss = 0.02722795
Iteration 72, loss = 0.02653643
Iteration 73, loss = 0.02583732
Iteration 74, loss = 0.02516904
Iteration 75, loss = 0.02453599
Iteration 76, loss = 0.02393342
Iteration 77, loss = 0.02339996
Iteration 78, loss = 0.02283449
Iteration 79, loss = 0.02232913
Iteration 80, loss = 0.02181536
Iteration 81, loss = 0.02134128
Iteration 82, loss = 0.02087928
Iteration 83, loss = 0.02044667
Iteration 84, loss = 0.02000691
Iteration 85, loss = 0.01963948
Iteration 86, loss = 0.01922897
Iteration 87, loss = 0.01885285
Iteration 88, loss = 0.01851735
Iteration 89, loss = 0.01813150
Iteration 90, loss = 0.01779470
Iteration 91, loss = 0.01747867
Iteration 92, loss = 0.01714620
Iteration 93, loss = 0.01682510
Iteration 94, loss = 0.01653387
Iteration 95, loss = 0.01625384
Iteration 96, loss = 0.01597076
Iteration 97, loss = 0.01573789
Iteration 98, loss = 0.01546176
Iteration 99, loss = 0.01522274
Iteration 100, loss = 0.01499009
Iteration 101, loss = 0.01475718
Iteration 102, loss = 0.01451978
Iteration 103, loss = 0.01431093
Iteration 104, loss = 0.01409250
Iteration 105, loss = 0.01389263
Iteration 106, loss = 0.01369621
Iteration 107, loss = 0.01349344
Iteration 108, loss = 0.01331121
Iteration 109, loss = 0.01311838
Iteration 110, loss = 0.01294011
Iteration 111, loss = 0.01277654
Iteration 112, loss = 0.01263774
Iteration 113, loss = 0.01244078
Iteration 114, loss = 0.01228216
Iteration 115, loss = 0.01213928
Iteration 116, loss = 0.01198250
Iteration 117, loss = 0.01183114
Iteration 118, loss = 0.01168640
Iteration 119, loss = 0.01156834
Iteration 120, loss = 0.01142203
Iteration 121, loss = 0.01133877
Iteration 122, loss = 0.01116671
Iteration 123, loss = 0.01105234
Iteration 124, loss = 0.01092255
Iteration 125, loss = 0.01082415
Iteration 126, loss = 0.01068978
Iteration 127, loss = 0.01061288
Iteration 128, loss = 0.01047540
Iteration 129, loss = 0.01036737
Iteration 130, loss = 0.01026319
Iteration 131, loss = 0.01016539
Iteration 132, loss = 0.01007042
Iteration 133, loss = 0.01001679
Iteration 134, loss = 0.00987913
Iteration 135, loss = 0.00978917
Iteration 136, loss = 0.00971374
Iteration 137, loss = 0.00962146
Iteration 138, loss = 0.00953494
Iteration 139, loss = 0.00943510
Iteration 140, loss = 0.00935263
Iteration 141, loss = 0.00927024
Iteration 142, loss = 0.00920415
Iteration 143, loss = 0.00912663
Iteration 144, loss = 0.00903188
Iteration 145, loss = 0.00896452
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69933800
Iteration 2, loss = 0.63440817
Iteration 3, loss = 0.53706714
Iteration 4, loss = 0.42292598
Iteration 5, loss = 0.32057204
Iteration 6, loss = 0.24017418
Iteration 7, loss = 0.18241511
Iteration 8, loss = 0.14185270
Iteration 9, loss = 0.11355897
Iteration 10, loss = 0.09356976
Iteration 11, loss = 0.07921766
Iteration 12, loss = 0.06824571
Iteration 13, loss = 0.06046913
Iteration 14, loss = 0.05398148
Iteration 15, loss = 0.04903357
Iteration 16, loss = 0.04488052
Iteration 17, loss = 0.04162511
Iteration 18, loss = 0.03892282
Iteration 19, loss = 0.03640069
Iteration 20, loss = 0.03475138
Iteration 21, loss = 0.03315271
Iteration 22, loss = 0.03172478
Iteration 23, loss = 0.03032395
Iteration 24, loss = 0.02928238
Iteration 25, loss = 0.02837477
Iteration 26, loss = 0.02765344
Iteration 27, loss = 0.02711253
Iteration 28, loss = 0.02615447
Iteration 29, loss = 0.02565354
Iteration 30, loss = 0.02494897
Iteration 31, loss = 0.02452918
Iteration 32, loss = 0.02424267
Iteration 33, loss = 0.02362247
Iteration 34, loss = 0.02358428
Iteration 35, loss = 0.02319610
Iteration 36, loss = 0.02280527
Iteration 37, loss = 0.02267003
Iteration 38, loss = 0.02234065
Iteration 39, loss = 0.02209800
Iteration 40, loss = 0.02183166
Iteration 41, loss = 0.02166734
Iteration 42, loss = 0.02155346
Iteration 43, loss = 0.02128333
Iteration 44, loss = 0.02153783
Iteration 45, loss = 0.02136032
Iteration 46, loss = 0.02093201
Iteration 47, loss = 0.02067629
Iteration 48, loss = 0.02054702
Iteration 49, loss = 0.02051104
Iteration 50, loss = 0.02053842
Iteration 51, loss = 0.02025694
Iteration 52, loss = 0.02033303
Iteration 53, loss = 0.02013525
Iteration 54, loss = 0.02013902
Iteration 55, loss = 0.01991337
Iteration 56, loss = 0.02003020
Iteration 57, loss = 0.01973884
Iteration 58, loss = 0.01981427
Iteration 59, loss = 0.01984902
Iteration 60, loss = 0.01976761
Iteration 61, loss = 0.01964123
Iteration 62, loss = 0.01944242
Iteration 63, loss = 0.01937468
Iteration 64, loss = 0.01934311
Iteration 65, loss = 0.01939239
Iteration 66, loss = 0.01929846
Iteration 67, loss = 0.01926171
Iteration 68, loss = 0.01907727
Iteration 69, loss = 0.01915564
Iteration 70, loss = 0.01917415
Iteration 71, loss = 0.01905486
Iteration 72, loss = 0.01906344
Iteration 73, loss = 0.01911937
Iteration 74, loss = 0.01899907
Iteration 75, loss = 0.01905058
Iteration 76, loss = 0.01880799
Iteration 77, loss = 0.01882553
Iteration 78, loss = 0.01890775
Iteration 79, loss = 0.01871437
Iteration 80, loss = 0.01887102
Iteration 81, loss = 0.01876485
Iteration 82, loss = 0.01870175
Iteration 83, loss = 0.01876498
Iteration 84, loss = 0.01868816
Iteration 85, loss = 0.01898836
Iteration 86, loss = 0.01882890
Iteration 87, loss = 0.01871367
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67476343
Iteration 2, loss = 0.59204383
Iteration 3, loss = 0.48082219
Iteration 4, loss = 0.36913139
Iteration 5, loss = 0.27616169
Iteration 6, loss = 0.20727802
Iteration 7, loss = 0.15895827
Iteration 8, loss = 0.12570669
Iteration 9, loss = 0.10251193
Iteration 10, loss = 0.08652639
Iteration 11, loss = 0.07463401
Iteration 12, loss = 0.06585138
Iteration 13, loss = 0.05883418
Iteration 14, loss = 0.05383341
Iteration 15, loss = 0.04934938
Iteration 16, loss = 0.04602568
Iteration 17, loss = 0.04311607
Iteration 18, loss = 0.04094983
Iteration 19, loss = 0.03886092
Iteration 20, loss = 0.03721366
Iteration 21, loss = 0.03609706
Iteration 22, loss = 0.03438312
Iteration 23, loss = 0.03371968
Iteration 24, loss = 0.03279909
Iteration 25, loss = 0.03199183
Iteration 26, loss = 0.03113603
Iteration 27, loss = 0.03043298
Iteration 28, loss = 0.03005473
Iteration 29, loss = 0.02951772
Iteration 30, loss = 0.02903605
Iteration 31, loss = 0.02893951
Iteration 32, loss = 0.02817556
Iteration 33, loss = 0.02795878
Iteration 34, loss = 0.02791539
Iteration 35, loss = 0.02735267
Iteration 36, loss = 0.02705244
Iteration 37, loss = 0.02690026
Iteration 38, loss = 0.02688814
Iteration 39, loss = 0.02646764
Iteration 40, loss = 0.02648666
Iteration 41, loss = 0.02579045
Iteration 42, loss = 0.02587865
Iteration 43, loss = 0.02590975
Iteration 44, loss = 0.02557641
Iteration 45, loss = 0.02559273
Iteration 46, loss = 0.02574417
Iteration 47, loss = 0.02548870
Iteration 48, loss = 0.02538160
Iteration 49, loss = 0.02526010
Iteration 50, loss = 0.02500088
Iteration 51, loss = 0.02488729
Iteration 52, loss = 0.02492859
Iteration 53, loss = 0.02488793
Iteration 54, loss = 0.02472482
Iteration 55, loss = 0.02492020
Iteration 56, loss = 0.02475988
Iteration 57, loss = 0.02469182
Iteration 58, loss = 0.02458531
Iteration 59, loss = 0.02478050
Iteration 60, loss = 0.02444398
Iteration 61, loss = 0.02408857
Iteration 62, loss = 0.02440718
Iteration 63, loss = 0.02447574
Iteration 64, loss = 0.02433023
Iteration 65, loss = 0.02436224
Iteration 66, loss = 0.02425611
Iteration 67, loss = 0.02440663
Iteration 68, loss = 0.02418007
Iteration 69, loss = 0.02373169
Iteration 70, loss = 0.02404983
Iteration 71, loss = 0.02405792
Iteration 72, loss = 0.02400330
Iteration 73, loss = 0.02430276
Iteration 74, loss = 0.02416731
Iteration 75, loss = 0.02385474
Iteration 76, loss = 0.02407374
Iteration 77, loss = 0.02366730
Iteration 78, loss = 0.02366848
Iteration 79, loss = 0.02378420
Iteration 80, loss = 0.02379881
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70124995
Iteration 2, loss = 0.63246906
Iteration 3, loss = 0.52429991
Iteration 4, loss = 0.40384510
Iteration 5, loss = 0.29884172
Iteration 6, loss = 0.22100070
Iteration 7, loss = 0.16692008
Iteration 8, loss = 0.13044416
Iteration 9, loss = 0.10556590
Iteration 10, loss = 0.08825863
Iteration 11, loss = 0.07575094
Iteration 12, loss = 0.06633823
Iteration 13, loss = 0.05929684
Iteration 14, loss = 0.05361734
Iteration 15, loss = 0.04930117
Iteration 16, loss = 0.04570118
Iteration 17, loss = 0.04293929
Iteration 18, loss = 0.04030081
Iteration 19, loss = 0.03855468
Iteration 20, loss = 0.03688146
Iteration 21, loss = 0.03531430
Iteration 22, loss = 0.03414973
Iteration 23, loss = 0.03283925
Iteration 24, loss = 0.03223582
Iteration 25, loss = 0.03136790
Iteration 26, loss = 0.03051685
Iteration 27, loss = 0.02995128
Iteration 28, loss = 0.02936031
Iteration 29, loss = 0.02885093
Iteration 30, loss = 0.02831862
Iteration 31, loss = 0.02792950
Iteration 32, loss = 0.02752579
Iteration 33, loss = 0.02709489
Iteration 34, loss = 0.02689552
Iteration 35, loss = 0.02682008
Iteration 36, loss = 0.02638622
Iteration 37, loss = 0.02589328
Iteration 38, loss = 0.02589694
Iteration 39, loss = 0.02583947
Iteration 40, loss = 0.02570369
Iteration 41, loss = 0.02551263
Iteration 42, loss = 0.02521156
Iteration 43, loss = 0.02514160
Iteration 44, loss = 0.02474680
Iteration 45, loss = 0.02500844
Iteration 46, loss = 0.02509690
Iteration 47, loss = 0.02479879
Iteration 48, loss = 0.02432195
Iteration 49, loss = 0.02442186
Iteration 50, loss = 0.02416722
Iteration 51, loss = 0.02414524
Iteration 52, loss = 0.02394194
Iteration 53, loss = 0.02422956
Iteration 54, loss = 0.02408947
Iteration 55, loss = 0.02396611
Iteration 56, loss = 0.02393857
Iteration 57, loss = 0.02389019
Iteration 58, loss = 0.02378828
Iteration 59, loss = 0.02382877
Iteration 60, loss = 0.02349021
Iteration 61, loss = 0.02367843
Iteration 62, loss = 0.02363281
Iteration 63, loss = 0.02342623
Iteration 64, loss = 0.02366035
Iteration 65, loss = 0.02331388
Iteration 66, loss = 0.02331264
Iteration 67, loss = 0.02337510
Iteration 68, loss = 0.02349996
Iteration 69, loss = 0.02359903
Iteration 70, loss = 0.02337285
Iteration 71, loss = 0.02345579
Iteration 72, loss = 0.02337051
Iteration 73, loss = 0.02331100
Iteration 74, loss = 0.02321086
Iteration 75, loss = 0.02309372
Iteration 76, loss = 0.02322476
Iteration 77, loss = 0.02329469
Iteration 78, loss = 0.02335113
Iteration 79, loss = 0.02341553
Iteration 80, loss = 0.02302939
Iteration 81, loss = 0.02305848
Iteration 82, loss = 0.02311065
Iteration 83, loss = 0.02334604
Iteration 84, loss = 0.02311674
Iteration 85, loss = 0.02310237
Iteration 86, loss = 0.02314848
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69589486
Iteration 2, loss = 0.67020982
Iteration 3, loss = 0.63894529
Iteration 4, loss = 0.60073358
Iteration 5, loss = 0.55550624
Iteration 6, loss = 0.50501292
Iteration 7, loss = 0.45273123
Iteration 8, loss = 0.40068868
Iteration 9, loss = 0.35155327
Iteration 10, loss = 0.30666728
Iteration 11, loss = 0.26709589
Iteration 12, loss = 0.23237718
Iteration 13, loss = 0.20268948
Iteration 14, loss = 0.17744633
Iteration 15, loss = 0.15628887
Iteration 16, loss = 0.13816138
Iteration 17, loss = 0.12311075
Iteration 18, loss = 0.11035602
Iteration 19, loss = 0.09945133
Iteration 20, loss = 0.09009895
Iteration 21, loss = 0.08242399
Iteration 22, loss = 0.07556780
Iteration 23, loss = 0.06966641
Iteration 24, loss = 0.06435787
Iteration 25, loss = 0.05966190
Iteration 26, loss = 0.05566067
Iteration 27, loss = 0.05214690
Iteration 28, loss = 0.04904123
Iteration 29, loss = 0.04626282
Iteration 30, loss = 0.04373313
Iteration 31, loss = 0.04138455
Iteration 32, loss = 0.03955081
Iteration 33, loss = 0.03791228
Iteration 34, loss = 0.03640329
Iteration 35, loss = 0.03495245
Iteration 36, loss = 0.03357643
Iteration 37, loss = 0.03240347
Iteration 38, loss = 0.03123187
Iteration 39, loss = 0.03008047
Iteration 40, loss = 0.02908187
Iteration 41, loss = 0.02818429
Iteration 42, loss = 0.02741492
Iteration 43, loss = 0.02653597
Iteration 44, loss = 0.02589817
Iteration 45, loss = 0.02513493
Iteration 46, loss = 0.02451358
Iteration 47, loss = 0.02394338
Iteration 48, loss = 0.02337364
Iteration 49, loss = 0.02293313
Iteration 50, loss = 0.02245890
Iteration 51, loss = 0.02195879
Iteration 52, loss = 0.02174757
Iteration 53, loss = 0.02126147
Iteration 54, loss = 0.02092322
Iteration 55, loss = 0.02059599
Iteration 56, loss = 0.02050375
Iteration 57, loss = 0.02033018
Iteration 58, loss = 0.01991798
Iteration 59, loss = 0.01975127
Iteration 60, loss = 0.01951130
Iteration 61, loss = 0.01917691
Iteration 62, loss = 0.01893447
Iteration 63, loss = 0.01851644
Iteration 64, loss = 0.01833193
Iteration 65, loss = 0.01814277
Iteration 66, loss = 0.01788841
Iteration 67, loss = 0.01771137
Iteration 68, loss = 0.01755837
Iteration 69, loss = 0.01738198
Iteration 70, loss = 0.01739032
Iteration 71, loss = 0.01719747
Iteration 72, loss = 0.01702826
Iteration 73, loss = 0.01679769
Iteration 74, loss = 0.01646298
Iteration 75, loss = 0.01665236
Iteration 76, loss = 0.01677704
Iteration 77, loss = 0.01664024
Iteration 78, loss = 0.01644831
Iteration 79, loss = 0.01615273
Iteration 80, loss = 0.01596673
Iteration 81, loss = 0.01599616
Iteration 82, loss = 0.01591288
Iteration 83, loss = 0.01580322
Iteration 84, loss = 0.01562586
Iteration 85, loss = 0.01545847
Iteration 86, loss = 0.01550955
Iteration 87, loss = 0.01550690
Iteration 88, loss = 0.01537659
Iteration 89, loss = 0.01528204
Iteration 90, loss = 0.01524391
Iteration 91, loss = 0.01507816
Iteration 92, loss = 0.01503023
Iteration 93, loss = 0.01493695
Iteration 94, loss = 0.01483251
Iteration 95, loss = 0.01481918
Iteration 96, loss = 0.01467656
Iteration 97, loss = 0.01476014
Iteration 98, loss = 0.01461018
Iteration 99, loss = 0.01509915
Iteration 100, loss = 0.01491671
Iteration 101, loss = 0.01510928
Iteration 102, loss = 0.01499954
Iteration 103, loss = 0.01485028
Iteration 104, loss = 0.01458096
Iteration 105, loss = 0.01439347
Iteration 106, loss = 0.01434324
Iteration 107, loss = 0.01416541
Iteration 108, loss = 0.01414291
Iteration 109, loss = 0.01416059
Iteration 110, loss = 0.01403743
Iteration 111, loss = 0.01392606
Iteration 112, loss = 0.01410424
Iteration 113, loss = 0.01414552
Iteration 114, loss = 0.01424896
Iteration 115, loss = 0.01416642
Iteration 116, loss = 0.01409732
Iteration 117, loss = 0.01393997
Iteration 118, loss = 0.01424921
Iteration 119, loss = 0.01392313
Iteration 120, loss = 0.01392517
Iteration 121, loss = 0.01390941
Iteration 122, loss = 0.01368435
Iteration 123, loss = 0.01358184
Iteration 124, loss = 0.01354472
Iteration 125, loss = 0.01355166
Iteration 126, loss = 0.01350524
Iteration 127, loss = 0.01345225
Iteration 128, loss = 0.01349693
Iteration 129, loss = 0.01347082
Iteration 130, loss = 0.01339015
Iteration 131, loss = 0.01350348
Iteration 132, loss = 0.01371237
Iteration 133, loss = 0.01352187
Iteration 134, loss = 0.01337100
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71002256
Iteration 2, loss = 0.62799905
Iteration 3, loss = 0.52151472
Iteration 4, loss = 0.40451855
Iteration 5, loss = 0.30539808
Iteration 6, loss = 0.22967783
Iteration 7, loss = 0.17568513
Iteration 8, loss = 0.13798010
Iteration 9, loss = 0.11136911
Iteration 10, loss = 0.09277351
Iteration 11, loss = 0.07890389
Iteration 12, loss = 0.06855673
Iteration 13, loss = 0.06087068
Iteration 14, loss = 0.05469010
Iteration 15, loss = 0.04964804
Iteration 16, loss = 0.04567206
Iteration 17, loss = 0.04242136
Iteration 18, loss = 0.03976408
Iteration 19, loss = 0.03738216
Iteration 20, loss = 0.03578597
Iteration 21, loss = 0.03397198
Iteration 22, loss = 0.03249087
Iteration 23, loss = 0.03139544
Iteration 24, loss = 0.03021622
Iteration 25, loss = 0.02923788
Iteration 26, loss = 0.02869464
Iteration 27, loss = 0.02794162
Iteration 28, loss = 0.02733745
Iteration 29, loss = 0.02653799
Iteration 30, loss = 0.02609703
Iteration 31, loss = 0.02545007
Iteration 32, loss = 0.02515726
Iteration 33, loss = 0.02483731
Iteration 34, loss = 0.02438771
Iteration 35, loss = 0.02398830
Iteration 36, loss = 0.02396628
Iteration 37, loss = 0.02354300
Iteration 38, loss = 0.02334207
Iteration 39, loss = 0.02314030
Iteration 40, loss = 0.02274360
Iteration 41, loss = 0.02289728
Iteration 42, loss = 0.02245629
Iteration 43, loss = 0.02237845
Iteration 44, loss = 0.02201253
Iteration 45, loss = 0.02199666
Iteration 46, loss = 0.02193832
Iteration 47, loss = 0.02183139
Iteration 48, loss = 0.02147694
Iteration 49, loss = 0.02147893
Iteration 50, loss = 0.02116921
Iteration 51, loss = 0.02117959
Iteration 52, loss = 0.02130578
Iteration 53, loss = 0.02111423
Iteration 54, loss = 0.02128146
Iteration 55, loss = 0.02066386
Iteration 56, loss = 0.02094821
Iteration 57, loss = 0.02046627
Iteration 58, loss = 0.02102104
Iteration 59, loss = 0.02079322
Iteration 60, loss = 0.02078456
Iteration 61, loss = 0.02040572
Iteration 62, loss = 0.02065072
Iteration 63, loss = 0.02042030
Iteration 64, loss = 0.02071623
Iteration 65, loss = 0.02021809
Iteration 66, loss = 0.02023238
Iteration 67, loss = 0.02025837
Iteration 68, loss = 0.02033487
Iteration 69, loss = 0.02018416
Iteration 70, loss = 0.01995542
Iteration 71, loss = 0.02007475
Iteration 72, loss = 0.02016745
Iteration 73, loss = 0.02011787
Iteration 74, loss = 0.02002389
Iteration 75, loss = 0.02014150
Iteration 76, loss = 0.02015349
Iteration 77, loss = 0.01994284
Iteration 78, loss = 0.01981337
Iteration 79, loss = 0.01970364
Iteration 80, loss = 0.02016572
Iteration 81, loss = 0.02008166
Iteration 82, loss = 0.01985963
Iteration 83, loss = 0.01998092
Iteration 84, loss = 0.01971690
Iteration 85, loss = 0.01969167
Iteration 86, loss = 0.01988430
Iteration 87, loss = 0.01943779
Iteration 88, loss = 0.01965275
Iteration 89, loss = 0.01974514
Iteration 90, loss = 0.01956325
Iteration 91, loss = 0.01930479
Iteration 92, loss = 0.01957681
Iteration 93, loss = 0.01935874
Iteration 94, loss = 0.01967734
Iteration 95, loss = 0.01961939
Iteration 96, loss = 0.01979710
Iteration 97, loss = 0.01964947
Iteration 98, loss = 0.01959340
Iteration 99, loss = 0.01939426
Iteration 100, loss = 0.01974045
Iteration 101, loss = 0.01937782
Iteration 102, loss = 0.01945516
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68390601
Iteration 2, loss = 0.65033870
Iteration 3, loss = 0.60813713
Iteration 4, loss = 0.55473238
Iteration 5, loss = 0.49452382
Iteration 6, loss = 0.43326378
Iteration 7, loss = 0.37483678
Iteration 8, loss = 0.32120162
Iteration 9, loss = 0.27383708
Iteration 10, loss = 0.23286809
Iteration 11, loss = 0.19847726
Iteration 12, loss = 0.16976880
Iteration 13, loss = 0.14632041
Iteration 14, loss = 0.12693704
Iteration 15, loss = 0.11122311
Iteration 16, loss = 0.09833112
Iteration 17, loss = 0.08748762
Iteration 18, loss = 0.07866647
Iteration 19, loss = 0.07111419
Iteration 20, loss = 0.06497633
Iteration 21, loss = 0.05952764
Iteration 22, loss = 0.05494132
Iteration 23, loss = 0.05106176
Iteration 24, loss = 0.04747104
Iteration 25, loss = 0.04466595
Iteration 26, loss = 0.04208083
Iteration 27, loss = 0.03968179
Iteration 28, loss = 0.03770403
Iteration 29, loss = 0.03585492
Iteration 30, loss = 0.03434637
Iteration 31, loss = 0.03284628
Iteration 32, loss = 0.03148861
Iteration 33, loss = 0.03032032
Iteration 34, loss = 0.02929568
Iteration 35, loss = 0.02837401
Iteration 36, loss = 0.02741189
Iteration 37, loss = 0.02669327
Iteration 38, loss = 0.02595480
Iteration 39, loss = 0.02523280
Iteration 40, loss = 0.02463156
Iteration 41, loss = 0.02400992
Iteration 42, loss = 0.02354810
Iteration 43, loss = 0.02304943
Iteration 44, loss = 0.02252922
Iteration 45, loss = 0.02206168
Iteration 46, loss = 0.02168840
Iteration 47, loss = 0.02132607
Iteration 48, loss = 0.02098785
Iteration 49, loss = 0.02071610
Iteration 50, loss = 0.02034546
Iteration 51, loss = 0.02002869
Iteration 52, loss = 0.01988857
Iteration 53, loss = 0.01972841
Iteration 54, loss = 0.01934971
Iteration 55, loss = 0.01924249
Iteration 56, loss = 0.01900628
Iteration 57, loss = 0.01889820
Iteration 58, loss = 0.01859654
Iteration 59, loss = 0.01844341
Iteration 60, loss = 0.01828160
Iteration 61, loss = 0.01816300
Iteration 62, loss = 0.01805816
Iteration 63, loss = 0.01789386
Iteration 64, loss = 0.01770867
Iteration 65, loss = 0.01752237
Iteration 66, loss = 0.01738041
Iteration 67, loss = 0.01738013
Iteration 68, loss = 0.01712601
Iteration 69, loss = 0.01700931
Iteration 70, loss = 0.01693158
Iteration 71, loss = 0.01693257
Iteration 72, loss = 0.01688979
Iteration 73, loss = 0.01673967
Iteration 74, loss = 0.01664859
Iteration 75, loss = 0.01655856
Iteration 76, loss = 0.01657647
Iteration 77, loss = 0.01634498
Iteration 78, loss = 0.01622283
Iteration 79, loss = 0.01623867
Iteration 80, loss = 0.01615183
Iteration 81, loss = 0.01611808
Iteration 82, loss = 0.01601816
Iteration 83, loss = 0.01595513
Iteration 84, loss = 0.01599441
Iteration 85, loss = 0.01609095
Iteration 86, loss = 0.01595840
Iteration 87, loss = 0.01583326
Iteration 88, loss = 0.01579201
Iteration 89, loss = 0.01566573
Iteration 90, loss = 0.01562355
Iteration 91, loss = 0.01558084
Iteration 92, loss = 0.01555400
Iteration 93, loss = 0.01565584
Iteration 94, loss = 0.01549865
Iteration 95, loss = 0.01547878
Iteration 96, loss = 0.01540793
Iteration 97, loss = 0.01541229
Iteration 98, loss = 0.01530797
Iteration 99, loss = 0.01535371
Iteration 100, loss = 0.01536180
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67783500
Iteration 2, loss = 0.60782151
Iteration 3, loss = 0.49731193
Iteration 4, loss = 0.38187105
Iteration 5, loss = 0.28331017
Iteration 6, loss = 0.20980159
Iteration 7, loss = 0.15914240
Iteration 8, loss = 0.12454540
Iteration 9, loss = 0.10087046
Iteration 10, loss = 0.08431902
Iteration 11, loss = 0.07202362
Iteration 12, loss = 0.06302404
Iteration 13, loss = 0.05603625
Iteration 14, loss = 0.05082290
Iteration 15, loss = 0.04626809
Iteration 16, loss = 0.04293446
Iteration 17, loss = 0.03997736
Iteration 18, loss = 0.03776584
Iteration 19, loss = 0.03589921
Iteration 20, loss = 0.03413467
Iteration 21, loss = 0.03280176
Iteration 22, loss = 0.03154370
Iteration 23, loss = 0.03064192
Iteration 24, loss = 0.02956818
Iteration 25, loss = 0.02869138
Iteration 26, loss = 0.02801687
Iteration 27, loss = 0.02752379
Iteration 28, loss = 0.02689977
Iteration 29, loss = 0.02643693
Iteration 30, loss = 0.02611966
Iteration 31, loss = 0.02560575
Iteration 32, loss = 0.02524164
Iteration 33, loss = 0.02514199
Iteration 34, loss = 0.02466689
Iteration 35, loss = 0.02449626
Iteration 36, loss = 0.02417458
Iteration 37, loss = 0.02407246
Iteration 38, loss = 0.02385369
Iteration 39, loss = 0.02377834
Iteration 40, loss = 0.02345119
Iteration 41, loss = 0.02307971
Iteration 42, loss = 0.02288211
Iteration 43, loss = 0.02289812
Iteration 44, loss = 0.02262303
Iteration 45, loss = 0.02255153
Iteration 46, loss = 0.02279086
Iteration 47, loss = 0.02210872
Iteration 48, loss = 0.02229766
Iteration 49, loss = 0.02221023
Iteration 50, loss = 0.02213215
Iteration 51, loss = 0.02209283
Iteration 52, loss = 0.02201299
Iteration 53, loss = 0.02214353
Iteration 54, loss = 0.02174332
Iteration 55, loss = 0.02159720
Iteration 56, loss = 0.02171969
Iteration 57, loss = 0.02165086
Iteration 58, loss = 0.02187695
Iteration 59, loss = 0.02157746
Iteration 60, loss = 0.02153214
Iteration 61, loss = 0.02139745
Iteration 62, loss = 0.02161787
Iteration 63, loss = 0.02121933
Iteration 64, loss = 0.02124070
Iteration 65, loss = 0.02119468
Iteration 66, loss = 0.02125225
Iteration 67, loss = 0.02139299
Iteration 68, loss = 0.02125982
Iteration 69, loss = 0.02114493
Iteration 70, loss = 0.02120047
Iteration 71, loss = 0.02113764
Iteration 72, loss = 0.02112029
Iteration 73, loss = 0.02083822
Iteration 74, loss = 0.02103500
Iteration 75, loss = 0.02101400
Iteration 76, loss = 0.02096219
Iteration 77, loss = 0.02071654
Iteration 78, loss = 0.02081162
Iteration 79, loss = 0.02104014
Iteration 80, loss = 0.02109128
Iteration 81, loss = 0.02086091
Iteration 82, loss = 0.02090743
Iteration 83, loss = 0.02092826
Iteration 84, loss = 0.02070959
Iteration 85, loss = 0.02096978
Iteration 86, loss = 0.02063195
Iteration 87, loss = 0.02069234
Iteration 88, loss = 0.02066145
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67656745
Iteration 2, loss = 0.63717235
Iteration 3, loss = 0.58581845
Iteration 4, loss = 0.51945096
Iteration 5, loss = 0.44626575
Iteration 6, loss = 0.37632598
Iteration 7, loss = 0.31348019
Iteration 8, loss = 0.25920413
Iteration 9, loss = 0.21447531
Iteration 10, loss = 0.17821534
Iteration 11, loss = 0.14954808
Iteration 12, loss = 0.12692370
Iteration 13, loss = 0.10890493
Iteration 14, loss = 0.09458873
Iteration 15, loss = 0.08312241
Iteration 16, loss = 0.07384358
Iteration 17, loss = 0.06653496
Iteration 18, loss = 0.06000999
Iteration 19, loss = 0.05483099
Iteration 20, loss = 0.05034570
Iteration 21, loss = 0.04662916
Iteration 22, loss = 0.04332883
Iteration 23, loss = 0.04066000
Iteration 24, loss = 0.03827471
Iteration 25, loss = 0.03601213
Iteration 26, loss = 0.03429340
Iteration 27, loss = 0.03275898
Iteration 28, loss = 0.03125391
Iteration 29, loss = 0.02998859
Iteration 30, loss = 0.02883787
Iteration 31, loss = 0.02796262
Iteration 32, loss = 0.02699339
Iteration 33, loss = 0.02606775
Iteration 34, loss = 0.02537801
Iteration 35, loss = 0.02460234
Iteration 36, loss = 0.02401895
Iteration 37, loss = 0.02345657
Iteration 38, loss = 0.02286018
Iteration 39, loss = 0.02248166
Iteration 40, loss = 0.02198966
Iteration 41, loss = 0.02155379
Iteration 42, loss = 0.02119198
Iteration 43, loss = 0.02079208
Iteration 44, loss = 0.02059406
Iteration 45, loss = 0.02011835
Iteration 46, loss = 0.01998057
Iteration 47, loss = 0.01980858
Iteration 48, loss = 0.01931107
Iteration 49, loss = 0.01926749
Iteration 50, loss = 0.01887743
Iteration 51, loss = 0.01866805
Iteration 52, loss = 0.01852026
Iteration 53, loss = 0.01832816
Iteration 54, loss = 0.01819833
Iteration 55, loss = 0.01810883
Iteration 56, loss = 0.01798488
Iteration 57, loss = 0.01781578
Iteration 58, loss = 0.01752636
Iteration 59, loss = 0.01753342
Iteration 60, loss = 0.01736447
Iteration 61, loss = 0.01730650
Iteration 62, loss = 0.01715084
Iteration 63, loss = 0.01697814
Iteration 64, loss = 0.01687659
Iteration 65, loss = 0.01691202
Iteration 66, loss = 0.01669770
Iteration 67, loss = 0.01663638
Iteration 68, loss = 0.01660368
Iteration 69, loss = 0.01645590
Iteration 70, loss = 0.01649848
Iteration 71, loss = 0.01630901
Iteration 72, loss = 0.01639222
Iteration 73, loss = 0.01637952
Iteration 74, loss = 0.01613011
Iteration 75, loss = 0.01604591
Iteration 76, loss = 0.01599251
Iteration 77, loss = 0.01590364
Iteration 78, loss = 0.01577318
Iteration 79, loss = 0.01577395
Iteration 80, loss = 0.01580577
Iteration 81, loss = 0.01571418
Iteration 82, loss = 0.01570008
Iteration 83, loss = 0.01582083
Iteration 84, loss = 0.01565596
Iteration 85, loss = 0.01550805
Iteration 86, loss = 0.01558259
Iteration 87, loss = 0.01545234
Iteration 88, loss = 0.01559151
Iteration 89, loss = 0.01553275
Iteration 90, loss = 0.01539549
Iteration 91, loss = 0.01541142
Iteration 92, loss = 0.01529894
Iteration 93, loss = 0.01532993
Iteration 94, loss = 0.01515110
Iteration 95, loss = 0.01532185
Iteration 96, loss = 0.01514803
Iteration 97, loss = 0.01509076
Iteration 98, loss = 0.01511879
Iteration 99, loss = 0.01499896
Iteration 100, loss = 0.01500018
Iteration 101, loss = 0.01505223
Iteration 102, loss = 0.01499491
Iteration 103, loss = 0.01491491
Iteration 104, loss = 0.01484459
Iteration 105, loss = 0.01510490
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68673041
Iteration 2, loss = 0.61996951
Iteration 3, loss = 0.52509544
Iteration 4, loss = 0.42199396
Iteration 5, loss = 0.32922171
Iteration 6, loss = 0.25460947
Iteration 7, loss = 0.19905553
Iteration 8, loss = 0.15808986
Iteration 9, loss = 0.12876226
Iteration 10, loss = 0.10666812
Iteration 11, loss = 0.09040372
Iteration 12, loss = 0.07850395
Iteration 13, loss = 0.06891225
Iteration 14, loss = 0.06172348
Iteration 15, loss = 0.05607092
Iteration 16, loss = 0.05118005
Iteration 17, loss = 0.04716159
Iteration 18, loss = 0.04413042
Iteration 19, loss = 0.04144457
Iteration 20, loss = 0.03909714
Iteration 21, loss = 0.03702007
Iteration 22, loss = 0.03533491
Iteration 23, loss = 0.03399991
Iteration 24, loss = 0.03269289
Iteration 25, loss = 0.03132024
Iteration 26, loss = 0.03071303
Iteration 27, loss = 0.02964768
Iteration 28, loss = 0.02941540
Iteration 29, loss = 0.02850623
Iteration 30, loss = 0.02775073
Iteration 31, loss = 0.02703617
Iteration 32, loss = 0.02645811
Iteration 33, loss = 0.02598513
Iteration 34, loss = 0.02566074
Iteration 35, loss = 0.02507618
Iteration 36, loss = 0.02480005
Iteration 37, loss = 0.02434042
Iteration 38, loss = 0.02418932
Iteration 39, loss = 0.02378774
Iteration 40, loss = 0.02353608
Iteration 41, loss = 0.02337186
Iteration 42, loss = 0.02308668
Iteration 43, loss = 0.02289548
Iteration 44, loss = 0.02258615
Iteration 45, loss = 0.02303930
Iteration 46, loss = 0.02315891
Iteration 47, loss = 0.02277152
Iteration 48, loss = 0.02302494
Iteration 49, loss = 0.02357706
Iteration 50, loss = 0.02304785
Iteration 51, loss = 0.02261195
Iteration 52, loss = 0.02224454
Iteration 53, loss = 0.02275338
Iteration 54, loss = 0.02261867
Iteration 55, loss = 0.02225735
Iteration 56, loss = 0.02207717
Iteration 57, loss = 0.02180623
Iteration 58, loss = 0.02166638
Iteration 59, loss = 0.02163960
Iteration 60, loss = 0.02157846
Iteration 61, loss = 0.02127206
Iteration 62, loss = 0.02131174
Iteration 63, loss = 0.02099815
Iteration 64, loss = 0.02338575
Iteration 65, loss = 0.02546841
Iteration 66, loss = 0.02499328
Iteration 67, loss = 0.02380499
Iteration 68, loss = 0.02319236
Iteration 69, loss = 0.02246466
Iteration 70, loss = 0.02188022
Iteration 71, loss = 0.02145321
Iteration 72, loss = 0.02114214
Iteration 73, loss = 0.02076165
Iteration 74, loss = 0.02062012
Iteration 75, loss = 0.02031050
Iteration 76, loss = 0.02007683
Iteration 77, loss = 0.01994217
Iteration 78, loss = 0.02008859
Iteration 79, loss = 0.01991869
Iteration 80, loss = 0.01997238
Iteration 81, loss = 0.01971134
Iteration 82, loss = 0.01963145
Iteration 83, loss = 0.01956899
Iteration 84, loss = 0.01964129
Iteration 85, loss = 0.01952101
Iteration 86, loss = 0.01954147
Iteration 87, loss = 0.01941600
Iteration 88, loss = 0.01918238
Iteration 89, loss = 0.01920743
Iteration 90, loss = 0.01917655
Iteration 91, loss = 0.01956141
Iteration 92, loss = 0.01938600
Iteration 93, loss = 0.01923705
Iteration 94, loss = 0.01916225
Iteration 95, loss = 0.01929050
Iteration 96, loss = 0.01909271
Iteration 97, loss = 0.01917096
Iteration 98, loss = 0.01906269
Iteration 99, loss = 0.01893866
Iteration 100, loss = 0.01905364
Iteration 101, loss = 0.01914999
Iteration 102, loss = 0.01909807
Iteration 103, loss = 0.01914105
Iteration 104, loss = 0.01900642
Iteration 105, loss = 0.01892553
Iteration 106, loss = 0.01896287
Iteration 107, loss = 0.01899891
Iteration 108, loss = 0.01877173
Iteration 109, loss = 0.01899259
Iteration 110, loss = 0.01895138
Iteration 111, loss = 0.01868274
Iteration 112, loss = 0.01887241
Iteration 113, loss = 0.01860398
Iteration 114, loss = 0.01873080
Iteration 115, loss = 0.01883449
Iteration 116, loss = 0.01869891
Iteration 117, loss = 0.01878614
Iteration 118, loss = 0.01888867
Iteration 119, loss = 0.01902609
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68597195
Iteration 2, loss = 0.66385248
Iteration 3, loss = 0.63690541
Iteration 4, loss = 0.60109376
Iteration 5, loss = 0.55694212
Iteration 6, loss = 0.50851223
Iteration 7, loss = 0.45908903
Iteration 8, loss = 0.40987494
Iteration 9, loss = 0.36223268
Iteration 10, loss = 0.31810300
Iteration 11, loss = 0.27847149
Iteration 12, loss = 0.24324092
Iteration 13, loss = 0.21273354
Iteration 14, loss = 0.18634664
Iteration 15, loss = 0.16406961
Iteration 16, loss = 0.14502087
Iteration 17, loss = 0.12906548
Iteration 18, loss = 0.11550746
Iteration 19, loss = 0.10388046
Iteration 20, loss = 0.09419305
Iteration 21, loss = 0.08587420
Iteration 22, loss = 0.07865849
Iteration 23, loss = 0.07235203
Iteration 24, loss = 0.06701376
Iteration 25, loss = 0.06224776
Iteration 26, loss = 0.05817825
Iteration 27, loss = 0.05451974
Iteration 28, loss = 0.05131088
Iteration 29, loss = 0.04847530
Iteration 30, loss = 0.04602617
Iteration 31, loss = 0.04373529
Iteration 32, loss = 0.04176895
Iteration 33, loss = 0.03994590
Iteration 34, loss = 0.03819777
Iteration 35, loss = 0.03653878
Iteration 36, loss = 0.03532321
Iteration 37, loss = 0.03406618
Iteration 38, loss = 0.03288156
Iteration 39, loss = 0.03194243
Iteration 40, loss = 0.03095379
Iteration 41, loss = 0.03020093
Iteration 42, loss = 0.02934435
Iteration 43, loss = 0.02855921
Iteration 44, loss = 0.02781253
Iteration 45, loss = 0.02720121
Iteration 46, loss = 0.02672376
Iteration 47, loss = 0.02605487
Iteration 48, loss = 0.02544622
Iteration 49, loss = 0.02488678
Iteration 50, loss = 0.02448203
Iteration 51, loss = 0.02394555
Iteration 52, loss = 0.02339188
Iteration 53, loss = 0.02299090
Iteration 54, loss = 0.02259004
Iteration 55, loss = 0.02226664
Iteration 56, loss = 0.02191378
Iteration 57, loss = 0.02171949
Iteration 58, loss = 0.02164220
Iteration 59, loss = 0.02151829
Iteration 60, loss = 0.02128426
Iteration 61, loss = 0.02055155
Iteration 62, loss = 0.02036665
Iteration 63, loss = 0.02032593
Iteration 64, loss = 0.02002395
Iteration 65, loss = 0.01984363
Iteration 66, loss = 0.01952268
Iteration 67, loss = 0.01938051
Iteration 68, loss = 0.01912668
Iteration 69, loss = 0.01890863
Iteration 70, loss = 0.01883652
Iteration 71, loss = 0.01890348
Iteration 72, loss = 0.01867478
Iteration 73, loss = 0.01867815
Iteration 74, loss = 0.01858879
Iteration 75, loss = 0.01848391
Iteration 76, loss = 0.01824868
Iteration 77, loss = 0.01793368
Iteration 78, loss = 0.01771919
Iteration 79, loss = 0.01774434
Iteration 80, loss = 0.01754382
Iteration 81, loss = 0.01742864
Iteration 82, loss = 0.01751911
Iteration 83, loss = 0.01738885
Iteration 84, loss = 0.01729952
Iteration 85, loss = 0.01716746
Iteration 86, loss = 0.01713049
Iteration 87, loss = 0.01701213
Iteration 88, loss = 0.01737198
Iteration 89, loss = 0.01736436
Iteration 90, loss = 0.01717343
Iteration 91, loss = 0.01727088
Iteration 92, loss = 0.01695164
Iteration 93, loss = 0.01675479
Iteration 94, loss = 0.01665785
Iteration 95, loss = 0.01641906
Iteration 96, loss = 0.01647644
Iteration 97, loss = 0.01632101
Iteration 98, loss = 0.01623128
Iteration 99, loss = 0.01616320
Iteration 100, loss = 0.01618958
Iteration 101, loss = 0.01608014
Iteration 102, loss = 0.01610706
Iteration 103, loss = 0.01595749
Iteration 104, loss = 0.01589058
Iteration 105, loss = 0.01607630
Iteration 106, loss = 0.01589327
Iteration 107, loss = 0.01581230
Iteration 108, loss = 0.01585504
Iteration 109, loss = 0.01576931
Iteration 110, loss = 0.01570686
Iteration 111, loss = 0.01557706
Iteration 112, loss = 0.01561334
Iteration 113, loss = 0.01554369
Iteration 114, loss = 0.01549050
Iteration 115, loss = 0.01555023
Iteration 116, loss = 0.01547042
Iteration 117, loss = 0.01547314
Iteration 118, loss = 0.01534994
Iteration 119, loss = 0.01527998
Iteration 120, loss = 0.01536055
Iteration 121, loss = 0.01523892
Iteration 122, loss = 0.01530516
Iteration 123, loss = 0.01569916
Iteration 124, loss = 0.01601086
Iteration 125, loss = 0.01634335
Iteration 126, loss = 0.01600487
Iteration 127, loss = 0.01548500
Iteration 128, loss = 0.01528691
Iteration 129, loss = 0.01509757
Iteration 130, loss = 0.01496349
Iteration 131, loss = 0.01516896
Iteration 132, loss = 0.01519574
Iteration 133, loss = 0.01512145
Iteration 134, loss = 0.01512113
Iteration 135, loss = 0.01490930
Iteration 136, loss = 0.01483461
Iteration 137, loss = 0.01481676
Iteration 138, loss = 0.01486976
Iteration 139, loss = 0.01481562
Iteration 140, loss = 0.01493893
Iteration 141, loss = 0.01482072
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67375154
Iteration 2, loss = 0.65793888
Iteration 3, loss = 0.64472241
Iteration 4, loss = 0.63149236
Iteration 5, loss = 0.61750523
Iteration 6, loss = 0.60236985
Iteration 7, loss = 0.58594829
Iteration 8, loss = 0.56779106
Iteration 9, loss = 0.54818119
Iteration 10, loss = 0.52698643
Iteration 11, loss = 0.50456919
Iteration 12, loss = 0.48116674
Iteration 13, loss = 0.45715252
Iteration 14, loss = 0.43304094
Iteration 15, loss = 0.40917056
Iteration 16, loss = 0.38570255
Iteration 17, loss = 0.36289938
Iteration 18, loss = 0.34090240
Iteration 19, loss = 0.31982819
Iteration 20, loss = 0.29971596
Iteration 21, loss = 0.28076084
Iteration 22, loss = 0.26278990
Iteration 23, loss = 0.24584123
Iteration 24, loss = 0.23016588
Iteration 25, loss = 0.21543381
Iteration 26, loss = 0.20156744
Iteration 27, loss = 0.18872227
Iteration 28, loss = 0.17671607
Iteration 29, loss = 0.16575475
Iteration 30, loss = 0.15547804
Iteration 31, loss = 0.14602446
Iteration 32, loss = 0.13725569
Iteration 33, loss = 0.12916917
Iteration 34, loss = 0.12172776
Iteration 35, loss = 0.11483748
Iteration 36, loss = 0.10856957
Iteration 37, loss = 0.10263536
Iteration 38, loss = 0.09718507
Iteration 39, loss = 0.09217600
Iteration 40, loss = 0.08741595
Iteration 41, loss = 0.08313568
Iteration 42, loss = 0.07903355
Iteration 43, loss = 0.07532632
Iteration 44, loss = 0.07181775
Iteration 45, loss = 0.06857687
Iteration 46, loss = 0.06554019
Iteration 47, loss = 0.06271207
Iteration 48, loss = 0.06009538
Iteration 49, loss = 0.05759233
Iteration 50, loss = 0.05530333
Iteration 51, loss = 0.05310650
Iteration 52, loss = 0.05110611
Iteration 53, loss = 0.04919001
Iteration 54, loss = 0.04741495
Iteration 55, loss = 0.04570778
Iteration 56, loss = 0.04411427
Iteration 57, loss = 0.04259503
Iteration 58, loss = 0.04122445
Iteration 59, loss = 0.03982797
Iteration 60, loss = 0.03854372
Iteration 61, loss = 0.03738476
Iteration 62, loss = 0.03620646
Iteration 63, loss = 0.03511573
Iteration 64, loss = 0.03408969
Iteration 65, loss = 0.03311067
Iteration 66, loss = 0.03217579
Iteration 67, loss = 0.03128758
Iteration 68, loss = 0.03045670
Iteration 69, loss = 0.02964070
Iteration 70, loss = 0.02888153
Iteration 71, loss = 0.02815137
Iteration 72, loss = 0.02743510
Iteration 73, loss = 0.02673398
Iteration 74, loss = 0.02609705
Iteration 75, loss = 0.02547927
Iteration 76, loss = 0.02488519
Iteration 77, loss = 0.02432156
Iteration 78, loss = 0.02377824
Iteration 79, loss = 0.02326953
Iteration 80, loss = 0.02275967
Iteration 81, loss = 0.02226078
Iteration 82, loss = 0.02180866
Iteration 83, loss = 0.02141854
Iteration 84, loss = 0.02093140
Iteration 85, loss = 0.02055524
Iteration 86, loss = 0.02011786
Iteration 87, loss = 0.01975521
Iteration 88, loss = 0.01936430
Iteration 89, loss = 0.01899822
Iteration 90, loss = 0.01864340
Iteration 91, loss = 0.01832373
Iteration 92, loss = 0.01799015
Iteration 93, loss = 0.01766384
Iteration 94, loss = 0.01736812
Iteration 95, loss = 0.01707735
Iteration 96, loss = 0.01676316
Iteration 97, loss = 0.01650676
Iteration 98, loss = 0.01622456
Iteration 99, loss = 0.01594653
Iteration 100, loss = 0.01572227
Iteration 101, loss = 0.01546150
Iteration 102, loss = 0.01524318
Iteration 103, loss = 0.01501222
Iteration 104, loss = 0.01478667
Iteration 105, loss = 0.01459550
Iteration 106, loss = 0.01438161
Iteration 107, loss = 0.01417232
Iteration 108, loss = 0.01396251
Iteration 109, loss = 0.01379210
Iteration 110, loss = 0.01359565
Iteration 111, loss = 0.01340615
Iteration 112, loss = 0.01325079
Iteration 113, loss = 0.01307266
Iteration 114, loss = 0.01290560
Iteration 115, loss = 0.01273985
Iteration 116, loss = 0.01259031
Iteration 117, loss = 0.01243641
Iteration 118, loss = 0.01228361
Iteration 119, loss = 0.01213591
Iteration 120, loss = 0.01201787
Iteration 121, loss = 0.01183643
Iteration 122, loss = 0.01171924
Iteration 123, loss = 0.01156883
Iteration 124, loss = 0.01143715
Iteration 125, loss = 0.01131225
Iteration 126, loss = 0.01120300
Iteration 127, loss = 0.01107951
Iteration 128, loss = 0.01095453
Iteration 129, loss = 0.01086730
Iteration 130, loss = 0.01074034
Iteration 131, loss = 0.01063177
Iteration 132, loss = 0.01054036
Iteration 133, loss = 0.01044350
Iteration 134, loss = 0.01032808
Iteration 135, loss = 0.01024690
Iteration 136, loss = 0.01013612
Iteration 137, loss = 0.01004539
Iteration 138, loss = 0.00995595
Iteration 139, loss = 0.00987364
Iteration 140, loss = 0.00977454
Iteration 141, loss = 0.00974326
Iteration 142, loss = 0.00962018
Iteration 143, loss = 0.00953811
Iteration 144, loss = 0.00950471
Iteration 145, loss = 0.00938718
Iteration 146, loss = 0.00930426
Iteration 147, loss = 0.00922479
Iteration 148, loss = 0.00915606
Iteration 149, loss = 0.00909168
Iteration 150, loss = 0.00900434
Iteration 151, loss = 0.00893983
Iteration 152, loss = 0.00887268
Iteration 153, loss = 0.00879677
Iteration 154, loss = 0.00874746
Iteration 155, loss = 0.00872178
Iteration 156, loss = 0.00862535
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69106831
Iteration 2, loss = 0.65151818
Iteration 3, loss = 0.60287873
Iteration 4, loss = 0.54397454
Iteration 5, loss = 0.48028722
Iteration 6, loss = 0.41799560
Iteration 7, loss = 0.36053678
Iteration 8, loss = 0.30870944
Iteration 9, loss = 0.26347339
Iteration 10, loss = 0.22460285
Iteration 11, loss = 0.19212533
Iteration 12, loss = 0.16485714
Iteration 13, loss = 0.14248878
Iteration 14, loss = 0.12396575
Iteration 15, loss = 0.10866983
Iteration 16, loss = 0.09607945
Iteration 17, loss = 0.08552202
Iteration 18, loss = 0.07689300
Iteration 19, loss = 0.06944411
Iteration 20, loss = 0.06319423
Iteration 21, loss = 0.05796305
Iteration 22, loss = 0.05330484
Iteration 23, loss = 0.04933712
Iteration 24, loss = 0.04599809
Iteration 25, loss = 0.04302073
Iteration 26, loss = 0.04035523
Iteration 27, loss = 0.03808691
Iteration 28, loss = 0.03599569
Iteration 29, loss = 0.03422072
Iteration 30, loss = 0.03245173
Iteration 31, loss = 0.03113774
Iteration 32, loss = 0.02976246
Iteration 33, loss = 0.02857202
Iteration 34, loss = 0.02746757
Iteration 35, loss = 0.02655880
Iteration 36, loss = 0.02562422
Iteration 37, loss = 0.02477367
Iteration 38, loss = 0.02414183
Iteration 39, loss = 0.02352758
Iteration 40, loss = 0.02278182
Iteration 41, loss = 0.02216173
Iteration 42, loss = 0.02162330
Iteration 43, loss = 0.02123732
Iteration 44, loss = 0.02065462
Iteration 45, loss = 0.02027659
Iteration 46, loss = 0.01978989
Iteration 47, loss = 0.01952406
Iteration 48, loss = 0.01917948
Iteration 49, loss = 0.01883683
Iteration 50, loss = 0.01851452
Iteration 51, loss = 0.01819527
Iteration 52, loss = 0.01792409
Iteration 53, loss = 0.01784436
Iteration 54, loss = 0.01742116
Iteration 55, loss = 0.01727194
Iteration 56, loss = 0.01701230
Iteration 57, loss = 0.01692973
Iteration 58, loss = 0.01677050
Iteration 59, loss = 0.01656585
Iteration 60, loss = 0.01631551
Iteration 61, loss = 0.01624767
Iteration 62, loss = 0.01596857
Iteration 63, loss = 0.01596578
Iteration 64, loss = 0.01581585
Iteration 65, loss = 0.01549557
Iteration 66, loss = 0.01542249
Iteration 67, loss = 0.01533201
Iteration 68, loss = 0.01534665
Iteration 69, loss = 0.01505349
Iteration 70, loss = 0.01501688
Iteration 71, loss = 0.01489516
Iteration 72, loss = 0.01492206
Iteration 73, loss = 0.01476019
Iteration 74, loss = 0.01472938
Iteration 75, loss = 0.01456921
Iteration 76, loss = 0.01453957
Iteration 77, loss = 0.01442974
Iteration 78, loss = 0.01426238
Iteration 79, loss = 0.01426990
Iteration 80, loss = 0.01420458
Iteration 81, loss = 0.01407377
Iteration 82, loss = 0.01412184
Iteration 83, loss = 0.01415019
Iteration 84, loss = 0.01402421
Iteration 85, loss = 0.01392137
Iteration 86, loss = 0.01401952
Iteration 87, loss = 0.01380779
Iteration 88, loss = 0.01381640
Iteration 89, loss = 0.01374302
Iteration 90, loss = 0.01360369
Iteration 91, loss = 0.01363806
Iteration 92, loss = 0.01359735
Iteration 93, loss = 0.01347578
Iteration 94, loss = 0.01337613
Iteration 95, loss = 0.01348233
Iteration 96, loss = 0.01349271
Iteration 97, loss = 0.01349697
Iteration 98, loss = 0.01342225
Iteration 99, loss = 0.01343915
Iteration 100, loss = 0.01336738
Iteration 101, loss = 0.01327597
Iteration 102, loss = 0.01331749
Iteration 103, loss = 0.01314148
Iteration 104, loss = 0.01329204
Iteration 105, loss = 0.01307582
Iteration 106, loss = 0.01326592
Iteration 107, loss = 0.01311866
Iteration 108, loss = 0.01301818
Iteration 109, loss = 0.01309128
Iteration 110, loss = 0.01302668
Iteration 111, loss = 0.01291266
Iteration 112, loss = 0.01306310
Iteration 113, loss = 0.01304967
Iteration 114, loss = 0.01291575
Iteration 115, loss = 0.01295542
Iteration 116, loss = 0.01283225
Iteration 117, loss = 0.01288100
Iteration 118, loss = 0.01284965
Iteration 119, loss = 0.01274209
Iteration 120, loss = 0.01281594
Iteration 121, loss = 0.01279768
Iteration 122, loss = 0.01273839
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69880243
Iteration 2, loss = 0.62428761
Iteration 3, loss = 0.52082121
Iteration 4, loss = 0.40587489
Iteration 5, loss = 0.30777331
Iteration 6, loss = 0.23154939
Iteration 7, loss = 0.17686225
Iteration 8, loss = 0.13851549
Iteration 9, loss = 0.11161344
Iteration 10, loss = 0.09260048
Iteration 11, loss = 0.07826018
Iteration 12, loss = 0.06784718
Iteration 13, loss = 0.05999431
Iteration 14, loss = 0.05377397
Iteration 15, loss = 0.04896434
Iteration 16, loss = 0.04466815
Iteration 17, loss = 0.04169271
Iteration 18, loss = 0.03881418
Iteration 19, loss = 0.03664927
Iteration 20, loss = 0.03465237
Iteration 21, loss = 0.03296884
Iteration 22, loss = 0.03171067
Iteration 23, loss = 0.03064698
Iteration 24, loss = 0.02927511
Iteration 25, loss = 0.02846787
Iteration 26, loss = 0.02758986
Iteration 27, loss = 0.02708275
Iteration 28, loss = 0.02631625
Iteration 29, loss = 0.02578376
Iteration 30, loss = 0.02541943
Iteration 31, loss = 0.02460873
Iteration 32, loss = 0.02435003
Iteration 33, loss = 0.02379516
Iteration 34, loss = 0.02365764
Iteration 35, loss = 0.02301555
Iteration 36, loss = 0.02286947
Iteration 37, loss = 0.02261338
Iteration 38, loss = 0.02241870
Iteration 39, loss = 0.02223005
Iteration 40, loss = 0.02242801
Iteration 41, loss = 0.02165028
Iteration 42, loss = 0.02169782
Iteration 43, loss = 0.02142510
Iteration 44, loss = 0.02146390
Iteration 45, loss = 0.02096671
Iteration 46, loss = 0.02108759
Iteration 47, loss = 0.02122037
Iteration 48, loss = 0.02074170
Iteration 49, loss = 0.02058910
Iteration 50, loss = 0.02057273
Iteration 51, loss = 0.02065722
Iteration 52, loss = 0.02059280
Iteration 53, loss = 0.02028057
Iteration 54, loss = 0.02021457
Iteration 55, loss = 0.02051489
Iteration 56, loss = 0.02021778
Iteration 57, loss = 0.02003964
Iteration 58, loss = 0.02049152
Iteration 59, loss = 0.01994211
Iteration 60, loss = 0.02016346
Iteration 61, loss = 0.01987899
Iteration 62, loss = 0.01976229
Iteration 63, loss = 0.01972823
Iteration 64, loss = 0.01964655
Iteration 65, loss = 0.01990987
Iteration 66, loss = 0.02000550
Iteration 67, loss = 0.02008072
Iteration 68, loss = 0.02006816
Iteration 69, loss = 0.01972356
Iteration 70, loss = 0.01963614
Iteration 71, loss = 0.01946936
Iteration 72, loss = 0.01958663
Iteration 73, loss = 0.01952602
Iteration 74, loss = 0.01945694
Iteration 75, loss = 0.01936143
Iteration 76, loss = 0.01940699
Iteration 77, loss = 0.01932837
Iteration 78, loss = 0.01928442
Iteration 79, loss = 0.01946668
Iteration 80, loss = 0.01922315
Iteration 81, loss = 0.01904560
Iteration 82, loss = 0.01909820
Iteration 83, loss = 0.01936954
Iteration 84, loss = 0.01889916
Iteration 85, loss = 0.01903167
Iteration 86, loss = 0.01910537
Iteration 87, loss = 0.01901722
Iteration 88, loss = 0.01893893
Iteration 89, loss = 0.01919303
Iteration 90, loss = 0.01907403
Iteration 91, loss = 0.01920483
Iteration 92, loss = 0.01891308
Iteration 93, loss = 0.01889637
Iteration 94, loss = 0.01893757
Iteration 95, loss = 0.01887683
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71715768
Iteration 2, loss = 0.69569936
Iteration 3, loss = 0.67814302
Iteration 4, loss = 0.66100898
Iteration 5, loss = 0.64344959
Iteration 6, loss = 0.62487771
Iteration 7, loss = 0.60522473
Iteration 8, loss = 0.58435525
Iteration 9, loss = 0.56217248
Iteration 10, loss = 0.53918196
Iteration 11, loss = 0.51519165
Iteration 12, loss = 0.49085888
Iteration 13, loss = 0.46618744
Iteration 14, loss = 0.44157328
Iteration 15, loss = 0.41722176
Iteration 16, loss = 0.39330821
Iteration 17, loss = 0.37004499
Iteration 18, loss = 0.34731370
Iteration 19, loss = 0.32566785
Iteration 20, loss = 0.30484480
Iteration 21, loss = 0.28499520
Iteration 22, loss = 0.26626147
Iteration 23, loss = 0.24846893
Iteration 24, loss = 0.23181232
Iteration 25, loss = 0.21621893
Iteration 26, loss = 0.20171310
Iteration 27, loss = 0.18831983
Iteration 28, loss = 0.17589175
Iteration 29, loss = 0.16444634
Iteration 30, loss = 0.15382503
Iteration 31, loss = 0.14406634
Iteration 32, loss = 0.13506668
Iteration 33, loss = 0.12681921
Iteration 34, loss = 0.11917972
Iteration 35, loss = 0.11211492
Iteration 36, loss = 0.10567917
Iteration 37, loss = 0.09971935
Iteration 38, loss = 0.09421720
Iteration 39, loss = 0.08913868
Iteration 40, loss = 0.08445485
Iteration 41, loss = 0.08013294
Iteration 42, loss = 0.07605943
Iteration 43, loss = 0.07235924
Iteration 44, loss = 0.06884222
Iteration 45, loss = 0.06564054
Iteration 46, loss = 0.06259491
Iteration 47, loss = 0.05977319
Iteration 48, loss = 0.05712680
Iteration 49, loss = 0.05467556
Iteration 50, loss = 0.05238473
Iteration 51, loss = 0.05024500
Iteration 52, loss = 0.04823063
Iteration 53, loss = 0.04632653
Iteration 54, loss = 0.04453285
Iteration 55, loss = 0.04283832
Iteration 56, loss = 0.04126500
Iteration 57, loss = 0.03974628
Iteration 58, loss = 0.03834805
Iteration 59, loss = 0.03699884
Iteration 60, loss = 0.03573047
Iteration 61, loss = 0.03453137
Iteration 62, loss = 0.03338507
Iteration 63, loss = 0.03231247
Iteration 64, loss = 0.03126310
Iteration 65, loss = 0.03028529
Iteration 66, loss = 0.02936243
Iteration 67, loss = 0.02846184
Iteration 68, loss = 0.02762726
Iteration 69, loss = 0.02679314
Iteration 70, loss = 0.02602357
Iteration 71, loss = 0.02527895
Iteration 72, loss = 0.02458616
Iteration 73, loss = 0.02390255
Iteration 74, loss = 0.02323925
Iteration 75, loss = 0.02261844
Iteration 76, loss = 0.02201552
Iteration 77, loss = 0.02143469
Iteration 78, loss = 0.02087781
Iteration 79, loss = 0.02034870
Iteration 80, loss = 0.01984010
Iteration 81, loss = 0.01937001
Iteration 82, loss = 0.01888406
Iteration 83, loss = 0.01842775
Iteration 84, loss = 0.01799677
Iteration 85, loss = 0.01756466
Iteration 86, loss = 0.01715979
Iteration 87, loss = 0.01677428
Iteration 88, loss = 0.01639630
Iteration 89, loss = 0.01603438
Iteration 90, loss = 0.01567871
Iteration 91, loss = 0.01535642
Iteration 92, loss = 0.01503169
Iteration 93, loss = 0.01471791
Iteration 94, loss = 0.01440955
Iteration 95, loss = 0.01411509
Iteration 96, loss = 0.01382623
Iteration 97, loss = 0.01356255
Iteration 98, loss = 0.01328412
Iteration 99, loss = 0.01303448
Iteration 100, loss = 0.01277819
Iteration 101, loss = 0.01254048
Iteration 102, loss = 0.01230427
Iteration 103, loss = 0.01208223
Iteration 104, loss = 0.01186217
Iteration 105, loss = 0.01164764
Iteration 106, loss = 0.01144414
Iteration 107, loss = 0.01124018
Iteration 108, loss = 0.01104800
Iteration 109, loss = 0.01085579
Iteration 110, loss = 0.01066657
Iteration 111, loss = 0.01048401
Iteration 112, loss = 0.01030688
Iteration 113, loss = 0.01013570
Iteration 114, loss = 0.00996965
Iteration 115, loss = 0.00980954
Iteration 116, loss = 0.00964824
Iteration 117, loss = 0.00949835
Iteration 118, loss = 0.00934819
Iteration 119, loss = 0.00920142
Iteration 120, loss = 0.00906371
Iteration 121, loss = 0.00892807
Iteration 122, loss = 0.00878943
Iteration 123, loss = 0.00865976
Iteration 124, loss = 0.00852766
Iteration 125, loss = 0.00840874
Iteration 126, loss = 0.00827882
Iteration 127, loss = 0.00815682
Iteration 128, loss = 0.00804281
Iteration 129, loss = 0.00792688
Iteration 130, loss = 0.00781777
Iteration 131, loss = 0.00771066
Iteration 132, loss = 0.00760342
Iteration 133, loss = 0.00749869
Iteration 134, loss = 0.00739700
Iteration 135, loss = 0.00730046
Iteration 136, loss = 0.00720695
Iteration 137, loss = 0.00711448
Iteration 138, loss = 0.00702147
Iteration 139, loss = 0.00693512
Iteration 140, loss = 0.00684381
Iteration 141, loss = 0.00675727
Iteration 142, loss = 0.00667853
Iteration 143, loss = 0.00659429
Iteration 144, loss = 0.00651568
Iteration 145, loss = 0.00643435
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68856265
Iteration 2, loss = 0.64736349
Iteration 3, loss = 0.59409143
Iteration 4, loss = 0.52914842
Iteration 5, loss = 0.45830850
Iteration 6, loss = 0.38871398
Iteration 7, loss = 0.32538020
Iteration 8, loss = 0.27023246
Iteration 9, loss = 0.22401080
Iteration 10, loss = 0.18674584
Iteration 11, loss = 0.15692072
Iteration 12, loss = 0.13328820
Iteration 13, loss = 0.11467581
Iteration 14, loss = 0.09986578
Iteration 15, loss = 0.08794832
Iteration 16, loss = 0.07831473
Iteration 17, loss = 0.07041526
Iteration 18, loss = 0.06395386
Iteration 19, loss = 0.05843471
Iteration 20, loss = 0.05394205
Iteration 21, loss = 0.05002511
Iteration 22, loss = 0.04679424
Iteration 23, loss = 0.04389159
Iteration 24, loss = 0.04145040
Iteration 25, loss = 0.03921541
Iteration 26, loss = 0.03736463
Iteration 27, loss = 0.03570777
Iteration 28, loss = 0.03409519
Iteration 29, loss = 0.03285924
Iteration 30, loss = 0.03168865
Iteration 31, loss = 0.03061509
Iteration 32, loss = 0.02969542
Iteration 33, loss = 0.02877172
Iteration 34, loss = 0.02806659
Iteration 35, loss = 0.02729178
Iteration 36, loss = 0.02674766
Iteration 37, loss = 0.02605833
Iteration 38, loss = 0.02541592
Iteration 39, loss = 0.02504980
Iteration 40, loss = 0.02459058
Iteration 41, loss = 0.02407917
Iteration 42, loss = 0.02366629
Iteration 43, loss = 0.02340167
Iteration 44, loss = 0.02314026
Iteration 45, loss = 0.02283336
Iteration 46, loss = 0.02239540
Iteration 47, loss = 0.02240629
Iteration 48, loss = 0.02194783
Iteration 49, loss = 0.02174580
Iteration 50, loss = 0.02145956
Iteration 51, loss = 0.02122173
Iteration 52, loss = 0.02119850
Iteration 53, loss = 0.02080042
Iteration 54, loss = 0.02070688
Iteration 55, loss = 0.02056660
Iteration 56, loss = 0.02046448
Iteration 57, loss = 0.02034302
Iteration 58, loss = 0.01999741
Iteration 59, loss = 0.02005033
Iteration 60, loss = 0.01983904
Iteration 61, loss = 0.01972266
Iteration 62, loss = 0.01976306
Iteration 63, loss = 0.01963078
Iteration 64, loss = 0.01944718
Iteration 65, loss = 0.01927980
Iteration 66, loss = 0.01929443
Iteration 67, loss = 0.01917370
Iteration 68, loss = 0.01912256
Iteration 69, loss = 0.01902948
Iteration 70, loss = 0.01894449
Iteration 71, loss = 0.01879425
Iteration 72, loss = 0.01893229
Iteration 73, loss = 0.01876380
Iteration 74, loss = 0.01861314
Iteration 75, loss = 0.01842904
Iteration 76, loss = 0.01854205
Iteration 77, loss = 0.01856433
Iteration 78, loss = 0.01845309
Iteration 79, loss = 0.01836236
Iteration 80, loss = 0.01827265
Iteration 81, loss = 0.01825867
Iteration 82, loss = 0.01827976
Iteration 83, loss = 0.01826129
Iteration 84, loss = 0.01833457
Iteration 85, loss = 0.01811131
Iteration 86, loss = 0.01804127
Iteration 87, loss = 0.01801668
Iteration 88, loss = 0.01809211
Iteration 89, loss = 0.01809661
Iteration 90, loss = 0.01797723
Iteration 91, loss = 0.01779602
Iteration 92, loss = 0.01791793
Iteration 93, loss = 0.01789221
Iteration 94, loss = 0.01790328
Iteration 95, loss = 0.01780892
Iteration 96, loss = 0.01785440
Iteration 97, loss = 0.01775842
Iteration 98, loss = 0.01773640
Iteration 99, loss = 0.01759557
Iteration 100, loss = 0.01766435
Iteration 101, loss = 0.01773312
Iteration 102, loss = 0.01752325
Iteration 103, loss = 0.01778903
Iteration 104, loss = 0.01759668
Iteration 105, loss = 0.01764261
Iteration 106, loss = 0.01748137
Iteration 107, loss = 0.01755984
Iteration 108, loss = 0.01751281
Iteration 109, loss = 0.01750592
Iteration 110, loss = 0.01745109
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68450677
Iteration 2, loss = 0.66511631
Iteration 3, loss = 0.63960575
Iteration 4, loss = 0.60416147
Iteration 5, loss = 0.55893898
Iteration 6, loss = 0.50752007
Iteration 7, loss = 0.45428195
Iteration 8, loss = 0.40319817
Iteration 9, loss = 0.35525373
Iteration 10, loss = 0.31164851
Iteration 11, loss = 0.27257537
Iteration 12, loss = 0.23800921
Iteration 13, loss = 0.20815416
Iteration 14, loss = 0.18219778
Iteration 15, loss = 0.16030278
Iteration 16, loss = 0.14172163
Iteration 17, loss = 0.12609254
Iteration 18, loss = 0.11296064
Iteration 19, loss = 0.10170578
Iteration 20, loss = 0.09204017
Iteration 21, loss = 0.08388913
Iteration 22, loss = 0.07676131
Iteration 23, loss = 0.07071838
Iteration 24, loss = 0.06526937
Iteration 25, loss = 0.06050588
Iteration 26, loss = 0.05639938
Iteration 27, loss = 0.05274356
Iteration 28, loss = 0.04946227
Iteration 29, loss = 0.04653621
Iteration 30, loss = 0.04387519
Iteration 31, loss = 0.04158164
Iteration 32, loss = 0.03958269
Iteration 33, loss = 0.03767331
Iteration 34, loss = 0.03604652
Iteration 35, loss = 0.03439172
Iteration 36, loss = 0.03297031
Iteration 37, loss = 0.03169203
Iteration 38, loss = 0.03046021
Iteration 39, loss = 0.02947536
Iteration 40, loss = 0.02844379
Iteration 41, loss = 0.02743962
Iteration 42, loss = 0.02659038
Iteration 43, loss = 0.02582447
Iteration 44, loss = 0.02535492
Iteration 45, loss = 0.02455546
Iteration 46, loss = 0.02393558
Iteration 47, loss = 0.02333596
Iteration 48, loss = 0.02269562
Iteration 49, loss = 0.02221423
Iteration 50, loss = 0.02174530
Iteration 51, loss = 0.02123246
Iteration 52, loss = 0.02078159
Iteration 53, loss = 0.02039413
Iteration 54, loss = 0.02002569
Iteration 55, loss = 0.01960888
Iteration 56, loss = 0.01926630
Iteration 57, loss = 0.01896568
Iteration 58, loss = 0.01869515
Iteration 59, loss = 0.01844529
Iteration 60, loss = 0.01811839
Iteration 61, loss = 0.01783671
Iteration 62, loss = 0.01758758
Iteration 63, loss = 0.01732299
Iteration 64, loss = 0.01734547
Iteration 65, loss = 0.01709874
Iteration 66, loss = 0.01686848
Iteration 67, loss = 0.01666852
Iteration 68, loss = 0.01649123
Iteration 69, loss = 0.01619165
Iteration 70, loss = 0.01607474
Iteration 71, loss = 0.01587760
Iteration 72, loss = 0.01569404
Iteration 73, loss = 0.01553525
Iteration 74, loss = 0.01538381
Iteration 75, loss = 0.01532436
Iteration 76, loss = 0.01517170
Iteration 77, loss = 0.01525049
Iteration 78, loss = 0.01512234
Iteration 79, loss = 0.01507877
Iteration 80, loss = 0.01494186
Iteration 81, loss = 0.01478540
Iteration 82, loss = 0.01469232
Iteration 83, loss = 0.01454994
Iteration 84, loss = 0.01437493
Iteration 85, loss = 0.01426278
Iteration 86, loss = 0.01415952
Iteration 87, loss = 0.01409761
Iteration 88, loss = 0.01402495
Iteration 89, loss = 0.01400384
Iteration 90, loss = 0.01390550
Iteration 91, loss = 0.01383773
Iteration 92, loss = 0.01391163
Iteration 93, loss = 0.01391922
Iteration 94, loss = 0.01385437
Iteration 95, loss = 0.01374854
Iteration 96, loss = 0.01355965
Iteration 97, loss = 0.01356826
Iteration 98, loss = 0.01349745
Iteration 99, loss = 0.01338120
Iteration 100, loss = 0.01341129
Iteration 101, loss = 0.01330381
Iteration 102, loss = 0.01327774
Iteration 103, loss = 0.01314770
Iteration 104, loss = 0.01306242
Iteration 105, loss = 0.01300414
Iteration 106, loss = 0.01295125
Iteration 107, loss = 0.01296999
Iteration 108, loss = 0.01290415
Iteration 109, loss = 0.01288439
Iteration 110, loss = 0.01285048
Iteration 111, loss = 0.01275632
Iteration 112, loss = 0.01301732
Iteration 113, loss = 0.01302418
Iteration 114, loss = 0.01298906
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68589117
Iteration 2, loss = 0.61346800
Iteration 3, loss = 0.50175761
Iteration 4, loss = 0.38359737
Iteration 5, loss = 0.28581259
Iteration 6, loss = 0.21281678
Iteration 7, loss = 0.16269358
Iteration 8, loss = 0.12758314
Iteration 9, loss = 0.10342937
Iteration 10, loss = 0.08642155
Iteration 11, loss = 0.07399483
Iteration 12, loss = 0.06477978
Iteration 13, loss = 0.05756297
Iteration 14, loss = 0.05181485
Iteration 15, loss = 0.04746433
Iteration 16, loss = 0.04380011
Iteration 17, loss = 0.04064326
Iteration 18, loss = 0.03834447
Iteration 19, loss = 0.03624597
Iteration 20, loss = 0.03470425
Iteration 21, loss = 0.03322860
Iteration 22, loss = 0.03172842
Iteration 23, loss = 0.03045713
Iteration 24, loss = 0.02967692
Iteration 25, loss = 0.02876347
Iteration 26, loss = 0.02799411
Iteration 27, loss = 0.02735458
Iteration 28, loss = 0.02684168
Iteration 29, loss = 0.02620857
Iteration 30, loss = 0.02585302
Iteration 31, loss = 0.02545419
Iteration 32, loss = 0.02521719
Iteration 33, loss = 0.02475721
Iteration 34, loss = 0.02452048
Iteration 35, loss = 0.02398513
Iteration 36, loss = 0.02377381
Iteration 37, loss = 0.02351922
Iteration 38, loss = 0.02338562
Iteration 39, loss = 0.02330771
Iteration 40, loss = 0.02300048
Iteration 41, loss = 0.02286694
Iteration 42, loss = 0.02288204
Iteration 43, loss = 0.02278596
Iteration 44, loss = 0.02257975
Iteration 45, loss = 0.02245194
Iteration 46, loss = 0.02206497
Iteration 47, loss = 0.02188119
Iteration 48, loss = 0.02198573
Iteration 49, loss = 0.02172391
Iteration 50, loss = 0.02195517
Iteration 51, loss = 0.02147572
Iteration 52, loss = 0.02132312
Iteration 53, loss = 0.02162528
Iteration 54, loss = 0.02162085
Iteration 55, loss = 0.02138866
Iteration 56, loss = 0.02139866
Iteration 57, loss = 0.02152171
Iteration 58, loss = 0.02100105
Iteration 59, loss = 0.02118220
Iteration 60, loss = 0.02100843
Iteration 61, loss = 0.02103866
Iteration 62, loss = 0.02096506
Iteration 63, loss = 0.02060239
Iteration 64, loss = 0.02065711
Iteration 65, loss = 0.02072364
Iteration 66, loss = 0.02100826
Iteration 67, loss = 0.02096553
Iteration 68, loss = 0.02062792
Iteration 69, loss = 0.02087039
Iteration 70, loss = 0.02054271
Iteration 71, loss = 0.02067430
Iteration 72, loss = 0.02035894
Iteration 73, loss = 0.02068706
Iteration 74, loss = 0.02046321
Iteration 75, loss = 0.02073100
Iteration 76, loss = 0.02081796
Iteration 77, loss = 0.02046090
Iteration 78, loss = 0.02055041
Iteration 79, loss = 0.02039114
Iteration 80, loss = 0.02048060
Iteration 81, loss = 0.02017973
Iteration 82, loss = 0.02021888
Iteration 83, loss = 0.02025251
Iteration 84, loss = 0.02036470
Iteration 85, loss = 0.02036042
Iteration 86, loss = 0.02046761
Iteration 87, loss = 0.02024861
Iteration 88, loss = 0.02020966
Iteration 89, loss = 0.02009291
Iteration 90, loss = 0.02016617
Iteration 91, loss = 0.02023981
Iteration 92, loss = 0.01992727
Iteration 93, loss = 0.02022398
Iteration 94, loss = 0.02026367
Iteration 95, loss = 0.02032643
Iteration 96, loss = 0.02009480
Iteration 97, loss = 0.02020817
Iteration 98, loss = 0.02027901
Iteration 99, loss = 0.01998899
Iteration 100, loss = 0.01999975
Iteration 101, loss = 0.02002953
Iteration 102, loss = 0.02007057
Iteration 103, loss = 0.02010468
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	84m50.376s
user	237m2.403s
sys	24m27.253s
Elapsed time: 5090 seconds
