Processing dataset: prepended_v3_lemmatized

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: prepended_v3_lemmatized (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on prepended_v3_lemmatized...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on prepended_v3_lemmatized...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on prepended_v3_lemmatized...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on prepended_v3_lemmatized...
/opt/ohpc/pub/spack/v0.21.1/opt/spack/linux-rocky8-x86_64_v3/gcc-8.5.0/py-joblib-1.2.0-g6vh2usiiivwucfr57aoxwqvljmoxokc/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
  warnings.warn(
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on prepended_v3_lemmatized...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.69313829
Iteration 2, loss = 0.62275574
Iteration 3, loss = 0.51792880
Iteration 4, loss = 0.40196651
Iteration 5, loss = 0.30149995
Iteration 6, loss = 0.22480591
Iteration 7, loss = 0.17116987
Iteration 8, loss = 0.13461731
Iteration 9, loss = 0.10878412
Iteration 10, loss = 0.09115153
Iteration 11, loss = 0.07808539
Iteration 12, loss = 0.06813675
Iteration 13, loss = 0.06100303
Iteration 14, loss = 0.05506860
Iteration 15, loss = 0.05054127
Iteration 16, loss = 0.04672274
Iteration 17, loss = 0.04390426
Iteration 18, loss = 0.04131265
Iteration 19, loss = 0.03909464
Iteration 20, loss = 0.03746471
Iteration 21, loss = 0.03607677
Iteration 22, loss = 0.03460591
Iteration 23, loss = 0.03351068
Iteration 24, loss = 0.03250799
Iteration 25, loss = 0.03188219
Iteration 26, loss = 0.03097852
Iteration 27, loss = 0.03047173
Iteration 28, loss = 0.02949666
Iteration 29, loss = 0.02907036
Iteration 30, loss = 0.02868343
Iteration 31, loss = 0.02808791
Iteration 32, loss = 0.02755362
Iteration 33, loss = 0.02747761
Iteration 34, loss = 0.02694589
Iteration 35, loss = 0.02682544
Iteration 36, loss = 0.02669570
Iteration 37, loss = 0.02626835
Iteration 38, loss = 0.02626012
Iteration 39, loss = 0.02593069
Iteration 40, loss = 0.02578870
Iteration 41, loss = 0.02570701
Iteration 42, loss = 0.02550084
Iteration 43, loss = 0.02530942
Iteration 44, loss = 0.02504945
Iteration 45, loss = 0.02501412
Iteration 46, loss = 0.02488451
Iteration 47, loss = 0.02480675
Iteration 48, loss = 0.02461181
Iteration 49, loss = 0.02460729
Iteration 50, loss = 0.02451817
Iteration 51, loss = 0.02445322
Iteration 52, loss = 0.02416680
Iteration 53, loss = 0.02448436
Iteration 54, loss = 0.02421291
Iteration 55, loss = 0.02433924
Iteration 56, loss = 0.02400244
Iteration 57, loss = 0.02387091
Iteration 58, loss = 0.02412099
Iteration 59, loss = 0.02396535
Iteration 60, loss = 0.02382372
Iteration 61, loss = 0.02358240
Iteration 62, loss = 0.02371608
Iteration 63, loss = 0.02363434
Iteration 64, loss = 0.02358404
Iteration 65, loss = 0.02369661
Iteration 66, loss = 0.02371262
Iteration 67, loss = 0.02341985
Iteration 68, loss = 0.02362053
Iteration 69, loss = 0.02332902
Iteration 70, loss = 0.02344330
Iteration 71, loss = 0.02352090
Iteration 72, loss = 0.02341973
Iteration 73, loss = 0.02342092
Iteration 74, loss = 0.02322561
Iteration 75, loss = 0.02331384
Iteration 76, loss = 0.02351348
Iteration 77, loss = 0.02296099
Iteration 78, loss = 0.02312297
Iteration 79, loss = 0.02320615
Iteration 80, loss = 0.02311148
Iteration 81, loss = 0.02330269
Iteration 82, loss = 0.02353141
Iteration 83, loss = 0.02305651
Iteration 84, loss = 0.02298082
Iteration 85, loss = 0.02290485
Iteration 86, loss = 0.02319779
Iteration 87, loss = 0.02290068
Iteration 88, loss = 0.02311714
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67215703
Iteration 2, loss = 0.59287390
Iteration 3, loss = 0.47514346
Iteration 4, loss = 0.35992285
Iteration 5, loss = 0.26688761
Iteration 6, loss = 0.20029776
Iteration 7, loss = 0.15405966
Iteration 8, loss = 0.12228048
Iteration 9, loss = 0.10031482
Iteration 10, loss = 0.08459623
Iteration 11, loss = 0.07309153
Iteration 12, loss = 0.06453606
Iteration 13, loss = 0.05809090
Iteration 14, loss = 0.05286224
Iteration 15, loss = 0.04874797
Iteration 16, loss = 0.04541413
Iteration 17, loss = 0.04264959
Iteration 18, loss = 0.04028211
Iteration 19, loss = 0.03842316
Iteration 20, loss = 0.03689423
Iteration 21, loss = 0.03532336
Iteration 22, loss = 0.03417856
Iteration 23, loss = 0.03311696
Iteration 24, loss = 0.03241942
Iteration 25, loss = 0.03150990
Iteration 26, loss = 0.03073401
Iteration 27, loss = 0.03017380
Iteration 28, loss = 0.02953342
Iteration 29, loss = 0.02894171
Iteration 30, loss = 0.02881593
Iteration 31, loss = 0.02831536
Iteration 32, loss = 0.02777739
Iteration 33, loss = 0.02751486
Iteration 34, loss = 0.02724361
Iteration 35, loss = 0.02677786
Iteration 36, loss = 0.02674793
Iteration 37, loss = 0.02665584
Iteration 38, loss = 0.02630325
Iteration 39, loss = 0.02626589
Iteration 40, loss = 0.02640501
Iteration 41, loss = 0.02584904
Iteration 42, loss = 0.02564257
Iteration 43, loss = 0.02566779
Iteration 44, loss = 0.02544223
Iteration 45, loss = 0.02517765
Iteration 46, loss = 0.02507096
Iteration 47, loss = 0.02518346
Iteration 48, loss = 0.02526007
Iteration 49, loss = 0.02500947
Iteration 50, loss = 0.02469724
Iteration 51, loss = 0.02478091
Iteration 52, loss = 0.02466694
Iteration 53, loss = 0.02466122
Iteration 54, loss = 0.02449045
Iteration 55, loss = 0.02446415
Iteration 56, loss = 0.02481645
Iteration 57, loss = 0.02429942
Iteration 58, loss = 0.02443819
Iteration 59, loss = 0.02418807
Iteration 60, loss = 0.02419263
Iteration 61, loss = 0.02388317
Iteration 62, loss = 0.02417523
Iteration 63, loss = 0.02390556
Iteration 64, loss = 0.02393262
Iteration 65, loss = 0.02422015
Iteration 66, loss = 0.02405227
Iteration 67, loss = 0.02402596
Iteration 68, loss = 0.02387616
Iteration 69, loss = 0.02442734
Iteration 70, loss = 0.02408707
Iteration 71, loss = 0.02376602
Iteration 72, loss = 0.02384458
Iteration 73, loss = 0.02364397
Iteration 74, loss = 0.02391364
Iteration 75, loss = 0.02377630
Iteration 76, loss = 0.02340508
Iteration 77, loss = 0.02381004
Iteration 78, loss = 0.02381729
Iteration 79, loss = 0.02347669
Iteration 80, loss = 0.02371558
Iteration 81, loss = 0.02344465
Iteration 82, loss = 0.02374525
Iteration 83, loss = 0.02345336
Iteration 84, loss = 0.02360736
Iteration 85, loss = 0.02328454
Iteration 86, loss = 0.02347259
Iteration 87, loss = 0.02327149
Iteration 88, loss = 0.02329357
Iteration 89, loss = 0.02358872
Iteration 90, loss = 0.02361302
Iteration 91, loss = 0.02328487
Iteration 92, loss = 0.02358271
Iteration 93, loss = 0.02319726
Iteration 94, loss = 0.02351219
Iteration 95, loss = 0.02307637
Iteration 96, loss = 0.02312128
Iteration 97, loss = 0.02336522
Iteration 98, loss = 0.02303239
Iteration 99, loss = 0.02357725
Iteration 100, loss = 0.02373046
Iteration 101, loss = 0.02373578
Iteration 102, loss = 0.02307473
Iteration 103, loss = 0.02346381
Iteration 104, loss = 0.02313510
Iteration 105, loss = 0.02357210
Iteration 106, loss = 0.02319600
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68512769
Iteration 2, loss = 0.59057814
Iteration 3, loss = 0.47649425
Iteration 4, loss = 0.36848637
Iteration 5, loss = 0.27950626
Iteration 6, loss = 0.21174903
Iteration 7, loss = 0.16132459
Iteration 8, loss = 0.12666116
Iteration 9, loss = 0.10240247
Iteration 10, loss = 0.08513276
Iteration 11, loss = 0.07255471
Iteration 12, loss = 0.06326394
Iteration 13, loss = 0.05646556
Iteration 14, loss = 0.05066932
Iteration 15, loss = 0.04644045
Iteration 16, loss = 0.04271779
Iteration 17, loss = 0.03999737
Iteration 18, loss = 0.03751934
Iteration 19, loss = 0.03555862
Iteration 20, loss = 0.03389845
Iteration 21, loss = 0.03248057
Iteration 22, loss = 0.03110830
Iteration 23, loss = 0.03012122
Iteration 24, loss = 0.02924510
Iteration 25, loss = 0.02830048
Iteration 26, loss = 0.02792810
Iteration 27, loss = 0.02739477
Iteration 28, loss = 0.02693307
Iteration 29, loss = 0.02625739
Iteration 30, loss = 0.02546777
Iteration 31, loss = 0.02517992
Iteration 32, loss = 0.02483726
Iteration 33, loss = 0.02434427
Iteration 34, loss = 0.02421930
Iteration 35, loss = 0.02408442
Iteration 36, loss = 0.02391752
Iteration 37, loss = 0.02356216
Iteration 38, loss = 0.02332518
Iteration 39, loss = 0.02334039
Iteration 40, loss = 0.02303651
Iteration 41, loss = 0.02288191
Iteration 42, loss = 0.02270634
Iteration 43, loss = 0.02250782
Iteration 44, loss = 0.02257388
Iteration 45, loss = 0.02238777
Iteration 46, loss = 0.02213716
Iteration 47, loss = 0.02212549
Iteration 48, loss = 0.02187409
Iteration 49, loss = 0.02197415
Iteration 50, loss = 0.02185941
Iteration 51, loss = 0.02158441
Iteration 52, loss = 0.02165900
Iteration 53, loss = 0.02168757
Iteration 54, loss = 0.02154637
Iteration 55, loss = 0.02147468
Iteration 56, loss = 0.02144093
Iteration 57, loss = 0.02124044
Iteration 58, loss = 0.02122010
Iteration 59, loss = 0.02130891
Iteration 60, loss = 0.02140358
Iteration 61, loss = 0.02143409
Iteration 62, loss = 0.02097306
Iteration 63, loss = 0.02109258
Iteration 64, loss = 0.02125626
Iteration 65, loss = 0.02121465
Iteration 66, loss = 0.02089726
Iteration 67, loss = 0.02091115
Iteration 68, loss = 0.02104717
Iteration 69, loss = 0.02097392
Iteration 70, loss = 0.02080535
Iteration 71, loss = 0.02059616
Iteration 72, loss = 0.02079278
Iteration 73, loss = 0.02066301
Iteration 74, loss = 0.02056850
Iteration 75, loss = 0.02074859
Iteration 76, loss = 0.02051242
Iteration 77, loss = 0.02055704
Iteration 78, loss = 0.02042176
Iteration 79, loss = 0.02061962
Iteration 80, loss = 0.02064858
Iteration 81, loss = 0.02050299
Iteration 82, loss = 0.02042957
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69402462
Iteration 2, loss = 0.61454642
Iteration 3, loss = 0.49907108
Iteration 4, loss = 0.37814537
Iteration 5, loss = 0.27849242
Iteration 6, loss = 0.20667744
Iteration 7, loss = 0.15702081
Iteration 8, loss = 0.12330740
Iteration 9, loss = 0.10019183
Iteration 10, loss = 0.08365614
Iteration 11, loss = 0.07186345
Iteration 12, loss = 0.06300798
Iteration 13, loss = 0.05624570
Iteration 14, loss = 0.05084136
Iteration 15, loss = 0.04655538
Iteration 16, loss = 0.04330208
Iteration 17, loss = 0.04028008
Iteration 18, loss = 0.03813249
Iteration 19, loss = 0.03598397
Iteration 20, loss = 0.03453438
Iteration 21, loss = 0.03310077
Iteration 22, loss = 0.03168603
Iteration 23, loss = 0.03064667
Iteration 24, loss = 0.03005702
Iteration 25, loss = 0.02894702
Iteration 26, loss = 0.02804471
Iteration 27, loss = 0.02773923
Iteration 28, loss = 0.02687524
Iteration 29, loss = 0.02644053
Iteration 30, loss = 0.02604015
Iteration 31, loss = 0.02567109
Iteration 32, loss = 0.02518746
Iteration 33, loss = 0.02492151
Iteration 34, loss = 0.02450031
Iteration 35, loss = 0.02436015
Iteration 36, loss = 0.02409825
Iteration 37, loss = 0.02365077
Iteration 38, loss = 0.02357361
Iteration 39, loss = 0.02354832
Iteration 40, loss = 0.02329452
Iteration 41, loss = 0.02313483
Iteration 42, loss = 0.02313086
Iteration 43, loss = 0.02288823
Iteration 44, loss = 0.02274638
Iteration 45, loss = 0.02270160
Iteration 46, loss = 0.02285613
Iteration 47, loss = 0.02235851
Iteration 48, loss = 0.02217293
Iteration 49, loss = 0.02214265
Iteration 50, loss = 0.02221988
Iteration 51, loss = 0.02209509
Iteration 52, loss = 0.02180591
Iteration 53, loss = 0.02181985
Iteration 54, loss = 0.02160672
Iteration 55, loss = 0.02180584
Iteration 56, loss = 0.02148761
Iteration 57, loss = 0.02146949
Iteration 58, loss = 0.02158215
Iteration 59, loss = 0.02146851
Iteration 60, loss = 0.02148449
Iteration 61, loss = 0.02150892
Iteration 62, loss = 0.02148259
Iteration 63, loss = 0.02116005
Iteration 64, loss = 0.02148008
Iteration 65, loss = 0.02126394
Iteration 66, loss = 0.02104382
Iteration 67, loss = 0.02118349
Iteration 68, loss = 0.02135858
Iteration 69, loss = 0.02105245
Iteration 70, loss = 0.02116061
Iteration 71, loss = 0.02096082
Iteration 72, loss = 0.02101823
Iteration 73, loss = 0.02083130
Iteration 74, loss = 0.02087510
Iteration 75, loss = 0.02082029
Iteration 76, loss = 0.02063329
Iteration 77, loss = 0.02085553
Iteration 78, loss = 0.02067103
Iteration 79, loss = 0.02090906
Iteration 80, loss = 0.02058737
Iteration 81, loss = 0.02092958
Iteration 82, loss = 0.02075856
Iteration 83, loss = 0.02058939
Iteration 84, loss = 0.02064764
Iteration 85, loss = 0.02073469
Iteration 86, loss = 0.02098598
Iteration 87, loss = 0.02092496
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68789017
Iteration 2, loss = 0.60950487
Iteration 3, loss = 0.49106606
Iteration 4, loss = 0.37219736
Iteration 5, loss = 0.27462710
Iteration 6, loss = 0.20376483
Iteration 7, loss = 0.15522798
Iteration 8, loss = 0.12189208
Iteration 9, loss = 0.09926120
Iteration 10, loss = 0.08304556
Iteration 11, loss = 0.07111960
Iteration 12, loss = 0.06233387
Iteration 13, loss = 0.05531763
Iteration 14, loss = 0.05002912
Iteration 15, loss = 0.04586692
Iteration 16, loss = 0.04268581
Iteration 17, loss = 0.03971268
Iteration 18, loss = 0.03732981
Iteration 19, loss = 0.03554917
Iteration 20, loss = 0.03381754
Iteration 21, loss = 0.03248745
Iteration 22, loss = 0.03122877
Iteration 23, loss = 0.03029394
Iteration 24, loss = 0.02938716
Iteration 25, loss = 0.02853072
Iteration 26, loss = 0.02782233
Iteration 27, loss = 0.02703611
Iteration 28, loss = 0.02659923
Iteration 29, loss = 0.02617363
Iteration 30, loss = 0.02583816
Iteration 31, loss = 0.02535602
Iteration 32, loss = 0.02501029
Iteration 33, loss = 0.02462880
Iteration 34, loss = 0.02438328
Iteration 35, loss = 0.02398433
Iteration 36, loss = 0.02384666
Iteration 37, loss = 0.02366242
Iteration 38, loss = 0.02344900
Iteration 39, loss = 0.02345824
Iteration 40, loss = 0.02308339
Iteration 41, loss = 0.02300844
Iteration 42, loss = 0.02302055
Iteration 43, loss = 0.02275403
Iteration 44, loss = 0.02237971
Iteration 45, loss = 0.02245937
Iteration 46, loss = 0.02218904
Iteration 47, loss = 0.02234568
Iteration 48, loss = 0.02221181
Iteration 49, loss = 0.02185066
Iteration 50, loss = 0.02173466
Iteration 51, loss = 0.02177217
Iteration 52, loss = 0.02179549
Iteration 53, loss = 0.02164700
Iteration 54, loss = 0.02152021
Iteration 55, loss = 0.02145479
Iteration 56, loss = 0.02143671
Iteration 57, loss = 0.02148696
Iteration 58, loss = 0.02133389
Iteration 59, loss = 0.02137816
Iteration 60, loss = 0.02129899
Iteration 61, loss = 0.02137811
Iteration 62, loss = 0.02135297
Iteration 63, loss = 0.02125452
Iteration 64, loss = 0.02110738
Iteration 65, loss = 0.02086421
Iteration 66, loss = 0.02104442
Iteration 67, loss = 0.02092260
Iteration 68, loss = 0.02083820
Iteration 69, loss = 0.02091618
Iteration 70, loss = 0.02098572
Iteration 71, loss = 0.02105452
Iteration 72, loss = 0.02086983
Iteration 73, loss = 0.02079996
Iteration 74, loss = 0.02097237
Iteration 75, loss = 0.02095682
Iteration 76, loss = 0.02073285
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69313829
Iteration 2, loss = 0.62275574
Iteration 3, loss = 0.51792880
Iteration 4, loss = 0.40196651
Iteration 5, loss = 0.30149995
Iteration 6, loss = 0.22480591
Iteration 7, loss = 0.17116987
Iteration 8, loss = 0.13461731
Iteration 9, loss = 0.10878412
Iteration 10, loss = 0.09115153
Iteration 11, loss = 0.07808539
Iteration 12, loss = 0.06813675
Iteration 13, loss = 0.06100303
Iteration 14, loss = 0.05506860
Iteration 15, loss = 0.05054127
Iteration 16, loss = 0.04672274
Iteration 17, loss = 0.04390426
Iteration 18, loss = 0.04131265
Iteration 19, loss = 0.03909464
Iteration 20, loss = 0.03746471
Iteration 21, loss = 0.03607677
Iteration 22, loss = 0.03460591
Iteration 23, loss = 0.03351068
Iteration 24, loss = 0.03250799
Iteration 25, loss = 0.03188219
Iteration 26, loss = 0.03097852
Iteration 27, loss = 0.03047173
Iteration 28, loss = 0.02949666
Iteration 29, loss = 0.02907036
Iteration 30, loss = 0.02868343
Iteration 31, loss = 0.02808791
Iteration 32, loss = 0.02755362
Iteration 33, loss = 0.02747761
Iteration 34, loss = 0.02694589
Iteration 35, loss = 0.02682544
Iteration 36, loss = 0.02669570
Iteration 37, loss = 0.02626835
Iteration 38, loss = 0.02626012
Iteration 39, loss = 0.02593069
Iteration 40, loss = 0.02578870
Iteration 41, loss = 0.02570701
Iteration 42, loss = 0.02550084
Iteration 43, loss = 0.02530942
Iteration 44, loss = 0.02504945
Iteration 45, loss = 0.02501412
Iteration 46, loss = 0.02488451
Iteration 47, loss = 0.02480675
Iteration 48, loss = 0.02461181
Iteration 49, loss = 0.02460729
Iteration 50, loss = 0.02451817
Iteration 51, loss = 0.02445322
Iteration 52, loss = 0.02416680
Iteration 53, loss = 0.02448436
Iteration 54, loss = 0.02421291
Iteration 55, loss = 0.02433924
Iteration 56, loss = 0.02400244
Iteration 57, loss = 0.02387091
Iteration 58, loss = 0.02412099
Iteration 59, loss = 0.02396535
Iteration 60, loss = 0.02382372
Iteration 61, loss = 0.02358240
Iteration 62, loss = 0.02371608
Iteration 63, loss = 0.02363434
Iteration 64, loss = 0.02358404
Iteration 65, loss = 0.02369661
Iteration 66, loss = 0.02371262
Iteration 67, loss = 0.02341985
Iteration 68, loss = 0.02362053
Iteration 69, loss = 0.02332902
Iteration 70, loss = 0.02344330
Iteration 71, loss = 0.02352090
Iteration 72, loss = 0.02341973
Iteration 73, loss = 0.02342092
Iteration 74, loss = 0.02322561
Iteration 75, loss = 0.02331384
Iteration 76, loss = 0.02351348
Iteration 77, loss = 0.02296099
Iteration 78, loss = 0.02312297
Iteration 79, loss = 0.02320615
Iteration 80, loss = 0.02311148
Iteration 81, loss = 0.02330269
Iteration 82, loss = 0.02353141
Iteration 83, loss = 0.02305651
Iteration 84, loss = 0.02298082
Iteration 85, loss = 0.02290485
Iteration 86, loss = 0.02319779
Iteration 87, loss = 0.02290068
Iteration 88, loss = 0.02311714
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67215703
Iteration 2, loss = 0.59287390
Iteration 3, loss = 0.47514346
Iteration 4, loss = 0.35992285
Iteration 5, loss = 0.26688761
Iteration 6, loss = 0.20029776
Iteration 7, loss = 0.15405966
Iteration 8, loss = 0.12228048
Iteration 9, loss = 0.10031482
Iteration 10, loss = 0.08459623
Iteration 11, loss = 0.07309153
Iteration 12, loss = 0.06453606
Iteration 13, loss = 0.05809090
Iteration 14, loss = 0.05286224
Iteration 15, loss = 0.04874797
Iteration 16, loss = 0.04541413
Iteration 17, loss = 0.04264959
Iteration 18, loss = 0.04028211
Iteration 19, loss = 0.03842316
Iteration 20, loss = 0.03689423
Iteration 21, loss = 0.03532336
Iteration 22, loss = 0.03417856
Iteration 23, loss = 0.03311696
Iteration 24, loss = 0.03241942
Iteration 25, loss = 0.03150990
Iteration 26, loss = 0.03073401
Iteration 27, loss = 0.03017380
Iteration 28, loss = 0.02953342
Iteration 29, loss = 0.02894171
Iteration 30, loss = 0.02881593
Iteration 31, loss = 0.02831536
Iteration 32, loss = 0.02777739
Iteration 33, loss = 0.02751486
Iteration 34, loss = 0.02724361
Iteration 35, loss = 0.02677786
Iteration 36, loss = 0.02674793
Iteration 37, loss = 0.02665584
Iteration 38, loss = 0.02630325
Iteration 39, loss = 0.02626589
Iteration 40, loss = 0.02640501
Iteration 41, loss = 0.02584904
Iteration 42, loss = 0.02564257
Iteration 43, loss = 0.02566779
Iteration 44, loss = 0.02544223
Iteration 45, loss = 0.02517765
Iteration 46, loss = 0.02507096
Iteration 47, loss = 0.02518346
Iteration 48, loss = 0.02526007
Iteration 49, loss = 0.02500947
Iteration 50, loss = 0.02469724
Iteration 51, loss = 0.02478091
Iteration 52, loss = 0.02466694
Iteration 53, loss = 0.02466122
Iteration 54, loss = 0.02449045
Iteration 55, loss = 0.02446415
Iteration 56, loss = 0.02481645
Iteration 57, loss = 0.02429942
Iteration 58, loss = 0.02443819
Iteration 59, loss = 0.02418807
Iteration 60, loss = 0.02419263
Iteration 61, loss = 0.02388317
Iteration 62, loss = 0.02417523
Iteration 63, loss = 0.02390556
Iteration 64, loss = 0.02393262
Iteration 65, loss = 0.02422015
Iteration 66, loss = 0.02405227
Iteration 67, loss = 0.02402596
Iteration 68, loss = 0.02387616
Iteration 69, loss = 0.02442734
Iteration 70, loss = 0.02408707
Iteration 71, loss = 0.02376602
Iteration 72, loss = 0.02384458
Iteration 73, loss = 0.02364397
Iteration 74, loss = 0.02391364
Iteration 75, loss = 0.02377630
Iteration 76, loss = 0.02340508
Iteration 77, loss = 0.02381004
Iteration 78, loss = 0.02381729
Iteration 79, loss = 0.02347669
Iteration 80, loss = 0.02371558
Iteration 81, loss = 0.02344465
Iteration 82, loss = 0.02374525
Iteration 83, loss = 0.02345336
Iteration 84, loss = 0.02360736
Iteration 85, loss = 0.02328454
Iteration 86, loss = 0.02347259
Iteration 87, loss = 0.02327149
Iteration 88, loss = 0.02329357
Iteration 89, loss = 0.02358872
Iteration 90, loss = 0.02361302
Iteration 91, loss = 0.02328487
Iteration 92, loss = 0.02358271
Iteration 93, loss = 0.02319726
Iteration 94, loss = 0.02351219
Iteration 95, loss = 0.02307637
Iteration 96, loss = 0.02312128
Iteration 97, loss = 0.02336522
Iteration 98, loss = 0.02303239
Iteration 99, loss = 0.02357725
Iteration 100, loss = 0.02373046
Iteration 101, loss = 0.02373578
Iteration 102, loss = 0.02307473
Iteration 103, loss = 0.02346381
Iteration 104, loss = 0.02313510
Iteration 105, loss = 0.02357210
Iteration 106, loss = 0.02319600
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68512769
Iteration 2, loss = 0.59057814
Iteration 3, loss = 0.47649425
Iteration 4, loss = 0.36848637
Iteration 5, loss = 0.27950626
Iteration 6, loss = 0.21174903
Iteration 7, loss = 0.16132459
Iteration 8, loss = 0.12666116
Iteration 9, loss = 0.10240247
Iteration 10, loss = 0.08513276
Iteration 11, loss = 0.07255471
Iteration 12, loss = 0.06326394
Iteration 13, loss = 0.05646556
Iteration 14, loss = 0.05066932
Iteration 15, loss = 0.04644045
Iteration 16, loss = 0.04271779
Iteration 17, loss = 0.03999737
Iteration 18, loss = 0.03751934
Iteration 19, loss = 0.03555862
Iteration 20, loss = 0.03389845
Iteration 21, loss = 0.03248057
Iteration 22, loss = 0.03110830
Iteration 23, loss = 0.03012122
Iteration 24, loss = 0.02924510
Iteration 25, loss = 0.02830048
Iteration 26, loss = 0.02792810
Iteration 27, loss = 0.02739477
Iteration 28, loss = 0.02693307
Iteration 29, loss = 0.02625739
Iteration 30, loss = 0.02546777
Iteration 31, loss = 0.02517992
Iteration 32, loss = 0.02483726
Iteration 33, loss = 0.02434427
Iteration 34, loss = 0.02421930
Iteration 35, loss = 0.02408442
Iteration 36, loss = 0.02391752
Iteration 37, loss = 0.02356216
Iteration 38, loss = 0.02332518
Iteration 39, loss = 0.02334039
Iteration 40, loss = 0.02303651
Iteration 41, loss = 0.02288191
Iteration 42, loss = 0.02270634
Iteration 43, loss = 0.02250782
Iteration 44, loss = 0.02257388
Iteration 45, loss = 0.02238777
Iteration 46, loss = 0.02213716
Iteration 47, loss = 0.02212549
Iteration 48, loss = 0.02187409
Iteration 49, loss = 0.02197415
Iteration 50, loss = 0.02185941
Iteration 51, loss = 0.02158441
Iteration 52, loss = 0.02165900
Iteration 53, loss = 0.02168757
Iteration 54, loss = 0.02154637
Iteration 55, loss = 0.02147468
Iteration 56, loss = 0.02144093
Iteration 57, loss = 0.02124044
Iteration 58, loss = 0.02122010
Iteration 59, loss = 0.02130891
Iteration 60, loss = 0.02140358
Iteration 61, loss = 0.02143409
Iteration 62, loss = 0.02097306
Iteration 63, loss = 0.02109258
Iteration 64, loss = 0.02125626
Iteration 65, loss = 0.02121465
Iteration 66, loss = 0.02089726
Iteration 67, loss = 0.02091115
Iteration 68, loss = 0.02104717
Iteration 69, loss = 0.02097392
Iteration 70, loss = 0.02080535
Iteration 71, loss = 0.02059616
Iteration 72, loss = 0.02079278
Iteration 73, loss = 0.02066301
Iteration 74, loss = 0.02056850
Iteration 75, loss = 0.02074859
Iteration 76, loss = 0.02051242
Iteration 77, loss = 0.02055704
Iteration 78, loss = 0.02042176
Iteration 79, loss = 0.02061962
Iteration 80, loss = 0.02064858
Iteration 81, loss = 0.02050299
Iteration 82, loss = 0.02042957
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69402462
Iteration 2, loss = 0.61454642
Iteration 3, loss = 0.49907108
Iteration 4, loss = 0.37814537
Iteration 5, loss = 0.27849242
Iteration 6, loss = 0.20667744
Iteration 7, loss = 0.15702081
Iteration 8, loss = 0.12330740
Iteration 9, loss = 0.10019183
Iteration 10, loss = 0.08365614
Iteration 11, loss = 0.07186345
Iteration 12, loss = 0.06300798
Iteration 13, loss = 0.05624570
Iteration 14, loss = 0.05084136
Iteration 15, loss = 0.04655538
Iteration 16, loss = 0.04330208
Iteration 17, loss = 0.04028008
Iteration 18, loss = 0.03813249
Iteration 19, loss = 0.03598397
Iteration 20, loss = 0.03453438
Iteration 21, loss = 0.03310077
Iteration 22, loss = 0.03168603
Iteration 23, loss = 0.03064667
Iteration 24, loss = 0.03005702
Iteration 25, loss = 0.02894702
Iteration 26, loss = 0.02804471
Iteration 27, loss = 0.02773923
Iteration 28, loss = 0.02687524
Iteration 29, loss = 0.02644053
Iteration 30, loss = 0.02604015
Iteration 31, loss = 0.02567109
Iteration 32, loss = 0.02518746
Iteration 33, loss = 0.02492151
Iteration 34, loss = 0.02450031
Iteration 35, loss = 0.02436015
Iteration 36, loss = 0.02409825
Iteration 37, loss = 0.02365077
Iteration 38, loss = 0.02357361
Iteration 39, loss = 0.02354832
Iteration 40, loss = 0.02329452
Iteration 41, loss = 0.02313483
Iteration 42, loss = 0.02313086
Iteration 43, loss = 0.02288823
Iteration 44, loss = 0.02274638
Iteration 45, loss = 0.02270160
Iteration 46, loss = 0.02285613
Iteration 47, loss = 0.02235851
Iteration 48, loss = 0.02217293
Iteration 49, loss = 0.02214265
Iteration 50, loss = 0.02221988
Iteration 51, loss = 0.02209509
Iteration 52, loss = 0.02180591
Iteration 53, loss = 0.02181985
Iteration 54, loss = 0.02160672
Iteration 55, loss = 0.02180584
Iteration 56, loss = 0.02148761
Iteration 57, loss = 0.02146949
Iteration 58, loss = 0.02158215
Iteration 59, loss = 0.02146851
Iteration 60, loss = 0.02148449
Iteration 61, loss = 0.02150892
Iteration 62, loss = 0.02148259
Iteration 63, loss = 0.02116005
Iteration 64, loss = 0.02148008
Iteration 65, loss = 0.02126394
Iteration 66, loss = 0.02104382
Iteration 67, loss = 0.02118349
Iteration 68, loss = 0.02135858
Iteration 69, loss = 0.02105245
Iteration 70, loss = 0.02116061
Iteration 71, loss = 0.02096082
Iteration 72, loss = 0.02101823
Iteration 73, loss = 0.02083130
Iteration 74, loss = 0.02087510
Iteration 75, loss = 0.02082029
Iteration 76, loss = 0.02063329
Iteration 77, loss = 0.02085553
Iteration 78, loss = 0.02067103
Iteration 79, loss = 0.02090906
Iteration 80, loss = 0.02058737
Iteration 81, loss = 0.02092958
Iteration 82, loss = 0.02075856
Iteration 83, loss = 0.02058939
Iteration 84, loss = 0.02064764
Iteration 85, loss = 0.02073469
Iteration 86, loss = 0.02098598
Iteration 87, loss = 0.02092496
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68789017
Iteration 2, loss = 0.60950487
Iteration 3, loss = 0.49106606
Iteration 4, loss = 0.37219736
Iteration 5, loss = 0.27462710
Iteration 6, loss = 0.20376483
Iteration 7, loss = 0.15522798
Iteration 8, loss = 0.12189208
Iteration 9, loss = 0.09926120
Iteration 10, loss = 0.08304556
Iteration 11, loss = 0.07111960
Iteration 12, loss = 0.06233387
Iteration 13, loss = 0.05531763
Iteration 14, loss = 0.05002912
Iteration 15, loss = 0.04586692
Iteration 16, loss = 0.04268581
Iteration 17, loss = 0.03971268
Iteration 18, loss = 0.03732981
Iteration 19, loss = 0.03554917
Iteration 20, loss = 0.03381754
Iteration 21, loss = 0.03248745
Iteration 22, loss = 0.03122877
Iteration 23, loss = 0.03029394
Iteration 24, loss = 0.02938716
Iteration 25, loss = 0.02853072
Iteration 26, loss = 0.02782233
Iteration 27, loss = 0.02703611
Iteration 28, loss = 0.02659923
Iteration 29, loss = 0.02617363
Iteration 30, loss = 0.02583816
Iteration 31, loss = 0.02535602
Iteration 32, loss = 0.02501029
Iteration 33, loss = 0.02462880
Iteration 34, loss = 0.02438328
Iteration 35, loss = 0.02398433
Iteration 36, loss = 0.02384666
Iteration 37, loss = 0.02366242
Iteration 38, loss = 0.02344900
Iteration 39, loss = 0.02345824
Iteration 40, loss = 0.02308339
Iteration 41, loss = 0.02300844
Iteration 42, loss = 0.02302055
Iteration 43, loss = 0.02275403
Iteration 44, loss = 0.02237971
Iteration 45, loss = 0.02245937
Iteration 46, loss = 0.02218904
Iteration 47, loss = 0.02234568
Iteration 48, loss = 0.02221181
Iteration 49, loss = 0.02185066
Iteration 50, loss = 0.02173466
Iteration 51, loss = 0.02177217
Iteration 52, loss = 0.02179549
Iteration 53, loss = 0.02164700
Iteration 54, loss = 0.02152021
Iteration 55, loss = 0.02145479
Iteration 56, loss = 0.02143671
Iteration 57, loss = 0.02148696
Iteration 58, loss = 0.02133389
Iteration 59, loss = 0.02137816
Iteration 60, loss = 0.02129899
Iteration 61, loss = 0.02137811
Iteration 62, loss = 0.02135297
Iteration 63, loss = 0.02125452
Iteration 64, loss = 0.02110738
Iteration 65, loss = 0.02086421
Iteration 66, loss = 0.02104442
Iteration 67, loss = 0.02092260
Iteration 68, loss = 0.02083820
Iteration 69, loss = 0.02091618
Iteration 70, loss = 0.02098572
Iteration 71, loss = 0.02105452
Iteration 72, loss = 0.02086983
Iteration 73, loss = 0.02079996
Iteration 74, loss = 0.02097237
Iteration 75, loss = 0.02095682
Iteration 76, loss = 0.02073285
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69313829
Iteration 2, loss = 0.62275574
Iteration 3, loss = 0.51792880
Iteration 4, loss = 0.40196651
Iteration 5, loss = 0.30149995
Iteration 6, loss = 0.22480591
Iteration 7, loss = 0.17116987
Iteration 8, loss = 0.13461731
Iteration 9, loss = 0.10878412
Iteration 10, loss = 0.09115153
Iteration 11, loss = 0.07808539
Iteration 12, loss = 0.06813675
Iteration 13, loss = 0.06100303
Iteration 14, loss = 0.05506860
Iteration 15, loss = 0.05054127
Iteration 16, loss = 0.04672274
Iteration 17, loss = 0.04390426
Iteration 18, loss = 0.04131265
Iteration 19, loss = 0.03909464
Iteration 20, loss = 0.03746471
Iteration 21, loss = 0.03607677
Iteration 22, loss = 0.03460591
Iteration 23, loss = 0.03351068
Iteration 24, loss = 0.03250799
Iteration 25, loss = 0.03188219
Iteration 26, loss = 0.03097852
Iteration 27, loss = 0.03047173
Iteration 28, loss = 0.02949666
Iteration 29, loss = 0.02907036
Iteration 30, loss = 0.02868343
Iteration 31, loss = 0.02808791
Iteration 32, loss = 0.02755362
Iteration 33, loss = 0.02747761
Iteration 34, loss = 0.02694589
Iteration 35, loss = 0.02682544
Iteration 36, loss = 0.02669570
Iteration 37, loss = 0.02626835
Iteration 38, loss = 0.02626012
Iteration 39, loss = 0.02593069
Iteration 40, loss = 0.02578870
Iteration 41, loss = 0.02570701
Iteration 42, loss = 0.02550084
Iteration 43, loss = 0.02530942
Iteration 44, loss = 0.02504945
Iteration 45, loss = 0.02501412
Iteration 46, loss = 0.02488451
Iteration 47, loss = 0.02480675
Iteration 48, loss = 0.02461181
Iteration 49, loss = 0.02460729
Iteration 50, loss = 0.02451817
Iteration 51, loss = 0.02445322
Iteration 52, loss = 0.02416680
Iteration 53, loss = 0.02448436
Iteration 54, loss = 0.02421291
Iteration 55, loss = 0.02433924
Iteration 56, loss = 0.02400244
Iteration 57, loss = 0.02387091
Iteration 58, loss = 0.02412099
Iteration 59, loss = 0.02396535
Iteration 60, loss = 0.02382372
Iteration 61, loss = 0.02358240
Iteration 62, loss = 0.02371608
Iteration 63, loss = 0.02363434
Iteration 64, loss = 0.02358404
Iteration 65, loss = 0.02369661
Iteration 66, loss = 0.02371262
Iteration 67, loss = 0.02341985
Iteration 68, loss = 0.02362053
Iteration 69, loss = 0.02332902
Iteration 70, loss = 0.02344330
Iteration 71, loss = 0.02352090
Iteration 72, loss = 0.02341973
Iteration 73, loss = 0.02342092
Iteration 74, loss = 0.02322561
Iteration 75, loss = 0.02331384
Iteration 76, loss = 0.02351348
Iteration 77, loss = 0.02296099
Iteration 78, loss = 0.02312297
Iteration 79, loss = 0.02320615
Iteration 80, loss = 0.02311148
Iteration 81, loss = 0.02330269
Iteration 82, loss = 0.02353141
Iteration 83, loss = 0.02305651
Iteration 84, loss = 0.02298082
Iteration 85, loss = 0.02290485
Iteration 86, loss = 0.02319779
Iteration 87, loss = 0.02290068
Iteration 88, loss = 0.02311714
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67215703
Iteration 2, loss = 0.59287390
Iteration 3, loss = 0.47514346
Iteration 4, loss = 0.35992285
Iteration 5, loss = 0.26688761
Iteration 6, loss = 0.20029776
Iteration 7, loss = 0.15405966
Iteration 8, loss = 0.12228048
Iteration 9, loss = 0.10031482
Iteration 10, loss = 0.08459623
Iteration 11, loss = 0.07309153
Iteration 12, loss = 0.06453606
Iteration 13, loss = 0.05809090
Iteration 14, loss = 0.05286224
Iteration 15, loss = 0.04874797
Iteration 16, loss = 0.04541413
Iteration 17, loss = 0.04264959
Iteration 18, loss = 0.04028211
Iteration 19, loss = 0.03842316
Iteration 20, loss = 0.03689423
Iteration 21, loss = 0.03532336
Iteration 22, loss = 0.03417856
Iteration 23, loss = 0.03311696
Iteration 24, loss = 0.03241942
Iteration 25, loss = 0.03150990
Iteration 26, loss = 0.03073401
Iteration 27, loss = 0.03017380
Iteration 28, loss = 0.02953342
Iteration 29, loss = 0.02894171
Iteration 30, loss = 0.02881593
Iteration 31, loss = 0.02831536
Iteration 32, loss = 0.02777739
Iteration 33, loss = 0.02751486
Iteration 34, loss = 0.02724361
Iteration 35, loss = 0.02677786
Iteration 36, loss = 0.02674793
Iteration 37, loss = 0.02665584
Iteration 38, loss = 0.02630325
Iteration 39, loss = 0.02626589
Iteration 40, loss = 0.02640501
Iteration 41, loss = 0.02584904
Iteration 42, loss = 0.02564257
Iteration 43, loss = 0.02566779
Iteration 44, loss = 0.02544223
Iteration 45, loss = 0.02517765
Iteration 46, loss = 0.02507096
Iteration 47, loss = 0.02518346
Iteration 48, loss = 0.02526007
Iteration 49, loss = 0.02500947
Iteration 50, loss = 0.02469724
Iteration 51, loss = 0.02478091
Iteration 52, loss = 0.02466694
Iteration 53, loss = 0.02466122
Iteration 54, loss = 0.02449045
Iteration 55, loss = 0.02446415
Iteration 56, loss = 0.02481645
Iteration 57, loss = 0.02429942
Iteration 58, loss = 0.02443819
Iteration 59, loss = 0.02418807
Iteration 60, loss = 0.02419263
Iteration 61, loss = 0.02388317
Iteration 62, loss = 0.02417523
Iteration 63, loss = 0.02390556
Iteration 64, loss = 0.02393262
Iteration 65, loss = 0.02422015
Iteration 66, loss = 0.02405227
Iteration 67, loss = 0.02402596
Iteration 68, loss = 0.02387616
Iteration 69, loss = 0.02442734
Iteration 70, loss = 0.02408707
Iteration 71, loss = 0.02376602
Iteration 72, loss = 0.02384458
Iteration 73, loss = 0.02364397
Iteration 74, loss = 0.02391364
Iteration 75, loss = 0.02377630
Iteration 76, loss = 0.02340508
Iteration 77, loss = 0.02381004
Iteration 78, loss = 0.02381729
Iteration 79, loss = 0.02347669
Iteration 80, loss = 0.02371558
Iteration 81, loss = 0.02344465
Iteration 82, loss = 0.02374525
Iteration 83, loss = 0.02345336
Iteration 84, loss = 0.02360736
Iteration 85, loss = 0.02328454
Iteration 86, loss = 0.02347259
Iteration 87, loss = 0.02327149
Iteration 88, loss = 0.02329357
Iteration 89, loss = 0.02358872
Iteration 90, loss = 0.02361302
Iteration 91, loss = 0.02328487
Iteration 92, loss = 0.02358271
Iteration 93, loss = 0.02319726
Iteration 94, loss = 0.02351219
Iteration 95, loss = 0.02307637
Iteration 96, loss = 0.02312128
Iteration 97, loss = 0.02336522
Iteration 98, loss = 0.02303239
Iteration 99, loss = 0.02357725
Iteration 100, loss = 0.02373046
Iteration 101, loss = 0.02373578
Iteration 102, loss = 0.02307473
Iteration 103, loss = 0.02346381
Iteration 104, loss = 0.02313510
Iteration 105, loss = 0.02357210
Iteration 106, loss = 0.02319600
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68512769
Iteration 2, loss = 0.59057814
Iteration 3, loss = 0.47649425
Iteration 4, loss = 0.36848637
Iteration 5, loss = 0.27950626
Iteration 6, loss = 0.21174903
Iteration 7, loss = 0.16132459
Iteration 8, loss = 0.12666116
Iteration 9, loss = 0.10240247
Iteration 10, loss = 0.08513276
Iteration 11, loss = 0.07255471
Iteration 12, loss = 0.06326394
Iteration 13, loss = 0.05646556
Iteration 14, loss = 0.05066932
Iteration 15, loss = 0.04644045
Iteration 16, loss = 0.04271779
Iteration 17, loss = 0.03999737
Iteration 18, loss = 0.03751934
Iteration 19, loss = 0.03555862
Iteration 20, loss = 0.03389845
Iteration 21, loss = 0.03248057
Iteration 22, loss = 0.03110830
Iteration 23, loss = 0.03012122
Iteration 24, loss = 0.02924510
Iteration 25, loss = 0.02830048
Iteration 26, loss = 0.02792810
Iteration 27, loss = 0.02739477
Iteration 28, loss = 0.02693307
Iteration 29, loss = 0.02625739
Iteration 30, loss = 0.02546777
Iteration 31, loss = 0.02517992
Iteration 32, loss = 0.02483726
Iteration 33, loss = 0.02434427
Iteration 34, loss = 0.02421930
Iteration 35, loss = 0.02408442
Iteration 36, loss = 0.02391752
Iteration 37, loss = 0.02356216
Iteration 38, loss = 0.02332518
Iteration 39, loss = 0.02334039
Iteration 40, loss = 0.02303651
Iteration 41, loss = 0.02288191
Iteration 42, loss = 0.02270634
Iteration 43, loss = 0.02250782
Iteration 44, loss = 0.02257388
Iteration 45, loss = 0.02238777
Iteration 46, loss = 0.02213716
Iteration 47, loss = 0.02212549
Iteration 48, loss = 0.02187409
Iteration 49, loss = 0.02197415
Iteration 50, loss = 0.02185941
Iteration 51, loss = 0.02158441
Iteration 52, loss = 0.02165900
Iteration 53, loss = 0.02168757
Iteration 54, loss = 0.02154637
Iteration 55, loss = 0.02147468
Iteration 56, loss = 0.02144093
Iteration 57, loss = 0.02124044
Iteration 58, loss = 0.02122010
Iteration 59, loss = 0.02130891
Iteration 60, loss = 0.02140358
Iteration 61, loss = 0.02143409
Iteration 62, loss = 0.02097306
Iteration 63, loss = 0.02109258
Iteration 64, loss = 0.02125626
Iteration 65, loss = 0.02121465
Iteration 66, loss = 0.02089726
Iteration 67, loss = 0.02091115
Iteration 68, loss = 0.02104717
Iteration 69, loss = 0.02097392
Iteration 70, loss = 0.02080535
Iteration 71, loss = 0.02059616
Iteration 72, loss = 0.02079278
Iteration 73, loss = 0.02066301
Iteration 74, loss = 0.02056850
Iteration 75, loss = 0.02074859
Iteration 76, loss = 0.02051242
Iteration 77, loss = 0.02055704
Iteration 78, loss = 0.02042176
Iteration 79, loss = 0.02061962
Iteration 80, loss = 0.02064858
Iteration 81, loss = 0.02050299
Iteration 82, loss = 0.02042957
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69402462
Iteration 2, loss = 0.61454642
Iteration 3, loss = 0.49907108
Iteration 4, loss = 0.37814537
Iteration 5, loss = 0.27849242
Iteration 6, loss = 0.20667744
Iteration 7, loss = 0.15702081
Iteration 8, loss = 0.12330740
Iteration 9, loss = 0.10019183
Iteration 10, loss = 0.08365614
Iteration 11, loss = 0.07186345
Iteration 12, loss = 0.06300798
Iteration 13, loss = 0.05624570
Iteration 14, loss = 0.05084136
Iteration 15, loss = 0.04655538
Iteration 16, loss = 0.04330208
Iteration 17, loss = 0.04028008
Iteration 18, loss = 0.03813249
Iteration 19, loss = 0.03598397
Iteration 20, loss = 0.03453438
Iteration 21, loss = 0.03310077
Iteration 22, loss = 0.03168603
Iteration 23, loss = 0.03064667
Iteration 24, loss = 0.03005702
Iteration 25, loss = 0.02894702
Iteration 26, loss = 0.02804471
Iteration 27, loss = 0.02773923
Iteration 28, loss = 0.02687524
Iteration 29, loss = 0.02644053
Iteration 30, loss = 0.02604015
Iteration 31, loss = 0.02567109
Iteration 32, loss = 0.02518746
Iteration 33, loss = 0.02492151
Iteration 34, loss = 0.02450031
Iteration 35, loss = 0.02436015
Iteration 36, loss = 0.02409825
Iteration 37, loss = 0.02365077
Iteration 38, loss = 0.02357361
Iteration 39, loss = 0.02354832
Iteration 40, loss = 0.02329452
Iteration 41, loss = 0.02313483
Iteration 42, loss = 0.02313086
Iteration 43, loss = 0.02288823
Iteration 44, loss = 0.02274638
Iteration 45, loss = 0.02270160
Iteration 46, loss = 0.02285613
Iteration 47, loss = 0.02235851
Iteration 48, loss = 0.02217293
Iteration 49, loss = 0.02214265
Iteration 50, loss = 0.02221988
Iteration 51, loss = 0.02209509
Iteration 52, loss = 0.02180591
Iteration 53, loss = 0.02181985
Iteration 54, loss = 0.02160672
Iteration 55, loss = 0.02180584
Iteration 56, loss = 0.02148761
Iteration 57, loss = 0.02146949
Iteration 58, loss = 0.02158215
Iteration 59, loss = 0.02146851
Iteration 60, loss = 0.02148449
Iteration 61, loss = 0.02150892
Iteration 62, loss = 0.02148259
Iteration 63, loss = 0.02116005
Iteration 64, loss = 0.02148008
Iteration 65, loss = 0.02126394
Iteration 66, loss = 0.02104382
Iteration 67, loss = 0.02118349
Iteration 68, loss = 0.02135858
Iteration 69, loss = 0.02105245
Iteration 70, loss = 0.02116061
Iteration 71, loss = 0.02096082
Iteration 72, loss = 0.02101823
Iteration 73, loss = 0.02083130
Iteration 74, loss = 0.02087510
Iteration 75, loss = 0.02082029
Iteration 76, loss = 0.02063329
Iteration 77, loss = 0.02085553
Iteration 78, loss = 0.02067103
Iteration 79, loss = 0.02090906
Iteration 80, loss = 0.02058737
Iteration 81, loss = 0.02092958
Iteration 82, loss = 0.02075856
Iteration 83, loss = 0.02058939
Iteration 84, loss = 0.02064764
Iteration 85, loss = 0.02073469
Iteration 86, loss = 0.02098598
Iteration 87, loss = 0.02092496
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68789017
Iteration 2, loss = 0.60950487
Iteration 3, loss = 0.49106606
Iteration 4, loss = 0.37219736
Iteration 5, loss = 0.27462710
Iteration 6, loss = 0.20376483
Iteration 7, loss = 0.15522798
Iteration 8, loss = 0.12189208
Iteration 9, loss = 0.09926120
Iteration 10, loss = 0.08304556
Iteration 11, loss = 0.07111960
Iteration 12, loss = 0.06233387
Iteration 13, loss = 0.05531763
Iteration 14, loss = 0.05002912
Iteration 15, loss = 0.04586692
Iteration 16, loss = 0.04268581
Iteration 17, loss = 0.03971268
Iteration 18, loss = 0.03732981
Iteration 19, loss = 0.03554917
Iteration 20, loss = 0.03381754
Iteration 21, loss = 0.03248745
Iteration 22, loss = 0.03122877
Iteration 23, loss = 0.03029394
Iteration 24, loss = 0.02938716
Iteration 25, loss = 0.02853072
Iteration 26, loss = 0.02782233
Iteration 27, loss = 0.02703611
Iteration 28, loss = 0.02659923
Iteration 29, loss = 0.02617363
Iteration 30, loss = 0.02583816
Iteration 31, loss = 0.02535602
Iteration 32, loss = 0.02501029
Iteration 33, loss = 0.02462880
Iteration 34, loss = 0.02438328
Iteration 35, loss = 0.02398433
Iteration 36, loss = 0.02384666
Iteration 37, loss = 0.02366242
Iteration 38, loss = 0.02344900
Iteration 39, loss = 0.02345824
Iteration 40, loss = 0.02308339
Iteration 41, loss = 0.02300844
Iteration 42, loss = 0.02302055
Iteration 43, loss = 0.02275403
Iteration 44, loss = 0.02237971
Iteration 45, loss = 0.02245937
Iteration 46, loss = 0.02218904
Iteration 47, loss = 0.02234568
Iteration 48, loss = 0.02221181
Iteration 49, loss = 0.02185066
Iteration 50, loss = 0.02173466
Iteration 51, loss = 0.02177217
Iteration 52, loss = 0.02179549
Iteration 53, loss = 0.02164700
Iteration 54, loss = 0.02152021
Iteration 55, loss = 0.02145479
Iteration 56, loss = 0.02143671
Iteration 57, loss = 0.02148696
Iteration 58, loss = 0.02133389
Iteration 59, loss = 0.02137816
Iteration 60, loss = 0.02129899
Iteration 61, loss = 0.02137811
Iteration 62, loss = 0.02135297
Iteration 63, loss = 0.02125452
Iteration 64, loss = 0.02110738
Iteration 65, loss = 0.02086421
Iteration 66, loss = 0.02104442
Iteration 67, loss = 0.02092260
Iteration 68, loss = 0.02083820
Iteration 69, loss = 0.02091618
Iteration 70, loss = 0.02098572
Iteration 71, loss = 0.02105452
Iteration 72, loss = 0.02086983
Iteration 73, loss = 0.02079996
Iteration 74, loss = 0.02097237
Iteration 75, loss = 0.02095682
Iteration 76, loss = 0.02073285
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69313829
Iteration 2, loss = 0.62275574
Iteration 3, loss = 0.51792880
Iteration 4, loss = 0.40196651
Iteration 5, loss = 0.30149995
Iteration 6, loss = 0.22480591
Iteration 7, loss = 0.17116987
Iteration 8, loss = 0.13461731
Iteration 9, loss = 0.10878412
Iteration 10, loss = 0.09115153
Iteration 11, loss = 0.07808539
Iteration 12, loss = 0.06813675
Iteration 13, loss = 0.06100303
Iteration 14, loss = 0.05506860
Iteration 15, loss = 0.05054127
Iteration 16, loss = 0.04672274
Iteration 17, loss = 0.04390426
Iteration 18, loss = 0.04131265
Iteration 19, loss = 0.03909464
Iteration 20, loss = 0.03746471
Iteration 21, loss = 0.03607677
Iteration 22, loss = 0.03460591
Iteration 23, loss = 0.03351068
Iteration 24, loss = 0.03250799
Iteration 25, loss = 0.03188219
Iteration 26, loss = 0.03097852
Iteration 27, loss = 0.03047173
Iteration 28, loss = 0.02949666
Iteration 29, loss = 0.02907036
Iteration 30, loss = 0.02868343
Iteration 31, loss = 0.02808791
Iteration 32, loss = 0.02755362
Iteration 33, loss = 0.02747761
Iteration 34, loss = 0.02694589
Iteration 35, loss = 0.02682544
Iteration 36, loss = 0.02669570
Iteration 37, loss = 0.02626835
Iteration 38, loss = 0.02626012
Iteration 39, loss = 0.02593069
Iteration 40, loss = 0.02578870
Iteration 41, loss = 0.02570701
Iteration 42, loss = 0.02550084
Iteration 43, loss = 0.02530942
Iteration 44, loss = 0.02504945
Iteration 45, loss = 0.02501412
Iteration 46, loss = 0.02488451
Iteration 47, loss = 0.02480675
Iteration 48, loss = 0.02461181
Iteration 49, loss = 0.02460729
Iteration 50, loss = 0.02451817
Iteration 51, loss = 0.02445322
Iteration 52, loss = 0.02416680
Iteration 53, loss = 0.02448436
Iteration 54, loss = 0.02421291
Iteration 55, loss = 0.02433924
Iteration 56, loss = 0.02400244
Iteration 57, loss = 0.02387091
Iteration 58, loss = 0.02412099
Iteration 59, loss = 0.02396535
Iteration 60, loss = 0.02382372
Iteration 61, loss = 0.02358240
Iteration 62, loss = 0.02371608
Iteration 63, loss = 0.02363434
Iteration 64, loss = 0.02358404
Iteration 65, loss = 0.02369661
Iteration 66, loss = 0.02371262
Iteration 67, loss = 0.02341985
Iteration 68, loss = 0.02362053
Iteration 69, loss = 0.02332902
Iteration 70, loss = 0.02344330
Iteration 71, loss = 0.02352090
Iteration 72, loss = 0.02341973
Iteration 73, loss = 0.02342092
Iteration 74, loss = 0.02322561
Iteration 75, loss = 0.02331384
Iteration 76, loss = 0.02351348
Iteration 77, loss = 0.02296099
Iteration 78, loss = 0.02312297
Iteration 79, loss = 0.02320615
Iteration 80, loss = 0.02311148
Iteration 81, loss = 0.02330269
Iteration 82, loss = 0.02353141
Iteration 83, loss = 0.02305651
Iteration 84, loss = 0.02298082
Iteration 85, loss = 0.02290485
Iteration 86, loss = 0.02319779
Iteration 87, loss = 0.02290068
Iteration 88, loss = 0.02311714
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67215703
Iteration 2, loss = 0.59287390
Iteration 3, loss = 0.47514346
Iteration 4, loss = 0.35992285
Iteration 5, loss = 0.26688761
Iteration 6, loss = 0.20029776
Iteration 7, loss = 0.15405966
Iteration 8, loss = 0.12228048
Iteration 9, loss = 0.10031482
Iteration 10, loss = 0.08459623
Iteration 11, loss = 0.07309153
Iteration 12, loss = 0.06453606
Iteration 13, loss = 0.05809090
Iteration 14, loss = 0.05286224
Iteration 15, loss = 0.04874797
Iteration 16, loss = 0.04541413
Iteration 17, loss = 0.04264959
Iteration 18, loss = 0.04028211
Iteration 19, loss = 0.03842316
Iteration 20, loss = 0.03689423
Iteration 21, loss = 0.03532336
Iteration 22, loss = 0.03417856
Iteration 23, loss = 0.03311696
Iteration 24, loss = 0.03241942
Iteration 25, loss = 0.03150990
Iteration 26, loss = 0.03073401
Iteration 27, loss = 0.03017380
Iteration 28, loss = 0.02953342
Iteration 29, loss = 0.02894171
Iteration 30, loss = 0.02881593
Iteration 31, loss = 0.02831536
Iteration 32, loss = 0.02777739
Iteration 33, loss = 0.02751486
Iteration 34, loss = 0.02724361
Iteration 35, loss = 0.02677786
Iteration 36, loss = 0.02674793
Iteration 37, loss = 0.02665584
Iteration 38, loss = 0.02630325
Iteration 39, loss = 0.02626589
Iteration 40, loss = 0.02640501
Iteration 41, loss = 0.02584904
Iteration 42, loss = 0.02564257
Iteration 43, loss = 0.02566779
Iteration 44, loss = 0.02544223
Iteration 45, loss = 0.02517765
Iteration 46, loss = 0.02507096
Iteration 47, loss = 0.02518346
Iteration 48, loss = 0.02526007
Iteration 49, loss = 0.02500947
Iteration 50, loss = 0.02469724
Iteration 51, loss = 0.02478091
Iteration 52, loss = 0.02466694
Iteration 53, loss = 0.02466122
Iteration 54, loss = 0.02449045
Iteration 55, loss = 0.02446415
Iteration 56, loss = 0.02481645
Iteration 57, loss = 0.02429942
Iteration 58, loss = 0.02443819
Iteration 59, loss = 0.02418807
Iteration 60, loss = 0.02419263
Iteration 61, loss = 0.02388317
Iteration 62, loss = 0.02417523
Iteration 63, loss = 0.02390556
Iteration 64, loss = 0.02393262
Iteration 65, loss = 0.02422015
Iteration 66, loss = 0.02405227
Iteration 67, loss = 0.02402596
Iteration 68, loss = 0.02387616
Iteration 69, loss = 0.02442734
Iteration 70, loss = 0.02408707
Iteration 71, loss = 0.02376602
Iteration 72, loss = 0.02384458
Iteration 73, loss = 0.02364397
Iteration 74, loss = 0.02391364
Iteration 75, loss = 0.02377630
Iteration 76, loss = 0.02340508
Iteration 77, loss = 0.02381004
Iteration 78, loss = 0.02381729
Iteration 79, loss = 0.02347669
Iteration 80, loss = 0.02371558
Iteration 81, loss = 0.02344465
Iteration 82, loss = 0.02374525
Iteration 83, loss = 0.02345336
Iteration 84, loss = 0.02360736
Iteration 85, loss = 0.02328454
Iteration 86, loss = 0.02347259
Iteration 87, loss = 0.02327149
Iteration 88, loss = 0.02329357
Iteration 89, loss = 0.02358872
Iteration 90, loss = 0.02361302
Iteration 91, loss = 0.02328487
Iteration 92, loss = 0.02358271
Iteration 93, loss = 0.02319726
Iteration 94, loss = 0.02351219
Iteration 95, loss = 0.02307637
Iteration 96, loss = 0.02312128
Iteration 97, loss = 0.02336522
Iteration 98, loss = 0.02303239
Iteration 99, loss = 0.02357725
Iteration 100, loss = 0.02373046
Iteration 101, loss = 0.02373578
Iteration 102, loss = 0.02307473
Iteration 103, loss = 0.02346381
Iteration 104, loss = 0.02313510
Iteration 105, loss = 0.02357210
Iteration 106, loss = 0.02319600
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68512769
Iteration 2, loss = 0.59057814
Iteration 3, loss = 0.47649425
Iteration 4, loss = 0.36848637
Iteration 5, loss = 0.27950626
Iteration 6, loss = 0.21174903
Iteration 7, loss = 0.16132459
Iteration 8, loss = 0.12666116
Iteration 9, loss = 0.10240247
Iteration 10, loss = 0.08513276
Iteration 11, loss = 0.07255471
Iteration 12, loss = 0.06326394
Iteration 13, loss = 0.05646556
Iteration 14, loss = 0.05066932
Iteration 15, loss = 0.04644045
Iteration 16, loss = 0.04271779
Iteration 17, loss = 0.03999737
Iteration 18, loss = 0.03751934
Iteration 19, loss = 0.03555862
Iteration 20, loss = 0.03389845
Iteration 21, loss = 0.03248057
Iteration 22, loss = 0.03110830
Iteration 23, loss = 0.03012122
Iteration 24, loss = 0.02924510
Iteration 25, loss = 0.02830048
Iteration 26, loss = 0.02792810
Iteration 27, loss = 0.02739477
Iteration 28, loss = 0.02693307
Iteration 29, loss = 0.02625739
Iteration 30, loss = 0.02546777
Iteration 31, loss = 0.02517992
Iteration 32, loss = 0.02483726
Iteration 33, loss = 0.02434427
Iteration 34, loss = 0.02421930
Iteration 35, loss = 0.02408442
Iteration 36, loss = 0.02391752
Iteration 37, loss = 0.02356216
Iteration 38, loss = 0.02332518
Iteration 39, loss = 0.02334039
Iteration 40, loss = 0.02303651
Iteration 41, loss = 0.02288191
Iteration 42, loss = 0.02270634
Iteration 43, loss = 0.02250782
Iteration 44, loss = 0.02257388
Iteration 45, loss = 0.02238777
Iteration 46, loss = 0.02213716
Iteration 47, loss = 0.02212549
Iteration 48, loss = 0.02187409
Iteration 49, loss = 0.02197415
Iteration 50, loss = 0.02185941
Iteration 51, loss = 0.02158441
Iteration 52, loss = 0.02165900
Iteration 53, loss = 0.02168757
Iteration 54, loss = 0.02154637
Iteration 55, loss = 0.02147468
Iteration 56, loss = 0.02144093
Iteration 57, loss = 0.02124044
Iteration 58, loss = 0.02122010
Iteration 59, loss = 0.02130891
Iteration 60, loss = 0.02140358
Iteration 61, loss = 0.02143409
Iteration 62, loss = 0.02097306
Iteration 63, loss = 0.02109258
Iteration 64, loss = 0.02125626
Iteration 65, loss = 0.02121465
Iteration 66, loss = 0.02089726
Iteration 67, loss = 0.02091115
Iteration 68, loss = 0.02104717
Iteration 69, loss = 0.02097392
Iteration 70, loss = 0.02080535
Iteration 71, loss = 0.02059616
Iteration 72, loss = 0.02079278
Iteration 73, loss = 0.02066301
Iteration 74, loss = 0.02056850
Iteration 75, loss = 0.02074859
Iteration 76, loss = 0.02051242
Iteration 77, loss = 0.02055704
Iteration 78, loss = 0.02042176
Iteration 79, loss = 0.02061962
Iteration 80, loss = 0.02064858
Iteration 81, loss = 0.02050299
Iteration 82, loss = 0.02042957
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69402462
Iteration 2, loss = 0.61454642
Iteration 3, loss = 0.49907108
Iteration 4, loss = 0.37814537
Iteration 5, loss = 0.27849242
Iteration 6, loss = 0.20667744
Iteration 7, loss = 0.15702081
Iteration 8, loss = 0.12330740
Iteration 9, loss = 0.10019183
Iteration 10, loss = 0.08365614
Iteration 11, loss = 0.07186345
Iteration 12, loss = 0.06300798
Iteration 13, loss = 0.05624570
Iteration 14, loss = 0.05084136
Iteration 15, loss = 0.04655538
Iteration 16, loss = 0.04330208
Iteration 17, loss = 0.04028008
Iteration 18, loss = 0.03813249
Iteration 19, loss = 0.03598397
Iteration 20, loss = 0.03453438
Iteration 21, loss = 0.03310077
Iteration 22, loss = 0.03168603
Iteration 23, loss = 0.03064667
Iteration 24, loss = 0.03005702
Iteration 25, loss = 0.02894702
Iteration 26, loss = 0.02804471
Iteration 27, loss = 0.02773923
Iteration 28, loss = 0.02687524
Iteration 29, loss = 0.02644053
Iteration 30, loss = 0.02604015
Iteration 31, loss = 0.02567109
Iteration 32, loss = 0.02518746
Iteration 33, loss = 0.02492151
Iteration 34, loss = 0.02450031
Iteration 35, loss = 0.02436015
Iteration 36, loss = 0.02409825
Iteration 37, loss = 0.02365077
Iteration 38, loss = 0.02357361
Iteration 39, loss = 0.02354832
Iteration 40, loss = 0.02329452
Iteration 41, loss = 0.02313483
Iteration 42, loss = 0.02313086
Iteration 43, loss = 0.02288823
Iteration 44, loss = 0.02274638
Iteration 45, loss = 0.02270160
Iteration 46, loss = 0.02285613
Iteration 47, loss = 0.02235851
Iteration 48, loss = 0.02217293
Iteration 49, loss = 0.02214265
Iteration 50, loss = 0.02221988
Iteration 51, loss = 0.02209509
Iteration 52, loss = 0.02180591
Iteration 53, loss = 0.02181985
Iteration 54, loss = 0.02160672
Iteration 55, loss = 0.02180584
Iteration 56, loss = 0.02148761
Iteration 57, loss = 0.02146949
Iteration 58, loss = 0.02158215
Iteration 59, loss = 0.02146851
Iteration 60, loss = 0.02148449
Iteration 61, loss = 0.02150892
Iteration 62, loss = 0.02148259
Iteration 63, loss = 0.02116005
Iteration 64, loss = 0.02148008
Iteration 65, loss = 0.02126394
Iteration 66, loss = 0.02104382
Iteration 67, loss = 0.02118349
Iteration 68, loss = 0.02135858
Iteration 69, loss = 0.02105245
Iteration 70, loss = 0.02116061
Iteration 71, loss = 0.02096082
Iteration 72, loss = 0.02101823
Iteration 73, loss = 0.02083130
Iteration 74, loss = 0.02087510
Iteration 75, loss = 0.02082029
Iteration 76, loss = 0.02063329
Iteration 77, loss = 0.02085553
Iteration 78, loss = 0.02067103
Iteration 79, loss = 0.02090906
Iteration 80, loss = 0.02058737
Iteration 81, loss = 0.02092958
Iteration 82, loss = 0.02075856
Iteration 83, loss = 0.02058939
Iteration 84, loss = 0.02064764
Iteration 85, loss = 0.02073469
Iteration 86, loss = 0.02098598
Iteration 87, loss = 0.02092496
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68789017
Iteration 2, loss = 0.60950487
Iteration 3, loss = 0.49106606
Iteration 4, loss = 0.37219736
Iteration 5, loss = 0.27462710
Iteration 6, loss = 0.20376483
Iteration 7, loss = 0.15522798
Iteration 8, loss = 0.12189208
Iteration 9, loss = 0.09926120
Iteration 10, loss = 0.08304556
Iteration 11, loss = 0.07111960
Iteration 12, loss = 0.06233387
Iteration 13, loss = 0.05531763
Iteration 14, loss = 0.05002912
Iteration 15, loss = 0.04586692
Iteration 16, loss = 0.04268581
Iteration 17, loss = 0.03971268
Iteration 18, loss = 0.03732981
Iteration 19, loss = 0.03554917
Iteration 20, loss = 0.03381754
Iteration 21, loss = 0.03248745
Iteration 22, loss = 0.03122877
Iteration 23, loss = 0.03029394
Iteration 24, loss = 0.02938716
Iteration 25, loss = 0.02853072
Iteration 26, loss = 0.02782233
Iteration 27, loss = 0.02703611
Iteration 28, loss = 0.02659923
Iteration 29, loss = 0.02617363
Iteration 30, loss = 0.02583816
Iteration 31, loss = 0.02535602
Iteration 32, loss = 0.02501029
Iteration 33, loss = 0.02462880
Iteration 34, loss = 0.02438328
Iteration 35, loss = 0.02398433
Iteration 36, loss = 0.02384666
Iteration 37, loss = 0.02366242
Iteration 38, loss = 0.02344900
Iteration 39, loss = 0.02345824
Iteration 40, loss = 0.02308339
Iteration 41, loss = 0.02300844
Iteration 42, loss = 0.02302055
Iteration 43, loss = 0.02275403
Iteration 44, loss = 0.02237971
Iteration 45, loss = 0.02245937
Iteration 46, loss = 0.02218904
Iteration 47, loss = 0.02234568
Iteration 48, loss = 0.02221181
Iteration 49, loss = 0.02185066
Iteration 50, loss = 0.02173466
Iteration 51, loss = 0.02177217
Iteration 52, loss = 0.02179549
Iteration 53, loss = 0.02164700
Iteration 54, loss = 0.02152021
Iteration 55, loss = 0.02145479
Iteration 56, loss = 0.02143671
Iteration 57, loss = 0.02148696
Iteration 58, loss = 0.02133389
Iteration 59, loss = 0.02137816
Iteration 60, loss = 0.02129899
Iteration 61, loss = 0.02137811
Iteration 62, loss = 0.02135297
Iteration 63, loss = 0.02125452
Iteration 64, loss = 0.02110738
Iteration 65, loss = 0.02086421
Iteration 66, loss = 0.02104442
Iteration 67, loss = 0.02092260
Iteration 68, loss = 0.02083820
Iteration 69, loss = 0.02091618
Iteration 70, loss = 0.02098572
Iteration 71, loss = 0.02105452
Iteration 72, loss = 0.02086983
Iteration 73, loss = 0.02079996
Iteration 74, loss = 0.02097237
Iteration 75, loss = 0.02095682
Iteration 76, loss = 0.02073285
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67101455
Iteration 2, loss = 0.58310390
Iteration 3, loss = 0.45314124
Iteration 4, loss = 0.32955084
Iteration 5, loss = 0.23450896
Iteration 6, loss = 0.17101617
Iteration 7, loss = 0.12977562
Iteration 8, loss = 0.10297658
Iteration 9, loss = 0.08504618
Iteration 10, loss = 0.07260371
Iteration 11, loss = 0.06334615
Iteration 12, loss = 0.05660636
Iteration 13, loss = 0.05138432
Iteration 14, loss = 0.04749693
Iteration 15, loss = 0.04393592
Iteration 16, loss = 0.04152075
Iteration 17, loss = 0.03925474
Iteration 18, loss = 0.03753921
Iteration 19, loss = 0.03607682
Iteration 20, loss = 0.03455101
Iteration 21, loss = 0.03381710
Iteration 22, loss = 0.03282968
Iteration 23, loss = 0.03208197
Iteration 24, loss = 0.03142304
Iteration 25, loss = 0.03054546
Iteration 26, loss = 0.02998057
Iteration 27, loss = 0.02982697
Iteration 28, loss = 0.02929111
Iteration 29, loss = 0.02894831
Iteration 30, loss = 0.02837174
Iteration 31, loss = 0.02834618
Iteration 32, loss = 0.02775539
Iteration 33, loss = 0.02750509
Iteration 34, loss = 0.02713032
Iteration 35, loss = 0.02728717
Iteration 36, loss = 0.02701203
Iteration 37, loss = 0.02680486
Iteration 38, loss = 0.02693901
Iteration 39, loss = 0.02668100
Iteration 40, loss = 0.02636100
Iteration 41, loss = 0.02627907
Iteration 42, loss = 0.02628930
Iteration 43, loss = 0.02597220
Iteration 44, loss = 0.02623727
Iteration 45, loss = 0.02591092
Iteration 46, loss = 0.02592042
Iteration 47, loss = 0.02557383
Iteration 48, loss = 0.02573890
Iteration 49, loss = 0.02597600
Iteration 50, loss = 0.02592596
Iteration 51, loss = 0.02562321
Iteration 52, loss = 0.02558254
Iteration 53, loss = 0.02509929
Iteration 54, loss = 0.02545586
Iteration 55, loss = 0.02502983
Iteration 56, loss = 0.02521926
Iteration 57, loss = 0.02524768
Iteration 58, loss = 0.02508287
Iteration 59, loss = 0.02519130
Iteration 60, loss = 0.02522885
Iteration 61, loss = 0.02511552
Iteration 62, loss = 0.02492804
Iteration 63, loss = 0.02469637
Iteration 64, loss = 0.02494272
Iteration 65, loss = 0.02483987
Iteration 66, loss = 0.02498923
Iteration 67, loss = 0.02469644
Iteration 68, loss = 0.02515044
Iteration 69, loss = 0.02502443
Iteration 70, loss = 0.02470006
Iteration 71, loss = 0.02476338
Iteration 72, loss = 0.02453338
Iteration 73, loss = 0.02474568
Iteration 74, loss = 0.02495432
Iteration 75, loss = 0.02521835
Iteration 76, loss = 0.02465921
Iteration 77, loss = 0.02451655
Iteration 78, loss = 0.02483591
Iteration 79, loss = 0.02439021
Iteration 80, loss = 0.02461820
Iteration 81, loss = 0.02430646
Iteration 82, loss = 0.02439051
Iteration 83, loss = 0.02485916
Iteration 84, loss = 0.02426648
Iteration 85, loss = 0.02444819
Iteration 86, loss = 0.02433723
Iteration 87, loss = 0.02442051
Iteration 88, loss = 0.02451405
Iteration 89, loss = 0.02427471
Iteration 90, loss = 0.02405854
Iteration 91, loss = 0.02454516
Iteration 92, loss = 0.02457586
Iteration 93, loss = 0.02438567
Iteration 94, loss = 0.02409745
Iteration 95, loss = 0.02454255
Iteration 96, loss = 0.02422285
Iteration 97, loss = 0.02403611
Iteration 98, loss = 0.02407854
Iteration 99, loss = 0.02432959
Iteration 100, loss = 0.02432732
Iteration 101, loss = 0.02446706
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on prepended_v3_lemmatized...
Iteration 1, loss = 0.69046992
Iteration 2, loss = 0.66693358
Iteration 3, loss = 0.64242996
Iteration 4, loss = 0.61652713
Iteration 5, loss = 0.58915098
Iteration 6, loss = 0.55925610
Iteration 7, loss = 0.52775490
Iteration 8, loss = 0.49524102
Iteration 9, loss = 0.46149653
Iteration 10, loss = 0.42744711
Iteration 11, loss = 0.39428377
Iteration 12, loss = 0.36251358
Iteration 13, loss = 0.33244520
Iteration 14, loss = 0.30437274
Iteration 15, loss = 0.27819872
Iteration 16, loss = 0.25419645
Iteration 17, loss = 0.23217303
Iteration 18, loss = 0.21203956
Iteration 19, loss = 0.19390825
Iteration 20, loss = 0.17741419
Iteration 21, loss = 0.16262078
Iteration 22, loss = 0.14928648
Iteration 23, loss = 0.13743660
Iteration 24, loss = 0.12672891
Iteration 25, loss = 0.11712113
Iteration 26, loss = 0.10859057
Iteration 27, loss = 0.10102171
Iteration 28, loss = 0.09408146
Iteration 29, loss = 0.08785718
Iteration 30, loss = 0.08228222
Iteration 31, loss = 0.07728259
Iteration 32, loss = 0.07272788
Iteration 33, loss = 0.06857193
Iteration 34, loss = 0.06480731
Iteration 35, loss = 0.06138943
Iteration 36, loss = 0.05841759
Iteration 37, loss = 0.05547593
Iteration 38, loss = 0.05292388
Iteration 39, loss = 0.05050805
Iteration 40, loss = 0.04830372
Iteration 41, loss = 0.04637735
Iteration 42, loss = 0.04442566
Iteration 43, loss = 0.04273647
Iteration 44, loss = 0.04108162
Iteration 45, loss = 0.03961544
Iteration 46, loss = 0.03823559
Iteration 47, loss = 0.03698522
Iteration 48, loss = 0.03577432
Iteration 49, loss = 0.03466784
Iteration 50, loss = 0.03366666
Iteration 51, loss = 0.03263822
Iteration 52, loss = 0.03170190
Iteration 53, loss = 0.03082806
Iteration 54, loss = 0.03003820
Iteration 55, loss = 0.02925270
Iteration 56, loss = 0.02855404
Iteration 57, loss = 0.02786278
Iteration 58, loss = 0.02719696
Iteration 59, loss = 0.02663318
Iteration 60, loss = 0.02597569
Iteration 61, loss = 0.02547543
Iteration 62, loss = 0.02491611
Iteration 63, loss = 0.02444699
Iteration 64, loss = 0.02405034
Iteration 65, loss = 0.02354011
Iteration 66, loss = 0.02313951
Iteration 67, loss = 0.02271639
Iteration 68, loss = 0.02232890
Iteration 69, loss = 0.02195028
Iteration 70, loss = 0.02159693
Iteration 71, loss = 0.02125081
Iteration 72, loss = 0.02092726
Iteration 73, loss = 0.02061229
Iteration 74, loss = 0.02034039
Iteration 75, loss = 0.02006266
Iteration 76, loss = 0.01981165
Iteration 77, loss = 0.01959672
Iteration 78, loss = 0.01928432
Iteration 79, loss = 0.01902287
Iteration 80, loss = 0.01881134
Iteration 81, loss = 0.01856105
Iteration 82, loss = 0.01836063
Iteration 83, loss = 0.01816469
Iteration 84, loss = 0.01796812
Iteration 85, loss = 0.01776692
Iteration 86, loss = 0.01759492
Iteration 87, loss = 0.01745221
Iteration 88, loss = 0.01729016
Iteration 89, loss = 0.01707355
Iteration 90, loss = 0.01691671
Iteration 91, loss = 0.01677247
Iteration 92, loss = 0.01660410
Iteration 93, loss = 0.01649781
Iteration 94, loss = 0.01638249
Iteration 95, loss = 0.01621743
Iteration 96, loss = 0.01612827
Iteration 97, loss = 0.01594810
Iteration 98, loss = 0.01584578
Iteration 99, loss = 0.01573068
Iteration 100, loss = 0.01564260
Iteration 101, loss = 0.01558016
Iteration 102, loss = 0.01542845
Iteration 103, loss = 0.01530722
Iteration 104, loss = 0.01524287
Iteration 105, loss = 0.01514863
Iteration 106, loss = 0.01505767
Iteration 107, loss = 0.01493931
Iteration 108, loss = 0.01482632
Iteration 109, loss = 0.01480727
Iteration 110, loss = 0.01467749
Iteration 111, loss = 0.01462541
Iteration 112, loss = 0.01458798
Iteration 113, loss = 0.01443980
Iteration 114, loss = 0.01442558
Iteration 115, loss = 0.01426004
Iteration 116, loss = 0.01421947
Iteration 117, loss = 0.01419348
Iteration 118, loss = 0.01412630
Iteration 119, loss = 0.01401022
Iteration 120, loss = 0.01400969
Iteration 121, loss = 0.01392466
Iteration 122, loss = 0.01385561
Iteration 123, loss = 0.01385501
Iteration 124, loss = 0.01374707
Iteration 125, loss = 0.01370290
Iteration 126, loss = 0.01360450
Iteration 127, loss = 0.01356427
Iteration 128, loss = 0.01354046
Iteration 129, loss = 0.01344456
Iteration 130, loss = 0.01345192
Iteration 131, loss = 0.01334681
Iteration 132, loss = 0.01333895
Iteration 133, loss = 0.01335996
Iteration 134, loss = 0.01320291
Iteration 135, loss = 0.01329235
Iteration 136, loss = 0.01314514
Iteration 137, loss = 0.01315649
Iteration 138, loss = 0.01306441
Iteration 139, loss = 0.01306627
Iteration 140, loss = 0.01301053
Iteration 141, loss = 0.01295477
Iteration 142, loss = 0.01299622
Iteration 143, loss = 0.01293702
Iteration 144, loss = 0.01290074
Iteration 145, loss = 0.01286707
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73288922
Iteration 2, loss = 0.70577568
Iteration 3, loss = 0.67548474
Iteration 4, loss = 0.63850928
Iteration 5, loss = 0.59612429
Iteration 6, loss = 0.54958988
Iteration 7, loss = 0.50114097
Iteration 8, loss = 0.45150130
Iteration 9, loss = 0.40238544
Iteration 10, loss = 0.35569363
Iteration 11, loss = 0.31270044
Iteration 12, loss = 0.27444763
Iteration 13, loss = 0.24090027
Iteration 14, loss = 0.21170606
Iteration 15, loss = 0.18667437
Iteration 16, loss = 0.16515916
Iteration 17, loss = 0.14708782
Iteration 18, loss = 0.13152352
Iteration 19, loss = 0.11843428
Iteration 20, loss = 0.10713035
Iteration 21, loss = 0.09740207
Iteration 22, loss = 0.08905925
Iteration 23, loss = 0.08181622
Iteration 24, loss = 0.07557065
Iteration 25, loss = 0.07005228
Iteration 26, loss = 0.06522896
Iteration 27, loss = 0.06090132
Iteration 28, loss = 0.05712456
Iteration 29, loss = 0.05395794
Iteration 30, loss = 0.05098262
Iteration 31, loss = 0.04836228
Iteration 32, loss = 0.04577637
Iteration 33, loss = 0.04361536
Iteration 34, loss = 0.04157033
Iteration 35, loss = 0.03981025
Iteration 36, loss = 0.03810733
Iteration 37, loss = 0.03670414
Iteration 38, loss = 0.03532462
Iteration 39, loss = 0.03410004
Iteration 40, loss = 0.03292549
Iteration 41, loss = 0.03197813
Iteration 42, loss = 0.03095457
Iteration 43, loss = 0.03004613
Iteration 44, loss = 0.02920247
Iteration 45, loss = 0.02846886
Iteration 46, loss = 0.02774270
Iteration 47, loss = 0.02713389
Iteration 48, loss = 0.02639899
Iteration 49, loss = 0.02593112
Iteration 50, loss = 0.02534455
Iteration 51, loss = 0.02479993
Iteration 52, loss = 0.02430561
Iteration 53, loss = 0.02383426
Iteration 54, loss = 0.02336331
Iteration 55, loss = 0.02289856
Iteration 56, loss = 0.02254593
Iteration 57, loss = 0.02211909
Iteration 58, loss = 0.02180782
Iteration 59, loss = 0.02154762
Iteration 60, loss = 0.02162942
Iteration 61, loss = 0.02130740
Iteration 62, loss = 0.02100470
Iteration 63, loss = 0.02059508
Iteration 64, loss = 0.02029560
Iteration 65, loss = 0.01999082
Iteration 66, loss = 0.01973968
Iteration 67, loss = 0.01945640
Iteration 68, loss = 0.01939645
Iteration 69, loss = 0.01944881
Iteration 70, loss = 0.01925881
Iteration 71, loss = 0.01898131
Iteration 72, loss = 0.01872428
Iteration 73, loss = 0.01850186
Iteration 74, loss = 0.01838536
Iteration 75, loss = 0.01819670
Iteration 76, loss = 0.01807757
Iteration 77, loss = 0.01783119
Iteration 78, loss = 0.01770184
Iteration 79, loss = 0.01755050
Iteration 80, loss = 0.01751372
Iteration 81, loss = 0.01733267
Iteration 82, loss = 0.01720375
Iteration 83, loss = 0.01705013
Iteration 84, loss = 0.01700862
Iteration 85, loss = 0.01697573
Iteration 86, loss = 0.01686826
Iteration 87, loss = 0.01673833
Iteration 88, loss = 0.01665623
Iteration 89, loss = 0.01653202
Iteration 90, loss = 0.01635277
Iteration 91, loss = 0.01632622
Iteration 92, loss = 0.01626918
Iteration 93, loss = 0.01625660
Iteration 94, loss = 0.01616429
Iteration 95, loss = 0.01594613
Iteration 96, loss = 0.01602643
Iteration 97, loss = 0.01588884
Iteration 98, loss = 0.01588038
Iteration 99, loss = 0.01577574
Iteration 100, loss = 0.01592755
Iteration 101, loss = 0.01627151
Iteration 102, loss = 0.01613130
Iteration 103, loss = 0.01606625
Iteration 104, loss = 0.01587012
Iteration 105, loss = 0.01584010
Iteration 106, loss = 0.01588870
Iteration 107, loss = 0.01573395
Iteration 108, loss = 0.01564837
Iteration 109, loss = 0.01547595
Iteration 110, loss = 0.01525250
Iteration 111, loss = 0.01516376
Iteration 112, loss = 0.01508843
Iteration 113, loss = 0.01505499
Iteration 114, loss = 0.01495017
Iteration 115, loss = 0.01493299
Iteration 116, loss = 0.01481349
Iteration 117, loss = 0.01493214
Iteration 118, loss = 0.01499785
Iteration 119, loss = 0.01491794
Iteration 120, loss = 0.01510823
Iteration 121, loss = 0.01531599
Iteration 122, loss = 0.01521789
Iteration 123, loss = 0.01532627
Iteration 124, loss = 0.01541815
Iteration 125, loss = 0.01486021
Iteration 126, loss = 0.01492027
Iteration 127, loss = 0.01471811
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72041440
Iteration 2, loss = 0.70094920
Iteration 3, loss = 0.68421465
Iteration 4, loss = 0.66798202
Iteration 5, loss = 0.65067166
Iteration 6, loss = 0.63215398
Iteration 7, loss = 0.61312662
Iteration 8, loss = 0.59242321
Iteration 9, loss = 0.57078142
Iteration 10, loss = 0.54862385
Iteration 11, loss = 0.52547943
Iteration 12, loss = 0.50186866
Iteration 13, loss = 0.47796371
Iteration 14, loss = 0.45397480
Iteration 15, loss = 0.43023452
Iteration 16, loss = 0.40660018
Iteration 17, loss = 0.38342641
Iteration 18, loss = 0.36097356
Iteration 19, loss = 0.33916551
Iteration 20, loss = 0.31837014
Iteration 21, loss = 0.29841087
Iteration 22, loss = 0.27950149
Iteration 23, loss = 0.26162182
Iteration 24, loss = 0.24480396
Iteration 25, loss = 0.22907969
Iteration 26, loss = 0.21433146
Iteration 27, loss = 0.20062388
Iteration 28, loss = 0.18779098
Iteration 29, loss = 0.17592883
Iteration 30, loss = 0.16497732
Iteration 31, loss = 0.15475651
Iteration 32, loss = 0.14539073
Iteration 33, loss = 0.13672886
Iteration 34, loss = 0.12863549
Iteration 35, loss = 0.12125054
Iteration 36, loss = 0.11434853
Iteration 37, loss = 0.10802262
Iteration 38, loss = 0.10218180
Iteration 39, loss = 0.09677574
Iteration 40, loss = 0.09179554
Iteration 41, loss = 0.08713578
Iteration 42, loss = 0.08289136
Iteration 43, loss = 0.07887139
Iteration 44, loss = 0.07520576
Iteration 45, loss = 0.07177163
Iteration 46, loss = 0.06856961
Iteration 47, loss = 0.06557644
Iteration 48, loss = 0.06280833
Iteration 49, loss = 0.06021707
Iteration 50, loss = 0.05779446
Iteration 51, loss = 0.05552437
Iteration 52, loss = 0.05343471
Iteration 53, loss = 0.05145267
Iteration 54, loss = 0.04957248
Iteration 55, loss = 0.04781370
Iteration 56, loss = 0.04617030
Iteration 57, loss = 0.04460163
Iteration 58, loss = 0.04318405
Iteration 59, loss = 0.04179503
Iteration 60, loss = 0.04046180
Iteration 61, loss = 0.03923791
Iteration 62, loss = 0.03805319
Iteration 63, loss = 0.03699329
Iteration 64, loss = 0.03594198
Iteration 65, loss = 0.03492155
Iteration 66, loss = 0.03396040
Iteration 67, loss = 0.03304750
Iteration 68, loss = 0.03219487
Iteration 69, loss = 0.03139927
Iteration 70, loss = 0.03060872
Iteration 71, loss = 0.02986076
Iteration 72, loss = 0.02917797
Iteration 73, loss = 0.02852753
Iteration 74, loss = 0.02787371
Iteration 75, loss = 0.02727735
Iteration 76, loss = 0.02669384
Iteration 77, loss = 0.02612813
Iteration 78, loss = 0.02558767
Iteration 79, loss = 0.02506000
Iteration 80, loss = 0.02454963
Iteration 81, loss = 0.02407352
Iteration 82, loss = 0.02369035
Iteration 83, loss = 0.02323161
Iteration 84, loss = 0.02278590
Iteration 85, loss = 0.02236944
Iteration 86, loss = 0.02199615
Iteration 87, loss = 0.02163874
Iteration 88, loss = 0.02127430
Iteration 89, loss = 0.02092496
Iteration 90, loss = 0.02057942
Iteration 91, loss = 0.02028313
Iteration 92, loss = 0.01996754
Iteration 93, loss = 0.01966876
Iteration 94, loss = 0.01937108
Iteration 95, loss = 0.01911237
Iteration 96, loss = 0.01880675
Iteration 97, loss = 0.01854542
Iteration 98, loss = 0.01826791
Iteration 99, loss = 0.01805542
Iteration 100, loss = 0.01777882
Iteration 101, loss = 0.01757119
Iteration 102, loss = 0.01736289
Iteration 103, loss = 0.01710944
Iteration 104, loss = 0.01692333
Iteration 105, loss = 0.01672449
Iteration 106, loss = 0.01650898
Iteration 107, loss = 0.01631512
Iteration 108, loss = 0.01612754
Iteration 109, loss = 0.01594739
Iteration 110, loss = 0.01577849
Iteration 111, loss = 0.01560029
Iteration 112, loss = 0.01543367
Iteration 113, loss = 0.01527668
Iteration 114, loss = 0.01513807
Iteration 115, loss = 0.01495828
Iteration 116, loss = 0.01480873
Iteration 117, loss = 0.01464981
Iteration 118, loss = 0.01453177
Iteration 119, loss = 0.01439183
Iteration 120, loss = 0.01432695
Iteration 121, loss = 0.01415620
Iteration 122, loss = 0.01403979
Iteration 123, loss = 0.01389218
Iteration 124, loss = 0.01382027
Iteration 125, loss = 0.01363988
Iteration 126, loss = 0.01353469
Iteration 127, loss = 0.01343832
Iteration 128, loss = 0.01330321
Iteration 129, loss = 0.01319922
Iteration 130, loss = 0.01310305
Iteration 131, loss = 0.01299828
Iteration 132, loss = 0.01290438
Iteration 133, loss = 0.01281600
Iteration 134, loss = 0.01275808
Iteration 135, loss = 0.01265176
Iteration 136, loss = 0.01256464
Iteration 137, loss = 0.01246342
Iteration 138, loss = 0.01236159
Iteration 139, loss = 0.01227356
Iteration 140, loss = 0.01220652
Iteration 141, loss = 0.01211789
Iteration 142, loss = 0.01207268
Iteration 143, loss = 0.01198597
Iteration 144, loss = 0.01195305
Iteration 145, loss = 0.01184541
Iteration 146, loss = 0.01177078
Iteration 147, loss = 0.01170265
Iteration 148, loss = 0.01162180
Iteration 149, loss = 0.01157558
Iteration 150, loss = 0.01150530
Iteration 151, loss = 0.01144350
Iteration 152, loss = 0.01136156
Iteration 153, loss = 0.01131655
Iteration 154, loss = 0.01124678
Iteration 155, loss = 0.01119691
Iteration 156, loss = 0.01111330
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71152705
Iteration 2, loss = 0.68163257
Iteration 3, loss = 0.65047455
Iteration 4, loss = 0.61430253
Iteration 5, loss = 0.57224866
Iteration 6, loss = 0.52482811
Iteration 7, loss = 0.47540294
Iteration 8, loss = 0.42573176
Iteration 9, loss = 0.37792261
Iteration 10, loss = 0.33345263
Iteration 11, loss = 0.29296247
Iteration 12, loss = 0.25686603
Iteration 13, loss = 0.22523150
Iteration 14, loss = 0.19780812
Iteration 15, loss = 0.17436096
Iteration 16, loss = 0.15426564
Iteration 17, loss = 0.13717514
Iteration 18, loss = 0.12262729
Iteration 19, loss = 0.11015338
Iteration 20, loss = 0.09963165
Iteration 21, loss = 0.09048454
Iteration 22, loss = 0.08264489
Iteration 23, loss = 0.07576691
Iteration 24, loss = 0.06990633
Iteration 25, loss = 0.06473769
Iteration 26, loss = 0.06019842
Iteration 27, loss = 0.05620201
Iteration 28, loss = 0.05263301
Iteration 29, loss = 0.04951780
Iteration 30, loss = 0.04666594
Iteration 31, loss = 0.04405999
Iteration 32, loss = 0.04185782
Iteration 33, loss = 0.03987758
Iteration 34, loss = 0.03803872
Iteration 35, loss = 0.03626637
Iteration 36, loss = 0.03486053
Iteration 37, loss = 0.03344191
Iteration 38, loss = 0.03221306
Iteration 39, loss = 0.03102334
Iteration 40, loss = 0.02984925
Iteration 41, loss = 0.02902126
Iteration 42, loss = 0.02809776
Iteration 43, loss = 0.02724643
Iteration 44, loss = 0.02638427
Iteration 45, loss = 0.02572854
Iteration 46, loss = 0.02502551
Iteration 47, loss = 0.02427920
Iteration 48, loss = 0.02381990
Iteration 49, loss = 0.02325568
Iteration 50, loss = 0.02268684
Iteration 51, loss = 0.02216067
Iteration 52, loss = 0.02178376
Iteration 53, loss = 0.02136947
Iteration 54, loss = 0.02109222
Iteration 55, loss = 0.02074342
Iteration 56, loss = 0.02023171
Iteration 57, loss = 0.01982391
Iteration 58, loss = 0.01943184
Iteration 59, loss = 0.01912609
Iteration 60, loss = 0.01888335
Iteration 61, loss = 0.01857808
Iteration 62, loss = 0.01823912
Iteration 63, loss = 0.01800999
Iteration 64, loss = 0.01770921
Iteration 65, loss = 0.01747877
Iteration 66, loss = 0.01729779
Iteration 67, loss = 0.01708369
Iteration 68, loss = 0.01688305
Iteration 69, loss = 0.01674858
Iteration 70, loss = 0.01650762
Iteration 71, loss = 0.01630739
Iteration 72, loss = 0.01610540
Iteration 73, loss = 0.01595009
Iteration 74, loss = 0.01578172
Iteration 75, loss = 0.01565986
Iteration 76, loss = 0.01552812
Iteration 77, loss = 0.01538978
Iteration 78, loss = 0.01533155
Iteration 79, loss = 0.01521909
Iteration 80, loss = 0.01519058
Iteration 81, loss = 0.01535870
Iteration 82, loss = 0.01509438
Iteration 83, loss = 0.01505455
Iteration 84, loss = 0.01490918
Iteration 85, loss = 0.01471715
Iteration 86, loss = 0.01447930
Iteration 87, loss = 0.01460914
Iteration 88, loss = 0.01454996
Iteration 89, loss = 0.01443102
Iteration 90, loss = 0.01423270
Iteration 91, loss = 0.01410556
Iteration 92, loss = 0.01403698
Iteration 93, loss = 0.01391264
Iteration 94, loss = 0.01381850
Iteration 95, loss = 0.01379121
Iteration 96, loss = 0.01368732
Iteration 97, loss = 0.01372831
Iteration 98, loss = 0.01366493
Iteration 99, loss = 0.01347625
Iteration 100, loss = 0.01348716
Iteration 101, loss = 0.01351065
Iteration 102, loss = 0.01346644
Iteration 103, loss = 0.01369583
Iteration 104, loss = 0.01371840
Iteration 105, loss = 0.01355253
Iteration 106, loss = 0.01349940
Iteration 107, loss = 0.01320783
Iteration 108, loss = 0.01312007
Iteration 109, loss = 0.01312923
Iteration 110, loss = 0.01305311
Iteration 111, loss = 0.01303132
Iteration 112, loss = 0.01301208
Iteration 113, loss = 0.01290106
Iteration 114, loss = 0.01289231
Iteration 115, loss = 0.01285600
Iteration 116, loss = 0.01279925
Iteration 117, loss = 0.01321357
Iteration 118, loss = 0.01303344
Iteration 119, loss = 0.01312255
Iteration 120, loss = 0.01318062
Iteration 121, loss = 0.01293191
Iteration 122, loss = 0.01260769
Iteration 123, loss = 0.01262496
Iteration 124, loss = 0.01272237
Iteration 125, loss = 0.01273303
Iteration 126, loss = 0.01265263
Iteration 127, loss = 0.01256098
Iteration 128, loss = 0.01248392
Iteration 129, loss = 0.01237412
Iteration 130, loss = 0.01231859
Iteration 131, loss = 0.01225298
Iteration 132, loss = 0.01225623
Iteration 133, loss = 0.01223149
Iteration 134, loss = 0.01225990
Iteration 135, loss = 0.01225552
Iteration 136, loss = 0.01230522
Iteration 137, loss = 0.01237034
Iteration 138, loss = 0.01217021
Iteration 139, loss = 0.01233053
Iteration 140, loss = 0.01233660
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68492431
Iteration 2, loss = 0.64866068
Iteration 3, loss = 0.59534416
Iteration 4, loss = 0.52640288
Iteration 5, loss = 0.45382045
Iteration 6, loss = 0.38374357
Iteration 7, loss = 0.32017406
Iteration 8, loss = 0.26548998
Iteration 9, loss = 0.22006680
Iteration 10, loss = 0.18368270
Iteration 11, loss = 0.15448020
Iteration 12, loss = 0.13149674
Iteration 13, loss = 0.11326377
Iteration 14, loss = 0.09880621
Iteration 15, loss = 0.08698870
Iteration 16, loss = 0.07763281
Iteration 17, loss = 0.06986567
Iteration 18, loss = 0.06348033
Iteration 19, loss = 0.05803734
Iteration 20, loss = 0.05354491
Iteration 21, loss = 0.04962789
Iteration 22, loss = 0.04654367
Iteration 23, loss = 0.04364473
Iteration 24, loss = 0.04114022
Iteration 25, loss = 0.03887644
Iteration 26, loss = 0.03713506
Iteration 27, loss = 0.03578991
Iteration 28, loss = 0.03400275
Iteration 29, loss = 0.03271029
Iteration 30, loss = 0.03145772
Iteration 31, loss = 0.03056418
Iteration 32, loss = 0.02962271
Iteration 33, loss = 0.02856999
Iteration 34, loss = 0.02793239
Iteration 35, loss = 0.02718546
Iteration 36, loss = 0.02645830
Iteration 37, loss = 0.02588513
Iteration 38, loss = 0.02551521
Iteration 39, loss = 0.02495017
Iteration 40, loss = 0.02466900
Iteration 41, loss = 0.02411940
Iteration 42, loss = 0.02364735
Iteration 43, loss = 0.02333851
Iteration 44, loss = 0.02294685
Iteration 45, loss = 0.02258442
Iteration 46, loss = 0.02243647
Iteration 47, loss = 0.02204406
Iteration 48, loss = 0.02176213
Iteration 49, loss = 0.02149996
Iteration 50, loss = 0.02139373
Iteration 51, loss = 0.02108134
Iteration 52, loss = 0.02098652
Iteration 53, loss = 0.02086614
Iteration 54, loss = 0.02062467
Iteration 55, loss = 0.02045417
Iteration 56, loss = 0.02022032
Iteration 57, loss = 0.02025812
Iteration 58, loss = 0.01995456
Iteration 59, loss = 0.01992478
Iteration 60, loss = 0.01965546
Iteration 61, loss = 0.01950895
Iteration 62, loss = 0.01956792
Iteration 63, loss = 0.01948322
Iteration 64, loss = 0.01949793
Iteration 65, loss = 0.01930982
Iteration 66, loss = 0.01918384
Iteration 67, loss = 0.01914882
Iteration 68, loss = 0.01911827
Iteration 69, loss = 0.01890759
Iteration 70, loss = 0.01893960
Iteration 71, loss = 0.01895018
Iteration 72, loss = 0.01872756
Iteration 73, loss = 0.01865537
Iteration 74, loss = 0.01867575
Iteration 75, loss = 0.01856036
Iteration 76, loss = 0.01846305
Iteration 77, loss = 0.01842844
Iteration 78, loss = 0.01840353
Iteration 79, loss = 0.01832010
Iteration 80, loss = 0.01833274
Iteration 81, loss = 0.01831529
Iteration 82, loss = 0.01830246
Iteration 83, loss = 0.01817408
Iteration 84, loss = 0.01812467
Iteration 85, loss = 0.01802760
Iteration 86, loss = 0.01794110
Iteration 87, loss = 0.01813240
Iteration 88, loss = 0.01809695
Iteration 89, loss = 0.01781951
Iteration 90, loss = 0.01790783
Iteration 91, loss = 0.01781652
Iteration 92, loss = 0.01798302
Iteration 93, loss = 0.01779793
Iteration 94, loss = 0.01793893
Iteration 95, loss = 0.01767784
Iteration 96, loss = 0.01784398
Iteration 97, loss = 0.01759718
Iteration 98, loss = 0.01761921
Iteration 99, loss = 0.01751765
Iteration 100, loss = 0.01755023
Iteration 101, loss = 0.01753622
Iteration 102, loss = 0.01768491
Iteration 103, loss = 0.01747919
Iteration 104, loss = 0.01738615
Iteration 105, loss = 0.01751248
Iteration 106, loss = 0.01747885
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69733138
Iteration 2, loss = 0.66437072
Iteration 3, loss = 0.62327125
Iteration 4, loss = 0.57380891
Iteration 5, loss = 0.51432303
Iteration 6, loss = 0.45104408
Iteration 7, loss = 0.38828367
Iteration 8, loss = 0.33013640
Iteration 9, loss = 0.27938959
Iteration 10, loss = 0.23611394
Iteration 11, loss = 0.20000551
Iteration 12, loss = 0.17029883
Iteration 13, loss = 0.14603014
Iteration 14, loss = 0.12625118
Iteration 15, loss = 0.11023854
Iteration 16, loss = 0.09702389
Iteration 17, loss = 0.08603826
Iteration 18, loss = 0.07694597
Iteration 19, loss = 0.06934634
Iteration 20, loss = 0.06287518
Iteration 21, loss = 0.05743132
Iteration 22, loss = 0.05271883
Iteration 23, loss = 0.04869086
Iteration 24, loss = 0.04513850
Iteration 25, loss = 0.04216941
Iteration 26, loss = 0.03944582
Iteration 27, loss = 0.03708643
Iteration 28, loss = 0.03488041
Iteration 29, loss = 0.03307382
Iteration 30, loss = 0.03137090
Iteration 31, loss = 0.02989025
Iteration 32, loss = 0.02857333
Iteration 33, loss = 0.02733144
Iteration 34, loss = 0.02620921
Iteration 35, loss = 0.02526623
Iteration 36, loss = 0.02431851
Iteration 37, loss = 0.02345472
Iteration 38, loss = 0.02261819
Iteration 39, loss = 0.02191512
Iteration 40, loss = 0.02134653
Iteration 41, loss = 0.02068396
Iteration 42, loss = 0.02020563
Iteration 43, loss = 0.01960403
Iteration 44, loss = 0.01912133
Iteration 45, loss = 0.01864735
Iteration 46, loss = 0.01824604
Iteration 47, loss = 0.01783834
Iteration 48, loss = 0.01750207
Iteration 49, loss = 0.01713585
Iteration 50, loss = 0.01692405
Iteration 51, loss = 0.01656290
Iteration 52, loss = 0.01634836
Iteration 53, loss = 0.01606970
Iteration 54, loss = 0.01575648
Iteration 55, loss = 0.01560210
Iteration 56, loss = 0.01538277
Iteration 57, loss = 0.01515574
Iteration 58, loss = 0.01491313
Iteration 59, loss = 0.01475597
Iteration 60, loss = 0.01457670
Iteration 61, loss = 0.01437647
Iteration 62, loss = 0.01426709
Iteration 63, loss = 0.01416185
Iteration 64, loss = 0.01389697
Iteration 65, loss = 0.01377966
Iteration 66, loss = 0.01362109
Iteration 67, loss = 0.01351918
Iteration 68, loss = 0.01348372
Iteration 69, loss = 0.01326523
Iteration 70, loss = 0.01326731
Iteration 71, loss = 0.01308671
Iteration 72, loss = 0.01305040
Iteration 73, loss = 0.01291563
Iteration 74, loss = 0.01284862
Iteration 75, loss = 0.01278401
Iteration 76, loss = 0.01264639
Iteration 77, loss = 0.01267102
Iteration 78, loss = 0.01266202
Iteration 79, loss = 0.01248030
Iteration 80, loss = 0.01240562
Iteration 81, loss = 0.01227162
Iteration 82, loss = 0.01226216
Iteration 83, loss = 0.01209657
Iteration 84, loss = 0.01209005
Iteration 85, loss = 0.01209141
Iteration 86, loss = 0.01205311
Iteration 87, loss = 0.01196033
Iteration 88, loss = 0.01206472
Iteration 89, loss = 0.01191972
Iteration 90, loss = 0.01186618
Iteration 91, loss = 0.01177490
Iteration 92, loss = 0.01169542
Iteration 93, loss = 0.01181590
Iteration 94, loss = 0.01164848
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71725362
Iteration 2, loss = 0.67228276
Iteration 3, loss = 0.61230227
Iteration 4, loss = 0.53662948
Iteration 5, loss = 0.44989422
Iteration 6, loss = 0.36562227
Iteration 7, loss = 0.29356552
Iteration 8, loss = 0.23539910
Iteration 9, loss = 0.19078419
Iteration 10, loss = 0.15642661
Iteration 11, loss = 0.13084257
Iteration 12, loss = 0.11152160
Iteration 13, loss = 0.09635306
Iteration 14, loss = 0.08458789
Iteration 15, loss = 0.07537557
Iteration 16, loss = 0.06790688
Iteration 17, loss = 0.06170256
Iteration 18, loss = 0.05650985
Iteration 19, loss = 0.05237085
Iteration 20, loss = 0.04858001
Iteration 21, loss = 0.04557007
Iteration 22, loss = 0.04311823
Iteration 23, loss = 0.04067169
Iteration 24, loss = 0.03891652
Iteration 25, loss = 0.03722720
Iteration 26, loss = 0.03558646
Iteration 27, loss = 0.03456929
Iteration 28, loss = 0.03344206
Iteration 29, loss = 0.03210894
Iteration 30, loss = 0.03125862
Iteration 31, loss = 0.03038253
Iteration 32, loss = 0.02970533
Iteration 33, loss = 0.02894342
Iteration 34, loss = 0.02839710
Iteration 35, loss = 0.02778648
Iteration 36, loss = 0.02724217
Iteration 37, loss = 0.02681430
Iteration 38, loss = 0.02640231
Iteration 39, loss = 0.02585380
Iteration 40, loss = 0.02552891
Iteration 41, loss = 0.02514861
Iteration 42, loss = 0.02491828
Iteration 43, loss = 0.02420901
Iteration 44, loss = 0.02424184
Iteration 45, loss = 0.02394597
Iteration 46, loss = 0.02355330
Iteration 47, loss = 0.02346236
Iteration 48, loss = 0.02328246
Iteration 49, loss = 0.02303570
Iteration 50, loss = 0.02281464
Iteration 51, loss = 0.02270775
Iteration 52, loss = 0.02250189
Iteration 53, loss = 0.02244760
Iteration 54, loss = 0.02226055
Iteration 55, loss = 0.02233068
Iteration 56, loss = 0.02177708
Iteration 57, loss = 0.02183509
Iteration 58, loss = 0.02172361
Iteration 59, loss = 0.02173465
Iteration 60, loss = 0.02160466
Iteration 61, loss = 0.02147785
Iteration 62, loss = 0.02173817
Iteration 63, loss = 0.02124335
Iteration 64, loss = 0.02124674
Iteration 65, loss = 0.02128829
Iteration 66, loss = 0.02123347
Iteration 67, loss = 0.02100881
Iteration 68, loss = 0.02073366
Iteration 69, loss = 0.02091325
Iteration 70, loss = 0.02085628
Iteration 71, loss = 0.02093043
Iteration 72, loss = 0.02072472
Iteration 73, loss = 0.02064939
Iteration 74, loss = 0.02071338
Iteration 75, loss = 0.02059141
Iteration 76, loss = 0.02038845
Iteration 77, loss = 0.02034524
Iteration 78, loss = 0.02015275
Iteration 79, loss = 0.02053276
Iteration 80, loss = 0.02021726
Iteration 81, loss = 0.02027403
Iteration 82, loss = 0.02029484
Iteration 83, loss = 0.02006479
Iteration 84, loss = 0.02003287
Iteration 85, loss = 0.02009669
Iteration 86, loss = 0.01992862
Iteration 87, loss = 0.02017336
Iteration 88, loss = 0.02022917
Iteration 89, loss = 0.01975094
Iteration 90, loss = 0.02030703
Iteration 91, loss = 0.01993284
Iteration 92, loss = 0.02023491
Iteration 93, loss = 0.01995008
Iteration 94, loss = 0.02003269
Iteration 95, loss = 0.01977339
Iteration 96, loss = 0.02006400
Iteration 97, loss = 0.01983692
Iteration 98, loss = 0.02005019
Iteration 99, loss = 0.01968950
Iteration 100, loss = 0.01968906
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73688054
Iteration 2, loss = 0.71776841
Iteration 3, loss = 0.70274049
Iteration 4, loss = 0.68852898
Iteration 5, loss = 0.67411499
Iteration 6, loss = 0.65874145
Iteration 7, loss = 0.64248779
Iteration 8, loss = 0.62482709
Iteration 9, loss = 0.60609977
Iteration 10, loss = 0.58570943
Iteration 11, loss = 0.56410568
Iteration 12, loss = 0.54137057
Iteration 13, loss = 0.51783626
Iteration 14, loss = 0.49360746
Iteration 15, loss = 0.46933246
Iteration 16, loss = 0.44499268
Iteration 17, loss = 0.42110935
Iteration 18, loss = 0.39768082
Iteration 19, loss = 0.37491331
Iteration 20, loss = 0.35300161
Iteration 21, loss = 0.33184147
Iteration 22, loss = 0.31174885
Iteration 23, loss = 0.29270596
Iteration 24, loss = 0.27460492
Iteration 25, loss = 0.25754351
Iteration 26, loss = 0.24152802
Iteration 27, loss = 0.22648975
Iteration 28, loss = 0.21233939
Iteration 29, loss = 0.19921537
Iteration 30, loss = 0.18702317
Iteration 31, loss = 0.17565402
Iteration 32, loss = 0.16509794
Iteration 33, loss = 0.15529369
Iteration 34, loss = 0.14618699
Iteration 35, loss = 0.13779291
Iteration 36, loss = 0.12999086
Iteration 37, loss = 0.12269440
Iteration 38, loss = 0.11599666
Iteration 39, loss = 0.10975438
Iteration 40, loss = 0.10393769
Iteration 41, loss = 0.09860188
Iteration 42, loss = 0.09360963
Iteration 43, loss = 0.08893505
Iteration 44, loss = 0.08459999
Iteration 45, loss = 0.08059935
Iteration 46, loss = 0.07683435
Iteration 47, loss = 0.07331289
Iteration 48, loss = 0.07004987
Iteration 49, loss = 0.06696374
Iteration 50, loss = 0.06408013
Iteration 51, loss = 0.06139509
Iteration 52, loss = 0.05885926
Iteration 53, loss = 0.05650953
Iteration 54, loss = 0.05426104
Iteration 55, loss = 0.05216043
Iteration 56, loss = 0.05019198
Iteration 57, loss = 0.04830567
Iteration 58, loss = 0.04655023
Iteration 59, loss = 0.04488978
Iteration 60, loss = 0.04330087
Iteration 61, loss = 0.04178786
Iteration 62, loss = 0.04037044
Iteration 63, loss = 0.03901541
Iteration 64, loss = 0.03772980
Iteration 65, loss = 0.03650686
Iteration 66, loss = 0.03534384
Iteration 67, loss = 0.03423309
Iteration 68, loss = 0.03319235
Iteration 69, loss = 0.03219846
Iteration 70, loss = 0.03122564
Iteration 71, loss = 0.03033352
Iteration 72, loss = 0.02946036
Iteration 73, loss = 0.02863257
Iteration 74, loss = 0.02783546
Iteration 75, loss = 0.02705995
Iteration 76, loss = 0.02634129
Iteration 77, loss = 0.02562945
Iteration 78, loss = 0.02496762
Iteration 79, loss = 0.02432067
Iteration 80, loss = 0.02371469
Iteration 81, loss = 0.02311424
Iteration 82, loss = 0.02253807
Iteration 83, loss = 0.02199373
Iteration 84, loss = 0.02145987
Iteration 85, loss = 0.02094763
Iteration 86, loss = 0.02046325
Iteration 87, loss = 0.01999115
Iteration 88, loss = 0.01954229
Iteration 89, loss = 0.01909628
Iteration 90, loss = 0.01866097
Iteration 91, loss = 0.01825780
Iteration 92, loss = 0.01787162
Iteration 93, loss = 0.01748041
Iteration 94, loss = 0.01712467
Iteration 95, loss = 0.01677058
Iteration 96, loss = 0.01642553
Iteration 97, loss = 0.01607680
Iteration 98, loss = 0.01575688
Iteration 99, loss = 0.01543944
Iteration 100, loss = 0.01514493
Iteration 101, loss = 0.01484853
Iteration 102, loss = 0.01456641
Iteration 103, loss = 0.01428583
Iteration 104, loss = 0.01402167
Iteration 105, loss = 0.01376167
Iteration 106, loss = 0.01351328
Iteration 107, loss = 0.01327227
Iteration 108, loss = 0.01302858
Iteration 109, loss = 0.01280265
Iteration 110, loss = 0.01258229
Iteration 111, loss = 0.01236484
Iteration 112, loss = 0.01216063
Iteration 113, loss = 0.01195524
Iteration 114, loss = 0.01175398
Iteration 115, loss = 0.01156678
Iteration 116, loss = 0.01137263
Iteration 117, loss = 0.01119083
Iteration 118, loss = 0.01101575
Iteration 119, loss = 0.01083770
Iteration 120, loss = 0.01067648
Iteration 121, loss = 0.01050630
Iteration 122, loss = 0.01034923
Iteration 123, loss = 0.01018979
Iteration 124, loss = 0.01003921
Iteration 125, loss = 0.00988900
Iteration 126, loss = 0.00974644
Iteration 127, loss = 0.00960539
Iteration 128, loss = 0.00946357
Iteration 129, loss = 0.00933154
Iteration 130, loss = 0.00920084
Iteration 131, loss = 0.00906923
Iteration 132, loss = 0.00894537
Iteration 133, loss = 0.00882509
Iteration 134, loss = 0.00870573
Iteration 135, loss = 0.00858909
Iteration 136, loss = 0.00847445
Iteration 137, loss = 0.00836212
Iteration 138, loss = 0.00824979
Iteration 139, loss = 0.00814125
Iteration 140, loss = 0.00803537
Iteration 141, loss = 0.00793614
Iteration 142, loss = 0.00783927
Iteration 143, loss = 0.00773545
Iteration 144, loss = 0.00763778
Iteration 145, loss = 0.00754512
Iteration 146, loss = 0.00745772
Iteration 147, loss = 0.00736856
Iteration 148, loss = 0.00727652
Iteration 149, loss = 0.00719097
Iteration 150, loss = 0.00710604
Iteration 151, loss = 0.00702045
Iteration 152, loss = 0.00693931
Iteration 153, loss = 0.00685356
Iteration 154, loss = 0.00677623
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67873765
Iteration 2, loss = 0.65354053
Iteration 3, loss = 0.62462769
Iteration 4, loss = 0.58498865
Iteration 5, loss = 0.53525222
Iteration 6, loss = 0.48058373
Iteration 7, loss = 0.42724462
Iteration 8, loss = 0.37665429
Iteration 9, loss = 0.33062076
Iteration 10, loss = 0.28950008
Iteration 11, loss = 0.25326842
Iteration 12, loss = 0.22170719
Iteration 13, loss = 0.19480151
Iteration 14, loss = 0.17172200
Iteration 15, loss = 0.15215889
Iteration 16, loss = 0.13561420
Iteration 17, loss = 0.12161837
Iteration 18, loss = 0.10974669
Iteration 19, loss = 0.09958083
Iteration 20, loss = 0.09089185
Iteration 21, loss = 0.08327248
Iteration 22, loss = 0.07670881
Iteration 23, loss = 0.07128059
Iteration 24, loss = 0.06650224
Iteration 25, loss = 0.06217581
Iteration 26, loss = 0.05828475
Iteration 27, loss = 0.05485013
Iteration 28, loss = 0.05167969
Iteration 29, loss = 0.04910884
Iteration 30, loss = 0.04652048
Iteration 31, loss = 0.04427178
Iteration 32, loss = 0.04228792
Iteration 33, loss = 0.04060510
Iteration 34, loss = 0.03894830
Iteration 35, loss = 0.03730301
Iteration 36, loss = 0.03587856
Iteration 37, loss = 0.03461163
Iteration 38, loss = 0.03343021
Iteration 39, loss = 0.03237630
Iteration 40, loss = 0.03141650
Iteration 41, loss = 0.03081182
Iteration 42, loss = 0.02988484
Iteration 43, loss = 0.02917099
Iteration 44, loss = 0.02836739
Iteration 45, loss = 0.02765993
Iteration 46, loss = 0.02692020
Iteration 47, loss = 0.02634736
Iteration 48, loss = 0.02576510
Iteration 49, loss = 0.02526488
Iteration 50, loss = 0.02515960
Iteration 51, loss = 0.02470943
Iteration 52, loss = 0.02437492
Iteration 53, loss = 0.02376077
Iteration 54, loss = 0.02324748
Iteration 55, loss = 0.02300516
Iteration 56, loss = 0.02251782
Iteration 57, loss = 0.02231066
Iteration 58, loss = 0.02204440
Iteration 59, loss = 0.02163894
Iteration 60, loss = 0.02131788
Iteration 61, loss = 0.02130211
Iteration 62, loss = 0.02095418
Iteration 63, loss = 0.02072754
Iteration 64, loss = 0.02090199
Iteration 65, loss = 0.02093005
Iteration 66, loss = 0.02072514
Iteration 67, loss = 0.02017009
Iteration 68, loss = 0.01975376
Iteration 69, loss = 0.01943276
Iteration 70, loss = 0.01929534
Iteration 71, loss = 0.01902695
Iteration 72, loss = 0.01889027
Iteration 73, loss = 0.01862414
Iteration 74, loss = 0.01883476
Iteration 75, loss = 0.01856385
Iteration 76, loss = 0.01848492
Iteration 77, loss = 0.01824955
Iteration 78, loss = 0.01817941
Iteration 79, loss = 0.01811131
Iteration 80, loss = 0.01820117
Iteration 81, loss = 0.01793358
Iteration 82, loss = 0.01777715
Iteration 83, loss = 0.01770591
Iteration 84, loss = 0.01739975
Iteration 85, loss = 0.01730519
Iteration 86, loss = 0.01716230
Iteration 87, loss = 0.01708981
Iteration 88, loss = 0.01717185
Iteration 89, loss = 0.01725353
Iteration 90, loss = 0.01782944
Iteration 91, loss = 0.01790017
Iteration 92, loss = 0.01763884
Iteration 93, loss = 0.01713674
Iteration 94, loss = 0.01696614
Iteration 95, loss = 0.01676179
Iteration 96, loss = 0.01673151
Iteration 97, loss = 0.01667805
Iteration 98, loss = 0.01642047
Iteration 99, loss = 0.01630558
Iteration 100, loss = 0.01622613
Iteration 101, loss = 0.01616020
Iteration 102, loss = 0.01614910
Iteration 103, loss = 0.01606394
Iteration 104, loss = 0.01597905
Iteration 105, loss = 0.01604608
Iteration 106, loss = 0.01596212
Iteration 107, loss = 0.01605187
Iteration 108, loss = 0.01614367
Iteration 109, loss = 0.01594929
Iteration 110, loss = 0.01586328
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67793413
Iteration 2, loss = 0.64386128
Iteration 3, loss = 0.59424583
Iteration 4, loss = 0.52994933
Iteration 5, loss = 0.45919944
Iteration 6, loss = 0.39003362
Iteration 7, loss = 0.32716669
Iteration 8, loss = 0.27232548
Iteration 9, loss = 0.22624886
Iteration 10, loss = 0.18879707
Iteration 11, loss = 0.15860493
Iteration 12, loss = 0.13462172
Iteration 13, loss = 0.11560959
Iteration 14, loss = 0.10016302
Iteration 15, loss = 0.08786422
Iteration 16, loss = 0.07802237
Iteration 17, loss = 0.06984648
Iteration 18, loss = 0.06299674
Iteration 19, loss = 0.05734188
Iteration 20, loss = 0.05271706
Iteration 21, loss = 0.04869756
Iteration 22, loss = 0.04521409
Iteration 23, loss = 0.04219978
Iteration 24, loss = 0.03977800
Iteration 25, loss = 0.03761219
Iteration 26, loss = 0.03528179
Iteration 27, loss = 0.03372086
Iteration 28, loss = 0.03211145
Iteration 29, loss = 0.03068221
Iteration 30, loss = 0.02957027
Iteration 31, loss = 0.02835225
Iteration 32, loss = 0.02745574
Iteration 33, loss = 0.02663193
Iteration 34, loss = 0.02582599
Iteration 35, loss = 0.02505410
Iteration 36, loss = 0.02453203
Iteration 37, loss = 0.02380035
Iteration 38, loss = 0.02309866
Iteration 39, loss = 0.02282181
Iteration 40, loss = 0.02213236
Iteration 41, loss = 0.02171365
Iteration 42, loss = 0.02126167
Iteration 43, loss = 0.02097137
Iteration 44, loss = 0.02058436
Iteration 45, loss = 0.02032369
Iteration 46, loss = 0.02003500
Iteration 47, loss = 0.01975419
Iteration 48, loss = 0.01938095
Iteration 49, loss = 0.01915222
Iteration 50, loss = 0.01903084
Iteration 51, loss = 0.01875014
Iteration 52, loss = 0.01846192
Iteration 53, loss = 0.01839508
Iteration 54, loss = 0.01817535
Iteration 55, loss = 0.01813723
Iteration 56, loss = 0.01788249
Iteration 57, loss = 0.01769285
Iteration 58, loss = 0.01758146
Iteration 59, loss = 0.01753686
Iteration 60, loss = 0.01725496
Iteration 61, loss = 0.01717952
Iteration 62, loss = 0.01708724
Iteration 63, loss = 0.01704622
Iteration 64, loss = 0.01692860
Iteration 65, loss = 0.01693672
Iteration 66, loss = 0.01673035
Iteration 67, loss = 0.01663432
Iteration 68, loss = 0.01652792
Iteration 69, loss = 0.01644865
Iteration 70, loss = 0.01637443
Iteration 71, loss = 0.01626113
Iteration 72, loss = 0.01624918
Iteration 73, loss = 0.01609373
Iteration 74, loss = 0.01602594
Iteration 75, loss = 0.01598530
Iteration 76, loss = 0.01589108
Iteration 77, loss = 0.01587544
Iteration 78, loss = 0.01586737
Iteration 79, loss = 0.01587944
Iteration 80, loss = 0.01582982
Iteration 81, loss = 0.01579085
Iteration 82, loss = 0.01563433
Iteration 83, loss = 0.01561877
Iteration 84, loss = 0.01553556
Iteration 85, loss = 0.01552519
Iteration 86, loss = 0.01553571
Iteration 87, loss = 0.01533028
Iteration 88, loss = 0.01547652
Iteration 89, loss = 0.01541096
Iteration 90, loss = 0.01534487
Iteration 91, loss = 0.01537280
Iteration 92, loss = 0.01521496
Iteration 93, loss = 0.01530891
Iteration 94, loss = 0.01514531
Iteration 95, loss = 0.01515262
Iteration 96, loss = 0.01515118
Iteration 97, loss = 0.01512503
Iteration 98, loss = 0.01504038
Iteration 99, loss = 0.01509337
Iteration 100, loss = 0.01492743
Iteration 101, loss = 0.01497094
Iteration 102, loss = 0.01506556
Iteration 103, loss = 0.01497118
Iteration 104, loss = 0.01497302
Iteration 105, loss = 0.01500739
Iteration 106, loss = 0.01485007
Iteration 107, loss = 0.01496502
Iteration 108, loss = 0.01489325
Iteration 109, loss = 0.01485136
Iteration 110, loss = 0.01481138
Iteration 111, loss = 0.01470937
Iteration 112, loss = 0.01479450
Iteration 113, loss = 0.01479611
Iteration 114, loss = 0.01471559
Iteration 115, loss = 0.01469039
Iteration 116, loss = 0.01458463
Iteration 117, loss = 0.01468489
Iteration 118, loss = 0.01456031
Iteration 119, loss = 0.01472189
Iteration 120, loss = 0.01462932
Iteration 121, loss = 0.01475934
Iteration 122, loss = 0.01460150
Iteration 123, loss = 0.01468864
Iteration 124, loss = 0.01456692
Iteration 125, loss = 0.01455479
Iteration 126, loss = 0.01452028
Iteration 127, loss = 0.01471510
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67822900
Iteration 2, loss = 0.62102340
Iteration 3, loss = 0.53745591
Iteration 4, loss = 0.44360774
Iteration 5, loss = 0.35644083
Iteration 6, loss = 0.28259261
Iteration 7, loss = 0.22385910
Iteration 8, loss = 0.17914007
Iteration 9, loss = 0.14570153
Iteration 10, loss = 0.12100859
Iteration 11, loss = 0.10246305
Iteration 12, loss = 0.08829914
Iteration 13, loss = 0.07728718
Iteration 14, loss = 0.06882254
Iteration 15, loss = 0.06190228
Iteration 16, loss = 0.05643369
Iteration 17, loss = 0.05167415
Iteration 18, loss = 0.04789587
Iteration 19, loss = 0.04457541
Iteration 20, loss = 0.04220191
Iteration 21, loss = 0.03987370
Iteration 22, loss = 0.03781513
Iteration 23, loss = 0.03619555
Iteration 24, loss = 0.03453997
Iteration 25, loss = 0.03336255
Iteration 26, loss = 0.03238779
Iteration 27, loss = 0.03114275
Iteration 28, loss = 0.03012796
Iteration 29, loss = 0.02952257
Iteration 30, loss = 0.02870543
Iteration 31, loss = 0.02806379
Iteration 32, loss = 0.02776049
Iteration 33, loss = 0.02707472
Iteration 34, loss = 0.02665523
Iteration 35, loss = 0.02601550
Iteration 36, loss = 0.02575178
Iteration 37, loss = 0.02534793
Iteration 38, loss = 0.02491569
Iteration 39, loss = 0.02456250
Iteration 40, loss = 0.02450429
Iteration 41, loss = 0.02406872
Iteration 42, loss = 0.02387057
Iteration 43, loss = 0.02366765
Iteration 44, loss = 0.02330067
Iteration 45, loss = 0.02327427
Iteration 46, loss = 0.02314037
Iteration 47, loss = 0.02296642
Iteration 48, loss = 0.02265335
Iteration 49, loss = 0.02263246
Iteration 50, loss = 0.02224481
Iteration 51, loss = 0.02229703
Iteration 52, loss = 0.02218829
Iteration 53, loss = 0.02198746
Iteration 54, loss = 0.02197648
Iteration 55, loss = 0.02201442
Iteration 56, loss = 0.02186263
Iteration 57, loss = 0.02162868
Iteration 58, loss = 0.02161587
Iteration 59, loss = 0.02126768
Iteration 60, loss = 0.02123078
Iteration 61, loss = 0.02137272
Iteration 62, loss = 0.02104728
Iteration 63, loss = 0.02110444
Iteration 64, loss = 0.02118232
Iteration 65, loss = 0.02087883
Iteration 66, loss = 0.02121498
Iteration 67, loss = 0.02087735
Iteration 68, loss = 0.02099219
Iteration 69, loss = 0.02080308
Iteration 70, loss = 0.02076709
Iteration 71, loss = 0.02088527
Iteration 72, loss = 0.02090180
Iteration 73, loss = 0.02064033
Iteration 74, loss = 0.02061074
Iteration 75, loss = 0.02063715
Iteration 76, loss = 0.02059826
Iteration 77, loss = 0.02070229
Iteration 78, loss = 0.02028199
Iteration 79, loss = 0.02034452
Iteration 80, loss = 0.02040776
Iteration 81, loss = 0.02052923
Iteration 82, loss = 0.02033099
Iteration 83, loss = 0.02032049
Iteration 84, loss = 0.02041679
Iteration 85, loss = 0.02019215
Iteration 86, loss = 0.02048012
Iteration 87, loss = 0.02038336
Iteration 88, loss = 0.02012439
Iteration 89, loss = 0.02018319
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67774779
Iteration 2, loss = 0.63599552
Iteration 3, loss = 0.57460695
Iteration 4, loss = 0.49742119
Iteration 5, loss = 0.41693973
Iteration 6, loss = 0.34215364
Iteration 7, loss = 0.27737564
Iteration 8, loss = 0.22438887
Iteration 9, loss = 0.18263981
Iteration 10, loss = 0.15055595
Iteration 11, loss = 0.12596684
Iteration 12, loss = 0.10696870
Iteration 13, loss = 0.09238595
Iteration 14, loss = 0.08098849
Iteration 15, loss = 0.07172818
Iteration 16, loss = 0.06444233
Iteration 17, loss = 0.05844413
Iteration 18, loss = 0.05337875
Iteration 19, loss = 0.04944843
Iteration 20, loss = 0.04586942
Iteration 21, loss = 0.04269561
Iteration 22, loss = 0.04039154
Iteration 23, loss = 0.03813344
Iteration 24, loss = 0.03630807
Iteration 25, loss = 0.03457159
Iteration 26, loss = 0.03322648
Iteration 27, loss = 0.03177859
Iteration 28, loss = 0.03087209
Iteration 29, loss = 0.02975739
Iteration 30, loss = 0.02863364
Iteration 31, loss = 0.02780782
Iteration 32, loss = 0.02719931
Iteration 33, loss = 0.02640290
Iteration 34, loss = 0.02591935
Iteration 35, loss = 0.02523248
Iteration 36, loss = 0.02468932
Iteration 37, loss = 0.02432388
Iteration 38, loss = 0.02379889
Iteration 39, loss = 0.02335127
Iteration 40, loss = 0.02321988
Iteration 41, loss = 0.02298166
Iteration 42, loss = 0.02240860
Iteration 43, loss = 0.02222144
Iteration 44, loss = 0.02176593
Iteration 45, loss = 0.02145909
Iteration 46, loss = 0.02128910
Iteration 47, loss = 0.02119475
Iteration 48, loss = 0.02109898
Iteration 49, loss = 0.02095303
Iteration 50, loss = 0.02068916
Iteration 51, loss = 0.02035798
Iteration 52, loss = 0.02033692
Iteration 53, loss = 0.02006743
Iteration 54, loss = 0.01994506
Iteration 55, loss = 0.01985336
Iteration 56, loss = 0.01981087
Iteration 57, loss = 0.01951857
Iteration 58, loss = 0.01965011
Iteration 59, loss = 0.01936756
Iteration 60, loss = 0.01926693
Iteration 61, loss = 0.01944051
Iteration 62, loss = 0.01930911
Iteration 63, loss = 0.01905856
Iteration 64, loss = 0.01898538
Iteration 65, loss = 0.01889494
Iteration 66, loss = 0.01860975
Iteration 67, loss = 0.01883836
Iteration 68, loss = 0.01876560
Iteration 69, loss = 0.01868045
Iteration 70, loss = 0.01858487
Iteration 71, loss = 0.01848771
Iteration 72, loss = 0.01870628
Iteration 73, loss = 0.01869201
Iteration 74, loss = 0.01829722
Iteration 75, loss = 0.01875137
Iteration 76, loss = 0.01825206
Iteration 77, loss = 0.01811914
Iteration 78, loss = 0.01816964
Iteration 79, loss = 0.01813186
Iteration 80, loss = 0.01800138
Iteration 81, loss = 0.01777980
Iteration 82, loss = 0.01785557
Iteration 83, loss = 0.01802093
Iteration 84, loss = 0.01801972
Iteration 85, loss = 0.01818137
Iteration 86, loss = 0.01783252
Iteration 87, loss = 0.01763136
Iteration 88, loss = 0.01785393
Iteration 89, loss = 0.01768404
Iteration 90, loss = 0.01781241
Iteration 91, loss = 0.01741806
Iteration 92, loss = 0.01759513
Iteration 93, loss = 0.01772488
Iteration 94, loss = 0.01759541
Iteration 95, loss = 0.01761330
Iteration 96, loss = 0.01767869
Iteration 97, loss = 0.01775362
Iteration 98, loss = 0.01769545
Iteration 99, loss = 0.01720157
Iteration 100, loss = 0.01766271
Iteration 101, loss = 0.01740753
Iteration 102, loss = 0.01759159
Iteration 103, loss = 0.01726670
Iteration 104, loss = 0.01728648
Iteration 105, loss = 0.01747725
Iteration 106, loss = 0.01778707
Iteration 107, loss = 0.01745480
Iteration 108, loss = 0.01748765
Iteration 109, loss = 0.01734700
Iteration 110, loss = 0.01727258
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67653862
Iteration 2, loss = 0.62722496
Iteration 3, loss = 0.55040494
Iteration 4, loss = 0.45824724
Iteration 5, loss = 0.36889012
Iteration 6, loss = 0.29154964
Iteration 7, loss = 0.22951945
Iteration 8, loss = 0.18264152
Iteration 9, loss = 0.14761547
Iteration 10, loss = 0.12185304
Iteration 11, loss = 0.10260116
Iteration 12, loss = 0.08828530
Iteration 13, loss = 0.07720709
Iteration 14, loss = 0.06832782
Iteration 15, loss = 0.06162393
Iteration 16, loss = 0.05593691
Iteration 17, loss = 0.05163584
Iteration 18, loss = 0.04809437
Iteration 19, loss = 0.04506127
Iteration 20, loss = 0.04229980
Iteration 21, loss = 0.03996940
Iteration 22, loss = 0.03801553
Iteration 23, loss = 0.03635874
Iteration 24, loss = 0.03503231
Iteration 25, loss = 0.03371137
Iteration 26, loss = 0.03306695
Iteration 27, loss = 0.03160196
Iteration 28, loss = 0.03077447
Iteration 29, loss = 0.03029243
Iteration 30, loss = 0.02946633
Iteration 31, loss = 0.02877226
Iteration 32, loss = 0.02824092
Iteration 33, loss = 0.02780909
Iteration 34, loss = 0.02738734
Iteration 35, loss = 0.02704991
Iteration 36, loss = 0.02639736
Iteration 37, loss = 0.02636619
Iteration 38, loss = 0.02584138
Iteration 39, loss = 0.02553089
Iteration 40, loss = 0.02540994
Iteration 41, loss = 0.02518916
Iteration 42, loss = 0.02492174
Iteration 43, loss = 0.02480904
Iteration 44, loss = 0.02440383
Iteration 45, loss = 0.02447602
Iteration 46, loss = 0.02412887
Iteration 47, loss = 0.02407474
Iteration 48, loss = 0.02382628
Iteration 49, loss = 0.02362534
Iteration 50, loss = 0.02363347
Iteration 51, loss = 0.02360158
Iteration 52, loss = 0.02353121
Iteration 53, loss = 0.02301263
Iteration 54, loss = 0.02301131
Iteration 55, loss = 0.02305582
Iteration 56, loss = 0.02278437
Iteration 57, loss = 0.02288234
Iteration 58, loss = 0.02309433
Iteration 59, loss = 0.02253602
Iteration 60, loss = 0.02278309
Iteration 61, loss = 0.02246281
Iteration 62, loss = 0.02241897
Iteration 63, loss = 0.02231334
Iteration 64, loss = 0.02240027
Iteration 65, loss = 0.02238258
Iteration 66, loss = 0.02217927
Iteration 67, loss = 0.02219305
Iteration 68, loss = 0.02229112
Iteration 69, loss = 0.02237936
Iteration 70, loss = 0.02236727
Iteration 71, loss = 0.02223055
Iteration 72, loss = 0.02216033
Iteration 73, loss = 0.02195568
Iteration 74, loss = 0.02191186
Iteration 75, loss = 0.02202171
Iteration 76, loss = 0.02176052
Iteration 77, loss = 0.02182721
Iteration 78, loss = 0.02147628
Iteration 79, loss = 0.02172846
Iteration 80, loss = 0.02168184
Iteration 81, loss = 0.02171959
Iteration 82, loss = 0.02153732
Iteration 83, loss = 0.02150471
Iteration 84, loss = 0.02199408
Iteration 85, loss = 0.02161660
Iteration 86, loss = 0.02162379
Iteration 87, loss = 0.02158289
Iteration 88, loss = 0.02150563
Iteration 89, loss = 0.02151645
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69004597
Iteration 2, loss = 0.64263291
Iteration 3, loss = 0.56532325
Iteration 4, loss = 0.47198634
Iteration 5, loss = 0.38157268
Iteration 6, loss = 0.30127165
Iteration 7, loss = 0.23712936
Iteration 8, loss = 0.18787721
Iteration 9, loss = 0.15122362
Iteration 10, loss = 0.12422821
Iteration 11, loss = 0.10431836
Iteration 12, loss = 0.08891509
Iteration 13, loss = 0.07714176
Iteration 14, loss = 0.06827211
Iteration 15, loss = 0.06120335
Iteration 16, loss = 0.05518566
Iteration 17, loss = 0.05052774
Iteration 18, loss = 0.04661431
Iteration 19, loss = 0.04325019
Iteration 20, loss = 0.04040456
Iteration 21, loss = 0.03816799
Iteration 22, loss = 0.03624395
Iteration 23, loss = 0.03436328
Iteration 24, loss = 0.03265788
Iteration 25, loss = 0.03156442
Iteration 26, loss = 0.03040714
Iteration 27, loss = 0.02941192
Iteration 28, loss = 0.02838155
Iteration 29, loss = 0.02773411
Iteration 30, loss = 0.02695176
Iteration 31, loss = 0.02628160
Iteration 32, loss = 0.02556453
Iteration 33, loss = 0.02512351
Iteration 34, loss = 0.02466658
Iteration 35, loss = 0.02426147
Iteration 36, loss = 0.02374573
Iteration 37, loss = 0.02347655
Iteration 38, loss = 0.02316647
Iteration 39, loss = 0.02271613
Iteration 40, loss = 0.02247134
Iteration 41, loss = 0.02223984
Iteration 42, loss = 0.02209521
Iteration 43, loss = 0.02173839
Iteration 44, loss = 0.02137490
Iteration 45, loss = 0.02116329
Iteration 46, loss = 0.02117808
Iteration 47, loss = 0.02089942
Iteration 48, loss = 0.02070455
Iteration 49, loss = 0.02085960
Iteration 50, loss = 0.02061455
Iteration 51, loss = 0.02037023
Iteration 52, loss = 0.02021414
Iteration 53, loss = 0.02025507
Iteration 54, loss = 0.02040617
Iteration 55, loss = 0.01999328
Iteration 56, loss = 0.01982698
Iteration 57, loss = 0.01997713
Iteration 58, loss = 0.01959204
Iteration 59, loss = 0.01964316
Iteration 60, loss = 0.01953074
Iteration 61, loss = 0.01975857
Iteration 62, loss = 0.01926993
Iteration 63, loss = 0.01930785
Iteration 64, loss = 0.01897910
Iteration 65, loss = 0.01909264
Iteration 66, loss = 0.01895147
Iteration 67, loss = 0.01911222
Iteration 68, loss = 0.01929990
Iteration 69, loss = 0.01891829
Iteration 70, loss = 0.01890332
Iteration 71, loss = 0.01887498
Iteration 72, loss = 0.01873946
Iteration 73, loss = 0.01880059
Iteration 74, loss = 0.01868513
Iteration 75, loss = 0.01865151
Iteration 76, loss = 0.01852236
Iteration 77, loss = 0.01867553
Iteration 78, loss = 0.01863800
Iteration 79, loss = 0.01844885
Iteration 80, loss = 0.01859650
Iteration 81, loss = 0.01833397
Iteration 82, loss = 0.01832595
Iteration 83, loss = 0.01832017
Iteration 84, loss = 0.01829072
Iteration 85, loss = 0.01830040
Iteration 86, loss = 0.01820657
Iteration 87, loss = 0.01831419
Iteration 88, loss = 0.01827155
Iteration 89, loss = 0.01826501
Iteration 90, loss = 0.01844509
Iteration 91, loss = 0.01823194
Iteration 92, loss = 0.01823523
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69164755
Iteration 2, loss = 0.66894612
Iteration 3, loss = 0.64617703
Iteration 4, loss = 0.62129863
Iteration 5, loss = 0.59313147
Iteration 6, loss = 0.56250126
Iteration 7, loss = 0.52971431
Iteration 8, loss = 0.49588551
Iteration 9, loss = 0.46117467
Iteration 10, loss = 0.42697626
Iteration 11, loss = 0.39367461
Iteration 12, loss = 0.36173660
Iteration 13, loss = 0.33145179
Iteration 14, loss = 0.30307540
Iteration 15, loss = 0.27651863
Iteration 16, loss = 0.25217012
Iteration 17, loss = 0.22971124
Iteration 18, loss = 0.20941025
Iteration 19, loss = 0.19080147
Iteration 20, loss = 0.17412126
Iteration 21, loss = 0.15922352
Iteration 22, loss = 0.14580135
Iteration 23, loss = 0.13389332
Iteration 24, loss = 0.12312400
Iteration 25, loss = 0.11355401
Iteration 26, loss = 0.10508694
Iteration 27, loss = 0.09744626
Iteration 28, loss = 0.09065626
Iteration 29, loss = 0.08458486
Iteration 30, loss = 0.07908614
Iteration 31, loss = 0.07420185
Iteration 32, loss = 0.06974544
Iteration 33, loss = 0.06570556
Iteration 34, loss = 0.06206788
Iteration 35, loss = 0.05880323
Iteration 36, loss = 0.05579282
Iteration 37, loss = 0.05303627
Iteration 38, loss = 0.05053748
Iteration 39, loss = 0.04821706
Iteration 40, loss = 0.04610647
Iteration 41, loss = 0.04419260
Iteration 42, loss = 0.04240107
Iteration 43, loss = 0.04072737
Iteration 44, loss = 0.03917014
Iteration 45, loss = 0.03780984
Iteration 46, loss = 0.03650308
Iteration 47, loss = 0.03525412
Iteration 48, loss = 0.03406316
Iteration 49, loss = 0.03300131
Iteration 50, loss = 0.03201135
Iteration 51, loss = 0.03105364
Iteration 52, loss = 0.03016395
Iteration 53, loss = 0.02933736
Iteration 54, loss = 0.02852888
Iteration 55, loss = 0.02783815
Iteration 56, loss = 0.02711921
Iteration 57, loss = 0.02647006
Iteration 58, loss = 0.02584239
Iteration 59, loss = 0.02525950
Iteration 60, loss = 0.02475212
Iteration 61, loss = 0.02417762
Iteration 62, loss = 0.02369744
Iteration 63, loss = 0.02324153
Iteration 64, loss = 0.02290804
Iteration 65, loss = 0.02236645
Iteration 66, loss = 0.02200614
Iteration 67, loss = 0.02155042
Iteration 68, loss = 0.02118403
Iteration 69, loss = 0.02084916
Iteration 70, loss = 0.02051873
Iteration 71, loss = 0.02024289
Iteration 72, loss = 0.01989287
Iteration 73, loss = 0.01962034
Iteration 74, loss = 0.01933144
Iteration 75, loss = 0.01902407
Iteration 76, loss = 0.01877694
Iteration 77, loss = 0.01852986
Iteration 78, loss = 0.01829989
Iteration 79, loss = 0.01805630
Iteration 80, loss = 0.01784703
Iteration 81, loss = 0.01767690
Iteration 82, loss = 0.01745719
Iteration 83, loss = 0.01726077
Iteration 84, loss = 0.01707960
Iteration 85, loss = 0.01689004
Iteration 86, loss = 0.01668143
Iteration 87, loss = 0.01653755
Iteration 88, loss = 0.01640807
Iteration 89, loss = 0.01624024
Iteration 90, loss = 0.01605415
Iteration 91, loss = 0.01595683
Iteration 92, loss = 0.01581678
Iteration 93, loss = 0.01569438
Iteration 94, loss = 0.01550601
Iteration 95, loss = 0.01537713
Iteration 96, loss = 0.01525395
Iteration 97, loss = 0.01522256
Iteration 98, loss = 0.01515755
Iteration 99, loss = 0.01491718
Iteration 100, loss = 0.01484991
Iteration 101, loss = 0.01471992
Iteration 102, loss = 0.01461737
Iteration 103, loss = 0.01449492
Iteration 104, loss = 0.01445840
Iteration 105, loss = 0.01435050
Iteration 106, loss = 0.01427926
Iteration 107, loss = 0.01415069
Iteration 108, loss = 0.01405315
Iteration 109, loss = 0.01396531
Iteration 110, loss = 0.01388619
Iteration 111, loss = 0.01387795
Iteration 112, loss = 0.01375715
Iteration 113, loss = 0.01368338
Iteration 114, loss = 0.01366780
Iteration 115, loss = 0.01354922
Iteration 116, loss = 0.01347382
Iteration 117, loss = 0.01338008
Iteration 118, loss = 0.01336280
Iteration 119, loss = 0.01330135
Iteration 120, loss = 0.01324295
Iteration 121, loss = 0.01315354
Iteration 122, loss = 0.01311563
Iteration 123, loss = 0.01306752
Iteration 124, loss = 0.01303017
Iteration 125, loss = 0.01295221
Iteration 126, loss = 0.01291386
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70994134
Iteration 2, loss = 0.68774814
Iteration 3, loss = 0.66570752
Iteration 4, loss = 0.64025815
Iteration 5, loss = 0.61058965
Iteration 6, loss = 0.57753386
Iteration 7, loss = 0.54208926
Iteration 8, loss = 0.50572645
Iteration 9, loss = 0.46904929
Iteration 10, loss = 0.43313409
Iteration 11, loss = 0.39796123
Iteration 12, loss = 0.36433308
Iteration 13, loss = 0.33285071
Iteration 14, loss = 0.30331275
Iteration 15, loss = 0.27594105
Iteration 16, loss = 0.25073105
Iteration 17, loss = 0.22789720
Iteration 18, loss = 0.20728963
Iteration 19, loss = 0.18870673
Iteration 20, loss = 0.17213364
Iteration 21, loss = 0.15738471
Iteration 22, loss = 0.14415620
Iteration 23, loss = 0.13244684
Iteration 24, loss = 0.12193821
Iteration 25, loss = 0.11266183
Iteration 26, loss = 0.10436188
Iteration 27, loss = 0.09692079
Iteration 28, loss = 0.09035481
Iteration 29, loss = 0.08440865
Iteration 30, loss = 0.07905846
Iteration 31, loss = 0.07423360
Iteration 32, loss = 0.06995599
Iteration 33, loss = 0.06601520
Iteration 34, loss = 0.06256477
Iteration 35, loss = 0.05925192
Iteration 36, loss = 0.05631076
Iteration 37, loss = 0.05358123
Iteration 38, loss = 0.05113207
Iteration 39, loss = 0.04895366
Iteration 40, loss = 0.04683679
Iteration 41, loss = 0.04491371
Iteration 42, loss = 0.04318378
Iteration 43, loss = 0.04154997
Iteration 44, loss = 0.04001562
Iteration 45, loss = 0.03861777
Iteration 46, loss = 0.03728099
Iteration 47, loss = 0.03601908
Iteration 48, loss = 0.03491964
Iteration 49, loss = 0.03383896
Iteration 50, loss = 0.03285879
Iteration 51, loss = 0.03193091
Iteration 52, loss = 0.03102366
Iteration 53, loss = 0.03023129
Iteration 54, loss = 0.02942291
Iteration 55, loss = 0.02876286
Iteration 56, loss = 0.02801807
Iteration 57, loss = 0.02738804
Iteration 58, loss = 0.02676586
Iteration 59, loss = 0.02615356
Iteration 60, loss = 0.02565460
Iteration 61, loss = 0.02516116
Iteration 62, loss = 0.02462817
Iteration 63, loss = 0.02414636
Iteration 64, loss = 0.02370959
Iteration 65, loss = 0.02334351
Iteration 66, loss = 0.02286892
Iteration 67, loss = 0.02247990
Iteration 68, loss = 0.02209795
Iteration 69, loss = 0.02175008
Iteration 70, loss = 0.02137787
Iteration 71, loss = 0.02110652
Iteration 72, loss = 0.02078899
Iteration 73, loss = 0.02051504
Iteration 74, loss = 0.02021845
Iteration 75, loss = 0.01996117
Iteration 76, loss = 0.01970956
Iteration 77, loss = 0.01944827
Iteration 78, loss = 0.01923912
Iteration 79, loss = 0.01901639
Iteration 80, loss = 0.01873186
Iteration 81, loss = 0.01850417
Iteration 82, loss = 0.01835584
Iteration 83, loss = 0.01813848
Iteration 84, loss = 0.01795708
Iteration 85, loss = 0.01777091
Iteration 86, loss = 0.01756277
Iteration 87, loss = 0.01739747
Iteration 88, loss = 0.01727319
Iteration 89, loss = 0.01709121
Iteration 90, loss = 0.01699640
Iteration 91, loss = 0.01679567
Iteration 92, loss = 0.01664770
Iteration 93, loss = 0.01653115
Iteration 94, loss = 0.01640459
Iteration 95, loss = 0.01629384
Iteration 96, loss = 0.01613243
Iteration 97, loss = 0.01601912
Iteration 98, loss = 0.01592043
Iteration 99, loss = 0.01579734
Iteration 100, loss = 0.01570053
Iteration 101, loss = 0.01557940
Iteration 102, loss = 0.01551055
Iteration 103, loss = 0.01538707
Iteration 104, loss = 0.01526604
Iteration 105, loss = 0.01526118
Iteration 106, loss = 0.01516934
Iteration 107, loss = 0.01499056
Iteration 108, loss = 0.01493866
Iteration 109, loss = 0.01482800
Iteration 110, loss = 0.01477312
Iteration 111, loss = 0.01469490
Iteration 112, loss = 0.01460348
Iteration 113, loss = 0.01451102
Iteration 114, loss = 0.01445681
Iteration 115, loss = 0.01440306
Iteration 116, loss = 0.01429438
Iteration 117, loss = 0.01429349
Iteration 118, loss = 0.01418829
Iteration 119, loss = 0.01418282
Iteration 120, loss = 0.01408051
Iteration 121, loss = 0.01401022
Iteration 122, loss = 0.01398528
Iteration 123, loss = 0.01395169
Iteration 124, loss = 0.01388051
Iteration 125, loss = 0.01385824
Iteration 126, loss = 0.01384551
Iteration 127, loss = 0.01366418
Iteration 128, loss = 0.01364671
Iteration 129, loss = 0.01357487
Iteration 130, loss = 0.01353561
Iteration 131, loss = 0.01350184
Iteration 132, loss = 0.01342536
Iteration 133, loss = 0.01342910
Iteration 134, loss = 0.01337326
Iteration 135, loss = 0.01331082
Iteration 136, loss = 0.01328443
Iteration 137, loss = 0.01331501
Iteration 138, loss = 0.01320863
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67759068
Iteration 2, loss = 0.61969537
Iteration 3, loss = 0.53126668
Iteration 4, loss = 0.43609574
Iteration 5, loss = 0.34916801
Iteration 6, loss = 0.27593544
Iteration 7, loss = 0.21808065
Iteration 8, loss = 0.17382453
Iteration 9, loss = 0.14099777
Iteration 10, loss = 0.11684811
Iteration 11, loss = 0.09839714
Iteration 12, loss = 0.08462398
Iteration 13, loss = 0.07389813
Iteration 14, loss = 0.06559967
Iteration 15, loss = 0.05867589
Iteration 16, loss = 0.05337086
Iteration 17, loss = 0.04912477
Iteration 18, loss = 0.04522753
Iteration 19, loss = 0.04218293
Iteration 20, loss = 0.03966970
Iteration 21, loss = 0.03737334
Iteration 22, loss = 0.03532715
Iteration 23, loss = 0.03374644
Iteration 24, loss = 0.03227933
Iteration 25, loss = 0.03115378
Iteration 26, loss = 0.03007419
Iteration 27, loss = 0.02928263
Iteration 28, loss = 0.02819682
Iteration 29, loss = 0.02739056
Iteration 30, loss = 0.02677944
Iteration 31, loss = 0.02600647
Iteration 32, loss = 0.02554599
Iteration 33, loss = 0.02514700
Iteration 34, loss = 0.02485199
Iteration 35, loss = 0.02453912
Iteration 36, loss = 0.02375834
Iteration 37, loss = 0.02355372
Iteration 38, loss = 0.02332798
Iteration 39, loss = 0.02272854
Iteration 40, loss = 0.02264315
Iteration 41, loss = 0.02244501
Iteration 42, loss = 0.02208019
Iteration 43, loss = 0.02172523
Iteration 44, loss = 0.02168348
Iteration 45, loss = 0.02150281
Iteration 46, loss = 0.02151363
Iteration 47, loss = 0.02105497
Iteration 48, loss = 0.02130904
Iteration 49, loss = 0.02086168
Iteration 50, loss = 0.02079324
Iteration 51, loss = 0.02060181
Iteration 52, loss = 0.02058984
Iteration 53, loss = 0.02029604
Iteration 54, loss = 0.02036873
Iteration 55, loss = 0.02028797
Iteration 56, loss = 0.02013010
Iteration 57, loss = 0.01999347
Iteration 58, loss = 0.02007066
Iteration 59, loss = 0.01988162
Iteration 60, loss = 0.01997324
Iteration 61, loss = 0.01958465
Iteration 62, loss = 0.01965874
Iteration 63, loss = 0.01955889
Iteration 64, loss = 0.01995243
Iteration 65, loss = 0.01954536
Iteration 66, loss = 0.01932407
Iteration 67, loss = 0.01930903
Iteration 68, loss = 0.01919811
Iteration 69, loss = 0.01925540
Iteration 70, loss = 0.01922186
Iteration 71, loss = 0.01913603
Iteration 72, loss = 0.01919839
Iteration 73, loss = 0.01923758
Iteration 74, loss = 0.01911297
Iteration 75, loss = 0.01924310
Iteration 76, loss = 0.01913258
Iteration 77, loss = 0.01925923
Iteration 78, loss = 0.01910668
Iteration 79, loss = 0.01928832
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67867995
Iteration 2, loss = 0.66033034
Iteration 3, loss = 0.64218086
Iteration 4, loss = 0.62099763
Iteration 5, loss = 0.59636694
Iteration 6, loss = 0.56750626
Iteration 7, loss = 0.53463973
Iteration 8, loss = 0.49960010
Iteration 9, loss = 0.46304747
Iteration 10, loss = 0.42661771
Iteration 11, loss = 0.39137096
Iteration 12, loss = 0.35721484
Iteration 13, loss = 0.32569707
Iteration 14, loss = 0.29645658
Iteration 15, loss = 0.26970375
Iteration 16, loss = 0.24545931
Iteration 17, loss = 0.22366938
Iteration 18, loss = 0.20390436
Iteration 19, loss = 0.18630387
Iteration 20, loss = 0.17044112
Iteration 21, loss = 0.15627139
Iteration 22, loss = 0.14359363
Iteration 23, loss = 0.13227306
Iteration 24, loss = 0.12216068
Iteration 25, loss = 0.11307700
Iteration 26, loss = 0.10496533
Iteration 27, loss = 0.09764903
Iteration 28, loss = 0.09107878
Iteration 29, loss = 0.08517057
Iteration 30, loss = 0.07986126
Iteration 31, loss = 0.07506437
Iteration 32, loss = 0.07064690
Iteration 33, loss = 0.06665755
Iteration 34, loss = 0.06309326
Iteration 35, loss = 0.05975515
Iteration 36, loss = 0.05674814
Iteration 37, loss = 0.05402275
Iteration 38, loss = 0.05158437
Iteration 39, loss = 0.04921779
Iteration 40, loss = 0.04701496
Iteration 41, loss = 0.04505379
Iteration 42, loss = 0.04328815
Iteration 43, loss = 0.04153194
Iteration 44, loss = 0.03997660
Iteration 45, loss = 0.03856685
Iteration 46, loss = 0.03724975
Iteration 47, loss = 0.03594184
Iteration 48, loss = 0.03475217
Iteration 49, loss = 0.03368711
Iteration 50, loss = 0.03264944
Iteration 51, loss = 0.03166782
Iteration 52, loss = 0.03073381
Iteration 53, loss = 0.02995420
Iteration 54, loss = 0.02912228
Iteration 55, loss = 0.02837100
Iteration 56, loss = 0.02766483
Iteration 57, loss = 0.02703545
Iteration 58, loss = 0.02635851
Iteration 59, loss = 0.02573212
Iteration 60, loss = 0.02520449
Iteration 61, loss = 0.02468359
Iteration 62, loss = 0.02416278
Iteration 63, loss = 0.02366235
Iteration 64, loss = 0.02330511
Iteration 65, loss = 0.02277076
Iteration 66, loss = 0.02235328
Iteration 67, loss = 0.02197969
Iteration 68, loss = 0.02157242
Iteration 69, loss = 0.02126269
Iteration 70, loss = 0.02093064
Iteration 71, loss = 0.02060835
Iteration 72, loss = 0.02023917
Iteration 73, loss = 0.01998468
Iteration 74, loss = 0.01971662
Iteration 75, loss = 0.01936612
Iteration 76, loss = 0.01915195
Iteration 77, loss = 0.01888695
Iteration 78, loss = 0.01864388
Iteration 79, loss = 0.01839000
Iteration 80, loss = 0.01822663
Iteration 81, loss = 0.01801252
Iteration 82, loss = 0.01780468
Iteration 83, loss = 0.01758535
Iteration 84, loss = 0.01743084
Iteration 85, loss = 0.01719821
Iteration 86, loss = 0.01710728
Iteration 87, loss = 0.01684741
Iteration 88, loss = 0.01668909
Iteration 89, loss = 0.01653099
Iteration 90, loss = 0.01641886
Iteration 91, loss = 0.01625552
Iteration 92, loss = 0.01611743
Iteration 93, loss = 0.01596097
Iteration 94, loss = 0.01586934
Iteration 95, loss = 0.01576260
Iteration 96, loss = 0.01558939
Iteration 97, loss = 0.01554351
Iteration 98, loss = 0.01535693
Iteration 99, loss = 0.01530033
Iteration 100, loss = 0.01510688
Iteration 101, loss = 0.01500386
Iteration 102, loss = 0.01491902
Iteration 103, loss = 0.01488503
Iteration 104, loss = 0.01472168
Iteration 105, loss = 0.01462685
Iteration 106, loss = 0.01454606
Iteration 107, loss = 0.01445617
Iteration 108, loss = 0.01437484
Iteration 109, loss = 0.01432774
Iteration 110, loss = 0.01421324
Iteration 111, loss = 0.01409841
Iteration 112, loss = 0.01407620
Iteration 113, loss = 0.01400328
Iteration 114, loss = 0.01390345
Iteration 115, loss = 0.01389183
Iteration 116, loss = 0.01381903
Iteration 117, loss = 0.01368674
Iteration 118, loss = 0.01367507
Iteration 119, loss = 0.01361618
Iteration 120, loss = 0.01348454
Iteration 121, loss = 0.01349225
Iteration 122, loss = 0.01337537
Iteration 123, loss = 0.01339648
Iteration 124, loss = 0.01328192
Iteration 125, loss = 0.01329869
Iteration 126, loss = 0.01318814
Iteration 127, loss = 0.01317617
Iteration 128, loss = 0.01306751
Iteration 129, loss = 0.01301619
Iteration 130, loss = 0.01297864
Iteration 131, loss = 0.01295347
Iteration 132, loss = 0.01287318
Iteration 133, loss = 0.01288457
Iteration 134, loss = 0.01279160
Iteration 135, loss = 0.01274742
Iteration 136, loss = 0.01274265
Iteration 137, loss = 0.01268280
Iteration 138, loss = 0.01270208
Iteration 139, loss = 0.01265864
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70977691
Iteration 2, loss = 0.65896915
Iteration 3, loss = 0.59516436
Iteration 4, loss = 0.51315679
Iteration 5, loss = 0.42697930
Iteration 6, loss = 0.34736645
Iteration 7, loss = 0.28014834
Iteration 8, loss = 0.22612981
Iteration 9, loss = 0.18438729
Iteration 10, loss = 0.15245380
Iteration 11, loss = 0.12795135
Iteration 12, loss = 0.10891699
Iteration 13, loss = 0.09438071
Iteration 14, loss = 0.08253806
Iteration 15, loss = 0.07343984
Iteration 16, loss = 0.06584000
Iteration 17, loss = 0.05959131
Iteration 18, loss = 0.05416832
Iteration 19, loss = 0.04988340
Iteration 20, loss = 0.04623015
Iteration 21, loss = 0.04304184
Iteration 22, loss = 0.04043113
Iteration 23, loss = 0.03807902
Iteration 24, loss = 0.03607772
Iteration 25, loss = 0.03423072
Iteration 26, loss = 0.03279094
Iteration 27, loss = 0.03124850
Iteration 28, loss = 0.03023774
Iteration 29, loss = 0.02905593
Iteration 30, loss = 0.02807285
Iteration 31, loss = 0.02724494
Iteration 32, loss = 0.02640419
Iteration 33, loss = 0.02563962
Iteration 34, loss = 0.02505544
Iteration 35, loss = 0.02475872
Iteration 36, loss = 0.02400180
Iteration 37, loss = 0.02343140
Iteration 38, loss = 0.02287658
Iteration 39, loss = 0.02259296
Iteration 40, loss = 0.02207766
Iteration 41, loss = 0.02162877
Iteration 42, loss = 0.02151437
Iteration 43, loss = 0.02114894
Iteration 44, loss = 0.02086211
Iteration 45, loss = 0.02075634
Iteration 46, loss = 0.02035526
Iteration 47, loss = 0.02002680
Iteration 48, loss = 0.02017896
Iteration 49, loss = 0.01974046
Iteration 50, loss = 0.01962105
Iteration 51, loss = 0.01924475
Iteration 52, loss = 0.01931931
Iteration 53, loss = 0.01906847
Iteration 54, loss = 0.01888165
Iteration 55, loss = 0.01874952
Iteration 56, loss = 0.01878950
Iteration 57, loss = 0.01852694
Iteration 58, loss = 0.01844211
Iteration 59, loss = 0.01820179
Iteration 60, loss = 0.01806650
Iteration 61, loss = 0.01811040
Iteration 62, loss = 0.01783839
Iteration 63, loss = 0.01803473
Iteration 64, loss = 0.01789457
Iteration 65, loss = 0.01787466
Iteration 66, loss = 0.01751191
Iteration 67, loss = 0.01755583
Iteration 68, loss = 0.01763377
Iteration 69, loss = 0.01745597
Iteration 70, loss = 0.01731947
Iteration 71, loss = 0.01710454
Iteration 72, loss = 0.01721683
Iteration 73, loss = 0.01709980
Iteration 74, loss = 0.01734205
Iteration 75, loss = 0.01713877
Iteration 76, loss = 0.01699410
Iteration 77, loss = 0.01701914
Iteration 78, loss = 0.01689384
Iteration 79, loss = 0.01711290
Iteration 80, loss = 0.01699306
Iteration 81, loss = 0.01681970
Iteration 82, loss = 0.01683220
Iteration 83, loss = 0.01666616
Iteration 84, loss = 0.01687940
Iteration 85, loss = 0.01652454
Iteration 86, loss = 0.01674144
Iteration 87, loss = 0.01653048
Iteration 88, loss = 0.01640700
Iteration 89, loss = 0.01655039
Iteration 90, loss = 0.01668983
Iteration 91, loss = 0.01641715
Iteration 92, loss = 0.01647974
Iteration 93, loss = 0.01653487
Iteration 94, loss = 0.01659243
Iteration 95, loss = 0.01659142
Iteration 96, loss = 0.01651130
Iteration 97, loss = 0.01626420
Iteration 98, loss = 0.01629180
Iteration 99, loss = 0.01622877
Iteration 100, loss = 0.01633283
Iteration 101, loss = 0.01644664
Iteration 102, loss = 0.01646036
Iteration 103, loss = 0.01633087
Iteration 104, loss = 0.01614033
Iteration 105, loss = 0.01617297
Iteration 106, loss = 0.01613502
Iteration 107, loss = 0.01620389
Iteration 108, loss = 0.01619736
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67469778
Iteration 2, loss = 0.60781578
Iteration 3, loss = 0.51387673
Iteration 4, loss = 0.41487327
Iteration 5, loss = 0.32460893
Iteration 6, loss = 0.25134311
Iteration 7, loss = 0.19583690
Iteration 8, loss = 0.15507694
Iteration 9, loss = 0.12521227
Iteration 10, loss = 0.10390968
Iteration 11, loss = 0.08785512
Iteration 12, loss = 0.07600752
Iteration 13, loss = 0.06697759
Iteration 14, loss = 0.05971881
Iteration 15, loss = 0.05434494
Iteration 16, loss = 0.04943124
Iteration 17, loss = 0.04585809
Iteration 18, loss = 0.04266189
Iteration 19, loss = 0.04019335
Iteration 20, loss = 0.03785810
Iteration 21, loss = 0.03663282
Iteration 22, loss = 0.03491537
Iteration 23, loss = 0.03343781
Iteration 24, loss = 0.03210250
Iteration 25, loss = 0.03101416
Iteration 26, loss = 0.03009715
Iteration 27, loss = 0.02918301
Iteration 28, loss = 0.02832714
Iteration 29, loss = 0.02767835
Iteration 30, loss = 0.02703793
Iteration 31, loss = 0.02652347
Iteration 32, loss = 0.02600248
Iteration 33, loss = 0.02554578
Iteration 34, loss = 0.02502843
Iteration 35, loss = 0.02481738
Iteration 36, loss = 0.02427343
Iteration 37, loss = 0.02403338
Iteration 38, loss = 0.02362478
Iteration 39, loss = 0.02342494
Iteration 40, loss = 0.02309460
Iteration 41, loss = 0.02305333
Iteration 42, loss = 0.02283477
Iteration 43, loss = 0.02250949
Iteration 44, loss = 0.02229806
Iteration 45, loss = 0.02210721
Iteration 46, loss = 0.02209221
Iteration 47, loss = 0.02172256
Iteration 48, loss = 0.02161205
Iteration 49, loss = 0.02166143
Iteration 50, loss = 0.02168392
Iteration 51, loss = 0.02149237
Iteration 52, loss = 0.02126410
Iteration 53, loss = 0.02094777
Iteration 54, loss = 0.02095955
Iteration 55, loss = 0.02067405
Iteration 56, loss = 0.02071922
Iteration 57, loss = 0.02080517
Iteration 58, loss = 0.02073044
Iteration 59, loss = 0.02054880
Iteration 60, loss = 0.02046903
Iteration 61, loss = 0.02052943
Iteration 62, loss = 0.02034577
Iteration 63, loss = 0.02014455
Iteration 64, loss = 0.02010525
Iteration 65, loss = 0.02000800
Iteration 66, loss = 0.01995104
Iteration 67, loss = 0.01994119
Iteration 68, loss = 0.01981291
Iteration 69, loss = 0.01979330
Iteration 70, loss = 0.01983630
Iteration 71, loss = 0.01987257
Iteration 72, loss = 0.02006845
Iteration 73, loss = 0.02003852
Iteration 74, loss = 0.01995656
Iteration 75, loss = 0.02002126
Iteration 76, loss = 0.01977174
Iteration 77, loss = 0.02049316
Iteration 78, loss = 0.02041026
Iteration 79, loss = 0.02019329
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72302488
Iteration 2, loss = 0.68852926
Iteration 3, loss = 0.64546357
Iteration 4, loss = 0.59506726
Iteration 5, loss = 0.53755526
Iteration 6, loss = 0.47666766
Iteration 7, loss = 0.41468146
Iteration 8, loss = 0.35595647
Iteration 9, loss = 0.30280875
Iteration 10, loss = 0.25638789
Iteration 11, loss = 0.21690173
Iteration 12, loss = 0.18412773
Iteration 13, loss = 0.15718238
Iteration 14, loss = 0.13515596
Iteration 15, loss = 0.11728476
Iteration 16, loss = 0.10251839
Iteration 17, loss = 0.09041087
Iteration 18, loss = 0.08039934
Iteration 19, loss = 0.07202210
Iteration 20, loss = 0.06505354
Iteration 21, loss = 0.05915612
Iteration 22, loss = 0.05409368
Iteration 23, loss = 0.04979668
Iteration 24, loss = 0.04603444
Iteration 25, loss = 0.04281271
Iteration 26, loss = 0.04001798
Iteration 27, loss = 0.03752418
Iteration 28, loss = 0.03530451
Iteration 29, loss = 0.03338632
Iteration 30, loss = 0.03164654
Iteration 31, loss = 0.03015726
Iteration 32, loss = 0.02866484
Iteration 33, loss = 0.02741141
Iteration 34, loss = 0.02629209
Iteration 35, loss = 0.02526776
Iteration 36, loss = 0.02426626
Iteration 37, loss = 0.02355520
Iteration 38, loss = 0.02276061
Iteration 39, loss = 0.02202458
Iteration 40, loss = 0.02136587
Iteration 41, loss = 0.02073847
Iteration 42, loss = 0.02021707
Iteration 43, loss = 0.01968438
Iteration 44, loss = 0.01921013
Iteration 45, loss = 0.01885867
Iteration 46, loss = 0.01843546
Iteration 47, loss = 0.01797700
Iteration 48, loss = 0.01769199
Iteration 49, loss = 0.01739037
Iteration 50, loss = 0.01701253
Iteration 51, loss = 0.01667080
Iteration 52, loss = 0.01647003
Iteration 53, loss = 0.01616734
Iteration 54, loss = 0.01591412
Iteration 55, loss = 0.01575855
Iteration 56, loss = 0.01540762
Iteration 57, loss = 0.01531094
Iteration 58, loss = 0.01511548
Iteration 59, loss = 0.01482777
Iteration 60, loss = 0.01477443
Iteration 61, loss = 0.01463211
Iteration 62, loss = 0.01439141
Iteration 63, loss = 0.01427604
Iteration 64, loss = 0.01411767
Iteration 65, loss = 0.01400373
Iteration 66, loss = 0.01389453
Iteration 67, loss = 0.01372571
Iteration 68, loss = 0.01374831
Iteration 69, loss = 0.01359522
Iteration 70, loss = 0.01341555
Iteration 71, loss = 0.01330089
Iteration 72, loss = 0.01323452
Iteration 73, loss = 0.01315524
Iteration 74, loss = 0.01304019
Iteration 75, loss = 0.01290539
Iteration 76, loss = 0.01294733
Iteration 77, loss = 0.01285415
Iteration 78, loss = 0.01278827
Iteration 79, loss = 0.01293716
Iteration 80, loss = 0.01268918
Iteration 81, loss = 0.01252292
Iteration 82, loss = 0.01254514
Iteration 83, loss = 0.01246410
Iteration 84, loss = 0.01240658
Iteration 85, loss = 0.01240105
Iteration 86, loss = 0.01231402
Iteration 87, loss = 0.01219648
Iteration 88, loss = 0.01226782
Iteration 89, loss = 0.01212616
Iteration 90, loss = 0.01209496
Iteration 91, loss = 0.01208878
Iteration 92, loss = 0.01194526
Iteration 93, loss = 0.01204050
Iteration 94, loss = 0.01193365
Iteration 95, loss = 0.01186081
Iteration 96, loss = 0.01190966
Iteration 97, loss = 0.01181343
Iteration 98, loss = 0.01176836
Iteration 99, loss = 0.01173945
Iteration 100, loss = 0.01181864
Iteration 101, loss = 0.01160745
Iteration 102, loss = 0.01161543
Iteration 103, loss = 0.01164031
Iteration 104, loss = 0.01155898
Iteration 105, loss = 0.01149036
Iteration 106, loss = 0.01149136
Iteration 107, loss = 0.01150139
Iteration 108, loss = 0.01138726
Iteration 109, loss = 0.01147812
Iteration 110, loss = 0.01138474
Iteration 111, loss = 0.01139549
Iteration 112, loss = 0.01139283
Iteration 113, loss = 0.01131905
Iteration 114, loss = 0.01137682
Iteration 115, loss = 0.01128367
Iteration 116, loss = 0.01130899
Iteration 117, loss = 0.01133257
Iteration 118, loss = 0.01126313
Iteration 119, loss = 0.01134163
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70053920
Iteration 2, loss = 0.65142141
Iteration 3, loss = 0.59208267
Iteration 4, loss = 0.52540664
Iteration 5, loss = 0.45541764
Iteration 6, loss = 0.38773320
Iteration 7, loss = 0.32658123
Iteration 8, loss = 0.27344980
Iteration 9, loss = 0.22862397
Iteration 10, loss = 0.19168049
Iteration 11, loss = 0.16183212
Iteration 12, loss = 0.13764194
Iteration 13, loss = 0.11821357
Iteration 14, loss = 0.10274583
Iteration 15, loss = 0.09010765
Iteration 16, loss = 0.07986048
Iteration 17, loss = 0.07137576
Iteration 18, loss = 0.06450680
Iteration 19, loss = 0.05850966
Iteration 20, loss = 0.05366771
Iteration 21, loss = 0.04949913
Iteration 22, loss = 0.04589241
Iteration 23, loss = 0.04278002
Iteration 24, loss = 0.04019997
Iteration 25, loss = 0.03786728
Iteration 26, loss = 0.03568072
Iteration 27, loss = 0.03411434
Iteration 28, loss = 0.03249655
Iteration 29, loss = 0.03108021
Iteration 30, loss = 0.02958713
Iteration 31, loss = 0.02855242
Iteration 32, loss = 0.02735455
Iteration 33, loss = 0.02654647
Iteration 34, loss = 0.02567258
Iteration 35, loss = 0.02498743
Iteration 36, loss = 0.02425489
Iteration 37, loss = 0.02359832
Iteration 38, loss = 0.02294318
Iteration 39, loss = 0.02263690
Iteration 40, loss = 0.02200828
Iteration 41, loss = 0.02154808
Iteration 42, loss = 0.02120362
Iteration 43, loss = 0.02064770
Iteration 44, loss = 0.02049961
Iteration 45, loss = 0.02003265
Iteration 46, loss = 0.01967744
Iteration 47, loss = 0.01942913
Iteration 48, loss = 0.01912909
Iteration 49, loss = 0.01892346
Iteration 50, loss = 0.01863788
Iteration 51, loss = 0.01847239
Iteration 52, loss = 0.01822317
Iteration 53, loss = 0.01808120
Iteration 54, loss = 0.01783958
Iteration 55, loss = 0.01767616
Iteration 56, loss = 0.01752460
Iteration 57, loss = 0.01729339
Iteration 58, loss = 0.01733935
Iteration 59, loss = 0.01700910
Iteration 60, loss = 0.01691919
Iteration 61, loss = 0.01684231
Iteration 62, loss = 0.01662892
Iteration 63, loss = 0.01648479
Iteration 64, loss = 0.01645760
Iteration 65, loss = 0.01642996
Iteration 66, loss = 0.01620215
Iteration 67, loss = 0.01615949
Iteration 68, loss = 0.01597690
Iteration 69, loss = 0.01599560
Iteration 70, loss = 0.01594715
Iteration 71, loss = 0.01593775
Iteration 72, loss = 0.01583442
Iteration 73, loss = 0.01568758
Iteration 74, loss = 0.01571903
Iteration 75, loss = 0.01563934
Iteration 76, loss = 0.01558952
Iteration 77, loss = 0.01539158
Iteration 78, loss = 0.01549938
Iteration 79, loss = 0.01532956
Iteration 80, loss = 0.01533310
Iteration 81, loss = 0.01516078
Iteration 82, loss = 0.01516564
Iteration 83, loss = 0.01512016
Iteration 84, loss = 0.01514456
Iteration 85, loss = 0.01497457
Iteration 86, loss = 0.01504925
Iteration 87, loss = 0.01505753
Iteration 88, loss = 0.01479138
Iteration 89, loss = 0.01492719
Iteration 90, loss = 0.01487925
Iteration 91, loss = 0.01479188
Iteration 92, loss = 0.01483129
Iteration 93, loss = 0.01480460
Iteration 94, loss = 0.01473303
Iteration 95, loss = 0.01463089
Iteration 96, loss = 0.01480319
Iteration 97, loss = 0.01479869
Iteration 98, loss = 0.01451933
Iteration 99, loss = 0.01458625
Iteration 100, loss = 0.01456340
Iteration 101, loss = 0.01463534
Iteration 102, loss = 0.01449499
Iteration 103, loss = 0.01448056
Iteration 104, loss = 0.01445971
Iteration 105, loss = 0.01433276
Iteration 106, loss = 0.01442327
Iteration 107, loss = 0.01434284
Iteration 108, loss = 0.01430866
Iteration 109, loss = 0.01443228
Iteration 110, loss = 0.01453766
Iteration 111, loss = 0.01426243
Iteration 112, loss = 0.01440327
Iteration 113, loss = 0.01416904
Iteration 114, loss = 0.01423920
Iteration 115, loss = 0.01424282
Iteration 116, loss = 0.01423164
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70091153
Iteration 2, loss = 0.63009927
Iteration 3, loss = 0.53807149
Iteration 4, loss = 0.43453118
Iteration 5, loss = 0.33973742
Iteration 6, loss = 0.26180124
Iteration 7, loss = 0.20335605
Iteration 8, loss = 0.16073832
Iteration 9, loss = 0.13039941
Iteration 10, loss = 0.10823584
Iteration 11, loss = 0.09227749
Iteration 12, loss = 0.08002441
Iteration 13, loss = 0.07064486
Iteration 14, loss = 0.06334884
Iteration 15, loss = 0.05775429
Iteration 16, loss = 0.05282182
Iteration 17, loss = 0.04912997
Iteration 18, loss = 0.04592406
Iteration 19, loss = 0.04364305
Iteration 20, loss = 0.04124214
Iteration 21, loss = 0.03951636
Iteration 22, loss = 0.03766050
Iteration 23, loss = 0.03603252
Iteration 24, loss = 0.03491531
Iteration 25, loss = 0.03384274
Iteration 26, loss = 0.03297023
Iteration 27, loss = 0.03194394
Iteration 28, loss = 0.03107281
Iteration 29, loss = 0.03039333
Iteration 30, loss = 0.02985580
Iteration 31, loss = 0.02925488
Iteration 32, loss = 0.02868764
Iteration 33, loss = 0.02825849
Iteration 34, loss = 0.02801057
Iteration 35, loss = 0.02741903
Iteration 36, loss = 0.02712930
Iteration 37, loss = 0.02690374
Iteration 38, loss = 0.02644779
Iteration 39, loss = 0.02618549
Iteration 40, loss = 0.02570643
Iteration 41, loss = 0.02564022
Iteration 42, loss = 0.02540668
Iteration 43, loss = 0.02548845
Iteration 44, loss = 0.02496603
Iteration 45, loss = 0.02475938
Iteration 46, loss = 0.02475859
Iteration 47, loss = 0.02441943
Iteration 48, loss = 0.02442918
Iteration 49, loss = 0.02423924
Iteration 50, loss = 0.02407258
Iteration 51, loss = 0.02420655
Iteration 52, loss = 0.02382751
Iteration 53, loss = 0.02370531
Iteration 54, loss = 0.02441953
Iteration 55, loss = 0.02454954
Iteration 56, loss = 0.02429994
Iteration 57, loss = 0.02434951
Iteration 58, loss = 0.02404368
Iteration 59, loss = 0.02406086
Iteration 60, loss = 0.02380852
Iteration 61, loss = 0.02367987
Iteration 62, loss = 0.02363579
Iteration 63, loss = 0.02351568
Iteration 64, loss = 0.02356174
Iteration 65, loss = 0.02330739
Iteration 66, loss = 0.02295400
Iteration 67, loss = 0.02288175
Iteration 68, loss = 0.02280096
Iteration 69, loss = 0.02295101
Iteration 70, loss = 0.02264886
Iteration 71, loss = 0.02278773
Iteration 72, loss = 0.02288521
Iteration 73, loss = 0.02280135
Iteration 74, loss = 0.02251451
Iteration 75, loss = 0.02266358
Iteration 76, loss = 0.02250259
Iteration 77, loss = 0.02265659
Iteration 78, loss = 0.02273519
Iteration 79, loss = 0.02243440
Iteration 80, loss = 0.02256980
Iteration 81, loss = 0.02208934
Iteration 82, loss = 0.02216048
Iteration 83, loss = 0.02250676
Iteration 84, loss = 0.02217124
Iteration 85, loss = 0.02205786
Iteration 86, loss = 0.02205307
Iteration 87, loss = 0.02227347
Iteration 88, loss = 0.02194725
Iteration 89, loss = 0.02223992
Iteration 90, loss = 0.02199096
Iteration 91, loss = 0.02172400
Iteration 92, loss = 0.02216883
Iteration 93, loss = 0.02307420
Iteration 94, loss = 0.02309606
Iteration 95, loss = 0.02293933
Iteration 96, loss = 0.02408706
Iteration 97, loss = 0.02388880
Iteration 98, loss = 0.02346458
Iteration 99, loss = 0.02299377
Iteration 100, loss = 0.02283054
Iteration 101, loss = 0.02267145
Iteration 102, loss = 0.02275665
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67643215
Iteration 2, loss = 0.66262960
Iteration 3, loss = 0.65127297
Iteration 4, loss = 0.64037228
Iteration 5, loss = 0.62887953
Iteration 6, loss = 0.61625718
Iteration 7, loss = 0.60252089
Iteration 8, loss = 0.58734444
Iteration 9, loss = 0.57044569
Iteration 10, loss = 0.55245507
Iteration 11, loss = 0.53314176
Iteration 12, loss = 0.51270637
Iteration 13, loss = 0.49119947
Iteration 14, loss = 0.46924242
Iteration 15, loss = 0.44691636
Iteration 16, loss = 0.42458942
Iteration 17, loss = 0.40225281
Iteration 18, loss = 0.38051666
Iteration 19, loss = 0.35931064
Iteration 20, loss = 0.33873855
Iteration 21, loss = 0.31894515
Iteration 22, loss = 0.30007495
Iteration 23, loss = 0.28211072
Iteration 24, loss = 0.26518893
Iteration 25, loss = 0.24915714
Iteration 26, loss = 0.23412826
Iteration 27, loss = 0.22000410
Iteration 28, loss = 0.20691329
Iteration 29, loss = 0.19464108
Iteration 30, loss = 0.18325201
Iteration 31, loss = 0.17262390
Iteration 32, loss = 0.16272594
Iteration 33, loss = 0.15363764
Iteration 34, loss = 0.14505654
Iteration 35, loss = 0.13718212
Iteration 36, loss = 0.12983800
Iteration 37, loss = 0.12303215
Iteration 38, loss = 0.11670555
Iteration 39, loss = 0.11086273
Iteration 40, loss = 0.10540264
Iteration 41, loss = 0.10031076
Iteration 42, loss = 0.09561293
Iteration 43, loss = 0.09121226
Iteration 44, loss = 0.08709259
Iteration 45, loss = 0.08325608
Iteration 46, loss = 0.07968371
Iteration 47, loss = 0.07637468
Iteration 48, loss = 0.07320538
Iteration 49, loss = 0.07032040
Iteration 50, loss = 0.06756338
Iteration 51, loss = 0.06494692
Iteration 52, loss = 0.06253955
Iteration 53, loss = 0.06028137
Iteration 54, loss = 0.05811088
Iteration 55, loss = 0.05608968
Iteration 56, loss = 0.05421136
Iteration 57, loss = 0.05244608
Iteration 58, loss = 0.05073474
Iteration 59, loss = 0.04913466
Iteration 60, loss = 0.04760794
Iteration 61, loss = 0.04615252
Iteration 62, loss = 0.04480428
Iteration 63, loss = 0.04345923
Iteration 64, loss = 0.04228760
Iteration 65, loss = 0.04106137
Iteration 66, loss = 0.04000373
Iteration 67, loss = 0.03892062
Iteration 68, loss = 0.03788246
Iteration 69, loss = 0.03692482
Iteration 70, loss = 0.03606660
Iteration 71, loss = 0.03515024
Iteration 72, loss = 0.03428149
Iteration 73, loss = 0.03348510
Iteration 74, loss = 0.03270900
Iteration 75, loss = 0.03200132
Iteration 76, loss = 0.03126189
Iteration 77, loss = 0.03060142
Iteration 78, loss = 0.02998905
Iteration 79, loss = 0.02936302
Iteration 80, loss = 0.02878676
Iteration 81, loss = 0.02821963
Iteration 82, loss = 0.02763312
Iteration 83, loss = 0.02714041
Iteration 84, loss = 0.02661286
Iteration 85, loss = 0.02612806
Iteration 86, loss = 0.02568604
Iteration 87, loss = 0.02519426
Iteration 88, loss = 0.02478773
Iteration 89, loss = 0.02434707
Iteration 90, loss = 0.02393315
Iteration 91, loss = 0.02353660
Iteration 92, loss = 0.02317358
Iteration 93, loss = 0.02278811
Iteration 94, loss = 0.02240655
Iteration 95, loss = 0.02206485
Iteration 96, loss = 0.02174349
Iteration 97, loss = 0.02143227
Iteration 98, loss = 0.02111478
Iteration 99, loss = 0.02081352
Iteration 100, loss = 0.02052366
Iteration 101, loss = 0.02023553
Iteration 102, loss = 0.01996070
Iteration 103, loss = 0.01970494
Iteration 104, loss = 0.01943065
Iteration 105, loss = 0.01918113
Iteration 106, loss = 0.01896616
Iteration 107, loss = 0.01870418
Iteration 108, loss = 0.01847164
Iteration 109, loss = 0.01826116
Iteration 110, loss = 0.01804735
Iteration 111, loss = 0.01783326
Iteration 112, loss = 0.01763742
Iteration 113, loss = 0.01743791
Iteration 114, loss = 0.01727485
Iteration 115, loss = 0.01709950
Iteration 116, loss = 0.01689451
Iteration 117, loss = 0.01669240
Iteration 118, loss = 0.01652502
Iteration 119, loss = 0.01634908
Iteration 120, loss = 0.01620145
Iteration 121, loss = 0.01603484
Iteration 122, loss = 0.01585895
Iteration 123, loss = 0.01569641
Iteration 124, loss = 0.01556176
Iteration 125, loss = 0.01539944
Iteration 126, loss = 0.01522465
Iteration 127, loss = 0.01512915
Iteration 128, loss = 0.01497988
Iteration 129, loss = 0.01486386
Iteration 130, loss = 0.01473702
Iteration 131, loss = 0.01460996
Iteration 132, loss = 0.01452228
Iteration 133, loss = 0.01441149
Iteration 134, loss = 0.01429303
Iteration 135, loss = 0.01417840
Iteration 136, loss = 0.01404272
Iteration 137, loss = 0.01393533
Iteration 138, loss = 0.01381584
Iteration 139, loss = 0.01376687
Iteration 140, loss = 0.01361042
Iteration 141, loss = 0.01352635
Iteration 142, loss = 0.01343655
Iteration 143, loss = 0.01334331
Iteration 144, loss = 0.01324172
Iteration 145, loss = 0.01314841
Iteration 146, loss = 0.01307177
Iteration 147, loss = 0.01295588
Iteration 148, loss = 0.01287765
Iteration 149, loss = 0.01282997
Iteration 150, loss = 0.01276990
Iteration 151, loss = 0.01264323
Iteration 152, loss = 0.01255273
Iteration 153, loss = 0.01248943
Iteration 154, loss = 0.01240018
Iteration 155, loss = 0.01233656
Iteration 156, loss = 0.01227654
Iteration 157, loss = 0.01220368
Iteration 158, loss = 0.01213204
Iteration 159, loss = 0.01203374
Iteration 160, loss = 0.01197932
Iteration 161, loss = 0.01191020
Iteration 162, loss = 0.01189552
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67212955
Iteration 2, loss = 0.59480368
Iteration 3, loss = 0.48971452
Iteration 4, loss = 0.38091367
Iteration 5, loss = 0.28754139
Iteration 6, loss = 0.21673537
Iteration 7, loss = 0.16647964
Iteration 8, loss = 0.13112189
Iteration 9, loss = 0.10636627
Iteration 10, loss = 0.08866255
Iteration 11, loss = 0.07545135
Iteration 12, loss = 0.06598055
Iteration 13, loss = 0.05809692
Iteration 14, loss = 0.05209911
Iteration 15, loss = 0.04736311
Iteration 16, loss = 0.04387321
Iteration 17, loss = 0.04035486
Iteration 18, loss = 0.03780173
Iteration 19, loss = 0.03548537
Iteration 20, loss = 0.03388323
Iteration 21, loss = 0.03224947
Iteration 22, loss = 0.03089617
Iteration 23, loss = 0.02977500
Iteration 24, loss = 0.02859832
Iteration 25, loss = 0.02784787
Iteration 26, loss = 0.02696298
Iteration 27, loss = 0.02609374
Iteration 28, loss = 0.02542657
Iteration 29, loss = 0.02488371
Iteration 30, loss = 0.02443587
Iteration 31, loss = 0.02390825
Iteration 32, loss = 0.02365814
Iteration 33, loss = 0.02331579
Iteration 34, loss = 0.02283592
Iteration 35, loss = 0.02237208
Iteration 36, loss = 0.02227282
Iteration 37, loss = 0.02201896
Iteration 38, loss = 0.02179503
Iteration 39, loss = 0.02160198
Iteration 40, loss = 0.02150261
Iteration 41, loss = 0.02123021
Iteration 42, loss = 0.02108518
Iteration 43, loss = 0.02077753
Iteration 44, loss = 0.02055307
Iteration 45, loss = 0.02070669
Iteration 46, loss = 0.02082165
Iteration 47, loss = 0.02052482
Iteration 48, loss = 0.02017878
Iteration 49, loss = 0.01995817
Iteration 50, loss = 0.02016380
Iteration 51, loss = 0.02009132
Iteration 52, loss = 0.01980026
Iteration 53, loss = 0.01971334
Iteration 54, loss = 0.01976875
Iteration 55, loss = 0.01932080
Iteration 56, loss = 0.01956779
Iteration 57, loss = 0.01938760
Iteration 58, loss = 0.01926787
Iteration 59, loss = 0.01942070
Iteration 60, loss = 0.01950688
Iteration 61, loss = 0.01925848
Iteration 62, loss = 0.01927229
Iteration 63, loss = 0.01914863
Iteration 64, loss = 0.01923880
Iteration 65, loss = 0.01905714
Iteration 66, loss = 0.01920616
Iteration 67, loss = 0.01918073
Iteration 68, loss = 0.01897546
Iteration 69, loss = 0.01853459
Iteration 70, loss = 0.01891840
Iteration 71, loss = 0.01907331
Iteration 72, loss = 0.01886799
Iteration 73, loss = 0.01889446
Iteration 74, loss = 0.01867757
Iteration 75, loss = 0.01871891
Iteration 76, loss = 0.01916213
Iteration 77, loss = 0.01865421
Iteration 78, loss = 0.01902239
Iteration 79, loss = 0.01871458
Iteration 80, loss = 0.01879893
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.74336418
Iteration 2, loss = 0.72450655
Iteration 3, loss = 0.70900044
Iteration 4, loss = 0.69325062
Iteration 5, loss = 0.67723012
Iteration 6, loss = 0.66059220
Iteration 7, loss = 0.64254759
Iteration 8, loss = 0.62405345
Iteration 9, loss = 0.60406045
Iteration 10, loss = 0.58294111
Iteration 11, loss = 0.56088854
Iteration 12, loss = 0.53845533
Iteration 13, loss = 0.51510760
Iteration 14, loss = 0.49145405
Iteration 15, loss = 0.46762372
Iteration 16, loss = 0.44379345
Iteration 17, loss = 0.42020531
Iteration 18, loss = 0.39695126
Iteration 19, loss = 0.37428422
Iteration 20, loss = 0.35227896
Iteration 21, loss = 0.33102293
Iteration 22, loss = 0.31072431
Iteration 23, loss = 0.29145628
Iteration 24, loss = 0.27319268
Iteration 25, loss = 0.25596522
Iteration 26, loss = 0.23975773
Iteration 27, loss = 0.22467567
Iteration 28, loss = 0.21052605
Iteration 29, loss = 0.19733108
Iteration 30, loss = 0.18514511
Iteration 31, loss = 0.17379246
Iteration 32, loss = 0.16327642
Iteration 33, loss = 0.15350129
Iteration 34, loss = 0.14452368
Iteration 35, loss = 0.13614262
Iteration 36, loss = 0.12840607
Iteration 37, loss = 0.12122085
Iteration 38, loss = 0.11456002
Iteration 39, loss = 0.10844626
Iteration 40, loss = 0.10274973
Iteration 41, loss = 0.09744755
Iteration 42, loss = 0.09256106
Iteration 43, loss = 0.08800020
Iteration 44, loss = 0.08375849
Iteration 45, loss = 0.07982727
Iteration 46, loss = 0.07614085
Iteration 47, loss = 0.07276122
Iteration 48, loss = 0.06957618
Iteration 49, loss = 0.06656128
Iteration 50, loss = 0.06377571
Iteration 51, loss = 0.06116980
Iteration 52, loss = 0.05873296
Iteration 53, loss = 0.05642752
Iteration 54, loss = 0.05425356
Iteration 55, loss = 0.05224180
Iteration 56, loss = 0.05033581
Iteration 57, loss = 0.04851946
Iteration 58, loss = 0.04680193
Iteration 59, loss = 0.04519123
Iteration 60, loss = 0.04366737
Iteration 61, loss = 0.04224723
Iteration 62, loss = 0.04087494
Iteration 63, loss = 0.03959909
Iteration 64, loss = 0.03836519
Iteration 65, loss = 0.03723248
Iteration 66, loss = 0.03611616
Iteration 67, loss = 0.03507972
Iteration 68, loss = 0.03407455
Iteration 69, loss = 0.03314220
Iteration 70, loss = 0.03220719
Iteration 71, loss = 0.03138594
Iteration 72, loss = 0.03054267
Iteration 73, loss = 0.02973250
Iteration 74, loss = 0.02899093
Iteration 75, loss = 0.02824936
Iteration 76, loss = 0.02757589
Iteration 77, loss = 0.02690231
Iteration 78, loss = 0.02625757
Iteration 79, loss = 0.02563957
Iteration 80, loss = 0.02506357
Iteration 81, loss = 0.02448864
Iteration 82, loss = 0.02394746
Iteration 83, loss = 0.02342982
Iteration 84, loss = 0.02292314
Iteration 85, loss = 0.02244363
Iteration 86, loss = 0.02197284
Iteration 87, loss = 0.02152377
Iteration 88, loss = 0.02111484
Iteration 89, loss = 0.02067986
Iteration 90, loss = 0.02028567
Iteration 91, loss = 0.01990811
Iteration 92, loss = 0.01953495
Iteration 93, loss = 0.01917030
Iteration 94, loss = 0.01882730
Iteration 95, loss = 0.01849148
Iteration 96, loss = 0.01817602
Iteration 97, loss = 0.01786292
Iteration 98, loss = 0.01756985
Iteration 99, loss = 0.01726912
Iteration 100, loss = 0.01697976
Iteration 101, loss = 0.01672071
Iteration 102, loss = 0.01646968
Iteration 103, loss = 0.01618689
Iteration 104, loss = 0.01595579
Iteration 105, loss = 0.01569794
Iteration 106, loss = 0.01547922
Iteration 107, loss = 0.01524634
Iteration 108, loss = 0.01502261
Iteration 109, loss = 0.01483828
Iteration 110, loss = 0.01462636
Iteration 111, loss = 0.01440684
Iteration 112, loss = 0.01420330
Iteration 113, loss = 0.01402002
Iteration 114, loss = 0.01387181
Iteration 115, loss = 0.01369458
Iteration 116, loss = 0.01352582
Iteration 117, loss = 0.01335872
Iteration 118, loss = 0.01316306
Iteration 119, loss = 0.01302248
Iteration 120, loss = 0.01284153
Iteration 121, loss = 0.01268720
Iteration 122, loss = 0.01254830
Iteration 123, loss = 0.01238911
Iteration 124, loss = 0.01224690
Iteration 125, loss = 0.01210596
Iteration 126, loss = 0.01197678
Iteration 127, loss = 0.01183249
Iteration 128, loss = 0.01170795
Iteration 129, loss = 0.01158382
Iteration 130, loss = 0.01147599
Iteration 131, loss = 0.01135367
Iteration 132, loss = 0.01124452
Iteration 133, loss = 0.01111288
Iteration 134, loss = 0.01099421
Iteration 135, loss = 0.01091750
Iteration 136, loss = 0.01077244
Iteration 137, loss = 0.01067246
Iteration 138, loss = 0.01058798
Iteration 139, loss = 0.01050925
Iteration 140, loss = 0.01041865
Iteration 141, loss = 0.01032597
Iteration 142, loss = 0.01024827
Iteration 143, loss = 0.01017857
Iteration 144, loss = 0.01011265
Iteration 145, loss = 0.01002303
Iteration 146, loss = 0.00995307
Iteration 147, loss = 0.00986550
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69038500
Iteration 2, loss = 0.67753623
Iteration 3, loss = 0.66637896
Iteration 4, loss = 0.65489580
Iteration 5, loss = 0.64261572
Iteration 6, loss = 0.62907585
Iteration 7, loss = 0.61404170
Iteration 8, loss = 0.59747767
Iteration 9, loss = 0.57947051
Iteration 10, loss = 0.55997245
Iteration 11, loss = 0.53912409
Iteration 12, loss = 0.51714505
Iteration 13, loss = 0.49387534
Iteration 14, loss = 0.47031585
Iteration 15, loss = 0.44621534
Iteration 16, loss = 0.42199425
Iteration 17, loss = 0.39784963
Iteration 18, loss = 0.37426473
Iteration 19, loss = 0.35135378
Iteration 20, loss = 0.32926903
Iteration 21, loss = 0.30788244
Iteration 22, loss = 0.28756893
Iteration 23, loss = 0.26820257
Iteration 24, loss = 0.24965333
Iteration 25, loss = 0.23226832
Iteration 26, loss = 0.21603605
Iteration 27, loss = 0.20098593
Iteration 28, loss = 0.18704586
Iteration 29, loss = 0.17416666
Iteration 30, loss = 0.16245463
Iteration 31, loss = 0.15169663
Iteration 32, loss = 0.14184099
Iteration 33, loss = 0.13282487
Iteration 34, loss = 0.12449676
Iteration 35, loss = 0.11697416
Iteration 36, loss = 0.11005050
Iteration 37, loss = 0.10365374
Iteration 38, loss = 0.09781950
Iteration 39, loss = 0.09246054
Iteration 40, loss = 0.08748814
Iteration 41, loss = 0.08292364
Iteration 42, loss = 0.07878851
Iteration 43, loss = 0.07482220
Iteration 44, loss = 0.07123475
Iteration 45, loss = 0.06793679
Iteration 46, loss = 0.06479136
Iteration 47, loss = 0.06192127
Iteration 48, loss = 0.05928441
Iteration 49, loss = 0.05678701
Iteration 50, loss = 0.05447510
Iteration 51, loss = 0.05232801
Iteration 52, loss = 0.05025326
Iteration 53, loss = 0.04836945
Iteration 54, loss = 0.04654865
Iteration 55, loss = 0.04486695
Iteration 56, loss = 0.04323322
Iteration 57, loss = 0.04175965
Iteration 58, loss = 0.04036830
Iteration 59, loss = 0.03904261
Iteration 60, loss = 0.03777316
Iteration 61, loss = 0.03659513
Iteration 62, loss = 0.03545952
Iteration 63, loss = 0.03439574
Iteration 64, loss = 0.03337172
Iteration 65, loss = 0.03241691
Iteration 66, loss = 0.03150992
Iteration 67, loss = 0.03065119
Iteration 68, loss = 0.02980999
Iteration 69, loss = 0.02903086
Iteration 70, loss = 0.02826352
Iteration 71, loss = 0.02753270
Iteration 72, loss = 0.02686380
Iteration 73, loss = 0.02619405
Iteration 74, loss = 0.02558588
Iteration 75, loss = 0.02496390
Iteration 76, loss = 0.02437908
Iteration 77, loss = 0.02381745
Iteration 78, loss = 0.02331123
Iteration 79, loss = 0.02276968
Iteration 80, loss = 0.02226310
Iteration 81, loss = 0.02180078
Iteration 82, loss = 0.02133684
Iteration 83, loss = 0.02090130
Iteration 84, loss = 0.02047562
Iteration 85, loss = 0.02007202
Iteration 86, loss = 0.01970737
Iteration 87, loss = 0.01929764
Iteration 88, loss = 0.01896540
Iteration 89, loss = 0.01857493
Iteration 90, loss = 0.01828743
Iteration 91, loss = 0.01796113
Iteration 92, loss = 0.01763069
Iteration 93, loss = 0.01731077
Iteration 94, loss = 0.01703500
Iteration 95, loss = 0.01673064
Iteration 96, loss = 0.01643670
Iteration 97, loss = 0.01616659
Iteration 98, loss = 0.01589984
Iteration 99, loss = 0.01566015
Iteration 100, loss = 0.01541642
Iteration 101, loss = 0.01518347
Iteration 102, loss = 0.01495373
Iteration 103, loss = 0.01473087
Iteration 104, loss = 0.01450588
Iteration 105, loss = 0.01428577
Iteration 106, loss = 0.01409128
Iteration 107, loss = 0.01388815
Iteration 108, loss = 0.01369437
Iteration 109, loss = 0.01349714
Iteration 110, loss = 0.01331852
Iteration 111, loss = 0.01314967
Iteration 112, loss = 0.01299612
Iteration 113, loss = 0.01282650
Iteration 114, loss = 0.01265146
Iteration 115, loss = 0.01248723
Iteration 116, loss = 0.01232743
Iteration 117, loss = 0.01219937
Iteration 118, loss = 0.01203086
Iteration 119, loss = 0.01189924
Iteration 120, loss = 0.01175370
Iteration 121, loss = 0.01162180
Iteration 122, loss = 0.01149534
Iteration 123, loss = 0.01136269
Iteration 124, loss = 0.01124590
Iteration 125, loss = 0.01117111
Iteration 126, loss = 0.01101928
Iteration 127, loss = 0.01089540
Iteration 128, loss = 0.01078401
Iteration 129, loss = 0.01069968
Iteration 130, loss = 0.01056411
Iteration 131, loss = 0.01047796
Iteration 132, loss = 0.01035880
Iteration 133, loss = 0.01027033
Iteration 134, loss = 0.01017338
Iteration 135, loss = 0.01007649
Iteration 136, loss = 0.00998226
Iteration 137, loss = 0.00988055
Iteration 138, loss = 0.00983206
Iteration 139, loss = 0.00970542
Iteration 140, loss = 0.00961769
Iteration 141, loss = 0.00954561
Iteration 142, loss = 0.00946153
Iteration 143, loss = 0.00939341
Iteration 144, loss = 0.00931616
Iteration 145, loss = 0.00921624
Iteration 146, loss = 0.00914122
Iteration 147, loss = 0.00906526
Iteration 148, loss = 0.00899899
Iteration 149, loss = 0.00892103
Iteration 150, loss = 0.00885235
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67694688
Iteration 2, loss = 0.64427095
Iteration 3, loss = 0.60264633
Iteration 4, loss = 0.54938574
Iteration 5, loss = 0.48856083
Iteration 6, loss = 0.42704508
Iteration 7, loss = 0.36882413
Iteration 8, loss = 0.31522943
Iteration 9, loss = 0.26785466
Iteration 10, loss = 0.22728859
Iteration 11, loss = 0.19304320
Iteration 12, loss = 0.16459302
Iteration 13, loss = 0.14142958
Iteration 14, loss = 0.12260430
Iteration 15, loss = 0.10715083
Iteration 16, loss = 0.09444356
Iteration 17, loss = 0.08394595
Iteration 18, loss = 0.07527732
Iteration 19, loss = 0.06795555
Iteration 20, loss = 0.06183833
Iteration 21, loss = 0.05659578
Iteration 22, loss = 0.05217722
Iteration 23, loss = 0.04830825
Iteration 24, loss = 0.04495495
Iteration 25, loss = 0.04199899
Iteration 26, loss = 0.03943436
Iteration 27, loss = 0.03726041
Iteration 28, loss = 0.03528565
Iteration 29, loss = 0.03347476
Iteration 30, loss = 0.03191471
Iteration 31, loss = 0.03052034
Iteration 32, loss = 0.02923159
Iteration 33, loss = 0.02819698
Iteration 34, loss = 0.02705667
Iteration 35, loss = 0.02617499
Iteration 36, loss = 0.02521977
Iteration 37, loss = 0.02438204
Iteration 38, loss = 0.02374360
Iteration 39, loss = 0.02300417
Iteration 40, loss = 0.02241587
Iteration 41, loss = 0.02191442
Iteration 42, loss = 0.02125591
Iteration 43, loss = 0.02093890
Iteration 44, loss = 0.02049206
Iteration 45, loss = 0.02000034
Iteration 46, loss = 0.01967987
Iteration 47, loss = 0.01919068
Iteration 48, loss = 0.01906316
Iteration 49, loss = 0.01877838
Iteration 50, loss = 0.01832136
Iteration 51, loss = 0.01820580
Iteration 52, loss = 0.01775397
Iteration 53, loss = 0.01749082
Iteration 54, loss = 0.01735825
Iteration 55, loss = 0.01712165
Iteration 56, loss = 0.01684290
Iteration 57, loss = 0.01665138
Iteration 58, loss = 0.01651644
Iteration 59, loss = 0.01633996
Iteration 60, loss = 0.01630185
Iteration 61, loss = 0.01600093
Iteration 62, loss = 0.01592481
Iteration 63, loss = 0.01579947
Iteration 64, loss = 0.01562164
Iteration 65, loss = 0.01554110
Iteration 66, loss = 0.01539851
Iteration 67, loss = 0.01517650
Iteration 68, loss = 0.01504358
Iteration 69, loss = 0.01508065
Iteration 70, loss = 0.01496585
Iteration 71, loss = 0.01493504
Iteration 72, loss = 0.01482096
Iteration 73, loss = 0.01459952
Iteration 74, loss = 0.01471072
Iteration 75, loss = 0.01454693
Iteration 76, loss = 0.01438864
Iteration 77, loss = 0.01428674
Iteration 78, loss = 0.01429919
Iteration 79, loss = 0.01423194
Iteration 80, loss = 0.01415360
Iteration 81, loss = 0.01411028
Iteration 82, loss = 0.01414786
Iteration 83, loss = 0.01411598
Iteration 84, loss = 0.01420859
Iteration 85, loss = 0.01390340
Iteration 86, loss = 0.01399953
Iteration 87, loss = 0.01387123
Iteration 88, loss = 0.01373317
Iteration 89, loss = 0.01371724
Iteration 90, loss = 0.01368770
Iteration 91, loss = 0.01361671
Iteration 92, loss = 0.01357403
Iteration 93, loss = 0.01349517
Iteration 94, loss = 0.01359970
Iteration 95, loss = 0.01353969
Iteration 96, loss = 0.01334248
Iteration 97, loss = 0.01329096
Iteration 98, loss = 0.01335941
Iteration 99, loss = 0.01336586
Iteration 100, loss = 0.01332737
Iteration 101, loss = 0.01326101
Iteration 102, loss = 0.01326955
Iteration 103, loss = 0.01320168
Iteration 104, loss = 0.01317715
Iteration 105, loss = 0.01312098
Iteration 106, loss = 0.01331850
Iteration 107, loss = 0.01317658
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67909097
Iteration 2, loss = 0.63059157
Iteration 3, loss = 0.56392229
Iteration 4, loss = 0.48285799
Iteration 5, loss = 0.40090557
Iteration 6, loss = 0.32623893
Iteration 7, loss = 0.26297066
Iteration 8, loss = 0.21235550
Iteration 9, loss = 0.17312751
Iteration 10, loss = 0.14302610
Iteration 11, loss = 0.12031563
Iteration 12, loss = 0.10244715
Iteration 13, loss = 0.08863368
Iteration 14, loss = 0.07769950
Iteration 15, loss = 0.06886508
Iteration 16, loss = 0.06182078
Iteration 17, loss = 0.05625575
Iteration 18, loss = 0.05143620
Iteration 19, loss = 0.04723377
Iteration 20, loss = 0.04384944
Iteration 21, loss = 0.04076944
Iteration 22, loss = 0.03856204
Iteration 23, loss = 0.03645988
Iteration 24, loss = 0.03456749
Iteration 25, loss = 0.03300419
Iteration 26, loss = 0.03162935
Iteration 27, loss = 0.03012549
Iteration 28, loss = 0.02901380
Iteration 29, loss = 0.02803314
Iteration 30, loss = 0.02711933
Iteration 31, loss = 0.02626770
Iteration 32, loss = 0.02570919
Iteration 33, loss = 0.02487864
Iteration 34, loss = 0.02432240
Iteration 35, loss = 0.02379699
Iteration 36, loss = 0.02318282
Iteration 37, loss = 0.02272526
Iteration 38, loss = 0.02232694
Iteration 39, loss = 0.02212571
Iteration 40, loss = 0.02141513
Iteration 41, loss = 0.02117388
Iteration 42, loss = 0.02088655
Iteration 43, loss = 0.02057331
Iteration 44, loss = 0.02024025
Iteration 45, loss = 0.02006338
Iteration 46, loss = 0.01995274
Iteration 47, loss = 0.01979806
Iteration 48, loss = 0.01926880
Iteration 49, loss = 0.01927309
Iteration 50, loss = 0.01910151
Iteration 51, loss = 0.01899416
Iteration 52, loss = 0.01868698
Iteration 53, loss = 0.01868818
Iteration 54, loss = 0.01845993
Iteration 55, loss = 0.01845491
Iteration 56, loss = 0.01827226
Iteration 57, loss = 0.01810181
Iteration 58, loss = 0.01797092
Iteration 59, loss = 0.01783137
Iteration 60, loss = 0.01791158
Iteration 61, loss = 0.01783755
Iteration 62, loss = 0.01765278
Iteration 63, loss = 0.01749504
Iteration 64, loss = 0.01739895
Iteration 65, loss = 0.01745878
Iteration 66, loss = 0.01741981
Iteration 67, loss = 0.01715742
Iteration 68, loss = 0.01720795
Iteration 69, loss = 0.01716719
Iteration 70, loss = 0.01705827
Iteration 71, loss = 0.01702504
Iteration 72, loss = 0.01695896
Iteration 73, loss = 0.01708202
Iteration 74, loss = 0.01687416
Iteration 75, loss = 0.01673742
Iteration 76, loss = 0.01685389
Iteration 77, loss = 0.01655742
Iteration 78, loss = 0.01660830
Iteration 79, loss = 0.01661294
Iteration 80, loss = 0.01654430
Iteration 81, loss = 0.01651445
Iteration 82, loss = 0.01653615
Iteration 83, loss = 0.01637697
Iteration 84, loss = 0.01645372
Iteration 85, loss = 0.01658885
Iteration 86, loss = 0.01629409
Iteration 87, loss = 0.01645166
Iteration 88, loss = 0.01634444
Iteration 89, loss = 0.01630026
Iteration 90, loss = 0.01632790
Iteration 91, loss = 0.01650201
Iteration 92, loss = 0.01624883
Iteration 93, loss = 0.01611632
Iteration 94, loss = 0.01614655
Iteration 95, loss = 0.01606230
Iteration 96, loss = 0.01613700
Iteration 97, loss = 0.01594767
Iteration 98, loss = 0.01616676
Iteration 99, loss = 0.01620661
Iteration 100, loss = 0.01594395
Iteration 101, loss = 0.01604056
Iteration 102, loss = 0.01608087
Iteration 103, loss = 0.01592833
Iteration 104, loss = 0.01599688
Iteration 105, loss = 0.01591991
Iteration 106, loss = 0.01585544
Iteration 107, loss = 0.01585361
Iteration 108, loss = 0.01600008
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67649181
Iteration 2, loss = 0.61033202
Iteration 3, loss = 0.51491614
Iteration 4, loss = 0.41811662
Iteration 5, loss = 0.33180938
Iteration 6, loss = 0.25964058
Iteration 7, loss = 0.20317534
Iteration 8, loss = 0.16178902
Iteration 9, loss = 0.13268424
Iteration 10, loss = 0.11093269
Iteration 11, loss = 0.09449872
Iteration 12, loss = 0.08236499
Iteration 13, loss = 0.07274032
Iteration 14, loss = 0.06556487
Iteration 15, loss = 0.05966802
Iteration 16, loss = 0.05487340
Iteration 17, loss = 0.05090374
Iteration 18, loss = 0.04799935
Iteration 19, loss = 0.04507431
Iteration 20, loss = 0.04314650
Iteration 21, loss = 0.04093582
Iteration 22, loss = 0.03939255
Iteration 23, loss = 0.03773156
Iteration 24, loss = 0.03645165
Iteration 25, loss = 0.03559408
Iteration 26, loss = 0.03452643
Iteration 27, loss = 0.03347585
Iteration 28, loss = 0.03277342
Iteration 29, loss = 0.03219384
Iteration 30, loss = 0.03176900
Iteration 31, loss = 0.03086265
Iteration 32, loss = 0.03041126
Iteration 33, loss = 0.03005072
Iteration 34, loss = 0.02953573
Iteration 35, loss = 0.02897578
Iteration 36, loss = 0.02891830
Iteration 37, loss = 0.02863206
Iteration 38, loss = 0.02854435
Iteration 39, loss = 0.02789804
Iteration 40, loss = 0.02789001
Iteration 41, loss = 0.02777636
Iteration 42, loss = 0.02770698
Iteration 43, loss = 0.02718891
Iteration 44, loss = 0.02748204
Iteration 45, loss = 0.02782955
Iteration 46, loss = 0.02741732
Iteration 47, loss = 0.02714064
Iteration 48, loss = 0.02690009
Iteration 49, loss = 0.02665305
Iteration 50, loss = 0.02652534
Iteration 51, loss = 0.02619329
Iteration 52, loss = 0.02605583
Iteration 53, loss = 0.02565643
Iteration 54, loss = 0.02560299
Iteration 55, loss = 0.02563017
Iteration 56, loss = 0.02548795
Iteration 57, loss = 0.02607514
Iteration 58, loss = 0.02585535
Iteration 59, loss = 0.02555787
Iteration 60, loss = 0.02617539
Iteration 61, loss = 0.02585188
Iteration 62, loss = 0.02563917
Iteration 63, loss = 0.02563503
Iteration 64, loss = 0.02569329
Iteration 65, loss = 0.02532952
Iteration 66, loss = 0.02516108
Iteration 67, loss = 0.02497908
Iteration 68, loss = 0.02495768
Iteration 69, loss = 0.02504109
Iteration 70, loss = 0.02486905
Iteration 71, loss = 0.02477287
Iteration 72, loss = 0.02484299
Iteration 73, loss = 0.02479455
Iteration 74, loss = 0.02649537
Iteration 75, loss = 0.02755850
Iteration 76, loss = 0.02671097
Iteration 77, loss = 0.02620870
Iteration 78, loss = 0.02579749
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68572466
Iteration 2, loss = 0.59274804
Iteration 3, loss = 0.47707774
Iteration 4, loss = 0.36833000
Iteration 5, loss = 0.27733787
Iteration 6, loss = 0.20845188
Iteration 7, loss = 0.15972334
Iteration 8, loss = 0.12578101
Iteration 9, loss = 0.10227119
Iteration 10, loss = 0.08548586
Iteration 11, loss = 0.07294343
Iteration 12, loss = 0.06366730
Iteration 13, loss = 0.05660456
Iteration 14, loss = 0.05093816
Iteration 15, loss = 0.04656352
Iteration 16, loss = 0.04295570
Iteration 17, loss = 0.03998222
Iteration 18, loss = 0.03787358
Iteration 19, loss = 0.03582911
Iteration 20, loss = 0.03416926
Iteration 21, loss = 0.03290887
Iteration 22, loss = 0.03141934
Iteration 23, loss = 0.03047513
Iteration 24, loss = 0.02936533
Iteration 25, loss = 0.02846749
Iteration 26, loss = 0.02803227
Iteration 27, loss = 0.02699492
Iteration 28, loss = 0.02650374
Iteration 29, loss = 0.02620280
Iteration 30, loss = 0.02565764
Iteration 31, loss = 0.02537894
Iteration 32, loss = 0.02471846
Iteration 33, loss = 0.02465908
Iteration 34, loss = 0.02429483
Iteration 35, loss = 0.02409191
Iteration 36, loss = 0.02389492
Iteration 37, loss = 0.02350821
Iteration 38, loss = 0.02335819
Iteration 39, loss = 0.02306135
Iteration 40, loss = 0.02281562
Iteration 41, loss = 0.02296246
Iteration 42, loss = 0.02255564
Iteration 43, loss = 0.02264173
Iteration 44, loss = 0.02248389
Iteration 45, loss = 0.02236737
Iteration 46, loss = 0.02202753
Iteration 47, loss = 0.02215743
Iteration 48, loss = 0.02206795
Iteration 49, loss = 0.02180558
Iteration 50, loss = 0.02208112
Iteration 51, loss = 0.02172681
Iteration 52, loss = 0.02154842
Iteration 53, loss = 0.02170720
Iteration 54, loss = 0.02152193
Iteration 55, loss = 0.02128033
Iteration 56, loss = 0.02141446
Iteration 57, loss = 0.02130145
Iteration 58, loss = 0.02105858
Iteration 59, loss = 0.02100261
Iteration 60, loss = 0.02115243
Iteration 61, loss = 0.02122309
Iteration 62, loss = 0.02107576
Iteration 63, loss = 0.02098307
Iteration 64, loss = 0.02127329
Iteration 65, loss = 0.02121975
Iteration 66, loss = 0.02099313
Iteration 67, loss = 0.02098269
Iteration 68, loss = 0.02105312
Iteration 69, loss = 0.02089842
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67494067
Iteration 2, loss = 0.60695790
Iteration 3, loss = 0.50050560
Iteration 4, loss = 0.38913542
Iteration 5, loss = 0.29508524
Iteration 6, loss = 0.22336773
Iteration 7, loss = 0.17229062
Iteration 8, loss = 0.13641442
Iteration 9, loss = 0.11114092
Iteration 10, loss = 0.09308717
Iteration 11, loss = 0.07988858
Iteration 12, loss = 0.07007752
Iteration 13, loss = 0.06234841
Iteration 14, loss = 0.05638398
Iteration 15, loss = 0.05146912
Iteration 16, loss = 0.04768640
Iteration 17, loss = 0.04445298
Iteration 18, loss = 0.04162405
Iteration 19, loss = 0.03944594
Iteration 20, loss = 0.03764950
Iteration 21, loss = 0.03578067
Iteration 22, loss = 0.03453274
Iteration 23, loss = 0.03340056
Iteration 24, loss = 0.03237722
Iteration 25, loss = 0.03154206
Iteration 26, loss = 0.03061454
Iteration 27, loss = 0.02989943
Iteration 28, loss = 0.02937790
Iteration 29, loss = 0.02854122
Iteration 30, loss = 0.02819369
Iteration 31, loss = 0.02750225
Iteration 32, loss = 0.02722569
Iteration 33, loss = 0.02667376
Iteration 34, loss = 0.02640685
Iteration 35, loss = 0.02615637
Iteration 36, loss = 0.02595342
Iteration 37, loss = 0.02552278
Iteration 38, loss = 0.02552686
Iteration 39, loss = 0.02516582
Iteration 40, loss = 0.02510720
Iteration 41, loss = 0.02513584
Iteration 42, loss = 0.02513986
Iteration 43, loss = 0.02439022
Iteration 44, loss = 0.02417416
Iteration 45, loss = 0.02451872
Iteration 46, loss = 0.02391271
Iteration 47, loss = 0.02378125
Iteration 48, loss = 0.02380683
Iteration 49, loss = 0.02378967
Iteration 50, loss = 0.02343197
Iteration 51, loss = 0.02330307
Iteration 52, loss = 0.02344443
Iteration 53, loss = 0.02339507
Iteration 54, loss = 0.02341726
Iteration 55, loss = 0.02306517
Iteration 56, loss = 0.02313620
Iteration 57, loss = 0.02300824
Iteration 58, loss = 0.02267790
Iteration 59, loss = 0.02288885
Iteration 60, loss = 0.02295304
Iteration 61, loss = 0.02256287
Iteration 62, loss = 0.02258446
Iteration 63, loss = 0.02243693
Iteration 64, loss = 0.02247957
Iteration 65, loss = 0.02230064
Iteration 66, loss = 0.02234405
Iteration 67, loss = 0.02249208
Iteration 68, loss = 0.02240741
Iteration 69, loss = 0.02240246
Iteration 70, loss = 0.02215232
Iteration 71, loss = 0.02230470
Iteration 72, loss = 0.02239646
Iteration 73, loss = 0.02230858
Iteration 74, loss = 0.02231794
Iteration 75, loss = 0.02213802
Iteration 76, loss = 0.02207401
Iteration 77, loss = 0.02211326
Iteration 78, loss = 0.02188083
Iteration 79, loss = 0.02185223
Iteration 80, loss = 0.02216832
Iteration 81, loss = 0.02185736
Iteration 82, loss = 0.02211208
Iteration 83, loss = 0.02201282
Iteration 84, loss = 0.02191486
Iteration 85, loss = 0.02189434
Iteration 86, loss = 0.02227744
Iteration 87, loss = 0.02152392
Iteration 88, loss = 0.02173946
Iteration 89, loss = 0.02235283
Iteration 90, loss = 0.02220314
Iteration 91, loss = 0.02161723
Iteration 92, loss = 0.02164791
Iteration 93, loss = 0.02172945
Iteration 94, loss = 0.02161571
Iteration 95, loss = 0.02173661
Iteration 96, loss = 0.02170531
Iteration 97, loss = 0.02148858
Iteration 98, loss = 0.02148572
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70003708
Iteration 2, loss = 0.67153270
Iteration 3, loss = 0.63114372
Iteration 4, loss = 0.57796412
Iteration 5, loss = 0.51704965
Iteration 6, loss = 0.45492408
Iteration 7, loss = 0.39557518
Iteration 8, loss = 0.34078030
Iteration 9, loss = 0.29159359
Iteration 10, loss = 0.24890091
Iteration 11, loss = 0.21257374
Iteration 12, loss = 0.18207459
Iteration 13, loss = 0.15697013
Iteration 14, loss = 0.13590479
Iteration 15, loss = 0.11884611
Iteration 16, loss = 0.10479387
Iteration 17, loss = 0.09306890
Iteration 18, loss = 0.08337330
Iteration 19, loss = 0.07509645
Iteration 20, loss = 0.06830432
Iteration 21, loss = 0.06247225
Iteration 22, loss = 0.05759061
Iteration 23, loss = 0.05335100
Iteration 24, loss = 0.04957994
Iteration 25, loss = 0.04638065
Iteration 26, loss = 0.04373315
Iteration 27, loss = 0.04117378
Iteration 28, loss = 0.03896970
Iteration 29, loss = 0.03714639
Iteration 30, loss = 0.03536621
Iteration 31, loss = 0.03388534
Iteration 32, loss = 0.03244346
Iteration 33, loss = 0.03120097
Iteration 34, loss = 0.03012892
Iteration 35, loss = 0.02899487
Iteration 36, loss = 0.02808250
Iteration 37, loss = 0.02726688
Iteration 38, loss = 0.02651411
Iteration 39, loss = 0.02587537
Iteration 40, loss = 0.02513121
Iteration 41, loss = 0.02460459
Iteration 42, loss = 0.02393851
Iteration 43, loss = 0.02347666
Iteration 44, loss = 0.02299359
Iteration 45, loss = 0.02247953
Iteration 46, loss = 0.02233368
Iteration 47, loss = 0.02179163
Iteration 48, loss = 0.02152976
Iteration 49, loss = 0.02119345
Iteration 50, loss = 0.02078733
Iteration 51, loss = 0.02034538
Iteration 52, loss = 0.02016104
Iteration 53, loss = 0.01992161
Iteration 54, loss = 0.01970807
Iteration 55, loss = 0.01951188
Iteration 56, loss = 0.01929311
Iteration 57, loss = 0.01901810
Iteration 58, loss = 0.01877348
Iteration 59, loss = 0.01852623
Iteration 60, loss = 0.01851649
Iteration 61, loss = 0.01826228
Iteration 62, loss = 0.01811902
Iteration 63, loss = 0.01796185
Iteration 64, loss = 0.01780892
Iteration 65, loss = 0.01764741
Iteration 66, loss = 0.01773176
Iteration 67, loss = 0.01741755
Iteration 68, loss = 0.01731166
Iteration 69, loss = 0.01721280
Iteration 70, loss = 0.01721845
Iteration 71, loss = 0.01701988
Iteration 72, loss = 0.01696173
Iteration 73, loss = 0.01691367
Iteration 74, loss = 0.01675590
Iteration 75, loss = 0.01671195
Iteration 76, loss = 0.01666689
Iteration 77, loss = 0.01663502
Iteration 78, loss = 0.01646864
Iteration 79, loss = 0.01634227
Iteration 80, loss = 0.01630961
Iteration 81, loss = 0.01622059
Iteration 82, loss = 0.01614230
Iteration 83, loss = 0.01618450
Iteration 84, loss = 0.01618670
Iteration 85, loss = 0.01595099
Iteration 86, loss = 0.01596195
Iteration 87, loss = 0.01599118
Iteration 88, loss = 0.01594659
Iteration 89, loss = 0.01575570
Iteration 90, loss = 0.01590548
Iteration 91, loss = 0.01585928
Iteration 92, loss = 0.01565442
Iteration 93, loss = 0.01561539
Iteration 94, loss = 0.01564479
Iteration 95, loss = 0.01555307
Iteration 96, loss = 0.01554251
Iteration 97, loss = 0.01550100
Iteration 98, loss = 0.01539703
Iteration 99, loss = 0.01561200
Iteration 100, loss = 0.01539745
Iteration 101, loss = 0.01540262
Iteration 102, loss = 0.01543819
Iteration 103, loss = 0.01547807
Iteration 104, loss = 0.01520603
Iteration 105, loss = 0.01522149
Iteration 106, loss = 0.01512352
Iteration 107, loss = 0.01516031
Iteration 108, loss = 0.01505236
Iteration 109, loss = 0.01508915
Iteration 110, loss = 0.01516116
Iteration 111, loss = 0.01501562
Iteration 112, loss = 0.01506157
Iteration 113, loss = 0.01511477
Iteration 114, loss = 0.01503267
Iteration 115, loss = 0.01501423
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69998254
Iteration 2, loss = 0.67804124
Iteration 3, loss = 0.65672295
Iteration 4, loss = 0.63221444
Iteration 5, loss = 0.60442569
Iteration 6, loss = 0.57337487
Iteration 7, loss = 0.54018164
Iteration 8, loss = 0.50568011
Iteration 9, loss = 0.47046335
Iteration 10, loss = 0.43498736
Iteration 11, loss = 0.40016074
Iteration 12, loss = 0.36642551
Iteration 13, loss = 0.33431075
Iteration 14, loss = 0.30442733
Iteration 15, loss = 0.27661106
Iteration 16, loss = 0.25107829
Iteration 17, loss = 0.22789200
Iteration 18, loss = 0.20702679
Iteration 19, loss = 0.18825978
Iteration 20, loss = 0.17136880
Iteration 21, loss = 0.15654003
Iteration 22, loss = 0.14313129
Iteration 23, loss = 0.13127864
Iteration 24, loss = 0.12069771
Iteration 25, loss = 0.11135406
Iteration 26, loss = 0.10293373
Iteration 27, loss = 0.09554955
Iteration 28, loss = 0.08885007
Iteration 29, loss = 0.08296037
Iteration 30, loss = 0.07759757
Iteration 31, loss = 0.07285225
Iteration 32, loss = 0.06849843
Iteration 33, loss = 0.06457981
Iteration 34, loss = 0.06103149
Iteration 35, loss = 0.05784693
Iteration 36, loss = 0.05489215
Iteration 37, loss = 0.05224235
Iteration 38, loss = 0.04980895
Iteration 39, loss = 0.04757568
Iteration 40, loss = 0.04553631
Iteration 41, loss = 0.04369946
Iteration 42, loss = 0.04189938
Iteration 43, loss = 0.04029729
Iteration 44, loss = 0.03882624
Iteration 45, loss = 0.03741880
Iteration 46, loss = 0.03614582
Iteration 47, loss = 0.03495846
Iteration 48, loss = 0.03383405
Iteration 49, loss = 0.03278553
Iteration 50, loss = 0.03184338
Iteration 51, loss = 0.03092345
Iteration 52, loss = 0.03007181
Iteration 53, loss = 0.02933212
Iteration 54, loss = 0.02853564
Iteration 55, loss = 0.02782825
Iteration 56, loss = 0.02713890
Iteration 57, loss = 0.02656768
Iteration 58, loss = 0.02595138
Iteration 59, loss = 0.02538286
Iteration 60, loss = 0.02484214
Iteration 61, loss = 0.02435265
Iteration 62, loss = 0.02390038
Iteration 63, loss = 0.02348491
Iteration 64, loss = 0.02296580
Iteration 65, loss = 0.02256145
Iteration 66, loss = 0.02220948
Iteration 67, loss = 0.02180638
Iteration 68, loss = 0.02149302
Iteration 69, loss = 0.02114083
Iteration 70, loss = 0.02082280
Iteration 71, loss = 0.02049604
Iteration 72, loss = 0.02024231
Iteration 73, loss = 0.01993696
Iteration 74, loss = 0.01967697
Iteration 75, loss = 0.01941210
Iteration 76, loss = 0.01918669
Iteration 77, loss = 0.01894302
Iteration 78, loss = 0.01868487
Iteration 79, loss = 0.01851514
Iteration 80, loss = 0.01825845
Iteration 81, loss = 0.01806348
Iteration 82, loss = 0.01788404
Iteration 83, loss = 0.01772053
Iteration 84, loss = 0.01754046
Iteration 85, loss = 0.01735900
Iteration 86, loss = 0.01721382
Iteration 87, loss = 0.01701197
Iteration 88, loss = 0.01687716
Iteration 89, loss = 0.01673414
Iteration 90, loss = 0.01657165
Iteration 91, loss = 0.01644722
Iteration 92, loss = 0.01633282
Iteration 93, loss = 0.01622039
Iteration 94, loss = 0.01607571
Iteration 95, loss = 0.01595460
Iteration 96, loss = 0.01580352
Iteration 97, loss = 0.01573839
Iteration 98, loss = 0.01558575
Iteration 99, loss = 0.01548318
Iteration 100, loss = 0.01539036
Iteration 101, loss = 0.01529733
Iteration 102, loss = 0.01523035
Iteration 103, loss = 0.01515819
Iteration 104, loss = 0.01504665
Iteration 105, loss = 0.01492972
Iteration 106, loss = 0.01489230
Iteration 107, loss = 0.01477423
Iteration 108, loss = 0.01471383
Iteration 109, loss = 0.01464116
Iteration 110, loss = 0.01456104
Iteration 111, loss = 0.01446277
Iteration 112, loss = 0.01439567
Iteration 113, loss = 0.01431855
Iteration 114, loss = 0.01428978
Iteration 115, loss = 0.01420786
Iteration 116, loss = 0.01410324
Iteration 117, loss = 0.01408848
Iteration 118, loss = 0.01401155
Iteration 119, loss = 0.01396438
Iteration 120, loss = 0.01387208
Iteration 121, loss = 0.01383982
Iteration 122, loss = 0.01377274
Iteration 123, loss = 0.01371845
Iteration 124, loss = 0.01367752
Iteration 125, loss = 0.01364362
Iteration 126, loss = 0.01357472
Iteration 127, loss = 0.01356898
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67624530
Iteration 2, loss = 0.62384695
Iteration 3, loss = 0.54566572
Iteration 4, loss = 0.45117583
Iteration 5, loss = 0.35710994
Iteration 6, loss = 0.27742909
Iteration 7, loss = 0.21541743
Iteration 8, loss = 0.16966839
Iteration 9, loss = 0.13666055
Iteration 10, loss = 0.11264115
Iteration 11, loss = 0.09492105
Iteration 12, loss = 0.08258946
Iteration 13, loss = 0.07235025
Iteration 14, loss = 0.06437669
Iteration 15, loss = 0.05821685
Iteration 16, loss = 0.05309456
Iteration 17, loss = 0.04870694
Iteration 18, loss = 0.04538755
Iteration 19, loss = 0.04229302
Iteration 20, loss = 0.03995152
Iteration 21, loss = 0.03773149
Iteration 22, loss = 0.03602380
Iteration 23, loss = 0.03445062
Iteration 24, loss = 0.03310887
Iteration 25, loss = 0.03178613
Iteration 26, loss = 0.03086721
Iteration 27, loss = 0.03011403
Iteration 28, loss = 0.02925890
Iteration 29, loss = 0.02829651
Iteration 30, loss = 0.02765158
Iteration 31, loss = 0.02711887
Iteration 32, loss = 0.02649608
Iteration 33, loss = 0.02632798
Iteration 34, loss = 0.02558697
Iteration 35, loss = 0.02512488
Iteration 36, loss = 0.02495891
Iteration 37, loss = 0.02474067
Iteration 38, loss = 0.02446076
Iteration 39, loss = 0.02395354
Iteration 40, loss = 0.02366314
Iteration 41, loss = 0.02345095
Iteration 42, loss = 0.02340463
Iteration 43, loss = 0.02292534
Iteration 44, loss = 0.02284577
Iteration 45, loss = 0.02236238
Iteration 46, loss = 0.02231677
Iteration 47, loss = 0.02205541
Iteration 48, loss = 0.02210036
Iteration 49, loss = 0.02173669
Iteration 50, loss = 0.02180503
Iteration 51, loss = 0.02181208
Iteration 52, loss = 0.02157417
Iteration 53, loss = 0.02149980
Iteration 54, loss = 0.02142904
Iteration 55, loss = 0.02125161
Iteration 56, loss = 0.02118435
Iteration 57, loss = 0.02117880
Iteration 58, loss = 0.02122908
Iteration 59, loss = 0.02105064
Iteration 60, loss = 0.02080131
Iteration 61, loss = 0.02080039
Iteration 62, loss = 0.02053098
Iteration 63, loss = 0.02053444
Iteration 64, loss = 0.02042235
Iteration 65, loss = 0.02043313
Iteration 66, loss = 0.02021360
Iteration 67, loss = 0.02036612
Iteration 68, loss = 0.02028700
Iteration 69, loss = 0.02023963
Iteration 70, loss = 0.02066648
Iteration 71, loss = 0.02012357
Iteration 72, loss = 0.02129417
Iteration 73, loss = 0.02145143
Iteration 74, loss = 0.02059424
Iteration 75, loss = 0.02036372
Iteration 76, loss = 0.02091508
Iteration 77, loss = 0.02057980
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70213431
Iteration 2, loss = 0.67734760
Iteration 3, loss = 0.64977564
Iteration 4, loss = 0.61459295
Iteration 5, loss = 0.57114703
Iteration 6, loss = 0.52226309
Iteration 7, loss = 0.47185436
Iteration 8, loss = 0.42241889
Iteration 9, loss = 0.37565742
Iteration 10, loss = 0.33330407
Iteration 11, loss = 0.29519969
Iteration 12, loss = 0.26157645
Iteration 13, loss = 0.23201323
Iteration 14, loss = 0.20631048
Iteration 15, loss = 0.18399271
Iteration 16, loss = 0.16468107
Iteration 17, loss = 0.14794634
Iteration 18, loss = 0.13348914
Iteration 19, loss = 0.12099359
Iteration 20, loss = 0.11002597
Iteration 21, loss = 0.10072092
Iteration 22, loss = 0.09246942
Iteration 23, loss = 0.08527521
Iteration 24, loss = 0.07912229
Iteration 25, loss = 0.07356181
Iteration 26, loss = 0.06870444
Iteration 27, loss = 0.06443173
Iteration 28, loss = 0.06052603
Iteration 29, loss = 0.05698227
Iteration 30, loss = 0.05394012
Iteration 31, loss = 0.05112085
Iteration 32, loss = 0.04873223
Iteration 33, loss = 0.04635099
Iteration 34, loss = 0.04437340
Iteration 35, loss = 0.04244507
Iteration 36, loss = 0.04074130
Iteration 37, loss = 0.03912159
Iteration 38, loss = 0.03762537
Iteration 39, loss = 0.03627780
Iteration 40, loss = 0.03502531
Iteration 41, loss = 0.03387820
Iteration 42, loss = 0.03296410
Iteration 43, loss = 0.03193777
Iteration 44, loss = 0.03104880
Iteration 45, loss = 0.03017868
Iteration 46, loss = 0.02936439
Iteration 47, loss = 0.02863633
Iteration 48, loss = 0.02806899
Iteration 49, loss = 0.02749649
Iteration 50, loss = 0.02693827
Iteration 51, loss = 0.02627294
Iteration 52, loss = 0.02581403
Iteration 53, loss = 0.02550242
Iteration 54, loss = 0.02480314
Iteration 55, loss = 0.02459414
Iteration 56, loss = 0.02408216
Iteration 57, loss = 0.02364890
Iteration 58, loss = 0.02321360
Iteration 59, loss = 0.02288694
Iteration 60, loss = 0.02283779
Iteration 61, loss = 0.02239891
Iteration 62, loss = 0.02202157
Iteration 63, loss = 0.02168139
Iteration 64, loss = 0.02123612
Iteration 65, loss = 0.02152121
Iteration 66, loss = 0.02112435
Iteration 67, loss = 0.02095460
Iteration 68, loss = 0.02080329
Iteration 69, loss = 0.02052817
Iteration 70, loss = 0.02016974
Iteration 71, loss = 0.01994246
Iteration 72, loss = 0.01967210
Iteration 73, loss = 0.01954500
Iteration 74, loss = 0.01928411
Iteration 75, loss = 0.01947999
Iteration 76, loss = 0.01898480
Iteration 77, loss = 0.01897441
Iteration 78, loss = 0.01870265
Iteration 79, loss = 0.01851521
Iteration 80, loss = 0.01824068
Iteration 81, loss = 0.01813839
Iteration 82, loss = 0.01803550
Iteration 83, loss = 0.01794772
Iteration 84, loss = 0.01800420
Iteration 85, loss = 0.01788304
Iteration 86, loss = 0.01774266
Iteration 87, loss = 0.01753309
Iteration 88, loss = 0.01741038
Iteration 89, loss = 0.01724224
Iteration 90, loss = 0.01707957
Iteration 91, loss = 0.01708190
Iteration 92, loss = 0.01721102
Iteration 93, loss = 0.01723870
Iteration 94, loss = 0.01703313
Iteration 95, loss = 0.01695090
Iteration 96, loss = 0.01684588
Iteration 97, loss = 0.01653392
Iteration 98, loss = 0.01673405
Iteration 99, loss = 0.01673018
Iteration 100, loss = 0.01659313
Iteration 101, loss = 0.01647724
Iteration 102, loss = 0.01631628
Iteration 103, loss = 0.01631887
Iteration 104, loss = 0.01617047
Iteration 105, loss = 0.01615254
Iteration 106, loss = 0.01584757
Iteration 107, loss = 0.01618439
Iteration 108, loss = 0.01624206
Iteration 109, loss = 0.01625488
Iteration 110, loss = 0.01611624
Iteration 111, loss = 0.01591938
Iteration 112, loss = 0.01580071
Iteration 113, loss = 0.01604569
Iteration 114, loss = 0.01590529
Iteration 115, loss = 0.01591560
Iteration 116, loss = 0.01577169
Iteration 117, loss = 0.01570545
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71509822
Iteration 2, loss = 0.65370275
Iteration 3, loss = 0.57032782
Iteration 4, loss = 0.46594193
Iteration 5, loss = 0.36928565
Iteration 6, loss = 0.28698511
Iteration 7, loss = 0.22346856
Iteration 8, loss = 0.17642080
Iteration 9, loss = 0.14186290
Iteration 10, loss = 0.11695382
Iteration 11, loss = 0.09890900
Iteration 12, loss = 0.08508731
Iteration 13, loss = 0.07477909
Iteration 14, loss = 0.06667805
Iteration 15, loss = 0.06008253
Iteration 16, loss = 0.05525589
Iteration 17, loss = 0.05131288
Iteration 18, loss = 0.04779075
Iteration 19, loss = 0.04521606
Iteration 20, loss = 0.04258174
Iteration 21, loss = 0.04049227
Iteration 22, loss = 0.03857649
Iteration 23, loss = 0.03693808
Iteration 24, loss = 0.03545871
Iteration 25, loss = 0.03421726
Iteration 26, loss = 0.03294433
Iteration 27, loss = 0.03189736
Iteration 28, loss = 0.03176452
Iteration 29, loss = 0.03102332
Iteration 30, loss = 0.03011221
Iteration 31, loss = 0.02964834
Iteration 32, loss = 0.02875112
Iteration 33, loss = 0.02815314
Iteration 34, loss = 0.02775441
Iteration 35, loss = 0.02741874
Iteration 36, loss = 0.02709040
Iteration 37, loss = 0.02645823
Iteration 38, loss = 0.02628777
Iteration 39, loss = 0.02581366
Iteration 40, loss = 0.02561260
Iteration 41, loss = 0.02517750
Iteration 42, loss = 0.02520803
Iteration 43, loss = 0.02486058
Iteration 44, loss = 0.02447320
Iteration 45, loss = 0.02437357
Iteration 46, loss = 0.02405241
Iteration 47, loss = 0.02403151
Iteration 48, loss = 0.02376093
Iteration 49, loss = 0.02358416
Iteration 50, loss = 0.02358288
Iteration 51, loss = 0.02328579
Iteration 52, loss = 0.02311221
Iteration 53, loss = 0.02321958
Iteration 54, loss = 0.02464426
Iteration 55, loss = 0.02433944
Iteration 56, loss = 0.02401785
Iteration 57, loss = 0.02368518
Iteration 58, loss = 0.02394933
Iteration 59, loss = 0.02372483
Iteration 60, loss = 0.02352366
Iteration 61, loss = 0.02340311
Iteration 62, loss = 0.02327751
Iteration 63, loss = 0.02299843
Iteration 64, loss = 0.02267615
Iteration 65, loss = 0.02270881
Iteration 66, loss = 0.02241332
Iteration 67, loss = 0.02232142
Iteration 68, loss = 0.02212147
Iteration 69, loss = 0.02216341
Iteration 70, loss = 0.02190825
Iteration 71, loss = 0.02201111
Iteration 72, loss = 0.02168949
Iteration 73, loss = 0.02160584
Iteration 74, loss = 0.02142993
Iteration 75, loss = 0.02160805
Iteration 76, loss = 0.02120690
Iteration 77, loss = 0.02132622
Iteration 78, loss = 0.02133519
Iteration 79, loss = 0.02120747
Iteration 80, loss = 0.02105705
Iteration 81, loss = 0.02100601
Iteration 82, loss = 0.02104732
Iteration 83, loss = 0.02100552
Iteration 84, loss = 0.02115806
Iteration 85, loss = 0.02117447
Iteration 86, loss = 0.02082438
Iteration 87, loss = 0.02077775
Iteration 88, loss = 0.02078484
Iteration 89, loss = 0.02056981
Iteration 90, loss = 0.02067176
Iteration 91, loss = 0.02081342
Iteration 92, loss = 0.02086634
Iteration 93, loss = 0.02220111
Iteration 94, loss = 0.02287845
Iteration 95, loss = 0.02294802
Iteration 96, loss = 0.02297542
Iteration 97, loss = 0.02287407
Iteration 98, loss = 0.02254104
Iteration 99, loss = 0.02236330
Iteration 100, loss = 0.02231215
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68912154
Iteration 2, loss = 0.66285638
Iteration 3, loss = 0.62532983
Iteration 4, loss = 0.57204547
Iteration 5, loss = 0.50708756
Iteration 6, loss = 0.43882989
Iteration 7, loss = 0.37360586
Iteration 8, loss = 0.31485988
Iteration 9, loss = 0.26421268
Iteration 10, loss = 0.22178233
Iteration 11, loss = 0.18717027
Iteration 12, loss = 0.15918801
Iteration 13, loss = 0.13661858
Iteration 14, loss = 0.11844162
Iteration 15, loss = 0.10382146
Iteration 16, loss = 0.09187617
Iteration 17, loss = 0.08208048
Iteration 18, loss = 0.07409375
Iteration 19, loss = 0.06751523
Iteration 20, loss = 0.06189105
Iteration 21, loss = 0.05708743
Iteration 22, loss = 0.05291900
Iteration 23, loss = 0.04929991
Iteration 24, loss = 0.04651956
Iteration 25, loss = 0.04377593
Iteration 26, loss = 0.04145857
Iteration 27, loss = 0.03932770
Iteration 28, loss = 0.03745022
Iteration 29, loss = 0.03589758
Iteration 30, loss = 0.03454811
Iteration 31, loss = 0.03328102
Iteration 32, loss = 0.03200855
Iteration 33, loss = 0.03107927
Iteration 34, loss = 0.03003092
Iteration 35, loss = 0.02928212
Iteration 36, loss = 0.02845434
Iteration 37, loss = 0.02785842
Iteration 38, loss = 0.02722901
Iteration 39, loss = 0.02665003
Iteration 40, loss = 0.02593494
Iteration 41, loss = 0.02561791
Iteration 42, loss = 0.02505182
Iteration 43, loss = 0.02473783
Iteration 44, loss = 0.02416333
Iteration 45, loss = 0.02383640
Iteration 46, loss = 0.02358190
Iteration 47, loss = 0.02316562
Iteration 48, loss = 0.02304550
Iteration 49, loss = 0.02262060
Iteration 50, loss = 0.02228919
Iteration 51, loss = 0.02212309
Iteration 52, loss = 0.02196179
Iteration 53, loss = 0.02169263
Iteration 54, loss = 0.02143159
Iteration 55, loss = 0.02128886
Iteration 56, loss = 0.02105995
Iteration 57, loss = 0.02088856
Iteration 58, loss = 0.02073844
Iteration 59, loss = 0.02051079
Iteration 60, loss = 0.02042653
Iteration 61, loss = 0.02044806
Iteration 62, loss = 0.02028740
Iteration 63, loss = 0.02005661
Iteration 64, loss = 0.01996899
Iteration 65, loss = 0.01986749
Iteration 66, loss = 0.01989987
Iteration 67, loss = 0.01958482
Iteration 68, loss = 0.01950243
Iteration 69, loss = 0.01935845
Iteration 70, loss = 0.01929148
Iteration 71, loss = 0.01924611
Iteration 72, loss = 0.01935258
Iteration 73, loss = 0.01901790
Iteration 74, loss = 0.01895765
Iteration 75, loss = 0.01894191
Iteration 76, loss = 0.01901106
Iteration 77, loss = 0.01886583
Iteration 78, loss = 0.01875778
Iteration 79, loss = 0.01875093
Iteration 80, loss = 0.01858953
Iteration 81, loss = 0.01849930
Iteration 82, loss = 0.01842006
Iteration 83, loss = 0.01841742
Iteration 84, loss = 0.01853706
Iteration 85, loss = 0.01842951
Iteration 86, loss = 0.01827498
Iteration 87, loss = 0.01826489
Iteration 88, loss = 0.01830473
Iteration 89, loss = 0.01834926
Iteration 90, loss = 0.01817712
Iteration 91, loss = 0.01805014
Iteration 92, loss = 0.01802734
Iteration 93, loss = 0.01819885
Iteration 94, loss = 0.01803266
Iteration 95, loss = 0.01805515
Iteration 96, loss = 0.01805491
Iteration 97, loss = 0.01796184
Iteration 98, loss = 0.01802462
Iteration 99, loss = 0.01786561
Iteration 100, loss = 0.01796707
Iteration 101, loss = 0.01788043
Iteration 102, loss = 0.01789353
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69858165
Iteration 2, loss = 0.64005566
Iteration 3, loss = 0.56195896
Iteration 4, loss = 0.46597804
Iteration 5, loss = 0.37188639
Iteration 6, loss = 0.28997029
Iteration 7, loss = 0.22625353
Iteration 8, loss = 0.17831662
Iteration 9, loss = 0.14361917
Iteration 10, loss = 0.11789218
Iteration 11, loss = 0.09901877
Iteration 12, loss = 0.08484327
Iteration 13, loss = 0.07385971
Iteration 14, loss = 0.06525939
Iteration 15, loss = 0.05839085
Iteration 16, loss = 0.05272875
Iteration 17, loss = 0.04823152
Iteration 18, loss = 0.04462461
Iteration 19, loss = 0.04143884
Iteration 20, loss = 0.03888246
Iteration 21, loss = 0.03641137
Iteration 22, loss = 0.03475566
Iteration 23, loss = 0.03297470
Iteration 24, loss = 0.03163020
Iteration 25, loss = 0.03026465
Iteration 26, loss = 0.02902587
Iteration 27, loss = 0.02822394
Iteration 28, loss = 0.02714182
Iteration 29, loss = 0.02630199
Iteration 30, loss = 0.02601819
Iteration 31, loss = 0.02498166
Iteration 32, loss = 0.02455403
Iteration 33, loss = 0.02424644
Iteration 34, loss = 0.02358511
Iteration 35, loss = 0.02324292
Iteration 36, loss = 0.02296174
Iteration 37, loss = 0.02235065
Iteration 38, loss = 0.02224548
Iteration 39, loss = 0.02181989
Iteration 40, loss = 0.02168529
Iteration 41, loss = 0.02143814
Iteration 42, loss = 0.02100800
Iteration 43, loss = 0.02085146
Iteration 44, loss = 0.02069130
Iteration 45, loss = 0.02053466
Iteration 46, loss = 0.02032070
Iteration 47, loss = 0.02014879
Iteration 48, loss = 0.01995600
Iteration 49, loss = 0.01980688
Iteration 50, loss = 0.01973699
Iteration 51, loss = 0.01971574
Iteration 52, loss = 0.01949739
Iteration 53, loss = 0.01917366
Iteration 54, loss = 0.01954001
Iteration 55, loss = 0.01926966
Iteration 56, loss = 0.01902923
Iteration 57, loss = 0.01907364
Iteration 58, loss = 0.01894321
Iteration 59, loss = 0.01870184
Iteration 60, loss = 0.01865572
Iteration 61, loss = 0.01863783
Iteration 62, loss = 0.01870267
Iteration 63, loss = 0.01859644
Iteration 64, loss = 0.01862698
Iteration 65, loss = 0.01845662
Iteration 66, loss = 0.01837308
Iteration 67, loss = 0.01847426
Iteration 68, loss = 0.01875030
Iteration 69, loss = 0.01843236
Iteration 70, loss = 0.01819867
Iteration 71, loss = 0.01847450
Iteration 72, loss = 0.01831846
Iteration 73, loss = 0.01813478
Iteration 74, loss = 0.01799414
Iteration 75, loss = 0.01797385
Iteration 76, loss = 0.01792333
Iteration 77, loss = 0.01784516
Iteration 78, loss = 0.01800605
Iteration 79, loss = 0.01769560
Iteration 80, loss = 0.01787627
Iteration 81, loss = 0.01778177
Iteration 82, loss = 0.01771010
Iteration 83, loss = 0.01779755
Iteration 84, loss = 0.01779388
Iteration 85, loss = 0.01787946
Iteration 86, loss = 0.01757687
Iteration 87, loss = 0.01799471
Iteration 88, loss = 0.01749711
Iteration 89, loss = 0.01783464
Iteration 90, loss = 0.01746623
Iteration 91, loss = 0.01766256
Iteration 92, loss = 0.01772536
Iteration 93, loss = 0.01752051
Iteration 94, loss = 0.01765029
Iteration 95, loss = 0.01752542
Iteration 96, loss = 0.01748707
Iteration 97, loss = 0.01737961
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69313829
Iteration 2, loss = 0.62275574
Iteration 3, loss = 0.51792880
Iteration 4, loss = 0.40196651
Iteration 5, loss = 0.30149995
Iteration 6, loss = 0.22480591
Iteration 7, loss = 0.17116987
Iteration 8, loss = 0.13461731
Iteration 9, loss = 0.10878412
Iteration 10, loss = 0.09115153
Iteration 11, loss = 0.07808539
Iteration 12, loss = 0.06813675
Iteration 13, loss = 0.06100303
Iteration 14, loss = 0.05506860
Iteration 15, loss = 0.05054127
Iteration 16, loss = 0.04672274
Iteration 17, loss = 0.04390426
Iteration 18, loss = 0.04131265
Iteration 19, loss = 0.03909464
Iteration 20, loss = 0.03746471
Iteration 21, loss = 0.03607677
Iteration 22, loss = 0.03460591
Iteration 23, loss = 0.03351068
Iteration 24, loss = 0.03250799
Iteration 25, loss = 0.03188219
Iteration 26, loss = 0.03097852
Iteration 27, loss = 0.03047173
Iteration 28, loss = 0.02949666
Iteration 29, loss = 0.02907036
Iteration 30, loss = 0.02868343
Iteration 31, loss = 0.02808791
Iteration 32, loss = 0.02755362
Iteration 33, loss = 0.02747761
Iteration 34, loss = 0.02694589
Iteration 35, loss = 0.02682544
Iteration 36, loss = 0.02669570
Iteration 37, loss = 0.02626835
Iteration 38, loss = 0.02626012
Iteration 39, loss = 0.02593069
Iteration 40, loss = 0.02578870
Iteration 41, loss = 0.02570701
Iteration 42, loss = 0.02550084
Iteration 43, loss = 0.02530942
Iteration 44, loss = 0.02504945
Iteration 45, loss = 0.02501412
Iteration 46, loss = 0.02488451
Iteration 47, loss = 0.02480675
Iteration 48, loss = 0.02461181
Iteration 49, loss = 0.02460729
Iteration 50, loss = 0.02451817
Iteration 51, loss = 0.02445322
Iteration 52, loss = 0.02416680
Iteration 53, loss = 0.02448436
Iteration 54, loss = 0.02421291
Iteration 55, loss = 0.02433924
Iteration 56, loss = 0.02400244
Iteration 57, loss = 0.02387091
Iteration 58, loss = 0.02412099
Iteration 59, loss = 0.02396535
Iteration 60, loss = 0.02382372
Iteration 61, loss = 0.02358240
Iteration 62, loss = 0.02371608
Iteration 63, loss = 0.02363434
Iteration 64, loss = 0.02358404
Iteration 65, loss = 0.02369661
Iteration 66, loss = 0.02371262
Iteration 67, loss = 0.02341985
Iteration 68, loss = 0.02362053
Iteration 69, loss = 0.02332902
Iteration 70, loss = 0.02344330
Iteration 71, loss = 0.02352090
Iteration 72, loss = 0.02341973
Iteration 73, loss = 0.02342092
Iteration 74, loss = 0.02322561
Iteration 75, loss = 0.02331384
Iteration 76, loss = 0.02351348
Iteration 77, loss = 0.02296099
Iteration 78, loss = 0.02312297
Iteration 79, loss = 0.02320615
Iteration 80, loss = 0.02311148
Iteration 81, loss = 0.02330269
Iteration 82, loss = 0.02353141
Iteration 83, loss = 0.02305651
Iteration 84, loss = 0.02298082
Iteration 85, loss = 0.02290485
Iteration 86, loss = 0.02319779
Iteration 87, loss = 0.02290068
Iteration 88, loss = 0.02311714
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69067687
Iteration 2, loss = 0.62013700
Iteration 3, loss = 0.51537886
Iteration 4, loss = 0.40319987
Iteration 5, loss = 0.30516488
Iteration 6, loss = 0.23072677
Iteration 7, loss = 0.17692622
Iteration 8, loss = 0.13982337
Iteration 9, loss = 0.11350084
Iteration 10, loss = 0.09493766
Iteration 11, loss = 0.08134425
Iteration 12, loss = 0.07101469
Iteration 13, loss = 0.06326311
Iteration 14, loss = 0.05714249
Iteration 15, loss = 0.05251167
Iteration 16, loss = 0.04852707
Iteration 17, loss = 0.04547148
Iteration 18, loss = 0.04252390
Iteration 19, loss = 0.04044954
Iteration 20, loss = 0.03858454
Iteration 21, loss = 0.03697632
Iteration 22, loss = 0.03559578
Iteration 23, loss = 0.03428033
Iteration 24, loss = 0.03339300
Iteration 25, loss = 0.03228501
Iteration 26, loss = 0.03158851
Iteration 27, loss = 0.03081521
Iteration 28, loss = 0.02999277
Iteration 29, loss = 0.02955214
Iteration 30, loss = 0.02913111
Iteration 31, loss = 0.02867186
Iteration 32, loss = 0.02828976
Iteration 33, loss = 0.02800518
Iteration 34, loss = 0.02741718
Iteration 35, loss = 0.02720956
Iteration 36, loss = 0.02669013
Iteration 37, loss = 0.02659682
Iteration 38, loss = 0.02624093
Iteration 39, loss = 0.02619543
Iteration 40, loss = 0.02602264
Iteration 41, loss = 0.02549200
Iteration 42, loss = 0.02542679
Iteration 43, loss = 0.02518043
Iteration 44, loss = 0.02520484
Iteration 45, loss = 0.02468634
Iteration 46, loss = 0.02502634
Iteration 47, loss = 0.02477131
Iteration 48, loss = 0.02468391
Iteration 49, loss = 0.02469064
Iteration 50, loss = 0.02443753
Iteration 51, loss = 0.02444558
Iteration 52, loss = 0.02457540
Iteration 53, loss = 0.02429026
Iteration 54, loss = 0.02401263
Iteration 55, loss = 0.02388068
Iteration 56, loss = 0.02375431
Iteration 57, loss = 0.02407713
Iteration 58, loss = 0.02394700
Iteration 59, loss = 0.02375077
Iteration 60, loss = 0.02367019
Iteration 61, loss = 0.02364181
Iteration 62, loss = 0.02364375
Iteration 63, loss = 0.02346346
Iteration 64, loss = 0.02342693
Iteration 65, loss = 0.02362565
Iteration 66, loss = 0.02344113
Iteration 67, loss = 0.02325912
Iteration 68, loss = 0.02311561
Iteration 69, loss = 0.02317402
Iteration 70, loss = 0.02332932
Iteration 71, loss = 0.02376119
Iteration 72, loss = 0.02308588
Iteration 73, loss = 0.02295042
Iteration 74, loss = 0.02320612
Iteration 75, loss = 0.02301669
Iteration 76, loss = 0.02303684
Iteration 77, loss = 0.02288372
Iteration 78, loss = 0.02277609
Iteration 79, loss = 0.02276028
Iteration 80, loss = 0.02271799
Iteration 81, loss = 0.02274985
Iteration 82, loss = 0.02296128
Iteration 83, loss = 0.02278472
Iteration 84, loss = 0.02291459
Iteration 85, loss = 0.02297692
Iteration 86, loss = 0.02291945
Iteration 87, loss = 0.02303119
Iteration 88, loss = 0.02301008
Iteration 89, loss = 0.02252874
Iteration 90, loss = 0.02293599
Iteration 91, loss = 0.02297532
Iteration 92, loss = 0.02268560
Iteration 93, loss = 0.02268889
Iteration 94, loss = 0.02255100
Iteration 95, loss = 0.02299969
Iteration 96, loss = 0.02321177
Iteration 97, loss = 0.02262353
Iteration 98, loss = 0.02273736
Iteration 99, loss = 0.02252879
Iteration 100, loss = 0.02280556
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70788830
Iteration 2, loss = 0.68490938
Iteration 3, loss = 0.65487224
Iteration 4, loss = 0.61363813
Iteration 5, loss = 0.55793472
Iteration 6, loss = 0.49426943
Iteration 7, loss = 0.42868179
Iteration 8, loss = 0.36680809
Iteration 9, loss = 0.31230756
Iteration 10, loss = 0.26479513
Iteration 11, loss = 0.22491160
Iteration 12, loss = 0.19183269
Iteration 13, loss = 0.16458456
Iteration 14, loss = 0.14234908
Iteration 15, loss = 0.12403074
Iteration 16, loss = 0.10902719
Iteration 17, loss = 0.09673726
Iteration 18, loss = 0.08653000
Iteration 19, loss = 0.07808594
Iteration 20, loss = 0.07079820
Iteration 21, loss = 0.06473286
Iteration 22, loss = 0.05960812
Iteration 23, loss = 0.05521599
Iteration 24, loss = 0.05127529
Iteration 25, loss = 0.04790400
Iteration 26, loss = 0.04496015
Iteration 27, loss = 0.04232604
Iteration 28, loss = 0.04006997
Iteration 29, loss = 0.03807072
Iteration 30, loss = 0.03622812
Iteration 31, loss = 0.03489525
Iteration 32, loss = 0.03312900
Iteration 33, loss = 0.03181486
Iteration 34, loss = 0.03070250
Iteration 35, loss = 0.02953030
Iteration 36, loss = 0.02852314
Iteration 37, loss = 0.02767633
Iteration 38, loss = 0.02685049
Iteration 39, loss = 0.02610954
Iteration 40, loss = 0.02538186
Iteration 41, loss = 0.02480817
Iteration 42, loss = 0.02432747
Iteration 43, loss = 0.02364924
Iteration 44, loss = 0.02314611
Iteration 45, loss = 0.02284721
Iteration 46, loss = 0.02228164
Iteration 47, loss = 0.02188203
Iteration 48, loss = 0.02144408
Iteration 49, loss = 0.02112092
Iteration 50, loss = 0.02074027
Iteration 51, loss = 0.02042062
Iteration 52, loss = 0.02016178
Iteration 53, loss = 0.01991135
Iteration 54, loss = 0.01973297
Iteration 55, loss = 0.01935746
Iteration 56, loss = 0.01910601
Iteration 57, loss = 0.01894927
Iteration 58, loss = 0.01893907
Iteration 59, loss = 0.01881910
Iteration 60, loss = 0.01844401
Iteration 61, loss = 0.01825519
Iteration 62, loss = 0.01809659
Iteration 63, loss = 0.01789092
Iteration 64, loss = 0.01776328
Iteration 65, loss = 0.01769126
Iteration 66, loss = 0.01745833
Iteration 67, loss = 0.01741068
Iteration 68, loss = 0.01722097
Iteration 69, loss = 0.01711759
Iteration 70, loss = 0.01696676
Iteration 71, loss = 0.01692649
Iteration 72, loss = 0.01670873
Iteration 73, loss = 0.01673297
Iteration 74, loss = 0.01654787
Iteration 75, loss = 0.01647140
Iteration 76, loss = 0.01645518
Iteration 77, loss = 0.01628379
Iteration 78, loss = 0.01619176
Iteration 79, loss = 0.01622644
Iteration 80, loss = 0.01617150
Iteration 81, loss = 0.01601426
Iteration 82, loss = 0.01597748
Iteration 83, loss = 0.01599093
Iteration 84, loss = 0.01575715
Iteration 85, loss = 0.01577291
Iteration 86, loss = 0.01573253
Iteration 87, loss = 0.01576917
Iteration 88, loss = 0.01567071
Iteration 89, loss = 0.01554599
Iteration 90, loss = 0.01559628
Iteration 91, loss = 0.01538238
Iteration 92, loss = 0.01546150
Iteration 93, loss = 0.01552395
Iteration 94, loss = 0.01543439
Iteration 95, loss = 0.01534116
Iteration 96, loss = 0.01530398
Iteration 97, loss = 0.01524832
Iteration 98, loss = 0.01518832
Iteration 99, loss = 0.01518456
Iteration 100, loss = 0.01526513
Iteration 101, loss = 0.01508566
Iteration 102, loss = 0.01508830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69227884
Iteration 2, loss = 0.63215482
Iteration 3, loss = 0.53741498
Iteration 4, loss = 0.42438020
Iteration 5, loss = 0.32202875
Iteration 6, loss = 0.24234360
Iteration 7, loss = 0.18475900
Iteration 8, loss = 0.14452373
Iteration 9, loss = 0.11627143
Iteration 10, loss = 0.09632282
Iteration 11, loss = 0.08165801
Iteration 12, loss = 0.07068695
Iteration 13, loss = 0.06256062
Iteration 14, loss = 0.05602596
Iteration 15, loss = 0.05078935
Iteration 16, loss = 0.04664571
Iteration 17, loss = 0.04334246
Iteration 18, loss = 0.04062667
Iteration 19, loss = 0.03819975
Iteration 20, loss = 0.03614115
Iteration 21, loss = 0.03433910
Iteration 22, loss = 0.03301492
Iteration 23, loss = 0.03155563
Iteration 24, loss = 0.03063992
Iteration 25, loss = 0.02956884
Iteration 26, loss = 0.02887088
Iteration 27, loss = 0.02796649
Iteration 28, loss = 0.02717241
Iteration 29, loss = 0.02677924
Iteration 30, loss = 0.02630300
Iteration 31, loss = 0.02574921
Iteration 32, loss = 0.02554227
Iteration 33, loss = 0.02511686
Iteration 34, loss = 0.02466636
Iteration 35, loss = 0.02390601
Iteration 36, loss = 0.02397841
Iteration 37, loss = 0.02374352
Iteration 38, loss = 0.02332686
Iteration 39, loss = 0.02311520
Iteration 40, loss = 0.02290569
Iteration 41, loss = 0.02294786
Iteration 42, loss = 0.02240434
Iteration 43, loss = 0.02227779
Iteration 44, loss = 0.02249727
Iteration 45, loss = 0.02178335
Iteration 46, loss = 0.02217599
Iteration 47, loss = 0.02170948
Iteration 48, loss = 0.02173359
Iteration 49, loss = 0.02158157
Iteration 50, loss = 0.02142360
Iteration 51, loss = 0.02143081
Iteration 52, loss = 0.02132272
Iteration 53, loss = 0.02122545
Iteration 54, loss = 0.02113356
Iteration 55, loss = 0.02106886
Iteration 56, loss = 0.02116168
Iteration 57, loss = 0.02113088
Iteration 58, loss = 0.02096514
Iteration 59, loss = 0.02076573
Iteration 60, loss = 0.02074301
Iteration 61, loss = 0.02049461
Iteration 62, loss = 0.02049996
Iteration 63, loss = 0.02074293
Iteration 64, loss = 0.02059430
Iteration 65, loss = 0.02040439
Iteration 66, loss = 0.02036573
Iteration 67, loss = 0.02034558
Iteration 68, loss = 0.02030884
Iteration 69, loss = 0.02010111
Iteration 70, loss = 0.02034290
Iteration 71, loss = 0.02006522
Iteration 72, loss = 0.02023644
Iteration 73, loss = 0.02018677
Iteration 74, loss = 0.01998344
Iteration 75, loss = 0.02021299
Iteration 76, loss = 0.02022898
Iteration 77, loss = 0.02025210
Iteration 78, loss = 0.02005938
Iteration 79, loss = 0.02007108
Iteration 80, loss = 0.01976753
Iteration 81, loss = 0.01976766
Iteration 82, loss = 0.01992803
Iteration 83, loss = 0.01962986
Iteration 84, loss = 0.01969905
Iteration 85, loss = 0.01999257
Iteration 86, loss = 0.01986517
Iteration 87, loss = 0.01975189
Iteration 88, loss = 0.01991863
Iteration 89, loss = 0.01976385
Iteration 90, loss = 0.01955429
Iteration 91, loss = 0.01965481
Iteration 92, loss = 0.01977954
Iteration 93, loss = 0.01963696
Iteration 94, loss = 0.01978210
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67215703
Iteration 2, loss = 0.59287390
Iteration 3, loss = 0.47514346
Iteration 4, loss = 0.35992285
Iteration 5, loss = 0.26688761
Iteration 6, loss = 0.20029776
Iteration 7, loss = 0.15405966
Iteration 8, loss = 0.12228048
Iteration 9, loss = 0.10031482
Iteration 10, loss = 0.08459623
Iteration 11, loss = 0.07309153
Iteration 12, loss = 0.06453606
Iteration 13, loss = 0.05809090
Iteration 14, loss = 0.05286224
Iteration 15, loss = 0.04874797
Iteration 16, loss = 0.04541413
Iteration 17, loss = 0.04264959
Iteration 18, loss = 0.04028211
Iteration 19, loss = 0.03842316
Iteration 20, loss = 0.03689423
Iteration 21, loss = 0.03532336
Iteration 22, loss = 0.03417856
Iteration 23, loss = 0.03311696
Iteration 24, loss = 0.03241942
Iteration 25, loss = 0.03150990
Iteration 26, loss = 0.03073401
Iteration 27, loss = 0.03017380
Iteration 28, loss = 0.02953342
Iteration 29, loss = 0.02894171
Iteration 30, loss = 0.02881593
Iteration 31, loss = 0.02831536
Iteration 32, loss = 0.02777739
Iteration 33, loss = 0.02751486
Iteration 34, loss = 0.02724361
Iteration 35, loss = 0.02677786
Iteration 36, loss = 0.02674793
Iteration 37, loss = 0.02665584
Iteration 38, loss = 0.02630325
Iteration 39, loss = 0.02626589
Iteration 40, loss = 0.02640501
Iteration 41, loss = 0.02584904
Iteration 42, loss = 0.02564257
Iteration 43, loss = 0.02566779
Iteration 44, loss = 0.02544223
Iteration 45, loss = 0.02517765
Iteration 46, loss = 0.02507096
Iteration 47, loss = 0.02518346
Iteration 48, loss = 0.02526007
Iteration 49, loss = 0.02500947
Iteration 50, loss = 0.02469724
Iteration 51, loss = 0.02478091
Iteration 52, loss = 0.02466694
Iteration 53, loss = 0.02466122
Iteration 54, loss = 0.02449045
Iteration 55, loss = 0.02446415
Iteration 56, loss = 0.02481645
Iteration 57, loss = 0.02429942
Iteration 58, loss = 0.02443819
Iteration 59, loss = 0.02418807
Iteration 60, loss = 0.02419263
Iteration 61, loss = 0.02388317
Iteration 62, loss = 0.02417523
Iteration 63, loss = 0.02390556
Iteration 64, loss = 0.02393262
Iteration 65, loss = 0.02422015
Iteration 66, loss = 0.02405227
Iteration 67, loss = 0.02402596
Iteration 68, loss = 0.02387616
Iteration 69, loss = 0.02442734
Iteration 70, loss = 0.02408707
Iteration 71, loss = 0.02376602
Iteration 72, loss = 0.02384458
Iteration 73, loss = 0.02364397
Iteration 74, loss = 0.02391364
Iteration 75, loss = 0.02377630
Iteration 76, loss = 0.02340508
Iteration 77, loss = 0.02381004
Iteration 78, loss = 0.02381729
Iteration 79, loss = 0.02347669
Iteration 80, loss = 0.02371558
Iteration 81, loss = 0.02344465
Iteration 82, loss = 0.02374525
Iteration 83, loss = 0.02345336
Iteration 84, loss = 0.02360736
Iteration 85, loss = 0.02328454
Iteration 86, loss = 0.02347259
Iteration 87, loss = 0.02327149
Iteration 88, loss = 0.02329357
Iteration 89, loss = 0.02358872
Iteration 90, loss = 0.02361302
Iteration 91, loss = 0.02328487
Iteration 92, loss = 0.02358271
Iteration 93, loss = 0.02319726
Iteration 94, loss = 0.02351219
Iteration 95, loss = 0.02307637
Iteration 96, loss = 0.02312128
Iteration 97, loss = 0.02336522
Iteration 98, loss = 0.02303239
Iteration 99, loss = 0.02357725
Iteration 100, loss = 0.02373046
Iteration 101, loss = 0.02373578
Iteration 102, loss = 0.02307473
Iteration 103, loss = 0.02346381
Iteration 104, loss = 0.02313510
Iteration 105, loss = 0.02357210
Iteration 106, loss = 0.02319600
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71633534
Iteration 2, loss = 0.66806611
Iteration 3, loss = 0.60946715
Iteration 4, loss = 0.54282990
Iteration 5, loss = 0.46994580
Iteration 6, loss = 0.39707073
Iteration 7, loss = 0.33061522
Iteration 8, loss = 0.27336921
Iteration 9, loss = 0.22598947
Iteration 10, loss = 0.18772665
Iteration 11, loss = 0.15731581
Iteration 12, loss = 0.13316178
Iteration 13, loss = 0.11427228
Iteration 14, loss = 0.09896165
Iteration 15, loss = 0.08690847
Iteration 16, loss = 0.07709914
Iteration 17, loss = 0.06904269
Iteration 18, loss = 0.06235424
Iteration 19, loss = 0.05691192
Iteration 20, loss = 0.05240399
Iteration 21, loss = 0.04829105
Iteration 22, loss = 0.04487837
Iteration 23, loss = 0.04198266
Iteration 24, loss = 0.03942757
Iteration 25, loss = 0.03726127
Iteration 26, loss = 0.03539281
Iteration 27, loss = 0.03357506
Iteration 28, loss = 0.03217134
Iteration 29, loss = 0.03067675
Iteration 30, loss = 0.02941336
Iteration 31, loss = 0.02851889
Iteration 32, loss = 0.02744266
Iteration 33, loss = 0.02666910
Iteration 34, loss = 0.02574495
Iteration 35, loss = 0.02499353
Iteration 36, loss = 0.02430048
Iteration 37, loss = 0.02384539
Iteration 38, loss = 0.02332944
Iteration 39, loss = 0.02277075
Iteration 40, loss = 0.02224919
Iteration 41, loss = 0.02177444
Iteration 42, loss = 0.02143403
Iteration 43, loss = 0.02096615
Iteration 44, loss = 0.02066793
Iteration 45, loss = 0.02057537
Iteration 46, loss = 0.02012204
Iteration 47, loss = 0.01982981
Iteration 48, loss = 0.01954536
Iteration 49, loss = 0.01940225
Iteration 50, loss = 0.01916595
Iteration 51, loss = 0.01889188
Iteration 52, loss = 0.01875554
Iteration 53, loss = 0.01834471
Iteration 54, loss = 0.01822011
Iteration 55, loss = 0.01803177
Iteration 56, loss = 0.01799069
Iteration 57, loss = 0.01780071
Iteration 58, loss = 0.01762541
Iteration 59, loss = 0.01756108
Iteration 60, loss = 0.01739971
Iteration 61, loss = 0.01724555
Iteration 62, loss = 0.01712810
Iteration 63, loss = 0.01704436
Iteration 64, loss = 0.01678836
Iteration 65, loss = 0.01670334
Iteration 66, loss = 0.01683613
Iteration 67, loss = 0.01660481
Iteration 68, loss = 0.01649844
Iteration 69, loss = 0.01636547
Iteration 70, loss = 0.01634128
Iteration 71, loss = 0.01616051
Iteration 72, loss = 0.01619904
Iteration 73, loss = 0.01613064
Iteration 74, loss = 0.01626662
Iteration 75, loss = 0.01599548
Iteration 76, loss = 0.01602621
Iteration 77, loss = 0.01617691
Iteration 78, loss = 0.01595714
Iteration 79, loss = 0.01576301
Iteration 80, loss = 0.01582771
Iteration 81, loss = 0.01571879
Iteration 82, loss = 0.01563435
Iteration 83, loss = 0.01563340
Iteration 84, loss = 0.01550742
Iteration 85, loss = 0.01556746
Iteration 86, loss = 0.01551590
Iteration 87, loss = 0.01545843
Iteration 88, loss = 0.01531616
Iteration 89, loss = 0.01539785
Iteration 90, loss = 0.01528419
Iteration 91, loss = 0.01553970
Iteration 92, loss = 0.01539186
Iteration 93, loss = 0.01544701
Iteration 94, loss = 0.01526021
Iteration 95, loss = 0.01516653
Iteration 96, loss = 0.01507147
Iteration 97, loss = 0.01523601
Iteration 98, loss = 0.01500671
Iteration 99, loss = 0.01509630
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68072462
Iteration 2, loss = 0.60877691
Iteration 3, loss = 0.50224746
Iteration 4, loss = 0.39007619
Iteration 5, loss = 0.29500922
Iteration 6, loss = 0.22219317
Iteration 7, loss = 0.17047923
Iteration 8, loss = 0.13415578
Iteration 9, loss = 0.10876251
Iteration 10, loss = 0.09027393
Iteration 11, loss = 0.07719744
Iteration 12, loss = 0.06666746
Iteration 13, loss = 0.05947554
Iteration 14, loss = 0.05287953
Iteration 15, loss = 0.04820896
Iteration 16, loss = 0.04436454
Iteration 17, loss = 0.04091836
Iteration 18, loss = 0.03827566
Iteration 19, loss = 0.03608689
Iteration 20, loss = 0.03427533
Iteration 21, loss = 0.03280603
Iteration 22, loss = 0.03129195
Iteration 23, loss = 0.03012916
Iteration 24, loss = 0.02909144
Iteration 25, loss = 0.02821241
Iteration 26, loss = 0.02743253
Iteration 27, loss = 0.02667783
Iteration 28, loss = 0.02605549
Iteration 29, loss = 0.02544608
Iteration 30, loss = 0.02490749
Iteration 31, loss = 0.02419552
Iteration 32, loss = 0.02414051
Iteration 33, loss = 0.02389765
Iteration 34, loss = 0.02333780
Iteration 35, loss = 0.02293004
Iteration 36, loss = 0.02288195
Iteration 37, loss = 0.02243382
Iteration 38, loss = 0.02215236
Iteration 39, loss = 0.02230434
Iteration 40, loss = 0.02200550
Iteration 41, loss = 0.02174741
Iteration 42, loss = 0.02129405
Iteration 43, loss = 0.02111180
Iteration 44, loss = 0.02125931
Iteration 45, loss = 0.02099332
Iteration 46, loss = 0.02121469
Iteration 47, loss = 0.02097194
Iteration 48, loss = 0.02083306
Iteration 49, loss = 0.02082998
Iteration 50, loss = 0.02060938
Iteration 51, loss = 0.02054740
Iteration 52, loss = 0.02020657
Iteration 53, loss = 0.02046717
Iteration 54, loss = 0.02035082
Iteration 55, loss = 0.01998602
Iteration 56, loss = 0.01980196
Iteration 57, loss = 0.01984295
Iteration 58, loss = 0.01974851
Iteration 59, loss = 0.02004042
Iteration 60, loss = 0.01970444
Iteration 61, loss = 0.01970040
Iteration 62, loss = 0.01980765
Iteration 63, loss = 0.01944476
Iteration 64, loss = 0.01951003
Iteration 65, loss = 0.01950808
Iteration 66, loss = 0.01985139
Iteration 67, loss = 0.01937516
Iteration 68, loss = 0.01935009
Iteration 69, loss = 0.01929680
Iteration 70, loss = 0.01917110
Iteration 71, loss = 0.01935848
Iteration 72, loss = 0.01935150
Iteration 73, loss = 0.01917036
Iteration 74, loss = 0.01947500
Iteration 75, loss = 0.01906942
Iteration 76, loss = 0.01914519
Iteration 77, loss = 0.01930034
Iteration 78, loss = 0.01896872
Iteration 79, loss = 0.01901184
Iteration 80, loss = 0.01924126
Iteration 81, loss = 0.01907883
Iteration 82, loss = 0.01935401
Iteration 83, loss = 0.01895376
Iteration 84, loss = 0.01884181
Iteration 85, loss = 0.01897123
Iteration 86, loss = 0.01876170
Iteration 87, loss = 0.01888767
Iteration 88, loss = 0.01860490
Iteration 89, loss = 0.01879069
Iteration 90, loss = 0.01901107
Iteration 91, loss = 0.01896476
Iteration 92, loss = 0.01893303
Iteration 93, loss = 0.01930950
Iteration 94, loss = 0.01877450
Iteration 95, loss = 0.01891978
Iteration 96, loss = 0.01886955
Iteration 97, loss = 0.01864652
Iteration 98, loss = 0.01898479
Iteration 99, loss = 0.01870926
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69933081
Iteration 2, loss = 0.66768028
Iteration 3, loss = 0.63063216
Iteration 4, loss = 0.58543130
Iteration 5, loss = 0.53539740
Iteration 6, loss = 0.48349859
Iteration 7, loss = 0.43289024
Iteration 8, loss = 0.38349778
Iteration 9, loss = 0.33701817
Iteration 10, loss = 0.29510444
Iteration 11, loss = 0.25800997
Iteration 12, loss = 0.22578434
Iteration 13, loss = 0.19777408
Iteration 14, loss = 0.17374688
Iteration 15, loss = 0.15349732
Iteration 16, loss = 0.13615976
Iteration 17, loss = 0.12151214
Iteration 18, loss = 0.10896741
Iteration 19, loss = 0.09831566
Iteration 20, loss = 0.08926517
Iteration 21, loss = 0.08152114
Iteration 22, loss = 0.07475829
Iteration 23, loss = 0.06901951
Iteration 24, loss = 0.06412304
Iteration 25, loss = 0.05970327
Iteration 26, loss = 0.05579014
Iteration 27, loss = 0.05222231
Iteration 28, loss = 0.04904459
Iteration 29, loss = 0.04644297
Iteration 30, loss = 0.04394316
Iteration 31, loss = 0.04162217
Iteration 32, loss = 0.03964760
Iteration 33, loss = 0.03782913
Iteration 34, loss = 0.03615485
Iteration 35, loss = 0.03463689
Iteration 36, loss = 0.03329807
Iteration 37, loss = 0.03202804
Iteration 38, loss = 0.03094698
Iteration 39, loss = 0.02994922
Iteration 40, loss = 0.02909910
Iteration 41, loss = 0.02816953
Iteration 42, loss = 0.02731141
Iteration 43, loss = 0.02653798
Iteration 44, loss = 0.02586347
Iteration 45, loss = 0.02510844
Iteration 46, loss = 0.02447502
Iteration 47, loss = 0.02391487
Iteration 48, loss = 0.02341621
Iteration 49, loss = 0.02291256
Iteration 50, loss = 0.02251508
Iteration 51, loss = 0.02212747
Iteration 52, loss = 0.02158687
Iteration 53, loss = 0.02144341
Iteration 54, loss = 0.02106897
Iteration 55, loss = 0.02071818
Iteration 56, loss = 0.02028510
Iteration 57, loss = 0.02001704
Iteration 58, loss = 0.01979832
Iteration 59, loss = 0.01952487
Iteration 60, loss = 0.01927659
Iteration 61, loss = 0.01903980
Iteration 62, loss = 0.01897698
Iteration 63, loss = 0.01873724
Iteration 64, loss = 0.01839945
Iteration 65, loss = 0.01817812
Iteration 66, loss = 0.01789200
Iteration 67, loss = 0.01786087
Iteration 68, loss = 0.01766298
Iteration 69, loss = 0.01730737
Iteration 70, loss = 0.01712107
Iteration 71, loss = 0.01697746
Iteration 72, loss = 0.01685879
Iteration 73, loss = 0.01662012
Iteration 74, loss = 0.01659862
Iteration 75, loss = 0.01648653
Iteration 76, loss = 0.01652259
Iteration 77, loss = 0.01661804
Iteration 78, loss = 0.01652496
Iteration 79, loss = 0.01626920
Iteration 80, loss = 0.01607710
Iteration 81, loss = 0.01585811
Iteration 82, loss = 0.01585182
Iteration 83, loss = 0.01586116
Iteration 84, loss = 0.01566053
Iteration 85, loss = 0.01617573
Iteration 86, loss = 0.01594061
Iteration 87, loss = 0.01567556
Iteration 88, loss = 0.01543252
Iteration 89, loss = 0.01524632
Iteration 90, loss = 0.01508929
Iteration 91, loss = 0.01497742
Iteration 92, loss = 0.01493619
Iteration 93, loss = 0.01475630
Iteration 94, loss = 0.01474787
Iteration 95, loss = 0.01455391
Iteration 96, loss = 0.01464746
Iteration 97, loss = 0.01468302
Iteration 98, loss = 0.01452368
Iteration 99, loss = 0.01448964
Iteration 100, loss = 0.01438656
Iteration 101, loss = 0.01466490
Iteration 102, loss = 0.01460356
Iteration 103, loss = 0.01464020
Iteration 104, loss = 0.01414223
Iteration 105, loss = 0.01440980
Iteration 106, loss = 0.01417069
Iteration 107, loss = 0.01416892
Iteration 108, loss = 0.01412790
Iteration 109, loss = 0.01424781
Iteration 110, loss = 0.01428913
Iteration 111, loss = 0.01409770
Iteration 112, loss = 0.01404926
Iteration 113, loss = 0.01358433
Iteration 114, loss = 0.01398461
Iteration 115, loss = 0.01416666
Iteration 116, loss = 0.01422776
Iteration 117, loss = 0.01447832
Iteration 118, loss = 0.01468614
Iteration 119, loss = 0.01470354
Iteration 120, loss = 0.01408919
Iteration 121, loss = 0.01393983
Iteration 122, loss = 0.01380575
Iteration 123, loss = 0.01376631
Iteration 124, loss = 0.01375187
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69412828
Iteration 2, loss = 0.61471140
Iteration 3, loss = 0.49995711
Iteration 4, loss = 0.37818308
Iteration 5, loss = 0.27890213
Iteration 6, loss = 0.20678448
Iteration 7, loss = 0.15680619
Iteration 8, loss = 0.12337383
Iteration 9, loss = 0.10025348
Iteration 10, loss = 0.08377728
Iteration 11, loss = 0.07189500
Iteration 12, loss = 0.06274967
Iteration 13, loss = 0.05604830
Iteration 14, loss = 0.05076308
Iteration 15, loss = 0.04638140
Iteration 16, loss = 0.04284956
Iteration 17, loss = 0.04019577
Iteration 18, loss = 0.03783496
Iteration 19, loss = 0.03601887
Iteration 20, loss = 0.03427646
Iteration 21, loss = 0.03285227
Iteration 22, loss = 0.03155715
Iteration 23, loss = 0.03039525
Iteration 24, loss = 0.02962698
Iteration 25, loss = 0.02878840
Iteration 26, loss = 0.02811119
Iteration 27, loss = 0.02745658
Iteration 28, loss = 0.02690722
Iteration 29, loss = 0.02650630
Iteration 30, loss = 0.02575672
Iteration 31, loss = 0.02547168
Iteration 32, loss = 0.02523464
Iteration 33, loss = 0.02474201
Iteration 34, loss = 0.02443127
Iteration 35, loss = 0.02413897
Iteration 36, loss = 0.02401029
Iteration 37, loss = 0.02358444
Iteration 38, loss = 0.02355743
Iteration 39, loss = 0.02333470
Iteration 40, loss = 0.02310539
Iteration 41, loss = 0.02295824
Iteration 42, loss = 0.02278201
Iteration 43, loss = 0.02290196
Iteration 44, loss = 0.02246708
Iteration 45, loss = 0.02237944
Iteration 46, loss = 0.02231045
Iteration 47, loss = 0.02267845
Iteration 48, loss = 0.02252535
Iteration 49, loss = 0.02192012
Iteration 50, loss = 0.02203095
Iteration 51, loss = 0.02185777
Iteration 52, loss = 0.02165843
Iteration 53, loss = 0.02160835
Iteration 54, loss = 0.02157199
Iteration 55, loss = 0.02165564
Iteration 56, loss = 0.02154349
Iteration 57, loss = 0.02154370
Iteration 58, loss = 0.02150860
Iteration 59, loss = 0.02170182
Iteration 60, loss = 0.02124042
Iteration 61, loss = 0.02117275
Iteration 62, loss = 0.02119848
Iteration 63, loss = 0.02098248
Iteration 64, loss = 0.02134182
Iteration 65, loss = 0.02091932
Iteration 66, loss = 0.02091461
Iteration 67, loss = 0.02093900
Iteration 68, loss = 0.02090164
Iteration 69, loss = 0.02086163
Iteration 70, loss = 0.02086798
Iteration 71, loss = 0.02060443
Iteration 72, loss = 0.02115962
Iteration 73, loss = 0.02096495
Iteration 74, loss = 0.02079316
Iteration 75, loss = 0.02075919
Iteration 76, loss = 0.02062684
Iteration 77, loss = 0.02075781
Iteration 78, loss = 0.02049732
Iteration 79, loss = 0.02051292
Iteration 80, loss = 0.02035471
Iteration 81, loss = 0.02094314
Iteration 82, loss = 0.02054195
Iteration 83, loss = 0.02058673
Iteration 84, loss = 0.02063645
Iteration 85, loss = 0.02057810
Iteration 86, loss = 0.02072085
Iteration 87, loss = 0.02059727
Iteration 88, loss = 0.02039192
Iteration 89, loss = 0.02071780
Iteration 90, loss = 0.02026385
Iteration 91, loss = 0.02069797
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69033093
Iteration 2, loss = 0.64635308
Iteration 3, loss = 0.58408292
Iteration 4, loss = 0.50075430
Iteration 5, loss = 0.41682323
Iteration 6, loss = 0.34069270
Iteration 7, loss = 0.27621370
Iteration 8, loss = 0.22480298
Iteration 9, loss = 0.18431243
Iteration 10, loss = 0.15341325
Iteration 11, loss = 0.12916308
Iteration 12, loss = 0.11072903
Iteration 13, loss = 0.09621104
Iteration 14, loss = 0.08497765
Iteration 15, loss = 0.07595425
Iteration 16, loss = 0.06822406
Iteration 17, loss = 0.06213180
Iteration 18, loss = 0.05699919
Iteration 19, loss = 0.05272587
Iteration 20, loss = 0.04926499
Iteration 21, loss = 0.04623712
Iteration 22, loss = 0.04366993
Iteration 23, loss = 0.04141298
Iteration 24, loss = 0.03965881
Iteration 25, loss = 0.03778467
Iteration 26, loss = 0.03643161
Iteration 27, loss = 0.03498112
Iteration 28, loss = 0.03405027
Iteration 29, loss = 0.03289082
Iteration 30, loss = 0.03198564
Iteration 31, loss = 0.03094383
Iteration 32, loss = 0.03025303
Iteration 33, loss = 0.02946914
Iteration 34, loss = 0.02887274
Iteration 35, loss = 0.02831157
Iteration 36, loss = 0.02790658
Iteration 37, loss = 0.02747613
Iteration 38, loss = 0.02711596
Iteration 39, loss = 0.02643763
Iteration 40, loss = 0.02627929
Iteration 41, loss = 0.02587585
Iteration 42, loss = 0.02559087
Iteration 43, loss = 0.02514977
Iteration 44, loss = 0.02486369
Iteration 45, loss = 0.02478605
Iteration 46, loss = 0.02440765
Iteration 47, loss = 0.02411140
Iteration 48, loss = 0.02399523
Iteration 49, loss = 0.02375775
Iteration 50, loss = 0.02339871
Iteration 51, loss = 0.02329563
Iteration 52, loss = 0.02329033
Iteration 53, loss = 0.02290314
Iteration 54, loss = 0.02315387
Iteration 55, loss = 0.02273646
Iteration 56, loss = 0.02280253
Iteration 57, loss = 0.02250795
Iteration 58, loss = 0.02234587
Iteration 59, loss = 0.02257072
Iteration 60, loss = 0.02238866
Iteration 61, loss = 0.02209760
Iteration 62, loss = 0.02217424
Iteration 63, loss = 0.02238393
Iteration 64, loss = 0.02198987
Iteration 65, loss = 0.02178504
Iteration 66, loss = 0.02183260
Iteration 67, loss = 0.02158881
Iteration 68, loss = 0.02160540
Iteration 69, loss = 0.02143727
Iteration 70, loss = 0.02136882
Iteration 71, loss = 0.02145420
Iteration 72, loss = 0.02124790
Iteration 73, loss = 0.02145366
Iteration 74, loss = 0.02111727
Iteration 75, loss = 0.02101901
Iteration 76, loss = 0.02106816
Iteration 77, loss = 0.02108369
Iteration 78, loss = 0.02103029
Iteration 79, loss = 0.02099282
Iteration 80, loss = 0.02113230
Iteration 81, loss = 0.02086349
Iteration 82, loss = 0.02087443
Iteration 83, loss = 0.02104617
Iteration 84, loss = 0.02130742
Iteration 85, loss = 0.02104843
Iteration 86, loss = 0.02082954
Iteration 87, loss = 0.02097823
Iteration 88, loss = 0.02108545
Iteration 89, loss = 0.02102273
Iteration 90, loss = 0.02103357
Iteration 91, loss = 0.02110896
Iteration 92, loss = 0.02093600
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68849190
Iteration 2, loss = 0.61145476
Iteration 3, loss = 0.49223593
Iteration 4, loss = 0.37316339
Iteration 5, loss = 0.27601832
Iteration 6, loss = 0.20506673
Iteration 7, loss = 0.15587603
Iteration 8, loss = 0.12272171
Iteration 9, loss = 0.09942735
Iteration 10, loss = 0.08318528
Iteration 11, loss = 0.07134079
Iteration 12, loss = 0.06239434
Iteration 13, loss = 0.05556309
Iteration 14, loss = 0.05032574
Iteration 15, loss = 0.04591333
Iteration 16, loss = 0.04256642
Iteration 17, loss = 0.03956595
Iteration 18, loss = 0.03744161
Iteration 19, loss = 0.03536028
Iteration 20, loss = 0.03367631
Iteration 21, loss = 0.03227616
Iteration 22, loss = 0.03102816
Iteration 23, loss = 0.02997906
Iteration 24, loss = 0.02899112
Iteration 25, loss = 0.02825389
Iteration 26, loss = 0.02761034
Iteration 27, loss = 0.02711959
Iteration 28, loss = 0.02651135
Iteration 29, loss = 0.02581933
Iteration 30, loss = 0.02547581
Iteration 31, loss = 0.02497344
Iteration 32, loss = 0.02466715
Iteration 33, loss = 0.02443224
Iteration 34, loss = 0.02413134
Iteration 35, loss = 0.02371137
Iteration 36, loss = 0.02356227
Iteration 37, loss = 0.02313804
Iteration 38, loss = 0.02323677
Iteration 39, loss = 0.02282357
Iteration 40, loss = 0.02288051
Iteration 41, loss = 0.02286370
Iteration 42, loss = 0.02250252
Iteration 43, loss = 0.02224462
Iteration 44, loss = 0.02222350
Iteration 45, loss = 0.02222582
Iteration 46, loss = 0.02194818
Iteration 47, loss = 0.02185069
Iteration 48, loss = 0.02144658
Iteration 49, loss = 0.02169657
Iteration 50, loss = 0.02162088
Iteration 51, loss = 0.02131691
Iteration 52, loss = 0.02137396
Iteration 53, loss = 0.02142345
Iteration 54, loss = 0.02142090
Iteration 55, loss = 0.02141549
Iteration 56, loss = 0.02126444
Iteration 57, loss = 0.02102180
Iteration 58, loss = 0.02123703
Iteration 59, loss = 0.02081357
Iteration 60, loss = 0.02086143
Iteration 61, loss = 0.02102053
Iteration 62, loss = 0.02094412
Iteration 63, loss = 0.02102871
Iteration 64, loss = 0.02096025
Iteration 65, loss = 0.02088038
Iteration 66, loss = 0.02074031
Iteration 67, loss = 0.02074078
Iteration 68, loss = 0.02071050
Iteration 69, loss = 0.02067782
Iteration 70, loss = 0.02053130
Iteration 71, loss = 0.02052026
Iteration 72, loss = 0.02074161
Iteration 73, loss = 0.02060608
Iteration 74, loss = 0.02034409
Iteration 75, loss = 0.02074792
Iteration 76, loss = 0.02029144
Iteration 77, loss = 0.02045593
Iteration 78, loss = 0.02035984
Iteration 79, loss = 0.02065983
Iteration 80, loss = 0.02021676
Iteration 81, loss = 0.02055481
Iteration 82, loss = 0.02078276
Iteration 83, loss = 0.02032198
Iteration 84, loss = 0.02009328
Iteration 85, loss = 0.02045254
Iteration 86, loss = 0.02024790
Iteration 87, loss = 0.02017895
Iteration 88, loss = 0.01990582
Iteration 89, loss = 0.02025794
Iteration 90, loss = 0.02030426
Iteration 91, loss = 0.02028953
Iteration 92, loss = 0.02010050
Iteration 93, loss = 0.02008812
Iteration 94, loss = 0.02038731
Iteration 95, loss = 0.02041567
Iteration 96, loss = 0.02001269
Iteration 97, loss = 0.02009867
Iteration 98, loss = 0.01996962
Iteration 99, loss = 0.01995755
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	91m40.217s
user	375m5.276s
sys	38m7.202s
Elapsed time: 5500 seconds
