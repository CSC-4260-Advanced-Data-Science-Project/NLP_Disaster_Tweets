Processing dataset: kept_v4_stemmed

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: kept_v4_stemmed (7443 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on kept_v4_stemmed...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on kept_v4_stemmed...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on kept_v4_stemmed...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on kept_v4_stemmed...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on kept_v4_stemmed...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.67319437
Iteration 2, loss = 0.59940226
Iteration 3, loss = 0.49050904
Iteration 4, loss = 0.38106647
Iteration 5, loss = 0.29012542
Iteration 6, loss = 0.22122864
Iteration 7, loss = 0.17250240
Iteration 8, loss = 0.13804726
Iteration 9, loss = 0.11363285
Iteration 10, loss = 0.09625173
Iteration 11, loss = 0.08308133
Iteration 12, loss = 0.07360149
Iteration 13, loss = 0.06593208
Iteration 14, loss = 0.06019085
Iteration 15, loss = 0.05535468
Iteration 16, loss = 0.05181440
Iteration 17, loss = 0.04869939
Iteration 18, loss = 0.04612173
Iteration 19, loss = 0.04386513
Iteration 20, loss = 0.04203160
Iteration 21, loss = 0.04043755
Iteration 22, loss = 0.03883099
Iteration 23, loss = 0.03770666
Iteration 24, loss = 0.03648912
Iteration 25, loss = 0.03576482
Iteration 26, loss = 0.03479430
Iteration 27, loss = 0.03460467
Iteration 28, loss = 0.03367056
Iteration 29, loss = 0.03311503
Iteration 30, loss = 0.03272614
Iteration 31, loss = 0.03211417
Iteration 32, loss = 0.03200377
Iteration 33, loss = 0.03167259
Iteration 34, loss = 0.03107409
Iteration 35, loss = 0.03092821
Iteration 36, loss = 0.03079538
Iteration 37, loss = 0.03010579
Iteration 38, loss = 0.03003095
Iteration 39, loss = 0.02979901
Iteration 40, loss = 0.02937311
Iteration 41, loss = 0.02943506
Iteration 42, loss = 0.02927880
Iteration 43, loss = 0.02940746
Iteration 44, loss = 0.02947701
Iteration 45, loss = 0.02871881
Iteration 46, loss = 0.02867990
Iteration 47, loss = 0.02861420
Iteration 48, loss = 0.02833882
Iteration 49, loss = 0.02834670
Iteration 50, loss = 0.02820899
Iteration 51, loss = 0.02827352
Iteration 52, loss = 0.02825833
Iteration 53, loss = 0.02794410
Iteration 54, loss = 0.02770731
Iteration 55, loss = 0.02819988
Iteration 56, loss = 0.02782782
Iteration 57, loss = 0.02756505
Iteration 58, loss = 0.02772230
Iteration 59, loss = 0.02764805
Iteration 60, loss = 0.02757855
Iteration 61, loss = 0.02742337
Iteration 62, loss = 0.02735847
Iteration 63, loss = 0.02749360
Iteration 64, loss = 0.02757960
Iteration 65, loss = 0.02758257
Iteration 66, loss = 0.02708319
Iteration 67, loss = 0.02725240
Iteration 68, loss = 0.02721464
Iteration 69, loss = 0.02737860
Iteration 70, loss = 0.02733892
Iteration 71, loss = 0.02709446
Iteration 72, loss = 0.02711366
Iteration 73, loss = 0.02735974
Iteration 74, loss = 0.02724183
Iteration 75, loss = 0.02695650
Iteration 76, loss = 0.02689685
Iteration 77, loss = 0.02694560
Iteration 78, loss = 0.02729435
Iteration 79, loss = 0.02690746
Iteration 80, loss = 0.02681660
Iteration 81, loss = 0.02692310
Iteration 82, loss = 0.02657753
Iteration 83, loss = 0.02676489
Iteration 84, loss = 0.02711265
Iteration 85, loss = 0.02693454
Iteration 86, loss = 0.02697994
Iteration 87, loss = 0.02698188
Iteration 88, loss = 0.02669465
Iteration 89, loss = 0.02665258
Iteration 90, loss = 0.02669688
Iteration 91, loss = 0.02660853
Iteration 92, loss = 0.02680904
Iteration 93, loss = 0.02698565
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67508086
Iteration 2, loss = 0.60856440
Iteration 3, loss = 0.50565330
Iteration 4, loss = 0.39541598
Iteration 5, loss = 0.30027590
Iteration 6, loss = 0.22779266
Iteration 7, loss = 0.17596697
Iteration 8, loss = 0.13980122
Iteration 9, loss = 0.11406456
Iteration 10, loss = 0.09582464
Iteration 11, loss = 0.08243451
Iteration 12, loss = 0.07234315
Iteration 13, loss = 0.06460186
Iteration 14, loss = 0.05874645
Iteration 15, loss = 0.05384640
Iteration 16, loss = 0.04993292
Iteration 17, loss = 0.04652906
Iteration 18, loss = 0.04384806
Iteration 19, loss = 0.04133451
Iteration 20, loss = 0.03946586
Iteration 21, loss = 0.03808146
Iteration 22, loss = 0.03640482
Iteration 23, loss = 0.03517489
Iteration 24, loss = 0.03439451
Iteration 25, loss = 0.03321606
Iteration 26, loss = 0.03239582
Iteration 27, loss = 0.03163009
Iteration 28, loss = 0.03100716
Iteration 29, loss = 0.03032232
Iteration 30, loss = 0.02971530
Iteration 31, loss = 0.02949221
Iteration 32, loss = 0.02907929
Iteration 33, loss = 0.02849562
Iteration 34, loss = 0.02802690
Iteration 35, loss = 0.02785104
Iteration 36, loss = 0.02744101
Iteration 37, loss = 0.02726512
Iteration 38, loss = 0.02701779
Iteration 39, loss = 0.02678542
Iteration 40, loss = 0.02655011
Iteration 41, loss = 0.02632095
Iteration 42, loss = 0.02601852
Iteration 43, loss = 0.02584976
Iteration 44, loss = 0.02588259
Iteration 45, loss = 0.02557454
Iteration 46, loss = 0.02544638
Iteration 47, loss = 0.02554416
Iteration 48, loss = 0.02571561
Iteration 49, loss = 0.02500883
Iteration 50, loss = 0.02499160
Iteration 51, loss = 0.02494942
Iteration 52, loss = 0.02476894
Iteration 53, loss = 0.02484528
Iteration 54, loss = 0.02458376
Iteration 55, loss = 0.02458655
Iteration 56, loss = 0.02461451
Iteration 57, loss = 0.02458848
Iteration 58, loss = 0.02454659
Iteration 59, loss = 0.02432316
Iteration 60, loss = 0.02419246
Iteration 61, loss = 0.02406591
Iteration 62, loss = 0.02410820
Iteration 63, loss = 0.02446221
Iteration 64, loss = 0.02389383
Iteration 65, loss = 0.02406423
Iteration 66, loss = 0.02390590
Iteration 67, loss = 0.02396319
Iteration 68, loss = 0.02391275
Iteration 69, loss = 0.02403240
Iteration 70, loss = 0.02375158
Iteration 71, loss = 0.02370726
Iteration 72, loss = 0.02381179
Iteration 73, loss = 0.02372177
Iteration 74, loss = 0.02371493
Iteration 75, loss = 0.02376971
Iteration 76, loss = 0.02348443
Iteration 77, loss = 0.02337787
Iteration 78, loss = 0.02356715
Iteration 79, loss = 0.02361215
Iteration 80, loss = 0.02346901
Iteration 81, loss = 0.02389658
Iteration 82, loss = 0.02323773
Iteration 83, loss = 0.02362412
Iteration 84, loss = 0.02354524
Iteration 85, loss = 0.02350596
Iteration 86, loss = 0.02351432
Iteration 87, loss = 0.02371309
Iteration 88, loss = 0.02337041
Iteration 89, loss = 0.02320876
Iteration 90, loss = 0.02363982
Iteration 91, loss = 0.02317079
Iteration 92, loss = 0.02341842
Iteration 93, loss = 0.02322367
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67302844
Iteration 2, loss = 0.59754660
Iteration 3, loss = 0.48868972
Iteration 4, loss = 0.37787434
Iteration 5, loss = 0.28350119
Iteration 6, loss = 0.21338890
Iteration 7, loss = 0.16379244
Iteration 8, loss = 0.13003042
Iteration 9, loss = 0.10629538
Iteration 10, loss = 0.08916791
Iteration 11, loss = 0.07691788
Iteration 12, loss = 0.06741249
Iteration 13, loss = 0.06030117
Iteration 14, loss = 0.05438730
Iteration 15, loss = 0.04984868
Iteration 16, loss = 0.04622447
Iteration 17, loss = 0.04306611
Iteration 18, loss = 0.04044775
Iteration 19, loss = 0.03823924
Iteration 20, loss = 0.03662882
Iteration 21, loss = 0.03486072
Iteration 22, loss = 0.03376326
Iteration 23, loss = 0.03230904
Iteration 24, loss = 0.03147748
Iteration 25, loss = 0.03039087
Iteration 26, loss = 0.02957804
Iteration 27, loss = 0.02889826
Iteration 28, loss = 0.02830330
Iteration 29, loss = 0.02770223
Iteration 30, loss = 0.02707467
Iteration 31, loss = 0.02664289
Iteration 32, loss = 0.02645525
Iteration 33, loss = 0.02589269
Iteration 34, loss = 0.02570113
Iteration 35, loss = 0.02531316
Iteration 36, loss = 0.02494353
Iteration 37, loss = 0.02468118
Iteration 38, loss = 0.02444396
Iteration 39, loss = 0.02467073
Iteration 40, loss = 0.02397502
Iteration 41, loss = 0.02397108
Iteration 42, loss = 0.02380649
Iteration 43, loss = 0.02364905
Iteration 44, loss = 0.02344519
Iteration 45, loss = 0.02319993
Iteration 46, loss = 0.02315944
Iteration 47, loss = 0.02313717
Iteration 48, loss = 0.02294536
Iteration 49, loss = 0.02294976
Iteration 50, loss = 0.02260328
Iteration 51, loss = 0.02295513
Iteration 52, loss = 0.02276127
Iteration 53, loss = 0.02265390
Iteration 54, loss = 0.02255759
Iteration 55, loss = 0.02250514
Iteration 56, loss = 0.02233187
Iteration 57, loss = 0.02261069
Iteration 58, loss = 0.02221684
Iteration 59, loss = 0.02243883
Iteration 60, loss = 0.02241112
Iteration 61, loss = 0.02203768
Iteration 62, loss = 0.02202862
Iteration 63, loss = 0.02160115
Iteration 64, loss = 0.02191466
Iteration 65, loss = 0.02186604
Iteration 66, loss = 0.02175943
Iteration 67, loss = 0.02189494
Iteration 68, loss = 0.02184246
Iteration 69, loss = 0.02177836
Iteration 70, loss = 0.02167057
Iteration 71, loss = 0.02188390
Iteration 72, loss = 0.02173338
Iteration 73, loss = 0.02144586
Iteration 74, loss = 0.02149796
Iteration 75, loss = 0.02147333
Iteration 76, loss = 0.02138064
Iteration 77, loss = 0.02151364
Iteration 78, loss = 0.02139989
Iteration 79, loss = 0.02145162
Iteration 80, loss = 0.02133846
Iteration 81, loss = 0.02146345
Iteration 82, loss = 0.02136500
Iteration 83, loss = 0.02123996
Iteration 84, loss = 0.02155963
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67334953
Iteration 2, loss = 0.59964901
Iteration 3, loss = 0.49178018
Iteration 4, loss = 0.38301844
Iteration 5, loss = 0.29041317
Iteration 6, loss = 0.22057692
Iteration 7, loss = 0.17122587
Iteration 8, loss = 0.13712274
Iteration 9, loss = 0.11303298
Iteration 10, loss = 0.09575202
Iteration 11, loss = 0.08314022
Iteration 12, loss = 0.07313721
Iteration 13, loss = 0.06619903
Iteration 14, loss = 0.06036457
Iteration 15, loss = 0.05561995
Iteration 16, loss = 0.05204027
Iteration 17, loss = 0.04873162
Iteration 18, loss = 0.04607631
Iteration 19, loss = 0.04400260
Iteration 20, loss = 0.04211057
Iteration 21, loss = 0.04034425
Iteration 22, loss = 0.03900890
Iteration 23, loss = 0.03786066
Iteration 24, loss = 0.03696025
Iteration 25, loss = 0.03583244
Iteration 26, loss = 0.03497624
Iteration 27, loss = 0.03433167
Iteration 28, loss = 0.03382435
Iteration 29, loss = 0.03295103
Iteration 30, loss = 0.03248187
Iteration 31, loss = 0.03224030
Iteration 32, loss = 0.03178705
Iteration 33, loss = 0.03131647
Iteration 34, loss = 0.03127410
Iteration 35, loss = 0.03073179
Iteration 36, loss = 0.03023671
Iteration 37, loss = 0.03011837
Iteration 38, loss = 0.02989611
Iteration 39, loss = 0.02981144
Iteration 40, loss = 0.02945551
Iteration 41, loss = 0.02936944
Iteration 42, loss = 0.02898280
Iteration 43, loss = 0.02887624
Iteration 44, loss = 0.02871389
Iteration 45, loss = 0.02872972
Iteration 46, loss = 0.02857328
Iteration 47, loss = 0.02820012
Iteration 48, loss = 0.02836579
Iteration 49, loss = 0.02810232
Iteration 50, loss = 0.02808532
Iteration 51, loss = 0.02833331
Iteration 52, loss = 0.02817799
Iteration 53, loss = 0.02760556
Iteration 54, loss = 0.02774195
Iteration 55, loss = 0.02756850
Iteration 56, loss = 0.02759091
Iteration 57, loss = 0.02734415
Iteration 58, loss = 0.02740646
Iteration 59, loss = 0.02754761
Iteration 60, loss = 0.02737625
Iteration 61, loss = 0.02761916
Iteration 62, loss = 0.02719003
Iteration 63, loss = 0.02708135
Iteration 64, loss = 0.02714666
Iteration 65, loss = 0.02723841
Iteration 66, loss = 0.02711408
Iteration 67, loss = 0.02705038
Iteration 68, loss = 0.02671308
Iteration 69, loss = 0.02712939
Iteration 70, loss = 0.02691957
Iteration 71, loss = 0.02697344
Iteration 72, loss = 0.02682412
Iteration 73, loss = 0.02692975
Iteration 74, loss = 0.02734971
Iteration 75, loss = 0.02687484
Iteration 76, loss = 0.02679989
Iteration 77, loss = 0.02669808
Iteration 78, loss = 0.02656934
Iteration 79, loss = 0.02670883
Iteration 80, loss = 0.02674364
Iteration 81, loss = 0.02672867
Iteration 82, loss = 0.02657757
Iteration 83, loss = 0.02646723
Iteration 84, loss = 0.02679347
Iteration 85, loss = 0.02661499
Iteration 86, loss = 0.02618745
Iteration 87, loss = 0.02661719
Iteration 88, loss = 0.02653431
Iteration 89, loss = 0.02653934
Iteration 90, loss = 0.02620281
Iteration 91, loss = 0.02613446
Iteration 92, loss = 0.02637854
Iteration 93, loss = 0.02651993
Iteration 94, loss = 0.02628271
Iteration 95, loss = 0.02634772
Iteration 96, loss = 0.02594174
Iteration 97, loss = 0.02643394
Iteration 98, loss = 0.02652347
Iteration 99, loss = 0.02648562
Iteration 100, loss = 0.02615704
Iteration 101, loss = 0.02625797
Iteration 102, loss = 0.02594420
Iteration 103, loss = 0.02610010
Iteration 104, loss = 0.02610330
Iteration 105, loss = 0.02596964
Iteration 106, loss = 0.02613408
Iteration 107, loss = 0.02610113
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70350959
Iteration 2, loss = 0.63409378
Iteration 3, loss = 0.53529211
Iteration 4, loss = 0.41679485
Iteration 5, loss = 0.31106772
Iteration 6, loss = 0.23182578
Iteration 7, loss = 0.17622010
Iteration 8, loss = 0.13857233
Iteration 9, loss = 0.11242348
Iteration 10, loss = 0.09406362
Iteration 11, loss = 0.08096443
Iteration 12, loss = 0.07119863
Iteration 13, loss = 0.06380031
Iteration 14, loss = 0.05805095
Iteration 15, loss = 0.05322968
Iteration 16, loss = 0.04939364
Iteration 17, loss = 0.04634207
Iteration 18, loss = 0.04403201
Iteration 19, loss = 0.04172991
Iteration 20, loss = 0.03960219
Iteration 21, loss = 0.03813552
Iteration 22, loss = 0.03687666
Iteration 23, loss = 0.03580621
Iteration 24, loss = 0.03465400
Iteration 25, loss = 0.03380834
Iteration 26, loss = 0.03300482
Iteration 27, loss = 0.03222678
Iteration 28, loss = 0.03163924
Iteration 29, loss = 0.03096853
Iteration 30, loss = 0.03072714
Iteration 31, loss = 0.03003192
Iteration 32, loss = 0.02968451
Iteration 33, loss = 0.02924006
Iteration 34, loss = 0.02906539
Iteration 35, loss = 0.02865444
Iteration 36, loss = 0.02878224
Iteration 37, loss = 0.02789882
Iteration 38, loss = 0.02817093
Iteration 39, loss = 0.02779920
Iteration 40, loss = 0.02759821
Iteration 41, loss = 0.02742772
Iteration 42, loss = 0.02716443
Iteration 43, loss = 0.02683048
Iteration 44, loss = 0.02681988
Iteration 45, loss = 0.02673080
Iteration 46, loss = 0.02652627
Iteration 47, loss = 0.02649348
Iteration 48, loss = 0.02639717
Iteration 49, loss = 0.02620961
Iteration 50, loss = 0.02621841
Iteration 51, loss = 0.02573856
Iteration 52, loss = 0.02576134
Iteration 53, loss = 0.02596619
Iteration 54, loss = 0.02590715
Iteration 55, loss = 0.02575479
Iteration 56, loss = 0.02575813
Iteration 57, loss = 0.02563366
Iteration 58, loss = 0.02552980
Iteration 59, loss = 0.02547231
Iteration 60, loss = 0.02571548
Iteration 61, loss = 0.02551804
Iteration 62, loss = 0.02539546
Iteration 63, loss = 0.02533185
Iteration 64, loss = 0.02516519
Iteration 65, loss = 0.02523308
Iteration 66, loss = 0.02516041
Iteration 67, loss = 0.02536974
Iteration 68, loss = 0.02525138
Iteration 69, loss = 0.02511220
Iteration 70, loss = 0.02504969
Iteration 71, loss = 0.02487385
Iteration 72, loss = 0.02493391
Iteration 73, loss = 0.02503634
Iteration 74, loss = 0.02486270
Iteration 75, loss = 0.02489585
Iteration 76, loss = 0.02477578
Iteration 77, loss = 0.02466938
Iteration 78, loss = 0.02492619
Iteration 79, loss = 0.02475160
Iteration 80, loss = 0.02479159
Iteration 81, loss = 0.02487174
Iteration 82, loss = 0.02460489
Iteration 83, loss = 0.02472777
Iteration 84, loss = 0.02441009
Iteration 85, loss = 0.02462267
Iteration 86, loss = 0.02453846
Iteration 87, loss = 0.02456589
Iteration 88, loss = 0.02485225
Iteration 89, loss = 0.02453342
Iteration 90, loss = 0.02436249
Iteration 91, loss = 0.02446430
Iteration 92, loss = 0.02460683
Iteration 93, loss = 0.02462724
Iteration 94, loss = 0.02459139
Iteration 95, loss = 0.02456664
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67319437
Iteration 2, loss = 0.59940226
Iteration 3, loss = 0.49050904
Iteration 4, loss = 0.38106647
Iteration 5, loss = 0.29012542
Iteration 6, loss = 0.22122864
Iteration 7, loss = 0.17250240
Iteration 8, loss = 0.13804726
Iteration 9, loss = 0.11363285
Iteration 10, loss = 0.09625173
Iteration 11, loss = 0.08308133
Iteration 12, loss = 0.07360149
Iteration 13, loss = 0.06593208
Iteration 14, loss = 0.06019085
Iteration 15, loss = 0.05535468
Iteration 16, loss = 0.05181440
Iteration 17, loss = 0.04869939
Iteration 18, loss = 0.04612173
Iteration 19, loss = 0.04386513
Iteration 20, loss = 0.04203160
Iteration 21, loss = 0.04043755
Iteration 22, loss = 0.03883099
Iteration 23, loss = 0.03770666
Iteration 24, loss = 0.03648912
Iteration 25, loss = 0.03576482
Iteration 26, loss = 0.03479430
Iteration 27, loss = 0.03460467
Iteration 28, loss = 0.03367056
Iteration 29, loss = 0.03311503
Iteration 30, loss = 0.03272614
Iteration 31, loss = 0.03211417
Iteration 32, loss = 0.03200377
Iteration 33, loss = 0.03167259
Iteration 34, loss = 0.03107409
Iteration 35, loss = 0.03092821
Iteration 36, loss = 0.03079538
Iteration 37, loss = 0.03010579
Iteration 38, loss = 0.03003095
Iteration 39, loss = 0.02979901
Iteration 40, loss = 0.02937311
Iteration 41, loss = 0.02943506
Iteration 42, loss = 0.02927880
Iteration 43, loss = 0.02940746
Iteration 44, loss = 0.02947701
Iteration 45, loss = 0.02871881
Iteration 46, loss = 0.02867990
Iteration 47, loss = 0.02861420
Iteration 48, loss = 0.02833882
Iteration 49, loss = 0.02834670
Iteration 50, loss = 0.02820899
Iteration 51, loss = 0.02827352
Iteration 52, loss = 0.02825833
Iteration 53, loss = 0.02794410
Iteration 54, loss = 0.02770731
Iteration 55, loss = 0.02819988
Iteration 56, loss = 0.02782782
Iteration 57, loss = 0.02756505
Iteration 58, loss = 0.02772230
Iteration 59, loss = 0.02764805
Iteration 60, loss = 0.02757855
Iteration 61, loss = 0.02742337
Iteration 62, loss = 0.02735847
Iteration 63, loss = 0.02749360
Iteration 64, loss = 0.02757960
Iteration 65, loss = 0.02758257
Iteration 66, loss = 0.02708319
Iteration 67, loss = 0.02725240
Iteration 68, loss = 0.02721464
Iteration 69, loss = 0.02737860
Iteration 70, loss = 0.02733892
Iteration 71, loss = 0.02709446
Iteration 72, loss = 0.02711366
Iteration 73, loss = 0.02735974
Iteration 74, loss = 0.02724183
Iteration 75, loss = 0.02695650
Iteration 76, loss = 0.02689685
Iteration 77, loss = 0.02694560
Iteration 78, loss = 0.02729435
Iteration 79, loss = 0.02690746
Iteration 80, loss = 0.02681660
Iteration 81, loss = 0.02692310
Iteration 82, loss = 0.02657753
Iteration 83, loss = 0.02676489
Iteration 84, loss = 0.02711265
Iteration 85, loss = 0.02693454
Iteration 86, loss = 0.02697994
Iteration 87, loss = 0.02698188
Iteration 88, loss = 0.02669465
Iteration 89, loss = 0.02665258
Iteration 90, loss = 0.02669688
Iteration 91, loss = 0.02660853
Iteration 92, loss = 0.02680904
Iteration 93, loss = 0.02698565
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67508086
Iteration 2, loss = 0.60856440
Iteration 3, loss = 0.50565330
Iteration 4, loss = 0.39541598
Iteration 5, loss = 0.30027590
Iteration 6, loss = 0.22779266
Iteration 7, loss = 0.17596697
Iteration 8, loss = 0.13980122
Iteration 9, loss = 0.11406456
Iteration 10, loss = 0.09582464
Iteration 11, loss = 0.08243451
Iteration 12, loss = 0.07234315
Iteration 13, loss = 0.06460186
Iteration 14, loss = 0.05874645
Iteration 15, loss = 0.05384640
Iteration 16, loss = 0.04993292
Iteration 17, loss = 0.04652906
Iteration 18, loss = 0.04384806
Iteration 19, loss = 0.04133451
Iteration 20, loss = 0.03946586
Iteration 21, loss = 0.03808146
Iteration 22, loss = 0.03640482
Iteration 23, loss = 0.03517489
Iteration 24, loss = 0.03439451
Iteration 25, loss = 0.03321606
Iteration 26, loss = 0.03239582
Iteration 27, loss = 0.03163009
Iteration 28, loss = 0.03100716
Iteration 29, loss = 0.03032232
Iteration 30, loss = 0.02971530
Iteration 31, loss = 0.02949221
Iteration 32, loss = 0.02907929
Iteration 33, loss = 0.02849562
Iteration 34, loss = 0.02802690
Iteration 35, loss = 0.02785104
Iteration 36, loss = 0.02744101
Iteration 37, loss = 0.02726512
Iteration 38, loss = 0.02701779
Iteration 39, loss = 0.02678542
Iteration 40, loss = 0.02655011
Iteration 41, loss = 0.02632095
Iteration 42, loss = 0.02601852
Iteration 43, loss = 0.02584976
Iteration 44, loss = 0.02588259
Iteration 45, loss = 0.02557454
Iteration 46, loss = 0.02544638
Iteration 47, loss = 0.02554416
Iteration 48, loss = 0.02571561
Iteration 49, loss = 0.02500883
Iteration 50, loss = 0.02499160
Iteration 51, loss = 0.02494942
Iteration 52, loss = 0.02476894
Iteration 53, loss = 0.02484528
Iteration 54, loss = 0.02458376
Iteration 55, loss = 0.02458655
Iteration 56, loss = 0.02461451
Iteration 57, loss = 0.02458848
Iteration 58, loss = 0.02454659
Iteration 59, loss = 0.02432316
Iteration 60, loss = 0.02419246
Iteration 61, loss = 0.02406591
Iteration 62, loss = 0.02410820
Iteration 63, loss = 0.02446221
Iteration 64, loss = 0.02389383
Iteration 65, loss = 0.02406423
Iteration 66, loss = 0.02390590
Iteration 67, loss = 0.02396319
Iteration 68, loss = 0.02391275
Iteration 69, loss = 0.02403240
Iteration 70, loss = 0.02375158
Iteration 71, loss = 0.02370726
Iteration 72, loss = 0.02381179
Iteration 73, loss = 0.02372177
Iteration 74, loss = 0.02371493
Iteration 75, loss = 0.02376971
Iteration 76, loss = 0.02348443
Iteration 77, loss = 0.02337787
Iteration 78, loss = 0.02356715
Iteration 79, loss = 0.02361215
Iteration 80, loss = 0.02346901
Iteration 81, loss = 0.02389658
Iteration 82, loss = 0.02323773
Iteration 83, loss = 0.02362412
Iteration 84, loss = 0.02354524
Iteration 85, loss = 0.02350596
Iteration 86, loss = 0.02351432
Iteration 87, loss = 0.02371309
Iteration 88, loss = 0.02337041
Iteration 89, loss = 0.02320876
Iteration 90, loss = 0.02363982
Iteration 91, loss = 0.02317079
Iteration 92, loss = 0.02341842
Iteration 93, loss = 0.02322367
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67302844
Iteration 2, loss = 0.59754660
Iteration 3, loss = 0.48868972
Iteration 4, loss = 0.37787434
Iteration 5, loss = 0.28350119
Iteration 6, loss = 0.21338890
Iteration 7, loss = 0.16379244
Iteration 8, loss = 0.13003042
Iteration 9, loss = 0.10629538
Iteration 10, loss = 0.08916791
Iteration 11, loss = 0.07691788
Iteration 12, loss = 0.06741249
Iteration 13, loss = 0.06030117
Iteration 14, loss = 0.05438730
Iteration 15, loss = 0.04984868
Iteration 16, loss = 0.04622447
Iteration 17, loss = 0.04306611
Iteration 18, loss = 0.04044775
Iteration 19, loss = 0.03823924
Iteration 20, loss = 0.03662882
Iteration 21, loss = 0.03486072
Iteration 22, loss = 0.03376326
Iteration 23, loss = 0.03230904
Iteration 24, loss = 0.03147748
Iteration 25, loss = 0.03039087
Iteration 26, loss = 0.02957804
Iteration 27, loss = 0.02889826
Iteration 28, loss = 0.02830330
Iteration 29, loss = 0.02770223
Iteration 30, loss = 0.02707467
Iteration 31, loss = 0.02664289
Iteration 32, loss = 0.02645525
Iteration 33, loss = 0.02589269
Iteration 34, loss = 0.02570113
Iteration 35, loss = 0.02531316
Iteration 36, loss = 0.02494353
Iteration 37, loss = 0.02468118
Iteration 38, loss = 0.02444396
Iteration 39, loss = 0.02467073
Iteration 40, loss = 0.02397502
Iteration 41, loss = 0.02397108
Iteration 42, loss = 0.02380649
Iteration 43, loss = 0.02364905
Iteration 44, loss = 0.02344519
Iteration 45, loss = 0.02319993
Iteration 46, loss = 0.02315944
Iteration 47, loss = 0.02313717
Iteration 48, loss = 0.02294536
Iteration 49, loss = 0.02294976
Iteration 50, loss = 0.02260328
Iteration 51, loss = 0.02295513
Iteration 52, loss = 0.02276127
Iteration 53, loss = 0.02265390
Iteration 54, loss = 0.02255759
Iteration 55, loss = 0.02250514
Iteration 56, loss = 0.02233187
Iteration 57, loss = 0.02261069
Iteration 58, loss = 0.02221684
Iteration 59, loss = 0.02243883
Iteration 60, loss = 0.02241112
Iteration 61, loss = 0.02203768
Iteration 62, loss = 0.02202862
Iteration 63, loss = 0.02160115
Iteration 64, loss = 0.02191466
Iteration 65, loss = 0.02186604
Iteration 66, loss = 0.02175943
Iteration 67, loss = 0.02189494
Iteration 68, loss = 0.02184246
Iteration 69, loss = 0.02177836
Iteration 70, loss = 0.02167057
Iteration 71, loss = 0.02188390
Iteration 72, loss = 0.02173338
Iteration 73, loss = 0.02144586
Iteration 74, loss = 0.02149796
Iteration 75, loss = 0.02147333
Iteration 76, loss = 0.02138064
Iteration 77, loss = 0.02151364
Iteration 78, loss = 0.02139989
Iteration 79, loss = 0.02145162
Iteration 80, loss = 0.02133846
Iteration 81, loss = 0.02146345
Iteration 82, loss = 0.02136500
Iteration 83, loss = 0.02123996
Iteration 84, loss = 0.02155963
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67334953
Iteration 2, loss = 0.59964901
Iteration 3, loss = 0.49178018
Iteration 4, loss = 0.38301844
Iteration 5, loss = 0.29041317
Iteration 6, loss = 0.22057692
Iteration 7, loss = 0.17122587
Iteration 8, loss = 0.13712274
Iteration 9, loss = 0.11303298
Iteration 10, loss = 0.09575202
Iteration 11, loss = 0.08314022
Iteration 12, loss = 0.07313721
Iteration 13, loss = 0.06619903
Iteration 14, loss = 0.06036457
Iteration 15, loss = 0.05561995
Iteration 16, loss = 0.05204027
Iteration 17, loss = 0.04873162
Iteration 18, loss = 0.04607631
Iteration 19, loss = 0.04400260
Iteration 20, loss = 0.04211057
Iteration 21, loss = 0.04034425
Iteration 22, loss = 0.03900890
Iteration 23, loss = 0.03786066
Iteration 24, loss = 0.03696025
Iteration 25, loss = 0.03583244
Iteration 26, loss = 0.03497624
Iteration 27, loss = 0.03433167
Iteration 28, loss = 0.03382435
Iteration 29, loss = 0.03295103
Iteration 30, loss = 0.03248187
Iteration 31, loss = 0.03224030
Iteration 32, loss = 0.03178705
Iteration 33, loss = 0.03131647
Iteration 34, loss = 0.03127410
Iteration 35, loss = 0.03073179
Iteration 36, loss = 0.03023671
Iteration 37, loss = 0.03011837
Iteration 38, loss = 0.02989611
Iteration 39, loss = 0.02981144
Iteration 40, loss = 0.02945551
Iteration 41, loss = 0.02936944
Iteration 42, loss = 0.02898280
Iteration 43, loss = 0.02887624
Iteration 44, loss = 0.02871389
Iteration 45, loss = 0.02872972
Iteration 46, loss = 0.02857328
Iteration 47, loss = 0.02820012
Iteration 48, loss = 0.02836579
Iteration 49, loss = 0.02810232
Iteration 50, loss = 0.02808532
Iteration 51, loss = 0.02833331
Iteration 52, loss = 0.02817799
Iteration 53, loss = 0.02760556
Iteration 54, loss = 0.02774195
Iteration 55, loss = 0.02756850
Iteration 56, loss = 0.02759091
Iteration 57, loss = 0.02734415
Iteration 58, loss = 0.02740646
Iteration 59, loss = 0.02754761
Iteration 60, loss = 0.02737625
Iteration 61, loss = 0.02761916
Iteration 62, loss = 0.02719003
Iteration 63, loss = 0.02708135
Iteration 64, loss = 0.02714666
Iteration 65, loss = 0.02723841
Iteration 66, loss = 0.02711408
Iteration 67, loss = 0.02705038
Iteration 68, loss = 0.02671308
Iteration 69, loss = 0.02712939
Iteration 70, loss = 0.02691957
Iteration 71, loss = 0.02697344
Iteration 72, loss = 0.02682412
Iteration 73, loss = 0.02692975
Iteration 74, loss = 0.02734971
Iteration 75, loss = 0.02687484
Iteration 76, loss = 0.02679989
Iteration 77, loss = 0.02669808
Iteration 78, loss = 0.02656934
Iteration 79, loss = 0.02670883
Iteration 80, loss = 0.02674364
Iteration 81, loss = 0.02672867
Iteration 82, loss = 0.02657757
Iteration 83, loss = 0.02646723
Iteration 84, loss = 0.02679347
Iteration 85, loss = 0.02661499
Iteration 86, loss = 0.02618745
Iteration 87, loss = 0.02661719
Iteration 88, loss = 0.02653431
Iteration 89, loss = 0.02653934
Iteration 90, loss = 0.02620281
Iteration 91, loss = 0.02613446
Iteration 92, loss = 0.02637854
Iteration 93, loss = 0.02651993
Iteration 94, loss = 0.02628271
Iteration 95, loss = 0.02634772
Iteration 96, loss = 0.02594174
Iteration 97, loss = 0.02643394
Iteration 98, loss = 0.02652347
Iteration 99, loss = 0.02648562
Iteration 100, loss = 0.02615704
Iteration 101, loss = 0.02625797
Iteration 102, loss = 0.02594420
Iteration 103, loss = 0.02610010
Iteration 104, loss = 0.02610330
Iteration 105, loss = 0.02596964
Iteration 106, loss = 0.02613408
Iteration 107, loss = 0.02610113
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70350959
Iteration 2, loss = 0.63409378
Iteration 3, loss = 0.53529211
Iteration 4, loss = 0.41679485
Iteration 5, loss = 0.31106772
Iteration 6, loss = 0.23182578
Iteration 7, loss = 0.17622010
Iteration 8, loss = 0.13857233
Iteration 9, loss = 0.11242348
Iteration 10, loss = 0.09406362
Iteration 11, loss = 0.08096443
Iteration 12, loss = 0.07119863
Iteration 13, loss = 0.06380031
Iteration 14, loss = 0.05805095
Iteration 15, loss = 0.05322968
Iteration 16, loss = 0.04939364
Iteration 17, loss = 0.04634207
Iteration 18, loss = 0.04403201
Iteration 19, loss = 0.04172991
Iteration 20, loss = 0.03960219
Iteration 21, loss = 0.03813552
Iteration 22, loss = 0.03687666
Iteration 23, loss = 0.03580621
Iteration 24, loss = 0.03465400
Iteration 25, loss = 0.03380834
Iteration 26, loss = 0.03300482
Iteration 27, loss = 0.03222678
Iteration 28, loss = 0.03163924
Iteration 29, loss = 0.03096853
Iteration 30, loss = 0.03072714
Iteration 31, loss = 0.03003192
Iteration 32, loss = 0.02968451
Iteration 33, loss = 0.02924006
Iteration 34, loss = 0.02906539
Iteration 35, loss = 0.02865444
Iteration 36, loss = 0.02878224
Iteration 37, loss = 0.02789882
Iteration 38, loss = 0.02817093
Iteration 39, loss = 0.02779920
Iteration 40, loss = 0.02759821
Iteration 41, loss = 0.02742772
Iteration 42, loss = 0.02716443
Iteration 43, loss = 0.02683048
Iteration 44, loss = 0.02681988
Iteration 45, loss = 0.02673080
Iteration 46, loss = 0.02652627
Iteration 47, loss = 0.02649348
Iteration 48, loss = 0.02639717
Iteration 49, loss = 0.02620961
Iteration 50, loss = 0.02621841
Iteration 51, loss = 0.02573856
Iteration 52, loss = 0.02576134
Iteration 53, loss = 0.02596619
Iteration 54, loss = 0.02590715
Iteration 55, loss = 0.02575479
Iteration 56, loss = 0.02575813
Iteration 57, loss = 0.02563366
Iteration 58, loss = 0.02552980
Iteration 59, loss = 0.02547231
Iteration 60, loss = 0.02571548
Iteration 61, loss = 0.02551804
Iteration 62, loss = 0.02539546
Iteration 63, loss = 0.02533185
Iteration 64, loss = 0.02516519
Iteration 65, loss = 0.02523308
Iteration 66, loss = 0.02516041
Iteration 67, loss = 0.02536974
Iteration 68, loss = 0.02525138
Iteration 69, loss = 0.02511220
Iteration 70, loss = 0.02504969
Iteration 71, loss = 0.02487385
Iteration 72, loss = 0.02493391
Iteration 73, loss = 0.02503634
Iteration 74, loss = 0.02486270
Iteration 75, loss = 0.02489585
Iteration 76, loss = 0.02477578
Iteration 77, loss = 0.02466938
Iteration 78, loss = 0.02492619
Iteration 79, loss = 0.02475160
Iteration 80, loss = 0.02479159
Iteration 81, loss = 0.02487174
Iteration 82, loss = 0.02460489
Iteration 83, loss = 0.02472777
Iteration 84, loss = 0.02441009
Iteration 85, loss = 0.02462267
Iteration 86, loss = 0.02453846
Iteration 87, loss = 0.02456589
Iteration 88, loss = 0.02485225
Iteration 89, loss = 0.02453342
Iteration 90, loss = 0.02436249
Iteration 91, loss = 0.02446430
Iteration 92, loss = 0.02460683
Iteration 93, loss = 0.02462724
Iteration 94, loss = 0.02459139
Iteration 95, loss = 0.02456664
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67319437
Iteration 2, loss = 0.59940226
Iteration 3, loss = 0.49050904
Iteration 4, loss = 0.38106647
Iteration 5, loss = 0.29012542
Iteration 6, loss = 0.22122864
Iteration 7, loss = 0.17250240
Iteration 8, loss = 0.13804726
Iteration 9, loss = 0.11363285
Iteration 10, loss = 0.09625173
Iteration 11, loss = 0.08308133
Iteration 12, loss = 0.07360149
Iteration 13, loss = 0.06593208
Iteration 14, loss = 0.06019085
Iteration 15, loss = 0.05535468
Iteration 16, loss = 0.05181440
Iteration 17, loss = 0.04869939
Iteration 18, loss = 0.04612173
Iteration 19, loss = 0.04386513
Iteration 20, loss = 0.04203160
Iteration 21, loss = 0.04043755
Iteration 22, loss = 0.03883099
Iteration 23, loss = 0.03770666
Iteration 24, loss = 0.03648912
Iteration 25, loss = 0.03576482
Iteration 26, loss = 0.03479430
Iteration 27, loss = 0.03460467
Iteration 28, loss = 0.03367056
Iteration 29, loss = 0.03311503
Iteration 30, loss = 0.03272614
Iteration 31, loss = 0.03211417
Iteration 32, loss = 0.03200377
Iteration 33, loss = 0.03167259
Iteration 34, loss = 0.03107409
Iteration 35, loss = 0.03092821
Iteration 36, loss = 0.03079538
Iteration 37, loss = 0.03010579
Iteration 38, loss = 0.03003095
Iteration 39, loss = 0.02979901
Iteration 40, loss = 0.02937311
Iteration 41, loss = 0.02943506
Iteration 42, loss = 0.02927880
Iteration 43, loss = 0.02940746
Iteration 44, loss = 0.02947701
Iteration 45, loss = 0.02871881
Iteration 46, loss = 0.02867990
Iteration 47, loss = 0.02861420
Iteration 48, loss = 0.02833882
Iteration 49, loss = 0.02834670
Iteration 50, loss = 0.02820899
Iteration 51, loss = 0.02827352
Iteration 52, loss = 0.02825833
Iteration 53, loss = 0.02794410
Iteration 54, loss = 0.02770731
Iteration 55, loss = 0.02819988
Iteration 56, loss = 0.02782782
Iteration 57, loss = 0.02756505
Iteration 58, loss = 0.02772230
Iteration 59, loss = 0.02764805
Iteration 60, loss = 0.02757855
Iteration 61, loss = 0.02742337
Iteration 62, loss = 0.02735847
Iteration 63, loss = 0.02749360
Iteration 64, loss = 0.02757960
Iteration 65, loss = 0.02758257
Iteration 66, loss = 0.02708319
Iteration 67, loss = 0.02725240
Iteration 68, loss = 0.02721464
Iteration 69, loss = 0.02737860
Iteration 70, loss = 0.02733892
Iteration 71, loss = 0.02709446
Iteration 72, loss = 0.02711366
Iteration 73, loss = 0.02735974
Iteration 74, loss = 0.02724183
Iteration 75, loss = 0.02695650
Iteration 76, loss = 0.02689685
Iteration 77, loss = 0.02694560
Iteration 78, loss = 0.02729435
Iteration 79, loss = 0.02690746
Iteration 80, loss = 0.02681660
Iteration 81, loss = 0.02692310
Iteration 82, loss = 0.02657753
Iteration 83, loss = 0.02676489
Iteration 84, loss = 0.02711265
Iteration 85, loss = 0.02693454
Iteration 86, loss = 0.02697994
Iteration 87, loss = 0.02698188
Iteration 88, loss = 0.02669465
Iteration 89, loss = 0.02665258
Iteration 90, loss = 0.02669688
Iteration 91, loss = 0.02660853
Iteration 92, loss = 0.02680904
Iteration 93, loss = 0.02698565
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67508086
Iteration 2, loss = 0.60856440
Iteration 3, loss = 0.50565330
Iteration 4, loss = 0.39541598
Iteration 5, loss = 0.30027590
Iteration 6, loss = 0.22779266
Iteration 7, loss = 0.17596697
Iteration 8, loss = 0.13980122
Iteration 9, loss = 0.11406456
Iteration 10, loss = 0.09582464
Iteration 11, loss = 0.08243451
Iteration 12, loss = 0.07234315
Iteration 13, loss = 0.06460186
Iteration 14, loss = 0.05874645
Iteration 15, loss = 0.05384640
Iteration 16, loss = 0.04993292
Iteration 17, loss = 0.04652906
Iteration 18, loss = 0.04384806
Iteration 19, loss = 0.04133451
Iteration 20, loss = 0.03946586
Iteration 21, loss = 0.03808146
Iteration 22, loss = 0.03640482
Iteration 23, loss = 0.03517489
Iteration 24, loss = 0.03439451
Iteration 25, loss = 0.03321606
Iteration 26, loss = 0.03239582
Iteration 27, loss = 0.03163009
Iteration 28, loss = 0.03100716
Iteration 29, loss = 0.03032232
Iteration 30, loss = 0.02971530
Iteration 31, loss = 0.02949221
Iteration 32, loss = 0.02907929
Iteration 33, loss = 0.02849562
Iteration 34, loss = 0.02802690
Iteration 35, loss = 0.02785104
Iteration 36, loss = 0.02744101
Iteration 37, loss = 0.02726512
Iteration 38, loss = 0.02701779
Iteration 39, loss = 0.02678542
Iteration 40, loss = 0.02655011
Iteration 41, loss = 0.02632095
Iteration 42, loss = 0.02601852
Iteration 43, loss = 0.02584976
Iteration 44, loss = 0.02588259
Iteration 45, loss = 0.02557454
Iteration 46, loss = 0.02544638
Iteration 47, loss = 0.02554416
Iteration 48, loss = 0.02571561
Iteration 49, loss = 0.02500883
Iteration 50, loss = 0.02499160
Iteration 51, loss = 0.02494942
Iteration 52, loss = 0.02476894
Iteration 53, loss = 0.02484528
Iteration 54, loss = 0.02458376
Iteration 55, loss = 0.02458655
Iteration 56, loss = 0.02461451
Iteration 57, loss = 0.02458848
Iteration 58, loss = 0.02454659
Iteration 59, loss = 0.02432316
Iteration 60, loss = 0.02419246
Iteration 61, loss = 0.02406591
Iteration 62, loss = 0.02410820
Iteration 63, loss = 0.02446221
Iteration 64, loss = 0.02389383
Iteration 65, loss = 0.02406423
Iteration 66, loss = 0.02390590
Iteration 67, loss = 0.02396319
Iteration 68, loss = 0.02391275
Iteration 69, loss = 0.02403240
Iteration 70, loss = 0.02375158
Iteration 71, loss = 0.02370726
Iteration 72, loss = 0.02381179
Iteration 73, loss = 0.02372177
Iteration 74, loss = 0.02371493
Iteration 75, loss = 0.02376971
Iteration 76, loss = 0.02348443
Iteration 77, loss = 0.02337787
Iteration 78, loss = 0.02356715
Iteration 79, loss = 0.02361215
Iteration 80, loss = 0.02346901
Iteration 81, loss = 0.02389658
Iteration 82, loss = 0.02323773
Iteration 83, loss = 0.02362412
Iteration 84, loss = 0.02354524
Iteration 85, loss = 0.02350596
Iteration 86, loss = 0.02351432
Iteration 87, loss = 0.02371309
Iteration 88, loss = 0.02337041
Iteration 89, loss = 0.02320876
Iteration 90, loss = 0.02363982
Iteration 91, loss = 0.02317079
Iteration 92, loss = 0.02341842
Iteration 93, loss = 0.02322367
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67302844
Iteration 2, loss = 0.59754660
Iteration 3, loss = 0.48868972
Iteration 4, loss = 0.37787434
Iteration 5, loss = 0.28350119
Iteration 6, loss = 0.21338890
Iteration 7, loss = 0.16379244
Iteration 8, loss = 0.13003042
Iteration 9, loss = 0.10629538
Iteration 10, loss = 0.08916791
Iteration 11, loss = 0.07691788
Iteration 12, loss = 0.06741249
Iteration 13, loss = 0.06030117
Iteration 14, loss = 0.05438730
Iteration 15, loss = 0.04984868
Iteration 16, loss = 0.04622447
Iteration 17, loss = 0.04306611
Iteration 18, loss = 0.04044775
Iteration 19, loss = 0.03823924
Iteration 20, loss = 0.03662882
Iteration 21, loss = 0.03486072
Iteration 22, loss = 0.03376326
Iteration 23, loss = 0.03230904
Iteration 24, loss = 0.03147748
Iteration 25, loss = 0.03039087
Iteration 26, loss = 0.02957804
Iteration 27, loss = 0.02889826
Iteration 28, loss = 0.02830330
Iteration 29, loss = 0.02770223
Iteration 30, loss = 0.02707467
Iteration 31, loss = 0.02664289
Iteration 32, loss = 0.02645525
Iteration 33, loss = 0.02589269
Iteration 34, loss = 0.02570113
Iteration 35, loss = 0.02531316
Iteration 36, loss = 0.02494353
Iteration 37, loss = 0.02468118
Iteration 38, loss = 0.02444396
Iteration 39, loss = 0.02467073
Iteration 40, loss = 0.02397502
Iteration 41, loss = 0.02397108
Iteration 42, loss = 0.02380649
Iteration 43, loss = 0.02364905
Iteration 44, loss = 0.02344519
Iteration 45, loss = 0.02319993
Iteration 46, loss = 0.02315944
Iteration 47, loss = 0.02313717
Iteration 48, loss = 0.02294536
Iteration 49, loss = 0.02294976
Iteration 50, loss = 0.02260328
Iteration 51, loss = 0.02295513
Iteration 52, loss = 0.02276127
Iteration 53, loss = 0.02265390
Iteration 54, loss = 0.02255759
Iteration 55, loss = 0.02250514
Iteration 56, loss = 0.02233187
Iteration 57, loss = 0.02261069
Iteration 58, loss = 0.02221684
Iteration 59, loss = 0.02243883
Iteration 60, loss = 0.02241112
Iteration 61, loss = 0.02203768
Iteration 62, loss = 0.02202862
Iteration 63, loss = 0.02160115
Iteration 64, loss = 0.02191466
Iteration 65, loss = 0.02186604
Iteration 66, loss = 0.02175943
Iteration 67, loss = 0.02189494
Iteration 68, loss = 0.02184246
Iteration 69, loss = 0.02177836
Iteration 70, loss = 0.02167057
Iteration 71, loss = 0.02188390
Iteration 72, loss = 0.02173338
Iteration 73, loss = 0.02144586
Iteration 74, loss = 0.02149796
Iteration 75, loss = 0.02147333
Iteration 76, loss = 0.02138064
Iteration 77, loss = 0.02151364
Iteration 78, loss = 0.02139989
Iteration 79, loss = 0.02145162
Iteration 80, loss = 0.02133846
Iteration 81, loss = 0.02146345
Iteration 82, loss = 0.02136500
Iteration 83, loss = 0.02123996
Iteration 84, loss = 0.02155963
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67334953
Iteration 2, loss = 0.59964901
Iteration 3, loss = 0.49178018
Iteration 4, loss = 0.38301844
Iteration 5, loss = 0.29041317
Iteration 6, loss = 0.22057692
Iteration 7, loss = 0.17122587
Iteration 8, loss = 0.13712274
Iteration 9, loss = 0.11303298
Iteration 10, loss = 0.09575202
Iteration 11, loss = 0.08314022
Iteration 12, loss = 0.07313721
Iteration 13, loss = 0.06619903
Iteration 14, loss = 0.06036457
Iteration 15, loss = 0.05561995
Iteration 16, loss = 0.05204027
Iteration 17, loss = 0.04873162
Iteration 18, loss = 0.04607631
Iteration 19, loss = 0.04400260
Iteration 20, loss = 0.04211057
Iteration 21, loss = 0.04034425
Iteration 22, loss = 0.03900890
Iteration 23, loss = 0.03786066
Iteration 24, loss = 0.03696025
Iteration 25, loss = 0.03583244
Iteration 26, loss = 0.03497624
Iteration 27, loss = 0.03433167
Iteration 28, loss = 0.03382435
Iteration 29, loss = 0.03295103
Iteration 30, loss = 0.03248187
Iteration 31, loss = 0.03224030
Iteration 32, loss = 0.03178705
Iteration 33, loss = 0.03131647
Iteration 34, loss = 0.03127410
Iteration 35, loss = 0.03073179
Iteration 36, loss = 0.03023671
Iteration 37, loss = 0.03011837
Iteration 38, loss = 0.02989611
Iteration 39, loss = 0.02981144
Iteration 40, loss = 0.02945551
Iteration 41, loss = 0.02936944
Iteration 42, loss = 0.02898280
Iteration 43, loss = 0.02887624
Iteration 44, loss = 0.02871389
Iteration 45, loss = 0.02872972
Iteration 46, loss = 0.02857328
Iteration 47, loss = 0.02820012
Iteration 48, loss = 0.02836579
Iteration 49, loss = 0.02810232
Iteration 50, loss = 0.02808532
Iteration 51, loss = 0.02833331
Iteration 52, loss = 0.02817799
Iteration 53, loss = 0.02760556
Iteration 54, loss = 0.02774195
Iteration 55, loss = 0.02756850
Iteration 56, loss = 0.02759091
Iteration 57, loss = 0.02734415
Iteration 58, loss = 0.02740646
Iteration 59, loss = 0.02754761
Iteration 60, loss = 0.02737625
Iteration 61, loss = 0.02761916
Iteration 62, loss = 0.02719003
Iteration 63, loss = 0.02708135
Iteration 64, loss = 0.02714666
Iteration 65, loss = 0.02723841
Iteration 66, loss = 0.02711408
Iteration 67, loss = 0.02705038
Iteration 68, loss = 0.02671308
Iteration 69, loss = 0.02712939
Iteration 70, loss = 0.02691957
Iteration 71, loss = 0.02697344
Iteration 72, loss = 0.02682412
Iteration 73, loss = 0.02692975
Iteration 74, loss = 0.02734971
Iteration 75, loss = 0.02687484
Iteration 76, loss = 0.02679989
Iteration 77, loss = 0.02669808
Iteration 78, loss = 0.02656934
Iteration 79, loss = 0.02670883
Iteration 80, loss = 0.02674364
Iteration 81, loss = 0.02672867
Iteration 82, loss = 0.02657757
Iteration 83, loss = 0.02646723
Iteration 84, loss = 0.02679347
Iteration 85, loss = 0.02661499
Iteration 86, loss = 0.02618745
Iteration 87, loss = 0.02661719
Iteration 88, loss = 0.02653431
Iteration 89, loss = 0.02653934
Iteration 90, loss = 0.02620281
Iteration 91, loss = 0.02613446
Iteration 92, loss = 0.02637854
Iteration 93, loss = 0.02651993
Iteration 94, loss = 0.02628271
Iteration 95, loss = 0.02634772
Iteration 96, loss = 0.02594174
Iteration 97, loss = 0.02643394
Iteration 98, loss = 0.02652347
Iteration 99, loss = 0.02648562
Iteration 100, loss = 0.02615704
Iteration 101, loss = 0.02625797
Iteration 102, loss = 0.02594420
Iteration 103, loss = 0.02610010
Iteration 104, loss = 0.02610330
Iteration 105, loss = 0.02596964
Iteration 106, loss = 0.02613408
Iteration 107, loss = 0.02610113
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70350959
Iteration 2, loss = 0.63409378
Iteration 3, loss = 0.53529211
Iteration 4, loss = 0.41679485
Iteration 5, loss = 0.31106772
Iteration 6, loss = 0.23182578
Iteration 7, loss = 0.17622010
Iteration 8, loss = 0.13857233
Iteration 9, loss = 0.11242348
Iteration 10, loss = 0.09406362
Iteration 11, loss = 0.08096443
Iteration 12, loss = 0.07119863
Iteration 13, loss = 0.06380031
Iteration 14, loss = 0.05805095
Iteration 15, loss = 0.05322968
Iteration 16, loss = 0.04939364
Iteration 17, loss = 0.04634207
Iteration 18, loss = 0.04403201
Iteration 19, loss = 0.04172991
Iteration 20, loss = 0.03960219
Iteration 21, loss = 0.03813552
Iteration 22, loss = 0.03687666
Iteration 23, loss = 0.03580621
Iteration 24, loss = 0.03465400
Iteration 25, loss = 0.03380834
Iteration 26, loss = 0.03300482
Iteration 27, loss = 0.03222678
Iteration 28, loss = 0.03163924
Iteration 29, loss = 0.03096853
Iteration 30, loss = 0.03072714
Iteration 31, loss = 0.03003192
Iteration 32, loss = 0.02968451
Iteration 33, loss = 0.02924006
Iteration 34, loss = 0.02906539
Iteration 35, loss = 0.02865444
Iteration 36, loss = 0.02878224
Iteration 37, loss = 0.02789882
Iteration 38, loss = 0.02817093
Iteration 39, loss = 0.02779920
Iteration 40, loss = 0.02759821
Iteration 41, loss = 0.02742772
Iteration 42, loss = 0.02716443
Iteration 43, loss = 0.02683048
Iteration 44, loss = 0.02681988
Iteration 45, loss = 0.02673080
Iteration 46, loss = 0.02652627
Iteration 47, loss = 0.02649348
Iteration 48, loss = 0.02639717
Iteration 49, loss = 0.02620961
Iteration 50, loss = 0.02621841
Iteration 51, loss = 0.02573856
Iteration 52, loss = 0.02576134
Iteration 53, loss = 0.02596619
Iteration 54, loss = 0.02590715
Iteration 55, loss = 0.02575479
Iteration 56, loss = 0.02575813
Iteration 57, loss = 0.02563366
Iteration 58, loss = 0.02552980
Iteration 59, loss = 0.02547231
Iteration 60, loss = 0.02571548
Iteration 61, loss = 0.02551804
Iteration 62, loss = 0.02539546
Iteration 63, loss = 0.02533185
Iteration 64, loss = 0.02516519
Iteration 65, loss = 0.02523308
Iteration 66, loss = 0.02516041
Iteration 67, loss = 0.02536974
Iteration 68, loss = 0.02525138
Iteration 69, loss = 0.02511220
Iteration 70, loss = 0.02504969
Iteration 71, loss = 0.02487385
Iteration 72, loss = 0.02493391
Iteration 73, loss = 0.02503634
Iteration 74, loss = 0.02486270
Iteration 75, loss = 0.02489585
Iteration 76, loss = 0.02477578
Iteration 77, loss = 0.02466938
Iteration 78, loss = 0.02492619
Iteration 79, loss = 0.02475160
Iteration 80, loss = 0.02479159
Iteration 81, loss = 0.02487174
Iteration 82, loss = 0.02460489
Iteration 83, loss = 0.02472777
Iteration 84, loss = 0.02441009
Iteration 85, loss = 0.02462267
Iteration 86, loss = 0.02453846
Iteration 87, loss = 0.02456589
Iteration 88, loss = 0.02485225
Iteration 89, loss = 0.02453342
Iteration 90, loss = 0.02436249
Iteration 91, loss = 0.02446430
Iteration 92, loss = 0.02460683
Iteration 93, loss = 0.02462724
Iteration 94, loss = 0.02459139
Iteration 95, loss = 0.02456664
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67319437
Iteration 2, loss = 0.59940226
Iteration 3, loss = 0.49050904
Iteration 4, loss = 0.38106647
Iteration 5, loss = 0.29012542
Iteration 6, loss = 0.22122864
Iteration 7, loss = 0.17250240
Iteration 8, loss = 0.13804726
Iteration 9, loss = 0.11363285
Iteration 10, loss = 0.09625173
Iteration 11, loss = 0.08308133
Iteration 12, loss = 0.07360149
Iteration 13, loss = 0.06593208
Iteration 14, loss = 0.06019085
Iteration 15, loss = 0.05535468
Iteration 16, loss = 0.05181440
Iteration 17, loss = 0.04869939
Iteration 18, loss = 0.04612173
Iteration 19, loss = 0.04386513
Iteration 20, loss = 0.04203160
Iteration 21, loss = 0.04043755
Iteration 22, loss = 0.03883099
Iteration 23, loss = 0.03770666
Iteration 24, loss = 0.03648912
Iteration 25, loss = 0.03576482
Iteration 26, loss = 0.03479430
Iteration 27, loss = 0.03460467
Iteration 28, loss = 0.03367056
Iteration 29, loss = 0.03311503
Iteration 30, loss = 0.03272614
Iteration 31, loss = 0.03211417
Iteration 32, loss = 0.03200377
Iteration 33, loss = 0.03167259
Iteration 34, loss = 0.03107409
Iteration 35, loss = 0.03092821
Iteration 36, loss = 0.03079538
Iteration 37, loss = 0.03010579
Iteration 38, loss = 0.03003095
Iteration 39, loss = 0.02979901
Iteration 40, loss = 0.02937311
Iteration 41, loss = 0.02943506
Iteration 42, loss = 0.02927880
Iteration 43, loss = 0.02940746
Iteration 44, loss = 0.02947701
Iteration 45, loss = 0.02871881
Iteration 46, loss = 0.02867990
Iteration 47, loss = 0.02861420
Iteration 48, loss = 0.02833882
Iteration 49, loss = 0.02834670
Iteration 50, loss = 0.02820899
Iteration 51, loss = 0.02827352
Iteration 52, loss = 0.02825833
Iteration 53, loss = 0.02794410
Iteration 54, loss = 0.02770731
Iteration 55, loss = 0.02819988
Iteration 56, loss = 0.02782782
Iteration 57, loss = 0.02756505
Iteration 58, loss = 0.02772230
Iteration 59, loss = 0.02764805
Iteration 60, loss = 0.02757855
Iteration 61, loss = 0.02742337
Iteration 62, loss = 0.02735847
Iteration 63, loss = 0.02749360
Iteration 64, loss = 0.02757960
Iteration 65, loss = 0.02758257
Iteration 66, loss = 0.02708319
Iteration 67, loss = 0.02725240
Iteration 68, loss = 0.02721464
Iteration 69, loss = 0.02737860
Iteration 70, loss = 0.02733892
Iteration 71, loss = 0.02709446
Iteration 72, loss = 0.02711366
Iteration 73, loss = 0.02735974
Iteration 74, loss = 0.02724183
Iteration 75, loss = 0.02695650
Iteration 76, loss = 0.02689685
Iteration 77, loss = 0.02694560
Iteration 78, loss = 0.02729435
Iteration 79, loss = 0.02690746
Iteration 80, loss = 0.02681660
Iteration 81, loss = 0.02692310
Iteration 82, loss = 0.02657753
Iteration 83, loss = 0.02676489
Iteration 84, loss = 0.02711265
Iteration 85, loss = 0.02693454
Iteration 86, loss = 0.02697994
Iteration 87, loss = 0.02698188
Iteration 88, loss = 0.02669465
Iteration 89, loss = 0.02665258
Iteration 90, loss = 0.02669688
Iteration 91, loss = 0.02660853
Iteration 92, loss = 0.02680904
Iteration 93, loss = 0.02698565
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67508086
Iteration 2, loss = 0.60856440
Iteration 3, loss = 0.50565330
Iteration 4, loss = 0.39541598
Iteration 5, loss = 0.30027590
Iteration 6, loss = 0.22779266
Iteration 7, loss = 0.17596697
Iteration 8, loss = 0.13980122
Iteration 9, loss = 0.11406456
Iteration 10, loss = 0.09582464
Iteration 11, loss = 0.08243451
Iteration 12, loss = 0.07234315
Iteration 13, loss = 0.06460186
Iteration 14, loss = 0.05874645
Iteration 15, loss = 0.05384640
Iteration 16, loss = 0.04993292
Iteration 17, loss = 0.04652906
Iteration 18, loss = 0.04384806
Iteration 19, loss = 0.04133451
Iteration 20, loss = 0.03946586
Iteration 21, loss = 0.03808146
Iteration 22, loss = 0.03640482
Iteration 23, loss = 0.03517489
Iteration 24, loss = 0.03439451
Iteration 25, loss = 0.03321606
Iteration 26, loss = 0.03239582
Iteration 27, loss = 0.03163009
Iteration 28, loss = 0.03100716
Iteration 29, loss = 0.03032232
Iteration 30, loss = 0.02971530
Iteration 31, loss = 0.02949221
Iteration 32, loss = 0.02907929
Iteration 33, loss = 0.02849562
Iteration 34, loss = 0.02802690
Iteration 35, loss = 0.02785104
Iteration 36, loss = 0.02744101
Iteration 37, loss = 0.02726512
Iteration 38, loss = 0.02701779
Iteration 39, loss = 0.02678542
Iteration 40, loss = 0.02655011
Iteration 41, loss = 0.02632095
Iteration 42, loss = 0.02601852
Iteration 43, loss = 0.02584976
Iteration 44, loss = 0.02588259
Iteration 45, loss = 0.02557454
Iteration 46, loss = 0.02544638
Iteration 47, loss = 0.02554416
Iteration 48, loss = 0.02571561
Iteration 49, loss = 0.02500883
Iteration 50, loss = 0.02499160
Iteration 51, loss = 0.02494942
Iteration 52, loss = 0.02476894
Iteration 53, loss = 0.02484528
Iteration 54, loss = 0.02458376
Iteration 55, loss = 0.02458655
Iteration 56, loss = 0.02461451
Iteration 57, loss = 0.02458848
Iteration 58, loss = 0.02454659
Iteration 59, loss = 0.02432316
Iteration 60, loss = 0.02419246
Iteration 61, loss = 0.02406591
Iteration 62, loss = 0.02410820
Iteration 63, loss = 0.02446221
Iteration 64, loss = 0.02389383
Iteration 65, loss = 0.02406423
Iteration 66, loss = 0.02390590
Iteration 67, loss = 0.02396319
Iteration 68, loss = 0.02391275
Iteration 69, loss = 0.02403240
Iteration 70, loss = 0.02375158
Iteration 71, loss = 0.02370726
Iteration 72, loss = 0.02381179
Iteration 73, loss = 0.02372177
Iteration 74, loss = 0.02371493
Iteration 75, loss = 0.02376971
Iteration 76, loss = 0.02348443
Iteration 77, loss = 0.02337787
Iteration 78, loss = 0.02356715
Iteration 79, loss = 0.02361215
Iteration 80, loss = 0.02346901
Iteration 81, loss = 0.02389658
Iteration 82, loss = 0.02323773
Iteration 83, loss = 0.02362412
Iteration 84, loss = 0.02354524
Iteration 85, loss = 0.02350596
Iteration 86, loss = 0.02351432
Iteration 87, loss = 0.02371309
Iteration 88, loss = 0.02337041
Iteration 89, loss = 0.02320876
Iteration 90, loss = 0.02363982
Iteration 91, loss = 0.02317079
Iteration 92, loss = 0.02341842
Iteration 93, loss = 0.02322367
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67302844
Iteration 2, loss = 0.59754660
Iteration 3, loss = 0.48868972
Iteration 4, loss = 0.37787434
Iteration 5, loss = 0.28350119
Iteration 6, loss = 0.21338890
Iteration 7, loss = 0.16379244
Iteration 8, loss = 0.13003042
Iteration 9, loss = 0.10629538
Iteration 10, loss = 0.08916791
Iteration 11, loss = 0.07691788
Iteration 12, loss = 0.06741249
Iteration 13, loss = 0.06030117
Iteration 14, loss = 0.05438730
Iteration 15, loss = 0.04984868
Iteration 16, loss = 0.04622447
Iteration 17, loss = 0.04306611
Iteration 18, loss = 0.04044775
Iteration 19, loss = 0.03823924
Iteration 20, loss = 0.03662882
Iteration 21, loss = 0.03486072
Iteration 22, loss = 0.03376326
Iteration 23, loss = 0.03230904
Iteration 24, loss = 0.03147748
Iteration 25, loss = 0.03039087
Iteration 26, loss = 0.02957804
Iteration 27, loss = 0.02889826
Iteration 28, loss = 0.02830330
Iteration 29, loss = 0.02770223
Iteration 30, loss = 0.02707467
Iteration 31, loss = 0.02664289
Iteration 32, loss = 0.02645525
Iteration 33, loss = 0.02589269
Iteration 34, loss = 0.02570113
Iteration 35, loss = 0.02531316
Iteration 36, loss = 0.02494353
Iteration 37, loss = 0.02468118
Iteration 38, loss = 0.02444396
Iteration 39, loss = 0.02467073
Iteration 40, loss = 0.02397502
Iteration 41, loss = 0.02397108
Iteration 42, loss = 0.02380649
Iteration 43, loss = 0.02364905
Iteration 44, loss = 0.02344519
Iteration 45, loss = 0.02319993
Iteration 46, loss = 0.02315944
Iteration 47, loss = 0.02313717
Iteration 48, loss = 0.02294536
Iteration 49, loss = 0.02294976
Iteration 50, loss = 0.02260328
Iteration 51, loss = 0.02295513
Iteration 52, loss = 0.02276127
Iteration 53, loss = 0.02265390
Iteration 54, loss = 0.02255759
Iteration 55, loss = 0.02250514
Iteration 56, loss = 0.02233187
Iteration 57, loss = 0.02261069
Iteration 58, loss = 0.02221684
Iteration 59, loss = 0.02243883
Iteration 60, loss = 0.02241112
Iteration 61, loss = 0.02203768
Iteration 62, loss = 0.02202862
Iteration 63, loss = 0.02160115
Iteration 64, loss = 0.02191466
Iteration 65, loss = 0.02186604
Iteration 66, loss = 0.02175943
Iteration 67, loss = 0.02189494
Iteration 68, loss = 0.02184246
Iteration 69, loss = 0.02177836
Iteration 70, loss = 0.02167057
Iteration 71, loss = 0.02188390
Iteration 72, loss = 0.02173338
Iteration 73, loss = 0.02144586
Iteration 74, loss = 0.02149796
Iteration 75, loss = 0.02147333
Iteration 76, loss = 0.02138064
Iteration 77, loss = 0.02151364
Iteration 78, loss = 0.02139989
Iteration 79, loss = 0.02145162
Iteration 80, loss = 0.02133846
Iteration 81, loss = 0.02146345
Iteration 82, loss = 0.02136500
Iteration 83, loss = 0.02123996
Iteration 84, loss = 0.02155963
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67334953
Iteration 2, loss = 0.59964901
Iteration 3, loss = 0.49178018
Iteration 4, loss = 0.38301844
Iteration 5, loss = 0.29041317
Iteration 6, loss = 0.22057692
Iteration 7, loss = 0.17122587
Iteration 8, loss = 0.13712274
Iteration 9, loss = 0.11303298
Iteration 10, loss = 0.09575202
Iteration 11, loss = 0.08314022
Iteration 12, loss = 0.07313721
Iteration 13, loss = 0.06619903
Iteration 14, loss = 0.06036457
Iteration 15, loss = 0.05561995
Iteration 16, loss = 0.05204027
Iteration 17, loss = 0.04873162
Iteration 18, loss = 0.04607631
Iteration 19, loss = 0.04400260
Iteration 20, loss = 0.04211057
Iteration 21, loss = 0.04034425
Iteration 22, loss = 0.03900890
Iteration 23, loss = 0.03786066
Iteration 24, loss = 0.03696025
Iteration 25, loss = 0.03583244
Iteration 26, loss = 0.03497624
Iteration 27, loss = 0.03433167
Iteration 28, loss = 0.03382435
Iteration 29, loss = 0.03295103
Iteration 30, loss = 0.03248187
Iteration 31, loss = 0.03224030
Iteration 32, loss = 0.03178705
Iteration 33, loss = 0.03131647
Iteration 34, loss = 0.03127410
Iteration 35, loss = 0.03073179
Iteration 36, loss = 0.03023671
Iteration 37, loss = 0.03011837
Iteration 38, loss = 0.02989611
Iteration 39, loss = 0.02981144
Iteration 40, loss = 0.02945551
Iteration 41, loss = 0.02936944
Iteration 42, loss = 0.02898280
Iteration 43, loss = 0.02887624
Iteration 44, loss = 0.02871389
Iteration 45, loss = 0.02872972
Iteration 46, loss = 0.02857328
Iteration 47, loss = 0.02820012
Iteration 48, loss = 0.02836579
Iteration 49, loss = 0.02810232
Iteration 50, loss = 0.02808532
Iteration 51, loss = 0.02833331
Iteration 52, loss = 0.02817799
Iteration 53, loss = 0.02760556
Iteration 54, loss = 0.02774195
Iteration 55, loss = 0.02756850
Iteration 56, loss = 0.02759091
Iteration 57, loss = 0.02734415
Iteration 58, loss = 0.02740646
Iteration 59, loss = 0.02754761
Iteration 60, loss = 0.02737625
Iteration 61, loss = 0.02761916
Iteration 62, loss = 0.02719003
Iteration 63, loss = 0.02708135
Iteration 64, loss = 0.02714666
Iteration 65, loss = 0.02723841
Iteration 66, loss = 0.02711408
Iteration 67, loss = 0.02705038
Iteration 68, loss = 0.02671308
Iteration 69, loss = 0.02712939
Iteration 70, loss = 0.02691957
Iteration 71, loss = 0.02697344
Iteration 72, loss = 0.02682412
Iteration 73, loss = 0.02692975
Iteration 74, loss = 0.02734971
Iteration 75, loss = 0.02687484
Iteration 76, loss = 0.02679989
Iteration 77, loss = 0.02669808
Iteration 78, loss = 0.02656934
Iteration 79, loss = 0.02670883
Iteration 80, loss = 0.02674364
Iteration 81, loss = 0.02672867
Iteration 82, loss = 0.02657757
Iteration 83, loss = 0.02646723
Iteration 84, loss = 0.02679347
Iteration 85, loss = 0.02661499
Iteration 86, loss = 0.02618745
Iteration 87, loss = 0.02661719
Iteration 88, loss = 0.02653431
Iteration 89, loss = 0.02653934
Iteration 90, loss = 0.02620281
Iteration 91, loss = 0.02613446
Iteration 92, loss = 0.02637854
Iteration 93, loss = 0.02651993
Iteration 94, loss = 0.02628271
Iteration 95, loss = 0.02634772
Iteration 96, loss = 0.02594174
Iteration 97, loss = 0.02643394
Iteration 98, loss = 0.02652347
Iteration 99, loss = 0.02648562
Iteration 100, loss = 0.02615704
Iteration 101, loss = 0.02625797
Iteration 102, loss = 0.02594420
Iteration 103, loss = 0.02610010
Iteration 104, loss = 0.02610330
Iteration 105, loss = 0.02596964
Iteration 106, loss = 0.02613408
Iteration 107, loss = 0.02610113
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70350959
Iteration 2, loss = 0.63409378
Iteration 3, loss = 0.53529211
Iteration 4, loss = 0.41679485
Iteration 5, loss = 0.31106772
Iteration 6, loss = 0.23182578
Iteration 7, loss = 0.17622010
Iteration 8, loss = 0.13857233
Iteration 9, loss = 0.11242348
Iteration 10, loss = 0.09406362
Iteration 11, loss = 0.08096443
Iteration 12, loss = 0.07119863
Iteration 13, loss = 0.06380031
Iteration 14, loss = 0.05805095
Iteration 15, loss = 0.05322968
Iteration 16, loss = 0.04939364
Iteration 17, loss = 0.04634207
Iteration 18, loss = 0.04403201
Iteration 19, loss = 0.04172991
Iteration 20, loss = 0.03960219
Iteration 21, loss = 0.03813552
Iteration 22, loss = 0.03687666
Iteration 23, loss = 0.03580621
Iteration 24, loss = 0.03465400
Iteration 25, loss = 0.03380834
Iteration 26, loss = 0.03300482
Iteration 27, loss = 0.03222678
Iteration 28, loss = 0.03163924
Iteration 29, loss = 0.03096853
Iteration 30, loss = 0.03072714
Iteration 31, loss = 0.03003192
Iteration 32, loss = 0.02968451
Iteration 33, loss = 0.02924006
Iteration 34, loss = 0.02906539
Iteration 35, loss = 0.02865444
Iteration 36, loss = 0.02878224
Iteration 37, loss = 0.02789882
Iteration 38, loss = 0.02817093
Iteration 39, loss = 0.02779920
Iteration 40, loss = 0.02759821
Iteration 41, loss = 0.02742772
Iteration 42, loss = 0.02716443
Iteration 43, loss = 0.02683048
Iteration 44, loss = 0.02681988
Iteration 45, loss = 0.02673080
Iteration 46, loss = 0.02652627
Iteration 47, loss = 0.02649348
Iteration 48, loss = 0.02639717
Iteration 49, loss = 0.02620961
Iteration 50, loss = 0.02621841
Iteration 51, loss = 0.02573856
Iteration 52, loss = 0.02576134
Iteration 53, loss = 0.02596619
Iteration 54, loss = 0.02590715
Iteration 55, loss = 0.02575479
Iteration 56, loss = 0.02575813
Iteration 57, loss = 0.02563366
Iteration 58, loss = 0.02552980
Iteration 59, loss = 0.02547231
Iteration 60, loss = 0.02571548
Iteration 61, loss = 0.02551804
Iteration 62, loss = 0.02539546
Iteration 63, loss = 0.02533185
Iteration 64, loss = 0.02516519
Iteration 65, loss = 0.02523308
Iteration 66, loss = 0.02516041
Iteration 67, loss = 0.02536974
Iteration 68, loss = 0.02525138
Iteration 69, loss = 0.02511220
Iteration 70, loss = 0.02504969
Iteration 71, loss = 0.02487385
Iteration 72, loss = 0.02493391
Iteration 73, loss = 0.02503634
Iteration 74, loss = 0.02486270
Iteration 75, loss = 0.02489585
Iteration 76, loss = 0.02477578
Iteration 77, loss = 0.02466938
Iteration 78, loss = 0.02492619
Iteration 79, loss = 0.02475160
Iteration 80, loss = 0.02479159
Iteration 81, loss = 0.02487174
Iteration 82, loss = 0.02460489
Iteration 83, loss = 0.02472777
Iteration 84, loss = 0.02441009
Iteration 85, loss = 0.02462267
Iteration 86, loss = 0.02453846
Iteration 87, loss = 0.02456589
Iteration 88, loss = 0.02485225
Iteration 89, loss = 0.02453342
Iteration 90, loss = 0.02436249
Iteration 91, loss = 0.02446430
Iteration 92, loss = 0.02460683
Iteration 93, loss = 0.02462724
Iteration 94, loss = 0.02459139
Iteration 95, loss = 0.02456664
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67792695
Iteration 2, loss = 0.60191669
Iteration 3, loss = 0.48147148
Iteration 4, loss = 0.36040213
Iteration 5, loss = 0.26519952
Iteration 6, loss = 0.19770519
Iteration 7, loss = 0.15182832
Iteration 8, loss = 0.12123697
Iteration 9, loss = 0.09986592
Iteration 10, loss = 0.08497004
Iteration 11, loss = 0.07410826
Iteration 12, loss = 0.06611836
Iteration 13, loss = 0.05987352
Iteration 14, loss = 0.05490426
Iteration 15, loss = 0.05133937
Iteration 16, loss = 0.04791134
Iteration 17, loss = 0.04540773
Iteration 18, loss = 0.04322227
Iteration 19, loss = 0.04125595
Iteration 20, loss = 0.03970583
Iteration 21, loss = 0.03880476
Iteration 22, loss = 0.03742811
Iteration 23, loss = 0.03635408
Iteration 24, loss = 0.03588878
Iteration 25, loss = 0.03482469
Iteration 26, loss = 0.03439187
Iteration 27, loss = 0.03334439
Iteration 28, loss = 0.03329113
Iteration 29, loss = 0.03235731
Iteration 30, loss = 0.03205636
Iteration 31, loss = 0.03157731
Iteration 32, loss = 0.03150502
Iteration 33, loss = 0.03096503
Iteration 34, loss = 0.03064953
Iteration 35, loss = 0.03056135
Iteration 36, loss = 0.03023352
Iteration 37, loss = 0.03017252
Iteration 38, loss = 0.02997095
Iteration 39, loss = 0.02965753
Iteration 40, loss = 0.02986593
Iteration 41, loss = 0.02946948
Iteration 42, loss = 0.02937007
Iteration 43, loss = 0.02916387
Iteration 44, loss = 0.02905022
Iteration 45, loss = 0.02895608
Iteration 46, loss = 0.02935734
Iteration 47, loss = 0.02895663
Iteration 48, loss = 0.02870046
Iteration 49, loss = 0.02862218
Iteration 50, loss = 0.02855706
Iteration 51, loss = 0.02870470
Iteration 52, loss = 0.02826438
Iteration 53, loss = 0.02834794
Iteration 54, loss = 0.02835551
Iteration 55, loss = 0.02802263
Iteration 56, loss = 0.02817925
Iteration 57, loss = 0.02809988
Iteration 58, loss = 0.02778237
Iteration 59, loss = 0.02810373
Iteration 60, loss = 0.02818929
Iteration 61, loss = 0.02822608
Iteration 62, loss = 0.02789354
Iteration 63, loss = 0.02748343
Iteration 64, loss = 0.02801398
Iteration 65, loss = 0.02773648
Iteration 66, loss = 0.02776792
Iteration 67, loss = 0.02764837
Iteration 68, loss = 0.02762975
Iteration 69, loss = 0.02779426
Iteration 70, loss = 0.02786145
Iteration 71, loss = 0.02758850
Iteration 72, loss = 0.02742091
Iteration 73, loss = 0.02759345
Iteration 74, loss = 0.02732028
Iteration 75, loss = 0.02748701
Iteration 76, loss = 0.02741748
Iteration 77, loss = 0.02763257
Iteration 78, loss = 0.02752908
Iteration 79, loss = 0.02740857
Iteration 80, loss = 0.02722315
Iteration 81, loss = 0.02728500
Iteration 82, loss = 0.02742052
Iteration 83, loss = 0.02791887
Iteration 84, loss = 0.02729506
Iteration 85, loss = 0.02728098
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on kept_v4_stemmed...
Iteration 1, loss = 0.68567102
Iteration 2, loss = 0.64791811
Iteration 3, loss = 0.59928269
Iteration 4, loss = 0.53751156
Iteration 5, loss = 0.46872608
Iteration 6, loss = 0.40042847
Iteration 7, loss = 0.33818700
Iteration 8, loss = 0.28362793
Iteration 9, loss = 0.23815501
Iteration 10, loss = 0.20064439
Iteration 11, loss = 0.17044877
Iteration 12, loss = 0.14634942
Iteration 13, loss = 0.12701725
Iteration 14, loss = 0.11152249
Iteration 15, loss = 0.09898211
Iteration 16, loss = 0.08858465
Iteration 17, loss = 0.08047283
Iteration 18, loss = 0.07319362
Iteration 19, loss = 0.06724887
Iteration 20, loss = 0.06225505
Iteration 21, loss = 0.05780113
Iteration 22, loss = 0.05430405
Iteration 23, loss = 0.05083216
Iteration 24, loss = 0.04826671
Iteration 25, loss = 0.04572597
Iteration 26, loss = 0.04365677
Iteration 27, loss = 0.04154045
Iteration 28, loss = 0.03984603
Iteration 29, loss = 0.03832918
Iteration 30, loss = 0.03703227
Iteration 31, loss = 0.03576173
Iteration 32, loss = 0.03452167
Iteration 33, loss = 0.03353360
Iteration 34, loss = 0.03255003
Iteration 35, loss = 0.03182294
Iteration 36, loss = 0.03096533
Iteration 37, loss = 0.03012221
Iteration 38, loss = 0.02964670
Iteration 39, loss = 0.02913456
Iteration 40, loss = 0.02839552
Iteration 41, loss = 0.02796806
Iteration 42, loss = 0.02746174
Iteration 43, loss = 0.02708111
Iteration 44, loss = 0.02670563
Iteration 45, loss = 0.02614259
Iteration 46, loss = 0.02586435
Iteration 47, loss = 0.02563727
Iteration 48, loss = 0.02528329
Iteration 49, loss = 0.02491287
Iteration 50, loss = 0.02468948
Iteration 51, loss = 0.02432558
Iteration 52, loss = 0.02406074
Iteration 53, loss = 0.02374382
Iteration 54, loss = 0.02363637
Iteration 55, loss = 0.02338968
Iteration 56, loss = 0.02325025
Iteration 57, loss = 0.02312915
Iteration 58, loss = 0.02309664
Iteration 59, loss = 0.02265377
Iteration 60, loss = 0.02251265
Iteration 61, loss = 0.02236458
Iteration 62, loss = 0.02223071
Iteration 63, loss = 0.02228524
Iteration 64, loss = 0.02198076
Iteration 65, loss = 0.02169946
Iteration 66, loss = 0.02186311
Iteration 67, loss = 0.02153271
Iteration 68, loss = 0.02167189
Iteration 69, loss = 0.02142790
Iteration 70, loss = 0.02123955
Iteration 71, loss = 0.02122449
Iteration 72, loss = 0.02117486
Iteration 73, loss = 0.02117247
Iteration 74, loss = 0.02093136
Iteration 75, loss = 0.02096790
Iteration 76, loss = 0.02071185
Iteration 77, loss = 0.02082845
Iteration 78, loss = 0.02076414
Iteration 79, loss = 0.02057899
Iteration 80, loss = 0.02066567
Iteration 81, loss = 0.02029361
Iteration 82, loss = 0.02039451
Iteration 83, loss = 0.02028924
Iteration 84, loss = 0.02053180
Iteration 85, loss = 0.02026320
Iteration 86, loss = 0.02028943
Iteration 87, loss = 0.02012006
Iteration 88, loss = 0.02003680
Iteration 89, loss = 0.02003361
Iteration 90, loss = 0.02002719
Iteration 91, loss = 0.02003841
Iteration 92, loss = 0.01984359
Iteration 93, loss = 0.01983287
Iteration 94, loss = 0.01976039
Iteration 95, loss = 0.01978095
Iteration 96, loss = 0.01955940
Iteration 97, loss = 0.01981724
Iteration 98, loss = 0.01978328
Iteration 99, loss = 0.01950556
Iteration 100, loss = 0.01972770
Iteration 101, loss = 0.01975857
Iteration 102, loss = 0.01956533
Iteration 103, loss = 0.01942551
Iteration 104, loss = 0.01940292
Iteration 105, loss = 0.01942659
Iteration 106, loss = 0.01948644
Iteration 107, loss = 0.01943168
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71365000
Iteration 2, loss = 0.69815088
Iteration 3, loss = 0.68541563
Iteration 4, loss = 0.67299084
Iteration 5, loss = 0.66050852
Iteration 6, loss = 0.64690831
Iteration 7, loss = 0.63220504
Iteration 8, loss = 0.61617837
Iteration 9, loss = 0.59841930
Iteration 10, loss = 0.57919559
Iteration 11, loss = 0.55852600
Iteration 12, loss = 0.53671113
Iteration 13, loss = 0.51388185
Iteration 14, loss = 0.49029137
Iteration 15, loss = 0.46622121
Iteration 16, loss = 0.44221174
Iteration 17, loss = 0.41840552
Iteration 18, loss = 0.39492894
Iteration 19, loss = 0.37214890
Iteration 20, loss = 0.35022678
Iteration 21, loss = 0.32923352
Iteration 22, loss = 0.30938509
Iteration 23, loss = 0.29042150
Iteration 24, loss = 0.27259621
Iteration 25, loss = 0.25593165
Iteration 26, loss = 0.24017790
Iteration 27, loss = 0.22537997
Iteration 28, loss = 0.21166755
Iteration 29, loss = 0.19886178
Iteration 30, loss = 0.18686472
Iteration 31, loss = 0.17575990
Iteration 32, loss = 0.16544681
Iteration 33, loss = 0.15582647
Iteration 34, loss = 0.14689696
Iteration 35, loss = 0.13866758
Iteration 36, loss = 0.13093598
Iteration 37, loss = 0.12387166
Iteration 38, loss = 0.11724367
Iteration 39, loss = 0.11111552
Iteration 40, loss = 0.10548403
Iteration 41, loss = 0.10018016
Iteration 42, loss = 0.09533873
Iteration 43, loss = 0.09080368
Iteration 44, loss = 0.08658624
Iteration 45, loss = 0.08262830
Iteration 46, loss = 0.07897828
Iteration 47, loss = 0.07562008
Iteration 48, loss = 0.07241974
Iteration 49, loss = 0.06945091
Iteration 50, loss = 0.06666013
Iteration 51, loss = 0.06404094
Iteration 52, loss = 0.06159187
Iteration 53, loss = 0.05927881
Iteration 54, loss = 0.05708588
Iteration 55, loss = 0.05503208
Iteration 56, loss = 0.05312495
Iteration 57, loss = 0.05126887
Iteration 58, loss = 0.04955332
Iteration 59, loss = 0.04796685
Iteration 60, loss = 0.04642223
Iteration 61, loss = 0.04503010
Iteration 62, loss = 0.04364705
Iteration 63, loss = 0.04236595
Iteration 64, loss = 0.04114422
Iteration 65, loss = 0.03993431
Iteration 66, loss = 0.03882648
Iteration 67, loss = 0.03779051
Iteration 68, loss = 0.03674395
Iteration 69, loss = 0.03580382
Iteration 70, loss = 0.03495251
Iteration 71, loss = 0.03403997
Iteration 72, loss = 0.03320092
Iteration 73, loss = 0.03239317
Iteration 74, loss = 0.03162505
Iteration 75, loss = 0.03090427
Iteration 76, loss = 0.03018323
Iteration 77, loss = 0.02951297
Iteration 78, loss = 0.02888166
Iteration 79, loss = 0.02825790
Iteration 80, loss = 0.02767363
Iteration 81, loss = 0.02710707
Iteration 82, loss = 0.02657986
Iteration 83, loss = 0.02604536
Iteration 84, loss = 0.02554977
Iteration 85, loss = 0.02504764
Iteration 86, loss = 0.02456686
Iteration 87, loss = 0.02415974
Iteration 88, loss = 0.02375540
Iteration 89, loss = 0.02335755
Iteration 90, loss = 0.02298026
Iteration 91, loss = 0.02261298
Iteration 92, loss = 0.02225411
Iteration 93, loss = 0.02192811
Iteration 94, loss = 0.02158620
Iteration 95, loss = 0.02125149
Iteration 96, loss = 0.02092966
Iteration 97, loss = 0.02059055
Iteration 98, loss = 0.02028476
Iteration 99, loss = 0.01996699
Iteration 100, loss = 0.01962104
Iteration 101, loss = 0.01934351
Iteration 102, loss = 0.01908407
Iteration 103, loss = 0.01885208
Iteration 104, loss = 0.01858000
Iteration 105, loss = 0.01835481
Iteration 106, loss = 0.01809716
Iteration 107, loss = 0.01789042
Iteration 108, loss = 0.01773874
Iteration 109, loss = 0.01746811
Iteration 110, loss = 0.01728528
Iteration 111, loss = 0.01704524
Iteration 112, loss = 0.01684635
Iteration 113, loss = 0.01668206
Iteration 114, loss = 0.01647567
Iteration 115, loss = 0.01630831
Iteration 116, loss = 0.01613534
Iteration 117, loss = 0.01596890
Iteration 118, loss = 0.01580079
Iteration 119, loss = 0.01563873
Iteration 120, loss = 0.01551872
Iteration 121, loss = 0.01534942
Iteration 122, loss = 0.01522547
Iteration 123, loss = 0.01505506
Iteration 124, loss = 0.01492567
Iteration 125, loss = 0.01479120
Iteration 126, loss = 0.01470652
Iteration 127, loss = 0.01460636
Iteration 128, loss = 0.01447142
Iteration 129, loss = 0.01430369
Iteration 130, loss = 0.01416938
Iteration 131, loss = 0.01404325
Iteration 132, loss = 0.01393953
Iteration 133, loss = 0.01381835
Iteration 134, loss = 0.01369201
Iteration 135, loss = 0.01356304
Iteration 136, loss = 0.01348291
Iteration 137, loss = 0.01341414
Iteration 138, loss = 0.01324966
Iteration 139, loss = 0.01314751
Iteration 140, loss = 0.01307749
Iteration 141, loss = 0.01296182
Iteration 142, loss = 0.01286431
Iteration 143, loss = 0.01278469
Iteration 144, loss = 0.01269204
Iteration 145, loss = 0.01264586
Iteration 146, loss = 0.01255023
Iteration 147, loss = 0.01245329
Iteration 148, loss = 0.01238182
Iteration 149, loss = 0.01229897
Iteration 150, loss = 0.01220853
Iteration 151, loss = 0.01213455
Iteration 152, loss = 0.01210253
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68806545
Iteration 2, loss = 0.67088054
Iteration 3, loss = 0.65598819
Iteration 4, loss = 0.64148541
Iteration 5, loss = 0.62598029
Iteration 6, loss = 0.60932381
Iteration 7, loss = 0.59126586
Iteration 8, loss = 0.57154811
Iteration 9, loss = 0.55046585
Iteration 10, loss = 0.52836426
Iteration 11, loss = 0.50525442
Iteration 12, loss = 0.48158469
Iteration 13, loss = 0.45753367
Iteration 14, loss = 0.43342924
Iteration 15, loss = 0.40958120
Iteration 16, loss = 0.38613510
Iteration 17, loss = 0.36336513
Iteration 18, loss = 0.34150799
Iteration 19, loss = 0.32062336
Iteration 20, loss = 0.30073929
Iteration 21, loss = 0.28201450
Iteration 22, loss = 0.26426103
Iteration 23, loss = 0.24755088
Iteration 24, loss = 0.23194484
Iteration 25, loss = 0.21740132
Iteration 26, loss = 0.20384874
Iteration 27, loss = 0.19112841
Iteration 28, loss = 0.17931926
Iteration 29, loss = 0.16845553
Iteration 30, loss = 0.15820633
Iteration 31, loss = 0.14874756
Iteration 32, loss = 0.14008622
Iteration 33, loss = 0.13192519
Iteration 34, loss = 0.12446689
Iteration 35, loss = 0.11750000
Iteration 36, loss = 0.11114022
Iteration 37, loss = 0.10517035
Iteration 38, loss = 0.09962025
Iteration 39, loss = 0.09453727
Iteration 40, loss = 0.08978556
Iteration 41, loss = 0.08535826
Iteration 42, loss = 0.08129635
Iteration 43, loss = 0.07746513
Iteration 44, loss = 0.07392754
Iteration 45, loss = 0.07062397
Iteration 46, loss = 0.06754195
Iteration 47, loss = 0.06462200
Iteration 48, loss = 0.06197102
Iteration 49, loss = 0.05939562
Iteration 50, loss = 0.05702617
Iteration 51, loss = 0.05479989
Iteration 52, loss = 0.05270172
Iteration 53, loss = 0.05074451
Iteration 54, loss = 0.04887340
Iteration 55, loss = 0.04713346
Iteration 56, loss = 0.04548126
Iteration 57, loss = 0.04392810
Iteration 58, loss = 0.04243317
Iteration 59, loss = 0.04106619
Iteration 60, loss = 0.03969345
Iteration 61, loss = 0.03848190
Iteration 62, loss = 0.03727258
Iteration 63, loss = 0.03614260
Iteration 64, loss = 0.03507395
Iteration 65, loss = 0.03406794
Iteration 66, loss = 0.03310276
Iteration 67, loss = 0.03216202
Iteration 68, loss = 0.03130751
Iteration 69, loss = 0.03045036
Iteration 70, loss = 0.02966766
Iteration 71, loss = 0.02890287
Iteration 72, loss = 0.02816657
Iteration 73, loss = 0.02747280
Iteration 74, loss = 0.02682299
Iteration 75, loss = 0.02614938
Iteration 76, loss = 0.02554160
Iteration 77, loss = 0.02494276
Iteration 78, loss = 0.02437117
Iteration 79, loss = 0.02383690
Iteration 80, loss = 0.02329228
Iteration 81, loss = 0.02280515
Iteration 82, loss = 0.02231921
Iteration 83, loss = 0.02184375
Iteration 84, loss = 0.02141122
Iteration 85, loss = 0.02097220
Iteration 86, loss = 0.02055802
Iteration 87, loss = 0.02013714
Iteration 88, loss = 0.01975225
Iteration 89, loss = 0.01937200
Iteration 90, loss = 0.01901859
Iteration 91, loss = 0.01867142
Iteration 92, loss = 0.01833497
Iteration 93, loss = 0.01800877
Iteration 94, loss = 0.01770003
Iteration 95, loss = 0.01739809
Iteration 96, loss = 0.01710213
Iteration 97, loss = 0.01682663
Iteration 98, loss = 0.01654049
Iteration 99, loss = 0.01629151
Iteration 100, loss = 0.01603242
Iteration 101, loss = 0.01577697
Iteration 102, loss = 0.01553452
Iteration 103, loss = 0.01529380
Iteration 104, loss = 0.01507224
Iteration 105, loss = 0.01485466
Iteration 106, loss = 0.01464643
Iteration 107, loss = 0.01443155
Iteration 108, loss = 0.01422403
Iteration 109, loss = 0.01403205
Iteration 110, loss = 0.01383847
Iteration 111, loss = 0.01365216
Iteration 112, loss = 0.01346970
Iteration 113, loss = 0.01329768
Iteration 114, loss = 0.01312279
Iteration 115, loss = 0.01295833
Iteration 116, loss = 0.01279231
Iteration 117, loss = 0.01263786
Iteration 118, loss = 0.01248116
Iteration 119, loss = 0.01232916
Iteration 120, loss = 0.01218364
Iteration 121, loss = 0.01203913
Iteration 122, loss = 0.01191093
Iteration 123, loss = 0.01176282
Iteration 124, loss = 0.01163665
Iteration 125, loss = 0.01150866
Iteration 126, loss = 0.01138339
Iteration 127, loss = 0.01125963
Iteration 128, loss = 0.01114267
Iteration 129, loss = 0.01102564
Iteration 130, loss = 0.01091855
Iteration 131, loss = 0.01080220
Iteration 132, loss = 0.01068749
Iteration 133, loss = 0.01058176
Iteration 134, loss = 0.01049391
Iteration 135, loss = 0.01038451
Iteration 136, loss = 0.01028971
Iteration 137, loss = 0.01019611
Iteration 138, loss = 0.01009738
Iteration 139, loss = 0.01000000
Iteration 140, loss = 0.00990817
Iteration 141, loss = 0.00981892
Iteration 142, loss = 0.00973006
Iteration 143, loss = 0.00964247
Iteration 144, loss = 0.00956442
Iteration 145, loss = 0.00948470
Iteration 146, loss = 0.00940641
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69487165
Iteration 2, loss = 0.66298230
Iteration 3, loss = 0.61699633
Iteration 4, loss = 0.55739337
Iteration 5, loss = 0.49000244
Iteration 6, loss = 0.42150194
Iteration 7, loss = 0.35613218
Iteration 8, loss = 0.29725253
Iteration 9, loss = 0.24751241
Iteration 10, loss = 0.20690032
Iteration 11, loss = 0.17407776
Iteration 12, loss = 0.14799541
Iteration 13, loss = 0.12744035
Iteration 14, loss = 0.11106763
Iteration 15, loss = 0.09778423
Iteration 16, loss = 0.08704033
Iteration 17, loss = 0.07820235
Iteration 18, loss = 0.07093277
Iteration 19, loss = 0.06494457
Iteration 20, loss = 0.05985465
Iteration 21, loss = 0.05560986
Iteration 22, loss = 0.05180810
Iteration 23, loss = 0.04845132
Iteration 24, loss = 0.04564472
Iteration 25, loss = 0.04328747
Iteration 26, loss = 0.04112330
Iteration 27, loss = 0.03917584
Iteration 28, loss = 0.03757617
Iteration 29, loss = 0.03594764
Iteration 30, loss = 0.03467908
Iteration 31, loss = 0.03336751
Iteration 32, loss = 0.03232060
Iteration 33, loss = 0.03131935
Iteration 34, loss = 0.03042184
Iteration 35, loss = 0.02969667
Iteration 36, loss = 0.02890755
Iteration 37, loss = 0.02837119
Iteration 38, loss = 0.02756086
Iteration 39, loss = 0.02688259
Iteration 40, loss = 0.02639971
Iteration 41, loss = 0.02583914
Iteration 42, loss = 0.02544254
Iteration 43, loss = 0.02503009
Iteration 44, loss = 0.02463420
Iteration 45, loss = 0.02424201
Iteration 46, loss = 0.02399526
Iteration 47, loss = 0.02382087
Iteration 48, loss = 0.02335690
Iteration 49, loss = 0.02297968
Iteration 50, loss = 0.02280857
Iteration 51, loss = 0.02255889
Iteration 52, loss = 0.02230724
Iteration 53, loss = 0.02213153
Iteration 54, loss = 0.02195866
Iteration 55, loss = 0.02174782
Iteration 56, loss = 0.02147894
Iteration 57, loss = 0.02130979
Iteration 58, loss = 0.02128783
Iteration 59, loss = 0.02110330
Iteration 60, loss = 0.02094413
Iteration 61, loss = 0.02067815
Iteration 62, loss = 0.02047780
Iteration 63, loss = 0.02045282
Iteration 64, loss = 0.02043064
Iteration 65, loss = 0.02027393
Iteration 66, loss = 0.02013634
Iteration 67, loss = 0.02007905
Iteration 68, loss = 0.01990597
Iteration 69, loss = 0.01992847
Iteration 70, loss = 0.01971130
Iteration 71, loss = 0.01964187
Iteration 72, loss = 0.01952099
Iteration 73, loss = 0.01958066
Iteration 74, loss = 0.01939777
Iteration 75, loss = 0.01932389
Iteration 76, loss = 0.01916836
Iteration 77, loss = 0.01916269
Iteration 78, loss = 0.01914579
Iteration 79, loss = 0.01905964
Iteration 80, loss = 0.01903566
Iteration 81, loss = 0.01892144
Iteration 82, loss = 0.01871930
Iteration 83, loss = 0.01879215
Iteration 84, loss = 0.01890460
Iteration 85, loss = 0.01870852
Iteration 86, loss = 0.01866238
Iteration 87, loss = 0.01857234
Iteration 88, loss = 0.01855117
Iteration 89, loss = 0.01856576
Iteration 90, loss = 0.01857991
Iteration 91, loss = 0.01850945
Iteration 92, loss = 0.01837641
Iteration 93, loss = 0.01834368
Iteration 94, loss = 0.01843787
Iteration 95, loss = 0.01833492
Iteration 96, loss = 0.01826782
Iteration 97, loss = 0.01830703
Iteration 98, loss = 0.01821247
Iteration 99, loss = 0.01829515
Iteration 100, loss = 0.01821658
Iteration 101, loss = 0.01810159
Iteration 102, loss = 0.01805875
Iteration 103, loss = 0.01805823
Iteration 104, loss = 0.01812672
Iteration 105, loss = 0.01804533
Iteration 106, loss = 0.01797119
Iteration 107, loss = 0.01792498
Iteration 108, loss = 0.01794286
Iteration 109, loss = 0.01801843
Iteration 110, loss = 0.01808611
Iteration 111, loss = 0.01786044
Iteration 112, loss = 0.01783456
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72124116
Iteration 2, loss = 0.70076937
Iteration 3, loss = 0.68191025
Iteration 4, loss = 0.65976277
Iteration 5, loss = 0.63358742
Iteration 6, loss = 0.60307862
Iteration 7, loss = 0.56921608
Iteration 8, loss = 0.53245275
Iteration 9, loss = 0.49422499
Iteration 10, loss = 0.45577562
Iteration 11, loss = 0.41796046
Iteration 12, loss = 0.38140908
Iteration 13, loss = 0.34741225
Iteration 14, loss = 0.31541855
Iteration 15, loss = 0.28607308
Iteration 16, loss = 0.25931849
Iteration 17, loss = 0.23524536
Iteration 18, loss = 0.21329956
Iteration 19, loss = 0.19399957
Iteration 20, loss = 0.17654016
Iteration 21, loss = 0.16109368
Iteration 22, loss = 0.14738850
Iteration 23, loss = 0.13528757
Iteration 24, loss = 0.12439476
Iteration 25, loss = 0.11479775
Iteration 26, loss = 0.10620148
Iteration 27, loss = 0.09856798
Iteration 28, loss = 0.09180284
Iteration 29, loss = 0.08569978
Iteration 30, loss = 0.08016188
Iteration 31, loss = 0.07526231
Iteration 32, loss = 0.07081306
Iteration 33, loss = 0.06677579
Iteration 34, loss = 0.06314105
Iteration 35, loss = 0.05979763
Iteration 36, loss = 0.05672713
Iteration 37, loss = 0.05398776
Iteration 38, loss = 0.05142480
Iteration 39, loss = 0.04912753
Iteration 40, loss = 0.04693105
Iteration 41, loss = 0.04493439
Iteration 42, loss = 0.04310839
Iteration 43, loss = 0.04139984
Iteration 44, loss = 0.03979203
Iteration 45, loss = 0.03835003
Iteration 46, loss = 0.03696399
Iteration 47, loss = 0.03572388
Iteration 48, loss = 0.03453153
Iteration 49, loss = 0.03338448
Iteration 50, loss = 0.03229314
Iteration 51, loss = 0.03133186
Iteration 52, loss = 0.03041712
Iteration 53, loss = 0.02954097
Iteration 54, loss = 0.02870981
Iteration 55, loss = 0.02796047
Iteration 56, loss = 0.02724349
Iteration 57, loss = 0.02655536
Iteration 58, loss = 0.02589830
Iteration 59, loss = 0.02529525
Iteration 60, loss = 0.02468872
Iteration 61, loss = 0.02414751
Iteration 62, loss = 0.02359025
Iteration 63, loss = 0.02312056
Iteration 64, loss = 0.02260276
Iteration 65, loss = 0.02221577
Iteration 66, loss = 0.02175811
Iteration 67, loss = 0.02142416
Iteration 68, loss = 0.02093106
Iteration 69, loss = 0.02053844
Iteration 70, loss = 0.02021200
Iteration 71, loss = 0.01982849
Iteration 72, loss = 0.01958338
Iteration 73, loss = 0.01924104
Iteration 74, loss = 0.01890603
Iteration 75, loss = 0.01858691
Iteration 76, loss = 0.01831198
Iteration 77, loss = 0.01806194
Iteration 78, loss = 0.01779180
Iteration 79, loss = 0.01755484
Iteration 80, loss = 0.01732238
Iteration 81, loss = 0.01711284
Iteration 82, loss = 0.01692966
Iteration 83, loss = 0.01666406
Iteration 84, loss = 0.01649014
Iteration 85, loss = 0.01628487
Iteration 86, loss = 0.01607956
Iteration 87, loss = 0.01587511
Iteration 88, loss = 0.01570941
Iteration 89, loss = 0.01550013
Iteration 90, loss = 0.01536569
Iteration 91, loss = 0.01522680
Iteration 92, loss = 0.01503319
Iteration 93, loss = 0.01493073
Iteration 94, loss = 0.01474230
Iteration 95, loss = 0.01466804
Iteration 96, loss = 0.01448263
Iteration 97, loss = 0.01436781
Iteration 98, loss = 0.01425623
Iteration 99, loss = 0.01406637
Iteration 100, loss = 0.01402209
Iteration 101, loss = 0.01384499
Iteration 102, loss = 0.01372701
Iteration 103, loss = 0.01368353
Iteration 104, loss = 0.01359717
Iteration 105, loss = 0.01342364
Iteration 106, loss = 0.01334005
Iteration 107, loss = 0.01333311
Iteration 108, loss = 0.01315494
Iteration 109, loss = 0.01310222
Iteration 110, loss = 0.01297971
Iteration 111, loss = 0.01287032
Iteration 112, loss = 0.01278910
Iteration 113, loss = 0.01271476
Iteration 114, loss = 0.01263166
Iteration 115, loss = 0.01259386
Iteration 116, loss = 0.01253479
Iteration 117, loss = 0.01237565
Iteration 118, loss = 0.01232776
Iteration 119, loss = 0.01227158
Iteration 120, loss = 0.01220379
Iteration 121, loss = 0.01223290
Iteration 122, loss = 0.01206859
Iteration 123, loss = 0.01200765
Iteration 124, loss = 0.01195224
Iteration 125, loss = 0.01189690
Iteration 126, loss = 0.01190465
Iteration 127, loss = 0.01177713
Iteration 128, loss = 0.01174954
Iteration 129, loss = 0.01165483
Iteration 130, loss = 0.01161166
Iteration 131, loss = 0.01155791
Iteration 132, loss = 0.01149930
Iteration 133, loss = 0.01148528
Iteration 134, loss = 0.01138467
Iteration 135, loss = 0.01139676
Iteration 136, loss = 0.01129765
Iteration 137, loss = 0.01124664
Iteration 138, loss = 0.01119606
Iteration 139, loss = 0.01116105
Iteration 140, loss = 0.01117857
Iteration 141, loss = 0.01106767
Iteration 142, loss = 0.01112578
Iteration 143, loss = 0.01100748
Iteration 144, loss = 0.01097288
Iteration 145, loss = 0.01089862
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71309098
Iteration 2, loss = 0.69205737
Iteration 3, loss = 0.67111512
Iteration 4, loss = 0.64741677
Iteration 5, loss = 0.62046978
Iteration 6, loss = 0.58971139
Iteration 7, loss = 0.55636338
Iteration 8, loss = 0.52090740
Iteration 9, loss = 0.48479144
Iteration 10, loss = 0.44817210
Iteration 11, loss = 0.41169031
Iteration 12, loss = 0.37657468
Iteration 13, loss = 0.34293180
Iteration 14, loss = 0.31166979
Iteration 15, loss = 0.28286318
Iteration 16, loss = 0.25632668
Iteration 17, loss = 0.23242895
Iteration 18, loss = 0.21089908
Iteration 19, loss = 0.19160938
Iteration 20, loss = 0.17447487
Iteration 21, loss = 0.15905113
Iteration 22, loss = 0.14542260
Iteration 23, loss = 0.13337929
Iteration 24, loss = 0.12265607
Iteration 25, loss = 0.11305793
Iteration 26, loss = 0.10455906
Iteration 27, loss = 0.09697461
Iteration 28, loss = 0.09019665
Iteration 29, loss = 0.08413982
Iteration 30, loss = 0.07870075
Iteration 31, loss = 0.07385517
Iteration 32, loss = 0.06940759
Iteration 33, loss = 0.06538996
Iteration 34, loss = 0.06175273
Iteration 35, loss = 0.05841017
Iteration 36, loss = 0.05547434
Iteration 37, loss = 0.05266856
Iteration 38, loss = 0.05017731
Iteration 39, loss = 0.04789034
Iteration 40, loss = 0.04568238
Iteration 41, loss = 0.04372099
Iteration 42, loss = 0.04194809
Iteration 43, loss = 0.04021010
Iteration 44, loss = 0.03861893
Iteration 45, loss = 0.03716181
Iteration 46, loss = 0.03582823
Iteration 47, loss = 0.03453662
Iteration 48, loss = 0.03335286
Iteration 49, loss = 0.03224913
Iteration 50, loss = 0.03120800
Iteration 51, loss = 0.03022467
Iteration 52, loss = 0.02934871
Iteration 53, loss = 0.02848008
Iteration 54, loss = 0.02766622
Iteration 55, loss = 0.02689612
Iteration 56, loss = 0.02616277
Iteration 57, loss = 0.02547900
Iteration 58, loss = 0.02484733
Iteration 59, loss = 0.02426382
Iteration 60, loss = 0.02365512
Iteration 61, loss = 0.02311243
Iteration 62, loss = 0.02259473
Iteration 63, loss = 0.02217311
Iteration 64, loss = 0.02162675
Iteration 65, loss = 0.02116182
Iteration 66, loss = 0.02078231
Iteration 67, loss = 0.02032289
Iteration 68, loss = 0.01995542
Iteration 69, loss = 0.01957155
Iteration 70, loss = 0.01922976
Iteration 71, loss = 0.01887947
Iteration 72, loss = 0.01857063
Iteration 73, loss = 0.01822384
Iteration 74, loss = 0.01795308
Iteration 75, loss = 0.01764218
Iteration 76, loss = 0.01742683
Iteration 77, loss = 0.01714719
Iteration 78, loss = 0.01687362
Iteration 79, loss = 0.01665022
Iteration 80, loss = 0.01638860
Iteration 81, loss = 0.01619426
Iteration 82, loss = 0.01599566
Iteration 83, loss = 0.01577783
Iteration 84, loss = 0.01555350
Iteration 85, loss = 0.01533817
Iteration 86, loss = 0.01517528
Iteration 87, loss = 0.01496947
Iteration 88, loss = 0.01482464
Iteration 89, loss = 0.01467514
Iteration 90, loss = 0.01448610
Iteration 91, loss = 0.01432537
Iteration 92, loss = 0.01417789
Iteration 93, loss = 0.01412638
Iteration 94, loss = 0.01393005
Iteration 95, loss = 0.01380529
Iteration 96, loss = 0.01367765
Iteration 97, loss = 0.01348366
Iteration 98, loss = 0.01338811
Iteration 99, loss = 0.01322877
Iteration 100, loss = 0.01313911
Iteration 101, loss = 0.01303276
Iteration 102, loss = 0.01290449
Iteration 103, loss = 0.01282836
Iteration 104, loss = 0.01272445
Iteration 105, loss = 0.01262555
Iteration 106, loss = 0.01252447
Iteration 107, loss = 0.01240995
Iteration 108, loss = 0.01235953
Iteration 109, loss = 0.01228905
Iteration 110, loss = 0.01217923
Iteration 111, loss = 0.01209389
Iteration 112, loss = 0.01199079
Iteration 113, loss = 0.01193268
Iteration 114, loss = 0.01184142
Iteration 115, loss = 0.01177616
Iteration 116, loss = 0.01171078
Iteration 117, loss = 0.01161935
Iteration 118, loss = 0.01155689
Iteration 119, loss = 0.01156299
Iteration 120, loss = 0.01141590
Iteration 121, loss = 0.01136459
Iteration 122, loss = 0.01131004
Iteration 123, loss = 0.01124693
Iteration 124, loss = 0.01124959
Iteration 125, loss = 0.01113618
Iteration 126, loss = 0.01105791
Iteration 127, loss = 0.01097945
Iteration 128, loss = 0.01093863
Iteration 129, loss = 0.01092021
Iteration 130, loss = 0.01084982
Iteration 131, loss = 0.01081950
Iteration 132, loss = 0.01072733
Iteration 133, loss = 0.01071793
Iteration 134, loss = 0.01063835
Iteration 135, loss = 0.01062661
Iteration 136, loss = 0.01057314
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71060930
Iteration 2, loss = 0.67291144
Iteration 3, loss = 0.61678528
Iteration 4, loss = 0.54154736
Iteration 5, loss = 0.45944155
Iteration 6, loss = 0.37961610
Iteration 7, loss = 0.31012060
Iteration 8, loss = 0.25313594
Iteration 9, loss = 0.20762378
Iteration 10, loss = 0.17249164
Iteration 11, loss = 0.14529935
Iteration 12, loss = 0.12431777
Iteration 13, loss = 0.10810354
Iteration 14, loss = 0.09514329
Iteration 15, loss = 0.08499813
Iteration 16, loss = 0.07676958
Iteration 17, loss = 0.06986296
Iteration 18, loss = 0.06422638
Iteration 19, loss = 0.05967856
Iteration 20, loss = 0.05567584
Iteration 21, loss = 0.05209154
Iteration 22, loss = 0.04942699
Iteration 23, loss = 0.04694963
Iteration 24, loss = 0.04480079
Iteration 25, loss = 0.04282465
Iteration 26, loss = 0.04111021
Iteration 27, loss = 0.03956249
Iteration 28, loss = 0.03848522
Iteration 29, loss = 0.03714688
Iteration 30, loss = 0.03627886
Iteration 31, loss = 0.03508711
Iteration 32, loss = 0.03460950
Iteration 33, loss = 0.03347885
Iteration 34, loss = 0.03293001
Iteration 35, loss = 0.03214045
Iteration 36, loss = 0.03165134
Iteration 37, loss = 0.03102100
Iteration 38, loss = 0.03076579
Iteration 39, loss = 0.03026361
Iteration 40, loss = 0.02990749
Iteration 41, loss = 0.02929828
Iteration 42, loss = 0.02915277
Iteration 43, loss = 0.02858637
Iteration 44, loss = 0.02821865
Iteration 45, loss = 0.02807838
Iteration 46, loss = 0.02784080
Iteration 47, loss = 0.02760360
Iteration 48, loss = 0.02755521
Iteration 49, loss = 0.02717782
Iteration 50, loss = 0.02698661
Iteration 51, loss = 0.02691348
Iteration 52, loss = 0.02673027
Iteration 53, loss = 0.02664819
Iteration 54, loss = 0.02641086
Iteration 55, loss = 0.02599698
Iteration 56, loss = 0.02596455
Iteration 57, loss = 0.02577463
Iteration 58, loss = 0.02575220
Iteration 59, loss = 0.02566690
Iteration 60, loss = 0.02540859
Iteration 61, loss = 0.02537307
Iteration 62, loss = 0.02531246
Iteration 63, loss = 0.02505603
Iteration 64, loss = 0.02504657
Iteration 65, loss = 0.02492437
Iteration 66, loss = 0.02505663
Iteration 67, loss = 0.02479544
Iteration 68, loss = 0.02479258
Iteration 69, loss = 0.02456434
Iteration 70, loss = 0.02456012
Iteration 71, loss = 0.02450792
Iteration 72, loss = 0.02447974
Iteration 73, loss = 0.02452507
Iteration 74, loss = 0.02430580
Iteration 75, loss = 0.02437057
Iteration 76, loss = 0.02410421
Iteration 77, loss = 0.02392300
Iteration 78, loss = 0.02428537
Iteration 79, loss = 0.02404520
Iteration 80, loss = 0.02401934
Iteration 81, loss = 0.02386036
Iteration 82, loss = 0.02386412
Iteration 83, loss = 0.02371198
Iteration 84, loss = 0.02389128
Iteration 85, loss = 0.02390193
Iteration 86, loss = 0.02369160
Iteration 87, loss = 0.02386052
Iteration 88, loss = 0.02358845
Iteration 89, loss = 0.02358340
Iteration 90, loss = 0.02356439
Iteration 91, loss = 0.02361488
Iteration 92, loss = 0.02359801
Iteration 93, loss = 0.02373049
Iteration 94, loss = 0.02353051
Iteration 95, loss = 0.02364231
Iteration 96, loss = 0.02339513
Iteration 97, loss = 0.02335159
Iteration 98, loss = 0.02338753
Iteration 99, loss = 0.02345008
Iteration 100, loss = 0.02337619
Iteration 101, loss = 0.02336348
Iteration 102, loss = 0.02317562
Iteration 103, loss = 0.02340109
Iteration 104, loss = 0.02351312
Iteration 105, loss = 0.02310752
Iteration 106, loss = 0.02335904
Iteration 107, loss = 0.02321200
Iteration 108, loss = 0.02340884
Iteration 109, loss = 0.02340990
Iteration 110, loss = 0.02320024
Iteration 111, loss = 0.02330203
Iteration 112, loss = 0.02323567
Iteration 113, loss = 0.02317510
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71781316
Iteration 2, loss = 0.69555821
Iteration 3, loss = 0.67356208
Iteration 4, loss = 0.64822629
Iteration 5, loss = 0.61882693
Iteration 6, loss = 0.58564421
Iteration 7, loss = 0.54981463
Iteration 8, loss = 0.51232168
Iteration 9, loss = 0.47430719
Iteration 10, loss = 0.43673737
Iteration 11, loss = 0.40038639
Iteration 12, loss = 0.36586520
Iteration 13, loss = 0.33320645
Iteration 14, loss = 0.30284041
Iteration 15, loss = 0.27505057
Iteration 16, loss = 0.24983766
Iteration 17, loss = 0.22701655
Iteration 18, loss = 0.20645156
Iteration 19, loss = 0.18815437
Iteration 20, loss = 0.17178002
Iteration 21, loss = 0.15722693
Iteration 22, loss = 0.14436733
Iteration 23, loss = 0.13284793
Iteration 24, loss = 0.12259292
Iteration 25, loss = 0.11354217
Iteration 26, loss = 0.10541547
Iteration 27, loss = 0.09815085
Iteration 28, loss = 0.09174034
Iteration 29, loss = 0.08587883
Iteration 30, loss = 0.08063851
Iteration 31, loss = 0.07593502
Iteration 32, loss = 0.07166603
Iteration 33, loss = 0.06770874
Iteration 34, loss = 0.06418113
Iteration 35, loss = 0.06096772
Iteration 36, loss = 0.05810338
Iteration 37, loss = 0.05533909
Iteration 38, loss = 0.05284356
Iteration 39, loss = 0.05057443
Iteration 40, loss = 0.04848923
Iteration 41, loss = 0.04656999
Iteration 42, loss = 0.04470206
Iteration 43, loss = 0.04307859
Iteration 44, loss = 0.04150592
Iteration 45, loss = 0.04007000
Iteration 46, loss = 0.03871253
Iteration 47, loss = 0.03741865
Iteration 48, loss = 0.03628299
Iteration 49, loss = 0.03513240
Iteration 50, loss = 0.03419750
Iteration 51, loss = 0.03315902
Iteration 52, loss = 0.03219213
Iteration 53, loss = 0.03133752
Iteration 54, loss = 0.03055464
Iteration 55, loss = 0.02982355
Iteration 56, loss = 0.02905724
Iteration 57, loss = 0.02842740
Iteration 58, loss = 0.02774627
Iteration 59, loss = 0.02712586
Iteration 60, loss = 0.02650665
Iteration 61, loss = 0.02598387
Iteration 62, loss = 0.02538356
Iteration 63, loss = 0.02491046
Iteration 64, loss = 0.02444188
Iteration 65, loss = 0.02397301
Iteration 66, loss = 0.02354819
Iteration 67, loss = 0.02315849
Iteration 68, loss = 0.02274088
Iteration 69, loss = 0.02244165
Iteration 70, loss = 0.02210983
Iteration 71, loss = 0.02163628
Iteration 72, loss = 0.02132828
Iteration 73, loss = 0.02102324
Iteration 74, loss = 0.02069152
Iteration 75, loss = 0.02039458
Iteration 76, loss = 0.02014992
Iteration 77, loss = 0.01990202
Iteration 78, loss = 0.01960351
Iteration 79, loss = 0.01943024
Iteration 80, loss = 0.01910113
Iteration 81, loss = 0.01887808
Iteration 82, loss = 0.01862076
Iteration 83, loss = 0.01846018
Iteration 84, loss = 0.01825671
Iteration 85, loss = 0.01804929
Iteration 86, loss = 0.01782593
Iteration 87, loss = 0.01763148
Iteration 88, loss = 0.01750158
Iteration 89, loss = 0.01735036
Iteration 90, loss = 0.01714758
Iteration 91, loss = 0.01706241
Iteration 92, loss = 0.01682434
Iteration 93, loss = 0.01664914
Iteration 94, loss = 0.01657347
Iteration 95, loss = 0.01634102
Iteration 96, loss = 0.01624806
Iteration 97, loss = 0.01606728
Iteration 98, loss = 0.01602739
Iteration 99, loss = 0.01581962
Iteration 100, loss = 0.01568978
Iteration 101, loss = 0.01561217
Iteration 102, loss = 0.01556900
Iteration 103, loss = 0.01537967
Iteration 104, loss = 0.01526254
Iteration 105, loss = 0.01516058
Iteration 106, loss = 0.01506729
Iteration 107, loss = 0.01502552
Iteration 108, loss = 0.01491015
Iteration 109, loss = 0.01472963
Iteration 110, loss = 0.01468564
Iteration 111, loss = 0.01466620
Iteration 112, loss = 0.01456254
Iteration 113, loss = 0.01442373
Iteration 114, loss = 0.01435002
Iteration 115, loss = 0.01428377
Iteration 116, loss = 0.01423664
Iteration 117, loss = 0.01412725
Iteration 118, loss = 0.01402571
Iteration 119, loss = 0.01395031
Iteration 120, loss = 0.01388137
Iteration 121, loss = 0.01382462
Iteration 122, loss = 0.01374442
Iteration 123, loss = 0.01373247
Iteration 124, loss = 0.01362819
Iteration 125, loss = 0.01358481
Iteration 126, loss = 0.01353915
Iteration 127, loss = 0.01349905
Iteration 128, loss = 0.01350145
Iteration 129, loss = 0.01330841
Iteration 130, loss = 0.01325644
Iteration 131, loss = 0.01322902
Iteration 132, loss = 0.01321860
Iteration 133, loss = 0.01311495
Iteration 134, loss = 0.01311487
Iteration 135, loss = 0.01304380
Iteration 136, loss = 0.01298034
Iteration 137, loss = 0.01299502
Iteration 138, loss = 0.01288738
Iteration 139, loss = 0.01287009
Iteration 140, loss = 0.01278888
Iteration 141, loss = 0.01279544
Iteration 142, loss = 0.01272569
Iteration 143, loss = 0.01267261
Iteration 144, loss = 0.01263995
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70855035
Iteration 2, loss = 0.68727810
Iteration 3, loss = 0.66189514
Iteration 4, loss = 0.62837682
Iteration 5, loss = 0.58729362
Iteration 6, loss = 0.53944184
Iteration 7, loss = 0.48726355
Iteration 8, loss = 0.43499741
Iteration 9, loss = 0.38520426
Iteration 10, loss = 0.33907601
Iteration 11, loss = 0.29778533
Iteration 12, loss = 0.26148950
Iteration 13, loss = 0.22987915
Iteration 14, loss = 0.20253739
Iteration 15, loss = 0.17907562
Iteration 16, loss = 0.15902135
Iteration 17, loss = 0.14187564
Iteration 18, loss = 0.12726146
Iteration 19, loss = 0.11486271
Iteration 20, loss = 0.10418612
Iteration 21, loss = 0.09487698
Iteration 22, loss = 0.08692916
Iteration 23, loss = 0.08004312
Iteration 24, loss = 0.07391112
Iteration 25, loss = 0.06852868
Iteration 26, loss = 0.06402892
Iteration 27, loss = 0.05981992
Iteration 28, loss = 0.05600375
Iteration 29, loss = 0.05266730
Iteration 30, loss = 0.04953879
Iteration 31, loss = 0.04682415
Iteration 32, loss = 0.04435914
Iteration 33, loss = 0.04208240
Iteration 34, loss = 0.04019589
Iteration 35, loss = 0.03827111
Iteration 36, loss = 0.03656186
Iteration 37, loss = 0.03505368
Iteration 38, loss = 0.03366245
Iteration 39, loss = 0.03228682
Iteration 40, loss = 0.03102056
Iteration 41, loss = 0.02986513
Iteration 42, loss = 0.02888381
Iteration 43, loss = 0.02805555
Iteration 44, loss = 0.02713547
Iteration 45, loss = 0.02652667
Iteration 46, loss = 0.02559374
Iteration 47, loss = 0.02487611
Iteration 48, loss = 0.02415102
Iteration 49, loss = 0.02345696
Iteration 50, loss = 0.02282599
Iteration 51, loss = 0.02217954
Iteration 52, loss = 0.02164048
Iteration 53, loss = 0.02112700
Iteration 54, loss = 0.02068251
Iteration 55, loss = 0.02016827
Iteration 56, loss = 0.01984529
Iteration 57, loss = 0.01934510
Iteration 58, loss = 0.01899750
Iteration 59, loss = 0.01857056
Iteration 60, loss = 0.01820629
Iteration 61, loss = 0.01794654
Iteration 62, loss = 0.01769418
Iteration 63, loss = 0.01733770
Iteration 64, loss = 0.01697202
Iteration 65, loss = 0.01688090
Iteration 66, loss = 0.01645950
Iteration 67, loss = 0.01621011
Iteration 68, loss = 0.01597359
Iteration 69, loss = 0.01577237
Iteration 70, loss = 0.01552001
Iteration 71, loss = 0.01535575
Iteration 72, loss = 0.01513810
Iteration 73, loss = 0.01496031
Iteration 74, loss = 0.01479858
Iteration 75, loss = 0.01464453
Iteration 76, loss = 0.01447338
Iteration 77, loss = 0.01421536
Iteration 78, loss = 0.01444651
Iteration 79, loss = 0.01428944
Iteration 80, loss = 0.01428328
Iteration 81, loss = 0.01413687
Iteration 82, loss = 0.01393316
Iteration 83, loss = 0.01370110
Iteration 84, loss = 0.01351074
Iteration 85, loss = 0.01340587
Iteration 86, loss = 0.01331947
Iteration 87, loss = 0.01316979
Iteration 88, loss = 0.01315059
Iteration 89, loss = 0.01294894
Iteration 90, loss = 0.01285364
Iteration 91, loss = 0.01271465
Iteration 92, loss = 0.01249048
Iteration 93, loss = 0.01274030
Iteration 94, loss = 0.01262241
Iteration 95, loss = 0.01251388
Iteration 96, loss = 0.01237942
Iteration 97, loss = 0.01223080
Iteration 98, loss = 0.01217071
Iteration 99, loss = 0.01206849
Iteration 100, loss = 0.01195475
Iteration 101, loss = 0.01194200
Iteration 102, loss = 0.01190721
Iteration 103, loss = 0.01187252
Iteration 104, loss = 0.01182144
Iteration 105, loss = 0.01176188
Iteration 106, loss = 0.01182149
Iteration 107, loss = 0.01170597
Iteration 108, loss = 0.01166178
Iteration 109, loss = 0.01154703
Iteration 110, loss = 0.01151221
Iteration 111, loss = 0.01145833
Iteration 112, loss = 0.01128649
Iteration 113, loss = 0.01162275
Iteration 114, loss = 0.01175877
Iteration 115, loss = 0.01167782
Iteration 116, loss = 0.01151816
Iteration 117, loss = 0.01142949
Iteration 118, loss = 0.01138023
Iteration 119, loss = 0.01132489
Iteration 120, loss = 0.01121201
Iteration 121, loss = 0.01104650
Iteration 122, loss = 0.01118575
Iteration 123, loss = 0.01100219
Iteration 124, loss = 0.01092640
Iteration 125, loss = 0.01087882
Iteration 126, loss = 0.01078130
Iteration 127, loss = 0.01076824
Iteration 128, loss = 0.01070922
Iteration 129, loss = 0.01064583
Iteration 130, loss = 0.01065869
Iteration 131, loss = 0.01059214
Iteration 132, loss = 0.01053837
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67951083
Iteration 2, loss = 0.65238504
Iteration 3, loss = 0.61223202
Iteration 4, loss = 0.55298611
Iteration 5, loss = 0.48420592
Iteration 6, loss = 0.41494341
Iteration 7, loss = 0.34959997
Iteration 8, loss = 0.29216399
Iteration 9, loss = 0.24345409
Iteration 10, loss = 0.20345738
Iteration 11, loss = 0.17129399
Iteration 12, loss = 0.14598236
Iteration 13, loss = 0.12564185
Iteration 14, loss = 0.10972406
Iteration 15, loss = 0.09683035
Iteration 16, loss = 0.08630871
Iteration 17, loss = 0.07777024
Iteration 18, loss = 0.07058518
Iteration 19, loss = 0.06461428
Iteration 20, loss = 0.05968015
Iteration 21, loss = 0.05538007
Iteration 22, loss = 0.05180702
Iteration 23, loss = 0.04864862
Iteration 24, loss = 0.04577129
Iteration 25, loss = 0.04348712
Iteration 26, loss = 0.04125297
Iteration 27, loss = 0.03939688
Iteration 28, loss = 0.03786515
Iteration 29, loss = 0.03617851
Iteration 30, loss = 0.03502549
Iteration 31, loss = 0.03386063
Iteration 32, loss = 0.03265327
Iteration 33, loss = 0.03174050
Iteration 34, loss = 0.03087519
Iteration 35, loss = 0.03005800
Iteration 36, loss = 0.02914660
Iteration 37, loss = 0.02846830
Iteration 38, loss = 0.02799494
Iteration 39, loss = 0.02730423
Iteration 40, loss = 0.02684104
Iteration 41, loss = 0.02631439
Iteration 42, loss = 0.02600651
Iteration 43, loss = 0.02564952
Iteration 44, loss = 0.02514819
Iteration 45, loss = 0.02469712
Iteration 46, loss = 0.02435697
Iteration 47, loss = 0.02415239
Iteration 48, loss = 0.02370948
Iteration 49, loss = 0.02349897
Iteration 50, loss = 0.02337191
Iteration 51, loss = 0.02298621
Iteration 52, loss = 0.02292137
Iteration 53, loss = 0.02257721
Iteration 54, loss = 0.02228882
Iteration 55, loss = 0.02212651
Iteration 56, loss = 0.02180955
Iteration 57, loss = 0.02178375
Iteration 58, loss = 0.02175981
Iteration 59, loss = 0.02136174
Iteration 60, loss = 0.02135152
Iteration 61, loss = 0.02118660
Iteration 62, loss = 0.02112126
Iteration 63, loss = 0.02093708
Iteration 64, loss = 0.02071905
Iteration 65, loss = 0.02071859
Iteration 66, loss = 0.02054046
Iteration 67, loss = 0.02055371
Iteration 68, loss = 0.02044767
Iteration 69, loss = 0.02031983
Iteration 70, loss = 0.02004862
Iteration 71, loss = 0.02018341
Iteration 72, loss = 0.01996615
Iteration 73, loss = 0.01995271
Iteration 74, loss = 0.01989449
Iteration 75, loss = 0.01968637
Iteration 76, loss = 0.01971494
Iteration 77, loss = 0.01961822
Iteration 78, loss = 0.01969178
Iteration 79, loss = 0.01956357
Iteration 80, loss = 0.01950202
Iteration 81, loss = 0.01938695
Iteration 82, loss = 0.01942739
Iteration 83, loss = 0.01930282
Iteration 84, loss = 0.01932361
Iteration 85, loss = 0.01922574
Iteration 86, loss = 0.01913427
Iteration 87, loss = 0.01907310
Iteration 88, loss = 0.01903849
Iteration 89, loss = 0.01900833
Iteration 90, loss = 0.01894045
Iteration 91, loss = 0.01891004
Iteration 92, loss = 0.01889846
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67367351
Iteration 2, loss = 0.62149030
Iteration 3, loss = 0.55281692
Iteration 4, loss = 0.47369502
Iteration 5, loss = 0.39474285
Iteration 6, loss = 0.32410782
Iteration 7, loss = 0.26455446
Iteration 8, loss = 0.21606755
Iteration 9, loss = 0.17799279
Iteration 10, loss = 0.14860590
Iteration 11, loss = 0.12578138
Iteration 12, loss = 0.10800891
Iteration 13, loss = 0.09426763
Iteration 14, loss = 0.08296739
Iteration 15, loss = 0.07397159
Iteration 16, loss = 0.06673837
Iteration 17, loss = 0.06072513
Iteration 18, loss = 0.05577865
Iteration 19, loss = 0.05139288
Iteration 20, loss = 0.04788391
Iteration 21, loss = 0.04481446
Iteration 22, loss = 0.04235983
Iteration 23, loss = 0.04004821
Iteration 24, loss = 0.03822716
Iteration 25, loss = 0.03661693
Iteration 26, loss = 0.03472818
Iteration 27, loss = 0.03332449
Iteration 28, loss = 0.03204214
Iteration 29, loss = 0.03099076
Iteration 30, loss = 0.03003130
Iteration 31, loss = 0.02888818
Iteration 32, loss = 0.02825713
Iteration 33, loss = 0.02734374
Iteration 34, loss = 0.02711606
Iteration 35, loss = 0.02638206
Iteration 36, loss = 0.02585296
Iteration 37, loss = 0.02514940
Iteration 38, loss = 0.02463573
Iteration 39, loss = 0.02410623
Iteration 40, loss = 0.02375459
Iteration 41, loss = 0.02339492
Iteration 42, loss = 0.02310484
Iteration 43, loss = 0.02282109
Iteration 44, loss = 0.02237944
Iteration 45, loss = 0.02199667
Iteration 46, loss = 0.02172190
Iteration 47, loss = 0.02169359
Iteration 48, loss = 0.02137048
Iteration 49, loss = 0.02129618
Iteration 50, loss = 0.02096593
Iteration 51, loss = 0.02073010
Iteration 52, loss = 0.02065964
Iteration 53, loss = 0.02035394
Iteration 54, loss = 0.02014701
Iteration 55, loss = 0.02007400
Iteration 56, loss = 0.01995177
Iteration 57, loss = 0.01968141
Iteration 58, loss = 0.01969300
Iteration 59, loss = 0.01964859
Iteration 60, loss = 0.01963416
Iteration 61, loss = 0.01945592
Iteration 62, loss = 0.01923496
Iteration 63, loss = 0.01922667
Iteration 64, loss = 0.01881627
Iteration 65, loss = 0.01899666
Iteration 66, loss = 0.01872771
Iteration 67, loss = 0.01894738
Iteration 68, loss = 0.01860477
Iteration 69, loss = 0.01879680
Iteration 70, loss = 0.01876421
Iteration 71, loss = 0.01857620
Iteration 72, loss = 0.01855795
Iteration 73, loss = 0.01849195
Iteration 74, loss = 0.01839995
Iteration 75, loss = 0.01842952
Iteration 76, loss = 0.01830340
Iteration 77, loss = 0.01810147
Iteration 78, loss = 0.01827049
Iteration 79, loss = 0.01819767
Iteration 80, loss = 0.01797668
Iteration 81, loss = 0.01784529
Iteration 82, loss = 0.01782782
Iteration 83, loss = 0.01767094
Iteration 84, loss = 0.01793597
Iteration 85, loss = 0.01765274
Iteration 86, loss = 0.01782406
Iteration 87, loss = 0.01796837
Iteration 88, loss = 0.01760277
Iteration 89, loss = 0.01766032
Iteration 90, loss = 0.01750187
Iteration 91, loss = 0.01786915
Iteration 92, loss = 0.01758924
Iteration 93, loss = 0.01750226
Iteration 94, loss = 0.01758282
Iteration 95, loss = 0.01780156
Iteration 96, loss = 0.01747142
Iteration 97, loss = 0.01728392
Iteration 98, loss = 0.01737006
Iteration 99, loss = 0.01739528
Iteration 100, loss = 0.01743568
Iteration 101, loss = 0.01731488
Iteration 102, loss = 0.01720613
Iteration 103, loss = 0.01731144
Iteration 104, loss = 0.01726167
Iteration 105, loss = 0.01718530
Iteration 106, loss = 0.01736928
Iteration 107, loss = 0.01725582
Iteration 108, loss = 0.01724642
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71881316
Iteration 2, loss = 0.67555842
Iteration 3, loss = 0.61544978
Iteration 4, loss = 0.53971175
Iteration 5, loss = 0.45608787
Iteration 6, loss = 0.37633908
Iteration 7, loss = 0.30575815
Iteration 8, loss = 0.24783855
Iteration 9, loss = 0.20225793
Iteration 10, loss = 0.16731391
Iteration 11, loss = 0.14078567
Iteration 12, loss = 0.12044972
Iteration 13, loss = 0.10455902
Iteration 14, loss = 0.09244147
Iteration 15, loss = 0.08260891
Iteration 16, loss = 0.07450041
Iteration 17, loss = 0.06818608
Iteration 18, loss = 0.06280325
Iteration 19, loss = 0.05829313
Iteration 20, loss = 0.05440861
Iteration 21, loss = 0.05124765
Iteration 22, loss = 0.04837280
Iteration 23, loss = 0.04606800
Iteration 24, loss = 0.04379495
Iteration 25, loss = 0.04232261
Iteration 26, loss = 0.04064820
Iteration 27, loss = 0.03900119
Iteration 28, loss = 0.03792201
Iteration 29, loss = 0.03667299
Iteration 30, loss = 0.03579190
Iteration 31, loss = 0.03469764
Iteration 32, loss = 0.03400564
Iteration 33, loss = 0.03337121
Iteration 34, loss = 0.03237288
Iteration 35, loss = 0.03194324
Iteration 36, loss = 0.03100961
Iteration 37, loss = 0.03062536
Iteration 38, loss = 0.03019055
Iteration 39, loss = 0.03002810
Iteration 40, loss = 0.02940136
Iteration 41, loss = 0.02906668
Iteration 42, loss = 0.02857232
Iteration 43, loss = 0.02811167
Iteration 44, loss = 0.02782764
Iteration 45, loss = 0.02756212
Iteration 46, loss = 0.02748637
Iteration 47, loss = 0.02740024
Iteration 48, loss = 0.02714193
Iteration 49, loss = 0.02657661
Iteration 50, loss = 0.02640338
Iteration 51, loss = 0.02629627
Iteration 52, loss = 0.02609516
Iteration 53, loss = 0.02616342
Iteration 54, loss = 0.02559247
Iteration 55, loss = 0.02568785
Iteration 56, loss = 0.02539003
Iteration 57, loss = 0.02531492
Iteration 58, loss = 0.02499406
Iteration 59, loss = 0.02493298
Iteration 60, loss = 0.02477655
Iteration 61, loss = 0.02481199
Iteration 62, loss = 0.02456844
Iteration 63, loss = 0.02441887
Iteration 64, loss = 0.02452331
Iteration 65, loss = 0.02452797
Iteration 66, loss = 0.02426290
Iteration 67, loss = 0.02387591
Iteration 68, loss = 0.02379709
Iteration 69, loss = 0.02407512
Iteration 70, loss = 0.02396575
Iteration 71, loss = 0.02363744
Iteration 72, loss = 0.02371787
Iteration 73, loss = 0.02374850
Iteration 74, loss = 0.02368153
Iteration 75, loss = 0.02392682
Iteration 76, loss = 0.02347810
Iteration 77, loss = 0.02340262
Iteration 78, loss = 0.02325642
Iteration 79, loss = 0.02343189
Iteration 80, loss = 0.02329578
Iteration 81, loss = 0.02360408
Iteration 82, loss = 0.02316078
Iteration 83, loss = 0.02308383
Iteration 84, loss = 0.02331377
Iteration 85, loss = 0.02306198
Iteration 86, loss = 0.02299499
Iteration 87, loss = 0.02302873
Iteration 88, loss = 0.02293400
Iteration 89, loss = 0.02300877
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69016172
Iteration 2, loss = 0.67486084
Iteration 3, loss = 0.66192072
Iteration 4, loss = 0.64906709
Iteration 5, loss = 0.63564554
Iteration 6, loss = 0.62121417
Iteration 7, loss = 0.60542315
Iteration 8, loss = 0.58831548
Iteration 9, loss = 0.56951798
Iteration 10, loss = 0.54937145
Iteration 11, loss = 0.52775497
Iteration 12, loss = 0.50500013
Iteration 13, loss = 0.48122296
Iteration 14, loss = 0.45690620
Iteration 15, loss = 0.43237800
Iteration 16, loss = 0.40804944
Iteration 17, loss = 0.38402568
Iteration 18, loss = 0.36091179
Iteration 19, loss = 0.33859001
Iteration 20, loss = 0.31745613
Iteration 21, loss = 0.29724906
Iteration 22, loss = 0.27819952
Iteration 23, loss = 0.26030751
Iteration 24, loss = 0.24355852
Iteration 25, loss = 0.22780968
Iteration 26, loss = 0.21322135
Iteration 27, loss = 0.19964818
Iteration 28, loss = 0.18700083
Iteration 29, loss = 0.17532030
Iteration 30, loss = 0.16447061
Iteration 31, loss = 0.15442262
Iteration 32, loss = 0.14511767
Iteration 33, loss = 0.13653155
Iteration 34, loss = 0.12858724
Iteration 35, loss = 0.12125434
Iteration 36, loss = 0.11442512
Iteration 37, loss = 0.10816942
Iteration 38, loss = 0.10229422
Iteration 39, loss = 0.09693234
Iteration 40, loss = 0.09190741
Iteration 41, loss = 0.08728071
Iteration 42, loss = 0.08294044
Iteration 43, loss = 0.07895444
Iteration 44, loss = 0.07520156
Iteration 45, loss = 0.07171793
Iteration 46, loss = 0.06845856
Iteration 47, loss = 0.06539039
Iteration 48, loss = 0.06255334
Iteration 49, loss = 0.05986500
Iteration 50, loss = 0.05737533
Iteration 51, loss = 0.05500986
Iteration 52, loss = 0.05281497
Iteration 53, loss = 0.05071368
Iteration 54, loss = 0.04876060
Iteration 55, loss = 0.04691560
Iteration 56, loss = 0.04518620
Iteration 57, loss = 0.04353196
Iteration 58, loss = 0.04199895
Iteration 59, loss = 0.04050871
Iteration 60, loss = 0.03910543
Iteration 61, loss = 0.03778219
Iteration 62, loss = 0.03652557
Iteration 63, loss = 0.03532392
Iteration 64, loss = 0.03418526
Iteration 65, loss = 0.03310998
Iteration 66, loss = 0.03207445
Iteration 67, loss = 0.03109561
Iteration 68, loss = 0.03015601
Iteration 69, loss = 0.02928063
Iteration 70, loss = 0.02842911
Iteration 71, loss = 0.02760954
Iteration 72, loss = 0.02684445
Iteration 73, loss = 0.02611174
Iteration 74, loss = 0.02540006
Iteration 75, loss = 0.02471386
Iteration 76, loss = 0.02406306
Iteration 77, loss = 0.02343003
Iteration 78, loss = 0.02280337
Iteration 79, loss = 0.02225110
Iteration 80, loss = 0.02168140
Iteration 81, loss = 0.02112928
Iteration 82, loss = 0.02062262
Iteration 83, loss = 0.02012230
Iteration 84, loss = 0.01964849
Iteration 85, loss = 0.01918039
Iteration 86, loss = 0.01874459
Iteration 87, loss = 0.01832199
Iteration 88, loss = 0.01791108
Iteration 89, loss = 0.01751655
Iteration 90, loss = 0.01714206
Iteration 91, loss = 0.01676670
Iteration 92, loss = 0.01641813
Iteration 93, loss = 0.01607572
Iteration 94, loss = 0.01574435
Iteration 95, loss = 0.01541523
Iteration 96, loss = 0.01511108
Iteration 97, loss = 0.01479998
Iteration 98, loss = 0.01451420
Iteration 99, loss = 0.01423177
Iteration 100, loss = 0.01395533
Iteration 101, loss = 0.01368761
Iteration 102, loss = 0.01343302
Iteration 103, loss = 0.01318719
Iteration 104, loss = 0.01294671
Iteration 105, loss = 0.01271061
Iteration 106, loss = 0.01247480
Iteration 107, loss = 0.01225850
Iteration 108, loss = 0.01203965
Iteration 109, loss = 0.01183682
Iteration 110, loss = 0.01162699
Iteration 111, loss = 0.01143081
Iteration 112, loss = 0.01123561
Iteration 113, loss = 0.01104767
Iteration 114, loss = 0.01087127
Iteration 115, loss = 0.01069051
Iteration 116, loss = 0.01052065
Iteration 117, loss = 0.01035283
Iteration 118, loss = 0.01018706
Iteration 119, loss = 0.01002477
Iteration 120, loss = 0.00987609
Iteration 121, loss = 0.00971797
Iteration 122, loss = 0.00956817
Iteration 123, loss = 0.00941565
Iteration 124, loss = 0.00927439
Iteration 125, loss = 0.00913760
Iteration 126, loss = 0.00900374
Iteration 127, loss = 0.00887146
Iteration 128, loss = 0.00873781
Iteration 129, loss = 0.00861642
Iteration 130, loss = 0.00848971
Iteration 131, loss = 0.00836678
Iteration 132, loss = 0.00825072
Iteration 133, loss = 0.00814171
Iteration 134, loss = 0.00803273
Iteration 135, loss = 0.00792664
Iteration 136, loss = 0.00781848
Iteration 137, loss = 0.00771608
Iteration 138, loss = 0.00761456
Iteration 139, loss = 0.00751601
Iteration 140, loss = 0.00741655
Iteration 141, loss = 0.00732158
Iteration 142, loss = 0.00722756
Iteration 143, loss = 0.00713586
Iteration 144, loss = 0.00704771
Iteration 145, loss = 0.00695583
Iteration 146, loss = 0.00686922
Iteration 147, loss = 0.00678664
Iteration 148, loss = 0.00670640
Iteration 149, loss = 0.00662257
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68314653
Iteration 2, loss = 0.65485109
Iteration 3, loss = 0.61641856
Iteration 4, loss = 0.56405564
Iteration 5, loss = 0.50331861
Iteration 6, loss = 0.44156937
Iteration 7, loss = 0.38287876
Iteration 8, loss = 0.32888564
Iteration 9, loss = 0.28124773
Iteration 10, loss = 0.24032303
Iteration 11, loss = 0.20581342
Iteration 12, loss = 0.17709700
Iteration 13, loss = 0.15343620
Iteration 14, loss = 0.13418060
Iteration 15, loss = 0.11817140
Iteration 16, loss = 0.10498666
Iteration 17, loss = 0.09395613
Iteration 18, loss = 0.08485255
Iteration 19, loss = 0.07718791
Iteration 20, loss = 0.07065546
Iteration 21, loss = 0.06486285
Iteration 22, loss = 0.06008249
Iteration 23, loss = 0.05597511
Iteration 24, loss = 0.05216043
Iteration 25, loss = 0.04900833
Iteration 26, loss = 0.04622067
Iteration 27, loss = 0.04365122
Iteration 28, loss = 0.04150843
Iteration 29, loss = 0.03936495
Iteration 30, loss = 0.03759925
Iteration 31, loss = 0.03600625
Iteration 32, loss = 0.03463736
Iteration 33, loss = 0.03328671
Iteration 34, loss = 0.03214869
Iteration 35, loss = 0.03086816
Iteration 36, loss = 0.02986733
Iteration 37, loss = 0.02902389
Iteration 38, loss = 0.02823489
Iteration 39, loss = 0.02734997
Iteration 40, loss = 0.02678680
Iteration 41, loss = 0.02598904
Iteration 42, loss = 0.02548742
Iteration 43, loss = 0.02483857
Iteration 44, loss = 0.02433623
Iteration 45, loss = 0.02387324
Iteration 46, loss = 0.02332860
Iteration 47, loss = 0.02285871
Iteration 48, loss = 0.02244159
Iteration 49, loss = 0.02212541
Iteration 50, loss = 0.02186870
Iteration 51, loss = 0.02146765
Iteration 52, loss = 0.02110063
Iteration 53, loss = 0.02075909
Iteration 54, loss = 0.02055781
Iteration 55, loss = 0.02034206
Iteration 56, loss = 0.01997837
Iteration 57, loss = 0.01968107
Iteration 58, loss = 0.01957130
Iteration 59, loss = 0.01940337
Iteration 60, loss = 0.01917400
Iteration 61, loss = 0.01899452
Iteration 62, loss = 0.01881104
Iteration 63, loss = 0.01882784
Iteration 64, loss = 0.01850971
Iteration 65, loss = 0.01842509
Iteration 66, loss = 0.01820024
Iteration 67, loss = 0.01788628
Iteration 68, loss = 0.01791927
Iteration 69, loss = 0.01777246
Iteration 70, loss = 0.01765275
Iteration 71, loss = 0.01742755
Iteration 72, loss = 0.01732340
Iteration 73, loss = 0.01747685
Iteration 74, loss = 0.01711712
Iteration 75, loss = 0.01723132
Iteration 76, loss = 0.01701259
Iteration 77, loss = 0.01689706
Iteration 78, loss = 0.01669696
Iteration 79, loss = 0.01674930
Iteration 80, loss = 0.01669052
Iteration 81, loss = 0.01663616
Iteration 82, loss = 0.01655214
Iteration 83, loss = 0.01649354
Iteration 84, loss = 0.01638194
Iteration 85, loss = 0.01645674
Iteration 86, loss = 0.01625051
Iteration 87, loss = 0.01612660
Iteration 88, loss = 0.01611879
Iteration 89, loss = 0.01595871
Iteration 90, loss = 0.01596404
Iteration 91, loss = 0.01599530
Iteration 92, loss = 0.01584494
Iteration 93, loss = 0.01594585
Iteration 94, loss = 0.01588569
Iteration 95, loss = 0.01592147
Iteration 96, loss = 0.01576975
Iteration 97, loss = 0.01566766
Iteration 98, loss = 0.01559819
Iteration 99, loss = 0.01570735
Iteration 100, loss = 0.01569919
Iteration 101, loss = 0.01561214
Iteration 102, loss = 0.01543813
Iteration 103, loss = 0.01545795
Iteration 104, loss = 0.01554996
Iteration 105, loss = 0.01538140
Iteration 106, loss = 0.01539994
Iteration 107, loss = 0.01535906
Iteration 108, loss = 0.01534021
Iteration 109, loss = 0.01522242
Iteration 110, loss = 0.01534742
Iteration 111, loss = 0.01515038
Iteration 112, loss = 0.01514846
Iteration 113, loss = 0.01522791
Iteration 114, loss = 0.01506484
Iteration 115, loss = 0.01500769
Iteration 116, loss = 0.01513786
Iteration 117, loss = 0.01499791
Iteration 118, loss = 0.01507463
Iteration 119, loss = 0.01497481
Iteration 120, loss = 0.01487194
Iteration 121, loss = 0.01506254
Iteration 122, loss = 0.01482858
Iteration 123, loss = 0.01497585
Iteration 124, loss = 0.01493928
Iteration 125, loss = 0.01495610
Iteration 126, loss = 0.01485989
Iteration 127, loss = 0.01502440
Iteration 128, loss = 0.01474726
Iteration 129, loss = 0.01472009
Iteration 130, loss = 0.01486791
Iteration 131, loss = 0.01472825
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67903500
Iteration 2, loss = 0.63709671
Iteration 3, loss = 0.57096256
Iteration 4, loss = 0.49145195
Iteration 5, loss = 0.41308075
Iteration 6, loss = 0.34275084
Iteration 7, loss = 0.28282405
Iteration 8, loss = 0.23396679
Iteration 9, loss = 0.19502434
Iteration 10, loss = 0.16448288
Iteration 11, loss = 0.14050533
Iteration 12, loss = 0.12149282
Iteration 13, loss = 0.10674679
Iteration 14, loss = 0.09486609
Iteration 15, loss = 0.08509170
Iteration 16, loss = 0.07727689
Iteration 17, loss = 0.07089819
Iteration 18, loss = 0.06534902
Iteration 19, loss = 0.06102781
Iteration 20, loss = 0.05736426
Iteration 21, loss = 0.05403530
Iteration 22, loss = 0.05113860
Iteration 23, loss = 0.04866409
Iteration 24, loss = 0.04635629
Iteration 25, loss = 0.04467550
Iteration 26, loss = 0.04310864
Iteration 27, loss = 0.04137114
Iteration 28, loss = 0.04017530
Iteration 29, loss = 0.03900173
Iteration 30, loss = 0.03803602
Iteration 31, loss = 0.03710234
Iteration 32, loss = 0.03627146
Iteration 33, loss = 0.03554414
Iteration 34, loss = 0.03501701
Iteration 35, loss = 0.03411563
Iteration 36, loss = 0.03360810
Iteration 37, loss = 0.03271949
Iteration 38, loss = 0.03256659
Iteration 39, loss = 0.03218301
Iteration 40, loss = 0.03153453
Iteration 41, loss = 0.03116745
Iteration 42, loss = 0.03075615
Iteration 43, loss = 0.03058968
Iteration 44, loss = 0.03022011
Iteration 45, loss = 0.02993877
Iteration 46, loss = 0.02948245
Iteration 47, loss = 0.02941656
Iteration 48, loss = 0.02928155
Iteration 49, loss = 0.02886828
Iteration 50, loss = 0.02873900
Iteration 51, loss = 0.02844837
Iteration 52, loss = 0.02827157
Iteration 53, loss = 0.02818821
Iteration 54, loss = 0.02811633
Iteration 55, loss = 0.02762367
Iteration 56, loss = 0.02781659
Iteration 57, loss = 0.02765298
Iteration 58, loss = 0.02740673
Iteration 59, loss = 0.02745798
Iteration 60, loss = 0.02718802
Iteration 61, loss = 0.02712685
Iteration 62, loss = 0.02693852
Iteration 63, loss = 0.02696940
Iteration 64, loss = 0.02679666
Iteration 65, loss = 0.02656235
Iteration 66, loss = 0.02668990
Iteration 67, loss = 0.02671677
Iteration 68, loss = 0.02630469
Iteration 69, loss = 0.02665697
Iteration 70, loss = 0.02657787
Iteration 71, loss = 0.02627903
Iteration 72, loss = 0.02615995
Iteration 73, loss = 0.02587109
Iteration 74, loss = 0.02628875
Iteration 75, loss = 0.02635301
Iteration 76, loss = 0.02609498
Iteration 77, loss = 0.02628911
Iteration 78, loss = 0.02575018
Iteration 79, loss = 0.02571820
Iteration 80, loss = 0.02572321
Iteration 81, loss = 0.02570989
Iteration 82, loss = 0.02552355
Iteration 83, loss = 0.02573918
Iteration 84, loss = 0.02551550
Iteration 85, loss = 0.02561938
Iteration 86, loss = 0.02557471
Iteration 87, loss = 0.02541978
Iteration 88, loss = 0.02538798
Iteration 89, loss = 0.02543635
Iteration 90, loss = 0.02526746
Iteration 91, loss = 0.02580383
Iteration 92, loss = 0.02543725
Iteration 93, loss = 0.02548613
Iteration 94, loss = 0.02521953
Iteration 95, loss = 0.02536622
Iteration 96, loss = 0.02532311
Iteration 97, loss = 0.02520852
Iteration 98, loss = 0.02498413
Iteration 99, loss = 0.02527966
Iteration 100, loss = 0.02501400
Iteration 101, loss = 0.02508891
Iteration 102, loss = 0.02542888
Iteration 103, loss = 0.02507753
Iteration 104, loss = 0.02488638
Iteration 105, loss = 0.02505127
Iteration 106, loss = 0.02529268
Iteration 107, loss = 0.02515064
Iteration 108, loss = 0.02493388
Iteration 109, loss = 0.02491176
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68821861
Iteration 2, loss = 0.62254015
Iteration 3, loss = 0.53611943
Iteration 4, loss = 0.43967235
Iteration 5, loss = 0.35163331
Iteration 6, loss = 0.27691121
Iteration 7, loss = 0.21866879
Iteration 8, loss = 0.17484735
Iteration 9, loss = 0.14274893
Iteration 10, loss = 0.11935846
Iteration 11, loss = 0.10220825
Iteration 12, loss = 0.08899323
Iteration 13, loss = 0.07910660
Iteration 14, loss = 0.07099972
Iteration 15, loss = 0.06485157
Iteration 16, loss = 0.05964179
Iteration 17, loss = 0.05573756
Iteration 18, loss = 0.05183384
Iteration 19, loss = 0.04922787
Iteration 20, loss = 0.04647097
Iteration 21, loss = 0.04432086
Iteration 22, loss = 0.04241512
Iteration 23, loss = 0.04154788
Iteration 24, loss = 0.03999248
Iteration 25, loss = 0.03874121
Iteration 26, loss = 0.03769491
Iteration 27, loss = 0.03666743
Iteration 28, loss = 0.03586509
Iteration 29, loss = 0.03494001
Iteration 30, loss = 0.03431972
Iteration 31, loss = 0.03352841
Iteration 32, loss = 0.03291485
Iteration 33, loss = 0.03264354
Iteration 34, loss = 0.03214815
Iteration 35, loss = 0.03164306
Iteration 36, loss = 0.03121111
Iteration 37, loss = 0.03098790
Iteration 38, loss = 0.03073917
Iteration 39, loss = 0.03048882
Iteration 40, loss = 0.03055086
Iteration 41, loss = 0.03000503
Iteration 42, loss = 0.02978212
Iteration 43, loss = 0.02945298
Iteration 44, loss = 0.02929071
Iteration 45, loss = 0.02891724
Iteration 46, loss = 0.02859919
Iteration 47, loss = 0.02840838
Iteration 48, loss = 0.02822760
Iteration 49, loss = 0.02822535
Iteration 50, loss = 0.02835052
Iteration 51, loss = 0.02785134
Iteration 52, loss = 0.02796437
Iteration 53, loss = 0.02748593
Iteration 54, loss = 0.02730909
Iteration 55, loss = 0.02752706
Iteration 56, loss = 0.02723321
Iteration 57, loss = 0.02722109
Iteration 58, loss = 0.02708758
Iteration 59, loss = 0.02691148
Iteration 60, loss = 0.02683824
Iteration 61, loss = 0.02691825
Iteration 62, loss = 0.02693330
Iteration 63, loss = 0.02679680
Iteration 64, loss = 0.02674928
Iteration 65, loss = 0.02676137
Iteration 66, loss = 0.02656891
Iteration 67, loss = 0.02650393
Iteration 68, loss = 0.02663284
Iteration 69, loss = 0.02680388
Iteration 70, loss = 0.02691404
Iteration 71, loss = 0.02709376
Iteration 72, loss = 0.02645053
Iteration 73, loss = 0.02681448
Iteration 74, loss = 0.02646949
Iteration 75, loss = 0.02639701
Iteration 76, loss = 0.02658007
Iteration 77, loss = 0.02668519
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71377444
Iteration 2, loss = 0.65869039
Iteration 3, loss = 0.58650309
Iteration 4, loss = 0.49832802
Iteration 5, loss = 0.40752432
Iteration 6, loss = 0.32591524
Iteration 7, loss = 0.25917320
Iteration 8, loss = 0.20731361
Iteration 9, loss = 0.16840971
Iteration 10, loss = 0.13925905
Iteration 11, loss = 0.11747958
Iteration 12, loss = 0.10114503
Iteration 13, loss = 0.08847674
Iteration 14, loss = 0.07853534
Iteration 15, loss = 0.07069958
Iteration 16, loss = 0.06441972
Iteration 17, loss = 0.05920818
Iteration 18, loss = 0.05508470
Iteration 19, loss = 0.05147252
Iteration 20, loss = 0.04849149
Iteration 21, loss = 0.04608928
Iteration 22, loss = 0.04405075
Iteration 23, loss = 0.04187598
Iteration 24, loss = 0.04011680
Iteration 25, loss = 0.03865592
Iteration 26, loss = 0.03748341
Iteration 27, loss = 0.03608205
Iteration 28, loss = 0.03528582
Iteration 29, loss = 0.03432046
Iteration 30, loss = 0.03341392
Iteration 31, loss = 0.03274532
Iteration 32, loss = 0.03206982
Iteration 33, loss = 0.03161349
Iteration 34, loss = 0.03113480
Iteration 35, loss = 0.03062719
Iteration 36, loss = 0.03006698
Iteration 37, loss = 0.02954367
Iteration 38, loss = 0.02953490
Iteration 39, loss = 0.02883122
Iteration 40, loss = 0.02870232
Iteration 41, loss = 0.02827660
Iteration 42, loss = 0.02787056
Iteration 43, loss = 0.02777214
Iteration 44, loss = 0.02751933
Iteration 45, loss = 0.02713325
Iteration 46, loss = 0.02724082
Iteration 47, loss = 0.02675998
Iteration 48, loss = 0.02701636
Iteration 49, loss = 0.02629257
Iteration 50, loss = 0.02632831
Iteration 51, loss = 0.02617281
Iteration 52, loss = 0.02605623
Iteration 53, loss = 0.02585111
Iteration 54, loss = 0.02568049
Iteration 55, loss = 0.02564227
Iteration 56, loss = 0.02547284
Iteration 57, loss = 0.02537883
Iteration 58, loss = 0.02510518
Iteration 59, loss = 0.02497883
Iteration 60, loss = 0.02501128
Iteration 61, loss = 0.02503504
Iteration 62, loss = 0.02496689
Iteration 63, loss = 0.02482347
Iteration 64, loss = 0.02468328
Iteration 65, loss = 0.02481091
Iteration 66, loss = 0.02454982
Iteration 67, loss = 0.02459717
Iteration 68, loss = 0.02449459
Iteration 69, loss = 0.02434140
Iteration 70, loss = 0.02429556
Iteration 71, loss = 0.02426968
Iteration 72, loss = 0.02430025
Iteration 73, loss = 0.02440374
Iteration 74, loss = 0.02429863
Iteration 75, loss = 0.02405779
Iteration 76, loss = 0.02404512
Iteration 77, loss = 0.02385603
Iteration 78, loss = 0.02421794
Iteration 79, loss = 0.02374084
Iteration 80, loss = 0.02403537
Iteration 81, loss = 0.02428768
Iteration 82, loss = 0.02395031
Iteration 83, loss = 0.02364431
Iteration 84, loss = 0.02375742
Iteration 85, loss = 0.02367551
Iteration 86, loss = 0.02345845
Iteration 87, loss = 0.02401081
Iteration 88, loss = 0.02370787
Iteration 89, loss = 0.02360847
Iteration 90, loss = 0.02361317
Iteration 91, loss = 0.02355133
Iteration 92, loss = 0.02378111
Iteration 93, loss = 0.02353169
Iteration 94, loss = 0.02339273
Iteration 95, loss = 0.02333152
Iteration 96, loss = 0.02330795
Iteration 97, loss = 0.02361215
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68849895
Iteration 2, loss = 0.66446149
Iteration 3, loss = 0.63649224
Iteration 4, loss = 0.60174264
Iteration 5, loss = 0.55968148
Iteration 6, loss = 0.51237146
Iteration 7, loss = 0.46329072
Iteration 8, loss = 0.41459755
Iteration 9, loss = 0.36811439
Iteration 10, loss = 0.32545189
Iteration 11, loss = 0.28672697
Iteration 12, loss = 0.25232573
Iteration 13, loss = 0.22221238
Iteration 14, loss = 0.19614335
Iteration 15, loss = 0.17373691
Iteration 16, loss = 0.15477520
Iteration 17, loss = 0.13847491
Iteration 18, loss = 0.12466717
Iteration 19, loss = 0.11266596
Iteration 20, loss = 0.10274864
Iteration 21, loss = 0.09400562
Iteration 22, loss = 0.08652705
Iteration 23, loss = 0.07993474
Iteration 24, loss = 0.07433877
Iteration 25, loss = 0.06908824
Iteration 26, loss = 0.06460716
Iteration 27, loss = 0.06065283
Iteration 28, loss = 0.05717827
Iteration 29, loss = 0.05392075
Iteration 30, loss = 0.05126081
Iteration 31, loss = 0.04869398
Iteration 32, loss = 0.04638144
Iteration 33, loss = 0.04451714
Iteration 34, loss = 0.04256730
Iteration 35, loss = 0.04077825
Iteration 36, loss = 0.03924822
Iteration 37, loss = 0.03764464
Iteration 38, loss = 0.03631913
Iteration 39, loss = 0.03499409
Iteration 40, loss = 0.03400578
Iteration 41, loss = 0.03294342
Iteration 42, loss = 0.03198689
Iteration 43, loss = 0.03102455
Iteration 44, loss = 0.03012618
Iteration 45, loss = 0.02918656
Iteration 46, loss = 0.02847834
Iteration 47, loss = 0.02776166
Iteration 48, loss = 0.02709920
Iteration 49, loss = 0.02633624
Iteration 50, loss = 0.02618610
Iteration 51, loss = 0.02577170
Iteration 52, loss = 0.02543521
Iteration 53, loss = 0.02498008
Iteration 54, loss = 0.02438284
Iteration 55, loss = 0.02387643
Iteration 56, loss = 0.02358695
Iteration 57, loss = 0.02330165
Iteration 58, loss = 0.02272309
Iteration 59, loss = 0.02245133
Iteration 60, loss = 0.02211172
Iteration 61, loss = 0.02176016
Iteration 62, loss = 0.02138578
Iteration 63, loss = 0.02092136
Iteration 64, loss = 0.02066950
Iteration 65, loss = 0.02038725
Iteration 66, loss = 0.02026580
Iteration 67, loss = 0.01998251
Iteration 68, loss = 0.01979927
Iteration 69, loss = 0.01957409
Iteration 70, loss = 0.01950725
Iteration 71, loss = 0.01929238
Iteration 72, loss = 0.01912941
Iteration 73, loss = 0.01891997
Iteration 74, loss = 0.01859907
Iteration 75, loss = 0.01848175
Iteration 76, loss = 0.01833887
Iteration 77, loss = 0.01811942
Iteration 78, loss = 0.01803149
Iteration 79, loss = 0.01786755
Iteration 80, loss = 0.01763628
Iteration 81, loss = 0.01747837
Iteration 82, loss = 0.01738469
Iteration 83, loss = 0.01734759
Iteration 84, loss = 0.01743115
Iteration 85, loss = 0.01720945
Iteration 86, loss = 0.01711336
Iteration 87, loss = 0.01700355
Iteration 88, loss = 0.01693277
Iteration 89, loss = 0.01671778
Iteration 90, loss = 0.01672842
Iteration 91, loss = 0.01668085
Iteration 92, loss = 0.01657764
Iteration 93, loss = 0.01637774
Iteration 94, loss = 0.01635860
Iteration 95, loss = 0.01627508
Iteration 96, loss = 0.01623706
Iteration 97, loss = 0.01624119
Iteration 98, loss = 0.01610563
Iteration 99, loss = 0.01605864
Iteration 100, loss = 0.01583370
Iteration 101, loss = 0.01574800
Iteration 102, loss = 0.01565646
Iteration 103, loss = 0.01554918
Iteration 104, loss = 0.01546294
Iteration 105, loss = 0.01544994
Iteration 106, loss = 0.01535828
Iteration 107, loss = 0.01549709
Iteration 108, loss = 0.01548300
Iteration 109, loss = 0.01544660
Iteration 110, loss = 0.01540148
Iteration 111, loss = 0.01530497
Iteration 112, loss = 0.01516000
Iteration 113, loss = 0.01519361
Iteration 114, loss = 0.01512020
Iteration 115, loss = 0.01500473
Iteration 116, loss = 0.01496938
Iteration 117, loss = 0.01500668
Iteration 118, loss = 0.01491091
Iteration 119, loss = 0.01480235
Iteration 120, loss = 0.01471716
Iteration 121, loss = 0.01494186
Iteration 122, loss = 0.01494031
Iteration 123, loss = 0.01495196
Iteration 124, loss = 0.01493213
Iteration 125, loss = 0.01480894
Iteration 126, loss = 0.01465686
Iteration 127, loss = 0.01456517
Iteration 128, loss = 0.01439377
Iteration 129, loss = 0.01449883
Iteration 130, loss = 0.01450566
Iteration 131, loss = 0.01438974
Iteration 132, loss = 0.01432203
Iteration 133, loss = 0.01429135
Iteration 134, loss = 0.01430425
Iteration 135, loss = 0.01434120
Iteration 136, loss = 0.01426323
Iteration 137, loss = 0.01446348
Iteration 138, loss = 0.01452348
Iteration 139, loss = 0.01439863
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67679343
Iteration 2, loss = 0.62793721
Iteration 3, loss = 0.55054123
Iteration 4, loss = 0.45933715
Iteration 5, loss = 0.37253048
Iteration 6, loss = 0.29735249
Iteration 7, loss = 0.23708883
Iteration 8, loss = 0.19061067
Iteration 9, loss = 0.15607341
Iteration 10, loss = 0.13028654
Iteration 11, loss = 0.11051670
Iteration 12, loss = 0.09573916
Iteration 13, loss = 0.08410145
Iteration 14, loss = 0.07505901
Iteration 15, loss = 0.06785776
Iteration 16, loss = 0.06207406
Iteration 17, loss = 0.05734404
Iteration 18, loss = 0.05334857
Iteration 19, loss = 0.04973277
Iteration 20, loss = 0.04698646
Iteration 21, loss = 0.04456112
Iteration 22, loss = 0.04223601
Iteration 23, loss = 0.04046795
Iteration 24, loss = 0.03877222
Iteration 25, loss = 0.03745022
Iteration 26, loss = 0.03636541
Iteration 27, loss = 0.03519367
Iteration 28, loss = 0.03400288
Iteration 29, loss = 0.03338274
Iteration 30, loss = 0.03252218
Iteration 31, loss = 0.03161867
Iteration 32, loss = 0.03099295
Iteration 33, loss = 0.03075040
Iteration 34, loss = 0.03010268
Iteration 35, loss = 0.02963376
Iteration 36, loss = 0.02909817
Iteration 37, loss = 0.02868981
Iteration 38, loss = 0.02829759
Iteration 39, loss = 0.02790461
Iteration 40, loss = 0.02777087
Iteration 41, loss = 0.02734065
Iteration 42, loss = 0.02695011
Iteration 43, loss = 0.02671193
Iteration 44, loss = 0.02653758
Iteration 45, loss = 0.02631251
Iteration 46, loss = 0.02613359
Iteration 47, loss = 0.02621568
Iteration 48, loss = 0.02582412
Iteration 49, loss = 0.02548415
Iteration 50, loss = 0.02552059
Iteration 51, loss = 0.02510245
Iteration 52, loss = 0.02498828
Iteration 53, loss = 0.02498215
Iteration 54, loss = 0.02487279
Iteration 55, loss = 0.02472577
Iteration 56, loss = 0.02467257
Iteration 57, loss = 0.02432427
Iteration 58, loss = 0.02461346
Iteration 59, loss = 0.02484784
Iteration 60, loss = 0.02428134
Iteration 61, loss = 0.02410145
Iteration 62, loss = 0.02405482
Iteration 63, loss = 0.02396079
Iteration 64, loss = 0.02383337
Iteration 65, loss = 0.02381483
Iteration 66, loss = 0.02394613
Iteration 67, loss = 0.02372976
Iteration 68, loss = 0.02384053
Iteration 69, loss = 0.02367278
Iteration 70, loss = 0.02376016
Iteration 71, loss = 0.02390511
Iteration 72, loss = 0.02351350
Iteration 73, loss = 0.02322728
Iteration 74, loss = 0.02336464
Iteration 75, loss = 0.02350914
Iteration 76, loss = 0.02348789
Iteration 77, loss = 0.02324971
Iteration 78, loss = 0.02331303
Iteration 79, loss = 0.02310327
Iteration 80, loss = 0.02316777
Iteration 81, loss = 0.02315656
Iteration 82, loss = 0.02335730
Iteration 83, loss = 0.02315811
Iteration 84, loss = 0.02333584
Iteration 85, loss = 0.02309677
Iteration 86, loss = 0.02291420
Iteration 87, loss = 0.02280763
Iteration 88, loss = 0.02284610
Iteration 89, loss = 0.02279089
Iteration 90, loss = 0.02321485
Iteration 91, loss = 0.02258047
Iteration 92, loss = 0.02278059
Iteration 93, loss = 0.02282500
Iteration 94, loss = 0.02279748
Iteration 95, loss = 0.02283195
Iteration 96, loss = 0.02295212
Iteration 97, loss = 0.02286488
Iteration 98, loss = 0.02252528
Iteration 99, loss = 0.02272923
Iteration 100, loss = 0.02286197
Iteration 101, loss = 0.02264458
Iteration 102, loss = 0.02265721
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68789190
Iteration 2, loss = 0.62161595
Iteration 3, loss = 0.53526253
Iteration 4, loss = 0.44191982
Iteration 5, loss = 0.35445827
Iteration 6, loss = 0.28060563
Iteration 7, loss = 0.22227434
Iteration 8, loss = 0.17821749
Iteration 9, loss = 0.14553112
Iteration 10, loss = 0.12160827
Iteration 11, loss = 0.10342299
Iteration 12, loss = 0.08966493
Iteration 13, loss = 0.07925637
Iteration 14, loss = 0.07086705
Iteration 15, loss = 0.06421178
Iteration 16, loss = 0.05872567
Iteration 17, loss = 0.05430039
Iteration 18, loss = 0.05063028
Iteration 19, loss = 0.04752717
Iteration 20, loss = 0.04520284
Iteration 21, loss = 0.04288363
Iteration 22, loss = 0.04091234
Iteration 23, loss = 0.03987698
Iteration 24, loss = 0.03859398
Iteration 25, loss = 0.03677058
Iteration 26, loss = 0.03544671
Iteration 27, loss = 0.03436124
Iteration 28, loss = 0.03344376
Iteration 29, loss = 0.03307466
Iteration 30, loss = 0.03255336
Iteration 31, loss = 0.03171360
Iteration 32, loss = 0.03081144
Iteration 33, loss = 0.03007672
Iteration 34, loss = 0.02967821
Iteration 35, loss = 0.02911568
Iteration 36, loss = 0.02862798
Iteration 37, loss = 0.02835546
Iteration 38, loss = 0.02801437
Iteration 39, loss = 0.02774451
Iteration 40, loss = 0.02748562
Iteration 41, loss = 0.02725395
Iteration 42, loss = 0.02689287
Iteration 43, loss = 0.02680169
Iteration 44, loss = 0.02656891
Iteration 45, loss = 0.02642128
Iteration 46, loss = 0.02610486
Iteration 47, loss = 0.02589324
Iteration 48, loss = 0.02572405
Iteration 49, loss = 0.02542663
Iteration 50, loss = 0.02541547
Iteration 51, loss = 0.02536658
Iteration 52, loss = 0.02508893
Iteration 53, loss = 0.02499610
Iteration 54, loss = 0.02480960
Iteration 55, loss = 0.02497410
Iteration 56, loss = 0.02513833
Iteration 57, loss = 0.02480544
Iteration 58, loss = 0.02485891
Iteration 59, loss = 0.02455880
Iteration 60, loss = 0.02481626
Iteration 61, loss = 0.02486155
Iteration 62, loss = 0.02462995
Iteration 63, loss = 0.02452480
Iteration 64, loss = 0.02424596
Iteration 65, loss = 0.02502495
Iteration 66, loss = 0.02469527
Iteration 67, loss = 0.02433149
Iteration 68, loss = 0.02403772
Iteration 69, loss = 0.02399023
Iteration 70, loss = 0.02387716
Iteration 71, loss = 0.02365971
Iteration 72, loss = 0.02359243
Iteration 73, loss = 0.02357597
Iteration 74, loss = 0.02328786
Iteration 75, loss = 0.02341064
Iteration 76, loss = 0.02331573
Iteration 77, loss = 0.02320318
Iteration 78, loss = 0.02316009
Iteration 79, loss = 0.02344738
Iteration 80, loss = 0.02312348
Iteration 81, loss = 0.02301601
Iteration 82, loss = 0.02328326
Iteration 83, loss = 0.02310163
Iteration 84, loss = 0.02318971
Iteration 85, loss = 0.02334497
Iteration 86, loss = 0.02331032
Iteration 87, loss = 0.02344470
Iteration 88, loss = 0.02344582
Iteration 89, loss = 0.02319005
Iteration 90, loss = 0.02308571
Iteration 91, loss = 0.02341483
Iteration 92, loss = 0.02384543
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67877830
Iteration 2, loss = 0.63198900
Iteration 3, loss = 0.56249387
Iteration 4, loss = 0.47598544
Iteration 5, loss = 0.38967295
Iteration 6, loss = 0.31224846
Iteration 7, loss = 0.24841084
Iteration 8, loss = 0.19864670
Iteration 9, loss = 0.16106556
Iteration 10, loss = 0.13288798
Iteration 11, loss = 0.11166570
Iteration 12, loss = 0.09559197
Iteration 13, loss = 0.08314103
Iteration 14, loss = 0.07344915
Iteration 15, loss = 0.06563543
Iteration 16, loss = 0.05934673
Iteration 17, loss = 0.05431655
Iteration 18, loss = 0.05017845
Iteration 19, loss = 0.04643918
Iteration 20, loss = 0.04331697
Iteration 21, loss = 0.04070386
Iteration 22, loss = 0.03846737
Iteration 23, loss = 0.03670352
Iteration 24, loss = 0.03492503
Iteration 25, loss = 0.03351388
Iteration 26, loss = 0.03196757
Iteration 27, loss = 0.03106418
Iteration 28, loss = 0.02989012
Iteration 29, loss = 0.02906926
Iteration 30, loss = 0.02808264
Iteration 31, loss = 0.02751372
Iteration 32, loss = 0.02665173
Iteration 33, loss = 0.02611088
Iteration 34, loss = 0.02580486
Iteration 35, loss = 0.02487314
Iteration 36, loss = 0.02461168
Iteration 37, loss = 0.02391698
Iteration 38, loss = 0.02378361
Iteration 39, loss = 0.02322084
Iteration 40, loss = 0.02285239
Iteration 41, loss = 0.02254711
Iteration 42, loss = 0.02257050
Iteration 43, loss = 0.02217639
Iteration 44, loss = 0.02167485
Iteration 45, loss = 0.02166026
Iteration 46, loss = 0.02130952
Iteration 47, loss = 0.02124910
Iteration 48, loss = 0.02129883
Iteration 49, loss = 0.02067548
Iteration 50, loss = 0.02055611
Iteration 51, loss = 0.02037783
Iteration 52, loss = 0.02041020
Iteration 53, loss = 0.02007160
Iteration 54, loss = 0.02005124
Iteration 55, loss = 0.02011969
Iteration 56, loss = 0.01975201
Iteration 57, loss = 0.01976477
Iteration 58, loss = 0.01953031
Iteration 59, loss = 0.01955895
Iteration 60, loss = 0.01940045
Iteration 61, loss = 0.01925719
Iteration 62, loss = 0.01900831
Iteration 63, loss = 0.01908789
Iteration 64, loss = 0.01912800
Iteration 65, loss = 0.01878516
Iteration 66, loss = 0.01882474
Iteration 67, loss = 0.01872091
Iteration 68, loss = 0.01869500
Iteration 69, loss = 0.01859435
Iteration 70, loss = 0.01883823
Iteration 71, loss = 0.01851909
Iteration 72, loss = 0.01851216
Iteration 73, loss = 0.01862464
Iteration 74, loss = 0.01844508
Iteration 75, loss = 0.01824152
Iteration 76, loss = 0.01832881
Iteration 77, loss = 0.01818458
Iteration 78, loss = 0.01840620
Iteration 79, loss = 0.01818527
Iteration 80, loss = 0.01838354
Iteration 81, loss = 0.01803416
Iteration 82, loss = 0.01791432
Iteration 83, loss = 0.01798389
Iteration 84, loss = 0.01819766
Iteration 85, loss = 0.01801473
Iteration 86, loss = 0.01814382
Iteration 87, loss = 0.01792391
Iteration 88, loss = 0.01775221
Iteration 89, loss = 0.01780069
Iteration 90, loss = 0.01766787
Iteration 91, loss = 0.01780861
Iteration 92, loss = 0.01783325
Iteration 93, loss = 0.01782222
Iteration 94, loss = 0.01786404
Iteration 95, loss = 0.01763952
Iteration 96, loss = 0.01775843
Iteration 97, loss = 0.01779306
Iteration 98, loss = 0.01779635
Iteration 99, loss = 0.01770581
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71578502
Iteration 2, loss = 0.65111956
Iteration 3, loss = 0.56920545
Iteration 4, loss = 0.46710527
Iteration 5, loss = 0.36656855
Iteration 6, loss = 0.28295869
Iteration 7, loss = 0.21916756
Iteration 8, loss = 0.17261707
Iteration 9, loss = 0.13929389
Iteration 10, loss = 0.11552120
Iteration 11, loss = 0.09780809
Iteration 12, loss = 0.08443593
Iteration 13, loss = 0.07449991
Iteration 14, loss = 0.06664290
Iteration 15, loss = 0.05990051
Iteration 16, loss = 0.05506514
Iteration 17, loss = 0.05074200
Iteration 18, loss = 0.04738060
Iteration 19, loss = 0.04462279
Iteration 20, loss = 0.04209649
Iteration 21, loss = 0.04002882
Iteration 22, loss = 0.03775800
Iteration 23, loss = 0.03664028
Iteration 24, loss = 0.03508394
Iteration 25, loss = 0.03410215
Iteration 26, loss = 0.03282032
Iteration 27, loss = 0.03172912
Iteration 28, loss = 0.03060958
Iteration 29, loss = 0.02990195
Iteration 30, loss = 0.02898044
Iteration 31, loss = 0.02848842
Iteration 32, loss = 0.02782061
Iteration 33, loss = 0.02756252
Iteration 34, loss = 0.02696468
Iteration 35, loss = 0.02644239
Iteration 36, loss = 0.02594844
Iteration 37, loss = 0.02573084
Iteration 38, loss = 0.02535451
Iteration 39, loss = 0.02497521
Iteration 40, loss = 0.02445791
Iteration 41, loss = 0.02435034
Iteration 42, loss = 0.02401741
Iteration 43, loss = 0.02372659
Iteration 44, loss = 0.02364434
Iteration 45, loss = 0.02381075
Iteration 46, loss = 0.02347145
Iteration 47, loss = 0.02333951
Iteration 48, loss = 0.02328419
Iteration 49, loss = 0.02291997
Iteration 50, loss = 0.02257574
Iteration 51, loss = 0.02262898
Iteration 52, loss = 0.02230111
Iteration 53, loss = 0.02222330
Iteration 54, loss = 0.02201068
Iteration 55, loss = 0.02193407
Iteration 56, loss = 0.02164882
Iteration 57, loss = 0.02212245
Iteration 58, loss = 0.02221214
Iteration 59, loss = 0.02171425
Iteration 60, loss = 0.02173180
Iteration 61, loss = 0.02148018
Iteration 62, loss = 0.02127550
Iteration 63, loss = 0.02124738
Iteration 64, loss = 0.02122998
Iteration 65, loss = 0.02127639
Iteration 66, loss = 0.02144794
Iteration 67, loss = 0.02142398
Iteration 68, loss = 0.02131284
Iteration 69, loss = 0.02105916
Iteration 70, loss = 0.02140947
Iteration 71, loss = 0.02110760
Iteration 72, loss = 0.02131321
Iteration 73, loss = 0.02130276
Iteration 74, loss = 0.02115926
Iteration 75, loss = 0.02084696
Iteration 76, loss = 0.02075842
Iteration 77, loss = 0.02067888
Iteration 78, loss = 0.02052407
Iteration 79, loss = 0.02049512
Iteration 80, loss = 0.02029311
Iteration 81, loss = 0.02035484
Iteration 82, loss = 0.02032712
Iteration 83, loss = 0.02016517
Iteration 84, loss = 0.02019801
Iteration 85, loss = 0.02040059
Iteration 86, loss = 0.02058680
Iteration 87, loss = 0.02068791
Iteration 88, loss = 0.02071018
Iteration 89, loss = 0.02029613
Iteration 90, loss = 0.02064159
Iteration 91, loss = 0.02008203
Iteration 92, loss = 0.02010322
Iteration 93, loss = 0.02035374
Iteration 94, loss = 0.02043596
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67284199
Iteration 2, loss = 0.60373152
Iteration 3, loss = 0.50668635
Iteration 4, loss = 0.40226637
Iteration 5, loss = 0.31119987
Iteration 6, loss = 0.23963159
Iteration 7, loss = 0.18710151
Iteration 8, loss = 0.14948725
Iteration 9, loss = 0.12254249
Iteration 10, loss = 0.10307145
Iteration 11, loss = 0.08861793
Iteration 12, loss = 0.07776980
Iteration 13, loss = 0.06947041
Iteration 14, loss = 0.06273677
Iteration 15, loss = 0.05765640
Iteration 16, loss = 0.05311270
Iteration 17, loss = 0.04934478
Iteration 18, loss = 0.04646125
Iteration 19, loss = 0.04399980
Iteration 20, loss = 0.04189875
Iteration 21, loss = 0.04019908
Iteration 22, loss = 0.03862537
Iteration 23, loss = 0.03715910
Iteration 24, loss = 0.03594978
Iteration 25, loss = 0.03477777
Iteration 26, loss = 0.03394014
Iteration 27, loss = 0.03311406
Iteration 28, loss = 0.03210254
Iteration 29, loss = 0.03168908
Iteration 30, loss = 0.03104590
Iteration 31, loss = 0.03042581
Iteration 32, loss = 0.02990824
Iteration 33, loss = 0.02992007
Iteration 34, loss = 0.02923869
Iteration 35, loss = 0.02869658
Iteration 36, loss = 0.02841623
Iteration 37, loss = 0.02802448
Iteration 38, loss = 0.02780351
Iteration 39, loss = 0.02792063
Iteration 40, loss = 0.02725110
Iteration 41, loss = 0.02704732
Iteration 42, loss = 0.02662526
Iteration 43, loss = 0.02685010
Iteration 44, loss = 0.02641233
Iteration 45, loss = 0.02630778
Iteration 46, loss = 0.02636278
Iteration 47, loss = 0.02609817
Iteration 48, loss = 0.02598144
Iteration 49, loss = 0.02589592
Iteration 50, loss = 0.02561006
Iteration 51, loss = 0.02565601
Iteration 52, loss = 0.02553949
Iteration 53, loss = 0.02557953
Iteration 54, loss = 0.02514954
Iteration 55, loss = 0.02495698
Iteration 56, loss = 0.02509358
Iteration 57, loss = 0.02535907
Iteration 58, loss = 0.02466441
Iteration 59, loss = 0.02453572
Iteration 60, loss = 0.02461283
Iteration 61, loss = 0.02485662
Iteration 62, loss = 0.02483967
Iteration 63, loss = 0.02441870
Iteration 64, loss = 0.02459775
Iteration 65, loss = 0.02422003
Iteration 66, loss = 0.02425775
Iteration 67, loss = 0.02429053
Iteration 68, loss = 0.02416216
Iteration 69, loss = 0.02425229
Iteration 70, loss = 0.02402516
Iteration 71, loss = 0.02412971
Iteration 72, loss = 0.02422523
Iteration 73, loss = 0.02397078
Iteration 74, loss = 0.02438856
Iteration 75, loss = 0.02392433
Iteration 76, loss = 0.02390831
Iteration 77, loss = 0.02376661
Iteration 78, loss = 0.02382413
Iteration 79, loss = 0.02402673
Iteration 80, loss = 0.02379164
Iteration 81, loss = 0.02354947
Iteration 82, loss = 0.02372619
Iteration 83, loss = 0.02384402
Iteration 84, loss = 0.02346602
Iteration 85, loss = 0.02372087
Iteration 86, loss = 0.02344835
Iteration 87, loss = 0.02354596
Iteration 88, loss = 0.02373919
Iteration 89, loss = 0.02346565
Iteration 90, loss = 0.02350439
Iteration 91, loss = 0.02336032
Iteration 92, loss = 0.02379033
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68198051
Iteration 2, loss = 0.61647121
Iteration 3, loss = 0.51600644
Iteration 4, loss = 0.40570608
Iteration 5, loss = 0.31099829
Iteration 6, loss = 0.23730237
Iteration 7, loss = 0.18437913
Iteration 8, loss = 0.14683498
Iteration 9, loss = 0.12059054
Iteration 10, loss = 0.10156774
Iteration 11, loss = 0.08742211
Iteration 12, loss = 0.07718700
Iteration 13, loss = 0.06900491
Iteration 14, loss = 0.06264042
Iteration 15, loss = 0.05740057
Iteration 16, loss = 0.05353339
Iteration 17, loss = 0.05004452
Iteration 18, loss = 0.04724355
Iteration 19, loss = 0.04486536
Iteration 20, loss = 0.04294493
Iteration 21, loss = 0.04158822
Iteration 22, loss = 0.04012585
Iteration 23, loss = 0.03858422
Iteration 24, loss = 0.03756203
Iteration 25, loss = 0.03653085
Iteration 26, loss = 0.03537714
Iteration 27, loss = 0.03478377
Iteration 28, loss = 0.03409167
Iteration 29, loss = 0.03384430
Iteration 30, loss = 0.03319031
Iteration 31, loss = 0.03278161
Iteration 32, loss = 0.03215087
Iteration 33, loss = 0.03193341
Iteration 34, loss = 0.03112480
Iteration 35, loss = 0.03084021
Iteration 36, loss = 0.03088301
Iteration 37, loss = 0.03037317
Iteration 38, loss = 0.03032314
Iteration 39, loss = 0.02964133
Iteration 40, loss = 0.02953315
Iteration 41, loss = 0.02947112
Iteration 42, loss = 0.02922807
Iteration 43, loss = 0.02913305
Iteration 44, loss = 0.02885036
Iteration 45, loss = 0.02871074
Iteration 46, loss = 0.02879791
Iteration 47, loss = 0.02857331
Iteration 48, loss = 0.02825522
Iteration 49, loss = 0.02814772
Iteration 50, loss = 0.02838421
Iteration 51, loss = 0.02792057
Iteration 52, loss = 0.02803255
Iteration 53, loss = 0.02784493
Iteration 54, loss = 0.02767070
Iteration 55, loss = 0.02775578
Iteration 56, loss = 0.02799238
Iteration 57, loss = 0.02736047
Iteration 58, loss = 0.02759653
Iteration 59, loss = 0.02731353
Iteration 60, loss = 0.02720826
Iteration 61, loss = 0.02741047
Iteration 62, loss = 0.02696698
Iteration 63, loss = 0.02730720
Iteration 64, loss = 0.02725108
Iteration 65, loss = 0.02731628
Iteration 66, loss = 0.02711486
Iteration 67, loss = 0.02712609
Iteration 68, loss = 0.02680402
Iteration 69, loss = 0.02697669
Iteration 70, loss = 0.02733177
Iteration 71, loss = 0.02702719
Iteration 72, loss = 0.02682231
Iteration 73, loss = 0.02665805
Iteration 74, loss = 0.02666418
Iteration 75, loss = 0.02674646
Iteration 76, loss = 0.02658150
Iteration 77, loss = 0.02690381
Iteration 78, loss = 0.02689274
Iteration 79, loss = 0.02678780
Iteration 80, loss = 0.02680651
Iteration 81, loss = 0.02614457
Iteration 82, loss = 0.02657218
Iteration 83, loss = 0.02651009
Iteration 84, loss = 0.02660435
Iteration 85, loss = 0.02636317
Iteration 86, loss = 0.02634702
Iteration 87, loss = 0.02653831
Iteration 88, loss = 0.02631981
Iteration 89, loss = 0.02634706
Iteration 90, loss = 0.02640188
Iteration 91, loss = 0.02683096
Iteration 92, loss = 0.02609952
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69178071
Iteration 2, loss = 0.63925527
Iteration 3, loss = 0.56733241
Iteration 4, loss = 0.47966855
Iteration 5, loss = 0.39232760
Iteration 6, loss = 0.31331697
Iteration 7, loss = 0.24923400
Iteration 8, loss = 0.19989131
Iteration 9, loss = 0.16290703
Iteration 10, loss = 0.13545714
Iteration 11, loss = 0.11477465
Iteration 12, loss = 0.09909282
Iteration 13, loss = 0.08707482
Iteration 14, loss = 0.07745742
Iteration 15, loss = 0.06976681
Iteration 16, loss = 0.06383496
Iteration 17, loss = 0.05864918
Iteration 18, loss = 0.05451034
Iteration 19, loss = 0.05092438
Iteration 20, loss = 0.04814573
Iteration 21, loss = 0.04558132
Iteration 22, loss = 0.04330638
Iteration 23, loss = 0.04145317
Iteration 24, loss = 0.03958499
Iteration 25, loss = 0.03785540
Iteration 26, loss = 0.03668310
Iteration 27, loss = 0.03567577
Iteration 28, loss = 0.03460593
Iteration 29, loss = 0.03358016
Iteration 30, loss = 0.03270593
Iteration 31, loss = 0.03204380
Iteration 32, loss = 0.03150223
Iteration 33, loss = 0.03071304
Iteration 34, loss = 0.03022489
Iteration 35, loss = 0.02981214
Iteration 36, loss = 0.02930829
Iteration 37, loss = 0.02860787
Iteration 38, loss = 0.02850121
Iteration 39, loss = 0.02819842
Iteration 40, loss = 0.02758040
Iteration 41, loss = 0.02752490
Iteration 42, loss = 0.02727431
Iteration 43, loss = 0.02691654
Iteration 44, loss = 0.02668830
Iteration 45, loss = 0.02628419
Iteration 46, loss = 0.02585097
Iteration 47, loss = 0.02612306
Iteration 48, loss = 0.02558515
Iteration 49, loss = 0.02546771
Iteration 50, loss = 0.02538336
Iteration 51, loss = 0.02517040
Iteration 52, loss = 0.02494702
Iteration 53, loss = 0.02483500
Iteration 54, loss = 0.02470457
Iteration 55, loss = 0.02461080
Iteration 56, loss = 0.02438808
Iteration 57, loss = 0.02447236
Iteration 58, loss = 0.02451369
Iteration 59, loss = 0.02430673
Iteration 60, loss = 0.02389937
Iteration 61, loss = 0.02386678
Iteration 62, loss = 0.02387244
Iteration 63, loss = 0.02387032
Iteration 64, loss = 0.02391558
Iteration 65, loss = 0.02375896
Iteration 66, loss = 0.02360234
Iteration 67, loss = 0.02385590
Iteration 68, loss = 0.02343774
Iteration 69, loss = 0.02342007
Iteration 70, loss = 0.02330118
Iteration 71, loss = 0.02322157
Iteration 72, loss = 0.02321813
Iteration 73, loss = 0.02323216
Iteration 74, loss = 0.02316565
Iteration 75, loss = 0.02316257
Iteration 76, loss = 0.02324641
Iteration 77, loss = 0.02300441
Iteration 78, loss = 0.02304494
Iteration 79, loss = 0.02294227
Iteration 80, loss = 0.02291848
Iteration 81, loss = 0.02283846
Iteration 82, loss = 0.02284857
Iteration 83, loss = 0.02320134
Iteration 84, loss = 0.02266990
Iteration 85, loss = 0.02305632
Iteration 86, loss = 0.02272329
Iteration 87, loss = 0.02264394
Iteration 88, loss = 0.02260806
Iteration 89, loss = 0.02262085
Iteration 90, loss = 0.02248100
Iteration 91, loss = 0.02241341
Iteration 92, loss = 0.02264100
Iteration 93, loss = 0.02234027
Iteration 94, loss = 0.02239908
Iteration 95, loss = 0.02260486
Iteration 96, loss = 0.02237501
Iteration 97, loss = 0.02234705
Iteration 98, loss = 0.02228076
Iteration 99, loss = 0.02232774
Iteration 100, loss = 0.02230809
Iteration 101, loss = 0.02225110
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69309384
Iteration 2, loss = 0.67591746
Iteration 3, loss = 0.66172626
Iteration 4, loss = 0.64757983
Iteration 5, loss = 0.63296361
Iteration 6, loss = 0.61725053
Iteration 7, loss = 0.60040935
Iteration 8, loss = 0.58229519
Iteration 9, loss = 0.56325914
Iteration 10, loss = 0.54330462
Iteration 11, loss = 0.52263985
Iteration 12, loss = 0.50133265
Iteration 13, loss = 0.47960388
Iteration 14, loss = 0.45749412
Iteration 15, loss = 0.43549733
Iteration 16, loss = 0.41344851
Iteration 17, loss = 0.39181459
Iteration 18, loss = 0.37049083
Iteration 19, loss = 0.34995910
Iteration 20, loss = 0.33029142
Iteration 21, loss = 0.31128415
Iteration 22, loss = 0.29327137
Iteration 23, loss = 0.27609225
Iteration 24, loss = 0.25984215
Iteration 25, loss = 0.24441184
Iteration 26, loss = 0.23003494
Iteration 27, loss = 0.21635654
Iteration 28, loss = 0.20372175
Iteration 29, loss = 0.19184145
Iteration 30, loss = 0.18075271
Iteration 31, loss = 0.17040360
Iteration 32, loss = 0.16074609
Iteration 33, loss = 0.15179922
Iteration 34, loss = 0.14344272
Iteration 35, loss = 0.13564121
Iteration 36, loss = 0.12840893
Iteration 37, loss = 0.12170551
Iteration 38, loss = 0.11546863
Iteration 39, loss = 0.10961394
Iteration 40, loss = 0.10415358
Iteration 41, loss = 0.09912624
Iteration 42, loss = 0.09442914
Iteration 43, loss = 0.09002930
Iteration 44, loss = 0.08593843
Iteration 45, loss = 0.08209669
Iteration 46, loss = 0.07854164
Iteration 47, loss = 0.07515474
Iteration 48, loss = 0.07203330
Iteration 49, loss = 0.06922977
Iteration 50, loss = 0.06641091
Iteration 51, loss = 0.06382975
Iteration 52, loss = 0.06144944
Iteration 53, loss = 0.05917713
Iteration 54, loss = 0.05707124
Iteration 55, loss = 0.05504426
Iteration 56, loss = 0.05312235
Iteration 57, loss = 0.05135339
Iteration 58, loss = 0.04968736
Iteration 59, loss = 0.04806464
Iteration 60, loss = 0.04655966
Iteration 61, loss = 0.04511018
Iteration 62, loss = 0.04374958
Iteration 63, loss = 0.04244847
Iteration 64, loss = 0.04123748
Iteration 65, loss = 0.04010696
Iteration 66, loss = 0.03902183
Iteration 67, loss = 0.03792219
Iteration 68, loss = 0.03691251
Iteration 69, loss = 0.03597742
Iteration 70, loss = 0.03505201
Iteration 71, loss = 0.03416764
Iteration 72, loss = 0.03341701
Iteration 73, loss = 0.03256699
Iteration 74, loss = 0.03184713
Iteration 75, loss = 0.03108511
Iteration 76, loss = 0.03038675
Iteration 77, loss = 0.02972515
Iteration 78, loss = 0.02909473
Iteration 79, loss = 0.02850353
Iteration 80, loss = 0.02789445
Iteration 81, loss = 0.02734966
Iteration 82, loss = 0.02681263
Iteration 83, loss = 0.02626069
Iteration 84, loss = 0.02574835
Iteration 85, loss = 0.02527970
Iteration 86, loss = 0.02477686
Iteration 87, loss = 0.02433385
Iteration 88, loss = 0.02391606
Iteration 89, loss = 0.02350416
Iteration 90, loss = 0.02308858
Iteration 91, loss = 0.02271320
Iteration 92, loss = 0.02231586
Iteration 93, loss = 0.02196567
Iteration 94, loss = 0.02161288
Iteration 95, loss = 0.02127276
Iteration 96, loss = 0.02095389
Iteration 97, loss = 0.02063920
Iteration 98, loss = 0.02035340
Iteration 99, loss = 0.02004383
Iteration 100, loss = 0.01985352
Iteration 101, loss = 0.01952995
Iteration 102, loss = 0.01925207
Iteration 103, loss = 0.01900047
Iteration 104, loss = 0.01873862
Iteration 105, loss = 0.01848983
Iteration 106, loss = 0.01825987
Iteration 107, loss = 0.01802829
Iteration 108, loss = 0.01778660
Iteration 109, loss = 0.01757220
Iteration 110, loss = 0.01739050
Iteration 111, loss = 0.01717915
Iteration 112, loss = 0.01698561
Iteration 113, loss = 0.01678309
Iteration 114, loss = 0.01660571
Iteration 115, loss = 0.01642983
Iteration 116, loss = 0.01625313
Iteration 117, loss = 0.01610939
Iteration 118, loss = 0.01592507
Iteration 119, loss = 0.01575196
Iteration 120, loss = 0.01560333
Iteration 121, loss = 0.01544928
Iteration 122, loss = 0.01532795
Iteration 123, loss = 0.01518077
Iteration 124, loss = 0.01502445
Iteration 125, loss = 0.01492165
Iteration 126, loss = 0.01478518
Iteration 127, loss = 0.01461346
Iteration 128, loss = 0.01446115
Iteration 129, loss = 0.01435560
Iteration 130, loss = 0.01423410
Iteration 131, loss = 0.01412171
Iteration 132, loss = 0.01401904
Iteration 133, loss = 0.01390188
Iteration 134, loss = 0.01380405
Iteration 135, loss = 0.01372236
Iteration 136, loss = 0.01361807
Iteration 137, loss = 0.01353060
Iteration 138, loss = 0.01339918
Iteration 139, loss = 0.01328523
Iteration 140, loss = 0.01317688
Iteration 141, loss = 0.01307071
Iteration 142, loss = 0.01296431
Iteration 143, loss = 0.01291050
Iteration 144, loss = 0.01280998
Iteration 145, loss = 0.01270571
Iteration 146, loss = 0.01264946
Iteration 147, loss = 0.01252488
Iteration 148, loss = 0.01247830
Iteration 149, loss = 0.01239856
Iteration 150, loss = 0.01232358
Iteration 151, loss = 0.01224258
Iteration 152, loss = 0.01215711
Iteration 153, loss = 0.01207538
Iteration 154, loss = 0.01202716
Iteration 155, loss = 0.01197384
Iteration 156, loss = 0.01186469
Iteration 157, loss = 0.01178715
Iteration 158, loss = 0.01171082
Iteration 159, loss = 0.01171013
Iteration 160, loss = 0.01163178
Iteration 161, loss = 0.01156511
Iteration 162, loss = 0.01149447
Iteration 163, loss = 0.01145748
Iteration 164, loss = 0.01140667
Iteration 165, loss = 0.01138549
Iteration 166, loss = 0.01131040
Iteration 167, loss = 0.01128801
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67302844
Iteration 2, loss = 0.59754660
Iteration 3, loss = 0.48868972
Iteration 4, loss = 0.37787434
Iteration 5, loss = 0.28350119
Iteration 6, loss = 0.21338890
Iteration 7, loss = 0.16379244
Iteration 8, loss = 0.13003042
Iteration 9, loss = 0.10629538
Iteration 10, loss = 0.08916791
Iteration 11, loss = 0.07691788
Iteration 12, loss = 0.06741249
Iteration 13, loss = 0.06030117
Iteration 14, loss = 0.05438730
Iteration 15, loss = 0.04984868
Iteration 16, loss = 0.04622447
Iteration 17, loss = 0.04306611
Iteration 18, loss = 0.04044775
Iteration 19, loss = 0.03823924
Iteration 20, loss = 0.03662882
Iteration 21, loss = 0.03486072
Iteration 22, loss = 0.03376326
Iteration 23, loss = 0.03230904
Iteration 24, loss = 0.03147748
Iteration 25, loss = 0.03039087
Iteration 26, loss = 0.02957804
Iteration 27, loss = 0.02889826
Iteration 28, loss = 0.02830330
Iteration 29, loss = 0.02770223
Iteration 30, loss = 0.02707467
Iteration 31, loss = 0.02664289
Iteration 32, loss = 0.02645525
Iteration 33, loss = 0.02589269
Iteration 34, loss = 0.02570113
Iteration 35, loss = 0.02531316
Iteration 36, loss = 0.02494353
Iteration 37, loss = 0.02468118
Iteration 38, loss = 0.02444396
Iteration 39, loss = 0.02467073
Iteration 40, loss = 0.02397502
Iteration 41, loss = 0.02397108
Iteration 42, loss = 0.02380649
Iteration 43, loss = 0.02364905
Iteration 44, loss = 0.02344519
Iteration 45, loss = 0.02319993
Iteration 46, loss = 0.02315944
Iteration 47, loss = 0.02313717
Iteration 48, loss = 0.02294536
Iteration 49, loss = 0.02294976
Iteration 50, loss = 0.02260328
Iteration 51, loss = 0.02295513
Iteration 52, loss = 0.02276127
Iteration 53, loss = 0.02265390
Iteration 54, loss = 0.02255759
Iteration 55, loss = 0.02250514
Iteration 56, loss = 0.02233187
Iteration 57, loss = 0.02261069
Iteration 58, loss = 0.02221684
Iteration 59, loss = 0.02243883
Iteration 60, loss = 0.02241112
Iteration 61, loss = 0.02203768
Iteration 62, loss = 0.02202862
Iteration 63, loss = 0.02160115
Iteration 64, loss = 0.02191466
Iteration 65, loss = 0.02186604
Iteration 66, loss = 0.02175943
Iteration 67, loss = 0.02189494
Iteration 68, loss = 0.02184246
Iteration 69, loss = 0.02177836
Iteration 70, loss = 0.02167057
Iteration 71, loss = 0.02188390
Iteration 72, loss = 0.02173338
Iteration 73, loss = 0.02144586
Iteration 74, loss = 0.02149796
Iteration 75, loss = 0.02147333
Iteration 76, loss = 0.02138064
Iteration 77, loss = 0.02151364
Iteration 78, loss = 0.02139989
Iteration 79, loss = 0.02145162
Iteration 80, loss = 0.02133846
Iteration 81, loss = 0.02146345
Iteration 82, loss = 0.02136500
Iteration 83, loss = 0.02123996
Iteration 84, loss = 0.02155963
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69920213
Iteration 2, loss = 0.67615586
Iteration 3, loss = 0.64860685
Iteration 4, loss = 0.61298655
Iteration 5, loss = 0.56993844
Iteration 6, loss = 0.52180023
Iteration 7, loss = 0.47134596
Iteration 8, loss = 0.42144717
Iteration 9, loss = 0.37372480
Iteration 10, loss = 0.32964744
Iteration 11, loss = 0.28979375
Iteration 12, loss = 0.25443446
Iteration 13, loss = 0.22365824
Iteration 14, loss = 0.19706926
Iteration 15, loss = 0.17433726
Iteration 16, loss = 0.15500576
Iteration 17, loss = 0.13836070
Iteration 18, loss = 0.12425514
Iteration 19, loss = 0.11210978
Iteration 20, loss = 0.10171193
Iteration 21, loss = 0.09279505
Iteration 22, loss = 0.08525195
Iteration 23, loss = 0.07859229
Iteration 24, loss = 0.07278864
Iteration 25, loss = 0.06759848
Iteration 26, loss = 0.06308862
Iteration 27, loss = 0.05906430
Iteration 28, loss = 0.05551869
Iteration 29, loss = 0.05241238
Iteration 30, loss = 0.04949425
Iteration 31, loss = 0.04699785
Iteration 32, loss = 0.04457391
Iteration 33, loss = 0.04257840
Iteration 34, loss = 0.04064872
Iteration 35, loss = 0.03890007
Iteration 36, loss = 0.03724882
Iteration 37, loss = 0.03571731
Iteration 38, loss = 0.03438611
Iteration 39, loss = 0.03312999
Iteration 40, loss = 0.03196367
Iteration 41, loss = 0.03096678
Iteration 42, loss = 0.03010742
Iteration 43, loss = 0.02911701
Iteration 44, loss = 0.02835144
Iteration 45, loss = 0.02745085
Iteration 46, loss = 0.02679789
Iteration 47, loss = 0.02601453
Iteration 48, loss = 0.02537704
Iteration 49, loss = 0.02470191
Iteration 50, loss = 0.02418875
Iteration 51, loss = 0.02354788
Iteration 52, loss = 0.02294772
Iteration 53, loss = 0.02283498
Iteration 54, loss = 0.02233188
Iteration 55, loss = 0.02212573
Iteration 56, loss = 0.02158722
Iteration 57, loss = 0.02133491
Iteration 58, loss = 0.02073535
Iteration 59, loss = 0.02046570
Iteration 60, loss = 0.02006431
Iteration 61, loss = 0.01965894
Iteration 62, loss = 0.01928859
Iteration 63, loss = 0.01902960
Iteration 64, loss = 0.01871841
Iteration 65, loss = 0.01849509
Iteration 66, loss = 0.01827794
Iteration 67, loss = 0.01800972
Iteration 68, loss = 0.01777645
Iteration 69, loss = 0.01754886
Iteration 70, loss = 0.01731074
Iteration 71, loss = 0.01715239
Iteration 72, loss = 0.01699635
Iteration 73, loss = 0.01684570
Iteration 74, loss = 0.01660204
Iteration 75, loss = 0.01643675
Iteration 76, loss = 0.01626423
Iteration 77, loss = 0.01616647
Iteration 78, loss = 0.01583834
Iteration 79, loss = 0.01594229
Iteration 80, loss = 0.01579361
Iteration 81, loss = 0.01567435
Iteration 82, loss = 0.01555395
Iteration 83, loss = 0.01545468
Iteration 84, loss = 0.01530359
Iteration 85, loss = 0.01511200
Iteration 86, loss = 0.01510887
Iteration 87, loss = 0.01495556
Iteration 88, loss = 0.01488408
Iteration 89, loss = 0.01475791
Iteration 90, loss = 0.01457960
Iteration 91, loss = 0.01475057
Iteration 92, loss = 0.01462797
Iteration 93, loss = 0.01455593
Iteration 94, loss = 0.01437839
Iteration 95, loss = 0.01420446
Iteration 96, loss = 0.01410731
Iteration 97, loss = 0.01403661
Iteration 98, loss = 0.01390741
Iteration 99, loss = 0.01380157
Iteration 100, loss = 0.01392111
Iteration 101, loss = 0.01380969
Iteration 102, loss = 0.01382327
Iteration 103, loss = 0.01372078
Iteration 104, loss = 0.01370285
Iteration 105, loss = 0.01367001
Iteration 106, loss = 0.01352962
Iteration 107, loss = 0.01338456
Iteration 108, loss = 0.01331523
Iteration 109, loss = 0.01337770
Iteration 110, loss = 0.01341771
Iteration 111, loss = 0.01329050
Iteration 112, loss = 0.01331863
Iteration 113, loss = 0.01309585
Iteration 114, loss = 0.01327448
Iteration 115, loss = 0.01314733
Iteration 116, loss = 0.01306708
Iteration 117, loss = 0.01290779
Iteration 118, loss = 0.01288743
Iteration 119, loss = 0.01283146
Iteration 120, loss = 0.01288731
Iteration 121, loss = 0.01287169
Iteration 122, loss = 0.01289563
Iteration 123, loss = 0.01283287
Iteration 124, loss = 0.01294599
Iteration 125, loss = 0.01290977
Iteration 126, loss = 0.01273753
Iteration 127, loss = 0.01268065
Iteration 128, loss = 0.01269771
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67683110
Iteration 2, loss = 0.62253714
Iteration 3, loss = 0.53438478
Iteration 4, loss = 0.43350145
Iteration 5, loss = 0.34148894
Iteration 6, loss = 0.26618692
Iteration 7, loss = 0.20915061
Iteration 8, loss = 0.16698564
Iteration 9, loss = 0.13681110
Iteration 10, loss = 0.11476532
Iteration 11, loss = 0.09829389
Iteration 12, loss = 0.08611073
Iteration 13, loss = 0.07661276
Iteration 14, loss = 0.06910402
Iteration 15, loss = 0.06316318
Iteration 16, loss = 0.05835250
Iteration 17, loss = 0.05423364
Iteration 18, loss = 0.05120469
Iteration 19, loss = 0.04836507
Iteration 20, loss = 0.04581471
Iteration 21, loss = 0.04372179
Iteration 22, loss = 0.04190289
Iteration 23, loss = 0.04044959
Iteration 24, loss = 0.03903389
Iteration 25, loss = 0.03770066
Iteration 26, loss = 0.03691429
Iteration 27, loss = 0.03633026
Iteration 28, loss = 0.03548242
Iteration 29, loss = 0.03448704
Iteration 30, loss = 0.03347494
Iteration 31, loss = 0.03303767
Iteration 32, loss = 0.03282000
Iteration 33, loss = 0.03272021
Iteration 34, loss = 0.03223482
Iteration 35, loss = 0.03137165
Iteration 36, loss = 0.03094418
Iteration 37, loss = 0.03068981
Iteration 38, loss = 0.03052958
Iteration 39, loss = 0.03000725
Iteration 40, loss = 0.02951441
Iteration 41, loss = 0.02964613
Iteration 42, loss = 0.02923435
Iteration 43, loss = 0.02909310
Iteration 44, loss = 0.02897620
Iteration 45, loss = 0.02881252
Iteration 46, loss = 0.02862695
Iteration 47, loss = 0.02821372
Iteration 48, loss = 0.02806021
Iteration 49, loss = 0.02830178
Iteration 50, loss = 0.02813786
Iteration 51, loss = 0.02771332
Iteration 52, loss = 0.02756267
Iteration 53, loss = 0.02770484
Iteration 54, loss = 0.02768792
Iteration 55, loss = 0.02744864
Iteration 56, loss = 0.02705000
Iteration 57, loss = 0.02705164
Iteration 58, loss = 0.02676812
Iteration 59, loss = 0.02670905
Iteration 60, loss = 0.02667876
Iteration 61, loss = 0.02655710
Iteration 62, loss = 0.02633399
Iteration 63, loss = 0.02704673
Iteration 64, loss = 0.02686014
Iteration 65, loss = 0.02690911
Iteration 66, loss = 0.02646927
Iteration 67, loss = 0.02617878
Iteration 68, loss = 0.02601040
Iteration 69, loss = 0.02596973
Iteration 70, loss = 0.02596520
Iteration 71, loss = 0.02638468
Iteration 72, loss = 0.02651490
Iteration 73, loss = 0.02743189
Iteration 74, loss = 0.02708242
Iteration 75, loss = 0.02688553
Iteration 76, loss = 0.02674291
Iteration 77, loss = 0.02617036
Iteration 78, loss = 0.02592083
Iteration 79, loss = 0.02614713
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71176899
Iteration 2, loss = 0.69611879
Iteration 3, loss = 0.68279102
Iteration 4, loss = 0.66961716
Iteration 5, loss = 0.65590551
Iteration 6, loss = 0.64139944
Iteration 7, loss = 0.62541939
Iteration 8, loss = 0.60803276
Iteration 9, loss = 0.58945825
Iteration 10, loss = 0.56925158
Iteration 11, loss = 0.54785244
Iteration 12, loss = 0.52546702
Iteration 13, loss = 0.50220774
Iteration 14, loss = 0.47840219
Iteration 15, loss = 0.45432040
Iteration 16, loss = 0.43024329
Iteration 17, loss = 0.40641365
Iteration 18, loss = 0.38314577
Iteration 19, loss = 0.36069539
Iteration 20, loss = 0.33888431
Iteration 21, loss = 0.31816373
Iteration 22, loss = 0.29832410
Iteration 23, loss = 0.27963907
Iteration 24, loss = 0.26188970
Iteration 25, loss = 0.24529636
Iteration 26, loss = 0.22964417
Iteration 27, loss = 0.21508319
Iteration 28, loss = 0.20148384
Iteration 29, loss = 0.18889621
Iteration 30, loss = 0.17713633
Iteration 31, loss = 0.16623434
Iteration 32, loss = 0.15613060
Iteration 33, loss = 0.14683684
Iteration 34, loss = 0.13819054
Iteration 35, loss = 0.13020865
Iteration 36, loss = 0.12280961
Iteration 37, loss = 0.11597874
Iteration 38, loss = 0.10965323
Iteration 39, loss = 0.10378368
Iteration 40, loss = 0.09844343
Iteration 41, loss = 0.09338645
Iteration 42, loss = 0.08873251
Iteration 43, loss = 0.08440982
Iteration 44, loss = 0.08043503
Iteration 45, loss = 0.07673235
Iteration 46, loss = 0.07323179
Iteration 47, loss = 0.06996743
Iteration 48, loss = 0.06693014
Iteration 49, loss = 0.06417934
Iteration 50, loss = 0.06151574
Iteration 51, loss = 0.05905349
Iteration 52, loss = 0.05674151
Iteration 53, loss = 0.05456681
Iteration 54, loss = 0.05249833
Iteration 55, loss = 0.05056801
Iteration 56, loss = 0.04876042
Iteration 57, loss = 0.04707778
Iteration 58, loss = 0.04544245
Iteration 59, loss = 0.04391274
Iteration 60, loss = 0.04244711
Iteration 61, loss = 0.04108396
Iteration 62, loss = 0.03976489
Iteration 63, loss = 0.03855961
Iteration 64, loss = 0.03742668
Iteration 65, loss = 0.03628771
Iteration 66, loss = 0.03525661
Iteration 67, loss = 0.03423610
Iteration 68, loss = 0.03328278
Iteration 69, loss = 0.03238312
Iteration 70, loss = 0.03150472
Iteration 71, loss = 0.03067408
Iteration 72, loss = 0.02986746
Iteration 73, loss = 0.02911907
Iteration 74, loss = 0.02838095
Iteration 75, loss = 0.02768903
Iteration 76, loss = 0.02703537
Iteration 77, loss = 0.02646026
Iteration 78, loss = 0.02578758
Iteration 79, loss = 0.02524131
Iteration 80, loss = 0.02462440
Iteration 81, loss = 0.02407013
Iteration 82, loss = 0.02356148
Iteration 83, loss = 0.02306085
Iteration 84, loss = 0.02257265
Iteration 85, loss = 0.02213300
Iteration 86, loss = 0.02165199
Iteration 87, loss = 0.02130199
Iteration 88, loss = 0.02090529
Iteration 89, loss = 0.02051878
Iteration 90, loss = 0.02013067
Iteration 91, loss = 0.01976720
Iteration 92, loss = 0.01939395
Iteration 93, loss = 0.01905767
Iteration 94, loss = 0.01870453
Iteration 95, loss = 0.01835555
Iteration 96, loss = 0.01806297
Iteration 97, loss = 0.01775718
Iteration 98, loss = 0.01745761
Iteration 99, loss = 0.01717864
Iteration 100, loss = 0.01692655
Iteration 101, loss = 0.01671936
Iteration 102, loss = 0.01643855
Iteration 103, loss = 0.01617520
Iteration 104, loss = 0.01592926
Iteration 105, loss = 0.01563597
Iteration 106, loss = 0.01543963
Iteration 107, loss = 0.01514034
Iteration 108, loss = 0.01492158
Iteration 109, loss = 0.01470688
Iteration 110, loss = 0.01450912
Iteration 111, loss = 0.01430589
Iteration 112, loss = 0.01418103
Iteration 113, loss = 0.01393908
Iteration 114, loss = 0.01374950
Iteration 115, loss = 0.01356953
Iteration 116, loss = 0.01341701
Iteration 117, loss = 0.01323190
Iteration 118, loss = 0.01307210
Iteration 119, loss = 0.01291717
Iteration 120, loss = 0.01277919
Iteration 121, loss = 0.01264419
Iteration 122, loss = 0.01246660
Iteration 123, loss = 0.01233639
Iteration 124, loss = 0.01218976
Iteration 125, loss = 0.01207315
Iteration 126, loss = 0.01192419
Iteration 127, loss = 0.01179541
Iteration 128, loss = 0.01165791
Iteration 129, loss = 0.01154708
Iteration 130, loss = 0.01145851
Iteration 131, loss = 0.01132293
Iteration 132, loss = 0.01116575
Iteration 133, loss = 0.01109585
Iteration 134, loss = 0.01093754
Iteration 135, loss = 0.01083127
Iteration 136, loss = 0.01072286
Iteration 137, loss = 0.01069247
Iteration 138, loss = 0.01056341
Iteration 139, loss = 0.01047798
Iteration 140, loss = 0.01038798
Iteration 141, loss = 0.01029250
Iteration 142, loss = 0.01021522
Iteration 143, loss = 0.01010655
Iteration 144, loss = 0.01002672
Iteration 145, loss = 0.00996495
Iteration 146, loss = 0.00987729
Iteration 147, loss = 0.00973995
Iteration 148, loss = 0.00969420
Iteration 149, loss = 0.00956867
Iteration 150, loss = 0.00951931
Iteration 151, loss = 0.00942234
Iteration 152, loss = 0.00932968
Iteration 153, loss = 0.00928336
Iteration 154, loss = 0.00919841
Iteration 155, loss = 0.00910131
Iteration 156, loss = 0.00905314
Iteration 157, loss = 0.00895773
Iteration 158, loss = 0.00890243
Iteration 159, loss = 0.00892669
Iteration 160, loss = 0.00879746
Iteration 161, loss = 0.00872602
Iteration 162, loss = 0.00865556
Iteration 163, loss = 0.00862242
Iteration 164, loss = 0.00853468
Iteration 165, loss = 0.00847425
Iteration 166, loss = 0.00842832
Iteration 167, loss = 0.00837423
Iteration 168, loss = 0.00830593
Iteration 169, loss = 0.00828915
Iteration 170, loss = 0.00821564
Iteration 171, loss = 0.00816408
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68210942
Iteration 2, loss = 0.62260414
Iteration 3, loss = 0.52554358
Iteration 4, loss = 0.41076533
Iteration 5, loss = 0.31267441
Iteration 6, loss = 0.23722349
Iteration 7, loss = 0.18292259
Iteration 8, loss = 0.14508240
Iteration 9, loss = 0.11839594
Iteration 10, loss = 0.09900210
Iteration 11, loss = 0.08472918
Iteration 12, loss = 0.07411261
Iteration 13, loss = 0.06591196
Iteration 14, loss = 0.05940527
Iteration 15, loss = 0.05410617
Iteration 16, loss = 0.04993858
Iteration 17, loss = 0.04626052
Iteration 18, loss = 0.04331091
Iteration 19, loss = 0.04115456
Iteration 20, loss = 0.03895423
Iteration 21, loss = 0.03731750
Iteration 22, loss = 0.03575378
Iteration 23, loss = 0.03438541
Iteration 24, loss = 0.03305970
Iteration 25, loss = 0.03218353
Iteration 26, loss = 0.03128547
Iteration 27, loss = 0.03045443
Iteration 28, loss = 0.02973772
Iteration 29, loss = 0.02901420
Iteration 30, loss = 0.02834333
Iteration 31, loss = 0.02768309
Iteration 32, loss = 0.02764570
Iteration 33, loss = 0.02688097
Iteration 34, loss = 0.02656609
Iteration 35, loss = 0.02608270
Iteration 36, loss = 0.02582152
Iteration 37, loss = 0.02545771
Iteration 38, loss = 0.02526305
Iteration 39, loss = 0.02495652
Iteration 40, loss = 0.02465666
Iteration 41, loss = 0.02473632
Iteration 42, loss = 0.02426948
Iteration 43, loss = 0.02402241
Iteration 44, loss = 0.02425799
Iteration 45, loss = 0.02392044
Iteration 46, loss = 0.02392524
Iteration 47, loss = 0.02364618
Iteration 48, loss = 0.02375204
Iteration 49, loss = 0.02348004
Iteration 50, loss = 0.02368648
Iteration 51, loss = 0.02314714
Iteration 52, loss = 0.02290412
Iteration 53, loss = 0.02289537
Iteration 54, loss = 0.02296797
Iteration 55, loss = 0.02270467
Iteration 56, loss = 0.02266837
Iteration 57, loss = 0.02271526
Iteration 58, loss = 0.02287500
Iteration 59, loss = 0.02220903
Iteration 60, loss = 0.02234842
Iteration 61, loss = 0.02250568
Iteration 62, loss = 0.02225740
Iteration 63, loss = 0.02252343
Iteration 64, loss = 0.02207829
Iteration 65, loss = 0.02247527
Iteration 66, loss = 0.02199859
Iteration 67, loss = 0.02191749
Iteration 68, loss = 0.02190844
Iteration 69, loss = 0.02197033
Iteration 70, loss = 0.02195195
Iteration 71, loss = 0.02203068
Iteration 72, loss = 0.02202549
Iteration 73, loss = 0.02203377
Iteration 74, loss = 0.02165322
Iteration 75, loss = 0.02177225
Iteration 76, loss = 0.02194808
Iteration 77, loss = 0.02183877
Iteration 78, loss = 0.02151377
Iteration 79, loss = 0.02169955
Iteration 80, loss = 0.02186567
Iteration 81, loss = 0.02148981
Iteration 82, loss = 0.02169072
Iteration 83, loss = 0.02146629
Iteration 84, loss = 0.02144095
Iteration 85, loss = 0.02152453
Iteration 86, loss = 0.02125626
Iteration 87, loss = 0.02159202
Iteration 88, loss = 0.02157253
Iteration 89, loss = 0.02166920
Iteration 90, loss = 0.02146083
Iteration 91, loss = 0.02162838
Iteration 92, loss = 0.02153735
Iteration 93, loss = 0.02132431
Iteration 94, loss = 0.02123795
Iteration 95, loss = 0.02135763
Iteration 96, loss = 0.02106980
Iteration 97, loss = 0.02124181
Iteration 98, loss = 0.02128942
Iteration 99, loss = 0.02131205
Iteration 100, loss = 0.02125018
Iteration 101, loss = 0.02096138
Iteration 102, loss = 0.02100404
Iteration 103, loss = 0.02147921
Iteration 104, loss = 0.02088732
Iteration 105, loss = 0.02092109
Iteration 106, loss = 0.02125519
Iteration 107, loss = 0.02115622
Iteration 108, loss = 0.02109246
Iteration 109, loss = 0.02093252
Iteration 110, loss = 0.02086765
Iteration 111, loss = 0.02103201
Iteration 112, loss = 0.02098729
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71339225
Iteration 2, loss = 0.67559068
Iteration 3, loss = 0.62984765
Iteration 4, loss = 0.57727224
Iteration 5, loss = 0.51818731
Iteration 6, loss = 0.45711373
Iteration 7, loss = 0.39770174
Iteration 8, loss = 0.34253946
Iteration 9, loss = 0.29344023
Iteration 10, loss = 0.25101840
Iteration 11, loss = 0.21478436
Iteration 12, loss = 0.18475455
Iteration 13, loss = 0.15982324
Iteration 14, loss = 0.13923957
Iteration 15, loss = 0.12229875
Iteration 16, loss = 0.10845742
Iteration 17, loss = 0.09681615
Iteration 18, loss = 0.08721791
Iteration 19, loss = 0.07905445
Iteration 20, loss = 0.07224036
Iteration 21, loss = 0.06635139
Iteration 22, loss = 0.06135760
Iteration 23, loss = 0.05687556
Iteration 24, loss = 0.05322622
Iteration 25, loss = 0.04987019
Iteration 26, loss = 0.04697558
Iteration 27, loss = 0.04446168
Iteration 28, loss = 0.04210850
Iteration 29, loss = 0.04003438
Iteration 30, loss = 0.03831640
Iteration 31, loss = 0.03651490
Iteration 32, loss = 0.03503208
Iteration 33, loss = 0.03378622
Iteration 34, loss = 0.03253817
Iteration 35, loss = 0.03155512
Iteration 36, loss = 0.03066164
Iteration 37, loss = 0.02946127
Iteration 38, loss = 0.02860383
Iteration 39, loss = 0.02783879
Iteration 40, loss = 0.02713768
Iteration 41, loss = 0.02651256
Iteration 42, loss = 0.02586283
Iteration 43, loss = 0.02524743
Iteration 44, loss = 0.02479068
Iteration 45, loss = 0.02425381
Iteration 46, loss = 0.02374019
Iteration 47, loss = 0.02336979
Iteration 48, loss = 0.02298023
Iteration 49, loss = 0.02247600
Iteration 50, loss = 0.02209964
Iteration 51, loss = 0.02184097
Iteration 52, loss = 0.02153612
Iteration 53, loss = 0.02128131
Iteration 54, loss = 0.02088730
Iteration 55, loss = 0.02056802
Iteration 56, loss = 0.02044612
Iteration 57, loss = 0.02020573
Iteration 58, loss = 0.01992699
Iteration 59, loss = 0.01970954
Iteration 60, loss = 0.01952986
Iteration 61, loss = 0.01940498
Iteration 62, loss = 0.01912040
Iteration 63, loss = 0.01902728
Iteration 64, loss = 0.01870123
Iteration 65, loss = 0.01860946
Iteration 66, loss = 0.01853904
Iteration 67, loss = 0.01834063
Iteration 68, loss = 0.01811688
Iteration 69, loss = 0.01825823
Iteration 70, loss = 0.01785693
Iteration 71, loss = 0.01797455
Iteration 72, loss = 0.01770288
Iteration 73, loss = 0.01760897
Iteration 74, loss = 0.01750953
Iteration 75, loss = 0.01745793
Iteration 76, loss = 0.01743755
Iteration 77, loss = 0.01716848
Iteration 78, loss = 0.01713353
Iteration 79, loss = 0.01707361
Iteration 80, loss = 0.01697264
Iteration 81, loss = 0.01698695
Iteration 82, loss = 0.01674365
Iteration 83, loss = 0.01668755
Iteration 84, loss = 0.01669324
Iteration 85, loss = 0.01658452
Iteration 86, loss = 0.01658125
Iteration 87, loss = 0.01657032
Iteration 88, loss = 0.01636336
Iteration 89, loss = 0.01629449
Iteration 90, loss = 0.01638829
Iteration 91, loss = 0.01613096
Iteration 92, loss = 0.01626031
Iteration 93, loss = 0.01620017
Iteration 94, loss = 0.01602822
Iteration 95, loss = 0.01597936
Iteration 96, loss = 0.01597527
Iteration 97, loss = 0.01599013
Iteration 98, loss = 0.01589617
Iteration 99, loss = 0.01592195
Iteration 100, loss = 0.01595613
Iteration 101, loss = 0.01604601
Iteration 102, loss = 0.01578625
Iteration 103, loss = 0.01573019
Iteration 104, loss = 0.01585394
Iteration 105, loss = 0.01565270
Iteration 106, loss = 0.01554580
Iteration 107, loss = 0.01565231
Iteration 108, loss = 0.01555493
Iteration 109, loss = 0.01556919
Iteration 110, loss = 0.01548128
Iteration 111, loss = 0.01566450
Iteration 112, loss = 0.01546758
Iteration 113, loss = 0.01558601
Iteration 114, loss = 0.01552990
Iteration 115, loss = 0.01532537
Iteration 116, loss = 0.01552915
Iteration 117, loss = 0.01538535
Iteration 118, loss = 0.01527214
Iteration 119, loss = 0.01518910
Iteration 120, loss = 0.01524853
Iteration 121, loss = 0.01519407
Iteration 122, loss = 0.01507968
Iteration 123, loss = 0.01520861
Iteration 124, loss = 0.01510030
Iteration 125, loss = 0.01510979
Iteration 126, loss = 0.01509671
Iteration 127, loss = 0.01507180
Iteration 128, loss = 0.01511875
Iteration 129, loss = 0.01509847
Iteration 130, loss = 0.01500141
Iteration 131, loss = 0.01498959
Iteration 132, loss = 0.01507535
Iteration 133, loss = 0.01509979
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69437530
Iteration 2, loss = 0.64521689
Iteration 3, loss = 0.57329357
Iteration 4, loss = 0.48377782
Iteration 5, loss = 0.39323306
Iteration 6, loss = 0.31314215
Iteration 7, loss = 0.24805108
Iteration 8, loss = 0.19823893
Iteration 9, loss = 0.16130509
Iteration 10, loss = 0.13391605
Iteration 11, loss = 0.11311075
Iteration 12, loss = 0.09751945
Iteration 13, loss = 0.08534830
Iteration 14, loss = 0.07589491
Iteration 15, loss = 0.06827683
Iteration 16, loss = 0.06230569
Iteration 17, loss = 0.05718438
Iteration 18, loss = 0.05313776
Iteration 19, loss = 0.04975901
Iteration 20, loss = 0.04669651
Iteration 21, loss = 0.04405819
Iteration 22, loss = 0.04191933
Iteration 23, loss = 0.03982428
Iteration 24, loss = 0.03825147
Iteration 25, loss = 0.03685102
Iteration 26, loss = 0.03545703
Iteration 27, loss = 0.03445976
Iteration 28, loss = 0.03323608
Iteration 29, loss = 0.03240976
Iteration 30, loss = 0.03138763
Iteration 31, loss = 0.03081387
Iteration 32, loss = 0.03014990
Iteration 33, loss = 0.02968245
Iteration 34, loss = 0.02912833
Iteration 35, loss = 0.02859380
Iteration 36, loss = 0.02814889
Iteration 37, loss = 0.02769400
Iteration 38, loss = 0.02772271
Iteration 39, loss = 0.02685211
Iteration 40, loss = 0.02663890
Iteration 41, loss = 0.02618710
Iteration 42, loss = 0.02588743
Iteration 43, loss = 0.02575828
Iteration 44, loss = 0.02540458
Iteration 45, loss = 0.02536442
Iteration 46, loss = 0.02495834
Iteration 47, loss = 0.02489296
Iteration 48, loss = 0.02486893
Iteration 49, loss = 0.02465431
Iteration 50, loss = 0.02435902
Iteration 51, loss = 0.02431017
Iteration 52, loss = 0.02413203
Iteration 53, loss = 0.02389983
Iteration 54, loss = 0.02375325
Iteration 55, loss = 0.02358688
Iteration 56, loss = 0.02353692
Iteration 57, loss = 0.02357849
Iteration 58, loss = 0.02344891
Iteration 59, loss = 0.02321051
Iteration 60, loss = 0.02346849
Iteration 61, loss = 0.02303323
Iteration 62, loss = 0.02310987
Iteration 63, loss = 0.02296174
Iteration 64, loss = 0.02266395
Iteration 65, loss = 0.02266122
Iteration 66, loss = 0.02265067
Iteration 67, loss = 0.02274424
Iteration 68, loss = 0.02265611
Iteration 69, loss = 0.02260768
Iteration 70, loss = 0.02244353
Iteration 71, loss = 0.02255133
Iteration 72, loss = 0.02253549
Iteration 73, loss = 0.02230181
Iteration 74, loss = 0.02214217
Iteration 75, loss = 0.02212639
Iteration 76, loss = 0.02220347
Iteration 77, loss = 0.02228877
Iteration 78, loss = 0.02215570
Iteration 79, loss = 0.02195043
Iteration 80, loss = 0.02219241
Iteration 81, loss = 0.02197575
Iteration 82, loss = 0.02193775
Iteration 83, loss = 0.02234783
Iteration 84, loss = 0.02170280
Iteration 85, loss = 0.02171984
Iteration 86, loss = 0.02195308
Iteration 87, loss = 0.02178152
Iteration 88, loss = 0.02169048
Iteration 89, loss = 0.02178456
Iteration 90, loss = 0.02196305
Iteration 91, loss = 0.02179465
Iteration 92, loss = 0.02172531
Iteration 93, loss = 0.02159719
Iteration 94, loss = 0.02163356
Iteration 95, loss = 0.02139986
Iteration 96, loss = 0.02166344
Iteration 97, loss = 0.02189530
Iteration 98, loss = 0.02169287
Iteration 99, loss = 0.02143111
Iteration 100, loss = 0.02166935
Iteration 101, loss = 0.02156450
Iteration 102, loss = 0.02161593
Iteration 103, loss = 0.02137856
Iteration 104, loss = 0.02133732
Iteration 105, loss = 0.02140795
Iteration 106, loss = 0.02132100
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69824438
Iteration 2, loss = 0.65467508
Iteration 3, loss = 0.60111245
Iteration 4, loss = 0.53863966
Iteration 5, loss = 0.46942401
Iteration 6, loss = 0.40086033
Iteration 7, loss = 0.33738900
Iteration 8, loss = 0.28229309
Iteration 9, loss = 0.23542365
Iteration 10, loss = 0.19714059
Iteration 11, loss = 0.16628203
Iteration 12, loss = 0.14141238
Iteration 13, loss = 0.12167430
Iteration 14, loss = 0.10595039
Iteration 15, loss = 0.09317408
Iteration 16, loss = 0.08299520
Iteration 17, loss = 0.07423811
Iteration 18, loss = 0.06725528
Iteration 19, loss = 0.06129264
Iteration 20, loss = 0.05635405
Iteration 21, loss = 0.05202463
Iteration 22, loss = 0.04835119
Iteration 23, loss = 0.04523866
Iteration 24, loss = 0.04257081
Iteration 25, loss = 0.04011547
Iteration 26, loss = 0.03789002
Iteration 27, loss = 0.03605605
Iteration 28, loss = 0.03448021
Iteration 29, loss = 0.03293129
Iteration 30, loss = 0.03166393
Iteration 31, loss = 0.03035021
Iteration 32, loss = 0.02937916
Iteration 33, loss = 0.02828628
Iteration 34, loss = 0.02743352
Iteration 35, loss = 0.02655778
Iteration 36, loss = 0.02606766
Iteration 37, loss = 0.02511646
Iteration 38, loss = 0.02444073
Iteration 39, loss = 0.02408694
Iteration 40, loss = 0.02346121
Iteration 41, loss = 0.02294358
Iteration 42, loss = 0.02238509
Iteration 43, loss = 0.02186832
Iteration 44, loss = 0.02159480
Iteration 45, loss = 0.02121333
Iteration 46, loss = 0.02091125
Iteration 47, loss = 0.02050495
Iteration 48, loss = 0.02023085
Iteration 49, loss = 0.01999306
Iteration 50, loss = 0.01961736
Iteration 51, loss = 0.01938807
Iteration 52, loss = 0.01907569
Iteration 53, loss = 0.01907367
Iteration 54, loss = 0.01865678
Iteration 55, loss = 0.01859143
Iteration 56, loss = 0.01829382
Iteration 57, loss = 0.01813296
Iteration 58, loss = 0.01817607
Iteration 59, loss = 0.01783395
Iteration 60, loss = 0.01768100
Iteration 61, loss = 0.01747158
Iteration 62, loss = 0.01729561
Iteration 63, loss = 0.01717349
Iteration 64, loss = 0.01709299
Iteration 65, loss = 0.01718157
Iteration 66, loss = 0.01696206
Iteration 67, loss = 0.01676141
Iteration 68, loss = 0.01675242
Iteration 69, loss = 0.01680254
Iteration 70, loss = 0.01648335
Iteration 71, loss = 0.01640842
Iteration 72, loss = 0.01634323
Iteration 73, loss = 0.01629227
Iteration 74, loss = 0.01616506
Iteration 75, loss = 0.01616946
Iteration 76, loss = 0.01599979
Iteration 77, loss = 0.01607783
Iteration 78, loss = 0.01595283
Iteration 79, loss = 0.01579564
Iteration 80, loss = 0.01566601
Iteration 81, loss = 0.01571854
Iteration 82, loss = 0.01558462
Iteration 83, loss = 0.01565148
Iteration 84, loss = 0.01564629
Iteration 85, loss = 0.01551517
Iteration 86, loss = 0.01546262
Iteration 87, loss = 0.01538039
Iteration 88, loss = 0.01515367
Iteration 89, loss = 0.01520360
Iteration 90, loss = 0.01517999
Iteration 91, loss = 0.01512574
Iteration 92, loss = 0.01508457
Iteration 93, loss = 0.01510393
Iteration 94, loss = 0.01497673
Iteration 95, loss = 0.01518410
Iteration 96, loss = 0.01489306
Iteration 97, loss = 0.01501067
Iteration 98, loss = 0.01484144
Iteration 99, loss = 0.01485253
Iteration 100, loss = 0.01476857
Iteration 101, loss = 0.01472094
Iteration 102, loss = 0.01466039
Iteration 103, loss = 0.01471107
Iteration 104, loss = 0.01477766
Iteration 105, loss = 0.01474851
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69650821
Iteration 2, loss = 0.63648296
Iteration 3, loss = 0.54619630
Iteration 4, loss = 0.44181593
Iteration 5, loss = 0.34602088
Iteration 6, loss = 0.26817673
Iteration 7, loss = 0.20967881
Iteration 8, loss = 0.16726144
Iteration 9, loss = 0.13662142
Iteration 10, loss = 0.11428556
Iteration 11, loss = 0.09788987
Iteration 12, loss = 0.08523631
Iteration 13, loss = 0.07561132
Iteration 14, loss = 0.06826517
Iteration 15, loss = 0.06230062
Iteration 16, loss = 0.05740194
Iteration 17, loss = 0.05311598
Iteration 18, loss = 0.04992055
Iteration 19, loss = 0.04701370
Iteration 20, loss = 0.04454579
Iteration 21, loss = 0.04237773
Iteration 22, loss = 0.04080679
Iteration 23, loss = 0.03942218
Iteration 24, loss = 0.03779552
Iteration 25, loss = 0.03682977
Iteration 26, loss = 0.03562800
Iteration 27, loss = 0.03492681
Iteration 28, loss = 0.03372250
Iteration 29, loss = 0.03289107
Iteration 30, loss = 0.03228737
Iteration 31, loss = 0.03137313
Iteration 32, loss = 0.03102994
Iteration 33, loss = 0.03028630
Iteration 34, loss = 0.02980203
Iteration 35, loss = 0.02970614
Iteration 36, loss = 0.02934592
Iteration 37, loss = 0.02882667
Iteration 38, loss = 0.02831969
Iteration 39, loss = 0.02805634
Iteration 40, loss = 0.02746427
Iteration 41, loss = 0.02731996
Iteration 42, loss = 0.02720766
Iteration 43, loss = 0.02716548
Iteration 44, loss = 0.02700082
Iteration 45, loss = 0.02672451
Iteration 46, loss = 0.02632366
Iteration 47, loss = 0.02612521
Iteration 48, loss = 0.02609630
Iteration 49, loss = 0.02600015
Iteration 50, loss = 0.02596863
Iteration 51, loss = 0.02575495
Iteration 52, loss = 0.02566005
Iteration 53, loss = 0.02549249
Iteration 54, loss = 0.02537070
Iteration 55, loss = 0.02557171
Iteration 56, loss = 0.02563200
Iteration 57, loss = 0.02574925
Iteration 58, loss = 0.02543859
Iteration 59, loss = 0.02580745
Iteration 60, loss = 0.02584009
Iteration 61, loss = 0.02561949
Iteration 62, loss = 0.02507643
Iteration 63, loss = 0.02594596
Iteration 64, loss = 0.02602489
Iteration 65, loss = 0.02526360
Iteration 66, loss = 0.02483495
Iteration 67, loss = 0.02468385
Iteration 68, loss = 0.02448627
Iteration 69, loss = 0.02441187
Iteration 70, loss = 0.02418249
Iteration 71, loss = 0.02396801
Iteration 72, loss = 0.02468618
Iteration 73, loss = 0.02473807
Iteration 74, loss = 0.02418391
Iteration 75, loss = 0.02434641
Iteration 76, loss = 0.02420816
Iteration 77, loss = 0.02441259
Iteration 78, loss = 0.02422534
Iteration 79, loss = 0.02472148
Iteration 80, loss = 0.02539644
Iteration 81, loss = 0.02506957
Iteration 82, loss = 0.02468230
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67319437
Iteration 2, loss = 0.59940226
Iteration 3, loss = 0.49050904
Iteration 4, loss = 0.38106647
Iteration 5, loss = 0.29012542
Iteration 6, loss = 0.22122864
Iteration 7, loss = 0.17250240
Iteration 8, loss = 0.13804726
Iteration 9, loss = 0.11363285
Iteration 10, loss = 0.09625173
Iteration 11, loss = 0.08308133
Iteration 12, loss = 0.07360149
Iteration 13, loss = 0.06593208
Iteration 14, loss = 0.06019085
Iteration 15, loss = 0.05535468
Iteration 16, loss = 0.05181440
Iteration 17, loss = 0.04869939
Iteration 18, loss = 0.04612173
Iteration 19, loss = 0.04386513
Iteration 20, loss = 0.04203160
Iteration 21, loss = 0.04043755
Iteration 22, loss = 0.03883099
Iteration 23, loss = 0.03770666
Iteration 24, loss = 0.03648912
Iteration 25, loss = 0.03576482
Iteration 26, loss = 0.03479430
Iteration 27, loss = 0.03460467
Iteration 28, loss = 0.03367056
Iteration 29, loss = 0.03311503
Iteration 30, loss = 0.03272614
Iteration 31, loss = 0.03211417
Iteration 32, loss = 0.03200377
Iteration 33, loss = 0.03167259
Iteration 34, loss = 0.03107409
Iteration 35, loss = 0.03092821
Iteration 36, loss = 0.03079538
Iteration 37, loss = 0.03010579
Iteration 38, loss = 0.03003095
Iteration 39, loss = 0.02979901
Iteration 40, loss = 0.02937311
Iteration 41, loss = 0.02943506
Iteration 42, loss = 0.02927880
Iteration 43, loss = 0.02940746
Iteration 44, loss = 0.02947701
Iteration 45, loss = 0.02871881
Iteration 46, loss = 0.02867990
Iteration 47, loss = 0.02861420
Iteration 48, loss = 0.02833882
Iteration 49, loss = 0.02834670
Iteration 50, loss = 0.02820899
Iteration 51, loss = 0.02827352
Iteration 52, loss = 0.02825833
Iteration 53, loss = 0.02794410
Iteration 54, loss = 0.02770731
Iteration 55, loss = 0.02819988
Iteration 56, loss = 0.02782782
Iteration 57, loss = 0.02756505
Iteration 58, loss = 0.02772230
Iteration 59, loss = 0.02764805
Iteration 60, loss = 0.02757855
Iteration 61, loss = 0.02742337
Iteration 62, loss = 0.02735847
Iteration 63, loss = 0.02749360
Iteration 64, loss = 0.02757960
Iteration 65, loss = 0.02758257
Iteration 66, loss = 0.02708319
Iteration 67, loss = 0.02725240
Iteration 68, loss = 0.02721464
Iteration 69, loss = 0.02737860
Iteration 70, loss = 0.02733892
Iteration 71, loss = 0.02709446
Iteration 72, loss = 0.02711366
Iteration 73, loss = 0.02735974
Iteration 74, loss = 0.02724183
Iteration 75, loss = 0.02695650
Iteration 76, loss = 0.02689685
Iteration 77, loss = 0.02694560
Iteration 78, loss = 0.02729435
Iteration 79, loss = 0.02690746
Iteration 80, loss = 0.02681660
Iteration 81, loss = 0.02692310
Iteration 82, loss = 0.02657753
Iteration 83, loss = 0.02676489
Iteration 84, loss = 0.02711265
Iteration 85, loss = 0.02693454
Iteration 86, loss = 0.02697994
Iteration 87, loss = 0.02698188
Iteration 88, loss = 0.02669465
Iteration 89, loss = 0.02665258
Iteration 90, loss = 0.02669688
Iteration 91, loss = 0.02660853
Iteration 92, loss = 0.02680904
Iteration 93, loss = 0.02698565
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67508086
Iteration 2, loss = 0.60856440
Iteration 3, loss = 0.50565330
Iteration 4, loss = 0.39541598
Iteration 5, loss = 0.30027590
Iteration 6, loss = 0.22779266
Iteration 7, loss = 0.17596697
Iteration 8, loss = 0.13980122
Iteration 9, loss = 0.11406456
Iteration 10, loss = 0.09582464
Iteration 11, loss = 0.08243451
Iteration 12, loss = 0.07234315
Iteration 13, loss = 0.06460186
Iteration 14, loss = 0.05874645
Iteration 15, loss = 0.05384640
Iteration 16, loss = 0.04993292
Iteration 17, loss = 0.04652906
Iteration 18, loss = 0.04384806
Iteration 19, loss = 0.04133451
Iteration 20, loss = 0.03946586
Iteration 21, loss = 0.03808146
Iteration 22, loss = 0.03640482
Iteration 23, loss = 0.03517489
Iteration 24, loss = 0.03439451
Iteration 25, loss = 0.03321606
Iteration 26, loss = 0.03239582
Iteration 27, loss = 0.03163009
Iteration 28, loss = 0.03100716
Iteration 29, loss = 0.03032232
Iteration 30, loss = 0.02971530
Iteration 31, loss = 0.02949221
Iteration 32, loss = 0.02907929
Iteration 33, loss = 0.02849562
Iteration 34, loss = 0.02802690
Iteration 35, loss = 0.02785104
Iteration 36, loss = 0.02744101
Iteration 37, loss = 0.02726512
Iteration 38, loss = 0.02701779
Iteration 39, loss = 0.02678542
Iteration 40, loss = 0.02655011
Iteration 41, loss = 0.02632095
Iteration 42, loss = 0.02601852
Iteration 43, loss = 0.02584976
Iteration 44, loss = 0.02588259
Iteration 45, loss = 0.02557454
Iteration 46, loss = 0.02544638
Iteration 47, loss = 0.02554416
Iteration 48, loss = 0.02571561
Iteration 49, loss = 0.02500883
Iteration 50, loss = 0.02499160
Iteration 51, loss = 0.02494942
Iteration 52, loss = 0.02476894
Iteration 53, loss = 0.02484528
Iteration 54, loss = 0.02458376
Iteration 55, loss = 0.02458655
Iteration 56, loss = 0.02461451
Iteration 57, loss = 0.02458848
Iteration 58, loss = 0.02454659
Iteration 59, loss = 0.02432316
Iteration 60, loss = 0.02419246
Iteration 61, loss = 0.02406591
Iteration 62, loss = 0.02410820
Iteration 63, loss = 0.02446221
Iteration 64, loss = 0.02389383
Iteration 65, loss = 0.02406423
Iteration 66, loss = 0.02390590
Iteration 67, loss = 0.02396319
Iteration 68, loss = 0.02391275
Iteration 69, loss = 0.02403240
Iteration 70, loss = 0.02375158
Iteration 71, loss = 0.02370726
Iteration 72, loss = 0.02381179
Iteration 73, loss = 0.02372177
Iteration 74, loss = 0.02371493
Iteration 75, loss = 0.02376971
Iteration 76, loss = 0.02348443
Iteration 77, loss = 0.02337787
Iteration 78, loss = 0.02356715
Iteration 79, loss = 0.02361215
Iteration 80, loss = 0.02346901
Iteration 81, loss = 0.02389658
Iteration 82, loss = 0.02323773
Iteration 83, loss = 0.02362412
Iteration 84, loss = 0.02354524
Iteration 85, loss = 0.02350596
Iteration 86, loss = 0.02351432
Iteration 87, loss = 0.02371309
Iteration 88, loss = 0.02337041
Iteration 89, loss = 0.02320876
Iteration 90, loss = 0.02363982
Iteration 91, loss = 0.02317079
Iteration 92, loss = 0.02341842
Iteration 93, loss = 0.02322367
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69784657
Iteration 2, loss = 0.67354878
Iteration 3, loss = 0.64010638
Iteration 4, loss = 0.59421214
Iteration 5, loss = 0.53698653
Iteration 6, loss = 0.47550246
Iteration 7, loss = 0.41448361
Iteration 8, loss = 0.35754596
Iteration 9, loss = 0.30636900
Iteration 10, loss = 0.26172610
Iteration 11, loss = 0.22389636
Iteration 12, loss = 0.19214214
Iteration 13, loss = 0.16611825
Iteration 14, loss = 0.14464369
Iteration 15, loss = 0.12699301
Iteration 16, loss = 0.11243234
Iteration 17, loss = 0.10041590
Iteration 18, loss = 0.09046335
Iteration 19, loss = 0.08198329
Iteration 20, loss = 0.07479620
Iteration 21, loss = 0.06861510
Iteration 22, loss = 0.06352685
Iteration 23, loss = 0.05894198
Iteration 24, loss = 0.05498193
Iteration 25, loss = 0.05161000
Iteration 26, loss = 0.04846410
Iteration 27, loss = 0.04579160
Iteration 28, loss = 0.04351283
Iteration 29, loss = 0.04139324
Iteration 30, loss = 0.03943651
Iteration 31, loss = 0.03774110
Iteration 32, loss = 0.03618981
Iteration 33, loss = 0.03489965
Iteration 34, loss = 0.03350656
Iteration 35, loss = 0.03234176
Iteration 36, loss = 0.03130533
Iteration 37, loss = 0.03029261
Iteration 38, loss = 0.02951802
Iteration 39, loss = 0.02862143
Iteration 40, loss = 0.02791013
Iteration 41, loss = 0.02728486
Iteration 42, loss = 0.02658961
Iteration 43, loss = 0.02607089
Iteration 44, loss = 0.02549530
Iteration 45, loss = 0.02486973
Iteration 46, loss = 0.02436482
Iteration 47, loss = 0.02410989
Iteration 48, loss = 0.02355139
Iteration 49, loss = 0.02323891
Iteration 50, loss = 0.02281325
Iteration 51, loss = 0.02246296
Iteration 52, loss = 0.02206264
Iteration 53, loss = 0.02166923
Iteration 54, loss = 0.02149465
Iteration 55, loss = 0.02120563
Iteration 56, loss = 0.02092428
Iteration 57, loss = 0.02073492
Iteration 58, loss = 0.02045513
Iteration 59, loss = 0.02024298
Iteration 60, loss = 0.02011120
Iteration 61, loss = 0.01992940
Iteration 62, loss = 0.01971983
Iteration 63, loss = 0.01947930
Iteration 64, loss = 0.01926829
Iteration 65, loss = 0.01917885
Iteration 66, loss = 0.01885480
Iteration 67, loss = 0.01873963
Iteration 68, loss = 0.01875455
Iteration 69, loss = 0.01841683
Iteration 70, loss = 0.01841912
Iteration 71, loss = 0.01832174
Iteration 72, loss = 0.01827666
Iteration 73, loss = 0.01817761
Iteration 74, loss = 0.01794957
Iteration 75, loss = 0.01773537
Iteration 76, loss = 0.01772744
Iteration 77, loss = 0.01756266
Iteration 78, loss = 0.01754880
Iteration 79, loss = 0.01753134
Iteration 80, loss = 0.01742527
Iteration 81, loss = 0.01719203
Iteration 82, loss = 0.01719235
Iteration 83, loss = 0.01705588
Iteration 84, loss = 0.01706940
Iteration 85, loss = 0.01702610
Iteration 86, loss = 0.01688949
Iteration 87, loss = 0.01683056
Iteration 88, loss = 0.01685920
Iteration 89, loss = 0.01658433
Iteration 90, loss = 0.01668569
Iteration 91, loss = 0.01665815
Iteration 92, loss = 0.01655917
Iteration 93, loss = 0.01645037
Iteration 94, loss = 0.01641879
Iteration 95, loss = 0.01631425
Iteration 96, loss = 0.01629643
Iteration 97, loss = 0.01630318
Iteration 98, loss = 0.01621994
Iteration 99, loss = 0.01622606
Iteration 100, loss = 0.01617544
Iteration 101, loss = 0.01607149
Iteration 102, loss = 0.01602253
Iteration 103, loss = 0.01602849
Iteration 104, loss = 0.01604307
Iteration 105, loss = 0.01599702
Iteration 106, loss = 0.01587168
Iteration 107, loss = 0.01586454
Iteration 108, loss = 0.01585319
Iteration 109, loss = 0.01590406
Iteration 110, loss = 0.01582786
Iteration 111, loss = 0.01574852
Iteration 112, loss = 0.01562331
Iteration 113, loss = 0.01577643
Iteration 114, loss = 0.01568658
Iteration 115, loss = 0.01571623
Iteration 116, loss = 0.01546187
Iteration 117, loss = 0.01567322
Iteration 118, loss = 0.01554533
Iteration 119, loss = 0.01548948
Iteration 120, loss = 0.01550873
Iteration 121, loss = 0.01550484
Iteration 122, loss = 0.01549285
Iteration 123, loss = 0.01548207
Iteration 124, loss = 0.01539789
Iteration 125, loss = 0.01537748
Iteration 126, loss = 0.01542848
Iteration 127, loss = 0.01537943
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67947738
Iteration 2, loss = 0.61608785
Iteration 3, loss = 0.51586012
Iteration 4, loss = 0.40934262
Iteration 5, loss = 0.31581446
Iteration 6, loss = 0.24320720
Iteration 7, loss = 0.18944217
Iteration 8, loss = 0.15175457
Iteration 9, loss = 0.12455793
Iteration 10, loss = 0.10520239
Iteration 11, loss = 0.09078391
Iteration 12, loss = 0.07991828
Iteration 13, loss = 0.07167268
Iteration 14, loss = 0.06504207
Iteration 15, loss = 0.05978152
Iteration 16, loss = 0.05543645
Iteration 17, loss = 0.05220708
Iteration 18, loss = 0.04936396
Iteration 19, loss = 0.04684071
Iteration 20, loss = 0.04426598
Iteration 21, loss = 0.04263599
Iteration 22, loss = 0.04106636
Iteration 23, loss = 0.03986158
Iteration 24, loss = 0.03860641
Iteration 25, loss = 0.03749957
Iteration 26, loss = 0.03662221
Iteration 27, loss = 0.03592698
Iteration 28, loss = 0.03498994
Iteration 29, loss = 0.03442971
Iteration 30, loss = 0.03375818
Iteration 31, loss = 0.03318194
Iteration 32, loss = 0.03272640
Iteration 33, loss = 0.03212000
Iteration 34, loss = 0.03211406
Iteration 35, loss = 0.03155265
Iteration 36, loss = 0.03135406
Iteration 37, loss = 0.03101712
Iteration 38, loss = 0.03060978
Iteration 39, loss = 0.03030722
Iteration 40, loss = 0.03008415
Iteration 41, loss = 0.02996381
Iteration 42, loss = 0.02995333
Iteration 43, loss = 0.02964170
Iteration 44, loss = 0.02950807
Iteration 45, loss = 0.02905357
Iteration 46, loss = 0.02916455
Iteration 47, loss = 0.02894325
Iteration 48, loss = 0.02884555
Iteration 49, loss = 0.02872636
Iteration 50, loss = 0.02846781
Iteration 51, loss = 0.02864765
Iteration 52, loss = 0.02847541
Iteration 53, loss = 0.02859410
Iteration 54, loss = 0.02811105
Iteration 55, loss = 0.02817217
Iteration 56, loss = 0.02826139
Iteration 57, loss = 0.02823122
Iteration 58, loss = 0.02785432
Iteration 59, loss = 0.02778362
Iteration 60, loss = 0.02777252
Iteration 61, loss = 0.02765831
Iteration 62, loss = 0.02747067
Iteration 63, loss = 0.02740954
Iteration 64, loss = 0.02741693
Iteration 65, loss = 0.02747612
Iteration 66, loss = 0.02729302
Iteration 67, loss = 0.02726932
Iteration 68, loss = 0.02749411
Iteration 69, loss = 0.02746640
Iteration 70, loss = 0.02698313
Iteration 71, loss = 0.02719602
Iteration 72, loss = 0.02709729
Iteration 73, loss = 0.02711900
Iteration 74, loss = 0.02714618
Iteration 75, loss = 0.02677144
Iteration 76, loss = 0.02708267
Iteration 77, loss = 0.02690604
Iteration 78, loss = 0.02747337
Iteration 79, loss = 0.02693185
Iteration 80, loss = 0.02673857
Iteration 81, loss = 0.02697414
Iteration 82, loss = 0.02669229
Iteration 83, loss = 0.02680583
Iteration 84, loss = 0.02684038
Iteration 85, loss = 0.02674562
Iteration 86, loss = 0.02675109
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68083483
Iteration 2, loss = 0.66104961
Iteration 3, loss = 0.63758449
Iteration 4, loss = 0.60618487
Iteration 5, loss = 0.56633503
Iteration 6, loss = 0.52070687
Iteration 7, loss = 0.47252827
Iteration 8, loss = 0.42427452
Iteration 9, loss = 0.37812249
Iteration 10, loss = 0.33497353
Iteration 11, loss = 0.29567749
Iteration 12, loss = 0.26054531
Iteration 13, loss = 0.22979584
Iteration 14, loss = 0.20304196
Iteration 15, loss = 0.17996613
Iteration 16, loss = 0.16019574
Iteration 17, loss = 0.14333533
Iteration 18, loss = 0.12874089
Iteration 19, loss = 0.11637173
Iteration 20, loss = 0.10557863
Iteration 21, loss = 0.09632306
Iteration 22, loss = 0.08834300
Iteration 23, loss = 0.08127528
Iteration 24, loss = 0.07512913
Iteration 25, loss = 0.06966852
Iteration 26, loss = 0.06500778
Iteration 27, loss = 0.06088646
Iteration 28, loss = 0.05706805
Iteration 29, loss = 0.05366650
Iteration 30, loss = 0.05061716
Iteration 31, loss = 0.04796498
Iteration 32, loss = 0.04547189
Iteration 33, loss = 0.04319932
Iteration 34, loss = 0.04111954
Iteration 35, loss = 0.03934178
Iteration 36, loss = 0.03756027
Iteration 37, loss = 0.03606037
Iteration 38, loss = 0.03470145
Iteration 39, loss = 0.03331854
Iteration 40, loss = 0.03215238
Iteration 41, loss = 0.03106477
Iteration 42, loss = 0.02985512
Iteration 43, loss = 0.02888959
Iteration 44, loss = 0.02799120
Iteration 45, loss = 0.02712489
Iteration 46, loss = 0.02654997
Iteration 47, loss = 0.02558912
Iteration 48, loss = 0.02503908
Iteration 49, loss = 0.02429265
Iteration 50, loss = 0.02367333
Iteration 51, loss = 0.02319293
Iteration 52, loss = 0.02263797
Iteration 53, loss = 0.02224331
Iteration 54, loss = 0.02179163
Iteration 55, loss = 0.02137086
Iteration 56, loss = 0.02092162
Iteration 57, loss = 0.02044493
Iteration 58, loss = 0.02002638
Iteration 59, loss = 0.01957943
Iteration 60, loss = 0.01925726
Iteration 61, loss = 0.01896158
Iteration 62, loss = 0.01859988
Iteration 63, loss = 0.01828371
Iteration 64, loss = 0.01817625
Iteration 65, loss = 0.01790189
Iteration 66, loss = 0.01758417
Iteration 67, loss = 0.01739049
Iteration 68, loss = 0.01718492
Iteration 69, loss = 0.01699644
Iteration 70, loss = 0.01669090
Iteration 71, loss = 0.01642742
Iteration 72, loss = 0.01627336
Iteration 73, loss = 0.01599489
Iteration 74, loss = 0.01581953
Iteration 75, loss = 0.01565615
Iteration 76, loss = 0.01548224
Iteration 77, loss = 0.01526394
Iteration 78, loss = 0.01519117
Iteration 79, loss = 0.01510586
Iteration 80, loss = 0.01492813
Iteration 81, loss = 0.01490883
Iteration 82, loss = 0.01480313
Iteration 83, loss = 0.01458961
Iteration 84, loss = 0.01423804
Iteration 85, loss = 0.01440219
Iteration 86, loss = 0.01417774
Iteration 87, loss = 0.01403105
Iteration 88, loss = 0.01395538
Iteration 89, loss = 0.01382804
Iteration 90, loss = 0.01364143
Iteration 91, loss = 0.01361577
Iteration 92, loss = 0.01350123
Iteration 93, loss = 0.01345013
Iteration 94, loss = 0.01331539
Iteration 95, loss = 0.01335800
Iteration 96, loss = 0.01371165
Iteration 97, loss = 0.01379003
Iteration 98, loss = 0.01356944
Iteration 99, loss = 0.01326162
Iteration 100, loss = 0.01303225
Iteration 101, loss = 0.01280309
Iteration 102, loss = 0.01271387
Iteration 103, loss = 0.01250953
Iteration 104, loss = 0.01302855
Iteration 105, loss = 0.01284919
Iteration 106, loss = 0.01280291
Iteration 107, loss = 0.01256263
Iteration 108, loss = 0.01266457
Iteration 109, loss = 0.01239765
Iteration 110, loss = 0.01244498
Iteration 111, loss = 0.01228512
Iteration 112, loss = 0.01224828
Iteration 113, loss = 0.01208703
Iteration 114, loss = 0.01212493
Iteration 115, loss = 0.01204407
Iteration 116, loss = 0.01200079
Iteration 117, loss = 0.01185655
Iteration 118, loss = 0.01175798
Iteration 119, loss = 0.01187068
Iteration 120, loss = 0.01196992
Iteration 121, loss = 0.01177478
Iteration 122, loss = 0.01183625
Iteration 123, loss = 0.01162435
Iteration 124, loss = 0.01152146
Iteration 125, loss = 0.01151015
Iteration 126, loss = 0.01154169
Iteration 127, loss = 0.01152354
Iteration 128, loss = 0.01148333
Iteration 129, loss = 0.01164803
Iteration 130, loss = 0.01154542
Iteration 131, loss = 0.01143928
Iteration 132, loss = 0.01138347
Iteration 133, loss = 0.01126911
Iteration 134, loss = 0.01124595
Iteration 135, loss = 0.01134369
Iteration 136, loss = 0.01130421
Iteration 137, loss = 0.01123675
Iteration 138, loss = 0.01117618
Iteration 139, loss = 0.01111926
Iteration 140, loss = 0.01108637
Iteration 141, loss = 0.01103174
Iteration 142, loss = 0.01109149
Iteration 143, loss = 0.01101652
Iteration 144, loss = 0.01088152
Iteration 145, loss = 0.01094428
Iteration 146, loss = 0.01086696
Iteration 147, loss = 0.01089038
Iteration 148, loss = 0.01077117
Iteration 149, loss = 0.01082247
Iteration 150, loss = 0.01078058
Iteration 151, loss = 0.01070415
Iteration 152, loss = 0.01081658
Iteration 153, loss = 0.01096752
Iteration 154, loss = 0.01100155
Iteration 155, loss = 0.01084444
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68469219
Iteration 2, loss = 0.66943767
Iteration 3, loss = 0.65415057
Iteration 4, loss = 0.63581151
Iteration 5, loss = 0.61351035
Iteration 6, loss = 0.58692162
Iteration 7, loss = 0.55671414
Iteration 8, loss = 0.52326990
Iteration 9, loss = 0.48847799
Iteration 10, loss = 0.45363900
Iteration 11, loss = 0.41899329
Iteration 12, loss = 0.38542648
Iteration 13, loss = 0.35330450
Iteration 14, loss = 0.32278512
Iteration 15, loss = 0.29439369
Iteration 16, loss = 0.26802612
Iteration 17, loss = 0.24380226
Iteration 18, loss = 0.22204842
Iteration 19, loss = 0.20216234
Iteration 20, loss = 0.18433174
Iteration 21, loss = 0.16835295
Iteration 22, loss = 0.15405760
Iteration 23, loss = 0.14135416
Iteration 24, loss = 0.13001002
Iteration 25, loss = 0.11988405
Iteration 26, loss = 0.11089819
Iteration 27, loss = 0.10282556
Iteration 28, loss = 0.09562183
Iteration 29, loss = 0.08918244
Iteration 30, loss = 0.08334300
Iteration 31, loss = 0.07818492
Iteration 32, loss = 0.07340165
Iteration 33, loss = 0.06916441
Iteration 34, loss = 0.06522756
Iteration 35, loss = 0.06174329
Iteration 36, loss = 0.05855651
Iteration 37, loss = 0.05554520
Iteration 38, loss = 0.05293230
Iteration 39, loss = 0.05045195
Iteration 40, loss = 0.04816108
Iteration 41, loss = 0.04607761
Iteration 42, loss = 0.04409985
Iteration 43, loss = 0.04234131
Iteration 44, loss = 0.04068410
Iteration 45, loss = 0.03910923
Iteration 46, loss = 0.03771138
Iteration 47, loss = 0.03633876
Iteration 48, loss = 0.03512356
Iteration 49, loss = 0.03391302
Iteration 50, loss = 0.03278419
Iteration 51, loss = 0.03177559
Iteration 52, loss = 0.03078488
Iteration 53, loss = 0.02995171
Iteration 54, loss = 0.02900960
Iteration 55, loss = 0.02818849
Iteration 56, loss = 0.02745039
Iteration 57, loss = 0.02670387
Iteration 58, loss = 0.02605929
Iteration 59, loss = 0.02539906
Iteration 60, loss = 0.02477263
Iteration 61, loss = 0.02416800
Iteration 62, loss = 0.02362742
Iteration 63, loss = 0.02312754
Iteration 64, loss = 0.02261259
Iteration 65, loss = 0.02210742
Iteration 66, loss = 0.02164219
Iteration 67, loss = 0.02121773
Iteration 68, loss = 0.02079220
Iteration 69, loss = 0.02042190
Iteration 70, loss = 0.02003881
Iteration 71, loss = 0.01969395
Iteration 72, loss = 0.01927718
Iteration 73, loss = 0.01898980
Iteration 74, loss = 0.01865835
Iteration 75, loss = 0.01832326
Iteration 76, loss = 0.01804705
Iteration 77, loss = 0.01776585
Iteration 78, loss = 0.01751934
Iteration 79, loss = 0.01723247
Iteration 80, loss = 0.01703855
Iteration 81, loss = 0.01672409
Iteration 82, loss = 0.01649533
Iteration 83, loss = 0.01630256
Iteration 84, loss = 0.01606675
Iteration 85, loss = 0.01588018
Iteration 86, loss = 0.01565469
Iteration 87, loss = 0.01548219
Iteration 88, loss = 0.01524891
Iteration 89, loss = 0.01509906
Iteration 90, loss = 0.01493945
Iteration 91, loss = 0.01473709
Iteration 92, loss = 0.01461667
Iteration 93, loss = 0.01443708
Iteration 94, loss = 0.01424037
Iteration 95, loss = 0.01414537
Iteration 96, loss = 0.01398833
Iteration 97, loss = 0.01384699
Iteration 98, loss = 0.01371280
Iteration 99, loss = 0.01356613
Iteration 100, loss = 0.01344669
Iteration 101, loss = 0.01330275
Iteration 102, loss = 0.01323563
Iteration 103, loss = 0.01311093
Iteration 104, loss = 0.01296275
Iteration 105, loss = 0.01285956
Iteration 106, loss = 0.01275303
Iteration 107, loss = 0.01263950
Iteration 108, loss = 0.01256487
Iteration 109, loss = 0.01246154
Iteration 110, loss = 0.01234479
Iteration 111, loss = 0.01228420
Iteration 112, loss = 0.01217294
Iteration 113, loss = 0.01209949
Iteration 114, loss = 0.01199190
Iteration 115, loss = 0.01192811
Iteration 116, loss = 0.01190942
Iteration 117, loss = 0.01178961
Iteration 118, loss = 0.01170416
Iteration 119, loss = 0.01165809
Iteration 120, loss = 0.01159951
Iteration 121, loss = 0.01151117
Iteration 122, loss = 0.01143034
Iteration 123, loss = 0.01133356
Iteration 124, loss = 0.01128112
Iteration 125, loss = 0.01122588
Iteration 126, loss = 0.01112527
Iteration 127, loss = 0.01108521
Iteration 128, loss = 0.01102669
Iteration 129, loss = 0.01096403
Iteration 130, loss = 0.01087171
Iteration 131, loss = 0.01090082
Iteration 132, loss = 0.01077246
Iteration 133, loss = 0.01078328
Iteration 134, loss = 0.01070926
Iteration 135, loss = 0.01060978
Iteration 136, loss = 0.01057607
Iteration 137, loss = 0.01054795
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69211117
Iteration 2, loss = 0.64874421
Iteration 3, loss = 0.58733941
Iteration 4, loss = 0.50929044
Iteration 5, loss = 0.42576633
Iteration 6, loss = 0.34836874
Iteration 7, loss = 0.28216127
Iteration 8, loss = 0.22868250
Iteration 9, loss = 0.18695840
Iteration 10, loss = 0.15492305
Iteration 11, loss = 0.13089119
Iteration 12, loss = 0.11236397
Iteration 13, loss = 0.09767251
Iteration 14, loss = 0.08645246
Iteration 15, loss = 0.07742040
Iteration 16, loss = 0.07010215
Iteration 17, loss = 0.06400897
Iteration 18, loss = 0.05923358
Iteration 19, loss = 0.05490467
Iteration 20, loss = 0.05127721
Iteration 21, loss = 0.04855739
Iteration 22, loss = 0.04599011
Iteration 23, loss = 0.04390753
Iteration 24, loss = 0.04167203
Iteration 25, loss = 0.03983223
Iteration 26, loss = 0.03845037
Iteration 27, loss = 0.03704277
Iteration 28, loss = 0.03617584
Iteration 29, loss = 0.03504505
Iteration 30, loss = 0.03404983
Iteration 31, loss = 0.03329313
Iteration 32, loss = 0.03227247
Iteration 33, loss = 0.03164065
Iteration 34, loss = 0.03088291
Iteration 35, loss = 0.03027669
Iteration 36, loss = 0.02987841
Iteration 37, loss = 0.02924495
Iteration 38, loss = 0.02857309
Iteration 39, loss = 0.02855354
Iteration 40, loss = 0.02810663
Iteration 41, loss = 0.02778128
Iteration 42, loss = 0.02752326
Iteration 43, loss = 0.02706360
Iteration 44, loss = 0.02655946
Iteration 45, loss = 0.02635120
Iteration 46, loss = 0.02602216
Iteration 47, loss = 0.02588459
Iteration 48, loss = 0.02564115
Iteration 49, loss = 0.02556289
Iteration 50, loss = 0.02533662
Iteration 51, loss = 0.02514560
Iteration 52, loss = 0.02474061
Iteration 53, loss = 0.02496087
Iteration 54, loss = 0.02451365
Iteration 55, loss = 0.02444136
Iteration 56, loss = 0.02457063
Iteration 57, loss = 0.02465613
Iteration 58, loss = 0.02454152
Iteration 59, loss = 0.02402276
Iteration 60, loss = 0.02405346
Iteration 61, loss = 0.02392295
Iteration 62, loss = 0.02358775
Iteration 63, loss = 0.02361538
Iteration 64, loss = 0.02336673
Iteration 65, loss = 0.02330105
Iteration 66, loss = 0.02330225
Iteration 67, loss = 0.02320209
Iteration 68, loss = 0.02327364
Iteration 69, loss = 0.02350773
Iteration 70, loss = 0.02294827
Iteration 71, loss = 0.02313501
Iteration 72, loss = 0.02292723
Iteration 73, loss = 0.02264735
Iteration 74, loss = 0.02256873
Iteration 75, loss = 0.02279878
Iteration 76, loss = 0.02286942
Iteration 77, loss = 0.02286789
Iteration 78, loss = 0.02257352
Iteration 79, loss = 0.02263986
Iteration 80, loss = 0.02259528
Iteration 81, loss = 0.02268772
Iteration 82, loss = 0.02242998
Iteration 83, loss = 0.02225462
Iteration 84, loss = 0.02224279
Iteration 85, loss = 0.02236373
Iteration 86, loss = 0.02214417
Iteration 87, loss = 0.02244438
Iteration 88, loss = 0.02234176
Iteration 89, loss = 0.02201845
Iteration 90, loss = 0.02222885
Iteration 91, loss = 0.02227881
Iteration 92, loss = 0.02210500
Iteration 93, loss = 0.02183849
Iteration 94, loss = 0.02187057
Iteration 95, loss = 0.02192297
Iteration 96, loss = 0.02174807
Iteration 97, loss = 0.02187024
Iteration 98, loss = 0.02171489
Iteration 99, loss = 0.02185459
Iteration 100, loss = 0.02192994
Iteration 101, loss = 0.02193881
Iteration 102, loss = 0.02177075
Iteration 103, loss = 0.02158916
Iteration 104, loss = 0.02166130
Iteration 105, loss = 0.02195757
Iteration 106, loss = 0.02189072
Iteration 107, loss = 0.02195756
Iteration 108, loss = 0.02200304
Iteration 109, loss = 0.02176228
Iteration 110, loss = 0.02161941
Iteration 111, loss = 0.02151609
Iteration 112, loss = 0.02189548
Iteration 113, loss = 0.02140427
Iteration 114, loss = 0.02164005
Iteration 115, loss = 0.02142147
Iteration 116, loss = 0.02187576
Iteration 117, loss = 0.02173702
Iteration 118, loss = 0.02179235
Iteration 119, loss = 0.02173303
Iteration 120, loss = 0.02163862
Iteration 121, loss = 0.02169436
Iteration 122, loss = 0.02142737
Iteration 123, loss = 0.02161982
Iteration 124, loss = 0.02134702
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68185488
Iteration 2, loss = 0.64404267
Iteration 3, loss = 0.59237341
Iteration 4, loss = 0.52423292
Iteration 5, loss = 0.45014755
Iteration 6, loss = 0.37925437
Iteration 7, loss = 0.31702761
Iteration 8, loss = 0.26424368
Iteration 9, loss = 0.22092883
Iteration 10, loss = 0.18610058
Iteration 11, loss = 0.15832493
Iteration 12, loss = 0.13636220
Iteration 13, loss = 0.11880373
Iteration 14, loss = 0.10461447
Iteration 15, loss = 0.09323815
Iteration 16, loss = 0.08385063
Iteration 17, loss = 0.07603956
Iteration 18, loss = 0.06960447
Iteration 19, loss = 0.06419371
Iteration 20, loss = 0.05966670
Iteration 21, loss = 0.05561817
Iteration 22, loss = 0.05215984
Iteration 23, loss = 0.04937279
Iteration 24, loss = 0.04666170
Iteration 25, loss = 0.04430146
Iteration 26, loss = 0.04227658
Iteration 27, loss = 0.04050937
Iteration 28, loss = 0.03906210
Iteration 29, loss = 0.03756038
Iteration 30, loss = 0.03611492
Iteration 31, loss = 0.03507929
Iteration 32, loss = 0.03395553
Iteration 33, loss = 0.03297513
Iteration 34, loss = 0.03228179
Iteration 35, loss = 0.03144137
Iteration 36, loss = 0.03054739
Iteration 37, loss = 0.02998479
Iteration 38, loss = 0.02941548
Iteration 39, loss = 0.02869020
Iteration 40, loss = 0.02831737
Iteration 41, loss = 0.02772149
Iteration 42, loss = 0.02725886
Iteration 43, loss = 0.02688312
Iteration 44, loss = 0.02637764
Iteration 45, loss = 0.02604433
Iteration 46, loss = 0.02572440
Iteration 47, loss = 0.02534669
Iteration 48, loss = 0.02512000
Iteration 49, loss = 0.02467280
Iteration 50, loss = 0.02457914
Iteration 51, loss = 0.02436385
Iteration 52, loss = 0.02419925
Iteration 53, loss = 0.02372360
Iteration 54, loss = 0.02355989
Iteration 55, loss = 0.02330193
Iteration 56, loss = 0.02335046
Iteration 57, loss = 0.02317622
Iteration 58, loss = 0.02286194
Iteration 59, loss = 0.02271879
Iteration 60, loss = 0.02255792
Iteration 61, loss = 0.02245838
Iteration 62, loss = 0.02234050
Iteration 63, loss = 0.02225556
Iteration 64, loss = 0.02194603
Iteration 65, loss = 0.02196536
Iteration 66, loss = 0.02164856
Iteration 67, loss = 0.02158055
Iteration 68, loss = 0.02152262
Iteration 69, loss = 0.02156532
Iteration 70, loss = 0.02137369
Iteration 71, loss = 0.02122338
Iteration 72, loss = 0.02121875
Iteration 73, loss = 0.02105676
Iteration 74, loss = 0.02110684
Iteration 75, loss = 0.02108264
Iteration 76, loss = 0.02105348
Iteration 77, loss = 0.02076531
Iteration 78, loss = 0.02082986
Iteration 79, loss = 0.02067965
Iteration 80, loss = 0.02060793
Iteration 81, loss = 0.02042895
Iteration 82, loss = 0.02049032
Iteration 83, loss = 0.02043391
Iteration 84, loss = 0.02044772
Iteration 85, loss = 0.02046682
Iteration 86, loss = 0.02023264
Iteration 87, loss = 0.02020864
Iteration 88, loss = 0.02030643
Iteration 89, loss = 0.01999874
Iteration 90, loss = 0.02021533
Iteration 91, loss = 0.02015691
Iteration 92, loss = 0.01998665
Iteration 93, loss = 0.01981523
Iteration 94, loss = 0.01979245
Iteration 95, loss = 0.02001736
Iteration 96, loss = 0.01990836
Iteration 97, loss = 0.01995152
Iteration 98, loss = 0.01974174
Iteration 99, loss = 0.01976403
Iteration 100, loss = 0.01978086
Iteration 101, loss = 0.01968543
Iteration 102, loss = 0.01970022
Iteration 103, loss = 0.01966809
Iteration 104, loss = 0.01965922
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67539336
Iteration 2, loss = 0.61149523
Iteration 3, loss = 0.51806044
Iteration 4, loss = 0.41657010
Iteration 5, loss = 0.32400619
Iteration 6, loss = 0.24921978
Iteration 7, loss = 0.19340337
Iteration 8, loss = 0.15354937
Iteration 9, loss = 0.12512724
Iteration 10, loss = 0.10477721
Iteration 11, loss = 0.08959205
Iteration 12, loss = 0.07841258
Iteration 13, loss = 0.06975465
Iteration 14, loss = 0.06305972
Iteration 15, loss = 0.05779624
Iteration 16, loss = 0.05358508
Iteration 17, loss = 0.05018489
Iteration 18, loss = 0.04690494
Iteration 19, loss = 0.04456424
Iteration 20, loss = 0.04219731
Iteration 21, loss = 0.04019894
Iteration 22, loss = 0.03896930
Iteration 23, loss = 0.03769705
Iteration 24, loss = 0.03651116
Iteration 25, loss = 0.03552734
Iteration 26, loss = 0.03461239
Iteration 27, loss = 0.03365644
Iteration 28, loss = 0.03301417
Iteration 29, loss = 0.03215063
Iteration 30, loss = 0.03154856
Iteration 31, loss = 0.03104009
Iteration 32, loss = 0.03103968
Iteration 33, loss = 0.03040882
Iteration 34, loss = 0.02994847
Iteration 35, loss = 0.02985974
Iteration 36, loss = 0.02912890
Iteration 37, loss = 0.02912703
Iteration 38, loss = 0.02853541
Iteration 39, loss = 0.02842307
Iteration 40, loss = 0.02836616
Iteration 41, loss = 0.02774033
Iteration 42, loss = 0.02801433
Iteration 43, loss = 0.02760122
Iteration 44, loss = 0.02732217
Iteration 45, loss = 0.02716407
Iteration 46, loss = 0.02684615
Iteration 47, loss = 0.02689721
Iteration 48, loss = 0.02682294
Iteration 49, loss = 0.02637348
Iteration 50, loss = 0.02656333
Iteration 51, loss = 0.02665068
Iteration 52, loss = 0.02622114
Iteration 53, loss = 0.02604241
Iteration 54, loss = 0.02640935
Iteration 55, loss = 0.02622045
Iteration 56, loss = 0.02580284
Iteration 57, loss = 0.02591221
Iteration 58, loss = 0.02591663
Iteration 59, loss = 0.02566484
Iteration 60, loss = 0.02549847
Iteration 61, loss = 0.02543994
Iteration 62, loss = 0.02523021
Iteration 63, loss = 0.02548670
Iteration 64, loss = 0.02533742
Iteration 65, loss = 0.02523261
Iteration 66, loss = 0.02527668
Iteration 67, loss = 0.02527234
Iteration 68, loss = 0.02504736
Iteration 69, loss = 0.02531817
Iteration 70, loss = 0.02525413
Iteration 71, loss = 0.02541629
Iteration 72, loss = 0.02512608
Iteration 73, loss = 0.02498288
Iteration 74, loss = 0.02520985
Iteration 75, loss = 0.02504187
Iteration 76, loss = 0.02510184
Iteration 77, loss = 0.02498376
Iteration 78, loss = 0.02485708
Iteration 79, loss = 0.02471523
Iteration 80, loss = 0.02482949
Iteration 81, loss = 0.02487966
Iteration 82, loss = 0.02457570
Iteration 83, loss = 0.02441960
Iteration 84, loss = 0.02483136
Iteration 85, loss = 0.02461423
Iteration 86, loss = 0.02492281
Iteration 87, loss = 0.02477276
Iteration 88, loss = 0.02453041
Iteration 89, loss = 0.02427578
Iteration 90, loss = 0.02458556
Iteration 91, loss = 0.02484388
Iteration 92, loss = 0.02458596
Iteration 93, loss = 0.02404513
Iteration 94, loss = 0.02470324
Iteration 95, loss = 0.02470116
Iteration 96, loss = 0.02477057
Iteration 97, loss = 0.02410651
Iteration 98, loss = 0.02448211
Iteration 99, loss = 0.02465643
Iteration 100, loss = 0.02472860
Iteration 101, loss = 0.02454036
Iteration 102, loss = 0.02428581
Iteration 103, loss = 0.02424323
Iteration 104, loss = 0.02442510
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68729607
Iteration 2, loss = 0.66881020
Iteration 3, loss = 0.65038946
Iteration 4, loss = 0.62931905
Iteration 5, loss = 0.60420659
Iteration 6, loss = 0.57512963
Iteration 7, loss = 0.54187727
Iteration 8, loss = 0.50539695
Iteration 9, loss = 0.46721647
Iteration 10, loss = 0.42879093
Iteration 11, loss = 0.39136805
Iteration 12, loss = 0.35597691
Iteration 13, loss = 0.32294873
Iteration 14, loss = 0.29254983
Iteration 15, loss = 0.26469984
Iteration 16, loss = 0.23949676
Iteration 17, loss = 0.21692517
Iteration 18, loss = 0.19670193
Iteration 19, loss = 0.17862447
Iteration 20, loss = 0.16263203
Iteration 21, loss = 0.14843838
Iteration 22, loss = 0.13578632
Iteration 23, loss = 0.12454654
Iteration 24, loss = 0.11459445
Iteration 25, loss = 0.10577214
Iteration 26, loss = 0.09785953
Iteration 27, loss = 0.09076983
Iteration 28, loss = 0.08450981
Iteration 29, loss = 0.07881957
Iteration 30, loss = 0.07373750
Iteration 31, loss = 0.06911041
Iteration 32, loss = 0.06493694
Iteration 33, loss = 0.06120817
Iteration 34, loss = 0.05774104
Iteration 35, loss = 0.05460967
Iteration 36, loss = 0.05175930
Iteration 37, loss = 0.04913068
Iteration 38, loss = 0.04673973
Iteration 39, loss = 0.04453308
Iteration 40, loss = 0.04244379
Iteration 41, loss = 0.04060565
Iteration 42, loss = 0.03882134
Iteration 43, loss = 0.03717717
Iteration 44, loss = 0.03568694
Iteration 45, loss = 0.03429187
Iteration 46, loss = 0.03303204
Iteration 47, loss = 0.03174141
Iteration 48, loss = 0.03060477
Iteration 49, loss = 0.02952934
Iteration 50, loss = 0.02857903
Iteration 51, loss = 0.02757809
Iteration 52, loss = 0.02671010
Iteration 53, loss = 0.02585803
Iteration 54, loss = 0.02509968
Iteration 55, loss = 0.02435127
Iteration 56, loss = 0.02364463
Iteration 57, loss = 0.02294550
Iteration 58, loss = 0.02235448
Iteration 59, loss = 0.02177869
Iteration 60, loss = 0.02122412
Iteration 61, loss = 0.02064095
Iteration 62, loss = 0.02013342
Iteration 63, loss = 0.01967266
Iteration 64, loss = 0.01920750
Iteration 65, loss = 0.01876821
Iteration 66, loss = 0.01833363
Iteration 67, loss = 0.01792249
Iteration 68, loss = 0.01754839
Iteration 69, loss = 0.01721211
Iteration 70, loss = 0.01686025
Iteration 71, loss = 0.01656108
Iteration 72, loss = 0.01620086
Iteration 73, loss = 0.01588486
Iteration 74, loss = 0.01561313
Iteration 75, loss = 0.01536788
Iteration 76, loss = 0.01508880
Iteration 77, loss = 0.01485694
Iteration 78, loss = 0.01456953
Iteration 79, loss = 0.01433746
Iteration 80, loss = 0.01412050
Iteration 81, loss = 0.01392253
Iteration 82, loss = 0.01367342
Iteration 83, loss = 0.01347537
Iteration 84, loss = 0.01328533
Iteration 85, loss = 0.01309892
Iteration 86, loss = 0.01293023
Iteration 87, loss = 0.01270515
Iteration 88, loss = 0.01256230
Iteration 89, loss = 0.01241089
Iteration 90, loss = 0.01221928
Iteration 91, loss = 0.01207716
Iteration 92, loss = 0.01196824
Iteration 93, loss = 0.01180071
Iteration 94, loss = 0.01166616
Iteration 95, loss = 0.01155682
Iteration 96, loss = 0.01139799
Iteration 97, loss = 0.01127038
Iteration 98, loss = 0.01116091
Iteration 99, loss = 0.01109665
Iteration 100, loss = 0.01093812
Iteration 101, loss = 0.01081615
Iteration 102, loss = 0.01068311
Iteration 103, loss = 0.01061835
Iteration 104, loss = 0.01049117
Iteration 105, loss = 0.01044959
Iteration 106, loss = 0.01031250
Iteration 107, loss = 0.01022706
Iteration 108, loss = 0.01016261
Iteration 109, loss = 0.01001267
Iteration 110, loss = 0.00994568
Iteration 111, loss = 0.00990060
Iteration 112, loss = 0.00978633
Iteration 113, loss = 0.00969118
Iteration 114, loss = 0.00975207
Iteration 115, loss = 0.00958811
Iteration 116, loss = 0.00950718
Iteration 117, loss = 0.00942755
Iteration 118, loss = 0.00942446
Iteration 119, loss = 0.00934587
Iteration 120, loss = 0.00923627
Iteration 121, loss = 0.00915559
Iteration 122, loss = 0.00909953
Iteration 123, loss = 0.00902338
Iteration 124, loss = 0.00900684
Iteration 125, loss = 0.00894087
Iteration 126, loss = 0.00889028
Iteration 127, loss = 0.00888281
Iteration 128, loss = 0.00882988
Iteration 129, loss = 0.00873854
Iteration 130, loss = 0.00869379
Iteration 131, loss = 0.00862776
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73008150
Iteration 2, loss = 0.69644745
Iteration 3, loss = 0.65246326
Iteration 4, loss = 0.59907048
Iteration 5, loss = 0.53840171
Iteration 6, loss = 0.47618679
Iteration 7, loss = 0.41549416
Iteration 8, loss = 0.35878776
Iteration 9, loss = 0.30763921
Iteration 10, loss = 0.26316878
Iteration 11, loss = 0.22518407
Iteration 12, loss = 0.19330283
Iteration 13, loss = 0.16700083
Iteration 14, loss = 0.14544748
Iteration 15, loss = 0.12735037
Iteration 16, loss = 0.11263749
Iteration 17, loss = 0.10045266
Iteration 18, loss = 0.09015718
Iteration 19, loss = 0.08148671
Iteration 20, loss = 0.07424346
Iteration 21, loss = 0.06803092
Iteration 22, loss = 0.06268343
Iteration 23, loss = 0.05807088
Iteration 24, loss = 0.05405956
Iteration 25, loss = 0.05072654
Iteration 26, loss = 0.04765093
Iteration 27, loss = 0.04491340
Iteration 28, loss = 0.04245717
Iteration 29, loss = 0.04042092
Iteration 30, loss = 0.03835614
Iteration 31, loss = 0.03685087
Iteration 32, loss = 0.03520967
Iteration 33, loss = 0.03378107
Iteration 34, loss = 0.03259441
Iteration 35, loss = 0.03137727
Iteration 36, loss = 0.03035732
Iteration 37, loss = 0.02936322
Iteration 38, loss = 0.02854788
Iteration 39, loss = 0.02763525
Iteration 40, loss = 0.02706799
Iteration 41, loss = 0.02625159
Iteration 42, loss = 0.02551097
Iteration 43, loss = 0.02500456
Iteration 44, loss = 0.02440999
Iteration 45, loss = 0.02393225
Iteration 46, loss = 0.02338791
Iteration 47, loss = 0.02293713
Iteration 48, loss = 0.02258301
Iteration 49, loss = 0.02215901
Iteration 50, loss = 0.02175881
Iteration 51, loss = 0.02144811
Iteration 52, loss = 0.02107754
Iteration 53, loss = 0.02080060
Iteration 54, loss = 0.02049685
Iteration 55, loss = 0.02020740
Iteration 56, loss = 0.02010157
Iteration 57, loss = 0.01980214
Iteration 58, loss = 0.01948382
Iteration 59, loss = 0.01919861
Iteration 60, loss = 0.01902300
Iteration 61, loss = 0.01883968
Iteration 62, loss = 0.01858701
Iteration 63, loss = 0.01843667
Iteration 64, loss = 0.01831068
Iteration 65, loss = 0.01812502
Iteration 66, loss = 0.01808255
Iteration 67, loss = 0.01795102
Iteration 68, loss = 0.01783778
Iteration 69, loss = 0.01766987
Iteration 70, loss = 0.01742308
Iteration 71, loss = 0.01740851
Iteration 72, loss = 0.01722612
Iteration 73, loss = 0.01715632
Iteration 74, loss = 0.01692697
Iteration 75, loss = 0.01694911
Iteration 76, loss = 0.01671798
Iteration 77, loss = 0.01662887
Iteration 78, loss = 0.01670165
Iteration 79, loss = 0.01650081
Iteration 80, loss = 0.01640245
Iteration 81, loss = 0.01636341
Iteration 82, loss = 0.01631945
Iteration 83, loss = 0.01622671
Iteration 84, loss = 0.01617841
Iteration 85, loss = 0.01607901
Iteration 86, loss = 0.01602367
Iteration 87, loss = 0.01592884
Iteration 88, loss = 0.01589117
Iteration 89, loss = 0.01575127
Iteration 90, loss = 0.01575973
Iteration 91, loss = 0.01579150
Iteration 92, loss = 0.01566439
Iteration 93, loss = 0.01558757
Iteration 94, loss = 0.01562446
Iteration 95, loss = 0.01563021
Iteration 96, loss = 0.01549432
Iteration 97, loss = 0.01550862
Iteration 98, loss = 0.01543077
Iteration 99, loss = 0.01541911
Iteration 100, loss = 0.01536054
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72747802
Iteration 2, loss = 0.69607241
Iteration 3, loss = 0.66470989
Iteration 4, loss = 0.62716343
Iteration 5, loss = 0.58266448
Iteration 6, loss = 0.53281705
Iteration 7, loss = 0.48137035
Iteration 8, loss = 0.43071782
Iteration 9, loss = 0.38351661
Iteration 10, loss = 0.33987289
Iteration 11, loss = 0.30057513
Iteration 12, loss = 0.26591795
Iteration 13, loss = 0.23544667
Iteration 14, loss = 0.20939463
Iteration 15, loss = 0.18677638
Iteration 16, loss = 0.16712419
Iteration 17, loss = 0.15015530
Iteration 18, loss = 0.13546694
Iteration 19, loss = 0.12274189
Iteration 20, loss = 0.11177148
Iteration 21, loss = 0.10239815
Iteration 22, loss = 0.09407000
Iteration 23, loss = 0.08674544
Iteration 24, loss = 0.08034659
Iteration 25, loss = 0.07461105
Iteration 26, loss = 0.06959437
Iteration 27, loss = 0.06525198
Iteration 28, loss = 0.06131914
Iteration 29, loss = 0.05776152
Iteration 30, loss = 0.05465133
Iteration 31, loss = 0.05174842
Iteration 32, loss = 0.04908976
Iteration 33, loss = 0.04669128
Iteration 34, loss = 0.04452387
Iteration 35, loss = 0.04252434
Iteration 36, loss = 0.04072068
Iteration 37, loss = 0.03917666
Iteration 38, loss = 0.03759636
Iteration 39, loss = 0.03641900
Iteration 40, loss = 0.03484485
Iteration 41, loss = 0.03361425
Iteration 42, loss = 0.03256470
Iteration 43, loss = 0.03146119
Iteration 44, loss = 0.03046219
Iteration 45, loss = 0.02957620
Iteration 46, loss = 0.02869309
Iteration 47, loss = 0.02776501
Iteration 48, loss = 0.02724369
Iteration 49, loss = 0.02645697
Iteration 50, loss = 0.02610372
Iteration 51, loss = 0.02557684
Iteration 52, loss = 0.02452001
Iteration 53, loss = 0.02397571
Iteration 54, loss = 0.02351476
Iteration 55, loss = 0.02314789
Iteration 56, loss = 0.02264385
Iteration 57, loss = 0.02228953
Iteration 58, loss = 0.02195144
Iteration 59, loss = 0.02152289
Iteration 60, loss = 0.02109569
Iteration 61, loss = 0.02072930
Iteration 62, loss = 0.02027521
Iteration 63, loss = 0.01983628
Iteration 64, loss = 0.01953719
Iteration 65, loss = 0.01910721
Iteration 66, loss = 0.01892935
Iteration 67, loss = 0.01869743
Iteration 68, loss = 0.01843659
Iteration 69, loss = 0.01816703
Iteration 70, loss = 0.01799998
Iteration 71, loss = 0.01777980
Iteration 72, loss = 0.01750720
Iteration 73, loss = 0.01726069
Iteration 74, loss = 0.01703774
Iteration 75, loss = 0.01685713
Iteration 76, loss = 0.01657355
Iteration 77, loss = 0.01638420
Iteration 78, loss = 0.01625610
Iteration 79, loss = 0.01613689
Iteration 80, loss = 0.01601181
Iteration 81, loss = 0.01588300
Iteration 82, loss = 0.01600106
Iteration 83, loss = 0.01594278
Iteration 84, loss = 0.01582400
Iteration 85, loss = 0.01550332
Iteration 86, loss = 0.01541715
Iteration 87, loss = 0.01513023
Iteration 88, loss = 0.01489440
Iteration 89, loss = 0.01484354
Iteration 90, loss = 0.01464265
Iteration 91, loss = 0.01450667
Iteration 92, loss = 0.01439998
Iteration 93, loss = 0.01438361
Iteration 94, loss = 0.01440929
Iteration 95, loss = 0.01395489
Iteration 96, loss = 0.01420191
Iteration 97, loss = 0.01423944
Iteration 98, loss = 0.01407069
Iteration 99, loss = 0.01388434
Iteration 100, loss = 0.01392966
Iteration 101, loss = 0.01399917
Iteration 102, loss = 0.01380981
Iteration 103, loss = 0.01357723
Iteration 104, loss = 0.01340728
Iteration 105, loss = 0.01348273
Iteration 106, loss = 0.01344276
Iteration 107, loss = 0.01328155
Iteration 108, loss = 0.01340667
Iteration 109, loss = 0.01322720
Iteration 110, loss = 0.01320243
Iteration 111, loss = 0.01315850
Iteration 112, loss = 0.01307442
Iteration 113, loss = 0.01287749
Iteration 114, loss = 0.01277899
Iteration 115, loss = 0.01272923
Iteration 116, loss = 0.01293321
Iteration 117, loss = 0.01287812
Iteration 118, loss = 0.01304341
Iteration 119, loss = 0.01283355
Iteration 120, loss = 0.01267795
Iteration 121, loss = 0.01251463
Iteration 122, loss = 0.01249868
Iteration 123, loss = 0.01252442
Iteration 124, loss = 0.01273467
Iteration 125, loss = 0.01288581
Iteration 126, loss = 0.01304414
Iteration 127, loss = 0.01265496
Iteration 128, loss = 0.01294973
Iteration 129, loss = 0.01263421
Iteration 130, loss = 0.01257921
Iteration 131, loss = 0.01241988
Iteration 132, loss = 0.01234669
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67558085
Iteration 2, loss = 0.60632916
Iteration 3, loss = 0.49723109
Iteration 4, loss = 0.38315314
Iteration 5, loss = 0.28639772
Iteration 6, loss = 0.21486000
Iteration 7, loss = 0.16508343
Iteration 8, loss = 0.13100615
Iteration 9, loss = 0.10764793
Iteration 10, loss = 0.09083339
Iteration 11, loss = 0.07872687
Iteration 12, loss = 0.06940244
Iteration 13, loss = 0.06220781
Iteration 14, loss = 0.05668127
Iteration 15, loss = 0.05223614
Iteration 16, loss = 0.04863966
Iteration 17, loss = 0.04602995
Iteration 18, loss = 0.04337016
Iteration 19, loss = 0.04133334
Iteration 20, loss = 0.03938025
Iteration 21, loss = 0.03787742
Iteration 22, loss = 0.03656712
Iteration 23, loss = 0.03566229
Iteration 24, loss = 0.03452529
Iteration 25, loss = 0.03377535
Iteration 26, loss = 0.03292931
Iteration 27, loss = 0.03213303
Iteration 28, loss = 0.03150258
Iteration 29, loss = 0.03097109
Iteration 30, loss = 0.03033653
Iteration 31, loss = 0.02996821
Iteration 32, loss = 0.02963442
Iteration 33, loss = 0.02932244
Iteration 34, loss = 0.02905286
Iteration 35, loss = 0.02889438
Iteration 36, loss = 0.02828503
Iteration 37, loss = 0.02809852
Iteration 38, loss = 0.02789238
Iteration 39, loss = 0.02758556
Iteration 40, loss = 0.02745624
Iteration 41, loss = 0.02748966
Iteration 42, loss = 0.02757584
Iteration 43, loss = 0.02706379
Iteration 44, loss = 0.02668007
Iteration 45, loss = 0.02637960
Iteration 46, loss = 0.02655682
Iteration 47, loss = 0.02607384
Iteration 48, loss = 0.02626998
Iteration 49, loss = 0.02609842
Iteration 50, loss = 0.02626298
Iteration 51, loss = 0.02587965
Iteration 52, loss = 0.02587011
Iteration 53, loss = 0.02574258
Iteration 54, loss = 0.02615000
Iteration 55, loss = 0.02574910
Iteration 56, loss = 0.02557314
Iteration 57, loss = 0.02535496
Iteration 58, loss = 0.02552483
Iteration 59, loss = 0.02555365
Iteration 60, loss = 0.02526710
Iteration 61, loss = 0.02524487
Iteration 62, loss = 0.02541504
Iteration 63, loss = 0.02528895
Iteration 64, loss = 0.02519537
Iteration 65, loss = 0.02514400
Iteration 66, loss = 0.02535008
Iteration 67, loss = 0.02506282
Iteration 68, loss = 0.02503922
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68942636
Iteration 2, loss = 0.65973159
Iteration 3, loss = 0.61849466
Iteration 4, loss = 0.56631575
Iteration 5, loss = 0.50703679
Iteration 6, loss = 0.44645591
Iteration 7, loss = 0.38781016
Iteration 8, loss = 0.33268126
Iteration 9, loss = 0.28388055
Iteration 10, loss = 0.24195267
Iteration 11, loss = 0.20694550
Iteration 12, loss = 0.17780161
Iteration 13, loss = 0.15392951
Iteration 14, loss = 0.13423924
Iteration 15, loss = 0.11811648
Iteration 16, loss = 0.10467261
Iteration 17, loss = 0.09366618
Iteration 18, loss = 0.08437119
Iteration 19, loss = 0.07667953
Iteration 20, loss = 0.07008716
Iteration 21, loss = 0.06445090
Iteration 22, loss = 0.05972246
Iteration 23, loss = 0.05539606
Iteration 24, loss = 0.05191472
Iteration 25, loss = 0.04863813
Iteration 26, loss = 0.04558860
Iteration 27, loss = 0.04315909
Iteration 28, loss = 0.04090230
Iteration 29, loss = 0.03893334
Iteration 30, loss = 0.03714044
Iteration 31, loss = 0.03546747
Iteration 32, loss = 0.03406130
Iteration 33, loss = 0.03271583
Iteration 34, loss = 0.03147658
Iteration 35, loss = 0.03055395
Iteration 36, loss = 0.02938879
Iteration 37, loss = 0.02858852
Iteration 38, loss = 0.02769670
Iteration 39, loss = 0.02676434
Iteration 40, loss = 0.02627448
Iteration 41, loss = 0.02551683
Iteration 42, loss = 0.02491317
Iteration 43, loss = 0.02430584
Iteration 44, loss = 0.02364994
Iteration 45, loss = 0.02317880
Iteration 46, loss = 0.02277403
Iteration 47, loss = 0.02229634
Iteration 48, loss = 0.02193524
Iteration 49, loss = 0.02158469
Iteration 50, loss = 0.02109730
Iteration 51, loss = 0.02090580
Iteration 52, loss = 0.02050002
Iteration 53, loss = 0.02028469
Iteration 54, loss = 0.01988268
Iteration 55, loss = 0.01969427
Iteration 56, loss = 0.01939951
Iteration 57, loss = 0.01921288
Iteration 58, loss = 0.01900838
Iteration 59, loss = 0.01865082
Iteration 60, loss = 0.01860716
Iteration 61, loss = 0.01841510
Iteration 62, loss = 0.01822691
Iteration 63, loss = 0.01799639
Iteration 64, loss = 0.01780231
Iteration 65, loss = 0.01774707
Iteration 66, loss = 0.01740989
Iteration 67, loss = 0.01739849
Iteration 68, loss = 0.01719245
Iteration 69, loss = 0.01700941
Iteration 70, loss = 0.01681983
Iteration 71, loss = 0.01685855
Iteration 72, loss = 0.01677530
Iteration 73, loss = 0.01659162
Iteration 74, loss = 0.01646845
Iteration 75, loss = 0.01638173
Iteration 76, loss = 0.01633322
Iteration 77, loss = 0.01615231
Iteration 78, loss = 0.01612130
Iteration 79, loss = 0.01595547
Iteration 80, loss = 0.01592959
Iteration 81, loss = 0.01590276
Iteration 82, loss = 0.01577301
Iteration 83, loss = 0.01570300
Iteration 84, loss = 0.01564961
Iteration 85, loss = 0.01561144
Iteration 86, loss = 0.01548068
Iteration 87, loss = 0.01540331
Iteration 88, loss = 0.01532044
Iteration 89, loss = 0.01533548
Iteration 90, loss = 0.01525205
Iteration 91, loss = 0.01516968
Iteration 92, loss = 0.01513582
Iteration 93, loss = 0.01502058
Iteration 94, loss = 0.01495549
Iteration 95, loss = 0.01488790
Iteration 96, loss = 0.01493999
Iteration 97, loss = 0.01494589
Iteration 98, loss = 0.01470212
Iteration 99, loss = 0.01478996
Iteration 100, loss = 0.01470849
Iteration 101, loss = 0.01463732
Iteration 102, loss = 0.01470979
Iteration 103, loss = 0.01473916
Iteration 104, loss = 0.01456405
Iteration 105, loss = 0.01460350
Iteration 106, loss = 0.01465031
Iteration 107, loss = 0.01454096
Iteration 108, loss = 0.01437997
Iteration 109, loss = 0.01460572
Iteration 110, loss = 0.01444948
Iteration 111, loss = 0.01435174
Iteration 112, loss = 0.01439100
Iteration 113, loss = 0.01430971
Iteration 114, loss = 0.01427926
Iteration 115, loss = 0.01426145
Iteration 116, loss = 0.01427199
Iteration 117, loss = 0.01424423
Iteration 118, loss = 0.01421001
Iteration 119, loss = 0.01415233
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67334953
Iteration 2, loss = 0.59964901
Iteration 3, loss = 0.49178018
Iteration 4, loss = 0.38301844
Iteration 5, loss = 0.29041317
Iteration 6, loss = 0.22057692
Iteration 7, loss = 0.17122587
Iteration 8, loss = 0.13712274
Iteration 9, loss = 0.11303298
Iteration 10, loss = 0.09575202
Iteration 11, loss = 0.08314022
Iteration 12, loss = 0.07313721
Iteration 13, loss = 0.06619903
Iteration 14, loss = 0.06036457
Iteration 15, loss = 0.05561995
Iteration 16, loss = 0.05204027
Iteration 17, loss = 0.04873162
Iteration 18, loss = 0.04607631
Iteration 19, loss = 0.04400260
Iteration 20, loss = 0.04211057
Iteration 21, loss = 0.04034425
Iteration 22, loss = 0.03900890
Iteration 23, loss = 0.03786066
Iteration 24, loss = 0.03696025
Iteration 25, loss = 0.03583244
Iteration 26, loss = 0.03497624
Iteration 27, loss = 0.03433167
Iteration 28, loss = 0.03382435
Iteration 29, loss = 0.03295103
Iteration 30, loss = 0.03248187
Iteration 31, loss = 0.03224030
Iteration 32, loss = 0.03178705
Iteration 33, loss = 0.03131647
Iteration 34, loss = 0.03127410
Iteration 35, loss = 0.03073179
Iteration 36, loss = 0.03023671
Iteration 37, loss = 0.03011837
Iteration 38, loss = 0.02989611
Iteration 39, loss = 0.02981144
Iteration 40, loss = 0.02945551
Iteration 41, loss = 0.02936944
Iteration 42, loss = 0.02898280
Iteration 43, loss = 0.02887624
Iteration 44, loss = 0.02871389
Iteration 45, loss = 0.02872972
Iteration 46, loss = 0.02857328
Iteration 47, loss = 0.02820012
Iteration 48, loss = 0.02836579
Iteration 49, loss = 0.02810232
Iteration 50, loss = 0.02808532
Iteration 51, loss = 0.02833331
Iteration 52, loss = 0.02817799
Iteration 53, loss = 0.02760556
Iteration 54, loss = 0.02774195
Iteration 55, loss = 0.02756850
Iteration 56, loss = 0.02759091
Iteration 57, loss = 0.02734415
Iteration 58, loss = 0.02740646
Iteration 59, loss = 0.02754761
Iteration 60, loss = 0.02737625
Iteration 61, loss = 0.02761916
Iteration 62, loss = 0.02719003
Iteration 63, loss = 0.02708135
Iteration 64, loss = 0.02714666
Iteration 65, loss = 0.02723841
Iteration 66, loss = 0.02711408
Iteration 67, loss = 0.02705038
Iteration 68, loss = 0.02671308
Iteration 69, loss = 0.02712939
Iteration 70, loss = 0.02691957
Iteration 71, loss = 0.02697344
Iteration 72, loss = 0.02682412
Iteration 73, loss = 0.02692975
Iteration 74, loss = 0.02734971
Iteration 75, loss = 0.02687484
Iteration 76, loss = 0.02679989
Iteration 77, loss = 0.02669808
Iteration 78, loss = 0.02656934
Iteration 79, loss = 0.02670883
Iteration 80, loss = 0.02674364
Iteration 81, loss = 0.02672867
Iteration 82, loss = 0.02657757
Iteration 83, loss = 0.02646723
Iteration 84, loss = 0.02679347
Iteration 85, loss = 0.02661499
Iteration 86, loss = 0.02618745
Iteration 87, loss = 0.02661719
Iteration 88, loss = 0.02653431
Iteration 89, loss = 0.02653934
Iteration 90, loss = 0.02620281
Iteration 91, loss = 0.02613446
Iteration 92, loss = 0.02637854
Iteration 93, loss = 0.02651993
Iteration 94, loss = 0.02628271
Iteration 95, loss = 0.02634772
Iteration 96, loss = 0.02594174
Iteration 97, loss = 0.02643394
Iteration 98, loss = 0.02652347
Iteration 99, loss = 0.02648562
Iteration 100, loss = 0.02615704
Iteration 101, loss = 0.02625797
Iteration 102, loss = 0.02594420
Iteration 103, loss = 0.02610010
Iteration 104, loss = 0.02610330
Iteration 105, loss = 0.02596964
Iteration 106, loss = 0.02613408
Iteration 107, loss = 0.02610113
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	101m56.780s
user	741m22.605s
sys	84m43.390s
Elapsed time: 6117 seconds
