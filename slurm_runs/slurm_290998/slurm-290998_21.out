Processing dataset: prepended_v4_stemmed

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: prepended_v4_stemmed (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on prepended_v4_stemmed...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on prepended_v4_stemmed...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on prepended_v4_stemmed...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on prepended_v4_stemmed...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on prepended_v4_stemmed...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.68755767
Iteration 2, loss = 0.59347949
Iteration 3, loss = 0.47465973
Iteration 4, loss = 0.36233254
Iteration 5, loss = 0.27071613
Iteration 6, loss = 0.20301815
Iteration 7, loss = 0.15617873
Iteration 8, loss = 0.12386048
Iteration 9, loss = 0.10126530
Iteration 10, loss = 0.08521000
Iteration 11, loss = 0.07341488
Iteration 12, loss = 0.06471468
Iteration 13, loss = 0.05793608
Iteration 14, loss = 0.05282093
Iteration 15, loss = 0.04830907
Iteration 16, loss = 0.04506661
Iteration 17, loss = 0.04197658
Iteration 18, loss = 0.03977665
Iteration 19, loss = 0.03775398
Iteration 20, loss = 0.03641590
Iteration 21, loss = 0.03506539
Iteration 22, loss = 0.03376799
Iteration 23, loss = 0.03272740
Iteration 24, loss = 0.03180150
Iteration 25, loss = 0.03103396
Iteration 26, loss = 0.03047706
Iteration 27, loss = 0.03005197
Iteration 28, loss = 0.02932097
Iteration 29, loss = 0.02896827
Iteration 30, loss = 0.02860838
Iteration 31, loss = 0.02788369
Iteration 32, loss = 0.02757494
Iteration 33, loss = 0.02759016
Iteration 34, loss = 0.02706523
Iteration 35, loss = 0.02661279
Iteration 36, loss = 0.02666437
Iteration 37, loss = 0.02629314
Iteration 38, loss = 0.02635661
Iteration 39, loss = 0.02606398
Iteration 40, loss = 0.02582286
Iteration 41, loss = 0.02546388
Iteration 42, loss = 0.02568640
Iteration 43, loss = 0.02536386
Iteration 44, loss = 0.02534001
Iteration 45, loss = 0.02500719
Iteration 46, loss = 0.02487958
Iteration 47, loss = 0.02496176
Iteration 48, loss = 0.02454189
Iteration 49, loss = 0.02471906
Iteration 50, loss = 0.02468827
Iteration 51, loss = 0.02460552
Iteration 52, loss = 0.02457417
Iteration 53, loss = 0.02456995
Iteration 54, loss = 0.02417348
Iteration 55, loss = 0.02414682
Iteration 56, loss = 0.02431870
Iteration 57, loss = 0.02413313
Iteration 58, loss = 0.02416120
Iteration 59, loss = 0.02402885
Iteration 60, loss = 0.02435768
Iteration 61, loss = 0.02442893
Iteration 62, loss = 0.02388646
Iteration 63, loss = 0.02391045
Iteration 64, loss = 0.02407574
Iteration 65, loss = 0.02405298
Iteration 66, loss = 0.02373181
Iteration 67, loss = 0.02379209
Iteration 68, loss = 0.02362008
Iteration 69, loss = 0.02370873
Iteration 70, loss = 0.02380694
Iteration 71, loss = 0.02369690
Iteration 72, loss = 0.02345105
Iteration 73, loss = 0.02348169
Iteration 74, loss = 0.02368865
Iteration 75, loss = 0.02391114
Iteration 76, loss = 0.02349155
Iteration 77, loss = 0.02341046
Iteration 78, loss = 0.02343429
Iteration 79, loss = 0.02337987
Iteration 80, loss = 0.02342355
Iteration 81, loss = 0.02339738
Iteration 82, loss = 0.02370327
Iteration 83, loss = 0.02315517
Iteration 84, loss = 0.02321282
Iteration 85, loss = 0.02314144
Iteration 86, loss = 0.02349944
Iteration 87, loss = 0.02360392
Iteration 88, loss = 0.02325343
Iteration 89, loss = 0.02325325
Iteration 90, loss = 0.02334124
Iteration 91, loss = 0.02310918
Iteration 92, loss = 0.02346868
Iteration 93, loss = 0.02358307
Iteration 94, loss = 0.02311353
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68286323
Iteration 2, loss = 0.60639462
Iteration 3, loss = 0.49275418
Iteration 4, loss = 0.37565408
Iteration 5, loss = 0.27930049
Iteration 6, loss = 0.20890397
Iteration 7, loss = 0.15960767
Iteration 8, loss = 0.12629087
Iteration 9, loss = 0.10315987
Iteration 10, loss = 0.08697196
Iteration 11, loss = 0.07508461
Iteration 12, loss = 0.06604353
Iteration 13, loss = 0.05918660
Iteration 14, loss = 0.05404538
Iteration 15, loss = 0.04997191
Iteration 16, loss = 0.04618882
Iteration 17, loss = 0.04341582
Iteration 18, loss = 0.04096345
Iteration 19, loss = 0.03921817
Iteration 20, loss = 0.03749550
Iteration 21, loss = 0.03612185
Iteration 22, loss = 0.03484594
Iteration 23, loss = 0.03372152
Iteration 24, loss = 0.03298748
Iteration 25, loss = 0.03204032
Iteration 26, loss = 0.03151659
Iteration 27, loss = 0.03102403
Iteration 28, loss = 0.03043651
Iteration 29, loss = 0.02962716
Iteration 30, loss = 0.02919186
Iteration 31, loss = 0.02890186
Iteration 32, loss = 0.02866461
Iteration 33, loss = 0.02785907
Iteration 34, loss = 0.02783658
Iteration 35, loss = 0.02783062
Iteration 36, loss = 0.02726950
Iteration 37, loss = 0.02719892
Iteration 38, loss = 0.02699718
Iteration 39, loss = 0.02683873
Iteration 40, loss = 0.02630612
Iteration 41, loss = 0.02632587
Iteration 42, loss = 0.02616096
Iteration 43, loss = 0.02597658
Iteration 44, loss = 0.02574136
Iteration 45, loss = 0.02568910
Iteration 46, loss = 0.02581740
Iteration 47, loss = 0.02562572
Iteration 48, loss = 0.02528154
Iteration 49, loss = 0.02529669
Iteration 50, loss = 0.02520236
Iteration 51, loss = 0.02512206
Iteration 52, loss = 0.02524371
Iteration 53, loss = 0.02526340
Iteration 54, loss = 0.02488661
Iteration 55, loss = 0.02498211
Iteration 56, loss = 0.02451909
Iteration 57, loss = 0.02492235
Iteration 58, loss = 0.02481573
Iteration 59, loss = 0.02500319
Iteration 60, loss = 0.02457835
Iteration 61, loss = 0.02452273
Iteration 62, loss = 0.02454074
Iteration 63, loss = 0.02435770
Iteration 64, loss = 0.02463986
Iteration 65, loss = 0.02474477
Iteration 66, loss = 0.02446835
Iteration 67, loss = 0.02428051
Iteration 68, loss = 0.02410223
Iteration 69, loss = 0.02458308
Iteration 70, loss = 0.02431039
Iteration 71, loss = 0.02433996
Iteration 72, loss = 0.02435492
Iteration 73, loss = 0.02426436
Iteration 74, loss = 0.02407750
Iteration 75, loss = 0.02405523
Iteration 76, loss = 0.02440111
Iteration 77, loss = 0.02372381
Iteration 78, loss = 0.02407220
Iteration 79, loss = 0.02421607
Iteration 80, loss = 0.02374008
Iteration 81, loss = 0.02373719
Iteration 82, loss = 0.02389023
Iteration 83, loss = 0.02376161
Iteration 84, loss = 0.02362020
Iteration 85, loss = 0.02384167
Iteration 86, loss = 0.02394900
Iteration 87, loss = 0.02383703
Iteration 88, loss = 0.02392230
Iteration 89, loss = 0.02386220
Iteration 90, loss = 0.02379668
Iteration 91, loss = 0.02377766
Iteration 92, loss = 0.02377894
Iteration 93, loss = 0.02384864
Iteration 94, loss = 0.02358088
Iteration 95, loss = 0.02370136
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68120516
Iteration 2, loss = 0.60240939
Iteration 3, loss = 0.48875762
Iteration 4, loss = 0.37398564
Iteration 5, loss = 0.27872515
Iteration 6, loss = 0.20832928
Iteration 7, loss = 0.15896571
Iteration 8, loss = 0.12530335
Iteration 9, loss = 0.10193361
Iteration 10, loss = 0.08515744
Iteration 11, loss = 0.07311500
Iteration 12, loss = 0.06395383
Iteration 13, loss = 0.05699174
Iteration 14, loss = 0.05153457
Iteration 15, loss = 0.04724342
Iteration 16, loss = 0.04364205
Iteration 17, loss = 0.04074252
Iteration 18, loss = 0.03845232
Iteration 19, loss = 0.03678829
Iteration 20, loss = 0.03481771
Iteration 21, loss = 0.03328753
Iteration 22, loss = 0.03206854
Iteration 23, loss = 0.03129289
Iteration 24, loss = 0.02983166
Iteration 25, loss = 0.02940936
Iteration 26, loss = 0.02853175
Iteration 27, loss = 0.02789063
Iteration 28, loss = 0.02746004
Iteration 29, loss = 0.02711382
Iteration 30, loss = 0.02652221
Iteration 31, loss = 0.02595122
Iteration 32, loss = 0.02570601
Iteration 33, loss = 0.02535391
Iteration 34, loss = 0.02513437
Iteration 35, loss = 0.02497236
Iteration 36, loss = 0.02451500
Iteration 37, loss = 0.02423288
Iteration 38, loss = 0.02412383
Iteration 39, loss = 0.02388086
Iteration 40, loss = 0.02361234
Iteration 41, loss = 0.02357997
Iteration 42, loss = 0.02362998
Iteration 43, loss = 0.02347322
Iteration 44, loss = 0.02320893
Iteration 45, loss = 0.02304863
Iteration 46, loss = 0.02287787
Iteration 47, loss = 0.02292427
Iteration 48, loss = 0.02318289
Iteration 49, loss = 0.02259563
Iteration 50, loss = 0.02274562
Iteration 51, loss = 0.02281007
Iteration 52, loss = 0.02265629
Iteration 53, loss = 0.02248390
Iteration 54, loss = 0.02223820
Iteration 55, loss = 0.02256074
Iteration 56, loss = 0.02231013
Iteration 57, loss = 0.02223729
Iteration 58, loss = 0.02192570
Iteration 59, loss = 0.02198761
Iteration 60, loss = 0.02189855
Iteration 61, loss = 0.02187415
Iteration 62, loss = 0.02194847
Iteration 63, loss = 0.02176009
Iteration 64, loss = 0.02185855
Iteration 65, loss = 0.02183427
Iteration 66, loss = 0.02184282
Iteration 67, loss = 0.02148833
Iteration 68, loss = 0.02159766
Iteration 69, loss = 0.02173231
Iteration 70, loss = 0.02169511
Iteration 71, loss = 0.02185519
Iteration 72, loss = 0.02178407
Iteration 73, loss = 0.02155462
Iteration 74, loss = 0.02177182
Iteration 75, loss = 0.02163439
Iteration 76, loss = 0.02138168
Iteration 77, loss = 0.02139018
Iteration 78, loss = 0.02151728
Iteration 79, loss = 0.02129340
Iteration 80, loss = 0.02155433
Iteration 81, loss = 0.02133241
Iteration 82, loss = 0.02114399
Iteration 83, loss = 0.02141585
Iteration 84, loss = 0.02150407
Iteration 85, loss = 0.02126781
Iteration 86, loss = 0.02131063
Iteration 87, loss = 0.02116889
Iteration 88, loss = 0.02116198
Iteration 89, loss = 0.02136099
Iteration 90, loss = 0.02116816
Iteration 91, loss = 0.02118787
Iteration 92, loss = 0.02087475
Iteration 93, loss = 0.02104374
Iteration 94, loss = 0.02116971
Iteration 95, loss = 0.02104421
Iteration 96, loss = 0.02132347
Iteration 97, loss = 0.02115302
Iteration 98, loss = 0.02113968
Iteration 99, loss = 0.02141356
Iteration 100, loss = 0.02131007
Iteration 101, loss = 0.02060717
Iteration 102, loss = 0.02091822
Iteration 103, loss = 0.02105802
Iteration 104, loss = 0.02110012
Iteration 105, loss = 0.02104955
Iteration 106, loss = 0.02078253
Iteration 107, loss = 0.02099838
Iteration 108, loss = 0.02080167
Iteration 109, loss = 0.02104842
Iteration 110, loss = 0.02065322
Iteration 111, loss = 0.02077663
Iteration 112, loss = 0.02091623
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70038784
Iteration 2, loss = 0.60823767
Iteration 3, loss = 0.49488390
Iteration 4, loss = 0.38112099
Iteration 5, loss = 0.28571571
Iteration 6, loss = 0.21335423
Iteration 7, loss = 0.16215922
Iteration 8, loss = 0.12757824
Iteration 9, loss = 0.10298530
Iteration 10, loss = 0.08596776
Iteration 11, loss = 0.07354965
Iteration 12, loss = 0.06406051
Iteration 13, loss = 0.05693172
Iteration 14, loss = 0.05169365
Iteration 15, loss = 0.04718417
Iteration 16, loss = 0.04368520
Iteration 17, loss = 0.04080872
Iteration 18, loss = 0.03849959
Iteration 19, loss = 0.03637117
Iteration 20, loss = 0.03481127
Iteration 21, loss = 0.03351581
Iteration 22, loss = 0.03194899
Iteration 23, loss = 0.03092412
Iteration 24, loss = 0.03001254
Iteration 25, loss = 0.02930258
Iteration 26, loss = 0.02855459
Iteration 27, loss = 0.02790136
Iteration 28, loss = 0.02741740
Iteration 29, loss = 0.02693831
Iteration 30, loss = 0.02660147
Iteration 31, loss = 0.02590638
Iteration 32, loss = 0.02569048
Iteration 33, loss = 0.02548243
Iteration 34, loss = 0.02499062
Iteration 35, loss = 0.02474548
Iteration 36, loss = 0.02445388
Iteration 37, loss = 0.02451008
Iteration 38, loss = 0.02401457
Iteration 39, loss = 0.02412349
Iteration 40, loss = 0.02367137
Iteration 41, loss = 0.02355791
Iteration 42, loss = 0.02350821
Iteration 43, loss = 0.02326930
Iteration 44, loss = 0.02316575
Iteration 45, loss = 0.02334798
Iteration 46, loss = 0.02304455
Iteration 47, loss = 0.02281671
Iteration 48, loss = 0.02292627
Iteration 49, loss = 0.02255169
Iteration 50, loss = 0.02278826
Iteration 51, loss = 0.02235764
Iteration 52, loss = 0.02252962
Iteration 53, loss = 0.02225951
Iteration 54, loss = 0.02233652
Iteration 55, loss = 0.02240437
Iteration 56, loss = 0.02226139
Iteration 57, loss = 0.02196045
Iteration 58, loss = 0.02191502
Iteration 59, loss = 0.02195673
Iteration 60, loss = 0.02189081
Iteration 61, loss = 0.02201095
Iteration 62, loss = 0.02198087
Iteration 63, loss = 0.02183348
Iteration 64, loss = 0.02209815
Iteration 65, loss = 0.02172065
Iteration 66, loss = 0.02177448
Iteration 67, loss = 0.02207802
Iteration 68, loss = 0.02156587
Iteration 69, loss = 0.02164653
Iteration 70, loss = 0.02133507
Iteration 71, loss = 0.02171107
Iteration 72, loss = 0.02140086
Iteration 73, loss = 0.02139516
Iteration 74, loss = 0.02124276
Iteration 75, loss = 0.02131695
Iteration 76, loss = 0.02131449
Iteration 77, loss = 0.02141703
Iteration 78, loss = 0.02144030
Iteration 79, loss = 0.02118172
Iteration 80, loss = 0.02118593
Iteration 81, loss = 0.02129909
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67402025
Iteration 2, loss = 0.59732637
Iteration 3, loss = 0.47604295
Iteration 4, loss = 0.35889098
Iteration 5, loss = 0.26548904
Iteration 6, loss = 0.19802381
Iteration 7, loss = 0.15184290
Iteration 8, loss = 0.12028728
Iteration 9, loss = 0.09811866
Iteration 10, loss = 0.08247084
Iteration 11, loss = 0.07108922
Iteration 12, loss = 0.06239488
Iteration 13, loss = 0.05570203
Iteration 14, loss = 0.05076771
Iteration 15, loss = 0.04612793
Iteration 16, loss = 0.04294937
Iteration 17, loss = 0.04014565
Iteration 18, loss = 0.03794536
Iteration 19, loss = 0.03587608
Iteration 20, loss = 0.03447492
Iteration 21, loss = 0.03296897
Iteration 22, loss = 0.03171481
Iteration 23, loss = 0.03082015
Iteration 24, loss = 0.02974385
Iteration 25, loss = 0.02897951
Iteration 26, loss = 0.02827446
Iteration 27, loss = 0.02785915
Iteration 28, loss = 0.02704911
Iteration 29, loss = 0.02668262
Iteration 30, loss = 0.02633511
Iteration 31, loss = 0.02591412
Iteration 32, loss = 0.02547032
Iteration 33, loss = 0.02501870
Iteration 34, loss = 0.02486768
Iteration 35, loss = 0.02472323
Iteration 36, loss = 0.02424026
Iteration 37, loss = 0.02428361
Iteration 38, loss = 0.02431678
Iteration 39, loss = 0.02396002
Iteration 40, loss = 0.02351423
Iteration 41, loss = 0.02344631
Iteration 42, loss = 0.02317375
Iteration 43, loss = 0.02332983
Iteration 44, loss = 0.02327654
Iteration 45, loss = 0.02282302
Iteration 46, loss = 0.02318591
Iteration 47, loss = 0.02265238
Iteration 48, loss = 0.02280208
Iteration 49, loss = 0.02250502
Iteration 50, loss = 0.02279457
Iteration 51, loss = 0.02256768
Iteration 52, loss = 0.02246288
Iteration 53, loss = 0.02236079
Iteration 54, loss = 0.02199425
Iteration 55, loss = 0.02204112
Iteration 56, loss = 0.02228940
Iteration 57, loss = 0.02189597
Iteration 58, loss = 0.02208298
Iteration 59, loss = 0.02173572
Iteration 60, loss = 0.02169539
Iteration 61, loss = 0.02166965
Iteration 62, loss = 0.02179010
Iteration 63, loss = 0.02176207
Iteration 64, loss = 0.02198877
Iteration 65, loss = 0.02132754
Iteration 66, loss = 0.02154362
Iteration 67, loss = 0.02143869
Iteration 68, loss = 0.02129596
Iteration 69, loss = 0.02130905
Iteration 70, loss = 0.02147222
Iteration 71, loss = 0.02130716
Iteration 72, loss = 0.02134905
Iteration 73, loss = 0.02123829
Iteration 74, loss = 0.02123714
Iteration 75, loss = 0.02136621
Iteration 76, loss = 0.02140689
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68755767
Iteration 2, loss = 0.59347949
Iteration 3, loss = 0.47465973
Iteration 4, loss = 0.36233254
Iteration 5, loss = 0.27071613
Iteration 6, loss = 0.20301815
Iteration 7, loss = 0.15617873
Iteration 8, loss = 0.12386048
Iteration 9, loss = 0.10126530
Iteration 10, loss = 0.08521000
Iteration 11, loss = 0.07341488
Iteration 12, loss = 0.06471468
Iteration 13, loss = 0.05793608
Iteration 14, loss = 0.05282093
Iteration 15, loss = 0.04830907
Iteration 16, loss = 0.04506661
Iteration 17, loss = 0.04197658
Iteration 18, loss = 0.03977665
Iteration 19, loss = 0.03775398
Iteration 20, loss = 0.03641590
Iteration 21, loss = 0.03506539
Iteration 22, loss = 0.03376799
Iteration 23, loss = 0.03272740
Iteration 24, loss = 0.03180150
Iteration 25, loss = 0.03103396
Iteration 26, loss = 0.03047706
Iteration 27, loss = 0.03005197
Iteration 28, loss = 0.02932097
Iteration 29, loss = 0.02896827
Iteration 30, loss = 0.02860838
Iteration 31, loss = 0.02788369
Iteration 32, loss = 0.02757494
Iteration 33, loss = 0.02759016
Iteration 34, loss = 0.02706523
Iteration 35, loss = 0.02661279
Iteration 36, loss = 0.02666437
Iteration 37, loss = 0.02629314
Iteration 38, loss = 0.02635661
Iteration 39, loss = 0.02606398
Iteration 40, loss = 0.02582286
Iteration 41, loss = 0.02546388
Iteration 42, loss = 0.02568640
Iteration 43, loss = 0.02536386
Iteration 44, loss = 0.02534001
Iteration 45, loss = 0.02500719
Iteration 46, loss = 0.02487958
Iteration 47, loss = 0.02496176
Iteration 48, loss = 0.02454189
Iteration 49, loss = 0.02471906
Iteration 50, loss = 0.02468827
Iteration 51, loss = 0.02460552
Iteration 52, loss = 0.02457417
Iteration 53, loss = 0.02456995
Iteration 54, loss = 0.02417348
Iteration 55, loss = 0.02414682
Iteration 56, loss = 0.02431870
Iteration 57, loss = 0.02413313
Iteration 58, loss = 0.02416120
Iteration 59, loss = 0.02402885
Iteration 60, loss = 0.02435768
Iteration 61, loss = 0.02442893
Iteration 62, loss = 0.02388646
Iteration 63, loss = 0.02391045
Iteration 64, loss = 0.02407574
Iteration 65, loss = 0.02405298
Iteration 66, loss = 0.02373181
Iteration 67, loss = 0.02379209
Iteration 68, loss = 0.02362008
Iteration 69, loss = 0.02370873
Iteration 70, loss = 0.02380694
Iteration 71, loss = 0.02369690
Iteration 72, loss = 0.02345105
Iteration 73, loss = 0.02348169
Iteration 74, loss = 0.02368865
Iteration 75, loss = 0.02391114
Iteration 76, loss = 0.02349155
Iteration 77, loss = 0.02341046
Iteration 78, loss = 0.02343429
Iteration 79, loss = 0.02337987
Iteration 80, loss = 0.02342355
Iteration 81, loss = 0.02339738
Iteration 82, loss = 0.02370327
Iteration 83, loss = 0.02315517
Iteration 84, loss = 0.02321282
Iteration 85, loss = 0.02314144
Iteration 86, loss = 0.02349944
Iteration 87, loss = 0.02360392
Iteration 88, loss = 0.02325343
Iteration 89, loss = 0.02325325
Iteration 90, loss = 0.02334124
Iteration 91, loss = 0.02310918
Iteration 92, loss = 0.02346868
Iteration 93, loss = 0.02358307
Iteration 94, loss = 0.02311353
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68286323
Iteration 2, loss = 0.60639462
Iteration 3, loss = 0.49275418
Iteration 4, loss = 0.37565408
Iteration 5, loss = 0.27930049
Iteration 6, loss = 0.20890397
Iteration 7, loss = 0.15960767
Iteration 8, loss = 0.12629087
Iteration 9, loss = 0.10315987
Iteration 10, loss = 0.08697196
Iteration 11, loss = 0.07508461
Iteration 12, loss = 0.06604353
Iteration 13, loss = 0.05918660
Iteration 14, loss = 0.05404538
Iteration 15, loss = 0.04997191
Iteration 16, loss = 0.04618882
Iteration 17, loss = 0.04341582
Iteration 18, loss = 0.04096345
Iteration 19, loss = 0.03921817
Iteration 20, loss = 0.03749550
Iteration 21, loss = 0.03612185
Iteration 22, loss = 0.03484594
Iteration 23, loss = 0.03372152
Iteration 24, loss = 0.03298748
Iteration 25, loss = 0.03204032
Iteration 26, loss = 0.03151659
Iteration 27, loss = 0.03102403
Iteration 28, loss = 0.03043651
Iteration 29, loss = 0.02962716
Iteration 30, loss = 0.02919186
Iteration 31, loss = 0.02890186
Iteration 32, loss = 0.02866461
Iteration 33, loss = 0.02785907
Iteration 34, loss = 0.02783658
Iteration 35, loss = 0.02783062
Iteration 36, loss = 0.02726950
Iteration 37, loss = 0.02719892
Iteration 38, loss = 0.02699718
Iteration 39, loss = 0.02683873
Iteration 40, loss = 0.02630612
Iteration 41, loss = 0.02632587
Iteration 42, loss = 0.02616096
Iteration 43, loss = 0.02597658
Iteration 44, loss = 0.02574136
Iteration 45, loss = 0.02568910
Iteration 46, loss = 0.02581740
Iteration 47, loss = 0.02562572
Iteration 48, loss = 0.02528154
Iteration 49, loss = 0.02529669
Iteration 50, loss = 0.02520236
Iteration 51, loss = 0.02512206
Iteration 52, loss = 0.02524371
Iteration 53, loss = 0.02526340
Iteration 54, loss = 0.02488661
Iteration 55, loss = 0.02498211
Iteration 56, loss = 0.02451909
Iteration 57, loss = 0.02492235
Iteration 58, loss = 0.02481573
Iteration 59, loss = 0.02500319
Iteration 60, loss = 0.02457835
Iteration 61, loss = 0.02452273
Iteration 62, loss = 0.02454074
Iteration 63, loss = 0.02435770
Iteration 64, loss = 0.02463986
Iteration 65, loss = 0.02474477
Iteration 66, loss = 0.02446835
Iteration 67, loss = 0.02428051
Iteration 68, loss = 0.02410223
Iteration 69, loss = 0.02458308
Iteration 70, loss = 0.02431039
Iteration 71, loss = 0.02433996
Iteration 72, loss = 0.02435492
Iteration 73, loss = 0.02426436
Iteration 74, loss = 0.02407750
Iteration 75, loss = 0.02405523
Iteration 76, loss = 0.02440111
Iteration 77, loss = 0.02372381
Iteration 78, loss = 0.02407220
Iteration 79, loss = 0.02421607
Iteration 80, loss = 0.02374008
Iteration 81, loss = 0.02373719
Iteration 82, loss = 0.02389023
Iteration 83, loss = 0.02376161
Iteration 84, loss = 0.02362020
Iteration 85, loss = 0.02384167
Iteration 86, loss = 0.02394900
Iteration 87, loss = 0.02383703
Iteration 88, loss = 0.02392230
Iteration 89, loss = 0.02386220
Iteration 90, loss = 0.02379668
Iteration 91, loss = 0.02377766
Iteration 92, loss = 0.02377894
Iteration 93, loss = 0.02384864
Iteration 94, loss = 0.02358088
Iteration 95, loss = 0.02370136
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68120516
Iteration 2, loss = 0.60240939
Iteration 3, loss = 0.48875762
Iteration 4, loss = 0.37398564
Iteration 5, loss = 0.27872515
Iteration 6, loss = 0.20832928
Iteration 7, loss = 0.15896571
Iteration 8, loss = 0.12530335
Iteration 9, loss = 0.10193361
Iteration 10, loss = 0.08515744
Iteration 11, loss = 0.07311500
Iteration 12, loss = 0.06395383
Iteration 13, loss = 0.05699174
Iteration 14, loss = 0.05153457
Iteration 15, loss = 0.04724342
Iteration 16, loss = 0.04364205
Iteration 17, loss = 0.04074252
Iteration 18, loss = 0.03845232
Iteration 19, loss = 0.03678829
Iteration 20, loss = 0.03481771
Iteration 21, loss = 0.03328753
Iteration 22, loss = 0.03206854
Iteration 23, loss = 0.03129289
Iteration 24, loss = 0.02983166
Iteration 25, loss = 0.02940936
Iteration 26, loss = 0.02853175
Iteration 27, loss = 0.02789063
Iteration 28, loss = 0.02746004
Iteration 29, loss = 0.02711382
Iteration 30, loss = 0.02652221
Iteration 31, loss = 0.02595122
Iteration 32, loss = 0.02570601
Iteration 33, loss = 0.02535391
Iteration 34, loss = 0.02513437
Iteration 35, loss = 0.02497236
Iteration 36, loss = 0.02451500
Iteration 37, loss = 0.02423288
Iteration 38, loss = 0.02412383
Iteration 39, loss = 0.02388086
Iteration 40, loss = 0.02361234
Iteration 41, loss = 0.02357997
Iteration 42, loss = 0.02362998
Iteration 43, loss = 0.02347322
Iteration 44, loss = 0.02320893
Iteration 45, loss = 0.02304863
Iteration 46, loss = 0.02287787
Iteration 47, loss = 0.02292427
Iteration 48, loss = 0.02318289
Iteration 49, loss = 0.02259563
Iteration 50, loss = 0.02274562
Iteration 51, loss = 0.02281007
Iteration 52, loss = 0.02265629
Iteration 53, loss = 0.02248390
Iteration 54, loss = 0.02223820
Iteration 55, loss = 0.02256074
Iteration 56, loss = 0.02231013
Iteration 57, loss = 0.02223729
Iteration 58, loss = 0.02192570
Iteration 59, loss = 0.02198761
Iteration 60, loss = 0.02189855
Iteration 61, loss = 0.02187415
Iteration 62, loss = 0.02194847
Iteration 63, loss = 0.02176009
Iteration 64, loss = 0.02185855
Iteration 65, loss = 0.02183427
Iteration 66, loss = 0.02184282
Iteration 67, loss = 0.02148833
Iteration 68, loss = 0.02159766
Iteration 69, loss = 0.02173231
Iteration 70, loss = 0.02169511
Iteration 71, loss = 0.02185519
Iteration 72, loss = 0.02178407
Iteration 73, loss = 0.02155462
Iteration 74, loss = 0.02177182
Iteration 75, loss = 0.02163439
Iteration 76, loss = 0.02138168
Iteration 77, loss = 0.02139018
Iteration 78, loss = 0.02151728
Iteration 79, loss = 0.02129340
Iteration 80, loss = 0.02155433
Iteration 81, loss = 0.02133241
Iteration 82, loss = 0.02114399
Iteration 83, loss = 0.02141585
Iteration 84, loss = 0.02150407
Iteration 85, loss = 0.02126781
Iteration 86, loss = 0.02131063
Iteration 87, loss = 0.02116889
Iteration 88, loss = 0.02116198
Iteration 89, loss = 0.02136099
Iteration 90, loss = 0.02116816
Iteration 91, loss = 0.02118787
Iteration 92, loss = 0.02087475
Iteration 93, loss = 0.02104374
Iteration 94, loss = 0.02116971
Iteration 95, loss = 0.02104421
Iteration 96, loss = 0.02132347
Iteration 97, loss = 0.02115302
Iteration 98, loss = 0.02113968
Iteration 99, loss = 0.02141356
Iteration 100, loss = 0.02131007
Iteration 101, loss = 0.02060717
Iteration 102, loss = 0.02091822
Iteration 103, loss = 0.02105802
Iteration 104, loss = 0.02110012
Iteration 105, loss = 0.02104955
Iteration 106, loss = 0.02078253
Iteration 107, loss = 0.02099838
Iteration 108, loss = 0.02080167
Iteration 109, loss = 0.02104842
Iteration 110, loss = 0.02065322
Iteration 111, loss = 0.02077663
Iteration 112, loss = 0.02091623
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70038784
Iteration 2, loss = 0.60823767
Iteration 3, loss = 0.49488390
Iteration 4, loss = 0.38112099
Iteration 5, loss = 0.28571571
Iteration 6, loss = 0.21335423
Iteration 7, loss = 0.16215922
Iteration 8, loss = 0.12757824
Iteration 9, loss = 0.10298530
Iteration 10, loss = 0.08596776
Iteration 11, loss = 0.07354965
Iteration 12, loss = 0.06406051
Iteration 13, loss = 0.05693172
Iteration 14, loss = 0.05169365
Iteration 15, loss = 0.04718417
Iteration 16, loss = 0.04368520
Iteration 17, loss = 0.04080872
Iteration 18, loss = 0.03849959
Iteration 19, loss = 0.03637117
Iteration 20, loss = 0.03481127
Iteration 21, loss = 0.03351581
Iteration 22, loss = 0.03194899
Iteration 23, loss = 0.03092412
Iteration 24, loss = 0.03001254
Iteration 25, loss = 0.02930258
Iteration 26, loss = 0.02855459
Iteration 27, loss = 0.02790136
Iteration 28, loss = 0.02741740
Iteration 29, loss = 0.02693831
Iteration 30, loss = 0.02660147
Iteration 31, loss = 0.02590638
Iteration 32, loss = 0.02569048
Iteration 33, loss = 0.02548243
Iteration 34, loss = 0.02499062
Iteration 35, loss = 0.02474548
Iteration 36, loss = 0.02445388
Iteration 37, loss = 0.02451008
Iteration 38, loss = 0.02401457
Iteration 39, loss = 0.02412349
Iteration 40, loss = 0.02367137
Iteration 41, loss = 0.02355791
Iteration 42, loss = 0.02350821
Iteration 43, loss = 0.02326930
Iteration 44, loss = 0.02316575
Iteration 45, loss = 0.02334798
Iteration 46, loss = 0.02304455
Iteration 47, loss = 0.02281671
Iteration 48, loss = 0.02292627
Iteration 49, loss = 0.02255169
Iteration 50, loss = 0.02278826
Iteration 51, loss = 0.02235764
Iteration 52, loss = 0.02252962
Iteration 53, loss = 0.02225951
Iteration 54, loss = 0.02233652
Iteration 55, loss = 0.02240437
Iteration 56, loss = 0.02226139
Iteration 57, loss = 0.02196045
Iteration 58, loss = 0.02191502
Iteration 59, loss = 0.02195673
Iteration 60, loss = 0.02189081
Iteration 61, loss = 0.02201095
Iteration 62, loss = 0.02198087
Iteration 63, loss = 0.02183348
Iteration 64, loss = 0.02209815
Iteration 65, loss = 0.02172065
Iteration 66, loss = 0.02177448
Iteration 67, loss = 0.02207802
Iteration 68, loss = 0.02156587
Iteration 69, loss = 0.02164653
Iteration 70, loss = 0.02133507
Iteration 71, loss = 0.02171107
Iteration 72, loss = 0.02140086
Iteration 73, loss = 0.02139516
Iteration 74, loss = 0.02124276
Iteration 75, loss = 0.02131695
Iteration 76, loss = 0.02131449
Iteration 77, loss = 0.02141703
Iteration 78, loss = 0.02144030
Iteration 79, loss = 0.02118172
Iteration 80, loss = 0.02118593
Iteration 81, loss = 0.02129909
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67402025
Iteration 2, loss = 0.59732637
Iteration 3, loss = 0.47604295
Iteration 4, loss = 0.35889098
Iteration 5, loss = 0.26548904
Iteration 6, loss = 0.19802381
Iteration 7, loss = 0.15184290
Iteration 8, loss = 0.12028728
Iteration 9, loss = 0.09811866
Iteration 10, loss = 0.08247084
Iteration 11, loss = 0.07108922
Iteration 12, loss = 0.06239488
Iteration 13, loss = 0.05570203
Iteration 14, loss = 0.05076771
Iteration 15, loss = 0.04612793
Iteration 16, loss = 0.04294937
Iteration 17, loss = 0.04014565
Iteration 18, loss = 0.03794536
Iteration 19, loss = 0.03587608
Iteration 20, loss = 0.03447492
Iteration 21, loss = 0.03296897
Iteration 22, loss = 0.03171481
Iteration 23, loss = 0.03082015
Iteration 24, loss = 0.02974385
Iteration 25, loss = 0.02897951
Iteration 26, loss = 0.02827446
Iteration 27, loss = 0.02785915
Iteration 28, loss = 0.02704911
Iteration 29, loss = 0.02668262
Iteration 30, loss = 0.02633511
Iteration 31, loss = 0.02591412
Iteration 32, loss = 0.02547032
Iteration 33, loss = 0.02501870
Iteration 34, loss = 0.02486768
Iteration 35, loss = 0.02472323
Iteration 36, loss = 0.02424026
Iteration 37, loss = 0.02428361
Iteration 38, loss = 0.02431678
Iteration 39, loss = 0.02396002
Iteration 40, loss = 0.02351423
Iteration 41, loss = 0.02344631
Iteration 42, loss = 0.02317375
Iteration 43, loss = 0.02332983
Iteration 44, loss = 0.02327654
Iteration 45, loss = 0.02282302
Iteration 46, loss = 0.02318591
Iteration 47, loss = 0.02265238
Iteration 48, loss = 0.02280208
Iteration 49, loss = 0.02250502
Iteration 50, loss = 0.02279457
Iteration 51, loss = 0.02256768
Iteration 52, loss = 0.02246288
Iteration 53, loss = 0.02236079
Iteration 54, loss = 0.02199425
Iteration 55, loss = 0.02204112
Iteration 56, loss = 0.02228940
Iteration 57, loss = 0.02189597
Iteration 58, loss = 0.02208298
Iteration 59, loss = 0.02173572
Iteration 60, loss = 0.02169539
Iteration 61, loss = 0.02166965
Iteration 62, loss = 0.02179010
Iteration 63, loss = 0.02176207
Iteration 64, loss = 0.02198877
Iteration 65, loss = 0.02132754
Iteration 66, loss = 0.02154362
Iteration 67, loss = 0.02143869
Iteration 68, loss = 0.02129596
Iteration 69, loss = 0.02130905
Iteration 70, loss = 0.02147222
Iteration 71, loss = 0.02130716
Iteration 72, loss = 0.02134905
Iteration 73, loss = 0.02123829
Iteration 74, loss = 0.02123714
Iteration 75, loss = 0.02136621
Iteration 76, loss = 0.02140689
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68755767
Iteration 2, loss = 0.59347949
Iteration 3, loss = 0.47465973
Iteration 4, loss = 0.36233254
Iteration 5, loss = 0.27071613
Iteration 6, loss = 0.20301815
Iteration 7, loss = 0.15617873
Iteration 8, loss = 0.12386048
Iteration 9, loss = 0.10126530
Iteration 10, loss = 0.08521000
Iteration 11, loss = 0.07341488
Iteration 12, loss = 0.06471468
Iteration 13, loss = 0.05793608
Iteration 14, loss = 0.05282093
Iteration 15, loss = 0.04830907
Iteration 16, loss = 0.04506661
Iteration 17, loss = 0.04197658
Iteration 18, loss = 0.03977665
Iteration 19, loss = 0.03775398
Iteration 20, loss = 0.03641590
Iteration 21, loss = 0.03506539
Iteration 22, loss = 0.03376799
Iteration 23, loss = 0.03272740
Iteration 24, loss = 0.03180150
Iteration 25, loss = 0.03103396
Iteration 26, loss = 0.03047706
Iteration 27, loss = 0.03005197
Iteration 28, loss = 0.02932097
Iteration 29, loss = 0.02896827
Iteration 30, loss = 0.02860838
Iteration 31, loss = 0.02788369
Iteration 32, loss = 0.02757494
Iteration 33, loss = 0.02759016
Iteration 34, loss = 0.02706523
Iteration 35, loss = 0.02661279
Iteration 36, loss = 0.02666437
Iteration 37, loss = 0.02629314
Iteration 38, loss = 0.02635661
Iteration 39, loss = 0.02606398
Iteration 40, loss = 0.02582286
Iteration 41, loss = 0.02546388
Iteration 42, loss = 0.02568640
Iteration 43, loss = 0.02536386
Iteration 44, loss = 0.02534001
Iteration 45, loss = 0.02500719
Iteration 46, loss = 0.02487958
Iteration 47, loss = 0.02496176
Iteration 48, loss = 0.02454189
Iteration 49, loss = 0.02471906
Iteration 50, loss = 0.02468827
Iteration 51, loss = 0.02460552
Iteration 52, loss = 0.02457417
Iteration 53, loss = 0.02456995
Iteration 54, loss = 0.02417348
Iteration 55, loss = 0.02414682
Iteration 56, loss = 0.02431870
Iteration 57, loss = 0.02413313
Iteration 58, loss = 0.02416120
Iteration 59, loss = 0.02402885
Iteration 60, loss = 0.02435768
Iteration 61, loss = 0.02442893
Iteration 62, loss = 0.02388646
Iteration 63, loss = 0.02391045
Iteration 64, loss = 0.02407574
Iteration 65, loss = 0.02405298
Iteration 66, loss = 0.02373181
Iteration 67, loss = 0.02379209
Iteration 68, loss = 0.02362008
Iteration 69, loss = 0.02370873
Iteration 70, loss = 0.02380694
Iteration 71, loss = 0.02369690
Iteration 72, loss = 0.02345105
Iteration 73, loss = 0.02348169
Iteration 74, loss = 0.02368865
Iteration 75, loss = 0.02391114
Iteration 76, loss = 0.02349155
Iteration 77, loss = 0.02341046
Iteration 78, loss = 0.02343429
Iteration 79, loss = 0.02337987
Iteration 80, loss = 0.02342355
Iteration 81, loss = 0.02339738
Iteration 82, loss = 0.02370327
Iteration 83, loss = 0.02315517
Iteration 84, loss = 0.02321282
Iteration 85, loss = 0.02314144
Iteration 86, loss = 0.02349944
Iteration 87, loss = 0.02360392
Iteration 88, loss = 0.02325343
Iteration 89, loss = 0.02325325
Iteration 90, loss = 0.02334124
Iteration 91, loss = 0.02310918
Iteration 92, loss = 0.02346868
Iteration 93, loss = 0.02358307
Iteration 94, loss = 0.02311353
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68286323
Iteration 2, loss = 0.60639462
Iteration 3, loss = 0.49275418
Iteration 4, loss = 0.37565408
Iteration 5, loss = 0.27930049
Iteration 6, loss = 0.20890397
Iteration 7, loss = 0.15960767
Iteration 8, loss = 0.12629087
Iteration 9, loss = 0.10315987
Iteration 10, loss = 0.08697196
Iteration 11, loss = 0.07508461
Iteration 12, loss = 0.06604353
Iteration 13, loss = 0.05918660
Iteration 14, loss = 0.05404538
Iteration 15, loss = 0.04997191
Iteration 16, loss = 0.04618882
Iteration 17, loss = 0.04341582
Iteration 18, loss = 0.04096345
Iteration 19, loss = 0.03921817
Iteration 20, loss = 0.03749550
Iteration 21, loss = 0.03612185
Iteration 22, loss = 0.03484594
Iteration 23, loss = 0.03372152
Iteration 24, loss = 0.03298748
Iteration 25, loss = 0.03204032
Iteration 26, loss = 0.03151659
Iteration 27, loss = 0.03102403
Iteration 28, loss = 0.03043651
Iteration 29, loss = 0.02962716
Iteration 30, loss = 0.02919186
Iteration 31, loss = 0.02890186
Iteration 32, loss = 0.02866461
Iteration 33, loss = 0.02785907
Iteration 34, loss = 0.02783658
Iteration 35, loss = 0.02783062
Iteration 36, loss = 0.02726950
Iteration 37, loss = 0.02719892
Iteration 38, loss = 0.02699718
Iteration 39, loss = 0.02683873
Iteration 40, loss = 0.02630612
Iteration 41, loss = 0.02632587
Iteration 42, loss = 0.02616096
Iteration 43, loss = 0.02597658
Iteration 44, loss = 0.02574136
Iteration 45, loss = 0.02568910
Iteration 46, loss = 0.02581740
Iteration 47, loss = 0.02562572
Iteration 48, loss = 0.02528154
Iteration 49, loss = 0.02529669
Iteration 50, loss = 0.02520236
Iteration 51, loss = 0.02512206
Iteration 52, loss = 0.02524371
Iteration 53, loss = 0.02526340
Iteration 54, loss = 0.02488661
Iteration 55, loss = 0.02498211
Iteration 56, loss = 0.02451909
Iteration 57, loss = 0.02492235
Iteration 58, loss = 0.02481573
Iteration 59, loss = 0.02500319
Iteration 60, loss = 0.02457835
Iteration 61, loss = 0.02452273
Iteration 62, loss = 0.02454074
Iteration 63, loss = 0.02435770
Iteration 64, loss = 0.02463986
Iteration 65, loss = 0.02474477
Iteration 66, loss = 0.02446835
Iteration 67, loss = 0.02428051
Iteration 68, loss = 0.02410223
Iteration 69, loss = 0.02458308
Iteration 70, loss = 0.02431039
Iteration 71, loss = 0.02433996
Iteration 72, loss = 0.02435492
Iteration 73, loss = 0.02426436
Iteration 74, loss = 0.02407750
Iteration 75, loss = 0.02405523
Iteration 76, loss = 0.02440111
Iteration 77, loss = 0.02372381
Iteration 78, loss = 0.02407220
Iteration 79, loss = 0.02421607
Iteration 80, loss = 0.02374008
Iteration 81, loss = 0.02373719
Iteration 82, loss = 0.02389023
Iteration 83, loss = 0.02376161
Iteration 84, loss = 0.02362020
Iteration 85, loss = 0.02384167
Iteration 86, loss = 0.02394900
Iteration 87, loss = 0.02383703
Iteration 88, loss = 0.02392230
Iteration 89, loss = 0.02386220
Iteration 90, loss = 0.02379668
Iteration 91, loss = 0.02377766
Iteration 92, loss = 0.02377894
Iteration 93, loss = 0.02384864
Iteration 94, loss = 0.02358088
Iteration 95, loss = 0.02370136
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68120516
Iteration 2, loss = 0.60240939
Iteration 3, loss = 0.48875762
Iteration 4, loss = 0.37398564
Iteration 5, loss = 0.27872515
Iteration 6, loss = 0.20832928
Iteration 7, loss = 0.15896571
Iteration 8, loss = 0.12530335
Iteration 9, loss = 0.10193361
Iteration 10, loss = 0.08515744
Iteration 11, loss = 0.07311500
Iteration 12, loss = 0.06395383
Iteration 13, loss = 0.05699174
Iteration 14, loss = 0.05153457
Iteration 15, loss = 0.04724342
Iteration 16, loss = 0.04364205
Iteration 17, loss = 0.04074252
Iteration 18, loss = 0.03845232
Iteration 19, loss = 0.03678829
Iteration 20, loss = 0.03481771
Iteration 21, loss = 0.03328753
Iteration 22, loss = 0.03206854
Iteration 23, loss = 0.03129289
Iteration 24, loss = 0.02983166
Iteration 25, loss = 0.02940936
Iteration 26, loss = 0.02853175
Iteration 27, loss = 0.02789063
Iteration 28, loss = 0.02746004
Iteration 29, loss = 0.02711382
Iteration 30, loss = 0.02652221
Iteration 31, loss = 0.02595122
Iteration 32, loss = 0.02570601
Iteration 33, loss = 0.02535391
Iteration 34, loss = 0.02513437
Iteration 35, loss = 0.02497236
Iteration 36, loss = 0.02451500
Iteration 37, loss = 0.02423288
Iteration 38, loss = 0.02412383
Iteration 39, loss = 0.02388086
Iteration 40, loss = 0.02361234
Iteration 41, loss = 0.02357997
Iteration 42, loss = 0.02362998
Iteration 43, loss = 0.02347322
Iteration 44, loss = 0.02320893
Iteration 45, loss = 0.02304863
Iteration 46, loss = 0.02287787
Iteration 47, loss = 0.02292427
Iteration 48, loss = 0.02318289
Iteration 49, loss = 0.02259563
Iteration 50, loss = 0.02274562
Iteration 51, loss = 0.02281007
Iteration 52, loss = 0.02265629
Iteration 53, loss = 0.02248390
Iteration 54, loss = 0.02223820
Iteration 55, loss = 0.02256074
Iteration 56, loss = 0.02231013
Iteration 57, loss = 0.02223729
Iteration 58, loss = 0.02192570
Iteration 59, loss = 0.02198761
Iteration 60, loss = 0.02189855
Iteration 61, loss = 0.02187415
Iteration 62, loss = 0.02194847
Iteration 63, loss = 0.02176009
Iteration 64, loss = 0.02185855
Iteration 65, loss = 0.02183427
Iteration 66, loss = 0.02184282
Iteration 67, loss = 0.02148833
Iteration 68, loss = 0.02159766
Iteration 69, loss = 0.02173231
Iteration 70, loss = 0.02169511
Iteration 71, loss = 0.02185519
Iteration 72, loss = 0.02178407
Iteration 73, loss = 0.02155462
Iteration 74, loss = 0.02177182
Iteration 75, loss = 0.02163439
Iteration 76, loss = 0.02138168
Iteration 77, loss = 0.02139018
Iteration 78, loss = 0.02151728
Iteration 79, loss = 0.02129340
Iteration 80, loss = 0.02155433
Iteration 81, loss = 0.02133241
Iteration 82, loss = 0.02114399
Iteration 83, loss = 0.02141585
Iteration 84, loss = 0.02150407
Iteration 85, loss = 0.02126781
Iteration 86, loss = 0.02131063
Iteration 87, loss = 0.02116889
Iteration 88, loss = 0.02116198
Iteration 89, loss = 0.02136099
Iteration 90, loss = 0.02116816
Iteration 91, loss = 0.02118787
Iteration 92, loss = 0.02087475
Iteration 93, loss = 0.02104374
Iteration 94, loss = 0.02116971
Iteration 95, loss = 0.02104421
Iteration 96, loss = 0.02132347
Iteration 97, loss = 0.02115302
Iteration 98, loss = 0.02113968
Iteration 99, loss = 0.02141356
Iteration 100, loss = 0.02131007
Iteration 101, loss = 0.02060717
Iteration 102, loss = 0.02091822
Iteration 103, loss = 0.02105802
Iteration 104, loss = 0.02110012
Iteration 105, loss = 0.02104955
Iteration 106, loss = 0.02078253
Iteration 107, loss = 0.02099838
Iteration 108, loss = 0.02080167
Iteration 109, loss = 0.02104842
Iteration 110, loss = 0.02065322
Iteration 111, loss = 0.02077663
Iteration 112, loss = 0.02091623
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70038784
Iteration 2, loss = 0.60823767
Iteration 3, loss = 0.49488390
Iteration 4, loss = 0.38112099
Iteration 5, loss = 0.28571571
Iteration 6, loss = 0.21335423
Iteration 7, loss = 0.16215922
Iteration 8, loss = 0.12757824
Iteration 9, loss = 0.10298530
Iteration 10, loss = 0.08596776
Iteration 11, loss = 0.07354965
Iteration 12, loss = 0.06406051
Iteration 13, loss = 0.05693172
Iteration 14, loss = 0.05169365
Iteration 15, loss = 0.04718417
Iteration 16, loss = 0.04368520
Iteration 17, loss = 0.04080872
Iteration 18, loss = 0.03849959
Iteration 19, loss = 0.03637117
Iteration 20, loss = 0.03481127
Iteration 21, loss = 0.03351581
Iteration 22, loss = 0.03194899
Iteration 23, loss = 0.03092412
Iteration 24, loss = 0.03001254
Iteration 25, loss = 0.02930258
Iteration 26, loss = 0.02855459
Iteration 27, loss = 0.02790136
Iteration 28, loss = 0.02741740
Iteration 29, loss = 0.02693831
Iteration 30, loss = 0.02660147
Iteration 31, loss = 0.02590638
Iteration 32, loss = 0.02569048
Iteration 33, loss = 0.02548243
Iteration 34, loss = 0.02499062
Iteration 35, loss = 0.02474548
Iteration 36, loss = 0.02445388
Iteration 37, loss = 0.02451008
Iteration 38, loss = 0.02401457
Iteration 39, loss = 0.02412349
Iteration 40, loss = 0.02367137
Iteration 41, loss = 0.02355791
Iteration 42, loss = 0.02350821
Iteration 43, loss = 0.02326930
Iteration 44, loss = 0.02316575
Iteration 45, loss = 0.02334798
Iteration 46, loss = 0.02304455
Iteration 47, loss = 0.02281671
Iteration 48, loss = 0.02292627
Iteration 49, loss = 0.02255169
Iteration 50, loss = 0.02278826
Iteration 51, loss = 0.02235764
Iteration 52, loss = 0.02252962
Iteration 53, loss = 0.02225951
Iteration 54, loss = 0.02233652
Iteration 55, loss = 0.02240437
Iteration 56, loss = 0.02226139
Iteration 57, loss = 0.02196045
Iteration 58, loss = 0.02191502
Iteration 59, loss = 0.02195673
Iteration 60, loss = 0.02189081
Iteration 61, loss = 0.02201095
Iteration 62, loss = 0.02198087
Iteration 63, loss = 0.02183348
Iteration 64, loss = 0.02209815
Iteration 65, loss = 0.02172065
Iteration 66, loss = 0.02177448
Iteration 67, loss = 0.02207802
Iteration 68, loss = 0.02156587
Iteration 69, loss = 0.02164653
Iteration 70, loss = 0.02133507
Iteration 71, loss = 0.02171107
Iteration 72, loss = 0.02140086
Iteration 73, loss = 0.02139516
Iteration 74, loss = 0.02124276
Iteration 75, loss = 0.02131695
Iteration 76, loss = 0.02131449
Iteration 77, loss = 0.02141703
Iteration 78, loss = 0.02144030
Iteration 79, loss = 0.02118172
Iteration 80, loss = 0.02118593
Iteration 81, loss = 0.02129909
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67402025
Iteration 2, loss = 0.59732637
Iteration 3, loss = 0.47604295
Iteration 4, loss = 0.35889098
Iteration 5, loss = 0.26548904
Iteration 6, loss = 0.19802381
Iteration 7, loss = 0.15184290
Iteration 8, loss = 0.12028728
Iteration 9, loss = 0.09811866
Iteration 10, loss = 0.08247084
Iteration 11, loss = 0.07108922
Iteration 12, loss = 0.06239488
Iteration 13, loss = 0.05570203
Iteration 14, loss = 0.05076771
Iteration 15, loss = 0.04612793
Iteration 16, loss = 0.04294937
Iteration 17, loss = 0.04014565
Iteration 18, loss = 0.03794536
Iteration 19, loss = 0.03587608
Iteration 20, loss = 0.03447492
Iteration 21, loss = 0.03296897
Iteration 22, loss = 0.03171481
Iteration 23, loss = 0.03082015
Iteration 24, loss = 0.02974385
Iteration 25, loss = 0.02897951
Iteration 26, loss = 0.02827446
Iteration 27, loss = 0.02785915
Iteration 28, loss = 0.02704911
Iteration 29, loss = 0.02668262
Iteration 30, loss = 0.02633511
Iteration 31, loss = 0.02591412
Iteration 32, loss = 0.02547032
Iteration 33, loss = 0.02501870
Iteration 34, loss = 0.02486768
Iteration 35, loss = 0.02472323
Iteration 36, loss = 0.02424026
Iteration 37, loss = 0.02428361
Iteration 38, loss = 0.02431678
Iteration 39, loss = 0.02396002
Iteration 40, loss = 0.02351423
Iteration 41, loss = 0.02344631
Iteration 42, loss = 0.02317375
Iteration 43, loss = 0.02332983
Iteration 44, loss = 0.02327654
Iteration 45, loss = 0.02282302
Iteration 46, loss = 0.02318591
Iteration 47, loss = 0.02265238
Iteration 48, loss = 0.02280208
Iteration 49, loss = 0.02250502
Iteration 50, loss = 0.02279457
Iteration 51, loss = 0.02256768
Iteration 52, loss = 0.02246288
Iteration 53, loss = 0.02236079
Iteration 54, loss = 0.02199425
Iteration 55, loss = 0.02204112
Iteration 56, loss = 0.02228940
Iteration 57, loss = 0.02189597
Iteration 58, loss = 0.02208298
Iteration 59, loss = 0.02173572
Iteration 60, loss = 0.02169539
Iteration 61, loss = 0.02166965
Iteration 62, loss = 0.02179010
Iteration 63, loss = 0.02176207
Iteration 64, loss = 0.02198877
Iteration 65, loss = 0.02132754
Iteration 66, loss = 0.02154362
Iteration 67, loss = 0.02143869
Iteration 68, loss = 0.02129596
Iteration 69, loss = 0.02130905
Iteration 70, loss = 0.02147222
Iteration 71, loss = 0.02130716
Iteration 72, loss = 0.02134905
Iteration 73, loss = 0.02123829
Iteration 74, loss = 0.02123714
Iteration 75, loss = 0.02136621
Iteration 76, loss = 0.02140689
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68755767
Iteration 2, loss = 0.59347949
Iteration 3, loss = 0.47465973
Iteration 4, loss = 0.36233254
Iteration 5, loss = 0.27071613
Iteration 6, loss = 0.20301815
Iteration 7, loss = 0.15617873
Iteration 8, loss = 0.12386048
Iteration 9, loss = 0.10126530
Iteration 10, loss = 0.08521000
Iteration 11, loss = 0.07341488
Iteration 12, loss = 0.06471468
Iteration 13, loss = 0.05793608
Iteration 14, loss = 0.05282093
Iteration 15, loss = 0.04830907
Iteration 16, loss = 0.04506661
Iteration 17, loss = 0.04197658
Iteration 18, loss = 0.03977665
Iteration 19, loss = 0.03775398
Iteration 20, loss = 0.03641590
Iteration 21, loss = 0.03506539
Iteration 22, loss = 0.03376799
Iteration 23, loss = 0.03272740
Iteration 24, loss = 0.03180150
Iteration 25, loss = 0.03103396
Iteration 26, loss = 0.03047706
Iteration 27, loss = 0.03005197
Iteration 28, loss = 0.02932097
Iteration 29, loss = 0.02896827
Iteration 30, loss = 0.02860838
Iteration 31, loss = 0.02788369
Iteration 32, loss = 0.02757494
Iteration 33, loss = 0.02759016
Iteration 34, loss = 0.02706523
Iteration 35, loss = 0.02661279
Iteration 36, loss = 0.02666437
Iteration 37, loss = 0.02629314
Iteration 38, loss = 0.02635661
Iteration 39, loss = 0.02606398
Iteration 40, loss = 0.02582286
Iteration 41, loss = 0.02546388
Iteration 42, loss = 0.02568640
Iteration 43, loss = 0.02536386
Iteration 44, loss = 0.02534001
Iteration 45, loss = 0.02500719
Iteration 46, loss = 0.02487958
Iteration 47, loss = 0.02496176
Iteration 48, loss = 0.02454189
Iteration 49, loss = 0.02471906
Iteration 50, loss = 0.02468827
Iteration 51, loss = 0.02460552
Iteration 52, loss = 0.02457417
Iteration 53, loss = 0.02456995
Iteration 54, loss = 0.02417348
Iteration 55, loss = 0.02414682
Iteration 56, loss = 0.02431870
Iteration 57, loss = 0.02413313
Iteration 58, loss = 0.02416120
Iteration 59, loss = 0.02402885
Iteration 60, loss = 0.02435768
Iteration 61, loss = 0.02442893
Iteration 62, loss = 0.02388646
Iteration 63, loss = 0.02391045
Iteration 64, loss = 0.02407574
Iteration 65, loss = 0.02405298
Iteration 66, loss = 0.02373181
Iteration 67, loss = 0.02379209
Iteration 68, loss = 0.02362008
Iteration 69, loss = 0.02370873
Iteration 70, loss = 0.02380694
Iteration 71, loss = 0.02369690
Iteration 72, loss = 0.02345105
Iteration 73, loss = 0.02348169
Iteration 74, loss = 0.02368865
Iteration 75, loss = 0.02391114
Iteration 76, loss = 0.02349155
Iteration 77, loss = 0.02341046
Iteration 78, loss = 0.02343429
Iteration 79, loss = 0.02337987
Iteration 80, loss = 0.02342355
Iteration 81, loss = 0.02339738
Iteration 82, loss = 0.02370327
Iteration 83, loss = 0.02315517
Iteration 84, loss = 0.02321282
Iteration 85, loss = 0.02314144
Iteration 86, loss = 0.02349944
Iteration 87, loss = 0.02360392
Iteration 88, loss = 0.02325343
Iteration 89, loss = 0.02325325
Iteration 90, loss = 0.02334124
Iteration 91, loss = 0.02310918
Iteration 92, loss = 0.02346868
Iteration 93, loss = 0.02358307
Iteration 94, loss = 0.02311353
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68286323
Iteration 2, loss = 0.60639462
Iteration 3, loss = 0.49275418
Iteration 4, loss = 0.37565408
Iteration 5, loss = 0.27930049
Iteration 6, loss = 0.20890397
Iteration 7, loss = 0.15960767
Iteration 8, loss = 0.12629087
Iteration 9, loss = 0.10315987
Iteration 10, loss = 0.08697196
Iteration 11, loss = 0.07508461
Iteration 12, loss = 0.06604353
Iteration 13, loss = 0.05918660
Iteration 14, loss = 0.05404538
Iteration 15, loss = 0.04997191
Iteration 16, loss = 0.04618882
Iteration 17, loss = 0.04341582
Iteration 18, loss = 0.04096345
Iteration 19, loss = 0.03921817
Iteration 20, loss = 0.03749550
Iteration 21, loss = 0.03612185
Iteration 22, loss = 0.03484594
Iteration 23, loss = 0.03372152
Iteration 24, loss = 0.03298748
Iteration 25, loss = 0.03204032
Iteration 26, loss = 0.03151659
Iteration 27, loss = 0.03102403
Iteration 28, loss = 0.03043651
Iteration 29, loss = 0.02962716
Iteration 30, loss = 0.02919186
Iteration 31, loss = 0.02890186
Iteration 32, loss = 0.02866461
Iteration 33, loss = 0.02785907
Iteration 34, loss = 0.02783658
Iteration 35, loss = 0.02783062
Iteration 36, loss = 0.02726950
Iteration 37, loss = 0.02719892
Iteration 38, loss = 0.02699718
Iteration 39, loss = 0.02683873
Iteration 40, loss = 0.02630612
Iteration 41, loss = 0.02632587
Iteration 42, loss = 0.02616096
Iteration 43, loss = 0.02597658
Iteration 44, loss = 0.02574136
Iteration 45, loss = 0.02568910
Iteration 46, loss = 0.02581740
Iteration 47, loss = 0.02562572
Iteration 48, loss = 0.02528154
Iteration 49, loss = 0.02529669
Iteration 50, loss = 0.02520236
Iteration 51, loss = 0.02512206
Iteration 52, loss = 0.02524371
Iteration 53, loss = 0.02526340
Iteration 54, loss = 0.02488661
Iteration 55, loss = 0.02498211
Iteration 56, loss = 0.02451909
Iteration 57, loss = 0.02492235
Iteration 58, loss = 0.02481573
Iteration 59, loss = 0.02500319
Iteration 60, loss = 0.02457835
Iteration 61, loss = 0.02452273
Iteration 62, loss = 0.02454074
Iteration 63, loss = 0.02435770
Iteration 64, loss = 0.02463986
Iteration 65, loss = 0.02474477
Iteration 66, loss = 0.02446835
Iteration 67, loss = 0.02428051
Iteration 68, loss = 0.02410223
Iteration 69, loss = 0.02458308
Iteration 70, loss = 0.02431039
Iteration 71, loss = 0.02433996
Iteration 72, loss = 0.02435492
Iteration 73, loss = 0.02426436
Iteration 74, loss = 0.02407750
Iteration 75, loss = 0.02405523
Iteration 76, loss = 0.02440111
Iteration 77, loss = 0.02372381
Iteration 78, loss = 0.02407220
Iteration 79, loss = 0.02421607
Iteration 80, loss = 0.02374008
Iteration 81, loss = 0.02373719
Iteration 82, loss = 0.02389023
Iteration 83, loss = 0.02376161
Iteration 84, loss = 0.02362020
Iteration 85, loss = 0.02384167
Iteration 86, loss = 0.02394900
Iteration 87, loss = 0.02383703
Iteration 88, loss = 0.02392230
Iteration 89, loss = 0.02386220
Iteration 90, loss = 0.02379668
Iteration 91, loss = 0.02377766
Iteration 92, loss = 0.02377894
Iteration 93, loss = 0.02384864
Iteration 94, loss = 0.02358088
Iteration 95, loss = 0.02370136
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68120516
Iteration 2, loss = 0.60240939
Iteration 3, loss = 0.48875762
Iteration 4, loss = 0.37398564
Iteration 5, loss = 0.27872515
Iteration 6, loss = 0.20832928
Iteration 7, loss = 0.15896571
Iteration 8, loss = 0.12530335
Iteration 9, loss = 0.10193361
Iteration 10, loss = 0.08515744
Iteration 11, loss = 0.07311500
Iteration 12, loss = 0.06395383
Iteration 13, loss = 0.05699174
Iteration 14, loss = 0.05153457
Iteration 15, loss = 0.04724342
Iteration 16, loss = 0.04364205
Iteration 17, loss = 0.04074252
Iteration 18, loss = 0.03845232
Iteration 19, loss = 0.03678829
Iteration 20, loss = 0.03481771
Iteration 21, loss = 0.03328753
Iteration 22, loss = 0.03206854
Iteration 23, loss = 0.03129289
Iteration 24, loss = 0.02983166
Iteration 25, loss = 0.02940936
Iteration 26, loss = 0.02853175
Iteration 27, loss = 0.02789063
Iteration 28, loss = 0.02746004
Iteration 29, loss = 0.02711382
Iteration 30, loss = 0.02652221
Iteration 31, loss = 0.02595122
Iteration 32, loss = 0.02570601
Iteration 33, loss = 0.02535391
Iteration 34, loss = 0.02513437
Iteration 35, loss = 0.02497236
Iteration 36, loss = 0.02451500
Iteration 37, loss = 0.02423288
Iteration 38, loss = 0.02412383
Iteration 39, loss = 0.02388086
Iteration 40, loss = 0.02361234
Iteration 41, loss = 0.02357997
Iteration 42, loss = 0.02362998
Iteration 43, loss = 0.02347322
Iteration 44, loss = 0.02320893
Iteration 45, loss = 0.02304863
Iteration 46, loss = 0.02287787
Iteration 47, loss = 0.02292427
Iteration 48, loss = 0.02318289
Iteration 49, loss = 0.02259563
Iteration 50, loss = 0.02274562
Iteration 51, loss = 0.02281007
Iteration 52, loss = 0.02265629
Iteration 53, loss = 0.02248390
Iteration 54, loss = 0.02223820
Iteration 55, loss = 0.02256074
Iteration 56, loss = 0.02231013
Iteration 57, loss = 0.02223729
Iteration 58, loss = 0.02192570
Iteration 59, loss = 0.02198761
Iteration 60, loss = 0.02189855
Iteration 61, loss = 0.02187415
Iteration 62, loss = 0.02194847
Iteration 63, loss = 0.02176009
Iteration 64, loss = 0.02185855
Iteration 65, loss = 0.02183427
Iteration 66, loss = 0.02184282
Iteration 67, loss = 0.02148833
Iteration 68, loss = 0.02159766
Iteration 69, loss = 0.02173231
Iteration 70, loss = 0.02169511
Iteration 71, loss = 0.02185519
Iteration 72, loss = 0.02178407
Iteration 73, loss = 0.02155462
Iteration 74, loss = 0.02177182
Iteration 75, loss = 0.02163439
Iteration 76, loss = 0.02138168
Iteration 77, loss = 0.02139018
Iteration 78, loss = 0.02151728
Iteration 79, loss = 0.02129340
Iteration 80, loss = 0.02155433
Iteration 81, loss = 0.02133241
Iteration 82, loss = 0.02114399
Iteration 83, loss = 0.02141585
Iteration 84, loss = 0.02150407
Iteration 85, loss = 0.02126781
Iteration 86, loss = 0.02131063
Iteration 87, loss = 0.02116889
Iteration 88, loss = 0.02116198
Iteration 89, loss = 0.02136099
Iteration 90, loss = 0.02116816
Iteration 91, loss = 0.02118787
Iteration 92, loss = 0.02087475
Iteration 93, loss = 0.02104374
Iteration 94, loss = 0.02116971
Iteration 95, loss = 0.02104421
Iteration 96, loss = 0.02132347
Iteration 97, loss = 0.02115302
Iteration 98, loss = 0.02113968
Iteration 99, loss = 0.02141356
Iteration 100, loss = 0.02131007
Iteration 101, loss = 0.02060717
Iteration 102, loss = 0.02091822
Iteration 103, loss = 0.02105802
Iteration 104, loss = 0.02110012
Iteration 105, loss = 0.02104955
Iteration 106, loss = 0.02078253
Iteration 107, loss = 0.02099838
Iteration 108, loss = 0.02080167
Iteration 109, loss = 0.02104842
Iteration 110, loss = 0.02065322
Iteration 111, loss = 0.02077663
Iteration 112, loss = 0.02091623
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70038784
Iteration 2, loss = 0.60823767
Iteration 3, loss = 0.49488390
Iteration 4, loss = 0.38112099
Iteration 5, loss = 0.28571571
Iteration 6, loss = 0.21335423
Iteration 7, loss = 0.16215922
Iteration 8, loss = 0.12757824
Iteration 9, loss = 0.10298530
Iteration 10, loss = 0.08596776
Iteration 11, loss = 0.07354965
Iteration 12, loss = 0.06406051
Iteration 13, loss = 0.05693172
Iteration 14, loss = 0.05169365
Iteration 15, loss = 0.04718417
Iteration 16, loss = 0.04368520
Iteration 17, loss = 0.04080872
Iteration 18, loss = 0.03849959
Iteration 19, loss = 0.03637117
Iteration 20, loss = 0.03481127
Iteration 21, loss = 0.03351581
Iteration 22, loss = 0.03194899
Iteration 23, loss = 0.03092412
Iteration 24, loss = 0.03001254
Iteration 25, loss = 0.02930258
Iteration 26, loss = 0.02855459
Iteration 27, loss = 0.02790136
Iteration 28, loss = 0.02741740
Iteration 29, loss = 0.02693831
Iteration 30, loss = 0.02660147
Iteration 31, loss = 0.02590638
Iteration 32, loss = 0.02569048
Iteration 33, loss = 0.02548243
Iteration 34, loss = 0.02499062
Iteration 35, loss = 0.02474548
Iteration 36, loss = 0.02445388
Iteration 37, loss = 0.02451008
Iteration 38, loss = 0.02401457
Iteration 39, loss = 0.02412349
Iteration 40, loss = 0.02367137
Iteration 41, loss = 0.02355791
Iteration 42, loss = 0.02350821
Iteration 43, loss = 0.02326930
Iteration 44, loss = 0.02316575
Iteration 45, loss = 0.02334798
Iteration 46, loss = 0.02304455
Iteration 47, loss = 0.02281671
Iteration 48, loss = 0.02292627
Iteration 49, loss = 0.02255169
Iteration 50, loss = 0.02278826
Iteration 51, loss = 0.02235764
Iteration 52, loss = 0.02252962
Iteration 53, loss = 0.02225951
Iteration 54, loss = 0.02233652
Iteration 55, loss = 0.02240437
Iteration 56, loss = 0.02226139
Iteration 57, loss = 0.02196045
Iteration 58, loss = 0.02191502
Iteration 59, loss = 0.02195673
Iteration 60, loss = 0.02189081
Iteration 61, loss = 0.02201095
Iteration 62, loss = 0.02198087
Iteration 63, loss = 0.02183348
Iteration 64, loss = 0.02209815
Iteration 65, loss = 0.02172065
Iteration 66, loss = 0.02177448
Iteration 67, loss = 0.02207802
Iteration 68, loss = 0.02156587
Iteration 69, loss = 0.02164653
Iteration 70, loss = 0.02133507
Iteration 71, loss = 0.02171107
Iteration 72, loss = 0.02140086
Iteration 73, loss = 0.02139516
Iteration 74, loss = 0.02124276
Iteration 75, loss = 0.02131695
Iteration 76, loss = 0.02131449
Iteration 77, loss = 0.02141703
Iteration 78, loss = 0.02144030
Iteration 79, loss = 0.02118172
Iteration 80, loss = 0.02118593
Iteration 81, loss = 0.02129909
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67402025
Iteration 2, loss = 0.59732637
Iteration 3, loss = 0.47604295
Iteration 4, loss = 0.35889098
Iteration 5, loss = 0.26548904
Iteration 6, loss = 0.19802381
Iteration 7, loss = 0.15184290
Iteration 8, loss = 0.12028728
Iteration 9, loss = 0.09811866
Iteration 10, loss = 0.08247084
Iteration 11, loss = 0.07108922
Iteration 12, loss = 0.06239488
Iteration 13, loss = 0.05570203
Iteration 14, loss = 0.05076771
Iteration 15, loss = 0.04612793
Iteration 16, loss = 0.04294937
Iteration 17, loss = 0.04014565
Iteration 18, loss = 0.03794536
Iteration 19, loss = 0.03587608
Iteration 20, loss = 0.03447492
Iteration 21, loss = 0.03296897
Iteration 22, loss = 0.03171481
Iteration 23, loss = 0.03082015
Iteration 24, loss = 0.02974385
Iteration 25, loss = 0.02897951
Iteration 26, loss = 0.02827446
Iteration 27, loss = 0.02785915
Iteration 28, loss = 0.02704911
Iteration 29, loss = 0.02668262
Iteration 30, loss = 0.02633511
Iteration 31, loss = 0.02591412
Iteration 32, loss = 0.02547032
Iteration 33, loss = 0.02501870
Iteration 34, loss = 0.02486768
Iteration 35, loss = 0.02472323
Iteration 36, loss = 0.02424026
Iteration 37, loss = 0.02428361
Iteration 38, loss = 0.02431678
Iteration 39, loss = 0.02396002
Iteration 40, loss = 0.02351423
Iteration 41, loss = 0.02344631
Iteration 42, loss = 0.02317375
Iteration 43, loss = 0.02332983
Iteration 44, loss = 0.02327654
Iteration 45, loss = 0.02282302
Iteration 46, loss = 0.02318591
Iteration 47, loss = 0.02265238
Iteration 48, loss = 0.02280208
Iteration 49, loss = 0.02250502
Iteration 50, loss = 0.02279457
Iteration 51, loss = 0.02256768
Iteration 52, loss = 0.02246288
Iteration 53, loss = 0.02236079
Iteration 54, loss = 0.02199425
Iteration 55, loss = 0.02204112
Iteration 56, loss = 0.02228940
Iteration 57, loss = 0.02189597
Iteration 58, loss = 0.02208298
Iteration 59, loss = 0.02173572
Iteration 60, loss = 0.02169539
Iteration 61, loss = 0.02166965
Iteration 62, loss = 0.02179010
Iteration 63, loss = 0.02176207
Iteration 64, loss = 0.02198877
Iteration 65, loss = 0.02132754
Iteration 66, loss = 0.02154362
Iteration 67, loss = 0.02143869
Iteration 68, loss = 0.02129596
Iteration 69, loss = 0.02130905
Iteration 70, loss = 0.02147222
Iteration 71, loss = 0.02130716
Iteration 72, loss = 0.02134905
Iteration 73, loss = 0.02123829
Iteration 74, loss = 0.02123714
Iteration 75, loss = 0.02136621
Iteration 76, loss = 0.02140689
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68965443
Iteration 2, loss = 0.59449580
Iteration 3, loss = 0.46052984
Iteration 4, loss = 0.33326281
Iteration 5, loss = 0.23609283
Iteration 6, loss = 0.17160902
Iteration 7, loss = 0.12981868
Iteration 8, loss = 0.10304116
Iteration 9, loss = 0.08491993
Iteration 10, loss = 0.07250930
Iteration 11, loss = 0.06333166
Iteration 12, loss = 0.05640149
Iteration 13, loss = 0.05133006
Iteration 14, loss = 0.04735157
Iteration 15, loss = 0.04415928
Iteration 16, loss = 0.04165017
Iteration 17, loss = 0.03933340
Iteration 18, loss = 0.03779758
Iteration 19, loss = 0.03613196
Iteration 20, loss = 0.03486389
Iteration 21, loss = 0.03378932
Iteration 22, loss = 0.03285883
Iteration 23, loss = 0.03233305
Iteration 24, loss = 0.03171170
Iteration 25, loss = 0.03105684
Iteration 26, loss = 0.03030202
Iteration 27, loss = 0.03013582
Iteration 28, loss = 0.02988782
Iteration 29, loss = 0.02915580
Iteration 30, loss = 0.02900513
Iteration 31, loss = 0.02851678
Iteration 32, loss = 0.02859243
Iteration 33, loss = 0.02814638
Iteration 34, loss = 0.02783001
Iteration 35, loss = 0.02776016
Iteration 36, loss = 0.02745405
Iteration 37, loss = 0.02738640
Iteration 38, loss = 0.02702425
Iteration 39, loss = 0.02708204
Iteration 40, loss = 0.02721830
Iteration 41, loss = 0.02705535
Iteration 42, loss = 0.02663186
Iteration 43, loss = 0.02666020
Iteration 44, loss = 0.02688158
Iteration 45, loss = 0.02670118
Iteration 46, loss = 0.02656937
Iteration 47, loss = 0.02625952
Iteration 48, loss = 0.02650241
Iteration 49, loss = 0.02627182
Iteration 50, loss = 0.02617767
Iteration 51, loss = 0.02621855
Iteration 52, loss = 0.02610345
Iteration 53, loss = 0.02581961
Iteration 54, loss = 0.02565990
Iteration 55, loss = 0.02600768
Iteration 56, loss = 0.02618468
Iteration 57, loss = 0.02579666
Iteration 58, loss = 0.02563610
Iteration 59, loss = 0.02567470
Iteration 60, loss = 0.02572060
Iteration 61, loss = 0.02561415
Iteration 62, loss = 0.02552425
Iteration 63, loss = 0.02562649
Iteration 64, loss = 0.02558914
Iteration 65, loss = 0.02549913
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on prepended_v4_stemmed...
Iteration 1, loss = 0.68751201
Iteration 2, loss = 0.63391190
Iteration 3, loss = 0.55805056
Iteration 4, loss = 0.46771823
Iteration 5, loss = 0.38015651
Iteration 6, loss = 0.30313401
Iteration 7, loss = 0.23927989
Iteration 8, loss = 0.19093102
Iteration 9, loss = 0.15421155
Iteration 10, loss = 0.12719300
Iteration 11, loss = 0.10707677
Iteration 12, loss = 0.09203909
Iteration 13, loss = 0.08027944
Iteration 14, loss = 0.07135087
Iteration 15, loss = 0.06421882
Iteration 16, loss = 0.05854611
Iteration 17, loss = 0.05372611
Iteration 18, loss = 0.04967952
Iteration 19, loss = 0.04668888
Iteration 20, loss = 0.04382412
Iteration 21, loss = 0.04146973
Iteration 22, loss = 0.03970307
Iteration 23, loss = 0.03776388
Iteration 24, loss = 0.03653514
Iteration 25, loss = 0.03500444
Iteration 26, loss = 0.03396337
Iteration 27, loss = 0.03275758
Iteration 28, loss = 0.03190973
Iteration 29, loss = 0.03121679
Iteration 30, loss = 0.03053707
Iteration 31, loss = 0.02966917
Iteration 32, loss = 0.02924316
Iteration 33, loss = 0.02878937
Iteration 34, loss = 0.02822135
Iteration 35, loss = 0.02796539
Iteration 36, loss = 0.02745210
Iteration 37, loss = 0.02703372
Iteration 38, loss = 0.02671604
Iteration 39, loss = 0.02652377
Iteration 40, loss = 0.02607255
Iteration 41, loss = 0.02587087
Iteration 42, loss = 0.02561178
Iteration 43, loss = 0.02542328
Iteration 44, loss = 0.02530807
Iteration 45, loss = 0.02487841
Iteration 46, loss = 0.02491450
Iteration 47, loss = 0.02473166
Iteration 48, loss = 0.02441480
Iteration 49, loss = 0.02441831
Iteration 50, loss = 0.02430409
Iteration 51, loss = 0.02414604
Iteration 52, loss = 0.02394195
Iteration 53, loss = 0.02385098
Iteration 54, loss = 0.02375962
Iteration 55, loss = 0.02378496
Iteration 56, loss = 0.02366916
Iteration 57, loss = 0.02366177
Iteration 58, loss = 0.02313040
Iteration 59, loss = 0.02320351
Iteration 60, loss = 0.02336819
Iteration 61, loss = 0.02289239
Iteration 62, loss = 0.02311544
Iteration 63, loss = 0.02301188
Iteration 64, loss = 0.02300687
Iteration 65, loss = 0.02278304
Iteration 66, loss = 0.02293374
Iteration 67, loss = 0.02271321
Iteration 68, loss = 0.02255238
Iteration 69, loss = 0.02263177
Iteration 70, loss = 0.02273182
Iteration 71, loss = 0.02259229
Iteration 72, loss = 0.02242666
Iteration 73, loss = 0.02240379
Iteration 74, loss = 0.02245307
Iteration 75, loss = 0.02235890
Iteration 76, loss = 0.02218917
Iteration 77, loss = 0.02242246
Iteration 78, loss = 0.02227884
Iteration 79, loss = 0.02240246
Iteration 80, loss = 0.02251737
Iteration 81, loss = 0.02217722
Iteration 82, loss = 0.02221513
Iteration 83, loss = 0.02213377
Iteration 84, loss = 0.02244840
Iteration 85, loss = 0.02211486
Iteration 86, loss = 0.02222717
Iteration 87, loss = 0.02217604
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68306918
Iteration 2, loss = 0.61359610
Iteration 3, loss = 0.51869424
Iteration 4, loss = 0.41765782
Iteration 5, loss = 0.32740972
Iteration 6, loss = 0.25445767
Iteration 7, loss = 0.19937275
Iteration 8, loss = 0.15913591
Iteration 9, loss = 0.13006988
Iteration 10, loss = 0.10858022
Iteration 11, loss = 0.09276267
Iteration 12, loss = 0.08065186
Iteration 13, loss = 0.07160370
Iteration 14, loss = 0.06435366
Iteration 15, loss = 0.05838980
Iteration 16, loss = 0.05398824
Iteration 17, loss = 0.05010739
Iteration 18, loss = 0.04706046
Iteration 19, loss = 0.04407243
Iteration 20, loss = 0.04187601
Iteration 21, loss = 0.03987072
Iteration 22, loss = 0.03819188
Iteration 23, loss = 0.03652347
Iteration 24, loss = 0.03542771
Iteration 25, loss = 0.03419433
Iteration 26, loss = 0.03410632
Iteration 27, loss = 0.03321970
Iteration 28, loss = 0.03217935
Iteration 29, loss = 0.03116528
Iteration 30, loss = 0.03071912
Iteration 31, loss = 0.03011299
Iteration 32, loss = 0.02955019
Iteration 33, loss = 0.02908937
Iteration 34, loss = 0.02887309
Iteration 35, loss = 0.02818290
Iteration 36, loss = 0.02765755
Iteration 37, loss = 0.02742604
Iteration 38, loss = 0.02714908
Iteration 39, loss = 0.02713595
Iteration 40, loss = 0.02743957
Iteration 41, loss = 0.02718096
Iteration 42, loss = 0.02674357
Iteration 43, loss = 0.02627527
Iteration 44, loss = 0.02592900
Iteration 45, loss = 0.02596185
Iteration 46, loss = 0.02576280
Iteration 47, loss = 0.02562616
Iteration 48, loss = 0.02544353
Iteration 49, loss = 0.02510269
Iteration 50, loss = 0.02503980
Iteration 51, loss = 0.02655817
Iteration 52, loss = 0.02734101
Iteration 53, loss = 0.02657876
Iteration 54, loss = 0.02854338
Iteration 55, loss = 0.02871864
Iteration 56, loss = 0.02787245
Iteration 57, loss = 0.02733825
Iteration 58, loss = 0.02679573
Iteration 59, loss = 0.02643146
Iteration 60, loss = 0.02594260
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70254452
Iteration 2, loss = 0.66166036
Iteration 3, loss = 0.60903074
Iteration 4, loss = 0.54200430
Iteration 5, loss = 0.46623840
Iteration 6, loss = 0.39211210
Iteration 7, loss = 0.32557426
Iteration 8, loss = 0.26906606
Iteration 9, loss = 0.22261983
Iteration 10, loss = 0.18522530
Iteration 11, loss = 0.15576714
Iteration 12, loss = 0.13256270
Iteration 13, loss = 0.11409130
Iteration 14, loss = 0.09945215
Iteration 15, loss = 0.08753889
Iteration 16, loss = 0.07801731
Iteration 17, loss = 0.07030708
Iteration 18, loss = 0.06384527
Iteration 19, loss = 0.05841704
Iteration 20, loss = 0.05388124
Iteration 21, loss = 0.05008352
Iteration 22, loss = 0.04663100
Iteration 23, loss = 0.04389149
Iteration 24, loss = 0.04129474
Iteration 25, loss = 0.03923876
Iteration 26, loss = 0.03730856
Iteration 27, loss = 0.03556679
Iteration 28, loss = 0.03420658
Iteration 29, loss = 0.03290313
Iteration 30, loss = 0.03167016
Iteration 31, loss = 0.03057457
Iteration 32, loss = 0.02952417
Iteration 33, loss = 0.02874031
Iteration 34, loss = 0.02797555
Iteration 35, loss = 0.02726324
Iteration 36, loss = 0.02664462
Iteration 37, loss = 0.02591235
Iteration 38, loss = 0.02546155
Iteration 39, loss = 0.02495699
Iteration 40, loss = 0.02432952
Iteration 41, loss = 0.02399423
Iteration 42, loss = 0.02370682
Iteration 43, loss = 0.02324387
Iteration 44, loss = 0.02299931
Iteration 45, loss = 0.02255557
Iteration 46, loss = 0.02232915
Iteration 47, loss = 0.02224787
Iteration 48, loss = 0.02174910
Iteration 49, loss = 0.02163062
Iteration 50, loss = 0.02122231
Iteration 51, loss = 0.02113389
Iteration 52, loss = 0.02088482
Iteration 53, loss = 0.02084190
Iteration 54, loss = 0.02057379
Iteration 55, loss = 0.02039302
Iteration 56, loss = 0.02016167
Iteration 57, loss = 0.02013461
Iteration 58, loss = 0.01988622
Iteration 59, loss = 0.01993313
Iteration 60, loss = 0.01981337
Iteration 61, loss = 0.01950864
Iteration 62, loss = 0.01943006
Iteration 63, loss = 0.01926625
Iteration 64, loss = 0.01929937
Iteration 65, loss = 0.01913542
Iteration 66, loss = 0.01896829
Iteration 67, loss = 0.01897624
Iteration 68, loss = 0.01894120
Iteration 69, loss = 0.01866260
Iteration 70, loss = 0.01859809
Iteration 71, loss = 0.01872132
Iteration 72, loss = 0.01860779
Iteration 73, loss = 0.01854165
Iteration 74, loss = 0.01842841
Iteration 75, loss = 0.01835351
Iteration 76, loss = 0.01829370
Iteration 77, loss = 0.01825374
Iteration 78, loss = 0.01832082
Iteration 79, loss = 0.01828405
Iteration 80, loss = 0.01807185
Iteration 81, loss = 0.01802557
Iteration 82, loss = 0.01807070
Iteration 83, loss = 0.01796543
Iteration 84, loss = 0.01803591
Iteration 85, loss = 0.01778033
Iteration 86, loss = 0.01782538
Iteration 87, loss = 0.01773838
Iteration 88, loss = 0.01771090
Iteration 89, loss = 0.01768982
Iteration 90, loss = 0.01767880
Iteration 91, loss = 0.01766820
Iteration 92, loss = 0.01759078
Iteration 93, loss = 0.01748960
Iteration 94, loss = 0.01755586
Iteration 95, loss = 0.01744155
Iteration 96, loss = 0.01739090
Iteration 97, loss = 0.01748269
Iteration 98, loss = 0.01751615
Iteration 99, loss = 0.01738138
Iteration 100, loss = 0.01741419
Iteration 101, loss = 0.01742237
Iteration 102, loss = 0.01738588
Iteration 103, loss = 0.01734338
Iteration 104, loss = 0.01720072
Iteration 105, loss = 0.01726221
Iteration 106, loss = 0.01725553
Iteration 107, loss = 0.01731893
Iteration 108, loss = 0.01714466
Iteration 109, loss = 0.01722228
Iteration 110, loss = 0.01724785
Iteration 111, loss = 0.01722143
Iteration 112, loss = 0.01702555
Iteration 113, loss = 0.01716900
Iteration 114, loss = 0.01707615
Iteration 115, loss = 0.01699957
Iteration 116, loss = 0.01697758
Iteration 117, loss = 0.01695960
Iteration 118, loss = 0.01693629
Iteration 119, loss = 0.01707813
Iteration 120, loss = 0.01712350
Iteration 121, loss = 0.01718306
Iteration 122, loss = 0.01710063
Iteration 123, loss = 0.01681978
Iteration 124, loss = 0.01689505
Iteration 125, loss = 0.01699904
Iteration 126, loss = 0.01703986
Iteration 127, loss = 0.01706554
Iteration 128, loss = 0.01689409
Iteration 129, loss = 0.01699644
Iteration 130, loss = 0.01699056
Iteration 131, loss = 0.01694399
Iteration 132, loss = 0.01696567
Iteration 133, loss = 0.01688719
Iteration 134, loss = 0.01683656
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67892460
Iteration 2, loss = 0.63996302
Iteration 3, loss = 0.58454944
Iteration 4, loss = 0.51519317
Iteration 5, loss = 0.44309182
Iteration 6, loss = 0.37484756
Iteration 7, loss = 0.31334012
Iteration 8, loss = 0.26068411
Iteration 9, loss = 0.21689684
Iteration 10, loss = 0.18152909
Iteration 11, loss = 0.15330836
Iteration 12, loss = 0.13090411
Iteration 13, loss = 0.11310317
Iteration 14, loss = 0.09876059
Iteration 15, loss = 0.08724933
Iteration 16, loss = 0.07785215
Iteration 17, loss = 0.07014484
Iteration 18, loss = 0.06371535
Iteration 19, loss = 0.05855441
Iteration 20, loss = 0.05395664
Iteration 21, loss = 0.05009680
Iteration 22, loss = 0.04672688
Iteration 23, loss = 0.04397222
Iteration 24, loss = 0.04138248
Iteration 25, loss = 0.03924467
Iteration 26, loss = 0.03738557
Iteration 27, loss = 0.03569711
Iteration 28, loss = 0.03441548
Iteration 29, loss = 0.03299900
Iteration 30, loss = 0.03158958
Iteration 31, loss = 0.03063905
Iteration 32, loss = 0.02960000
Iteration 33, loss = 0.02880028
Iteration 34, loss = 0.02804672
Iteration 35, loss = 0.02733661
Iteration 36, loss = 0.02662894
Iteration 37, loss = 0.02615539
Iteration 38, loss = 0.02555569
Iteration 39, loss = 0.02522490
Iteration 40, loss = 0.02479026
Iteration 41, loss = 0.02405768
Iteration 42, loss = 0.02373636
Iteration 43, loss = 0.02346259
Iteration 44, loss = 0.02308233
Iteration 45, loss = 0.02274894
Iteration 46, loss = 0.02257518
Iteration 47, loss = 0.02232203
Iteration 48, loss = 0.02172415
Iteration 49, loss = 0.02169855
Iteration 50, loss = 0.02149569
Iteration 51, loss = 0.02136001
Iteration 52, loss = 0.02095766
Iteration 53, loss = 0.02083420
Iteration 54, loss = 0.02071800
Iteration 55, loss = 0.02043025
Iteration 56, loss = 0.02032079
Iteration 57, loss = 0.02031125
Iteration 58, loss = 0.01998269
Iteration 59, loss = 0.01990091
Iteration 60, loss = 0.01967398
Iteration 61, loss = 0.01965430
Iteration 62, loss = 0.01958399
Iteration 63, loss = 0.01945186
Iteration 64, loss = 0.01936291
Iteration 65, loss = 0.01938299
Iteration 66, loss = 0.01928665
Iteration 67, loss = 0.01907110
Iteration 68, loss = 0.01899964
Iteration 69, loss = 0.01886440
Iteration 70, loss = 0.01899164
Iteration 71, loss = 0.01877143
Iteration 72, loss = 0.01892298
Iteration 73, loss = 0.01875741
Iteration 74, loss = 0.01860202
Iteration 75, loss = 0.01852782
Iteration 76, loss = 0.01843888
Iteration 77, loss = 0.01836506
Iteration 78, loss = 0.01835389
Iteration 79, loss = 0.01820045
Iteration 80, loss = 0.01835813
Iteration 81, loss = 0.01832419
Iteration 82, loss = 0.01826990
Iteration 83, loss = 0.01801943
Iteration 84, loss = 0.01819777
Iteration 85, loss = 0.01812326
Iteration 86, loss = 0.01790209
Iteration 87, loss = 0.01801081
Iteration 88, loss = 0.01815792
Iteration 89, loss = 0.01796367
Iteration 90, loss = 0.01786629
Iteration 91, loss = 0.01795665
Iteration 92, loss = 0.01775514
Iteration 93, loss = 0.01776999
Iteration 94, loss = 0.01778809
Iteration 95, loss = 0.01785743
Iteration 96, loss = 0.01756397
Iteration 97, loss = 0.01777091
Iteration 98, loss = 0.01762281
Iteration 99, loss = 0.01766019
Iteration 100, loss = 0.01752146
Iteration 101, loss = 0.01762553
Iteration 102, loss = 0.01759480
Iteration 103, loss = 0.01758435
Iteration 104, loss = 0.01744758
Iteration 105, loss = 0.01744994
Iteration 106, loss = 0.01753982
Iteration 107, loss = 0.01736452
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67704088
Iteration 2, loss = 0.64117249
Iteration 3, loss = 0.58950868
Iteration 4, loss = 0.52042881
Iteration 5, loss = 0.44558179
Iteration 6, loss = 0.37343490
Iteration 7, loss = 0.30904473
Iteration 8, loss = 0.25460489
Iteration 9, loss = 0.21017150
Iteration 10, loss = 0.17486044
Iteration 11, loss = 0.14695217
Iteration 12, loss = 0.12496849
Iteration 13, loss = 0.10757842
Iteration 14, loss = 0.09375891
Iteration 15, loss = 0.08269140
Iteration 16, loss = 0.07363486
Iteration 17, loss = 0.06624528
Iteration 18, loss = 0.06017502
Iteration 19, loss = 0.05493792
Iteration 20, loss = 0.05057571
Iteration 21, loss = 0.04698372
Iteration 22, loss = 0.04376205
Iteration 23, loss = 0.04093845
Iteration 24, loss = 0.03866590
Iteration 25, loss = 0.03667773
Iteration 26, loss = 0.03483378
Iteration 27, loss = 0.03318449
Iteration 28, loss = 0.03171846
Iteration 29, loss = 0.03047885
Iteration 30, loss = 0.02929990
Iteration 31, loss = 0.02834900
Iteration 32, loss = 0.02755310
Iteration 33, loss = 0.02643982
Iteration 34, loss = 0.02576693
Iteration 35, loss = 0.02502979
Iteration 36, loss = 0.02454374
Iteration 37, loss = 0.02376771
Iteration 38, loss = 0.02334348
Iteration 39, loss = 0.02286869
Iteration 40, loss = 0.02238190
Iteration 41, loss = 0.02196238
Iteration 42, loss = 0.02161158
Iteration 43, loss = 0.02120902
Iteration 44, loss = 0.02098023
Iteration 45, loss = 0.02072591
Iteration 46, loss = 0.02032902
Iteration 47, loss = 0.02005147
Iteration 48, loss = 0.01985258
Iteration 49, loss = 0.01962694
Iteration 50, loss = 0.01923846
Iteration 51, loss = 0.01910843
Iteration 52, loss = 0.01924038
Iteration 53, loss = 0.01873465
Iteration 54, loss = 0.01868216
Iteration 55, loss = 0.01848205
Iteration 56, loss = 0.01837079
Iteration 57, loss = 0.01808281
Iteration 58, loss = 0.01808344
Iteration 59, loss = 0.01790557
Iteration 60, loss = 0.01786567
Iteration 61, loss = 0.01766186
Iteration 62, loss = 0.01745731
Iteration 63, loss = 0.01748652
Iteration 64, loss = 0.01727307
Iteration 65, loss = 0.01704356
Iteration 66, loss = 0.01720303
Iteration 67, loss = 0.01694568
Iteration 68, loss = 0.01687373
Iteration 69, loss = 0.01677577
Iteration 70, loss = 0.01680558
Iteration 71, loss = 0.01662998
Iteration 72, loss = 0.01684143
Iteration 73, loss = 0.01674688
Iteration 74, loss = 0.01655363
Iteration 75, loss = 0.01639708
Iteration 76, loss = 0.01646742
Iteration 77, loss = 0.01637023
Iteration 78, loss = 0.01621743
Iteration 79, loss = 0.01635050
Iteration 80, loss = 0.01633157
Iteration 81, loss = 0.01632337
Iteration 82, loss = 0.01610353
Iteration 83, loss = 0.01610762
Iteration 84, loss = 0.01603628
Iteration 85, loss = 0.01593276
Iteration 86, loss = 0.01607452
Iteration 87, loss = 0.01600753
Iteration 88, loss = 0.01589324
Iteration 89, loss = 0.01589890
Iteration 90, loss = 0.01592537
Iteration 91, loss = 0.01580286
Iteration 92, loss = 0.01569201
Iteration 93, loss = 0.01573794
Iteration 94, loss = 0.01577569
Iteration 95, loss = 0.01564226
Iteration 96, loss = 0.01556581
Iteration 97, loss = 0.01569070
Iteration 98, loss = 0.01555355
Iteration 99, loss = 0.01570388
Iteration 100, loss = 0.01550581
Iteration 101, loss = 0.01557373
Iteration 102, loss = 0.01552851
Iteration 103, loss = 0.01540819
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68874229
Iteration 2, loss = 0.65206965
Iteration 3, loss = 0.60757849
Iteration 4, loss = 0.55268159
Iteration 5, loss = 0.49031756
Iteration 6, loss = 0.42719102
Iteration 7, loss = 0.36776920
Iteration 8, loss = 0.31361620
Iteration 9, loss = 0.26643148
Iteration 10, loss = 0.22595248
Iteration 11, loss = 0.19220676
Iteration 12, loss = 0.16431223
Iteration 13, loss = 0.14158727
Iteration 14, loss = 0.12282259
Iteration 15, loss = 0.10757270
Iteration 16, loss = 0.09490310
Iteration 17, loss = 0.08435867
Iteration 18, loss = 0.07574101
Iteration 19, loss = 0.06846511
Iteration 20, loss = 0.06227422
Iteration 21, loss = 0.05688318
Iteration 22, loss = 0.05236521
Iteration 23, loss = 0.04849157
Iteration 24, loss = 0.04502230
Iteration 25, loss = 0.04210076
Iteration 26, loss = 0.03951499
Iteration 27, loss = 0.03719343
Iteration 28, loss = 0.03512919
Iteration 29, loss = 0.03329447
Iteration 30, loss = 0.03175757
Iteration 31, loss = 0.03026982
Iteration 32, loss = 0.02887174
Iteration 33, loss = 0.02776048
Iteration 34, loss = 0.02667681
Iteration 35, loss = 0.02557668
Iteration 36, loss = 0.02471060
Iteration 37, loss = 0.02389629
Iteration 38, loss = 0.02320041
Iteration 39, loss = 0.02245077
Iteration 40, loss = 0.02187833
Iteration 41, loss = 0.02125097
Iteration 42, loss = 0.02074161
Iteration 43, loss = 0.02019163
Iteration 44, loss = 0.01975336
Iteration 45, loss = 0.01929200
Iteration 46, loss = 0.01886992
Iteration 47, loss = 0.01857716
Iteration 48, loss = 0.01818405
Iteration 49, loss = 0.01781479
Iteration 50, loss = 0.01758098
Iteration 51, loss = 0.01729218
Iteration 52, loss = 0.01696224
Iteration 53, loss = 0.01670806
Iteration 54, loss = 0.01642125
Iteration 55, loss = 0.01620054
Iteration 56, loss = 0.01597137
Iteration 57, loss = 0.01577812
Iteration 58, loss = 0.01558800
Iteration 59, loss = 0.01548061
Iteration 60, loss = 0.01523478
Iteration 61, loss = 0.01508308
Iteration 62, loss = 0.01497188
Iteration 63, loss = 0.01481409
Iteration 64, loss = 0.01471828
Iteration 65, loss = 0.01452793
Iteration 66, loss = 0.01433750
Iteration 67, loss = 0.01431397
Iteration 68, loss = 0.01420136
Iteration 69, loss = 0.01403877
Iteration 70, loss = 0.01393503
Iteration 71, loss = 0.01377127
Iteration 72, loss = 0.01375592
Iteration 73, loss = 0.01372838
Iteration 74, loss = 0.01355263
Iteration 75, loss = 0.01344263
Iteration 76, loss = 0.01350790
Iteration 77, loss = 0.01335860
Iteration 78, loss = 0.01324323
Iteration 79, loss = 0.01309331
Iteration 80, loss = 0.01305972
Iteration 81, loss = 0.01308902
Iteration 82, loss = 0.01295750
Iteration 83, loss = 0.01292883
Iteration 84, loss = 0.01296747
Iteration 85, loss = 0.01287114
Iteration 86, loss = 0.01276465
Iteration 87, loss = 0.01265486
Iteration 88, loss = 0.01264460
Iteration 89, loss = 0.01268743
Iteration 90, loss = 0.01263791
Iteration 91, loss = 0.01252201
Iteration 92, loss = 0.01246007
Iteration 93, loss = 0.01243278
Iteration 94, loss = 0.01235610
Iteration 95, loss = 0.01235568
Iteration 96, loss = 0.01232883
Iteration 97, loss = 0.01231784
Iteration 98, loss = 0.01221877
Iteration 99, loss = 0.01217151
Iteration 100, loss = 0.01212605
Iteration 101, loss = 0.01218825
Iteration 102, loss = 0.01205577
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67811356
Iteration 2, loss = 0.63157960
Iteration 3, loss = 0.56307295
Iteration 4, loss = 0.47820889
Iteration 5, loss = 0.39499765
Iteration 6, loss = 0.32068916
Iteration 7, loss = 0.25895584
Iteration 8, loss = 0.21004768
Iteration 9, loss = 0.17218690
Iteration 10, loss = 0.14316166
Iteration 11, loss = 0.12082319
Iteration 12, loss = 0.10384456
Iteration 13, loss = 0.09047250
Iteration 14, loss = 0.08000425
Iteration 15, loss = 0.07155222
Iteration 16, loss = 0.06455910
Iteration 17, loss = 0.05909130
Iteration 18, loss = 0.05461845
Iteration 19, loss = 0.05064218
Iteration 20, loss = 0.04739823
Iteration 21, loss = 0.04457585
Iteration 22, loss = 0.04211833
Iteration 23, loss = 0.03998481
Iteration 24, loss = 0.03812068
Iteration 25, loss = 0.03661936
Iteration 26, loss = 0.03532877
Iteration 27, loss = 0.03412361
Iteration 28, loss = 0.03289028
Iteration 29, loss = 0.03196679
Iteration 30, loss = 0.03123757
Iteration 31, loss = 0.03057082
Iteration 32, loss = 0.02961416
Iteration 33, loss = 0.02908342
Iteration 34, loss = 0.02832339
Iteration 35, loss = 0.02788993
Iteration 36, loss = 0.02734148
Iteration 37, loss = 0.02684232
Iteration 38, loss = 0.02645932
Iteration 39, loss = 0.02608403
Iteration 40, loss = 0.02584407
Iteration 41, loss = 0.02561094
Iteration 42, loss = 0.02518030
Iteration 43, loss = 0.02495213
Iteration 44, loss = 0.02473918
Iteration 45, loss = 0.02451567
Iteration 46, loss = 0.02403688
Iteration 47, loss = 0.02430229
Iteration 48, loss = 0.02378397
Iteration 49, loss = 0.02340414
Iteration 50, loss = 0.02355285
Iteration 51, loss = 0.02338298
Iteration 52, loss = 0.02323032
Iteration 53, loss = 0.02344161
Iteration 54, loss = 0.02329348
Iteration 55, loss = 0.02282452
Iteration 56, loss = 0.02247099
Iteration 57, loss = 0.02244598
Iteration 58, loss = 0.02258465
Iteration 59, loss = 0.02222820
Iteration 60, loss = 0.02225390
Iteration 61, loss = 0.02204099
Iteration 62, loss = 0.02201396
Iteration 63, loss = 0.02184348
Iteration 64, loss = 0.02178706
Iteration 65, loss = 0.02194870
Iteration 66, loss = 0.02159426
Iteration 67, loss = 0.02147174
Iteration 68, loss = 0.02154458
Iteration 69, loss = 0.02139378
Iteration 70, loss = 0.02194492
Iteration 71, loss = 0.02172419
Iteration 72, loss = 0.02142328
Iteration 73, loss = 0.02151947
Iteration 74, loss = 0.02140907
Iteration 75, loss = 0.02126753
Iteration 76, loss = 0.02129640
Iteration 77, loss = 0.02109974
Iteration 78, loss = 0.02101429
Iteration 79, loss = 0.02123598
Iteration 80, loss = 0.02109871
Iteration 81, loss = 0.02098183
Iteration 82, loss = 0.02077571
Iteration 83, loss = 0.02066894
Iteration 84, loss = 0.02090216
Iteration 85, loss = 0.02142864
Iteration 86, loss = 0.02095030
Iteration 87, loss = 0.02117846
Iteration 88, loss = 0.02081382
Iteration 89, loss = 0.02076599
Iteration 90, loss = 0.02083218
Iteration 91, loss = 0.02072664
Iteration 92, loss = 0.02059184
Iteration 93, loss = 0.02053566
Iteration 94, loss = 0.02055051
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67755526
Iteration 2, loss = 0.65373758
Iteration 3, loss = 0.62534309
Iteration 4, loss = 0.58742532
Iteration 5, loss = 0.53932989
Iteration 6, loss = 0.48576735
Iteration 7, loss = 0.43177966
Iteration 8, loss = 0.38101773
Iteration 9, loss = 0.33431214
Iteration 10, loss = 0.29240724
Iteration 11, loss = 0.25573379
Iteration 12, loss = 0.22364959
Iteration 13, loss = 0.19646066
Iteration 14, loss = 0.17301099
Iteration 15, loss = 0.15334119
Iteration 16, loss = 0.13675813
Iteration 17, loss = 0.12279694
Iteration 18, loss = 0.11061960
Iteration 19, loss = 0.10037899
Iteration 20, loss = 0.09150507
Iteration 21, loss = 0.08389154
Iteration 22, loss = 0.07732007
Iteration 23, loss = 0.07161683
Iteration 24, loss = 0.06649966
Iteration 25, loss = 0.06209628
Iteration 26, loss = 0.05827735
Iteration 27, loss = 0.05476770
Iteration 28, loss = 0.05177707
Iteration 29, loss = 0.04897348
Iteration 30, loss = 0.04646800
Iteration 31, loss = 0.04447178
Iteration 32, loss = 0.04225760
Iteration 33, loss = 0.04065650
Iteration 34, loss = 0.03959328
Iteration 35, loss = 0.03806589
Iteration 36, loss = 0.03634460
Iteration 37, loss = 0.03490413
Iteration 38, loss = 0.03366829
Iteration 39, loss = 0.03255749
Iteration 40, loss = 0.03172057
Iteration 41, loss = 0.03066536
Iteration 42, loss = 0.02985955
Iteration 43, loss = 0.02920332
Iteration 44, loss = 0.02838898
Iteration 45, loss = 0.02760974
Iteration 46, loss = 0.02715399
Iteration 47, loss = 0.02691798
Iteration 48, loss = 0.02635409
Iteration 49, loss = 0.02581510
Iteration 50, loss = 0.02507372
Iteration 51, loss = 0.02461962
Iteration 52, loss = 0.02423209
Iteration 53, loss = 0.02388362
Iteration 54, loss = 0.02345063
Iteration 55, loss = 0.02302355
Iteration 56, loss = 0.02259755
Iteration 57, loss = 0.02220065
Iteration 58, loss = 0.02190269
Iteration 59, loss = 0.02166020
Iteration 60, loss = 0.02144600
Iteration 61, loss = 0.02122852
Iteration 62, loss = 0.02071366
Iteration 63, loss = 0.02104457
Iteration 64, loss = 0.02090069
Iteration 65, loss = 0.02076720
Iteration 66, loss = 0.02058138
Iteration 67, loss = 0.02025758
Iteration 68, loss = 0.01995292
Iteration 69, loss = 0.01976943
Iteration 70, loss = 0.01970025
Iteration 71, loss = 0.01937668
Iteration 72, loss = 0.01927862
Iteration 73, loss = 0.01902604
Iteration 74, loss = 0.01878324
Iteration 75, loss = 0.01899517
Iteration 76, loss = 0.01875188
Iteration 77, loss = 0.01846683
Iteration 78, loss = 0.01832310
Iteration 79, loss = 0.01835510
Iteration 80, loss = 0.01832411
Iteration 81, loss = 0.01827343
Iteration 82, loss = 0.01804881
Iteration 83, loss = 0.01769858
Iteration 84, loss = 0.01755358
Iteration 85, loss = 0.01757868
Iteration 86, loss = 0.01731880
Iteration 87, loss = 0.01742220
Iteration 88, loss = 0.01735319
Iteration 89, loss = 0.01758334
Iteration 90, loss = 0.01730966
Iteration 91, loss = 0.01727961
Iteration 92, loss = 0.01705962
Iteration 93, loss = 0.01696084
Iteration 94, loss = 0.01683000
Iteration 95, loss = 0.01697173
Iteration 96, loss = 0.01693847
Iteration 97, loss = 0.01676794
Iteration 98, loss = 0.01656314
Iteration 99, loss = 0.01638935
Iteration 100, loss = 0.01645701
Iteration 101, loss = 0.01618036
Iteration 102, loss = 0.01614761
Iteration 103, loss = 0.01620928
Iteration 104, loss = 0.01605824
Iteration 105, loss = 0.01608116
Iteration 106, loss = 0.01599845
Iteration 107, loss = 0.01599034
Iteration 108, loss = 0.01595427
Iteration 109, loss = 0.01610815
Iteration 110, loss = 0.01596523
Iteration 111, loss = 0.01595198
Iteration 112, loss = 0.01583221
Iteration 113, loss = 0.01603610
Iteration 114, loss = 0.01609690
Iteration 115, loss = 0.01561944
Iteration 116, loss = 0.01584304
Iteration 117, loss = 0.01590193
Iteration 118, loss = 0.01573862
Iteration 119, loss = 0.01575262
Iteration 120, loss = 0.01585523
Iteration 121, loss = 0.01589597
Iteration 122, loss = 0.01575147
Iteration 123, loss = 0.01546280
Iteration 124, loss = 0.01539220
Iteration 125, loss = 0.01537257
Iteration 126, loss = 0.01512362
Iteration 127, loss = 0.01542721
Iteration 128, loss = 0.01531690
Iteration 129, loss = 0.01531075
Iteration 130, loss = 0.01520044
Iteration 131, loss = 0.01521564
Iteration 132, loss = 0.01514383
Iteration 133, loss = 0.01504444
Iteration 134, loss = 0.01513635
Iteration 135, loss = 0.01523052
Iteration 136, loss = 0.01535791
Iteration 137, loss = 0.01554728
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69292142
Iteration 2, loss = 0.64290656
Iteration 3, loss = 0.56545929
Iteration 4, loss = 0.47049645
Iteration 5, loss = 0.38020628
Iteration 6, loss = 0.30147299
Iteration 7, loss = 0.23783947
Iteration 8, loss = 0.18886305
Iteration 9, loss = 0.15249570
Iteration 10, loss = 0.12533733
Iteration 11, loss = 0.10513891
Iteration 12, loss = 0.08982186
Iteration 13, loss = 0.07814443
Iteration 14, loss = 0.06895243
Iteration 15, loss = 0.06175390
Iteration 16, loss = 0.05592777
Iteration 17, loss = 0.05096807
Iteration 18, loss = 0.04714797
Iteration 19, loss = 0.04404203
Iteration 20, loss = 0.04121507
Iteration 21, loss = 0.03874929
Iteration 22, loss = 0.03696314
Iteration 23, loss = 0.03518892
Iteration 24, loss = 0.03349139
Iteration 25, loss = 0.03224862
Iteration 26, loss = 0.03110945
Iteration 27, loss = 0.03022645
Iteration 28, loss = 0.02904398
Iteration 29, loss = 0.02835530
Iteration 30, loss = 0.02773886
Iteration 31, loss = 0.02701928
Iteration 32, loss = 0.02641969
Iteration 33, loss = 0.02600061
Iteration 34, loss = 0.02553922
Iteration 35, loss = 0.02506506
Iteration 36, loss = 0.02459586
Iteration 37, loss = 0.02429241
Iteration 38, loss = 0.02398363
Iteration 39, loss = 0.02394602
Iteration 40, loss = 0.02363186
Iteration 41, loss = 0.02293782
Iteration 42, loss = 0.02299232
Iteration 43, loss = 0.02268866
Iteration 44, loss = 0.02236586
Iteration 45, loss = 0.02229702
Iteration 46, loss = 0.02206478
Iteration 47, loss = 0.02187181
Iteration 48, loss = 0.02177263
Iteration 49, loss = 0.02142042
Iteration 50, loss = 0.02142748
Iteration 51, loss = 0.02124939
Iteration 52, loss = 0.02113943
Iteration 53, loss = 0.02115239
Iteration 54, loss = 0.02096068
Iteration 55, loss = 0.02080897
Iteration 56, loss = 0.02085821
Iteration 57, loss = 0.02082111
Iteration 58, loss = 0.02047899
Iteration 59, loss = 0.02052539
Iteration 60, loss = 0.02040831
Iteration 61, loss = 0.02024781
Iteration 62, loss = 0.02048216
Iteration 63, loss = 0.02045858
Iteration 64, loss = 0.02008062
Iteration 65, loss = 0.02012851
Iteration 66, loss = 0.02010047
Iteration 67, loss = 0.02009316
Iteration 68, loss = 0.02014294
Iteration 69, loss = 0.01981033
Iteration 70, loss = 0.01978779
Iteration 71, loss = 0.02005120
Iteration 72, loss = 0.01968574
Iteration 73, loss = 0.01974616
Iteration 74, loss = 0.01963753
Iteration 75, loss = 0.01969642
Iteration 76, loss = 0.01959987
Iteration 77, loss = 0.01950633
Iteration 78, loss = 0.01962363
Iteration 79, loss = 0.01953150
Iteration 80, loss = 0.01943911
Iteration 81, loss = 0.01943015
Iteration 82, loss = 0.01949700
Iteration 83, loss = 0.01936937
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72337814
Iteration 2, loss = 0.70190419
Iteration 3, loss = 0.68108694
Iteration 4, loss = 0.65712241
Iteration 5, loss = 0.62955359
Iteration 6, loss = 0.59766196
Iteration 7, loss = 0.56260701
Iteration 8, loss = 0.52538069
Iteration 9, loss = 0.48714871
Iteration 10, loss = 0.44875946
Iteration 11, loss = 0.41125909
Iteration 12, loss = 0.37551571
Iteration 13, loss = 0.34158054
Iteration 14, loss = 0.31016521
Iteration 15, loss = 0.28106584
Iteration 16, loss = 0.25459372
Iteration 17, loss = 0.23061492
Iteration 18, loss = 0.20907728
Iteration 19, loss = 0.18967857
Iteration 20, loss = 0.17250260
Iteration 21, loss = 0.15721425
Iteration 22, loss = 0.14374575
Iteration 23, loss = 0.13179043
Iteration 24, loss = 0.12110529
Iteration 25, loss = 0.11162512
Iteration 26, loss = 0.10325147
Iteration 27, loss = 0.09585919
Iteration 28, loss = 0.08914149
Iteration 29, loss = 0.08326321
Iteration 30, loss = 0.07794098
Iteration 31, loss = 0.07314500
Iteration 32, loss = 0.06885106
Iteration 33, loss = 0.06496975
Iteration 34, loss = 0.06153112
Iteration 35, loss = 0.05832292
Iteration 36, loss = 0.05547042
Iteration 37, loss = 0.05276619
Iteration 38, loss = 0.05040710
Iteration 39, loss = 0.04815869
Iteration 40, loss = 0.04610827
Iteration 41, loss = 0.04424621
Iteration 42, loss = 0.04254784
Iteration 43, loss = 0.04092544
Iteration 44, loss = 0.03952521
Iteration 45, loss = 0.03813959
Iteration 46, loss = 0.03682207
Iteration 47, loss = 0.03564847
Iteration 48, loss = 0.03453102
Iteration 49, loss = 0.03356496
Iteration 50, loss = 0.03256276
Iteration 51, loss = 0.03164458
Iteration 52, loss = 0.03085494
Iteration 53, loss = 0.03002245
Iteration 54, loss = 0.02929613
Iteration 55, loss = 0.02856832
Iteration 56, loss = 0.02791114
Iteration 57, loss = 0.02730314
Iteration 58, loss = 0.02670875
Iteration 59, loss = 0.02614807
Iteration 60, loss = 0.02563595
Iteration 61, loss = 0.02510235
Iteration 62, loss = 0.02468596
Iteration 63, loss = 0.02424439
Iteration 64, loss = 0.02385154
Iteration 65, loss = 0.02338569
Iteration 66, loss = 0.02299398
Iteration 67, loss = 0.02265015
Iteration 68, loss = 0.02231965
Iteration 69, loss = 0.02195382
Iteration 70, loss = 0.02162365
Iteration 71, loss = 0.02132259
Iteration 72, loss = 0.02105878
Iteration 73, loss = 0.02079341
Iteration 74, loss = 0.02051093
Iteration 75, loss = 0.02027806
Iteration 76, loss = 0.01999299
Iteration 77, loss = 0.01975273
Iteration 78, loss = 0.01952990
Iteration 79, loss = 0.01932088
Iteration 80, loss = 0.01912103
Iteration 81, loss = 0.01890387
Iteration 82, loss = 0.01873948
Iteration 83, loss = 0.01860875
Iteration 84, loss = 0.01841147
Iteration 85, loss = 0.01820817
Iteration 86, loss = 0.01804568
Iteration 87, loss = 0.01785420
Iteration 88, loss = 0.01773139
Iteration 89, loss = 0.01760280
Iteration 90, loss = 0.01745555
Iteration 91, loss = 0.01738816
Iteration 92, loss = 0.01727408
Iteration 93, loss = 0.01700932
Iteration 94, loss = 0.01690882
Iteration 95, loss = 0.01680395
Iteration 96, loss = 0.01672795
Iteration 97, loss = 0.01658466
Iteration 98, loss = 0.01644224
Iteration 99, loss = 0.01649765
Iteration 100, loss = 0.01628028
Iteration 101, loss = 0.01614273
Iteration 102, loss = 0.01608817
Iteration 103, loss = 0.01600611
Iteration 104, loss = 0.01594361
Iteration 105, loss = 0.01579631
Iteration 106, loss = 0.01575421
Iteration 107, loss = 0.01565632
Iteration 108, loss = 0.01552773
Iteration 109, loss = 0.01552589
Iteration 110, loss = 0.01549550
Iteration 111, loss = 0.01532842
Iteration 112, loss = 0.01525493
Iteration 113, loss = 0.01525369
Iteration 114, loss = 0.01518202
Iteration 115, loss = 0.01508496
Iteration 116, loss = 0.01500151
Iteration 117, loss = 0.01499747
Iteration 118, loss = 0.01489144
Iteration 119, loss = 0.01480968
Iteration 120, loss = 0.01475804
Iteration 121, loss = 0.01473636
Iteration 122, loss = 0.01467241
Iteration 123, loss = 0.01461512
Iteration 124, loss = 0.01454116
Iteration 125, loss = 0.01450897
Iteration 126, loss = 0.01446484
Iteration 127, loss = 0.01442237
Iteration 128, loss = 0.01435098
Iteration 129, loss = 0.01430444
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69935626
Iteration 2, loss = 0.63868530
Iteration 3, loss = 0.56619937
Iteration 4, loss = 0.48121360
Iteration 5, loss = 0.39888378
Iteration 6, loss = 0.32529322
Iteration 7, loss = 0.26381596
Iteration 8, loss = 0.21452392
Iteration 9, loss = 0.17584177
Iteration 10, loss = 0.14600803
Iteration 11, loss = 0.12279576
Iteration 12, loss = 0.10507951
Iteration 13, loss = 0.09085900
Iteration 14, loss = 0.07969976
Iteration 15, loss = 0.07062394
Iteration 16, loss = 0.06344753
Iteration 17, loss = 0.05758715
Iteration 18, loss = 0.05290321
Iteration 19, loss = 0.04862437
Iteration 20, loss = 0.04519813
Iteration 21, loss = 0.04222068
Iteration 22, loss = 0.03980349
Iteration 23, loss = 0.03757319
Iteration 24, loss = 0.03563894
Iteration 25, loss = 0.03391024
Iteration 26, loss = 0.03243522
Iteration 27, loss = 0.03125423
Iteration 28, loss = 0.03019463
Iteration 29, loss = 0.02917478
Iteration 30, loss = 0.02814925
Iteration 31, loss = 0.02756432
Iteration 32, loss = 0.02675133
Iteration 33, loss = 0.02580867
Iteration 34, loss = 0.02524614
Iteration 35, loss = 0.02458816
Iteration 36, loss = 0.02404534
Iteration 37, loss = 0.02359296
Iteration 38, loss = 0.02299077
Iteration 39, loss = 0.02281054
Iteration 40, loss = 0.02255245
Iteration 41, loss = 0.02214467
Iteration 42, loss = 0.02185429
Iteration 43, loss = 0.02152317
Iteration 44, loss = 0.02121757
Iteration 45, loss = 0.02120507
Iteration 46, loss = 0.02066938
Iteration 47, loss = 0.02054427
Iteration 48, loss = 0.02038541
Iteration 49, loss = 0.02007370
Iteration 50, loss = 0.01993566
Iteration 51, loss = 0.01969669
Iteration 52, loss = 0.01948274
Iteration 53, loss = 0.01950641
Iteration 54, loss = 0.01940351
Iteration 55, loss = 0.01916725
Iteration 56, loss = 0.01915969
Iteration 57, loss = 0.01909617
Iteration 58, loss = 0.01878816
Iteration 59, loss = 0.01875405
Iteration 60, loss = 0.01879434
Iteration 61, loss = 0.01867048
Iteration 62, loss = 0.01842036
Iteration 63, loss = 0.01830463
Iteration 64, loss = 0.01846051
Iteration 65, loss = 0.01825732
Iteration 66, loss = 0.01811440
Iteration 67, loss = 0.01797420
Iteration 68, loss = 0.01801973
Iteration 69, loss = 0.01822617
Iteration 70, loss = 0.01801406
Iteration 71, loss = 0.01787960
Iteration 72, loss = 0.01771443
Iteration 73, loss = 0.01756751
Iteration 74, loss = 0.01778099
Iteration 75, loss = 0.01798253
Iteration 76, loss = 0.01764696
Iteration 77, loss = 0.01769799
Iteration 78, loss = 0.01747721
Iteration 79, loss = 0.01760232
Iteration 80, loss = 0.01743962
Iteration 81, loss = 0.01789090
Iteration 82, loss = 0.01736485
Iteration 83, loss = 0.01770455
Iteration 84, loss = 0.01745701
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69122488
Iteration 2, loss = 0.63191020
Iteration 3, loss = 0.53444546
Iteration 4, loss = 0.42449650
Iteration 5, loss = 0.32574156
Iteration 6, loss = 0.24719860
Iteration 7, loss = 0.18948798
Iteration 8, loss = 0.14876335
Iteration 9, loss = 0.12059459
Iteration 10, loss = 0.09986270
Iteration 11, loss = 0.08517715
Iteration 12, loss = 0.07421142
Iteration 13, loss = 0.06554514
Iteration 14, loss = 0.05898514
Iteration 15, loss = 0.05368312
Iteration 16, loss = 0.04960504
Iteration 17, loss = 0.04606346
Iteration 18, loss = 0.04292092
Iteration 19, loss = 0.04083374
Iteration 20, loss = 0.03893210
Iteration 21, loss = 0.03725485
Iteration 22, loss = 0.03562626
Iteration 23, loss = 0.03450195
Iteration 24, loss = 0.03331464
Iteration 25, loss = 0.03207368
Iteration 26, loss = 0.03134365
Iteration 27, loss = 0.03080225
Iteration 28, loss = 0.02981104
Iteration 29, loss = 0.02915288
Iteration 30, loss = 0.02868961
Iteration 31, loss = 0.02823025
Iteration 32, loss = 0.02768417
Iteration 33, loss = 0.02722621
Iteration 34, loss = 0.02685775
Iteration 35, loss = 0.02681555
Iteration 36, loss = 0.02623265
Iteration 37, loss = 0.02619942
Iteration 38, loss = 0.02608302
Iteration 39, loss = 0.02587993
Iteration 40, loss = 0.02544906
Iteration 41, loss = 0.02506435
Iteration 42, loss = 0.02488336
Iteration 43, loss = 0.02503088
Iteration 44, loss = 0.02497615
Iteration 45, loss = 0.02492801
Iteration 46, loss = 0.02458840
Iteration 47, loss = 0.02432290
Iteration 48, loss = 0.02433389
Iteration 49, loss = 0.02411236
Iteration 50, loss = 0.02405841
Iteration 51, loss = 0.02364987
Iteration 52, loss = 0.02391210
Iteration 53, loss = 0.02378879
Iteration 54, loss = 0.02365785
Iteration 55, loss = 0.02349748
Iteration 56, loss = 0.02351252
Iteration 57, loss = 0.02360792
Iteration 58, loss = 0.02338184
Iteration 59, loss = 0.02346981
Iteration 60, loss = 0.02333759
Iteration 61, loss = 0.02312980
Iteration 62, loss = 0.02312173
Iteration 63, loss = 0.02311454
Iteration 64, loss = 0.02314931
Iteration 65, loss = 0.02304569
Iteration 66, loss = 0.02294224
Iteration 67, loss = 0.02283814
Iteration 68, loss = 0.02283068
Iteration 69, loss = 0.02265333
Iteration 70, loss = 0.02288598
Iteration 71, loss = 0.02265558
Iteration 72, loss = 0.02269193
Iteration 73, loss = 0.02287961
Iteration 74, loss = 0.02284694
Iteration 75, loss = 0.02289986
Iteration 76, loss = 0.02239217
Iteration 77, loss = 0.02247664
Iteration 78, loss = 0.02256533
Iteration 79, loss = 0.02271003
Iteration 80, loss = 0.02255427
Iteration 81, loss = 0.02260911
Iteration 82, loss = 0.02255655
Iteration 83, loss = 0.02234391
Iteration 84, loss = 0.02238045
Iteration 85, loss = 0.02254451
Iteration 86, loss = 0.02236206
Iteration 87, loss = 0.02231837
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68034512
Iteration 2, loss = 0.62398972
Iteration 3, loss = 0.54212391
Iteration 4, loss = 0.45059883
Iteration 5, loss = 0.36522415
Iteration 6, loss = 0.29072752
Iteration 7, loss = 0.23088038
Iteration 8, loss = 0.18429920
Iteration 9, loss = 0.14950743
Iteration 10, loss = 0.12323554
Iteration 11, loss = 0.10387867
Iteration 12, loss = 0.08888011
Iteration 13, loss = 0.07739880
Iteration 14, loss = 0.06844677
Iteration 15, loss = 0.06122808
Iteration 16, loss = 0.05546380
Iteration 17, loss = 0.05077200
Iteration 18, loss = 0.04704401
Iteration 19, loss = 0.04359015
Iteration 20, loss = 0.04089207
Iteration 21, loss = 0.03868961
Iteration 22, loss = 0.03659046
Iteration 23, loss = 0.03487059
Iteration 24, loss = 0.03352354
Iteration 25, loss = 0.03217968
Iteration 26, loss = 0.03091129
Iteration 27, loss = 0.03007938
Iteration 28, loss = 0.02927753
Iteration 29, loss = 0.02851367
Iteration 30, loss = 0.02791240
Iteration 31, loss = 0.02703468
Iteration 32, loss = 0.02631916
Iteration 33, loss = 0.02605092
Iteration 34, loss = 0.02538075
Iteration 35, loss = 0.02487838
Iteration 36, loss = 0.02469725
Iteration 37, loss = 0.02424183
Iteration 38, loss = 0.02408306
Iteration 39, loss = 0.02340449
Iteration 40, loss = 0.02329353
Iteration 41, loss = 0.02329617
Iteration 42, loss = 0.02289400
Iteration 43, loss = 0.02241185
Iteration 44, loss = 0.02239940
Iteration 45, loss = 0.02207481
Iteration 46, loss = 0.02191736
Iteration 47, loss = 0.02164795
Iteration 48, loss = 0.02157347
Iteration 49, loss = 0.02143173
Iteration 50, loss = 0.02140945
Iteration 51, loss = 0.02125268
Iteration 52, loss = 0.02103905
Iteration 53, loss = 0.02104779
Iteration 54, loss = 0.02093799
Iteration 55, loss = 0.02071731
Iteration 56, loss = 0.02046166
Iteration 57, loss = 0.02049107
Iteration 58, loss = 0.02040944
Iteration 59, loss = 0.02020696
Iteration 60, loss = 0.02025069
Iteration 61, loss = 0.02012998
Iteration 62, loss = 0.02005045
Iteration 63, loss = 0.02013204
Iteration 64, loss = 0.01999493
Iteration 65, loss = 0.02005204
Iteration 66, loss = 0.01994017
Iteration 67, loss = 0.01991375
Iteration 68, loss = 0.01987407
Iteration 69, loss = 0.01963210
Iteration 70, loss = 0.01979833
Iteration 71, loss = 0.01987931
Iteration 72, loss = 0.01953963
Iteration 73, loss = 0.01966712
Iteration 74, loss = 0.01985053
Iteration 75, loss = 0.01949102
Iteration 76, loss = 0.01966545
Iteration 77, loss = 0.01962020
Iteration 78, loss = 0.01933230
Iteration 79, loss = 0.01940901
Iteration 80, loss = 0.01924360
Iteration 81, loss = 0.01958324
Iteration 82, loss = 0.01912735
Iteration 83, loss = 0.01917720
Iteration 84, loss = 0.01923723
Iteration 85, loss = 0.01907288
Iteration 86, loss = 0.01921465
Iteration 87, loss = 0.01924702
Iteration 88, loss = 0.01939057
Iteration 89, loss = 0.01902363
Iteration 90, loss = 0.01943290
Iteration 91, loss = 0.01881661
Iteration 92, loss = 0.01911087
Iteration 93, loss = 0.01901871
Iteration 94, loss = 0.01895614
Iteration 95, loss = 0.01907323
Iteration 96, loss = 0.01899438
Iteration 97, loss = 0.01884498
Iteration 98, loss = 0.01919724
Iteration 99, loss = 0.01897218
Iteration 100, loss = 0.01907459
Iteration 101, loss = 0.01894808
Iteration 102, loss = 0.01894787
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68837982
Iteration 2, loss = 0.63653558
Iteration 3, loss = 0.55874765
Iteration 4, loss = 0.46591961
Iteration 5, loss = 0.37669278
Iteration 6, loss = 0.29872123
Iteration 7, loss = 0.23579303
Iteration 8, loss = 0.18830521
Iteration 9, loss = 0.15249465
Iteration 10, loss = 0.12615763
Iteration 11, loss = 0.10619228
Iteration 12, loss = 0.09136519
Iteration 13, loss = 0.07975548
Iteration 14, loss = 0.07076797
Iteration 15, loss = 0.06363193
Iteration 16, loss = 0.05764866
Iteration 17, loss = 0.05314838
Iteration 18, loss = 0.04903147
Iteration 19, loss = 0.04598494
Iteration 20, loss = 0.04300680
Iteration 21, loss = 0.04062318
Iteration 22, loss = 0.03861373
Iteration 23, loss = 0.03696558
Iteration 24, loss = 0.03535898
Iteration 25, loss = 0.03403738
Iteration 26, loss = 0.03306124
Iteration 27, loss = 0.03171282
Iteration 28, loss = 0.03099627
Iteration 29, loss = 0.03022356
Iteration 30, loss = 0.02937998
Iteration 31, loss = 0.02877964
Iteration 32, loss = 0.02816880
Iteration 33, loss = 0.02759470
Iteration 34, loss = 0.02704173
Iteration 35, loss = 0.02654448
Iteration 36, loss = 0.02621981
Iteration 37, loss = 0.02586577
Iteration 38, loss = 0.02544532
Iteration 39, loss = 0.02512309
Iteration 40, loss = 0.02485013
Iteration 41, loss = 0.02446742
Iteration 42, loss = 0.02432260
Iteration 43, loss = 0.02409945
Iteration 44, loss = 0.02398222
Iteration 45, loss = 0.02370386
Iteration 46, loss = 0.02343433
Iteration 47, loss = 0.02335071
Iteration 48, loss = 0.02322551
Iteration 49, loss = 0.02304862
Iteration 50, loss = 0.02311083
Iteration 51, loss = 0.02309488
Iteration 52, loss = 0.02277419
Iteration 53, loss = 0.02253855
Iteration 54, loss = 0.02253829
Iteration 55, loss = 0.02238756
Iteration 56, loss = 0.02236236
Iteration 57, loss = 0.02187689
Iteration 58, loss = 0.02234773
Iteration 59, loss = 0.02185094
Iteration 60, loss = 0.02173431
Iteration 61, loss = 0.02180932
Iteration 62, loss = 0.02176718
Iteration 63, loss = 0.02154045
Iteration 64, loss = 0.02151311
Iteration 65, loss = 0.02161027
Iteration 66, loss = 0.02145770
Iteration 67, loss = 0.02154351
Iteration 68, loss = 0.02152218
Iteration 69, loss = 0.02131131
Iteration 70, loss = 0.02145469
Iteration 71, loss = 0.02114918
Iteration 72, loss = 0.02157053
Iteration 73, loss = 0.02103254
Iteration 74, loss = 0.02099559
Iteration 75, loss = 0.02116737
Iteration 76, loss = 0.02108757
Iteration 77, loss = 0.02124160
Iteration 78, loss = 0.02092245
Iteration 79, loss = 0.02110091
Iteration 80, loss = 0.02100494
Iteration 81, loss = 0.02097166
Iteration 82, loss = 0.02073431
Iteration 83, loss = 0.02070523
Iteration 84, loss = 0.02082554
Iteration 85, loss = 0.02101758
Iteration 86, loss = 0.02085129
Iteration 87, loss = 0.02071664
Iteration 88, loss = 0.02066387
Iteration 89, loss = 0.02067337
Iteration 90, loss = 0.02076854
Iteration 91, loss = 0.02085033
Iteration 92, loss = 0.02076404
Iteration 93, loss = 0.02050445
Iteration 94, loss = 0.02091522
Iteration 95, loss = 0.02111795
Iteration 96, loss = 0.02060459
Iteration 97, loss = 0.02063563
Iteration 98, loss = 0.02059664
Iteration 99, loss = 0.02063096
Iteration 100, loss = 0.02070038
Iteration 101, loss = 0.02058421
Iteration 102, loss = 0.02075349
Iteration 103, loss = 0.02052050
Iteration 104, loss = 0.02044619
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67802091
Iteration 2, loss = 0.63469707
Iteration 3, loss = 0.56784943
Iteration 4, loss = 0.48449204
Iteration 5, loss = 0.40182863
Iteration 6, loss = 0.32783594
Iteration 7, loss = 0.26534661
Iteration 8, loss = 0.21523546
Iteration 9, loss = 0.17616649
Iteration 10, loss = 0.14646023
Iteration 11, loss = 0.12335713
Iteration 12, loss = 0.10571601
Iteration 13, loss = 0.09189371
Iteration 14, loss = 0.08127639
Iteration 15, loss = 0.07259600
Iteration 16, loss = 0.06529758
Iteration 17, loss = 0.05966345
Iteration 18, loss = 0.05486855
Iteration 19, loss = 0.05083707
Iteration 20, loss = 0.04733461
Iteration 21, loss = 0.04476916
Iteration 22, loss = 0.04222490
Iteration 23, loss = 0.03982004
Iteration 24, loss = 0.03805260
Iteration 25, loss = 0.03653249
Iteration 26, loss = 0.03497121
Iteration 27, loss = 0.03380871
Iteration 28, loss = 0.03299185
Iteration 29, loss = 0.03188668
Iteration 30, loss = 0.03084070
Iteration 31, loss = 0.02992074
Iteration 32, loss = 0.02924526
Iteration 33, loss = 0.02868344
Iteration 34, loss = 0.02804514
Iteration 35, loss = 0.02748588
Iteration 36, loss = 0.02699695
Iteration 37, loss = 0.02653405
Iteration 38, loss = 0.02628520
Iteration 39, loss = 0.02594057
Iteration 40, loss = 0.02537422
Iteration 41, loss = 0.02514841
Iteration 42, loss = 0.02498107
Iteration 43, loss = 0.02428614
Iteration 44, loss = 0.02407464
Iteration 45, loss = 0.02394938
Iteration 46, loss = 0.02405159
Iteration 47, loss = 0.02360963
Iteration 48, loss = 0.02296374
Iteration 49, loss = 0.02294745
Iteration 50, loss = 0.02317944
Iteration 51, loss = 0.02284426
Iteration 52, loss = 0.02239052
Iteration 53, loss = 0.02228895
Iteration 54, loss = 0.02248285
Iteration 55, loss = 0.02224020
Iteration 56, loss = 0.02203495
Iteration 57, loss = 0.02185801
Iteration 58, loss = 0.02159476
Iteration 59, loss = 0.02152503
Iteration 60, loss = 0.02136208
Iteration 61, loss = 0.02157423
Iteration 62, loss = 0.02138827
Iteration 63, loss = 0.02124782
Iteration 64, loss = 0.02110280
Iteration 65, loss = 0.02130514
Iteration 66, loss = 0.02127191
Iteration 67, loss = 0.02097067
Iteration 68, loss = 0.02093252
Iteration 69, loss = 0.02084601
Iteration 70, loss = 0.02074294
Iteration 71, loss = 0.02064513
Iteration 72, loss = 0.02078918
Iteration 73, loss = 0.02059972
Iteration 74, loss = 0.02043323
Iteration 75, loss = 0.02050957
Iteration 76, loss = 0.02046089
Iteration 77, loss = 0.02048676
Iteration 78, loss = 0.02066888
Iteration 79, loss = 0.02074290
Iteration 80, loss = 0.02052769
Iteration 81, loss = 0.02007942
Iteration 82, loss = 0.02059965
Iteration 83, loss = 0.02039400
Iteration 84, loss = 0.02021586
Iteration 85, loss = 0.02013188
Iteration 86, loss = 0.02006349
Iteration 87, loss = 0.02039244
Iteration 88, loss = 0.02004744
Iteration 89, loss = 0.01997779
Iteration 90, loss = 0.01988293
Iteration 91, loss = 0.01971913
Iteration 92, loss = 0.01980547
Iteration 93, loss = 0.01963024
Iteration 94, loss = 0.01997659
Iteration 95, loss = 0.01985370
Iteration 96, loss = 0.02000825
Iteration 97, loss = 0.02002596
Iteration 98, loss = 0.01979314
Iteration 99, loss = 0.01988775
Iteration 100, loss = 0.01977547
Iteration 101, loss = 0.01977611
Iteration 102, loss = 0.01953356
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68067449
Iteration 2, loss = 0.61324118
Iteration 3, loss = 0.51826139
Iteration 4, loss = 0.41660454
Iteration 5, loss = 0.32543955
Iteration 6, loss = 0.25150698
Iteration 7, loss = 0.19561613
Iteration 8, loss = 0.15536152
Iteration 9, loss = 0.12612673
Iteration 10, loss = 0.10513882
Iteration 11, loss = 0.08922770
Iteration 12, loss = 0.07747651
Iteration 13, loss = 0.06819923
Iteration 14, loss = 0.06119740
Iteration 15, loss = 0.05543088
Iteration 16, loss = 0.05091952
Iteration 17, loss = 0.04721371
Iteration 18, loss = 0.04397950
Iteration 19, loss = 0.04121601
Iteration 20, loss = 0.03897863
Iteration 21, loss = 0.03712355
Iteration 22, loss = 0.03555385
Iteration 23, loss = 0.03403605
Iteration 24, loss = 0.03298224
Iteration 25, loss = 0.03161016
Iteration 26, loss = 0.03092209
Iteration 27, loss = 0.02983852
Iteration 28, loss = 0.02915618
Iteration 29, loss = 0.02851961
Iteration 30, loss = 0.02782924
Iteration 31, loss = 0.02714425
Iteration 32, loss = 0.02672739
Iteration 33, loss = 0.02621470
Iteration 34, loss = 0.02612750
Iteration 35, loss = 0.02543097
Iteration 36, loss = 0.02515248
Iteration 37, loss = 0.02496568
Iteration 38, loss = 0.02441310
Iteration 39, loss = 0.02395063
Iteration 40, loss = 0.02383664
Iteration 41, loss = 0.02372476
Iteration 42, loss = 0.02383834
Iteration 43, loss = 0.02327590
Iteration 44, loss = 0.02334791
Iteration 45, loss = 0.02284730
Iteration 46, loss = 0.02278248
Iteration 47, loss = 0.02262617
Iteration 48, loss = 0.02259605
Iteration 49, loss = 0.02313301
Iteration 50, loss = 0.02290764
Iteration 51, loss = 0.02252136
Iteration 52, loss = 0.02269715
Iteration 53, loss = 0.02235610
Iteration 54, loss = 0.02206857
Iteration 55, loss = 0.02203208
Iteration 56, loss = 0.02164824
Iteration 57, loss = 0.02182804
Iteration 58, loss = 0.02167045
Iteration 59, loss = 0.02159712
Iteration 60, loss = 0.02120640
Iteration 61, loss = 0.02188154
Iteration 62, loss = 0.02147197
Iteration 63, loss = 0.02097440
Iteration 64, loss = 0.02140265
Iteration 65, loss = 0.02118350
Iteration 66, loss = 0.02092429
Iteration 67, loss = 0.02069492
Iteration 68, loss = 0.02060664
Iteration 69, loss = 0.02070012
Iteration 70, loss = 0.02063263
Iteration 71, loss = 0.02059923
Iteration 72, loss = 0.02057010
Iteration 73, loss = 0.02042300
Iteration 74, loss = 0.02041269
Iteration 75, loss = 0.02037503
Iteration 76, loss = 0.02028022
Iteration 77, loss = 0.02033566
Iteration 78, loss = 0.02222217
Iteration 79, loss = 0.02228287
Iteration 80, loss = 0.02139290
Iteration 81, loss = 0.02157473
Iteration 82, loss = 0.02123588
Iteration 83, loss = 0.02083912
Iteration 84, loss = 0.02096728
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67310491
Iteration 2, loss = 0.59843781
Iteration 3, loss = 0.49462804
Iteration 4, loss = 0.38823724
Iteration 5, loss = 0.29715007
Iteration 6, loss = 0.22573042
Iteration 7, loss = 0.17455324
Iteration 8, loss = 0.13837317
Iteration 9, loss = 0.11290655
Iteration 10, loss = 0.09453465
Iteration 11, loss = 0.08110167
Iteration 12, loss = 0.07110285
Iteration 13, loss = 0.06321025
Iteration 14, loss = 0.05740605
Iteration 15, loss = 0.05263691
Iteration 16, loss = 0.04858765
Iteration 17, loss = 0.04523883
Iteration 18, loss = 0.04253457
Iteration 19, loss = 0.04048719
Iteration 20, loss = 0.03839953
Iteration 21, loss = 0.03714973
Iteration 22, loss = 0.03583532
Iteration 23, loss = 0.03453481
Iteration 24, loss = 0.03332779
Iteration 25, loss = 0.03227171
Iteration 26, loss = 0.03178392
Iteration 27, loss = 0.03086153
Iteration 28, loss = 0.03040386
Iteration 29, loss = 0.02978685
Iteration 30, loss = 0.02947353
Iteration 31, loss = 0.02904028
Iteration 32, loss = 0.02837079
Iteration 33, loss = 0.02783269
Iteration 34, loss = 0.02767780
Iteration 35, loss = 0.02748035
Iteration 36, loss = 0.02703047
Iteration 37, loss = 0.02691664
Iteration 38, loss = 0.02665614
Iteration 39, loss = 0.02648411
Iteration 40, loss = 0.02635062
Iteration 41, loss = 0.02592113
Iteration 42, loss = 0.02591079
Iteration 43, loss = 0.02569802
Iteration 44, loss = 0.02545078
Iteration 45, loss = 0.02527285
Iteration 46, loss = 0.02546554
Iteration 47, loss = 0.02507604
Iteration 48, loss = 0.02481408
Iteration 49, loss = 0.02497528
Iteration 50, loss = 0.02494345
Iteration 51, loss = 0.02489283
Iteration 52, loss = 0.02467486
Iteration 53, loss = 0.02456322
Iteration 54, loss = 0.02440057
Iteration 55, loss = 0.02446742
Iteration 56, loss = 0.02440620
Iteration 57, loss = 0.02433610
Iteration 58, loss = 0.02417181
Iteration 59, loss = 0.02418157
Iteration 60, loss = 0.02411898
Iteration 61, loss = 0.02441666
Iteration 62, loss = 0.02420927
Iteration 63, loss = 0.02425039
Iteration 64, loss = 0.02387870
Iteration 65, loss = 0.02420108
Iteration 66, loss = 0.02424229
Iteration 67, loss = 0.02373626
Iteration 68, loss = 0.02379960
Iteration 69, loss = 0.02361050
Iteration 70, loss = 0.02373087
Iteration 71, loss = 0.02327479
Iteration 72, loss = 0.02352592
Iteration 73, loss = 0.02336843
Iteration 74, loss = 0.02359752
Iteration 75, loss = 0.02337389
Iteration 76, loss = 0.02368043
Iteration 77, loss = 0.02338643
Iteration 78, loss = 0.02363601
Iteration 79, loss = 0.02372517
Iteration 80, loss = 0.02345590
Iteration 81, loss = 0.02335993
Iteration 82, loss = 0.02349599
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67693309
Iteration 2, loss = 0.66140177
Iteration 3, loss = 0.64829421
Iteration 4, loss = 0.63511152
Iteration 5, loss = 0.62114120
Iteration 6, loss = 0.60594319
Iteration 7, loss = 0.58947439
Iteration 8, loss = 0.57168363
Iteration 9, loss = 0.55231869
Iteration 10, loss = 0.53129789
Iteration 11, loss = 0.50896332
Iteration 12, loss = 0.48570082
Iteration 13, loss = 0.46142898
Iteration 14, loss = 0.43684544
Iteration 15, loss = 0.41214937
Iteration 16, loss = 0.38773782
Iteration 17, loss = 0.36400997
Iteration 18, loss = 0.34109147
Iteration 19, loss = 0.31914076
Iteration 20, loss = 0.29822867
Iteration 21, loss = 0.27864105
Iteration 22, loss = 0.26017633
Iteration 23, loss = 0.24274806
Iteration 24, loss = 0.22666928
Iteration 25, loss = 0.21182333
Iteration 26, loss = 0.19781282
Iteration 27, loss = 0.18498274
Iteration 28, loss = 0.17309404
Iteration 29, loss = 0.16222145
Iteration 30, loss = 0.15221742
Iteration 31, loss = 0.14286707
Iteration 32, loss = 0.13437857
Iteration 33, loss = 0.12656990
Iteration 34, loss = 0.11926182
Iteration 35, loss = 0.11264009
Iteration 36, loss = 0.10650564
Iteration 37, loss = 0.10088846
Iteration 38, loss = 0.09561003
Iteration 39, loss = 0.09085033
Iteration 40, loss = 0.08633918
Iteration 41, loss = 0.08223037
Iteration 42, loss = 0.07838199
Iteration 43, loss = 0.07480981
Iteration 44, loss = 0.07147968
Iteration 45, loss = 0.06840682
Iteration 46, loss = 0.06555014
Iteration 47, loss = 0.06284361
Iteration 48, loss = 0.06035617
Iteration 49, loss = 0.05804247
Iteration 50, loss = 0.05589049
Iteration 51, loss = 0.05380173
Iteration 52, loss = 0.05191645
Iteration 53, loss = 0.05007556
Iteration 54, loss = 0.04842838
Iteration 55, loss = 0.04686029
Iteration 56, loss = 0.04531780
Iteration 57, loss = 0.04389548
Iteration 58, loss = 0.04257163
Iteration 59, loss = 0.04126329
Iteration 60, loss = 0.04005201
Iteration 61, loss = 0.03893935
Iteration 62, loss = 0.03787731
Iteration 63, loss = 0.03681376
Iteration 64, loss = 0.03583682
Iteration 65, loss = 0.03494216
Iteration 66, loss = 0.03411670
Iteration 67, loss = 0.03326921
Iteration 68, loss = 0.03243540
Iteration 69, loss = 0.03169366
Iteration 70, loss = 0.03096311
Iteration 71, loss = 0.03023741
Iteration 72, loss = 0.02958010
Iteration 73, loss = 0.02891159
Iteration 74, loss = 0.02829124
Iteration 75, loss = 0.02771658
Iteration 76, loss = 0.02714892
Iteration 77, loss = 0.02659566
Iteration 78, loss = 0.02611807
Iteration 79, loss = 0.02556116
Iteration 80, loss = 0.02507298
Iteration 81, loss = 0.02459320
Iteration 82, loss = 0.02414605
Iteration 83, loss = 0.02371761
Iteration 84, loss = 0.02331174
Iteration 85, loss = 0.02290026
Iteration 86, loss = 0.02249091
Iteration 87, loss = 0.02211659
Iteration 88, loss = 0.02175687
Iteration 89, loss = 0.02142670
Iteration 90, loss = 0.02109585
Iteration 91, loss = 0.02077828
Iteration 92, loss = 0.02046403
Iteration 93, loss = 0.02019579
Iteration 94, loss = 0.01988262
Iteration 95, loss = 0.01958711
Iteration 96, loss = 0.01936977
Iteration 97, loss = 0.01909038
Iteration 98, loss = 0.01885886
Iteration 99, loss = 0.01857299
Iteration 100, loss = 0.01833090
Iteration 101, loss = 0.01810171
Iteration 102, loss = 0.01789662
Iteration 103, loss = 0.01766096
Iteration 104, loss = 0.01745302
Iteration 105, loss = 0.01722951
Iteration 106, loss = 0.01703779
Iteration 107, loss = 0.01679427
Iteration 108, loss = 0.01662505
Iteration 109, loss = 0.01644369
Iteration 110, loss = 0.01624605
Iteration 111, loss = 0.01607224
Iteration 112, loss = 0.01593539
Iteration 113, loss = 0.01574223
Iteration 114, loss = 0.01560397
Iteration 115, loss = 0.01542534
Iteration 116, loss = 0.01530892
Iteration 117, loss = 0.01511406
Iteration 118, loss = 0.01497849
Iteration 119, loss = 0.01484404
Iteration 120, loss = 0.01471613
Iteration 121, loss = 0.01457378
Iteration 122, loss = 0.01444954
Iteration 123, loss = 0.01434743
Iteration 124, loss = 0.01423695
Iteration 125, loss = 0.01410924
Iteration 126, loss = 0.01405914
Iteration 127, loss = 0.01392071
Iteration 128, loss = 0.01382588
Iteration 129, loss = 0.01372912
Iteration 130, loss = 0.01361207
Iteration 131, loss = 0.01353575
Iteration 132, loss = 0.01338641
Iteration 133, loss = 0.01330984
Iteration 134, loss = 0.01316210
Iteration 135, loss = 0.01307390
Iteration 136, loss = 0.01297118
Iteration 137, loss = 0.01284216
Iteration 138, loss = 0.01274739
Iteration 139, loss = 0.01267249
Iteration 140, loss = 0.01258825
Iteration 141, loss = 0.01250699
Iteration 142, loss = 0.01239880
Iteration 143, loss = 0.01231903
Iteration 144, loss = 0.01224415
Iteration 145, loss = 0.01216294
Iteration 146, loss = 0.01210208
Iteration 147, loss = 0.01200788
Iteration 148, loss = 0.01193214
Iteration 149, loss = 0.01188588
Iteration 150, loss = 0.01178413
Iteration 151, loss = 0.01171599
Iteration 152, loss = 0.01169245
Iteration 153, loss = 0.01157879
Iteration 154, loss = 0.01151547
Iteration 155, loss = 0.01151643
Iteration 156, loss = 0.01141115
Iteration 157, loss = 0.01134165
Iteration 158, loss = 0.01129824
Iteration 159, loss = 0.01121173
Iteration 160, loss = 0.01118211
Iteration 161, loss = 0.01110489
Iteration 162, loss = 0.01105938
Iteration 163, loss = 0.01099196
Iteration 164, loss = 0.01095670
Iteration 165, loss = 0.01087409
Iteration 166, loss = 0.01086092
Iteration 167, loss = 0.01079598
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.66817575
Iteration 2, loss = 0.65354300
Iteration 3, loss = 0.64087885
Iteration 4, loss = 0.62817017
Iteration 5, loss = 0.61492747
Iteration 6, loss = 0.60089331
Iteration 7, loss = 0.58581545
Iteration 8, loss = 0.56943384
Iteration 9, loss = 0.55186186
Iteration 10, loss = 0.53313480
Iteration 11, loss = 0.51336922
Iteration 12, loss = 0.49266134
Iteration 13, loss = 0.47132954
Iteration 14, loss = 0.44954207
Iteration 15, loss = 0.42776743
Iteration 16, loss = 0.40589345
Iteration 17, loss = 0.38437890
Iteration 18, loss = 0.36329262
Iteration 19, loss = 0.34290321
Iteration 20, loss = 0.32311721
Iteration 21, loss = 0.30431022
Iteration 22, loss = 0.28602377
Iteration 23, loss = 0.26884659
Iteration 24, loss = 0.25243241
Iteration 25, loss = 0.23706364
Iteration 26, loss = 0.22250206
Iteration 27, loss = 0.20882939
Iteration 28, loss = 0.19613893
Iteration 29, loss = 0.18419591
Iteration 30, loss = 0.17316851
Iteration 31, loss = 0.16274889
Iteration 32, loss = 0.15302919
Iteration 33, loss = 0.14411569
Iteration 34, loss = 0.13576240
Iteration 35, loss = 0.12803861
Iteration 36, loss = 0.12092317
Iteration 37, loss = 0.11431921
Iteration 38, loss = 0.10817910
Iteration 39, loss = 0.10245773
Iteration 40, loss = 0.09716048
Iteration 41, loss = 0.09223905
Iteration 42, loss = 0.08768806
Iteration 43, loss = 0.08344281
Iteration 44, loss = 0.07950334
Iteration 45, loss = 0.07583934
Iteration 46, loss = 0.07240743
Iteration 47, loss = 0.06926643
Iteration 48, loss = 0.06632243
Iteration 49, loss = 0.06345799
Iteration 50, loss = 0.06083220
Iteration 51, loss = 0.05840485
Iteration 52, loss = 0.05606121
Iteration 53, loss = 0.05390675
Iteration 54, loss = 0.05185376
Iteration 55, loss = 0.04997933
Iteration 56, loss = 0.04820525
Iteration 57, loss = 0.04645931
Iteration 58, loss = 0.04487509
Iteration 59, loss = 0.04335046
Iteration 60, loss = 0.04195552
Iteration 61, loss = 0.04060622
Iteration 62, loss = 0.03933768
Iteration 63, loss = 0.03808735
Iteration 64, loss = 0.03697922
Iteration 65, loss = 0.03588545
Iteration 66, loss = 0.03481782
Iteration 67, loss = 0.03387097
Iteration 68, loss = 0.03289808
Iteration 69, loss = 0.03201059
Iteration 70, loss = 0.03112964
Iteration 71, loss = 0.03035284
Iteration 72, loss = 0.02951071
Iteration 73, loss = 0.02876173
Iteration 74, loss = 0.02804550
Iteration 75, loss = 0.02734808
Iteration 76, loss = 0.02671580
Iteration 77, loss = 0.02607677
Iteration 78, loss = 0.02546577
Iteration 79, loss = 0.02487332
Iteration 80, loss = 0.02434885
Iteration 81, loss = 0.02382787
Iteration 82, loss = 0.02332251
Iteration 83, loss = 0.02283693
Iteration 84, loss = 0.02237455
Iteration 85, loss = 0.02192528
Iteration 86, loss = 0.02148479
Iteration 87, loss = 0.02109666
Iteration 88, loss = 0.02068536
Iteration 89, loss = 0.02026131
Iteration 90, loss = 0.01987035
Iteration 91, loss = 0.01954095
Iteration 92, loss = 0.01914098
Iteration 93, loss = 0.01882351
Iteration 94, loss = 0.01845741
Iteration 95, loss = 0.01817630
Iteration 96, loss = 0.01782864
Iteration 97, loss = 0.01753339
Iteration 98, loss = 0.01724837
Iteration 99, loss = 0.01697925
Iteration 100, loss = 0.01673968
Iteration 101, loss = 0.01646647
Iteration 102, loss = 0.01623972
Iteration 103, loss = 0.01598160
Iteration 104, loss = 0.01576290
Iteration 105, loss = 0.01554500
Iteration 106, loss = 0.01530527
Iteration 107, loss = 0.01508081
Iteration 108, loss = 0.01486846
Iteration 109, loss = 0.01465012
Iteration 110, loss = 0.01445511
Iteration 111, loss = 0.01425789
Iteration 112, loss = 0.01407856
Iteration 113, loss = 0.01388018
Iteration 114, loss = 0.01371516
Iteration 115, loss = 0.01354148
Iteration 116, loss = 0.01336526
Iteration 117, loss = 0.01319810
Iteration 118, loss = 0.01303090
Iteration 119, loss = 0.01287176
Iteration 120, loss = 0.01274118
Iteration 121, loss = 0.01254943
Iteration 122, loss = 0.01238925
Iteration 123, loss = 0.01223567
Iteration 124, loss = 0.01211598
Iteration 125, loss = 0.01196738
Iteration 126, loss = 0.01183234
Iteration 127, loss = 0.01170724
Iteration 128, loss = 0.01159833
Iteration 129, loss = 0.01144679
Iteration 130, loss = 0.01133062
Iteration 131, loss = 0.01120937
Iteration 132, loss = 0.01109925
Iteration 133, loss = 0.01098836
Iteration 134, loss = 0.01087155
Iteration 135, loss = 0.01076899
Iteration 136, loss = 0.01067073
Iteration 137, loss = 0.01057061
Iteration 138, loss = 0.01048132
Iteration 139, loss = 0.01036535
Iteration 140, loss = 0.01027368
Iteration 141, loss = 0.01018008
Iteration 142, loss = 0.01009720
Iteration 143, loss = 0.01001380
Iteration 144, loss = 0.00993153
Iteration 145, loss = 0.00984468
Iteration 146, loss = 0.00974844
Iteration 147, loss = 0.00967293
Iteration 148, loss = 0.00959875
Iteration 149, loss = 0.00953109
Iteration 150, loss = 0.00947572
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68017172
Iteration 2, loss = 0.65066727
Iteration 3, loss = 0.61031132
Iteration 4, loss = 0.55750348
Iteration 5, loss = 0.49968934
Iteration 6, loss = 0.44175534
Iteration 7, loss = 0.38588629
Iteration 8, loss = 0.33369342
Iteration 9, loss = 0.28667617
Iteration 10, loss = 0.24530266
Iteration 11, loss = 0.20985002
Iteration 12, loss = 0.18000042
Iteration 13, loss = 0.15527188
Iteration 14, loss = 0.13451921
Iteration 15, loss = 0.11752935
Iteration 16, loss = 0.10356964
Iteration 17, loss = 0.09194146
Iteration 18, loss = 0.08227225
Iteration 19, loss = 0.07416337
Iteration 20, loss = 0.06739106
Iteration 21, loss = 0.06154422
Iteration 22, loss = 0.05662775
Iteration 23, loss = 0.05232132
Iteration 24, loss = 0.04873584
Iteration 25, loss = 0.04546878
Iteration 26, loss = 0.04276225
Iteration 27, loss = 0.04015299
Iteration 28, loss = 0.03803202
Iteration 29, loss = 0.03608538
Iteration 30, loss = 0.03434655
Iteration 31, loss = 0.03274016
Iteration 32, loss = 0.03145387
Iteration 33, loss = 0.03014492
Iteration 34, loss = 0.02916723
Iteration 35, loss = 0.02801880
Iteration 36, loss = 0.02709623
Iteration 37, loss = 0.02624635
Iteration 38, loss = 0.02542234
Iteration 39, loss = 0.02483702
Iteration 40, loss = 0.02410967
Iteration 41, loss = 0.02354530
Iteration 42, loss = 0.02297310
Iteration 43, loss = 0.02238349
Iteration 44, loss = 0.02194158
Iteration 45, loss = 0.02159188
Iteration 46, loss = 0.02124647
Iteration 47, loss = 0.02074142
Iteration 48, loss = 0.02025284
Iteration 49, loss = 0.01990001
Iteration 50, loss = 0.01965808
Iteration 51, loss = 0.01947380
Iteration 52, loss = 0.01914815
Iteration 53, loss = 0.01881612
Iteration 54, loss = 0.01853189
Iteration 55, loss = 0.01844468
Iteration 56, loss = 0.01797502
Iteration 57, loss = 0.01797394
Iteration 58, loss = 0.01776818
Iteration 59, loss = 0.01757455
Iteration 60, loss = 0.01737106
Iteration 61, loss = 0.01719908
Iteration 62, loss = 0.01703858
Iteration 63, loss = 0.01696456
Iteration 64, loss = 0.01683648
Iteration 65, loss = 0.01664910
Iteration 66, loss = 0.01652529
Iteration 67, loss = 0.01641638
Iteration 68, loss = 0.01627004
Iteration 69, loss = 0.01621102
Iteration 70, loss = 0.01605205
Iteration 71, loss = 0.01606812
Iteration 72, loss = 0.01588912
Iteration 73, loss = 0.01577332
Iteration 74, loss = 0.01577714
Iteration 75, loss = 0.01558241
Iteration 76, loss = 0.01552907
Iteration 77, loss = 0.01536254
Iteration 78, loss = 0.01545770
Iteration 79, loss = 0.01520389
Iteration 80, loss = 0.01519352
Iteration 81, loss = 0.01526151
Iteration 82, loss = 0.01509287
Iteration 83, loss = 0.01508288
Iteration 84, loss = 0.01493626
Iteration 85, loss = 0.01497490
Iteration 86, loss = 0.01479073
Iteration 87, loss = 0.01482398
Iteration 88, loss = 0.01477248
Iteration 89, loss = 0.01468551
Iteration 90, loss = 0.01469391
Iteration 91, loss = 0.01459191
Iteration 92, loss = 0.01467145
Iteration 93, loss = 0.01454519
Iteration 94, loss = 0.01446667
Iteration 95, loss = 0.01439335
Iteration 96, loss = 0.01435695
Iteration 97, loss = 0.01446100
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70898942
Iteration 2, loss = 0.65959901
Iteration 3, loss = 0.59945775
Iteration 4, loss = 0.53104642
Iteration 5, loss = 0.45905420
Iteration 6, loss = 0.38996832
Iteration 7, loss = 0.32703241
Iteration 8, loss = 0.27279762
Iteration 9, loss = 0.22727591
Iteration 10, loss = 0.18999047
Iteration 11, loss = 0.16000566
Iteration 12, loss = 0.13605195
Iteration 13, loss = 0.11708046
Iteration 14, loss = 0.10168685
Iteration 15, loss = 0.08938267
Iteration 16, loss = 0.07945767
Iteration 17, loss = 0.07106849
Iteration 18, loss = 0.06426929
Iteration 19, loss = 0.05855469
Iteration 20, loss = 0.05376040
Iteration 21, loss = 0.04974053
Iteration 22, loss = 0.04613027
Iteration 23, loss = 0.04316667
Iteration 24, loss = 0.04037404
Iteration 25, loss = 0.03813322
Iteration 26, loss = 0.03622313
Iteration 27, loss = 0.03436652
Iteration 28, loss = 0.03273994
Iteration 29, loss = 0.03145904
Iteration 30, loss = 0.03010880
Iteration 31, loss = 0.02903872
Iteration 32, loss = 0.02797714
Iteration 33, loss = 0.02709032
Iteration 34, loss = 0.02620352
Iteration 35, loss = 0.02556078
Iteration 36, loss = 0.02476788
Iteration 37, loss = 0.02415096
Iteration 38, loss = 0.02355335
Iteration 39, loss = 0.02299906
Iteration 40, loss = 0.02248751
Iteration 41, loss = 0.02203739
Iteration 42, loss = 0.02167873
Iteration 43, loss = 0.02113497
Iteration 44, loss = 0.02086107
Iteration 45, loss = 0.02057135
Iteration 46, loss = 0.02017079
Iteration 47, loss = 0.01986134
Iteration 48, loss = 0.01976438
Iteration 49, loss = 0.01945552
Iteration 50, loss = 0.01918140
Iteration 51, loss = 0.01897697
Iteration 52, loss = 0.01873733
Iteration 53, loss = 0.01858339
Iteration 54, loss = 0.01839074
Iteration 55, loss = 0.01832206
Iteration 56, loss = 0.01792554
Iteration 57, loss = 0.01781762
Iteration 58, loss = 0.01761797
Iteration 59, loss = 0.01756636
Iteration 60, loss = 0.01738884
Iteration 61, loss = 0.01740970
Iteration 62, loss = 0.01713866
Iteration 63, loss = 0.01705867
Iteration 64, loss = 0.01704690
Iteration 65, loss = 0.01701443
Iteration 66, loss = 0.01680843
Iteration 67, loss = 0.01669865
Iteration 68, loss = 0.01662548
Iteration 69, loss = 0.01653899
Iteration 70, loss = 0.01639891
Iteration 71, loss = 0.01619388
Iteration 72, loss = 0.01623704
Iteration 73, loss = 0.01639388
Iteration 74, loss = 0.01616327
Iteration 75, loss = 0.01599059
Iteration 76, loss = 0.01601340
Iteration 77, loss = 0.01602504
Iteration 78, loss = 0.01580316
Iteration 79, loss = 0.01594189
Iteration 80, loss = 0.01577301
Iteration 81, loss = 0.01579379
Iteration 82, loss = 0.01569440
Iteration 83, loss = 0.01574745
Iteration 84, loss = 0.01558015
Iteration 85, loss = 0.01548875
Iteration 86, loss = 0.01548776
Iteration 87, loss = 0.01537455
Iteration 88, loss = 0.01547608
Iteration 89, loss = 0.01537772
Iteration 90, loss = 0.01556274
Iteration 91, loss = 0.01539763
Iteration 92, loss = 0.01531436
Iteration 93, loss = 0.01537334
Iteration 94, loss = 0.01523661
Iteration 95, loss = 0.01512048
Iteration 96, loss = 0.01521438
Iteration 97, loss = 0.01512889
Iteration 98, loss = 0.01524488
Iteration 99, loss = 0.01507466
Iteration 100, loss = 0.01501804
Iteration 101, loss = 0.01507184
Iteration 102, loss = 0.01502850
Iteration 103, loss = 0.01508686
Iteration 104, loss = 0.01500321
Iteration 105, loss = 0.01498301
Iteration 106, loss = 0.01495466
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69358365
Iteration 2, loss = 0.67360478
Iteration 3, loss = 0.65707133
Iteration 4, loss = 0.64138783
Iteration 5, loss = 0.62527512
Iteration 6, loss = 0.60841914
Iteration 7, loss = 0.59034016
Iteration 8, loss = 0.57116845
Iteration 9, loss = 0.55083433
Iteration 10, loss = 0.52967760
Iteration 11, loss = 0.50769481
Iteration 12, loss = 0.48510446
Iteration 13, loss = 0.46206495
Iteration 14, loss = 0.43882712
Iteration 15, loss = 0.41578616
Iteration 16, loss = 0.39295846
Iteration 17, loss = 0.37072294
Iteration 18, loss = 0.34895498
Iteration 19, loss = 0.32803600
Iteration 20, loss = 0.30802139
Iteration 21, loss = 0.28884853
Iteration 22, loss = 0.27072272
Iteration 23, loss = 0.25349994
Iteration 24, loss = 0.23727902
Iteration 25, loss = 0.22206019
Iteration 26, loss = 0.20787720
Iteration 27, loss = 0.19452307
Iteration 28, loss = 0.18215909
Iteration 29, loss = 0.17058694
Iteration 30, loss = 0.15994286
Iteration 31, loss = 0.14997141
Iteration 32, loss = 0.14084045
Iteration 33, loss = 0.13236164
Iteration 34, loss = 0.12455936
Iteration 35, loss = 0.11728237
Iteration 36, loss = 0.11053349
Iteration 37, loss = 0.10436667
Iteration 38, loss = 0.09865207
Iteration 39, loss = 0.09331797
Iteration 40, loss = 0.08837023
Iteration 41, loss = 0.08383111
Iteration 42, loss = 0.07955628
Iteration 43, loss = 0.07562597
Iteration 44, loss = 0.07195937
Iteration 45, loss = 0.06854876
Iteration 46, loss = 0.06534749
Iteration 47, loss = 0.06240022
Iteration 48, loss = 0.05961737
Iteration 49, loss = 0.05704007
Iteration 50, loss = 0.05459270
Iteration 51, loss = 0.05232054
Iteration 52, loss = 0.05019538
Iteration 53, loss = 0.04818655
Iteration 54, loss = 0.04628808
Iteration 55, loss = 0.04450896
Iteration 56, loss = 0.04281556
Iteration 57, loss = 0.04123073
Iteration 58, loss = 0.03973149
Iteration 59, loss = 0.03831525
Iteration 60, loss = 0.03699498
Iteration 61, loss = 0.03573942
Iteration 62, loss = 0.03454979
Iteration 63, loss = 0.03339146
Iteration 64, loss = 0.03232146
Iteration 65, loss = 0.03129218
Iteration 66, loss = 0.03028202
Iteration 67, loss = 0.02935105
Iteration 68, loss = 0.02845618
Iteration 69, loss = 0.02761145
Iteration 70, loss = 0.02676750
Iteration 71, loss = 0.02599210
Iteration 72, loss = 0.02524801
Iteration 73, loss = 0.02455414
Iteration 74, loss = 0.02388951
Iteration 75, loss = 0.02326106
Iteration 76, loss = 0.02263435
Iteration 77, loss = 0.02205360
Iteration 78, loss = 0.02147727
Iteration 79, loss = 0.02092975
Iteration 80, loss = 0.02039944
Iteration 81, loss = 0.01988282
Iteration 82, loss = 0.01940251
Iteration 83, loss = 0.01892740
Iteration 84, loss = 0.01847529
Iteration 85, loss = 0.01803080
Iteration 86, loss = 0.01761784
Iteration 87, loss = 0.01722186
Iteration 88, loss = 0.01683105
Iteration 89, loss = 0.01645199
Iteration 90, loss = 0.01609232
Iteration 91, loss = 0.01574707
Iteration 92, loss = 0.01540475
Iteration 93, loss = 0.01507815
Iteration 94, loss = 0.01478381
Iteration 95, loss = 0.01446199
Iteration 96, loss = 0.01417374
Iteration 97, loss = 0.01388689
Iteration 98, loss = 0.01360279
Iteration 99, loss = 0.01335155
Iteration 100, loss = 0.01308570
Iteration 101, loss = 0.01283738
Iteration 102, loss = 0.01259455
Iteration 103, loss = 0.01236481
Iteration 104, loss = 0.01213275
Iteration 105, loss = 0.01191953
Iteration 106, loss = 0.01169031
Iteration 107, loss = 0.01147514
Iteration 108, loss = 0.01127355
Iteration 109, loss = 0.01108197
Iteration 110, loss = 0.01088644
Iteration 111, loss = 0.01070659
Iteration 112, loss = 0.01052226
Iteration 113, loss = 0.01034655
Iteration 114, loss = 0.01017717
Iteration 115, loss = 0.01001238
Iteration 116, loss = 0.00985398
Iteration 117, loss = 0.00969706
Iteration 118, loss = 0.00954889
Iteration 119, loss = 0.00939567
Iteration 120, loss = 0.00925264
Iteration 121, loss = 0.00911033
Iteration 122, loss = 0.00897901
Iteration 123, loss = 0.00884513
Iteration 124, loss = 0.00870832
Iteration 125, loss = 0.00858242
Iteration 126, loss = 0.00846357
Iteration 127, loss = 0.00833636
Iteration 128, loss = 0.00821978
Iteration 129, loss = 0.00810326
Iteration 130, loss = 0.00799065
Iteration 131, loss = 0.00788511
Iteration 132, loss = 0.00777550
Iteration 133, loss = 0.00767371
Iteration 134, loss = 0.00757198
Iteration 135, loss = 0.00746670
Iteration 136, loss = 0.00736984
Iteration 137, loss = 0.00727234
Iteration 138, loss = 0.00718153
Iteration 139, loss = 0.00709119
Iteration 140, loss = 0.00699902
Iteration 141, loss = 0.00690828
Iteration 142, loss = 0.00682755
Iteration 143, loss = 0.00674082
Iteration 144, loss = 0.00666262
Iteration 145, loss = 0.00657903
Iteration 146, loss = 0.00650271
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70193991
Iteration 2, loss = 0.63246323
Iteration 3, loss = 0.53428001
Iteration 4, loss = 0.42047272
Iteration 5, loss = 0.31918091
Iteration 6, loss = 0.23982595
Iteration 7, loss = 0.18247549
Iteration 8, loss = 0.14270791
Iteration 9, loss = 0.11471039
Iteration 10, loss = 0.09474629
Iteration 11, loss = 0.08029353
Iteration 12, loss = 0.06968014
Iteration 13, loss = 0.06130384
Iteration 14, loss = 0.05506195
Iteration 15, loss = 0.05014235
Iteration 16, loss = 0.04590023
Iteration 17, loss = 0.04244856
Iteration 18, loss = 0.03980458
Iteration 19, loss = 0.03722677
Iteration 20, loss = 0.03588854
Iteration 21, loss = 0.03351392
Iteration 22, loss = 0.03228069
Iteration 23, loss = 0.03105814
Iteration 24, loss = 0.02979836
Iteration 25, loss = 0.02882826
Iteration 26, loss = 0.02823780
Iteration 27, loss = 0.02745563
Iteration 28, loss = 0.02668910
Iteration 29, loss = 0.02609412
Iteration 30, loss = 0.02569248
Iteration 31, loss = 0.02513863
Iteration 32, loss = 0.02467744
Iteration 33, loss = 0.02448164
Iteration 34, loss = 0.02392234
Iteration 35, loss = 0.02352042
Iteration 36, loss = 0.02338381
Iteration 37, loss = 0.02301377
Iteration 38, loss = 0.02307216
Iteration 39, loss = 0.02249830
Iteration 40, loss = 0.02246099
Iteration 41, loss = 0.02231775
Iteration 42, loss = 0.02196784
Iteration 43, loss = 0.02191872
Iteration 44, loss = 0.02163284
Iteration 45, loss = 0.02153516
Iteration 46, loss = 0.02125204
Iteration 47, loss = 0.02121571
Iteration 48, loss = 0.02115221
Iteration 49, loss = 0.02096731
Iteration 50, loss = 0.02106117
Iteration 51, loss = 0.02073112
Iteration 52, loss = 0.02066402
Iteration 53, loss = 0.02079928
Iteration 54, loss = 0.02069483
Iteration 55, loss = 0.02067623
Iteration 56, loss = 0.02047044
Iteration 57, loss = 0.02071045
Iteration 58, loss = 0.02015832
Iteration 59, loss = 0.02028742
Iteration 60, loss = 0.02048516
Iteration 61, loss = 0.01995494
Iteration 62, loss = 0.02011510
Iteration 63, loss = 0.02025778
Iteration 64, loss = 0.01986042
Iteration 65, loss = 0.01984197
Iteration 66, loss = 0.02025413
Iteration 67, loss = 0.02003438
Iteration 68, loss = 0.01967243
Iteration 69, loss = 0.01974027
Iteration 70, loss = 0.01986994
Iteration 71, loss = 0.01977550
Iteration 72, loss = 0.01962580
Iteration 73, loss = 0.02002664
Iteration 74, loss = 0.01929923
Iteration 75, loss = 0.01996673
Iteration 76, loss = 0.01959947
Iteration 77, loss = 0.01975754
Iteration 78, loss = 0.01945106
Iteration 79, loss = 0.01935404
Iteration 80, loss = 0.01933365
Iteration 81, loss = 0.01945857
Iteration 82, loss = 0.01958563
Iteration 83, loss = 0.01928783
Iteration 84, loss = 0.01940863
Iteration 85, loss = 0.01950742
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69624344
Iteration 2, loss = 0.66410264
Iteration 3, loss = 0.62137926
Iteration 4, loss = 0.56911587
Iteration 5, loss = 0.51070252
Iteration 6, loss = 0.45018934
Iteration 7, loss = 0.39045636
Iteration 8, loss = 0.33523651
Iteration 9, loss = 0.28600370
Iteration 10, loss = 0.24330102
Iteration 11, loss = 0.20732763
Iteration 12, loss = 0.17727892
Iteration 13, loss = 0.15255326
Iteration 14, loss = 0.13243615
Iteration 15, loss = 0.11585584
Iteration 16, loss = 0.10210683
Iteration 17, loss = 0.09077945
Iteration 18, loss = 0.08157279
Iteration 19, loss = 0.07365308
Iteration 20, loss = 0.06698499
Iteration 21, loss = 0.06148826
Iteration 22, loss = 0.05656770
Iteration 23, loss = 0.05251399
Iteration 24, loss = 0.04894677
Iteration 25, loss = 0.04572084
Iteration 26, loss = 0.04303674
Iteration 27, loss = 0.04073654
Iteration 28, loss = 0.03862221
Iteration 29, loss = 0.03669010
Iteration 30, loss = 0.03512085
Iteration 31, loss = 0.03363060
Iteration 32, loss = 0.03217557
Iteration 33, loss = 0.03099063
Iteration 34, loss = 0.02993905
Iteration 35, loss = 0.02885075
Iteration 36, loss = 0.02814494
Iteration 37, loss = 0.02722631
Iteration 38, loss = 0.02638753
Iteration 39, loss = 0.02564707
Iteration 40, loss = 0.02500605
Iteration 41, loss = 0.02434667
Iteration 42, loss = 0.02394075
Iteration 43, loss = 0.02336569
Iteration 44, loss = 0.02291240
Iteration 45, loss = 0.02243982
Iteration 46, loss = 0.02200804
Iteration 47, loss = 0.02173166
Iteration 48, loss = 0.02121718
Iteration 49, loss = 0.02110031
Iteration 50, loss = 0.02073500
Iteration 51, loss = 0.02037591
Iteration 52, loss = 0.02034797
Iteration 53, loss = 0.01999680
Iteration 54, loss = 0.01970987
Iteration 55, loss = 0.01944127
Iteration 56, loss = 0.01915090
Iteration 57, loss = 0.01891015
Iteration 58, loss = 0.01884959
Iteration 59, loss = 0.01860569
Iteration 60, loss = 0.01845343
Iteration 61, loss = 0.01833973
Iteration 62, loss = 0.01808020
Iteration 63, loss = 0.01790466
Iteration 64, loss = 0.01773785
Iteration 65, loss = 0.01768095
Iteration 66, loss = 0.01759261
Iteration 67, loss = 0.01757417
Iteration 68, loss = 0.01738395
Iteration 69, loss = 0.01732694
Iteration 70, loss = 0.01714899
Iteration 71, loss = 0.01692151
Iteration 72, loss = 0.01700286
Iteration 73, loss = 0.01690244
Iteration 74, loss = 0.01673025
Iteration 75, loss = 0.01672214
Iteration 76, loss = 0.01661374
Iteration 77, loss = 0.01655605
Iteration 78, loss = 0.01630684
Iteration 79, loss = 0.01637811
Iteration 80, loss = 0.01631008
Iteration 81, loss = 0.01632077
Iteration 82, loss = 0.01622395
Iteration 83, loss = 0.01614590
Iteration 84, loss = 0.01603774
Iteration 85, loss = 0.01603259
Iteration 86, loss = 0.01602457
Iteration 87, loss = 0.01592732
Iteration 88, loss = 0.01594714
Iteration 89, loss = 0.01577742
Iteration 90, loss = 0.01578095
Iteration 91, loss = 0.01573001
Iteration 92, loss = 0.01557154
Iteration 93, loss = 0.01557986
Iteration 94, loss = 0.01561347
Iteration 95, loss = 0.01549951
Iteration 96, loss = 0.01555289
Iteration 97, loss = 0.01547583
Iteration 98, loss = 0.01542282
Iteration 99, loss = 0.01548325
Iteration 100, loss = 0.01528911
Iteration 101, loss = 0.01546443
Iteration 102, loss = 0.01528998
Iteration 103, loss = 0.01532733
Iteration 104, loss = 0.01531755
Iteration 105, loss = 0.01529300
Iteration 106, loss = 0.01523030
Iteration 107, loss = 0.01506832
Iteration 108, loss = 0.01511025
Iteration 109, loss = 0.01505774
Iteration 110, loss = 0.01499828
Iteration 111, loss = 0.01501526
Iteration 112, loss = 0.01510659
Iteration 113, loss = 0.01513044
Iteration 114, loss = 0.01496734
Iteration 115, loss = 0.01493302
Iteration 116, loss = 0.01515656
Iteration 117, loss = 0.01494858
Iteration 118, loss = 0.01500259
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67640659
Iteration 2, loss = 0.62195180
Iteration 3, loss = 0.54287976
Iteration 4, loss = 0.45235175
Iteration 5, loss = 0.36510850
Iteration 6, loss = 0.28949153
Iteration 7, loss = 0.22874957
Iteration 8, loss = 0.18229827
Iteration 9, loss = 0.14760967
Iteration 10, loss = 0.12166343
Iteration 11, loss = 0.10254024
Iteration 12, loss = 0.08784120
Iteration 13, loss = 0.07648378
Iteration 14, loss = 0.06754800
Iteration 15, loss = 0.06044276
Iteration 16, loss = 0.05492952
Iteration 17, loss = 0.05035198
Iteration 18, loss = 0.04649944
Iteration 19, loss = 0.04312108
Iteration 20, loss = 0.04036929
Iteration 21, loss = 0.03793609
Iteration 22, loss = 0.03607141
Iteration 23, loss = 0.03445966
Iteration 24, loss = 0.03290762
Iteration 25, loss = 0.03150715
Iteration 26, loss = 0.03042519
Iteration 27, loss = 0.02955856
Iteration 28, loss = 0.02865806
Iteration 29, loss = 0.02785283
Iteration 30, loss = 0.02706800
Iteration 31, loss = 0.02636759
Iteration 32, loss = 0.02575232
Iteration 33, loss = 0.02522623
Iteration 34, loss = 0.02476957
Iteration 35, loss = 0.02416881
Iteration 36, loss = 0.02398183
Iteration 37, loss = 0.02368467
Iteration 38, loss = 0.02320644
Iteration 39, loss = 0.02312343
Iteration 40, loss = 0.02278130
Iteration 41, loss = 0.02265349
Iteration 42, loss = 0.02222734
Iteration 43, loss = 0.02181098
Iteration 44, loss = 0.02170134
Iteration 45, loss = 0.02151509
Iteration 46, loss = 0.02136436
Iteration 47, loss = 0.02115161
Iteration 48, loss = 0.02100184
Iteration 49, loss = 0.02103593
Iteration 50, loss = 0.02083080
Iteration 51, loss = 0.02066005
Iteration 52, loss = 0.02042583
Iteration 53, loss = 0.02046976
Iteration 54, loss = 0.02053957
Iteration 55, loss = 0.02027031
Iteration 56, loss = 0.02018207
Iteration 57, loss = 0.02026475
Iteration 58, loss = 0.01994742
Iteration 59, loss = 0.02003563
Iteration 60, loss = 0.01977437
Iteration 61, loss = 0.01982283
Iteration 62, loss = 0.01957135
Iteration 63, loss = 0.01974908
Iteration 64, loss = 0.01951967
Iteration 65, loss = 0.01955106
Iteration 66, loss = 0.01940873
Iteration 67, loss = 0.01948973
Iteration 68, loss = 0.01943944
Iteration 69, loss = 0.01932173
Iteration 70, loss = 0.01955257
Iteration 71, loss = 0.01935634
Iteration 72, loss = 0.01928243
Iteration 73, loss = 0.01912156
Iteration 74, loss = 0.01912579
Iteration 75, loss = 0.01903239
Iteration 76, loss = 0.01896875
Iteration 77, loss = 0.01884328
Iteration 78, loss = 0.01874550
Iteration 79, loss = 0.01883238
Iteration 80, loss = 0.01892474
Iteration 81, loss = 0.01882680
Iteration 82, loss = 0.01901825
Iteration 83, loss = 0.01905559
Iteration 84, loss = 0.01880791
Iteration 85, loss = 0.01864080
Iteration 86, loss = 0.01867731
Iteration 87, loss = 0.01886703
Iteration 88, loss = 0.01844408
Iteration 89, loss = 0.01861635
Iteration 90, loss = 0.01873440
Iteration 91, loss = 0.01847280
Iteration 92, loss = 0.01881642
Iteration 93, loss = 0.01863886
Iteration 94, loss = 0.01876021
Iteration 95, loss = 0.01859932
Iteration 96, loss = 0.01839776
Iteration 97, loss = 0.01863959
Iteration 98, loss = 0.01855740
Iteration 99, loss = 0.01851826
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69057585
Iteration 2, loss = 0.66785667
Iteration 3, loss = 0.64647902
Iteration 4, loss = 0.62244362
Iteration 5, loss = 0.59509800
Iteration 6, loss = 0.56439656
Iteration 7, loss = 0.53039507
Iteration 8, loss = 0.49415000
Iteration 9, loss = 0.45672477
Iteration 10, loss = 0.41969141
Iteration 11, loss = 0.38347938
Iteration 12, loss = 0.34921400
Iteration 13, loss = 0.31725512
Iteration 14, loss = 0.28780569
Iteration 15, loss = 0.26104903
Iteration 16, loss = 0.23692830
Iteration 17, loss = 0.21521241
Iteration 18, loss = 0.19577467
Iteration 19, loss = 0.17843815
Iteration 20, loss = 0.16305723
Iteration 21, loss = 0.14934063
Iteration 22, loss = 0.13718001
Iteration 23, loss = 0.12633670
Iteration 24, loss = 0.11670872
Iteration 25, loss = 0.10823666
Iteration 26, loss = 0.10041521
Iteration 27, loss = 0.09362840
Iteration 28, loss = 0.08747069
Iteration 29, loss = 0.08190310
Iteration 30, loss = 0.07692408
Iteration 31, loss = 0.07244639
Iteration 32, loss = 0.06842103
Iteration 33, loss = 0.06464882
Iteration 34, loss = 0.06128741
Iteration 35, loss = 0.05829062
Iteration 36, loss = 0.05538654
Iteration 37, loss = 0.05289255
Iteration 38, loss = 0.05048435
Iteration 39, loss = 0.04837993
Iteration 40, loss = 0.04632686
Iteration 41, loss = 0.04449557
Iteration 42, loss = 0.04275547
Iteration 43, loss = 0.04113775
Iteration 44, loss = 0.03965923
Iteration 45, loss = 0.03825038
Iteration 46, loss = 0.03701909
Iteration 47, loss = 0.03578150
Iteration 48, loss = 0.03469683
Iteration 49, loss = 0.03366916
Iteration 50, loss = 0.03266948
Iteration 51, loss = 0.03172143
Iteration 52, loss = 0.03090008
Iteration 53, loss = 0.03006230
Iteration 54, loss = 0.02926822
Iteration 55, loss = 0.02863709
Iteration 56, loss = 0.02790532
Iteration 57, loss = 0.02727922
Iteration 58, loss = 0.02664517
Iteration 59, loss = 0.02608186
Iteration 60, loss = 0.02548475
Iteration 61, loss = 0.02496622
Iteration 62, loss = 0.02450960
Iteration 63, loss = 0.02405452
Iteration 64, loss = 0.02355927
Iteration 65, loss = 0.02311913
Iteration 66, loss = 0.02266423
Iteration 67, loss = 0.02233418
Iteration 68, loss = 0.02194756
Iteration 69, loss = 0.02164145
Iteration 70, loss = 0.02124797
Iteration 71, loss = 0.02097132
Iteration 72, loss = 0.02063515
Iteration 73, loss = 0.02030991
Iteration 74, loss = 0.02005683
Iteration 75, loss = 0.01977686
Iteration 76, loss = 0.01950539
Iteration 77, loss = 0.01926249
Iteration 78, loss = 0.01904047
Iteration 79, loss = 0.01882486
Iteration 80, loss = 0.01859287
Iteration 81, loss = 0.01833169
Iteration 82, loss = 0.01822974
Iteration 83, loss = 0.01797122
Iteration 84, loss = 0.01781319
Iteration 85, loss = 0.01760395
Iteration 86, loss = 0.01750435
Iteration 87, loss = 0.01727578
Iteration 88, loss = 0.01709730
Iteration 89, loss = 0.01689708
Iteration 90, loss = 0.01675279
Iteration 91, loss = 0.01664476
Iteration 92, loss = 0.01649004
Iteration 93, loss = 0.01634686
Iteration 94, loss = 0.01624140
Iteration 95, loss = 0.01605211
Iteration 96, loss = 0.01592683
Iteration 97, loss = 0.01584217
Iteration 98, loss = 0.01570116
Iteration 99, loss = 0.01564607
Iteration 100, loss = 0.01550171
Iteration 101, loss = 0.01536767
Iteration 102, loss = 0.01532248
Iteration 103, loss = 0.01517808
Iteration 104, loss = 0.01506376
Iteration 105, loss = 0.01498506
Iteration 106, loss = 0.01489225
Iteration 107, loss = 0.01475927
Iteration 108, loss = 0.01469468
Iteration 109, loss = 0.01465534
Iteration 110, loss = 0.01455295
Iteration 111, loss = 0.01448642
Iteration 112, loss = 0.01437639
Iteration 113, loss = 0.01435020
Iteration 114, loss = 0.01424238
Iteration 115, loss = 0.01412662
Iteration 116, loss = 0.01410656
Iteration 117, loss = 0.01406799
Iteration 118, loss = 0.01400844
Iteration 119, loss = 0.01388251
Iteration 120, loss = 0.01386385
Iteration 121, loss = 0.01375584
Iteration 122, loss = 0.01376542
Iteration 123, loss = 0.01369745
Iteration 124, loss = 0.01358142
Iteration 125, loss = 0.01355112
Iteration 126, loss = 0.01345700
Iteration 127, loss = 0.01349219
Iteration 128, loss = 0.01343109
Iteration 129, loss = 0.01336120
Iteration 130, loss = 0.01332369
Iteration 131, loss = 0.01330981
Iteration 132, loss = 0.01319188
Iteration 133, loss = 0.01317189
Iteration 134, loss = 0.01317614
Iteration 135, loss = 0.01311752
Iteration 136, loss = 0.01300580
Iteration 137, loss = 0.01295044
Iteration 138, loss = 0.01297219
Iteration 139, loss = 0.01293004
Iteration 140, loss = 0.01289082
Iteration 141, loss = 0.01283532
Iteration 142, loss = 0.01279824
Iteration 143, loss = 0.01275699
Iteration 144, loss = 0.01274258
Iteration 145, loss = 0.01267364
Iteration 146, loss = 0.01265887
Iteration 147, loss = 0.01267483
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73719175
Iteration 2, loss = 0.71750429
Iteration 3, loss = 0.69853953
Iteration 4, loss = 0.67673887
Iteration 5, loss = 0.65127272
Iteration 6, loss = 0.62271630
Iteration 7, loss = 0.59183097
Iteration 8, loss = 0.55827555
Iteration 9, loss = 0.52360396
Iteration 10, loss = 0.48778797
Iteration 11, loss = 0.45141012
Iteration 12, loss = 0.41555482
Iteration 13, loss = 0.38103444
Iteration 14, loss = 0.34813589
Iteration 15, loss = 0.31752147
Iteration 16, loss = 0.28904823
Iteration 17, loss = 0.26294599
Iteration 18, loss = 0.23930618
Iteration 19, loss = 0.21792304
Iteration 20, loss = 0.19864014
Iteration 21, loss = 0.18150311
Iteration 22, loss = 0.16599290
Iteration 23, loss = 0.15237886
Iteration 24, loss = 0.14011446
Iteration 25, loss = 0.12932819
Iteration 26, loss = 0.11960529
Iteration 27, loss = 0.11097553
Iteration 28, loss = 0.10323964
Iteration 29, loss = 0.09633153
Iteration 30, loss = 0.09016810
Iteration 31, loss = 0.08451680
Iteration 32, loss = 0.07959933
Iteration 33, loss = 0.07497449
Iteration 34, loss = 0.07089001
Iteration 35, loss = 0.06717969
Iteration 36, loss = 0.06381755
Iteration 37, loss = 0.06074147
Iteration 38, loss = 0.05787927
Iteration 39, loss = 0.05531443
Iteration 40, loss = 0.05287135
Iteration 41, loss = 0.05078107
Iteration 42, loss = 0.04870695
Iteration 43, loss = 0.04682556
Iteration 44, loss = 0.04509470
Iteration 45, loss = 0.04352224
Iteration 46, loss = 0.04200763
Iteration 47, loss = 0.04057709
Iteration 48, loss = 0.03935039
Iteration 49, loss = 0.03808450
Iteration 50, loss = 0.03699316
Iteration 51, loss = 0.03590065
Iteration 52, loss = 0.03491687
Iteration 53, loss = 0.03401017
Iteration 54, loss = 0.03313548
Iteration 55, loss = 0.03229740
Iteration 56, loss = 0.03153879
Iteration 57, loss = 0.03082056
Iteration 58, loss = 0.03008314
Iteration 59, loss = 0.02942700
Iteration 60, loss = 0.02882458
Iteration 61, loss = 0.02822405
Iteration 62, loss = 0.02771410
Iteration 63, loss = 0.02711813
Iteration 64, loss = 0.02660808
Iteration 65, loss = 0.02621529
Iteration 66, loss = 0.02576219
Iteration 67, loss = 0.02531135
Iteration 68, loss = 0.02486439
Iteration 69, loss = 0.02453216
Iteration 70, loss = 0.02411747
Iteration 71, loss = 0.02377958
Iteration 72, loss = 0.02340288
Iteration 73, loss = 0.02307729
Iteration 74, loss = 0.02276929
Iteration 75, loss = 0.02247666
Iteration 76, loss = 0.02216249
Iteration 77, loss = 0.02190206
Iteration 78, loss = 0.02163102
Iteration 79, loss = 0.02136100
Iteration 80, loss = 0.02115993
Iteration 81, loss = 0.02093881
Iteration 82, loss = 0.02064327
Iteration 83, loss = 0.02046800
Iteration 84, loss = 0.02022616
Iteration 85, loss = 0.02004448
Iteration 86, loss = 0.01990581
Iteration 87, loss = 0.01967106
Iteration 88, loss = 0.01951008
Iteration 89, loss = 0.01932737
Iteration 90, loss = 0.01917210
Iteration 91, loss = 0.01906588
Iteration 92, loss = 0.01887551
Iteration 93, loss = 0.01865053
Iteration 94, loss = 0.01853457
Iteration 95, loss = 0.01844713
Iteration 96, loss = 0.01838717
Iteration 97, loss = 0.01813857
Iteration 98, loss = 0.01802834
Iteration 99, loss = 0.01787395
Iteration 100, loss = 0.01781329
Iteration 101, loss = 0.01762355
Iteration 102, loss = 0.01755201
Iteration 103, loss = 0.01743049
Iteration 104, loss = 0.01735294
Iteration 105, loss = 0.01727591
Iteration 106, loss = 0.01712138
Iteration 107, loss = 0.01708003
Iteration 108, loss = 0.01693912
Iteration 109, loss = 0.01687470
Iteration 110, loss = 0.01671387
Iteration 111, loss = 0.01670135
Iteration 112, loss = 0.01657333
Iteration 113, loss = 0.01652288
Iteration 114, loss = 0.01640876
Iteration 115, loss = 0.01635308
Iteration 116, loss = 0.01631327
Iteration 117, loss = 0.01624558
Iteration 118, loss = 0.01613588
Iteration 119, loss = 0.01603895
Iteration 120, loss = 0.01597417
Iteration 121, loss = 0.01593193
Iteration 122, loss = 0.01585342
Iteration 123, loss = 0.01580686
Iteration 124, loss = 0.01572877
Iteration 125, loss = 0.01570947
Iteration 126, loss = 0.01561493
Iteration 127, loss = 0.01564019
Iteration 128, loss = 0.01547902
Iteration 129, loss = 0.01547146
Iteration 130, loss = 0.01545008
Iteration 131, loss = 0.01537341
Iteration 132, loss = 0.01530119
Iteration 133, loss = 0.01527759
Iteration 134, loss = 0.01522055
Iteration 135, loss = 0.01514570
Iteration 136, loss = 0.01514416
Iteration 137, loss = 0.01508582
Iteration 138, loss = 0.01505795
Iteration 139, loss = 0.01498993
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68118273
Iteration 2, loss = 0.64550912
Iteration 3, loss = 0.59400850
Iteration 4, loss = 0.52755871
Iteration 5, loss = 0.45484797
Iteration 6, loss = 0.38405582
Iteration 7, loss = 0.32026928
Iteration 8, loss = 0.26502703
Iteration 9, loss = 0.21983961
Iteration 10, loss = 0.18310370
Iteration 11, loss = 0.15402783
Iteration 12, loss = 0.13068573
Iteration 13, loss = 0.11245318
Iteration 14, loss = 0.09780435
Iteration 15, loss = 0.08611433
Iteration 16, loss = 0.07643956
Iteration 17, loss = 0.06864036
Iteration 18, loss = 0.06213865
Iteration 19, loss = 0.05681508
Iteration 20, loss = 0.05219201
Iteration 21, loss = 0.04838150
Iteration 22, loss = 0.04487797
Iteration 23, loss = 0.04207028
Iteration 24, loss = 0.03959948
Iteration 25, loss = 0.03740528
Iteration 26, loss = 0.03539099
Iteration 27, loss = 0.03376539
Iteration 28, loss = 0.03223019
Iteration 29, loss = 0.03092035
Iteration 30, loss = 0.02971066
Iteration 31, loss = 0.02879206
Iteration 32, loss = 0.02781309
Iteration 33, loss = 0.02685068
Iteration 34, loss = 0.02595735
Iteration 35, loss = 0.02540439
Iteration 36, loss = 0.02470233
Iteration 37, loss = 0.02411197
Iteration 38, loss = 0.02344041
Iteration 39, loss = 0.02296801
Iteration 40, loss = 0.02254135
Iteration 41, loss = 0.02209236
Iteration 42, loss = 0.02179356
Iteration 43, loss = 0.02125342
Iteration 44, loss = 0.02105894
Iteration 45, loss = 0.02077286
Iteration 46, loss = 0.02047651
Iteration 47, loss = 0.02012829
Iteration 48, loss = 0.02007379
Iteration 49, loss = 0.01954365
Iteration 50, loss = 0.01941021
Iteration 51, loss = 0.01938713
Iteration 52, loss = 0.01901768
Iteration 53, loss = 0.01903672
Iteration 54, loss = 0.01863872
Iteration 55, loss = 0.01835994
Iteration 56, loss = 0.01830309
Iteration 57, loss = 0.01822629
Iteration 58, loss = 0.01805390
Iteration 59, loss = 0.01787376
Iteration 60, loss = 0.01785018
Iteration 61, loss = 0.01771875
Iteration 62, loss = 0.01757497
Iteration 63, loss = 0.01748011
Iteration 64, loss = 0.01745918
Iteration 65, loss = 0.01722687
Iteration 66, loss = 0.01712925
Iteration 67, loss = 0.01706347
Iteration 68, loss = 0.01704952
Iteration 69, loss = 0.01700384
Iteration 70, loss = 0.01677735
Iteration 71, loss = 0.01678831
Iteration 72, loss = 0.01658137
Iteration 73, loss = 0.01661295
Iteration 74, loss = 0.01656742
Iteration 75, loss = 0.01645796
Iteration 76, loss = 0.01637403
Iteration 77, loss = 0.01638470
Iteration 78, loss = 0.01629869
Iteration 79, loss = 0.01622489
Iteration 80, loss = 0.01624764
Iteration 81, loss = 0.01616635
Iteration 82, loss = 0.01608932
Iteration 83, loss = 0.01601049
Iteration 84, loss = 0.01599727
Iteration 85, loss = 0.01597675
Iteration 86, loss = 0.01583251
Iteration 87, loss = 0.01607472
Iteration 88, loss = 0.01590292
Iteration 89, loss = 0.01606830
Iteration 90, loss = 0.01580828
Iteration 91, loss = 0.01563964
Iteration 92, loss = 0.01565620
Iteration 93, loss = 0.01560193
Iteration 94, loss = 0.01565364
Iteration 95, loss = 0.01563466
Iteration 96, loss = 0.01563027
Iteration 97, loss = 0.01556170
Iteration 98, loss = 0.01561104
Iteration 99, loss = 0.01554944
Iteration 100, loss = 0.01555468
Iteration 101, loss = 0.01552263
Iteration 102, loss = 0.01543123
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70685157
Iteration 2, loss = 0.64081361
Iteration 3, loss = 0.55232966
Iteration 4, loss = 0.45060151
Iteration 5, loss = 0.35472418
Iteration 6, loss = 0.27425348
Iteration 7, loss = 0.21277945
Iteration 8, loss = 0.16751575
Iteration 9, loss = 0.13501739
Iteration 10, loss = 0.11155049
Iteration 11, loss = 0.09437586
Iteration 12, loss = 0.08165818
Iteration 13, loss = 0.07142816
Iteration 14, loss = 0.06381635
Iteration 15, loss = 0.05738243
Iteration 16, loss = 0.05273023
Iteration 17, loss = 0.04857291
Iteration 18, loss = 0.04523130
Iteration 19, loss = 0.04230063
Iteration 20, loss = 0.04006635
Iteration 21, loss = 0.03780251
Iteration 22, loss = 0.03630665
Iteration 23, loss = 0.03469661
Iteration 24, loss = 0.03333691
Iteration 25, loss = 0.03232550
Iteration 26, loss = 0.03124116
Iteration 27, loss = 0.03039613
Iteration 28, loss = 0.02959696
Iteration 29, loss = 0.02874266
Iteration 30, loss = 0.02821627
Iteration 31, loss = 0.02758046
Iteration 32, loss = 0.02704470
Iteration 33, loss = 0.02653285
Iteration 34, loss = 0.02607723
Iteration 35, loss = 0.02570968
Iteration 36, loss = 0.02536342
Iteration 37, loss = 0.02508057
Iteration 38, loss = 0.02452239
Iteration 39, loss = 0.02489919
Iteration 40, loss = 0.02526470
Iteration 41, loss = 0.02490833
Iteration 42, loss = 0.02436911
Iteration 43, loss = 0.02408758
Iteration 44, loss = 0.02376592
Iteration 45, loss = 0.02347698
Iteration 46, loss = 0.02352730
Iteration 47, loss = 0.02330822
Iteration 48, loss = 0.02295702
Iteration 49, loss = 0.02306565
Iteration 50, loss = 0.02278355
Iteration 51, loss = 0.02267362
Iteration 52, loss = 0.02257133
Iteration 53, loss = 0.02224281
Iteration 54, loss = 0.02217043
Iteration 55, loss = 0.02210353
Iteration 56, loss = 0.02201921
Iteration 57, loss = 0.02348380
Iteration 58, loss = 0.02347514
Iteration 59, loss = 0.02307428
Iteration 60, loss = 0.02301726
Iteration 61, loss = 0.02278369
Iteration 62, loss = 0.02249123
Iteration 63, loss = 0.02197624
Iteration 64, loss = 0.02187558
Iteration 65, loss = 0.02194020
Iteration 66, loss = 0.02154981
Iteration 67, loss = 0.02153437
Iteration 68, loss = 0.02137569
Iteration 69, loss = 0.02140405
Iteration 70, loss = 0.02110496
Iteration 71, loss = 0.02087134
Iteration 72, loss = 0.02084059
Iteration 73, loss = 0.02054920
Iteration 74, loss = 0.02075225
Iteration 75, loss = 0.02066896
Iteration 76, loss = 0.02055288
Iteration 77, loss = 0.02200682
Iteration 78, loss = 0.02391483
Iteration 79, loss = 0.02369871
Iteration 80, loss = 0.02268363
Iteration 81, loss = 0.02220698
Iteration 82, loss = 0.02200530
Iteration 83, loss = 0.02192597
Iteration 84, loss = 0.02313579
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70919607
Iteration 2, loss = 0.67513704
Iteration 3, loss = 0.63072295
Iteration 4, loss = 0.57729456
Iteration 5, loss = 0.51447553
Iteration 6, loss = 0.44871693
Iteration 7, loss = 0.38503289
Iteration 8, loss = 0.32735503
Iteration 9, loss = 0.27719764
Iteration 10, loss = 0.23473030
Iteration 11, loss = 0.19917396
Iteration 12, loss = 0.17013405
Iteration 13, loss = 0.14644549
Iteration 14, loss = 0.12718478
Iteration 15, loss = 0.11141145
Iteration 16, loss = 0.09846066
Iteration 17, loss = 0.08768728
Iteration 18, loss = 0.07880417
Iteration 19, loss = 0.07142947
Iteration 20, loss = 0.06506342
Iteration 21, loss = 0.05970800
Iteration 22, loss = 0.05520823
Iteration 23, loss = 0.05112319
Iteration 24, loss = 0.04782770
Iteration 25, loss = 0.04482582
Iteration 26, loss = 0.04216295
Iteration 27, loss = 0.03994438
Iteration 28, loss = 0.03777286
Iteration 29, loss = 0.03597094
Iteration 30, loss = 0.03443402
Iteration 31, loss = 0.03290401
Iteration 32, loss = 0.03152382
Iteration 33, loss = 0.03036332
Iteration 34, loss = 0.02921223
Iteration 35, loss = 0.02830552
Iteration 36, loss = 0.02730515
Iteration 37, loss = 0.02646707
Iteration 38, loss = 0.02576567
Iteration 39, loss = 0.02504285
Iteration 40, loss = 0.02443202
Iteration 41, loss = 0.02386876
Iteration 42, loss = 0.02335259
Iteration 43, loss = 0.02290472
Iteration 44, loss = 0.02247305
Iteration 45, loss = 0.02200454
Iteration 46, loss = 0.02156816
Iteration 47, loss = 0.02126046
Iteration 48, loss = 0.02078306
Iteration 49, loss = 0.02049213
Iteration 50, loss = 0.02016914
Iteration 51, loss = 0.02001161
Iteration 52, loss = 0.01964217
Iteration 53, loss = 0.01943129
Iteration 54, loss = 0.01919009
Iteration 55, loss = 0.01888726
Iteration 56, loss = 0.01869327
Iteration 57, loss = 0.01847801
Iteration 58, loss = 0.01833125
Iteration 59, loss = 0.01814562
Iteration 60, loss = 0.01816836
Iteration 61, loss = 0.01793199
Iteration 62, loss = 0.01778982
Iteration 63, loss = 0.01762986
Iteration 64, loss = 0.01746110
Iteration 65, loss = 0.01732807
Iteration 66, loss = 0.01727379
Iteration 67, loss = 0.01695297
Iteration 68, loss = 0.01689961
Iteration 69, loss = 0.01680586
Iteration 70, loss = 0.01679311
Iteration 71, loss = 0.01663687
Iteration 72, loss = 0.01659001
Iteration 73, loss = 0.01659626
Iteration 74, loss = 0.01633488
Iteration 75, loss = 0.01620410
Iteration 76, loss = 0.01619717
Iteration 77, loss = 0.01608961
Iteration 78, loss = 0.01597273
Iteration 79, loss = 0.01597813
Iteration 80, loss = 0.01591155
Iteration 81, loss = 0.01595094
Iteration 82, loss = 0.01584047
Iteration 83, loss = 0.01575784
Iteration 84, loss = 0.01574272
Iteration 85, loss = 0.01553917
Iteration 86, loss = 0.01569971
Iteration 87, loss = 0.01564568
Iteration 88, loss = 0.01540854
Iteration 89, loss = 0.01543396
Iteration 90, loss = 0.01533923
Iteration 91, loss = 0.01530060
Iteration 92, loss = 0.01529500
Iteration 93, loss = 0.01533259
Iteration 94, loss = 0.01510311
Iteration 95, loss = 0.01507339
Iteration 96, loss = 0.01513991
Iteration 97, loss = 0.01504933
Iteration 98, loss = 0.01514523
Iteration 99, loss = 0.01519457
Iteration 100, loss = 0.01501970
Iteration 101, loss = 0.01496281
Iteration 102, loss = 0.01485392
Iteration 103, loss = 0.01487290
Iteration 104, loss = 0.01486183
Iteration 105, loss = 0.01489048
Iteration 106, loss = 0.01481291
Iteration 107, loss = 0.01469619
Iteration 108, loss = 0.01476895
Iteration 109, loss = 0.01460934
Iteration 110, loss = 0.01473372
Iteration 111, loss = 0.01464932
Iteration 112, loss = 0.01460711
Iteration 113, loss = 0.01459327
Iteration 114, loss = 0.01451791
Iteration 115, loss = 0.01453951
Iteration 116, loss = 0.01460438
Iteration 117, loss = 0.01447691
Iteration 118, loss = 0.01450875
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67905118
Iteration 2, loss = 0.61480658
Iteration 3, loss = 0.51405145
Iteration 4, loss = 0.40391259
Iteration 5, loss = 0.30748127
Iteration 6, loss = 0.23300131
Iteration 7, loss = 0.17828625
Iteration 8, loss = 0.14034213
Iteration 9, loss = 0.11327053
Iteration 10, loss = 0.09421470
Iteration 11, loss = 0.08026193
Iteration 12, loss = 0.06960874
Iteration 13, loss = 0.06150236
Iteration 14, loss = 0.05532541
Iteration 15, loss = 0.05048946
Iteration 16, loss = 0.04634899
Iteration 17, loss = 0.04308666
Iteration 18, loss = 0.04025620
Iteration 19, loss = 0.03800046
Iteration 20, loss = 0.03614011
Iteration 21, loss = 0.03430742
Iteration 22, loss = 0.03297916
Iteration 23, loss = 0.03182661
Iteration 24, loss = 0.03080317
Iteration 25, loss = 0.02975249
Iteration 26, loss = 0.02901469
Iteration 27, loss = 0.02800613
Iteration 28, loss = 0.02735008
Iteration 29, loss = 0.02695253
Iteration 30, loss = 0.02636077
Iteration 31, loss = 0.02627647
Iteration 32, loss = 0.02563357
Iteration 33, loss = 0.02526872
Iteration 34, loss = 0.02474939
Iteration 35, loss = 0.02451448
Iteration 36, loss = 0.02420209
Iteration 37, loss = 0.02405292
Iteration 38, loss = 0.02355245
Iteration 39, loss = 0.02354031
Iteration 40, loss = 0.02321066
Iteration 41, loss = 0.02307861
Iteration 42, loss = 0.02281706
Iteration 43, loss = 0.02266891
Iteration 44, loss = 0.02265606
Iteration 45, loss = 0.02253046
Iteration 46, loss = 0.02233992
Iteration 47, loss = 0.02229308
Iteration 48, loss = 0.02223806
Iteration 49, loss = 0.02188662
Iteration 50, loss = 0.02197783
Iteration 51, loss = 0.02176887
Iteration 52, loss = 0.02155446
Iteration 53, loss = 0.02154055
Iteration 54, loss = 0.02139178
Iteration 55, loss = 0.02155026
Iteration 56, loss = 0.02136179
Iteration 57, loss = 0.02114382
Iteration 58, loss = 0.02100184
Iteration 59, loss = 0.02101765
Iteration 60, loss = 0.02095954
Iteration 61, loss = 0.02100600
Iteration 62, loss = 0.02104366
Iteration 63, loss = 0.02053920
Iteration 64, loss = 0.02088198
Iteration 65, loss = 0.02068191
Iteration 66, loss = 0.02059358
Iteration 67, loss = 0.02070077
Iteration 68, loss = 0.02072436
Iteration 69, loss = 0.02071147
Iteration 70, loss = 0.02067095
Iteration 71, loss = 0.02076946
Iteration 72, loss = 0.02037345
Iteration 73, loss = 0.02068230
Iteration 74, loss = 0.02056412
Iteration 75, loss = 0.02046761
Iteration 76, loss = 0.02053941
Iteration 77, loss = 0.02017883
Iteration 78, loss = 0.02025661
Iteration 79, loss = 0.02031582
Iteration 80, loss = 0.02033166
Iteration 81, loss = 0.02009291
Iteration 82, loss = 0.02037840
Iteration 83, loss = 0.02029014
Iteration 84, loss = 0.02032992
Iteration 85, loss = 0.01999843
Iteration 86, loss = 0.02058540
Iteration 87, loss = 0.02013060
Iteration 88, loss = 0.02007227
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68755767
Iteration 2, loss = 0.59347949
Iteration 3, loss = 0.47465973
Iteration 4, loss = 0.36233254
Iteration 5, loss = 0.27071613
Iteration 6, loss = 0.20301815
Iteration 7, loss = 0.15617873
Iteration 8, loss = 0.12386048
Iteration 9, loss = 0.10126530
Iteration 10, loss = 0.08521000
Iteration 11, loss = 0.07341488
Iteration 12, loss = 0.06471468
Iteration 13, loss = 0.05793608
Iteration 14, loss = 0.05282093
Iteration 15, loss = 0.04830907
Iteration 16, loss = 0.04506661
Iteration 17, loss = 0.04197658
Iteration 18, loss = 0.03977665
Iteration 19, loss = 0.03775398
Iteration 20, loss = 0.03641590
Iteration 21, loss = 0.03506539
Iteration 22, loss = 0.03376799
Iteration 23, loss = 0.03272740
Iteration 24, loss = 0.03180150
Iteration 25, loss = 0.03103396
Iteration 26, loss = 0.03047706
Iteration 27, loss = 0.03005197
Iteration 28, loss = 0.02932097
Iteration 29, loss = 0.02896827
Iteration 30, loss = 0.02860838
Iteration 31, loss = 0.02788369
Iteration 32, loss = 0.02757494
Iteration 33, loss = 0.02759016
Iteration 34, loss = 0.02706523
Iteration 35, loss = 0.02661279
Iteration 36, loss = 0.02666437
Iteration 37, loss = 0.02629314
Iteration 38, loss = 0.02635661
Iteration 39, loss = 0.02606398
Iteration 40, loss = 0.02582286
Iteration 41, loss = 0.02546388
Iteration 42, loss = 0.02568640
Iteration 43, loss = 0.02536386
Iteration 44, loss = 0.02534001
Iteration 45, loss = 0.02500719
Iteration 46, loss = 0.02487958
Iteration 47, loss = 0.02496176
Iteration 48, loss = 0.02454189
Iteration 49, loss = 0.02471906
Iteration 50, loss = 0.02468827
Iteration 51, loss = 0.02460552
Iteration 52, loss = 0.02457417
Iteration 53, loss = 0.02456995
Iteration 54, loss = 0.02417348
Iteration 55, loss = 0.02414682
Iteration 56, loss = 0.02431870
Iteration 57, loss = 0.02413313
Iteration 58, loss = 0.02416120
Iteration 59, loss = 0.02402885
Iteration 60, loss = 0.02435768
Iteration 61, loss = 0.02442893
Iteration 62, loss = 0.02388646
Iteration 63, loss = 0.02391045
Iteration 64, loss = 0.02407574
Iteration 65, loss = 0.02405298
Iteration 66, loss = 0.02373181
Iteration 67, loss = 0.02379209
Iteration 68, loss = 0.02362008
Iteration 69, loss = 0.02370873
Iteration 70, loss = 0.02380694
Iteration 71, loss = 0.02369690
Iteration 72, loss = 0.02345105
Iteration 73, loss = 0.02348169
Iteration 74, loss = 0.02368865
Iteration 75, loss = 0.02391114
Iteration 76, loss = 0.02349155
Iteration 77, loss = 0.02341046
Iteration 78, loss = 0.02343429
Iteration 79, loss = 0.02337987
Iteration 80, loss = 0.02342355
Iteration 81, loss = 0.02339738
Iteration 82, loss = 0.02370327
Iteration 83, loss = 0.02315517
Iteration 84, loss = 0.02321282
Iteration 85, loss = 0.02314144
Iteration 86, loss = 0.02349944
Iteration 87, loss = 0.02360392
Iteration 88, loss = 0.02325343
Iteration 89, loss = 0.02325325
Iteration 90, loss = 0.02334124
Iteration 91, loss = 0.02310918
Iteration 92, loss = 0.02346868
Iteration 93, loss = 0.02358307
Iteration 94, loss = 0.02311353
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68286323
Iteration 2, loss = 0.60639462
Iteration 3, loss = 0.49275418
Iteration 4, loss = 0.37565408
Iteration 5, loss = 0.27930049
Iteration 6, loss = 0.20890397
Iteration 7, loss = 0.15960767
Iteration 8, loss = 0.12629087
Iteration 9, loss = 0.10315987
Iteration 10, loss = 0.08697196
Iteration 11, loss = 0.07508461
Iteration 12, loss = 0.06604353
Iteration 13, loss = 0.05918660
Iteration 14, loss = 0.05404538
Iteration 15, loss = 0.04997191
Iteration 16, loss = 0.04618882
Iteration 17, loss = 0.04341582
Iteration 18, loss = 0.04096345
Iteration 19, loss = 0.03921817
Iteration 20, loss = 0.03749550
Iteration 21, loss = 0.03612185
Iteration 22, loss = 0.03484594
Iteration 23, loss = 0.03372152
Iteration 24, loss = 0.03298748
Iteration 25, loss = 0.03204032
Iteration 26, loss = 0.03151659
Iteration 27, loss = 0.03102403
Iteration 28, loss = 0.03043651
Iteration 29, loss = 0.02962716
Iteration 30, loss = 0.02919186
Iteration 31, loss = 0.02890186
Iteration 32, loss = 0.02866461
Iteration 33, loss = 0.02785907
Iteration 34, loss = 0.02783658
Iteration 35, loss = 0.02783062
Iteration 36, loss = 0.02726950
Iteration 37, loss = 0.02719892
Iteration 38, loss = 0.02699718
Iteration 39, loss = 0.02683873
Iteration 40, loss = 0.02630612
Iteration 41, loss = 0.02632587
Iteration 42, loss = 0.02616096
Iteration 43, loss = 0.02597658
Iteration 44, loss = 0.02574136
Iteration 45, loss = 0.02568910
Iteration 46, loss = 0.02581740
Iteration 47, loss = 0.02562572
Iteration 48, loss = 0.02528154
Iteration 49, loss = 0.02529669
Iteration 50, loss = 0.02520236
Iteration 51, loss = 0.02512206
Iteration 52, loss = 0.02524371
Iteration 53, loss = 0.02526340
Iteration 54, loss = 0.02488661
Iteration 55, loss = 0.02498211
Iteration 56, loss = 0.02451909
Iteration 57, loss = 0.02492235
Iteration 58, loss = 0.02481573
Iteration 59, loss = 0.02500319
Iteration 60, loss = 0.02457835
Iteration 61, loss = 0.02452273
Iteration 62, loss = 0.02454074
Iteration 63, loss = 0.02435770
Iteration 64, loss = 0.02463986
Iteration 65, loss = 0.02474477
Iteration 66, loss = 0.02446835
Iteration 67, loss = 0.02428051
Iteration 68, loss = 0.02410223
Iteration 69, loss = 0.02458308
Iteration 70, loss = 0.02431039
Iteration 71, loss = 0.02433996
Iteration 72, loss = 0.02435492
Iteration 73, loss = 0.02426436
Iteration 74, loss = 0.02407750
Iteration 75, loss = 0.02405523
Iteration 76, loss = 0.02440111
Iteration 77, loss = 0.02372381
Iteration 78, loss = 0.02407220
Iteration 79, loss = 0.02421607
Iteration 80, loss = 0.02374008
Iteration 81, loss = 0.02373719
Iteration 82, loss = 0.02389023
Iteration 83, loss = 0.02376161
Iteration 84, loss = 0.02362020
Iteration 85, loss = 0.02384167
Iteration 86, loss = 0.02394900
Iteration 87, loss = 0.02383703
Iteration 88, loss = 0.02392230
Iteration 89, loss = 0.02386220
Iteration 90, loss = 0.02379668
Iteration 91, loss = 0.02377766
Iteration 92, loss = 0.02377894
Iteration 93, loss = 0.02384864
Iteration 94, loss = 0.02358088
Iteration 95, loss = 0.02370136
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67478086
Iteration 2, loss = 0.61118358
Iteration 3, loss = 0.51972834
Iteration 4, loss = 0.42301403
Iteration 5, loss = 0.33462092
Iteration 6, loss = 0.26136671
Iteration 7, loss = 0.20519913
Iteration 8, loss = 0.16571179
Iteration 9, loss = 0.13590644
Iteration 10, loss = 0.11411195
Iteration 11, loss = 0.09747615
Iteration 12, loss = 0.08512578
Iteration 13, loss = 0.07542729
Iteration 14, loss = 0.06793767
Iteration 15, loss = 0.06177265
Iteration 16, loss = 0.05694395
Iteration 17, loss = 0.05308127
Iteration 18, loss = 0.04953818
Iteration 19, loss = 0.04665601
Iteration 20, loss = 0.04439548
Iteration 21, loss = 0.04240672
Iteration 22, loss = 0.04057736
Iteration 23, loss = 0.03909541
Iteration 24, loss = 0.03850912
Iteration 25, loss = 0.03761497
Iteration 26, loss = 0.03640054
Iteration 27, loss = 0.03529847
Iteration 28, loss = 0.03448609
Iteration 29, loss = 0.03364038
Iteration 30, loss = 0.03327847
Iteration 31, loss = 0.03245555
Iteration 32, loss = 0.03178827
Iteration 33, loss = 0.03128970
Iteration 34, loss = 0.03080170
Iteration 35, loss = 0.03038253
Iteration 36, loss = 0.03010419
Iteration 37, loss = 0.02961577
Iteration 38, loss = 0.02934924
Iteration 39, loss = 0.02942375
Iteration 40, loss = 0.02958007
Iteration 41, loss = 0.02883606
Iteration 42, loss = 0.02852815
Iteration 43, loss = 0.02846521
Iteration 44, loss = 0.02812624
Iteration 45, loss = 0.02766784
Iteration 46, loss = 0.02779272
Iteration 47, loss = 0.02777849
Iteration 48, loss = 0.02782640
Iteration 49, loss = 0.02763092
Iteration 50, loss = 0.02747158
Iteration 51, loss = 0.02729201
Iteration 52, loss = 0.02782630
Iteration 53, loss = 0.02792271
Iteration 54, loss = 0.02734806
Iteration 55, loss = 0.02745828
Iteration 56, loss = 0.02752287
Iteration 57, loss = 0.02825734
Iteration 58, loss = 0.02830494
Iteration 59, loss = 0.02789571
Iteration 60, loss = 0.02767102
Iteration 61, loss = 0.02714265
Iteration 62, loss = 0.02721592
Iteration 63, loss = 0.02688285
Iteration 64, loss = 0.02672648
Iteration 65, loss = 0.02651611
Iteration 66, loss = 0.02628462
Iteration 67, loss = 0.02611473
Iteration 68, loss = 0.02600955
Iteration 69, loss = 0.02581091
Iteration 70, loss = 0.02576382
Iteration 71, loss = 0.02558516
Iteration 72, loss = 0.02540583
Iteration 73, loss = 0.02524442
Iteration 74, loss = 0.02526965
Iteration 75, loss = 0.02517491
Iteration 76, loss = 0.02508030
Iteration 77, loss = 0.02515242
Iteration 78, loss = 0.02497156
Iteration 79, loss = 0.02454963
Iteration 80, loss = 0.02461904
Iteration 81, loss = 0.02466556
Iteration 82, loss = 0.02437425
Iteration 83, loss = 0.02438956
Iteration 84, loss = 0.02452192
Iteration 85, loss = 0.02430929
Iteration 86, loss = 0.02419447
Iteration 87, loss = 0.02454154
Iteration 88, loss = 0.02427872
Iteration 89, loss = 0.02478653
Iteration 90, loss = 0.02413525
Iteration 91, loss = 0.02415082
Iteration 92, loss = 0.02410467
Iteration 93, loss = 0.02426591
Iteration 94, loss = 0.02403632
Iteration 95, loss = 0.02429432
Iteration 96, loss = 0.02374257
Iteration 97, loss = 0.02526915
Iteration 98, loss = 0.02616524
Iteration 99, loss = 0.02590772
Iteration 100, loss = 0.02543168
Iteration 101, loss = 0.02522079
Iteration 102, loss = 0.02516287
Iteration 103, loss = 0.02512585
Iteration 104, loss = 0.02493884
Iteration 105, loss = 0.02503872
Iteration 106, loss = 0.02481120
Iteration 107, loss = 0.02467364
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70222102
Iteration 2, loss = 0.67898366
Iteration 3, loss = 0.65736768
Iteration 4, loss = 0.63252581
Iteration 5, loss = 0.60390191
Iteration 6, loss = 0.57183632
Iteration 7, loss = 0.53664446
Iteration 8, loss = 0.49936847
Iteration 9, loss = 0.46134135
Iteration 10, loss = 0.42365751
Iteration 11, loss = 0.38718561
Iteration 12, loss = 0.35284870
Iteration 13, loss = 0.32048014
Iteration 14, loss = 0.29072529
Iteration 15, loss = 0.26351444
Iteration 16, loss = 0.23888329
Iteration 17, loss = 0.21662982
Iteration 18, loss = 0.19666090
Iteration 19, loss = 0.17897254
Iteration 20, loss = 0.16303379
Iteration 21, loss = 0.14903972
Iteration 22, loss = 0.13659881
Iteration 23, loss = 0.12554457
Iteration 24, loss = 0.11580102
Iteration 25, loss = 0.10695217
Iteration 26, loss = 0.09925202
Iteration 27, loss = 0.09232515
Iteration 28, loss = 0.08614145
Iteration 29, loss = 0.08056188
Iteration 30, loss = 0.07553479
Iteration 31, loss = 0.07106352
Iteration 32, loss = 0.06697670
Iteration 33, loss = 0.06334318
Iteration 34, loss = 0.06000114
Iteration 35, loss = 0.05689947
Iteration 36, loss = 0.05410226
Iteration 37, loss = 0.05151569
Iteration 38, loss = 0.04922026
Iteration 39, loss = 0.04707217
Iteration 40, loss = 0.04502703
Iteration 41, loss = 0.04318673
Iteration 42, loss = 0.04158182
Iteration 43, loss = 0.03996188
Iteration 44, loss = 0.03855534
Iteration 45, loss = 0.03716651
Iteration 46, loss = 0.03583373
Iteration 47, loss = 0.03469772
Iteration 48, loss = 0.03357612
Iteration 49, loss = 0.03255758
Iteration 50, loss = 0.03160188
Iteration 51, loss = 0.03071136
Iteration 52, loss = 0.02982024
Iteration 53, loss = 0.02902099
Iteration 54, loss = 0.02825437
Iteration 55, loss = 0.02756624
Iteration 56, loss = 0.02688333
Iteration 57, loss = 0.02626647
Iteration 58, loss = 0.02564537
Iteration 59, loss = 0.02508626
Iteration 60, loss = 0.02452482
Iteration 61, loss = 0.02401853
Iteration 62, loss = 0.02352197
Iteration 63, loss = 0.02306995
Iteration 64, loss = 0.02265199
Iteration 65, loss = 0.02220542
Iteration 66, loss = 0.02179043
Iteration 67, loss = 0.02142496
Iteration 68, loss = 0.02106718
Iteration 69, loss = 0.02074540
Iteration 70, loss = 0.02038082
Iteration 71, loss = 0.02006400
Iteration 72, loss = 0.01976902
Iteration 73, loss = 0.01949150
Iteration 74, loss = 0.01929085
Iteration 75, loss = 0.01894773
Iteration 76, loss = 0.01876518
Iteration 77, loss = 0.01845962
Iteration 78, loss = 0.01823909
Iteration 79, loss = 0.01797078
Iteration 80, loss = 0.01777098
Iteration 81, loss = 0.01753896
Iteration 82, loss = 0.01735077
Iteration 83, loss = 0.01715757
Iteration 84, loss = 0.01699816
Iteration 85, loss = 0.01682417
Iteration 86, loss = 0.01662307
Iteration 87, loss = 0.01648857
Iteration 88, loss = 0.01632546
Iteration 89, loss = 0.01618046
Iteration 90, loss = 0.01602056
Iteration 91, loss = 0.01583809
Iteration 92, loss = 0.01571689
Iteration 93, loss = 0.01557094
Iteration 94, loss = 0.01544799
Iteration 95, loss = 0.01531055
Iteration 96, loss = 0.01520842
Iteration 97, loss = 0.01508074
Iteration 98, loss = 0.01497383
Iteration 99, loss = 0.01487243
Iteration 100, loss = 0.01475123
Iteration 101, loss = 0.01464003
Iteration 102, loss = 0.01457753
Iteration 103, loss = 0.01444748
Iteration 104, loss = 0.01438219
Iteration 105, loss = 0.01428137
Iteration 106, loss = 0.01418228
Iteration 107, loss = 0.01409523
Iteration 108, loss = 0.01401074
Iteration 109, loss = 0.01393904
Iteration 110, loss = 0.01391845
Iteration 111, loss = 0.01383917
Iteration 112, loss = 0.01364912
Iteration 113, loss = 0.01362942
Iteration 114, loss = 0.01352605
Iteration 115, loss = 0.01350946
Iteration 116, loss = 0.01342923
Iteration 117, loss = 0.01337903
Iteration 118, loss = 0.01328766
Iteration 119, loss = 0.01320320
Iteration 120, loss = 0.01315354
Iteration 121, loss = 0.01310295
Iteration 122, loss = 0.01307678
Iteration 123, loss = 0.01295666
Iteration 124, loss = 0.01294289
Iteration 125, loss = 0.01289493
Iteration 126, loss = 0.01285076
Iteration 127, loss = 0.01278725
Iteration 128, loss = 0.01271223
Iteration 129, loss = 0.01270322
Iteration 130, loss = 0.01265826
Iteration 131, loss = 0.01257965
Iteration 132, loss = 0.01256406
Iteration 133, loss = 0.01248741
Iteration 134, loss = 0.01248576
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71392277
Iteration 2, loss = 0.67643991
Iteration 3, loss = 0.63547588
Iteration 4, loss = 0.58869375
Iteration 5, loss = 0.53926414
Iteration 6, loss = 0.48734487
Iteration 7, loss = 0.43573985
Iteration 8, loss = 0.38610283
Iteration 9, loss = 0.34089255
Iteration 10, loss = 0.30004482
Iteration 11, loss = 0.26384949
Iteration 12, loss = 0.23228637
Iteration 13, loss = 0.20486279
Iteration 14, loss = 0.18116026
Iteration 15, loss = 0.16087991
Iteration 16, loss = 0.14358025
Iteration 17, loss = 0.12893665
Iteration 18, loss = 0.11611813
Iteration 19, loss = 0.10540021
Iteration 20, loss = 0.09601750
Iteration 21, loss = 0.08792103
Iteration 22, loss = 0.08093259
Iteration 23, loss = 0.07466730
Iteration 24, loss = 0.06949485
Iteration 25, loss = 0.06476419
Iteration 26, loss = 0.06064463
Iteration 27, loss = 0.05698293
Iteration 28, loss = 0.05369868
Iteration 29, loss = 0.05074330
Iteration 30, loss = 0.04810019
Iteration 31, loss = 0.04560615
Iteration 32, loss = 0.04345761
Iteration 33, loss = 0.04167464
Iteration 34, loss = 0.03985696
Iteration 35, loss = 0.03844224
Iteration 36, loss = 0.03698949
Iteration 37, loss = 0.03583493
Iteration 38, loss = 0.03462219
Iteration 39, loss = 0.03334312
Iteration 40, loss = 0.03229432
Iteration 41, loss = 0.03129995
Iteration 42, loss = 0.03034969
Iteration 43, loss = 0.02947464
Iteration 44, loss = 0.02867254
Iteration 45, loss = 0.02810489
Iteration 46, loss = 0.02753316
Iteration 47, loss = 0.02694945
Iteration 48, loss = 0.02632626
Iteration 49, loss = 0.02577221
Iteration 50, loss = 0.02554004
Iteration 51, loss = 0.02511731
Iteration 52, loss = 0.02469667
Iteration 53, loss = 0.02444350
Iteration 54, loss = 0.02401367
Iteration 55, loss = 0.02367732
Iteration 56, loss = 0.02297926
Iteration 57, loss = 0.02242985
Iteration 58, loss = 0.02206926
Iteration 59, loss = 0.02177789
Iteration 60, loss = 0.02145038
Iteration 61, loss = 0.02118917
Iteration 62, loss = 0.02085488
Iteration 63, loss = 0.02094940
Iteration 64, loss = 0.02079027
Iteration 65, loss = 0.02048364
Iteration 66, loss = 0.02030868
Iteration 67, loss = 0.02014535
Iteration 68, loss = 0.01980474
Iteration 69, loss = 0.01977293
Iteration 70, loss = 0.01951401
Iteration 71, loss = 0.01940500
Iteration 72, loss = 0.01917738
Iteration 73, loss = 0.01908752
Iteration 74, loss = 0.01885976
Iteration 75, loss = 0.01872036
Iteration 76, loss = 0.01854162
Iteration 77, loss = 0.01842615
Iteration 78, loss = 0.01821772
Iteration 79, loss = 0.01835678
Iteration 80, loss = 0.01832944
Iteration 81, loss = 0.01817459
Iteration 82, loss = 0.01791905
Iteration 83, loss = 0.01763800
Iteration 84, loss = 0.01751315
Iteration 85, loss = 0.01743403
Iteration 86, loss = 0.01726729
Iteration 87, loss = 0.01719359
Iteration 88, loss = 0.01702570
Iteration 89, loss = 0.01705502
Iteration 90, loss = 0.01705756
Iteration 91, loss = 0.01698292
Iteration 92, loss = 0.01693062
Iteration 93, loss = 0.01680364
Iteration 94, loss = 0.01673698
Iteration 95, loss = 0.01660554
Iteration 96, loss = 0.01657580
Iteration 97, loss = 0.01651223
Iteration 98, loss = 0.01649081
Iteration 99, loss = 0.01632322
Iteration 100, loss = 0.01646542
Iteration 101, loss = 0.01648043
Iteration 102, loss = 0.01630949
Iteration 103, loss = 0.01625565
Iteration 104, loss = 0.01620136
Iteration 105, loss = 0.01618217
Iteration 106, loss = 0.01603674
Iteration 107, loss = 0.01601436
Iteration 108, loss = 0.01594516
Iteration 109, loss = 0.01589418
Iteration 110, loss = 0.01598257
Iteration 111, loss = 0.01585246
Iteration 112, loss = 0.01619116
Iteration 113, loss = 0.01613543
Iteration 114, loss = 0.01625248
Iteration 115, loss = 0.01624873
Iteration 116, loss = 0.01605182
Iteration 117, loss = 0.01589065
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67092842
Iteration 2, loss = 0.65576550
Iteration 3, loss = 0.64252733
Iteration 4, loss = 0.62880151
Iteration 5, loss = 0.61455518
Iteration 6, loss = 0.59963599
Iteration 7, loss = 0.58408204
Iteration 8, loss = 0.56745038
Iteration 9, loss = 0.54976977
Iteration 10, loss = 0.53170321
Iteration 11, loss = 0.51237894
Iteration 12, loss = 0.49261494
Iteration 13, loss = 0.47215951
Iteration 14, loss = 0.45121862
Iteration 15, loss = 0.42990665
Iteration 16, loss = 0.40845506
Iteration 17, loss = 0.38705209
Iteration 18, loss = 0.36621471
Iteration 19, loss = 0.34569356
Iteration 20, loss = 0.32612797
Iteration 21, loss = 0.30723263
Iteration 22, loss = 0.28912988
Iteration 23, loss = 0.27210886
Iteration 24, loss = 0.25580680
Iteration 25, loss = 0.24040608
Iteration 26, loss = 0.22572751
Iteration 27, loss = 0.21212855
Iteration 28, loss = 0.19939781
Iteration 29, loss = 0.18732956
Iteration 30, loss = 0.17620617
Iteration 31, loss = 0.16567187
Iteration 32, loss = 0.15582805
Iteration 33, loss = 0.14684947
Iteration 34, loss = 0.13836817
Iteration 35, loss = 0.13048796
Iteration 36, loss = 0.12325663
Iteration 37, loss = 0.11648055
Iteration 38, loss = 0.11027295
Iteration 39, loss = 0.10439130
Iteration 40, loss = 0.09897955
Iteration 41, loss = 0.09397597
Iteration 42, loss = 0.08928269
Iteration 43, loss = 0.08494534
Iteration 44, loss = 0.08094523
Iteration 45, loss = 0.07720393
Iteration 46, loss = 0.07370965
Iteration 47, loss = 0.07045833
Iteration 48, loss = 0.06740397
Iteration 49, loss = 0.06454646
Iteration 50, loss = 0.06185755
Iteration 51, loss = 0.05939778
Iteration 52, loss = 0.05706201
Iteration 53, loss = 0.05486234
Iteration 54, loss = 0.05281947
Iteration 55, loss = 0.05094997
Iteration 56, loss = 0.04912545
Iteration 57, loss = 0.04745708
Iteration 58, loss = 0.04585240
Iteration 59, loss = 0.04435900
Iteration 60, loss = 0.04295235
Iteration 61, loss = 0.04159087
Iteration 62, loss = 0.04032032
Iteration 63, loss = 0.03911132
Iteration 64, loss = 0.03799124
Iteration 65, loss = 0.03688303
Iteration 66, loss = 0.03587331
Iteration 67, loss = 0.03490585
Iteration 68, loss = 0.03396587
Iteration 69, loss = 0.03306462
Iteration 70, loss = 0.03224516
Iteration 71, loss = 0.03142084
Iteration 72, loss = 0.03067099
Iteration 73, loss = 0.02994774
Iteration 74, loss = 0.02925032
Iteration 75, loss = 0.02857119
Iteration 76, loss = 0.02793392
Iteration 77, loss = 0.02732720
Iteration 78, loss = 0.02675006
Iteration 79, loss = 0.02621659
Iteration 80, loss = 0.02567474
Iteration 81, loss = 0.02514654
Iteration 82, loss = 0.02465666
Iteration 83, loss = 0.02419413
Iteration 84, loss = 0.02373258
Iteration 85, loss = 0.02329634
Iteration 86, loss = 0.02289135
Iteration 87, loss = 0.02245769
Iteration 88, loss = 0.02207616
Iteration 89, loss = 0.02171064
Iteration 90, loss = 0.02134719
Iteration 91, loss = 0.02101179
Iteration 92, loss = 0.02064328
Iteration 93, loss = 0.02035329
Iteration 94, loss = 0.02002057
Iteration 95, loss = 0.01970961
Iteration 96, loss = 0.01942116
Iteration 97, loss = 0.01916437
Iteration 98, loss = 0.01885738
Iteration 99, loss = 0.01859671
Iteration 100, loss = 0.01835597
Iteration 101, loss = 0.01809973
Iteration 102, loss = 0.01787334
Iteration 103, loss = 0.01765254
Iteration 104, loss = 0.01746177
Iteration 105, loss = 0.01726840
Iteration 106, loss = 0.01702961
Iteration 107, loss = 0.01684638
Iteration 108, loss = 0.01664717
Iteration 109, loss = 0.01643484
Iteration 110, loss = 0.01622639
Iteration 111, loss = 0.01605324
Iteration 112, loss = 0.01586272
Iteration 113, loss = 0.01570839
Iteration 114, loss = 0.01553640
Iteration 115, loss = 0.01538206
Iteration 116, loss = 0.01521107
Iteration 117, loss = 0.01508003
Iteration 118, loss = 0.01492075
Iteration 119, loss = 0.01479279
Iteration 120, loss = 0.01464216
Iteration 121, loss = 0.01451068
Iteration 122, loss = 0.01440163
Iteration 123, loss = 0.01431050
Iteration 124, loss = 0.01414998
Iteration 125, loss = 0.01399522
Iteration 126, loss = 0.01386729
Iteration 127, loss = 0.01375875
Iteration 128, loss = 0.01367836
Iteration 129, loss = 0.01351260
Iteration 130, loss = 0.01341722
Iteration 131, loss = 0.01329362
Iteration 132, loss = 0.01320186
Iteration 133, loss = 0.01309899
Iteration 134, loss = 0.01300591
Iteration 135, loss = 0.01291439
Iteration 136, loss = 0.01282418
Iteration 137, loss = 0.01273081
Iteration 138, loss = 0.01264570
Iteration 139, loss = 0.01256197
Iteration 140, loss = 0.01248329
Iteration 141, loss = 0.01238736
Iteration 142, loss = 0.01231977
Iteration 143, loss = 0.01225731
Iteration 144, loss = 0.01218524
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69789159
Iteration 2, loss = 0.68206652
Iteration 3, loss = 0.66648667
Iteration 4, loss = 0.64736810
Iteration 5, loss = 0.62372505
Iteration 6, loss = 0.59547555
Iteration 7, loss = 0.56352624
Iteration 8, loss = 0.52871659
Iteration 9, loss = 0.49237813
Iteration 10, loss = 0.45567969
Iteration 11, loss = 0.41927180
Iteration 12, loss = 0.38361073
Iteration 13, loss = 0.34967138
Iteration 14, loss = 0.31768566
Iteration 15, loss = 0.28801845
Iteration 16, loss = 0.26097298
Iteration 17, loss = 0.23629395
Iteration 18, loss = 0.21394611
Iteration 19, loss = 0.19407675
Iteration 20, loss = 0.17628439
Iteration 21, loss = 0.16054051
Iteration 22, loss = 0.14650820
Iteration 23, loss = 0.13411171
Iteration 24, loss = 0.12313363
Iteration 25, loss = 0.11342073
Iteration 26, loss = 0.10481831
Iteration 27, loss = 0.09718048
Iteration 28, loss = 0.09036786
Iteration 29, loss = 0.08428072
Iteration 30, loss = 0.07884474
Iteration 31, loss = 0.07401705
Iteration 32, loss = 0.06972321
Iteration 33, loss = 0.06568714
Iteration 34, loss = 0.06212029
Iteration 35, loss = 0.05892541
Iteration 36, loss = 0.05601968
Iteration 37, loss = 0.05331989
Iteration 38, loss = 0.05088999
Iteration 39, loss = 0.04864681
Iteration 40, loss = 0.04658788
Iteration 41, loss = 0.04473263
Iteration 42, loss = 0.04298746
Iteration 43, loss = 0.04140756
Iteration 44, loss = 0.03992437
Iteration 45, loss = 0.03855709
Iteration 46, loss = 0.03729778
Iteration 47, loss = 0.03610266
Iteration 48, loss = 0.03506428
Iteration 49, loss = 0.03394393
Iteration 50, loss = 0.03300005
Iteration 51, loss = 0.03212711
Iteration 52, loss = 0.03128119
Iteration 53, loss = 0.03051720
Iteration 54, loss = 0.02979080
Iteration 55, loss = 0.02905852
Iteration 56, loss = 0.02838276
Iteration 57, loss = 0.02774517
Iteration 58, loss = 0.02716904
Iteration 59, loss = 0.02660114
Iteration 60, loss = 0.02611053
Iteration 61, loss = 0.02565000
Iteration 62, loss = 0.02517268
Iteration 63, loss = 0.02467507
Iteration 64, loss = 0.02430050
Iteration 65, loss = 0.02386852
Iteration 66, loss = 0.02353491
Iteration 67, loss = 0.02314040
Iteration 68, loss = 0.02277015
Iteration 69, loss = 0.02245852
Iteration 70, loss = 0.02215239
Iteration 71, loss = 0.02180767
Iteration 72, loss = 0.02151713
Iteration 73, loss = 0.02125410
Iteration 74, loss = 0.02096626
Iteration 75, loss = 0.02073856
Iteration 76, loss = 0.02050562
Iteration 77, loss = 0.02028164
Iteration 78, loss = 0.02005432
Iteration 79, loss = 0.01984370
Iteration 80, loss = 0.01965553
Iteration 81, loss = 0.01947788
Iteration 82, loss = 0.01921314
Iteration 83, loss = 0.01906481
Iteration 84, loss = 0.01889580
Iteration 85, loss = 0.01872052
Iteration 86, loss = 0.01855716
Iteration 87, loss = 0.01841627
Iteration 88, loss = 0.01823943
Iteration 89, loss = 0.01811965
Iteration 90, loss = 0.01798429
Iteration 91, loss = 0.01785472
Iteration 92, loss = 0.01772776
Iteration 93, loss = 0.01762851
Iteration 94, loss = 0.01748879
Iteration 95, loss = 0.01737505
Iteration 96, loss = 0.01721934
Iteration 97, loss = 0.01712704
Iteration 98, loss = 0.01702979
Iteration 99, loss = 0.01689434
Iteration 100, loss = 0.01680808
Iteration 101, loss = 0.01669828
Iteration 102, loss = 0.01661197
Iteration 103, loss = 0.01651320
Iteration 104, loss = 0.01643168
Iteration 105, loss = 0.01638777
Iteration 106, loss = 0.01628502
Iteration 107, loss = 0.01628550
Iteration 108, loss = 0.01613440
Iteration 109, loss = 0.01602340
Iteration 110, loss = 0.01600487
Iteration 111, loss = 0.01590897
Iteration 112, loss = 0.01583297
Iteration 113, loss = 0.01579856
Iteration 114, loss = 0.01570018
Iteration 115, loss = 0.01560057
Iteration 116, loss = 0.01556295
Iteration 117, loss = 0.01549364
Iteration 118, loss = 0.01544037
Iteration 119, loss = 0.01543234
Iteration 120, loss = 0.01543796
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68109127
Iteration 2, loss = 0.63642842
Iteration 3, loss = 0.57196928
Iteration 4, loss = 0.49101732
Iteration 5, loss = 0.40776951
Iteration 6, loss = 0.33147248
Iteration 7, loss = 0.26694413
Iteration 8, loss = 0.21511547
Iteration 9, loss = 0.17498929
Iteration 10, loss = 0.14442662
Iteration 11, loss = 0.12079311
Iteration 12, loss = 0.10271211
Iteration 13, loss = 0.08889559
Iteration 14, loss = 0.07800891
Iteration 15, loss = 0.06910046
Iteration 16, loss = 0.06187889
Iteration 17, loss = 0.05631820
Iteration 18, loss = 0.05141022
Iteration 19, loss = 0.04754939
Iteration 20, loss = 0.04401983
Iteration 21, loss = 0.04129424
Iteration 22, loss = 0.03877744
Iteration 23, loss = 0.03661380
Iteration 24, loss = 0.03473738
Iteration 25, loss = 0.03310676
Iteration 26, loss = 0.03171655
Iteration 27, loss = 0.03061167
Iteration 28, loss = 0.02934717
Iteration 29, loss = 0.02850249
Iteration 30, loss = 0.02742104
Iteration 31, loss = 0.02673294
Iteration 32, loss = 0.02593271
Iteration 33, loss = 0.02515641
Iteration 34, loss = 0.02484374
Iteration 35, loss = 0.02420912
Iteration 36, loss = 0.02356231
Iteration 37, loss = 0.02302896
Iteration 38, loss = 0.02299102
Iteration 39, loss = 0.02233813
Iteration 40, loss = 0.02194911
Iteration 41, loss = 0.02167990
Iteration 42, loss = 0.02121477
Iteration 43, loss = 0.02110766
Iteration 44, loss = 0.02080851
Iteration 45, loss = 0.02052444
Iteration 46, loss = 0.02035294
Iteration 47, loss = 0.02023881
Iteration 48, loss = 0.01988466
Iteration 49, loss = 0.01982131
Iteration 50, loss = 0.01963306
Iteration 51, loss = 0.01942732
Iteration 52, loss = 0.01914951
Iteration 53, loss = 0.01902274
Iteration 54, loss = 0.01890512
Iteration 55, loss = 0.01879662
Iteration 56, loss = 0.01877052
Iteration 57, loss = 0.01855673
Iteration 58, loss = 0.01855844
Iteration 59, loss = 0.01838933
Iteration 60, loss = 0.01831051
Iteration 61, loss = 0.01813100
Iteration 62, loss = 0.01803857
Iteration 63, loss = 0.01801389
Iteration 64, loss = 0.01797962
Iteration 65, loss = 0.01766432
Iteration 66, loss = 0.01767006
Iteration 67, loss = 0.01775403
Iteration 68, loss = 0.01772400
Iteration 69, loss = 0.01772574
Iteration 70, loss = 0.01777046
Iteration 71, loss = 0.01740218
Iteration 72, loss = 0.01733883
Iteration 73, loss = 0.01740610
Iteration 74, loss = 0.01791437
Iteration 75, loss = 0.01731429
Iteration 76, loss = 0.01710959
Iteration 77, loss = 0.01699437
Iteration 78, loss = 0.01710174
Iteration 79, loss = 0.01716954
Iteration 80, loss = 0.01695036
Iteration 81, loss = 0.01688589
Iteration 82, loss = 0.01702831
Iteration 83, loss = 0.01690804
Iteration 84, loss = 0.01687561
Iteration 85, loss = 0.01688039
Iteration 86, loss = 0.01710437
Iteration 87, loss = 0.01668297
Iteration 88, loss = 0.01677156
Iteration 89, loss = 0.01666461
Iteration 90, loss = 0.01663272
Iteration 91, loss = 0.01675731
Iteration 92, loss = 0.01666479
Iteration 93, loss = 0.01668020
Iteration 94, loss = 0.01665622
Iteration 95, loss = 0.01670670
Iteration 96, loss = 0.01678159
Iteration 97, loss = 0.01659496
Iteration 98, loss = 0.01663444
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70695439
Iteration 2, loss = 0.68412423
Iteration 3, loss = 0.66628378
Iteration 4, loss = 0.64976309
Iteration 5, loss = 0.63328475
Iteration 6, loss = 0.61616338
Iteration 7, loss = 0.59775721
Iteration 8, loss = 0.57830798
Iteration 9, loss = 0.55776718
Iteration 10, loss = 0.53619521
Iteration 11, loss = 0.51382595
Iteration 12, loss = 0.49093661
Iteration 13, loss = 0.46779954
Iteration 14, loss = 0.44451151
Iteration 15, loss = 0.42145170
Iteration 16, loss = 0.39873126
Iteration 17, loss = 0.37647595
Iteration 18, loss = 0.35490809
Iteration 19, loss = 0.33404671
Iteration 20, loss = 0.31407497
Iteration 21, loss = 0.29503973
Iteration 22, loss = 0.27692671
Iteration 23, loss = 0.25986570
Iteration 24, loss = 0.24375335
Iteration 25, loss = 0.22862192
Iteration 26, loss = 0.21444316
Iteration 27, loss = 0.20113852
Iteration 28, loss = 0.18882411
Iteration 29, loss = 0.17730559
Iteration 30, loss = 0.16653796
Iteration 31, loss = 0.15666279
Iteration 32, loss = 0.14736802
Iteration 33, loss = 0.13877404
Iteration 34, loss = 0.13083414
Iteration 35, loss = 0.12341326
Iteration 36, loss = 0.11658933
Iteration 37, loss = 0.11021583
Iteration 38, loss = 0.10431831
Iteration 39, loss = 0.09884471
Iteration 40, loss = 0.09376006
Iteration 41, loss = 0.08905128
Iteration 42, loss = 0.08466973
Iteration 43, loss = 0.08058587
Iteration 44, loss = 0.07677844
Iteration 45, loss = 0.07327154
Iteration 46, loss = 0.06995242
Iteration 47, loss = 0.06683112
Iteration 48, loss = 0.06396317
Iteration 49, loss = 0.06130184
Iteration 50, loss = 0.05876782
Iteration 51, loss = 0.05640889
Iteration 52, loss = 0.05421865
Iteration 53, loss = 0.05214046
Iteration 54, loss = 0.05021333
Iteration 55, loss = 0.04835718
Iteration 56, loss = 0.04662900
Iteration 57, loss = 0.04497627
Iteration 58, loss = 0.04341620
Iteration 59, loss = 0.04191561
Iteration 60, loss = 0.04050658
Iteration 61, loss = 0.03919327
Iteration 62, loss = 0.03793571
Iteration 63, loss = 0.03673349
Iteration 64, loss = 0.03560840
Iteration 65, loss = 0.03456667
Iteration 66, loss = 0.03352726
Iteration 67, loss = 0.03255944
Iteration 68, loss = 0.03165646
Iteration 69, loss = 0.03075734
Iteration 70, loss = 0.02992415
Iteration 71, loss = 0.02911506
Iteration 72, loss = 0.02835994
Iteration 73, loss = 0.02767820
Iteration 74, loss = 0.02693607
Iteration 75, loss = 0.02628327
Iteration 76, loss = 0.02562616
Iteration 77, loss = 0.02501729
Iteration 78, loss = 0.02443687
Iteration 79, loss = 0.02388961
Iteration 80, loss = 0.02333141
Iteration 81, loss = 0.02280662
Iteration 82, loss = 0.02228999
Iteration 83, loss = 0.02182249
Iteration 84, loss = 0.02141049
Iteration 85, loss = 0.02092118
Iteration 86, loss = 0.02049539
Iteration 87, loss = 0.02008694
Iteration 88, loss = 0.01969065
Iteration 89, loss = 0.01931643
Iteration 90, loss = 0.01893280
Iteration 91, loss = 0.01861917
Iteration 92, loss = 0.01823854
Iteration 93, loss = 0.01791883
Iteration 94, loss = 0.01760302
Iteration 95, loss = 0.01730167
Iteration 96, loss = 0.01700817
Iteration 97, loss = 0.01671186
Iteration 98, loss = 0.01644953
Iteration 99, loss = 0.01616243
Iteration 100, loss = 0.01592093
Iteration 101, loss = 0.01566065
Iteration 102, loss = 0.01541714
Iteration 103, loss = 0.01518726
Iteration 104, loss = 0.01492874
Iteration 105, loss = 0.01471105
Iteration 106, loss = 0.01448625
Iteration 107, loss = 0.01427754
Iteration 108, loss = 0.01410281
Iteration 109, loss = 0.01387359
Iteration 110, loss = 0.01370195
Iteration 111, loss = 0.01350591
Iteration 112, loss = 0.01331844
Iteration 113, loss = 0.01315764
Iteration 114, loss = 0.01296541
Iteration 115, loss = 0.01282943
Iteration 116, loss = 0.01265104
Iteration 117, loss = 0.01251636
Iteration 118, loss = 0.01233381
Iteration 119, loss = 0.01218086
Iteration 120, loss = 0.01203488
Iteration 121, loss = 0.01190971
Iteration 122, loss = 0.01175422
Iteration 123, loss = 0.01161569
Iteration 124, loss = 0.01149797
Iteration 125, loss = 0.01135606
Iteration 126, loss = 0.01124073
Iteration 127, loss = 0.01111373
Iteration 128, loss = 0.01100213
Iteration 129, loss = 0.01090088
Iteration 130, loss = 0.01076949
Iteration 131, loss = 0.01067974
Iteration 132, loss = 0.01055481
Iteration 133, loss = 0.01045559
Iteration 134, loss = 0.01040293
Iteration 135, loss = 0.01025898
Iteration 136, loss = 0.01016618
Iteration 137, loss = 0.01006682
Iteration 138, loss = 0.00996616
Iteration 139, loss = 0.00991359
Iteration 140, loss = 0.00980234
Iteration 141, loss = 0.00970054
Iteration 142, loss = 0.00963116
Iteration 143, loss = 0.00954642
Iteration 144, loss = 0.00946425
Iteration 145, loss = 0.00938254
Iteration 146, loss = 0.00930463
Iteration 147, loss = 0.00921853
Iteration 148, loss = 0.00914163
Iteration 149, loss = 0.00906651
Iteration 150, loss = 0.00901030
Iteration 151, loss = 0.00894318
Iteration 152, loss = 0.00887050
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68085453
Iteration 2, loss = 0.60090064
Iteration 3, loss = 0.48984873
Iteration 4, loss = 0.37542128
Iteration 5, loss = 0.28010333
Iteration 6, loss = 0.20904506
Iteration 7, loss = 0.15953118
Iteration 8, loss = 0.12573682
Iteration 9, loss = 0.10200774
Iteration 10, loss = 0.08529896
Iteration 11, loss = 0.07316196
Iteration 12, loss = 0.06419655
Iteration 13, loss = 0.05693356
Iteration 14, loss = 0.05184482
Iteration 15, loss = 0.04728060
Iteration 16, loss = 0.04369703
Iteration 17, loss = 0.04073212
Iteration 18, loss = 0.03864513
Iteration 19, loss = 0.03634122
Iteration 20, loss = 0.03476611
Iteration 21, loss = 0.03328300
Iteration 22, loss = 0.03211185
Iteration 23, loss = 0.03111652
Iteration 24, loss = 0.03034301
Iteration 25, loss = 0.02961882
Iteration 26, loss = 0.02883131
Iteration 27, loss = 0.02799719
Iteration 28, loss = 0.02727991
Iteration 29, loss = 0.02695488
Iteration 30, loss = 0.02672707
Iteration 31, loss = 0.02617963
Iteration 32, loss = 0.02550806
Iteration 33, loss = 0.02519971
Iteration 34, loss = 0.02485545
Iteration 35, loss = 0.02490331
Iteration 36, loss = 0.02443634
Iteration 37, loss = 0.02451648
Iteration 38, loss = 0.02410653
Iteration 39, loss = 0.02386925
Iteration 40, loss = 0.02364119
Iteration 41, loss = 0.02354597
Iteration 42, loss = 0.02342783
Iteration 43, loss = 0.02324270
Iteration 44, loss = 0.02344572
Iteration 45, loss = 0.02305039
Iteration 46, loss = 0.02309935
Iteration 47, loss = 0.02269857
Iteration 48, loss = 0.02257462
Iteration 49, loss = 0.02250248
Iteration 50, loss = 0.02283049
Iteration 51, loss = 0.02232504
Iteration 52, loss = 0.02248344
Iteration 53, loss = 0.02221976
Iteration 54, loss = 0.02211289
Iteration 55, loss = 0.02203761
Iteration 56, loss = 0.02205243
Iteration 57, loss = 0.02209013
Iteration 58, loss = 0.02193295
Iteration 59, loss = 0.02178239
Iteration 60, loss = 0.02185985
Iteration 61, loss = 0.02204435
Iteration 62, loss = 0.02158437
Iteration 63, loss = 0.02235567
Iteration 64, loss = 0.02198037
Iteration 65, loss = 0.02196434
Iteration 66, loss = 0.02169208
Iteration 67, loss = 0.02151444
Iteration 68, loss = 0.02171371
Iteration 69, loss = 0.02149440
Iteration 70, loss = 0.02168730
Iteration 71, loss = 0.02176067
Iteration 72, loss = 0.02148978
Iteration 73, loss = 0.02151854
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71624999
Iteration 2, loss = 0.69225350
Iteration 3, loss = 0.66149408
Iteration 4, loss = 0.62121188
Iteration 5, loss = 0.57394026
Iteration 6, loss = 0.52305735
Iteration 7, loss = 0.47056488
Iteration 8, loss = 0.41899715
Iteration 9, loss = 0.36963928
Iteration 10, loss = 0.32405057
Iteration 11, loss = 0.28336454
Iteration 12, loss = 0.24757495
Iteration 13, loss = 0.21655473
Iteration 14, loss = 0.18995538
Iteration 15, loss = 0.16732258
Iteration 16, loss = 0.14816643
Iteration 17, loss = 0.13200206
Iteration 18, loss = 0.11818271
Iteration 19, loss = 0.10666928
Iteration 20, loss = 0.09677096
Iteration 21, loss = 0.08834164
Iteration 22, loss = 0.08113696
Iteration 23, loss = 0.07470255
Iteration 24, loss = 0.06921368
Iteration 25, loss = 0.06435056
Iteration 26, loss = 0.06010857
Iteration 27, loss = 0.05639552
Iteration 28, loss = 0.05307447
Iteration 29, loss = 0.05014390
Iteration 30, loss = 0.04743109
Iteration 31, loss = 0.04511087
Iteration 32, loss = 0.04299503
Iteration 33, loss = 0.04110988
Iteration 34, loss = 0.03936514
Iteration 35, loss = 0.03782571
Iteration 36, loss = 0.03643976
Iteration 37, loss = 0.03527682
Iteration 38, loss = 0.03399892
Iteration 39, loss = 0.03284594
Iteration 40, loss = 0.03184995
Iteration 41, loss = 0.03088002
Iteration 42, loss = 0.02999899
Iteration 43, loss = 0.02910620
Iteration 44, loss = 0.02838360
Iteration 45, loss = 0.02763789
Iteration 46, loss = 0.02702788
Iteration 47, loss = 0.02642579
Iteration 48, loss = 0.02591428
Iteration 49, loss = 0.02533615
Iteration 50, loss = 0.02500114
Iteration 51, loss = 0.02447201
Iteration 52, loss = 0.02397403
Iteration 53, loss = 0.02343806
Iteration 54, loss = 0.02315347
Iteration 55, loss = 0.02285649
Iteration 56, loss = 0.02251930
Iteration 57, loss = 0.02234047
Iteration 58, loss = 0.02208756
Iteration 59, loss = 0.02170358
Iteration 60, loss = 0.02131324
Iteration 61, loss = 0.02105773
Iteration 62, loss = 0.02087214
Iteration 63, loss = 0.02059175
Iteration 64, loss = 0.02035659
Iteration 65, loss = 0.02009790
Iteration 66, loss = 0.01986621
Iteration 67, loss = 0.01958756
Iteration 68, loss = 0.01946325
Iteration 69, loss = 0.01937724
Iteration 70, loss = 0.01945162
Iteration 71, loss = 0.01921994
Iteration 72, loss = 0.01898221
Iteration 73, loss = 0.01876856
Iteration 74, loss = 0.01854035
Iteration 75, loss = 0.01832847
Iteration 76, loss = 0.01824156
Iteration 77, loss = 0.01815275
Iteration 78, loss = 0.01807393
Iteration 79, loss = 0.01775814
Iteration 80, loss = 0.01776404
Iteration 81, loss = 0.01762872
Iteration 82, loss = 0.01748563
Iteration 83, loss = 0.01746965
Iteration 84, loss = 0.01745280
Iteration 85, loss = 0.01728640
Iteration 86, loss = 0.01728424
Iteration 87, loss = 0.01706577
Iteration 88, loss = 0.01708656
Iteration 89, loss = 0.01709724
Iteration 90, loss = 0.01720408
Iteration 91, loss = 0.01734997
Iteration 92, loss = 0.01711617
Iteration 93, loss = 0.01696738
Iteration 94, loss = 0.01668814
Iteration 95, loss = 0.01646917
Iteration 96, loss = 0.01639847
Iteration 97, loss = 0.01636737
Iteration 98, loss = 0.01650562
Iteration 99, loss = 0.01633517
Iteration 100, loss = 0.01655929
Iteration 101, loss = 0.01665762
Iteration 102, loss = 0.01631011
Iteration 103, loss = 0.01623075
Iteration 104, loss = 0.01597850
Iteration 105, loss = 0.01594877
Iteration 106, loss = 0.01579253
Iteration 107, loss = 0.01582346
Iteration 108, loss = 0.01568541
Iteration 109, loss = 0.01568857
Iteration 110, loss = 0.01583388
Iteration 111, loss = 0.01574249
Iteration 112, loss = 0.01571466
Iteration 113, loss = 0.01582214
Iteration 114, loss = 0.01554228
Iteration 115, loss = 0.01550389
Iteration 116, loss = 0.01556005
Iteration 117, loss = 0.01551975
Iteration 118, loss = 0.01558493
Iteration 119, loss = 0.01570437
Iteration 120, loss = 0.01533627
Iteration 121, loss = 0.01568407
Iteration 122, loss = 0.01542011
Iteration 123, loss = 0.01541095
Iteration 124, loss = 0.01566439
Iteration 125, loss = 0.01564349
Iteration 126, loss = 0.01552181
Iteration 127, loss = 0.01529124
Iteration 128, loss = 0.01525998
Iteration 129, loss = 0.01510027
Iteration 130, loss = 0.01502014
Iteration 131, loss = 0.01523733
Iteration 132, loss = 0.01530996
Iteration 133, loss = 0.01523688
Iteration 134, loss = 0.01524182
Iteration 135, loss = 0.01506113
Iteration 136, loss = 0.01500451
Iteration 137, loss = 0.01502408
Iteration 138, loss = 0.01491566
Iteration 139, loss = 0.01520677
Iteration 140, loss = 0.01504641
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71212264
Iteration 2, loss = 0.66061931
Iteration 3, loss = 0.58436935
Iteration 4, loss = 0.48609227
Iteration 5, loss = 0.38774962
Iteration 6, loss = 0.30351301
Iteration 7, loss = 0.23656461
Iteration 8, loss = 0.18709031
Iteration 9, loss = 0.15069913
Iteration 10, loss = 0.12450316
Iteration 11, loss = 0.10464974
Iteration 12, loss = 0.09011869
Iteration 13, loss = 0.07860658
Iteration 14, loss = 0.06973109
Iteration 15, loss = 0.06291337
Iteration 16, loss = 0.05768229
Iteration 17, loss = 0.05335917
Iteration 18, loss = 0.04931922
Iteration 19, loss = 0.04617634
Iteration 20, loss = 0.04381394
Iteration 21, loss = 0.04133974
Iteration 22, loss = 0.03932692
Iteration 23, loss = 0.03752341
Iteration 24, loss = 0.03601991
Iteration 25, loss = 0.03462639
Iteration 26, loss = 0.03338767
Iteration 27, loss = 0.03297613
Iteration 28, loss = 0.03191162
Iteration 29, loss = 0.03096893
Iteration 30, loss = 0.03018494
Iteration 31, loss = 0.02958988
Iteration 32, loss = 0.02883239
Iteration 33, loss = 0.02818231
Iteration 34, loss = 0.02789163
Iteration 35, loss = 0.02755892
Iteration 36, loss = 0.02688521
Iteration 37, loss = 0.02643707
Iteration 38, loss = 0.02639716
Iteration 39, loss = 0.02597039
Iteration 40, loss = 0.02547782
Iteration 41, loss = 0.02523459
Iteration 42, loss = 0.02536124
Iteration 43, loss = 0.02504626
Iteration 44, loss = 0.02478846
Iteration 45, loss = 0.02454514
Iteration 46, loss = 0.02425530
Iteration 47, loss = 0.02418606
Iteration 48, loss = 0.02416985
Iteration 49, loss = 0.02394693
Iteration 50, loss = 0.02367078
Iteration 51, loss = 0.02347658
Iteration 52, loss = 0.02337840
Iteration 53, loss = 0.02308198
Iteration 54, loss = 0.02337594
Iteration 55, loss = 0.02289735
Iteration 56, loss = 0.02270283
Iteration 57, loss = 0.02283316
Iteration 58, loss = 0.02300067
Iteration 59, loss = 0.02270048
Iteration 60, loss = 0.02247899
Iteration 61, loss = 0.02244447
Iteration 62, loss = 0.02264829
Iteration 63, loss = 0.02221866
Iteration 64, loss = 0.02229469
Iteration 65, loss = 0.02211216
Iteration 66, loss = 0.02201962
Iteration 67, loss = 0.02212866
Iteration 68, loss = 0.02194933
Iteration 69, loss = 0.02182634
Iteration 70, loss = 0.02198878
Iteration 71, loss = 0.02187504
Iteration 72, loss = 0.02181500
Iteration 73, loss = 0.02166695
Iteration 74, loss = 0.02134205
Iteration 75, loss = 0.02152107
Iteration 76, loss = 0.02140637
Iteration 77, loss = 0.02175882
Iteration 78, loss = 0.02138832
Iteration 79, loss = 0.02159932
Iteration 80, loss = 0.02140561
Iteration 81, loss = 0.02152389
Iteration 82, loss = 0.02114074
Iteration 83, loss = 0.02111208
Iteration 84, loss = 0.02122517
Iteration 85, loss = 0.02112262
Iteration 86, loss = 0.02087986
Iteration 87, loss = 0.02111760
Iteration 88, loss = 0.02107763
Iteration 89, loss = 0.02122155
Iteration 90, loss = 0.02093091
Iteration 91, loss = 0.02089939
Iteration 92, loss = 0.02095432
Iteration 93, loss = 0.02081838
Iteration 94, loss = 0.02095816
Iteration 95, loss = 0.02082337
Iteration 96, loss = 0.02098290
Iteration 97, loss = 0.02065726
Iteration 98, loss = 0.02075030
Iteration 99, loss = 0.02068589
Iteration 100, loss = 0.02060576
Iteration 101, loss = 0.02086926
Iteration 102, loss = 0.02057823
Iteration 103, loss = 0.02053383
Iteration 104, loss = 0.02075359
Iteration 105, loss = 0.02074735
Iteration 106, loss = 0.02057414
Iteration 107, loss = 0.02079426
Iteration 108, loss = 0.02050158
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68124770
Iteration 2, loss = 0.65923425
Iteration 3, loss = 0.63170240
Iteration 4, loss = 0.59362802
Iteration 5, loss = 0.54667876
Iteration 6, loss = 0.49485490
Iteration 7, loss = 0.44311518
Iteration 8, loss = 0.39310387
Iteration 9, loss = 0.34633012
Iteration 10, loss = 0.30385671
Iteration 11, loss = 0.26578545
Iteration 12, loss = 0.23249590
Iteration 13, loss = 0.20351344
Iteration 14, loss = 0.17870232
Iteration 15, loss = 0.15771326
Iteration 16, loss = 0.13974879
Iteration 17, loss = 0.12457923
Iteration 18, loss = 0.11180130
Iteration 19, loss = 0.10095249
Iteration 20, loss = 0.09163046
Iteration 21, loss = 0.08363274
Iteration 22, loss = 0.07687190
Iteration 23, loss = 0.07092659
Iteration 24, loss = 0.06570375
Iteration 25, loss = 0.06113746
Iteration 26, loss = 0.05710653
Iteration 27, loss = 0.05359828
Iteration 28, loss = 0.05042917
Iteration 29, loss = 0.04765210
Iteration 30, loss = 0.04514945
Iteration 31, loss = 0.04303002
Iteration 32, loss = 0.04091451
Iteration 33, loss = 0.03914344
Iteration 34, loss = 0.03751757
Iteration 35, loss = 0.03595588
Iteration 36, loss = 0.03454661
Iteration 37, loss = 0.03329353
Iteration 38, loss = 0.03218500
Iteration 39, loss = 0.03109643
Iteration 40, loss = 0.03020345
Iteration 41, loss = 0.02911522
Iteration 42, loss = 0.02869084
Iteration 43, loss = 0.02791581
Iteration 44, loss = 0.02718559
Iteration 45, loss = 0.02679304
Iteration 46, loss = 0.02617899
Iteration 47, loss = 0.02532578
Iteration 48, loss = 0.02473294
Iteration 49, loss = 0.02417083
Iteration 50, loss = 0.02369089
Iteration 51, loss = 0.02345245
Iteration 52, loss = 0.02299474
Iteration 53, loss = 0.02260149
Iteration 54, loss = 0.02222248
Iteration 55, loss = 0.02180026
Iteration 56, loss = 0.02143811
Iteration 57, loss = 0.02107072
Iteration 58, loss = 0.02073067
Iteration 59, loss = 0.02056676
Iteration 60, loss = 0.02038797
Iteration 61, loss = 0.02021488
Iteration 62, loss = 0.01995262
Iteration 63, loss = 0.01991113
Iteration 64, loss = 0.01966191
Iteration 65, loss = 0.01917279
Iteration 66, loss = 0.01895455
Iteration 67, loss = 0.01901217
Iteration 68, loss = 0.01897170
Iteration 69, loss = 0.01872731
Iteration 70, loss = 0.01834807
Iteration 71, loss = 0.01816500
Iteration 72, loss = 0.01798098
Iteration 73, loss = 0.01783069
Iteration 74, loss = 0.01776630
Iteration 75, loss = 0.01761815
Iteration 76, loss = 0.01746485
Iteration 77, loss = 0.01730902
Iteration 78, loss = 0.01731056
Iteration 79, loss = 0.01723842
Iteration 80, loss = 0.01724501
Iteration 81, loss = 0.01713203
Iteration 82, loss = 0.01698389
Iteration 83, loss = 0.01712815
Iteration 84, loss = 0.01701851
Iteration 85, loss = 0.01682146
Iteration 86, loss = 0.01664590
Iteration 87, loss = 0.01659142
Iteration 88, loss = 0.01641655
Iteration 89, loss = 0.01624627
Iteration 90, loss = 0.01614470
Iteration 91, loss = 0.01607156
Iteration 92, loss = 0.01595649
Iteration 93, loss = 0.01589831
Iteration 94, loss = 0.01584395
Iteration 95, loss = 0.01576946
Iteration 96, loss = 0.01577337
Iteration 97, loss = 0.01575736
Iteration 98, loss = 0.01561012
Iteration 99, loss = 0.01557511
Iteration 100, loss = 0.01556978
Iteration 101, loss = 0.01548394
Iteration 102, loss = 0.01540398
Iteration 103, loss = 0.01537220
Iteration 104, loss = 0.01535468
Iteration 105, loss = 0.01543025
Iteration 106, loss = 0.01546163
Iteration 107, loss = 0.01547352
Iteration 108, loss = 0.01527393
Iteration 109, loss = 0.01535613
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69994232
Iteration 2, loss = 0.60701587
Iteration 3, loss = 0.49501161
Iteration 4, loss = 0.38299776
Iteration 5, loss = 0.28641744
Iteration 6, loss = 0.21371749
Iteration 7, loss = 0.16244089
Iteration 8, loss = 0.12769724
Iteration 9, loss = 0.10296414
Iteration 10, loss = 0.08580396
Iteration 11, loss = 0.07350701
Iteration 12, loss = 0.06402428
Iteration 13, loss = 0.05711954
Iteration 14, loss = 0.05151335
Iteration 15, loss = 0.04681928
Iteration 16, loss = 0.04354080
Iteration 17, loss = 0.04052807
Iteration 18, loss = 0.03845656
Iteration 19, loss = 0.03637500
Iteration 20, loss = 0.03495915
Iteration 21, loss = 0.03309417
Iteration 22, loss = 0.03207678
Iteration 23, loss = 0.03084994
Iteration 24, loss = 0.02995661
Iteration 25, loss = 0.02943174
Iteration 26, loss = 0.02861159
Iteration 27, loss = 0.02778224
Iteration 28, loss = 0.02712870
Iteration 29, loss = 0.02703973
Iteration 30, loss = 0.02650434
Iteration 31, loss = 0.02619391
Iteration 32, loss = 0.02564863
Iteration 33, loss = 0.02530488
Iteration 34, loss = 0.02505501
Iteration 35, loss = 0.02451877
Iteration 36, loss = 0.02451552
Iteration 37, loss = 0.02404117
Iteration 38, loss = 0.02412006
Iteration 39, loss = 0.02364566
Iteration 40, loss = 0.02351244
Iteration 41, loss = 0.02341405
Iteration 42, loss = 0.02353839
Iteration 43, loss = 0.02329950
Iteration 44, loss = 0.02314066
Iteration 45, loss = 0.02295594
Iteration 46, loss = 0.02265615
Iteration 47, loss = 0.02263300
Iteration 48, loss = 0.02256298
Iteration 49, loss = 0.02258043
Iteration 50, loss = 0.02222654
Iteration 51, loss = 0.02230388
Iteration 52, loss = 0.02223651
Iteration 53, loss = 0.02235384
Iteration 54, loss = 0.02210347
Iteration 55, loss = 0.02205916
Iteration 56, loss = 0.02223306
Iteration 57, loss = 0.02194943
Iteration 58, loss = 0.02180135
Iteration 59, loss = 0.02206485
Iteration 60, loss = 0.02173300
Iteration 61, loss = 0.02198070
Iteration 62, loss = 0.02164652
Iteration 63, loss = 0.02163887
Iteration 64, loss = 0.02181576
Iteration 65, loss = 0.02155270
Iteration 66, loss = 0.02142081
Iteration 67, loss = 0.02145912
Iteration 68, loss = 0.02163980
Iteration 69, loss = 0.02142665
Iteration 70, loss = 0.02151269
Iteration 71, loss = 0.02145865
Iteration 72, loss = 0.02151875
Iteration 73, loss = 0.02139639
Iteration 74, loss = 0.02112415
Iteration 75, loss = 0.02126678
Iteration 76, loss = 0.02120839
Iteration 77, loss = 0.02129017
Iteration 78, loss = 0.02116609
Iteration 79, loss = 0.02153889
Iteration 80, loss = 0.02116373
Iteration 81, loss = 0.02095630
Iteration 82, loss = 0.02131622
Iteration 83, loss = 0.02102652
Iteration 84, loss = 0.02127879
Iteration 85, loss = 0.02080089
Iteration 86, loss = 0.02090509
Iteration 87, loss = 0.02099829
Iteration 88, loss = 0.02113009
Iteration 89, loss = 0.02104805
Iteration 90, loss = 0.02125836
Iteration 91, loss = 0.02083993
Iteration 92, loss = 0.02105230
Iteration 93, loss = 0.02085945
Iteration 94, loss = 0.02085903
Iteration 95, loss = 0.02054661
Iteration 96, loss = 0.02085149
Iteration 97, loss = 0.02098292
Iteration 98, loss = 0.02086264
Iteration 99, loss = 0.02081806
Iteration 100, loss = 0.02069322
Iteration 101, loss = 0.02050567
Iteration 102, loss = 0.02068185
Iteration 103, loss = 0.02090047
Iteration 104, loss = 0.02054316
Iteration 105, loss = 0.02057187
Iteration 106, loss = 0.02089399
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68011011
Iteration 2, loss = 0.64374587
Iteration 3, loss = 0.59755041
Iteration 4, loss = 0.53842753
Iteration 5, loss = 0.47320066
Iteration 6, loss = 0.40923358
Iteration 7, loss = 0.34991354
Iteration 8, loss = 0.29721002
Iteration 9, loss = 0.25143601
Iteration 10, loss = 0.21270633
Iteration 11, loss = 0.18057647
Iteration 12, loss = 0.15443862
Iteration 13, loss = 0.13293501
Iteration 14, loss = 0.11536159
Iteration 15, loss = 0.10113119
Iteration 16, loss = 0.08912744
Iteration 17, loss = 0.07936455
Iteration 18, loss = 0.07126398
Iteration 19, loss = 0.06439963
Iteration 20, loss = 0.05859570
Iteration 21, loss = 0.05364135
Iteration 22, loss = 0.04936853
Iteration 23, loss = 0.04583559
Iteration 24, loss = 0.04257558
Iteration 25, loss = 0.03992028
Iteration 26, loss = 0.03736042
Iteration 27, loss = 0.03521002
Iteration 28, loss = 0.03339346
Iteration 29, loss = 0.03156686
Iteration 30, loss = 0.03006933
Iteration 31, loss = 0.02879394
Iteration 32, loss = 0.02759347
Iteration 33, loss = 0.02648867
Iteration 34, loss = 0.02544228
Iteration 35, loss = 0.02460721
Iteration 36, loss = 0.02364151
Iteration 37, loss = 0.02293695
Iteration 38, loss = 0.02225101
Iteration 39, loss = 0.02153424
Iteration 40, loss = 0.02094464
Iteration 41, loss = 0.02038162
Iteration 42, loss = 0.02001288
Iteration 43, loss = 0.01946573
Iteration 44, loss = 0.01899033
Iteration 45, loss = 0.01862494
Iteration 46, loss = 0.01837648
Iteration 47, loss = 0.01787478
Iteration 48, loss = 0.01752720
Iteration 49, loss = 0.01727052
Iteration 50, loss = 0.01689455
Iteration 51, loss = 0.01670323
Iteration 52, loss = 0.01649438
Iteration 53, loss = 0.01623513
Iteration 54, loss = 0.01604487
Iteration 55, loss = 0.01598808
Iteration 56, loss = 0.01558467
Iteration 57, loss = 0.01544650
Iteration 58, loss = 0.01528181
Iteration 59, loss = 0.01528035
Iteration 60, loss = 0.01496658
Iteration 61, loss = 0.01481391
Iteration 62, loss = 0.01463942
Iteration 63, loss = 0.01456069
Iteration 64, loss = 0.01439883
Iteration 65, loss = 0.01441351
Iteration 66, loss = 0.01414124
Iteration 67, loss = 0.01404210
Iteration 68, loss = 0.01390556
Iteration 69, loss = 0.01399452
Iteration 70, loss = 0.01374423
Iteration 71, loss = 0.01369075
Iteration 72, loss = 0.01350591
Iteration 73, loss = 0.01354914
Iteration 74, loss = 0.01345246
Iteration 75, loss = 0.01332543
Iteration 76, loss = 0.01336190
Iteration 77, loss = 0.01314989
Iteration 78, loss = 0.01305324
Iteration 79, loss = 0.01311777
Iteration 80, loss = 0.01302214
Iteration 81, loss = 0.01289889
Iteration 82, loss = 0.01288121
Iteration 83, loss = 0.01277275
Iteration 84, loss = 0.01274902
Iteration 85, loss = 0.01275215
Iteration 86, loss = 0.01258175
Iteration 87, loss = 0.01263032
Iteration 88, loss = 0.01253536
Iteration 89, loss = 0.01260277
Iteration 90, loss = 0.01252445
Iteration 91, loss = 0.01248932
Iteration 92, loss = 0.01244914
Iteration 93, loss = 0.01243843
Iteration 94, loss = 0.01229170
Iteration 95, loss = 0.01223826
Iteration 96, loss = 0.01230302
Iteration 97, loss = 0.01217856
Iteration 98, loss = 0.01211821
Iteration 99, loss = 0.01225002
Iteration 100, loss = 0.01227335
Iteration 101, loss = 0.01209567
Iteration 102, loss = 0.01215477
Iteration 103, loss = 0.01192978
Iteration 104, loss = 0.01198072
Iteration 105, loss = 0.01199658
Iteration 106, loss = 0.01195822
Iteration 107, loss = 0.01192595
Iteration 108, loss = 0.01192143
Iteration 109, loss = 0.01200387
Iteration 110, loss = 0.01185636
Iteration 111, loss = 0.01184781
Iteration 112, loss = 0.01178583
Iteration 113, loss = 0.01181601
Iteration 114, loss = 0.01174834
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68697244
Iteration 2, loss = 0.66167447
Iteration 3, loss = 0.63268072
Iteration 4, loss = 0.59728685
Iteration 5, loss = 0.55519169
Iteration 6, loss = 0.50762857
Iteration 7, loss = 0.45808489
Iteration 8, loss = 0.40920650
Iteration 9, loss = 0.36272580
Iteration 10, loss = 0.31973155
Iteration 11, loss = 0.28085351
Iteration 12, loss = 0.24635170
Iteration 13, loss = 0.21606139
Iteration 14, loss = 0.19010726
Iteration 15, loss = 0.16794688
Iteration 16, loss = 0.14899703
Iteration 17, loss = 0.13285276
Iteration 18, loss = 0.11908696
Iteration 19, loss = 0.10738791
Iteration 20, loss = 0.09736644
Iteration 21, loss = 0.08867096
Iteration 22, loss = 0.08114924
Iteration 23, loss = 0.07470883
Iteration 24, loss = 0.06901146
Iteration 25, loss = 0.06406731
Iteration 26, loss = 0.05967689
Iteration 27, loss = 0.05582421
Iteration 28, loss = 0.05234558
Iteration 29, loss = 0.04938064
Iteration 30, loss = 0.04663970
Iteration 31, loss = 0.04425683
Iteration 32, loss = 0.04208992
Iteration 33, loss = 0.03999133
Iteration 34, loss = 0.03822700
Iteration 35, loss = 0.03658606
Iteration 36, loss = 0.03516772
Iteration 37, loss = 0.03387951
Iteration 38, loss = 0.03261383
Iteration 39, loss = 0.03140236
Iteration 40, loss = 0.03015903
Iteration 41, loss = 0.02930393
Iteration 42, loss = 0.02849454
Iteration 43, loss = 0.02777297
Iteration 44, loss = 0.02710640
Iteration 45, loss = 0.02638603
Iteration 46, loss = 0.02569875
Iteration 47, loss = 0.02510690
Iteration 48, loss = 0.02449144
Iteration 49, loss = 0.02391243
Iteration 50, loss = 0.02327395
Iteration 51, loss = 0.02278583
Iteration 52, loss = 0.02238978
Iteration 53, loss = 0.02187334
Iteration 54, loss = 0.02155909
Iteration 55, loss = 0.02126400
Iteration 56, loss = 0.02085783
Iteration 57, loss = 0.02051133
Iteration 58, loss = 0.02026704
Iteration 59, loss = 0.02041231
Iteration 60, loss = 0.02028035
Iteration 61, loss = 0.01980883
Iteration 62, loss = 0.01931726
Iteration 63, loss = 0.01887393
Iteration 64, loss = 0.01861152
Iteration 65, loss = 0.01839454
Iteration 66, loss = 0.01827451
Iteration 67, loss = 0.01808163
Iteration 68, loss = 0.01791727
Iteration 69, loss = 0.01771130
Iteration 70, loss = 0.01747309
Iteration 71, loss = 0.01737142
Iteration 72, loss = 0.01723468
Iteration 73, loss = 0.01715721
Iteration 74, loss = 0.01700649
Iteration 75, loss = 0.01677545
Iteration 76, loss = 0.01638382
Iteration 77, loss = 0.01644304
Iteration 78, loss = 0.01641909
Iteration 79, loss = 0.01610271
Iteration 80, loss = 0.01625357
Iteration 81, loss = 0.01619574
Iteration 82, loss = 0.01598247
Iteration 83, loss = 0.01596652
Iteration 84, loss = 0.01578297
Iteration 85, loss = 0.01566730
Iteration 86, loss = 0.01556361
Iteration 87, loss = 0.01537350
Iteration 88, loss = 0.01537524
Iteration 89, loss = 0.01519899
Iteration 90, loss = 0.01518257
Iteration 91, loss = 0.01517918
Iteration 92, loss = 0.01510346
Iteration 93, loss = 0.01498591
Iteration 94, loss = 0.01487959
Iteration 95, loss = 0.01483584
Iteration 96, loss = 0.01474241
Iteration 97, loss = 0.01464353
Iteration 98, loss = 0.01471921
Iteration 99, loss = 0.01464684
Iteration 100, loss = 0.01457190
Iteration 101, loss = 0.01443596
Iteration 102, loss = 0.01444169
Iteration 103, loss = 0.01407238
Iteration 104, loss = 0.01457749
Iteration 105, loss = 0.01465373
Iteration 106, loss = 0.01456257
Iteration 107, loss = 0.01445891
Iteration 108, loss = 0.01444999
Iteration 109, loss = 0.01429792
Iteration 110, loss = 0.01445550
Iteration 111, loss = 0.01430673
Iteration 112, loss = 0.01417124
Iteration 113, loss = 0.01411056
Iteration 114, loss = 0.01396136
Iteration 115, loss = 0.01368749
Iteration 116, loss = 0.01381994
Iteration 117, loss = 0.01383180
Iteration 118, loss = 0.01370019
Iteration 119, loss = 0.01362421
Iteration 120, loss = 0.01367755
Iteration 121, loss = 0.01375105
Iteration 122, loss = 0.01390683
Iteration 123, loss = 0.01390344
Iteration 124, loss = 0.01381517
Iteration 125, loss = 0.01363547
Iteration 126, loss = 0.01353394
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67256910
Iteration 2, loss = 0.59063012
Iteration 3, loss = 0.48231708
Iteration 4, loss = 0.37588012
Iteration 5, loss = 0.28570096
Iteration 6, loss = 0.21654337
Iteration 7, loss = 0.16710592
Iteration 8, loss = 0.13203574
Iteration 9, loss = 0.10740509
Iteration 10, loss = 0.08966266
Iteration 11, loss = 0.07634448
Iteration 12, loss = 0.06669164
Iteration 13, loss = 0.05907358
Iteration 14, loss = 0.05312792
Iteration 15, loss = 0.04842448
Iteration 16, loss = 0.04435735
Iteration 17, loss = 0.04114426
Iteration 18, loss = 0.03862703
Iteration 19, loss = 0.03665947
Iteration 20, loss = 0.03462917
Iteration 21, loss = 0.03302672
Iteration 22, loss = 0.03180518
Iteration 23, loss = 0.03056395
Iteration 24, loss = 0.02953979
Iteration 25, loss = 0.02854576
Iteration 26, loss = 0.02781447
Iteration 27, loss = 0.02717808
Iteration 28, loss = 0.02661182
Iteration 29, loss = 0.02592722
Iteration 30, loss = 0.02566214
Iteration 31, loss = 0.02503218
Iteration 32, loss = 0.02472445
Iteration 33, loss = 0.02437618
Iteration 34, loss = 0.02406211
Iteration 35, loss = 0.02363255
Iteration 36, loss = 0.02347549
Iteration 37, loss = 0.02328026
Iteration 38, loss = 0.02298184
Iteration 39, loss = 0.02281727
Iteration 40, loss = 0.02247767
Iteration 41, loss = 0.02239901
Iteration 42, loss = 0.02212843
Iteration 43, loss = 0.02193394
Iteration 44, loss = 0.02205740
Iteration 45, loss = 0.02217314
Iteration 46, loss = 0.02147538
Iteration 47, loss = 0.02156736
Iteration 48, loss = 0.02148019
Iteration 49, loss = 0.02143169
Iteration 50, loss = 0.02106672
Iteration 51, loss = 0.02103678
Iteration 52, loss = 0.02093200
Iteration 53, loss = 0.02098110
Iteration 54, loss = 0.02075893
Iteration 55, loss = 0.02071786
Iteration 56, loss = 0.02078322
Iteration 57, loss = 0.02097232
Iteration 58, loss = 0.02055219
Iteration 59, loss = 0.02042059
Iteration 60, loss = 0.02066246
Iteration 61, loss = 0.02060278
Iteration 62, loss = 0.02069894
Iteration 63, loss = 0.02022010
Iteration 64, loss = 0.02036729
Iteration 65, loss = 0.02011175
Iteration 66, loss = 0.02017796
Iteration 67, loss = 0.02036174
Iteration 68, loss = 0.01991246
Iteration 69, loss = 0.02009355
Iteration 70, loss = 0.01996994
Iteration 71, loss = 0.01993052
Iteration 72, loss = 0.01997105
Iteration 73, loss = 0.01981936
Iteration 74, loss = 0.01988780
Iteration 75, loss = 0.01992425
Iteration 76, loss = 0.01956923
Iteration 77, loss = 0.01976082
Iteration 78, loss = 0.01983002
Iteration 79, loss = 0.01986283
Iteration 80, loss = 0.01985136
Iteration 81, loss = 0.01972432
Iteration 82, loss = 0.01993304
Iteration 83, loss = 0.01998786
Iteration 84, loss = 0.01981566
Iteration 85, loss = 0.01961218
Iteration 86, loss = 0.01949459
Iteration 87, loss = 0.01970731
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67852914
Iteration 2, loss = 0.62798394
Iteration 3, loss = 0.55938124
Iteration 4, loss = 0.47716272
Iteration 5, loss = 0.39571198
Iteration 6, loss = 0.32259776
Iteration 7, loss = 0.26111311
Iteration 8, loss = 0.21109791
Iteration 9, loss = 0.17253120
Iteration 10, loss = 0.14261851
Iteration 11, loss = 0.12004227
Iteration 12, loss = 0.10236665
Iteration 13, loss = 0.08878463
Iteration 14, loss = 0.07807875
Iteration 15, loss = 0.06947109
Iteration 16, loss = 0.06261451
Iteration 17, loss = 0.05686966
Iteration 18, loss = 0.05217185
Iteration 19, loss = 0.04849322
Iteration 20, loss = 0.04500420
Iteration 21, loss = 0.04213485
Iteration 22, loss = 0.03991204
Iteration 23, loss = 0.03761286
Iteration 24, loss = 0.03574896
Iteration 25, loss = 0.03410396
Iteration 26, loss = 0.03274329
Iteration 27, loss = 0.03154011
Iteration 28, loss = 0.03043757
Iteration 29, loss = 0.02951142
Iteration 30, loss = 0.02866354
Iteration 31, loss = 0.02788750
Iteration 32, loss = 0.02704857
Iteration 33, loss = 0.02637565
Iteration 34, loss = 0.02600472
Iteration 35, loss = 0.02521065
Iteration 36, loss = 0.02486606
Iteration 37, loss = 0.02438607
Iteration 38, loss = 0.02413156
Iteration 39, loss = 0.02362550
Iteration 40, loss = 0.02339331
Iteration 41, loss = 0.02298886
Iteration 42, loss = 0.02257112
Iteration 43, loss = 0.02237671
Iteration 44, loss = 0.02204427
Iteration 45, loss = 0.02180016
Iteration 46, loss = 0.02164652
Iteration 47, loss = 0.02144534
Iteration 48, loss = 0.02112690
Iteration 49, loss = 0.02105071
Iteration 50, loss = 0.02076597
Iteration 51, loss = 0.02043347
Iteration 52, loss = 0.02040860
Iteration 53, loss = 0.02013195
Iteration 54, loss = 0.02012906
Iteration 55, loss = 0.02025903
Iteration 56, loss = 0.01993239
Iteration 57, loss = 0.01989052
Iteration 58, loss = 0.01973655
Iteration 59, loss = 0.01956790
Iteration 60, loss = 0.01955169
Iteration 61, loss = 0.01943545
Iteration 62, loss = 0.01913950
Iteration 63, loss = 0.01958052
Iteration 64, loss = 0.01917455
Iteration 65, loss = 0.01922591
Iteration 66, loss = 0.01900817
Iteration 67, loss = 0.01884693
Iteration 68, loss = 0.01868673
Iteration 69, loss = 0.01882208
Iteration 70, loss = 0.01885152
Iteration 71, loss = 0.01877428
Iteration 72, loss = 0.01868543
Iteration 73, loss = 0.01909379
Iteration 74, loss = 0.01857510
Iteration 75, loss = 0.01837786
Iteration 76, loss = 0.01878129
Iteration 77, loss = 0.01818933
Iteration 78, loss = 0.01827373
Iteration 79, loss = 0.01847233
Iteration 80, loss = 0.01872104
Iteration 81, loss = 0.01830510
Iteration 82, loss = 0.01812208
Iteration 83, loss = 0.01818586
Iteration 84, loss = 0.01816582
Iteration 85, loss = 0.01819093
Iteration 86, loss = 0.01855273
Iteration 87, loss = 0.01826469
Iteration 88, loss = 0.01794076
Iteration 89, loss = 0.01818201
Iteration 90, loss = 0.01799572
Iteration 91, loss = 0.01802113
Iteration 92, loss = 0.01808154
Iteration 93, loss = 0.01772621
Iteration 94, loss = 0.01783777
Iteration 95, loss = 0.01795283
Iteration 96, loss = 0.01786256
Iteration 97, loss = 0.01799315
Iteration 98, loss = 0.01781342
Iteration 99, loss = 0.01775691
Iteration 100, loss = 0.01782295
Iteration 101, loss = 0.01782053
Iteration 102, loss = 0.01768375
Iteration 103, loss = 0.01771413
Iteration 104, loss = 0.01772238
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67406157
Iteration 2, loss = 0.59788144
Iteration 3, loss = 0.47623395
Iteration 4, loss = 0.35872600
Iteration 5, loss = 0.26523583
Iteration 6, loss = 0.19816160
Iteration 7, loss = 0.15166537
Iteration 8, loss = 0.12017883
Iteration 9, loss = 0.09821760
Iteration 10, loss = 0.08235720
Iteration 11, loss = 0.07092722
Iteration 12, loss = 0.06224070
Iteration 13, loss = 0.05561064
Iteration 14, loss = 0.05018731
Iteration 15, loss = 0.04628257
Iteration 16, loss = 0.04272314
Iteration 17, loss = 0.04003200
Iteration 18, loss = 0.03770462
Iteration 19, loss = 0.03574038
Iteration 20, loss = 0.03388707
Iteration 21, loss = 0.03260894
Iteration 22, loss = 0.03139366
Iteration 23, loss = 0.03054510
Iteration 24, loss = 0.02960239
Iteration 25, loss = 0.02879266
Iteration 26, loss = 0.02798963
Iteration 27, loss = 0.02734894
Iteration 28, loss = 0.02671935
Iteration 29, loss = 0.02640091
Iteration 30, loss = 0.02600851
Iteration 31, loss = 0.02559258
Iteration 32, loss = 0.02512577
Iteration 33, loss = 0.02497728
Iteration 34, loss = 0.02468924
Iteration 35, loss = 0.02426539
Iteration 36, loss = 0.02398786
Iteration 37, loss = 0.02373423
Iteration 38, loss = 0.02387839
Iteration 39, loss = 0.02360440
Iteration 40, loss = 0.02343906
Iteration 41, loss = 0.02301111
Iteration 42, loss = 0.02303025
Iteration 43, loss = 0.02276123
Iteration 44, loss = 0.02280534
Iteration 45, loss = 0.02285863
Iteration 46, loss = 0.02246489
Iteration 47, loss = 0.02225166
Iteration 48, loss = 0.02215328
Iteration 49, loss = 0.02220910
Iteration 50, loss = 0.02210944
Iteration 51, loss = 0.02207865
Iteration 52, loss = 0.02197388
Iteration 53, loss = 0.02178324
Iteration 54, loss = 0.02194780
Iteration 55, loss = 0.02144980
Iteration 56, loss = 0.02182050
Iteration 57, loss = 0.02150556
Iteration 58, loss = 0.02157585
Iteration 59, loss = 0.02142173
Iteration 60, loss = 0.02161150
Iteration 61, loss = 0.02145608
Iteration 62, loss = 0.02141143
Iteration 63, loss = 0.02119898
Iteration 64, loss = 0.02117413
Iteration 65, loss = 0.02124835
Iteration 66, loss = 0.02131298
Iteration 67, loss = 0.02143062
Iteration 68, loss = 0.02138641
Iteration 69, loss = 0.02110191
Iteration 70, loss = 0.02128655
Iteration 71, loss = 0.02122542
Iteration 72, loss = 0.02108529
Iteration 73, loss = 0.02116287
Iteration 74, loss = 0.02108075
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	108m46.531s
user	743m44.178s
sys	85m5.829s
Elapsed time: 6527 seconds
