Processing dataset: dropped_v3_lemmatized

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: dropped_v3_lemmatized (7428 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on dropped_v3_lemmatized...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on dropped_v3_lemmatized...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on dropped_v3_lemmatized...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on dropped_v3_lemmatized...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on dropped_v3_lemmatized...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.68359338
Iteration 2, loss = 0.60702714
Iteration 3, loss = 0.49582773
Iteration 4, loss = 0.38327770
Iteration 5, loss = 0.28806132
Iteration 6, loss = 0.21679715
Iteration 7, loss = 0.16649194
Iteration 8, loss = 0.13191588
Iteration 9, loss = 0.10787687
Iteration 10, loss = 0.09083743
Iteration 11, loss = 0.07812737
Iteration 12, loss = 0.06867409
Iteration 13, loss = 0.06133743
Iteration 14, loss = 0.05565338
Iteration 15, loss = 0.05138922
Iteration 16, loss = 0.04733456
Iteration 17, loss = 0.04430813
Iteration 18, loss = 0.04221805
Iteration 19, loss = 0.03979677
Iteration 20, loss = 0.03792123
Iteration 21, loss = 0.03647952
Iteration 22, loss = 0.03537662
Iteration 23, loss = 0.03413184
Iteration 24, loss = 0.03302651
Iteration 25, loss = 0.03234297
Iteration 26, loss = 0.03140091
Iteration 27, loss = 0.03080501
Iteration 28, loss = 0.03038275
Iteration 29, loss = 0.02962193
Iteration 30, loss = 0.02924539
Iteration 31, loss = 0.02884642
Iteration 32, loss = 0.02833502
Iteration 33, loss = 0.02800257
Iteration 34, loss = 0.02761325
Iteration 35, loss = 0.02752216
Iteration 36, loss = 0.02726059
Iteration 37, loss = 0.02677221
Iteration 38, loss = 0.02662763
Iteration 39, loss = 0.02623170
Iteration 40, loss = 0.02637224
Iteration 41, loss = 0.02637732
Iteration 42, loss = 0.02619273
Iteration 43, loss = 0.02575831
Iteration 44, loss = 0.02546467
Iteration 45, loss = 0.02564156
Iteration 46, loss = 0.02530502
Iteration 47, loss = 0.02569480
Iteration 48, loss = 0.02528725
Iteration 49, loss = 0.02520862
Iteration 50, loss = 0.02488831
Iteration 51, loss = 0.02527608
Iteration 52, loss = 0.02495834
Iteration 53, loss = 0.02483808
Iteration 54, loss = 0.02451398
Iteration 55, loss = 0.02454234
Iteration 56, loss = 0.02431641
Iteration 57, loss = 0.02445213
Iteration 58, loss = 0.02447744
Iteration 59, loss = 0.02438983
Iteration 60, loss = 0.02462421
Iteration 61, loss = 0.02428509
Iteration 62, loss = 0.02404321
Iteration 63, loss = 0.02395720
Iteration 64, loss = 0.02398536
Iteration 65, loss = 0.02415047
Iteration 66, loss = 0.02391287
Iteration 67, loss = 0.02386936
Iteration 68, loss = 0.02406426
Iteration 69, loss = 0.02419156
Iteration 70, loss = 0.02407958
Iteration 71, loss = 0.02379886
Iteration 72, loss = 0.02368272
Iteration 73, loss = 0.02417386
Iteration 74, loss = 0.02374903
Iteration 75, loss = 0.02374972
Iteration 76, loss = 0.02410653
Iteration 77, loss = 0.02390678
Iteration 78, loss = 0.02394854
Iteration 79, loss = 0.02343387
Iteration 80, loss = 0.02390684
Iteration 81, loss = 0.02366119
Iteration 82, loss = 0.02379728
Iteration 83, loss = 0.02348665
Iteration 84, loss = 0.02353383
Iteration 85, loss = 0.02369906
Iteration 86, loss = 0.02369625
Iteration 87, loss = 0.02334266
Iteration 88, loss = 0.02344456
Iteration 89, loss = 0.02344673
Iteration 90, loss = 0.02364765
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69472992
Iteration 2, loss = 0.62967250
Iteration 3, loss = 0.52620821
Iteration 4, loss = 0.40969811
Iteration 5, loss = 0.30883357
Iteration 6, loss = 0.23213267
Iteration 7, loss = 0.17788244
Iteration 8, loss = 0.14043711
Iteration 9, loss = 0.11465311
Iteration 10, loss = 0.09617521
Iteration 11, loss = 0.08279736
Iteration 12, loss = 0.07251134
Iteration 13, loss = 0.06509185
Iteration 14, loss = 0.05926437
Iteration 15, loss = 0.05419366
Iteration 16, loss = 0.05031638
Iteration 17, loss = 0.04701227
Iteration 18, loss = 0.04423825
Iteration 19, loss = 0.04224352
Iteration 20, loss = 0.04045541
Iteration 21, loss = 0.03857726
Iteration 22, loss = 0.03722258
Iteration 23, loss = 0.03588856
Iteration 24, loss = 0.03475239
Iteration 25, loss = 0.03410623
Iteration 26, loss = 0.03303010
Iteration 27, loss = 0.03264703
Iteration 28, loss = 0.03174936
Iteration 29, loss = 0.03127144
Iteration 30, loss = 0.03079850
Iteration 31, loss = 0.03018362
Iteration 32, loss = 0.02972910
Iteration 33, loss = 0.02919318
Iteration 34, loss = 0.02912720
Iteration 35, loss = 0.02882653
Iteration 36, loss = 0.02880901
Iteration 37, loss = 0.02807569
Iteration 38, loss = 0.02795421
Iteration 39, loss = 0.02751569
Iteration 40, loss = 0.02746285
Iteration 41, loss = 0.02743218
Iteration 42, loss = 0.02714230
Iteration 43, loss = 0.02697528
Iteration 44, loss = 0.02696840
Iteration 45, loss = 0.02667609
Iteration 46, loss = 0.02678704
Iteration 47, loss = 0.02628992
Iteration 48, loss = 0.02611158
Iteration 49, loss = 0.02627337
Iteration 50, loss = 0.02641912
Iteration 51, loss = 0.02576395
Iteration 52, loss = 0.02573076
Iteration 53, loss = 0.02558589
Iteration 54, loss = 0.02566479
Iteration 55, loss = 0.02565092
Iteration 56, loss = 0.02561225
Iteration 57, loss = 0.02525385
Iteration 58, loss = 0.02541220
Iteration 59, loss = 0.02576816
Iteration 60, loss = 0.02522537
Iteration 61, loss = 0.02517694
Iteration 62, loss = 0.02511151
Iteration 63, loss = 0.02493929
Iteration 64, loss = 0.02492542
Iteration 65, loss = 0.02498883
Iteration 66, loss = 0.02541283
Iteration 67, loss = 0.02501097
Iteration 68, loss = 0.02489163
Iteration 69, loss = 0.02502548
Iteration 70, loss = 0.02474015
Iteration 71, loss = 0.02459459
Iteration 72, loss = 0.02501358
Iteration 73, loss = 0.02464260
Iteration 74, loss = 0.02503109
Iteration 75, loss = 0.02494771
Iteration 76, loss = 0.02460687
Iteration 77, loss = 0.02455323
Iteration 78, loss = 0.02447864
Iteration 79, loss = 0.02459148
Iteration 80, loss = 0.02432615
Iteration 81, loss = 0.02457169
Iteration 82, loss = 0.02463812
Iteration 83, loss = 0.02430039
Iteration 84, loss = 0.02499861
Iteration 85, loss = 0.02437507
Iteration 86, loss = 0.02457843
Iteration 87, loss = 0.02447340
Iteration 88, loss = 0.02407179
Iteration 89, loss = 0.02434898
Iteration 90, loss = 0.02441366
Iteration 91, loss = 0.02431994
Iteration 92, loss = 0.02465458
Iteration 93, loss = 0.02441107
Iteration 94, loss = 0.02414900
Iteration 95, loss = 0.02414084
Iteration 96, loss = 0.02409817
Iteration 97, loss = 0.02398523
Iteration 98, loss = 0.02391966
Iteration 99, loss = 0.02430718
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67382725
Iteration 2, loss = 0.59909056
Iteration 3, loss = 0.48784689
Iteration 4, loss = 0.37436238
Iteration 5, loss = 0.28043593
Iteration 6, loss = 0.21102399
Iteration 7, loss = 0.16209501
Iteration 8, loss = 0.12820361
Iteration 9, loss = 0.10500314
Iteration 10, loss = 0.08825874
Iteration 11, loss = 0.07601096
Iteration 12, loss = 0.06656115
Iteration 13, loss = 0.05937524
Iteration 14, loss = 0.05377458
Iteration 15, loss = 0.04922887
Iteration 16, loss = 0.04555065
Iteration 17, loss = 0.04255790
Iteration 18, loss = 0.04022566
Iteration 19, loss = 0.03790246
Iteration 20, loss = 0.03613991
Iteration 21, loss = 0.03469569
Iteration 22, loss = 0.03309815
Iteration 23, loss = 0.03194040
Iteration 24, loss = 0.03094521
Iteration 25, loss = 0.03038856
Iteration 26, loss = 0.02933713
Iteration 27, loss = 0.02858635
Iteration 28, loss = 0.02806835
Iteration 29, loss = 0.02755064
Iteration 30, loss = 0.02681070
Iteration 31, loss = 0.02656399
Iteration 32, loss = 0.02611226
Iteration 33, loss = 0.02575065
Iteration 34, loss = 0.02541442
Iteration 35, loss = 0.02531302
Iteration 36, loss = 0.02486235
Iteration 37, loss = 0.02444921
Iteration 38, loss = 0.02429770
Iteration 39, loss = 0.02421576
Iteration 40, loss = 0.02377071
Iteration 41, loss = 0.02388969
Iteration 42, loss = 0.02349538
Iteration 43, loss = 0.02347254
Iteration 44, loss = 0.02329021
Iteration 45, loss = 0.02299053
Iteration 46, loss = 0.02291953
Iteration 47, loss = 0.02259668
Iteration 48, loss = 0.02252125
Iteration 49, loss = 0.02243322
Iteration 50, loss = 0.02252949
Iteration 51, loss = 0.02251592
Iteration 52, loss = 0.02240822
Iteration 53, loss = 0.02228935
Iteration 54, loss = 0.02225447
Iteration 55, loss = 0.02213707
Iteration 56, loss = 0.02221962
Iteration 57, loss = 0.02194408
Iteration 58, loss = 0.02186505
Iteration 59, loss = 0.02209142
Iteration 60, loss = 0.02179332
Iteration 61, loss = 0.02171497
Iteration 62, loss = 0.02160053
Iteration 63, loss = 0.02180247
Iteration 64, loss = 0.02173552
Iteration 65, loss = 0.02152038
Iteration 66, loss = 0.02156988
Iteration 67, loss = 0.02159665
Iteration 68, loss = 0.02155379
Iteration 69, loss = 0.02151858
Iteration 70, loss = 0.02144993
Iteration 71, loss = 0.02146046
Iteration 72, loss = 0.02122695
Iteration 73, loss = 0.02142911
Iteration 74, loss = 0.02114794
Iteration 75, loss = 0.02152019
Iteration 76, loss = 0.02151851
Iteration 77, loss = 0.02138445
Iteration 78, loss = 0.02091540
Iteration 79, loss = 0.02118276
Iteration 80, loss = 0.02094182
Iteration 81, loss = 0.02124480
Iteration 82, loss = 0.02113971
Iteration 83, loss = 0.02101464
Iteration 84, loss = 0.02114582
Iteration 85, loss = 0.02110153
Iteration 86, loss = 0.02128555
Iteration 87, loss = 0.02098877
Iteration 88, loss = 0.02115902
Iteration 89, loss = 0.02064353
Iteration 90, loss = 0.02101392
Iteration 91, loss = 0.02097778
Iteration 92, loss = 0.02108294
Iteration 93, loss = 0.02068076
Iteration 94, loss = 0.02086616
Iteration 95, loss = 0.02099781
Iteration 96, loss = 0.02087812
Iteration 97, loss = 0.02070530
Iteration 98, loss = 0.02089687
Iteration 99, loss = 0.02089072
Iteration 100, loss = 0.02078370
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67710413
Iteration 2, loss = 0.60655585
Iteration 3, loss = 0.49663965
Iteration 4, loss = 0.38534789
Iteration 5, loss = 0.29024671
Iteration 6, loss = 0.21845646
Iteration 7, loss = 0.16769566
Iteration 8, loss = 0.13280482
Iteration 9, loss = 0.10826492
Iteration 10, loss = 0.09025664
Iteration 11, loss = 0.07758446
Iteration 12, loss = 0.06801831
Iteration 13, loss = 0.06056274
Iteration 14, loss = 0.05465804
Iteration 15, loss = 0.04992360
Iteration 16, loss = 0.04616409
Iteration 17, loss = 0.04295443
Iteration 18, loss = 0.04041935
Iteration 19, loss = 0.03850389
Iteration 20, loss = 0.03637206
Iteration 21, loss = 0.03491354
Iteration 22, loss = 0.03364217
Iteration 23, loss = 0.03237646
Iteration 24, loss = 0.03142812
Iteration 25, loss = 0.03049543
Iteration 26, loss = 0.02973296
Iteration 27, loss = 0.02904092
Iteration 28, loss = 0.02821079
Iteration 29, loss = 0.02751391
Iteration 30, loss = 0.02755637
Iteration 31, loss = 0.02688457
Iteration 32, loss = 0.02644484
Iteration 33, loss = 0.02606150
Iteration 34, loss = 0.02586516
Iteration 35, loss = 0.02509784
Iteration 36, loss = 0.02517239
Iteration 37, loss = 0.02493433
Iteration 38, loss = 0.02450955
Iteration 39, loss = 0.02426425
Iteration 40, loss = 0.02427665
Iteration 41, loss = 0.02401272
Iteration 42, loss = 0.02386508
Iteration 43, loss = 0.02383156
Iteration 44, loss = 0.02344942
Iteration 45, loss = 0.02356542
Iteration 46, loss = 0.02330209
Iteration 47, loss = 0.02315969
Iteration 48, loss = 0.02298792
Iteration 49, loss = 0.02287298
Iteration 50, loss = 0.02299236
Iteration 51, loss = 0.02264355
Iteration 52, loss = 0.02285960
Iteration 53, loss = 0.02283057
Iteration 54, loss = 0.02280332
Iteration 55, loss = 0.02248466
Iteration 56, loss = 0.02220698
Iteration 57, loss = 0.02224066
Iteration 58, loss = 0.02236277
Iteration 59, loss = 0.02211716
Iteration 60, loss = 0.02225225
Iteration 61, loss = 0.02213266
Iteration 62, loss = 0.02210593
Iteration 63, loss = 0.02224359
Iteration 64, loss = 0.02181535
Iteration 65, loss = 0.02167761
Iteration 66, loss = 0.02208233
Iteration 67, loss = 0.02173781
Iteration 68, loss = 0.02169669
Iteration 69, loss = 0.02204623
Iteration 70, loss = 0.02188285
Iteration 71, loss = 0.02161976
Iteration 72, loss = 0.02179101
Iteration 73, loss = 0.02150181
Iteration 74, loss = 0.02171536
Iteration 75, loss = 0.02145466
Iteration 76, loss = 0.02144218
Iteration 77, loss = 0.02160705
Iteration 78, loss = 0.02121865
Iteration 79, loss = 0.02138325
Iteration 80, loss = 0.02156293
Iteration 81, loss = 0.02136841
Iteration 82, loss = 0.02148016
Iteration 83, loss = 0.02147303
Iteration 84, loss = 0.02156214
Iteration 85, loss = 0.02114093
Iteration 86, loss = 0.02119678
Iteration 87, loss = 0.02143412
Iteration 88, loss = 0.02159264
Iteration 89, loss = 0.02134211
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70807586
Iteration 2, loss = 0.63298868
Iteration 3, loss = 0.53124194
Iteration 4, loss = 0.41772439
Iteration 5, loss = 0.31524733
Iteration 6, loss = 0.23592397
Iteration 7, loss = 0.17935981
Iteration 8, loss = 0.14024348
Iteration 9, loss = 0.11320964
Iteration 10, loss = 0.09438028
Iteration 11, loss = 0.08060308
Iteration 12, loss = 0.07032840
Iteration 13, loss = 0.06239015
Iteration 14, loss = 0.05624564
Iteration 15, loss = 0.05133975
Iteration 16, loss = 0.04725190
Iteration 17, loss = 0.04433000
Iteration 18, loss = 0.04140265
Iteration 19, loss = 0.03930447
Iteration 20, loss = 0.03732285
Iteration 21, loss = 0.03547742
Iteration 22, loss = 0.03414498
Iteration 23, loss = 0.03289253
Iteration 24, loss = 0.03172018
Iteration 25, loss = 0.03089099
Iteration 26, loss = 0.03010949
Iteration 27, loss = 0.02920078
Iteration 28, loss = 0.02882171
Iteration 29, loss = 0.02814798
Iteration 30, loss = 0.02773182
Iteration 31, loss = 0.02697733
Iteration 32, loss = 0.02685048
Iteration 33, loss = 0.02635751
Iteration 34, loss = 0.02609180
Iteration 35, loss = 0.02572454
Iteration 36, loss = 0.02541928
Iteration 37, loss = 0.02521761
Iteration 38, loss = 0.02497070
Iteration 39, loss = 0.02479751
Iteration 40, loss = 0.02441325
Iteration 41, loss = 0.02451671
Iteration 42, loss = 0.02418626
Iteration 43, loss = 0.02415106
Iteration 44, loss = 0.02415005
Iteration 45, loss = 0.02360011
Iteration 46, loss = 0.02365238
Iteration 47, loss = 0.02338360
Iteration 48, loss = 0.02348682
Iteration 49, loss = 0.02324578
Iteration 50, loss = 0.02325735
Iteration 51, loss = 0.02331569
Iteration 52, loss = 0.02317055
Iteration 53, loss = 0.02291543
Iteration 54, loss = 0.02265213
Iteration 55, loss = 0.02261044
Iteration 56, loss = 0.02271345
Iteration 57, loss = 0.02239445
Iteration 58, loss = 0.02268556
Iteration 59, loss = 0.02239944
Iteration 60, loss = 0.02241653
Iteration 61, loss = 0.02232900
Iteration 62, loss = 0.02230943
Iteration 63, loss = 0.02225298
Iteration 64, loss = 0.02215308
Iteration 65, loss = 0.02220016
Iteration 66, loss = 0.02191284
Iteration 67, loss = 0.02196932
Iteration 68, loss = 0.02208916
Iteration 69, loss = 0.02237996
Iteration 70, loss = 0.02186722
Iteration 71, loss = 0.02217276
Iteration 72, loss = 0.02183088
Iteration 73, loss = 0.02207672
Iteration 74, loss = 0.02191841
Iteration 75, loss = 0.02191458
Iteration 76, loss = 0.02185126
Iteration 77, loss = 0.02167823
Iteration 78, loss = 0.02168050
Iteration 79, loss = 0.02191087
Iteration 80, loss = 0.02139180
Iteration 81, loss = 0.02169366
Iteration 82, loss = 0.02148659
Iteration 83, loss = 0.02162477
Iteration 84, loss = 0.02168415
Iteration 85, loss = 0.02154376
Iteration 86, loss = 0.02148681
Iteration 87, loss = 0.02148294
Iteration 88, loss = 0.02150230
Iteration 89, loss = 0.02152260
Iteration 90, loss = 0.02161400
Iteration 91, loss = 0.02158926
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68359338
Iteration 2, loss = 0.60702714
Iteration 3, loss = 0.49582773
Iteration 4, loss = 0.38327770
Iteration 5, loss = 0.28806132
Iteration 6, loss = 0.21679715
Iteration 7, loss = 0.16649194
Iteration 8, loss = 0.13191588
Iteration 9, loss = 0.10787687
Iteration 10, loss = 0.09083743
Iteration 11, loss = 0.07812737
Iteration 12, loss = 0.06867409
Iteration 13, loss = 0.06133743
Iteration 14, loss = 0.05565338
Iteration 15, loss = 0.05138922
Iteration 16, loss = 0.04733456
Iteration 17, loss = 0.04430813
Iteration 18, loss = 0.04221805
Iteration 19, loss = 0.03979677
Iteration 20, loss = 0.03792123
Iteration 21, loss = 0.03647952
Iteration 22, loss = 0.03537662
Iteration 23, loss = 0.03413184
Iteration 24, loss = 0.03302651
Iteration 25, loss = 0.03234297
Iteration 26, loss = 0.03140091
Iteration 27, loss = 0.03080501
Iteration 28, loss = 0.03038275
Iteration 29, loss = 0.02962193
Iteration 30, loss = 0.02924539
Iteration 31, loss = 0.02884642
Iteration 32, loss = 0.02833502
Iteration 33, loss = 0.02800257
Iteration 34, loss = 0.02761325
Iteration 35, loss = 0.02752216
Iteration 36, loss = 0.02726059
Iteration 37, loss = 0.02677221
Iteration 38, loss = 0.02662763
Iteration 39, loss = 0.02623170
Iteration 40, loss = 0.02637224
Iteration 41, loss = 0.02637732
Iteration 42, loss = 0.02619273
Iteration 43, loss = 0.02575831
Iteration 44, loss = 0.02546467
Iteration 45, loss = 0.02564156
Iteration 46, loss = 0.02530502
Iteration 47, loss = 0.02569480
Iteration 48, loss = 0.02528725
Iteration 49, loss = 0.02520862
Iteration 50, loss = 0.02488831
Iteration 51, loss = 0.02527608
Iteration 52, loss = 0.02495834
Iteration 53, loss = 0.02483808
Iteration 54, loss = 0.02451398
Iteration 55, loss = 0.02454234
Iteration 56, loss = 0.02431641
Iteration 57, loss = 0.02445213
Iteration 58, loss = 0.02447744
Iteration 59, loss = 0.02438983
Iteration 60, loss = 0.02462421
Iteration 61, loss = 0.02428509
Iteration 62, loss = 0.02404321
Iteration 63, loss = 0.02395720
Iteration 64, loss = 0.02398536
Iteration 65, loss = 0.02415047
Iteration 66, loss = 0.02391287
Iteration 67, loss = 0.02386936
Iteration 68, loss = 0.02406426
Iteration 69, loss = 0.02419156
Iteration 70, loss = 0.02407958
Iteration 71, loss = 0.02379886
Iteration 72, loss = 0.02368272
Iteration 73, loss = 0.02417386
Iteration 74, loss = 0.02374903
Iteration 75, loss = 0.02374972
Iteration 76, loss = 0.02410653
Iteration 77, loss = 0.02390678
Iteration 78, loss = 0.02394854
Iteration 79, loss = 0.02343387
Iteration 80, loss = 0.02390684
Iteration 81, loss = 0.02366119
Iteration 82, loss = 0.02379728
Iteration 83, loss = 0.02348665
Iteration 84, loss = 0.02353383
Iteration 85, loss = 0.02369906
Iteration 86, loss = 0.02369625
Iteration 87, loss = 0.02334266
Iteration 88, loss = 0.02344456
Iteration 89, loss = 0.02344673
Iteration 90, loss = 0.02364765
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69472992
Iteration 2, loss = 0.62967250
Iteration 3, loss = 0.52620821
Iteration 4, loss = 0.40969811
Iteration 5, loss = 0.30883357
Iteration 6, loss = 0.23213267
Iteration 7, loss = 0.17788244
Iteration 8, loss = 0.14043711
Iteration 9, loss = 0.11465311
Iteration 10, loss = 0.09617521
Iteration 11, loss = 0.08279736
Iteration 12, loss = 0.07251134
Iteration 13, loss = 0.06509185
Iteration 14, loss = 0.05926437
Iteration 15, loss = 0.05419366
Iteration 16, loss = 0.05031638
Iteration 17, loss = 0.04701227
Iteration 18, loss = 0.04423825
Iteration 19, loss = 0.04224352
Iteration 20, loss = 0.04045541
Iteration 21, loss = 0.03857726
Iteration 22, loss = 0.03722258
Iteration 23, loss = 0.03588856
Iteration 24, loss = 0.03475239
Iteration 25, loss = 0.03410623
Iteration 26, loss = 0.03303010
Iteration 27, loss = 0.03264703
Iteration 28, loss = 0.03174936
Iteration 29, loss = 0.03127144
Iteration 30, loss = 0.03079850
Iteration 31, loss = 0.03018362
Iteration 32, loss = 0.02972910
Iteration 33, loss = 0.02919318
Iteration 34, loss = 0.02912720
Iteration 35, loss = 0.02882653
Iteration 36, loss = 0.02880901
Iteration 37, loss = 0.02807569
Iteration 38, loss = 0.02795421
Iteration 39, loss = 0.02751569
Iteration 40, loss = 0.02746285
Iteration 41, loss = 0.02743218
Iteration 42, loss = 0.02714230
Iteration 43, loss = 0.02697528
Iteration 44, loss = 0.02696840
Iteration 45, loss = 0.02667609
Iteration 46, loss = 0.02678704
Iteration 47, loss = 0.02628992
Iteration 48, loss = 0.02611158
Iteration 49, loss = 0.02627337
Iteration 50, loss = 0.02641912
Iteration 51, loss = 0.02576395
Iteration 52, loss = 0.02573076
Iteration 53, loss = 0.02558589
Iteration 54, loss = 0.02566479
Iteration 55, loss = 0.02565092
Iteration 56, loss = 0.02561225
Iteration 57, loss = 0.02525385
Iteration 58, loss = 0.02541220
Iteration 59, loss = 0.02576816
Iteration 60, loss = 0.02522537
Iteration 61, loss = 0.02517694
Iteration 62, loss = 0.02511151
Iteration 63, loss = 0.02493929
Iteration 64, loss = 0.02492542
Iteration 65, loss = 0.02498883
Iteration 66, loss = 0.02541283
Iteration 67, loss = 0.02501097
Iteration 68, loss = 0.02489163
Iteration 69, loss = 0.02502548
Iteration 70, loss = 0.02474015
Iteration 71, loss = 0.02459459
Iteration 72, loss = 0.02501358
Iteration 73, loss = 0.02464260
Iteration 74, loss = 0.02503109
Iteration 75, loss = 0.02494771
Iteration 76, loss = 0.02460687
Iteration 77, loss = 0.02455323
Iteration 78, loss = 0.02447864
Iteration 79, loss = 0.02459148
Iteration 80, loss = 0.02432615
Iteration 81, loss = 0.02457169
Iteration 82, loss = 0.02463812
Iteration 83, loss = 0.02430039
Iteration 84, loss = 0.02499861
Iteration 85, loss = 0.02437507
Iteration 86, loss = 0.02457843
Iteration 87, loss = 0.02447340
Iteration 88, loss = 0.02407179
Iteration 89, loss = 0.02434898
Iteration 90, loss = 0.02441366
Iteration 91, loss = 0.02431994
Iteration 92, loss = 0.02465458
Iteration 93, loss = 0.02441107
Iteration 94, loss = 0.02414900
Iteration 95, loss = 0.02414084
Iteration 96, loss = 0.02409817
Iteration 97, loss = 0.02398523
Iteration 98, loss = 0.02391966
Iteration 99, loss = 0.02430718
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67382725
Iteration 2, loss = 0.59909056
Iteration 3, loss = 0.48784689
Iteration 4, loss = 0.37436238
Iteration 5, loss = 0.28043593
Iteration 6, loss = 0.21102399
Iteration 7, loss = 0.16209501
Iteration 8, loss = 0.12820361
Iteration 9, loss = 0.10500314
Iteration 10, loss = 0.08825874
Iteration 11, loss = 0.07601096
Iteration 12, loss = 0.06656115
Iteration 13, loss = 0.05937524
Iteration 14, loss = 0.05377458
Iteration 15, loss = 0.04922887
Iteration 16, loss = 0.04555065
Iteration 17, loss = 0.04255790
Iteration 18, loss = 0.04022566
Iteration 19, loss = 0.03790246
Iteration 20, loss = 0.03613991
Iteration 21, loss = 0.03469569
Iteration 22, loss = 0.03309815
Iteration 23, loss = 0.03194040
Iteration 24, loss = 0.03094521
Iteration 25, loss = 0.03038856
Iteration 26, loss = 0.02933713
Iteration 27, loss = 0.02858635
Iteration 28, loss = 0.02806835
Iteration 29, loss = 0.02755064
Iteration 30, loss = 0.02681070
Iteration 31, loss = 0.02656399
Iteration 32, loss = 0.02611226
Iteration 33, loss = 0.02575065
Iteration 34, loss = 0.02541442
Iteration 35, loss = 0.02531302
Iteration 36, loss = 0.02486235
Iteration 37, loss = 0.02444921
Iteration 38, loss = 0.02429770
Iteration 39, loss = 0.02421576
Iteration 40, loss = 0.02377071
Iteration 41, loss = 0.02388969
Iteration 42, loss = 0.02349538
Iteration 43, loss = 0.02347254
Iteration 44, loss = 0.02329021
Iteration 45, loss = 0.02299053
Iteration 46, loss = 0.02291953
Iteration 47, loss = 0.02259668
Iteration 48, loss = 0.02252125
Iteration 49, loss = 0.02243322
Iteration 50, loss = 0.02252949
Iteration 51, loss = 0.02251592
Iteration 52, loss = 0.02240822
Iteration 53, loss = 0.02228935
Iteration 54, loss = 0.02225447
Iteration 55, loss = 0.02213707
Iteration 56, loss = 0.02221962
Iteration 57, loss = 0.02194408
Iteration 58, loss = 0.02186505
Iteration 59, loss = 0.02209142
Iteration 60, loss = 0.02179332
Iteration 61, loss = 0.02171497
Iteration 62, loss = 0.02160053
Iteration 63, loss = 0.02180247
Iteration 64, loss = 0.02173552
Iteration 65, loss = 0.02152038
Iteration 66, loss = 0.02156988
Iteration 67, loss = 0.02159665
Iteration 68, loss = 0.02155379
Iteration 69, loss = 0.02151858
Iteration 70, loss = 0.02144993
Iteration 71, loss = 0.02146046
Iteration 72, loss = 0.02122695
Iteration 73, loss = 0.02142911
Iteration 74, loss = 0.02114794
Iteration 75, loss = 0.02152019
Iteration 76, loss = 0.02151851
Iteration 77, loss = 0.02138445
Iteration 78, loss = 0.02091540
Iteration 79, loss = 0.02118276
Iteration 80, loss = 0.02094182
Iteration 81, loss = 0.02124480
Iteration 82, loss = 0.02113971
Iteration 83, loss = 0.02101464
Iteration 84, loss = 0.02114582
Iteration 85, loss = 0.02110153
Iteration 86, loss = 0.02128555
Iteration 87, loss = 0.02098877
Iteration 88, loss = 0.02115902
Iteration 89, loss = 0.02064353
Iteration 90, loss = 0.02101392
Iteration 91, loss = 0.02097778
Iteration 92, loss = 0.02108294
Iteration 93, loss = 0.02068076
Iteration 94, loss = 0.02086616
Iteration 95, loss = 0.02099781
Iteration 96, loss = 0.02087812
Iteration 97, loss = 0.02070530
Iteration 98, loss = 0.02089687
Iteration 99, loss = 0.02089072
Iteration 100, loss = 0.02078370
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67710413
Iteration 2, loss = 0.60655585
Iteration 3, loss = 0.49663965
Iteration 4, loss = 0.38534789
Iteration 5, loss = 0.29024671
Iteration 6, loss = 0.21845646
Iteration 7, loss = 0.16769566
Iteration 8, loss = 0.13280482
Iteration 9, loss = 0.10826492
Iteration 10, loss = 0.09025664
Iteration 11, loss = 0.07758446
Iteration 12, loss = 0.06801831
Iteration 13, loss = 0.06056274
Iteration 14, loss = 0.05465804
Iteration 15, loss = 0.04992360
Iteration 16, loss = 0.04616409
Iteration 17, loss = 0.04295443
Iteration 18, loss = 0.04041935
Iteration 19, loss = 0.03850389
Iteration 20, loss = 0.03637206
Iteration 21, loss = 0.03491354
Iteration 22, loss = 0.03364217
Iteration 23, loss = 0.03237646
Iteration 24, loss = 0.03142812
Iteration 25, loss = 0.03049543
Iteration 26, loss = 0.02973296
Iteration 27, loss = 0.02904092
Iteration 28, loss = 0.02821079
Iteration 29, loss = 0.02751391
Iteration 30, loss = 0.02755637
Iteration 31, loss = 0.02688457
Iteration 32, loss = 0.02644484
Iteration 33, loss = 0.02606150
Iteration 34, loss = 0.02586516
Iteration 35, loss = 0.02509784
Iteration 36, loss = 0.02517239
Iteration 37, loss = 0.02493433
Iteration 38, loss = 0.02450955
Iteration 39, loss = 0.02426425
Iteration 40, loss = 0.02427665
Iteration 41, loss = 0.02401272
Iteration 42, loss = 0.02386508
Iteration 43, loss = 0.02383156
Iteration 44, loss = 0.02344942
Iteration 45, loss = 0.02356542
Iteration 46, loss = 0.02330209
Iteration 47, loss = 0.02315969
Iteration 48, loss = 0.02298792
Iteration 49, loss = 0.02287298
Iteration 50, loss = 0.02299236
Iteration 51, loss = 0.02264355
Iteration 52, loss = 0.02285960
Iteration 53, loss = 0.02283057
Iteration 54, loss = 0.02280332
Iteration 55, loss = 0.02248466
Iteration 56, loss = 0.02220698
Iteration 57, loss = 0.02224066
Iteration 58, loss = 0.02236277
Iteration 59, loss = 0.02211716
Iteration 60, loss = 0.02225225
Iteration 61, loss = 0.02213266
Iteration 62, loss = 0.02210593
Iteration 63, loss = 0.02224359
Iteration 64, loss = 0.02181535
Iteration 65, loss = 0.02167761
Iteration 66, loss = 0.02208233
Iteration 67, loss = 0.02173781
Iteration 68, loss = 0.02169669
Iteration 69, loss = 0.02204623
Iteration 70, loss = 0.02188285
Iteration 71, loss = 0.02161976
Iteration 72, loss = 0.02179101
Iteration 73, loss = 0.02150181
Iteration 74, loss = 0.02171536
Iteration 75, loss = 0.02145466
Iteration 76, loss = 0.02144218
Iteration 77, loss = 0.02160705
Iteration 78, loss = 0.02121865
Iteration 79, loss = 0.02138325
Iteration 80, loss = 0.02156293
Iteration 81, loss = 0.02136841
Iteration 82, loss = 0.02148016
Iteration 83, loss = 0.02147303
Iteration 84, loss = 0.02156214
Iteration 85, loss = 0.02114093
Iteration 86, loss = 0.02119678
Iteration 87, loss = 0.02143412
Iteration 88, loss = 0.02159264
Iteration 89, loss = 0.02134211
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70807586
Iteration 2, loss = 0.63298868
Iteration 3, loss = 0.53124194
Iteration 4, loss = 0.41772439
Iteration 5, loss = 0.31524733
Iteration 6, loss = 0.23592397
Iteration 7, loss = 0.17935981
Iteration 8, loss = 0.14024348
Iteration 9, loss = 0.11320964
Iteration 10, loss = 0.09438028
Iteration 11, loss = 0.08060308
Iteration 12, loss = 0.07032840
Iteration 13, loss = 0.06239015
Iteration 14, loss = 0.05624564
Iteration 15, loss = 0.05133975
Iteration 16, loss = 0.04725190
Iteration 17, loss = 0.04433000
Iteration 18, loss = 0.04140265
Iteration 19, loss = 0.03930447
Iteration 20, loss = 0.03732285
Iteration 21, loss = 0.03547742
Iteration 22, loss = 0.03414498
Iteration 23, loss = 0.03289253
Iteration 24, loss = 0.03172018
Iteration 25, loss = 0.03089099
Iteration 26, loss = 0.03010949
Iteration 27, loss = 0.02920078
Iteration 28, loss = 0.02882171
Iteration 29, loss = 0.02814798
Iteration 30, loss = 0.02773182
Iteration 31, loss = 0.02697733
Iteration 32, loss = 0.02685048
Iteration 33, loss = 0.02635751
Iteration 34, loss = 0.02609180
Iteration 35, loss = 0.02572454
Iteration 36, loss = 0.02541928
Iteration 37, loss = 0.02521761
Iteration 38, loss = 0.02497070
Iteration 39, loss = 0.02479751
Iteration 40, loss = 0.02441325
Iteration 41, loss = 0.02451671
Iteration 42, loss = 0.02418626
Iteration 43, loss = 0.02415106
Iteration 44, loss = 0.02415005
Iteration 45, loss = 0.02360011
Iteration 46, loss = 0.02365238
Iteration 47, loss = 0.02338360
Iteration 48, loss = 0.02348682
Iteration 49, loss = 0.02324578
Iteration 50, loss = 0.02325735
Iteration 51, loss = 0.02331569
Iteration 52, loss = 0.02317055
Iteration 53, loss = 0.02291543
Iteration 54, loss = 0.02265213
Iteration 55, loss = 0.02261044
Iteration 56, loss = 0.02271345
Iteration 57, loss = 0.02239445
Iteration 58, loss = 0.02268556
Iteration 59, loss = 0.02239944
Iteration 60, loss = 0.02241653
Iteration 61, loss = 0.02232900
Iteration 62, loss = 0.02230943
Iteration 63, loss = 0.02225298
Iteration 64, loss = 0.02215308
Iteration 65, loss = 0.02220016
Iteration 66, loss = 0.02191284
Iteration 67, loss = 0.02196932
Iteration 68, loss = 0.02208916
Iteration 69, loss = 0.02237996
Iteration 70, loss = 0.02186722
Iteration 71, loss = 0.02217276
Iteration 72, loss = 0.02183088
Iteration 73, loss = 0.02207672
Iteration 74, loss = 0.02191841
Iteration 75, loss = 0.02191458
Iteration 76, loss = 0.02185126
Iteration 77, loss = 0.02167823
Iteration 78, loss = 0.02168050
Iteration 79, loss = 0.02191087
Iteration 80, loss = 0.02139180
Iteration 81, loss = 0.02169366
Iteration 82, loss = 0.02148659
Iteration 83, loss = 0.02162477
Iteration 84, loss = 0.02168415
Iteration 85, loss = 0.02154376
Iteration 86, loss = 0.02148681
Iteration 87, loss = 0.02148294
Iteration 88, loss = 0.02150230
Iteration 89, loss = 0.02152260
Iteration 90, loss = 0.02161400
Iteration 91, loss = 0.02158926
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68359338
Iteration 2, loss = 0.60702714
Iteration 3, loss = 0.49582773
Iteration 4, loss = 0.38327770
Iteration 5, loss = 0.28806132
Iteration 6, loss = 0.21679715
Iteration 7, loss = 0.16649194
Iteration 8, loss = 0.13191588
Iteration 9, loss = 0.10787687
Iteration 10, loss = 0.09083743
Iteration 11, loss = 0.07812737
Iteration 12, loss = 0.06867409
Iteration 13, loss = 0.06133743
Iteration 14, loss = 0.05565338
Iteration 15, loss = 0.05138922
Iteration 16, loss = 0.04733456
Iteration 17, loss = 0.04430813
Iteration 18, loss = 0.04221805
Iteration 19, loss = 0.03979677
Iteration 20, loss = 0.03792123
Iteration 21, loss = 0.03647952
Iteration 22, loss = 0.03537662
Iteration 23, loss = 0.03413184
Iteration 24, loss = 0.03302651
Iteration 25, loss = 0.03234297
Iteration 26, loss = 0.03140091
Iteration 27, loss = 0.03080501
Iteration 28, loss = 0.03038275
Iteration 29, loss = 0.02962193
Iteration 30, loss = 0.02924539
Iteration 31, loss = 0.02884642
Iteration 32, loss = 0.02833502
Iteration 33, loss = 0.02800257
Iteration 34, loss = 0.02761325
Iteration 35, loss = 0.02752216
Iteration 36, loss = 0.02726059
Iteration 37, loss = 0.02677221
Iteration 38, loss = 0.02662763
Iteration 39, loss = 0.02623170
Iteration 40, loss = 0.02637224
Iteration 41, loss = 0.02637732
Iteration 42, loss = 0.02619273
Iteration 43, loss = 0.02575831
Iteration 44, loss = 0.02546467
Iteration 45, loss = 0.02564156
Iteration 46, loss = 0.02530502
Iteration 47, loss = 0.02569480
Iteration 48, loss = 0.02528725
Iteration 49, loss = 0.02520862
Iteration 50, loss = 0.02488831
Iteration 51, loss = 0.02527608
Iteration 52, loss = 0.02495834
Iteration 53, loss = 0.02483808
Iteration 54, loss = 0.02451398
Iteration 55, loss = 0.02454234
Iteration 56, loss = 0.02431641
Iteration 57, loss = 0.02445213
Iteration 58, loss = 0.02447744
Iteration 59, loss = 0.02438983
Iteration 60, loss = 0.02462421
Iteration 61, loss = 0.02428509
Iteration 62, loss = 0.02404321
Iteration 63, loss = 0.02395720
Iteration 64, loss = 0.02398536
Iteration 65, loss = 0.02415047
Iteration 66, loss = 0.02391287
Iteration 67, loss = 0.02386936
Iteration 68, loss = 0.02406426
Iteration 69, loss = 0.02419156
Iteration 70, loss = 0.02407958
Iteration 71, loss = 0.02379886
Iteration 72, loss = 0.02368272
Iteration 73, loss = 0.02417386
Iteration 74, loss = 0.02374903
Iteration 75, loss = 0.02374972
Iteration 76, loss = 0.02410653
Iteration 77, loss = 0.02390678
Iteration 78, loss = 0.02394854
Iteration 79, loss = 0.02343387
Iteration 80, loss = 0.02390684
Iteration 81, loss = 0.02366119
Iteration 82, loss = 0.02379728
Iteration 83, loss = 0.02348665
Iteration 84, loss = 0.02353383
Iteration 85, loss = 0.02369906
Iteration 86, loss = 0.02369625
Iteration 87, loss = 0.02334266
Iteration 88, loss = 0.02344456
Iteration 89, loss = 0.02344673
Iteration 90, loss = 0.02364765
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69472992
Iteration 2, loss = 0.62967250
Iteration 3, loss = 0.52620821
Iteration 4, loss = 0.40969811
Iteration 5, loss = 0.30883357
Iteration 6, loss = 0.23213267
Iteration 7, loss = 0.17788244
Iteration 8, loss = 0.14043711
Iteration 9, loss = 0.11465311
Iteration 10, loss = 0.09617521
Iteration 11, loss = 0.08279736
Iteration 12, loss = 0.07251134
Iteration 13, loss = 0.06509185
Iteration 14, loss = 0.05926437
Iteration 15, loss = 0.05419366
Iteration 16, loss = 0.05031638
Iteration 17, loss = 0.04701227
Iteration 18, loss = 0.04423825
Iteration 19, loss = 0.04224352
Iteration 20, loss = 0.04045541
Iteration 21, loss = 0.03857726
Iteration 22, loss = 0.03722258
Iteration 23, loss = 0.03588856
Iteration 24, loss = 0.03475239
Iteration 25, loss = 0.03410623
Iteration 26, loss = 0.03303010
Iteration 27, loss = 0.03264703
Iteration 28, loss = 0.03174936
Iteration 29, loss = 0.03127144
Iteration 30, loss = 0.03079850
Iteration 31, loss = 0.03018362
Iteration 32, loss = 0.02972910
Iteration 33, loss = 0.02919318
Iteration 34, loss = 0.02912720
Iteration 35, loss = 0.02882653
Iteration 36, loss = 0.02880901
Iteration 37, loss = 0.02807569
Iteration 38, loss = 0.02795421
Iteration 39, loss = 0.02751569
Iteration 40, loss = 0.02746285
Iteration 41, loss = 0.02743218
Iteration 42, loss = 0.02714230
Iteration 43, loss = 0.02697528
Iteration 44, loss = 0.02696840
Iteration 45, loss = 0.02667609
Iteration 46, loss = 0.02678704
Iteration 47, loss = 0.02628992
Iteration 48, loss = 0.02611158
Iteration 49, loss = 0.02627337
Iteration 50, loss = 0.02641912
Iteration 51, loss = 0.02576395
Iteration 52, loss = 0.02573076
Iteration 53, loss = 0.02558589
Iteration 54, loss = 0.02566479
Iteration 55, loss = 0.02565092
Iteration 56, loss = 0.02561225
Iteration 57, loss = 0.02525385
Iteration 58, loss = 0.02541220
Iteration 59, loss = 0.02576816
Iteration 60, loss = 0.02522537
Iteration 61, loss = 0.02517694
Iteration 62, loss = 0.02511151
Iteration 63, loss = 0.02493929
Iteration 64, loss = 0.02492542
Iteration 65, loss = 0.02498883
Iteration 66, loss = 0.02541283
Iteration 67, loss = 0.02501097
Iteration 68, loss = 0.02489163
Iteration 69, loss = 0.02502548
Iteration 70, loss = 0.02474015
Iteration 71, loss = 0.02459459
Iteration 72, loss = 0.02501358
Iteration 73, loss = 0.02464260
Iteration 74, loss = 0.02503109
Iteration 75, loss = 0.02494771
Iteration 76, loss = 0.02460687
Iteration 77, loss = 0.02455323
Iteration 78, loss = 0.02447864
Iteration 79, loss = 0.02459148
Iteration 80, loss = 0.02432615
Iteration 81, loss = 0.02457169
Iteration 82, loss = 0.02463812
Iteration 83, loss = 0.02430039
Iteration 84, loss = 0.02499861
Iteration 85, loss = 0.02437507
Iteration 86, loss = 0.02457843
Iteration 87, loss = 0.02447340
Iteration 88, loss = 0.02407179
Iteration 89, loss = 0.02434898
Iteration 90, loss = 0.02441366
Iteration 91, loss = 0.02431994
Iteration 92, loss = 0.02465458
Iteration 93, loss = 0.02441107
Iteration 94, loss = 0.02414900
Iteration 95, loss = 0.02414084
Iteration 96, loss = 0.02409817
Iteration 97, loss = 0.02398523
Iteration 98, loss = 0.02391966
Iteration 99, loss = 0.02430718
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67382725
Iteration 2, loss = 0.59909056
Iteration 3, loss = 0.48784689
Iteration 4, loss = 0.37436238
Iteration 5, loss = 0.28043593
Iteration 6, loss = 0.21102399
Iteration 7, loss = 0.16209501
Iteration 8, loss = 0.12820361
Iteration 9, loss = 0.10500314
Iteration 10, loss = 0.08825874
Iteration 11, loss = 0.07601096
Iteration 12, loss = 0.06656115
Iteration 13, loss = 0.05937524
Iteration 14, loss = 0.05377458
Iteration 15, loss = 0.04922887
Iteration 16, loss = 0.04555065
Iteration 17, loss = 0.04255790
Iteration 18, loss = 0.04022566
Iteration 19, loss = 0.03790246
Iteration 20, loss = 0.03613991
Iteration 21, loss = 0.03469569
Iteration 22, loss = 0.03309815
Iteration 23, loss = 0.03194040
Iteration 24, loss = 0.03094521
Iteration 25, loss = 0.03038856
Iteration 26, loss = 0.02933713
Iteration 27, loss = 0.02858635
Iteration 28, loss = 0.02806835
Iteration 29, loss = 0.02755064
Iteration 30, loss = 0.02681070
Iteration 31, loss = 0.02656399
Iteration 32, loss = 0.02611226
Iteration 33, loss = 0.02575065
Iteration 34, loss = 0.02541442
Iteration 35, loss = 0.02531302
Iteration 36, loss = 0.02486235
Iteration 37, loss = 0.02444921
Iteration 38, loss = 0.02429770
Iteration 39, loss = 0.02421576
Iteration 40, loss = 0.02377071
Iteration 41, loss = 0.02388969
Iteration 42, loss = 0.02349538
Iteration 43, loss = 0.02347254
Iteration 44, loss = 0.02329021
Iteration 45, loss = 0.02299053
Iteration 46, loss = 0.02291953
Iteration 47, loss = 0.02259668
Iteration 48, loss = 0.02252125
Iteration 49, loss = 0.02243322
Iteration 50, loss = 0.02252949
Iteration 51, loss = 0.02251592
Iteration 52, loss = 0.02240822
Iteration 53, loss = 0.02228935
Iteration 54, loss = 0.02225447
Iteration 55, loss = 0.02213707
Iteration 56, loss = 0.02221962
Iteration 57, loss = 0.02194408
Iteration 58, loss = 0.02186505
Iteration 59, loss = 0.02209142
Iteration 60, loss = 0.02179332
Iteration 61, loss = 0.02171497
Iteration 62, loss = 0.02160053
Iteration 63, loss = 0.02180247
Iteration 64, loss = 0.02173552
Iteration 65, loss = 0.02152038
Iteration 66, loss = 0.02156988
Iteration 67, loss = 0.02159665
Iteration 68, loss = 0.02155379
Iteration 69, loss = 0.02151858
Iteration 70, loss = 0.02144993
Iteration 71, loss = 0.02146046
Iteration 72, loss = 0.02122695
Iteration 73, loss = 0.02142911
Iteration 74, loss = 0.02114794
Iteration 75, loss = 0.02152019
Iteration 76, loss = 0.02151851
Iteration 77, loss = 0.02138445
Iteration 78, loss = 0.02091540
Iteration 79, loss = 0.02118276
Iteration 80, loss = 0.02094182
Iteration 81, loss = 0.02124480
Iteration 82, loss = 0.02113971
Iteration 83, loss = 0.02101464
Iteration 84, loss = 0.02114582
Iteration 85, loss = 0.02110153
Iteration 86, loss = 0.02128555
Iteration 87, loss = 0.02098877
Iteration 88, loss = 0.02115902
Iteration 89, loss = 0.02064353
Iteration 90, loss = 0.02101392
Iteration 91, loss = 0.02097778
Iteration 92, loss = 0.02108294
Iteration 93, loss = 0.02068076
Iteration 94, loss = 0.02086616
Iteration 95, loss = 0.02099781
Iteration 96, loss = 0.02087812
Iteration 97, loss = 0.02070530
Iteration 98, loss = 0.02089687
Iteration 99, loss = 0.02089072
Iteration 100, loss = 0.02078370
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67710413
Iteration 2, loss = 0.60655585
Iteration 3, loss = 0.49663965
Iteration 4, loss = 0.38534789
Iteration 5, loss = 0.29024671
Iteration 6, loss = 0.21845646
Iteration 7, loss = 0.16769566
Iteration 8, loss = 0.13280482
Iteration 9, loss = 0.10826492
Iteration 10, loss = 0.09025664
Iteration 11, loss = 0.07758446
Iteration 12, loss = 0.06801831
Iteration 13, loss = 0.06056274
Iteration 14, loss = 0.05465804
Iteration 15, loss = 0.04992360
Iteration 16, loss = 0.04616409
Iteration 17, loss = 0.04295443
Iteration 18, loss = 0.04041935
Iteration 19, loss = 0.03850389
Iteration 20, loss = 0.03637206
Iteration 21, loss = 0.03491354
Iteration 22, loss = 0.03364217
Iteration 23, loss = 0.03237646
Iteration 24, loss = 0.03142812
Iteration 25, loss = 0.03049543
Iteration 26, loss = 0.02973296
Iteration 27, loss = 0.02904092
Iteration 28, loss = 0.02821079
Iteration 29, loss = 0.02751391
Iteration 30, loss = 0.02755637
Iteration 31, loss = 0.02688457
Iteration 32, loss = 0.02644484
Iteration 33, loss = 0.02606150
Iteration 34, loss = 0.02586516
Iteration 35, loss = 0.02509784
Iteration 36, loss = 0.02517239
Iteration 37, loss = 0.02493433
Iteration 38, loss = 0.02450955
Iteration 39, loss = 0.02426425
Iteration 40, loss = 0.02427665
Iteration 41, loss = 0.02401272
Iteration 42, loss = 0.02386508
Iteration 43, loss = 0.02383156
Iteration 44, loss = 0.02344942
Iteration 45, loss = 0.02356542
Iteration 46, loss = 0.02330209
Iteration 47, loss = 0.02315969
Iteration 48, loss = 0.02298792
Iteration 49, loss = 0.02287298
Iteration 50, loss = 0.02299236
Iteration 51, loss = 0.02264355
Iteration 52, loss = 0.02285960
Iteration 53, loss = 0.02283057
Iteration 54, loss = 0.02280332
Iteration 55, loss = 0.02248466
Iteration 56, loss = 0.02220698
Iteration 57, loss = 0.02224066
Iteration 58, loss = 0.02236277
Iteration 59, loss = 0.02211716
Iteration 60, loss = 0.02225225
Iteration 61, loss = 0.02213266
Iteration 62, loss = 0.02210593
Iteration 63, loss = 0.02224359
Iteration 64, loss = 0.02181535
Iteration 65, loss = 0.02167761
Iteration 66, loss = 0.02208233
Iteration 67, loss = 0.02173781
Iteration 68, loss = 0.02169669
Iteration 69, loss = 0.02204623
Iteration 70, loss = 0.02188285
Iteration 71, loss = 0.02161976
Iteration 72, loss = 0.02179101
Iteration 73, loss = 0.02150181
Iteration 74, loss = 0.02171536
Iteration 75, loss = 0.02145466
Iteration 76, loss = 0.02144218
Iteration 77, loss = 0.02160705
Iteration 78, loss = 0.02121865
Iteration 79, loss = 0.02138325
Iteration 80, loss = 0.02156293
Iteration 81, loss = 0.02136841
Iteration 82, loss = 0.02148016
Iteration 83, loss = 0.02147303
Iteration 84, loss = 0.02156214
Iteration 85, loss = 0.02114093
Iteration 86, loss = 0.02119678
Iteration 87, loss = 0.02143412
Iteration 88, loss = 0.02159264
Iteration 89, loss = 0.02134211
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70807586
Iteration 2, loss = 0.63298868
Iteration 3, loss = 0.53124194
Iteration 4, loss = 0.41772439
Iteration 5, loss = 0.31524733
Iteration 6, loss = 0.23592397
Iteration 7, loss = 0.17935981
Iteration 8, loss = 0.14024348
Iteration 9, loss = 0.11320964
Iteration 10, loss = 0.09438028
Iteration 11, loss = 0.08060308
Iteration 12, loss = 0.07032840
Iteration 13, loss = 0.06239015
Iteration 14, loss = 0.05624564
Iteration 15, loss = 0.05133975
Iteration 16, loss = 0.04725190
Iteration 17, loss = 0.04433000
Iteration 18, loss = 0.04140265
Iteration 19, loss = 0.03930447
Iteration 20, loss = 0.03732285
Iteration 21, loss = 0.03547742
Iteration 22, loss = 0.03414498
Iteration 23, loss = 0.03289253
Iteration 24, loss = 0.03172018
Iteration 25, loss = 0.03089099
Iteration 26, loss = 0.03010949
Iteration 27, loss = 0.02920078
Iteration 28, loss = 0.02882171
Iteration 29, loss = 0.02814798
Iteration 30, loss = 0.02773182
Iteration 31, loss = 0.02697733
Iteration 32, loss = 0.02685048
Iteration 33, loss = 0.02635751
Iteration 34, loss = 0.02609180
Iteration 35, loss = 0.02572454
Iteration 36, loss = 0.02541928
Iteration 37, loss = 0.02521761
Iteration 38, loss = 0.02497070
Iteration 39, loss = 0.02479751
Iteration 40, loss = 0.02441325
Iteration 41, loss = 0.02451671
Iteration 42, loss = 0.02418626
Iteration 43, loss = 0.02415106
Iteration 44, loss = 0.02415005
Iteration 45, loss = 0.02360011
Iteration 46, loss = 0.02365238
Iteration 47, loss = 0.02338360
Iteration 48, loss = 0.02348682
Iteration 49, loss = 0.02324578
Iteration 50, loss = 0.02325735
Iteration 51, loss = 0.02331569
Iteration 52, loss = 0.02317055
Iteration 53, loss = 0.02291543
Iteration 54, loss = 0.02265213
Iteration 55, loss = 0.02261044
Iteration 56, loss = 0.02271345
Iteration 57, loss = 0.02239445
Iteration 58, loss = 0.02268556
Iteration 59, loss = 0.02239944
Iteration 60, loss = 0.02241653
Iteration 61, loss = 0.02232900
Iteration 62, loss = 0.02230943
Iteration 63, loss = 0.02225298
Iteration 64, loss = 0.02215308
Iteration 65, loss = 0.02220016
Iteration 66, loss = 0.02191284
Iteration 67, loss = 0.02196932
Iteration 68, loss = 0.02208916
Iteration 69, loss = 0.02237996
Iteration 70, loss = 0.02186722
Iteration 71, loss = 0.02217276
Iteration 72, loss = 0.02183088
Iteration 73, loss = 0.02207672
Iteration 74, loss = 0.02191841
Iteration 75, loss = 0.02191458
Iteration 76, loss = 0.02185126
Iteration 77, loss = 0.02167823
Iteration 78, loss = 0.02168050
Iteration 79, loss = 0.02191087
Iteration 80, loss = 0.02139180
Iteration 81, loss = 0.02169366
Iteration 82, loss = 0.02148659
Iteration 83, loss = 0.02162477
Iteration 84, loss = 0.02168415
Iteration 85, loss = 0.02154376
Iteration 86, loss = 0.02148681
Iteration 87, loss = 0.02148294
Iteration 88, loss = 0.02150230
Iteration 89, loss = 0.02152260
Iteration 90, loss = 0.02161400
Iteration 91, loss = 0.02158926
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68359338
Iteration 2, loss = 0.60702714
Iteration 3, loss = 0.49582773
Iteration 4, loss = 0.38327770
Iteration 5, loss = 0.28806132
Iteration 6, loss = 0.21679715
Iteration 7, loss = 0.16649194
Iteration 8, loss = 0.13191588
Iteration 9, loss = 0.10787687
Iteration 10, loss = 0.09083743
Iteration 11, loss = 0.07812737
Iteration 12, loss = 0.06867409
Iteration 13, loss = 0.06133743
Iteration 14, loss = 0.05565338
Iteration 15, loss = 0.05138922
Iteration 16, loss = 0.04733456
Iteration 17, loss = 0.04430813
Iteration 18, loss = 0.04221805
Iteration 19, loss = 0.03979677
Iteration 20, loss = 0.03792123
Iteration 21, loss = 0.03647952
Iteration 22, loss = 0.03537662
Iteration 23, loss = 0.03413184
Iteration 24, loss = 0.03302651
Iteration 25, loss = 0.03234297
Iteration 26, loss = 0.03140091
Iteration 27, loss = 0.03080501
Iteration 28, loss = 0.03038275
Iteration 29, loss = 0.02962193
Iteration 30, loss = 0.02924539
Iteration 31, loss = 0.02884642
Iteration 32, loss = 0.02833502
Iteration 33, loss = 0.02800257
Iteration 34, loss = 0.02761325
Iteration 35, loss = 0.02752216
Iteration 36, loss = 0.02726059
Iteration 37, loss = 0.02677221
Iteration 38, loss = 0.02662763
Iteration 39, loss = 0.02623170
Iteration 40, loss = 0.02637224
Iteration 41, loss = 0.02637732
Iteration 42, loss = 0.02619273
Iteration 43, loss = 0.02575831
Iteration 44, loss = 0.02546467
Iteration 45, loss = 0.02564156
Iteration 46, loss = 0.02530502
Iteration 47, loss = 0.02569480
Iteration 48, loss = 0.02528725
Iteration 49, loss = 0.02520862
Iteration 50, loss = 0.02488831
Iteration 51, loss = 0.02527608
Iteration 52, loss = 0.02495834
Iteration 53, loss = 0.02483808
Iteration 54, loss = 0.02451398
Iteration 55, loss = 0.02454234
Iteration 56, loss = 0.02431641
Iteration 57, loss = 0.02445213
Iteration 58, loss = 0.02447744
Iteration 59, loss = 0.02438983
Iteration 60, loss = 0.02462421
Iteration 61, loss = 0.02428509
Iteration 62, loss = 0.02404321
Iteration 63, loss = 0.02395720
Iteration 64, loss = 0.02398536
Iteration 65, loss = 0.02415047
Iteration 66, loss = 0.02391287
Iteration 67, loss = 0.02386936
Iteration 68, loss = 0.02406426
Iteration 69, loss = 0.02419156
Iteration 70, loss = 0.02407958
Iteration 71, loss = 0.02379886
Iteration 72, loss = 0.02368272
Iteration 73, loss = 0.02417386
Iteration 74, loss = 0.02374903
Iteration 75, loss = 0.02374972
Iteration 76, loss = 0.02410653
Iteration 77, loss = 0.02390678
Iteration 78, loss = 0.02394854
Iteration 79, loss = 0.02343387
Iteration 80, loss = 0.02390684
Iteration 81, loss = 0.02366119
Iteration 82, loss = 0.02379728
Iteration 83, loss = 0.02348665
Iteration 84, loss = 0.02353383
Iteration 85, loss = 0.02369906
Iteration 86, loss = 0.02369625
Iteration 87, loss = 0.02334266
Iteration 88, loss = 0.02344456
Iteration 89, loss = 0.02344673
Iteration 90, loss = 0.02364765
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69472992
Iteration 2, loss = 0.62967250
Iteration 3, loss = 0.52620821
Iteration 4, loss = 0.40969811
Iteration 5, loss = 0.30883357
Iteration 6, loss = 0.23213267
Iteration 7, loss = 0.17788244
Iteration 8, loss = 0.14043711
Iteration 9, loss = 0.11465311
Iteration 10, loss = 0.09617521
Iteration 11, loss = 0.08279736
Iteration 12, loss = 0.07251134
Iteration 13, loss = 0.06509185
Iteration 14, loss = 0.05926437
Iteration 15, loss = 0.05419366
Iteration 16, loss = 0.05031638
Iteration 17, loss = 0.04701227
Iteration 18, loss = 0.04423825
Iteration 19, loss = 0.04224352
Iteration 20, loss = 0.04045541
Iteration 21, loss = 0.03857726
Iteration 22, loss = 0.03722258
Iteration 23, loss = 0.03588856
Iteration 24, loss = 0.03475239
Iteration 25, loss = 0.03410623
Iteration 26, loss = 0.03303010
Iteration 27, loss = 0.03264703
Iteration 28, loss = 0.03174936
Iteration 29, loss = 0.03127144
Iteration 30, loss = 0.03079850
Iteration 31, loss = 0.03018362
Iteration 32, loss = 0.02972910
Iteration 33, loss = 0.02919318
Iteration 34, loss = 0.02912720
Iteration 35, loss = 0.02882653
Iteration 36, loss = 0.02880901
Iteration 37, loss = 0.02807569
Iteration 38, loss = 0.02795421
Iteration 39, loss = 0.02751569
Iteration 40, loss = 0.02746285
Iteration 41, loss = 0.02743218
Iteration 42, loss = 0.02714230
Iteration 43, loss = 0.02697528
Iteration 44, loss = 0.02696840
Iteration 45, loss = 0.02667609
Iteration 46, loss = 0.02678704
Iteration 47, loss = 0.02628992
Iteration 48, loss = 0.02611158
Iteration 49, loss = 0.02627337
Iteration 50, loss = 0.02641912
Iteration 51, loss = 0.02576395
Iteration 52, loss = 0.02573076
Iteration 53, loss = 0.02558589
Iteration 54, loss = 0.02566479
Iteration 55, loss = 0.02565092
Iteration 56, loss = 0.02561225
Iteration 57, loss = 0.02525385
Iteration 58, loss = 0.02541220
Iteration 59, loss = 0.02576816
Iteration 60, loss = 0.02522537
Iteration 61, loss = 0.02517694
Iteration 62, loss = 0.02511151
Iteration 63, loss = 0.02493929
Iteration 64, loss = 0.02492542
Iteration 65, loss = 0.02498883
Iteration 66, loss = 0.02541283
Iteration 67, loss = 0.02501097
Iteration 68, loss = 0.02489163
Iteration 69, loss = 0.02502548
Iteration 70, loss = 0.02474015
Iteration 71, loss = 0.02459459
Iteration 72, loss = 0.02501358
Iteration 73, loss = 0.02464260
Iteration 74, loss = 0.02503109
Iteration 75, loss = 0.02494771
Iteration 76, loss = 0.02460687
Iteration 77, loss = 0.02455323
Iteration 78, loss = 0.02447864
Iteration 79, loss = 0.02459148
Iteration 80, loss = 0.02432615
Iteration 81, loss = 0.02457169
Iteration 82, loss = 0.02463812
Iteration 83, loss = 0.02430039
Iteration 84, loss = 0.02499861
Iteration 85, loss = 0.02437507
Iteration 86, loss = 0.02457843
Iteration 87, loss = 0.02447340
Iteration 88, loss = 0.02407179
Iteration 89, loss = 0.02434898
Iteration 90, loss = 0.02441366
Iteration 91, loss = 0.02431994
Iteration 92, loss = 0.02465458
Iteration 93, loss = 0.02441107
Iteration 94, loss = 0.02414900
Iteration 95, loss = 0.02414084
Iteration 96, loss = 0.02409817
Iteration 97, loss = 0.02398523
Iteration 98, loss = 0.02391966
Iteration 99, loss = 0.02430718
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67382725
Iteration 2, loss = 0.59909056
Iteration 3, loss = 0.48784689
Iteration 4, loss = 0.37436238
Iteration 5, loss = 0.28043593
Iteration 6, loss = 0.21102399
Iteration 7, loss = 0.16209501
Iteration 8, loss = 0.12820361
Iteration 9, loss = 0.10500314
Iteration 10, loss = 0.08825874
Iteration 11, loss = 0.07601096
Iteration 12, loss = 0.06656115
Iteration 13, loss = 0.05937524
Iteration 14, loss = 0.05377458
Iteration 15, loss = 0.04922887
Iteration 16, loss = 0.04555065
Iteration 17, loss = 0.04255790
Iteration 18, loss = 0.04022566
Iteration 19, loss = 0.03790246
Iteration 20, loss = 0.03613991
Iteration 21, loss = 0.03469569
Iteration 22, loss = 0.03309815
Iteration 23, loss = 0.03194040
Iteration 24, loss = 0.03094521
Iteration 25, loss = 0.03038856
Iteration 26, loss = 0.02933713
Iteration 27, loss = 0.02858635
Iteration 28, loss = 0.02806835
Iteration 29, loss = 0.02755064
Iteration 30, loss = 0.02681070
Iteration 31, loss = 0.02656399
Iteration 32, loss = 0.02611226
Iteration 33, loss = 0.02575065
Iteration 34, loss = 0.02541442
Iteration 35, loss = 0.02531302
Iteration 36, loss = 0.02486235
Iteration 37, loss = 0.02444921
Iteration 38, loss = 0.02429770
Iteration 39, loss = 0.02421576
Iteration 40, loss = 0.02377071
Iteration 41, loss = 0.02388969
Iteration 42, loss = 0.02349538
Iteration 43, loss = 0.02347254
Iteration 44, loss = 0.02329021
Iteration 45, loss = 0.02299053
Iteration 46, loss = 0.02291953
Iteration 47, loss = 0.02259668
Iteration 48, loss = 0.02252125
Iteration 49, loss = 0.02243322
Iteration 50, loss = 0.02252949
Iteration 51, loss = 0.02251592
Iteration 52, loss = 0.02240822
Iteration 53, loss = 0.02228935
Iteration 54, loss = 0.02225447
Iteration 55, loss = 0.02213707
Iteration 56, loss = 0.02221962
Iteration 57, loss = 0.02194408
Iteration 58, loss = 0.02186505
Iteration 59, loss = 0.02209142
Iteration 60, loss = 0.02179332
Iteration 61, loss = 0.02171497
Iteration 62, loss = 0.02160053
Iteration 63, loss = 0.02180247
Iteration 64, loss = 0.02173552
Iteration 65, loss = 0.02152038
Iteration 66, loss = 0.02156988
Iteration 67, loss = 0.02159665
Iteration 68, loss = 0.02155379
Iteration 69, loss = 0.02151858
Iteration 70, loss = 0.02144993
Iteration 71, loss = 0.02146046
Iteration 72, loss = 0.02122695
Iteration 73, loss = 0.02142911
Iteration 74, loss = 0.02114794
Iteration 75, loss = 0.02152019
Iteration 76, loss = 0.02151851
Iteration 77, loss = 0.02138445
Iteration 78, loss = 0.02091540
Iteration 79, loss = 0.02118276
Iteration 80, loss = 0.02094182
Iteration 81, loss = 0.02124480
Iteration 82, loss = 0.02113971
Iteration 83, loss = 0.02101464
Iteration 84, loss = 0.02114582
Iteration 85, loss = 0.02110153
Iteration 86, loss = 0.02128555
Iteration 87, loss = 0.02098877
Iteration 88, loss = 0.02115902
Iteration 89, loss = 0.02064353
Iteration 90, loss = 0.02101392
Iteration 91, loss = 0.02097778
Iteration 92, loss = 0.02108294
Iteration 93, loss = 0.02068076
Iteration 94, loss = 0.02086616
Iteration 95, loss = 0.02099781
Iteration 96, loss = 0.02087812
Iteration 97, loss = 0.02070530
Iteration 98, loss = 0.02089687
Iteration 99, loss = 0.02089072
Iteration 100, loss = 0.02078370
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67710413
Iteration 2, loss = 0.60655585
Iteration 3, loss = 0.49663965
Iteration 4, loss = 0.38534789
Iteration 5, loss = 0.29024671
Iteration 6, loss = 0.21845646
Iteration 7, loss = 0.16769566
Iteration 8, loss = 0.13280482
Iteration 9, loss = 0.10826492
Iteration 10, loss = 0.09025664
Iteration 11, loss = 0.07758446
Iteration 12, loss = 0.06801831
Iteration 13, loss = 0.06056274
Iteration 14, loss = 0.05465804
Iteration 15, loss = 0.04992360
Iteration 16, loss = 0.04616409
Iteration 17, loss = 0.04295443
Iteration 18, loss = 0.04041935
Iteration 19, loss = 0.03850389
Iteration 20, loss = 0.03637206
Iteration 21, loss = 0.03491354
Iteration 22, loss = 0.03364217
Iteration 23, loss = 0.03237646
Iteration 24, loss = 0.03142812
Iteration 25, loss = 0.03049543
Iteration 26, loss = 0.02973296
Iteration 27, loss = 0.02904092
Iteration 28, loss = 0.02821079
Iteration 29, loss = 0.02751391
Iteration 30, loss = 0.02755637
Iteration 31, loss = 0.02688457
Iteration 32, loss = 0.02644484
Iteration 33, loss = 0.02606150
Iteration 34, loss = 0.02586516
Iteration 35, loss = 0.02509784
Iteration 36, loss = 0.02517239
Iteration 37, loss = 0.02493433
Iteration 38, loss = 0.02450955
Iteration 39, loss = 0.02426425
Iteration 40, loss = 0.02427665
Iteration 41, loss = 0.02401272
Iteration 42, loss = 0.02386508
Iteration 43, loss = 0.02383156
Iteration 44, loss = 0.02344942
Iteration 45, loss = 0.02356542
Iteration 46, loss = 0.02330209
Iteration 47, loss = 0.02315969
Iteration 48, loss = 0.02298792
Iteration 49, loss = 0.02287298
Iteration 50, loss = 0.02299236
Iteration 51, loss = 0.02264355
Iteration 52, loss = 0.02285960
Iteration 53, loss = 0.02283057
Iteration 54, loss = 0.02280332
Iteration 55, loss = 0.02248466
Iteration 56, loss = 0.02220698
Iteration 57, loss = 0.02224066
Iteration 58, loss = 0.02236277
Iteration 59, loss = 0.02211716
Iteration 60, loss = 0.02225225
Iteration 61, loss = 0.02213266
Iteration 62, loss = 0.02210593
Iteration 63, loss = 0.02224359
Iteration 64, loss = 0.02181535
Iteration 65, loss = 0.02167761
Iteration 66, loss = 0.02208233
Iteration 67, loss = 0.02173781
Iteration 68, loss = 0.02169669
Iteration 69, loss = 0.02204623
Iteration 70, loss = 0.02188285
Iteration 71, loss = 0.02161976
Iteration 72, loss = 0.02179101
Iteration 73, loss = 0.02150181
Iteration 74, loss = 0.02171536
Iteration 75, loss = 0.02145466
Iteration 76, loss = 0.02144218
Iteration 77, loss = 0.02160705
Iteration 78, loss = 0.02121865
Iteration 79, loss = 0.02138325
Iteration 80, loss = 0.02156293
Iteration 81, loss = 0.02136841
Iteration 82, loss = 0.02148016
Iteration 83, loss = 0.02147303
Iteration 84, loss = 0.02156214
Iteration 85, loss = 0.02114093
Iteration 86, loss = 0.02119678
Iteration 87, loss = 0.02143412
Iteration 88, loss = 0.02159264
Iteration 89, loss = 0.02134211
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70807586
Iteration 2, loss = 0.63298868
Iteration 3, loss = 0.53124194
Iteration 4, loss = 0.41772439
Iteration 5, loss = 0.31524733
Iteration 6, loss = 0.23592397
Iteration 7, loss = 0.17935981
Iteration 8, loss = 0.14024348
Iteration 9, loss = 0.11320964
Iteration 10, loss = 0.09438028
Iteration 11, loss = 0.08060308
Iteration 12, loss = 0.07032840
Iteration 13, loss = 0.06239015
Iteration 14, loss = 0.05624564
Iteration 15, loss = 0.05133975
Iteration 16, loss = 0.04725190
Iteration 17, loss = 0.04433000
Iteration 18, loss = 0.04140265
Iteration 19, loss = 0.03930447
Iteration 20, loss = 0.03732285
Iteration 21, loss = 0.03547742
Iteration 22, loss = 0.03414498
Iteration 23, loss = 0.03289253
Iteration 24, loss = 0.03172018
Iteration 25, loss = 0.03089099
Iteration 26, loss = 0.03010949
Iteration 27, loss = 0.02920078
Iteration 28, loss = 0.02882171
Iteration 29, loss = 0.02814798
Iteration 30, loss = 0.02773182
Iteration 31, loss = 0.02697733
Iteration 32, loss = 0.02685048
Iteration 33, loss = 0.02635751
Iteration 34, loss = 0.02609180
Iteration 35, loss = 0.02572454
Iteration 36, loss = 0.02541928
Iteration 37, loss = 0.02521761
Iteration 38, loss = 0.02497070
Iteration 39, loss = 0.02479751
Iteration 40, loss = 0.02441325
Iteration 41, loss = 0.02451671
Iteration 42, loss = 0.02418626
Iteration 43, loss = 0.02415106
Iteration 44, loss = 0.02415005
Iteration 45, loss = 0.02360011
Iteration 46, loss = 0.02365238
Iteration 47, loss = 0.02338360
Iteration 48, loss = 0.02348682
Iteration 49, loss = 0.02324578
Iteration 50, loss = 0.02325735
Iteration 51, loss = 0.02331569
Iteration 52, loss = 0.02317055
Iteration 53, loss = 0.02291543
Iteration 54, loss = 0.02265213
Iteration 55, loss = 0.02261044
Iteration 56, loss = 0.02271345
Iteration 57, loss = 0.02239445
Iteration 58, loss = 0.02268556
Iteration 59, loss = 0.02239944
Iteration 60, loss = 0.02241653
Iteration 61, loss = 0.02232900
Iteration 62, loss = 0.02230943
Iteration 63, loss = 0.02225298
Iteration 64, loss = 0.02215308
Iteration 65, loss = 0.02220016
Iteration 66, loss = 0.02191284
Iteration 67, loss = 0.02196932
Iteration 68, loss = 0.02208916
Iteration 69, loss = 0.02237996
Iteration 70, loss = 0.02186722
Iteration 71, loss = 0.02217276
Iteration 72, loss = 0.02183088
Iteration 73, loss = 0.02207672
Iteration 74, loss = 0.02191841
Iteration 75, loss = 0.02191458
Iteration 76, loss = 0.02185126
Iteration 77, loss = 0.02167823
Iteration 78, loss = 0.02168050
Iteration 79, loss = 0.02191087
Iteration 80, loss = 0.02139180
Iteration 81, loss = 0.02169366
Iteration 82, loss = 0.02148659
Iteration 83, loss = 0.02162477
Iteration 84, loss = 0.02168415
Iteration 85, loss = 0.02154376
Iteration 86, loss = 0.02148681
Iteration 87, loss = 0.02148294
Iteration 88, loss = 0.02150230
Iteration 89, loss = 0.02152260
Iteration 90, loss = 0.02161400
Iteration 91, loss = 0.02158926
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68960905
Iteration 2, loss = 0.59528541
Iteration 3, loss = 0.46086253
Iteration 4, loss = 0.33547648
Iteration 5, loss = 0.23993295
Iteration 6, loss = 0.17634314
Iteration 7, loss = 0.13484337
Iteration 8, loss = 0.10756173
Iteration 9, loss = 0.08924154
Iteration 10, loss = 0.07616122
Iteration 11, loss = 0.06677679
Iteration 12, loss = 0.05966990
Iteration 13, loss = 0.05428943
Iteration 14, loss = 0.04984886
Iteration 15, loss = 0.04623269
Iteration 16, loss = 0.04362802
Iteration 17, loss = 0.04116520
Iteration 18, loss = 0.03922589
Iteration 19, loss = 0.03789997
Iteration 20, loss = 0.03625299
Iteration 21, loss = 0.03514647
Iteration 22, loss = 0.03426462
Iteration 23, loss = 0.03305351
Iteration 24, loss = 0.03233293
Iteration 25, loss = 0.03200569
Iteration 26, loss = 0.03109016
Iteration 27, loss = 0.03048209
Iteration 28, loss = 0.03021626
Iteration 29, loss = 0.02972739
Iteration 30, loss = 0.02935454
Iteration 31, loss = 0.02882419
Iteration 32, loss = 0.02910025
Iteration 33, loss = 0.02883316
Iteration 34, loss = 0.02809604
Iteration 35, loss = 0.02837613
Iteration 36, loss = 0.02784256
Iteration 37, loss = 0.02775860
Iteration 38, loss = 0.02742122
Iteration 39, loss = 0.02709853
Iteration 40, loss = 0.02698975
Iteration 41, loss = 0.02715294
Iteration 42, loss = 0.02673728
Iteration 43, loss = 0.02693839
Iteration 44, loss = 0.02671461
Iteration 45, loss = 0.02639781
Iteration 46, loss = 0.02630002
Iteration 47, loss = 0.02639379
Iteration 48, loss = 0.02624461
Iteration 49, loss = 0.02626358
Iteration 50, loss = 0.02585830
Iteration 51, loss = 0.02626656
Iteration 52, loss = 0.02590222
Iteration 53, loss = 0.02579126
Iteration 54, loss = 0.02599418
Iteration 55, loss = 0.02611713
Iteration 56, loss = 0.02572712
Iteration 57, loss = 0.02568368
Iteration 58, loss = 0.02520773
Iteration 59, loss = 0.02575191
Iteration 60, loss = 0.02533909
Iteration 61, loss = 0.02526213
Iteration 62, loss = 0.02567752
Iteration 63, loss = 0.02551588
Iteration 64, loss = 0.02557053
Iteration 65, loss = 0.02519777
Iteration 66, loss = 0.02526398
Iteration 67, loss = 0.02537860
Iteration 68, loss = 0.02525684
Iteration 69, loss = 0.02516507
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on dropped_v3_lemmatized...
Iteration 1, loss = 0.68032332
Iteration 2, loss = 0.66406350
Iteration 3, loss = 0.64790972
Iteration 4, loss = 0.62889457
Iteration 5, loss = 0.60571803
Iteration 6, loss = 0.57867660
Iteration 7, loss = 0.54813376
Iteration 8, loss = 0.51512387
Iteration 9, loss = 0.48068011
Iteration 10, loss = 0.44604295
Iteration 11, loss = 0.41200207
Iteration 12, loss = 0.37929349
Iteration 13, loss = 0.34763098
Iteration 14, loss = 0.31800941
Iteration 15, loss = 0.29022458
Iteration 16, loss = 0.26467720
Iteration 17, loss = 0.24123919
Iteration 18, loss = 0.22002798
Iteration 19, loss = 0.20049911
Iteration 20, loss = 0.18321620
Iteration 21, loss = 0.16749765
Iteration 22, loss = 0.15356321
Iteration 23, loss = 0.14115894
Iteration 24, loss = 0.12995687
Iteration 25, loss = 0.12003668
Iteration 26, loss = 0.11112319
Iteration 27, loss = 0.10313841
Iteration 28, loss = 0.09604713
Iteration 29, loss = 0.08969740
Iteration 30, loss = 0.08395940
Iteration 31, loss = 0.07884084
Iteration 32, loss = 0.07414037
Iteration 33, loss = 0.06994382
Iteration 34, loss = 0.06607680
Iteration 35, loss = 0.06262120
Iteration 36, loss = 0.05942878
Iteration 37, loss = 0.05652061
Iteration 38, loss = 0.05387099
Iteration 39, loss = 0.05142288
Iteration 40, loss = 0.04923432
Iteration 41, loss = 0.04711114
Iteration 42, loss = 0.04515674
Iteration 43, loss = 0.04341760
Iteration 44, loss = 0.04176391
Iteration 45, loss = 0.04026540
Iteration 46, loss = 0.03884409
Iteration 47, loss = 0.03748317
Iteration 48, loss = 0.03624813
Iteration 49, loss = 0.03507997
Iteration 50, loss = 0.03400947
Iteration 51, loss = 0.03295556
Iteration 52, loss = 0.03205090
Iteration 53, loss = 0.03117230
Iteration 54, loss = 0.03025557
Iteration 55, loss = 0.02947791
Iteration 56, loss = 0.02874791
Iteration 57, loss = 0.02801927
Iteration 58, loss = 0.02735552
Iteration 59, loss = 0.02668644
Iteration 60, loss = 0.02609124
Iteration 61, loss = 0.02552681
Iteration 62, loss = 0.02497281
Iteration 63, loss = 0.02445654
Iteration 64, loss = 0.02398338
Iteration 65, loss = 0.02348393
Iteration 66, loss = 0.02307709
Iteration 67, loss = 0.02264597
Iteration 68, loss = 0.02225829
Iteration 69, loss = 0.02190722
Iteration 70, loss = 0.02151031
Iteration 71, loss = 0.02115085
Iteration 72, loss = 0.02079552
Iteration 73, loss = 0.02049279
Iteration 74, loss = 0.02014675
Iteration 75, loss = 0.01986942
Iteration 76, loss = 0.01958952
Iteration 77, loss = 0.01932109
Iteration 78, loss = 0.01904603
Iteration 79, loss = 0.01885002
Iteration 80, loss = 0.01859667
Iteration 81, loss = 0.01837100
Iteration 82, loss = 0.01818715
Iteration 83, loss = 0.01789904
Iteration 84, loss = 0.01771079
Iteration 85, loss = 0.01749991
Iteration 86, loss = 0.01732344
Iteration 87, loss = 0.01716020
Iteration 88, loss = 0.01695484
Iteration 89, loss = 0.01686958
Iteration 90, loss = 0.01660919
Iteration 91, loss = 0.01646978
Iteration 92, loss = 0.01628715
Iteration 93, loss = 0.01613854
Iteration 94, loss = 0.01601197
Iteration 95, loss = 0.01584289
Iteration 96, loss = 0.01574174
Iteration 97, loss = 0.01563385
Iteration 98, loss = 0.01547912
Iteration 99, loss = 0.01533179
Iteration 100, loss = 0.01522915
Iteration 101, loss = 0.01511664
Iteration 102, loss = 0.01500849
Iteration 103, loss = 0.01489371
Iteration 104, loss = 0.01478051
Iteration 105, loss = 0.01471899
Iteration 106, loss = 0.01458828
Iteration 107, loss = 0.01452539
Iteration 108, loss = 0.01439122
Iteration 109, loss = 0.01429994
Iteration 110, loss = 0.01427534
Iteration 111, loss = 0.01420697
Iteration 112, loss = 0.01405328
Iteration 113, loss = 0.01397362
Iteration 114, loss = 0.01387595
Iteration 115, loss = 0.01380531
Iteration 116, loss = 0.01374390
Iteration 117, loss = 0.01368105
Iteration 118, loss = 0.01360908
Iteration 119, loss = 0.01350807
Iteration 120, loss = 0.01346091
Iteration 121, loss = 0.01342237
Iteration 122, loss = 0.01335410
Iteration 123, loss = 0.01330500
Iteration 124, loss = 0.01321950
Iteration 125, loss = 0.01315371
Iteration 126, loss = 0.01308971
Iteration 127, loss = 0.01302119
Iteration 128, loss = 0.01299940
Iteration 129, loss = 0.01293152
Iteration 130, loss = 0.01290278
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68178420
Iteration 2, loss = 0.64066555
Iteration 3, loss = 0.57901279
Iteration 4, loss = 0.49888816
Iteration 5, loss = 0.41873519
Iteration 6, loss = 0.34498995
Iteration 7, loss = 0.28185625
Iteration 8, loss = 0.23056828
Iteration 9, loss = 0.18980696
Iteration 10, loss = 0.15827790
Iteration 11, loss = 0.13371603
Iteration 12, loss = 0.11464653
Iteration 13, loss = 0.09978927
Iteration 14, loss = 0.08812967
Iteration 15, loss = 0.07887227
Iteration 16, loss = 0.07101585
Iteration 17, loss = 0.06481891
Iteration 18, loss = 0.05962224
Iteration 19, loss = 0.05537480
Iteration 20, loss = 0.05150313
Iteration 21, loss = 0.04848408
Iteration 22, loss = 0.04604521
Iteration 23, loss = 0.04356589
Iteration 24, loss = 0.04135881
Iteration 25, loss = 0.03951021
Iteration 26, loss = 0.03801107
Iteration 27, loss = 0.03658263
Iteration 28, loss = 0.03535911
Iteration 29, loss = 0.03406514
Iteration 30, loss = 0.03337709
Iteration 31, loss = 0.03257655
Iteration 32, loss = 0.03148807
Iteration 33, loss = 0.03072305
Iteration 34, loss = 0.02994846
Iteration 35, loss = 0.02956678
Iteration 36, loss = 0.02907623
Iteration 37, loss = 0.02830829
Iteration 38, loss = 0.02781670
Iteration 39, loss = 0.02730231
Iteration 40, loss = 0.02698724
Iteration 41, loss = 0.02672434
Iteration 42, loss = 0.02652330
Iteration 43, loss = 0.02605701
Iteration 44, loss = 0.02581454
Iteration 45, loss = 0.02528053
Iteration 46, loss = 0.02523824
Iteration 47, loss = 0.02513235
Iteration 48, loss = 0.02471740
Iteration 49, loss = 0.02448720
Iteration 50, loss = 0.02426840
Iteration 51, loss = 0.02419729
Iteration 52, loss = 0.02384980
Iteration 53, loss = 0.02371379
Iteration 54, loss = 0.02365869
Iteration 55, loss = 0.02358890
Iteration 56, loss = 0.02326020
Iteration 57, loss = 0.02343918
Iteration 58, loss = 0.02336693
Iteration 59, loss = 0.02299716
Iteration 60, loss = 0.02298174
Iteration 61, loss = 0.02267521
Iteration 62, loss = 0.02273616
Iteration 63, loss = 0.02285241
Iteration 64, loss = 0.02251588
Iteration 65, loss = 0.02224375
Iteration 66, loss = 0.02228099
Iteration 67, loss = 0.02231431
Iteration 68, loss = 0.02225492
Iteration 69, loss = 0.02209181
Iteration 70, loss = 0.02214800
Iteration 71, loss = 0.02202486
Iteration 72, loss = 0.02183725
Iteration 73, loss = 0.02195227
Iteration 74, loss = 0.02181990
Iteration 75, loss = 0.02191549
Iteration 76, loss = 0.02166006
Iteration 77, loss = 0.02164695
Iteration 78, loss = 0.02146034
Iteration 79, loss = 0.02152696
Iteration 80, loss = 0.02142269
Iteration 81, loss = 0.02143232
Iteration 82, loss = 0.02158103
Iteration 83, loss = 0.02152350
Iteration 84, loss = 0.02127201
Iteration 85, loss = 0.02127280
Iteration 86, loss = 0.02125794
Iteration 87, loss = 0.02138285
Iteration 88, loss = 0.02101673
Iteration 89, loss = 0.02116672
Iteration 90, loss = 0.02118836
Iteration 91, loss = 0.02107750
Iteration 92, loss = 0.02122554
Iteration 93, loss = 0.02136249
Iteration 94, loss = 0.02128908
Iteration 95, loss = 0.02109926
Iteration 96, loss = 0.02119170
Iteration 97, loss = 0.02130131
Iteration 98, loss = 0.02096401
Iteration 99, loss = 0.02102978
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67806435
Iteration 2, loss = 0.65923094
Iteration 3, loss = 0.64093104
Iteration 4, loss = 0.61971640
Iteration 5, loss = 0.59491764
Iteration 6, loss = 0.56639856
Iteration 7, loss = 0.53487485
Iteration 8, loss = 0.50162830
Iteration 9, loss = 0.46741380
Iteration 10, loss = 0.43338941
Iteration 11, loss = 0.40018551
Iteration 12, loss = 0.36813276
Iteration 13, loss = 0.33751801
Iteration 14, loss = 0.30921457
Iteration 15, loss = 0.28248126
Iteration 16, loss = 0.25787542
Iteration 17, loss = 0.23545724
Iteration 18, loss = 0.21507047
Iteration 19, loss = 0.19663570
Iteration 20, loss = 0.18000966
Iteration 21, loss = 0.16508712
Iteration 22, loss = 0.15182569
Iteration 23, loss = 0.13990064
Iteration 24, loss = 0.12929683
Iteration 25, loss = 0.11975650
Iteration 26, loss = 0.11120334
Iteration 27, loss = 0.10352307
Iteration 28, loss = 0.09663204
Iteration 29, loss = 0.09051666
Iteration 30, loss = 0.08488225
Iteration 31, loss = 0.07981278
Iteration 32, loss = 0.07523972
Iteration 33, loss = 0.07104930
Iteration 34, loss = 0.06731557
Iteration 35, loss = 0.06393215
Iteration 36, loss = 0.06076297
Iteration 37, loss = 0.05782510
Iteration 38, loss = 0.05521199
Iteration 39, loss = 0.05270618
Iteration 40, loss = 0.05049913
Iteration 41, loss = 0.04844627
Iteration 42, loss = 0.04647497
Iteration 43, loss = 0.04469500
Iteration 44, loss = 0.04306017
Iteration 45, loss = 0.04150341
Iteration 46, loss = 0.04007523
Iteration 47, loss = 0.03873806
Iteration 48, loss = 0.03749461
Iteration 49, loss = 0.03628788
Iteration 50, loss = 0.03514212
Iteration 51, loss = 0.03415738
Iteration 52, loss = 0.03317961
Iteration 53, loss = 0.03226719
Iteration 54, loss = 0.03139321
Iteration 55, loss = 0.03060411
Iteration 56, loss = 0.02989204
Iteration 57, loss = 0.02910340
Iteration 58, loss = 0.02845441
Iteration 59, loss = 0.02778900
Iteration 60, loss = 0.02715132
Iteration 61, loss = 0.02658867
Iteration 62, loss = 0.02609033
Iteration 63, loss = 0.02555404
Iteration 64, loss = 0.02502631
Iteration 65, loss = 0.02454169
Iteration 66, loss = 0.02406167
Iteration 67, loss = 0.02365723
Iteration 68, loss = 0.02328635
Iteration 69, loss = 0.02280439
Iteration 70, loss = 0.02248319
Iteration 71, loss = 0.02214498
Iteration 72, loss = 0.02173809
Iteration 73, loss = 0.02142216
Iteration 74, loss = 0.02109752
Iteration 75, loss = 0.02080163
Iteration 76, loss = 0.02051071
Iteration 77, loss = 0.02021677
Iteration 78, loss = 0.02005088
Iteration 79, loss = 0.01972521
Iteration 80, loss = 0.01949074
Iteration 81, loss = 0.01922782
Iteration 82, loss = 0.01897383
Iteration 83, loss = 0.01877185
Iteration 84, loss = 0.01852154
Iteration 85, loss = 0.01834966
Iteration 86, loss = 0.01815931
Iteration 87, loss = 0.01800257
Iteration 88, loss = 0.01778161
Iteration 89, loss = 0.01757680
Iteration 90, loss = 0.01743361
Iteration 91, loss = 0.01728445
Iteration 92, loss = 0.01711135
Iteration 93, loss = 0.01700238
Iteration 94, loss = 0.01681550
Iteration 95, loss = 0.01666391
Iteration 96, loss = 0.01654592
Iteration 97, loss = 0.01639930
Iteration 98, loss = 0.01627447
Iteration 99, loss = 0.01616071
Iteration 100, loss = 0.01603676
Iteration 101, loss = 0.01594464
Iteration 102, loss = 0.01581767
Iteration 103, loss = 0.01563132
Iteration 104, loss = 0.01559466
Iteration 105, loss = 0.01550554
Iteration 106, loss = 0.01539312
Iteration 107, loss = 0.01532882
Iteration 108, loss = 0.01515122
Iteration 109, loss = 0.01504978
Iteration 110, loss = 0.01503241
Iteration 111, loss = 0.01491946
Iteration 112, loss = 0.01481053
Iteration 113, loss = 0.01483162
Iteration 114, loss = 0.01463804
Iteration 115, loss = 0.01455958
Iteration 116, loss = 0.01448385
Iteration 117, loss = 0.01445132
Iteration 118, loss = 0.01435683
Iteration 119, loss = 0.01426700
Iteration 120, loss = 0.01425226
Iteration 121, loss = 0.01412430
Iteration 122, loss = 0.01408020
Iteration 123, loss = 0.01400361
Iteration 124, loss = 0.01391993
Iteration 125, loss = 0.01391283
Iteration 126, loss = 0.01384666
Iteration 127, loss = 0.01375389
Iteration 128, loss = 0.01370302
Iteration 129, loss = 0.01370106
Iteration 130, loss = 0.01361725
Iteration 131, loss = 0.01355947
Iteration 132, loss = 0.01352125
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71181929
Iteration 2, loss = 0.68212817
Iteration 3, loss = 0.63986271
Iteration 4, loss = 0.58399117
Iteration 5, loss = 0.51741486
Iteration 6, loss = 0.44714598
Iteration 7, loss = 0.37958555
Iteration 8, loss = 0.31911400
Iteration 9, loss = 0.26663378
Iteration 10, loss = 0.22326760
Iteration 11, loss = 0.18782331
Iteration 12, loss = 0.15929076
Iteration 13, loss = 0.13663578
Iteration 14, loss = 0.11818880
Iteration 15, loss = 0.10354475
Iteration 16, loss = 0.09162961
Iteration 17, loss = 0.08181982
Iteration 18, loss = 0.07378339
Iteration 19, loss = 0.06695074
Iteration 20, loss = 0.06114977
Iteration 21, loss = 0.05638281
Iteration 22, loss = 0.05219764
Iteration 23, loss = 0.04855520
Iteration 24, loss = 0.04557770
Iteration 25, loss = 0.04270828
Iteration 26, loss = 0.04035535
Iteration 27, loss = 0.03833497
Iteration 28, loss = 0.03640301
Iteration 29, loss = 0.03477703
Iteration 30, loss = 0.03316566
Iteration 31, loss = 0.03197068
Iteration 32, loss = 0.03074085
Iteration 33, loss = 0.02962279
Iteration 34, loss = 0.02867764
Iteration 35, loss = 0.02774797
Iteration 36, loss = 0.02688408
Iteration 37, loss = 0.02623651
Iteration 38, loss = 0.02547728
Iteration 39, loss = 0.02481468
Iteration 40, loss = 0.02426630
Iteration 41, loss = 0.02373489
Iteration 42, loss = 0.02339585
Iteration 43, loss = 0.02276207
Iteration 44, loss = 0.02236552
Iteration 45, loss = 0.02203670
Iteration 46, loss = 0.02165810
Iteration 47, loss = 0.02126506
Iteration 48, loss = 0.02099878
Iteration 49, loss = 0.02060066
Iteration 50, loss = 0.02041046
Iteration 51, loss = 0.02014266
Iteration 52, loss = 0.01992530
Iteration 53, loss = 0.01959075
Iteration 54, loss = 0.01943086
Iteration 55, loss = 0.01926569
Iteration 56, loss = 0.01900469
Iteration 57, loss = 0.01889935
Iteration 58, loss = 0.01862999
Iteration 59, loss = 0.01856866
Iteration 60, loss = 0.01836778
Iteration 61, loss = 0.01832942
Iteration 62, loss = 0.01812592
Iteration 63, loss = 0.01810238
Iteration 64, loss = 0.01780174
Iteration 65, loss = 0.01770756
Iteration 66, loss = 0.01758826
Iteration 67, loss = 0.01751563
Iteration 68, loss = 0.01744784
Iteration 69, loss = 0.01733001
Iteration 70, loss = 0.01715177
Iteration 71, loss = 0.01709772
Iteration 72, loss = 0.01704124
Iteration 73, loss = 0.01700357
Iteration 74, loss = 0.01684311
Iteration 75, loss = 0.01679503
Iteration 76, loss = 0.01674728
Iteration 77, loss = 0.01660884
Iteration 78, loss = 0.01656928
Iteration 79, loss = 0.01658641
Iteration 80, loss = 0.01640756
Iteration 81, loss = 0.01638669
Iteration 82, loss = 0.01636020
Iteration 83, loss = 0.01634214
Iteration 84, loss = 0.01625871
Iteration 85, loss = 0.01607273
Iteration 86, loss = 0.01615068
Iteration 87, loss = 0.01605388
Iteration 88, loss = 0.01604936
Iteration 89, loss = 0.01605299
Iteration 90, loss = 0.01585144
Iteration 91, loss = 0.01591642
Iteration 92, loss = 0.01585863
Iteration 93, loss = 0.01584049
Iteration 94, loss = 0.01596146
Iteration 95, loss = 0.01585049
Iteration 96, loss = 0.01565129
Iteration 97, loss = 0.01568187
Iteration 98, loss = 0.01588496
Iteration 99, loss = 0.01558548
Iteration 100, loss = 0.01556713
Iteration 101, loss = 0.01551677
Iteration 102, loss = 0.01557030
Iteration 103, loss = 0.01556338
Iteration 104, loss = 0.01533908
Iteration 105, loss = 0.01541471
Iteration 106, loss = 0.01532289
Iteration 107, loss = 0.01540292
Iteration 108, loss = 0.01543638
Iteration 109, loss = 0.01527937
Iteration 110, loss = 0.01536943
Iteration 111, loss = 0.01526896
Iteration 112, loss = 0.01535004
Iteration 113, loss = 0.01516271
Iteration 114, loss = 0.01525305
Iteration 115, loss = 0.01527107
Iteration 116, loss = 0.01534150
Iteration 117, loss = 0.01512846
Iteration 118, loss = 0.01509481
Iteration 119, loss = 0.01511941
Iteration 120, loss = 0.01509893
Iteration 121, loss = 0.01511661
Iteration 122, loss = 0.01508041
Iteration 123, loss = 0.01498614
Iteration 124, loss = 0.01506074
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68190604
Iteration 2, loss = 0.63015224
Iteration 3, loss = 0.56222902
Iteration 4, loss = 0.47886107
Iteration 5, loss = 0.39611269
Iteration 6, loss = 0.32266931
Iteration 7, loss = 0.26172492
Iteration 8, loss = 0.21317995
Iteration 9, loss = 0.17531826
Iteration 10, loss = 0.14658282
Iteration 11, loss = 0.12436930
Iteration 12, loss = 0.10710986
Iteration 13, loss = 0.09371064
Iteration 14, loss = 0.08298006
Iteration 15, loss = 0.07433792
Iteration 16, loss = 0.06758753
Iteration 17, loss = 0.06160157
Iteration 18, loss = 0.05683010
Iteration 19, loss = 0.05274915
Iteration 20, loss = 0.04948126
Iteration 21, loss = 0.04658151
Iteration 22, loss = 0.04391505
Iteration 23, loss = 0.04183509
Iteration 24, loss = 0.03990099
Iteration 25, loss = 0.03833930
Iteration 26, loss = 0.03661681
Iteration 27, loss = 0.03543280
Iteration 28, loss = 0.03414663
Iteration 29, loss = 0.03334815
Iteration 30, loss = 0.03238795
Iteration 31, loss = 0.03175543
Iteration 32, loss = 0.03094368
Iteration 33, loss = 0.03006527
Iteration 34, loss = 0.02923534
Iteration 35, loss = 0.02866294
Iteration 36, loss = 0.02821100
Iteration 37, loss = 0.02774864
Iteration 38, loss = 0.02726430
Iteration 39, loss = 0.02679953
Iteration 40, loss = 0.02663785
Iteration 41, loss = 0.02630660
Iteration 42, loss = 0.02582350
Iteration 43, loss = 0.02552066
Iteration 44, loss = 0.02524446
Iteration 45, loss = 0.02510412
Iteration 46, loss = 0.02477115
Iteration 47, loss = 0.02449668
Iteration 48, loss = 0.02474145
Iteration 49, loss = 0.02438693
Iteration 50, loss = 0.02402456
Iteration 51, loss = 0.02385339
Iteration 52, loss = 0.02368119
Iteration 53, loss = 0.02345825
Iteration 54, loss = 0.02329378
Iteration 55, loss = 0.02321774
Iteration 56, loss = 0.02296781
Iteration 57, loss = 0.02275860
Iteration 58, loss = 0.02279008
Iteration 59, loss = 0.02251391
Iteration 60, loss = 0.02266943
Iteration 61, loss = 0.02236850
Iteration 62, loss = 0.02226190
Iteration 63, loss = 0.02233686
Iteration 64, loss = 0.02217876
Iteration 65, loss = 0.02196927
Iteration 66, loss = 0.02235612
Iteration 67, loss = 0.02200160
Iteration 68, loss = 0.02198229
Iteration 69, loss = 0.02179490
Iteration 70, loss = 0.02180110
Iteration 71, loss = 0.02173082
Iteration 72, loss = 0.02174541
Iteration 73, loss = 0.02182052
Iteration 74, loss = 0.02164151
Iteration 75, loss = 0.02171501
Iteration 76, loss = 0.02142774
Iteration 77, loss = 0.02135032
Iteration 78, loss = 0.02128386
Iteration 79, loss = 0.02140046
Iteration 80, loss = 0.02121979
Iteration 81, loss = 0.02099016
Iteration 82, loss = 0.02108922
Iteration 83, loss = 0.02107894
Iteration 84, loss = 0.02138885
Iteration 85, loss = 0.02104990
Iteration 86, loss = 0.02103474
Iteration 87, loss = 0.02089958
Iteration 88, loss = 0.02106804
Iteration 89, loss = 0.02094197
Iteration 90, loss = 0.02089370
Iteration 91, loss = 0.02075190
Iteration 92, loss = 0.02098152
Iteration 93, loss = 0.02093062
Iteration 94, loss = 0.02079689
Iteration 95, loss = 0.02084726
Iteration 96, loss = 0.02063226
Iteration 97, loss = 0.02088062
Iteration 98, loss = 0.02042619
Iteration 99, loss = 0.02070614
Iteration 100, loss = 0.02062440
Iteration 101, loss = 0.02064520
Iteration 102, loss = 0.02068950
Iteration 103, loss = 0.02049966
Iteration 104, loss = 0.02058924
Iteration 105, loss = 0.02058673
Iteration 106, loss = 0.02058544
Iteration 107, loss = 0.02099484
Iteration 108, loss = 0.02043445
Iteration 109, loss = 0.02064074
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70148757
Iteration 2, loss = 0.66830603
Iteration 3, loss = 0.62812203
Iteration 4, loss = 0.57843590
Iteration 5, loss = 0.51815565
Iteration 6, loss = 0.45504645
Iteration 7, loss = 0.39446493
Iteration 8, loss = 0.33871232
Iteration 9, loss = 0.28932340
Iteration 10, loss = 0.24694818
Iteration 11, loss = 0.21090612
Iteration 12, loss = 0.18121451
Iteration 13, loss = 0.15654849
Iteration 14, loss = 0.13631958
Iteration 15, loss = 0.11962831
Iteration 16, loss = 0.10591534
Iteration 17, loss = 0.09456058
Iteration 18, loss = 0.08500736
Iteration 19, loss = 0.07692684
Iteration 20, loss = 0.07030040
Iteration 21, loss = 0.06436339
Iteration 22, loss = 0.05948324
Iteration 23, loss = 0.05513919
Iteration 24, loss = 0.05152659
Iteration 25, loss = 0.04825973
Iteration 26, loss = 0.04534823
Iteration 27, loss = 0.04293133
Iteration 28, loss = 0.04068481
Iteration 29, loss = 0.03860254
Iteration 30, loss = 0.03696274
Iteration 31, loss = 0.03530770
Iteration 32, loss = 0.03403820
Iteration 33, loss = 0.03256632
Iteration 34, loss = 0.03155766
Iteration 35, loss = 0.03040598
Iteration 36, loss = 0.02936048
Iteration 37, loss = 0.02851853
Iteration 38, loss = 0.02776425
Iteration 39, loss = 0.02692510
Iteration 40, loss = 0.02632402
Iteration 41, loss = 0.02571950
Iteration 42, loss = 0.02502548
Iteration 43, loss = 0.02446877
Iteration 44, loss = 0.02403789
Iteration 45, loss = 0.02342225
Iteration 46, loss = 0.02311013
Iteration 47, loss = 0.02261706
Iteration 48, loss = 0.02225128
Iteration 49, loss = 0.02193540
Iteration 50, loss = 0.02167452
Iteration 51, loss = 0.02131969
Iteration 52, loss = 0.02102491
Iteration 53, loss = 0.02073754
Iteration 54, loss = 0.02049466
Iteration 55, loss = 0.02022784
Iteration 56, loss = 0.02000884
Iteration 57, loss = 0.01975965
Iteration 58, loss = 0.01963029
Iteration 59, loss = 0.01941488
Iteration 60, loss = 0.01919511
Iteration 61, loss = 0.01900287
Iteration 62, loss = 0.01889725
Iteration 63, loss = 0.01880218
Iteration 64, loss = 0.01857024
Iteration 65, loss = 0.01833043
Iteration 66, loss = 0.01824000
Iteration 67, loss = 0.01805771
Iteration 68, loss = 0.01794457
Iteration 69, loss = 0.01784562
Iteration 70, loss = 0.01782607
Iteration 71, loss = 0.01769784
Iteration 72, loss = 0.01764095
Iteration 73, loss = 0.01752142
Iteration 74, loss = 0.01732478
Iteration 75, loss = 0.01730304
Iteration 76, loss = 0.01712092
Iteration 77, loss = 0.01712822
Iteration 78, loss = 0.01707367
Iteration 79, loss = 0.01698358
Iteration 80, loss = 0.01699755
Iteration 81, loss = 0.01686832
Iteration 82, loss = 0.01683638
Iteration 83, loss = 0.01676477
Iteration 84, loss = 0.01662397
Iteration 85, loss = 0.01653291
Iteration 86, loss = 0.01645398
Iteration 87, loss = 0.01642841
Iteration 88, loss = 0.01641261
Iteration 89, loss = 0.01635114
Iteration 90, loss = 0.01643621
Iteration 91, loss = 0.01629705
Iteration 92, loss = 0.01630642
Iteration 93, loss = 0.01627324
Iteration 94, loss = 0.01620866
Iteration 95, loss = 0.01607062
Iteration 96, loss = 0.01612751
Iteration 97, loss = 0.01598032
Iteration 98, loss = 0.01587481
Iteration 99, loss = 0.01598554
Iteration 100, loss = 0.01582922
Iteration 101, loss = 0.01576801
Iteration 102, loss = 0.01580916
Iteration 103, loss = 0.01582776
Iteration 104, loss = 0.01575579
Iteration 105, loss = 0.01570915
Iteration 106, loss = 0.01579523
Iteration 107, loss = 0.01587919
Iteration 108, loss = 0.01567480
Iteration 109, loss = 0.01576704
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67479072
Iteration 2, loss = 0.65843760
Iteration 3, loss = 0.64516921
Iteration 4, loss = 0.63260716
Iteration 5, loss = 0.61946687
Iteration 6, loss = 0.60537468
Iteration 7, loss = 0.58983368
Iteration 8, loss = 0.57285252
Iteration 9, loss = 0.55455252
Iteration 10, loss = 0.53488129
Iteration 11, loss = 0.51404597
Iteration 12, loss = 0.49218455
Iteration 13, loss = 0.46954765
Iteration 14, loss = 0.44648274
Iteration 15, loss = 0.42313233
Iteration 16, loss = 0.39999299
Iteration 17, loss = 0.37716099
Iteration 18, loss = 0.35482662
Iteration 19, loss = 0.33333065
Iteration 20, loss = 0.31260950
Iteration 21, loss = 0.29294437
Iteration 22, loss = 0.27436926
Iteration 23, loss = 0.25701166
Iteration 24, loss = 0.24063453
Iteration 25, loss = 0.22540306
Iteration 26, loss = 0.21110636
Iteration 27, loss = 0.19787080
Iteration 28, loss = 0.18560304
Iteration 29, loss = 0.17425108
Iteration 30, loss = 0.16370545
Iteration 31, loss = 0.15398667
Iteration 32, loss = 0.14499856
Iteration 33, loss = 0.13666741
Iteration 34, loss = 0.12897882
Iteration 35, loss = 0.12187920
Iteration 36, loss = 0.11531098
Iteration 37, loss = 0.10922208
Iteration 38, loss = 0.10356087
Iteration 39, loss = 0.09839198
Iteration 40, loss = 0.09361037
Iteration 41, loss = 0.08910232
Iteration 42, loss = 0.08492740
Iteration 43, loss = 0.08099820
Iteration 44, loss = 0.07743916
Iteration 45, loss = 0.07398872
Iteration 46, loss = 0.07084047
Iteration 47, loss = 0.06789130
Iteration 48, loss = 0.06517435
Iteration 49, loss = 0.06261352
Iteration 50, loss = 0.06014751
Iteration 51, loss = 0.05791160
Iteration 52, loss = 0.05578118
Iteration 53, loss = 0.05379837
Iteration 54, loss = 0.05190438
Iteration 55, loss = 0.05014355
Iteration 56, loss = 0.04846841
Iteration 57, loss = 0.04690134
Iteration 58, loss = 0.04543559
Iteration 59, loss = 0.04401245
Iteration 60, loss = 0.04265746
Iteration 61, loss = 0.04143669
Iteration 62, loss = 0.04022829
Iteration 63, loss = 0.03909259
Iteration 64, loss = 0.03799965
Iteration 65, loss = 0.03698932
Iteration 66, loss = 0.03603046
Iteration 67, loss = 0.03510811
Iteration 68, loss = 0.03421984
Iteration 69, loss = 0.03336609
Iteration 70, loss = 0.03251673
Iteration 71, loss = 0.03174347
Iteration 72, loss = 0.03101595
Iteration 73, loss = 0.03031511
Iteration 74, loss = 0.02965106
Iteration 75, loss = 0.02898431
Iteration 76, loss = 0.02836304
Iteration 77, loss = 0.02780001
Iteration 78, loss = 0.02724928
Iteration 79, loss = 0.02666483
Iteration 80, loss = 0.02612742
Iteration 81, loss = 0.02562832
Iteration 82, loss = 0.02514088
Iteration 83, loss = 0.02470080
Iteration 84, loss = 0.02420662
Iteration 85, loss = 0.02377180
Iteration 86, loss = 0.02334556
Iteration 87, loss = 0.02295384
Iteration 88, loss = 0.02256751
Iteration 89, loss = 0.02217760
Iteration 90, loss = 0.02181351
Iteration 91, loss = 0.02148491
Iteration 92, loss = 0.02113725
Iteration 93, loss = 0.02079121
Iteration 94, loss = 0.02048086
Iteration 95, loss = 0.02019928
Iteration 96, loss = 0.01990770
Iteration 97, loss = 0.01961738
Iteration 98, loss = 0.01932986
Iteration 99, loss = 0.01909316
Iteration 100, loss = 0.01882954
Iteration 101, loss = 0.01857086
Iteration 102, loss = 0.01830780
Iteration 103, loss = 0.01806861
Iteration 104, loss = 0.01785860
Iteration 105, loss = 0.01769717
Iteration 106, loss = 0.01744997
Iteration 107, loss = 0.01726397
Iteration 108, loss = 0.01705608
Iteration 109, loss = 0.01682426
Iteration 110, loss = 0.01661635
Iteration 111, loss = 0.01647245
Iteration 112, loss = 0.01627271
Iteration 113, loss = 0.01606980
Iteration 114, loss = 0.01591076
Iteration 115, loss = 0.01572423
Iteration 116, loss = 0.01559115
Iteration 117, loss = 0.01543110
Iteration 118, loss = 0.01529192
Iteration 119, loss = 0.01512668
Iteration 120, loss = 0.01499210
Iteration 121, loss = 0.01484587
Iteration 122, loss = 0.01470985
Iteration 123, loss = 0.01457769
Iteration 124, loss = 0.01447487
Iteration 125, loss = 0.01432010
Iteration 126, loss = 0.01418022
Iteration 127, loss = 0.01403470
Iteration 128, loss = 0.01391780
Iteration 129, loss = 0.01385069
Iteration 130, loss = 0.01369384
Iteration 131, loss = 0.01358477
Iteration 132, loss = 0.01347891
Iteration 133, loss = 0.01337817
Iteration 134, loss = 0.01326339
Iteration 135, loss = 0.01316637
Iteration 136, loss = 0.01309863
Iteration 137, loss = 0.01301596
Iteration 138, loss = 0.01289219
Iteration 139, loss = 0.01281505
Iteration 140, loss = 0.01268664
Iteration 141, loss = 0.01264350
Iteration 142, loss = 0.01253058
Iteration 143, loss = 0.01246252
Iteration 144, loss = 0.01237983
Iteration 145, loss = 0.01233027
Iteration 146, loss = 0.01223435
Iteration 147, loss = 0.01215390
Iteration 148, loss = 0.01209810
Iteration 149, loss = 0.01199287
Iteration 150, loss = 0.01191772
Iteration 151, loss = 0.01183775
Iteration 152, loss = 0.01178942
Iteration 153, loss = 0.01171103
Iteration 154, loss = 0.01162846
Iteration 155, loss = 0.01157493
Iteration 156, loss = 0.01150691
Iteration 157, loss = 0.01144554
Iteration 158, loss = 0.01138027
Iteration 159, loss = 0.01131782
Iteration 160, loss = 0.01125230
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67676655
Iteration 2, loss = 0.64954962
Iteration 3, loss = 0.61282990
Iteration 4, loss = 0.56390469
Iteration 5, loss = 0.50649234
Iteration 6, loss = 0.44594820
Iteration 7, loss = 0.38743171
Iteration 8, loss = 0.33327512
Iteration 9, loss = 0.28494810
Iteration 10, loss = 0.24329707
Iteration 11, loss = 0.20797949
Iteration 12, loss = 0.17835030
Iteration 13, loss = 0.15403063
Iteration 14, loss = 0.13390041
Iteration 15, loss = 0.11746665
Iteration 16, loss = 0.10379619
Iteration 17, loss = 0.09235565
Iteration 18, loss = 0.08292221
Iteration 19, loss = 0.07488152
Iteration 20, loss = 0.06814949
Iteration 21, loss = 0.06237106
Iteration 22, loss = 0.05723500
Iteration 23, loss = 0.05302846
Iteration 24, loss = 0.04925552
Iteration 25, loss = 0.04597205
Iteration 26, loss = 0.04308129
Iteration 27, loss = 0.04045403
Iteration 28, loss = 0.03814303
Iteration 29, loss = 0.03610814
Iteration 30, loss = 0.03427972
Iteration 31, loss = 0.03259166
Iteration 32, loss = 0.03126001
Iteration 33, loss = 0.02984858
Iteration 34, loss = 0.02862973
Iteration 35, loss = 0.02757072
Iteration 36, loss = 0.02654392
Iteration 37, loss = 0.02565316
Iteration 38, loss = 0.02474176
Iteration 39, loss = 0.02387592
Iteration 40, loss = 0.02315434
Iteration 41, loss = 0.02256419
Iteration 42, loss = 0.02197676
Iteration 43, loss = 0.02134546
Iteration 44, loss = 0.02080396
Iteration 45, loss = 0.02042967
Iteration 46, loss = 0.01981607
Iteration 47, loss = 0.01945430
Iteration 48, loss = 0.01898412
Iteration 49, loss = 0.01859836
Iteration 50, loss = 0.01829437
Iteration 51, loss = 0.01792355
Iteration 52, loss = 0.01774087
Iteration 53, loss = 0.01731155
Iteration 54, loss = 0.01707454
Iteration 55, loss = 0.01677822
Iteration 56, loss = 0.01659573
Iteration 57, loss = 0.01629101
Iteration 58, loss = 0.01607992
Iteration 59, loss = 0.01603591
Iteration 60, loss = 0.01565568
Iteration 61, loss = 0.01553398
Iteration 62, loss = 0.01531510
Iteration 63, loss = 0.01519235
Iteration 64, loss = 0.01516017
Iteration 65, loss = 0.01489350
Iteration 66, loss = 0.01468888
Iteration 67, loss = 0.01464838
Iteration 68, loss = 0.01440650
Iteration 69, loss = 0.01424893
Iteration 70, loss = 0.01421291
Iteration 71, loss = 0.01412054
Iteration 72, loss = 0.01393689
Iteration 73, loss = 0.01392311
Iteration 74, loss = 0.01377232
Iteration 75, loss = 0.01375165
Iteration 76, loss = 0.01359864
Iteration 77, loss = 0.01358706
Iteration 78, loss = 0.01339297
Iteration 79, loss = 0.01342194
Iteration 80, loss = 0.01331155
Iteration 81, loss = 0.01328568
Iteration 82, loss = 0.01308048
Iteration 83, loss = 0.01304629
Iteration 84, loss = 0.01297719
Iteration 85, loss = 0.01284742
Iteration 86, loss = 0.01280904
Iteration 87, loss = 0.01277951
Iteration 88, loss = 0.01279842
Iteration 89, loss = 0.01256842
Iteration 90, loss = 0.01283047
Iteration 91, loss = 0.01256157
Iteration 92, loss = 0.01252820
Iteration 93, loss = 0.01247344
Iteration 94, loss = 0.01245983
Iteration 95, loss = 0.01241846
Iteration 96, loss = 0.01241686
Iteration 97, loss = 0.01234794
Iteration 98, loss = 0.01234140
Iteration 99, loss = 0.01220209
Iteration 100, loss = 0.01220151
Iteration 101, loss = 0.01207446
Iteration 102, loss = 0.01209072
Iteration 103, loss = 0.01205738
Iteration 104, loss = 0.01202702
Iteration 105, loss = 0.01193719
Iteration 106, loss = 0.01188504
Iteration 107, loss = 0.01188525
Iteration 108, loss = 0.01187859
Iteration 109, loss = 0.01183417
Iteration 110, loss = 0.01177537
Iteration 111, loss = 0.01181605
Iteration 112, loss = 0.01171609
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67915826
Iteration 2, loss = 0.62842289
Iteration 3, loss = 0.55693043
Iteration 4, loss = 0.47145153
Iteration 5, loss = 0.38902574
Iteration 6, loss = 0.31579133
Iteration 7, loss = 0.25535595
Iteration 8, loss = 0.20727157
Iteration 9, loss = 0.17032437
Iteration 10, loss = 0.14230785
Iteration 11, loss = 0.12031748
Iteration 12, loss = 0.10360147
Iteration 13, loss = 0.09018920
Iteration 14, loss = 0.07964252
Iteration 15, loss = 0.07145511
Iteration 16, loss = 0.06441963
Iteration 17, loss = 0.05890523
Iteration 18, loss = 0.05415302
Iteration 19, loss = 0.05025041
Iteration 20, loss = 0.04679981
Iteration 21, loss = 0.04390594
Iteration 22, loss = 0.04146610
Iteration 23, loss = 0.03942874
Iteration 24, loss = 0.03749928
Iteration 25, loss = 0.03606434
Iteration 26, loss = 0.03425970
Iteration 27, loss = 0.03307739
Iteration 28, loss = 0.03200831
Iteration 29, loss = 0.03078080
Iteration 30, loss = 0.02977645
Iteration 31, loss = 0.02921349
Iteration 32, loss = 0.02834965
Iteration 33, loss = 0.02754753
Iteration 34, loss = 0.02682566
Iteration 35, loss = 0.02645681
Iteration 36, loss = 0.02587961
Iteration 37, loss = 0.02541193
Iteration 38, loss = 0.02503088
Iteration 39, loss = 0.02455160
Iteration 40, loss = 0.02404677
Iteration 41, loss = 0.02367686
Iteration 42, loss = 0.02335479
Iteration 43, loss = 0.02292622
Iteration 44, loss = 0.02284772
Iteration 45, loss = 0.02257887
Iteration 46, loss = 0.02233512
Iteration 47, loss = 0.02223429
Iteration 48, loss = 0.02203530
Iteration 49, loss = 0.02172266
Iteration 50, loss = 0.02165952
Iteration 51, loss = 0.02125028
Iteration 52, loss = 0.02148272
Iteration 53, loss = 0.02174925
Iteration 54, loss = 0.02106752
Iteration 55, loss = 0.02071147
Iteration 56, loss = 0.02080806
Iteration 57, loss = 0.02058920
Iteration 58, loss = 0.02035431
Iteration 59, loss = 0.02025962
Iteration 60, loss = 0.01998292
Iteration 61, loss = 0.01994542
Iteration 62, loss = 0.01996484
Iteration 63, loss = 0.01984311
Iteration 64, loss = 0.01977269
Iteration 65, loss = 0.01971525
Iteration 66, loss = 0.01967193
Iteration 67, loss = 0.01936664
Iteration 68, loss = 0.01942401
Iteration 69, loss = 0.01946892
Iteration 70, loss = 0.01916533
Iteration 71, loss = 0.01912454
Iteration 72, loss = 0.01936376
Iteration 73, loss = 0.01939104
Iteration 74, loss = 0.01910251
Iteration 75, loss = 0.01919198
Iteration 76, loss = 0.01904864
Iteration 77, loss = 0.01878684
Iteration 78, loss = 0.01872700
Iteration 79, loss = 0.01862629
Iteration 80, loss = 0.01878996
Iteration 81, loss = 0.01894675
Iteration 82, loss = 0.01891448
Iteration 83, loss = 0.01893969
Iteration 84, loss = 0.01863002
Iteration 85, loss = 0.01863681
Iteration 86, loss = 0.01845915
Iteration 87, loss = 0.01859997
Iteration 88, loss = 0.01848761
Iteration 89, loss = 0.01829667
Iteration 90, loss = 0.01861363
Iteration 91, loss = 0.01856387
Iteration 92, loss = 0.01834341
Iteration 93, loss = 0.01823359
Iteration 94, loss = 0.01869088
Iteration 95, loss = 0.01853914
Iteration 96, loss = 0.01806625
Iteration 97, loss = 0.01847007
Iteration 98, loss = 0.01817913
Iteration 99, loss = 0.01813610
Iteration 100, loss = 0.01799482
Iteration 101, loss = 0.01815410
Iteration 102, loss = 0.01821013
Iteration 103, loss = 0.01811912
Iteration 104, loss = 0.01813646
Iteration 105, loss = 0.01809418
Iteration 106, loss = 0.01800027
Iteration 107, loss = 0.01829762
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69445824
Iteration 2, loss = 0.64462479
Iteration 3, loss = 0.57367545
Iteration 4, loss = 0.48392901
Iteration 5, loss = 0.39397165
Iteration 6, loss = 0.31330819
Iteration 7, loss = 0.24773549
Iteration 8, loss = 0.19740537
Iteration 9, loss = 0.15975044
Iteration 10, loss = 0.13189184
Iteration 11, loss = 0.11103239
Iteration 12, loss = 0.09573175
Iteration 13, loss = 0.08339426
Iteration 14, loss = 0.07386875
Iteration 15, loss = 0.06661056
Iteration 16, loss = 0.06068304
Iteration 17, loss = 0.05535759
Iteration 18, loss = 0.05135314
Iteration 19, loss = 0.04792469
Iteration 20, loss = 0.04491017
Iteration 21, loss = 0.04291490
Iteration 22, loss = 0.04032434
Iteration 23, loss = 0.03860579
Iteration 24, loss = 0.03718091
Iteration 25, loss = 0.03581883
Iteration 26, loss = 0.03439017
Iteration 27, loss = 0.03322626
Iteration 28, loss = 0.03231431
Iteration 29, loss = 0.03140544
Iteration 30, loss = 0.03067069
Iteration 31, loss = 0.03003759
Iteration 32, loss = 0.02936013
Iteration 33, loss = 0.02897420
Iteration 34, loss = 0.02825011
Iteration 35, loss = 0.02789367
Iteration 36, loss = 0.02722752
Iteration 37, loss = 0.02707842
Iteration 38, loss = 0.02666818
Iteration 39, loss = 0.02613714
Iteration 40, loss = 0.02586111
Iteration 41, loss = 0.02577630
Iteration 42, loss = 0.02541981
Iteration 43, loss = 0.02498356
Iteration 44, loss = 0.02508112
Iteration 45, loss = 0.02468606
Iteration 46, loss = 0.02436342
Iteration 47, loss = 0.02422798
Iteration 48, loss = 0.02421353
Iteration 49, loss = 0.02383427
Iteration 50, loss = 0.02362620
Iteration 51, loss = 0.02371321
Iteration 52, loss = 0.02357679
Iteration 53, loss = 0.02368640
Iteration 54, loss = 0.02327068
Iteration 55, loss = 0.02318032
Iteration 56, loss = 0.02289371
Iteration 57, loss = 0.02295152
Iteration 58, loss = 0.02253502
Iteration 59, loss = 0.02268899
Iteration 60, loss = 0.02275792
Iteration 61, loss = 0.02262139
Iteration 62, loss = 0.02272019
Iteration 63, loss = 0.02245756
Iteration 64, loss = 0.02226332
Iteration 65, loss = 0.02244679
Iteration 66, loss = 0.02226661
Iteration 67, loss = 0.02228706
Iteration 68, loss = 0.02188322
Iteration 69, loss = 0.02203773
Iteration 70, loss = 0.02204508
Iteration 71, loss = 0.02214343
Iteration 72, loss = 0.02179204
Iteration 73, loss = 0.02159055
Iteration 74, loss = 0.02180917
Iteration 75, loss = 0.02173384
Iteration 76, loss = 0.02198441
Iteration 77, loss = 0.02174360
Iteration 78, loss = 0.02172258
Iteration 79, loss = 0.02153159
Iteration 80, loss = 0.02145093
Iteration 81, loss = 0.02172242
Iteration 82, loss = 0.02120659
Iteration 83, loss = 0.02157374
Iteration 84, loss = 0.02151319
Iteration 85, loss = 0.02154087
Iteration 86, loss = 0.02132213
Iteration 87, loss = 0.02133211
Iteration 88, loss = 0.02127709
Iteration 89, loss = 0.02106085
Iteration 90, loss = 0.02136081
Iteration 91, loss = 0.02123337
Iteration 92, loss = 0.02107061
Iteration 93, loss = 0.02105826
Iteration 94, loss = 0.02112263
Iteration 95, loss = 0.02102997
Iteration 96, loss = 0.02109232
Iteration 97, loss = 0.02096253
Iteration 98, loss = 0.02102678
Iteration 99, loss = 0.02087155
Iteration 100, loss = 0.02103771
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68773034
Iteration 2, loss = 0.63724856
Iteration 3, loss = 0.56729411
Iteration 4, loss = 0.47863882
Iteration 5, loss = 0.39111664
Iteration 6, loss = 0.31327620
Iteration 7, loss = 0.24952909
Iteration 8, loss = 0.19998223
Iteration 9, loss = 0.16264606
Iteration 10, loss = 0.13446997
Iteration 11, loss = 0.11357512
Iteration 12, loss = 0.09757128
Iteration 13, loss = 0.08528604
Iteration 14, loss = 0.07550948
Iteration 15, loss = 0.06803282
Iteration 16, loss = 0.06167189
Iteration 17, loss = 0.05693293
Iteration 18, loss = 0.05245966
Iteration 19, loss = 0.04885401
Iteration 20, loss = 0.04593862
Iteration 21, loss = 0.04342195
Iteration 22, loss = 0.04135338
Iteration 23, loss = 0.03957288
Iteration 24, loss = 0.03780708
Iteration 25, loss = 0.03634125
Iteration 26, loss = 0.03532164
Iteration 27, loss = 0.03406890
Iteration 28, loss = 0.03301858
Iteration 29, loss = 0.03216107
Iteration 30, loss = 0.03140849
Iteration 31, loss = 0.03084840
Iteration 32, loss = 0.02999010
Iteration 33, loss = 0.02945992
Iteration 34, loss = 0.02903130
Iteration 35, loss = 0.02879336
Iteration 36, loss = 0.02783602
Iteration 37, loss = 0.02772483
Iteration 38, loss = 0.02700788
Iteration 39, loss = 0.02679759
Iteration 40, loss = 0.02650782
Iteration 41, loss = 0.02600151
Iteration 42, loss = 0.02603338
Iteration 43, loss = 0.02582333
Iteration 44, loss = 0.02545911
Iteration 45, loss = 0.02533513
Iteration 46, loss = 0.02514801
Iteration 47, loss = 0.02503510
Iteration 48, loss = 0.02464904
Iteration 49, loss = 0.02490294
Iteration 50, loss = 0.02415634
Iteration 51, loss = 0.02423234
Iteration 52, loss = 0.02397222
Iteration 53, loss = 0.02390330
Iteration 54, loss = 0.02382776
Iteration 55, loss = 0.02341865
Iteration 56, loss = 0.02350508
Iteration 57, loss = 0.02343284
Iteration 58, loss = 0.02330616
Iteration 59, loss = 0.02320931
Iteration 60, loss = 0.02337772
Iteration 61, loss = 0.02331765
Iteration 62, loss = 0.02307845
Iteration 63, loss = 0.02313784
Iteration 64, loss = 0.02305987
Iteration 65, loss = 0.02291478
Iteration 66, loss = 0.02268057
Iteration 67, loss = 0.02256234
Iteration 68, loss = 0.02252579
Iteration 69, loss = 0.02260414
Iteration 70, loss = 0.02249859
Iteration 71, loss = 0.02238939
Iteration 72, loss = 0.02256756
Iteration 73, loss = 0.02243073
Iteration 74, loss = 0.02221370
Iteration 75, loss = 0.02227284
Iteration 76, loss = 0.02206685
Iteration 77, loss = 0.02208707
Iteration 78, loss = 0.02211041
Iteration 79, loss = 0.02210680
Iteration 80, loss = 0.02223939
Iteration 81, loss = 0.02196910
Iteration 82, loss = 0.02218629
Iteration 83, loss = 0.02228457
Iteration 84, loss = 0.02185905
Iteration 85, loss = 0.02217287
Iteration 86, loss = 0.02178107
Iteration 87, loss = 0.02174346
Iteration 88, loss = 0.02163357
Iteration 89, loss = 0.02182221
Iteration 90, loss = 0.02175645
Iteration 91, loss = 0.02171941
Iteration 92, loss = 0.02171244
Iteration 93, loss = 0.02167930
Iteration 94, loss = 0.02199492
Iteration 95, loss = 0.02206806
Iteration 96, loss = 0.02168877
Iteration 97, loss = 0.02170772
Iteration 98, loss = 0.02173240
Iteration 99, loss = 0.02173519
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71515128
Iteration 2, loss = 0.64915678
Iteration 3, loss = 0.56459692
Iteration 4, loss = 0.46411500
Iteration 5, loss = 0.36755675
Iteration 6, loss = 0.28486116
Iteration 7, loss = 0.22136496
Iteration 8, loss = 0.17494504
Iteration 9, loss = 0.14107051
Iteration 10, loss = 0.11697517
Iteration 11, loss = 0.09882767
Iteration 12, loss = 0.08519552
Iteration 13, loss = 0.07507083
Iteration 14, loss = 0.06682802
Iteration 15, loss = 0.06024473
Iteration 16, loss = 0.05499809
Iteration 17, loss = 0.05090958
Iteration 18, loss = 0.04739333
Iteration 19, loss = 0.04433870
Iteration 20, loss = 0.04174495
Iteration 21, loss = 0.03972462
Iteration 22, loss = 0.03792718
Iteration 23, loss = 0.03622081
Iteration 24, loss = 0.03492812
Iteration 25, loss = 0.03342888
Iteration 26, loss = 0.03260817
Iteration 27, loss = 0.03146987
Iteration 28, loss = 0.03095731
Iteration 29, loss = 0.02982875
Iteration 30, loss = 0.02923989
Iteration 31, loss = 0.02855241
Iteration 32, loss = 0.02805622
Iteration 33, loss = 0.02738701
Iteration 34, loss = 0.02698221
Iteration 35, loss = 0.02647815
Iteration 36, loss = 0.02609770
Iteration 37, loss = 0.02571242
Iteration 38, loss = 0.02555716
Iteration 39, loss = 0.02506422
Iteration 40, loss = 0.02522675
Iteration 41, loss = 0.02447559
Iteration 42, loss = 0.02440109
Iteration 43, loss = 0.02419192
Iteration 44, loss = 0.02379200
Iteration 45, loss = 0.02387913
Iteration 46, loss = 0.02356714
Iteration 47, loss = 0.02346453
Iteration 48, loss = 0.02345070
Iteration 49, loss = 0.02313474
Iteration 50, loss = 0.02329059
Iteration 51, loss = 0.02369142
Iteration 52, loss = 0.02318625
Iteration 53, loss = 0.02288906
Iteration 54, loss = 0.02271652
Iteration 55, loss = 0.02284345
Iteration 56, loss = 0.02230940
Iteration 57, loss = 0.02222605
Iteration 58, loss = 0.02237560
Iteration 59, loss = 0.02232410
Iteration 60, loss = 0.02219369
Iteration 61, loss = 0.02174791
Iteration 62, loss = 0.02177985
Iteration 63, loss = 0.02159204
Iteration 64, loss = 0.02149906
Iteration 65, loss = 0.02167167
Iteration 66, loss = 0.02268997
Iteration 67, loss = 0.02223260
Iteration 68, loss = 0.02221396
Iteration 69, loss = 0.02201422
Iteration 70, loss = 0.02180714
Iteration 71, loss = 0.02183816
Iteration 72, loss = 0.02144663
Iteration 73, loss = 0.02167639
Iteration 74, loss = 0.02157345
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67643639
Iteration 2, loss = 0.65614951
Iteration 3, loss = 0.63619718
Iteration 4, loss = 0.61360504
Iteration 5, loss = 0.58692242
Iteration 6, loss = 0.55660858
Iteration 7, loss = 0.52360432
Iteration 8, loss = 0.48872322
Iteration 9, loss = 0.45379845
Iteration 10, loss = 0.41937321
Iteration 11, loss = 0.38627035
Iteration 12, loss = 0.35478874
Iteration 13, loss = 0.32514768
Iteration 14, loss = 0.29776093
Iteration 15, loss = 0.27196548
Iteration 16, loss = 0.24854154
Iteration 17, loss = 0.22697671
Iteration 18, loss = 0.20738038
Iteration 19, loss = 0.18959778
Iteration 20, loss = 0.17355440
Iteration 21, loss = 0.15918260
Iteration 22, loss = 0.14624521
Iteration 23, loss = 0.13463014
Iteration 24, loss = 0.12419837
Iteration 25, loss = 0.11486984
Iteration 26, loss = 0.10655227
Iteration 27, loss = 0.09905984
Iteration 28, loss = 0.09240516
Iteration 29, loss = 0.08625211
Iteration 30, loss = 0.08086042
Iteration 31, loss = 0.07586380
Iteration 32, loss = 0.07150457
Iteration 33, loss = 0.06735508
Iteration 34, loss = 0.06374846
Iteration 35, loss = 0.06036910
Iteration 36, loss = 0.05738938
Iteration 37, loss = 0.05455270
Iteration 38, loss = 0.05195651
Iteration 39, loss = 0.04965579
Iteration 40, loss = 0.04742305
Iteration 41, loss = 0.04547846
Iteration 42, loss = 0.04365928
Iteration 43, loss = 0.04194112
Iteration 44, loss = 0.04037548
Iteration 45, loss = 0.03887996
Iteration 46, loss = 0.03747883
Iteration 47, loss = 0.03625687
Iteration 48, loss = 0.03507452
Iteration 49, loss = 0.03392746
Iteration 50, loss = 0.03290810
Iteration 51, loss = 0.03193769
Iteration 52, loss = 0.03096997
Iteration 53, loss = 0.03014949
Iteration 54, loss = 0.02936135
Iteration 55, loss = 0.02858971
Iteration 56, loss = 0.02788019
Iteration 57, loss = 0.02722235
Iteration 58, loss = 0.02656882
Iteration 59, loss = 0.02595087
Iteration 60, loss = 0.02537093
Iteration 61, loss = 0.02479362
Iteration 62, loss = 0.02431679
Iteration 63, loss = 0.02378612
Iteration 64, loss = 0.02333945
Iteration 65, loss = 0.02289114
Iteration 66, loss = 0.02246807
Iteration 67, loss = 0.02211220
Iteration 68, loss = 0.02169172
Iteration 69, loss = 0.02141186
Iteration 70, loss = 0.02103102
Iteration 71, loss = 0.02069126
Iteration 72, loss = 0.02033751
Iteration 73, loss = 0.02004927
Iteration 74, loss = 0.01974158
Iteration 75, loss = 0.01946423
Iteration 76, loss = 0.01923616
Iteration 77, loss = 0.01897095
Iteration 78, loss = 0.01873028
Iteration 79, loss = 0.01845870
Iteration 80, loss = 0.01825063
Iteration 81, loss = 0.01803412
Iteration 82, loss = 0.01782825
Iteration 83, loss = 0.01762417
Iteration 84, loss = 0.01743337
Iteration 85, loss = 0.01724249
Iteration 86, loss = 0.01708168
Iteration 87, loss = 0.01686848
Iteration 88, loss = 0.01677059
Iteration 89, loss = 0.01657815
Iteration 90, loss = 0.01642520
Iteration 91, loss = 0.01631642
Iteration 92, loss = 0.01611584
Iteration 93, loss = 0.01599026
Iteration 94, loss = 0.01584935
Iteration 95, loss = 0.01573711
Iteration 96, loss = 0.01562618
Iteration 97, loss = 0.01546697
Iteration 98, loss = 0.01540729
Iteration 99, loss = 0.01528544
Iteration 100, loss = 0.01516165
Iteration 101, loss = 0.01501284
Iteration 102, loss = 0.01497846
Iteration 103, loss = 0.01480896
Iteration 104, loss = 0.01471231
Iteration 105, loss = 0.01460964
Iteration 106, loss = 0.01454992
Iteration 107, loss = 0.01444642
Iteration 108, loss = 0.01433830
Iteration 109, loss = 0.01424876
Iteration 110, loss = 0.01425184
Iteration 111, loss = 0.01412804
Iteration 112, loss = 0.01402171
Iteration 113, loss = 0.01398797
Iteration 114, loss = 0.01388727
Iteration 115, loss = 0.01381977
Iteration 116, loss = 0.01373142
Iteration 117, loss = 0.01370103
Iteration 118, loss = 0.01359731
Iteration 119, loss = 0.01355898
Iteration 120, loss = 0.01344722
Iteration 121, loss = 0.01345025
Iteration 122, loss = 0.01336726
Iteration 123, loss = 0.01331059
Iteration 124, loss = 0.01328308
Iteration 125, loss = 0.01318214
Iteration 126, loss = 0.01316967
Iteration 127, loss = 0.01309719
Iteration 128, loss = 0.01302256
Iteration 129, loss = 0.01301536
Iteration 130, loss = 0.01298218
Iteration 131, loss = 0.01293256
Iteration 132, loss = 0.01283938
Iteration 133, loss = 0.01277370
Iteration 134, loss = 0.01280653
Iteration 135, loss = 0.01273629
Iteration 136, loss = 0.01269306
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68409150
Iteration 2, loss = 0.64197303
Iteration 3, loss = 0.58132965
Iteration 4, loss = 0.50825957
Iteration 5, loss = 0.43375931
Iteration 6, loss = 0.36098356
Iteration 7, loss = 0.29697199
Iteration 8, loss = 0.24300493
Iteration 9, loss = 0.19951380
Iteration 10, loss = 0.16546233
Iteration 11, loss = 0.13886700
Iteration 12, loss = 0.11805086
Iteration 13, loss = 0.10188462
Iteration 14, loss = 0.08905843
Iteration 15, loss = 0.07884255
Iteration 16, loss = 0.07038184
Iteration 17, loss = 0.06378219
Iteration 18, loss = 0.05821311
Iteration 19, loss = 0.05345715
Iteration 20, loss = 0.04940748
Iteration 21, loss = 0.04619554
Iteration 22, loss = 0.04335661
Iteration 23, loss = 0.04101490
Iteration 24, loss = 0.03891494
Iteration 25, loss = 0.03703485
Iteration 26, loss = 0.03522895
Iteration 27, loss = 0.03369019
Iteration 28, loss = 0.03235245
Iteration 29, loss = 0.03124834
Iteration 30, loss = 0.03019555
Iteration 31, loss = 0.02918619
Iteration 32, loss = 0.02832726
Iteration 33, loss = 0.02750184
Iteration 34, loss = 0.02676391
Iteration 35, loss = 0.02627247
Iteration 36, loss = 0.02551131
Iteration 37, loss = 0.02501781
Iteration 38, loss = 0.02447933
Iteration 39, loss = 0.02407565
Iteration 40, loss = 0.02371678
Iteration 41, loss = 0.02331911
Iteration 42, loss = 0.02292481
Iteration 43, loss = 0.02267980
Iteration 44, loss = 0.02262531
Iteration 45, loss = 0.02219390
Iteration 46, loss = 0.02176914
Iteration 47, loss = 0.02137558
Iteration 48, loss = 0.02135433
Iteration 49, loss = 0.02087258
Iteration 50, loss = 0.02089535
Iteration 51, loss = 0.02067805
Iteration 52, loss = 0.02046745
Iteration 53, loss = 0.02046117
Iteration 54, loss = 0.02014930
Iteration 55, loss = 0.01998614
Iteration 56, loss = 0.01982856
Iteration 57, loss = 0.01987010
Iteration 58, loss = 0.01965535
Iteration 59, loss = 0.01963542
Iteration 60, loss = 0.01948054
Iteration 61, loss = 0.01959408
Iteration 62, loss = 0.01916067
Iteration 63, loss = 0.01891659
Iteration 64, loss = 0.01900303
Iteration 65, loss = 0.01884330
Iteration 66, loss = 0.01893979
Iteration 67, loss = 0.01862654
Iteration 68, loss = 0.01897765
Iteration 69, loss = 0.01854061
Iteration 70, loss = 0.01860098
Iteration 71, loss = 0.01852696
Iteration 72, loss = 0.01854911
Iteration 73, loss = 0.01831646
Iteration 74, loss = 0.01853587
Iteration 75, loss = 0.01844753
Iteration 76, loss = 0.01836549
Iteration 77, loss = 0.01817515
Iteration 78, loss = 0.01800740
Iteration 79, loss = 0.01828766
Iteration 80, loss = 0.01801060
Iteration 81, loss = 0.01811048
Iteration 82, loss = 0.01808034
Iteration 83, loss = 0.01817745
Iteration 84, loss = 0.01795761
Iteration 85, loss = 0.01776116
Iteration 86, loss = 0.01783085
Iteration 87, loss = 0.01769296
Iteration 88, loss = 0.01766479
Iteration 89, loss = 0.01781279
Iteration 90, loss = 0.01777715
Iteration 91, loss = 0.01760324
Iteration 92, loss = 0.01744755
Iteration 93, loss = 0.01749462
Iteration 94, loss = 0.01756804
Iteration 95, loss = 0.01748589
Iteration 96, loss = 0.01741982
Iteration 97, loss = 0.01746229
Iteration 98, loss = 0.01730764
Iteration 99, loss = 0.01733629
Iteration 100, loss = 0.01737745
Iteration 101, loss = 0.01727759
Iteration 102, loss = 0.01711226
Iteration 103, loss = 0.01749188
Iteration 104, loss = 0.01733853
Iteration 105, loss = 0.01732103
Iteration 106, loss = 0.01688583
Iteration 107, loss = 0.01733049
Iteration 108, loss = 0.01729719
Iteration 109, loss = 0.01724449
Iteration 110, loss = 0.01736337
Iteration 111, loss = 0.01731011
Iteration 112, loss = 0.01703865
Iteration 113, loss = 0.01707572
Iteration 114, loss = 0.01698432
Iteration 115, loss = 0.01704015
Iteration 116, loss = 0.01704792
Iteration 117, loss = 0.01700309
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67535502
Iteration 2, loss = 0.61229981
Iteration 3, loss = 0.52385750
Iteration 4, loss = 0.42455013
Iteration 5, loss = 0.33374499
Iteration 6, loss = 0.26001702
Iteration 7, loss = 0.20373745
Iteration 8, loss = 0.16282518
Iteration 9, loss = 0.13319746
Iteration 10, loss = 0.11161215
Iteration 11, loss = 0.09547086
Iteration 12, loss = 0.08369965
Iteration 13, loss = 0.07438085
Iteration 14, loss = 0.06689585
Iteration 15, loss = 0.06100023
Iteration 16, loss = 0.05626896
Iteration 17, loss = 0.05225255
Iteration 18, loss = 0.04894543
Iteration 19, loss = 0.04627555
Iteration 20, loss = 0.04383291
Iteration 21, loss = 0.04207993
Iteration 22, loss = 0.04018731
Iteration 23, loss = 0.03857423
Iteration 24, loss = 0.03738708
Iteration 25, loss = 0.03607457
Iteration 26, loss = 0.03548264
Iteration 27, loss = 0.03447502
Iteration 28, loss = 0.03375026
Iteration 29, loss = 0.03319541
Iteration 30, loss = 0.03246888
Iteration 31, loss = 0.03451417
Iteration 32, loss = 0.03380609
Iteration 33, loss = 0.03265243
Iteration 34, loss = 0.03178081
Iteration 35, loss = 0.03177446
Iteration 36, loss = 0.03165579
Iteration 37, loss = 0.03085725
Iteration 38, loss = 0.03002127
Iteration 39, loss = 0.02970338
Iteration 40, loss = 0.02927607
Iteration 41, loss = 0.02898483
Iteration 42, loss = 0.02839973
Iteration 43, loss = 0.02817932
Iteration 44, loss = 0.02783269
Iteration 45, loss = 0.02760350
Iteration 46, loss = 0.02724721
Iteration 47, loss = 0.02717557
Iteration 48, loss = 0.02672248
Iteration 49, loss = 0.02666035
Iteration 50, loss = 0.02657368
Iteration 51, loss = 0.02635972
Iteration 52, loss = 0.02632287
Iteration 53, loss = 0.02595319
Iteration 54, loss = 0.02609747
Iteration 55, loss = 0.02594279
Iteration 56, loss = 0.02590751
Iteration 57, loss = 0.02542535
Iteration 58, loss = 0.02563098
Iteration 59, loss = 0.02542999
Iteration 60, loss = 0.02512173
Iteration 61, loss = 0.02517767
Iteration 62, loss = 0.02530378
Iteration 63, loss = 0.02509112
Iteration 64, loss = 0.02494704
Iteration 65, loss = 0.02516913
Iteration 66, loss = 0.02525034
Iteration 67, loss = 0.02519689
Iteration 68, loss = 0.02516411
Iteration 69, loss = 0.02494594
Iteration 70, loss = 0.02464708
Iteration 71, loss = 0.02470957
Iteration 72, loss = 0.02478217
Iteration 73, loss = 0.02470582
Iteration 74, loss = 0.02513239
Iteration 75, loss = 0.02639228
Iteration 76, loss = 0.02640351
Iteration 77, loss = 0.02602641
Iteration 78, loss = 0.02582736
Iteration 79, loss = 0.02574684
Iteration 80, loss = 0.02541874
Iteration 81, loss = 0.02533729
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.74575386
Iteration 2, loss = 0.72623530
Iteration 3, loss = 0.70972642
Iteration 4, loss = 0.69380181
Iteration 5, loss = 0.67793555
Iteration 6, loss = 0.66119747
Iteration 7, loss = 0.64348794
Iteration 8, loss = 0.62508319
Iteration 9, loss = 0.60573915
Iteration 10, loss = 0.58520365
Iteration 11, loss = 0.56382479
Iteration 12, loss = 0.54163433
Iteration 13, loss = 0.51906513
Iteration 14, loss = 0.49606748
Iteration 15, loss = 0.47272122
Iteration 16, loss = 0.44947793
Iteration 17, loss = 0.42630052
Iteration 18, loss = 0.40371804
Iteration 19, loss = 0.38155333
Iteration 20, loss = 0.35999219
Iteration 21, loss = 0.33919482
Iteration 22, loss = 0.31921010
Iteration 23, loss = 0.30010863
Iteration 24, loss = 0.28199503
Iteration 25, loss = 0.26484799
Iteration 26, loss = 0.24869125
Iteration 27, loss = 0.23366551
Iteration 28, loss = 0.21950193
Iteration 29, loss = 0.20622455
Iteration 30, loss = 0.19398081
Iteration 31, loss = 0.18258673
Iteration 32, loss = 0.17197684
Iteration 33, loss = 0.16211592
Iteration 34, loss = 0.15300849
Iteration 35, loss = 0.14454597
Iteration 36, loss = 0.13666710
Iteration 37, loss = 0.12937715
Iteration 38, loss = 0.12268299
Iteration 39, loss = 0.11642631
Iteration 40, loss = 0.11058099
Iteration 41, loss = 0.10523575
Iteration 42, loss = 0.10018981
Iteration 43, loss = 0.09559900
Iteration 44, loss = 0.09123726
Iteration 45, loss = 0.08718424
Iteration 46, loss = 0.08338147
Iteration 47, loss = 0.07990272
Iteration 48, loss = 0.07661635
Iteration 49, loss = 0.07352917
Iteration 50, loss = 0.07061851
Iteration 51, loss = 0.06790633
Iteration 52, loss = 0.06534023
Iteration 53, loss = 0.06296441
Iteration 54, loss = 0.06067397
Iteration 55, loss = 0.05855247
Iteration 56, loss = 0.05658872
Iteration 57, loss = 0.05469901
Iteration 58, loss = 0.05286759
Iteration 59, loss = 0.05125067
Iteration 60, loss = 0.04959453
Iteration 61, loss = 0.04808394
Iteration 62, loss = 0.04664949
Iteration 63, loss = 0.04527076
Iteration 64, loss = 0.04398835
Iteration 65, loss = 0.04276906
Iteration 66, loss = 0.04159377
Iteration 67, loss = 0.04048461
Iteration 68, loss = 0.03942464
Iteration 69, loss = 0.03841671
Iteration 70, loss = 0.03745225
Iteration 71, loss = 0.03655284
Iteration 72, loss = 0.03565032
Iteration 73, loss = 0.03481018
Iteration 74, loss = 0.03399707
Iteration 75, loss = 0.03321570
Iteration 76, loss = 0.03249781
Iteration 77, loss = 0.03181592
Iteration 78, loss = 0.03111207
Iteration 79, loss = 0.03045837
Iteration 80, loss = 0.02982491
Iteration 81, loss = 0.02923848
Iteration 82, loss = 0.02865024
Iteration 83, loss = 0.02808220
Iteration 84, loss = 0.02756199
Iteration 85, loss = 0.02703789
Iteration 86, loss = 0.02654042
Iteration 87, loss = 0.02604291
Iteration 88, loss = 0.02558955
Iteration 89, loss = 0.02513969
Iteration 90, loss = 0.02471872
Iteration 91, loss = 0.02428661
Iteration 92, loss = 0.02388433
Iteration 93, loss = 0.02353292
Iteration 94, loss = 0.02312060
Iteration 95, loss = 0.02275759
Iteration 96, loss = 0.02240940
Iteration 97, loss = 0.02206189
Iteration 98, loss = 0.02174354
Iteration 99, loss = 0.02141114
Iteration 100, loss = 0.02111812
Iteration 101, loss = 0.02080766
Iteration 102, loss = 0.02053382
Iteration 103, loss = 0.02028419
Iteration 104, loss = 0.02000220
Iteration 105, loss = 0.01970538
Iteration 106, loss = 0.01945621
Iteration 107, loss = 0.01920260
Iteration 108, loss = 0.01895594
Iteration 109, loss = 0.01871004
Iteration 110, loss = 0.01855052
Iteration 111, loss = 0.01836142
Iteration 112, loss = 0.01815606
Iteration 113, loss = 0.01792439
Iteration 114, loss = 0.01771006
Iteration 115, loss = 0.01750493
Iteration 116, loss = 0.01729570
Iteration 117, loss = 0.01714128
Iteration 118, loss = 0.01692583
Iteration 119, loss = 0.01675566
Iteration 120, loss = 0.01656856
Iteration 121, loss = 0.01640858
Iteration 122, loss = 0.01621507
Iteration 123, loss = 0.01613464
Iteration 124, loss = 0.01590528
Iteration 125, loss = 0.01577133
Iteration 126, loss = 0.01562621
Iteration 127, loss = 0.01548883
Iteration 128, loss = 0.01532805
Iteration 129, loss = 0.01519000
Iteration 130, loss = 0.01507656
Iteration 131, loss = 0.01495031
Iteration 132, loss = 0.01477862
Iteration 133, loss = 0.01467008
Iteration 134, loss = 0.01458856
Iteration 135, loss = 0.01443071
Iteration 136, loss = 0.01436171
Iteration 137, loss = 0.01421880
Iteration 138, loss = 0.01410728
Iteration 139, loss = 0.01399121
Iteration 140, loss = 0.01387435
Iteration 141, loss = 0.01381550
Iteration 142, loss = 0.01370574
Iteration 143, loss = 0.01359993
Iteration 144, loss = 0.01351068
Iteration 145, loss = 0.01342594
Iteration 146, loss = 0.01334762
Iteration 147, loss = 0.01325089
Iteration 148, loss = 0.01318140
Iteration 149, loss = 0.01308648
Iteration 150, loss = 0.01300938
Iteration 151, loss = 0.01290628
Iteration 152, loss = 0.01283986
Iteration 153, loss = 0.01274370
Iteration 154, loss = 0.01265704
Iteration 155, loss = 0.01258922
Iteration 156, loss = 0.01248306
Iteration 157, loss = 0.01240228
Iteration 158, loss = 0.01238417
Iteration 159, loss = 0.01227347
Iteration 160, loss = 0.01221024
Iteration 161, loss = 0.01215840
Iteration 162, loss = 0.01206919
Iteration 163, loss = 0.01196789
Iteration 164, loss = 0.01192404
Iteration 165, loss = 0.01182483
Iteration 166, loss = 0.01175994
Iteration 167, loss = 0.01169810
Iteration 168, loss = 0.01163450
Iteration 169, loss = 0.01158390
Iteration 170, loss = 0.01155969
Iteration 171, loss = 0.01147950
Iteration 172, loss = 0.01140040
Iteration 173, loss = 0.01135473
Iteration 174, loss = 0.01130140
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73391371
Iteration 2, loss = 0.71175696
Iteration 3, loss = 0.69063994
Iteration 4, loss = 0.66696332
Iteration 5, loss = 0.63925884
Iteration 6, loss = 0.60908614
Iteration 7, loss = 0.57585406
Iteration 8, loss = 0.54127814
Iteration 9, loss = 0.50514990
Iteration 10, loss = 0.46865109
Iteration 11, loss = 0.43210884
Iteration 12, loss = 0.39650420
Iteration 13, loss = 0.36204005
Iteration 14, loss = 0.32976669
Iteration 15, loss = 0.30003973
Iteration 16, loss = 0.27278682
Iteration 17, loss = 0.24804240
Iteration 18, loss = 0.22573480
Iteration 19, loss = 0.20572257
Iteration 20, loss = 0.18791273
Iteration 21, loss = 0.17193151
Iteration 22, loss = 0.15776753
Iteration 23, loss = 0.14519333
Iteration 24, loss = 0.13395061
Iteration 25, loss = 0.12389427
Iteration 26, loss = 0.11501595
Iteration 27, loss = 0.10700396
Iteration 28, loss = 0.09988382
Iteration 29, loss = 0.09340230
Iteration 30, loss = 0.08761398
Iteration 31, loss = 0.08245036
Iteration 32, loss = 0.07768944
Iteration 33, loss = 0.07340464
Iteration 34, loss = 0.06950133
Iteration 35, loss = 0.06590618
Iteration 36, loss = 0.06263492
Iteration 37, loss = 0.05968388
Iteration 38, loss = 0.05692380
Iteration 39, loss = 0.05446754
Iteration 40, loss = 0.05209850
Iteration 41, loss = 0.04995558
Iteration 42, loss = 0.04795362
Iteration 43, loss = 0.04611660
Iteration 44, loss = 0.04442881
Iteration 45, loss = 0.04281322
Iteration 46, loss = 0.04136624
Iteration 47, loss = 0.03996136
Iteration 48, loss = 0.03867791
Iteration 49, loss = 0.03744462
Iteration 50, loss = 0.03630915
Iteration 51, loss = 0.03527053
Iteration 52, loss = 0.03424041
Iteration 53, loss = 0.03328555
Iteration 54, loss = 0.03240898
Iteration 55, loss = 0.03152872
Iteration 56, loss = 0.03075999
Iteration 57, loss = 0.03003416
Iteration 58, loss = 0.02934796
Iteration 59, loss = 0.02861975
Iteration 60, loss = 0.02807241
Iteration 61, loss = 0.02740675
Iteration 62, loss = 0.02686948
Iteration 63, loss = 0.02634905
Iteration 64, loss = 0.02574927
Iteration 65, loss = 0.02531239
Iteration 66, loss = 0.02480864
Iteration 67, loss = 0.02436243
Iteration 68, loss = 0.02394030
Iteration 69, loss = 0.02351869
Iteration 70, loss = 0.02312268
Iteration 71, loss = 0.02271410
Iteration 72, loss = 0.02241798
Iteration 73, loss = 0.02206332
Iteration 74, loss = 0.02170242
Iteration 75, loss = 0.02141355
Iteration 76, loss = 0.02108120
Iteration 77, loss = 0.02087749
Iteration 78, loss = 0.02055943
Iteration 79, loss = 0.02030977
Iteration 80, loss = 0.02006859
Iteration 81, loss = 0.01982383
Iteration 82, loss = 0.01956222
Iteration 83, loss = 0.01934017
Iteration 84, loss = 0.01915494
Iteration 85, loss = 0.01890450
Iteration 86, loss = 0.01868874
Iteration 87, loss = 0.01848271
Iteration 88, loss = 0.01832544
Iteration 89, loss = 0.01810842
Iteration 90, loss = 0.01794828
Iteration 91, loss = 0.01779256
Iteration 92, loss = 0.01762458
Iteration 93, loss = 0.01744354
Iteration 94, loss = 0.01733149
Iteration 95, loss = 0.01715865
Iteration 96, loss = 0.01704160
Iteration 97, loss = 0.01692543
Iteration 98, loss = 0.01672757
Iteration 99, loss = 0.01663948
Iteration 100, loss = 0.01649022
Iteration 101, loss = 0.01633177
Iteration 102, loss = 0.01622644
Iteration 103, loss = 0.01613882
Iteration 104, loss = 0.01599187
Iteration 105, loss = 0.01591594
Iteration 106, loss = 0.01578533
Iteration 107, loss = 0.01571401
Iteration 108, loss = 0.01563435
Iteration 109, loss = 0.01548324
Iteration 110, loss = 0.01537194
Iteration 111, loss = 0.01531318
Iteration 112, loss = 0.01522179
Iteration 113, loss = 0.01514086
Iteration 114, loss = 0.01508376
Iteration 115, loss = 0.01496894
Iteration 116, loss = 0.01494457
Iteration 117, loss = 0.01479849
Iteration 118, loss = 0.01476480
Iteration 119, loss = 0.01466093
Iteration 120, loss = 0.01459522
Iteration 121, loss = 0.01450116
Iteration 122, loss = 0.01446639
Iteration 123, loss = 0.01443886
Iteration 124, loss = 0.01431001
Iteration 125, loss = 0.01426479
Iteration 126, loss = 0.01420431
Iteration 127, loss = 0.01416608
Iteration 128, loss = 0.01411601
Iteration 129, loss = 0.01397284
Iteration 130, loss = 0.01399025
Iteration 131, loss = 0.01391738
Iteration 132, loss = 0.01386771
Iteration 133, loss = 0.01390560
Iteration 134, loss = 0.01379892
Iteration 135, loss = 0.01368018
Iteration 136, loss = 0.01369600
Iteration 137, loss = 0.01364094
Iteration 138, loss = 0.01355907
Iteration 139, loss = 0.01360567
Iteration 140, loss = 0.01347857
Iteration 141, loss = 0.01345321
Iteration 142, loss = 0.01340912
Iteration 143, loss = 0.01338333
Iteration 144, loss = 0.01338391
Iteration 145, loss = 0.01332149
Iteration 146, loss = 0.01324886
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69045859
Iteration 2, loss = 0.64426111
Iteration 3, loss = 0.57725606
Iteration 4, loss = 0.49060164
Iteration 5, loss = 0.40236209
Iteration 6, loss = 0.32220956
Iteration 7, loss = 0.25594472
Iteration 8, loss = 0.20422138
Iteration 9, loss = 0.16530054
Iteration 10, loss = 0.13626032
Iteration 11, loss = 0.11446237
Iteration 12, loss = 0.09783821
Iteration 13, loss = 0.08519678
Iteration 14, loss = 0.07523588
Iteration 15, loss = 0.06758237
Iteration 16, loss = 0.06109327
Iteration 17, loss = 0.05573376
Iteration 18, loss = 0.05151576
Iteration 19, loss = 0.04782482
Iteration 20, loss = 0.04477283
Iteration 21, loss = 0.04214099
Iteration 22, loss = 0.03969787
Iteration 23, loss = 0.03790124
Iteration 24, loss = 0.03620023
Iteration 25, loss = 0.03493685
Iteration 26, loss = 0.03351518
Iteration 27, loss = 0.03229826
Iteration 28, loss = 0.03124961
Iteration 29, loss = 0.03024623
Iteration 30, loss = 0.02941556
Iteration 31, loss = 0.02885363
Iteration 32, loss = 0.02802171
Iteration 33, loss = 0.02738676
Iteration 34, loss = 0.02674922
Iteration 35, loss = 0.02651274
Iteration 36, loss = 0.02591429
Iteration 37, loss = 0.02541803
Iteration 38, loss = 0.02498289
Iteration 39, loss = 0.02486428
Iteration 40, loss = 0.02438525
Iteration 41, loss = 0.02410418
Iteration 42, loss = 0.02389746
Iteration 43, loss = 0.02348882
Iteration 44, loss = 0.02336216
Iteration 45, loss = 0.02310881
Iteration 46, loss = 0.02298125
Iteration 47, loss = 0.02277136
Iteration 48, loss = 0.02261805
Iteration 49, loss = 0.02235096
Iteration 50, loss = 0.02235038
Iteration 51, loss = 0.02227693
Iteration 52, loss = 0.02178998
Iteration 53, loss = 0.02172417
Iteration 54, loss = 0.02185602
Iteration 55, loss = 0.02142444
Iteration 56, loss = 0.02127638
Iteration 57, loss = 0.02119231
Iteration 58, loss = 0.02119817
Iteration 59, loss = 0.02106717
Iteration 60, loss = 0.02106089
Iteration 61, loss = 0.02084731
Iteration 62, loss = 0.02068188
Iteration 63, loss = 0.02066945
Iteration 64, loss = 0.02064644
Iteration 65, loss = 0.02065214
Iteration 66, loss = 0.02033288
Iteration 67, loss = 0.02047069
Iteration 68, loss = 0.02039752
Iteration 69, loss = 0.02033347
Iteration 70, loss = 0.02022194
Iteration 71, loss = 0.02023215
Iteration 72, loss = 0.02023482
Iteration 73, loss = 0.01994033
Iteration 74, loss = 0.02005278
Iteration 75, loss = 0.02008531
Iteration 76, loss = 0.02001826
Iteration 77, loss = 0.02003066
Iteration 78, loss = 0.01983292
Iteration 79, loss = 0.01986655
Iteration 80, loss = 0.01976704
Iteration 81, loss = 0.01972697
Iteration 82, loss = 0.01979027
Iteration 83, loss = 0.01968519
Iteration 84, loss = 0.01964314
Iteration 85, loss = 0.01955402
Iteration 86, loss = 0.01959042
Iteration 87, loss = 0.01959244
Iteration 88, loss = 0.01963206
Iteration 89, loss = 0.01946871
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72354242
Iteration 2, loss = 0.68167244
Iteration 3, loss = 0.62615439
Iteration 4, loss = 0.56103665
Iteration 5, loss = 0.48922096
Iteration 6, loss = 0.41733223
Iteration 7, loss = 0.35004775
Iteration 8, loss = 0.29177471
Iteration 9, loss = 0.24266026
Iteration 10, loss = 0.20281164
Iteration 11, loss = 0.17089619
Iteration 12, loss = 0.14559438
Iteration 13, loss = 0.12525124
Iteration 14, loss = 0.10900024
Iteration 15, loss = 0.09599758
Iteration 16, loss = 0.08556836
Iteration 17, loss = 0.07658965
Iteration 18, loss = 0.06945418
Iteration 19, loss = 0.06336192
Iteration 20, loss = 0.05833988
Iteration 21, loss = 0.05403499
Iteration 22, loss = 0.05026130
Iteration 23, loss = 0.04712744
Iteration 24, loss = 0.04440084
Iteration 25, loss = 0.04183161
Iteration 26, loss = 0.03985704
Iteration 27, loss = 0.03792775
Iteration 28, loss = 0.03617485
Iteration 29, loss = 0.03484364
Iteration 30, loss = 0.03349214
Iteration 31, loss = 0.03255673
Iteration 32, loss = 0.03130266
Iteration 33, loss = 0.03022684
Iteration 34, loss = 0.02941129
Iteration 35, loss = 0.02853433
Iteration 36, loss = 0.02793641
Iteration 37, loss = 0.02708418
Iteration 38, loss = 0.02650727
Iteration 39, loss = 0.02601259
Iteration 40, loss = 0.02537002
Iteration 41, loss = 0.02501301
Iteration 42, loss = 0.02461187
Iteration 43, loss = 0.02415939
Iteration 44, loss = 0.02368966
Iteration 45, loss = 0.02340201
Iteration 46, loss = 0.02304283
Iteration 47, loss = 0.02276128
Iteration 48, loss = 0.02251854
Iteration 49, loss = 0.02217796
Iteration 50, loss = 0.02195535
Iteration 51, loss = 0.02175707
Iteration 52, loss = 0.02147079
Iteration 53, loss = 0.02128011
Iteration 54, loss = 0.02121478
Iteration 55, loss = 0.02088831
Iteration 56, loss = 0.02077830
Iteration 57, loss = 0.02065595
Iteration 58, loss = 0.02041671
Iteration 59, loss = 0.02034970
Iteration 60, loss = 0.02029989
Iteration 61, loss = 0.02003113
Iteration 62, loss = 0.01989430
Iteration 63, loss = 0.01977932
Iteration 64, loss = 0.01977351
Iteration 65, loss = 0.01965352
Iteration 66, loss = 0.01957240
Iteration 67, loss = 0.01935437
Iteration 68, loss = 0.01919989
Iteration 69, loss = 0.01921138
Iteration 70, loss = 0.01901633
Iteration 71, loss = 0.01917595
Iteration 72, loss = 0.01907670
Iteration 73, loss = 0.01888491
Iteration 74, loss = 0.01890749
Iteration 75, loss = 0.01867838
Iteration 76, loss = 0.01856349
Iteration 77, loss = 0.01870936
Iteration 78, loss = 0.01853014
Iteration 79, loss = 0.01848264
Iteration 80, loss = 0.01849245
Iteration 81, loss = 0.01863072
Iteration 82, loss = 0.01824084
Iteration 83, loss = 0.01812064
Iteration 84, loss = 0.01828762
Iteration 85, loss = 0.01820580
Iteration 86, loss = 0.01811155
Iteration 87, loss = 0.01829325
Iteration 88, loss = 0.01795017
Iteration 89, loss = 0.01796670
Iteration 90, loss = 0.01791540
Iteration 91, loss = 0.01807720
Iteration 92, loss = 0.01792898
Iteration 93, loss = 0.01781370
Iteration 94, loss = 0.01772606
Iteration 95, loss = 0.01788038
Iteration 96, loss = 0.01783971
Iteration 97, loss = 0.01787443
Iteration 98, loss = 0.01757700
Iteration 99, loss = 0.01768423
Iteration 100, loss = 0.01765833
Iteration 101, loss = 0.01766988
Iteration 102, loss = 0.01756908
Iteration 103, loss = 0.01750656
Iteration 104, loss = 0.01754857
Iteration 105, loss = 0.01758173
Iteration 106, loss = 0.01756923
Iteration 107, loss = 0.01739480
Iteration 108, loss = 0.01735700
Iteration 109, loss = 0.01749390
Iteration 110, loss = 0.01740146
Iteration 111, loss = 0.01736919
Iteration 112, loss = 0.01740391
Iteration 113, loss = 0.01739852
Iteration 114, loss = 0.01756948
Iteration 115, loss = 0.01732240
Iteration 116, loss = 0.01724613
Iteration 117, loss = 0.01718861
Iteration 118, loss = 0.01734119
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67646546
Iteration 2, loss = 0.64077424
Iteration 3, loss = 0.59148094
Iteration 4, loss = 0.52745799
Iteration 5, loss = 0.45732263
Iteration 6, loss = 0.38933926
Iteration 7, loss = 0.32732632
Iteration 8, loss = 0.27327315
Iteration 9, loss = 0.22814162
Iteration 10, loss = 0.19111218
Iteration 11, loss = 0.16137968
Iteration 12, loss = 0.13774261
Iteration 13, loss = 0.11882251
Iteration 14, loss = 0.10373280
Iteration 15, loss = 0.09135725
Iteration 16, loss = 0.08127625
Iteration 17, loss = 0.07318568
Iteration 18, loss = 0.06634516
Iteration 19, loss = 0.06057709
Iteration 20, loss = 0.05559948
Iteration 21, loss = 0.05132941
Iteration 22, loss = 0.04775250
Iteration 23, loss = 0.04466963
Iteration 24, loss = 0.04195184
Iteration 25, loss = 0.03956829
Iteration 26, loss = 0.03749942
Iteration 27, loss = 0.03554470
Iteration 28, loss = 0.03404670
Iteration 29, loss = 0.03236710
Iteration 30, loss = 0.03111551
Iteration 31, loss = 0.02999086
Iteration 32, loss = 0.02889031
Iteration 33, loss = 0.02789509
Iteration 34, loss = 0.02695292
Iteration 35, loss = 0.02626934
Iteration 36, loss = 0.02557896
Iteration 37, loss = 0.02465599
Iteration 38, loss = 0.02411480
Iteration 39, loss = 0.02354326
Iteration 40, loss = 0.02307632
Iteration 41, loss = 0.02266608
Iteration 42, loss = 0.02206470
Iteration 43, loss = 0.02164667
Iteration 44, loss = 0.02129313
Iteration 45, loss = 0.02092417
Iteration 46, loss = 0.02056900
Iteration 47, loss = 0.02035910
Iteration 48, loss = 0.02001124
Iteration 49, loss = 0.01976301
Iteration 50, loss = 0.01942005
Iteration 51, loss = 0.01928035
Iteration 52, loss = 0.01894795
Iteration 53, loss = 0.01876842
Iteration 54, loss = 0.01857602
Iteration 55, loss = 0.01842860
Iteration 56, loss = 0.01825210
Iteration 57, loss = 0.01804046
Iteration 58, loss = 0.01781728
Iteration 59, loss = 0.01770721
Iteration 60, loss = 0.01751410
Iteration 61, loss = 0.01741114
Iteration 62, loss = 0.01747870
Iteration 63, loss = 0.01715673
Iteration 64, loss = 0.01712676
Iteration 65, loss = 0.01686727
Iteration 66, loss = 0.01682074
Iteration 67, loss = 0.01660293
Iteration 68, loss = 0.01667684
Iteration 69, loss = 0.01656177
Iteration 70, loss = 0.01637567
Iteration 71, loss = 0.01647311
Iteration 72, loss = 0.01633749
Iteration 73, loss = 0.01616811
Iteration 74, loss = 0.01614464
Iteration 75, loss = 0.01596974
Iteration 76, loss = 0.01613468
Iteration 77, loss = 0.01605292
Iteration 78, loss = 0.01594203
Iteration 79, loss = 0.01578401
Iteration 80, loss = 0.01572405
Iteration 81, loss = 0.01568096
Iteration 82, loss = 0.01561507
Iteration 83, loss = 0.01554487
Iteration 84, loss = 0.01558211
Iteration 85, loss = 0.01537757
Iteration 86, loss = 0.01549031
Iteration 87, loss = 0.01538839
Iteration 88, loss = 0.01535068
Iteration 89, loss = 0.01542353
Iteration 90, loss = 0.01535038
Iteration 91, loss = 0.01536931
Iteration 92, loss = 0.01521021
Iteration 93, loss = 0.01513987
Iteration 94, loss = 0.01514381
Iteration 95, loss = 0.01506880
Iteration 96, loss = 0.01520578
Iteration 97, loss = 0.01511641
Iteration 98, loss = 0.01511967
Iteration 99, loss = 0.01508785
Iteration 100, loss = 0.01496875
Iteration 101, loss = 0.01495661
Iteration 102, loss = 0.01497295
Iteration 103, loss = 0.01496996
Iteration 104, loss = 0.01480671
Iteration 105, loss = 0.01486230
Iteration 106, loss = 0.01478154
Iteration 107, loss = 0.01481379
Iteration 108, loss = 0.01469804
Iteration 109, loss = 0.01478975
Iteration 110, loss = 0.01468962
Iteration 111, loss = 0.01477555
Iteration 112, loss = 0.01457888
Iteration 113, loss = 0.01474960
Iteration 114, loss = 0.01462102
Iteration 115, loss = 0.01465372
Iteration 116, loss = 0.01460864
Iteration 117, loss = 0.01468239
Iteration 118, loss = 0.01455621
Iteration 119, loss = 0.01460776
Iteration 120, loss = 0.01453250
Iteration 121, loss = 0.01455100
Iteration 122, loss = 0.01457034
Iteration 123, loss = 0.01451342
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69686787
Iteration 2, loss = 0.64232478
Iteration 3, loss = 0.55745028
Iteration 4, loss = 0.45484703
Iteration 5, loss = 0.36017492
Iteration 6, loss = 0.28057236
Iteration 7, loss = 0.21959752
Iteration 8, loss = 0.17446695
Iteration 9, loss = 0.14164247
Iteration 10, loss = 0.11762354
Iteration 11, loss = 0.10016201
Iteration 12, loss = 0.08671646
Iteration 13, loss = 0.07644676
Iteration 14, loss = 0.06865757
Iteration 15, loss = 0.06217590
Iteration 16, loss = 0.05738781
Iteration 17, loss = 0.05318727
Iteration 18, loss = 0.04963730
Iteration 19, loss = 0.04690629
Iteration 20, loss = 0.04400338
Iteration 21, loss = 0.04220853
Iteration 22, loss = 0.04048001
Iteration 23, loss = 0.03869832
Iteration 24, loss = 0.03725949
Iteration 25, loss = 0.03671596
Iteration 26, loss = 0.03562633
Iteration 27, loss = 0.03507102
Iteration 28, loss = 0.03403241
Iteration 29, loss = 0.03305290
Iteration 30, loss = 0.03228014
Iteration 31, loss = 0.03175744
Iteration 32, loss = 0.03108248
Iteration 33, loss = 0.03069272
Iteration 34, loss = 0.02996867
Iteration 35, loss = 0.02960479
Iteration 36, loss = 0.02935307
Iteration 37, loss = 0.02877470
Iteration 38, loss = 0.02833400
Iteration 39, loss = 0.02796945
Iteration 40, loss = 0.02788973
Iteration 41, loss = 0.02766714
Iteration 42, loss = 0.02730745
Iteration 43, loss = 0.02710685
Iteration 44, loss = 0.02676070
Iteration 45, loss = 0.02687804
Iteration 46, loss = 0.02629512
Iteration 47, loss = 0.02617355
Iteration 48, loss = 0.02621287
Iteration 49, loss = 0.02595945
Iteration 50, loss = 0.02572840
Iteration 51, loss = 0.02547323
Iteration 52, loss = 0.02570258
Iteration 53, loss = 0.02519759
Iteration 54, loss = 0.02516458
Iteration 55, loss = 0.02515582
Iteration 56, loss = 0.02496468
Iteration 57, loss = 0.02475693
Iteration 58, loss = 0.02461964
Iteration 59, loss = 0.02467184
Iteration 60, loss = 0.02479978
Iteration 61, loss = 0.02468892
Iteration 62, loss = 0.02429685
Iteration 63, loss = 0.02436171
Iteration 64, loss = 0.02414169
Iteration 65, loss = 0.02573839
Iteration 66, loss = 0.02561718
Iteration 67, loss = 0.02518191
Iteration 68, loss = 0.02508234
Iteration 69, loss = 0.02494601
Iteration 70, loss = 0.02479722
Iteration 71, loss = 0.02453138
Iteration 72, loss = 0.02436123
Iteration 73, loss = 0.02407423
Iteration 74, loss = 0.02378331
Iteration 75, loss = 0.02387004
Iteration 76, loss = 0.02372742
Iteration 77, loss = 0.02353098
Iteration 78, loss = 0.02353818
Iteration 79, loss = 0.02339414
Iteration 80, loss = 0.02320353
Iteration 81, loss = 0.02354157
Iteration 82, loss = 0.02315491
Iteration 83, loss = 0.02310683
Iteration 84, loss = 0.02301148
Iteration 85, loss = 0.02311836
Iteration 86, loss = 0.02312792
Iteration 87, loss = 0.02315224
Iteration 88, loss = 0.02315939
Iteration 89, loss = 0.02318390
Iteration 90, loss = 0.02296475
Iteration 91, loss = 0.02304456
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68759316
Iteration 2, loss = 0.63565139
Iteration 3, loss = 0.54260566
Iteration 4, loss = 0.43392603
Iteration 5, loss = 0.33577790
Iteration 6, loss = 0.25650100
Iteration 7, loss = 0.19792338
Iteration 8, loss = 0.15625534
Iteration 9, loss = 0.12697817
Iteration 10, loss = 0.10595758
Iteration 11, loss = 0.09041267
Iteration 12, loss = 0.07893736
Iteration 13, loss = 0.06995084
Iteration 14, loss = 0.06326485
Iteration 15, loss = 0.05774849
Iteration 16, loss = 0.05333130
Iteration 17, loss = 0.04946987
Iteration 18, loss = 0.04646712
Iteration 19, loss = 0.04400851
Iteration 20, loss = 0.04190227
Iteration 21, loss = 0.03997376
Iteration 22, loss = 0.03882622
Iteration 23, loss = 0.03668607
Iteration 24, loss = 0.03585541
Iteration 25, loss = 0.03478141
Iteration 26, loss = 0.03387262
Iteration 27, loss = 0.03300015
Iteration 28, loss = 0.03258255
Iteration 29, loss = 0.03155196
Iteration 30, loss = 0.03098010
Iteration 31, loss = 0.03036437
Iteration 32, loss = 0.02974079
Iteration 33, loss = 0.02928989
Iteration 34, loss = 0.02899926
Iteration 35, loss = 0.02869740
Iteration 36, loss = 0.02824126
Iteration 37, loss = 0.02805814
Iteration 38, loss = 0.02800452
Iteration 39, loss = 0.02741037
Iteration 40, loss = 0.02764351
Iteration 41, loss = 0.02687347
Iteration 42, loss = 0.02683806
Iteration 43, loss = 0.02652461
Iteration 44, loss = 0.02639205
Iteration 45, loss = 0.02622392
Iteration 46, loss = 0.02584628
Iteration 47, loss = 0.02560362
Iteration 48, loss = 0.02607208
Iteration 49, loss = 0.02582521
Iteration 50, loss = 0.02533311
Iteration 51, loss = 0.02548710
Iteration 52, loss = 0.02537315
Iteration 53, loss = 0.02500559
Iteration 54, loss = 0.02504568
Iteration 55, loss = 0.02506994
Iteration 56, loss = 0.02532405
Iteration 57, loss = 0.02511385
Iteration 58, loss = 0.02469386
Iteration 59, loss = 0.02477119
Iteration 60, loss = 0.02511900
Iteration 61, loss = 0.02484340
Iteration 62, loss = 0.02464250
Iteration 63, loss = 0.02466901
Iteration 64, loss = 0.02439070
Iteration 65, loss = 0.02444839
Iteration 66, loss = 0.02440793
Iteration 67, loss = 0.02425606
Iteration 68, loss = 0.02417768
Iteration 69, loss = 0.02433344
Iteration 70, loss = 0.02411829
Iteration 71, loss = 0.02412797
Iteration 72, loss = 0.02394863
Iteration 73, loss = 0.02379213
Iteration 74, loss = 0.02392981
Iteration 75, loss = 0.02423581
Iteration 76, loss = 0.02384492
Iteration 77, loss = 0.02422726
Iteration 78, loss = 0.02389574
Iteration 79, loss = 0.02383916
Iteration 80, loss = 0.02375625
Iteration 81, loss = 0.02367352
Iteration 82, loss = 0.02382733
Iteration 83, loss = 0.02390658
Iteration 84, loss = 0.02371549
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69478900
Iteration 2, loss = 0.61974271
Iteration 3, loss = 0.52050111
Iteration 4, loss = 0.41499976
Iteration 5, loss = 0.32005864
Iteration 6, loss = 0.24502651
Iteration 7, loss = 0.18927129
Iteration 8, loss = 0.14943066
Iteration 9, loss = 0.12161961
Iteration 10, loss = 0.10150246
Iteration 11, loss = 0.08683515
Iteration 12, loss = 0.07564959
Iteration 13, loss = 0.06726047
Iteration 14, loss = 0.06046877
Iteration 15, loss = 0.05520943
Iteration 16, loss = 0.05102494
Iteration 17, loss = 0.04738898
Iteration 18, loss = 0.04428816
Iteration 19, loss = 0.04193934
Iteration 20, loss = 0.03976907
Iteration 21, loss = 0.03818934
Iteration 22, loss = 0.03646966
Iteration 23, loss = 0.03529828
Iteration 24, loss = 0.03398835
Iteration 25, loss = 0.03322891
Iteration 26, loss = 0.03217008
Iteration 27, loss = 0.03158760
Iteration 28, loss = 0.03086864
Iteration 29, loss = 0.03013163
Iteration 30, loss = 0.02978034
Iteration 31, loss = 0.02904686
Iteration 32, loss = 0.02845085
Iteration 33, loss = 0.02826369
Iteration 34, loss = 0.02802416
Iteration 35, loss = 0.02761976
Iteration 36, loss = 0.02714803
Iteration 37, loss = 0.02672962
Iteration 38, loss = 0.02666310
Iteration 39, loss = 0.02618796
Iteration 40, loss = 0.02619545
Iteration 41, loss = 0.02589025
Iteration 42, loss = 0.02556208
Iteration 43, loss = 0.02564600
Iteration 44, loss = 0.02567232
Iteration 45, loss = 0.02542296
Iteration 46, loss = 0.02492323
Iteration 47, loss = 0.02509479
Iteration 48, loss = 0.02469243
Iteration 49, loss = 0.02489191
Iteration 50, loss = 0.02449915
Iteration 51, loss = 0.02468067
Iteration 52, loss = 0.02416327
Iteration 53, loss = 0.02446404
Iteration 54, loss = 0.02409442
Iteration 55, loss = 0.02413127
Iteration 56, loss = 0.02412093
Iteration 57, loss = 0.02410976
Iteration 58, loss = 0.02408049
Iteration 59, loss = 0.02402951
Iteration 60, loss = 0.02364160
Iteration 61, loss = 0.02356497
Iteration 62, loss = 0.02367417
Iteration 63, loss = 0.02350925
Iteration 64, loss = 0.02368615
Iteration 65, loss = 0.02398054
Iteration 66, loss = 0.02343972
Iteration 67, loss = 0.02351369
Iteration 68, loss = 0.02317532
Iteration 69, loss = 0.02343017
Iteration 70, loss = 0.02316794
Iteration 71, loss = 0.02326142
Iteration 72, loss = 0.02337044
Iteration 73, loss = 0.02323421
Iteration 74, loss = 0.02333552
Iteration 75, loss = 0.02310816
Iteration 76, loss = 0.02280268
Iteration 77, loss = 0.02286229
Iteration 78, loss = 0.02298947
Iteration 79, loss = 0.02325739
Iteration 80, loss = 0.02319179
Iteration 81, loss = 0.02322774
Iteration 82, loss = 0.02309720
Iteration 83, loss = 0.02316306
Iteration 84, loss = 0.02290040
Iteration 85, loss = 0.02286631
Iteration 86, loss = 0.02271569
Iteration 87, loss = 0.02280938
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67866355
Iteration 2, loss = 0.65164674
Iteration 3, loss = 0.62096718
Iteration 4, loss = 0.58129720
Iteration 5, loss = 0.53354156
Iteration 6, loss = 0.48163283
Iteration 7, loss = 0.43016449
Iteration 8, loss = 0.38175826
Iteration 9, loss = 0.33742872
Iteration 10, loss = 0.29714187
Iteration 11, loss = 0.26133871
Iteration 12, loss = 0.23005269
Iteration 13, loss = 0.20301804
Iteration 14, loss = 0.17969309
Iteration 15, loss = 0.15971551
Iteration 16, loss = 0.14249382
Iteration 17, loss = 0.12796888
Iteration 18, loss = 0.11537577
Iteration 19, loss = 0.10472284
Iteration 20, loss = 0.09556174
Iteration 21, loss = 0.08770758
Iteration 22, loss = 0.08073769
Iteration 23, loss = 0.07477975
Iteration 24, loss = 0.06953356
Iteration 25, loss = 0.06491939
Iteration 26, loss = 0.06076296
Iteration 27, loss = 0.05706810
Iteration 28, loss = 0.05394210
Iteration 29, loss = 0.05106130
Iteration 30, loss = 0.04849481
Iteration 31, loss = 0.04600544
Iteration 32, loss = 0.04386054
Iteration 33, loss = 0.04195280
Iteration 34, loss = 0.04025859
Iteration 35, loss = 0.03870740
Iteration 36, loss = 0.03718278
Iteration 37, loss = 0.03598036
Iteration 38, loss = 0.03454462
Iteration 39, loss = 0.03335402
Iteration 40, loss = 0.03253990
Iteration 41, loss = 0.03147154
Iteration 42, loss = 0.03060376
Iteration 43, loss = 0.02999314
Iteration 44, loss = 0.02909867
Iteration 45, loss = 0.02830132
Iteration 46, loss = 0.02756328
Iteration 47, loss = 0.02712943
Iteration 48, loss = 0.02649677
Iteration 49, loss = 0.02611247
Iteration 50, loss = 0.02544739
Iteration 51, loss = 0.02491296
Iteration 52, loss = 0.02432807
Iteration 53, loss = 0.02377013
Iteration 54, loss = 0.02346426
Iteration 55, loss = 0.02307392
Iteration 56, loss = 0.02267834
Iteration 57, loss = 0.02234563
Iteration 58, loss = 0.02201282
Iteration 59, loss = 0.02177483
Iteration 60, loss = 0.02144608
Iteration 61, loss = 0.02113769
Iteration 62, loss = 0.02084644
Iteration 63, loss = 0.02045926
Iteration 64, loss = 0.02037175
Iteration 65, loss = 0.02009810
Iteration 66, loss = 0.02032522
Iteration 67, loss = 0.02026552
Iteration 68, loss = 0.01996121
Iteration 69, loss = 0.01962752
Iteration 70, loss = 0.01935644
Iteration 71, loss = 0.01900857
Iteration 72, loss = 0.01900439
Iteration 73, loss = 0.01854094
Iteration 74, loss = 0.01842054
Iteration 75, loss = 0.01832642
Iteration 76, loss = 0.01821926
Iteration 77, loss = 0.01809772
Iteration 78, loss = 0.01772870
Iteration 79, loss = 0.01818839
Iteration 80, loss = 0.01810085
Iteration 81, loss = 0.01797649
Iteration 82, loss = 0.01774355
Iteration 83, loss = 0.01774294
Iteration 84, loss = 0.01782685
Iteration 85, loss = 0.01765752
Iteration 86, loss = 0.01747677
Iteration 87, loss = 0.01723566
Iteration 88, loss = 0.01702347
Iteration 89, loss = 0.01680390
Iteration 90, loss = 0.01676434
Iteration 91, loss = 0.01665552
Iteration 92, loss = 0.01651153
Iteration 93, loss = 0.01651610
Iteration 94, loss = 0.01644535
Iteration 95, loss = 0.01638882
Iteration 96, loss = 0.01625621
Iteration 97, loss = 0.01626712
Iteration 98, loss = 0.01613548
Iteration 99, loss = 0.01617761
Iteration 100, loss = 0.01617287
Iteration 101, loss = 0.01606128
Iteration 102, loss = 0.01641636
Iteration 103, loss = 0.01655148
Iteration 104, loss = 0.01633192
Iteration 105, loss = 0.01611686
Iteration 106, loss = 0.01614733
Iteration 107, loss = 0.01565057
Iteration 108, loss = 0.01597848
Iteration 109, loss = 0.01584452
Iteration 110, loss = 0.01581285
Iteration 111, loss = 0.01557368
Iteration 112, loss = 0.01562522
Iteration 113, loss = 0.01552307
Iteration 114, loss = 0.01578585
Iteration 115, loss = 0.01609179
Iteration 116, loss = 0.01587794
Iteration 117, loss = 0.01547756
Iteration 118, loss = 0.01537899
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67978572
Iteration 2, loss = 0.65064789
Iteration 3, loss = 0.61160537
Iteration 4, loss = 0.55774013
Iteration 5, loss = 0.49563741
Iteration 6, loss = 0.43306344
Iteration 7, loss = 0.37412557
Iteration 8, loss = 0.32035310
Iteration 9, loss = 0.27335000
Iteration 10, loss = 0.23283000
Iteration 11, loss = 0.19889222
Iteration 12, loss = 0.17065975
Iteration 13, loss = 0.14761350
Iteration 14, loss = 0.12854060
Iteration 15, loss = 0.11295117
Iteration 16, loss = 0.09999636
Iteration 17, loss = 0.08929239
Iteration 18, loss = 0.08016739
Iteration 19, loss = 0.07267745
Iteration 20, loss = 0.06621118
Iteration 21, loss = 0.06086079
Iteration 22, loss = 0.05602703
Iteration 23, loss = 0.05205240
Iteration 24, loss = 0.04850765
Iteration 25, loss = 0.04537366
Iteration 26, loss = 0.04274230
Iteration 27, loss = 0.04022418
Iteration 28, loss = 0.03807407
Iteration 29, loss = 0.03626836
Iteration 30, loss = 0.03447878
Iteration 31, loss = 0.03302022
Iteration 32, loss = 0.03151118
Iteration 33, loss = 0.03030811
Iteration 34, loss = 0.02922353
Iteration 35, loss = 0.02815793
Iteration 36, loss = 0.02705624
Iteration 37, loss = 0.02639494
Iteration 38, loss = 0.02546963
Iteration 39, loss = 0.02477574
Iteration 40, loss = 0.02404143
Iteration 41, loss = 0.02351419
Iteration 42, loss = 0.02286307
Iteration 43, loss = 0.02232515
Iteration 44, loss = 0.02188909
Iteration 45, loss = 0.02147687
Iteration 46, loss = 0.02101448
Iteration 47, loss = 0.02060311
Iteration 48, loss = 0.02026077
Iteration 49, loss = 0.01989974
Iteration 50, loss = 0.01960764
Iteration 51, loss = 0.01926447
Iteration 52, loss = 0.01895017
Iteration 53, loss = 0.01869687
Iteration 54, loss = 0.01848462
Iteration 55, loss = 0.01820202
Iteration 56, loss = 0.01806560
Iteration 57, loss = 0.01772516
Iteration 58, loss = 0.01761781
Iteration 59, loss = 0.01731130
Iteration 60, loss = 0.01724151
Iteration 61, loss = 0.01701515
Iteration 62, loss = 0.01680716
Iteration 63, loss = 0.01664509
Iteration 64, loss = 0.01653035
Iteration 65, loss = 0.01631658
Iteration 66, loss = 0.01631224
Iteration 67, loss = 0.01613472
Iteration 68, loss = 0.01603541
Iteration 69, loss = 0.01595290
Iteration 70, loss = 0.01578825
Iteration 71, loss = 0.01567363
Iteration 72, loss = 0.01558021
Iteration 73, loss = 0.01548336
Iteration 74, loss = 0.01533054
Iteration 75, loss = 0.01527673
Iteration 76, loss = 0.01518676
Iteration 77, loss = 0.01513661
Iteration 78, loss = 0.01505893
Iteration 79, loss = 0.01500046
Iteration 80, loss = 0.01490374
Iteration 81, loss = 0.01491546
Iteration 82, loss = 0.01480097
Iteration 83, loss = 0.01462017
Iteration 84, loss = 0.01463552
Iteration 85, loss = 0.01456536
Iteration 86, loss = 0.01443660
Iteration 87, loss = 0.01446732
Iteration 88, loss = 0.01458490
Iteration 89, loss = 0.01436004
Iteration 90, loss = 0.01437034
Iteration 91, loss = 0.01434691
Iteration 92, loss = 0.01421381
Iteration 93, loss = 0.01418676
Iteration 94, loss = 0.01414131
Iteration 95, loss = 0.01414295
Iteration 96, loss = 0.01397525
Iteration 97, loss = 0.01403938
Iteration 98, loss = 0.01403934
Iteration 99, loss = 0.01398332
Iteration 100, loss = 0.01405219
Iteration 101, loss = 0.01393702
Iteration 102, loss = 0.01383875
Iteration 103, loss = 0.01379884
Iteration 104, loss = 0.01379394
Iteration 105, loss = 0.01375646
Iteration 106, loss = 0.01361542
Iteration 107, loss = 0.01367699
Iteration 108, loss = 0.01361573
Iteration 109, loss = 0.01365921
Iteration 110, loss = 0.01360898
Iteration 111, loss = 0.01360538
Iteration 112, loss = 0.01349663
Iteration 113, loss = 0.01363977
Iteration 114, loss = 0.01349549
Iteration 115, loss = 0.01341000
Iteration 116, loss = 0.01340194
Iteration 117, loss = 0.01354309
Iteration 118, loss = 0.01333474
Iteration 119, loss = 0.01341480
Iteration 120, loss = 0.01355954
Iteration 121, loss = 0.01336150
Iteration 122, loss = 0.01330012
Iteration 123, loss = 0.01340941
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67872693
Iteration 2, loss = 0.62815336
Iteration 3, loss = 0.54894081
Iteration 4, loss = 0.45775494
Iteration 5, loss = 0.37200871
Iteration 6, loss = 0.29755884
Iteration 7, loss = 0.23601528
Iteration 8, loss = 0.18848392
Iteration 9, loss = 0.15323329
Iteration 10, loss = 0.12667840
Iteration 11, loss = 0.10692169
Iteration 12, loss = 0.09179789
Iteration 13, loss = 0.07993753
Iteration 14, loss = 0.07096786
Iteration 15, loss = 0.06397863
Iteration 16, loss = 0.05771222
Iteration 17, loss = 0.05290487
Iteration 18, loss = 0.04896354
Iteration 19, loss = 0.04547464
Iteration 20, loss = 0.04252817
Iteration 21, loss = 0.04027834
Iteration 22, loss = 0.03830504
Iteration 23, loss = 0.03643149
Iteration 24, loss = 0.03451811
Iteration 25, loss = 0.03333926
Iteration 26, loss = 0.03198017
Iteration 27, loss = 0.03082626
Iteration 28, loss = 0.03004625
Iteration 29, loss = 0.02895592
Iteration 30, loss = 0.02842884
Iteration 31, loss = 0.02756431
Iteration 32, loss = 0.02709154
Iteration 33, loss = 0.02638623
Iteration 34, loss = 0.02582576
Iteration 35, loss = 0.02545628
Iteration 36, loss = 0.02492765
Iteration 37, loss = 0.02465275
Iteration 38, loss = 0.02437911
Iteration 39, loss = 0.02379063
Iteration 40, loss = 0.02348879
Iteration 41, loss = 0.02326050
Iteration 42, loss = 0.02303055
Iteration 43, loss = 0.02277809
Iteration 44, loss = 0.02250140
Iteration 45, loss = 0.02220798
Iteration 46, loss = 0.02232813
Iteration 47, loss = 0.02209834
Iteration 48, loss = 0.02159004
Iteration 49, loss = 0.02166975
Iteration 50, loss = 0.02156618
Iteration 51, loss = 0.02125291
Iteration 52, loss = 0.02105940
Iteration 53, loss = 0.02116536
Iteration 54, loss = 0.02079304
Iteration 55, loss = 0.02088203
Iteration 56, loss = 0.02092562
Iteration 57, loss = 0.02063335
Iteration 58, loss = 0.02058747
Iteration 59, loss = 0.02061620
Iteration 60, loss = 0.02045349
Iteration 61, loss = 0.02026204
Iteration 62, loss = 0.02007658
Iteration 63, loss = 0.02013503
Iteration 64, loss = 0.02002783
Iteration 65, loss = 0.01984491
Iteration 66, loss = 0.01978076
Iteration 67, loss = 0.01981477
Iteration 68, loss = 0.01968513
Iteration 69, loss = 0.01979560
Iteration 70, loss = 0.01973959
Iteration 71, loss = 0.01952566
Iteration 72, loss = 0.01977354
Iteration 73, loss = 0.01938917
Iteration 74, loss = 0.01940451
Iteration 75, loss = 0.01936511
Iteration 76, loss = 0.01952750
Iteration 77, loss = 0.01952252
Iteration 78, loss = 0.01944675
Iteration 79, loss = 0.01932592
Iteration 80, loss = 0.01945625
Iteration 81, loss = 0.01923260
Iteration 82, loss = 0.01931302
Iteration 83, loss = 0.01946064
Iteration 84, loss = 0.01930069
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70995650
Iteration 2, loss = 0.68974513
Iteration 3, loss = 0.67392671
Iteration 4, loss = 0.65908345
Iteration 5, loss = 0.64421521
Iteration 6, loss = 0.62874670
Iteration 7, loss = 0.61249475
Iteration 8, loss = 0.59521340
Iteration 9, loss = 0.57683050
Iteration 10, loss = 0.55706123
Iteration 11, loss = 0.53647777
Iteration 12, loss = 0.51470881
Iteration 13, loss = 0.49239228
Iteration 14, loss = 0.46945115
Iteration 15, loss = 0.44620640
Iteration 16, loss = 0.42310727
Iteration 17, loss = 0.40006174
Iteration 18, loss = 0.37762859
Iteration 19, loss = 0.35576680
Iteration 20, loss = 0.33479716
Iteration 21, loss = 0.31470135
Iteration 22, loss = 0.29561444
Iteration 23, loss = 0.27756865
Iteration 24, loss = 0.26048643
Iteration 25, loss = 0.24448958
Iteration 26, loss = 0.22945704
Iteration 27, loss = 0.21533529
Iteration 28, loss = 0.20218268
Iteration 29, loss = 0.18980731
Iteration 30, loss = 0.17839297
Iteration 31, loss = 0.16775407
Iteration 32, loss = 0.15788150
Iteration 33, loss = 0.14865472
Iteration 34, loss = 0.14013921
Iteration 35, loss = 0.13223045
Iteration 36, loss = 0.12493034
Iteration 37, loss = 0.11807264
Iteration 38, loss = 0.11178264
Iteration 39, loss = 0.10598107
Iteration 40, loss = 0.10048433
Iteration 41, loss = 0.09541158
Iteration 42, loss = 0.09070090
Iteration 43, loss = 0.08628281
Iteration 44, loss = 0.08217814
Iteration 45, loss = 0.07836014
Iteration 46, loss = 0.07476712
Iteration 47, loss = 0.07146365
Iteration 48, loss = 0.06834020
Iteration 49, loss = 0.06542146
Iteration 50, loss = 0.06267203
Iteration 51, loss = 0.06012493
Iteration 52, loss = 0.05768094
Iteration 53, loss = 0.05542534
Iteration 54, loss = 0.05327986
Iteration 55, loss = 0.05124858
Iteration 56, loss = 0.04938171
Iteration 57, loss = 0.04753134
Iteration 58, loss = 0.04584456
Iteration 59, loss = 0.04420899
Iteration 60, loss = 0.04268811
Iteration 61, loss = 0.04122718
Iteration 62, loss = 0.03986267
Iteration 63, loss = 0.03855231
Iteration 64, loss = 0.03730739
Iteration 65, loss = 0.03610409
Iteration 66, loss = 0.03498021
Iteration 67, loss = 0.03388612
Iteration 68, loss = 0.03286404
Iteration 69, loss = 0.03187654
Iteration 70, loss = 0.03095048
Iteration 71, loss = 0.03004425
Iteration 72, loss = 0.02920175
Iteration 73, loss = 0.02836569
Iteration 74, loss = 0.02759681
Iteration 75, loss = 0.02684249
Iteration 76, loss = 0.02611367
Iteration 77, loss = 0.02544469
Iteration 78, loss = 0.02478420
Iteration 79, loss = 0.02415316
Iteration 80, loss = 0.02354988
Iteration 81, loss = 0.02296985
Iteration 82, loss = 0.02240063
Iteration 83, loss = 0.02186821
Iteration 84, loss = 0.02134257
Iteration 85, loss = 0.02082444
Iteration 86, loss = 0.02034413
Iteration 87, loss = 0.01987686
Iteration 88, loss = 0.01941190
Iteration 89, loss = 0.01897326
Iteration 90, loss = 0.01854189
Iteration 91, loss = 0.01813045
Iteration 92, loss = 0.01774090
Iteration 93, loss = 0.01736278
Iteration 94, loss = 0.01700028
Iteration 95, loss = 0.01664405
Iteration 96, loss = 0.01630912
Iteration 97, loss = 0.01597080
Iteration 98, loss = 0.01565744
Iteration 99, loss = 0.01535359
Iteration 100, loss = 0.01505156
Iteration 101, loss = 0.01476061
Iteration 102, loss = 0.01447357
Iteration 103, loss = 0.01420652
Iteration 104, loss = 0.01393161
Iteration 105, loss = 0.01367251
Iteration 106, loss = 0.01342065
Iteration 107, loss = 0.01318023
Iteration 108, loss = 0.01293853
Iteration 109, loss = 0.01271498
Iteration 110, loss = 0.01249891
Iteration 111, loss = 0.01227706
Iteration 112, loss = 0.01207210
Iteration 113, loss = 0.01186887
Iteration 114, loss = 0.01167795
Iteration 115, loss = 0.01147346
Iteration 116, loss = 0.01129047
Iteration 117, loss = 0.01110224
Iteration 118, loss = 0.01092406
Iteration 119, loss = 0.01074670
Iteration 120, loss = 0.01058362
Iteration 121, loss = 0.01041163
Iteration 122, loss = 0.01024958
Iteration 123, loss = 0.01009222
Iteration 124, loss = 0.00993408
Iteration 125, loss = 0.00978699
Iteration 126, loss = 0.00963873
Iteration 127, loss = 0.00949693
Iteration 128, loss = 0.00935530
Iteration 129, loss = 0.00921648
Iteration 130, loss = 0.00909028
Iteration 131, loss = 0.00894691
Iteration 132, loss = 0.00882213
Iteration 133, loss = 0.00869305
Iteration 134, loss = 0.00857036
Iteration 135, loss = 0.00844853
Iteration 136, loss = 0.00833673
Iteration 137, loss = 0.00821989
Iteration 138, loss = 0.00810948
Iteration 139, loss = 0.00800583
Iteration 140, loss = 0.00789686
Iteration 141, loss = 0.00779863
Iteration 142, loss = 0.00769646
Iteration 143, loss = 0.00760272
Iteration 144, loss = 0.00750495
Iteration 145, loss = 0.00741125
Iteration 146, loss = 0.00732080
Iteration 147, loss = 0.00723243
Iteration 148, loss = 0.00714250
Iteration 149, loss = 0.00705727
Iteration 150, loss = 0.00696872
Iteration 151, loss = 0.00688618
Iteration 152, loss = 0.00680508
Iteration 153, loss = 0.00672335
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67905711
Iteration 2, loss = 0.65546898
Iteration 3, loss = 0.62347000
Iteration 4, loss = 0.57837672
Iteration 5, loss = 0.52242577
Iteration 6, loss = 0.46314618
Iteration 7, loss = 0.40529473
Iteration 8, loss = 0.35024787
Iteration 9, loss = 0.30071054
Iteration 10, loss = 0.25728407
Iteration 11, loss = 0.22027093
Iteration 12, loss = 0.18906326
Iteration 13, loss = 0.16309385
Iteration 14, loss = 0.14173949
Iteration 15, loss = 0.12421363
Iteration 16, loss = 0.10967210
Iteration 17, loss = 0.09761066
Iteration 18, loss = 0.08745415
Iteration 19, loss = 0.07918336
Iteration 20, loss = 0.07203285
Iteration 21, loss = 0.06596010
Iteration 22, loss = 0.06100986
Iteration 23, loss = 0.05622549
Iteration 24, loss = 0.05247473
Iteration 25, loss = 0.04894025
Iteration 26, loss = 0.04607270
Iteration 27, loss = 0.04346553
Iteration 28, loss = 0.04112119
Iteration 29, loss = 0.03902454
Iteration 30, loss = 0.03712277
Iteration 31, loss = 0.03544703
Iteration 32, loss = 0.03392522
Iteration 33, loss = 0.03269003
Iteration 34, loss = 0.03147462
Iteration 35, loss = 0.03021876
Iteration 36, loss = 0.02939772
Iteration 37, loss = 0.02838388
Iteration 38, loss = 0.02760394
Iteration 39, loss = 0.02676385
Iteration 40, loss = 0.02605427
Iteration 41, loss = 0.02544969
Iteration 42, loss = 0.02486575
Iteration 43, loss = 0.02415425
Iteration 44, loss = 0.02369396
Iteration 45, loss = 0.02321630
Iteration 46, loss = 0.02271964
Iteration 47, loss = 0.02234327
Iteration 48, loss = 0.02194507
Iteration 49, loss = 0.02158217
Iteration 50, loss = 0.02116609
Iteration 51, loss = 0.02082769
Iteration 52, loss = 0.02060927
Iteration 53, loss = 0.02024384
Iteration 54, loss = 0.01997816
Iteration 55, loss = 0.01978615
Iteration 56, loss = 0.01943674
Iteration 57, loss = 0.01933277
Iteration 58, loss = 0.01905163
Iteration 59, loss = 0.01883630
Iteration 60, loss = 0.01856985
Iteration 61, loss = 0.01845179
Iteration 62, loss = 0.01831701
Iteration 63, loss = 0.01811857
Iteration 64, loss = 0.01801087
Iteration 65, loss = 0.01782076
Iteration 66, loss = 0.01774922
Iteration 67, loss = 0.01749974
Iteration 68, loss = 0.01736202
Iteration 69, loss = 0.01734350
Iteration 70, loss = 0.01719951
Iteration 71, loss = 0.01707671
Iteration 72, loss = 0.01696488
Iteration 73, loss = 0.01683937
Iteration 74, loss = 0.01682578
Iteration 75, loss = 0.01668613
Iteration 76, loss = 0.01656099
Iteration 77, loss = 0.01637975
Iteration 78, loss = 0.01633742
Iteration 79, loss = 0.01633102
Iteration 80, loss = 0.01630105
Iteration 81, loss = 0.01615403
Iteration 82, loss = 0.01612043
Iteration 83, loss = 0.01601526
Iteration 84, loss = 0.01608760
Iteration 85, loss = 0.01598470
Iteration 86, loss = 0.01587156
Iteration 87, loss = 0.01578715
Iteration 88, loss = 0.01575722
Iteration 89, loss = 0.01575275
Iteration 90, loss = 0.01570439
Iteration 91, loss = 0.01556724
Iteration 92, loss = 0.01549676
Iteration 93, loss = 0.01558902
Iteration 94, loss = 0.01553754
Iteration 95, loss = 0.01536717
Iteration 96, loss = 0.01536708
Iteration 97, loss = 0.01525845
Iteration 98, loss = 0.01531203
Iteration 99, loss = 0.01525289
Iteration 100, loss = 0.01524205
Iteration 101, loss = 0.01509713
Iteration 102, loss = 0.01525783
Iteration 103, loss = 0.01515958
Iteration 104, loss = 0.01500438
Iteration 105, loss = 0.01510430
Iteration 106, loss = 0.01498621
Iteration 107, loss = 0.01497150
Iteration 108, loss = 0.01486730
Iteration 109, loss = 0.01484190
Iteration 110, loss = 0.01486852
Iteration 111, loss = 0.01488353
Iteration 112, loss = 0.01501847
Iteration 113, loss = 0.01491907
Iteration 114, loss = 0.01481376
Iteration 115, loss = 0.01476798
Iteration 116, loss = 0.01472045
Iteration 117, loss = 0.01469675
Iteration 118, loss = 0.01461781
Iteration 119, loss = 0.01465619
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68304184
Iteration 2, loss = 0.63319120
Iteration 3, loss = 0.55795975
Iteration 4, loss = 0.45961136
Iteration 5, loss = 0.36191883
Iteration 6, loss = 0.28127962
Iteration 7, loss = 0.21983551
Iteration 8, loss = 0.17437107
Iteration 9, loss = 0.14109402
Iteration 10, loss = 0.11711272
Iteration 11, loss = 0.09946858
Iteration 12, loss = 0.08608812
Iteration 13, loss = 0.07568526
Iteration 14, loss = 0.06779883
Iteration 15, loss = 0.06146567
Iteration 16, loss = 0.05587242
Iteration 17, loss = 0.05166322
Iteration 18, loss = 0.04813714
Iteration 19, loss = 0.04666563
Iteration 20, loss = 0.04416150
Iteration 21, loss = 0.04138348
Iteration 22, loss = 0.03954085
Iteration 23, loss = 0.03784491
Iteration 24, loss = 0.03658517
Iteration 25, loss = 0.03525628
Iteration 26, loss = 0.03415164
Iteration 27, loss = 0.03311007
Iteration 28, loss = 0.03214140
Iteration 29, loss = 0.03160985
Iteration 30, loss = 0.03087009
Iteration 31, loss = 0.03012979
Iteration 32, loss = 0.02969882
Iteration 33, loss = 0.02902719
Iteration 34, loss = 0.02872104
Iteration 35, loss = 0.02835153
Iteration 36, loss = 0.02780886
Iteration 37, loss = 0.02747038
Iteration 38, loss = 0.02693821
Iteration 39, loss = 0.02648600
Iteration 40, loss = 0.02631328
Iteration 41, loss = 0.02579684
Iteration 42, loss = 0.02579524
Iteration 43, loss = 0.02528178
Iteration 44, loss = 0.02508289
Iteration 45, loss = 0.02503649
Iteration 46, loss = 0.02495692
Iteration 47, loss = 0.02480236
Iteration 48, loss = 0.02447373
Iteration 49, loss = 0.02434485
Iteration 50, loss = 0.02384010
Iteration 51, loss = 0.02375212
Iteration 52, loss = 0.02364307
Iteration 53, loss = 0.02361529
Iteration 54, loss = 0.02346470
Iteration 55, loss = 0.02327784
Iteration 56, loss = 0.02320467
Iteration 57, loss = 0.02313314
Iteration 58, loss = 0.02328568
Iteration 59, loss = 0.02287205
Iteration 60, loss = 0.02285516
Iteration 61, loss = 0.02302914
Iteration 62, loss = 0.02271251
Iteration 63, loss = 0.02249987
Iteration 64, loss = 0.02244885
Iteration 65, loss = 0.02236284
Iteration 66, loss = 0.02250011
Iteration 67, loss = 0.02228451
Iteration 68, loss = 0.02263348
Iteration 69, loss = 0.02244993
Iteration 70, loss = 0.02227192
Iteration 71, loss = 0.02243195
Iteration 72, loss = 0.02176183
Iteration 73, loss = 0.02235950
Iteration 74, loss = 0.02203935
Iteration 75, loss = 0.02195880
Iteration 76, loss = 0.02164251
Iteration 77, loss = 0.02177170
Iteration 78, loss = 0.02181764
Iteration 79, loss = 0.02175666
Iteration 80, loss = 0.02174008
Iteration 81, loss = 0.02163845
Iteration 82, loss = 0.02188175
Iteration 83, loss = 0.02274189
Iteration 84, loss = 0.02214038
Iteration 85, loss = 0.02235810
Iteration 86, loss = 0.02221985
Iteration 87, loss = 0.02203997
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69020822
Iteration 2, loss = 0.64706437
Iteration 3, loss = 0.57726186
Iteration 4, loss = 0.48578069
Iteration 5, loss = 0.39250438
Iteration 6, loss = 0.31054300
Iteration 7, loss = 0.24504338
Iteration 8, loss = 0.19476106
Iteration 9, loss = 0.15764917
Iteration 10, loss = 0.13020248
Iteration 11, loss = 0.10925582
Iteration 12, loss = 0.09385762
Iteration 13, loss = 0.08180578
Iteration 14, loss = 0.07237658
Iteration 15, loss = 0.06471944
Iteration 16, loss = 0.05864841
Iteration 17, loss = 0.05398029
Iteration 18, loss = 0.04955515
Iteration 19, loss = 0.04619284
Iteration 20, loss = 0.04319753
Iteration 21, loss = 0.04072050
Iteration 22, loss = 0.03887546
Iteration 23, loss = 0.03699635
Iteration 24, loss = 0.03523155
Iteration 25, loss = 0.03353218
Iteration 26, loss = 0.03238508
Iteration 27, loss = 0.03121759
Iteration 28, loss = 0.03037479
Iteration 29, loss = 0.02931030
Iteration 30, loss = 0.02861593
Iteration 31, loss = 0.02796921
Iteration 32, loss = 0.02733604
Iteration 33, loss = 0.02665835
Iteration 34, loss = 0.02622983
Iteration 35, loss = 0.02556360
Iteration 36, loss = 0.02530655
Iteration 37, loss = 0.02459465
Iteration 38, loss = 0.02445571
Iteration 39, loss = 0.02406117
Iteration 40, loss = 0.02384596
Iteration 41, loss = 0.02348142
Iteration 42, loss = 0.02316229
Iteration 43, loss = 0.02333775
Iteration 44, loss = 0.02258561
Iteration 45, loss = 0.02274780
Iteration 46, loss = 0.02224205
Iteration 47, loss = 0.02198286
Iteration 48, loss = 0.02161351
Iteration 49, loss = 0.02163432
Iteration 50, loss = 0.02154845
Iteration 51, loss = 0.02142918
Iteration 52, loss = 0.02099771
Iteration 53, loss = 0.02105356
Iteration 54, loss = 0.02087708
Iteration 55, loss = 0.02084292
Iteration 56, loss = 0.02058206
Iteration 57, loss = 0.02061399
Iteration 58, loss = 0.02054792
Iteration 59, loss = 0.02072630
Iteration 60, loss = 0.02030800
Iteration 61, loss = 0.02018609
Iteration 62, loss = 0.02031702
Iteration 63, loss = 0.02006166
Iteration 64, loss = 0.02018393
Iteration 65, loss = 0.02042697
Iteration 66, loss = 0.02000060
Iteration 67, loss = 0.01976956
Iteration 68, loss = 0.01997475
Iteration 69, loss = 0.01967593
Iteration 70, loss = 0.01989926
Iteration 71, loss = 0.01974399
Iteration 72, loss = 0.01970870
Iteration 73, loss = 0.01950044
Iteration 74, loss = 0.01956634
Iteration 75, loss = 0.01970362
Iteration 76, loss = 0.01939576
Iteration 77, loss = 0.01934195
Iteration 78, loss = 0.01948041
Iteration 79, loss = 0.01949370
Iteration 80, loss = 0.01932895
Iteration 81, loss = 0.01923110
Iteration 82, loss = 0.01929166
Iteration 83, loss = 0.01903785
Iteration 84, loss = 0.01918398
Iteration 85, loss = 0.01896403
Iteration 86, loss = 0.01931663
Iteration 87, loss = 0.01891610
Iteration 88, loss = 0.01916480
Iteration 89, loss = 0.01889059
Iteration 90, loss = 0.01893211
Iteration 91, loss = 0.01888233
Iteration 92, loss = 0.01888977
Iteration 93, loss = 0.01898092
Iteration 94, loss = 0.01884930
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73604245
Iteration 2, loss = 0.71928401
Iteration 3, loss = 0.70456993
Iteration 4, loss = 0.69026029
Iteration 5, loss = 0.67532985
Iteration 6, loss = 0.65911249
Iteration 7, loss = 0.64136347
Iteration 8, loss = 0.62234443
Iteration 9, loss = 0.60189742
Iteration 10, loss = 0.58013269
Iteration 11, loss = 0.55722649
Iteration 12, loss = 0.53338757
Iteration 13, loss = 0.50863576
Iteration 14, loss = 0.48349450
Iteration 15, loss = 0.45827302
Iteration 16, loss = 0.43315079
Iteration 17, loss = 0.40828693
Iteration 18, loss = 0.38390313
Iteration 19, loss = 0.36048237
Iteration 20, loss = 0.33778782
Iteration 21, loss = 0.31627241
Iteration 22, loss = 0.29585957
Iteration 23, loss = 0.27661832
Iteration 24, loss = 0.25846929
Iteration 25, loss = 0.24155408
Iteration 26, loss = 0.22576939
Iteration 27, loss = 0.21108437
Iteration 28, loss = 0.19744758
Iteration 29, loss = 0.18483461
Iteration 30, loss = 0.17313119
Iteration 31, loss = 0.16231394
Iteration 32, loss = 0.15234971
Iteration 33, loss = 0.14311975
Iteration 34, loss = 0.13464008
Iteration 35, loss = 0.12676969
Iteration 36, loss = 0.11956668
Iteration 37, loss = 0.11287049
Iteration 38, loss = 0.10667518
Iteration 39, loss = 0.10098966
Iteration 40, loss = 0.09574271
Iteration 41, loss = 0.09084642
Iteration 42, loss = 0.08633049
Iteration 43, loss = 0.08211902
Iteration 44, loss = 0.07820361
Iteration 45, loss = 0.07455951
Iteration 46, loss = 0.07118848
Iteration 47, loss = 0.06802266
Iteration 48, loss = 0.06508064
Iteration 49, loss = 0.06235469
Iteration 50, loss = 0.05973294
Iteration 51, loss = 0.05730972
Iteration 52, loss = 0.05508309
Iteration 53, loss = 0.05294225
Iteration 54, loss = 0.05097703
Iteration 55, loss = 0.04910910
Iteration 56, loss = 0.04732802
Iteration 57, loss = 0.04567989
Iteration 58, loss = 0.04408120
Iteration 59, loss = 0.04257785
Iteration 60, loss = 0.04117677
Iteration 61, loss = 0.03982947
Iteration 62, loss = 0.03856688
Iteration 63, loss = 0.03738247
Iteration 64, loss = 0.03624797
Iteration 65, loss = 0.03519709
Iteration 66, loss = 0.03417837
Iteration 67, loss = 0.03319970
Iteration 68, loss = 0.03227767
Iteration 69, loss = 0.03140111
Iteration 70, loss = 0.03056107
Iteration 71, loss = 0.02973112
Iteration 72, loss = 0.02897769
Iteration 73, loss = 0.02821773
Iteration 74, loss = 0.02748799
Iteration 75, loss = 0.02680552
Iteration 76, loss = 0.02614099
Iteration 77, loss = 0.02549983
Iteration 78, loss = 0.02490734
Iteration 79, loss = 0.02435228
Iteration 80, loss = 0.02377248
Iteration 81, loss = 0.02324546
Iteration 82, loss = 0.02273965
Iteration 83, loss = 0.02225263
Iteration 84, loss = 0.02179523
Iteration 85, loss = 0.02135681
Iteration 86, loss = 0.02090803
Iteration 87, loss = 0.02048823
Iteration 88, loss = 0.02009207
Iteration 89, loss = 0.01972392
Iteration 90, loss = 0.01932508
Iteration 91, loss = 0.01895426
Iteration 92, loss = 0.01863470
Iteration 93, loss = 0.01827198
Iteration 94, loss = 0.01795201
Iteration 95, loss = 0.01764959
Iteration 96, loss = 0.01733837
Iteration 97, loss = 0.01706228
Iteration 98, loss = 0.01680014
Iteration 99, loss = 0.01649363
Iteration 100, loss = 0.01622476
Iteration 101, loss = 0.01596824
Iteration 102, loss = 0.01571632
Iteration 103, loss = 0.01547546
Iteration 104, loss = 0.01524079
Iteration 105, loss = 0.01502315
Iteration 106, loss = 0.01478720
Iteration 107, loss = 0.01458349
Iteration 108, loss = 0.01436189
Iteration 109, loss = 0.01416237
Iteration 110, loss = 0.01396661
Iteration 111, loss = 0.01377485
Iteration 112, loss = 0.01359987
Iteration 113, loss = 0.01340208
Iteration 114, loss = 0.01323196
Iteration 115, loss = 0.01309190
Iteration 116, loss = 0.01291291
Iteration 117, loss = 0.01275416
Iteration 118, loss = 0.01259410
Iteration 119, loss = 0.01241655
Iteration 120, loss = 0.01226248
Iteration 121, loss = 0.01211636
Iteration 122, loss = 0.01197326
Iteration 123, loss = 0.01183672
Iteration 124, loss = 0.01169781
Iteration 125, loss = 0.01158956
Iteration 126, loss = 0.01145567
Iteration 127, loss = 0.01131324
Iteration 128, loss = 0.01119663
Iteration 129, loss = 0.01107430
Iteration 130, loss = 0.01096297
Iteration 131, loss = 0.01085341
Iteration 132, loss = 0.01076832
Iteration 133, loss = 0.01066192
Iteration 134, loss = 0.01054304
Iteration 135, loss = 0.01044703
Iteration 136, loss = 0.01034200
Iteration 137, loss = 0.01025051
Iteration 138, loss = 0.01014329
Iteration 139, loss = 0.01004635
Iteration 140, loss = 0.00996403
Iteration 141, loss = 0.00986107
Iteration 142, loss = 0.00977750
Iteration 143, loss = 0.00970351
Iteration 144, loss = 0.00962365
Iteration 145, loss = 0.00951631
Iteration 146, loss = 0.00945317
Iteration 147, loss = 0.00939363
Iteration 148, loss = 0.00929904
Iteration 149, loss = 0.00920004
Iteration 150, loss = 0.00912860
Iteration 151, loss = 0.00905027
Iteration 152, loss = 0.00897195
Iteration 153, loss = 0.00890031
Iteration 154, loss = 0.00885243
Iteration 155, loss = 0.00876875
Iteration 156, loss = 0.00870376
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73618818
Iteration 2, loss = 0.71962847
Iteration 3, loss = 0.70591695
Iteration 4, loss = 0.69252550
Iteration 5, loss = 0.67898086
Iteration 6, loss = 0.66429440
Iteration 7, loss = 0.64894990
Iteration 8, loss = 0.63228194
Iteration 9, loss = 0.61451049
Iteration 10, loss = 0.59551240
Iteration 11, loss = 0.57577326
Iteration 12, loss = 0.55473947
Iteration 13, loss = 0.53289753
Iteration 14, loss = 0.51052001
Iteration 15, loss = 0.48750434
Iteration 16, loss = 0.46422439
Iteration 17, loss = 0.44091467
Iteration 18, loss = 0.41749095
Iteration 19, loss = 0.39447884
Iteration 20, loss = 0.37182937
Iteration 21, loss = 0.34983785
Iteration 22, loss = 0.32875442
Iteration 23, loss = 0.30859560
Iteration 24, loss = 0.28950186
Iteration 25, loss = 0.27151676
Iteration 26, loss = 0.25457228
Iteration 27, loss = 0.23864133
Iteration 28, loss = 0.22385990
Iteration 29, loss = 0.21007639
Iteration 30, loss = 0.19718523
Iteration 31, loss = 0.18527170
Iteration 32, loss = 0.17418564
Iteration 33, loss = 0.16386549
Iteration 34, loss = 0.15439684
Iteration 35, loss = 0.14555863
Iteration 36, loss = 0.13736375
Iteration 37, loss = 0.12978511
Iteration 38, loss = 0.12274863
Iteration 39, loss = 0.11625968
Iteration 40, loss = 0.11022047
Iteration 41, loss = 0.10459358
Iteration 42, loss = 0.09940040
Iteration 43, loss = 0.09461414
Iteration 44, loss = 0.09008840
Iteration 45, loss = 0.08589767
Iteration 46, loss = 0.08203159
Iteration 47, loss = 0.07840672
Iteration 48, loss = 0.07499798
Iteration 49, loss = 0.07185344
Iteration 50, loss = 0.06888299
Iteration 51, loss = 0.06610490
Iteration 52, loss = 0.06351016
Iteration 53, loss = 0.06105687
Iteration 54, loss = 0.05874537
Iteration 55, loss = 0.05658553
Iteration 56, loss = 0.05452488
Iteration 57, loss = 0.05265434
Iteration 58, loss = 0.05082670
Iteration 59, loss = 0.04908862
Iteration 60, loss = 0.04744547
Iteration 61, loss = 0.04589865
Iteration 62, loss = 0.04439903
Iteration 63, loss = 0.04302857
Iteration 64, loss = 0.04170104
Iteration 65, loss = 0.04045744
Iteration 66, loss = 0.03923420
Iteration 67, loss = 0.03809959
Iteration 68, loss = 0.03704460
Iteration 69, loss = 0.03599641
Iteration 70, loss = 0.03502391
Iteration 71, loss = 0.03409245
Iteration 72, loss = 0.03322428
Iteration 73, loss = 0.03236691
Iteration 74, loss = 0.03155850
Iteration 75, loss = 0.03077461
Iteration 76, loss = 0.03003049
Iteration 77, loss = 0.02929269
Iteration 78, loss = 0.02862311
Iteration 79, loss = 0.02793204
Iteration 80, loss = 0.02730828
Iteration 81, loss = 0.02666912
Iteration 82, loss = 0.02609527
Iteration 83, loss = 0.02551189
Iteration 84, loss = 0.02497645
Iteration 85, loss = 0.02445751
Iteration 86, loss = 0.02395067
Iteration 87, loss = 0.02344981
Iteration 88, loss = 0.02298965
Iteration 89, loss = 0.02254251
Iteration 90, loss = 0.02212328
Iteration 91, loss = 0.02167703
Iteration 92, loss = 0.02127350
Iteration 93, loss = 0.02088445
Iteration 94, loss = 0.02051102
Iteration 95, loss = 0.02011617
Iteration 96, loss = 0.01976799
Iteration 97, loss = 0.01941259
Iteration 98, loss = 0.01908109
Iteration 99, loss = 0.01877423
Iteration 100, loss = 0.01845418
Iteration 101, loss = 0.01815372
Iteration 102, loss = 0.01786150
Iteration 103, loss = 0.01758244
Iteration 104, loss = 0.01730780
Iteration 105, loss = 0.01704977
Iteration 106, loss = 0.01676945
Iteration 107, loss = 0.01650517
Iteration 108, loss = 0.01625801
Iteration 109, loss = 0.01601213
Iteration 110, loss = 0.01577986
Iteration 111, loss = 0.01558519
Iteration 112, loss = 0.01536121
Iteration 113, loss = 0.01515063
Iteration 114, loss = 0.01492689
Iteration 115, loss = 0.01477813
Iteration 116, loss = 0.01454786
Iteration 117, loss = 0.01435550
Iteration 118, loss = 0.01417477
Iteration 119, loss = 0.01397842
Iteration 120, loss = 0.01380329
Iteration 121, loss = 0.01364856
Iteration 122, loss = 0.01345902
Iteration 123, loss = 0.01328603
Iteration 124, loss = 0.01313315
Iteration 125, loss = 0.01298808
Iteration 126, loss = 0.01283644
Iteration 127, loss = 0.01268867
Iteration 128, loss = 0.01256592
Iteration 129, loss = 0.01241945
Iteration 130, loss = 0.01226581
Iteration 131, loss = 0.01214871
Iteration 132, loss = 0.01203496
Iteration 133, loss = 0.01191119
Iteration 134, loss = 0.01182277
Iteration 135, loss = 0.01169759
Iteration 136, loss = 0.01155916
Iteration 137, loss = 0.01142653
Iteration 138, loss = 0.01132404
Iteration 139, loss = 0.01124033
Iteration 140, loss = 0.01111668
Iteration 141, loss = 0.01098522
Iteration 142, loss = 0.01093331
Iteration 143, loss = 0.01077071
Iteration 144, loss = 0.01068770
Iteration 145, loss = 0.01055626
Iteration 146, loss = 0.01045914
Iteration 147, loss = 0.01042296
Iteration 148, loss = 0.01027653
Iteration 149, loss = 0.01018608
Iteration 150, loss = 0.01009783
Iteration 151, loss = 0.01002133
Iteration 152, loss = 0.00992353
Iteration 153, loss = 0.00984868
Iteration 154, loss = 0.00979742
Iteration 155, loss = 0.00969949
Iteration 156, loss = 0.00962978
Iteration 157, loss = 0.00958129
Iteration 158, loss = 0.00949273
Iteration 159, loss = 0.00942754
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67763434
Iteration 2, loss = 0.65365539
Iteration 3, loss = 0.62599874
Iteration 4, loss = 0.59053597
Iteration 5, loss = 0.54870613
Iteration 6, loss = 0.50364505
Iteration 7, loss = 0.45794481
Iteration 8, loss = 0.41318583
Iteration 9, loss = 0.37018677
Iteration 10, loss = 0.32996803
Iteration 11, loss = 0.29282757
Iteration 12, loss = 0.25921380
Iteration 13, loss = 0.22928326
Iteration 14, loss = 0.20289605
Iteration 15, loss = 0.18041198
Iteration 16, loss = 0.16065026
Iteration 17, loss = 0.14396884
Iteration 18, loss = 0.12943623
Iteration 19, loss = 0.11721691
Iteration 20, loss = 0.10645993
Iteration 21, loss = 0.09733361
Iteration 22, loss = 0.08933696
Iteration 23, loss = 0.08230580
Iteration 24, loss = 0.07623364
Iteration 25, loss = 0.07092466
Iteration 26, loss = 0.06620808
Iteration 27, loss = 0.06197738
Iteration 28, loss = 0.05831554
Iteration 29, loss = 0.05508537
Iteration 30, loss = 0.05214377
Iteration 31, loss = 0.04944004
Iteration 32, loss = 0.04702402
Iteration 33, loss = 0.04485564
Iteration 34, loss = 0.04298365
Iteration 35, loss = 0.04126616
Iteration 36, loss = 0.03950056
Iteration 37, loss = 0.03801483
Iteration 38, loss = 0.03667719
Iteration 39, loss = 0.03532592
Iteration 40, loss = 0.03416202
Iteration 41, loss = 0.03323713
Iteration 42, loss = 0.03217565
Iteration 43, loss = 0.03121160
Iteration 44, loss = 0.03042471
Iteration 45, loss = 0.02953717
Iteration 46, loss = 0.02888233
Iteration 47, loss = 0.02810982
Iteration 48, loss = 0.02752449
Iteration 49, loss = 0.02703121
Iteration 50, loss = 0.02647729
Iteration 51, loss = 0.02596911
Iteration 52, loss = 0.02555893
Iteration 53, loss = 0.02502634
Iteration 54, loss = 0.02441404
Iteration 55, loss = 0.02397817
Iteration 56, loss = 0.02349967
Iteration 57, loss = 0.02318451
Iteration 58, loss = 0.02276195
Iteration 59, loss = 0.02245354
Iteration 60, loss = 0.02211477
Iteration 61, loss = 0.02179328
Iteration 62, loss = 0.02160856
Iteration 63, loss = 0.02134207
Iteration 64, loss = 0.02105122
Iteration 65, loss = 0.02090434
Iteration 66, loss = 0.02066576
Iteration 67, loss = 0.02043444
Iteration 68, loss = 0.02017951
Iteration 69, loss = 0.01998681
Iteration 70, loss = 0.01980750
Iteration 71, loss = 0.01963066
Iteration 72, loss = 0.01958036
Iteration 73, loss = 0.01937286
Iteration 74, loss = 0.01916461
Iteration 75, loss = 0.01904003
Iteration 76, loss = 0.01874498
Iteration 77, loss = 0.01861586
Iteration 78, loss = 0.01843319
Iteration 79, loss = 0.01832310
Iteration 80, loss = 0.01821017
Iteration 81, loss = 0.01797525
Iteration 82, loss = 0.01789507
Iteration 83, loss = 0.01774866
Iteration 84, loss = 0.01766592
Iteration 85, loss = 0.01761053
Iteration 86, loss = 0.01768467
Iteration 87, loss = 0.01750124
Iteration 88, loss = 0.01754361
Iteration 89, loss = 0.01763384
Iteration 90, loss = 0.01738377
Iteration 91, loss = 0.01719938
Iteration 92, loss = 0.01701935
Iteration 93, loss = 0.01696013
Iteration 94, loss = 0.01718321
Iteration 95, loss = 0.01718591
Iteration 96, loss = 0.01705682
Iteration 97, loss = 0.01690705
Iteration 98, loss = 0.01675749
Iteration 99, loss = 0.01648954
Iteration 100, loss = 0.01658972
Iteration 101, loss = 0.01656903
Iteration 102, loss = 0.01661856
Iteration 103, loss = 0.01639681
Iteration 104, loss = 0.01629893
Iteration 105, loss = 0.01614448
Iteration 106, loss = 0.01620327
Iteration 107, loss = 0.01612133
Iteration 108, loss = 0.01605535
Iteration 109, loss = 0.01603440
Iteration 110, loss = 0.01592375
Iteration 111, loss = 0.01580261
Iteration 112, loss = 0.01590454
Iteration 113, loss = 0.01588039
Iteration 114, loss = 0.01576700
Iteration 115, loss = 0.01568946
Iteration 116, loss = 0.01604667
Iteration 117, loss = 0.01636578
Iteration 118, loss = 0.01639482
Iteration 119, loss = 0.01617194
Iteration 120, loss = 0.01596347
Iteration 121, loss = 0.01568778
Iteration 122, loss = 0.01552328
Iteration 123, loss = 0.01541568
Iteration 124, loss = 0.01536271
Iteration 125, loss = 0.01532004
Iteration 126, loss = 0.01523406
Iteration 127, loss = 0.01519552
Iteration 128, loss = 0.01530386
Iteration 129, loss = 0.01520413
Iteration 130, loss = 0.01522590
Iteration 131, loss = 0.01514698
Iteration 132, loss = 0.01509038
Iteration 133, loss = 0.01506502
Iteration 134, loss = 0.01500278
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70831060
Iteration 2, loss = 0.68945652
Iteration 3, loss = 0.67005514
Iteration 4, loss = 0.64797214
Iteration 5, loss = 0.62195880
Iteration 6, loss = 0.59179084
Iteration 7, loss = 0.55878533
Iteration 8, loss = 0.52299776
Iteration 9, loss = 0.48610173
Iteration 10, loss = 0.44872108
Iteration 11, loss = 0.41172784
Iteration 12, loss = 0.37594159
Iteration 13, loss = 0.34227454
Iteration 14, loss = 0.31098254
Iteration 15, loss = 0.28225179
Iteration 16, loss = 0.25610856
Iteration 17, loss = 0.23256477
Iteration 18, loss = 0.21138163
Iteration 19, loss = 0.19244804
Iteration 20, loss = 0.17551032
Iteration 21, loss = 0.16054312
Iteration 22, loss = 0.14722702
Iteration 23, loss = 0.13540722
Iteration 24, loss = 0.12487634
Iteration 25, loss = 0.11540254
Iteration 26, loss = 0.10713368
Iteration 27, loss = 0.09966642
Iteration 28, loss = 0.09299466
Iteration 29, loss = 0.08694782
Iteration 30, loss = 0.08155963
Iteration 31, loss = 0.07670143
Iteration 32, loss = 0.07235658
Iteration 33, loss = 0.06832347
Iteration 34, loss = 0.06469470
Iteration 35, loss = 0.06135821
Iteration 36, loss = 0.05838154
Iteration 37, loss = 0.05562860
Iteration 38, loss = 0.05305964
Iteration 39, loss = 0.05077453
Iteration 40, loss = 0.04858137
Iteration 41, loss = 0.04657904
Iteration 42, loss = 0.04477130
Iteration 43, loss = 0.04308803
Iteration 44, loss = 0.04149719
Iteration 45, loss = 0.04002980
Iteration 46, loss = 0.03867708
Iteration 47, loss = 0.03736245
Iteration 48, loss = 0.03618218
Iteration 49, loss = 0.03507096
Iteration 50, loss = 0.03403189
Iteration 51, loss = 0.03305691
Iteration 52, loss = 0.03210486
Iteration 53, loss = 0.03128101
Iteration 54, loss = 0.03043283
Iteration 55, loss = 0.02965973
Iteration 56, loss = 0.02892126
Iteration 57, loss = 0.02822045
Iteration 58, loss = 0.02761375
Iteration 59, loss = 0.02704989
Iteration 60, loss = 0.02639217
Iteration 61, loss = 0.02584121
Iteration 62, loss = 0.02538450
Iteration 63, loss = 0.02479902
Iteration 64, loss = 0.02434898
Iteration 65, loss = 0.02389556
Iteration 66, loss = 0.02345990
Iteration 67, loss = 0.02303722
Iteration 68, loss = 0.02265882
Iteration 69, loss = 0.02225138
Iteration 70, loss = 0.02193172
Iteration 71, loss = 0.02160016
Iteration 72, loss = 0.02127496
Iteration 73, loss = 0.02101152
Iteration 74, loss = 0.02064570
Iteration 75, loss = 0.02037229
Iteration 76, loss = 0.02008464
Iteration 77, loss = 0.01984152
Iteration 78, loss = 0.01958990
Iteration 79, loss = 0.01932553
Iteration 80, loss = 0.01911128
Iteration 81, loss = 0.01886194
Iteration 82, loss = 0.01863863
Iteration 83, loss = 0.01844016
Iteration 84, loss = 0.01827656
Iteration 85, loss = 0.01810676
Iteration 86, loss = 0.01787031
Iteration 87, loss = 0.01772568
Iteration 88, loss = 0.01750036
Iteration 89, loss = 0.01735787
Iteration 90, loss = 0.01717403
Iteration 91, loss = 0.01705305
Iteration 92, loss = 0.01689602
Iteration 93, loss = 0.01678293
Iteration 94, loss = 0.01661538
Iteration 95, loss = 0.01647723
Iteration 96, loss = 0.01636266
Iteration 97, loss = 0.01622749
Iteration 98, loss = 0.01609208
Iteration 99, loss = 0.01597791
Iteration 100, loss = 0.01588261
Iteration 101, loss = 0.01571275
Iteration 102, loss = 0.01566749
Iteration 103, loss = 0.01554365
Iteration 104, loss = 0.01542716
Iteration 105, loss = 0.01530561
Iteration 106, loss = 0.01524474
Iteration 107, loss = 0.01515174
Iteration 108, loss = 0.01502794
Iteration 109, loss = 0.01496016
Iteration 110, loss = 0.01490999
Iteration 111, loss = 0.01482099
Iteration 112, loss = 0.01470634
Iteration 113, loss = 0.01464345
Iteration 114, loss = 0.01460728
Iteration 115, loss = 0.01454449
Iteration 116, loss = 0.01441688
Iteration 117, loss = 0.01437500
Iteration 118, loss = 0.01431014
Iteration 119, loss = 0.01421323
Iteration 120, loss = 0.01416180
Iteration 121, loss = 0.01410932
Iteration 122, loss = 0.01407401
Iteration 123, loss = 0.01399115
Iteration 124, loss = 0.01388564
Iteration 125, loss = 0.01386383
Iteration 126, loss = 0.01379343
Iteration 127, loss = 0.01376225
Iteration 128, loss = 0.01367508
Iteration 129, loss = 0.01366780
Iteration 130, loss = 0.01358004
Iteration 131, loss = 0.01352156
Iteration 132, loss = 0.01350992
Iteration 133, loss = 0.01341031
Iteration 134, loss = 0.01343208
Iteration 135, loss = 0.01332410
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67704858
Iteration 2, loss = 0.63648213
Iteration 3, loss = 0.57539768
Iteration 4, loss = 0.49884862
Iteration 5, loss = 0.41992562
Iteration 6, loss = 0.34625974
Iteration 7, loss = 0.28326004
Iteration 8, loss = 0.23091420
Iteration 9, loss = 0.18989783
Iteration 10, loss = 0.15757994
Iteration 11, loss = 0.13277085
Iteration 12, loss = 0.11361635
Iteration 13, loss = 0.09860389
Iteration 14, loss = 0.08666745
Iteration 15, loss = 0.07717575
Iteration 16, loss = 0.06929853
Iteration 17, loss = 0.06304721
Iteration 18, loss = 0.05760638
Iteration 19, loss = 0.05307871
Iteration 20, loss = 0.04923530
Iteration 21, loss = 0.04583827
Iteration 22, loss = 0.04325947
Iteration 23, loss = 0.04075573
Iteration 24, loss = 0.03871740
Iteration 25, loss = 0.03676929
Iteration 26, loss = 0.03517788
Iteration 27, loss = 0.03387201
Iteration 28, loss = 0.03257150
Iteration 29, loss = 0.03117138
Iteration 30, loss = 0.03049274
Iteration 31, loss = 0.02941925
Iteration 32, loss = 0.02845362
Iteration 33, loss = 0.02780935
Iteration 34, loss = 0.02710672
Iteration 35, loss = 0.02626284
Iteration 36, loss = 0.02591486
Iteration 37, loss = 0.02542164
Iteration 38, loss = 0.02491078
Iteration 39, loss = 0.02444925
Iteration 40, loss = 0.02398757
Iteration 41, loss = 0.02358401
Iteration 42, loss = 0.02325136
Iteration 43, loss = 0.02310822
Iteration 44, loss = 0.02256769
Iteration 45, loss = 0.02231122
Iteration 46, loss = 0.02208032
Iteration 47, loss = 0.02170902
Iteration 48, loss = 0.02168803
Iteration 49, loss = 0.02128559
Iteration 50, loss = 0.02111020
Iteration 51, loss = 0.02098885
Iteration 52, loss = 0.02068941
Iteration 53, loss = 0.02056998
Iteration 54, loss = 0.02041831
Iteration 55, loss = 0.02040644
Iteration 56, loss = 0.02009319
Iteration 57, loss = 0.02009156
Iteration 58, loss = 0.01994829
Iteration 59, loss = 0.01990047
Iteration 60, loss = 0.01946916
Iteration 61, loss = 0.01957066
Iteration 62, loss = 0.01932666
Iteration 63, loss = 0.01928708
Iteration 64, loss = 0.01918073
Iteration 65, loss = 0.01917385
Iteration 66, loss = 0.01900428
Iteration 67, loss = 0.01913304
Iteration 68, loss = 0.01888153
Iteration 69, loss = 0.01895907
Iteration 70, loss = 0.01886229
Iteration 71, loss = 0.01881234
Iteration 72, loss = 0.01870240
Iteration 73, loss = 0.01860670
Iteration 74, loss = 0.01849148
Iteration 75, loss = 0.01837341
Iteration 76, loss = 0.01847605
Iteration 77, loss = 0.01842684
Iteration 78, loss = 0.01831173
Iteration 79, loss = 0.01822398
Iteration 80, loss = 0.01819583
Iteration 81, loss = 0.01826109
Iteration 82, loss = 0.01822390
Iteration 83, loss = 0.01803528
Iteration 84, loss = 0.01801256
Iteration 85, loss = 0.01796652
Iteration 86, loss = 0.01805757
Iteration 87, loss = 0.01789577
Iteration 88, loss = 0.01786021
Iteration 89, loss = 0.01790999
Iteration 90, loss = 0.01781599
Iteration 91, loss = 0.01780583
Iteration 92, loss = 0.01775872
Iteration 93, loss = 0.01771399
Iteration 94, loss = 0.01760139
Iteration 95, loss = 0.01767623
Iteration 96, loss = 0.01782801
Iteration 97, loss = 0.01770690
Iteration 98, loss = 0.01791717
Iteration 99, loss = 0.01744547
Iteration 100, loss = 0.01775580
Iteration 101, loss = 0.01759886
Iteration 102, loss = 0.01752184
Iteration 103, loss = 0.01760075
Iteration 104, loss = 0.01750939
Iteration 105, loss = 0.01752969
Iteration 106, loss = 0.01752721
Iteration 107, loss = 0.01754750
Iteration 108, loss = 0.01761101
Iteration 109, loss = 0.01722000
Iteration 110, loss = 0.01742015
Iteration 111, loss = 0.01756932
Iteration 112, loss = 0.01744751
Iteration 113, loss = 0.01746904
Iteration 114, loss = 0.01727272
Iteration 115, loss = 0.01786582
Iteration 116, loss = 0.01791670
Iteration 117, loss = 0.01750389
Iteration 118, loss = 0.01735600
Iteration 119, loss = 0.01735559
Iteration 120, loss = 0.01732283
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67991437
Iteration 2, loss = 0.61570534
Iteration 3, loss = 0.51822778
Iteration 4, loss = 0.40891405
Iteration 5, loss = 0.31088945
Iteration 6, loss = 0.23519420
Iteration 7, loss = 0.18029306
Iteration 8, loss = 0.14191680
Iteration 9, loss = 0.11486454
Iteration 10, loss = 0.09582955
Iteration 11, loss = 0.08178712
Iteration 12, loss = 0.07098769
Iteration 13, loss = 0.06259103
Iteration 14, loss = 0.05634290
Iteration 15, loss = 0.05122051
Iteration 16, loss = 0.04714201
Iteration 17, loss = 0.04361199
Iteration 18, loss = 0.04125217
Iteration 19, loss = 0.03842218
Iteration 20, loss = 0.03641145
Iteration 21, loss = 0.03451466
Iteration 22, loss = 0.03292817
Iteration 23, loss = 0.03207982
Iteration 24, loss = 0.03061008
Iteration 25, loss = 0.02977475
Iteration 26, loss = 0.02867306
Iteration 27, loss = 0.02802963
Iteration 28, loss = 0.02713525
Iteration 29, loss = 0.02658725
Iteration 30, loss = 0.02613047
Iteration 31, loss = 0.02563019
Iteration 32, loss = 0.02517267
Iteration 33, loss = 0.02490504
Iteration 34, loss = 0.02421712
Iteration 35, loss = 0.02406768
Iteration 36, loss = 0.02385581
Iteration 37, loss = 0.02368914
Iteration 38, loss = 0.02342795
Iteration 39, loss = 0.02280901
Iteration 40, loss = 0.02262018
Iteration 41, loss = 0.02230271
Iteration 42, loss = 0.02234438
Iteration 43, loss = 0.02216250
Iteration 44, loss = 0.02178949
Iteration 45, loss = 0.02185924
Iteration 46, loss = 0.02157182
Iteration 47, loss = 0.02139286
Iteration 48, loss = 0.02133173
Iteration 49, loss = 0.02112549
Iteration 50, loss = 0.02112352
Iteration 51, loss = 0.02090674
Iteration 52, loss = 0.02065615
Iteration 53, loss = 0.02078565
Iteration 54, loss = 0.02086416
Iteration 55, loss = 0.02058017
Iteration 56, loss = 0.02039189
Iteration 57, loss = 0.02044515
Iteration 58, loss = 0.02003021
Iteration 59, loss = 0.02006367
Iteration 60, loss = 0.02026493
Iteration 61, loss = 0.02003455
Iteration 62, loss = 0.02005438
Iteration 63, loss = 0.01987892
Iteration 64, loss = 0.02028285
Iteration 65, loss = 0.01996366
Iteration 66, loss = 0.01995053
Iteration 67, loss = 0.01982325
Iteration 68, loss = 0.01967184
Iteration 69, loss = 0.01969151
Iteration 70, loss = 0.01966998
Iteration 71, loss = 0.01955265
Iteration 72, loss = 0.01980611
Iteration 73, loss = 0.01957504
Iteration 74, loss = 0.01949741
Iteration 75, loss = 0.01929348
Iteration 76, loss = 0.01943140
Iteration 77, loss = 0.01934090
Iteration 78, loss = 0.01932152
Iteration 79, loss = 0.01931420
Iteration 80, loss = 0.01940317
Iteration 81, loss = 0.01934786
Iteration 82, loss = 0.01931508
Iteration 83, loss = 0.01908762
Iteration 84, loss = 0.01922011
Iteration 85, loss = 0.01935529
Iteration 86, loss = 0.01943120
Iteration 87, loss = 0.01927262
Iteration 88, loss = 0.01905726
Iteration 89, loss = 0.01928111
Iteration 90, loss = 0.01936203
Iteration 91, loss = 0.01922113
Iteration 92, loss = 0.01913915
Iteration 93, loss = 0.01913998
Iteration 94, loss = 0.01941708
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71523317
Iteration 2, loss = 0.67898767
Iteration 3, loss = 0.62717026
Iteration 4, loss = 0.56380337
Iteration 5, loss = 0.49185045
Iteration 6, loss = 0.41735334
Iteration 7, loss = 0.34920066
Iteration 8, loss = 0.28994068
Iteration 9, loss = 0.24053759
Iteration 10, loss = 0.20060608
Iteration 11, loss = 0.16834602
Iteration 12, loss = 0.14317935
Iteration 13, loss = 0.12291295
Iteration 14, loss = 0.10677270
Iteration 15, loss = 0.09385078
Iteration 16, loss = 0.08324217
Iteration 17, loss = 0.07469671
Iteration 18, loss = 0.06740322
Iteration 19, loss = 0.06155780
Iteration 20, loss = 0.05637925
Iteration 21, loss = 0.05208727
Iteration 22, loss = 0.04847034
Iteration 23, loss = 0.04521148
Iteration 24, loss = 0.04259721
Iteration 25, loss = 0.04010289
Iteration 26, loss = 0.03793154
Iteration 27, loss = 0.03610796
Iteration 28, loss = 0.03449364
Iteration 29, loss = 0.03305582
Iteration 30, loss = 0.03167041
Iteration 31, loss = 0.03038906
Iteration 32, loss = 0.02922448
Iteration 33, loss = 0.02838764
Iteration 34, loss = 0.02752644
Iteration 35, loss = 0.02679289
Iteration 36, loss = 0.02594690
Iteration 37, loss = 0.02528643
Iteration 38, loss = 0.02462138
Iteration 39, loss = 0.02411094
Iteration 40, loss = 0.02359411
Iteration 41, loss = 0.02303067
Iteration 42, loss = 0.02260986
Iteration 43, loss = 0.02236052
Iteration 44, loss = 0.02191268
Iteration 45, loss = 0.02158532
Iteration 46, loss = 0.02123491
Iteration 47, loss = 0.02074116
Iteration 48, loss = 0.02051373
Iteration 49, loss = 0.02022870
Iteration 50, loss = 0.01991035
Iteration 51, loss = 0.01981063
Iteration 52, loss = 0.01942668
Iteration 53, loss = 0.01927585
Iteration 54, loss = 0.01905532
Iteration 55, loss = 0.01901931
Iteration 56, loss = 0.01862114
Iteration 57, loss = 0.01852711
Iteration 58, loss = 0.01833164
Iteration 59, loss = 0.01831501
Iteration 60, loss = 0.01811984
Iteration 61, loss = 0.01801590
Iteration 62, loss = 0.01773837
Iteration 63, loss = 0.01772151
Iteration 64, loss = 0.01765445
Iteration 65, loss = 0.01752714
Iteration 66, loss = 0.01746829
Iteration 67, loss = 0.01731621
Iteration 68, loss = 0.01721163
Iteration 69, loss = 0.01716173
Iteration 70, loss = 0.01696385
Iteration 71, loss = 0.01694956
Iteration 72, loss = 0.01690642
Iteration 73, loss = 0.01669743
Iteration 74, loss = 0.01688081
Iteration 75, loss = 0.01656677
Iteration 76, loss = 0.01647090
Iteration 77, loss = 0.01643057
Iteration 78, loss = 0.01649407
Iteration 79, loss = 0.01639617
Iteration 80, loss = 0.01629822
Iteration 81, loss = 0.01623686
Iteration 82, loss = 0.01615002
Iteration 83, loss = 0.01620326
Iteration 84, loss = 0.01610490
Iteration 85, loss = 0.01613330
Iteration 86, loss = 0.01595151
Iteration 87, loss = 0.01598283
Iteration 88, loss = 0.01593982
Iteration 89, loss = 0.01585746
Iteration 90, loss = 0.01574376
Iteration 91, loss = 0.01566473
Iteration 92, loss = 0.01574818
Iteration 93, loss = 0.01572182
Iteration 94, loss = 0.01561984
Iteration 95, loss = 0.01579923
Iteration 96, loss = 0.01558065
Iteration 97, loss = 0.01555827
Iteration 98, loss = 0.01545856
Iteration 99, loss = 0.01563904
Iteration 100, loss = 0.01538084
Iteration 101, loss = 0.01537412
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67624097
Iteration 2, loss = 0.62270515
Iteration 3, loss = 0.54567877
Iteration 4, loss = 0.45672734
Iteration 5, loss = 0.36906389
Iteration 6, loss = 0.29222396
Iteration 7, loss = 0.23095077
Iteration 8, loss = 0.18386166
Iteration 9, loss = 0.14915572
Iteration 10, loss = 0.12356038
Iteration 11, loss = 0.10457575
Iteration 12, loss = 0.09003048
Iteration 13, loss = 0.07904249
Iteration 14, loss = 0.07052042
Iteration 15, loss = 0.06364388
Iteration 16, loss = 0.05782333
Iteration 17, loss = 0.05342353
Iteration 18, loss = 0.04954404
Iteration 19, loss = 0.04640389
Iteration 20, loss = 0.04358635
Iteration 21, loss = 0.04135647
Iteration 22, loss = 0.03927432
Iteration 23, loss = 0.03754494
Iteration 24, loss = 0.03594863
Iteration 25, loss = 0.03540028
Iteration 26, loss = 0.03432345
Iteration 27, loss = 0.03308250
Iteration 28, loss = 0.03219436
Iteration 29, loss = 0.03122574
Iteration 30, loss = 0.03052513
Iteration 31, loss = 0.02944811
Iteration 32, loss = 0.02907146
Iteration 33, loss = 0.02857354
Iteration 34, loss = 0.02789684
Iteration 35, loss = 0.02767043
Iteration 36, loss = 0.02713646
Iteration 37, loss = 0.02668808
Iteration 38, loss = 0.02670568
Iteration 39, loss = 0.02632854
Iteration 40, loss = 0.02594782
Iteration 41, loss = 0.02543539
Iteration 42, loss = 0.02512603
Iteration 43, loss = 0.02501339
Iteration 44, loss = 0.02478021
Iteration 45, loss = 0.02439586
Iteration 46, loss = 0.02426935
Iteration 47, loss = 0.02382848
Iteration 48, loss = 0.02412040
Iteration 49, loss = 0.02381558
Iteration 50, loss = 0.02350558
Iteration 51, loss = 0.02346905
Iteration 52, loss = 0.02310322
Iteration 53, loss = 0.02321706
Iteration 54, loss = 0.02308620
Iteration 55, loss = 0.02298881
Iteration 56, loss = 0.02286305
Iteration 57, loss = 0.02268959
Iteration 58, loss = 0.02252462
Iteration 59, loss = 0.02296002
Iteration 60, loss = 0.02304000
Iteration 61, loss = 0.02267221
Iteration 62, loss = 0.02243366
Iteration 63, loss = 0.02218292
Iteration 64, loss = 0.02221143
Iteration 65, loss = 0.02198741
Iteration 66, loss = 0.02207378
Iteration 67, loss = 0.02178659
Iteration 68, loss = 0.02178272
Iteration 69, loss = 0.02183894
Iteration 70, loss = 0.02154842
Iteration 71, loss = 0.02161941
Iteration 72, loss = 0.02226193
Iteration 73, loss = 0.02256347
Iteration 74, loss = 0.02219593
Iteration 75, loss = 0.02197134
Iteration 76, loss = 0.02169786
Iteration 77, loss = 0.02187838
Iteration 78, loss = 0.02162791
Iteration 79, loss = 0.02190392
Iteration 80, loss = 0.02191452
Iteration 81, loss = 0.02159008
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68359338
Iteration 2, loss = 0.60702714
Iteration 3, loss = 0.49582773
Iteration 4, loss = 0.38327770
Iteration 5, loss = 0.28806132
Iteration 6, loss = 0.21679715
Iteration 7, loss = 0.16649194
Iteration 8, loss = 0.13191588
Iteration 9, loss = 0.10787687
Iteration 10, loss = 0.09083743
Iteration 11, loss = 0.07812737
Iteration 12, loss = 0.06867409
Iteration 13, loss = 0.06133743
Iteration 14, loss = 0.05565338
Iteration 15, loss = 0.05138922
Iteration 16, loss = 0.04733456
Iteration 17, loss = 0.04430813
Iteration 18, loss = 0.04221805
Iteration 19, loss = 0.03979677
Iteration 20, loss = 0.03792123
Iteration 21, loss = 0.03647952
Iteration 22, loss = 0.03537662
Iteration 23, loss = 0.03413184
Iteration 24, loss = 0.03302651
Iteration 25, loss = 0.03234297
Iteration 26, loss = 0.03140091
Iteration 27, loss = 0.03080501
Iteration 28, loss = 0.03038275
Iteration 29, loss = 0.02962193
Iteration 30, loss = 0.02924539
Iteration 31, loss = 0.02884642
Iteration 32, loss = 0.02833502
Iteration 33, loss = 0.02800257
Iteration 34, loss = 0.02761325
Iteration 35, loss = 0.02752216
Iteration 36, loss = 0.02726059
Iteration 37, loss = 0.02677221
Iteration 38, loss = 0.02662763
Iteration 39, loss = 0.02623170
Iteration 40, loss = 0.02637224
Iteration 41, loss = 0.02637732
Iteration 42, loss = 0.02619273
Iteration 43, loss = 0.02575831
Iteration 44, loss = 0.02546467
Iteration 45, loss = 0.02564156
Iteration 46, loss = 0.02530502
Iteration 47, loss = 0.02569480
Iteration 48, loss = 0.02528725
Iteration 49, loss = 0.02520862
Iteration 50, loss = 0.02488831
Iteration 51, loss = 0.02527608
Iteration 52, loss = 0.02495834
Iteration 53, loss = 0.02483808
Iteration 54, loss = 0.02451398
Iteration 55, loss = 0.02454234
Iteration 56, loss = 0.02431641
Iteration 57, loss = 0.02445213
Iteration 58, loss = 0.02447744
Iteration 59, loss = 0.02438983
Iteration 60, loss = 0.02462421
Iteration 61, loss = 0.02428509
Iteration 62, loss = 0.02404321
Iteration 63, loss = 0.02395720
Iteration 64, loss = 0.02398536
Iteration 65, loss = 0.02415047
Iteration 66, loss = 0.02391287
Iteration 67, loss = 0.02386936
Iteration 68, loss = 0.02406426
Iteration 69, loss = 0.02419156
Iteration 70, loss = 0.02407958
Iteration 71, loss = 0.02379886
Iteration 72, loss = 0.02368272
Iteration 73, loss = 0.02417386
Iteration 74, loss = 0.02374903
Iteration 75, loss = 0.02374972
Iteration 76, loss = 0.02410653
Iteration 77, loss = 0.02390678
Iteration 78, loss = 0.02394854
Iteration 79, loss = 0.02343387
Iteration 80, loss = 0.02390684
Iteration 81, loss = 0.02366119
Iteration 82, loss = 0.02379728
Iteration 83, loss = 0.02348665
Iteration 84, loss = 0.02353383
Iteration 85, loss = 0.02369906
Iteration 86, loss = 0.02369625
Iteration 87, loss = 0.02334266
Iteration 88, loss = 0.02344456
Iteration 89, loss = 0.02344673
Iteration 90, loss = 0.02364765
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68273549
Iteration 2, loss = 0.66360294
Iteration 3, loss = 0.64101447
Iteration 4, loss = 0.61115099
Iteration 5, loss = 0.57419454
Iteration 6, loss = 0.53156811
Iteration 7, loss = 0.48533167
Iteration 8, loss = 0.43780739
Iteration 9, loss = 0.39103633
Iteration 10, loss = 0.34684519
Iteration 11, loss = 0.30606215
Iteration 12, loss = 0.26934592
Iteration 13, loss = 0.23674538
Iteration 14, loss = 0.20833764
Iteration 15, loss = 0.18389198
Iteration 16, loss = 0.16289739
Iteration 17, loss = 0.14510075
Iteration 18, loss = 0.12968401
Iteration 19, loss = 0.11658611
Iteration 20, loss = 0.10539793
Iteration 21, loss = 0.09581493
Iteration 22, loss = 0.08749105
Iteration 23, loss = 0.08031267
Iteration 24, loss = 0.07410913
Iteration 25, loss = 0.06874084
Iteration 26, loss = 0.06401204
Iteration 27, loss = 0.05988461
Iteration 28, loss = 0.05613236
Iteration 29, loss = 0.05282394
Iteration 30, loss = 0.04996181
Iteration 31, loss = 0.04737418
Iteration 32, loss = 0.04491482
Iteration 33, loss = 0.04299137
Iteration 34, loss = 0.04104007
Iteration 35, loss = 0.03922972
Iteration 36, loss = 0.03768444
Iteration 37, loss = 0.03618366
Iteration 38, loss = 0.03485483
Iteration 39, loss = 0.03373452
Iteration 40, loss = 0.03256378
Iteration 41, loss = 0.03145951
Iteration 42, loss = 0.03044666
Iteration 43, loss = 0.02960738
Iteration 44, loss = 0.02870241
Iteration 45, loss = 0.02803045
Iteration 46, loss = 0.02751009
Iteration 47, loss = 0.02671070
Iteration 48, loss = 0.02599773
Iteration 49, loss = 0.02547331
Iteration 50, loss = 0.02488333
Iteration 51, loss = 0.02445898
Iteration 52, loss = 0.02399635
Iteration 53, loss = 0.02353023
Iteration 54, loss = 0.02301742
Iteration 55, loss = 0.02276721
Iteration 56, loss = 0.02220048
Iteration 57, loss = 0.02198218
Iteration 58, loss = 0.02161081
Iteration 59, loss = 0.02135313
Iteration 60, loss = 0.02116180
Iteration 61, loss = 0.02095572
Iteration 62, loss = 0.02068966
Iteration 63, loss = 0.02047728
Iteration 64, loss = 0.02023299
Iteration 65, loss = 0.02015164
Iteration 66, loss = 0.01989143
Iteration 67, loss = 0.01975072
Iteration 68, loss = 0.01935168
Iteration 69, loss = 0.01913504
Iteration 70, loss = 0.01889848
Iteration 71, loss = 0.01863073
Iteration 72, loss = 0.01846842
Iteration 73, loss = 0.01826301
Iteration 74, loss = 0.01807772
Iteration 75, loss = 0.01792338
Iteration 76, loss = 0.01779482
Iteration 77, loss = 0.01767576
Iteration 78, loss = 0.01748665
Iteration 79, loss = 0.01743144
Iteration 80, loss = 0.01725315
Iteration 81, loss = 0.01722846
Iteration 82, loss = 0.01712762
Iteration 83, loss = 0.01698043
Iteration 84, loss = 0.01693289
Iteration 85, loss = 0.01679396
Iteration 86, loss = 0.01675139
Iteration 87, loss = 0.01662033
Iteration 88, loss = 0.01652431
Iteration 89, loss = 0.01643075
Iteration 90, loss = 0.01666035
Iteration 91, loss = 0.01690913
Iteration 92, loss = 0.01659374
Iteration 93, loss = 0.01671301
Iteration 94, loss = 0.01642499
Iteration 95, loss = 0.01621866
Iteration 96, loss = 0.01603880
Iteration 97, loss = 0.01593384
Iteration 98, loss = 0.01582522
Iteration 99, loss = 0.01586042
Iteration 100, loss = 0.01570415
Iteration 101, loss = 0.01592505
Iteration 102, loss = 0.01601145
Iteration 103, loss = 0.01600745
Iteration 104, loss = 0.01588145
Iteration 105, loss = 0.01563193
Iteration 106, loss = 0.01549348
Iteration 107, loss = 0.01552791
Iteration 108, loss = 0.01555892
Iteration 109, loss = 0.01590687
Iteration 110, loss = 0.01575554
Iteration 111, loss = 0.01567626
Iteration 112, loss = 0.01548976
Iteration 113, loss = 0.01537974
Iteration 114, loss = 0.01523576
Iteration 115, loss = 0.01524424
Iteration 116, loss = 0.01526403
Iteration 117, loss = 0.01513610
Iteration 118, loss = 0.01505045
Iteration 119, loss = 0.01513198
Iteration 120, loss = 0.01496865
Iteration 121, loss = 0.01492010
Iteration 122, loss = 0.01495779
Iteration 123, loss = 0.01491920
Iteration 124, loss = 0.01488629
Iteration 125, loss = 0.01486156
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68187263
Iteration 2, loss = 0.61166127
Iteration 3, loss = 0.51377977
Iteration 4, loss = 0.40720310
Iteration 5, loss = 0.31362952
Iteration 6, loss = 0.23991126
Iteration 7, loss = 0.18595250
Iteration 8, loss = 0.14751867
Iteration 9, loss = 0.11982826
Iteration 10, loss = 0.10023051
Iteration 11, loss = 0.08586155
Iteration 12, loss = 0.07455348
Iteration 13, loss = 0.06634619
Iteration 14, loss = 0.05944052
Iteration 15, loss = 0.05413996
Iteration 16, loss = 0.04973279
Iteration 17, loss = 0.04608139
Iteration 18, loss = 0.04317895
Iteration 19, loss = 0.04067822
Iteration 20, loss = 0.03866341
Iteration 21, loss = 0.03641776
Iteration 22, loss = 0.03499037
Iteration 23, loss = 0.03346936
Iteration 24, loss = 0.03247273
Iteration 25, loss = 0.03143414
Iteration 26, loss = 0.03036646
Iteration 27, loss = 0.02958882
Iteration 28, loss = 0.02884036
Iteration 29, loss = 0.02812634
Iteration 30, loss = 0.02774284
Iteration 31, loss = 0.02688674
Iteration 32, loss = 0.02640575
Iteration 33, loss = 0.02622894
Iteration 34, loss = 0.02549531
Iteration 35, loss = 0.02529314
Iteration 36, loss = 0.02499480
Iteration 37, loss = 0.02476172
Iteration 38, loss = 0.02444930
Iteration 39, loss = 0.02416909
Iteration 40, loss = 0.02406726
Iteration 41, loss = 0.02384632
Iteration 42, loss = 0.02339988
Iteration 43, loss = 0.02307429
Iteration 44, loss = 0.02315588
Iteration 45, loss = 0.02295770
Iteration 46, loss = 0.02256826
Iteration 47, loss = 0.02262582
Iteration 48, loss = 0.02236859
Iteration 49, loss = 0.02250610
Iteration 50, loss = 0.02228393
Iteration 51, loss = 0.02226227
Iteration 52, loss = 0.02207077
Iteration 53, loss = 0.02192729
Iteration 54, loss = 0.02181278
Iteration 55, loss = 0.02189449
Iteration 56, loss = 0.02157169
Iteration 57, loss = 0.02160700
Iteration 58, loss = 0.02156301
Iteration 59, loss = 0.02149094
Iteration 60, loss = 0.02169210
Iteration 61, loss = 0.02111737
Iteration 62, loss = 0.02137840
Iteration 63, loss = 0.02171810
Iteration 64, loss = 0.02124550
Iteration 65, loss = 0.02108999
Iteration 66, loss = 0.02109024
Iteration 67, loss = 0.02086116
Iteration 68, loss = 0.02108243
Iteration 69, loss = 0.02117847
Iteration 70, loss = 0.02074015
Iteration 71, loss = 0.02086349
Iteration 72, loss = 0.02074799
Iteration 73, loss = 0.02095549
Iteration 74, loss = 0.02079254
Iteration 75, loss = 0.02077813
Iteration 76, loss = 0.02057197
Iteration 77, loss = 0.02042984
Iteration 78, loss = 0.02069719
Iteration 79, loss = 0.02052859
Iteration 80, loss = 0.02064691
Iteration 81, loss = 0.02039608
Iteration 82, loss = 0.02059135
Iteration 83, loss = 0.02040044
Iteration 84, loss = 0.02038698
Iteration 85, loss = 0.02035710
Iteration 86, loss = 0.02025912
Iteration 87, loss = 0.02054532
Iteration 88, loss = 0.02038830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67283099
Iteration 2, loss = 0.59458150
Iteration 3, loss = 0.48415763
Iteration 4, loss = 0.37233549
Iteration 5, loss = 0.27930575
Iteration 6, loss = 0.21020690
Iteration 7, loss = 0.16156299
Iteration 8, loss = 0.12783123
Iteration 9, loss = 0.10465074
Iteration 10, loss = 0.08775262
Iteration 11, loss = 0.07539429
Iteration 12, loss = 0.06603220
Iteration 13, loss = 0.05904516
Iteration 14, loss = 0.05337562
Iteration 15, loss = 0.04907786
Iteration 16, loss = 0.04504989
Iteration 17, loss = 0.04223584
Iteration 18, loss = 0.03972042
Iteration 19, loss = 0.03747769
Iteration 20, loss = 0.03587385
Iteration 21, loss = 0.03402661
Iteration 22, loss = 0.03284929
Iteration 23, loss = 0.03157716
Iteration 24, loss = 0.03064151
Iteration 25, loss = 0.02998329
Iteration 26, loss = 0.02924831
Iteration 27, loss = 0.02835227
Iteration 28, loss = 0.02808242
Iteration 29, loss = 0.02716874
Iteration 30, loss = 0.02672493
Iteration 31, loss = 0.02620364
Iteration 32, loss = 0.02584542
Iteration 33, loss = 0.02535512
Iteration 34, loss = 0.02520877
Iteration 35, loss = 0.02478713
Iteration 36, loss = 0.02447833
Iteration 37, loss = 0.02437725
Iteration 38, loss = 0.02415179
Iteration 39, loss = 0.02412372
Iteration 40, loss = 0.02362698
Iteration 41, loss = 0.02341418
Iteration 42, loss = 0.02354014
Iteration 43, loss = 0.02318744
Iteration 44, loss = 0.02313744
Iteration 45, loss = 0.02294881
Iteration 46, loss = 0.02266984
Iteration 47, loss = 0.02287739
Iteration 48, loss = 0.02258791
Iteration 49, loss = 0.02267475
Iteration 50, loss = 0.02232856
Iteration 51, loss = 0.02226888
Iteration 52, loss = 0.02231933
Iteration 53, loss = 0.02200020
Iteration 54, loss = 0.02206573
Iteration 55, loss = 0.02185433
Iteration 56, loss = 0.02174020
Iteration 57, loss = 0.02199820
Iteration 58, loss = 0.02179120
Iteration 59, loss = 0.02185310
Iteration 60, loss = 0.02159308
Iteration 61, loss = 0.02146685
Iteration 62, loss = 0.02179310
Iteration 63, loss = 0.02155311
Iteration 64, loss = 0.02165751
Iteration 65, loss = 0.02146053
Iteration 66, loss = 0.02137605
Iteration 67, loss = 0.02148720
Iteration 68, loss = 0.02149841
Iteration 69, loss = 0.02109558
Iteration 70, loss = 0.02132691
Iteration 71, loss = 0.02139044
Iteration 72, loss = 0.02152122
Iteration 73, loss = 0.02102631
Iteration 74, loss = 0.02121962
Iteration 75, loss = 0.02117421
Iteration 76, loss = 0.02101558
Iteration 77, loss = 0.02107217
Iteration 78, loss = 0.02083352
Iteration 79, loss = 0.02100816
Iteration 80, loss = 0.02101093
Iteration 81, loss = 0.02101219
Iteration 82, loss = 0.02083462
Iteration 83, loss = 0.02090462
Iteration 84, loss = 0.02079137
Iteration 85, loss = 0.02070637
Iteration 86, loss = 0.02084369
Iteration 87, loss = 0.02101599
Iteration 88, loss = 0.02087645
Iteration 89, loss = 0.02080570
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69472992
Iteration 2, loss = 0.62967250
Iteration 3, loss = 0.52620821
Iteration 4, loss = 0.40969811
Iteration 5, loss = 0.30883357
Iteration 6, loss = 0.23213267
Iteration 7, loss = 0.17788244
Iteration 8, loss = 0.14043711
Iteration 9, loss = 0.11465311
Iteration 10, loss = 0.09617521
Iteration 11, loss = 0.08279736
Iteration 12, loss = 0.07251134
Iteration 13, loss = 0.06509185
Iteration 14, loss = 0.05926437
Iteration 15, loss = 0.05419366
Iteration 16, loss = 0.05031638
Iteration 17, loss = 0.04701227
Iteration 18, loss = 0.04423825
Iteration 19, loss = 0.04224352
Iteration 20, loss = 0.04045541
Iteration 21, loss = 0.03857726
Iteration 22, loss = 0.03722258
Iteration 23, loss = 0.03588856
Iteration 24, loss = 0.03475239
Iteration 25, loss = 0.03410623
Iteration 26, loss = 0.03303010
Iteration 27, loss = 0.03264703
Iteration 28, loss = 0.03174936
Iteration 29, loss = 0.03127144
Iteration 30, loss = 0.03079850
Iteration 31, loss = 0.03018362
Iteration 32, loss = 0.02972910
Iteration 33, loss = 0.02919318
Iteration 34, loss = 0.02912720
Iteration 35, loss = 0.02882653
Iteration 36, loss = 0.02880901
Iteration 37, loss = 0.02807569
Iteration 38, loss = 0.02795421
Iteration 39, loss = 0.02751569
Iteration 40, loss = 0.02746285
Iteration 41, loss = 0.02743218
Iteration 42, loss = 0.02714230
Iteration 43, loss = 0.02697528
Iteration 44, loss = 0.02696840
Iteration 45, loss = 0.02667609
Iteration 46, loss = 0.02678704
Iteration 47, loss = 0.02628992
Iteration 48, loss = 0.02611158
Iteration 49, loss = 0.02627337
Iteration 50, loss = 0.02641912
Iteration 51, loss = 0.02576395
Iteration 52, loss = 0.02573076
Iteration 53, loss = 0.02558589
Iteration 54, loss = 0.02566479
Iteration 55, loss = 0.02565092
Iteration 56, loss = 0.02561225
Iteration 57, loss = 0.02525385
Iteration 58, loss = 0.02541220
Iteration 59, loss = 0.02576816
Iteration 60, loss = 0.02522537
Iteration 61, loss = 0.02517694
Iteration 62, loss = 0.02511151
Iteration 63, loss = 0.02493929
Iteration 64, loss = 0.02492542
Iteration 65, loss = 0.02498883
Iteration 66, loss = 0.02541283
Iteration 67, loss = 0.02501097
Iteration 68, loss = 0.02489163
Iteration 69, loss = 0.02502548
Iteration 70, loss = 0.02474015
Iteration 71, loss = 0.02459459
Iteration 72, loss = 0.02501358
Iteration 73, loss = 0.02464260
Iteration 74, loss = 0.02503109
Iteration 75, loss = 0.02494771
Iteration 76, loss = 0.02460687
Iteration 77, loss = 0.02455323
Iteration 78, loss = 0.02447864
Iteration 79, loss = 0.02459148
Iteration 80, loss = 0.02432615
Iteration 81, loss = 0.02457169
Iteration 82, loss = 0.02463812
Iteration 83, loss = 0.02430039
Iteration 84, loss = 0.02499861
Iteration 85, loss = 0.02437507
Iteration 86, loss = 0.02457843
Iteration 87, loss = 0.02447340
Iteration 88, loss = 0.02407179
Iteration 89, loss = 0.02434898
Iteration 90, loss = 0.02441366
Iteration 91, loss = 0.02431994
Iteration 92, loss = 0.02465458
Iteration 93, loss = 0.02441107
Iteration 94, loss = 0.02414900
Iteration 95, loss = 0.02414084
Iteration 96, loss = 0.02409817
Iteration 97, loss = 0.02398523
Iteration 98, loss = 0.02391966
Iteration 99, loss = 0.02430718
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71473842
Iteration 2, loss = 0.67422956
Iteration 3, loss = 0.62213821
Iteration 4, loss = 0.56045524
Iteration 5, loss = 0.48986971
Iteration 6, loss = 0.41751123
Iteration 7, loss = 0.35083694
Iteration 8, loss = 0.29285128
Iteration 9, loss = 0.24403435
Iteration 10, loss = 0.20460221
Iteration 11, loss = 0.17274263
Iteration 12, loss = 0.14736922
Iteration 13, loss = 0.12703839
Iteration 14, loss = 0.11095529
Iteration 15, loss = 0.09775288
Iteration 16, loss = 0.08705623
Iteration 17, loss = 0.07843635
Iteration 18, loss = 0.07108828
Iteration 19, loss = 0.06489196
Iteration 20, loss = 0.05976656
Iteration 21, loss = 0.05539425
Iteration 22, loss = 0.05162278
Iteration 23, loss = 0.04836799
Iteration 24, loss = 0.04558590
Iteration 25, loss = 0.04301782
Iteration 26, loss = 0.04095790
Iteration 27, loss = 0.03912686
Iteration 28, loss = 0.03729517
Iteration 29, loss = 0.03581245
Iteration 30, loss = 0.03428414
Iteration 31, loss = 0.03313217
Iteration 32, loss = 0.03209860
Iteration 33, loss = 0.03095880
Iteration 34, loss = 0.03006304
Iteration 35, loss = 0.02926512
Iteration 36, loss = 0.02844712
Iteration 37, loss = 0.02784224
Iteration 38, loss = 0.02716514
Iteration 39, loss = 0.02659188
Iteration 40, loss = 0.02605117
Iteration 41, loss = 0.02561956
Iteration 42, loss = 0.02507212
Iteration 43, loss = 0.02464782
Iteration 44, loss = 0.02432077
Iteration 45, loss = 0.02386655
Iteration 46, loss = 0.02359325
Iteration 47, loss = 0.02346575
Iteration 48, loss = 0.02291760
Iteration 49, loss = 0.02268819
Iteration 50, loss = 0.02244640
Iteration 51, loss = 0.02218204
Iteration 52, loss = 0.02203806
Iteration 53, loss = 0.02181644
Iteration 54, loss = 0.02149907
Iteration 55, loss = 0.02127179
Iteration 56, loss = 0.02105648
Iteration 57, loss = 0.02102947
Iteration 58, loss = 0.02083013
Iteration 59, loss = 0.02084202
Iteration 60, loss = 0.02044063
Iteration 61, loss = 0.02048035
Iteration 62, loss = 0.02024702
Iteration 63, loss = 0.02023100
Iteration 64, loss = 0.01994458
Iteration 65, loss = 0.01994232
Iteration 66, loss = 0.01976854
Iteration 67, loss = 0.01964165
Iteration 68, loss = 0.01958645
Iteration 69, loss = 0.01962602
Iteration 70, loss = 0.01929181
Iteration 71, loss = 0.01950403
Iteration 72, loss = 0.01931994
Iteration 73, loss = 0.01922363
Iteration 74, loss = 0.01916417
Iteration 75, loss = 0.01905072
Iteration 76, loss = 0.01908505
Iteration 77, loss = 0.01896090
Iteration 78, loss = 0.01899161
Iteration 79, loss = 0.01890659
Iteration 80, loss = 0.01875084
Iteration 81, loss = 0.01868813
Iteration 82, loss = 0.01864681
Iteration 83, loss = 0.01860598
Iteration 84, loss = 0.01858876
Iteration 85, loss = 0.01861382
Iteration 86, loss = 0.01839295
Iteration 87, loss = 0.01844349
Iteration 88, loss = 0.01819417
Iteration 89, loss = 0.01832885
Iteration 90, loss = 0.01815963
Iteration 91, loss = 0.01816147
Iteration 92, loss = 0.01823457
Iteration 93, loss = 0.01818632
Iteration 94, loss = 0.01814653
Iteration 95, loss = 0.01820689
Iteration 96, loss = 0.01809565
Iteration 97, loss = 0.01793539
Iteration 98, loss = 0.01796532
Iteration 99, loss = 0.01786242
Iteration 100, loss = 0.01807580
Iteration 101, loss = 0.01792135
Iteration 102, loss = 0.01766800
Iteration 103, loss = 0.01773863
Iteration 104, loss = 0.01784747
Iteration 105, loss = 0.01798736
Iteration 106, loss = 0.01777551
Iteration 107, loss = 0.01796395
Iteration 108, loss = 0.01766391
Iteration 109, loss = 0.01776701
Iteration 110, loss = 0.01768756
Iteration 111, loss = 0.01751000
Iteration 112, loss = 0.01777441
Iteration 113, loss = 0.01752559
Iteration 114, loss = 0.01759703
Iteration 115, loss = 0.01766524
Iteration 116, loss = 0.01755051
Iteration 117, loss = 0.01761857
Iteration 118, loss = 0.01744269
Iteration 119, loss = 0.01739792
Iteration 120, loss = 0.01756085
Iteration 121, loss = 0.01753319
Iteration 122, loss = 0.01749618
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67703517
Iteration 2, loss = 0.61240723
Iteration 3, loss = 0.50902245
Iteration 4, loss = 0.39937925
Iteration 5, loss = 0.30468272
Iteration 6, loss = 0.23164850
Iteration 7, loss = 0.17857602
Iteration 8, loss = 0.14127156
Iteration 9, loss = 0.11470798
Iteration 10, loss = 0.09561402
Iteration 11, loss = 0.08202516
Iteration 12, loss = 0.07120332
Iteration 13, loss = 0.06314555
Iteration 14, loss = 0.05689215
Iteration 15, loss = 0.05191827
Iteration 16, loss = 0.04759981
Iteration 17, loss = 0.04417331
Iteration 18, loss = 0.04143258
Iteration 19, loss = 0.03886270
Iteration 20, loss = 0.03726381
Iteration 21, loss = 0.03514803
Iteration 22, loss = 0.03373007
Iteration 23, loss = 0.03245667
Iteration 24, loss = 0.03098029
Iteration 25, loss = 0.03027133
Iteration 26, loss = 0.02933029
Iteration 27, loss = 0.02858042
Iteration 28, loss = 0.02799558
Iteration 29, loss = 0.02705117
Iteration 30, loss = 0.02687960
Iteration 31, loss = 0.02607618
Iteration 32, loss = 0.02583688
Iteration 33, loss = 0.02530244
Iteration 34, loss = 0.02478664
Iteration 35, loss = 0.02464504
Iteration 36, loss = 0.02424539
Iteration 37, loss = 0.02425691
Iteration 38, loss = 0.02368298
Iteration 39, loss = 0.02343969
Iteration 40, loss = 0.02316372
Iteration 41, loss = 0.02319296
Iteration 42, loss = 0.02304899
Iteration 43, loss = 0.02292479
Iteration 44, loss = 0.02252070
Iteration 45, loss = 0.02246018
Iteration 46, loss = 0.02237168
Iteration 47, loss = 0.02216857
Iteration 48, loss = 0.02206270
Iteration 49, loss = 0.02189097
Iteration 50, loss = 0.02162892
Iteration 51, loss = 0.02176757
Iteration 52, loss = 0.02127599
Iteration 53, loss = 0.02161592
Iteration 54, loss = 0.02167828
Iteration 55, loss = 0.02109850
Iteration 56, loss = 0.02116122
Iteration 57, loss = 0.02114702
Iteration 58, loss = 0.02123603
Iteration 59, loss = 0.02106258
Iteration 60, loss = 0.02103833
Iteration 61, loss = 0.02086008
Iteration 62, loss = 0.02074589
Iteration 63, loss = 0.02111215
Iteration 64, loss = 0.02090860
Iteration 65, loss = 0.02078592
Iteration 66, loss = 0.02085200
Iteration 67, loss = 0.02045522
Iteration 68, loss = 0.02054537
Iteration 69, loss = 0.02082204
Iteration 70, loss = 0.02030159
Iteration 71, loss = 0.02067739
Iteration 72, loss = 0.02033741
Iteration 73, loss = 0.02032207
Iteration 74, loss = 0.02066534
Iteration 75, loss = 0.02036180
Iteration 76, loss = 0.02025784
Iteration 77, loss = 0.02018858
Iteration 78, loss = 0.02026516
Iteration 79, loss = 0.02011835
Iteration 80, loss = 0.02041847
Iteration 81, loss = 0.02004387
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71642024
Iteration 2, loss = 0.68728143
Iteration 3, loss = 0.65469231
Iteration 4, loss = 0.61703389
Iteration 5, loss = 0.57298604
Iteration 6, loss = 0.52458346
Iteration 7, loss = 0.47480214
Iteration 8, loss = 0.42532448
Iteration 9, loss = 0.37815756
Iteration 10, loss = 0.33481032
Iteration 11, loss = 0.29530645
Iteration 12, loss = 0.26033127
Iteration 13, loss = 0.22978156
Iteration 14, loss = 0.20337589
Iteration 15, loss = 0.18054621
Iteration 16, loss = 0.16077905
Iteration 17, loss = 0.14415368
Iteration 18, loss = 0.12996425
Iteration 19, loss = 0.11767693
Iteration 20, loss = 0.10714023
Iteration 21, loss = 0.09801800
Iteration 22, loss = 0.09016514
Iteration 23, loss = 0.08324256
Iteration 24, loss = 0.07719210
Iteration 25, loss = 0.07195080
Iteration 26, loss = 0.06733368
Iteration 27, loss = 0.06320091
Iteration 28, loss = 0.05955266
Iteration 29, loss = 0.05620756
Iteration 30, loss = 0.05331898
Iteration 31, loss = 0.05073526
Iteration 32, loss = 0.04822467
Iteration 33, loss = 0.04609291
Iteration 34, loss = 0.04407523
Iteration 35, loss = 0.04218206
Iteration 36, loss = 0.04056756
Iteration 37, loss = 0.03909816
Iteration 38, loss = 0.03766331
Iteration 39, loss = 0.03647154
Iteration 40, loss = 0.03514837
Iteration 41, loss = 0.03425198
Iteration 42, loss = 0.03324857
Iteration 43, loss = 0.03240518
Iteration 44, loss = 0.03155141
Iteration 45, loss = 0.03071407
Iteration 46, loss = 0.02998004
Iteration 47, loss = 0.02922406
Iteration 48, loss = 0.02872503
Iteration 49, loss = 0.02818151
Iteration 50, loss = 0.02740062
Iteration 51, loss = 0.02694202
Iteration 52, loss = 0.02629578
Iteration 53, loss = 0.02581053
Iteration 54, loss = 0.02526580
Iteration 55, loss = 0.02513949
Iteration 56, loss = 0.02474695
Iteration 57, loss = 0.02423509
Iteration 58, loss = 0.02410149
Iteration 59, loss = 0.02343211
Iteration 60, loss = 0.02332842
Iteration 61, loss = 0.02294018
Iteration 62, loss = 0.02268818
Iteration 63, loss = 0.02249173
Iteration 64, loss = 0.02224915
Iteration 65, loss = 0.02192889
Iteration 66, loss = 0.02173388
Iteration 67, loss = 0.02164816
Iteration 68, loss = 0.02142072
Iteration 69, loss = 0.02138683
Iteration 70, loss = 0.02098135
Iteration 71, loss = 0.02076485
Iteration 72, loss = 0.02072766
Iteration 73, loss = 0.02043383
Iteration 74, loss = 0.02023527
Iteration 75, loss = 0.02016202
Iteration 76, loss = 0.01987962
Iteration 77, loss = 0.01969154
Iteration 78, loss = 0.01955525
Iteration 79, loss = 0.01932416
Iteration 80, loss = 0.01910156
Iteration 81, loss = 0.01897715
Iteration 82, loss = 0.01895547
Iteration 83, loss = 0.01874687
Iteration 84, loss = 0.01865722
Iteration 85, loss = 0.01871426
Iteration 86, loss = 0.01866783
Iteration 87, loss = 0.01839816
Iteration 88, loss = 0.01838589
Iteration 89, loss = 0.01836981
Iteration 90, loss = 0.01837225
Iteration 91, loss = 0.01815894
Iteration 92, loss = 0.01801754
Iteration 93, loss = 0.01809263
Iteration 94, loss = 0.01818456
Iteration 95, loss = 0.01819263
Iteration 96, loss = 0.01798945
Iteration 97, loss = 0.01776573
Iteration 98, loss = 0.01761728
Iteration 99, loss = 0.01736547
Iteration 100, loss = 0.01726450
Iteration 101, loss = 0.01730065
Iteration 102, loss = 0.01722898
Iteration 103, loss = 0.01713067
Iteration 104, loss = 0.01715172
Iteration 105, loss = 0.01714480
Iteration 106, loss = 0.01693966
Iteration 107, loss = 0.01720255
Iteration 108, loss = 0.01708953
Iteration 109, loss = 0.01709136
Iteration 110, loss = 0.01705985
Iteration 111, loss = 0.01709991
Iteration 112, loss = 0.01699348
Iteration 113, loss = 0.01676705
Iteration 114, loss = 0.01670080
Iteration 115, loss = 0.01651932
Iteration 116, loss = 0.01658111
Iteration 117, loss = 0.01660453
Iteration 118, loss = 0.01665487
Iteration 119, loss = 0.01681734
Iteration 120, loss = 0.01687766
Iteration 121, loss = 0.01692701
Iteration 122, loss = 0.01668781
Iteration 123, loss = 0.01662036
Iteration 124, loss = 0.01645463
Iteration 125, loss = 0.01634094
Iteration 126, loss = 0.01620677
Iteration 127, loss = 0.01614576
Iteration 128, loss = 0.01621784
Iteration 129, loss = 0.01619707
Iteration 130, loss = 0.01628903
Iteration 131, loss = 0.01625808
Iteration 132, loss = 0.01639014
Iteration 133, loss = 0.01622992
Iteration 134, loss = 0.01676408
Iteration 135, loss = 0.01618528
Iteration 136, loss = 0.01591432
Iteration 137, loss = 0.01588398
Iteration 138, loss = 0.01583762
Iteration 139, loss = 0.01599507
Iteration 140, loss = 0.01598512
Iteration 141, loss = 0.01608439
Iteration 142, loss = 0.01620065
Iteration 143, loss = 0.01619799
Iteration 144, loss = 0.01604271
Iteration 145, loss = 0.01558751
Iteration 146, loss = 0.01598780
Iteration 147, loss = 0.01578707
Iteration 148, loss = 0.01574894
Iteration 149, loss = 0.01570901
Iteration 150, loss = 0.01571395
Iteration 151, loss = 0.01567082
Iteration 152, loss = 0.01560025
Iteration 153, loss = 0.01605834
Iteration 154, loss = 0.01608643
Iteration 155, loss = 0.01591245
Iteration 156, loss = 0.01584314
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67679175
Iteration 2, loss = 0.60633355
Iteration 3, loss = 0.49709574
Iteration 4, loss = 0.38564032
Iteration 5, loss = 0.29021079
Iteration 6, loss = 0.21828372
Iteration 7, loss = 0.16744181
Iteration 8, loss = 0.13207085
Iteration 9, loss = 0.10773410
Iteration 10, loss = 0.09011905
Iteration 11, loss = 0.07741656
Iteration 12, loss = 0.06780503
Iteration 13, loss = 0.06026669
Iteration 14, loss = 0.05440937
Iteration 15, loss = 0.04996320
Iteration 16, loss = 0.04592784
Iteration 17, loss = 0.04287609
Iteration 18, loss = 0.04049751
Iteration 19, loss = 0.03833206
Iteration 20, loss = 0.03636358
Iteration 21, loss = 0.03471810
Iteration 22, loss = 0.03350104
Iteration 23, loss = 0.03227062
Iteration 24, loss = 0.03110880
Iteration 25, loss = 0.03038960
Iteration 26, loss = 0.02954510
Iteration 27, loss = 0.02872360
Iteration 28, loss = 0.02813217
Iteration 29, loss = 0.02752957
Iteration 30, loss = 0.02724238
Iteration 31, loss = 0.02670743
Iteration 32, loss = 0.02615286
Iteration 33, loss = 0.02591789
Iteration 34, loss = 0.02529709
Iteration 35, loss = 0.02543925
Iteration 36, loss = 0.02478175
Iteration 37, loss = 0.02456224
Iteration 38, loss = 0.02463787
Iteration 39, loss = 0.02416440
Iteration 40, loss = 0.02412141
Iteration 41, loss = 0.02374994
Iteration 42, loss = 0.02363204
Iteration 43, loss = 0.02385642
Iteration 44, loss = 0.02339430
Iteration 45, loss = 0.02317258
Iteration 46, loss = 0.02325811
Iteration 47, loss = 0.02284355
Iteration 48, loss = 0.02312263
Iteration 49, loss = 0.02275941
Iteration 50, loss = 0.02269729
Iteration 51, loss = 0.02280476
Iteration 52, loss = 0.02290230
Iteration 53, loss = 0.02288779
Iteration 54, loss = 0.02243075
Iteration 55, loss = 0.02263725
Iteration 56, loss = 0.02220072
Iteration 57, loss = 0.02203697
Iteration 58, loss = 0.02207867
Iteration 59, loss = 0.02196579
Iteration 60, loss = 0.02201193
Iteration 61, loss = 0.02190089
Iteration 62, loss = 0.02165876
Iteration 63, loss = 0.02217648
Iteration 64, loss = 0.02168751
Iteration 65, loss = 0.02185360
Iteration 66, loss = 0.02159665
Iteration 67, loss = 0.02179669
Iteration 68, loss = 0.02154272
Iteration 69, loss = 0.02162702
Iteration 70, loss = 0.02160840
Iteration 71, loss = 0.02143640
Iteration 72, loss = 0.02153032
Iteration 73, loss = 0.02124495
Iteration 74, loss = 0.02155260
Iteration 75, loss = 0.02171159
Iteration 76, loss = 0.02143994
Iteration 77, loss = 0.02129392
Iteration 78, loss = 0.02120541
Iteration 79, loss = 0.02130682
Iteration 80, loss = 0.02112048
Iteration 81, loss = 0.02139890
Iteration 82, loss = 0.02109951
Iteration 83, loss = 0.02121739
Iteration 84, loss = 0.02140681
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.73073551
Iteration 2, loss = 0.69012759
Iteration 3, loss = 0.64248361
Iteration 4, loss = 0.58705809
Iteration 5, loss = 0.52565529
Iteration 6, loss = 0.46129686
Iteration 7, loss = 0.39884000
Iteration 8, loss = 0.34048764
Iteration 9, loss = 0.28895008
Iteration 10, loss = 0.24490619
Iteration 11, loss = 0.20780068
Iteration 12, loss = 0.17731329
Iteration 13, loss = 0.15236179
Iteration 14, loss = 0.13194753
Iteration 15, loss = 0.11519081
Iteration 16, loss = 0.10152007
Iteration 17, loss = 0.09003146
Iteration 18, loss = 0.08056264
Iteration 19, loss = 0.07267620
Iteration 20, loss = 0.06582006
Iteration 21, loss = 0.06019515
Iteration 22, loss = 0.05538256
Iteration 23, loss = 0.05103611
Iteration 24, loss = 0.04730569
Iteration 25, loss = 0.04410853
Iteration 26, loss = 0.04142294
Iteration 27, loss = 0.03883057
Iteration 28, loss = 0.03666280
Iteration 29, loss = 0.03480936
Iteration 30, loss = 0.03290901
Iteration 31, loss = 0.03144029
Iteration 32, loss = 0.03006871
Iteration 33, loss = 0.02863227
Iteration 34, loss = 0.02763464
Iteration 35, loss = 0.02654473
Iteration 36, loss = 0.02549389
Iteration 37, loss = 0.02464644
Iteration 38, loss = 0.02392778
Iteration 39, loss = 0.02311026
Iteration 40, loss = 0.02235908
Iteration 41, loss = 0.02190311
Iteration 42, loss = 0.02133078
Iteration 43, loss = 0.02063354
Iteration 44, loss = 0.02016517
Iteration 45, loss = 0.01985776
Iteration 46, loss = 0.01927022
Iteration 47, loss = 0.01889159
Iteration 48, loss = 0.01847766
Iteration 49, loss = 0.01810702
Iteration 50, loss = 0.01794584
Iteration 51, loss = 0.01745220
Iteration 52, loss = 0.01711614
Iteration 53, loss = 0.01696886
Iteration 54, loss = 0.01660656
Iteration 55, loss = 0.01641243
Iteration 56, loss = 0.01617637
Iteration 57, loss = 0.01592342
Iteration 58, loss = 0.01581088
Iteration 59, loss = 0.01558451
Iteration 60, loss = 0.01546644
Iteration 61, loss = 0.01524744
Iteration 62, loss = 0.01502044
Iteration 63, loss = 0.01487151
Iteration 64, loss = 0.01470098
Iteration 65, loss = 0.01459622
Iteration 66, loss = 0.01439062
Iteration 67, loss = 0.01451203
Iteration 68, loss = 0.01422552
Iteration 69, loss = 0.01408876
Iteration 70, loss = 0.01396049
Iteration 71, loss = 0.01397980
Iteration 72, loss = 0.01405257
Iteration 73, loss = 0.01368156
Iteration 74, loss = 0.01360948
Iteration 75, loss = 0.01359437
Iteration 76, loss = 0.01338468
Iteration 77, loss = 0.01330519
Iteration 78, loss = 0.01325015
Iteration 79, loss = 0.01314986
Iteration 80, loss = 0.01304572
Iteration 81, loss = 0.01308320
Iteration 82, loss = 0.01295914
Iteration 83, loss = 0.01286799
Iteration 84, loss = 0.01292982
Iteration 85, loss = 0.01276772
Iteration 86, loss = 0.01263888
Iteration 87, loss = 0.01262975
Iteration 88, loss = 0.01263681
Iteration 89, loss = 0.01254126
Iteration 90, loss = 0.01258309
Iteration 91, loss = 0.01240635
Iteration 92, loss = 0.01239159
Iteration 93, loss = 0.01230252
Iteration 94, loss = 0.01224832
Iteration 95, loss = 0.01232271
Iteration 96, loss = 0.01214990
Iteration 97, loss = 0.01216919
Iteration 98, loss = 0.01208715
Iteration 99, loss = 0.01217894
Iteration 100, loss = 0.01208600
Iteration 101, loss = 0.01212562
Iteration 102, loss = 0.01193931
Iteration 103, loss = 0.01206787
Iteration 104, loss = 0.01191338
Iteration 105, loss = 0.01192891
Iteration 106, loss = 0.01192005
Iteration 107, loss = 0.01185358
Iteration 108, loss = 0.01181693
Iteration 109, loss = 0.01179877
Iteration 110, loss = 0.01177843
Iteration 111, loss = 0.01179503
Iteration 112, loss = 0.01171122
Iteration 113, loss = 0.01175785
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67817997
Iteration 2, loss = 0.65259443
Iteration 3, loss = 0.62352056
Iteration 4, loss = 0.58540739
Iteration 5, loss = 0.53952426
Iteration 6, loss = 0.48971453
Iteration 7, loss = 0.44033199
Iteration 8, loss = 0.39341268
Iteration 9, loss = 0.34945542
Iteration 10, loss = 0.30953172
Iteration 11, loss = 0.27300978
Iteration 12, loss = 0.24088979
Iteration 13, loss = 0.21238319
Iteration 14, loss = 0.18786794
Iteration 15, loss = 0.16681432
Iteration 16, loss = 0.14872359
Iteration 17, loss = 0.13317943
Iteration 18, loss = 0.11984476
Iteration 19, loss = 0.10840019
Iteration 20, loss = 0.09856809
Iteration 21, loss = 0.09011777
Iteration 22, loss = 0.08276315
Iteration 23, loss = 0.07635973
Iteration 24, loss = 0.07074495
Iteration 25, loss = 0.06577818
Iteration 26, loss = 0.06141742
Iteration 27, loss = 0.05771242
Iteration 28, loss = 0.05418556
Iteration 29, loss = 0.05106053
Iteration 30, loss = 0.04834322
Iteration 31, loss = 0.04592531
Iteration 32, loss = 0.04368421
Iteration 33, loss = 0.04171738
Iteration 34, loss = 0.03986928
Iteration 35, loss = 0.03815145
Iteration 36, loss = 0.03658140
Iteration 37, loss = 0.03511572
Iteration 38, loss = 0.03381674
Iteration 39, loss = 0.03274232
Iteration 40, loss = 0.03155239
Iteration 41, loss = 0.03043659
Iteration 42, loss = 0.02944845
Iteration 43, loss = 0.02855887
Iteration 44, loss = 0.02771142
Iteration 45, loss = 0.02703312
Iteration 46, loss = 0.02612886
Iteration 47, loss = 0.02548599
Iteration 48, loss = 0.02488235
Iteration 49, loss = 0.02427750
Iteration 50, loss = 0.02375751
Iteration 51, loss = 0.02326956
Iteration 52, loss = 0.02278793
Iteration 53, loss = 0.02230250
Iteration 54, loss = 0.02194839
Iteration 55, loss = 0.02156971
Iteration 56, loss = 0.02116680
Iteration 57, loss = 0.02077385
Iteration 58, loss = 0.02039706
Iteration 59, loss = 0.02004125
Iteration 60, loss = 0.01971572
Iteration 61, loss = 0.01943507
Iteration 62, loss = 0.01917604
Iteration 63, loss = 0.01888440
Iteration 64, loss = 0.01866547
Iteration 65, loss = 0.01844051
Iteration 66, loss = 0.01825214
Iteration 67, loss = 0.01796867
Iteration 68, loss = 0.01827604
Iteration 69, loss = 0.01812249
Iteration 70, loss = 0.01777680
Iteration 71, loss = 0.01748623
Iteration 72, loss = 0.01704997
Iteration 73, loss = 0.01708522
Iteration 74, loss = 0.01687303
Iteration 75, loss = 0.01674999
Iteration 76, loss = 0.01652460
Iteration 77, loss = 0.01641577
Iteration 78, loss = 0.01627756
Iteration 79, loss = 0.01607750
Iteration 80, loss = 0.01596143
Iteration 81, loss = 0.01578624
Iteration 82, loss = 0.01578729
Iteration 83, loss = 0.01589016
Iteration 84, loss = 0.01591725
Iteration 85, loss = 0.01583404
Iteration 86, loss = 0.01627501
Iteration 87, loss = 0.01604424
Iteration 88, loss = 0.01565073
Iteration 89, loss = 0.01528923
Iteration 90, loss = 0.01509496
Iteration 91, loss = 0.01484825
Iteration 92, loss = 0.01476466
Iteration 93, loss = 0.01461863
Iteration 94, loss = 0.01463365
Iteration 95, loss = 0.01483544
Iteration 96, loss = 0.01475011
Iteration 97, loss = 0.01461180
Iteration 98, loss = 0.01449273
Iteration 99, loss = 0.01439953
Iteration 100, loss = 0.01433626
Iteration 101, loss = 0.01421204
Iteration 102, loss = 0.01416007
Iteration 103, loss = 0.01411053
Iteration 104, loss = 0.01408538
Iteration 105, loss = 0.01398148
Iteration 106, loss = 0.01384188
Iteration 107, loss = 0.01389902
Iteration 108, loss = 0.01387098
Iteration 109, loss = 0.01390608
Iteration 110, loss = 0.01380329
Iteration 111, loss = 0.01375715
Iteration 112, loss = 0.01366702
Iteration 113, loss = 0.01354572
Iteration 114, loss = 0.01351488
Iteration 115, loss = 0.01343755
Iteration 116, loss = 0.01341281
Iteration 117, loss = 0.01333621
Iteration 118, loss = 0.01323717
Iteration 119, loss = 0.01327646
Iteration 120, loss = 0.01324548
Iteration 121, loss = 0.01321202
Iteration 122, loss = 0.01315953
Iteration 123, loss = 0.01319786
Iteration 124, loss = 0.01318779
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70791667
Iteration 2, loss = 0.63326218
Iteration 3, loss = 0.53273738
Iteration 4, loss = 0.41777650
Iteration 5, loss = 0.31475469
Iteration 6, loss = 0.23556969
Iteration 7, loss = 0.17904633
Iteration 8, loss = 0.13999383
Iteration 9, loss = 0.11339495
Iteration 10, loss = 0.09443179
Iteration 11, loss = 0.08058230
Iteration 12, loss = 0.07016998
Iteration 13, loss = 0.06213716
Iteration 14, loss = 0.05614708
Iteration 15, loss = 0.05130863
Iteration 16, loss = 0.04739631
Iteration 17, loss = 0.04412112
Iteration 18, loss = 0.04152408
Iteration 19, loss = 0.03939151
Iteration 20, loss = 0.03718433
Iteration 21, loss = 0.03542223
Iteration 22, loss = 0.03411083
Iteration 23, loss = 0.03294012
Iteration 24, loss = 0.03179303
Iteration 25, loss = 0.03073784
Iteration 26, loss = 0.03027020
Iteration 27, loss = 0.02942480
Iteration 28, loss = 0.02869787
Iteration 29, loss = 0.02805597
Iteration 30, loss = 0.02762710
Iteration 31, loss = 0.02728220
Iteration 32, loss = 0.02661930
Iteration 33, loss = 0.02621242
Iteration 34, loss = 0.02606759
Iteration 35, loss = 0.02583495
Iteration 36, loss = 0.02550451
Iteration 37, loss = 0.02501566
Iteration 38, loss = 0.02493416
Iteration 39, loss = 0.02474633
Iteration 40, loss = 0.02462133
Iteration 41, loss = 0.02436828
Iteration 42, loss = 0.02407832
Iteration 43, loss = 0.02409724
Iteration 44, loss = 0.02373533
Iteration 45, loss = 0.02378247
Iteration 46, loss = 0.02367836
Iteration 47, loss = 0.02330347
Iteration 48, loss = 0.02335561
Iteration 49, loss = 0.02332001
Iteration 50, loss = 0.02308460
Iteration 51, loss = 0.02312675
Iteration 52, loss = 0.02301072
Iteration 53, loss = 0.02276473
Iteration 54, loss = 0.02265389
Iteration 55, loss = 0.02280425
Iteration 56, loss = 0.02247679
Iteration 57, loss = 0.02253219
Iteration 58, loss = 0.02252401
Iteration 59, loss = 0.02251476
Iteration 60, loss = 0.02248880
Iteration 61, loss = 0.02218549
Iteration 62, loss = 0.02211574
Iteration 63, loss = 0.02236615
Iteration 64, loss = 0.02221925
Iteration 65, loss = 0.02238209
Iteration 66, loss = 0.02215707
Iteration 67, loss = 0.02184797
Iteration 68, loss = 0.02205647
Iteration 69, loss = 0.02219532
Iteration 70, loss = 0.02202762
Iteration 71, loss = 0.02165806
Iteration 72, loss = 0.02209187
Iteration 73, loss = 0.02179042
Iteration 74, loss = 0.02193003
Iteration 75, loss = 0.02179381
Iteration 76, loss = 0.02163428
Iteration 77, loss = 0.02161196
Iteration 78, loss = 0.02174365
Iteration 79, loss = 0.02168606
Iteration 80, loss = 0.02147167
Iteration 81, loss = 0.02165129
Iteration 82, loss = 0.02149883
Iteration 83, loss = 0.02155784
Iteration 84, loss = 0.02149238
Iteration 85, loss = 0.02145250
Iteration 86, loss = 0.02143245
Iteration 87, loss = 0.02162569
Iteration 88, loss = 0.02137101
Iteration 89, loss = 0.02134912
Iteration 90, loss = 0.02168579
Iteration 91, loss = 0.02149742
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	84m8.688s
user	344m57.825s
sys	34m55.384s
Elapsed time: 5049 seconds
