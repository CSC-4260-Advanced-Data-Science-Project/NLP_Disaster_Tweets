Processing dataset: kept_v5_lemma_stem

üì• Loaded 27 datasets.

‚ö†Ô∏è Datasets with NaNs (before cleaning):
 - kept_v3_lemmatized: 1 NaNs removed
 - dropped_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v4_stemmed: 1 NaNs removed
 - dropped_v6_custom_stopwords: 5 NaNs removed
 - prepended_v6_custom_stopwords: 1 NaNs removed
 - prepended_v5_lemma_stem: 1 NaNs removed
 - kept_v6_custom_stopwords: 5 NaNs removed
 - prepended_v9_minimal_processing: 1 NaNs removed
 - kept_v8_keep_hashtags: 1 NaNs removed
 - prepended_v8_keep_hashtags: 1 NaNs removed
 - kept_v7_lowercase_words_only: 1 NaNs removed
 - kept_v1_basic_clean: 1 NaNs removed
 - kept_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v5_lemma_stem: 1 NaNs removed
 - prepended_v3_lemmatized: 1 NaNs removed
 - prepended_v2_no_emojis_mentions: 1 NaNs removed
 - dropped_v1_basic_clean: 1 NaNs removed
 - kept_v4_stemmed: 1 NaNs removed
 - dropped_v3_lemmatized: 1 NaNs removed
 - dropped_v2_no_emojis_mentions: 1 NaNs removed
 - kept_v9_minimal_processing: 6 NaNs removed
 - prepended_v7_lowercase_words_only: 1 NaNs removed
 - dropped_v9_minimal_processing: 6 NaNs removed
 - dropped_v8_keep_hashtags: 1 NaNs removed
 - prepended_v1_basic_clean: 1 NaNs removed
 - prepended_v4_stemmed: 1 NaNs removed
 - dropped_v5_lemma_stem: 1 NaNs removed

üì¶ Dataset: kept_v5_lemma_stem (7443 samples)
  üîç Evaluating: MultinomialNB
üìà Plotting learning curve for MultinomialNB on kept_v5_lemma_stem...
  üîç Evaluating: LogisticRegression
üìà Plotting learning curve for LogisticRegression on kept_v5_lemma_stem...
  üîç Evaluating: PassiveAggressive
üìà Plotting learning curve for PassiveAggressive on kept_v5_lemma_stem...
  üîç Evaluating: SVM
üìà Plotting learning curve for SVM on kept_v5_lemma_stem...
  üîç Evaluating: KNN
üìà Plotting learning curve for KNN on kept_v5_lemma_stem...
  üîç Evaluating: NeuralNetwork
Iteration 1, loss = 0.70393776
Iteration 2, loss = 0.62907738
Iteration 3, loss = 0.51816283
Iteration 4, loss = 0.39857074
Iteration 5, loss = 0.29919314
Iteration 6, loss = 0.22503505
Iteration 7, loss = 0.17324714
Iteration 8, loss = 0.13738718
Iteration 9, loss = 0.11250095
Iteration 10, loss = 0.09503745
Iteration 11, loss = 0.08191342
Iteration 12, loss = 0.07261644
Iteration 13, loss = 0.06500632
Iteration 14, loss = 0.05922121
Iteration 15, loss = 0.05452259
Iteration 16, loss = 0.05087606
Iteration 17, loss = 0.04800358
Iteration 18, loss = 0.04533397
Iteration 19, loss = 0.04311764
Iteration 20, loss = 0.04149621
Iteration 21, loss = 0.03980834
Iteration 22, loss = 0.03865304
Iteration 23, loss = 0.03750935
Iteration 24, loss = 0.03605692
Iteration 25, loss = 0.03549678
Iteration 26, loss = 0.03462696
Iteration 27, loss = 0.03395803
Iteration 28, loss = 0.03359752
Iteration 29, loss = 0.03297150
Iteration 30, loss = 0.03238308
Iteration 31, loss = 0.03194150
Iteration 32, loss = 0.03162035
Iteration 33, loss = 0.03100792
Iteration 34, loss = 0.03086787
Iteration 35, loss = 0.03073347
Iteration 36, loss = 0.03020784
Iteration 37, loss = 0.02996116
Iteration 38, loss = 0.02990188
Iteration 39, loss = 0.02966894
Iteration 40, loss = 0.02941451
Iteration 41, loss = 0.02919324
Iteration 42, loss = 0.02908330
Iteration 43, loss = 0.02910178
Iteration 44, loss = 0.02854724
Iteration 45, loss = 0.02890075
Iteration 46, loss = 0.02849706
Iteration 47, loss = 0.02842010
Iteration 48, loss = 0.02835584
Iteration 49, loss = 0.02807547
Iteration 50, loss = 0.02836795
Iteration 51, loss = 0.02814169
Iteration 52, loss = 0.02809502
Iteration 53, loss = 0.02782602
Iteration 54, loss = 0.02781832
Iteration 55, loss = 0.02780211
Iteration 56, loss = 0.02773356
Iteration 57, loss = 0.02811542
Iteration 58, loss = 0.02734964
Iteration 59, loss = 0.02756471
Iteration 60, loss = 0.02764567
Iteration 61, loss = 0.02757725
Iteration 62, loss = 0.02742625
Iteration 63, loss = 0.02718596
Iteration 64, loss = 0.02733337
Iteration 65, loss = 0.02724558
Iteration 66, loss = 0.02760567
Iteration 67, loss = 0.02720137
Iteration 68, loss = 0.02722499
Iteration 69, loss = 0.02747859
Iteration 70, loss = 0.02691664
Iteration 71, loss = 0.02684947
Iteration 72, loss = 0.02715414
Iteration 73, loss = 0.02712474
Iteration 74, loss = 0.02707320
Iteration 75, loss = 0.02692289
Iteration 76, loss = 0.02669649
Iteration 77, loss = 0.02693585
Iteration 78, loss = 0.02670577
Iteration 79, loss = 0.02693392
Iteration 80, loss = 0.02690050
Iteration 81, loss = 0.02676742
Iteration 82, loss = 0.02686124
Iteration 83, loss = 0.02701414
Iteration 84, loss = 0.02665174
Iteration 85, loss = 0.02651486
Iteration 86, loss = 0.02668587
Iteration 87, loss = 0.02661350
Iteration 88, loss = 0.02675176
Iteration 89, loss = 0.02676560
Iteration 90, loss = 0.02674057
Iteration 91, loss = 0.02660266
Iteration 92, loss = 0.02647604
Iteration 93, loss = 0.02642691
Iteration 94, loss = 0.02663227
Iteration 95, loss = 0.02664859
Iteration 96, loss = 0.02673878
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67635298
Iteration 2, loss = 0.61186943
Iteration 3, loss = 0.50486471
Iteration 4, loss = 0.39588679
Iteration 5, loss = 0.30077257
Iteration 6, loss = 0.22823763
Iteration 7, loss = 0.17590938
Iteration 8, loss = 0.13945401
Iteration 9, loss = 0.11411005
Iteration 10, loss = 0.09597307
Iteration 11, loss = 0.08258171
Iteration 12, loss = 0.07227932
Iteration 13, loss = 0.06463120
Iteration 14, loss = 0.05868464
Iteration 15, loss = 0.05362018
Iteration 16, loss = 0.04973026
Iteration 17, loss = 0.04649052
Iteration 18, loss = 0.04387191
Iteration 19, loss = 0.04150494
Iteration 20, loss = 0.03964647
Iteration 21, loss = 0.03799972
Iteration 22, loss = 0.03656977
Iteration 23, loss = 0.03522808
Iteration 24, loss = 0.03429952
Iteration 25, loss = 0.03334758
Iteration 26, loss = 0.03257549
Iteration 27, loss = 0.03154366
Iteration 28, loss = 0.03094715
Iteration 29, loss = 0.03030580
Iteration 30, loss = 0.02981552
Iteration 31, loss = 0.02906921
Iteration 32, loss = 0.02890472
Iteration 33, loss = 0.02837295
Iteration 34, loss = 0.02813547
Iteration 35, loss = 0.02777149
Iteration 36, loss = 0.02752379
Iteration 37, loss = 0.02712983
Iteration 38, loss = 0.02714626
Iteration 39, loss = 0.02653452
Iteration 40, loss = 0.02650338
Iteration 41, loss = 0.02636578
Iteration 42, loss = 0.02595963
Iteration 43, loss = 0.02589087
Iteration 44, loss = 0.02594832
Iteration 45, loss = 0.02569228
Iteration 46, loss = 0.02541004
Iteration 47, loss = 0.02516694
Iteration 48, loss = 0.02528681
Iteration 49, loss = 0.02507930
Iteration 50, loss = 0.02510232
Iteration 51, loss = 0.02492347
Iteration 52, loss = 0.02477045
Iteration 53, loss = 0.02497960
Iteration 54, loss = 0.02489614
Iteration 55, loss = 0.02474110
Iteration 56, loss = 0.02460002
Iteration 57, loss = 0.02438635
Iteration 58, loss = 0.02441642
Iteration 59, loss = 0.02414729
Iteration 60, loss = 0.02436808
Iteration 61, loss = 0.02428036
Iteration 62, loss = 0.02432799
Iteration 63, loss = 0.02418779
Iteration 64, loss = 0.02392510
Iteration 65, loss = 0.02402328
Iteration 66, loss = 0.02454641
Iteration 67, loss = 0.02400546
Iteration 68, loss = 0.02409480
Iteration 69, loss = 0.02404104
Iteration 70, loss = 0.02372989
Iteration 71, loss = 0.02381738
Iteration 72, loss = 0.02384184
Iteration 73, loss = 0.02375181
Iteration 74, loss = 0.02369178
Iteration 75, loss = 0.02366691
Iteration 76, loss = 0.02377722
Iteration 77, loss = 0.02372754
Iteration 78, loss = 0.02358414
Iteration 79, loss = 0.02344950
Iteration 80, loss = 0.02348967
Iteration 81, loss = 0.02327811
Iteration 82, loss = 0.02348244
Iteration 83, loss = 0.02341560
Iteration 84, loss = 0.02359075
Iteration 85, loss = 0.02349291
Iteration 86, loss = 0.02321645
Iteration 87, loss = 0.02347603
Iteration 88, loss = 0.02366358
Iteration 89, loss = 0.02360565
Iteration 90, loss = 0.02333575
Iteration 91, loss = 0.02327762
Iteration 92, loss = 0.02313646
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67336023
Iteration 2, loss = 0.60326762
Iteration 3, loss = 0.50033480
Iteration 4, loss = 0.38849110
Iteration 5, loss = 0.29195688
Iteration 6, loss = 0.21971345
Iteration 7, loss = 0.16862650
Iteration 8, loss = 0.13333278
Iteration 9, loss = 0.10854156
Iteration 10, loss = 0.09109623
Iteration 11, loss = 0.07816499
Iteration 12, loss = 0.06853481
Iteration 13, loss = 0.06125950
Iteration 14, loss = 0.05529524
Iteration 15, loss = 0.05052465
Iteration 16, loss = 0.04663874
Iteration 17, loss = 0.04356343
Iteration 18, loss = 0.04084519
Iteration 19, loss = 0.03884169
Iteration 20, loss = 0.03684128
Iteration 21, loss = 0.03514480
Iteration 22, loss = 0.03398007
Iteration 23, loss = 0.03271559
Iteration 24, loss = 0.03157734
Iteration 25, loss = 0.03065007
Iteration 26, loss = 0.02989402
Iteration 27, loss = 0.02932604
Iteration 28, loss = 0.02830735
Iteration 29, loss = 0.02792861
Iteration 30, loss = 0.02740400
Iteration 31, loss = 0.02686504
Iteration 32, loss = 0.02629210
Iteration 33, loss = 0.02623176
Iteration 34, loss = 0.02553322
Iteration 35, loss = 0.02539740
Iteration 36, loss = 0.02503904
Iteration 37, loss = 0.02487270
Iteration 38, loss = 0.02460529
Iteration 39, loss = 0.02466000
Iteration 40, loss = 0.02417959
Iteration 41, loss = 0.02404546
Iteration 42, loss = 0.02409100
Iteration 43, loss = 0.02368799
Iteration 44, loss = 0.02346607
Iteration 45, loss = 0.02343507
Iteration 46, loss = 0.02327598
Iteration 47, loss = 0.02369184
Iteration 48, loss = 0.02268123
Iteration 49, loss = 0.02293680
Iteration 50, loss = 0.02287328
Iteration 51, loss = 0.02290706
Iteration 52, loss = 0.02241469
Iteration 53, loss = 0.02266101
Iteration 54, loss = 0.02249233
Iteration 55, loss = 0.02257031
Iteration 56, loss = 0.02229821
Iteration 57, loss = 0.02252794
Iteration 58, loss = 0.02209017
Iteration 59, loss = 0.02200653
Iteration 60, loss = 0.02219748
Iteration 61, loss = 0.02206366
Iteration 62, loss = 0.02184082
Iteration 63, loss = 0.02212086
Iteration 64, loss = 0.02184242
Iteration 65, loss = 0.02161812
Iteration 66, loss = 0.02174481
Iteration 67, loss = 0.02183570
Iteration 68, loss = 0.02182486
Iteration 69, loss = 0.02131701
Iteration 70, loss = 0.02145553
Iteration 71, loss = 0.02174691
Iteration 72, loss = 0.02146873
Iteration 73, loss = 0.02153997
Iteration 74, loss = 0.02155473
Iteration 75, loss = 0.02147554
Iteration 76, loss = 0.02181722
Iteration 77, loss = 0.02154031
Iteration 78, loss = 0.02158040
Iteration 79, loss = 0.02127322
Iteration 80, loss = 0.02115266
Iteration 81, loss = 0.02133751
Iteration 82, loss = 0.02142379
Iteration 83, loss = 0.02112255
Iteration 84, loss = 0.02168214
Iteration 85, loss = 0.02112461
Iteration 86, loss = 0.02153264
Iteration 87, loss = 0.02113019
Iteration 88, loss = 0.02131679
Iteration 89, loss = 0.02109793
Iteration 90, loss = 0.02107114
Iteration 91, loss = 0.02140909
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71173438
Iteration 2, loss = 0.62371794
Iteration 3, loss = 0.51437526
Iteration 4, loss = 0.39867635
Iteration 5, loss = 0.29982270
Iteration 6, loss = 0.22583461
Iteration 7, loss = 0.17405112
Iteration 8, loss = 0.13859592
Iteration 9, loss = 0.11402108
Iteration 10, loss = 0.09591331
Iteration 11, loss = 0.08323103
Iteration 12, loss = 0.07348901
Iteration 13, loss = 0.06596774
Iteration 14, loss = 0.06021603
Iteration 15, loss = 0.05572713
Iteration 16, loss = 0.05191118
Iteration 17, loss = 0.04855080
Iteration 18, loss = 0.04611921
Iteration 19, loss = 0.04366639
Iteration 20, loss = 0.04185631
Iteration 21, loss = 0.04022809
Iteration 22, loss = 0.03884037
Iteration 23, loss = 0.03766765
Iteration 24, loss = 0.03674703
Iteration 25, loss = 0.03566250
Iteration 26, loss = 0.03513797
Iteration 27, loss = 0.03400024
Iteration 28, loss = 0.03353904
Iteration 29, loss = 0.03296099
Iteration 30, loss = 0.03276457
Iteration 31, loss = 0.03238254
Iteration 32, loss = 0.03143090
Iteration 33, loss = 0.03124840
Iteration 34, loss = 0.03060127
Iteration 35, loss = 0.03062002
Iteration 36, loss = 0.03015941
Iteration 37, loss = 0.03027893
Iteration 38, loss = 0.02995055
Iteration 39, loss = 0.02955521
Iteration 40, loss = 0.02930258
Iteration 41, loss = 0.02916721
Iteration 42, loss = 0.02872385
Iteration 43, loss = 0.02904335
Iteration 44, loss = 0.02883335
Iteration 45, loss = 0.02854688
Iteration 46, loss = 0.02831571
Iteration 47, loss = 0.02827445
Iteration 48, loss = 0.02812074
Iteration 49, loss = 0.02823475
Iteration 50, loss = 0.02834312
Iteration 51, loss = 0.02767853
Iteration 52, loss = 0.02780060
Iteration 53, loss = 0.02777575
Iteration 54, loss = 0.02773998
Iteration 55, loss = 0.02734665
Iteration 56, loss = 0.02757145
Iteration 57, loss = 0.02725732
Iteration 58, loss = 0.02740797
Iteration 59, loss = 0.02699385
Iteration 60, loss = 0.02727797
Iteration 61, loss = 0.02698477
Iteration 62, loss = 0.02706085
Iteration 63, loss = 0.02715931
Iteration 64, loss = 0.02707959
Iteration 65, loss = 0.02692748
Iteration 66, loss = 0.02693664
Iteration 67, loss = 0.02691465
Iteration 68, loss = 0.02696375
Iteration 69, loss = 0.02691853
Iteration 70, loss = 0.02716390
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69647361
Iteration 2, loss = 0.61748513
Iteration 3, loss = 0.50940997
Iteration 4, loss = 0.39659491
Iteration 5, loss = 0.30124975
Iteration 6, loss = 0.22731689
Iteration 7, loss = 0.17446046
Iteration 8, loss = 0.13798917
Iteration 9, loss = 0.11228805
Iteration 10, loss = 0.09433176
Iteration 11, loss = 0.08095944
Iteration 12, loss = 0.07122409
Iteration 13, loss = 0.06380918
Iteration 14, loss = 0.05782791
Iteration 15, loss = 0.05293448
Iteration 16, loss = 0.04943559
Iteration 17, loss = 0.04620564
Iteration 18, loss = 0.04359391
Iteration 19, loss = 0.04148611
Iteration 20, loss = 0.03969933
Iteration 21, loss = 0.03817111
Iteration 22, loss = 0.03660920
Iteration 23, loss = 0.03529180
Iteration 24, loss = 0.03431518
Iteration 25, loss = 0.03395600
Iteration 26, loss = 0.03271706
Iteration 27, loss = 0.03221255
Iteration 28, loss = 0.03157131
Iteration 29, loss = 0.03085937
Iteration 30, loss = 0.03045828
Iteration 31, loss = 0.03018026
Iteration 32, loss = 0.02948170
Iteration 33, loss = 0.02946583
Iteration 34, loss = 0.02898802
Iteration 35, loss = 0.02853536
Iteration 36, loss = 0.02846909
Iteration 37, loss = 0.02805303
Iteration 38, loss = 0.02787013
Iteration 39, loss = 0.02761038
Iteration 40, loss = 0.02741846
Iteration 41, loss = 0.02741903
Iteration 42, loss = 0.02704674
Iteration 43, loss = 0.02672233
Iteration 44, loss = 0.02688766
Iteration 45, loss = 0.02636563
Iteration 46, loss = 0.02648648
Iteration 47, loss = 0.02649657
Iteration 48, loss = 0.02621448
Iteration 49, loss = 0.02647154
Iteration 50, loss = 0.02612408
Iteration 51, loss = 0.02632738
Iteration 52, loss = 0.02610570
Iteration 53, loss = 0.02582703
Iteration 54, loss = 0.02573287
Iteration 55, loss = 0.02591219
Iteration 56, loss = 0.02559795
Iteration 57, loss = 0.02560995
Iteration 58, loss = 0.02535720
Iteration 59, loss = 0.02555682
Iteration 60, loss = 0.02555261
Iteration 61, loss = 0.02529000
Iteration 62, loss = 0.02539583
Iteration 63, loss = 0.02518244
Iteration 64, loss = 0.02518717
Iteration 65, loss = 0.02508248
Iteration 66, loss = 0.02536968
Iteration 67, loss = 0.02503323
Iteration 68, loss = 0.02516941
Iteration 69, loss = 0.02493311
Iteration 70, loss = 0.02505644
Iteration 71, loss = 0.02518644
Iteration 72, loss = 0.02494164
Iteration 73, loss = 0.02474553
Iteration 74, loss = 0.02516208
Iteration 75, loss = 0.02483411
Iteration 76, loss = 0.02508898
Iteration 77, loss = 0.02468447
Iteration 78, loss = 0.02493985
Iteration 79, loss = 0.02476292
Iteration 80, loss = 0.02487576
Iteration 81, loss = 0.02481896
Iteration 82, loss = 0.02482807
Iteration 83, loss = 0.02473300
Iteration 84, loss = 0.02450908
Iteration 85, loss = 0.02456019
Iteration 86, loss = 0.02476754
Iteration 87, loss = 0.02456780
Iteration 88, loss = 0.02448711
Iteration 89, loss = 0.02489782
Iteration 90, loss = 0.02445558
Iteration 91, loss = 0.02462377
Iteration 92, loss = 0.02451233
Iteration 93, loss = 0.02461082
Iteration 94, loss = 0.02459018
Iteration 95, loss = 0.02479830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70393776
Iteration 2, loss = 0.62907738
Iteration 3, loss = 0.51816283
Iteration 4, loss = 0.39857074
Iteration 5, loss = 0.29919314
Iteration 6, loss = 0.22503505
Iteration 7, loss = 0.17324714
Iteration 8, loss = 0.13738718
Iteration 9, loss = 0.11250095
Iteration 10, loss = 0.09503745
Iteration 11, loss = 0.08191342
Iteration 12, loss = 0.07261644
Iteration 13, loss = 0.06500632
Iteration 14, loss = 0.05922121
Iteration 15, loss = 0.05452259
Iteration 16, loss = 0.05087606
Iteration 17, loss = 0.04800358
Iteration 18, loss = 0.04533397
Iteration 19, loss = 0.04311764
Iteration 20, loss = 0.04149621
Iteration 21, loss = 0.03980834
Iteration 22, loss = 0.03865304
Iteration 23, loss = 0.03750935
Iteration 24, loss = 0.03605692
Iteration 25, loss = 0.03549678
Iteration 26, loss = 0.03462696
Iteration 27, loss = 0.03395803
Iteration 28, loss = 0.03359752
Iteration 29, loss = 0.03297150
Iteration 30, loss = 0.03238308
Iteration 31, loss = 0.03194150
Iteration 32, loss = 0.03162035
Iteration 33, loss = 0.03100792
Iteration 34, loss = 0.03086787
Iteration 35, loss = 0.03073347
Iteration 36, loss = 0.03020784
Iteration 37, loss = 0.02996116
Iteration 38, loss = 0.02990188
Iteration 39, loss = 0.02966894
Iteration 40, loss = 0.02941451
Iteration 41, loss = 0.02919324
Iteration 42, loss = 0.02908330
Iteration 43, loss = 0.02910178
Iteration 44, loss = 0.02854724
Iteration 45, loss = 0.02890075
Iteration 46, loss = 0.02849706
Iteration 47, loss = 0.02842010
Iteration 48, loss = 0.02835584
Iteration 49, loss = 0.02807547
Iteration 50, loss = 0.02836795
Iteration 51, loss = 0.02814169
Iteration 52, loss = 0.02809502
Iteration 53, loss = 0.02782602
Iteration 54, loss = 0.02781832
Iteration 55, loss = 0.02780211
Iteration 56, loss = 0.02773356
Iteration 57, loss = 0.02811542
Iteration 58, loss = 0.02734964
Iteration 59, loss = 0.02756471
Iteration 60, loss = 0.02764567
Iteration 61, loss = 0.02757725
Iteration 62, loss = 0.02742625
Iteration 63, loss = 0.02718596
Iteration 64, loss = 0.02733337
Iteration 65, loss = 0.02724558
Iteration 66, loss = 0.02760567
Iteration 67, loss = 0.02720137
Iteration 68, loss = 0.02722499
Iteration 69, loss = 0.02747859
Iteration 70, loss = 0.02691664
Iteration 71, loss = 0.02684947
Iteration 72, loss = 0.02715414
Iteration 73, loss = 0.02712474
Iteration 74, loss = 0.02707320
Iteration 75, loss = 0.02692289
Iteration 76, loss = 0.02669649
Iteration 77, loss = 0.02693585
Iteration 78, loss = 0.02670577
Iteration 79, loss = 0.02693392
Iteration 80, loss = 0.02690050
Iteration 81, loss = 0.02676742
Iteration 82, loss = 0.02686124
Iteration 83, loss = 0.02701414
Iteration 84, loss = 0.02665174
Iteration 85, loss = 0.02651486
Iteration 86, loss = 0.02668587
Iteration 87, loss = 0.02661350
Iteration 88, loss = 0.02675176
Iteration 89, loss = 0.02676560
Iteration 90, loss = 0.02674057
Iteration 91, loss = 0.02660266
Iteration 92, loss = 0.02647604
Iteration 93, loss = 0.02642691
Iteration 94, loss = 0.02663227
Iteration 95, loss = 0.02664859
Iteration 96, loss = 0.02673878
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67635298
Iteration 2, loss = 0.61186943
Iteration 3, loss = 0.50486471
Iteration 4, loss = 0.39588679
Iteration 5, loss = 0.30077257
Iteration 6, loss = 0.22823763
Iteration 7, loss = 0.17590938
Iteration 8, loss = 0.13945401
Iteration 9, loss = 0.11411005
Iteration 10, loss = 0.09597307
Iteration 11, loss = 0.08258171
Iteration 12, loss = 0.07227932
Iteration 13, loss = 0.06463120
Iteration 14, loss = 0.05868464
Iteration 15, loss = 0.05362018
Iteration 16, loss = 0.04973026
Iteration 17, loss = 0.04649052
Iteration 18, loss = 0.04387191
Iteration 19, loss = 0.04150494
Iteration 20, loss = 0.03964647
Iteration 21, loss = 0.03799972
Iteration 22, loss = 0.03656977
Iteration 23, loss = 0.03522808
Iteration 24, loss = 0.03429952
Iteration 25, loss = 0.03334758
Iteration 26, loss = 0.03257549
Iteration 27, loss = 0.03154366
Iteration 28, loss = 0.03094715
Iteration 29, loss = 0.03030580
Iteration 30, loss = 0.02981552
Iteration 31, loss = 0.02906921
Iteration 32, loss = 0.02890472
Iteration 33, loss = 0.02837295
Iteration 34, loss = 0.02813547
Iteration 35, loss = 0.02777149
Iteration 36, loss = 0.02752379
Iteration 37, loss = 0.02712983
Iteration 38, loss = 0.02714626
Iteration 39, loss = 0.02653452
Iteration 40, loss = 0.02650338
Iteration 41, loss = 0.02636578
Iteration 42, loss = 0.02595963
Iteration 43, loss = 0.02589087
Iteration 44, loss = 0.02594832
Iteration 45, loss = 0.02569228
Iteration 46, loss = 0.02541004
Iteration 47, loss = 0.02516694
Iteration 48, loss = 0.02528681
Iteration 49, loss = 0.02507930
Iteration 50, loss = 0.02510232
Iteration 51, loss = 0.02492347
Iteration 52, loss = 0.02477045
Iteration 53, loss = 0.02497960
Iteration 54, loss = 0.02489614
Iteration 55, loss = 0.02474110
Iteration 56, loss = 0.02460002
Iteration 57, loss = 0.02438635
Iteration 58, loss = 0.02441642
Iteration 59, loss = 0.02414729
Iteration 60, loss = 0.02436808
Iteration 61, loss = 0.02428036
Iteration 62, loss = 0.02432799
Iteration 63, loss = 0.02418779
Iteration 64, loss = 0.02392510
Iteration 65, loss = 0.02402328
Iteration 66, loss = 0.02454641
Iteration 67, loss = 0.02400546
Iteration 68, loss = 0.02409480
Iteration 69, loss = 0.02404104
Iteration 70, loss = 0.02372989
Iteration 71, loss = 0.02381738
Iteration 72, loss = 0.02384184
Iteration 73, loss = 0.02375181
Iteration 74, loss = 0.02369178
Iteration 75, loss = 0.02366691
Iteration 76, loss = 0.02377722
Iteration 77, loss = 0.02372754
Iteration 78, loss = 0.02358414
Iteration 79, loss = 0.02344950
Iteration 80, loss = 0.02348967
Iteration 81, loss = 0.02327811
Iteration 82, loss = 0.02348244
Iteration 83, loss = 0.02341560
Iteration 84, loss = 0.02359075
Iteration 85, loss = 0.02349291
Iteration 86, loss = 0.02321645
Iteration 87, loss = 0.02347603
Iteration 88, loss = 0.02366358
Iteration 89, loss = 0.02360565
Iteration 90, loss = 0.02333575
Iteration 91, loss = 0.02327762
Iteration 92, loss = 0.02313646
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67336023
Iteration 2, loss = 0.60326762
Iteration 3, loss = 0.50033480
Iteration 4, loss = 0.38849110
Iteration 5, loss = 0.29195688
Iteration 6, loss = 0.21971345
Iteration 7, loss = 0.16862650
Iteration 8, loss = 0.13333278
Iteration 9, loss = 0.10854156
Iteration 10, loss = 0.09109623
Iteration 11, loss = 0.07816499
Iteration 12, loss = 0.06853481
Iteration 13, loss = 0.06125950
Iteration 14, loss = 0.05529524
Iteration 15, loss = 0.05052465
Iteration 16, loss = 0.04663874
Iteration 17, loss = 0.04356343
Iteration 18, loss = 0.04084519
Iteration 19, loss = 0.03884169
Iteration 20, loss = 0.03684128
Iteration 21, loss = 0.03514480
Iteration 22, loss = 0.03398007
Iteration 23, loss = 0.03271559
Iteration 24, loss = 0.03157734
Iteration 25, loss = 0.03065007
Iteration 26, loss = 0.02989402
Iteration 27, loss = 0.02932604
Iteration 28, loss = 0.02830735
Iteration 29, loss = 0.02792861
Iteration 30, loss = 0.02740400
Iteration 31, loss = 0.02686504
Iteration 32, loss = 0.02629210
Iteration 33, loss = 0.02623176
Iteration 34, loss = 0.02553322
Iteration 35, loss = 0.02539740
Iteration 36, loss = 0.02503904
Iteration 37, loss = 0.02487270
Iteration 38, loss = 0.02460529
Iteration 39, loss = 0.02466000
Iteration 40, loss = 0.02417959
Iteration 41, loss = 0.02404546
Iteration 42, loss = 0.02409100
Iteration 43, loss = 0.02368799
Iteration 44, loss = 0.02346607
Iteration 45, loss = 0.02343507
Iteration 46, loss = 0.02327598
Iteration 47, loss = 0.02369184
Iteration 48, loss = 0.02268123
Iteration 49, loss = 0.02293680
Iteration 50, loss = 0.02287328
Iteration 51, loss = 0.02290706
Iteration 52, loss = 0.02241469
Iteration 53, loss = 0.02266101
Iteration 54, loss = 0.02249233
Iteration 55, loss = 0.02257031
Iteration 56, loss = 0.02229821
Iteration 57, loss = 0.02252794
Iteration 58, loss = 0.02209017
Iteration 59, loss = 0.02200653
Iteration 60, loss = 0.02219748
Iteration 61, loss = 0.02206366
Iteration 62, loss = 0.02184082
Iteration 63, loss = 0.02212086
Iteration 64, loss = 0.02184242
Iteration 65, loss = 0.02161812
Iteration 66, loss = 0.02174481
Iteration 67, loss = 0.02183570
Iteration 68, loss = 0.02182486
Iteration 69, loss = 0.02131701
Iteration 70, loss = 0.02145553
Iteration 71, loss = 0.02174691
Iteration 72, loss = 0.02146873
Iteration 73, loss = 0.02153997
Iteration 74, loss = 0.02155473
Iteration 75, loss = 0.02147554
Iteration 76, loss = 0.02181722
Iteration 77, loss = 0.02154031
Iteration 78, loss = 0.02158040
Iteration 79, loss = 0.02127322
Iteration 80, loss = 0.02115266
Iteration 81, loss = 0.02133751
Iteration 82, loss = 0.02142379
Iteration 83, loss = 0.02112255
Iteration 84, loss = 0.02168214
Iteration 85, loss = 0.02112461
Iteration 86, loss = 0.02153264
Iteration 87, loss = 0.02113019
Iteration 88, loss = 0.02131679
Iteration 89, loss = 0.02109793
Iteration 90, loss = 0.02107114
Iteration 91, loss = 0.02140909
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71173438
Iteration 2, loss = 0.62371794
Iteration 3, loss = 0.51437526
Iteration 4, loss = 0.39867635
Iteration 5, loss = 0.29982270
Iteration 6, loss = 0.22583461
Iteration 7, loss = 0.17405112
Iteration 8, loss = 0.13859592
Iteration 9, loss = 0.11402108
Iteration 10, loss = 0.09591331
Iteration 11, loss = 0.08323103
Iteration 12, loss = 0.07348901
Iteration 13, loss = 0.06596774
Iteration 14, loss = 0.06021603
Iteration 15, loss = 0.05572713
Iteration 16, loss = 0.05191118
Iteration 17, loss = 0.04855080
Iteration 18, loss = 0.04611921
Iteration 19, loss = 0.04366639
Iteration 20, loss = 0.04185631
Iteration 21, loss = 0.04022809
Iteration 22, loss = 0.03884037
Iteration 23, loss = 0.03766765
Iteration 24, loss = 0.03674703
Iteration 25, loss = 0.03566250
Iteration 26, loss = 0.03513797
Iteration 27, loss = 0.03400024
Iteration 28, loss = 0.03353904
Iteration 29, loss = 0.03296099
Iteration 30, loss = 0.03276457
Iteration 31, loss = 0.03238254
Iteration 32, loss = 0.03143090
Iteration 33, loss = 0.03124840
Iteration 34, loss = 0.03060127
Iteration 35, loss = 0.03062002
Iteration 36, loss = 0.03015941
Iteration 37, loss = 0.03027893
Iteration 38, loss = 0.02995055
Iteration 39, loss = 0.02955521
Iteration 40, loss = 0.02930258
Iteration 41, loss = 0.02916721
Iteration 42, loss = 0.02872385
Iteration 43, loss = 0.02904335
Iteration 44, loss = 0.02883335
Iteration 45, loss = 0.02854688
Iteration 46, loss = 0.02831571
Iteration 47, loss = 0.02827445
Iteration 48, loss = 0.02812074
Iteration 49, loss = 0.02823475
Iteration 50, loss = 0.02834312
Iteration 51, loss = 0.02767853
Iteration 52, loss = 0.02780060
Iteration 53, loss = 0.02777575
Iteration 54, loss = 0.02773998
Iteration 55, loss = 0.02734665
Iteration 56, loss = 0.02757145
Iteration 57, loss = 0.02725732
Iteration 58, loss = 0.02740797
Iteration 59, loss = 0.02699385
Iteration 60, loss = 0.02727797
Iteration 61, loss = 0.02698477
Iteration 62, loss = 0.02706085
Iteration 63, loss = 0.02715931
Iteration 64, loss = 0.02707959
Iteration 65, loss = 0.02692748
Iteration 66, loss = 0.02693664
Iteration 67, loss = 0.02691465
Iteration 68, loss = 0.02696375
Iteration 69, loss = 0.02691853
Iteration 70, loss = 0.02716390
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69647361
Iteration 2, loss = 0.61748513
Iteration 3, loss = 0.50940997
Iteration 4, loss = 0.39659491
Iteration 5, loss = 0.30124975
Iteration 6, loss = 0.22731689
Iteration 7, loss = 0.17446046
Iteration 8, loss = 0.13798917
Iteration 9, loss = 0.11228805
Iteration 10, loss = 0.09433176
Iteration 11, loss = 0.08095944
Iteration 12, loss = 0.07122409
Iteration 13, loss = 0.06380918
Iteration 14, loss = 0.05782791
Iteration 15, loss = 0.05293448
Iteration 16, loss = 0.04943559
Iteration 17, loss = 0.04620564
Iteration 18, loss = 0.04359391
Iteration 19, loss = 0.04148611
Iteration 20, loss = 0.03969933
Iteration 21, loss = 0.03817111
Iteration 22, loss = 0.03660920
Iteration 23, loss = 0.03529180
Iteration 24, loss = 0.03431518
Iteration 25, loss = 0.03395600
Iteration 26, loss = 0.03271706
Iteration 27, loss = 0.03221255
Iteration 28, loss = 0.03157131
Iteration 29, loss = 0.03085937
Iteration 30, loss = 0.03045828
Iteration 31, loss = 0.03018026
Iteration 32, loss = 0.02948170
Iteration 33, loss = 0.02946583
Iteration 34, loss = 0.02898802
Iteration 35, loss = 0.02853536
Iteration 36, loss = 0.02846909
Iteration 37, loss = 0.02805303
Iteration 38, loss = 0.02787013
Iteration 39, loss = 0.02761038
Iteration 40, loss = 0.02741846
Iteration 41, loss = 0.02741903
Iteration 42, loss = 0.02704674
Iteration 43, loss = 0.02672233
Iteration 44, loss = 0.02688766
Iteration 45, loss = 0.02636563
Iteration 46, loss = 0.02648648
Iteration 47, loss = 0.02649657
Iteration 48, loss = 0.02621448
Iteration 49, loss = 0.02647154
Iteration 50, loss = 0.02612408
Iteration 51, loss = 0.02632738
Iteration 52, loss = 0.02610570
Iteration 53, loss = 0.02582703
Iteration 54, loss = 0.02573287
Iteration 55, loss = 0.02591219
Iteration 56, loss = 0.02559795
Iteration 57, loss = 0.02560995
Iteration 58, loss = 0.02535720
Iteration 59, loss = 0.02555682
Iteration 60, loss = 0.02555261
Iteration 61, loss = 0.02529000
Iteration 62, loss = 0.02539583
Iteration 63, loss = 0.02518244
Iteration 64, loss = 0.02518717
Iteration 65, loss = 0.02508248
Iteration 66, loss = 0.02536968
Iteration 67, loss = 0.02503323
Iteration 68, loss = 0.02516941
Iteration 69, loss = 0.02493311
Iteration 70, loss = 0.02505644
Iteration 71, loss = 0.02518644
Iteration 72, loss = 0.02494164
Iteration 73, loss = 0.02474553
Iteration 74, loss = 0.02516208
Iteration 75, loss = 0.02483411
Iteration 76, loss = 0.02508898
Iteration 77, loss = 0.02468447
Iteration 78, loss = 0.02493985
Iteration 79, loss = 0.02476292
Iteration 80, loss = 0.02487576
Iteration 81, loss = 0.02481896
Iteration 82, loss = 0.02482807
Iteration 83, loss = 0.02473300
Iteration 84, loss = 0.02450908
Iteration 85, loss = 0.02456019
Iteration 86, loss = 0.02476754
Iteration 87, loss = 0.02456780
Iteration 88, loss = 0.02448711
Iteration 89, loss = 0.02489782
Iteration 90, loss = 0.02445558
Iteration 91, loss = 0.02462377
Iteration 92, loss = 0.02451233
Iteration 93, loss = 0.02461082
Iteration 94, loss = 0.02459018
Iteration 95, loss = 0.02479830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70393776
Iteration 2, loss = 0.62907738
Iteration 3, loss = 0.51816283
Iteration 4, loss = 0.39857074
Iteration 5, loss = 0.29919314
Iteration 6, loss = 0.22503505
Iteration 7, loss = 0.17324714
Iteration 8, loss = 0.13738718
Iteration 9, loss = 0.11250095
Iteration 10, loss = 0.09503745
Iteration 11, loss = 0.08191342
Iteration 12, loss = 0.07261644
Iteration 13, loss = 0.06500632
Iteration 14, loss = 0.05922121
Iteration 15, loss = 0.05452259
Iteration 16, loss = 0.05087606
Iteration 17, loss = 0.04800358
Iteration 18, loss = 0.04533397
Iteration 19, loss = 0.04311764
Iteration 20, loss = 0.04149621
Iteration 21, loss = 0.03980834
Iteration 22, loss = 0.03865304
Iteration 23, loss = 0.03750935
Iteration 24, loss = 0.03605692
Iteration 25, loss = 0.03549678
Iteration 26, loss = 0.03462696
Iteration 27, loss = 0.03395803
Iteration 28, loss = 0.03359752
Iteration 29, loss = 0.03297150
Iteration 30, loss = 0.03238308
Iteration 31, loss = 0.03194150
Iteration 32, loss = 0.03162035
Iteration 33, loss = 0.03100792
Iteration 34, loss = 0.03086787
Iteration 35, loss = 0.03073347
Iteration 36, loss = 0.03020784
Iteration 37, loss = 0.02996116
Iteration 38, loss = 0.02990188
Iteration 39, loss = 0.02966894
Iteration 40, loss = 0.02941451
Iteration 41, loss = 0.02919324
Iteration 42, loss = 0.02908330
Iteration 43, loss = 0.02910178
Iteration 44, loss = 0.02854724
Iteration 45, loss = 0.02890075
Iteration 46, loss = 0.02849706
Iteration 47, loss = 0.02842010
Iteration 48, loss = 0.02835584
Iteration 49, loss = 0.02807547
Iteration 50, loss = 0.02836795
Iteration 51, loss = 0.02814169
Iteration 52, loss = 0.02809502
Iteration 53, loss = 0.02782602
Iteration 54, loss = 0.02781832
Iteration 55, loss = 0.02780211
Iteration 56, loss = 0.02773356
Iteration 57, loss = 0.02811542
Iteration 58, loss = 0.02734964
Iteration 59, loss = 0.02756471
Iteration 60, loss = 0.02764567
Iteration 61, loss = 0.02757725
Iteration 62, loss = 0.02742625
Iteration 63, loss = 0.02718596
Iteration 64, loss = 0.02733337
Iteration 65, loss = 0.02724558
Iteration 66, loss = 0.02760567
Iteration 67, loss = 0.02720137
Iteration 68, loss = 0.02722499
Iteration 69, loss = 0.02747859
Iteration 70, loss = 0.02691664
Iteration 71, loss = 0.02684947
Iteration 72, loss = 0.02715414
Iteration 73, loss = 0.02712474
Iteration 74, loss = 0.02707320
Iteration 75, loss = 0.02692289
Iteration 76, loss = 0.02669649
Iteration 77, loss = 0.02693585
Iteration 78, loss = 0.02670577
Iteration 79, loss = 0.02693392
Iteration 80, loss = 0.02690050
Iteration 81, loss = 0.02676742
Iteration 82, loss = 0.02686124
Iteration 83, loss = 0.02701414
Iteration 84, loss = 0.02665174
Iteration 85, loss = 0.02651486
Iteration 86, loss = 0.02668587
Iteration 87, loss = 0.02661350
Iteration 88, loss = 0.02675176
Iteration 89, loss = 0.02676560
Iteration 90, loss = 0.02674057
Iteration 91, loss = 0.02660266
Iteration 92, loss = 0.02647604
Iteration 93, loss = 0.02642691
Iteration 94, loss = 0.02663227
Iteration 95, loss = 0.02664859
Iteration 96, loss = 0.02673878
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67635298
Iteration 2, loss = 0.61186943
Iteration 3, loss = 0.50486471
Iteration 4, loss = 0.39588679
Iteration 5, loss = 0.30077257
Iteration 6, loss = 0.22823763
Iteration 7, loss = 0.17590938
Iteration 8, loss = 0.13945401
Iteration 9, loss = 0.11411005
Iteration 10, loss = 0.09597307
Iteration 11, loss = 0.08258171
Iteration 12, loss = 0.07227932
Iteration 13, loss = 0.06463120
Iteration 14, loss = 0.05868464
Iteration 15, loss = 0.05362018
Iteration 16, loss = 0.04973026
Iteration 17, loss = 0.04649052
Iteration 18, loss = 0.04387191
Iteration 19, loss = 0.04150494
Iteration 20, loss = 0.03964647
Iteration 21, loss = 0.03799972
Iteration 22, loss = 0.03656977
Iteration 23, loss = 0.03522808
Iteration 24, loss = 0.03429952
Iteration 25, loss = 0.03334758
Iteration 26, loss = 0.03257549
Iteration 27, loss = 0.03154366
Iteration 28, loss = 0.03094715
Iteration 29, loss = 0.03030580
Iteration 30, loss = 0.02981552
Iteration 31, loss = 0.02906921
Iteration 32, loss = 0.02890472
Iteration 33, loss = 0.02837295
Iteration 34, loss = 0.02813547
Iteration 35, loss = 0.02777149
Iteration 36, loss = 0.02752379
Iteration 37, loss = 0.02712983
Iteration 38, loss = 0.02714626
Iteration 39, loss = 0.02653452
Iteration 40, loss = 0.02650338
Iteration 41, loss = 0.02636578
Iteration 42, loss = 0.02595963
Iteration 43, loss = 0.02589087
Iteration 44, loss = 0.02594832
Iteration 45, loss = 0.02569228
Iteration 46, loss = 0.02541004
Iteration 47, loss = 0.02516694
Iteration 48, loss = 0.02528681
Iteration 49, loss = 0.02507930
Iteration 50, loss = 0.02510232
Iteration 51, loss = 0.02492347
Iteration 52, loss = 0.02477045
Iteration 53, loss = 0.02497960
Iteration 54, loss = 0.02489614
Iteration 55, loss = 0.02474110
Iteration 56, loss = 0.02460002
Iteration 57, loss = 0.02438635
Iteration 58, loss = 0.02441642
Iteration 59, loss = 0.02414729
Iteration 60, loss = 0.02436808
Iteration 61, loss = 0.02428036
Iteration 62, loss = 0.02432799
Iteration 63, loss = 0.02418779
Iteration 64, loss = 0.02392510
Iteration 65, loss = 0.02402328
Iteration 66, loss = 0.02454641
Iteration 67, loss = 0.02400546
Iteration 68, loss = 0.02409480
Iteration 69, loss = 0.02404104
Iteration 70, loss = 0.02372989
Iteration 71, loss = 0.02381738
Iteration 72, loss = 0.02384184
Iteration 73, loss = 0.02375181
Iteration 74, loss = 0.02369178
Iteration 75, loss = 0.02366691
Iteration 76, loss = 0.02377722
Iteration 77, loss = 0.02372754
Iteration 78, loss = 0.02358414
Iteration 79, loss = 0.02344950
Iteration 80, loss = 0.02348967
Iteration 81, loss = 0.02327811
Iteration 82, loss = 0.02348244
Iteration 83, loss = 0.02341560
Iteration 84, loss = 0.02359075
Iteration 85, loss = 0.02349291
Iteration 86, loss = 0.02321645
Iteration 87, loss = 0.02347603
Iteration 88, loss = 0.02366358
Iteration 89, loss = 0.02360565
Iteration 90, loss = 0.02333575
Iteration 91, loss = 0.02327762
Iteration 92, loss = 0.02313646
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67336023
Iteration 2, loss = 0.60326762
Iteration 3, loss = 0.50033480
Iteration 4, loss = 0.38849110
Iteration 5, loss = 0.29195688
Iteration 6, loss = 0.21971345
Iteration 7, loss = 0.16862650
Iteration 8, loss = 0.13333278
Iteration 9, loss = 0.10854156
Iteration 10, loss = 0.09109623
Iteration 11, loss = 0.07816499
Iteration 12, loss = 0.06853481
Iteration 13, loss = 0.06125950
Iteration 14, loss = 0.05529524
Iteration 15, loss = 0.05052465
Iteration 16, loss = 0.04663874
Iteration 17, loss = 0.04356343
Iteration 18, loss = 0.04084519
Iteration 19, loss = 0.03884169
Iteration 20, loss = 0.03684128
Iteration 21, loss = 0.03514480
Iteration 22, loss = 0.03398007
Iteration 23, loss = 0.03271559
Iteration 24, loss = 0.03157734
Iteration 25, loss = 0.03065007
Iteration 26, loss = 0.02989402
Iteration 27, loss = 0.02932604
Iteration 28, loss = 0.02830735
Iteration 29, loss = 0.02792861
Iteration 30, loss = 0.02740400
Iteration 31, loss = 0.02686504
Iteration 32, loss = 0.02629210
Iteration 33, loss = 0.02623176
Iteration 34, loss = 0.02553322
Iteration 35, loss = 0.02539740
Iteration 36, loss = 0.02503904
Iteration 37, loss = 0.02487270
Iteration 38, loss = 0.02460529
Iteration 39, loss = 0.02466000
Iteration 40, loss = 0.02417959
Iteration 41, loss = 0.02404546
Iteration 42, loss = 0.02409100
Iteration 43, loss = 0.02368799
Iteration 44, loss = 0.02346607
Iteration 45, loss = 0.02343507
Iteration 46, loss = 0.02327598
Iteration 47, loss = 0.02369184
Iteration 48, loss = 0.02268123
Iteration 49, loss = 0.02293680
Iteration 50, loss = 0.02287328
Iteration 51, loss = 0.02290706
Iteration 52, loss = 0.02241469
Iteration 53, loss = 0.02266101
Iteration 54, loss = 0.02249233
Iteration 55, loss = 0.02257031
Iteration 56, loss = 0.02229821
Iteration 57, loss = 0.02252794
Iteration 58, loss = 0.02209017
Iteration 59, loss = 0.02200653
Iteration 60, loss = 0.02219748
Iteration 61, loss = 0.02206366
Iteration 62, loss = 0.02184082
Iteration 63, loss = 0.02212086
Iteration 64, loss = 0.02184242
Iteration 65, loss = 0.02161812
Iteration 66, loss = 0.02174481
Iteration 67, loss = 0.02183570
Iteration 68, loss = 0.02182486
Iteration 69, loss = 0.02131701
Iteration 70, loss = 0.02145553
Iteration 71, loss = 0.02174691
Iteration 72, loss = 0.02146873
Iteration 73, loss = 0.02153997
Iteration 74, loss = 0.02155473
Iteration 75, loss = 0.02147554
Iteration 76, loss = 0.02181722
Iteration 77, loss = 0.02154031
Iteration 78, loss = 0.02158040
Iteration 79, loss = 0.02127322
Iteration 80, loss = 0.02115266
Iteration 81, loss = 0.02133751
Iteration 82, loss = 0.02142379
Iteration 83, loss = 0.02112255
Iteration 84, loss = 0.02168214
Iteration 85, loss = 0.02112461
Iteration 86, loss = 0.02153264
Iteration 87, loss = 0.02113019
Iteration 88, loss = 0.02131679
Iteration 89, loss = 0.02109793
Iteration 90, loss = 0.02107114
Iteration 91, loss = 0.02140909
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71173438
Iteration 2, loss = 0.62371794
Iteration 3, loss = 0.51437526
Iteration 4, loss = 0.39867635
Iteration 5, loss = 0.29982270
Iteration 6, loss = 0.22583461
Iteration 7, loss = 0.17405112
Iteration 8, loss = 0.13859592
Iteration 9, loss = 0.11402108
Iteration 10, loss = 0.09591331
Iteration 11, loss = 0.08323103
Iteration 12, loss = 0.07348901
Iteration 13, loss = 0.06596774
Iteration 14, loss = 0.06021603
Iteration 15, loss = 0.05572713
Iteration 16, loss = 0.05191118
Iteration 17, loss = 0.04855080
Iteration 18, loss = 0.04611921
Iteration 19, loss = 0.04366639
Iteration 20, loss = 0.04185631
Iteration 21, loss = 0.04022809
Iteration 22, loss = 0.03884037
Iteration 23, loss = 0.03766765
Iteration 24, loss = 0.03674703
Iteration 25, loss = 0.03566250
Iteration 26, loss = 0.03513797
Iteration 27, loss = 0.03400024
Iteration 28, loss = 0.03353904
Iteration 29, loss = 0.03296099
Iteration 30, loss = 0.03276457
Iteration 31, loss = 0.03238254
Iteration 32, loss = 0.03143090
Iteration 33, loss = 0.03124840
Iteration 34, loss = 0.03060127
Iteration 35, loss = 0.03062002
Iteration 36, loss = 0.03015941
Iteration 37, loss = 0.03027893
Iteration 38, loss = 0.02995055
Iteration 39, loss = 0.02955521
Iteration 40, loss = 0.02930258
Iteration 41, loss = 0.02916721
Iteration 42, loss = 0.02872385
Iteration 43, loss = 0.02904335
Iteration 44, loss = 0.02883335
Iteration 45, loss = 0.02854688
Iteration 46, loss = 0.02831571
Iteration 47, loss = 0.02827445
Iteration 48, loss = 0.02812074
Iteration 49, loss = 0.02823475
Iteration 50, loss = 0.02834312
Iteration 51, loss = 0.02767853
Iteration 52, loss = 0.02780060
Iteration 53, loss = 0.02777575
Iteration 54, loss = 0.02773998
Iteration 55, loss = 0.02734665
Iteration 56, loss = 0.02757145
Iteration 57, loss = 0.02725732
Iteration 58, loss = 0.02740797
Iteration 59, loss = 0.02699385
Iteration 60, loss = 0.02727797
Iteration 61, loss = 0.02698477
Iteration 62, loss = 0.02706085
Iteration 63, loss = 0.02715931
Iteration 64, loss = 0.02707959
Iteration 65, loss = 0.02692748
Iteration 66, loss = 0.02693664
Iteration 67, loss = 0.02691465
Iteration 68, loss = 0.02696375
Iteration 69, loss = 0.02691853
Iteration 70, loss = 0.02716390
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69647361
Iteration 2, loss = 0.61748513
Iteration 3, loss = 0.50940997
Iteration 4, loss = 0.39659491
Iteration 5, loss = 0.30124975
Iteration 6, loss = 0.22731689
Iteration 7, loss = 0.17446046
Iteration 8, loss = 0.13798917
Iteration 9, loss = 0.11228805
Iteration 10, loss = 0.09433176
Iteration 11, loss = 0.08095944
Iteration 12, loss = 0.07122409
Iteration 13, loss = 0.06380918
Iteration 14, loss = 0.05782791
Iteration 15, loss = 0.05293448
Iteration 16, loss = 0.04943559
Iteration 17, loss = 0.04620564
Iteration 18, loss = 0.04359391
Iteration 19, loss = 0.04148611
Iteration 20, loss = 0.03969933
Iteration 21, loss = 0.03817111
Iteration 22, loss = 0.03660920
Iteration 23, loss = 0.03529180
Iteration 24, loss = 0.03431518
Iteration 25, loss = 0.03395600
Iteration 26, loss = 0.03271706
Iteration 27, loss = 0.03221255
Iteration 28, loss = 0.03157131
Iteration 29, loss = 0.03085937
Iteration 30, loss = 0.03045828
Iteration 31, loss = 0.03018026
Iteration 32, loss = 0.02948170
Iteration 33, loss = 0.02946583
Iteration 34, loss = 0.02898802
Iteration 35, loss = 0.02853536
Iteration 36, loss = 0.02846909
Iteration 37, loss = 0.02805303
Iteration 38, loss = 0.02787013
Iteration 39, loss = 0.02761038
Iteration 40, loss = 0.02741846
Iteration 41, loss = 0.02741903
Iteration 42, loss = 0.02704674
Iteration 43, loss = 0.02672233
Iteration 44, loss = 0.02688766
Iteration 45, loss = 0.02636563
Iteration 46, loss = 0.02648648
Iteration 47, loss = 0.02649657
Iteration 48, loss = 0.02621448
Iteration 49, loss = 0.02647154
Iteration 50, loss = 0.02612408
Iteration 51, loss = 0.02632738
Iteration 52, loss = 0.02610570
Iteration 53, loss = 0.02582703
Iteration 54, loss = 0.02573287
Iteration 55, loss = 0.02591219
Iteration 56, loss = 0.02559795
Iteration 57, loss = 0.02560995
Iteration 58, loss = 0.02535720
Iteration 59, loss = 0.02555682
Iteration 60, loss = 0.02555261
Iteration 61, loss = 0.02529000
Iteration 62, loss = 0.02539583
Iteration 63, loss = 0.02518244
Iteration 64, loss = 0.02518717
Iteration 65, loss = 0.02508248
Iteration 66, loss = 0.02536968
Iteration 67, loss = 0.02503323
Iteration 68, loss = 0.02516941
Iteration 69, loss = 0.02493311
Iteration 70, loss = 0.02505644
Iteration 71, loss = 0.02518644
Iteration 72, loss = 0.02494164
Iteration 73, loss = 0.02474553
Iteration 74, loss = 0.02516208
Iteration 75, loss = 0.02483411
Iteration 76, loss = 0.02508898
Iteration 77, loss = 0.02468447
Iteration 78, loss = 0.02493985
Iteration 79, loss = 0.02476292
Iteration 80, loss = 0.02487576
Iteration 81, loss = 0.02481896
Iteration 82, loss = 0.02482807
Iteration 83, loss = 0.02473300
Iteration 84, loss = 0.02450908
Iteration 85, loss = 0.02456019
Iteration 86, loss = 0.02476754
Iteration 87, loss = 0.02456780
Iteration 88, loss = 0.02448711
Iteration 89, loss = 0.02489782
Iteration 90, loss = 0.02445558
Iteration 91, loss = 0.02462377
Iteration 92, loss = 0.02451233
Iteration 93, loss = 0.02461082
Iteration 94, loss = 0.02459018
Iteration 95, loss = 0.02479830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70393776
Iteration 2, loss = 0.62907738
Iteration 3, loss = 0.51816283
Iteration 4, loss = 0.39857074
Iteration 5, loss = 0.29919314
Iteration 6, loss = 0.22503505
Iteration 7, loss = 0.17324714
Iteration 8, loss = 0.13738718
Iteration 9, loss = 0.11250095
Iteration 10, loss = 0.09503745
Iteration 11, loss = 0.08191342
Iteration 12, loss = 0.07261644
Iteration 13, loss = 0.06500632
Iteration 14, loss = 0.05922121
Iteration 15, loss = 0.05452259
Iteration 16, loss = 0.05087606
Iteration 17, loss = 0.04800358
Iteration 18, loss = 0.04533397
Iteration 19, loss = 0.04311764
Iteration 20, loss = 0.04149621
Iteration 21, loss = 0.03980834
Iteration 22, loss = 0.03865304
Iteration 23, loss = 0.03750935
Iteration 24, loss = 0.03605692
Iteration 25, loss = 0.03549678
Iteration 26, loss = 0.03462696
Iteration 27, loss = 0.03395803
Iteration 28, loss = 0.03359752
Iteration 29, loss = 0.03297150
Iteration 30, loss = 0.03238308
Iteration 31, loss = 0.03194150
Iteration 32, loss = 0.03162035
Iteration 33, loss = 0.03100792
Iteration 34, loss = 0.03086787
Iteration 35, loss = 0.03073347
Iteration 36, loss = 0.03020784
Iteration 37, loss = 0.02996116
Iteration 38, loss = 0.02990188
Iteration 39, loss = 0.02966894
Iteration 40, loss = 0.02941451
Iteration 41, loss = 0.02919324
Iteration 42, loss = 0.02908330
Iteration 43, loss = 0.02910178
Iteration 44, loss = 0.02854724
Iteration 45, loss = 0.02890075
Iteration 46, loss = 0.02849706
Iteration 47, loss = 0.02842010
Iteration 48, loss = 0.02835584
Iteration 49, loss = 0.02807547
Iteration 50, loss = 0.02836795
Iteration 51, loss = 0.02814169
Iteration 52, loss = 0.02809502
Iteration 53, loss = 0.02782602
Iteration 54, loss = 0.02781832
Iteration 55, loss = 0.02780211
Iteration 56, loss = 0.02773356
Iteration 57, loss = 0.02811542
Iteration 58, loss = 0.02734964
Iteration 59, loss = 0.02756471
Iteration 60, loss = 0.02764567
Iteration 61, loss = 0.02757725
Iteration 62, loss = 0.02742625
Iteration 63, loss = 0.02718596
Iteration 64, loss = 0.02733337
Iteration 65, loss = 0.02724558
Iteration 66, loss = 0.02760567
Iteration 67, loss = 0.02720137
Iteration 68, loss = 0.02722499
Iteration 69, loss = 0.02747859
Iteration 70, loss = 0.02691664
Iteration 71, loss = 0.02684947
Iteration 72, loss = 0.02715414
Iteration 73, loss = 0.02712474
Iteration 74, loss = 0.02707320
Iteration 75, loss = 0.02692289
Iteration 76, loss = 0.02669649
Iteration 77, loss = 0.02693585
Iteration 78, loss = 0.02670577
Iteration 79, loss = 0.02693392
Iteration 80, loss = 0.02690050
Iteration 81, loss = 0.02676742
Iteration 82, loss = 0.02686124
Iteration 83, loss = 0.02701414
Iteration 84, loss = 0.02665174
Iteration 85, loss = 0.02651486
Iteration 86, loss = 0.02668587
Iteration 87, loss = 0.02661350
Iteration 88, loss = 0.02675176
Iteration 89, loss = 0.02676560
Iteration 90, loss = 0.02674057
Iteration 91, loss = 0.02660266
Iteration 92, loss = 0.02647604
Iteration 93, loss = 0.02642691
Iteration 94, loss = 0.02663227
Iteration 95, loss = 0.02664859
Iteration 96, loss = 0.02673878
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67635298
Iteration 2, loss = 0.61186943
Iteration 3, loss = 0.50486471
Iteration 4, loss = 0.39588679
Iteration 5, loss = 0.30077257
Iteration 6, loss = 0.22823763
Iteration 7, loss = 0.17590938
Iteration 8, loss = 0.13945401
Iteration 9, loss = 0.11411005
Iteration 10, loss = 0.09597307
Iteration 11, loss = 0.08258171
Iteration 12, loss = 0.07227932
Iteration 13, loss = 0.06463120
Iteration 14, loss = 0.05868464
Iteration 15, loss = 0.05362018
Iteration 16, loss = 0.04973026
Iteration 17, loss = 0.04649052
Iteration 18, loss = 0.04387191
Iteration 19, loss = 0.04150494
Iteration 20, loss = 0.03964647
Iteration 21, loss = 0.03799972
Iteration 22, loss = 0.03656977
Iteration 23, loss = 0.03522808
Iteration 24, loss = 0.03429952
Iteration 25, loss = 0.03334758
Iteration 26, loss = 0.03257549
Iteration 27, loss = 0.03154366
Iteration 28, loss = 0.03094715
Iteration 29, loss = 0.03030580
Iteration 30, loss = 0.02981552
Iteration 31, loss = 0.02906921
Iteration 32, loss = 0.02890472
Iteration 33, loss = 0.02837295
Iteration 34, loss = 0.02813547
Iteration 35, loss = 0.02777149
Iteration 36, loss = 0.02752379
Iteration 37, loss = 0.02712983
Iteration 38, loss = 0.02714626
Iteration 39, loss = 0.02653452
Iteration 40, loss = 0.02650338
Iteration 41, loss = 0.02636578
Iteration 42, loss = 0.02595963
Iteration 43, loss = 0.02589087
Iteration 44, loss = 0.02594832
Iteration 45, loss = 0.02569228
Iteration 46, loss = 0.02541004
Iteration 47, loss = 0.02516694
Iteration 48, loss = 0.02528681
Iteration 49, loss = 0.02507930
Iteration 50, loss = 0.02510232
Iteration 51, loss = 0.02492347
Iteration 52, loss = 0.02477045
Iteration 53, loss = 0.02497960
Iteration 54, loss = 0.02489614
Iteration 55, loss = 0.02474110
Iteration 56, loss = 0.02460002
Iteration 57, loss = 0.02438635
Iteration 58, loss = 0.02441642
Iteration 59, loss = 0.02414729
Iteration 60, loss = 0.02436808
Iteration 61, loss = 0.02428036
Iteration 62, loss = 0.02432799
Iteration 63, loss = 0.02418779
Iteration 64, loss = 0.02392510
Iteration 65, loss = 0.02402328
Iteration 66, loss = 0.02454641
Iteration 67, loss = 0.02400546
Iteration 68, loss = 0.02409480
Iteration 69, loss = 0.02404104
Iteration 70, loss = 0.02372989
Iteration 71, loss = 0.02381738
Iteration 72, loss = 0.02384184
Iteration 73, loss = 0.02375181
Iteration 74, loss = 0.02369178
Iteration 75, loss = 0.02366691
Iteration 76, loss = 0.02377722
Iteration 77, loss = 0.02372754
Iteration 78, loss = 0.02358414
Iteration 79, loss = 0.02344950
Iteration 80, loss = 0.02348967
Iteration 81, loss = 0.02327811
Iteration 82, loss = 0.02348244
Iteration 83, loss = 0.02341560
Iteration 84, loss = 0.02359075
Iteration 85, loss = 0.02349291
Iteration 86, loss = 0.02321645
Iteration 87, loss = 0.02347603
Iteration 88, loss = 0.02366358
Iteration 89, loss = 0.02360565
Iteration 90, loss = 0.02333575
Iteration 91, loss = 0.02327762
Iteration 92, loss = 0.02313646
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67336023
Iteration 2, loss = 0.60326762
Iteration 3, loss = 0.50033480
Iteration 4, loss = 0.38849110
Iteration 5, loss = 0.29195688
Iteration 6, loss = 0.21971345
Iteration 7, loss = 0.16862650
Iteration 8, loss = 0.13333278
Iteration 9, loss = 0.10854156
Iteration 10, loss = 0.09109623
Iteration 11, loss = 0.07816499
Iteration 12, loss = 0.06853481
Iteration 13, loss = 0.06125950
Iteration 14, loss = 0.05529524
Iteration 15, loss = 0.05052465
Iteration 16, loss = 0.04663874
Iteration 17, loss = 0.04356343
Iteration 18, loss = 0.04084519
Iteration 19, loss = 0.03884169
Iteration 20, loss = 0.03684128
Iteration 21, loss = 0.03514480
Iteration 22, loss = 0.03398007
Iteration 23, loss = 0.03271559
Iteration 24, loss = 0.03157734
Iteration 25, loss = 0.03065007
Iteration 26, loss = 0.02989402
Iteration 27, loss = 0.02932604
Iteration 28, loss = 0.02830735
Iteration 29, loss = 0.02792861
Iteration 30, loss = 0.02740400
Iteration 31, loss = 0.02686504
Iteration 32, loss = 0.02629210
Iteration 33, loss = 0.02623176
Iteration 34, loss = 0.02553322
Iteration 35, loss = 0.02539740
Iteration 36, loss = 0.02503904
Iteration 37, loss = 0.02487270
Iteration 38, loss = 0.02460529
Iteration 39, loss = 0.02466000
Iteration 40, loss = 0.02417959
Iteration 41, loss = 0.02404546
Iteration 42, loss = 0.02409100
Iteration 43, loss = 0.02368799
Iteration 44, loss = 0.02346607
Iteration 45, loss = 0.02343507
Iteration 46, loss = 0.02327598
Iteration 47, loss = 0.02369184
Iteration 48, loss = 0.02268123
Iteration 49, loss = 0.02293680
Iteration 50, loss = 0.02287328
Iteration 51, loss = 0.02290706
Iteration 52, loss = 0.02241469
Iteration 53, loss = 0.02266101
Iteration 54, loss = 0.02249233
Iteration 55, loss = 0.02257031
Iteration 56, loss = 0.02229821
Iteration 57, loss = 0.02252794
Iteration 58, loss = 0.02209017
Iteration 59, loss = 0.02200653
Iteration 60, loss = 0.02219748
Iteration 61, loss = 0.02206366
Iteration 62, loss = 0.02184082
Iteration 63, loss = 0.02212086
Iteration 64, loss = 0.02184242
Iteration 65, loss = 0.02161812
Iteration 66, loss = 0.02174481
Iteration 67, loss = 0.02183570
Iteration 68, loss = 0.02182486
Iteration 69, loss = 0.02131701
Iteration 70, loss = 0.02145553
Iteration 71, loss = 0.02174691
Iteration 72, loss = 0.02146873
Iteration 73, loss = 0.02153997
Iteration 74, loss = 0.02155473
Iteration 75, loss = 0.02147554
Iteration 76, loss = 0.02181722
Iteration 77, loss = 0.02154031
Iteration 78, loss = 0.02158040
Iteration 79, loss = 0.02127322
Iteration 80, loss = 0.02115266
Iteration 81, loss = 0.02133751
Iteration 82, loss = 0.02142379
Iteration 83, loss = 0.02112255
Iteration 84, loss = 0.02168214
Iteration 85, loss = 0.02112461
Iteration 86, loss = 0.02153264
Iteration 87, loss = 0.02113019
Iteration 88, loss = 0.02131679
Iteration 89, loss = 0.02109793
Iteration 90, loss = 0.02107114
Iteration 91, loss = 0.02140909
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71173438
Iteration 2, loss = 0.62371794
Iteration 3, loss = 0.51437526
Iteration 4, loss = 0.39867635
Iteration 5, loss = 0.29982270
Iteration 6, loss = 0.22583461
Iteration 7, loss = 0.17405112
Iteration 8, loss = 0.13859592
Iteration 9, loss = 0.11402108
Iteration 10, loss = 0.09591331
Iteration 11, loss = 0.08323103
Iteration 12, loss = 0.07348901
Iteration 13, loss = 0.06596774
Iteration 14, loss = 0.06021603
Iteration 15, loss = 0.05572713
Iteration 16, loss = 0.05191118
Iteration 17, loss = 0.04855080
Iteration 18, loss = 0.04611921
Iteration 19, loss = 0.04366639
Iteration 20, loss = 0.04185631
Iteration 21, loss = 0.04022809
Iteration 22, loss = 0.03884037
Iteration 23, loss = 0.03766765
Iteration 24, loss = 0.03674703
Iteration 25, loss = 0.03566250
Iteration 26, loss = 0.03513797
Iteration 27, loss = 0.03400024
Iteration 28, loss = 0.03353904
Iteration 29, loss = 0.03296099
Iteration 30, loss = 0.03276457
Iteration 31, loss = 0.03238254
Iteration 32, loss = 0.03143090
Iteration 33, loss = 0.03124840
Iteration 34, loss = 0.03060127
Iteration 35, loss = 0.03062002
Iteration 36, loss = 0.03015941
Iteration 37, loss = 0.03027893
Iteration 38, loss = 0.02995055
Iteration 39, loss = 0.02955521
Iteration 40, loss = 0.02930258
Iteration 41, loss = 0.02916721
Iteration 42, loss = 0.02872385
Iteration 43, loss = 0.02904335
Iteration 44, loss = 0.02883335
Iteration 45, loss = 0.02854688
Iteration 46, loss = 0.02831571
Iteration 47, loss = 0.02827445
Iteration 48, loss = 0.02812074
Iteration 49, loss = 0.02823475
Iteration 50, loss = 0.02834312
Iteration 51, loss = 0.02767853
Iteration 52, loss = 0.02780060
Iteration 53, loss = 0.02777575
Iteration 54, loss = 0.02773998
Iteration 55, loss = 0.02734665
Iteration 56, loss = 0.02757145
Iteration 57, loss = 0.02725732
Iteration 58, loss = 0.02740797
Iteration 59, loss = 0.02699385
Iteration 60, loss = 0.02727797
Iteration 61, loss = 0.02698477
Iteration 62, loss = 0.02706085
Iteration 63, loss = 0.02715931
Iteration 64, loss = 0.02707959
Iteration 65, loss = 0.02692748
Iteration 66, loss = 0.02693664
Iteration 67, loss = 0.02691465
Iteration 68, loss = 0.02696375
Iteration 69, loss = 0.02691853
Iteration 70, loss = 0.02716390
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69647361
Iteration 2, loss = 0.61748513
Iteration 3, loss = 0.50940997
Iteration 4, loss = 0.39659491
Iteration 5, loss = 0.30124975
Iteration 6, loss = 0.22731689
Iteration 7, loss = 0.17446046
Iteration 8, loss = 0.13798917
Iteration 9, loss = 0.11228805
Iteration 10, loss = 0.09433176
Iteration 11, loss = 0.08095944
Iteration 12, loss = 0.07122409
Iteration 13, loss = 0.06380918
Iteration 14, loss = 0.05782791
Iteration 15, loss = 0.05293448
Iteration 16, loss = 0.04943559
Iteration 17, loss = 0.04620564
Iteration 18, loss = 0.04359391
Iteration 19, loss = 0.04148611
Iteration 20, loss = 0.03969933
Iteration 21, loss = 0.03817111
Iteration 22, loss = 0.03660920
Iteration 23, loss = 0.03529180
Iteration 24, loss = 0.03431518
Iteration 25, loss = 0.03395600
Iteration 26, loss = 0.03271706
Iteration 27, loss = 0.03221255
Iteration 28, loss = 0.03157131
Iteration 29, loss = 0.03085937
Iteration 30, loss = 0.03045828
Iteration 31, loss = 0.03018026
Iteration 32, loss = 0.02948170
Iteration 33, loss = 0.02946583
Iteration 34, loss = 0.02898802
Iteration 35, loss = 0.02853536
Iteration 36, loss = 0.02846909
Iteration 37, loss = 0.02805303
Iteration 38, loss = 0.02787013
Iteration 39, loss = 0.02761038
Iteration 40, loss = 0.02741846
Iteration 41, loss = 0.02741903
Iteration 42, loss = 0.02704674
Iteration 43, loss = 0.02672233
Iteration 44, loss = 0.02688766
Iteration 45, loss = 0.02636563
Iteration 46, loss = 0.02648648
Iteration 47, loss = 0.02649657
Iteration 48, loss = 0.02621448
Iteration 49, loss = 0.02647154
Iteration 50, loss = 0.02612408
Iteration 51, loss = 0.02632738
Iteration 52, loss = 0.02610570
Iteration 53, loss = 0.02582703
Iteration 54, loss = 0.02573287
Iteration 55, loss = 0.02591219
Iteration 56, loss = 0.02559795
Iteration 57, loss = 0.02560995
Iteration 58, loss = 0.02535720
Iteration 59, loss = 0.02555682
Iteration 60, loss = 0.02555261
Iteration 61, loss = 0.02529000
Iteration 62, loss = 0.02539583
Iteration 63, loss = 0.02518244
Iteration 64, loss = 0.02518717
Iteration 65, loss = 0.02508248
Iteration 66, loss = 0.02536968
Iteration 67, loss = 0.02503323
Iteration 68, loss = 0.02516941
Iteration 69, loss = 0.02493311
Iteration 70, loss = 0.02505644
Iteration 71, loss = 0.02518644
Iteration 72, loss = 0.02494164
Iteration 73, loss = 0.02474553
Iteration 74, loss = 0.02516208
Iteration 75, loss = 0.02483411
Iteration 76, loss = 0.02508898
Iteration 77, loss = 0.02468447
Iteration 78, loss = 0.02493985
Iteration 79, loss = 0.02476292
Iteration 80, loss = 0.02487576
Iteration 81, loss = 0.02481896
Iteration 82, loss = 0.02482807
Iteration 83, loss = 0.02473300
Iteration 84, loss = 0.02450908
Iteration 85, loss = 0.02456019
Iteration 86, loss = 0.02476754
Iteration 87, loss = 0.02456780
Iteration 88, loss = 0.02448711
Iteration 89, loss = 0.02489782
Iteration 90, loss = 0.02445558
Iteration 91, loss = 0.02462377
Iteration 92, loss = 0.02451233
Iteration 93, loss = 0.02461082
Iteration 94, loss = 0.02459018
Iteration 95, loss = 0.02479830
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70183962
Iteration 2, loss = 0.60599594
Iteration 3, loss = 0.47516082
Iteration 4, loss = 0.34693213
Iteration 5, loss = 0.24812741
Iteration 6, loss = 0.18143020
Iteration 7, loss = 0.13813866
Iteration 8, loss = 0.11048749
Iteration 9, loss = 0.09150479
Iteration 10, loss = 0.07819872
Iteration 11, loss = 0.06877465
Iteration 12, loss = 0.06153347
Iteration 13, loss = 0.05620868
Iteration 14, loss = 0.05184326
Iteration 15, loss = 0.04842700
Iteration 16, loss = 0.04532392
Iteration 17, loss = 0.04339965
Iteration 18, loss = 0.04114277
Iteration 19, loss = 0.03981931
Iteration 20, loss = 0.03841616
Iteration 21, loss = 0.03690872
Iteration 22, loss = 0.03616860
Iteration 23, loss = 0.03542269
Iteration 24, loss = 0.03471766
Iteration 25, loss = 0.03386899
Iteration 26, loss = 0.03346648
Iteration 27, loss = 0.03291017
Iteration 28, loss = 0.03250214
Iteration 29, loss = 0.03189940
Iteration 30, loss = 0.03163015
Iteration 31, loss = 0.03136092
Iteration 32, loss = 0.03089468
Iteration 33, loss = 0.03051528
Iteration 34, loss = 0.03036249
Iteration 35, loss = 0.03008180
Iteration 36, loss = 0.03022479
Iteration 37, loss = 0.02985716
Iteration 38, loss = 0.02980758
Iteration 39, loss = 0.02934142
Iteration 40, loss = 0.02934607
Iteration 41, loss = 0.02958817
Iteration 42, loss = 0.02952722
Iteration 43, loss = 0.02921337
Iteration 44, loss = 0.02891653
Iteration 45, loss = 0.02913141
Iteration 46, loss = 0.02889699
Iteration 47, loss = 0.02871326
Iteration 48, loss = 0.02903576
Iteration 49, loss = 0.02877204
Iteration 50, loss = 0.02855013
Iteration 51, loss = 0.02821185
Iteration 52, loss = 0.02852537
Iteration 53, loss = 0.02833508
Iteration 54, loss = 0.02843729
Iteration 55, loss = 0.02826432
Iteration 56, loss = 0.02805193
Iteration 57, loss = 0.02785713
Iteration 58, loss = 0.02773865
Iteration 59, loss = 0.02801098
Iteration 60, loss = 0.02817381
Iteration 61, loss = 0.02767151
Iteration 62, loss = 0.02789031
Iteration 63, loss = 0.02768787
Iteration 64, loss = 0.02756096
Iteration 65, loss = 0.02788895
Iteration 66, loss = 0.02768018
Iteration 67, loss = 0.02741612
Iteration 68, loss = 0.02808091
Iteration 69, loss = 0.02775876
Iteration 70, loss = 0.02764415
Iteration 71, loss = 0.02761316
Iteration 72, loss = 0.02745398
Iteration 73, loss = 0.02759139
Iteration 74, loss = 0.02780332
Iteration 75, loss = 0.02745475
Iteration 76, loss = 0.02746478
Iteration 77, loss = 0.02727312
Iteration 78, loss = 0.02757462
Iteration 79, loss = 0.02777418
Iteration 80, loss = 0.02745230
Iteration 81, loss = 0.02758133
Iteration 82, loss = 0.02786499
Iteration 83, loss = 0.02752842
Iteration 84, loss = 0.02752555
Iteration 85, loss = 0.02761269
Iteration 86, loss = 0.02737004
Iteration 87, loss = 0.02782034
Iteration 88, loss = 0.02715557
Iteration 89, loss = 0.02731891
Iteration 90, loss = 0.02719757
Iteration 91, loss = 0.02743250
Iteration 92, loss = 0.02711984
Iteration 93, loss = 0.02748519
Iteration 94, loss = 0.02761871
Iteration 95, loss = 0.02713407
Iteration 96, loss = 0.02725370
Iteration 97, loss = 0.02748267
Iteration 98, loss = 0.02732492
Iteration 99, loss = 0.02757523
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
üìà Plotting learning curve for NeuralNetwork on kept_v5_lemma_stem...
Iteration 1, loss = 0.69735583
Iteration 2, loss = 0.65520228
Iteration 3, loss = 0.59042714
Iteration 4, loss = 0.50913960
Iteration 5, loss = 0.42793832
Iteration 6, loss = 0.35319681
Iteration 7, loss = 0.28939528
Iteration 8, loss = 0.23672387
Iteration 9, loss = 0.19516819
Iteration 10, loss = 0.16281151
Iteration 11, loss = 0.13782188
Iteration 12, loss = 0.11844668
Iteration 13, loss = 0.10321266
Iteration 14, loss = 0.09142869
Iteration 15, loss = 0.08188501
Iteration 16, loss = 0.07418412
Iteration 17, loss = 0.06768029
Iteration 18, loss = 0.06248116
Iteration 19, loss = 0.05817644
Iteration 20, loss = 0.05435005
Iteration 21, loss = 0.05108481
Iteration 22, loss = 0.04824569
Iteration 23, loss = 0.04599203
Iteration 24, loss = 0.04407353
Iteration 25, loss = 0.04202518
Iteration 26, loss = 0.04056304
Iteration 27, loss = 0.03897315
Iteration 28, loss = 0.03781158
Iteration 29, loss = 0.03669440
Iteration 30, loss = 0.03555560
Iteration 31, loss = 0.03485109
Iteration 32, loss = 0.03396786
Iteration 33, loss = 0.03334476
Iteration 34, loss = 0.03232557
Iteration 35, loss = 0.03189402
Iteration 36, loss = 0.03104296
Iteration 37, loss = 0.03061330
Iteration 38, loss = 0.03021623
Iteration 39, loss = 0.02964809
Iteration 40, loss = 0.02902535
Iteration 41, loss = 0.02864561
Iteration 42, loss = 0.02822148
Iteration 43, loss = 0.02824992
Iteration 44, loss = 0.02779769
Iteration 45, loss = 0.02742048
Iteration 46, loss = 0.02696318
Iteration 47, loss = 0.02704496
Iteration 48, loss = 0.02685170
Iteration 49, loss = 0.02640619
Iteration 50, loss = 0.02632630
Iteration 51, loss = 0.02633530
Iteration 52, loss = 0.02613422
Iteration 53, loss = 0.02601692
Iteration 54, loss = 0.02581863
Iteration 55, loss = 0.02549613
Iteration 56, loss = 0.02518722
Iteration 57, loss = 0.02502858
Iteration 58, loss = 0.02508286
Iteration 59, loss = 0.02485596
Iteration 60, loss = 0.02467640
Iteration 61, loss = 0.02476888
Iteration 62, loss = 0.02456817
Iteration 63, loss = 0.02463616
Iteration 64, loss = 0.02440596
Iteration 65, loss = 0.02404888
Iteration 66, loss = 0.02424384
Iteration 67, loss = 0.02424308
Iteration 68, loss = 0.02401833
Iteration 69, loss = 0.02401107
Iteration 70, loss = 0.02402915
Iteration 71, loss = 0.02391970
Iteration 72, loss = 0.02376931
Iteration 73, loss = 0.02347375
Iteration 74, loss = 0.02380288
Iteration 75, loss = 0.02366197
Iteration 76, loss = 0.02338951
Iteration 77, loss = 0.02327928
Iteration 78, loss = 0.02343578
Iteration 79, loss = 0.02320777
Iteration 80, loss = 0.02338875
Iteration 81, loss = 0.02326368
Iteration 82, loss = 0.02322712
Iteration 83, loss = 0.02300077
Iteration 84, loss = 0.02308003
Iteration 85, loss = 0.02293753
Iteration 86, loss = 0.02283692
Iteration 87, loss = 0.02303596
Iteration 88, loss = 0.02305200
Iteration 89, loss = 0.02279240
Iteration 90, loss = 0.02284682
Iteration 91, loss = 0.02269271
Iteration 92, loss = 0.02264058
Iteration 93, loss = 0.02272836
Iteration 94, loss = 0.02268905
Iteration 95, loss = 0.02260666
Iteration 96, loss = 0.02262075
Iteration 97, loss = 0.02269554
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68734988
Iteration 2, loss = 0.66920208
Iteration 3, loss = 0.65151923
Iteration 4, loss = 0.63017807
Iteration 5, loss = 0.60375941
Iteration 6, loss = 0.57168054
Iteration 7, loss = 0.53554866
Iteration 8, loss = 0.49665306
Iteration 9, loss = 0.45723574
Iteration 10, loss = 0.41866302
Iteration 11, loss = 0.38174954
Iteration 12, loss = 0.34705986
Iteration 13, loss = 0.31486620
Iteration 14, loss = 0.28530767
Iteration 15, loss = 0.25853904
Iteration 16, loss = 0.23429523
Iteration 17, loss = 0.21248982
Iteration 18, loss = 0.19304854
Iteration 19, loss = 0.17567023
Iteration 20, loss = 0.16018358
Iteration 21, loss = 0.14648363
Iteration 22, loss = 0.13428433
Iteration 23, loss = 0.12341208
Iteration 24, loss = 0.11374183
Iteration 25, loss = 0.10514129
Iteration 26, loss = 0.09746113
Iteration 27, loss = 0.09056176
Iteration 28, loss = 0.08434631
Iteration 29, loss = 0.07879710
Iteration 30, loss = 0.07380955
Iteration 31, loss = 0.06923845
Iteration 32, loss = 0.06510196
Iteration 33, loss = 0.06143947
Iteration 34, loss = 0.05801755
Iteration 35, loss = 0.05491651
Iteration 36, loss = 0.05203736
Iteration 37, loss = 0.04942275
Iteration 38, loss = 0.04705261
Iteration 39, loss = 0.04481389
Iteration 40, loss = 0.04280006
Iteration 41, loss = 0.04090115
Iteration 42, loss = 0.03916675
Iteration 43, loss = 0.03755520
Iteration 44, loss = 0.03603872
Iteration 45, loss = 0.03461757
Iteration 46, loss = 0.03325951
Iteration 47, loss = 0.03208071
Iteration 48, loss = 0.03093731
Iteration 49, loss = 0.02983052
Iteration 50, loss = 0.02883775
Iteration 51, loss = 0.02784345
Iteration 52, loss = 0.02699469
Iteration 53, loss = 0.02609854
Iteration 54, loss = 0.02540342
Iteration 55, loss = 0.02462357
Iteration 56, loss = 0.02387650
Iteration 57, loss = 0.02321496
Iteration 58, loss = 0.02258892
Iteration 59, loss = 0.02197889
Iteration 60, loss = 0.02139660
Iteration 61, loss = 0.02086783
Iteration 62, loss = 0.02035049
Iteration 63, loss = 0.01988047
Iteration 64, loss = 0.01942229
Iteration 65, loss = 0.01898568
Iteration 66, loss = 0.01854515
Iteration 67, loss = 0.01810681
Iteration 68, loss = 0.01774989
Iteration 69, loss = 0.01739322
Iteration 70, loss = 0.01703307
Iteration 71, loss = 0.01673210
Iteration 72, loss = 0.01640478
Iteration 73, loss = 0.01603585
Iteration 74, loss = 0.01575706
Iteration 75, loss = 0.01555050
Iteration 76, loss = 0.01521607
Iteration 77, loss = 0.01496109
Iteration 78, loss = 0.01475715
Iteration 79, loss = 0.01444596
Iteration 80, loss = 0.01419431
Iteration 81, loss = 0.01395691
Iteration 82, loss = 0.01380253
Iteration 83, loss = 0.01359122
Iteration 84, loss = 0.01337124
Iteration 85, loss = 0.01321577
Iteration 86, loss = 0.01299525
Iteration 87, loss = 0.01280210
Iteration 88, loss = 0.01262926
Iteration 89, loss = 0.01247205
Iteration 90, loss = 0.01234378
Iteration 91, loss = 0.01215105
Iteration 92, loss = 0.01205704
Iteration 93, loss = 0.01186451
Iteration 94, loss = 0.01176250
Iteration 95, loss = 0.01162808
Iteration 96, loss = 0.01149143
Iteration 97, loss = 0.01133796
Iteration 98, loss = 0.01118892
Iteration 99, loss = 0.01111246
Iteration 100, loss = 0.01101834
Iteration 101, loss = 0.01088785
Iteration 102, loss = 0.01074903
Iteration 103, loss = 0.01062992
Iteration 104, loss = 0.01060227
Iteration 105, loss = 0.01048343
Iteration 106, loss = 0.01042969
Iteration 107, loss = 0.01030968
Iteration 108, loss = 0.01020244
Iteration 109, loss = 0.01007744
Iteration 110, loss = 0.00998540
Iteration 111, loss = 0.00987572
Iteration 112, loss = 0.00982744
Iteration 113, loss = 0.00976529
Iteration 114, loss = 0.00973055
Iteration 115, loss = 0.00959810
Iteration 116, loss = 0.00950619
Iteration 117, loss = 0.00942425
Iteration 118, loss = 0.00940314
Iteration 119, loss = 0.00928754
Iteration 120, loss = 0.00925019
Iteration 121, loss = 0.00918962
Iteration 122, loss = 0.00911483
Iteration 123, loss = 0.00908351
Iteration 124, loss = 0.00899664
Iteration 125, loss = 0.00893721
Iteration 126, loss = 0.00893784
Iteration 127, loss = 0.00881103
Iteration 128, loss = 0.00878436
Iteration 129, loss = 0.00869533
Iteration 130, loss = 0.00865808
Iteration 131, loss = 0.00864677
Iteration 132, loss = 0.00858145
Iteration 133, loss = 0.00858269
Iteration 134, loss = 0.00849174
Iteration 135, loss = 0.00840452
Iteration 136, loss = 0.00838947
Iteration 137, loss = 0.00834204
Iteration 138, loss = 0.00828126
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68445822
Iteration 2, loss = 0.66108649
Iteration 3, loss = 0.63384773
Iteration 4, loss = 0.59806655
Iteration 5, loss = 0.55369663
Iteration 6, loss = 0.50320633
Iteration 7, loss = 0.45108398
Iteration 8, loss = 0.40071353
Iteration 9, loss = 0.35347045
Iteration 10, loss = 0.31068220
Iteration 11, loss = 0.27238200
Iteration 12, loss = 0.23903680
Iteration 13, loss = 0.21014277
Iteration 14, loss = 0.18530108
Iteration 15, loss = 0.16420284
Iteration 16, loss = 0.14618122
Iteration 17, loss = 0.13074738
Iteration 18, loss = 0.11771383
Iteration 19, loss = 0.10630857
Iteration 20, loss = 0.09663794
Iteration 21, loss = 0.08816112
Iteration 22, loss = 0.08091311
Iteration 23, loss = 0.07458973
Iteration 24, loss = 0.06902041
Iteration 25, loss = 0.06419241
Iteration 26, loss = 0.05986184
Iteration 27, loss = 0.05606364
Iteration 28, loss = 0.05261595
Iteration 29, loss = 0.04967417
Iteration 30, loss = 0.04695750
Iteration 31, loss = 0.04446885
Iteration 32, loss = 0.04221241
Iteration 33, loss = 0.04004414
Iteration 34, loss = 0.03816963
Iteration 35, loss = 0.03642305
Iteration 36, loss = 0.03478848
Iteration 37, loss = 0.03329195
Iteration 38, loss = 0.03196409
Iteration 39, loss = 0.03081596
Iteration 40, loss = 0.02981703
Iteration 41, loss = 0.02871933
Iteration 42, loss = 0.02778989
Iteration 43, loss = 0.02685516
Iteration 44, loss = 0.02609403
Iteration 45, loss = 0.02522740
Iteration 46, loss = 0.02449847
Iteration 47, loss = 0.02383910
Iteration 48, loss = 0.02332567
Iteration 49, loss = 0.02272836
Iteration 50, loss = 0.02206934
Iteration 51, loss = 0.02154101
Iteration 52, loss = 0.02103303
Iteration 53, loss = 0.02060935
Iteration 54, loss = 0.02011245
Iteration 55, loss = 0.01963716
Iteration 56, loss = 0.01923298
Iteration 57, loss = 0.01889938
Iteration 58, loss = 0.01846302
Iteration 59, loss = 0.01814584
Iteration 60, loss = 0.01785728
Iteration 61, loss = 0.01772282
Iteration 62, loss = 0.01733068
Iteration 63, loss = 0.01699405
Iteration 64, loss = 0.01679487
Iteration 65, loss = 0.01648664
Iteration 66, loss = 0.01623462
Iteration 67, loss = 0.01599324
Iteration 68, loss = 0.01576701
Iteration 69, loss = 0.01556657
Iteration 70, loss = 0.01535503
Iteration 71, loss = 0.01513292
Iteration 72, loss = 0.01496051
Iteration 73, loss = 0.01478471
Iteration 74, loss = 0.01460977
Iteration 75, loss = 0.01463095
Iteration 76, loss = 0.01433697
Iteration 77, loss = 0.01424492
Iteration 78, loss = 0.01406660
Iteration 79, loss = 0.01397642
Iteration 80, loss = 0.01379193
Iteration 81, loss = 0.01369025
Iteration 82, loss = 0.01347328
Iteration 83, loss = 0.01341706
Iteration 84, loss = 0.01327363
Iteration 85, loss = 0.01320289
Iteration 86, loss = 0.01304047
Iteration 87, loss = 0.01295630
Iteration 88, loss = 0.01284143
Iteration 89, loss = 0.01273232
Iteration 90, loss = 0.01264113
Iteration 91, loss = 0.01258784
Iteration 92, loss = 0.01246925
Iteration 93, loss = 0.01235848
Iteration 94, loss = 0.01238878
Iteration 95, loss = 0.01220664
Iteration 96, loss = 0.01219290
Iteration 97, loss = 0.01212452
Iteration 98, loss = 0.01212178
Iteration 99, loss = 0.01212981
Iteration 100, loss = 0.01214322
Iteration 101, loss = 0.01180993
Iteration 102, loss = 0.01185075
Iteration 103, loss = 0.01170740
Iteration 104, loss = 0.01168371
Iteration 105, loss = 0.01159233
Iteration 106, loss = 0.01153485
Iteration 107, loss = 0.01152334
Iteration 108, loss = 0.01164457
Iteration 109, loss = 0.01148363
Iteration 110, loss = 0.01137890
Iteration 111, loss = 0.01135775
Iteration 112, loss = 0.01138147
Iteration 113, loss = 0.01127602
Iteration 114, loss = 0.01123674
Iteration 115, loss = 0.01118614
Iteration 116, loss = 0.01124707
Iteration 117, loss = 0.01125326
Iteration 118, loss = 0.01123405
Iteration 119, loss = 0.01115578
Iteration 120, loss = 0.01103841
Iteration 121, loss = 0.01091180
Iteration 122, loss = 0.01079549
Iteration 123, loss = 0.01078972
Iteration 124, loss = 0.01075844
Iteration 125, loss = 0.01072577
Iteration 126, loss = 0.01063706
Iteration 127, loss = 0.01064478
Iteration 128, loss = 0.01059095
Iteration 129, loss = 0.01054950
Iteration 130, loss = 0.01054649
Iteration 131, loss = 0.01051235
Iteration 132, loss = 0.01050651
Iteration 133, loss = 0.01062968
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68410895
Iteration 2, loss = 0.66181151
Iteration 3, loss = 0.63546062
Iteration 4, loss = 0.60223020
Iteration 5, loss = 0.56216338
Iteration 6, loss = 0.51867244
Iteration 7, loss = 0.47423570
Iteration 8, loss = 0.43009057
Iteration 9, loss = 0.38704911
Iteration 10, loss = 0.34648679
Iteration 11, loss = 0.30878712
Iteration 12, loss = 0.27451699
Iteration 13, loss = 0.24359733
Iteration 14, loss = 0.21641607
Iteration 15, loss = 0.19222412
Iteration 16, loss = 0.17127551
Iteration 17, loss = 0.15313308
Iteration 18, loss = 0.13753963
Iteration 19, loss = 0.12396632
Iteration 20, loss = 0.11227857
Iteration 21, loss = 0.10237730
Iteration 22, loss = 0.09389438
Iteration 23, loss = 0.08622583
Iteration 24, loss = 0.07960127
Iteration 25, loss = 0.07388294
Iteration 26, loss = 0.06874696
Iteration 27, loss = 0.06420672
Iteration 28, loss = 0.06022797
Iteration 29, loss = 0.05666713
Iteration 30, loss = 0.05347577
Iteration 31, loss = 0.05047219
Iteration 32, loss = 0.04792381
Iteration 33, loss = 0.04565359
Iteration 34, loss = 0.04346807
Iteration 35, loss = 0.04160604
Iteration 36, loss = 0.03975286
Iteration 37, loss = 0.03812749
Iteration 38, loss = 0.03657286
Iteration 39, loss = 0.03512662
Iteration 40, loss = 0.03386762
Iteration 41, loss = 0.03274723
Iteration 42, loss = 0.03166714
Iteration 43, loss = 0.03056925
Iteration 44, loss = 0.02971508
Iteration 45, loss = 0.02886791
Iteration 46, loss = 0.02804514
Iteration 47, loss = 0.02718966
Iteration 48, loss = 0.02649051
Iteration 49, loss = 0.02589433
Iteration 50, loss = 0.02515906
Iteration 51, loss = 0.02454411
Iteration 52, loss = 0.02398628
Iteration 53, loss = 0.02349679
Iteration 54, loss = 0.02299110
Iteration 55, loss = 0.02255495
Iteration 56, loss = 0.02207747
Iteration 57, loss = 0.02170254
Iteration 58, loss = 0.02128159
Iteration 59, loss = 0.02092439
Iteration 60, loss = 0.02052366
Iteration 61, loss = 0.02018429
Iteration 62, loss = 0.01956237
Iteration 63, loss = 0.02000031
Iteration 64, loss = 0.01971448
Iteration 65, loss = 0.01928981
Iteration 66, loss = 0.01886247
Iteration 67, loss = 0.01868099
Iteration 68, loss = 0.01829365
Iteration 69, loss = 0.01799487
Iteration 70, loss = 0.01766434
Iteration 71, loss = 0.01751239
Iteration 72, loss = 0.01735903
Iteration 73, loss = 0.01715566
Iteration 74, loss = 0.01688330
Iteration 75, loss = 0.01679976
Iteration 76, loss = 0.01666120
Iteration 77, loss = 0.01650784
Iteration 78, loss = 0.01637464
Iteration 79, loss = 0.01612999
Iteration 80, loss = 0.01590900
Iteration 81, loss = 0.01575443
Iteration 82, loss = 0.01564489
Iteration 83, loss = 0.01553354
Iteration 84, loss = 0.01550153
Iteration 85, loss = 0.01568161
Iteration 86, loss = 0.01554620
Iteration 87, loss = 0.01545068
Iteration 88, loss = 0.01510490
Iteration 89, loss = 0.01518034
Iteration 90, loss = 0.01502703
Iteration 91, loss = 0.01505245
Iteration 92, loss = 0.01487936
Iteration 93, loss = 0.01464010
Iteration 94, loss = 0.01452128
Iteration 95, loss = 0.01435082
Iteration 96, loss = 0.01430499
Iteration 97, loss = 0.01415001
Iteration 98, loss = 0.01402439
Iteration 99, loss = 0.01397620
Iteration 100, loss = 0.01397043
Iteration 101, loss = 0.01379253
Iteration 102, loss = 0.01368051
Iteration 103, loss = 0.01366551
Iteration 104, loss = 0.01363153
Iteration 105, loss = 0.01357209
Iteration 106, loss = 0.01343931
Iteration 107, loss = 0.01337631
Iteration 108, loss = 0.01329387
Iteration 109, loss = 0.01313674
Iteration 110, loss = 0.01324963
Iteration 111, loss = 0.01317255
Iteration 112, loss = 0.01319718
Iteration 113, loss = 0.01319246
Iteration 114, loss = 0.01301074
Iteration 115, loss = 0.01293140
Iteration 116, loss = 0.01282779
Iteration 117, loss = 0.01276334
Iteration 118, loss = 0.01274009
Iteration 119, loss = 0.01261676
Iteration 120, loss = 0.01251675
Iteration 121, loss = 0.01276216
Iteration 122, loss = 0.01284938
Iteration 123, loss = 0.01272104
Iteration 124, loss = 0.01279622
Iteration 125, loss = 0.01262011
Iteration 126, loss = 0.01254880
Iteration 127, loss = 0.01246633
Iteration 128, loss = 0.01231523
Iteration 129, loss = 0.01201042
Iteration 130, loss = 0.01219194
Iteration 131, loss = 0.01263342
Iteration 132, loss = 0.01243891
Iteration 133, loss = 0.01239065
Iteration 134, loss = 0.01215069
Iteration 135, loss = 0.01222403
Iteration 136, loss = 0.01227295
Iteration 137, loss = 0.01221137
Iteration 138, loss = 0.01210937
Iteration 139, loss = 0.01207984
Iteration 140, loss = 0.01200152
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67951663
Iteration 2, loss = 0.64107282
Iteration 3, loss = 0.58076015
Iteration 4, loss = 0.50429710
Iteration 5, loss = 0.42560982
Iteration 6, loss = 0.35400957
Iteration 7, loss = 0.29226118
Iteration 8, loss = 0.24145877
Iteration 9, loss = 0.20091722
Iteration 10, loss = 0.16897914
Iteration 11, loss = 0.14385465
Iteration 12, loss = 0.12404646
Iteration 13, loss = 0.10874916
Iteration 14, loss = 0.09665469
Iteration 15, loss = 0.08652286
Iteration 16, loss = 0.07846485
Iteration 17, loss = 0.07192125
Iteration 18, loss = 0.06625517
Iteration 19, loss = 0.06160879
Iteration 20, loss = 0.05777871
Iteration 21, loss = 0.05449194
Iteration 22, loss = 0.05175580
Iteration 23, loss = 0.04903642
Iteration 24, loss = 0.04708656
Iteration 25, loss = 0.04515153
Iteration 26, loss = 0.04328432
Iteration 27, loss = 0.04201156
Iteration 28, loss = 0.04051229
Iteration 29, loss = 0.03943612
Iteration 30, loss = 0.03829672
Iteration 31, loss = 0.03727763
Iteration 32, loss = 0.03653529
Iteration 33, loss = 0.03591800
Iteration 34, loss = 0.03487926
Iteration 35, loss = 0.03449492
Iteration 36, loss = 0.03388643
Iteration 37, loss = 0.03335082
Iteration 38, loss = 0.03253627
Iteration 39, loss = 0.03195475
Iteration 40, loss = 0.03172206
Iteration 41, loss = 0.03132040
Iteration 42, loss = 0.03111553
Iteration 43, loss = 0.03086398
Iteration 44, loss = 0.03023713
Iteration 45, loss = 0.03012921
Iteration 46, loss = 0.03003538
Iteration 47, loss = 0.02963564
Iteration 48, loss = 0.02934105
Iteration 49, loss = 0.02888843
Iteration 50, loss = 0.02889775
Iteration 51, loss = 0.02918850
Iteration 52, loss = 0.02846378
Iteration 53, loss = 0.02850422
Iteration 54, loss = 0.02808102
Iteration 55, loss = 0.02797209
Iteration 56, loss = 0.02794508
Iteration 57, loss = 0.02769234
Iteration 58, loss = 0.02752019
Iteration 59, loss = 0.02756578
Iteration 60, loss = 0.02716565
Iteration 61, loss = 0.02724452
Iteration 62, loss = 0.02725455
Iteration 63, loss = 0.02712160
Iteration 64, loss = 0.02691602
Iteration 65, loss = 0.02680433
Iteration 66, loss = 0.02692487
Iteration 67, loss = 0.02655495
Iteration 68, loss = 0.02639624
Iteration 69, loss = 0.02637877
Iteration 70, loss = 0.02630072
Iteration 71, loss = 0.02602442
Iteration 72, loss = 0.02612942
Iteration 73, loss = 0.02640989
Iteration 74, loss = 0.02625954
Iteration 75, loss = 0.02605214
Iteration 76, loss = 0.02619316
Iteration 77, loss = 0.02592341
Iteration 78, loss = 0.02593118
Iteration 79, loss = 0.02612403
Iteration 80, loss = 0.02566917
Iteration 81, loss = 0.02577408
Iteration 82, loss = 0.02590439
Iteration 83, loss = 0.02581008
Iteration 84, loss = 0.02564766
Iteration 85, loss = 0.02557910
Iteration 86, loss = 0.02542597
Iteration 87, loss = 0.02571775
Iteration 88, loss = 0.02577852
Iteration 89, loss = 0.02543935
Iteration 90, loss = 0.02568262
Iteration 91, loss = 0.02542446
Iteration 92, loss = 0.02524491
Iteration 93, loss = 0.02525557
Iteration 94, loss = 0.02531806
Iteration 95, loss = 0.02526548
Iteration 96, loss = 0.02515099
Iteration 97, loss = 0.02499472
Iteration 98, loss = 0.02510397
Iteration 99, loss = 0.02520920
Iteration 100, loss = 0.02523449
Iteration 101, loss = 0.02554077
Iteration 102, loss = 0.02535057
Iteration 103, loss = 0.02515501
Iteration 104, loss = 0.02494032
Iteration 105, loss = 0.02580980
Iteration 106, loss = 0.02554839
Iteration 107, loss = 0.02502951
Iteration 108, loss = 0.02472606
Iteration 109, loss = 0.02528658
Iteration 110, loss = 0.02509356
Iteration 111, loss = 0.02507503
Iteration 112, loss = 0.02513754
Iteration 113, loss = 0.02500682
Iteration 114, loss = 0.02477016
Iteration 115, loss = 0.02491434
Iteration 116, loss = 0.02512415
Iteration 117, loss = 0.02475419
Iteration 118, loss = 0.02485809
Iteration 119, loss = 0.02467157
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67600998
Iteration 2, loss = 0.62862135
Iteration 3, loss = 0.56433881
Iteration 4, loss = 0.48728838
Iteration 5, loss = 0.40871498
Iteration 6, loss = 0.33606294
Iteration 7, loss = 0.27325349
Iteration 8, loss = 0.22209056
Iteration 9, loss = 0.18187725
Iteration 10, loss = 0.15093804
Iteration 11, loss = 0.12675938
Iteration 12, loss = 0.10848287
Iteration 13, loss = 0.09435194
Iteration 14, loss = 0.08293219
Iteration 15, loss = 0.07403711
Iteration 16, loss = 0.06644606
Iteration 17, loss = 0.06036094
Iteration 18, loss = 0.05546789
Iteration 19, loss = 0.05128489
Iteration 20, loss = 0.04772978
Iteration 21, loss = 0.04445717
Iteration 22, loss = 0.04193345
Iteration 23, loss = 0.03971914
Iteration 24, loss = 0.03771314
Iteration 25, loss = 0.03598396
Iteration 26, loss = 0.03446252
Iteration 27, loss = 0.03309050
Iteration 28, loss = 0.03189258
Iteration 29, loss = 0.03085147
Iteration 30, loss = 0.02966651
Iteration 31, loss = 0.02900387
Iteration 32, loss = 0.02831926
Iteration 33, loss = 0.02741095
Iteration 34, loss = 0.02661764
Iteration 35, loss = 0.02610744
Iteration 36, loss = 0.02558451
Iteration 37, loss = 0.02519780
Iteration 38, loss = 0.02467346
Iteration 39, loss = 0.02412382
Iteration 40, loss = 0.02377381
Iteration 41, loss = 0.02342290
Iteration 42, loss = 0.02290737
Iteration 43, loss = 0.02270676
Iteration 44, loss = 0.02255769
Iteration 45, loss = 0.02218247
Iteration 46, loss = 0.02201538
Iteration 47, loss = 0.02174343
Iteration 48, loss = 0.02134363
Iteration 49, loss = 0.02128359
Iteration 50, loss = 0.02102686
Iteration 51, loss = 0.02099747
Iteration 52, loss = 0.02067428
Iteration 53, loss = 0.02045080
Iteration 54, loss = 0.02035181
Iteration 55, loss = 0.02016368
Iteration 56, loss = 0.01998374
Iteration 57, loss = 0.01994390
Iteration 58, loss = 0.01963776
Iteration 59, loss = 0.01951841
Iteration 60, loss = 0.01967793
Iteration 61, loss = 0.01949055
Iteration 62, loss = 0.01931789
Iteration 63, loss = 0.01932913
Iteration 64, loss = 0.01932248
Iteration 65, loss = 0.01902423
Iteration 66, loss = 0.01913042
Iteration 67, loss = 0.01897975
Iteration 68, loss = 0.01892995
Iteration 69, loss = 0.01894946
Iteration 70, loss = 0.01880758
Iteration 71, loss = 0.01860145
Iteration 72, loss = 0.01846979
Iteration 73, loss = 0.01865778
Iteration 74, loss = 0.01867522
Iteration 75, loss = 0.01850648
Iteration 76, loss = 0.01835616
Iteration 77, loss = 0.01818683
Iteration 78, loss = 0.01819564
Iteration 79, loss = 0.01814987
Iteration 80, loss = 0.01819853
Iteration 81, loss = 0.01814892
Iteration 82, loss = 0.01803531
Iteration 83, loss = 0.01816538
Iteration 84, loss = 0.01800247
Iteration 85, loss = 0.01779855
Iteration 86, loss = 0.01797895
Iteration 87, loss = 0.01812248
Iteration 88, loss = 0.01787756
Iteration 89, loss = 0.01789184
Iteration 90, loss = 0.01769180
Iteration 91, loss = 0.01749444
Iteration 92, loss = 0.01761890
Iteration 93, loss = 0.01756196
Iteration 94, loss = 0.01761628
Iteration 95, loss = 0.01758581
Iteration 96, loss = 0.01766232
Iteration 97, loss = 0.01741904
Iteration 98, loss = 0.01744012
Iteration 99, loss = 0.01731696
Iteration 100, loss = 0.01739456
Iteration 101, loss = 0.01739976
Iteration 102, loss = 0.01749473
Iteration 103, loss = 0.01739744
Iteration 104, loss = 0.01739170
Iteration 105, loss = 0.01726955
Iteration 106, loss = 0.01734168
Iteration 107, loss = 0.01735094
Iteration 108, loss = 0.01724447
Iteration 109, loss = 0.01729207
Iteration 110, loss = 0.01721864
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70842077
Iteration 2, loss = 0.67790254
Iteration 3, loss = 0.63770351
Iteration 4, loss = 0.58864451
Iteration 5, loss = 0.53254818
Iteration 6, loss = 0.47369512
Iteration 7, loss = 0.41569891
Iteration 8, loss = 0.36077111
Iteration 9, loss = 0.31088880
Iteration 10, loss = 0.26686915
Iteration 11, loss = 0.22861260
Iteration 12, loss = 0.19618698
Iteration 13, loss = 0.16924569
Iteration 14, loss = 0.14709822
Iteration 15, loss = 0.12871096
Iteration 16, loss = 0.11368448
Iteration 17, loss = 0.10135039
Iteration 18, loss = 0.09102887
Iteration 19, loss = 0.08234259
Iteration 20, loss = 0.07498975
Iteration 21, loss = 0.06875728
Iteration 22, loss = 0.06340906
Iteration 23, loss = 0.05894158
Iteration 24, loss = 0.05500889
Iteration 25, loss = 0.05150153
Iteration 26, loss = 0.04849963
Iteration 27, loss = 0.04574920
Iteration 28, loss = 0.04335433
Iteration 29, loss = 0.04120559
Iteration 30, loss = 0.03934082
Iteration 31, loss = 0.03761652
Iteration 32, loss = 0.03607264
Iteration 33, loss = 0.03462234
Iteration 34, loss = 0.03338438
Iteration 35, loss = 0.03238207
Iteration 36, loss = 0.03124062
Iteration 37, loss = 0.03025441
Iteration 38, loss = 0.02953748
Iteration 39, loss = 0.02851758
Iteration 40, loss = 0.02785754
Iteration 41, loss = 0.02717809
Iteration 42, loss = 0.02643205
Iteration 43, loss = 0.02590613
Iteration 44, loss = 0.02537216
Iteration 45, loss = 0.02484394
Iteration 46, loss = 0.02447072
Iteration 47, loss = 0.02394497
Iteration 48, loss = 0.02339184
Iteration 49, loss = 0.02318935
Iteration 50, loss = 0.02269699
Iteration 51, loss = 0.02244916
Iteration 52, loss = 0.02206923
Iteration 53, loss = 0.02174709
Iteration 54, loss = 0.02153103
Iteration 55, loss = 0.02114478
Iteration 56, loss = 0.02087920
Iteration 57, loss = 0.02065882
Iteration 58, loss = 0.02042661
Iteration 59, loss = 0.02018589
Iteration 60, loss = 0.02005701
Iteration 61, loss = 0.01991739
Iteration 62, loss = 0.01973672
Iteration 63, loss = 0.01945345
Iteration 64, loss = 0.01938687
Iteration 65, loss = 0.01901306
Iteration 66, loss = 0.01890125
Iteration 67, loss = 0.01897374
Iteration 68, loss = 0.01872682
Iteration 69, loss = 0.01856930
Iteration 70, loss = 0.01835678
Iteration 71, loss = 0.01824974
Iteration 72, loss = 0.01819847
Iteration 73, loss = 0.01804256
Iteration 74, loss = 0.01790300
Iteration 75, loss = 0.01786588
Iteration 76, loss = 0.01773048
Iteration 77, loss = 0.01770964
Iteration 78, loss = 0.01751596
Iteration 79, loss = 0.01738634
Iteration 80, loss = 0.01742995
Iteration 81, loss = 0.01724021
Iteration 82, loss = 0.01719607
Iteration 83, loss = 0.01712645
Iteration 84, loss = 0.01714361
Iteration 85, loss = 0.01706757
Iteration 86, loss = 0.01690823
Iteration 87, loss = 0.01686216
Iteration 88, loss = 0.01686591
Iteration 89, loss = 0.01667204
Iteration 90, loss = 0.01668028
Iteration 91, loss = 0.01669074
Iteration 92, loss = 0.01663190
Iteration 93, loss = 0.01656257
Iteration 94, loss = 0.01640374
Iteration 95, loss = 0.01642732
Iteration 96, loss = 0.01633366
Iteration 97, loss = 0.01632800
Iteration 98, loss = 0.01629331
Iteration 99, loss = 0.01620425
Iteration 100, loss = 0.01625128
Iteration 101, loss = 0.01607704
Iteration 102, loss = 0.01622103
Iteration 103, loss = 0.01603976
Iteration 104, loss = 0.01608074
Iteration 105, loss = 0.01597804
Iteration 106, loss = 0.01602927
Iteration 107, loss = 0.01598041
Iteration 108, loss = 0.01603795
Iteration 109, loss = 0.01577694
Iteration 110, loss = 0.01582475
Iteration 111, loss = 0.01588575
Iteration 112, loss = 0.01578612
Iteration 113, loss = 0.01583258
Iteration 114, loss = 0.01573810
Iteration 115, loss = 0.01576344
Iteration 116, loss = 0.01576393
Iteration 117, loss = 0.01564649
Iteration 118, loss = 0.01571116
Iteration 119, loss = 0.01550632
Iteration 120, loss = 0.01553916
Iteration 121, loss = 0.01570771
Iteration 122, loss = 0.01559773
Iteration 123, loss = 0.01564125
Iteration 124, loss = 0.01560677
Iteration 125, loss = 0.01551625
Iteration 126, loss = 0.01550229
Iteration 127, loss = 0.01538464
Iteration 128, loss = 0.01535504
Iteration 129, loss = 0.01545135
Iteration 130, loss = 0.01534566
Iteration 131, loss = 0.01523702
Iteration 132, loss = 0.01523030
Iteration 133, loss = 0.01525537
Iteration 134, loss = 0.01533983
Iteration 135, loss = 0.01537630
Iteration 136, loss = 0.01515829
Iteration 137, loss = 0.01525402
Iteration 138, loss = 0.01524170
Iteration 139, loss = 0.01513485
Iteration 140, loss = 0.01519270
Iteration 141, loss = 0.01522643
Iteration 142, loss = 0.01516136
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67999413
Iteration 2, loss = 0.65424101
Iteration 3, loss = 0.61999172
Iteration 4, loss = 0.57266722
Iteration 5, loss = 0.51634309
Iteration 6, loss = 0.45723239
Iteration 7, loss = 0.39978740
Iteration 8, loss = 0.34561942
Iteration 9, loss = 0.29694100
Iteration 10, loss = 0.25427627
Iteration 11, loss = 0.21771766
Iteration 12, loss = 0.18710817
Iteration 13, loss = 0.16192259
Iteration 14, loss = 0.14088209
Iteration 15, loss = 0.12376189
Iteration 16, loss = 0.10959683
Iteration 17, loss = 0.09779738
Iteration 18, loss = 0.08806204
Iteration 19, loss = 0.07986706
Iteration 20, loss = 0.07269475
Iteration 21, loss = 0.06674400
Iteration 22, loss = 0.06179261
Iteration 23, loss = 0.05739070
Iteration 24, loss = 0.05344685
Iteration 25, loss = 0.05018299
Iteration 26, loss = 0.04716090
Iteration 27, loss = 0.04460299
Iteration 28, loss = 0.04223543
Iteration 29, loss = 0.04021146
Iteration 30, loss = 0.03835390
Iteration 31, loss = 0.03665386
Iteration 32, loss = 0.03507846
Iteration 33, loss = 0.03381816
Iteration 34, loss = 0.03247973
Iteration 35, loss = 0.03139459
Iteration 36, loss = 0.03034919
Iteration 37, loss = 0.02930270
Iteration 38, loss = 0.02856456
Iteration 39, loss = 0.02786235
Iteration 40, loss = 0.02702635
Iteration 41, loss = 0.02626641
Iteration 42, loss = 0.02578939
Iteration 43, loss = 0.02509681
Iteration 44, loss = 0.02447638
Iteration 45, loss = 0.02407988
Iteration 46, loss = 0.02347879
Iteration 47, loss = 0.02316685
Iteration 48, loss = 0.02265846
Iteration 49, loss = 0.02234078
Iteration 50, loss = 0.02188170
Iteration 51, loss = 0.02150470
Iteration 52, loss = 0.02138387
Iteration 53, loss = 0.02094765
Iteration 54, loss = 0.02084327
Iteration 55, loss = 0.02046261
Iteration 56, loss = 0.02021679
Iteration 57, loss = 0.02003687
Iteration 58, loss = 0.01980558
Iteration 59, loss = 0.01946432
Iteration 60, loss = 0.01932153
Iteration 61, loss = 0.01914018
Iteration 62, loss = 0.01906394
Iteration 63, loss = 0.01881556
Iteration 64, loss = 0.01859077
Iteration 65, loss = 0.01845366
Iteration 66, loss = 0.01820834
Iteration 67, loss = 0.01811435
Iteration 68, loss = 0.01797815
Iteration 69, loss = 0.01793608
Iteration 70, loss = 0.01783704
Iteration 71, loss = 0.01761334
Iteration 72, loss = 0.01762562
Iteration 73, loss = 0.01741826
Iteration 74, loss = 0.01737290
Iteration 75, loss = 0.01729320
Iteration 76, loss = 0.01704444
Iteration 77, loss = 0.01708010
Iteration 78, loss = 0.01686598
Iteration 79, loss = 0.01686149
Iteration 80, loss = 0.01690804
Iteration 81, loss = 0.01678149
Iteration 82, loss = 0.01688826
Iteration 83, loss = 0.01649700
Iteration 84, loss = 0.01668421
Iteration 85, loss = 0.01641685
Iteration 86, loss = 0.01629857
Iteration 87, loss = 0.01646602
Iteration 88, loss = 0.01628593
Iteration 89, loss = 0.01624097
Iteration 90, loss = 0.01618646
Iteration 91, loss = 0.01602711
Iteration 92, loss = 0.01595643
Iteration 93, loss = 0.01601558
Iteration 94, loss = 0.01593336
Iteration 95, loss = 0.01590369
Iteration 96, loss = 0.01581842
Iteration 97, loss = 0.01572855
Iteration 98, loss = 0.01579317
Iteration 99, loss = 0.01572493
Iteration 100, loss = 0.01561082
Iteration 101, loss = 0.01571859
Iteration 102, loss = 0.01547321
Iteration 103, loss = 0.01547429
Iteration 104, loss = 0.01547931
Iteration 105, loss = 0.01549499
Iteration 106, loss = 0.01547807
Iteration 107, loss = 0.01539808
Iteration 108, loss = 0.01538048
Iteration 109, loss = 0.01544694
Iteration 110, loss = 0.01551472
Iteration 111, loss = 0.01532562
Iteration 112, loss = 0.01530180
Iteration 113, loss = 0.01528350
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68459836
Iteration 2, loss = 0.63999080
Iteration 3, loss = 0.57596890
Iteration 4, loss = 0.49584364
Iteration 5, loss = 0.41466494
Iteration 6, loss = 0.34197239
Iteration 7, loss = 0.27964314
Iteration 8, loss = 0.22863191
Iteration 9, loss = 0.18869174
Iteration 10, loss = 0.15777907
Iteration 11, loss = 0.13365317
Iteration 12, loss = 0.11509844
Iteration 13, loss = 0.10062272
Iteration 14, loss = 0.08909462
Iteration 15, loss = 0.07985239
Iteration 16, loss = 0.07239015
Iteration 17, loss = 0.06624313
Iteration 18, loss = 0.06115292
Iteration 19, loss = 0.05704882
Iteration 20, loss = 0.05329048
Iteration 21, loss = 0.05036805
Iteration 22, loss = 0.04758496
Iteration 23, loss = 0.04520430
Iteration 24, loss = 0.04338176
Iteration 25, loss = 0.04151778
Iteration 26, loss = 0.04013689
Iteration 27, loss = 0.03862710
Iteration 28, loss = 0.03726148
Iteration 29, loss = 0.03633905
Iteration 30, loss = 0.03550675
Iteration 31, loss = 0.03441759
Iteration 32, loss = 0.03376792
Iteration 33, loss = 0.03289146
Iteration 34, loss = 0.03286448
Iteration 35, loss = 0.03193239
Iteration 36, loss = 0.03113228
Iteration 37, loss = 0.03063149
Iteration 38, loss = 0.03043851
Iteration 39, loss = 0.02995267
Iteration 40, loss = 0.02963317
Iteration 41, loss = 0.02933973
Iteration 42, loss = 0.02881791
Iteration 43, loss = 0.02845904
Iteration 44, loss = 0.02830393
Iteration 45, loss = 0.02801759
Iteration 46, loss = 0.02755119
Iteration 47, loss = 0.02738989
Iteration 48, loss = 0.02701890
Iteration 49, loss = 0.02685358
Iteration 50, loss = 0.02675336
Iteration 51, loss = 0.02647116
Iteration 52, loss = 0.02646457
Iteration 53, loss = 0.02633138
Iteration 54, loss = 0.02611989
Iteration 55, loss = 0.02609214
Iteration 56, loss = 0.02578912
Iteration 57, loss = 0.02560759
Iteration 58, loss = 0.02559925
Iteration 59, loss = 0.02537303
Iteration 60, loss = 0.02521882
Iteration 61, loss = 0.02524374
Iteration 62, loss = 0.02516559
Iteration 63, loss = 0.02501732
Iteration 64, loss = 0.02476242
Iteration 65, loss = 0.02474809
Iteration 66, loss = 0.02484331
Iteration 67, loss = 0.02487233
Iteration 68, loss = 0.02450482
Iteration 69, loss = 0.02449200
Iteration 70, loss = 0.02455005
Iteration 71, loss = 0.02433404
Iteration 72, loss = 0.02441067
Iteration 73, loss = 0.02436409
Iteration 74, loss = 0.02402890
Iteration 75, loss = 0.02422110
Iteration 76, loss = 0.02431523
Iteration 77, loss = 0.02387285
Iteration 78, loss = 0.02428984
Iteration 79, loss = 0.02424946
Iteration 80, loss = 0.02404605
Iteration 81, loss = 0.02405122
Iteration 82, loss = 0.02381130
Iteration 83, loss = 0.02384992
Iteration 84, loss = 0.02417935
Iteration 85, loss = 0.02396252
Iteration 86, loss = 0.02405633
Iteration 87, loss = 0.02361588
Iteration 88, loss = 0.02351323
Iteration 89, loss = 0.02364035
Iteration 90, loss = 0.02349166
Iteration 91, loss = 0.02347240
Iteration 92, loss = 0.02376460
Iteration 93, loss = 0.02359982
Iteration 94, loss = 0.02354193
Iteration 95, loss = 0.02351447
Iteration 96, loss = 0.02339126
Iteration 97, loss = 0.02338523
Iteration 98, loss = 0.02320868
Iteration 99, loss = 0.02347631
Iteration 100, loss = 0.02356914
Iteration 101, loss = 0.02351822
Iteration 102, loss = 0.02348907
Iteration 103, loss = 0.02346001
Iteration 104, loss = 0.02319370
Iteration 105, loss = 0.02330730
Iteration 106, loss = 0.02320250
Iteration 107, loss = 0.02317660
Iteration 108, loss = 0.02326988
Iteration 109, loss = 0.02317515
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68452922
Iteration 2, loss = 0.66561864
Iteration 3, loss = 0.64683521
Iteration 4, loss = 0.62475088
Iteration 5, loss = 0.59858967
Iteration 6, loss = 0.56886662
Iteration 7, loss = 0.53669127
Iteration 8, loss = 0.50295526
Iteration 9, loss = 0.46855176
Iteration 10, loss = 0.43417826
Iteration 11, loss = 0.40018512
Iteration 12, loss = 0.36745125
Iteration 13, loss = 0.33644475
Iteration 14, loss = 0.30708313
Iteration 15, loss = 0.27989208
Iteration 16, loss = 0.25488440
Iteration 17, loss = 0.23209813
Iteration 18, loss = 0.21130836
Iteration 19, loss = 0.19267485
Iteration 20, loss = 0.17591385
Iteration 21, loss = 0.16107913
Iteration 22, loss = 0.14765353
Iteration 23, loss = 0.13572324
Iteration 24, loss = 0.12507490
Iteration 25, loss = 0.11562127
Iteration 26, loss = 0.10713756
Iteration 27, loss = 0.09950888
Iteration 28, loss = 0.09272606
Iteration 29, loss = 0.08666996
Iteration 30, loss = 0.08113705
Iteration 31, loss = 0.07618665
Iteration 32, loss = 0.07172585
Iteration 33, loss = 0.06764224
Iteration 34, loss = 0.06398100
Iteration 35, loss = 0.06064717
Iteration 36, loss = 0.05750537
Iteration 37, loss = 0.05474512
Iteration 38, loss = 0.05218701
Iteration 39, loss = 0.04978844
Iteration 40, loss = 0.04756937
Iteration 41, loss = 0.04560502
Iteration 42, loss = 0.04372814
Iteration 43, loss = 0.04199451
Iteration 44, loss = 0.04033228
Iteration 45, loss = 0.03891985
Iteration 46, loss = 0.03749382
Iteration 47, loss = 0.03623056
Iteration 48, loss = 0.03496644
Iteration 49, loss = 0.03383905
Iteration 50, loss = 0.03276667
Iteration 51, loss = 0.03180338
Iteration 52, loss = 0.03085917
Iteration 53, loss = 0.02991583
Iteration 54, loss = 0.02916511
Iteration 55, loss = 0.02833575
Iteration 56, loss = 0.02755249
Iteration 57, loss = 0.02695981
Iteration 58, loss = 0.02624085
Iteration 59, loss = 0.02558304
Iteration 60, loss = 0.02499544
Iteration 61, loss = 0.02443432
Iteration 62, loss = 0.02391424
Iteration 63, loss = 0.02343295
Iteration 64, loss = 0.02293178
Iteration 65, loss = 0.02249240
Iteration 66, loss = 0.02206393
Iteration 67, loss = 0.02155919
Iteration 68, loss = 0.02118269
Iteration 69, loss = 0.02079295
Iteration 70, loss = 0.02043784
Iteration 71, loss = 0.02007173
Iteration 72, loss = 0.01972651
Iteration 73, loss = 0.01946335
Iteration 74, loss = 0.01913707
Iteration 75, loss = 0.01882437
Iteration 76, loss = 0.01848320
Iteration 77, loss = 0.01827430
Iteration 78, loss = 0.01806043
Iteration 79, loss = 0.01771534
Iteration 80, loss = 0.01747886
Iteration 81, loss = 0.01725509
Iteration 82, loss = 0.01707427
Iteration 83, loss = 0.01686861
Iteration 84, loss = 0.01663459
Iteration 85, loss = 0.01639387
Iteration 86, loss = 0.01632782
Iteration 87, loss = 0.01604742
Iteration 88, loss = 0.01591183
Iteration 89, loss = 0.01569279
Iteration 90, loss = 0.01553544
Iteration 91, loss = 0.01536520
Iteration 92, loss = 0.01520669
Iteration 93, loss = 0.01508275
Iteration 94, loss = 0.01489620
Iteration 95, loss = 0.01483990
Iteration 96, loss = 0.01462661
Iteration 97, loss = 0.01451063
Iteration 98, loss = 0.01432920
Iteration 99, loss = 0.01431981
Iteration 100, loss = 0.01409552
Iteration 101, loss = 0.01402169
Iteration 102, loss = 0.01383994
Iteration 103, loss = 0.01375876
Iteration 104, loss = 0.01368273
Iteration 105, loss = 0.01356879
Iteration 106, loss = 0.01347334
Iteration 107, loss = 0.01337002
Iteration 108, loss = 0.01325101
Iteration 109, loss = 0.01321502
Iteration 110, loss = 0.01312845
Iteration 111, loss = 0.01299273
Iteration 112, loss = 0.01298817
Iteration 113, loss = 0.01287557
Iteration 114, loss = 0.01273609
Iteration 115, loss = 0.01262526
Iteration 116, loss = 0.01256461
Iteration 117, loss = 0.01253561
Iteration 118, loss = 0.01246383
Iteration 119, loss = 0.01238380
Iteration 120, loss = 0.01227974
Iteration 121, loss = 0.01226051
Iteration 122, loss = 0.01216360
Iteration 123, loss = 0.01209433
Iteration 124, loss = 0.01206971
Iteration 125, loss = 0.01201458
Iteration 126, loss = 0.01190425
Iteration 127, loss = 0.01183720
Iteration 128, loss = 0.01183038
Iteration 129, loss = 0.01181697
Iteration 130, loss = 0.01166862
Iteration 131, loss = 0.01157961
Iteration 132, loss = 0.01157173
Iteration 133, loss = 0.01152803
Iteration 134, loss = 0.01150673
Iteration 135, loss = 0.01144800
Iteration 136, loss = 0.01142130
Iteration 137, loss = 0.01132281
Iteration 138, loss = 0.01131513
Iteration 139, loss = 0.01126159
Iteration 140, loss = 0.01124414
Iteration 141, loss = 0.01116590
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67723046
Iteration 2, loss = 0.62702435
Iteration 3, loss = 0.54608196
Iteration 4, loss = 0.45496870
Iteration 5, loss = 0.36906662
Iteration 6, loss = 0.29538033
Iteration 7, loss = 0.23606305
Iteration 8, loss = 0.19060308
Iteration 9, loss = 0.15599520
Iteration 10, loss = 0.13032342
Iteration 11, loss = 0.11113110
Iteration 12, loss = 0.09592903
Iteration 13, loss = 0.08458183
Iteration 14, loss = 0.07555256
Iteration 15, loss = 0.06858927
Iteration 16, loss = 0.06264320
Iteration 17, loss = 0.05751047
Iteration 18, loss = 0.05367979
Iteration 19, loss = 0.05011535
Iteration 20, loss = 0.04746877
Iteration 21, loss = 0.04508351
Iteration 22, loss = 0.04294839
Iteration 23, loss = 0.04104925
Iteration 24, loss = 0.03949892
Iteration 25, loss = 0.03816953
Iteration 26, loss = 0.03675451
Iteration 27, loss = 0.03562600
Iteration 28, loss = 0.03458920
Iteration 29, loss = 0.03386523
Iteration 30, loss = 0.03282801
Iteration 31, loss = 0.03237332
Iteration 32, loss = 0.03157414
Iteration 33, loss = 0.03127087
Iteration 34, loss = 0.03089474
Iteration 35, loss = 0.03003576
Iteration 36, loss = 0.02975182
Iteration 37, loss = 0.02926663
Iteration 38, loss = 0.02885327
Iteration 39, loss = 0.02853226
Iteration 40, loss = 0.02798662
Iteration 41, loss = 0.02777931
Iteration 42, loss = 0.02747052
Iteration 43, loss = 0.02750595
Iteration 44, loss = 0.02702171
Iteration 45, loss = 0.02706080
Iteration 46, loss = 0.02670503
Iteration 47, loss = 0.02633833
Iteration 48, loss = 0.02629861
Iteration 49, loss = 0.02630656
Iteration 50, loss = 0.02617919
Iteration 51, loss = 0.02600352
Iteration 52, loss = 0.02579904
Iteration 53, loss = 0.02555699
Iteration 54, loss = 0.02543181
Iteration 55, loss = 0.02550371
Iteration 56, loss = 0.02522844
Iteration 57, loss = 0.02514235
Iteration 58, loss = 0.02513136
Iteration 59, loss = 0.02505586
Iteration 60, loss = 0.02472050
Iteration 61, loss = 0.02508373
Iteration 62, loss = 0.02496442
Iteration 63, loss = 0.02466571
Iteration 64, loss = 0.02449943
Iteration 65, loss = 0.02442418
Iteration 66, loss = 0.02451227
Iteration 67, loss = 0.02450077
Iteration 68, loss = 0.02453903
Iteration 69, loss = 0.02433225
Iteration 70, loss = 0.02422531
Iteration 71, loss = 0.02428325
Iteration 72, loss = 0.02421629
Iteration 73, loss = 0.02412525
Iteration 74, loss = 0.02393299
Iteration 75, loss = 0.02405502
Iteration 76, loss = 0.02385409
Iteration 77, loss = 0.02388976
Iteration 78, loss = 0.02390751
Iteration 79, loss = 0.02358849
Iteration 80, loss = 0.02389496
Iteration 81, loss = 0.02376300
Iteration 82, loss = 0.02410847
Iteration 83, loss = 0.02345383
Iteration 84, loss = 0.02403218
Iteration 85, loss = 0.02377607
Iteration 86, loss = 0.02373036
Iteration 87, loss = 0.02347598
Iteration 88, loss = 0.02368292
Iteration 89, loss = 0.02356390
Iteration 90, loss = 0.02352706
Iteration 91, loss = 0.02379714
Iteration 92, loss = 0.02328458
Iteration 93, loss = 0.02355356
Iteration 94, loss = 0.02347446
Iteration 95, loss = 0.02350732
Iteration 96, loss = 0.02347946
Iteration 97, loss = 0.02345052
Iteration 98, loss = 0.02313217
Iteration 99, loss = 0.02332842
Iteration 100, loss = 0.02328554
Iteration 101, loss = 0.02316445
Iteration 102, loss = 0.02331454
Iteration 103, loss = 0.02321904
Iteration 104, loss = 0.02327785
Iteration 105, loss = 0.02318728
Iteration 106, loss = 0.02325724
Iteration 107, loss = 0.02332073
Iteration 108, loss = 0.02313106
Iteration 109, loss = 0.02311044
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69877135
Iteration 2, loss = 0.65330961
Iteration 3, loss = 0.58665092
Iteration 4, loss = 0.50076342
Iteration 5, loss = 0.41162623
Iteration 6, loss = 0.33024534
Iteration 7, loss = 0.26326005
Iteration 8, loss = 0.21050946
Iteration 9, loss = 0.17105555
Iteration 10, loss = 0.14140718
Iteration 11, loss = 0.11924793
Iteration 12, loss = 0.10261765
Iteration 13, loss = 0.08962730
Iteration 14, loss = 0.07964035
Iteration 15, loss = 0.07177910
Iteration 16, loss = 0.06528280
Iteration 17, loss = 0.05990619
Iteration 18, loss = 0.05555795
Iteration 19, loss = 0.05176542
Iteration 20, loss = 0.04875406
Iteration 21, loss = 0.04619177
Iteration 22, loss = 0.04358121
Iteration 23, loss = 0.04179067
Iteration 24, loss = 0.04003630
Iteration 25, loss = 0.03865087
Iteration 26, loss = 0.03724319
Iteration 27, loss = 0.03597406
Iteration 28, loss = 0.03486599
Iteration 29, loss = 0.03393266
Iteration 30, loss = 0.03323750
Iteration 31, loss = 0.03232482
Iteration 32, loss = 0.03153062
Iteration 33, loss = 0.03096203
Iteration 34, loss = 0.03021937
Iteration 35, loss = 0.03000255
Iteration 36, loss = 0.02947776
Iteration 37, loss = 0.02890054
Iteration 38, loss = 0.02861511
Iteration 39, loss = 0.02815442
Iteration 40, loss = 0.02770105
Iteration 41, loss = 0.02756578
Iteration 42, loss = 0.02714662
Iteration 43, loss = 0.02696836
Iteration 44, loss = 0.02655935
Iteration 45, loss = 0.02620911
Iteration 46, loss = 0.02616723
Iteration 47, loss = 0.02597045
Iteration 48, loss = 0.02565230
Iteration 49, loss = 0.02566997
Iteration 50, loss = 0.02541386
Iteration 51, loss = 0.02557973
Iteration 52, loss = 0.02518969
Iteration 53, loss = 0.02516208
Iteration 54, loss = 0.02498730
Iteration 55, loss = 0.02468261
Iteration 56, loss = 0.02467554
Iteration 57, loss = 0.02446570
Iteration 58, loss = 0.02417565
Iteration 59, loss = 0.02443670
Iteration 60, loss = 0.02408434
Iteration 61, loss = 0.02417828
Iteration 62, loss = 0.02401863
Iteration 63, loss = 0.02410541
Iteration 64, loss = 0.02377850
Iteration 65, loss = 0.02376956
Iteration 66, loss = 0.02373804
Iteration 67, loss = 0.02339588
Iteration 68, loss = 0.02339659
Iteration 69, loss = 0.02328630
Iteration 70, loss = 0.02343771
Iteration 71, loss = 0.02335199
Iteration 72, loss = 0.02332076
Iteration 73, loss = 0.02312504
Iteration 74, loss = 0.02309759
Iteration 75, loss = 0.02294995
Iteration 76, loss = 0.02305258
Iteration 77, loss = 0.02303800
Iteration 78, loss = 0.02283671
Iteration 79, loss = 0.02294806
Iteration 80, loss = 0.02278720
Iteration 81, loss = 0.02295630
Iteration 82, loss = 0.02282179
Iteration 83, loss = 0.02281222
Iteration 84, loss = 0.02268655
Iteration 85, loss = 0.02275225
Iteration 86, loss = 0.02268482
Iteration 87, loss = 0.02250050
Iteration 88, loss = 0.02238497
Iteration 89, loss = 0.02256758
Iteration 90, loss = 0.02252001
Iteration 91, loss = 0.02234060
Iteration 92, loss = 0.02257164
Iteration 93, loss = 0.02223522
Iteration 94, loss = 0.02243852
Iteration 95, loss = 0.02230498
Iteration 96, loss = 0.02229991
Iteration 97, loss = 0.02213457
Iteration 98, loss = 0.02254326
Iteration 99, loss = 0.02242643
Iteration 100, loss = 0.02220299
Iteration 101, loss = 0.02215052
Iteration 102, loss = 0.02249266
Iteration 103, loss = 0.02215709
Iteration 104, loss = 0.02230185
Iteration 105, loss = 0.02207743
Iteration 106, loss = 0.02249223
Iteration 107, loss = 0.02233885
Iteration 108, loss = 0.02214740
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70667116
Iteration 2, loss = 0.69005185
Iteration 3, loss = 0.67594895
Iteration 4, loss = 0.66146352
Iteration 5, loss = 0.64641124
Iteration 6, loss = 0.63041701
Iteration 7, loss = 0.61277122
Iteration 8, loss = 0.59410030
Iteration 9, loss = 0.57373282
Iteration 10, loss = 0.55214959
Iteration 11, loss = 0.52958357
Iteration 12, loss = 0.50621856
Iteration 13, loss = 0.48218775
Iteration 14, loss = 0.45818237
Iteration 15, loss = 0.43409745
Iteration 16, loss = 0.41003551
Iteration 17, loss = 0.38644199
Iteration 18, loss = 0.36341639
Iteration 19, loss = 0.34128536
Iteration 20, loss = 0.32005029
Iteration 21, loss = 0.29992145
Iteration 22, loss = 0.28081826
Iteration 23, loss = 0.26301353
Iteration 24, loss = 0.24608071
Iteration 25, loss = 0.23041830
Iteration 26, loss = 0.21577183
Iteration 27, loss = 0.20218412
Iteration 28, loss = 0.18957075
Iteration 29, loss = 0.17786845
Iteration 30, loss = 0.16701169
Iteration 31, loss = 0.15702509
Iteration 32, loss = 0.14775673
Iteration 33, loss = 0.13919914
Iteration 34, loss = 0.13129544
Iteration 35, loss = 0.12399372
Iteration 36, loss = 0.11719455
Iteration 37, loss = 0.11100040
Iteration 38, loss = 0.10522068
Iteration 39, loss = 0.09985227
Iteration 40, loss = 0.09487056
Iteration 41, loss = 0.09028612
Iteration 42, loss = 0.08596992
Iteration 43, loss = 0.08201756
Iteration 44, loss = 0.07824900
Iteration 45, loss = 0.07484063
Iteration 46, loss = 0.07159368
Iteration 47, loss = 0.06861428
Iteration 48, loss = 0.06580125
Iteration 49, loss = 0.06316626
Iteration 50, loss = 0.06071015
Iteration 51, loss = 0.05839865
Iteration 52, loss = 0.05625356
Iteration 53, loss = 0.05423778
Iteration 54, loss = 0.05232498
Iteration 55, loss = 0.05050360
Iteration 56, loss = 0.04882037
Iteration 57, loss = 0.04731529
Iteration 58, loss = 0.04573465
Iteration 59, loss = 0.04429931
Iteration 60, loss = 0.04300207
Iteration 61, loss = 0.04164092
Iteration 62, loss = 0.04044853
Iteration 63, loss = 0.03927515
Iteration 64, loss = 0.03821118
Iteration 65, loss = 0.03715188
Iteration 66, loss = 0.03616021
Iteration 67, loss = 0.03521962
Iteration 68, loss = 0.03433160
Iteration 69, loss = 0.03346131
Iteration 70, loss = 0.03267756
Iteration 71, loss = 0.03187899
Iteration 72, loss = 0.03112487
Iteration 73, loss = 0.03041467
Iteration 74, loss = 0.02972221
Iteration 75, loss = 0.02905596
Iteration 76, loss = 0.02844328
Iteration 77, loss = 0.02783475
Iteration 78, loss = 0.02724292
Iteration 79, loss = 0.02668066
Iteration 80, loss = 0.02614253
Iteration 81, loss = 0.02565563
Iteration 82, loss = 0.02514594
Iteration 83, loss = 0.02470222
Iteration 84, loss = 0.02431364
Iteration 85, loss = 0.02385822
Iteration 86, loss = 0.02345244
Iteration 87, loss = 0.02305014
Iteration 88, loss = 0.02265459
Iteration 89, loss = 0.02226325
Iteration 90, loss = 0.02190886
Iteration 91, loss = 0.02158944
Iteration 92, loss = 0.02125001
Iteration 93, loss = 0.02095693
Iteration 94, loss = 0.02066680
Iteration 95, loss = 0.02026404
Iteration 96, loss = 0.01997409
Iteration 97, loss = 0.01969722
Iteration 98, loss = 0.01938569
Iteration 99, loss = 0.01913298
Iteration 100, loss = 0.01889634
Iteration 101, loss = 0.01866570
Iteration 102, loss = 0.01835359
Iteration 103, loss = 0.01815930
Iteration 104, loss = 0.01788299
Iteration 105, loss = 0.01771831
Iteration 106, loss = 0.01750242
Iteration 107, loss = 0.01723732
Iteration 108, loss = 0.01702675
Iteration 109, loss = 0.01684016
Iteration 110, loss = 0.01662225
Iteration 111, loss = 0.01647080
Iteration 112, loss = 0.01629917
Iteration 113, loss = 0.01614389
Iteration 114, loss = 0.01597809
Iteration 115, loss = 0.01582220
Iteration 116, loss = 0.01567401
Iteration 117, loss = 0.01549838
Iteration 118, loss = 0.01535810
Iteration 119, loss = 0.01519271
Iteration 120, loss = 0.01508514
Iteration 121, loss = 0.01495165
Iteration 122, loss = 0.01481429
Iteration 123, loss = 0.01462840
Iteration 124, loss = 0.01452884
Iteration 125, loss = 0.01437928
Iteration 126, loss = 0.01421947
Iteration 127, loss = 0.01410427
Iteration 128, loss = 0.01400315
Iteration 129, loss = 0.01388830
Iteration 130, loss = 0.01377131
Iteration 131, loss = 0.01364547
Iteration 132, loss = 0.01355163
Iteration 133, loss = 0.01342546
Iteration 134, loss = 0.01330609
Iteration 135, loss = 0.01327387
Iteration 136, loss = 0.01312821
Iteration 137, loss = 0.01299527
Iteration 138, loss = 0.01289778
Iteration 139, loss = 0.01284804
Iteration 140, loss = 0.01275208
Iteration 141, loss = 0.01266319
Iteration 142, loss = 0.01255121
Iteration 143, loss = 0.01245855
Iteration 144, loss = 0.01236961
Iteration 145, loss = 0.01231364
Iteration 146, loss = 0.01224567
Iteration 147, loss = 0.01217047
Iteration 148, loss = 0.01213737
Iteration 149, loss = 0.01203431
Iteration 150, loss = 0.01192749
Iteration 151, loss = 0.01191823
Iteration 152, loss = 0.01180303
Iteration 153, loss = 0.01174066
Iteration 154, loss = 0.01167651
Iteration 155, loss = 0.01165904
Iteration 156, loss = 0.01161975
Iteration 157, loss = 0.01151148
Iteration 158, loss = 0.01143301
Iteration 159, loss = 0.01134770
Iteration 160, loss = 0.01131178
Iteration 161, loss = 0.01122814
Iteration 162, loss = 0.01119340
Iteration 163, loss = 0.01113234
Iteration 164, loss = 0.01107276
Iteration 165, loss = 0.01102023
Iteration 166, loss = 0.01096300
Iteration 167, loss = 0.01090021
Iteration 168, loss = 0.01086672
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71999307
Iteration 2, loss = 0.65898645
Iteration 3, loss = 0.57576884
Iteration 4, loss = 0.47993358
Iteration 5, loss = 0.38588854
Iteration 6, loss = 0.30467656
Iteration 7, loss = 0.24020162
Iteration 8, loss = 0.19161087
Iteration 9, loss = 0.15522117
Iteration 10, loss = 0.12818859
Iteration 11, loss = 0.10814638
Iteration 12, loss = 0.09270324
Iteration 13, loss = 0.08105796
Iteration 14, loss = 0.07145954
Iteration 15, loss = 0.06398053
Iteration 16, loss = 0.05831403
Iteration 17, loss = 0.05309953
Iteration 18, loss = 0.04904043
Iteration 19, loss = 0.04546427
Iteration 20, loss = 0.04262909
Iteration 21, loss = 0.04001253
Iteration 22, loss = 0.03793761
Iteration 23, loss = 0.03594973
Iteration 24, loss = 0.03436559
Iteration 25, loss = 0.03278530
Iteration 26, loss = 0.03154490
Iteration 27, loss = 0.03044921
Iteration 28, loss = 0.02927895
Iteration 29, loss = 0.02854201
Iteration 30, loss = 0.02763643
Iteration 31, loss = 0.02698810
Iteration 32, loss = 0.02634462
Iteration 33, loss = 0.02556903
Iteration 34, loss = 0.02517797
Iteration 35, loss = 0.02477418
Iteration 36, loss = 0.02404070
Iteration 37, loss = 0.02369541
Iteration 38, loss = 0.02346955
Iteration 39, loss = 0.02284956
Iteration 40, loss = 0.02280721
Iteration 41, loss = 0.02240328
Iteration 42, loss = 0.02186575
Iteration 43, loss = 0.02176498
Iteration 44, loss = 0.02167712
Iteration 45, loss = 0.02120350
Iteration 46, loss = 0.02085465
Iteration 47, loss = 0.02087900
Iteration 48, loss = 0.02068059
Iteration 49, loss = 0.02057670
Iteration 50, loss = 0.02011344
Iteration 51, loss = 0.02015762
Iteration 52, loss = 0.01982500
Iteration 53, loss = 0.01971532
Iteration 54, loss = 0.01974034
Iteration 55, loss = 0.01958061
Iteration 56, loss = 0.01942680
Iteration 57, loss = 0.01930360
Iteration 58, loss = 0.01934851
Iteration 59, loss = 0.01903651
Iteration 60, loss = 0.01894373
Iteration 61, loss = 0.01907153
Iteration 62, loss = 0.01911627
Iteration 63, loss = 0.01866789
Iteration 64, loss = 0.01853516
Iteration 65, loss = 0.01872957
Iteration 66, loss = 0.01850069
Iteration 67, loss = 0.01858596
Iteration 68, loss = 0.01845528
Iteration 69, loss = 0.01850048
Iteration 70, loss = 0.01853481
Iteration 71, loss = 0.01825340
Iteration 72, loss = 0.01845735
Iteration 73, loss = 0.01809001
Iteration 74, loss = 0.01799008
Iteration 75, loss = 0.01804436
Iteration 76, loss = 0.01823190
Iteration 77, loss = 0.01792687
Iteration 78, loss = 0.01777210
Iteration 79, loss = 0.01792831
Iteration 80, loss = 0.01814690
Iteration 81, loss = 0.01778638
Iteration 82, loss = 0.01787505
Iteration 83, loss = 0.01773871
Iteration 84, loss = 0.01782103
Iteration 85, loss = 0.01760124
Iteration 86, loss = 0.01769057
Iteration 87, loss = 0.01749850
Iteration 88, loss = 0.01776333
Iteration 89, loss = 0.01797404
Iteration 90, loss = 0.01766676
Iteration 91, loss = 0.01755547
Iteration 92, loss = 0.01756933
Iteration 93, loss = 0.01767957
Iteration 94, loss = 0.01761968
Iteration 95, loss = 0.01741619
Iteration 96, loss = 0.01741594
Iteration 97, loss = 0.01713744
Iteration 98, loss = 0.01730097
Iteration 99, loss = 0.01730446
Iteration 100, loss = 0.01723034
Iteration 101, loss = 0.01752316
Iteration 102, loss = 0.01719852
Iteration 103, loss = 0.01736729
Iteration 104, loss = 0.01728667
Iteration 105, loss = 0.01716477
Iteration 106, loss = 0.01732110
Iteration 107, loss = 0.01722780
Iteration 108, loss = 0.01734708
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68230440
Iteration 2, loss = 0.65493961
Iteration 3, loss = 0.61334855
Iteration 4, loss = 0.55419994
Iteration 5, loss = 0.48631538
Iteration 6, loss = 0.41703467
Iteration 7, loss = 0.35199159
Iteration 8, loss = 0.29428461
Iteration 9, loss = 0.24533082
Iteration 10, loss = 0.20481097
Iteration 11, loss = 0.17232169
Iteration 12, loss = 0.14618417
Iteration 13, loss = 0.12551818
Iteration 14, loss = 0.10911327
Iteration 15, loss = 0.09575218
Iteration 16, loss = 0.08488959
Iteration 17, loss = 0.07607050
Iteration 18, loss = 0.06876247
Iteration 19, loss = 0.06270816
Iteration 20, loss = 0.05741202
Iteration 21, loss = 0.05309761
Iteration 22, loss = 0.04933470
Iteration 23, loss = 0.04598861
Iteration 24, loss = 0.04331869
Iteration 25, loss = 0.04075496
Iteration 26, loss = 0.03855216
Iteration 27, loss = 0.03667154
Iteration 28, loss = 0.03493120
Iteration 29, loss = 0.03345573
Iteration 30, loss = 0.03215814
Iteration 31, loss = 0.03081110
Iteration 32, loss = 0.02968225
Iteration 33, loss = 0.02862891
Iteration 34, loss = 0.02782259
Iteration 35, loss = 0.02684971
Iteration 36, loss = 0.02606572
Iteration 37, loss = 0.02552206
Iteration 38, loss = 0.02471944
Iteration 39, loss = 0.02410763
Iteration 40, loss = 0.02366247
Iteration 41, loss = 0.02310456
Iteration 42, loss = 0.02266578
Iteration 43, loss = 0.02221608
Iteration 44, loss = 0.02179555
Iteration 45, loss = 0.02140105
Iteration 46, loss = 0.02120452
Iteration 47, loss = 0.02073634
Iteration 48, loss = 0.02054098
Iteration 49, loss = 0.02029887
Iteration 50, loss = 0.01979727
Iteration 51, loss = 0.01969252
Iteration 52, loss = 0.01932365
Iteration 53, loss = 0.01932537
Iteration 54, loss = 0.01892479
Iteration 55, loss = 0.01873841
Iteration 56, loss = 0.01843651
Iteration 57, loss = 0.01846316
Iteration 58, loss = 0.01810347
Iteration 59, loss = 0.01792895
Iteration 60, loss = 0.01776092
Iteration 61, loss = 0.01765469
Iteration 62, loss = 0.01756939
Iteration 63, loss = 0.01729380
Iteration 64, loss = 0.01728825
Iteration 65, loss = 0.01716988
Iteration 66, loss = 0.01696544
Iteration 67, loss = 0.01693303
Iteration 68, loss = 0.01679018
Iteration 69, loss = 0.01673895
Iteration 70, loss = 0.01673972
Iteration 71, loss = 0.01643550
Iteration 72, loss = 0.01649942
Iteration 73, loss = 0.01642108
Iteration 74, loss = 0.01622131
Iteration 75, loss = 0.01625080
Iteration 76, loss = 0.01614551
Iteration 77, loss = 0.01607659
Iteration 78, loss = 0.01595403
Iteration 79, loss = 0.01604949
Iteration 80, loss = 0.01588138
Iteration 81, loss = 0.01571935
Iteration 82, loss = 0.01580950
Iteration 83, loss = 0.01564893
Iteration 84, loss = 0.01552880
Iteration 85, loss = 0.01569244
Iteration 86, loss = 0.01543354
Iteration 87, loss = 0.01539960
Iteration 88, loss = 0.01543720
Iteration 89, loss = 0.01539393
Iteration 90, loss = 0.01532319
Iteration 91, loss = 0.01531183
Iteration 92, loss = 0.01534166
Iteration 93, loss = 0.01526205
Iteration 94, loss = 0.01529449
Iteration 95, loss = 0.01520622
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.72297951
Iteration 2, loss = 0.68100829
Iteration 3, loss = 0.62597279
Iteration 4, loss = 0.56108340
Iteration 5, loss = 0.48814075
Iteration 6, loss = 0.41548167
Iteration 7, loss = 0.34818372
Iteration 8, loss = 0.29022400
Iteration 9, loss = 0.24178093
Iteration 10, loss = 0.20253118
Iteration 11, loss = 0.17116220
Iteration 12, loss = 0.14611019
Iteration 13, loss = 0.12611435
Iteration 14, loss = 0.11029120
Iteration 15, loss = 0.09732731
Iteration 16, loss = 0.08693866
Iteration 17, loss = 0.07831788
Iteration 18, loss = 0.07130782
Iteration 19, loss = 0.06534467
Iteration 20, loss = 0.06016303
Iteration 21, loss = 0.05597414
Iteration 22, loss = 0.05224634
Iteration 23, loss = 0.04906096
Iteration 24, loss = 0.04624858
Iteration 25, loss = 0.04382566
Iteration 26, loss = 0.04178918
Iteration 27, loss = 0.03985716
Iteration 28, loss = 0.03812983
Iteration 29, loss = 0.03650084
Iteration 30, loss = 0.03520324
Iteration 31, loss = 0.03395075
Iteration 32, loss = 0.03292996
Iteration 33, loss = 0.03190230
Iteration 34, loss = 0.03089062
Iteration 35, loss = 0.02998345
Iteration 36, loss = 0.02931821
Iteration 37, loss = 0.02873121
Iteration 38, loss = 0.02817024
Iteration 39, loss = 0.02752479
Iteration 40, loss = 0.02693201
Iteration 41, loss = 0.02642955
Iteration 42, loss = 0.02601957
Iteration 43, loss = 0.02562141
Iteration 44, loss = 0.02507813
Iteration 45, loss = 0.02476047
Iteration 46, loss = 0.02450596
Iteration 47, loss = 0.02403395
Iteration 48, loss = 0.02378597
Iteration 49, loss = 0.02356251
Iteration 50, loss = 0.02316067
Iteration 51, loss = 0.02304226
Iteration 52, loss = 0.02262555
Iteration 53, loss = 0.02246253
Iteration 54, loss = 0.02237909
Iteration 55, loss = 0.02208733
Iteration 56, loss = 0.02200888
Iteration 57, loss = 0.02186392
Iteration 58, loss = 0.02151404
Iteration 59, loss = 0.02139743
Iteration 60, loss = 0.02147793
Iteration 61, loss = 0.02105169
Iteration 62, loss = 0.02093202
Iteration 63, loss = 0.02082772
Iteration 64, loss = 0.02073134
Iteration 65, loss = 0.02074117
Iteration 66, loss = 0.02053490
Iteration 67, loss = 0.02050934
Iteration 68, loss = 0.02021663
Iteration 69, loss = 0.02006221
Iteration 70, loss = 0.02013757
Iteration 71, loss = 0.02013649
Iteration 72, loss = 0.01998443
Iteration 73, loss = 0.01989916
Iteration 74, loss = 0.01982650
Iteration 75, loss = 0.01968928
Iteration 76, loss = 0.01956881
Iteration 77, loss = 0.01941984
Iteration 78, loss = 0.01946646
Iteration 79, loss = 0.01946956
Iteration 80, loss = 0.01942529
Iteration 81, loss = 0.01934983
Iteration 82, loss = 0.01920683
Iteration 83, loss = 0.01933861
Iteration 84, loss = 0.01920384
Iteration 85, loss = 0.01911395
Iteration 86, loss = 0.01900353
Iteration 87, loss = 0.01898839
Iteration 88, loss = 0.01894623
Iteration 89, loss = 0.01893071
Iteration 90, loss = 0.01875065
Iteration 91, loss = 0.01876976
Iteration 92, loss = 0.01867926
Iteration 93, loss = 0.01870370
Iteration 94, loss = 0.01878869
Iteration 95, loss = 0.01874544
Iteration 96, loss = 0.01870570
Iteration 97, loss = 0.01850130
Iteration 98, loss = 0.01853759
Iteration 99, loss = 0.01849856
Iteration 100, loss = 0.01861834
Iteration 101, loss = 0.01841890
Iteration 102, loss = 0.01835939
Iteration 103, loss = 0.01829842
Iteration 104, loss = 0.01832519
Iteration 105, loss = 0.01845876
Iteration 106, loss = 0.01833358
Iteration 107, loss = 0.01832751
Iteration 108, loss = 0.01841347
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69356664
Iteration 2, loss = 0.67647626
Iteration 3, loss = 0.66209528
Iteration 4, loss = 0.64788454
Iteration 5, loss = 0.63275916
Iteration 6, loss = 0.61634422
Iteration 7, loss = 0.59846401
Iteration 8, loss = 0.57931444
Iteration 9, loss = 0.55886135
Iteration 10, loss = 0.53724101
Iteration 11, loss = 0.51474826
Iteration 12, loss = 0.49153925
Iteration 13, loss = 0.46814102
Iteration 14, loss = 0.44442854
Iteration 15, loss = 0.42094817
Iteration 16, loss = 0.39783396
Iteration 17, loss = 0.37519437
Iteration 18, loss = 0.35332451
Iteration 19, loss = 0.33213770
Iteration 20, loss = 0.31181405
Iteration 21, loss = 0.29258147
Iteration 22, loss = 0.27423355
Iteration 23, loss = 0.25701799
Iteration 24, loss = 0.24084488
Iteration 25, loss = 0.22578269
Iteration 26, loss = 0.21159998
Iteration 27, loss = 0.19845057
Iteration 28, loss = 0.18619588
Iteration 29, loss = 0.17486952
Iteration 30, loss = 0.16427213
Iteration 31, loss = 0.15449653
Iteration 32, loss = 0.14549147
Iteration 33, loss = 0.13709317
Iteration 34, loss = 0.12935916
Iteration 35, loss = 0.12215175
Iteration 36, loss = 0.11555119
Iteration 37, loss = 0.10939345
Iteration 38, loss = 0.10373445
Iteration 39, loss = 0.09845686
Iteration 40, loss = 0.09354778
Iteration 41, loss = 0.08901038
Iteration 42, loss = 0.08474595
Iteration 43, loss = 0.08084392
Iteration 44, loss = 0.07718559
Iteration 45, loss = 0.07379040
Iteration 46, loss = 0.07062235
Iteration 47, loss = 0.06769872
Iteration 48, loss = 0.06491871
Iteration 49, loss = 0.06234102
Iteration 50, loss = 0.05996985
Iteration 51, loss = 0.05765842
Iteration 52, loss = 0.05558169
Iteration 53, loss = 0.05358182
Iteration 54, loss = 0.05169364
Iteration 55, loss = 0.04989192
Iteration 56, loss = 0.04822015
Iteration 57, loss = 0.04665248
Iteration 58, loss = 0.04515492
Iteration 59, loss = 0.04373398
Iteration 60, loss = 0.04237511
Iteration 61, loss = 0.04110694
Iteration 62, loss = 0.03992060
Iteration 63, loss = 0.03878815
Iteration 64, loss = 0.03767399
Iteration 65, loss = 0.03667192
Iteration 66, loss = 0.03567478
Iteration 67, loss = 0.03474524
Iteration 68, loss = 0.03387418
Iteration 69, loss = 0.03302591
Iteration 70, loss = 0.03223890
Iteration 71, loss = 0.03142976
Iteration 72, loss = 0.03068332
Iteration 73, loss = 0.02997742
Iteration 74, loss = 0.02932886
Iteration 75, loss = 0.02865589
Iteration 76, loss = 0.02803512
Iteration 77, loss = 0.02743652
Iteration 78, loss = 0.02687284
Iteration 79, loss = 0.02637483
Iteration 80, loss = 0.02584051
Iteration 81, loss = 0.02534872
Iteration 82, loss = 0.02492636
Iteration 83, loss = 0.02444335
Iteration 84, loss = 0.02401163
Iteration 85, loss = 0.02358559
Iteration 86, loss = 0.02318511
Iteration 87, loss = 0.02276665
Iteration 88, loss = 0.02240201
Iteration 89, loss = 0.02203175
Iteration 90, loss = 0.02165851
Iteration 91, loss = 0.02129530
Iteration 92, loss = 0.02093639
Iteration 93, loss = 0.02060192
Iteration 94, loss = 0.02027337
Iteration 95, loss = 0.01996752
Iteration 96, loss = 0.01970536
Iteration 97, loss = 0.01939160
Iteration 98, loss = 0.01912037
Iteration 99, loss = 0.01887602
Iteration 100, loss = 0.01863693
Iteration 101, loss = 0.01842073
Iteration 102, loss = 0.01815615
Iteration 103, loss = 0.01793107
Iteration 104, loss = 0.01768994
Iteration 105, loss = 0.01745593
Iteration 106, loss = 0.01723863
Iteration 107, loss = 0.01702419
Iteration 108, loss = 0.01688197
Iteration 109, loss = 0.01662635
Iteration 110, loss = 0.01643917
Iteration 111, loss = 0.01627187
Iteration 112, loss = 0.01611436
Iteration 113, loss = 0.01591556
Iteration 114, loss = 0.01573748
Iteration 115, loss = 0.01560692
Iteration 116, loss = 0.01546792
Iteration 117, loss = 0.01533308
Iteration 118, loss = 0.01514468
Iteration 119, loss = 0.01503386
Iteration 120, loss = 0.01485930
Iteration 121, loss = 0.01470234
Iteration 122, loss = 0.01457647
Iteration 123, loss = 0.01443643
Iteration 124, loss = 0.01429369
Iteration 125, loss = 0.01419540
Iteration 126, loss = 0.01403912
Iteration 127, loss = 0.01394549
Iteration 128, loss = 0.01385599
Iteration 129, loss = 0.01369224
Iteration 130, loss = 0.01358259
Iteration 131, loss = 0.01351978
Iteration 132, loss = 0.01345758
Iteration 133, loss = 0.01327911
Iteration 134, loss = 0.01316749
Iteration 135, loss = 0.01307512
Iteration 136, loss = 0.01298381
Iteration 137, loss = 0.01287924
Iteration 138, loss = 0.01280814
Iteration 139, loss = 0.01269884
Iteration 140, loss = 0.01262676
Iteration 141, loss = 0.01252226
Iteration 142, loss = 0.01243953
Iteration 143, loss = 0.01237650
Iteration 144, loss = 0.01231770
Iteration 145, loss = 0.01220170
Iteration 146, loss = 0.01212890
Iteration 147, loss = 0.01207910
Iteration 148, loss = 0.01199880
Iteration 149, loss = 0.01192639
Iteration 150, loss = 0.01187495
Iteration 151, loss = 0.01180676
Iteration 152, loss = 0.01176858
Iteration 153, loss = 0.01171763
Iteration 154, loss = 0.01162919
Iteration 155, loss = 0.01161678
Iteration 156, loss = 0.01150262
Iteration 157, loss = 0.01140727
Iteration 158, loss = 0.01137652
Iteration 159, loss = 0.01125335
Iteration 160, loss = 0.01119047
Iteration 161, loss = 0.01114277
Iteration 162, loss = 0.01108379
Iteration 163, loss = 0.01102297
Iteration 164, loss = 0.01099622
Iteration 165, loss = 0.01091918
Iteration 166, loss = 0.01086962
Iteration 167, loss = 0.01084919
Iteration 168, loss = 0.01078918
Iteration 169, loss = 0.01070848
Iteration 170, loss = 0.01066537
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68603215
Iteration 2, loss = 0.66685959
Iteration 3, loss = 0.64154704
Iteration 4, loss = 0.60620501
Iteration 5, loss = 0.56172619
Iteration 6, loss = 0.51221146
Iteration 7, loss = 0.46189742
Iteration 8, loss = 0.41323957
Iteration 9, loss = 0.36702953
Iteration 10, loss = 0.32457333
Iteration 11, loss = 0.28620210
Iteration 12, loss = 0.25226262
Iteration 13, loss = 0.22246611
Iteration 14, loss = 0.19680864
Iteration 15, loss = 0.17478721
Iteration 16, loss = 0.15572708
Iteration 17, loss = 0.13969903
Iteration 18, loss = 0.12584921
Iteration 19, loss = 0.11423377
Iteration 20, loss = 0.10405911
Iteration 21, loss = 0.09526688
Iteration 22, loss = 0.08762679
Iteration 23, loss = 0.08104057
Iteration 24, loss = 0.07520712
Iteration 25, loss = 0.06996935
Iteration 26, loss = 0.06544830
Iteration 27, loss = 0.06137018
Iteration 28, loss = 0.05795887
Iteration 29, loss = 0.05478189
Iteration 30, loss = 0.05190453
Iteration 31, loss = 0.04929726
Iteration 32, loss = 0.04692083
Iteration 33, loss = 0.04482439
Iteration 34, loss = 0.04292215
Iteration 35, loss = 0.04117783
Iteration 36, loss = 0.03948029
Iteration 37, loss = 0.03793058
Iteration 38, loss = 0.03661624
Iteration 39, loss = 0.03523940
Iteration 40, loss = 0.03422820
Iteration 41, loss = 0.03318188
Iteration 42, loss = 0.03241709
Iteration 43, loss = 0.03139174
Iteration 44, loss = 0.03069142
Iteration 45, loss = 0.02974376
Iteration 46, loss = 0.02902037
Iteration 47, loss = 0.02817692
Iteration 48, loss = 0.02751312
Iteration 49, loss = 0.02671168
Iteration 50, loss = 0.02630781
Iteration 51, loss = 0.02575619
Iteration 52, loss = 0.02529092
Iteration 53, loss = 0.02475230
Iteration 54, loss = 0.02432845
Iteration 55, loss = 0.02397676
Iteration 56, loss = 0.02374731
Iteration 57, loss = 0.02329890
Iteration 58, loss = 0.02286968
Iteration 59, loss = 0.02245624
Iteration 60, loss = 0.02212818
Iteration 61, loss = 0.02171958
Iteration 62, loss = 0.02139601
Iteration 63, loss = 0.02112019
Iteration 64, loss = 0.02096326
Iteration 65, loss = 0.02080304
Iteration 66, loss = 0.02047807
Iteration 67, loss = 0.02034927
Iteration 68, loss = 0.02014831
Iteration 69, loss = 0.01984356
Iteration 70, loss = 0.01976527
Iteration 71, loss = 0.01966717
Iteration 72, loss = 0.01933092
Iteration 73, loss = 0.01923353
Iteration 74, loss = 0.01908644
Iteration 75, loss = 0.01881412
Iteration 76, loss = 0.01862944
Iteration 77, loss = 0.01821629
Iteration 78, loss = 0.01838202
Iteration 79, loss = 0.01818931
Iteration 80, loss = 0.01795877
Iteration 81, loss = 0.01786815
Iteration 82, loss = 0.01768991
Iteration 83, loss = 0.01757656
Iteration 84, loss = 0.01744403
Iteration 85, loss = 0.01728422
Iteration 86, loss = 0.01731490
Iteration 87, loss = 0.01718527
Iteration 88, loss = 0.01700637
Iteration 89, loss = 0.01674474
Iteration 90, loss = 0.01664010
Iteration 91, loss = 0.01656014
Iteration 92, loss = 0.01643796
Iteration 93, loss = 0.01646709
Iteration 94, loss = 0.01642342
Iteration 95, loss = 0.01626295
Iteration 96, loss = 0.01613014
Iteration 97, loss = 0.01605789
Iteration 98, loss = 0.01614127
Iteration 99, loss = 0.01597276
Iteration 100, loss = 0.01592274
Iteration 101, loss = 0.01582798
Iteration 102, loss = 0.01593813
Iteration 103, loss = 0.01605582
Iteration 104, loss = 0.01587293
Iteration 105, loss = 0.01574326
Iteration 106, loss = 0.01551996
Iteration 107, loss = 0.01546547
Iteration 108, loss = 0.01541321
Iteration 109, loss = 0.01551938
Iteration 110, loss = 0.01538404
Iteration 111, loss = 0.01542266
Iteration 112, loss = 0.01540584
Iteration 113, loss = 0.01528128
Iteration 114, loss = 0.01520368
Iteration 115, loss = 0.01513036
Iteration 116, loss = 0.01489891
Iteration 117, loss = 0.01491331
Iteration 118, loss = 0.01492158
Iteration 119, loss = 0.01483381
Iteration 120, loss = 0.01479845
Iteration 121, loss = 0.01477155
Iteration 122, loss = 0.01475529
Iteration 123, loss = 0.01464100
Iteration 124, loss = 0.01461384
Iteration 125, loss = 0.01472368
Iteration 126, loss = 0.01470845
Iteration 127, loss = 0.01452915
Iteration 128, loss = 0.01452690
Iteration 129, loss = 0.01438392
Iteration 130, loss = 0.01451972
Iteration 131, loss = 0.01477937
Iteration 132, loss = 0.01520660
Iteration 133, loss = 0.01512368
Iteration 134, loss = 0.01453396
Iteration 135, loss = 0.01431208
Iteration 136, loss = 0.01440588
Iteration 137, loss = 0.01444915
Iteration 138, loss = 0.01436769
Iteration 139, loss = 0.01430405
Iteration 140, loss = 0.01423431
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68055797
Iteration 2, loss = 0.65891326
Iteration 3, loss = 0.63764268
Iteration 4, loss = 0.61374587
Iteration 5, loss = 0.58622388
Iteration 6, loss = 0.55495265
Iteration 7, loss = 0.52062821
Iteration 8, loss = 0.48355857
Iteration 9, loss = 0.44529083
Iteration 10, loss = 0.40762878
Iteration 11, loss = 0.37107055
Iteration 12, loss = 0.33710940
Iteration 13, loss = 0.30532161
Iteration 14, loss = 0.27611936
Iteration 15, loss = 0.24959897
Iteration 16, loss = 0.22566605
Iteration 17, loss = 0.20426737
Iteration 18, loss = 0.18519450
Iteration 19, loss = 0.16816271
Iteration 20, loss = 0.15304802
Iteration 21, loss = 0.13988636
Iteration 22, loss = 0.12794348
Iteration 23, loss = 0.11754715
Iteration 24, loss = 0.10830994
Iteration 25, loss = 0.10008592
Iteration 26, loss = 0.09274893
Iteration 27, loss = 0.08626891
Iteration 28, loss = 0.08040897
Iteration 29, loss = 0.07519413
Iteration 30, loss = 0.07051952
Iteration 31, loss = 0.06624485
Iteration 32, loss = 0.06245708
Iteration 33, loss = 0.05892997
Iteration 34, loss = 0.05575995
Iteration 35, loss = 0.05286778
Iteration 36, loss = 0.05024584
Iteration 37, loss = 0.04789566
Iteration 38, loss = 0.04566551
Iteration 39, loss = 0.04361469
Iteration 40, loss = 0.04168649
Iteration 41, loss = 0.03998882
Iteration 42, loss = 0.03839918
Iteration 43, loss = 0.03686786
Iteration 44, loss = 0.03545565
Iteration 45, loss = 0.03420346
Iteration 46, loss = 0.03299416
Iteration 47, loss = 0.03183971
Iteration 48, loss = 0.03080401
Iteration 49, loss = 0.02977854
Iteration 50, loss = 0.02887631
Iteration 51, loss = 0.02800217
Iteration 52, loss = 0.02719348
Iteration 53, loss = 0.02639639
Iteration 54, loss = 0.02565767
Iteration 55, loss = 0.02497763
Iteration 56, loss = 0.02432193
Iteration 57, loss = 0.02373963
Iteration 58, loss = 0.02316985
Iteration 59, loss = 0.02262401
Iteration 60, loss = 0.02210178
Iteration 61, loss = 0.02158609
Iteration 62, loss = 0.02113088
Iteration 63, loss = 0.02068788
Iteration 64, loss = 0.02022648
Iteration 65, loss = 0.01987499
Iteration 66, loss = 0.01948210
Iteration 67, loss = 0.01911603
Iteration 68, loss = 0.01876343
Iteration 69, loss = 0.01839340
Iteration 70, loss = 0.01810160
Iteration 71, loss = 0.01780349
Iteration 72, loss = 0.01753627
Iteration 73, loss = 0.01718115
Iteration 74, loss = 0.01693125
Iteration 75, loss = 0.01673855
Iteration 76, loss = 0.01645012
Iteration 77, loss = 0.01621836
Iteration 78, loss = 0.01596695
Iteration 79, loss = 0.01571841
Iteration 80, loss = 0.01553597
Iteration 81, loss = 0.01533472
Iteration 82, loss = 0.01512380
Iteration 83, loss = 0.01493733
Iteration 84, loss = 0.01476491
Iteration 85, loss = 0.01460315
Iteration 86, loss = 0.01438144
Iteration 87, loss = 0.01424447
Iteration 88, loss = 0.01407509
Iteration 89, loss = 0.01393001
Iteration 90, loss = 0.01381505
Iteration 91, loss = 0.01365003
Iteration 92, loss = 0.01350263
Iteration 93, loss = 0.01340719
Iteration 94, loss = 0.01324355
Iteration 95, loss = 0.01316581
Iteration 96, loss = 0.01299945
Iteration 97, loss = 0.01289823
Iteration 98, loss = 0.01280821
Iteration 99, loss = 0.01266738
Iteration 100, loss = 0.01254742
Iteration 101, loss = 0.01247122
Iteration 102, loss = 0.01235512
Iteration 103, loss = 0.01227415
Iteration 104, loss = 0.01215226
Iteration 105, loss = 0.01207343
Iteration 106, loss = 0.01199519
Iteration 107, loss = 0.01187897
Iteration 108, loss = 0.01182133
Iteration 109, loss = 0.01175620
Iteration 110, loss = 0.01163199
Iteration 111, loss = 0.01159986
Iteration 112, loss = 0.01153578
Iteration 113, loss = 0.01141902
Iteration 114, loss = 0.01133903
Iteration 115, loss = 0.01129816
Iteration 116, loss = 0.01118463
Iteration 117, loss = 0.01114323
Iteration 118, loss = 0.01110177
Iteration 119, loss = 0.01104510
Iteration 120, loss = 0.01097496
Iteration 121, loss = 0.01094642
Iteration 122, loss = 0.01086371
Iteration 123, loss = 0.01083586
Iteration 124, loss = 0.01078702
Iteration 125, loss = 0.01074142
Iteration 126, loss = 0.01066115
Iteration 127, loss = 0.01058820
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71341459
Iteration 2, loss = 0.65925206
Iteration 3, loss = 0.58550766
Iteration 4, loss = 0.49565949
Iteration 5, loss = 0.40530837
Iteration 6, loss = 0.32301370
Iteration 7, loss = 0.25608840
Iteration 8, loss = 0.20459394
Iteration 9, loss = 0.16610266
Iteration 10, loss = 0.13750404
Iteration 11, loss = 0.11632844
Iteration 12, loss = 0.10027597
Iteration 13, loss = 0.08768901
Iteration 14, loss = 0.07802085
Iteration 15, loss = 0.07039877
Iteration 16, loss = 0.06384193
Iteration 17, loss = 0.05880812
Iteration 18, loss = 0.05457913
Iteration 19, loss = 0.05134061
Iteration 20, loss = 0.04802735
Iteration 21, loss = 0.04540592
Iteration 22, loss = 0.04312480
Iteration 23, loss = 0.04125838
Iteration 24, loss = 0.03946029
Iteration 25, loss = 0.03807688
Iteration 26, loss = 0.03697153
Iteration 27, loss = 0.03578777
Iteration 28, loss = 0.03450188
Iteration 29, loss = 0.03365804
Iteration 30, loss = 0.03286184
Iteration 31, loss = 0.03200597
Iteration 32, loss = 0.03153252
Iteration 33, loss = 0.03089461
Iteration 34, loss = 0.03030760
Iteration 35, loss = 0.02987545
Iteration 36, loss = 0.02931777
Iteration 37, loss = 0.02889947
Iteration 38, loss = 0.02855209
Iteration 39, loss = 0.02822291
Iteration 40, loss = 0.02786656
Iteration 41, loss = 0.02750677
Iteration 42, loss = 0.02720154
Iteration 43, loss = 0.02695970
Iteration 44, loss = 0.02672828
Iteration 45, loss = 0.02657749
Iteration 46, loss = 0.02630049
Iteration 47, loss = 0.02615153
Iteration 48, loss = 0.02605947
Iteration 49, loss = 0.02580034
Iteration 50, loss = 0.02552967
Iteration 51, loss = 0.02531751
Iteration 52, loss = 0.02546640
Iteration 53, loss = 0.02496403
Iteration 54, loss = 0.02505305
Iteration 55, loss = 0.02482866
Iteration 56, loss = 0.02488284
Iteration 57, loss = 0.02461943
Iteration 58, loss = 0.02446768
Iteration 59, loss = 0.02466519
Iteration 60, loss = 0.02438537
Iteration 61, loss = 0.02418496
Iteration 62, loss = 0.02426346
Iteration 63, loss = 0.02399881
Iteration 64, loss = 0.02398238
Iteration 65, loss = 0.02395900
Iteration 66, loss = 0.02379093
Iteration 67, loss = 0.02366199
Iteration 68, loss = 0.02377353
Iteration 69, loss = 0.02359805
Iteration 70, loss = 0.02390974
Iteration 71, loss = 0.02352636
Iteration 72, loss = 0.02348937
Iteration 73, loss = 0.02356287
Iteration 74, loss = 0.02370980
Iteration 75, loss = 0.02326114
Iteration 76, loss = 0.02329018
Iteration 77, loss = 0.02306775
Iteration 78, loss = 0.02307965
Iteration 79, loss = 0.02328072
Iteration 80, loss = 0.02316623
Iteration 81, loss = 0.02331782
Iteration 82, loss = 0.02289308
Iteration 83, loss = 0.02304276
Iteration 84, loss = 0.02308948
Iteration 85, loss = 0.02288485
Iteration 86, loss = 0.02277847
Iteration 87, loss = 0.02301564
Iteration 88, loss = 0.02297945
Iteration 89, loss = 0.02294735
Iteration 90, loss = 0.02301900
Iteration 91, loss = 0.02277232
Iteration 92, loss = 0.02276319
Iteration 93, loss = 0.02294942
Iteration 94, loss = 0.02302176
Iteration 95, loss = 0.02278895
Iteration 96, loss = 0.02270740
Iteration 97, loss = 0.02286005
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69885943
Iteration 2, loss = 0.67635420
Iteration 3, loss = 0.65697590
Iteration 4, loss = 0.63782099
Iteration 5, loss = 0.61848063
Iteration 6, loss = 0.59854226
Iteration 7, loss = 0.57764412
Iteration 8, loss = 0.55564242
Iteration 9, loss = 0.53290525
Iteration 10, loss = 0.50915834
Iteration 11, loss = 0.48515192
Iteration 12, loss = 0.46094406
Iteration 13, loss = 0.43672111
Iteration 14, loss = 0.41273595
Iteration 15, loss = 0.38938229
Iteration 16, loss = 0.36678228
Iteration 17, loss = 0.34497609
Iteration 18, loss = 0.32437961
Iteration 19, loss = 0.30478402
Iteration 20, loss = 0.28639864
Iteration 21, loss = 0.26907181
Iteration 22, loss = 0.25286170
Iteration 23, loss = 0.23762223
Iteration 24, loss = 0.22338149
Iteration 25, loss = 0.21019496
Iteration 26, loss = 0.19775006
Iteration 27, loss = 0.18616485
Iteration 28, loss = 0.17546005
Iteration 29, loss = 0.16538823
Iteration 30, loss = 0.15607698
Iteration 31, loss = 0.14742387
Iteration 32, loss = 0.13929492
Iteration 33, loss = 0.13186251
Iteration 34, loss = 0.12488535
Iteration 35, loss = 0.11831475
Iteration 36, loss = 0.11226507
Iteration 37, loss = 0.10656978
Iteration 38, loss = 0.10128275
Iteration 39, loss = 0.09638422
Iteration 40, loss = 0.09174559
Iteration 41, loss = 0.08746509
Iteration 42, loss = 0.08343383
Iteration 43, loss = 0.07964197
Iteration 44, loss = 0.07620963
Iteration 45, loss = 0.07285170
Iteration 46, loss = 0.06983292
Iteration 47, loss = 0.06694884
Iteration 48, loss = 0.06425611
Iteration 49, loss = 0.06170997
Iteration 50, loss = 0.05935151
Iteration 51, loss = 0.05705507
Iteration 52, loss = 0.05497120
Iteration 53, loss = 0.05297232
Iteration 54, loss = 0.05112004
Iteration 55, loss = 0.04930608
Iteration 56, loss = 0.04768486
Iteration 57, loss = 0.04604969
Iteration 58, loss = 0.04450200
Iteration 59, loss = 0.04308154
Iteration 60, loss = 0.04169934
Iteration 61, loss = 0.04044121
Iteration 62, loss = 0.03920217
Iteration 63, loss = 0.03799842
Iteration 64, loss = 0.03688656
Iteration 65, loss = 0.03584165
Iteration 66, loss = 0.03480034
Iteration 67, loss = 0.03382224
Iteration 68, loss = 0.03291638
Iteration 69, loss = 0.03200674
Iteration 70, loss = 0.03116536
Iteration 71, loss = 0.03036903
Iteration 72, loss = 0.02959248
Iteration 73, loss = 0.02885841
Iteration 74, loss = 0.02813378
Iteration 75, loss = 0.02746867
Iteration 76, loss = 0.02681290
Iteration 77, loss = 0.02619887
Iteration 78, loss = 0.02557614
Iteration 79, loss = 0.02502438
Iteration 80, loss = 0.02448354
Iteration 81, loss = 0.02395484
Iteration 82, loss = 0.02342623
Iteration 83, loss = 0.02292697
Iteration 84, loss = 0.02246436
Iteration 85, loss = 0.02202254
Iteration 86, loss = 0.02154848
Iteration 87, loss = 0.02117921
Iteration 88, loss = 0.02074632
Iteration 89, loss = 0.02038241
Iteration 90, loss = 0.01999301
Iteration 91, loss = 0.01969216
Iteration 92, loss = 0.01932012
Iteration 93, loss = 0.01898266
Iteration 94, loss = 0.01865865
Iteration 95, loss = 0.01832354
Iteration 96, loss = 0.01798418
Iteration 97, loss = 0.01768957
Iteration 98, loss = 0.01739936
Iteration 99, loss = 0.01710654
Iteration 100, loss = 0.01683128
Iteration 101, loss = 0.01655059
Iteration 102, loss = 0.01627617
Iteration 103, loss = 0.01601097
Iteration 104, loss = 0.01576426
Iteration 105, loss = 0.01550378
Iteration 106, loss = 0.01537883
Iteration 107, loss = 0.01507499
Iteration 108, loss = 0.01484244
Iteration 109, loss = 0.01464855
Iteration 110, loss = 0.01441146
Iteration 111, loss = 0.01424412
Iteration 112, loss = 0.01405102
Iteration 113, loss = 0.01389510
Iteration 114, loss = 0.01375185
Iteration 115, loss = 0.01355337
Iteration 116, loss = 0.01339594
Iteration 117, loss = 0.01321514
Iteration 118, loss = 0.01302113
Iteration 119, loss = 0.01284799
Iteration 120, loss = 0.01267823
Iteration 121, loss = 0.01259406
Iteration 122, loss = 0.01237368
Iteration 123, loss = 0.01225761
Iteration 124, loss = 0.01213337
Iteration 125, loss = 0.01195693
Iteration 126, loss = 0.01185446
Iteration 127, loss = 0.01169846
Iteration 128, loss = 0.01155062
Iteration 129, loss = 0.01153412
Iteration 130, loss = 0.01133608
Iteration 131, loss = 0.01121601
Iteration 132, loss = 0.01109189
Iteration 133, loss = 0.01097809
Iteration 134, loss = 0.01086532
Iteration 135, loss = 0.01075034
Iteration 136, loss = 0.01064766
Iteration 137, loss = 0.01055480
Iteration 138, loss = 0.01046800
Iteration 139, loss = 0.01038116
Iteration 140, loss = 0.01028101
Iteration 141, loss = 0.01020373
Iteration 142, loss = 0.01007169
Iteration 143, loss = 0.00997776
Iteration 144, loss = 0.00992198
Iteration 145, loss = 0.00980414
Iteration 146, loss = 0.00974576
Iteration 147, loss = 0.00963569
Iteration 148, loss = 0.00956939
Iteration 149, loss = 0.00950507
Iteration 150, loss = 0.00940398
Iteration 151, loss = 0.00936786
Iteration 152, loss = 0.00929303
Iteration 153, loss = 0.00923220
Iteration 154, loss = 0.00911049
Iteration 155, loss = 0.00909335
Iteration 156, loss = 0.00902605
Iteration 157, loss = 0.00889634
Iteration 158, loss = 0.00882972
Iteration 159, loss = 0.00877372
Iteration 160, loss = 0.00879998
Iteration 161, loss = 0.00864317
Iteration 162, loss = 0.00860231
Iteration 163, loss = 0.00852013
Iteration 164, loss = 0.00846474
Iteration 165, loss = 0.00841984
Iteration 166, loss = 0.00836306
Iteration 167, loss = 0.00830792
Iteration 168, loss = 0.00825615
Iteration 169, loss = 0.00818601
Iteration 170, loss = 0.00813174
Iteration 171, loss = 0.00808877
Iteration 172, loss = 0.00802703
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68408863
Iteration 2, loss = 0.62593706
Iteration 3, loss = 0.53988028
Iteration 4, loss = 0.43822332
Iteration 5, loss = 0.34401984
Iteration 6, loss = 0.26635794
Iteration 7, loss = 0.20751845
Iteration 8, loss = 0.16460126
Iteration 9, loss = 0.13393697
Iteration 10, loss = 0.11157491
Iteration 11, loss = 0.09524926
Iteration 12, loss = 0.08280440
Iteration 13, loss = 0.07301922
Iteration 14, loss = 0.06536618
Iteration 15, loss = 0.05932038
Iteration 16, loss = 0.05419391
Iteration 17, loss = 0.05012719
Iteration 18, loss = 0.04682586
Iteration 19, loss = 0.04411423
Iteration 20, loss = 0.04162911
Iteration 21, loss = 0.03958712
Iteration 22, loss = 0.03773860
Iteration 23, loss = 0.03606983
Iteration 24, loss = 0.03434533
Iteration 25, loss = 0.03314064
Iteration 26, loss = 0.03205350
Iteration 27, loss = 0.03094899
Iteration 28, loss = 0.03029157
Iteration 29, loss = 0.02950554
Iteration 30, loss = 0.02887116
Iteration 31, loss = 0.02809732
Iteration 32, loss = 0.02755815
Iteration 33, loss = 0.02737089
Iteration 34, loss = 0.02707725
Iteration 35, loss = 0.02636948
Iteration 36, loss = 0.02606593
Iteration 37, loss = 0.02569198
Iteration 38, loss = 0.02502619
Iteration 39, loss = 0.02475664
Iteration 40, loss = 0.02448361
Iteration 41, loss = 0.02412400
Iteration 42, loss = 0.02401527
Iteration 43, loss = 0.02361063
Iteration 44, loss = 0.02351924
Iteration 45, loss = 0.02327430
Iteration 46, loss = 0.02314814
Iteration 47, loss = 0.02306690
Iteration 48, loss = 0.02271843
Iteration 49, loss = 0.02271704
Iteration 50, loss = 0.02250080
Iteration 51, loss = 0.02222213
Iteration 52, loss = 0.02230433
Iteration 53, loss = 0.02181058
Iteration 54, loss = 0.02195508
Iteration 55, loss = 0.02231542
Iteration 56, loss = 0.02209852
Iteration 57, loss = 0.02187608
Iteration 58, loss = 0.02191542
Iteration 59, loss = 0.02154774
Iteration 60, loss = 0.02153107
Iteration 61, loss = 0.02151246
Iteration 62, loss = 0.02115285
Iteration 63, loss = 0.02106293
Iteration 64, loss = 0.02113914
Iteration 65, loss = 0.02106182
Iteration 66, loss = 0.02126826
Iteration 67, loss = 0.02154473
Iteration 68, loss = 0.02155472
Iteration 69, loss = 0.02265359
Iteration 70, loss = 0.02160269
Iteration 71, loss = 0.02102972
Iteration 72, loss = 0.02091725
Iteration 73, loss = 0.02064873
Iteration 74, loss = 0.02061723
Iteration 75, loss = 0.02059371
Iteration 76, loss = 0.02058335
Iteration 77, loss = 0.02068455
Iteration 78, loss = 0.02050577
Iteration 79, loss = 0.02037900
Iteration 80, loss = 0.02032764
Iteration 81, loss = 0.02017072
Iteration 82, loss = 0.02002854
Iteration 83, loss = 0.02019904
Iteration 84, loss = 0.02004462
Iteration 85, loss = 0.02008778
Iteration 86, loss = 0.01996740
Iteration 87, loss = 0.02021237
Iteration 88, loss = 0.01998843
Iteration 89, loss = 0.01998150
Iteration 90, loss = 0.01989349
Iteration 91, loss = 0.02016108
Iteration 92, loss = 0.01984064
Iteration 93, loss = 0.02042428
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68088186
Iteration 2, loss = 0.62772803
Iteration 3, loss = 0.54407350
Iteration 4, loss = 0.44700466
Iteration 5, loss = 0.35469567
Iteration 6, loss = 0.27789741
Iteration 7, loss = 0.21862135
Iteration 8, loss = 0.17417162
Iteration 9, loss = 0.14228139
Iteration 10, loss = 0.11916440
Iteration 11, loss = 0.10192653
Iteration 12, loss = 0.08921878
Iteration 13, loss = 0.07869172
Iteration 14, loss = 0.07084653
Iteration 15, loss = 0.06454835
Iteration 16, loss = 0.05936424
Iteration 17, loss = 0.05528725
Iteration 18, loss = 0.05199635
Iteration 19, loss = 0.04897311
Iteration 20, loss = 0.04633366
Iteration 21, loss = 0.04432580
Iteration 22, loss = 0.04248978
Iteration 23, loss = 0.04092918
Iteration 24, loss = 0.03973103
Iteration 25, loss = 0.03872836
Iteration 26, loss = 0.03802168
Iteration 27, loss = 0.03727120
Iteration 28, loss = 0.03592673
Iteration 29, loss = 0.03485337
Iteration 30, loss = 0.03442253
Iteration 31, loss = 0.03389079
Iteration 32, loss = 0.03327546
Iteration 33, loss = 0.03271249
Iteration 34, loss = 0.03241034
Iteration 35, loss = 0.03251142
Iteration 36, loss = 0.03204614
Iteration 37, loss = 0.03147885
Iteration 38, loss = 0.03090392
Iteration 39, loss = 0.03083954
Iteration 40, loss = 0.03035442
Iteration 41, loss = 0.03039218
Iteration 42, loss = 0.02988749
Iteration 43, loss = 0.02996066
Iteration 44, loss = 0.02965981
Iteration 45, loss = 0.02901804
Iteration 46, loss = 0.02897315
Iteration 47, loss = 0.02881732
Iteration 48, loss = 0.02900099
Iteration 49, loss = 0.02899196
Iteration 50, loss = 0.02840318
Iteration 51, loss = 0.02807111
Iteration 52, loss = 0.02826388
Iteration 53, loss = 0.02775592
Iteration 54, loss = 0.02762315
Iteration 55, loss = 0.02779612
Iteration 56, loss = 0.02783694
Iteration 57, loss = 0.02747814
Iteration 58, loss = 0.02722846
Iteration 59, loss = 0.02861326
Iteration 60, loss = 0.02780644
Iteration 61, loss = 0.02735859
Iteration 62, loss = 0.02719179
Iteration 63, loss = 0.02699552
Iteration 64, loss = 0.02701490
Iteration 65, loss = 0.02700526
Iteration 66, loss = 0.02732569
Iteration 67, loss = 0.02719669
Iteration 68, loss = 0.02693265
Iteration 69, loss = 0.02660106
Iteration 70, loss = 0.02651737
Iteration 71, loss = 0.02733114
Iteration 72, loss = 0.02849348
Iteration 73, loss = 0.02781695
Iteration 74, loss = 0.02790376
Iteration 75, loss = 0.02744080
Iteration 76, loss = 0.02728092
Iteration 77, loss = 0.02680159
Iteration 78, loss = 0.02635441
Iteration 79, loss = 0.02635285
Iteration 80, loss = 0.02604314
Iteration 81, loss = 0.02601840
Iteration 82, loss = 0.02603202
Iteration 83, loss = 0.02577314
Iteration 84, loss = 0.02565627
Iteration 85, loss = 0.02569969
Iteration 86, loss = 0.02587365
Iteration 87, loss = 0.02580560
Iteration 88, loss = 0.02562008
Iteration 89, loss = 0.02552465
Iteration 90, loss = 0.02553157
Iteration 91, loss = 0.02587805
Iteration 92, loss = 0.02580509
Iteration 93, loss = 0.02549811
Iteration 94, loss = 0.02534389
Iteration 95, loss = 0.02557279
Iteration 96, loss = 0.02546718
Iteration 97, loss = 0.02557372
Iteration 98, loss = 0.02539305
Iteration 99, loss = 0.02607324
Iteration 100, loss = 0.02588682
Iteration 101, loss = 0.02586458
Iteration 102, loss = 0.02690315
Iteration 103, loss = 0.02678757
Iteration 104, loss = 0.02657935
Iteration 105, loss = 0.02634021
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68165897
Iteration 2, loss = 0.65457002
Iteration 3, loss = 0.62398019
Iteration 4, loss = 0.58534377
Iteration 5, loss = 0.53920652
Iteration 6, loss = 0.48882951
Iteration 7, loss = 0.43774201
Iteration 8, loss = 0.38871254
Iteration 9, loss = 0.34311691
Iteration 10, loss = 0.30182048
Iteration 11, loss = 0.26491494
Iteration 12, loss = 0.23270901
Iteration 13, loss = 0.20468830
Iteration 14, loss = 0.18076180
Iteration 15, loss = 0.16028675
Iteration 16, loss = 0.14297869
Iteration 17, loss = 0.12805414
Iteration 18, loss = 0.11529231
Iteration 19, loss = 0.10446704
Iteration 20, loss = 0.09520928
Iteration 21, loss = 0.08722471
Iteration 22, loss = 0.08014487
Iteration 23, loss = 0.07401456
Iteration 24, loss = 0.06853444
Iteration 25, loss = 0.06377858
Iteration 26, loss = 0.05951795
Iteration 27, loss = 0.05592762
Iteration 28, loss = 0.05254497
Iteration 29, loss = 0.04962071
Iteration 30, loss = 0.04691017
Iteration 31, loss = 0.04459905
Iteration 32, loss = 0.04235230
Iteration 33, loss = 0.04022759
Iteration 34, loss = 0.03845797
Iteration 35, loss = 0.03679173
Iteration 36, loss = 0.03524890
Iteration 37, loss = 0.03388560
Iteration 38, loss = 0.03254595
Iteration 39, loss = 0.03140880
Iteration 40, loss = 0.03033265
Iteration 41, loss = 0.02928729
Iteration 42, loss = 0.02832877
Iteration 43, loss = 0.02737851
Iteration 44, loss = 0.02657041
Iteration 45, loss = 0.02583246
Iteration 46, loss = 0.02511652
Iteration 47, loss = 0.02442599
Iteration 48, loss = 0.02374982
Iteration 49, loss = 0.02316903
Iteration 50, loss = 0.02266110
Iteration 51, loss = 0.02212588
Iteration 52, loss = 0.02165960
Iteration 53, loss = 0.02116094
Iteration 54, loss = 0.02071110
Iteration 55, loss = 0.02034520
Iteration 56, loss = 0.01998240
Iteration 57, loss = 0.01962154
Iteration 58, loss = 0.01932838
Iteration 59, loss = 0.01898141
Iteration 60, loss = 0.01862113
Iteration 61, loss = 0.01826092
Iteration 62, loss = 0.01795119
Iteration 63, loss = 0.01768872
Iteration 64, loss = 0.01740374
Iteration 65, loss = 0.01720096
Iteration 66, loss = 0.01693123
Iteration 67, loss = 0.01675189
Iteration 68, loss = 0.01648274
Iteration 69, loss = 0.01635372
Iteration 70, loss = 0.01610520
Iteration 71, loss = 0.01589623
Iteration 72, loss = 0.01579535
Iteration 73, loss = 0.01575884
Iteration 74, loss = 0.01559927
Iteration 75, loss = 0.01571098
Iteration 76, loss = 0.01562284
Iteration 77, loss = 0.01539597
Iteration 78, loss = 0.01481757
Iteration 79, loss = 0.01465231
Iteration 80, loss = 0.01443157
Iteration 81, loss = 0.01444035
Iteration 82, loss = 0.01429903
Iteration 83, loss = 0.01396520
Iteration 84, loss = 0.01407956
Iteration 85, loss = 0.01407365
Iteration 86, loss = 0.01393362
Iteration 87, loss = 0.01373327
Iteration 88, loss = 0.01386860
Iteration 89, loss = 0.01376597
Iteration 90, loss = 0.01366454
Iteration 91, loss = 0.01352190
Iteration 92, loss = 0.01350798
Iteration 93, loss = 0.01336548
Iteration 94, loss = 0.01374722
Iteration 95, loss = 0.01356784
Iteration 96, loss = 0.01327914
Iteration 97, loss = 0.01289830
Iteration 98, loss = 0.01273145
Iteration 99, loss = 0.01262807
Iteration 100, loss = 0.01260545
Iteration 101, loss = 0.01256822
Iteration 102, loss = 0.01250405
Iteration 103, loss = 0.01243655
Iteration 104, loss = 0.01231111
Iteration 105, loss = 0.01223872
Iteration 106, loss = 0.01216977
Iteration 107, loss = 0.01212261
Iteration 108, loss = 0.01201514
Iteration 109, loss = 0.01198586
Iteration 110, loss = 0.01190562
Iteration 111, loss = 0.01185702
Iteration 112, loss = 0.01181482
Iteration 113, loss = 0.01176066
Iteration 114, loss = 0.01174347
Iteration 115, loss = 0.01174294
Iteration 116, loss = 0.01164967
Iteration 117, loss = 0.01160451
Iteration 118, loss = 0.01163999
Iteration 119, loss = 0.01155629
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68162982
Iteration 2, loss = 0.62721373
Iteration 3, loss = 0.54613278
Iteration 4, loss = 0.44850291
Iteration 5, loss = 0.35614204
Iteration 6, loss = 0.27862338
Iteration 7, loss = 0.21875826
Iteration 8, loss = 0.17385858
Iteration 9, loss = 0.14157160
Iteration 10, loss = 0.11813924
Iteration 11, loss = 0.10098042
Iteration 12, loss = 0.08815367
Iteration 13, loss = 0.07836616
Iteration 14, loss = 0.07042083
Iteration 15, loss = 0.06429209
Iteration 16, loss = 0.05949684
Iteration 17, loss = 0.05526020
Iteration 18, loss = 0.05187947
Iteration 19, loss = 0.04875409
Iteration 20, loss = 0.04617444
Iteration 21, loss = 0.04421495
Iteration 22, loss = 0.04235226
Iteration 23, loss = 0.04083388
Iteration 24, loss = 0.03952032
Iteration 25, loss = 0.03833075
Iteration 26, loss = 0.03737833
Iteration 27, loss = 0.03629235
Iteration 28, loss = 0.03605988
Iteration 29, loss = 0.03524546
Iteration 30, loss = 0.03423653
Iteration 31, loss = 0.03347789
Iteration 32, loss = 0.03283875
Iteration 33, loss = 0.03253264
Iteration 34, loss = 0.03185545
Iteration 35, loss = 0.03199215
Iteration 36, loss = 0.03163432
Iteration 37, loss = 0.03089493
Iteration 38, loss = 0.03056881
Iteration 39, loss = 0.03037465
Iteration 40, loss = 0.03005118
Iteration 41, loss = 0.03000887
Iteration 42, loss = 0.02997521
Iteration 43, loss = 0.03001675
Iteration 44, loss = 0.02931535
Iteration 45, loss = 0.02881498
Iteration 46, loss = 0.02850776
Iteration 47, loss = 0.02834464
Iteration 48, loss = 0.02808236
Iteration 49, loss = 0.02793666
Iteration 50, loss = 0.02762517
Iteration 51, loss = 0.02812733
Iteration 52, loss = 0.02791118
Iteration 53, loss = 0.02797506
Iteration 54, loss = 0.02767772
Iteration 55, loss = 0.02818408
Iteration 56, loss = 0.02763038
Iteration 57, loss = 0.02745971
Iteration 58, loss = 0.02704613
Iteration 59, loss = 0.02708305
Iteration 60, loss = 0.02752451
Iteration 61, loss = 0.02740080
Iteration 62, loss = 0.02706358
Iteration 63, loss = 0.02687835
Iteration 64, loss = 0.02680191
Iteration 65, loss = 0.02739622
Iteration 66, loss = 0.02732539
Iteration 67, loss = 0.02666218
Iteration 68, loss = 0.02646476
Iteration 69, loss = 0.02625561
Iteration 70, loss = 0.02615710
Iteration 71, loss = 0.02610771
Iteration 72, loss = 0.02596651
Iteration 73, loss = 0.02615456
Iteration 74, loss = 0.02635407
Iteration 75, loss = 0.02645567
Iteration 76, loss = 0.02657050
Iteration 77, loss = 0.02674617
Iteration 78, loss = 0.02709578
Iteration 79, loss = 0.02650816
Iteration 80, loss = 0.02638001
Iteration 81, loss = 0.02611727
Iteration 82, loss = 0.02588913
Iteration 83, loss = 0.02594974
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70424100
Iteration 2, loss = 0.64345505
Iteration 3, loss = 0.55961326
Iteration 4, loss = 0.45785271
Iteration 5, loss = 0.36126372
Iteration 6, loss = 0.28082256
Iteration 7, loss = 0.21953771
Iteration 8, loss = 0.17415534
Iteration 9, loss = 0.14159321
Iteration 10, loss = 0.11780843
Iteration 11, loss = 0.10102591
Iteration 12, loss = 0.08761923
Iteration 13, loss = 0.07738500
Iteration 14, loss = 0.06948416
Iteration 15, loss = 0.06336134
Iteration 16, loss = 0.05848895
Iteration 17, loss = 0.05400606
Iteration 18, loss = 0.05052273
Iteration 19, loss = 0.04752758
Iteration 20, loss = 0.04514468
Iteration 21, loss = 0.04259798
Iteration 22, loss = 0.04088509
Iteration 23, loss = 0.03919353
Iteration 24, loss = 0.03773209
Iteration 25, loss = 0.03632642
Iteration 26, loss = 0.03529733
Iteration 27, loss = 0.03431092
Iteration 28, loss = 0.03323583
Iteration 29, loss = 0.03314448
Iteration 30, loss = 0.03239189
Iteration 31, loss = 0.03140899
Iteration 32, loss = 0.03091420
Iteration 33, loss = 0.03050089
Iteration 34, loss = 0.03002940
Iteration 35, loss = 0.02924324
Iteration 36, loss = 0.02880432
Iteration 37, loss = 0.02831840
Iteration 38, loss = 0.02827116
Iteration 39, loss = 0.02779795
Iteration 40, loss = 0.02790644
Iteration 41, loss = 0.02727894
Iteration 42, loss = 0.02697238
Iteration 43, loss = 0.02681605
Iteration 44, loss = 0.02653402
Iteration 45, loss = 0.02628669
Iteration 46, loss = 0.02609374
Iteration 47, loss = 0.02598553
Iteration 48, loss = 0.02569945
Iteration 49, loss = 0.02565331
Iteration 50, loss = 0.02569869
Iteration 51, loss = 0.02626182
Iteration 52, loss = 0.02667656
Iteration 53, loss = 0.02617443
Iteration 54, loss = 0.02554441
Iteration 55, loss = 0.02568195
Iteration 56, loss = 0.02516405
Iteration 57, loss = 0.02533127
Iteration 58, loss = 0.02501977
Iteration 59, loss = 0.02476381
Iteration 60, loss = 0.02491345
Iteration 61, loss = 0.02475947
Iteration 62, loss = 0.02471288
Iteration 63, loss = 0.02434766
Iteration 64, loss = 0.02454182
Iteration 65, loss = 0.02438428
Iteration 66, loss = 0.02394061
Iteration 67, loss = 0.02425024
Iteration 68, loss = 0.02395395
Iteration 69, loss = 0.02401936
Iteration 70, loss = 0.02403720
Iteration 71, loss = 0.02413674
Iteration 72, loss = 0.02412497
Iteration 73, loss = 0.02377435
Iteration 74, loss = 0.02393255
Iteration 75, loss = 0.02390168
Iteration 76, loss = 0.02360228
Iteration 77, loss = 0.02359510
Iteration 78, loss = 0.02333190
Iteration 79, loss = 0.02322499
Iteration 80, loss = 0.02326357
Iteration 81, loss = 0.02323722
Iteration 82, loss = 0.02319659
Iteration 83, loss = 0.02344399
Iteration 84, loss = 0.02308061
Iteration 85, loss = 0.02317404
Iteration 86, loss = 0.02301328
Iteration 87, loss = 0.02286872
Iteration 88, loss = 0.02325909
Iteration 89, loss = 0.02383352
Iteration 90, loss = 0.02360383
Iteration 91, loss = 0.02311502
Iteration 92, loss = 0.02293219
Iteration 93, loss = 0.02323943
Iteration 94, loss = 0.02296897
Iteration 95, loss = 0.02262361
Iteration 96, loss = 0.02324944
Iteration 97, loss = 0.02357509
Iteration 98, loss = 0.02341786
Iteration 99, loss = 0.02315539
Iteration 100, loss = 0.02394777
Iteration 101, loss = 0.02418967
Iteration 102, loss = 0.02362334
Iteration 103, loss = 0.02335140
Iteration 104, loss = 0.02348684
Iteration 105, loss = 0.02371636
Iteration 106, loss = 0.02349405
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68318401
Iteration 2, loss = 0.65009373
Iteration 3, loss = 0.60332933
Iteration 4, loss = 0.54316737
Iteration 5, loss = 0.47751954
Iteration 6, loss = 0.41099203
Iteration 7, loss = 0.34864071
Iteration 8, loss = 0.29328094
Iteration 9, loss = 0.24618727
Iteration 10, loss = 0.20707539
Iteration 11, loss = 0.17545265
Iteration 12, loss = 0.15006039
Iteration 13, loss = 0.12989794
Iteration 14, loss = 0.11363070
Iteration 15, loss = 0.10051951
Iteration 16, loss = 0.08989367
Iteration 17, loss = 0.08118958
Iteration 18, loss = 0.07404770
Iteration 19, loss = 0.06785547
Iteration 20, loss = 0.06261751
Iteration 21, loss = 0.05828390
Iteration 22, loss = 0.05447230
Iteration 23, loss = 0.05130899
Iteration 24, loss = 0.04849000
Iteration 25, loss = 0.04602845
Iteration 26, loss = 0.04402034
Iteration 27, loss = 0.04178523
Iteration 28, loss = 0.04010861
Iteration 29, loss = 0.03857067
Iteration 30, loss = 0.03729224
Iteration 31, loss = 0.03607413
Iteration 32, loss = 0.03486486
Iteration 33, loss = 0.03382973
Iteration 34, loss = 0.03293871
Iteration 35, loss = 0.03215356
Iteration 36, loss = 0.03132154
Iteration 37, loss = 0.03064389
Iteration 38, loss = 0.03006401
Iteration 39, loss = 0.02942399
Iteration 40, loss = 0.02889251
Iteration 41, loss = 0.02815633
Iteration 42, loss = 0.02773490
Iteration 43, loss = 0.02743988
Iteration 44, loss = 0.02704622
Iteration 45, loss = 0.02662522
Iteration 46, loss = 0.02617989
Iteration 47, loss = 0.02577897
Iteration 48, loss = 0.02561412
Iteration 49, loss = 0.02526071
Iteration 50, loss = 0.02499948
Iteration 51, loss = 0.02480148
Iteration 52, loss = 0.02442728
Iteration 53, loss = 0.02425090
Iteration 54, loss = 0.02390512
Iteration 55, loss = 0.02374660
Iteration 56, loss = 0.02350541
Iteration 57, loss = 0.02335535
Iteration 58, loss = 0.02330834
Iteration 59, loss = 0.02298066
Iteration 60, loss = 0.02298910
Iteration 61, loss = 0.02276287
Iteration 62, loss = 0.02255869
Iteration 63, loss = 0.02257826
Iteration 64, loss = 0.02248244
Iteration 65, loss = 0.02227213
Iteration 66, loss = 0.02207899
Iteration 67, loss = 0.02201235
Iteration 68, loss = 0.02199064
Iteration 69, loss = 0.02175521
Iteration 70, loss = 0.02182605
Iteration 71, loss = 0.02172657
Iteration 72, loss = 0.02139679
Iteration 73, loss = 0.02153180
Iteration 74, loss = 0.02132334
Iteration 75, loss = 0.02120096
Iteration 76, loss = 0.02119107
Iteration 77, loss = 0.02112149
Iteration 78, loss = 0.02108918
Iteration 79, loss = 0.02092473
Iteration 80, loss = 0.02083918
Iteration 81, loss = 0.02095418
Iteration 82, loss = 0.02072607
Iteration 83, loss = 0.02071674
Iteration 84, loss = 0.02058700
Iteration 85, loss = 0.02057785
Iteration 86, loss = 0.02068475
Iteration 87, loss = 0.02051887
Iteration 88, loss = 0.02035965
Iteration 89, loss = 0.02039630
Iteration 90, loss = 0.02042952
Iteration 91, loss = 0.02038937
Iteration 92, loss = 0.02025479
Iteration 93, loss = 0.02026634
Iteration 94, loss = 0.02028536
Iteration 95, loss = 0.02007656
Iteration 96, loss = 0.02021433
Iteration 97, loss = 0.02012465
Iteration 98, loss = 0.02002389
Iteration 99, loss = 0.02019510
Iteration 100, loss = 0.01993377
Iteration 101, loss = 0.02002638
Iteration 102, loss = 0.01996019
Iteration 103, loss = 0.01984572
Iteration 104, loss = 0.01985412
Iteration 105, loss = 0.01995269
Iteration 106, loss = 0.01985289
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70380185
Iteration 2, loss = 0.65328130
Iteration 3, loss = 0.58992831
Iteration 4, loss = 0.51066675
Iteration 5, loss = 0.42697202
Iteration 6, loss = 0.35011556
Iteration 7, loss = 0.28375069
Iteration 8, loss = 0.23054204
Iteration 9, loss = 0.18907780
Iteration 10, loss = 0.15729425
Iteration 11, loss = 0.13293323
Iteration 12, loss = 0.11410687
Iteration 13, loss = 0.09951728
Iteration 14, loss = 0.08818529
Iteration 15, loss = 0.07876204
Iteration 16, loss = 0.07127554
Iteration 17, loss = 0.06534445
Iteration 18, loss = 0.06008111
Iteration 19, loss = 0.05583003
Iteration 20, loss = 0.05237306
Iteration 21, loss = 0.04929917
Iteration 22, loss = 0.04645420
Iteration 23, loss = 0.04423085
Iteration 24, loss = 0.04210470
Iteration 25, loss = 0.04056915
Iteration 26, loss = 0.03890358
Iteration 27, loss = 0.03776096
Iteration 28, loss = 0.03629646
Iteration 29, loss = 0.03517391
Iteration 30, loss = 0.03411978
Iteration 31, loss = 0.03316662
Iteration 32, loss = 0.03234114
Iteration 33, loss = 0.03181353
Iteration 34, loss = 0.03113354
Iteration 35, loss = 0.03033607
Iteration 36, loss = 0.02997699
Iteration 37, loss = 0.02950540
Iteration 38, loss = 0.02894800
Iteration 39, loss = 0.02849543
Iteration 40, loss = 0.02814901
Iteration 41, loss = 0.02773323
Iteration 42, loss = 0.02725093
Iteration 43, loss = 0.02722902
Iteration 44, loss = 0.02653185
Iteration 45, loss = 0.02657025
Iteration 46, loss = 0.02629023
Iteration 47, loss = 0.02587860
Iteration 48, loss = 0.02602124
Iteration 49, loss = 0.02550768
Iteration 50, loss = 0.02541706
Iteration 51, loss = 0.02507722
Iteration 52, loss = 0.02481846
Iteration 53, loss = 0.02473689
Iteration 54, loss = 0.02469234
Iteration 55, loss = 0.02410733
Iteration 56, loss = 0.02443381
Iteration 57, loss = 0.02433111
Iteration 58, loss = 0.02424913
Iteration 59, loss = 0.02402599
Iteration 60, loss = 0.02382540
Iteration 61, loss = 0.02368587
Iteration 62, loss = 0.02353707
Iteration 63, loss = 0.02343355
Iteration 64, loss = 0.02328881
Iteration 65, loss = 0.02313780
Iteration 66, loss = 0.02333208
Iteration 67, loss = 0.02317091
Iteration 68, loss = 0.02339817
Iteration 69, loss = 0.02317950
Iteration 70, loss = 0.02332291
Iteration 71, loss = 0.02285171
Iteration 72, loss = 0.02301314
Iteration 73, loss = 0.02262076
Iteration 74, loss = 0.02272935
Iteration 75, loss = 0.02281789
Iteration 76, loss = 0.02264049
Iteration 77, loss = 0.02240358
Iteration 78, loss = 0.02252044
Iteration 79, loss = 0.02230451
Iteration 80, loss = 0.02247524
Iteration 81, loss = 0.02253284
Iteration 82, loss = 0.02245858
Iteration 83, loss = 0.02250447
Iteration 84, loss = 0.02232540
Iteration 85, loss = 0.02216480
Iteration 86, loss = 0.02212864
Iteration 87, loss = 0.02219462
Iteration 88, loss = 0.02200894
Iteration 89, loss = 0.02192747
Iteration 90, loss = 0.02205045
Iteration 91, loss = 0.02199621
Iteration 92, loss = 0.02188202
Iteration 93, loss = 0.02184991
Iteration 94, loss = 0.02201989
Iteration 95, loss = 0.02182665
Iteration 96, loss = 0.02169116
Iteration 97, loss = 0.02176917
Iteration 98, loss = 0.02186698
Iteration 99, loss = 0.02172733
Iteration 100, loss = 0.02150326
Iteration 101, loss = 0.02189315
Iteration 102, loss = 0.02174027
Iteration 103, loss = 0.02167116
Iteration 104, loss = 0.02186949
Iteration 105, loss = 0.02172231
Iteration 106, loss = 0.02164474
Iteration 107, loss = 0.02159432
Iteration 108, loss = 0.02158860
Iteration 109, loss = 0.02155582
Iteration 110, loss = 0.02161220
Iteration 111, loss = 0.02159875
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68463024
Iteration 2, loss = 0.62813791
Iteration 3, loss = 0.53490415
Iteration 4, loss = 0.42997481
Iteration 5, loss = 0.33347821
Iteration 6, loss = 0.25641511
Iteration 7, loss = 0.19886234
Iteration 8, loss = 0.15798938
Iteration 9, loss = 0.12903251
Iteration 10, loss = 0.10843046
Iteration 11, loss = 0.09262285
Iteration 12, loss = 0.08149006
Iteration 13, loss = 0.07261888
Iteration 14, loss = 0.06582161
Iteration 15, loss = 0.06013847
Iteration 16, loss = 0.05570142
Iteration 17, loss = 0.05210517
Iteration 18, loss = 0.04911772
Iteration 19, loss = 0.04633950
Iteration 20, loss = 0.04424211
Iteration 21, loss = 0.04234170
Iteration 22, loss = 0.04089136
Iteration 23, loss = 0.03965770
Iteration 24, loss = 0.03823897
Iteration 25, loss = 0.03719294
Iteration 26, loss = 0.03638680
Iteration 27, loss = 0.03555058
Iteration 28, loss = 0.03478548
Iteration 29, loss = 0.03409668
Iteration 30, loss = 0.03344167
Iteration 31, loss = 0.03324194
Iteration 32, loss = 0.03267346
Iteration 33, loss = 0.03231600
Iteration 34, loss = 0.03196543
Iteration 35, loss = 0.03142551
Iteration 36, loss = 0.03139105
Iteration 37, loss = 0.03098531
Iteration 38, loss = 0.03039941
Iteration 39, loss = 0.03012543
Iteration 40, loss = 0.03010204
Iteration 41, loss = 0.02983906
Iteration 42, loss = 0.02965847
Iteration 43, loss = 0.02961119
Iteration 44, loss = 0.02902329
Iteration 45, loss = 0.02941404
Iteration 46, loss = 0.02902667
Iteration 47, loss = 0.02884115
Iteration 48, loss = 0.02838362
Iteration 49, loss = 0.02865231
Iteration 50, loss = 0.02836545
Iteration 51, loss = 0.02820264
Iteration 52, loss = 0.02820771
Iteration 53, loss = 0.02791230
Iteration 54, loss = 0.02811140
Iteration 55, loss = 0.02818545
Iteration 56, loss = 0.02799761
Iteration 57, loss = 0.02785259
Iteration 58, loss = 0.02766807
Iteration 59, loss = 0.02793767
Iteration 60, loss = 0.02774786
Iteration 61, loss = 0.02758711
Iteration 62, loss = 0.02745174
Iteration 63, loss = 0.02722474
Iteration 64, loss = 0.02735050
Iteration 65, loss = 0.02720438
Iteration 66, loss = 0.02748939
Iteration 67, loss = 0.02776341
Iteration 68, loss = 0.02737115
Iteration 69, loss = 0.02708025
Iteration 70, loss = 0.02708776
Iteration 71, loss = 0.02711039
Iteration 72, loss = 0.02707426
Iteration 73, loss = 0.02704865
Iteration 74, loss = 0.02693819
Iteration 75, loss = 0.02690336
Iteration 76, loss = 0.02696647
Iteration 77, loss = 0.02672459
Iteration 78, loss = 0.02710636
Iteration 79, loss = 0.02705393
Iteration 80, loss = 0.02671618
Iteration 81, loss = 0.02694990
Iteration 82, loss = 0.02665181
Iteration 83, loss = 0.02669592
Iteration 84, loss = 0.02686276
Iteration 85, loss = 0.02672178
Iteration 86, loss = 0.02689294
Iteration 87, loss = 0.02697987
Iteration 88, loss = 0.02664513
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67816079
Iteration 2, loss = 0.62188196
Iteration 3, loss = 0.52828819
Iteration 4, loss = 0.42262925
Iteration 5, loss = 0.32766865
Iteration 6, loss = 0.25159406
Iteration 7, loss = 0.19485954
Iteration 8, loss = 0.15459827
Iteration 9, loss = 0.12578478
Iteration 10, loss = 0.10505812
Iteration 11, loss = 0.08973882
Iteration 12, loss = 0.07799991
Iteration 13, loss = 0.06920999
Iteration 14, loss = 0.06227613
Iteration 15, loss = 0.05653357
Iteration 16, loss = 0.05207569
Iteration 17, loss = 0.04862455
Iteration 18, loss = 0.04509421
Iteration 19, loss = 0.04264617
Iteration 20, loss = 0.04047380
Iteration 21, loss = 0.03846183
Iteration 22, loss = 0.03698188
Iteration 23, loss = 0.03546848
Iteration 24, loss = 0.03402705
Iteration 25, loss = 0.03304857
Iteration 26, loss = 0.03203935
Iteration 27, loss = 0.03117314
Iteration 28, loss = 0.03039024
Iteration 29, loss = 0.02954169
Iteration 30, loss = 0.02889755
Iteration 31, loss = 0.02829941
Iteration 32, loss = 0.02806440
Iteration 33, loss = 0.02749217
Iteration 34, loss = 0.02705421
Iteration 35, loss = 0.02667079
Iteration 36, loss = 0.02631827
Iteration 37, loss = 0.02584045
Iteration 38, loss = 0.02565883
Iteration 39, loss = 0.02532312
Iteration 40, loss = 0.02506832
Iteration 41, loss = 0.02490723
Iteration 42, loss = 0.02490212
Iteration 43, loss = 0.02463914
Iteration 44, loss = 0.02431019
Iteration 45, loss = 0.02429853
Iteration 46, loss = 0.02423933
Iteration 47, loss = 0.02393659
Iteration 48, loss = 0.02383951
Iteration 49, loss = 0.02365007
Iteration 50, loss = 0.02362340
Iteration 51, loss = 0.02335360
Iteration 52, loss = 0.02328398
Iteration 53, loss = 0.02320939
Iteration 54, loss = 0.02320685
Iteration 55, loss = 0.02279573
Iteration 56, loss = 0.02278814
Iteration 57, loss = 0.02313715
Iteration 58, loss = 0.02295538
Iteration 59, loss = 0.02267879
Iteration 60, loss = 0.02268748
Iteration 61, loss = 0.02236193
Iteration 62, loss = 0.02221691
Iteration 63, loss = 0.02249342
Iteration 64, loss = 0.02260574
Iteration 65, loss = 0.02244385
Iteration 66, loss = 0.02262548
Iteration 67, loss = 0.02242095
Iteration 68, loss = 0.02219501
Iteration 69, loss = 0.02207932
Iteration 70, loss = 0.02207774
Iteration 71, loss = 0.02237787
Iteration 72, loss = 0.02237981
Iteration 73, loss = 0.02185880
Iteration 74, loss = 0.02201862
Iteration 75, loss = 0.02191975
Iteration 76, loss = 0.02177554
Iteration 77, loss = 0.02174122
Iteration 78, loss = 0.02185157
Iteration 79, loss = 0.02142551
Iteration 80, loss = 0.02175330
Iteration 81, loss = 0.02193039
Iteration 82, loss = 0.02144841
Iteration 83, loss = 0.02176071
Iteration 84, loss = 0.02148353
Iteration 85, loss = 0.02184941
Iteration 86, loss = 0.02160142
Iteration 87, loss = 0.02141428
Iteration 88, loss = 0.02170168
Iteration 89, loss = 0.02155537
Iteration 90, loss = 0.02159888
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69832691
Iteration 2, loss = 0.67058955
Iteration 3, loss = 0.63360855
Iteration 4, loss = 0.58619322
Iteration 5, loss = 0.52801594
Iteration 6, loss = 0.46487399
Iteration 7, loss = 0.40275708
Iteration 8, loss = 0.34547770
Iteration 9, loss = 0.29470634
Iteration 10, loss = 0.25144131
Iteration 11, loss = 0.21455814
Iteration 12, loss = 0.18420406
Iteration 13, loss = 0.15913294
Iteration 14, loss = 0.13859059
Iteration 15, loss = 0.12178979
Iteration 16, loss = 0.10791299
Iteration 17, loss = 0.09637815
Iteration 18, loss = 0.08677491
Iteration 19, loss = 0.07859714
Iteration 20, loss = 0.07179294
Iteration 21, loss = 0.06592237
Iteration 22, loss = 0.06080404
Iteration 23, loss = 0.05654142
Iteration 24, loss = 0.05274065
Iteration 25, loss = 0.04937728
Iteration 26, loss = 0.04642518
Iteration 27, loss = 0.04397142
Iteration 28, loss = 0.04151614
Iteration 29, loss = 0.03941935
Iteration 30, loss = 0.03766573
Iteration 31, loss = 0.03592065
Iteration 32, loss = 0.03455318
Iteration 33, loss = 0.03326110
Iteration 34, loss = 0.03194696
Iteration 35, loss = 0.03082306
Iteration 36, loss = 0.02978773
Iteration 37, loss = 0.02878110
Iteration 38, loss = 0.02802388
Iteration 39, loss = 0.02705620
Iteration 40, loss = 0.02649538
Iteration 41, loss = 0.02570047
Iteration 42, loss = 0.02520820
Iteration 43, loss = 0.02453620
Iteration 44, loss = 0.02389794
Iteration 45, loss = 0.02347158
Iteration 46, loss = 0.02297985
Iteration 47, loss = 0.02258529
Iteration 48, loss = 0.02208889
Iteration 49, loss = 0.02162088
Iteration 50, loss = 0.02133584
Iteration 51, loss = 0.02099459
Iteration 52, loss = 0.02059143
Iteration 53, loss = 0.02041770
Iteration 54, loss = 0.01999369
Iteration 55, loss = 0.01987837
Iteration 56, loss = 0.01955300
Iteration 57, loss = 0.01917533
Iteration 58, loss = 0.01889459
Iteration 59, loss = 0.01880232
Iteration 60, loss = 0.01876285
Iteration 61, loss = 0.01852192
Iteration 62, loss = 0.01835309
Iteration 63, loss = 0.01819003
Iteration 64, loss = 0.01785025
Iteration 65, loss = 0.01771653
Iteration 66, loss = 0.01757448
Iteration 67, loss = 0.01745652
Iteration 68, loss = 0.01729339
Iteration 69, loss = 0.01694991
Iteration 70, loss = 0.01702429
Iteration 71, loss = 0.01684371
Iteration 72, loss = 0.01673900
Iteration 73, loss = 0.01657563
Iteration 74, loss = 0.01657244
Iteration 75, loss = 0.01658601
Iteration 76, loss = 0.01633734
Iteration 77, loss = 0.01623394
Iteration 78, loss = 0.01611842
Iteration 79, loss = 0.01606145
Iteration 80, loss = 0.01590248
Iteration 81, loss = 0.01598536
Iteration 82, loss = 0.01586981
Iteration 83, loss = 0.01570312
Iteration 84, loss = 0.01564754
Iteration 85, loss = 0.01555423
Iteration 86, loss = 0.01545035
Iteration 87, loss = 0.01547511
Iteration 88, loss = 0.01525284
Iteration 89, loss = 0.01534363
Iteration 90, loss = 0.01533371
Iteration 91, loss = 0.01527900
Iteration 92, loss = 0.01522911
Iteration 93, loss = 0.01520390
Iteration 94, loss = 0.01498988
Iteration 95, loss = 0.01488886
Iteration 96, loss = 0.01500951
Iteration 97, loss = 0.01492763
Iteration 98, loss = 0.01484971
Iteration 99, loss = 0.01485527
Iteration 100, loss = 0.01478241
Iteration 101, loss = 0.01469743
Iteration 102, loss = 0.01471372
Iteration 103, loss = 0.01467704
Iteration 104, loss = 0.01461940
Iteration 105, loss = 0.01452886
Iteration 106, loss = 0.01454807
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69806365
Iteration 2, loss = 0.67818479
Iteration 3, loss = 0.66085665
Iteration 4, loss = 0.64354688
Iteration 5, loss = 0.62605885
Iteration 6, loss = 0.60708292
Iteration 7, loss = 0.58756273
Iteration 8, loss = 0.56656444
Iteration 9, loss = 0.54473737
Iteration 10, loss = 0.52212513
Iteration 11, loss = 0.49853166
Iteration 12, loss = 0.47443112
Iteration 13, loss = 0.45017366
Iteration 14, loss = 0.42583334
Iteration 15, loss = 0.40174304
Iteration 16, loss = 0.37818479
Iteration 17, loss = 0.35494172
Iteration 18, loss = 0.33261297
Iteration 19, loss = 0.31103112
Iteration 20, loss = 0.29052554
Iteration 21, loss = 0.27117903
Iteration 22, loss = 0.25299298
Iteration 23, loss = 0.23601851
Iteration 24, loss = 0.22024199
Iteration 25, loss = 0.20555148
Iteration 26, loss = 0.19201025
Iteration 27, loss = 0.17941602
Iteration 28, loss = 0.16773762
Iteration 29, loss = 0.15704999
Iteration 30, loss = 0.14716266
Iteration 31, loss = 0.13807217
Iteration 32, loss = 0.12966429
Iteration 33, loss = 0.12193327
Iteration 34, loss = 0.11482127
Iteration 35, loss = 0.10828586
Iteration 36, loss = 0.10221781
Iteration 37, loss = 0.09660486
Iteration 38, loss = 0.09141167
Iteration 39, loss = 0.08663766
Iteration 40, loss = 0.08217912
Iteration 41, loss = 0.07804986
Iteration 42, loss = 0.07425893
Iteration 43, loss = 0.07068244
Iteration 44, loss = 0.06737254
Iteration 45, loss = 0.06430287
Iteration 46, loss = 0.06144842
Iteration 47, loss = 0.05877928
Iteration 48, loss = 0.05625775
Iteration 49, loss = 0.05391372
Iteration 50, loss = 0.05172414
Iteration 51, loss = 0.04966145
Iteration 52, loss = 0.04770608
Iteration 53, loss = 0.04585654
Iteration 54, loss = 0.04411253
Iteration 55, loss = 0.04247853
Iteration 56, loss = 0.04095219
Iteration 57, loss = 0.03949532
Iteration 58, loss = 0.03809021
Iteration 59, loss = 0.03680451
Iteration 60, loss = 0.03556625
Iteration 61, loss = 0.03438705
Iteration 62, loss = 0.03328920
Iteration 63, loss = 0.03222532
Iteration 64, loss = 0.03120786
Iteration 65, loss = 0.03026661
Iteration 66, loss = 0.02934960
Iteration 67, loss = 0.02847199
Iteration 68, loss = 0.02764906
Iteration 69, loss = 0.02685072
Iteration 70, loss = 0.02608849
Iteration 71, loss = 0.02536611
Iteration 72, loss = 0.02465961
Iteration 73, loss = 0.02399577
Iteration 74, loss = 0.02335444
Iteration 75, loss = 0.02273125
Iteration 76, loss = 0.02215516
Iteration 77, loss = 0.02158742
Iteration 78, loss = 0.02104556
Iteration 79, loss = 0.02052842
Iteration 80, loss = 0.02001830
Iteration 81, loss = 0.01954867
Iteration 82, loss = 0.01908143
Iteration 83, loss = 0.01863628
Iteration 84, loss = 0.01820400
Iteration 85, loss = 0.01780085
Iteration 86, loss = 0.01739481
Iteration 87, loss = 0.01700169
Iteration 88, loss = 0.01662697
Iteration 89, loss = 0.01625550
Iteration 90, loss = 0.01591098
Iteration 91, loss = 0.01556817
Iteration 92, loss = 0.01523747
Iteration 93, loss = 0.01492232
Iteration 94, loss = 0.01461284
Iteration 95, loss = 0.01431502
Iteration 96, loss = 0.01404598
Iteration 97, loss = 0.01375360
Iteration 98, loss = 0.01349367
Iteration 99, loss = 0.01322106
Iteration 100, loss = 0.01296587
Iteration 101, loss = 0.01271743
Iteration 102, loss = 0.01248317
Iteration 103, loss = 0.01224661
Iteration 104, loss = 0.01202286
Iteration 105, loss = 0.01180716
Iteration 106, loss = 0.01159974
Iteration 107, loss = 0.01139069
Iteration 108, loss = 0.01119655
Iteration 109, loss = 0.01100535
Iteration 110, loss = 0.01081726
Iteration 111, loss = 0.01063560
Iteration 112, loss = 0.01046013
Iteration 113, loss = 0.01028112
Iteration 114, loss = 0.01011645
Iteration 115, loss = 0.00995197
Iteration 116, loss = 0.00979291
Iteration 117, loss = 0.00963562
Iteration 118, loss = 0.00948687
Iteration 119, loss = 0.00934062
Iteration 120, loss = 0.00919836
Iteration 121, loss = 0.00905627
Iteration 122, loss = 0.00892031
Iteration 123, loss = 0.00878627
Iteration 124, loss = 0.00865921
Iteration 125, loss = 0.00852761
Iteration 126, loss = 0.00840540
Iteration 127, loss = 0.00827888
Iteration 128, loss = 0.00817143
Iteration 129, loss = 0.00804855
Iteration 130, loss = 0.00793046
Iteration 131, loss = 0.00782319
Iteration 132, loss = 0.00771303
Iteration 133, loss = 0.00760844
Iteration 134, loss = 0.00750290
Iteration 135, loss = 0.00740218
Iteration 136, loss = 0.00730671
Iteration 137, loss = 0.00721113
Iteration 138, loss = 0.00712078
Iteration 139, loss = 0.00702787
Iteration 140, loss = 0.00693388
Iteration 141, loss = 0.00684660
Iteration 142, loss = 0.00676324
Iteration 143, loss = 0.00667928
Iteration 144, loss = 0.00659676
Iteration 145, loss = 0.00651301
Iteration 146, loss = 0.00643875
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68579811
Iteration 2, loss = 0.66797905
Iteration 3, loss = 0.65047731
Iteration 4, loss = 0.62968450
Iteration 5, loss = 0.60486769
Iteration 6, loss = 0.57588830
Iteration 7, loss = 0.54320984
Iteration 8, loss = 0.50810680
Iteration 9, loss = 0.47184720
Iteration 10, loss = 0.43613403
Iteration 11, loss = 0.40101756
Iteration 12, loss = 0.36726480
Iteration 13, loss = 0.33564434
Iteration 14, loss = 0.30586609
Iteration 15, loss = 0.27838857
Iteration 16, loss = 0.25335263
Iteration 17, loss = 0.23042992
Iteration 18, loss = 0.20985180
Iteration 19, loss = 0.19129761
Iteration 20, loss = 0.17466151
Iteration 21, loss = 0.15981851
Iteration 22, loss = 0.14664074
Iteration 23, loss = 0.13478159
Iteration 24, loss = 0.12429389
Iteration 25, loss = 0.11486347
Iteration 26, loss = 0.10651596
Iteration 27, loss = 0.09900733
Iteration 28, loss = 0.09228938
Iteration 29, loss = 0.08625842
Iteration 30, loss = 0.08083156
Iteration 31, loss = 0.07585148
Iteration 32, loss = 0.07147506
Iteration 33, loss = 0.06738303
Iteration 34, loss = 0.06374553
Iteration 35, loss = 0.06043673
Iteration 36, loss = 0.05738888
Iteration 37, loss = 0.05452869
Iteration 38, loss = 0.05194012
Iteration 39, loss = 0.04956341
Iteration 40, loss = 0.04738119
Iteration 41, loss = 0.04537004
Iteration 42, loss = 0.04353979
Iteration 43, loss = 0.04172498
Iteration 44, loss = 0.04009200
Iteration 45, loss = 0.03859569
Iteration 46, loss = 0.03719751
Iteration 47, loss = 0.03583974
Iteration 48, loss = 0.03469495
Iteration 49, loss = 0.03349709
Iteration 50, loss = 0.03244352
Iteration 51, loss = 0.03141518
Iteration 52, loss = 0.03047858
Iteration 53, loss = 0.02956537
Iteration 54, loss = 0.02869998
Iteration 55, loss = 0.02794476
Iteration 56, loss = 0.02720433
Iteration 57, loss = 0.02642501
Iteration 58, loss = 0.02578711
Iteration 59, loss = 0.02513066
Iteration 60, loss = 0.02455682
Iteration 61, loss = 0.02393931
Iteration 62, loss = 0.02336259
Iteration 63, loss = 0.02285579
Iteration 64, loss = 0.02238349
Iteration 65, loss = 0.02193604
Iteration 66, loss = 0.02145979
Iteration 67, loss = 0.02104861
Iteration 68, loss = 0.02062799
Iteration 69, loss = 0.02022001
Iteration 70, loss = 0.01983814
Iteration 71, loss = 0.01956509
Iteration 72, loss = 0.01910950
Iteration 73, loss = 0.01881624
Iteration 74, loss = 0.01851413
Iteration 75, loss = 0.01815327
Iteration 76, loss = 0.01788658
Iteration 77, loss = 0.01762343
Iteration 78, loss = 0.01730909
Iteration 79, loss = 0.01707207
Iteration 80, loss = 0.01682792
Iteration 81, loss = 0.01659785
Iteration 82, loss = 0.01635655
Iteration 83, loss = 0.01616960
Iteration 84, loss = 0.01591259
Iteration 85, loss = 0.01574091
Iteration 86, loss = 0.01551611
Iteration 87, loss = 0.01530425
Iteration 88, loss = 0.01515070
Iteration 89, loss = 0.01497772
Iteration 90, loss = 0.01476918
Iteration 91, loss = 0.01461521
Iteration 92, loss = 0.01446874
Iteration 93, loss = 0.01429655
Iteration 94, loss = 0.01412339
Iteration 95, loss = 0.01398989
Iteration 96, loss = 0.01384341
Iteration 97, loss = 0.01372679
Iteration 98, loss = 0.01358979
Iteration 99, loss = 0.01342543
Iteration 100, loss = 0.01336617
Iteration 101, loss = 0.01318525
Iteration 102, loss = 0.01307573
Iteration 103, loss = 0.01298630
Iteration 104, loss = 0.01278753
Iteration 105, loss = 0.01275918
Iteration 106, loss = 0.01263123
Iteration 107, loss = 0.01254946
Iteration 108, loss = 0.01238086
Iteration 109, loss = 0.01233370
Iteration 110, loss = 0.01230005
Iteration 111, loss = 0.01215401
Iteration 112, loss = 0.01205471
Iteration 113, loss = 0.01198260
Iteration 114, loss = 0.01185704
Iteration 115, loss = 0.01181651
Iteration 116, loss = 0.01173177
Iteration 117, loss = 0.01161632
Iteration 118, loss = 0.01153199
Iteration 119, loss = 0.01146998
Iteration 120, loss = 0.01138137
Iteration 121, loss = 0.01139059
Iteration 122, loss = 0.01125687
Iteration 123, loss = 0.01118197
Iteration 124, loss = 0.01116809
Iteration 125, loss = 0.01106674
Iteration 126, loss = 0.01100682
Iteration 127, loss = 0.01093581
Iteration 128, loss = 0.01085522
Iteration 129, loss = 0.01079885
Iteration 130, loss = 0.01081078
Iteration 131, loss = 0.01077968
Iteration 132, loss = 0.01061868
Iteration 133, loss = 0.01060810
Iteration 134, loss = 0.01052244
Iteration 135, loss = 0.01053904
Iteration 136, loss = 0.01041782
Iteration 137, loss = 0.01035877
Iteration 138, loss = 0.01036113
Iteration 139, loss = 0.01027262
Iteration 140, loss = 0.01021081
Iteration 141, loss = 0.01019010
Iteration 142, loss = 0.01013793
Iteration 143, loss = 0.01011411
Iteration 144, loss = 0.01009347
Iteration 145, loss = 0.01004209
Iteration 146, loss = 0.00998373
Iteration 147, loss = 0.00992926
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67705658
Iteration 2, loss = 0.62594971
Iteration 3, loss = 0.54884617
Iteration 4, loss = 0.46207245
Iteration 5, loss = 0.37886025
Iteration 6, loss = 0.30599158
Iteration 7, loss = 0.24642748
Iteration 8, loss = 0.20005041
Iteration 9, loss = 0.16466450
Iteration 10, loss = 0.13760685
Iteration 11, loss = 0.11709103
Iteration 12, loss = 0.10118874
Iteration 13, loss = 0.08890174
Iteration 14, loss = 0.07888640
Iteration 15, loss = 0.07127032
Iteration 16, loss = 0.06485034
Iteration 17, loss = 0.05956624
Iteration 18, loss = 0.05521979
Iteration 19, loss = 0.05142754
Iteration 20, loss = 0.04827147
Iteration 21, loss = 0.04562844
Iteration 22, loss = 0.04328600
Iteration 23, loss = 0.04132008
Iteration 24, loss = 0.03935972
Iteration 25, loss = 0.03785920
Iteration 26, loss = 0.03658771
Iteration 27, loss = 0.03504087
Iteration 28, loss = 0.03426160
Iteration 29, loss = 0.03323045
Iteration 30, loss = 0.03239512
Iteration 31, loss = 0.03144680
Iteration 32, loss = 0.03068353
Iteration 33, loss = 0.03028888
Iteration 34, loss = 0.02957634
Iteration 35, loss = 0.02901065
Iteration 36, loss = 0.02868088
Iteration 37, loss = 0.02817012
Iteration 38, loss = 0.02773646
Iteration 39, loss = 0.02730838
Iteration 40, loss = 0.02691538
Iteration 41, loss = 0.02675885
Iteration 42, loss = 0.02632077
Iteration 43, loss = 0.02596905
Iteration 44, loss = 0.02593059
Iteration 45, loss = 0.02548242
Iteration 46, loss = 0.02534055
Iteration 47, loss = 0.02511993
Iteration 48, loss = 0.02512364
Iteration 49, loss = 0.02477379
Iteration 50, loss = 0.02460629
Iteration 51, loss = 0.02448540
Iteration 52, loss = 0.02435091
Iteration 53, loss = 0.02436170
Iteration 54, loss = 0.02428194
Iteration 55, loss = 0.02377798
Iteration 56, loss = 0.02381927
Iteration 57, loss = 0.02350409
Iteration 58, loss = 0.02363033
Iteration 59, loss = 0.02359355
Iteration 60, loss = 0.02339955
Iteration 61, loss = 0.02321853
Iteration 62, loss = 0.02316086
Iteration 63, loss = 0.02327480
Iteration 64, loss = 0.02308332
Iteration 65, loss = 0.02264703
Iteration 66, loss = 0.02292733
Iteration 67, loss = 0.02266662
Iteration 68, loss = 0.02260947
Iteration 69, loss = 0.02275749
Iteration 70, loss = 0.02249847
Iteration 71, loss = 0.02261740
Iteration 72, loss = 0.02248394
Iteration 73, loss = 0.02252003
Iteration 74, loss = 0.02249604
Iteration 75, loss = 0.02239775
Iteration 76, loss = 0.02208818
Iteration 77, loss = 0.02230014
Iteration 78, loss = 0.02231031
Iteration 79, loss = 0.02222159
Iteration 80, loss = 0.02214317
Iteration 81, loss = 0.02209613
Iteration 82, loss = 0.02202834
Iteration 83, loss = 0.02182307
Iteration 84, loss = 0.02214381
Iteration 85, loss = 0.02217329
Iteration 86, loss = 0.02173194
Iteration 87, loss = 0.02189046
Iteration 88, loss = 0.02189733
Iteration 89, loss = 0.02204364
Iteration 90, loss = 0.02177771
Iteration 91, loss = 0.02165353
Iteration 92, loss = 0.02183059
Iteration 93, loss = 0.02201061
Iteration 94, loss = 0.02187877
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67767828
Iteration 2, loss = 0.60591852
Iteration 3, loss = 0.50493675
Iteration 4, loss = 0.40043903
Iteration 5, loss = 0.30908647
Iteration 6, loss = 0.23740149
Iteration 7, loss = 0.18498928
Iteration 8, loss = 0.14772277
Iteration 9, loss = 0.12092363
Iteration 10, loss = 0.10170594
Iteration 11, loss = 0.08748966
Iteration 12, loss = 0.07682226
Iteration 13, loss = 0.06858222
Iteration 14, loss = 0.06167066
Iteration 15, loss = 0.05649094
Iteration 16, loss = 0.05231330
Iteration 17, loss = 0.04873668
Iteration 18, loss = 0.04598047
Iteration 19, loss = 0.04336764
Iteration 20, loss = 0.04129138
Iteration 21, loss = 0.03964711
Iteration 22, loss = 0.03801781
Iteration 23, loss = 0.03694024
Iteration 24, loss = 0.03555936
Iteration 25, loss = 0.03450460
Iteration 26, loss = 0.03369171
Iteration 27, loss = 0.03272357
Iteration 28, loss = 0.03201262
Iteration 29, loss = 0.03118042
Iteration 30, loss = 0.03074228
Iteration 31, loss = 0.03047036
Iteration 32, loss = 0.02965327
Iteration 33, loss = 0.02923193
Iteration 34, loss = 0.02900316
Iteration 35, loss = 0.02860887
Iteration 36, loss = 0.02801905
Iteration 37, loss = 0.02821786
Iteration 38, loss = 0.02778581
Iteration 39, loss = 0.02741291
Iteration 40, loss = 0.02716621
Iteration 41, loss = 0.02688108
Iteration 42, loss = 0.02685123
Iteration 43, loss = 0.02649180
Iteration 44, loss = 0.02615327
Iteration 45, loss = 0.02613942
Iteration 46, loss = 0.02583530
Iteration 47, loss = 0.02585549
Iteration 48, loss = 0.02594935
Iteration 49, loss = 0.02545846
Iteration 50, loss = 0.02551813
Iteration 51, loss = 0.02543764
Iteration 52, loss = 0.02527525
Iteration 53, loss = 0.02502433
Iteration 54, loss = 0.02497598
Iteration 55, loss = 0.02524608
Iteration 56, loss = 0.02491654
Iteration 57, loss = 0.02459925
Iteration 58, loss = 0.02458963
Iteration 59, loss = 0.02488030
Iteration 60, loss = 0.02485623
Iteration 61, loss = 0.02461659
Iteration 62, loss = 0.02488965
Iteration 63, loss = 0.02473520
Iteration 64, loss = 0.02432978
Iteration 65, loss = 0.02459369
Iteration 66, loss = 0.02399672
Iteration 67, loss = 0.02424716
Iteration 68, loss = 0.02448920
Iteration 69, loss = 0.02437904
Iteration 70, loss = 0.02411662
Iteration 71, loss = 0.02411472
Iteration 72, loss = 0.02423809
Iteration 73, loss = 0.02383675
Iteration 74, loss = 0.02404488
Iteration 75, loss = 0.02400419
Iteration 76, loss = 0.02400154
Iteration 77, loss = 0.02358568
Iteration 78, loss = 0.02383383
Iteration 79, loss = 0.02386577
Iteration 80, loss = 0.02367537
Iteration 81, loss = 0.02375630
Iteration 82, loss = 0.02384847
Iteration 83, loss = 0.02364276
Iteration 84, loss = 0.02378664
Iteration 85, loss = 0.02377454
Iteration 86, loss = 0.02343594
Iteration 87, loss = 0.02358136
Iteration 88, loss = 0.02359675
Iteration 89, loss = 0.02381595
Iteration 90, loss = 0.02349916
Iteration 91, loss = 0.02359001
Iteration 92, loss = 0.02346531
Iteration 93, loss = 0.02355316
Iteration 94, loss = 0.02345060
Iteration 95, loss = 0.02355717
Iteration 96, loss = 0.02341394
Iteration 97, loss = 0.02337413
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68128468
Iteration 2, loss = 0.65551990
Iteration 3, loss = 0.62480298
Iteration 4, loss = 0.58328986
Iteration 5, loss = 0.53354583
Iteration 6, loss = 0.48077494
Iteration 7, loss = 0.42895982
Iteration 8, loss = 0.38021220
Iteration 9, loss = 0.33497260
Iteration 10, loss = 0.29464578
Iteration 11, loss = 0.25851219
Iteration 12, loss = 0.22691831
Iteration 13, loss = 0.20000761
Iteration 14, loss = 0.17679334
Iteration 15, loss = 0.15712576
Iteration 16, loss = 0.14020568
Iteration 17, loss = 0.12568015
Iteration 18, loss = 0.11348560
Iteration 19, loss = 0.10288980
Iteration 20, loss = 0.09381370
Iteration 21, loss = 0.08599771
Iteration 22, loss = 0.07923050
Iteration 23, loss = 0.07334404
Iteration 24, loss = 0.06811733
Iteration 25, loss = 0.06352639
Iteration 26, loss = 0.05943371
Iteration 27, loss = 0.05581379
Iteration 28, loss = 0.05255374
Iteration 29, loss = 0.04953893
Iteration 30, loss = 0.04699147
Iteration 31, loss = 0.04457710
Iteration 32, loss = 0.04257359
Iteration 33, loss = 0.04073309
Iteration 34, loss = 0.03889366
Iteration 35, loss = 0.03740662
Iteration 36, loss = 0.03589105
Iteration 37, loss = 0.03458616
Iteration 38, loss = 0.03330776
Iteration 39, loss = 0.03209780
Iteration 40, loss = 0.03109493
Iteration 41, loss = 0.03002864
Iteration 42, loss = 0.02908082
Iteration 43, loss = 0.02828461
Iteration 44, loss = 0.02721051
Iteration 45, loss = 0.02670986
Iteration 46, loss = 0.02598838
Iteration 47, loss = 0.02533231
Iteration 48, loss = 0.02470054
Iteration 49, loss = 0.02415314
Iteration 50, loss = 0.02346972
Iteration 51, loss = 0.02293189
Iteration 52, loss = 0.02243763
Iteration 53, loss = 0.02202847
Iteration 54, loss = 0.02159175
Iteration 55, loss = 0.02119188
Iteration 56, loss = 0.02073089
Iteration 57, loss = 0.02042064
Iteration 58, loss = 0.02007318
Iteration 59, loss = 0.01985487
Iteration 60, loss = 0.01959459
Iteration 61, loss = 0.01923883
Iteration 62, loss = 0.01901281
Iteration 63, loss = 0.01895943
Iteration 64, loss = 0.01816538
Iteration 65, loss = 0.01839062
Iteration 66, loss = 0.01809545
Iteration 67, loss = 0.01774748
Iteration 68, loss = 0.01745515
Iteration 69, loss = 0.01719515
Iteration 70, loss = 0.01699430
Iteration 71, loss = 0.01677317
Iteration 72, loss = 0.01653765
Iteration 73, loss = 0.01641658
Iteration 74, loss = 0.01638895
Iteration 75, loss = 0.01621831
Iteration 76, loss = 0.01611183
Iteration 77, loss = 0.01620636
Iteration 78, loss = 0.01589247
Iteration 79, loss = 0.01577492
Iteration 80, loss = 0.01544860
Iteration 81, loss = 0.01525640
Iteration 82, loss = 0.01511574
Iteration 83, loss = 0.01503180
Iteration 84, loss = 0.01489070
Iteration 85, loss = 0.01487161
Iteration 86, loss = 0.01480795
Iteration 87, loss = 0.01468799
Iteration 88, loss = 0.01457658
Iteration 89, loss = 0.01440521
Iteration 90, loss = 0.01436126
Iteration 91, loss = 0.01445585
Iteration 92, loss = 0.01431259
Iteration 93, loss = 0.01414946
Iteration 94, loss = 0.01400998
Iteration 95, loss = 0.01391077
Iteration 96, loss = 0.01384191
Iteration 97, loss = 0.01394633
Iteration 98, loss = 0.01363939
Iteration 99, loss = 0.01369979
Iteration 100, loss = 0.01354935
Iteration 101, loss = 0.01344181
Iteration 102, loss = 0.01338102
Iteration 103, loss = 0.01328454
Iteration 104, loss = 0.01319673
Iteration 105, loss = 0.01315329
Iteration 106, loss = 0.01311521
Iteration 107, loss = 0.01306214
Iteration 108, loss = 0.01293591
Iteration 109, loss = 0.01301605
Iteration 110, loss = 0.01300639
Iteration 111, loss = 0.01295930
Iteration 112, loss = 0.01282701
Iteration 113, loss = 0.01273641
Iteration 114, loss = 0.01275432
Iteration 115, loss = 0.01257642
Iteration 116, loss = 0.01264645
Iteration 117, loss = 0.01271014
Iteration 118, loss = 0.01262974
Iteration 119, loss = 0.01262652
Iteration 120, loss = 0.01256563
Iteration 121, loss = 0.01256051
Iteration 122, loss = 0.01280419
Iteration 123, loss = 0.01267901
Iteration 124, loss = 0.01254136
Iteration 125, loss = 0.01255200
Iteration 126, loss = 0.01235174
Iteration 127, loss = 0.01230523
Iteration 128, loss = 0.01233443
Iteration 129, loss = 0.01251879
Iteration 130, loss = 0.01246367
Iteration 131, loss = 0.01234993
Iteration 132, loss = 0.01220583
Iteration 133, loss = 0.01188827
Iteration 134, loss = 0.01223064
Iteration 135, loss = 0.01205431
Iteration 136, loss = 0.01195192
Iteration 137, loss = 0.01188420
Iteration 138, loss = 0.01186632
Iteration 139, loss = 0.01177217
Iteration 140, loss = 0.01178361
Iteration 141, loss = 0.01162454
Iteration 142, loss = 0.01184365
Iteration 143, loss = 0.01197527
Iteration 144, loss = 0.01189405
Iteration 145, loss = 0.01161080
Iteration 146, loss = 0.01185955
Iteration 147, loss = 0.01176693
Iteration 148, loss = 0.01178050
Iteration 149, loss = 0.01173181
Iteration 150, loss = 0.01169498
Iteration 151, loss = 0.01168388
Iteration 152, loss = 0.01162832
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71173438
Iteration 2, loss = 0.62371794
Iteration 3, loss = 0.51437526
Iteration 4, loss = 0.39867635
Iteration 5, loss = 0.29982270
Iteration 6, loss = 0.22583461
Iteration 7, loss = 0.17405112
Iteration 8, loss = 0.13859592
Iteration 9, loss = 0.11402108
Iteration 10, loss = 0.09591331
Iteration 11, loss = 0.08323103
Iteration 12, loss = 0.07348901
Iteration 13, loss = 0.06596774
Iteration 14, loss = 0.06021603
Iteration 15, loss = 0.05572713
Iteration 16, loss = 0.05191118
Iteration 17, loss = 0.04855080
Iteration 18, loss = 0.04611921
Iteration 19, loss = 0.04366639
Iteration 20, loss = 0.04185631
Iteration 21, loss = 0.04022809
Iteration 22, loss = 0.03884037
Iteration 23, loss = 0.03766765
Iteration 24, loss = 0.03674703
Iteration 25, loss = 0.03566250
Iteration 26, loss = 0.03513797
Iteration 27, loss = 0.03400024
Iteration 28, loss = 0.03353904
Iteration 29, loss = 0.03296099
Iteration 30, loss = 0.03276457
Iteration 31, loss = 0.03238254
Iteration 32, loss = 0.03143090
Iteration 33, loss = 0.03124840
Iteration 34, loss = 0.03060127
Iteration 35, loss = 0.03062002
Iteration 36, loss = 0.03015941
Iteration 37, loss = 0.03027893
Iteration 38, loss = 0.02995055
Iteration 39, loss = 0.02955521
Iteration 40, loss = 0.02930258
Iteration 41, loss = 0.02916721
Iteration 42, loss = 0.02872385
Iteration 43, loss = 0.02904335
Iteration 44, loss = 0.02883335
Iteration 45, loss = 0.02854688
Iteration 46, loss = 0.02831571
Iteration 47, loss = 0.02827445
Iteration 48, loss = 0.02812074
Iteration 49, loss = 0.02823475
Iteration 50, loss = 0.02834312
Iteration 51, loss = 0.02767853
Iteration 52, loss = 0.02780060
Iteration 53, loss = 0.02777575
Iteration 54, loss = 0.02773998
Iteration 55, loss = 0.02734665
Iteration 56, loss = 0.02757145
Iteration 57, loss = 0.02725732
Iteration 58, loss = 0.02740797
Iteration 59, loss = 0.02699385
Iteration 60, loss = 0.02727797
Iteration 61, loss = 0.02698477
Iteration 62, loss = 0.02706085
Iteration 63, loss = 0.02715931
Iteration 64, loss = 0.02707959
Iteration 65, loss = 0.02692748
Iteration 66, loss = 0.02693664
Iteration 67, loss = 0.02691465
Iteration 68, loss = 0.02696375
Iteration 69, loss = 0.02691853
Iteration 70, loss = 0.02716390
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67900941
Iteration 2, loss = 0.64794553
Iteration 3, loss = 0.60707110
Iteration 4, loss = 0.55258502
Iteration 5, loss = 0.49185721
Iteration 6, loss = 0.43158306
Iteration 7, loss = 0.37437528
Iteration 8, loss = 0.32198168
Iteration 9, loss = 0.27588129
Iteration 10, loss = 0.23597989
Iteration 11, loss = 0.20223032
Iteration 12, loss = 0.17427829
Iteration 13, loss = 0.15129881
Iteration 14, loss = 0.13234084
Iteration 15, loss = 0.11653827
Iteration 16, loss = 0.10370220
Iteration 17, loss = 0.09292575
Iteration 18, loss = 0.08376745
Iteration 19, loss = 0.07627227
Iteration 20, loss = 0.06982454
Iteration 21, loss = 0.06441450
Iteration 22, loss = 0.05951138
Iteration 23, loss = 0.05534868
Iteration 24, loss = 0.05189127
Iteration 25, loss = 0.04867274
Iteration 26, loss = 0.04591743
Iteration 27, loss = 0.04345828
Iteration 28, loss = 0.04120744
Iteration 29, loss = 0.03934013
Iteration 30, loss = 0.03743012
Iteration 31, loss = 0.03600988
Iteration 32, loss = 0.03446732
Iteration 33, loss = 0.03315069
Iteration 34, loss = 0.03203394
Iteration 35, loss = 0.03090813
Iteration 36, loss = 0.02996346
Iteration 37, loss = 0.02893667
Iteration 38, loss = 0.02827161
Iteration 39, loss = 0.02744851
Iteration 40, loss = 0.02674658
Iteration 41, loss = 0.02606275
Iteration 42, loss = 0.02538165
Iteration 43, loss = 0.02489258
Iteration 44, loss = 0.02436140
Iteration 45, loss = 0.02383720
Iteration 46, loss = 0.02338374
Iteration 47, loss = 0.02299487
Iteration 48, loss = 0.02256756
Iteration 49, loss = 0.02215528
Iteration 50, loss = 0.02188533
Iteration 51, loss = 0.02149006
Iteration 52, loss = 0.02118617
Iteration 53, loss = 0.02103890
Iteration 54, loss = 0.02066416
Iteration 55, loss = 0.02057322
Iteration 56, loss = 0.02029194
Iteration 57, loss = 0.01991893
Iteration 58, loss = 0.01973362
Iteration 59, loss = 0.01949112
Iteration 60, loss = 0.01933048
Iteration 61, loss = 0.01908134
Iteration 62, loss = 0.01894745
Iteration 63, loss = 0.01872220
Iteration 64, loss = 0.01863614
Iteration 65, loss = 0.01852139
Iteration 66, loss = 0.01828228
Iteration 67, loss = 0.01824614
Iteration 68, loss = 0.01809132
Iteration 69, loss = 0.01792271
Iteration 70, loss = 0.01780017
Iteration 71, loss = 0.01760785
Iteration 72, loss = 0.01745253
Iteration 73, loss = 0.01743899
Iteration 74, loss = 0.01735520
Iteration 75, loss = 0.01723485
Iteration 76, loss = 0.01712087
Iteration 77, loss = 0.01706352
Iteration 78, loss = 0.01708811
Iteration 79, loss = 0.01693790
Iteration 80, loss = 0.01685217
Iteration 81, loss = 0.01672732
Iteration 82, loss = 0.01668423
Iteration 83, loss = 0.01664433
Iteration 84, loss = 0.01661753
Iteration 85, loss = 0.01645118
Iteration 86, loss = 0.01650620
Iteration 87, loss = 0.01632180
Iteration 88, loss = 0.01638591
Iteration 89, loss = 0.01611966
Iteration 90, loss = 0.01627040
Iteration 91, loss = 0.01617876
Iteration 92, loss = 0.01609976
Iteration 93, loss = 0.01606338
Iteration 94, loss = 0.01601467
Iteration 95, loss = 0.01594937
Iteration 96, loss = 0.01598315
Iteration 97, loss = 0.01580690
Iteration 98, loss = 0.01578785
Iteration 99, loss = 0.01570877
Iteration 100, loss = 0.01572475
Iteration 101, loss = 0.01573620
Iteration 102, loss = 0.01565976
Iteration 103, loss = 0.01561522
Iteration 104, loss = 0.01558245
Iteration 105, loss = 0.01562516
Iteration 106, loss = 0.01548058
Iteration 107, loss = 0.01554407
Iteration 108, loss = 0.01540016
Iteration 109, loss = 0.01558779
Iteration 110, loss = 0.01539612
Iteration 111, loss = 0.01537334
Iteration 112, loss = 0.01537886
Iteration 113, loss = 0.01534973
Iteration 114, loss = 0.01528906
Iteration 115, loss = 0.01526185
Iteration 116, loss = 0.01532216
Iteration 117, loss = 0.01522541
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67338524
Iteration 2, loss = 0.59799283
Iteration 3, loss = 0.49534884
Iteration 4, loss = 0.39141843
Iteration 5, loss = 0.30280512
Iteration 6, loss = 0.23409466
Iteration 7, loss = 0.18324866
Iteration 8, loss = 0.14742939
Iteration 9, loss = 0.12168994
Iteration 10, loss = 0.10283127
Iteration 11, loss = 0.08895567
Iteration 12, loss = 0.07861296
Iteration 13, loss = 0.07039527
Iteration 14, loss = 0.06413895
Iteration 15, loss = 0.05889322
Iteration 16, loss = 0.05491798
Iteration 17, loss = 0.05137017
Iteration 18, loss = 0.04817913
Iteration 19, loss = 0.04593660
Iteration 20, loss = 0.04418479
Iteration 21, loss = 0.04199384
Iteration 22, loss = 0.04066193
Iteration 23, loss = 0.03922781
Iteration 24, loss = 0.03800641
Iteration 25, loss = 0.03709393
Iteration 26, loss = 0.03634248
Iteration 27, loss = 0.03533823
Iteration 28, loss = 0.03483044
Iteration 29, loss = 0.03404822
Iteration 30, loss = 0.03342724
Iteration 31, loss = 0.03282917
Iteration 32, loss = 0.03255377
Iteration 33, loss = 0.03209547
Iteration 34, loss = 0.03173747
Iteration 35, loss = 0.03134318
Iteration 36, loss = 0.03111054
Iteration 37, loss = 0.03088397
Iteration 38, loss = 0.03040052
Iteration 39, loss = 0.03006771
Iteration 40, loss = 0.02997932
Iteration 41, loss = 0.02983040
Iteration 42, loss = 0.02982194
Iteration 43, loss = 0.02946730
Iteration 44, loss = 0.02923502
Iteration 45, loss = 0.02917690
Iteration 46, loss = 0.02888819
Iteration 47, loss = 0.02844272
Iteration 48, loss = 0.02893894
Iteration 49, loss = 0.02859302
Iteration 50, loss = 0.02835336
Iteration 51, loss = 0.02855901
Iteration 52, loss = 0.02831775
Iteration 53, loss = 0.02828475
Iteration 54, loss = 0.02803712
Iteration 55, loss = 0.02812971
Iteration 56, loss = 0.02762161
Iteration 57, loss = 0.02800345
Iteration 58, loss = 0.02764371
Iteration 59, loss = 0.02766122
Iteration 60, loss = 0.02744583
Iteration 61, loss = 0.02746371
Iteration 62, loss = 0.02723967
Iteration 63, loss = 0.02741659
Iteration 64, loss = 0.02722488
Iteration 65, loss = 0.02727558
Iteration 66, loss = 0.02760766
Iteration 67, loss = 0.02728480
Iteration 68, loss = 0.02738728
Iteration 69, loss = 0.02729785
Iteration 70, loss = 0.02698237
Iteration 71, loss = 0.02709900
Iteration 72, loss = 0.02731827
Iteration 73, loss = 0.02717828
Iteration 74, loss = 0.02670614
Iteration 75, loss = 0.02686202
Iteration 76, loss = 0.02674138
Iteration 77, loss = 0.02688120
Iteration 78, loss = 0.02662370
Iteration 79, loss = 0.02704757
Iteration 80, loss = 0.02675948
Iteration 81, loss = 0.02698782
Iteration 82, loss = 0.02659742
Iteration 83, loss = 0.02652112
Iteration 84, loss = 0.02658906
Iteration 85, loss = 0.02662398
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67336023
Iteration 2, loss = 0.60326762
Iteration 3, loss = 0.50033480
Iteration 4, loss = 0.38849110
Iteration 5, loss = 0.29195688
Iteration 6, loss = 0.21971345
Iteration 7, loss = 0.16862650
Iteration 8, loss = 0.13333278
Iteration 9, loss = 0.10854156
Iteration 10, loss = 0.09109623
Iteration 11, loss = 0.07816499
Iteration 12, loss = 0.06853481
Iteration 13, loss = 0.06125950
Iteration 14, loss = 0.05529524
Iteration 15, loss = 0.05052465
Iteration 16, loss = 0.04663874
Iteration 17, loss = 0.04356343
Iteration 18, loss = 0.04084519
Iteration 19, loss = 0.03884169
Iteration 20, loss = 0.03684128
Iteration 21, loss = 0.03514480
Iteration 22, loss = 0.03398007
Iteration 23, loss = 0.03271559
Iteration 24, loss = 0.03157734
Iteration 25, loss = 0.03065007
Iteration 26, loss = 0.02989402
Iteration 27, loss = 0.02932604
Iteration 28, loss = 0.02830735
Iteration 29, loss = 0.02792861
Iteration 30, loss = 0.02740400
Iteration 31, loss = 0.02686504
Iteration 32, loss = 0.02629210
Iteration 33, loss = 0.02623176
Iteration 34, loss = 0.02553322
Iteration 35, loss = 0.02539740
Iteration 36, loss = 0.02503904
Iteration 37, loss = 0.02487270
Iteration 38, loss = 0.02460529
Iteration 39, loss = 0.02466000
Iteration 40, loss = 0.02417959
Iteration 41, loss = 0.02404546
Iteration 42, loss = 0.02409100
Iteration 43, loss = 0.02368799
Iteration 44, loss = 0.02346607
Iteration 45, loss = 0.02343507
Iteration 46, loss = 0.02327598
Iteration 47, loss = 0.02369184
Iteration 48, loss = 0.02268123
Iteration 49, loss = 0.02293680
Iteration 50, loss = 0.02287328
Iteration 51, loss = 0.02290706
Iteration 52, loss = 0.02241469
Iteration 53, loss = 0.02266101
Iteration 54, loss = 0.02249233
Iteration 55, loss = 0.02257031
Iteration 56, loss = 0.02229821
Iteration 57, loss = 0.02252794
Iteration 58, loss = 0.02209017
Iteration 59, loss = 0.02200653
Iteration 60, loss = 0.02219748
Iteration 61, loss = 0.02206366
Iteration 62, loss = 0.02184082
Iteration 63, loss = 0.02212086
Iteration 64, loss = 0.02184242
Iteration 65, loss = 0.02161812
Iteration 66, loss = 0.02174481
Iteration 67, loss = 0.02183570
Iteration 68, loss = 0.02182486
Iteration 69, loss = 0.02131701
Iteration 70, loss = 0.02145553
Iteration 71, loss = 0.02174691
Iteration 72, loss = 0.02146873
Iteration 73, loss = 0.02153997
Iteration 74, loss = 0.02155473
Iteration 75, loss = 0.02147554
Iteration 76, loss = 0.02181722
Iteration 77, loss = 0.02154031
Iteration 78, loss = 0.02158040
Iteration 79, loss = 0.02127322
Iteration 80, loss = 0.02115266
Iteration 81, loss = 0.02133751
Iteration 82, loss = 0.02142379
Iteration 83, loss = 0.02112255
Iteration 84, loss = 0.02168214
Iteration 85, loss = 0.02112461
Iteration 86, loss = 0.02153264
Iteration 87, loss = 0.02113019
Iteration 88, loss = 0.02131679
Iteration 89, loss = 0.02109793
Iteration 90, loss = 0.02107114
Iteration 91, loss = 0.02140909
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67635298
Iteration 2, loss = 0.61186943
Iteration 3, loss = 0.50486471
Iteration 4, loss = 0.39588679
Iteration 5, loss = 0.30077257
Iteration 6, loss = 0.22823763
Iteration 7, loss = 0.17590938
Iteration 8, loss = 0.13945401
Iteration 9, loss = 0.11411005
Iteration 10, loss = 0.09597307
Iteration 11, loss = 0.08258171
Iteration 12, loss = 0.07227932
Iteration 13, loss = 0.06463120
Iteration 14, loss = 0.05868464
Iteration 15, loss = 0.05362018
Iteration 16, loss = 0.04973026
Iteration 17, loss = 0.04649052
Iteration 18, loss = 0.04387191
Iteration 19, loss = 0.04150494
Iteration 20, loss = 0.03964647
Iteration 21, loss = 0.03799972
Iteration 22, loss = 0.03656977
Iteration 23, loss = 0.03522808
Iteration 24, loss = 0.03429952
Iteration 25, loss = 0.03334758
Iteration 26, loss = 0.03257549
Iteration 27, loss = 0.03154366
Iteration 28, loss = 0.03094715
Iteration 29, loss = 0.03030580
Iteration 30, loss = 0.02981552
Iteration 31, loss = 0.02906921
Iteration 32, loss = 0.02890472
Iteration 33, loss = 0.02837295
Iteration 34, loss = 0.02813547
Iteration 35, loss = 0.02777149
Iteration 36, loss = 0.02752379
Iteration 37, loss = 0.02712983
Iteration 38, loss = 0.02714626
Iteration 39, loss = 0.02653452
Iteration 40, loss = 0.02650338
Iteration 41, loss = 0.02636578
Iteration 42, loss = 0.02595963
Iteration 43, loss = 0.02589087
Iteration 44, loss = 0.02594832
Iteration 45, loss = 0.02569228
Iteration 46, loss = 0.02541004
Iteration 47, loss = 0.02516694
Iteration 48, loss = 0.02528681
Iteration 49, loss = 0.02507930
Iteration 50, loss = 0.02510232
Iteration 51, loss = 0.02492347
Iteration 52, loss = 0.02477045
Iteration 53, loss = 0.02497960
Iteration 54, loss = 0.02489614
Iteration 55, loss = 0.02474110
Iteration 56, loss = 0.02460002
Iteration 57, loss = 0.02438635
Iteration 58, loss = 0.02441642
Iteration 59, loss = 0.02414729
Iteration 60, loss = 0.02436808
Iteration 61, loss = 0.02428036
Iteration 62, loss = 0.02432799
Iteration 63, loss = 0.02418779
Iteration 64, loss = 0.02392510
Iteration 65, loss = 0.02402328
Iteration 66, loss = 0.02454641
Iteration 67, loss = 0.02400546
Iteration 68, loss = 0.02409480
Iteration 69, loss = 0.02404104
Iteration 70, loss = 0.02372989
Iteration 71, loss = 0.02381738
Iteration 72, loss = 0.02384184
Iteration 73, loss = 0.02375181
Iteration 74, loss = 0.02369178
Iteration 75, loss = 0.02366691
Iteration 76, loss = 0.02377722
Iteration 77, loss = 0.02372754
Iteration 78, loss = 0.02358414
Iteration 79, loss = 0.02344950
Iteration 80, loss = 0.02348967
Iteration 81, loss = 0.02327811
Iteration 82, loss = 0.02348244
Iteration 83, loss = 0.02341560
Iteration 84, loss = 0.02359075
Iteration 85, loss = 0.02349291
Iteration 86, loss = 0.02321645
Iteration 87, loss = 0.02347603
Iteration 88, loss = 0.02366358
Iteration 89, loss = 0.02360565
Iteration 90, loss = 0.02333575
Iteration 91, loss = 0.02327762
Iteration 92, loss = 0.02313646
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70393776
Iteration 2, loss = 0.62907738
Iteration 3, loss = 0.51816283
Iteration 4, loss = 0.39857074
Iteration 5, loss = 0.29919314
Iteration 6, loss = 0.22503505
Iteration 7, loss = 0.17324714
Iteration 8, loss = 0.13738718
Iteration 9, loss = 0.11250095
Iteration 10, loss = 0.09503745
Iteration 11, loss = 0.08191342
Iteration 12, loss = 0.07261644
Iteration 13, loss = 0.06500632
Iteration 14, loss = 0.05922121
Iteration 15, loss = 0.05452259
Iteration 16, loss = 0.05087606
Iteration 17, loss = 0.04800358
Iteration 18, loss = 0.04533397
Iteration 19, loss = 0.04311764
Iteration 20, loss = 0.04149621
Iteration 21, loss = 0.03980834
Iteration 22, loss = 0.03865304
Iteration 23, loss = 0.03750935
Iteration 24, loss = 0.03605692
Iteration 25, loss = 0.03549678
Iteration 26, loss = 0.03462696
Iteration 27, loss = 0.03395803
Iteration 28, loss = 0.03359752
Iteration 29, loss = 0.03297150
Iteration 30, loss = 0.03238308
Iteration 31, loss = 0.03194150
Iteration 32, loss = 0.03162035
Iteration 33, loss = 0.03100792
Iteration 34, loss = 0.03086787
Iteration 35, loss = 0.03073347
Iteration 36, loss = 0.03020784
Iteration 37, loss = 0.02996116
Iteration 38, loss = 0.02990188
Iteration 39, loss = 0.02966894
Iteration 40, loss = 0.02941451
Iteration 41, loss = 0.02919324
Iteration 42, loss = 0.02908330
Iteration 43, loss = 0.02910178
Iteration 44, loss = 0.02854724
Iteration 45, loss = 0.02890075
Iteration 46, loss = 0.02849706
Iteration 47, loss = 0.02842010
Iteration 48, loss = 0.02835584
Iteration 49, loss = 0.02807547
Iteration 50, loss = 0.02836795
Iteration 51, loss = 0.02814169
Iteration 52, loss = 0.02809502
Iteration 53, loss = 0.02782602
Iteration 54, loss = 0.02781832
Iteration 55, loss = 0.02780211
Iteration 56, loss = 0.02773356
Iteration 57, loss = 0.02811542
Iteration 58, loss = 0.02734964
Iteration 59, loss = 0.02756471
Iteration 60, loss = 0.02764567
Iteration 61, loss = 0.02757725
Iteration 62, loss = 0.02742625
Iteration 63, loss = 0.02718596
Iteration 64, loss = 0.02733337
Iteration 65, loss = 0.02724558
Iteration 66, loss = 0.02760567
Iteration 67, loss = 0.02720137
Iteration 68, loss = 0.02722499
Iteration 69, loss = 0.02747859
Iteration 70, loss = 0.02691664
Iteration 71, loss = 0.02684947
Iteration 72, loss = 0.02715414
Iteration 73, loss = 0.02712474
Iteration 74, loss = 0.02707320
Iteration 75, loss = 0.02692289
Iteration 76, loss = 0.02669649
Iteration 77, loss = 0.02693585
Iteration 78, loss = 0.02670577
Iteration 79, loss = 0.02693392
Iteration 80, loss = 0.02690050
Iteration 81, loss = 0.02676742
Iteration 82, loss = 0.02686124
Iteration 83, loss = 0.02701414
Iteration 84, loss = 0.02665174
Iteration 85, loss = 0.02651486
Iteration 86, loss = 0.02668587
Iteration 87, loss = 0.02661350
Iteration 88, loss = 0.02675176
Iteration 89, loss = 0.02676560
Iteration 90, loss = 0.02674057
Iteration 91, loss = 0.02660266
Iteration 92, loss = 0.02647604
Iteration 93, loss = 0.02642691
Iteration 94, loss = 0.02663227
Iteration 95, loss = 0.02664859
Iteration 96, loss = 0.02673878
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.71188861
Iteration 2, loss = 0.67029938
Iteration 3, loss = 0.61408293
Iteration 4, loss = 0.54911315
Iteration 5, loss = 0.47885592
Iteration 6, loss = 0.40944210
Iteration 7, loss = 0.34618429
Iteration 8, loss = 0.29086372
Iteration 9, loss = 0.24418086
Iteration 10, loss = 0.20566504
Iteration 11, loss = 0.17440550
Iteration 12, loss = 0.14957059
Iteration 13, loss = 0.12960254
Iteration 14, loss = 0.11355176
Iteration 15, loss = 0.10047634
Iteration 16, loss = 0.08979402
Iteration 17, loss = 0.08101146
Iteration 18, loss = 0.07382322
Iteration 19, loss = 0.06786206
Iteration 20, loss = 0.06257089
Iteration 21, loss = 0.05826906
Iteration 22, loss = 0.05429637
Iteration 23, loss = 0.05128744
Iteration 24, loss = 0.04836061
Iteration 25, loss = 0.04577367
Iteration 26, loss = 0.04370226
Iteration 27, loss = 0.04159731
Iteration 28, loss = 0.03995608
Iteration 29, loss = 0.03865833
Iteration 30, loss = 0.03701747
Iteration 31, loss = 0.03575446
Iteration 32, loss = 0.03470305
Iteration 33, loss = 0.03378406
Iteration 34, loss = 0.03278175
Iteration 35, loss = 0.03184936
Iteration 36, loss = 0.03115397
Iteration 37, loss = 0.03037819
Iteration 38, loss = 0.02961678
Iteration 39, loss = 0.02909457
Iteration 40, loss = 0.02855021
Iteration 41, loss = 0.02800394
Iteration 42, loss = 0.02753088
Iteration 43, loss = 0.02704464
Iteration 44, loss = 0.02680528
Iteration 45, loss = 0.02650418
Iteration 46, loss = 0.02579785
Iteration 47, loss = 0.02549541
Iteration 48, loss = 0.02522258
Iteration 49, loss = 0.02483770
Iteration 50, loss = 0.02463724
Iteration 51, loss = 0.02446280
Iteration 52, loss = 0.02414572
Iteration 53, loss = 0.02387258
Iteration 54, loss = 0.02366421
Iteration 55, loss = 0.02345200
Iteration 56, loss = 0.02329952
Iteration 57, loss = 0.02308828
Iteration 58, loss = 0.02273866
Iteration 59, loss = 0.02274336
Iteration 60, loss = 0.02243402
Iteration 61, loss = 0.02242910
Iteration 62, loss = 0.02237474
Iteration 63, loss = 0.02236250
Iteration 64, loss = 0.02203971
Iteration 65, loss = 0.02202583
Iteration 66, loss = 0.02174318
Iteration 67, loss = 0.02161485
Iteration 68, loss = 0.02166140
Iteration 69, loss = 0.02138769
Iteration 70, loss = 0.02129719
Iteration 71, loss = 0.02135355
Iteration 72, loss = 0.02100091
Iteration 73, loss = 0.02106447
Iteration 74, loss = 0.02099881
Iteration 75, loss = 0.02094154
Iteration 76, loss = 0.02077227
Iteration 77, loss = 0.02065157
Iteration 78, loss = 0.02075614
Iteration 79, loss = 0.02051944
Iteration 80, loss = 0.02048495
Iteration 81, loss = 0.02058637
Iteration 82, loss = 0.02048774
Iteration 83, loss = 0.02031177
Iteration 84, loss = 0.02028214
Iteration 85, loss = 0.02044058
Iteration 86, loss = 0.02033315
Iteration 87, loss = 0.02002988
Iteration 88, loss = 0.02015843
Iteration 89, loss = 0.02009334
Iteration 90, loss = 0.02006775
Iteration 91, loss = 0.01988498
Iteration 92, loss = 0.01999782
Iteration 93, loss = 0.01985581
Iteration 94, loss = 0.01976831
Iteration 95, loss = 0.01980331
Iteration 96, loss = 0.01973270
Iteration 97, loss = 0.01962317
Iteration 98, loss = 0.01974631
Iteration 99, loss = 0.01967947
Iteration 100, loss = 0.01984954
Iteration 101, loss = 0.01964475
Iteration 102, loss = 0.01963972
Iteration 103, loss = 0.01961796
Iteration 104, loss = 0.01955665
Iteration 105, loss = 0.01957348
Iteration 106, loss = 0.01940611
Iteration 107, loss = 0.01935625
Iteration 108, loss = 0.01968259
Iteration 109, loss = 0.01945387
Iteration 110, loss = 0.01926769
Iteration 111, loss = 0.01929791
Iteration 112, loss = 0.01937012
Iteration 113, loss = 0.01925880
Iteration 114, loss = 0.01931085
Iteration 115, loss = 0.01921855
Iteration 116, loss = 0.01927799
Iteration 117, loss = 0.01910089
Iteration 118, loss = 0.01914854
Iteration 119, loss = 0.01912738
Iteration 120, loss = 0.01944863
Iteration 121, loss = 0.01934964
Iteration 122, loss = 0.01909422
Iteration 123, loss = 0.01909336
Iteration 124, loss = 0.01910279
Iteration 125, loss = 0.01909277
Iteration 126, loss = 0.01928918
Iteration 127, loss = 0.01901026
Iteration 128, loss = 0.01906642
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69194626
Iteration 2, loss = 0.62793830
Iteration 3, loss = 0.53986617
Iteration 4, loss = 0.43392640
Iteration 5, loss = 0.33910231
Iteration 6, loss = 0.26305030
Iteration 7, loss = 0.20544761
Iteration 8, loss = 0.16420017
Iteration 9, loss = 0.13425095
Iteration 10, loss = 0.11275913
Iteration 11, loss = 0.09640225
Iteration 12, loss = 0.08418149
Iteration 13, loss = 0.07478675
Iteration 14, loss = 0.06737766
Iteration 15, loss = 0.06130903
Iteration 16, loss = 0.05689120
Iteration 17, loss = 0.05272944
Iteration 18, loss = 0.04930375
Iteration 19, loss = 0.04663357
Iteration 20, loss = 0.04429815
Iteration 21, loss = 0.04201699
Iteration 22, loss = 0.04026607
Iteration 23, loss = 0.03868863
Iteration 24, loss = 0.03732528
Iteration 25, loss = 0.03592136
Iteration 26, loss = 0.03477719
Iteration 27, loss = 0.03390896
Iteration 28, loss = 0.03320415
Iteration 29, loss = 0.03250961
Iteration 30, loss = 0.03189244
Iteration 31, loss = 0.03123352
Iteration 32, loss = 0.03083536
Iteration 33, loss = 0.03083527
Iteration 34, loss = 0.03022508
Iteration 35, loss = 0.03003898
Iteration 36, loss = 0.02951252
Iteration 37, loss = 0.02898855
Iteration 38, loss = 0.02893938
Iteration 39, loss = 0.02888506
Iteration 40, loss = 0.02890474
Iteration 41, loss = 0.02896386
Iteration 42, loss = 0.02834025
Iteration 43, loss = 0.02796077
Iteration 44, loss = 0.02740294
Iteration 45, loss = 0.02694294
Iteration 46, loss = 0.02693692
Iteration 47, loss = 0.02641228
Iteration 48, loss = 0.02608228
Iteration 49, loss = 0.02603951
Iteration 50, loss = 0.02585580
Iteration 51, loss = 0.02568901
Iteration 52, loss = 0.02552214
Iteration 53, loss = 0.02532864
Iteration 54, loss = 0.02536759
Iteration 55, loss = 0.02511600
Iteration 56, loss = 0.02499534
Iteration 57, loss = 0.02478478
Iteration 58, loss = 0.02481693
Iteration 59, loss = 0.02491631
Iteration 60, loss = 0.02490228
Iteration 61, loss = 0.02472055
Iteration 62, loss = 0.02483077
Iteration 63, loss = 0.02479144
Iteration 64, loss = 0.02519506
Iteration 65, loss = 0.02504376
Iteration 66, loss = 0.02465717
Iteration 67, loss = 0.02446230
Iteration 68, loss = 0.02432360
Iteration 69, loss = 0.02414971
Iteration 70, loss = 0.02411400
Iteration 71, loss = 0.02396963
Iteration 72, loss = 0.02452222
Iteration 73, loss = 0.02452728
Iteration 74, loss = 0.02499311
Iteration 75, loss = 0.02470242
Iteration 76, loss = 0.02470787
Iteration 77, loss = 0.02474184
Iteration 78, loss = 0.02459090
Iteration 79, loss = 0.02434053
Iteration 80, loss = 0.02400611
Iteration 81, loss = 0.02403991
Iteration 82, loss = 0.02372834
Iteration 83, loss = 0.02363015
Iteration 84, loss = 0.02417337
Iteration 85, loss = 0.02408180
Iteration 86, loss = 0.02418124
Iteration 87, loss = 0.02366740
Iteration 88, loss = 0.02361336
Iteration 89, loss = 0.02345363
Iteration 90, loss = 0.02352028
Iteration 91, loss = 0.02343392
Iteration 92, loss = 0.02343449
Iteration 93, loss = 0.02323799
Iteration 94, loss = 0.02332203
Iteration 95, loss = 0.02369161
Iteration 96, loss = 0.02360378
Iteration 97, loss = 0.02395577
Iteration 98, loss = 0.02330613
Iteration 99, loss = 0.02334383
Iteration 100, loss = 0.02315638
Iteration 101, loss = 0.02313956
Iteration 102, loss = 0.02307374
Iteration 103, loss = 0.02300562
Iteration 104, loss = 0.02314672
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70956786
Iteration 2, loss = 0.68415415
Iteration 3, loss = 0.66124241
Iteration 4, loss = 0.63562971
Iteration 5, loss = 0.60704149
Iteration 6, loss = 0.57526724
Iteration 7, loss = 0.54138794
Iteration 8, loss = 0.50604705
Iteration 9, loss = 0.47024881
Iteration 10, loss = 0.43464955
Iteration 11, loss = 0.40016602
Iteration 12, loss = 0.36678191
Iteration 13, loss = 0.33552279
Iteration 14, loss = 0.30627003
Iteration 15, loss = 0.27935331
Iteration 16, loss = 0.25441760
Iteration 17, loss = 0.23197343
Iteration 18, loss = 0.21160351
Iteration 19, loss = 0.19320508
Iteration 20, loss = 0.17675118
Iteration 21, loss = 0.16204047
Iteration 22, loss = 0.14880295
Iteration 23, loss = 0.13714883
Iteration 24, loss = 0.12662883
Iteration 25, loss = 0.11728646
Iteration 26, loss = 0.10887277
Iteration 27, loss = 0.10128767
Iteration 28, loss = 0.09460981
Iteration 29, loss = 0.08855381
Iteration 30, loss = 0.08308185
Iteration 31, loss = 0.07823095
Iteration 32, loss = 0.07374667
Iteration 33, loss = 0.06972754
Iteration 34, loss = 0.06602866
Iteration 35, loss = 0.06261226
Iteration 36, loss = 0.05958255
Iteration 37, loss = 0.05679770
Iteration 38, loss = 0.05417298
Iteration 39, loss = 0.05178949
Iteration 40, loss = 0.04961765
Iteration 41, loss = 0.04764244
Iteration 42, loss = 0.04578500
Iteration 43, loss = 0.04396962
Iteration 44, loss = 0.04239208
Iteration 45, loss = 0.04087771
Iteration 46, loss = 0.03958739
Iteration 47, loss = 0.03819032
Iteration 48, loss = 0.03690549
Iteration 49, loss = 0.03580327
Iteration 50, loss = 0.03471053
Iteration 51, loss = 0.03369398
Iteration 52, loss = 0.03280591
Iteration 53, loss = 0.03186176
Iteration 54, loss = 0.03100254
Iteration 55, loss = 0.03023511
Iteration 56, loss = 0.02955168
Iteration 57, loss = 0.02883207
Iteration 58, loss = 0.02812987
Iteration 59, loss = 0.02748937
Iteration 60, loss = 0.02687317
Iteration 61, loss = 0.02630044
Iteration 62, loss = 0.02572784
Iteration 63, loss = 0.02530164
Iteration 64, loss = 0.02481805
Iteration 65, loss = 0.02437179
Iteration 66, loss = 0.02388393
Iteration 67, loss = 0.02347558
Iteration 68, loss = 0.02301628
Iteration 69, loss = 0.02260755
Iteration 70, loss = 0.02223930
Iteration 71, loss = 0.02194508
Iteration 72, loss = 0.02153265
Iteration 73, loss = 0.02124460
Iteration 74, loss = 0.02094009
Iteration 75, loss = 0.02062590
Iteration 76, loss = 0.02032413
Iteration 77, loss = 0.02007743
Iteration 78, loss = 0.01982286
Iteration 79, loss = 0.01961672
Iteration 80, loss = 0.01931711
Iteration 81, loss = 0.01907679
Iteration 82, loss = 0.01885508
Iteration 83, loss = 0.01863694
Iteration 84, loss = 0.01838621
Iteration 85, loss = 0.01820797
Iteration 86, loss = 0.01804402
Iteration 87, loss = 0.01784874
Iteration 88, loss = 0.01758222
Iteration 89, loss = 0.01750318
Iteration 90, loss = 0.01726779
Iteration 91, loss = 0.01712379
Iteration 92, loss = 0.01700639
Iteration 93, loss = 0.01686537
Iteration 94, loss = 0.01662598
Iteration 95, loss = 0.01654296
Iteration 96, loss = 0.01635358
Iteration 97, loss = 0.01623086
Iteration 98, loss = 0.01610324
Iteration 99, loss = 0.01595085
Iteration 100, loss = 0.01583067
Iteration 101, loss = 0.01574546
Iteration 102, loss = 0.01563210
Iteration 103, loss = 0.01554079
Iteration 104, loss = 0.01542182
Iteration 105, loss = 0.01528242
Iteration 106, loss = 0.01520591
Iteration 107, loss = 0.01505953
Iteration 108, loss = 0.01497855
Iteration 109, loss = 0.01487811
Iteration 110, loss = 0.01486554
Iteration 111, loss = 0.01473747
Iteration 112, loss = 0.01461231
Iteration 113, loss = 0.01456478
Iteration 114, loss = 0.01441994
Iteration 115, loss = 0.01442940
Iteration 116, loss = 0.01424105
Iteration 117, loss = 0.01419335
Iteration 118, loss = 0.01414189
Iteration 119, loss = 0.01403287
Iteration 120, loss = 0.01401783
Iteration 121, loss = 0.01390981
Iteration 122, loss = 0.01389034
Iteration 123, loss = 0.01378574
Iteration 124, loss = 0.01367457
Iteration 125, loss = 0.01362963
Iteration 126, loss = 0.01362736
Iteration 127, loss = 0.01354831
Iteration 128, loss = 0.01345258
Iteration 129, loss = 0.01341061
Iteration 130, loss = 0.01341861
Iteration 131, loss = 0.01335752
Iteration 132, loss = 0.01319351
Iteration 133, loss = 0.01325969
Iteration 134, loss = 0.01316112
Iteration 135, loss = 0.01306282
Iteration 136, loss = 0.01314219
Iteration 137, loss = 0.01307474
Iteration 138, loss = 0.01293822
Iteration 139, loss = 0.01286865
Iteration 140, loss = 0.01283240
Iteration 141, loss = 0.01280473
Iteration 142, loss = 0.01276644
Iteration 143, loss = 0.01274814
Iteration 144, loss = 0.01270592
Iteration 145, loss = 0.01260504
Iteration 146, loss = 0.01260752
Iteration 147, loss = 0.01259319
Iteration 148, loss = 0.01257300
Iteration 149, loss = 0.01247980
Iteration 150, loss = 0.01245124
Iteration 151, loss = 0.01244632
Iteration 152, loss = 0.01243715
Iteration 153, loss = 0.01232875
Iteration 154, loss = 0.01231451
Iteration 155, loss = 0.01232664
Iteration 156, loss = 0.01222721
Iteration 157, loss = 0.01219925
Iteration 158, loss = 0.01212691
Iteration 159, loss = 0.01214681
Iteration 160, loss = 0.01209043
Iteration 161, loss = 0.01210256
Iteration 162, loss = 0.01206370
Iteration 163, loss = 0.01201420
Iteration 164, loss = 0.01197892
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69105574
Iteration 2, loss = 0.65259063
Iteration 3, loss = 0.60228579
Iteration 4, loss = 0.53825835
Iteration 5, loss = 0.46805435
Iteration 6, loss = 0.39966121
Iteration 7, loss = 0.33703571
Iteration 8, loss = 0.28185193
Iteration 9, loss = 0.23582116
Iteration 10, loss = 0.19796318
Iteration 11, loss = 0.16738631
Iteration 12, loss = 0.14311446
Iteration 13, loss = 0.12367015
Iteration 14, loss = 0.10814911
Iteration 15, loss = 0.09556500
Iteration 16, loss = 0.08533868
Iteration 17, loss = 0.07696155
Iteration 18, loss = 0.06992957
Iteration 19, loss = 0.06387823
Iteration 20, loss = 0.05904755
Iteration 21, loss = 0.05468622
Iteration 22, loss = 0.05110084
Iteration 23, loss = 0.04788560
Iteration 24, loss = 0.04515275
Iteration 25, loss = 0.04295070
Iteration 26, loss = 0.04073105
Iteration 27, loss = 0.03884057
Iteration 28, loss = 0.03715238
Iteration 29, loss = 0.03570000
Iteration 30, loss = 0.03441152
Iteration 31, loss = 0.03320152
Iteration 32, loss = 0.03217924
Iteration 33, loss = 0.03107129
Iteration 34, loss = 0.03030245
Iteration 35, loss = 0.02939612
Iteration 36, loss = 0.02868206
Iteration 37, loss = 0.02811207
Iteration 38, loss = 0.02732689
Iteration 39, loss = 0.02683462
Iteration 40, loss = 0.02627042
Iteration 41, loss = 0.02574991
Iteration 42, loss = 0.02534032
Iteration 43, loss = 0.02487860
Iteration 44, loss = 0.02447396
Iteration 45, loss = 0.02419598
Iteration 46, loss = 0.02380398
Iteration 47, loss = 0.02349236
Iteration 48, loss = 0.02327156
Iteration 49, loss = 0.02283305
Iteration 50, loss = 0.02252522
Iteration 51, loss = 0.02254083
Iteration 52, loss = 0.02235769
Iteration 53, loss = 0.02198468
Iteration 54, loss = 0.02194889
Iteration 55, loss = 0.02161270
Iteration 56, loss = 0.02133369
Iteration 57, loss = 0.02124508
Iteration 58, loss = 0.02114641
Iteration 59, loss = 0.02103578
Iteration 60, loss = 0.02079539
Iteration 61, loss = 0.02056025
Iteration 62, loss = 0.02056964
Iteration 63, loss = 0.02033086
Iteration 64, loss = 0.02025628
Iteration 65, loss = 0.01997993
Iteration 66, loss = 0.01987928
Iteration 67, loss = 0.01977289
Iteration 68, loss = 0.01981241
Iteration 69, loss = 0.01973432
Iteration 70, loss = 0.01965350
Iteration 71, loss = 0.01954133
Iteration 72, loss = 0.01954392
Iteration 73, loss = 0.01940245
Iteration 74, loss = 0.01938766
Iteration 75, loss = 0.01925170
Iteration 76, loss = 0.01915731
Iteration 77, loss = 0.01904436
Iteration 78, loss = 0.01916557
Iteration 79, loss = 0.01893796
Iteration 80, loss = 0.01895442
Iteration 81, loss = 0.01892867
Iteration 82, loss = 0.01890223
Iteration 83, loss = 0.01873204
Iteration 84, loss = 0.01879933
Iteration 85, loss = 0.01864543
Iteration 86, loss = 0.01858058
Iteration 87, loss = 0.01852329
Iteration 88, loss = 0.01845800
Iteration 89, loss = 0.01849328
Iteration 90, loss = 0.01839108
Iteration 91, loss = 0.01835355
Iteration 92, loss = 0.01832121
Iteration 93, loss = 0.01822412
Iteration 94, loss = 0.01830843
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.69431690
Iteration 2, loss = 0.61358189
Iteration 3, loss = 0.50606308
Iteration 4, loss = 0.39089891
Iteration 5, loss = 0.29408252
Iteration 6, loss = 0.22149534
Iteration 7, loss = 0.17049578
Iteration 8, loss = 0.13500674
Iteration 9, loss = 0.11075145
Iteration 10, loss = 0.09318155
Iteration 11, loss = 0.08048451
Iteration 12, loss = 0.07083563
Iteration 13, loss = 0.06348285
Iteration 14, loss = 0.05760644
Iteration 15, loss = 0.05306776
Iteration 16, loss = 0.04953792
Iteration 17, loss = 0.04620293
Iteration 18, loss = 0.04360931
Iteration 19, loss = 0.04171320
Iteration 20, loss = 0.03980398
Iteration 21, loss = 0.03834227
Iteration 22, loss = 0.03665893
Iteration 23, loss = 0.03559558
Iteration 24, loss = 0.03460991
Iteration 25, loss = 0.03382645
Iteration 26, loss = 0.03287364
Iteration 27, loss = 0.03209175
Iteration 28, loss = 0.03169278
Iteration 29, loss = 0.03119539
Iteration 30, loss = 0.03045056
Iteration 31, loss = 0.02997806
Iteration 32, loss = 0.02962776
Iteration 33, loss = 0.02946768
Iteration 34, loss = 0.02909900
Iteration 35, loss = 0.02892274
Iteration 36, loss = 0.02865207
Iteration 37, loss = 0.02809355
Iteration 38, loss = 0.02812471
Iteration 39, loss = 0.02773364
Iteration 40, loss = 0.02769553
Iteration 41, loss = 0.02728387
Iteration 42, loss = 0.02723554
Iteration 43, loss = 0.02695856
Iteration 44, loss = 0.02705829
Iteration 45, loss = 0.02670224
Iteration 46, loss = 0.02648001
Iteration 47, loss = 0.02649388
Iteration 48, loss = 0.02656390
Iteration 49, loss = 0.02615904
Iteration 50, loss = 0.02625281
Iteration 51, loss = 0.02622151
Iteration 52, loss = 0.02590523
Iteration 53, loss = 0.02561938
Iteration 54, loss = 0.02568461
Iteration 55, loss = 0.02568746
Iteration 56, loss = 0.02593929
Iteration 57, loss = 0.02558207
Iteration 58, loss = 0.02547501
Iteration 59, loss = 0.02534849
Iteration 60, loss = 0.02555875
Iteration 61, loss = 0.02543566
Iteration 62, loss = 0.02561838
Iteration 63, loss = 0.02556102
Iteration 64, loss = 0.02515952
Iteration 65, loss = 0.02529502
Iteration 66, loss = 0.02502059
Iteration 67, loss = 0.02524304
Iteration 68, loss = 0.02507452
Iteration 69, loss = 0.02519499
Iteration 70, loss = 0.02526338
Iteration 71, loss = 0.02475847
Iteration 72, loss = 0.02478587
Iteration 73, loss = 0.02491797
Iteration 74, loss = 0.02490040
Iteration 75, loss = 0.02495321
Iteration 76, loss = 0.02520992
Iteration 77, loss = 0.02486394
Iteration 78, loss = 0.02482739
Iteration 79, loss = 0.02481794
Iteration 80, loss = 0.02508459
Iteration 81, loss = 0.02447507
Iteration 82, loss = 0.02462305
Iteration 83, loss = 0.02493348
Iteration 84, loss = 0.02439340
Iteration 85, loss = 0.02467114
Iteration 86, loss = 0.02435494
Iteration 87, loss = 0.02465159
Iteration 88, loss = 0.02429059
Iteration 89, loss = 0.02445498
Iteration 90, loss = 0.02438246
Iteration 91, loss = 0.02439371
Iteration 92, loss = 0.02435111
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.70177239
Iteration 2, loss = 0.68805200
Iteration 3, loss = 0.67607393
Iteration 4, loss = 0.66386843
Iteration 5, loss = 0.65066149
Iteration 6, loss = 0.63648321
Iteration 7, loss = 0.62074732
Iteration 8, loss = 0.60374554
Iteration 9, loss = 0.58490093
Iteration 10, loss = 0.56498579
Iteration 11, loss = 0.54361428
Iteration 12, loss = 0.52121566
Iteration 13, loss = 0.49809693
Iteration 14, loss = 0.47455433
Iteration 15, loss = 0.45089504
Iteration 16, loss = 0.42723785
Iteration 17, loss = 0.40408289
Iteration 18, loss = 0.38139374
Iteration 19, loss = 0.35943262
Iteration 20, loss = 0.33829144
Iteration 21, loss = 0.31821395
Iteration 22, loss = 0.29888800
Iteration 23, loss = 0.28070868
Iteration 24, loss = 0.26340071
Iteration 25, loss = 0.24713144
Iteration 26, loss = 0.23186476
Iteration 27, loss = 0.21753166
Iteration 28, loss = 0.20417179
Iteration 29, loss = 0.19165281
Iteration 30, loss = 0.18006336
Iteration 31, loss = 0.16919754
Iteration 32, loss = 0.15917957
Iteration 33, loss = 0.14984858
Iteration 34, loss = 0.14113292
Iteration 35, loss = 0.13310413
Iteration 36, loss = 0.12564029
Iteration 37, loss = 0.11868855
Iteration 38, loss = 0.11229053
Iteration 39, loss = 0.10629503
Iteration 40, loss = 0.10079734
Iteration 41, loss = 0.09565853
Iteration 42, loss = 0.09086894
Iteration 43, loss = 0.08645359
Iteration 44, loss = 0.08233429
Iteration 45, loss = 0.07850323
Iteration 46, loss = 0.07492959
Iteration 47, loss = 0.07158499
Iteration 48, loss = 0.06847228
Iteration 49, loss = 0.06555304
Iteration 50, loss = 0.06281140
Iteration 51, loss = 0.06026915
Iteration 52, loss = 0.05787294
Iteration 53, loss = 0.05563511
Iteration 54, loss = 0.05351207
Iteration 55, loss = 0.05153893
Iteration 56, loss = 0.04966708
Iteration 57, loss = 0.04790202
Iteration 58, loss = 0.04624152
Iteration 59, loss = 0.04467118
Iteration 60, loss = 0.04319109
Iteration 61, loss = 0.04180349
Iteration 62, loss = 0.04046379
Iteration 63, loss = 0.03919349
Iteration 64, loss = 0.03799952
Iteration 65, loss = 0.03685994
Iteration 66, loss = 0.03576579
Iteration 67, loss = 0.03473110
Iteration 68, loss = 0.03373938
Iteration 69, loss = 0.03279846
Iteration 70, loss = 0.03190819
Iteration 71, loss = 0.03106352
Iteration 72, loss = 0.03024101
Iteration 73, loss = 0.02947923
Iteration 74, loss = 0.02871744
Iteration 75, loss = 0.02801932
Iteration 76, loss = 0.02734452
Iteration 77, loss = 0.02667916
Iteration 78, loss = 0.02606039
Iteration 79, loss = 0.02547552
Iteration 80, loss = 0.02489800
Iteration 81, loss = 0.02434394
Iteration 82, loss = 0.02381976
Iteration 83, loss = 0.02329652
Iteration 84, loss = 0.02281029
Iteration 85, loss = 0.02233502
Iteration 86, loss = 0.02187194
Iteration 87, loss = 0.02143873
Iteration 88, loss = 0.02101976
Iteration 89, loss = 0.02060348
Iteration 90, loss = 0.02021771
Iteration 91, loss = 0.01983772
Iteration 92, loss = 0.01947670
Iteration 93, loss = 0.01912778
Iteration 94, loss = 0.01879075
Iteration 95, loss = 0.01846010
Iteration 96, loss = 0.01814479
Iteration 97, loss = 0.01784050
Iteration 98, loss = 0.01753436
Iteration 99, loss = 0.01724427
Iteration 100, loss = 0.01696665
Iteration 101, loss = 0.01670202
Iteration 102, loss = 0.01644065
Iteration 103, loss = 0.01619451
Iteration 104, loss = 0.01594754
Iteration 105, loss = 0.01570515
Iteration 106, loss = 0.01547953
Iteration 107, loss = 0.01525216
Iteration 108, loss = 0.01503583
Iteration 109, loss = 0.01482700
Iteration 110, loss = 0.01462243
Iteration 111, loss = 0.01441236
Iteration 112, loss = 0.01422639
Iteration 113, loss = 0.01403347
Iteration 114, loss = 0.01384300
Iteration 115, loss = 0.01365945
Iteration 116, loss = 0.01348764
Iteration 117, loss = 0.01331381
Iteration 118, loss = 0.01314351
Iteration 119, loss = 0.01298272
Iteration 120, loss = 0.01283132
Iteration 121, loss = 0.01268495
Iteration 122, loss = 0.01253523
Iteration 123, loss = 0.01238587
Iteration 124, loss = 0.01224186
Iteration 125, loss = 0.01210138
Iteration 126, loss = 0.01196604
Iteration 127, loss = 0.01183771
Iteration 128, loss = 0.01170548
Iteration 129, loss = 0.01159551
Iteration 130, loss = 0.01146020
Iteration 131, loss = 0.01134613
Iteration 132, loss = 0.01122264
Iteration 133, loss = 0.01110813
Iteration 134, loss = 0.01100101
Iteration 135, loss = 0.01088542
Iteration 136, loss = 0.01077823
Iteration 137, loss = 0.01067341
Iteration 138, loss = 0.01057228
Iteration 139, loss = 0.01047094
Iteration 140, loss = 0.01037277
Iteration 141, loss = 0.01027657
Iteration 142, loss = 0.01018698
Iteration 143, loss = 0.01009374
Iteration 144, loss = 0.01000689
Iteration 145, loss = 0.00991303
Iteration 146, loss = 0.00983014
Iteration 147, loss = 0.00974785
Iteration 148, loss = 0.00967320
Iteration 149, loss = 0.00958808
Iteration 150, loss = 0.00950903
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.68853048
Iteration 2, loss = 0.65763986
Iteration 3, loss = 0.61808163
Iteration 4, loss = 0.56879941
Iteration 5, loss = 0.51329833
Iteration 6, loss = 0.45606689
Iteration 7, loss = 0.39982238
Iteration 8, loss = 0.34701895
Iteration 9, loss = 0.29935031
Iteration 10, loss = 0.25730094
Iteration 11, loss = 0.22118577
Iteration 12, loss = 0.19071844
Iteration 13, loss = 0.16530512
Iteration 14, loss = 0.14399222
Iteration 15, loss = 0.12645228
Iteration 16, loss = 0.11208581
Iteration 17, loss = 0.09997627
Iteration 18, loss = 0.08980237
Iteration 19, loss = 0.08124851
Iteration 20, loss = 0.07408036
Iteration 21, loss = 0.06787724
Iteration 22, loss = 0.06259758
Iteration 23, loss = 0.05801763
Iteration 24, loss = 0.05418889
Iteration 25, loss = 0.05065855
Iteration 26, loss = 0.04748779
Iteration 27, loss = 0.04483996
Iteration 28, loss = 0.04248583
Iteration 29, loss = 0.04034033
Iteration 30, loss = 0.03846583
Iteration 31, loss = 0.03676161
Iteration 32, loss = 0.03503763
Iteration 33, loss = 0.03380063
Iteration 34, loss = 0.03260830
Iteration 35, loss = 0.03131621
Iteration 36, loss = 0.03031574
Iteration 37, loss = 0.02928709
Iteration 38, loss = 0.02833954
Iteration 39, loss = 0.02749289
Iteration 40, loss = 0.02687714
Iteration 41, loss = 0.02625688
Iteration 42, loss = 0.02542277
Iteration 43, loss = 0.02493536
Iteration 44, loss = 0.02439707
Iteration 45, loss = 0.02392615
Iteration 46, loss = 0.02341061
Iteration 47, loss = 0.02302158
Iteration 48, loss = 0.02253536
Iteration 49, loss = 0.02206645
Iteration 50, loss = 0.02175523
Iteration 51, loss = 0.02140267
Iteration 52, loss = 0.02109823
Iteration 53, loss = 0.02076324
Iteration 54, loss = 0.02045353
Iteration 55, loss = 0.02017118
Iteration 56, loss = 0.01986820
Iteration 57, loss = 0.01973306
Iteration 58, loss = 0.01961277
Iteration 59, loss = 0.01931013
Iteration 60, loss = 0.01908053
Iteration 61, loss = 0.01887471
Iteration 62, loss = 0.01865431
Iteration 63, loss = 0.01846286
Iteration 64, loss = 0.01831824
Iteration 65, loss = 0.01813622
Iteration 66, loss = 0.01796912
Iteration 67, loss = 0.01784549
Iteration 68, loss = 0.01770588
Iteration 69, loss = 0.01769037
Iteration 70, loss = 0.01749641
Iteration 71, loss = 0.01737476
Iteration 72, loss = 0.01728276
Iteration 73, loss = 0.01728464
Iteration 74, loss = 0.01706675
Iteration 75, loss = 0.01699999
Iteration 76, loss = 0.01678147
Iteration 77, loss = 0.01679698
Iteration 78, loss = 0.01663971
Iteration 79, loss = 0.01655230
Iteration 80, loss = 0.01640883
Iteration 81, loss = 0.01633835
Iteration 82, loss = 0.01628909
Iteration 83, loss = 0.01640862
Iteration 84, loss = 0.01610447
Iteration 85, loss = 0.01612583
Iteration 86, loss = 0.01615324
Iteration 87, loss = 0.01604461
Iteration 88, loss = 0.01587086
Iteration 89, loss = 0.01580786
Iteration 90, loss = 0.01585736
Iteration 91, loss = 0.01581269
Iteration 92, loss = 0.01573691
Iteration 93, loss = 0.01557203
Iteration 94, loss = 0.01561382
Iteration 95, loss = 0.01556919
Iteration 96, loss = 0.01552162
Iteration 97, loss = 0.01557997
Iteration 98, loss = 0.01549080
Iteration 99, loss = 0.01532293
Iteration 100, loss = 0.01536253
Iteration 101, loss = 0.01534118
Iteration 102, loss = 0.01525137
Iteration 103, loss = 0.01533642
Iteration 104, loss = 0.01520984
Iteration 105, loss = 0.01523102
Iteration 106, loss = 0.01518637
Iteration 107, loss = 0.01507501
Iteration 108, loss = 0.01511475
Iteration 109, loss = 0.01500476
Iteration 110, loss = 0.01499993
Iteration 111, loss = 0.01497801
Iteration 112, loss = 0.01492616
Iteration 113, loss = 0.01488415
Iteration 114, loss = 0.01497703
Iteration 115, loss = 0.01504137
Iteration 116, loss = 0.01485036
Iteration 117, loss = 0.01480556
Iteration 118, loss = 0.01488187
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67657697
Iteration 2, loss = 0.61722712
Iteration 3, loss = 0.52382169
Iteration 4, loss = 0.41935663
Iteration 5, loss = 0.32392059
Iteration 6, loss = 0.24783160
Iteration 7, loss = 0.19218170
Iteration 8, loss = 0.15222787
Iteration 9, loss = 0.12433775
Iteration 10, loss = 0.10397676
Iteration 11, loss = 0.08905743
Iteration 12, loss = 0.07800439
Iteration 13, loss = 0.06941830
Iteration 14, loss = 0.06279856
Iteration 15, loss = 0.05754703
Iteration 16, loss = 0.05301785
Iteration 17, loss = 0.04971545
Iteration 18, loss = 0.04676696
Iteration 19, loss = 0.04433826
Iteration 20, loss = 0.04227604
Iteration 21, loss = 0.04047401
Iteration 22, loss = 0.03900376
Iteration 23, loss = 0.03749032
Iteration 24, loss = 0.03657625
Iteration 25, loss = 0.03519504
Iteration 26, loss = 0.03429874
Iteration 27, loss = 0.03342443
Iteration 28, loss = 0.03296795
Iteration 29, loss = 0.03221720
Iteration 30, loss = 0.03182147
Iteration 31, loss = 0.03113128
Iteration 32, loss = 0.03055467
Iteration 33, loss = 0.03018144
Iteration 34, loss = 0.02983514
Iteration 35, loss = 0.02956746
Iteration 36, loss = 0.02917214
Iteration 37, loss = 0.02884064
Iteration 38, loss = 0.02849042
Iteration 39, loss = 0.02830916
Iteration 40, loss = 0.02805230
Iteration 41, loss = 0.02773923
Iteration 42, loss = 0.02765110
Iteration 43, loss = 0.02752719
Iteration 44, loss = 0.02699116
Iteration 45, loss = 0.02731107
Iteration 46, loss = 0.02688339
Iteration 47, loss = 0.02698916
Iteration 48, loss = 0.02639136
Iteration 49, loss = 0.02670573
Iteration 50, loss = 0.02645496
Iteration 51, loss = 0.02634964
Iteration 52, loss = 0.02621488
Iteration 53, loss = 0.02640280
Iteration 54, loss = 0.02615201
Iteration 55, loss = 0.02601776
Iteration 56, loss = 0.02599198
Iteration 57, loss = 0.02592083
Iteration 58, loss = 0.02573255
Iteration 59, loss = 0.02572450
Iteration 60, loss = 0.02572073
Iteration 61, loss = 0.02562509
Iteration 62, loss = 0.02559576
Iteration 63, loss = 0.02566047
Iteration 64, loss = 0.02549221
Iteration 65, loss = 0.02537374
Iteration 66, loss = 0.02547843
Iteration 67, loss = 0.02552698
Iteration 68, loss = 0.02545893
Iteration 69, loss = 0.02515978
Iteration 70, loss = 0.02506435
Iteration 71, loss = 0.02492997
Iteration 72, loss = 0.02501286
Iteration 73, loss = 0.02517853
Iteration 74, loss = 0.02477957
Iteration 75, loss = 0.02497805
Iteration 76, loss = 0.02514921
Iteration 77, loss = 0.02498929
Iteration 78, loss = 0.02461877
Iteration 79, loss = 0.02495660
Iteration 80, loss = 0.02490402
Iteration 81, loss = 0.02482306
Iteration 82, loss = 0.02474362
Iteration 83, loss = 0.02460054
Iteration 84, loss = 0.02485826
Iteration 85, loss = 0.02481531
Iteration 86, loss = 0.02462035
Iteration 87, loss = 0.02499665
Iteration 88, loss = 0.02460400
Iteration 89, loss = 0.02450646
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.

real	81m2.490s
user	351m47.084s
sys	35m42.092s
Elapsed time: 4863 seconds
